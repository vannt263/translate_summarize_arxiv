{
  "article_text": [
    "let @xmath3 be @xmath4 pairs of input  output and assume that each pair has been independently drawn from the same unknown distribution @xmath5 .",
    "let @xmath6 denote the input space and let the output space be the set of real numbers @xmath7 , so that @xmath5 is a probability distribution on the product space @xmath8 .",
    "the target of learning algorithms is to predict the output @xmath9 associated with an input @xmath10 for pairs @xmath11 drawn from the distribution @xmath5 .",
    "the quality of a ( prediction ) function @xmath12 is measured by the least squares _ risk _ : @xmath13 ^ 2 \\ } .\\ ] ] through the paper , we assume that the output and all the prediction functions we consider are square integrable . let @xmath14 be a closed convex set of  @xmath15 , and @xmath16 be @xmath0 prediction functions . consider the regression model @xmath17 the best function @xmath18 in @xmath19 is defined by @xmath20 such a function always exists but is not necessarily unique .",
    "besides , it is unknown since the probability generating the data is unknown .    we will study the problem of predicting ( at least ) as well as function  @xmath18 . in other words ,",
    "we want to deduce from the observations @xmath21 a  function  @xmath22 having with high probability a risk bounded by the minimal risk @xmath23 on @xmath19 plus a small remainder term , which is typically of order @xmath1 up to a  possible logarithmic factor . except in particular settings ( e.g. , @xmath14 is a  simplex and @xmath24 ) , it is known that the convergence rate @xmath1 can not be improved in a minimax sense ( see @xcite and @xcite for related results ) .",
    "more formally , the target of the paper is to develop estimators @xmath22 for which the excess risk is controlled _ in deviations _ , that is , such that for an appropriate constant @xmath25 , for any @xmath26 , with probability at least @xmath27 , @xmath28}{n}.\\ ] ] note that by integrating the deviations [ using the identity @xmath29 which holds true for any non - negative random variable  @xmath30 , inequality  ( [ eqdevtarget ] ) implies @xmath31 in this work , we do not assume that the function @xmath32,\\ ] ] which minimizes the risk @xmath33 among all possible measurable functions , belongs to the model @xmath19 .",
    "so we might have @xmath34 and in this case , bounds of the form @xmath35 + { \\kappa}\\frac{d}{n}\\ ] ] with a constant @xmath36 larger than @xmath37 , do not even ensure that @xmath38 tends to  @xmath23 when @xmath2 goes to infinity .",
    "these kinds of bounds with @xmath39 have been developed to analyze nonparametric estimators using linear approximation spaces , in which case the dimension @xmath0 is a function of @xmath2 chosen so that the bias term @xmath40 has the order @xmath1 of the estimation term ( see @xcite and references within ) .",
    "here we intend to assess the generalization ability of the estimator even when the model is misspecified [ namely , when @xmath41 .",
    "moreover , we do not assume either that @xmath42 and @xmath10 are independent or that @xmath9 has a subexponential tail distribution : for the moment , we just assume that @xmath43 admits a finite second - order moment in order that the risk of @xmath18 is finite",
    ".    several risk bounds with @xmath44 can be found in the literature .",
    "a survey on these bounds is given in @xcite , section 1 .",
    "let us mention here the closest bound to what we are looking for . from the work of birg and massart @xcite , we may derive the following risk bound for the empirical risk minimizer on a @xmath45 ball ( see appendix  b of @xcite ) .    [ thbmnew ] assume that @xmath19 has a diameter @xmath46 for @xmath45-norm , that is , for any @xmath47 in @xmath19 , @xmath48 and there exists a function @xmath49 satisfying the exponential moment condition @xmath50 | x = x \\ } \\le m\\ ] ] for some positive constants @xmath51 and @xmath52 .",
    "let @xmath53 where the infimum is taken with respect to all possible orthonormal bases of @xmath19 for the dot product @xmath54 $ ] ( when the set @xmath19 admits no basis with exactly @xmath0 functions , we set @xmath55 ) .",
    "then the empirical risk minimizer satisfies for any @xmath26 , with probability at least @xmath27 , @xmath56 + { \\log(\\varepsilon^{-1})}}{n},\\ ] ] where @xmath57 is a positive constant depending only on @xmath52 .",
    "the theorem gives exponential deviation inequalities of order at worse @xmath58 and , asymptotically , when @xmath2 goes to infinity , of order @xmath1 .",
    "this work will provide similar results under weaker assumptions on the output distribution .",
    "_ notation .",
    "_ when @xmath59 , the function @xmath18 and the space @xmath19 will be written  @xmath60 and @xmath61 to emphasize that @xmath19 is the whole linear space spanned by @xmath16 : @xmath62 the euclidean norm will simply be written as , and @xmath63 will be its associated inner product .",
    "we will consider the vector valued function defined by @xmath64_{k=1}^d$ ] , so that for any @xmath65 , we have @xmath66 the gram matrix is the @xmath67-matrix @xmath68 $ ] .",
    "the empirical risk of a function @xmath69 is @xmath70 ^ 2 $ ] and for @xmath71 , the ridge regression estimator on @xmath19 is defined by @xmath72 with @xmath73 where @xmath74 is some non - negative real parameter . in the case when @xmath75 , the ridge regression @xmath76 is nothing but the empirical risk minimizer @xmath77 .",
    "besides , the empirical risk minimizer when @xmath78 is also called the ordinary least squares estimator , and will be denoted by @xmath79 .    in the same way , we introduce the optimal ridge function optimizing the expected ridge risk : @xmath80 with @xmath81 finally , let @xmath82 be the ridge regularization of @xmath83 , where @xmath84 is the identity matrix .",
    "there are four main reasons .",
    "first , we intend to provide a nonasymptotic analysis of the parametric linear least squares method .",
    "second , the task is central in nonparametric estimation for linear approximation spaces ( piecewise polynomials based on a  regular partition , wavelet expansions , trigonometric polynomials@xmath85 ) .",
    "third , it naturally arises in two - stage model selection .",
    "precisely , when facing the data , the statistician often has to choose several models which are likely to be relevant for the task .",
    "these models can be of similar structure ( like embedded balls of functional spaces ) or , on the contrary , of a very different nature ( e.g. , based on kernels , splines , wavelets or on a parametric approach ) .",
    "for each of these models , we assume that we have a learning scheme which produces a `` good '' prediction function in the sense that it predicts as well as the best function of the model up to some small additive term .",
    "then the question is to decide on how we use or combine / aggregate these schemes .",
    "one possible answer is to split the data into two groups , use the first group to train the prediction function associated with each model , and finally use the second group to build a prediction function which is as good as ( i ) the best of the previously learned prediction functions , ( ii ) the best convex combination of these functions or ( iii ) the best linear combination of these functions .",
    "this point of view has been introduced by nemirovski in @xcite and optimal rates of aggregation are given in @xcite and the references within .",
    "this paper focuses more on the linear aggregation task [ even if ( ii ) enters in our setting ] , assuming implicitly here that the models are given in advance and are beyond our control and that the goal is to combine them appropriately .",
    "finally , in practice , the noise distribution often departs from the normal distribution . in particular",
    ", it can exhibit much heavier tails , and consequently induce highly non - gaussian residuals .",
    "it is then natural to ask whether classical estimators such as the ridge regression and the ordinary least squares estimator are sensitive to this type of noise , and whether we can design more robust estimators .",
    "section [ secridge ] provides a new analysis of the ridge estimator and the ordinary least squares estimator , and their variants .",
    "theorem [ thhfrlam ] provides an asymptotic result for the ridge estimator , while theorem  [ thermom ] gives a nonasymptotic risk bound for the empirical risk minimizer , which is complementary to the theorems put in the survey section .",
    "in particular , the result has the benefit to hold for the ordinary least squares estimator and for heavy - tailed outputs .",
    "we show quantitatively that the ridge penalty leads to an implicit reduction of the input space dimension .",
    "section [ seccomputable ] shows a nonasymptotic @xmath1 exponential deviation risk bound under weak moment conditions on the output @xmath9 and on the @xmath0-dimensional input representation  @xmath86 .",
    "the main contribution of this paper is to show through a pac - bayesian analysis on truncated differences of losses that the output distribution does not need to have bounded conditional exponential moments in order for the excess risk of appropriate estimators to concentrate exponentially .",
    "our results tend to say that truncation leads to more robust algorithms .",
    "local robustness to contamination is usually invoked to advocate the removal of outliers , claiming that estimators should be made insensitive to small amounts of spurious data .",
    "our work leads to a different theoretical explanation .",
    "the observed points having unusually large outputs when compared with the ( empirical ) variance should be down - weighted in the estimation of the mean , since they contain less information than noise .",
    "in short , huge outputs should be truncated because of their low signal - to - noise ratio .",
    "we recall the definition @xmath87 where @xmath14 is a closed convex set , not necessarily bounded ( so that @xmath88 is allowed ) . in this section",
    "we provide exponential deviation inequalities for the empirical risk minimizer and the ridge regression estimator on @xmath19 under weak conditions on the tail of the output distribution .    the most general theorem which can be obtained from the route followed in this section is theorem 1.5 of the supplementary material @xcite .",
    "it is expressed in terms of a series of empirical bounds .",
    "the first deduction we can make from this technical result is of an asymptotic nature .",
    "it is stated under weak hypotheses , taking advantage of the weak law of large numbers .",
    "[ thhfrlam ] for @xmath71 , let @xmath89 be its associated optimal ridge function [ see  ( [ eqfrid ] ) ] . let us assume that @xmath90 < + \\infty\\ ] ] and @xmath91 ^ 2 \\ } < + \\infty.\\ ] ] let @xmath92 be the eigenvalues of the gram matrix @xmath68 $ ] , and let @xmath82 be the ridge regularization of @xmath83 .",
    "let us define the _ effective ridge dimension _ @xmath93 = \\mathbb{e } [ \\vert q_{\\lambda}^{-1/2 } { \\varphi}(x ) \\vert^2 ] .\\ ] ] when @xmath75 , @xmath94 is equal to the rank of @xmath83 and is otherwise smaller . for any @xmath95 , there is @xmath96 , such that for any @xmath97 , with probability at least @xmath98 , @xmath99 ^ 2\\ } } { \\mathbb{e } \\",
    "{ \\vert q_{\\lambda}^{-1/2 } { \\varphi}(x ) \\vert^2 \\ } } \\frac{d } { n } \\\\ & & \\qquad\\quad { } + 1\\mbox{,}000 \\sup _ { v \\in\\mathbb{r}^d } \\frac{\\mathbb{e } [ \\langle v , { \\varphi}(x ) \\rangle^2 [ { \\tilde{f}}(x ) - y ] ^2 ] } { \\mathbb{e } ( \\langle v , { \\varphi}(x ) \\rangle^2 ) + \\lambda\\vert v \\vert^2 } \\frac{\\log(3\\varepsilon^{-1})}{n}\\\\ & & \\qquad \\leq\\min_{\\theta\\in\\theta } \\ { r(f_{\\theta } ) + \\lambda\\vert\\theta\\vert^2 \\ } \\\\ & & \\qquad\\quad { } + { \\operatorname{ess}}\\sup{\\mathbb{e}}\\{[y-{\\tilde{f}}(x)]^2 | x\\ } \\frac { 30 d + 1\\mbox{,}000 \\log ( 3\\varepsilon^{-1 } ) } { n}.\\end{aligned}\\ ] ]    see section 1 of the supplementary material @xcite .",
    "this theorem shows that the ordinary least squares estimator ( obtained when @xmath88 and @xmath75 ) , as well as the empirical risk minimizer on any closed convex set , asymptotically reaches a @xmath1 speed of convergence under very weak hypotheses .",
    "it shows also the regularization effect of the ridge regression .",
    "there emerges an _ effective dimension _",
    "@xmath94 , where the ridge penalty has a threshold effect on the eigenvalues of the gram matrix .",
    "let us remark that the second inequality stated in the theorem provides a simplified bound which makes sense only when @xmath100 ^ 2 | x \\ }",
    "< + \\infty\\ ] ] implying that @xmath101 .",
    "we chose to state the first inequality as well , since it does not require such a tight relationship between @xmath89 and @xmath102 .",
    "on the other hand , the weakness of this result is its asymptotic nature : @xmath103 may be arbitrarily large under such weak hypotheses , and this happens even in the simplest case of the estimation of the mean of a real - valued random variable by its empirical mean [ which is the case when @xmath104 and @xmath105 .",
    "let us now give some nonasymptotic rate under stronger hypotheses and for the empirical risk minimizer ( i.e. , @xmath75 ) .",
    "[ thermom ] assume that @xmath106 ^ 4\\ } < + \\infty$ ] and @xmath107 } < + \\infty.\\ ] ] consider the ( unique ) empirical risk minimizer @xmath108 on @xmath19 for which @xmath109 . , we have @xmath110 , with @xmath111 , @xmath112_{j=1}^n$ ] and @xmath113 is the moore ",
    "penrose pseudoinverse of @xmath114 . ] for any values of  @xmath115 and @xmath2 such that @xmath116 and @xmath117\\ ] ] with probability at least @xmath27 , @xmath118\\\\[-8pt ] & & \\qquad\\le1920 { b}\\sqrt{{\\mathbb{e}}\\ { [ y - f^*(x)]^4 \\ } } \\biggl [ \\frac{3 b d + \\log ( 2\\varepsilon ^{-1})}{n } + \\biggl(\\frac{4 b d}n\\biggr)^2 \\biggr ] .",
    "\\nonumber\\end{aligned}\\ ] ]    see section 1 of the supplementary material @xcite .",
    "it is quite surprising that the traditional assumption of uniform boundedness of the conditional exponential moments of the output can be replaced by a simple moment condition for reasonable confidence levels ( i.e. , @xmath119 ) . for highest confidence levels , things are more tricky since we need to control with high probability a term of order @xmath120d / n$ ] ( see theorem  1.6 ) . the cost to pay to get the exponential deviations under only a fourth - order moment condition on the output is the appearance of the geometrical quantity @xmath121 as a multiplicative factor .    to better understand the quantity @xmath121 , let us consider two cases .",
    "first , consider that the input is uniformly distributed on @xmath122 $ ] , and that the functions @xmath16 belong to the fourier basis .",
    "then the quantity @xmath121 behaves like a numerical constant . on the contrary , if we take @xmath16 as the first @xmath0 elements of a wavelet expansion , the more localized wavelets induce high values of @xmath121 , and @xmath121 scales like @xmath123 , meaning that theorem [ thermom ] fails to give a @xmath1-excess risk bound in this case .",
    "this limitation does not appear in theorem [ thhfrlam ] .",
    "to conclude , theorem [ thermom ] is limited in at least four ways : it involves the quantity  @xmath121 , it applies only to uniformly bounded @xmath86 , the output needs to have a fourth moment , and the confidence level should be as great as @xmath119 .",
    "these limitations will be addressed in the next section by considering a more involved algorithm .",
    "this section provides an alternative to the empirical risk minimizer with nonasymptotic exponential risk deviations of order @xmath1 for any confidence level .",
    "moreover , we will assume only a second - order moment condition on the output and cover the case of unbounded inputs , the requirement on @xmath124 being only a finite fourth - order moment .",
    "on the other hand , we assume here that the set @xmath125 of the vectors of coefficients is bounded .",
    "the computability of the proposed estimator and numerical experiments are discussed at the end of the section .",
    "let @xmath126 , @xmath71 , and consider the truncation function : @xmath127 for any @xmath128 , introduce @xmath129 ^ 2-\\alpha [ y_i - f_{{\\theta}'}(x_i)]^2\\bigr).\\ ] ] we recall that @xmath80 with @xmath130 , and that the effective ridge dimension is defined as @xmath131 = { \\operatorname{tr } } [ ( q+ \\lambda i)^{-1 } q ] = \\sum_{i=1}^d \\frac{\\nu_i}{\\nu_i+{\\lambda } } \\mathbh{1}(\\nu_i>0 ) \\le d,\\ ] ] where @xmath132 are the eigenvalues of the gram matrix @xmath133 $ ] .",
    "let us assume in this section that @xmath134 ^ 4\\}<+\\infty,\\ ] ] and that for any @xmath135 , @xmath136<+\\infty.\\ ] ]    define @xmath137 = 1 \\ } , \\\\",
    "\\sigma & = & \\sqrt{{\\mathbb{e}}\\{[y-{\\tilde{f}}(x)]^2\\}}= \\sqrt{r({\\tilde{f } } ) } , \\\\",
    "\\chi & = & \\max_{f\\in{\\mathcal{s } } } \\sqrt{{\\mathbb{e } } [ f(x)^4 ] } , \\\\",
    "\\kappa & = & \\frac { \\sqrt{{\\mathbb{e}}\\{[{\\varphi}(x)^tq_{\\lambda}^{-1}{\\varphi}(x)]^2\\ } } } { { \\mathbb{e}}[{\\varphi}(x)^tq_{\\lambda}^{-1}{\\varphi}(x ) ] } , \\\\ \\kappa ' &",
    "= & \\frac{\\sqrt{{\\mathbb{e}}\\{[y-{\\tilde{f}}(x)]^4\\}}}{{\\mathbb{e}}\\ { [ y-{\\tilde{f}}(x)]^2\\ } } = \\frac{\\sqrt{{\\mathbb{e}}\\{[y-{\\tilde{f}}(x)]^4\\}}}{\\sigma^2},\\\\ \\label{eqkapp } t & = & \\max_{{\\theta}\\in\\theta,{\\theta}'\\in\\theta } \\sqrt { { \\lambda}\\|{\\theta}-{\\theta}'\\|^2 + { \\mathbb{e}}\\ { [ f_{{\\theta}}(x)-f_{{\\theta}'}(x ) ] ^2 \\ } } .\\end{aligned}\\ ] ]    [ th31 ] let us assume that ( [ eqas1 ] ) and ( [ eqas2 ] ) hold . for some numerical constants @xmath138 and @xmath139 , for @xmath140 by taking @xmath141 ^ 2 } \\biggl(1 - \\frac{c \\kappa\\chi d}{n } \\biggr)\\ ] ] for any estimator @xmath142 satisfying @xmath143 a.s . , for any @xmath26 and any @xmath71 , with probability at least @xmath27 , we have @xmath144 ^ 2 } { 1 - { c\\kappa\\chi d}/{n } } .\\end{aligned}\\ ] ]    see section 2 of the supplementary material @xcite .    by choosing an estimator such that @xmath145 theorem [ th31 ] provides a nonasymptotic bound for the excess ( ridge ) risk with a @xmath146 convergence rate and an exponential tail even when neither the output @xmath9 nor the input vector @xmath86 have exponential moments . this stronger",
    "nonasymptotic bound compared to the bounds of the previous section comes at the price of replacing the empirical risk minimizer by a more involved estimator .",
    "section [ seccomput ] provides a way of computing it approximately .",
    "theorem [ th31 ] requires a fourth - order moment condition on the output .",
    "in fact , one can replace ( [ eqas1 ] ) by the following second - order moment condition on the output : for any @xmath135 , @xmath147 ^ 2\\}<+\\infty,\\ ] ] and still obtain a @xmath146 excess risk bound .",
    "this comes at the price of a more lengthy formula , where terms with @xmath148 become terms involving the quantities @xmath149 ^ 2\\}$ ] and @xmath150 ^ 2\\}$ ] .",
    "( this can be seen by not using cauchy  schwarz s inequality in ( 2.5 ) and ( 2.6 ) of the supplementary material @xcite . )",
    "we see that the speed of convergence of the excess risk in theorem [ th31 ] ( page ) depends on three kurtosis - like coefficients , @xmath151 , @xmath152 and @xmath153 .",
    "the third , @xmath153 , is concerned with the noise , conceived as the difference between the observed output @xmath9 and its best explanation @xmath154 according to the ridge criterion .",
    "the aim of this section is to study the order of magnitude of the two other coefficients @xmath151 and @xmath152 , which are related to the design distribution , @xmath155 and @xmath156 we will review a few typical situations .      let us assume first that @xmath124 is a multivariate centered gaussian random variable . in this case ,",
    "its covariance matrix coincides with its gram matrix @xmath157 and can be written as @xmath158 where @xmath159 is an orthogonal matrix .",
    "using @xmath159 , we can introduce @xmath160 .",
    "it is also a gaussian vector , with covariance @xmath161 $ ] .",
    "moreover , since @xmath159 is orthogonal , @xmath162 , and since @xmath163 are uncorrelated when @xmath164 , they are independent , leading to @xmath165 \\\\ & = & \\sum_{i=1}^d \\mathbb{e } ( w_i^4 ) + 2 \\sum_{1 \\leq i < j \\leq d } \\mathbb{e } ( w_i^2 ) \\mathbb{e } ( w_j^2 ) \\\\ & = & d^2 + 2 d_2,\\end{aligned}\\ ] ] where @xmath166 .",
    "thus , in this case , @xmath167 moreover , as for any value of @xmath168 , @xmath169 is a gaussian random variable , @xmath170 .",
    "this situation arises in compressed sensing using random projections on gaussian vectors .",
    "specifically , assume that we want to recover a signal @xmath171 that we know to be well approximated by a linear combination of @xmath0 basis vectors @xmath172 .",
    "we measure @xmath173 projections of the signal @xmath69 on i.i.d .",
    "@xmath52-dimensional standard normal random vectors @xmath174 , @xmath175 .",
    "then , recovering the coefficient @xmath176 such that @xmath177 is associated to the least squares regression problem , @xmath178 with @xmath179 , and @xmath10 having a @xmath52-dimensional standard normal distribution .",
    "let us study now the case when almost surely @xmath180 and @xmath181 are independent .",
    "to compute @xmath151 , we can assume without loss of generality that @xmath182 are centered and of unit variance , since this renormalization is precisely the linear transformation that turns the gram matrix into the identity matrix .",
    "let us introduce @xmath183^{1/2}}{\\mathbb{e } [ \\varphi_j(x)^2 ] } \\ ] ] with the convention @xmath184 .",
    "a computation similar to the one made in the gaussian case shows that @xmath185 moreover , for any @xmath186 such that @xmath187 , @xmath188 \\mathbb{e } [ \\varphi_j(x)^2 ] \\\\ & & { } + 4 \\sum_{i=2}^d u_1 u_i^3 \\mathbb{e } [ \\varphi_i(x)^3 ] \\\\ & \\leq&\\chi_*^2 \\sum_{i=1}^d u_i^4 + 6 \\sum_{i < j } u_i^2 u_j^2 + 4 \\chi_*^{3/2 } \\sum_{i=2}^d \\vert u_1 u_i \\vert^3 \\\\ & \\leq&\\sup_{u \\in{\\mathbb{r}}_{+}^d , \\vert u \\vert= 1 } ( \\chi_*^2 - 3 ) \\sum_{i=1}^d u_i^4 + 3 \\biggl ( \\sum_{i=1}^d u_i^2 \\biggr)^2 + 4 \\chi_*^{3/2 } u_1 \\sum_{i=2}^d u_i^3 \\\\ &",
    "\\leq&\\frac{3^{3/2}}{4 } \\chi_*^{3/2 } + \\cases { \\chi_*^2 , & \\quad $ \\chi_*^2 \\geq3$,\\vspace*{2pt}\\cr 3 + \\dfrac{\\chi_*^2 - 3}{d } , & \\quad $ 1 \\leq\\chi_*^2 < 3$.}\\end{aligned}\\ ] ] thus , in this case , @xmath189    if , moreover , the random variables @xmath190 are not skewed , in the sense that @xmath191 = 0 $ ] , @xmath192 , then @xmath193      let us assume now that the distribution of @xmath124 is almost surely bounded and nearly orthogonal .",
    "these hypotheses are suited to the study of regression in usual function bases , like the fourier basis , wavelet bases , histograms or splines .",
    "more precisely , let us assume that @xmath194 and that for some positive constant @xmath51 and any @xmath195 , @xmath196^{1/2}.\\ ] ] this appears as some stability property of the partial basis @xmath197 with respect to the @xmath198-norm , since it can also be written as @xmath199,\\qquad u \\in\\mathbb{r}^d.\\ ] ] in terms of eigenvalues , @xmath200 can be taken to be the lowest eigenvalue @xmath201 of the gram matrix @xmath83 .",
    "the value of @xmath51 can also be deduced from a condition saying that @xmath197 are nearly orthogonal in the sense that @xmath202 \\geq1\\quad \\mbox{and } \\quad\\vert\\mathbb{e } [ \\varphi_j(x ) \\varphi_k(x ) ] \\vert\\leq\\frac{1 - a^{-2}}{d-1}.\\ ] ] in this situation , the chain of inequalities @xmath203 \\leq\\vert u \\vert^2 b^2 \\mathbb{e } [ \\langle u , \\varphi(x ) \\rangle^2 ] \\leq a^2",
    "b^2 \\mathbb { e } [ \\langle u , \\varphi(x ) \\rangle^2 ] ^2\\ ] ] shows that @xmath204 . on the other hand , @xmath205\\\\[1pt ] & & \\qquad= \\mathbb{e } [ \\sup\\ { \\langle u , \\varphi(x ) \\rangle^4 ; u \\in\\mathbb{r}^d , \\vert q_{\\lambda}^{1/2 } u \\vert\\leq1 \\ } ] \\\\[1pt ] & & \\qquad\\leq\\mathbb{e } [ \\sup\\ { \\vert u \\vert^2 b^2 \\langle u , \\varphi(x ) \\rangle^2 ; \\vert q^{1/2}_{\\lambda } u \\vert \\leq1 \\ } ] \\\\[1pt ] & & \\qquad\\leq\\mathbb{e } [ \\sup\\ { ( 1 + \\lambda a^2 ) ^{-1 } a^2 b^2 \\vert q_{\\lambda}^{1/2 } u \\vert^2 \\langle u , \\varphi(x ) \\rangle^2 ; \\vert q_{\\lambda}^{1/2 } u",
    "\\vert\\leq1 \\ } ] \\\\[1pt ] & & \\qquad\\leq\\frac{a^2b^2}{1 + \\lambda a^2 } \\mathbb{e } [ \\vert q_{\\lambda}^{-1/2 } \\varphi(x ) \\vert^2 ] = \\frac{a^2 b^2 d}{1 + \\lambda a^2}\\end{aligned}\\ ] ] showing that @xmath206 .    for example , if @xmath10 is the uniform random variable on the unit interval and  @xmath197 , @xmath207 , are any functions from the fourier basis [ meaning that they are of the form @xmath208 or @xmath209 , then @xmath210 ( because they form an orthogonal system ) and @xmath211",
    ".    a localized basis like the evenly spaced histogram basis of the unit interval @xmath212 will also be such that @xmath213 and @xmath214 .",
    "similar computations could be made for other local bases , like wavelet bases .",
    "note that when @xmath151 is of order @xmath123 , and @xmath152 and @xmath153 of order @xmath37 , theo - rem  [ th31 ] means that the excess risk of the min  max truncated estimator  @xmath22 is upper bounded by @xmath215 provided that @xmath216 for a large enough constant  @xmath36.=1      let us discuss the case when @xmath10 is some observed random variable whose distribution is only approximately known .",
    "namely , let us assume that @xmath217 is some basis of functions in @xmath218 $ ] with some known coefficient @xmath219 , where @xmath220 is an approximation of the true distribution of @xmath10 in the sense that the density of the true distribution @xmath221 of @xmath10 with respect to the distribution @xmath220 is in the range @xmath222 .",
    "in this situation , the coefficient @xmath151 satisfies the inequality @xmath223 .",
    "indeed , @xmath224 & \\leq & \\eta{\\mathbb{e}}_{x\\sim{\\tilde{{\\mathbb{p } } } } } [ \\langle u , \\varphi ( x ) \\rangle^4 ] \\\\[1pt ] & \\leq & \\eta{\\tilde{\\chi}}^2 { \\mathbb{e}}_{x\\sim{\\tilde{{\\mathbb{p } } } } } [ \\langle u , \\varphi(x ) \\rangle^2 ] ^2\\\\[1pt ] & \\leq&\\eta^3 { \\tilde{\\chi}}^2 \\mathbb{e}_{x\\sim{{\\mathbb{p } } } } [ \\langle u , \\varphi(x ) \\rangle^2 ] ^2.\\end{aligned}\\ ] ] in the same way , @xmath225 .",
    "indeed , @xmath226 \\\\ & & \\qquad\\leq\\eta{\\tilde{\\mathbb{e } } } [ \\sup\\ { \\langle u , \\varphi ( x ) \\rangle^4 ; { \\tilde{\\mathbb{e } } } ( \\langle u , \\varphi(x ) \\rangle^2 ) \\leq\\eta\\ } ] \\\\ & & \\qquad\\leq\\eta^3 { \\tilde{\\mathbb{e } } } [ \\sup\\ { \\langle u , \\varphi(x ) \\rangle^4 ; { \\tilde{\\mathbb{e } } } ( \\langle u , \\varphi(x ) \\rangle^2 ) \\leq1 \\ } ] \\\\ & & \\qquad\\leq\\eta^3 { \\tilde{\\kappa}}^2 { \\tilde{\\mathbb{e } } } [ \\sup\\ { \\langle u , \\varphi(x ) \\rangle^2 ; { \\tilde{\\mathbb{e } } } ( \\langle u , \\varphi(x ) \\rangle^2 ) \\leq1 \\ } ] ^2 \\\\ & & \\qquad\\leq\\eta^7 { \\tilde{\\kappa}}^2 \\mathbb{e } [ \\sup \\ { \\langle u , \\varphi(x ) \\rangle^2 ; \\mathbb{e } ( \\langle u , \\varphi(x ) \\rangle^2 ) \\leq1 \\ } ] ^2.\\end{aligned}\\ ] ]    let us conclude this section with some scenario for the case when @xmath10 is a  real - valued random variable .",
    "let us consider the distribution function of  @xmath220 , @xmath227 then , if @xmath220 has no atoms , the distribution of @xmath228 would be uniform on @xmath229 if @xmath10 were distributed according to @xmath220 .",
    "in other words , @xmath230 , the uniform distribution on the unit interval . starting from some suitable partial basis @xmath217 of @xmath231 $ ] like the ones discussed above , we can build a basis for our problem as @xmath232.\\ ] ] moreover , if @xmath233 is absolutely continuous with respect to @xmath220 with density @xmath234 , then @xmath235 is absolutely continuous with respect to @xmath230 , with density @xmath236 , and , of course , the fact that @xmath234 takes values in @xmath222 implies the same property for @xmath236 .",
    "thus , if @xmath219 and @xmath237 are the coefficients corresponding to @xmath238 when @xmath159 is the uniform random variable on the unit interval , then the true coefficient @xmath151 [ corresponding to @xmath239 will be such that @xmath223 and @xmath225 .      for ease of description of the algorithm ,",
    "we will write @xmath10 for @xmath86 , which is equivalent to considering without loss of generality that the input space is @xmath15 and that the functions @xmath240 are the coordinate functions . therefore , the function @xmath241 maps an input @xmath242 to @xmath243 .",
    "let us introduce @xmath244 for any subset of indices @xmath245 , let us define @xmath246    we suggest the following heuristics to compute an approximation of @xmath247    * start from @xmath248 with the ordinary least squares estimate @xmath249 * at step number @xmath250 , compute @xmath251 * consider the sets @xmath252^{-1 } } \\bigr)^2 < \\eta\\bigr\\},\\ ] ] where @xmath253 is the ( pseudo-)inverse of the matrix @xmath254 .",
    "* let us define @xmath255 * stop when @xmath256 and set @xmath257 as the final estimator of @xmath258 .",
    "note that there will be at most @xmath2 steps , since @xmath259 and in practice much less in this iterative scheme .",
    "let us give some justification for this proposal .",
    "let us notice first that @xmath260 \\bigr).\\end{aligned}\\ ] ] hopefully , @xmath261 is in some small neighborhood of @xmath262 already , according to the distance defined by @xmath263 .",
    "so we may try to look for improvements of @xmath262 by exploring neighborhoods of @xmath262 of increasing sizes with respect to some approximation of the relevant norm @xmath264 $ ] .    since the truncation function @xmath265 is constant on @xmath266 $ ] and @xmath267 , the map @xmath268 induces a decomposition of the parameter space into cells corresponding to different sets @xmath84 of examples .",
    "indeed , such a set @xmath84 is associated to the set @xmath269 of @xmath270 such that @xmath271 if and only if @xmath272 .",
    "although this may not be the case , we will do as if the map @xmath268 restricted to the cell @xmath269 reached its minimum at some interior point of @xmath269 , and approximates this minimizer by the minimizer of @xmath273 .",
    "the idea is to remove first the examples which will become inactive in the closest cells to the current estimate @xmath262 .",
    "the cells for which the contribution of example number @xmath274 is constant are delimited by at most four parallel hyperplanes .",
    "it is easy to see that the square of the inverse of the distance of @xmath262 to the closest of these hyperplanes is equal to @xmath275 indeed , this distance is the infimum of @xmath276 , where @xmath277 is a solution of @xmath278 it is computed by considering @xmath277 of the form @xmath279 and solving an equation of order two in @xmath280 .",
    "this explains the proposed choice of @xmath281 .",
    "then a first estimate @xmath282 is computed on the basis of this reduced sample , and the sample is readjusted to @xmath283 by checking which constraints are really activated in the computation of @xmath284 .",
    "the estimated parameter is then readjusted , taking into account the readjusted sample ( this could as a variant be iterated more than once ) .",
    "now that we have some new candidates @xmath285 , we check the minimax property against them to elect @xmath286 and @xmath287 .",
    "since we did not check the minimax property against the whole parameter set @xmath78 , we have no theoretical warranty for this simplified algorithm .",
    "nonetheless , similar computations to what we did could prove that we are close to solving @xmath288 , since we checked the minimax property on the reduced parameter set @xmath289 .",
    "thus , the proposed heuristics are capable of improving on the performance of the ordinary least squares estimator , while being guaranteed not to degrade its performance significantly .      in section [ secnoise ]",
    ", we detail the different kinds of noises we work with .",
    "then , sections [ secexpind ] , [ sechcc ] and [ sects ] describe the three types of functional relationships between the input , the output and the noise involved in our experiments . a motivation for choosing these input  output distributions was the ability to compute exactly the excess risk , and thus to compare easily estimators .",
    "section [ secexper ] provides details about the implementation , its computational efficiency and the main conclusions of the numerical experiments .",
    "figures and tables are postponed to the .      in our experiments ,",
    "we consider different types of noise that are centered and with unit variance :    * the standard gaussian noise , @xmath290 , * a heavy - tailed noise defined by @xmath291 , with @xmath292 , a  standard gaussian random variable and @xmath293 ( the real number @xmath294 is taken strictly larger than @xmath295 as for @xmath296 , the random variable @xmath297 would not admit a finite second moment ) . * an asymmetric heavy - tailed noise defined by @xmath298 with @xmath293 with @xmath292 a standard gaussian random variable . * a mixture of a dirac random variable with a low - variance gaussian random variable defined by , with probability @xmath299 , @xmath300 , and with probability @xmath301 , @xmath297 is drawn from @xmath302 the parameter @xmath303 $ ] characterizes the part of the variance of @xmath297 explained by the gaussian part of the mixture .",
    "note that this noise admits exponential moments , but for @xmath2 of order @xmath304 , the dirac part of the mixture generates low signal - to - noise points .      in inc@xmath305 , we consider @xmath306 , and the input ",
    "output pair is such that @xmath307 where the components of @xmath10 are independent standard normal distributions , @xmath308 and @xmath309 .      in @xmath310 , we consider @xmath306 , and the input ",
    "output pair is such that @xmath307 where @xmath10 is a multivariate centered normal gaussian with covariance matrix  @xmath83 obtained by drawing a @xmath311-matrix @xmath51 of uniform random variables in @xmath312 $ ] and by computing @xmath313 , @xmath308 and @xmath309 .",
    "so the only difference with the setting of section [ secexpind ] is the correlation between the covariates .",
    "let @xmath10 be a uniform random variable on @xmath312 $ ] .",
    "let @xmath0 be an even number . in ts@xmath305 , we consider @xmath314 and the input ",
    "output pair is such that @xmath315 with @xmath309 .",
    "one can check that this implies @xmath316      [ [ choice - of - the - parameters - and - implementation - details ] ] choice of the parameters and implementation details + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the min ",
    "max truncated algorithm has two parameters @xmath317 and @xmath318 . in the subsequent experiments",
    ", we set the ridge parameter @xmath74 to the natural default choice for it : @xmath319 .",
    "for the truncation parameter @xmath317 , according to our analysis [ see ( [ eqalpha ] ) ] , it roughly should be of order @xmath320 up to kurtosis coefficients . by using the ordinary least squares estimator , we roughly estimate this value , and test values of @xmath317 in a geometric grid ( of @xmath321 points ) around it ( with ratio @xmath322 ) .",
    "cross - validation can be used to select the final  @xmath317 .",
    "nevertheless , it is computationally expensive and is significantly outperformed in our experiments by the following simple procedure : start with the smallest @xmath317 in the geometric grid and increase it as long as @xmath323 , that is , as long as we stop at the end of the first iteration and output the empirical risk minimizer .",
    "to compute @xmath282 or @xmath324 , one needs to determine a least squares estimate ( for a modified sample ) . to reduce the computational burden",
    ", we do not want to test all possible values of @xmath325 ( note that there are at most  @xmath2 values leading to different estimates ) .",
    "our experiments show that testing only three levels of @xmath325 is sufficient .",
    "precisely , we sort the quantity @xmath326^{-1 } } \\bigr)^2\\ ] ] by decreasing order and consider @xmath325 being the first , @xmath327th and @xmath328th value of the ordered list .",
    "overall , in our experiments , the computational complexity is approximately fifty times larger than the one of computing the ordinary least squares estimator .",
    "[ [ results ] ] results + + + + + + +    the tables and figures have been gathered in the .",
    "tables  [ tabb01 ] and [ tabb04 ] give the results for the mixture noise . tables [ taba201 ] , [ taba-201 ] and [ taba0 ] provide the results for the heavy - tailed noise and the standard gaussian noise .",
    "each line of the tables has been obtained after 1,000 generations of the training set .",
    "these results show that the min ",
    "max truncated estimator is often equal to the ordinary least squares estimator @xmath79 , while it ensures impressive consistent improvements when it differs from @xmath79 . in this latter case ,",
    "the number of points that are not considered in @xmath22 , that is , the number of points with low signal - to - noise ratio , varies a lot from @xmath37 to @xmath329 and is often of order @xmath330 .",
    "note that not only the points that we expect to be considered as outliers ( i.e. , very large output points ) are erased , and that these points seem to be taken out by local groups : see figures [ fig1 ] and [ fig2 ] in which the erased points are marked by surrounding circles .",
    "besides , the heavier the noise tail is ( and also the larger the variance of the noise is ) , the more often the truncation modifies the initial ordinary least squares estimator , and the more improvements we get from the min  max truncated estimator , which also becomes much more robust than the ordinary least squares estimator ( see the confidence intervals in the tables ) .",
    "finally , we have also tested more traditional methods in robust regression , namely , the m - estimators with huber s loss , @xmath331-loss and tukey s bisquare influence function , and also the least trimmed squares estimator , the s - estimator and the mm - estimator ( see @xcite and the references within ) .",
    "these methods rely on diminishing the influence of points having `` unreasonably '' large residuals .",
    "they were developed to handle training sets containing true outliers , that is , points @xmath332 not generated by the distribution @xmath5 .",
    "this is not the case in our estimation framework . by overweighting points having reasonably small residuals ,",
    "these methods are often biased even in settings where the noise is symmetric and the regression function @xmath333 $ ] belongs to @xmath61 ( i.e. , @xmath334 ) , and also even when there is no noise ( but @xmath335 ) .",
    "the worst results were obtained by the @xmath331-loss , since estimating the ( conditional ) median is here really different from estimating the ( conditional ) mean .",
    "the mm - estimator and the m - estimators with huber s loss and tukey s bisquare influence function give good results as long as the signal - to - noise ratio is low .",
    "when the signal - to - noise ratio is high , a lack of consistency drastically appears in part of our simulations , showing that these methods are thus not suited for our estimation framework .",
    "the s - estimator is almost consistently improving on the ordinary least squares estimator ( in our simulations ) .",
    "however , when the signal - to - noise ratio is low ( i.e. , in the setting of the aforementioned simulations with ) , the improvements are much less significant than the ones of the min  max truncated estimator .",
    "the goal of this section is to explain the key ingredients appearing in the proofs which both allow to obtain subexponential tails for the excess risk under a nonexponential moment assumption and get rid of the logarithmic factor in the excess risk bound .",
    "let us start with the idea allowing us to prove exponential inequalities under just a moment assumption ( instead of the traditional exponential moment assumption ) .",
    "to understand it , we can consider the ( apparently ) simplistic @xmath37-dimensional situation in which we have @xmath336 and the marginal distribution of @xmath337 is the dirac distribution at @xmath37 . in this case",
    ", the risk of the prediction function @xmath241 is @xmath338={\\mathbb{e}}[(y-{\\mathbb{e}}y)^2 ] + ( { \\mathbb{e}}y -{\\theta})^2 $ ] , so that the least squares regression problem boils down to the estimation of the mean of the output variable .",
    "if we only assume that @xmath9 admits a finite second moment , say , @xmath339 , it is not clear whether for any @xmath26 , it is possible to find @xmath340 such that , with probability at least @xmath341 , @xmath342 for some numerical constant @xmath138 .",
    "indeed , from chebyshev s inequality , the trivial choice @xmath343 just satisfies , with probability at least @xmath341 , @xmath344 which is far from the objective ( [ eqtar1 ] ) for small confidence levels [ consider @xmath345 , e.g. ] .",
    "the key idea is thus to average ( soft ) _ truncated _ values of the outputs .",
    "this is performed by taking @xmath346 with @xmath347 .",
    "since we have @xmath348 the exponential chebyshev s inequality guarantees that with probability at least @xmath27 , we have @xmath349 , hence , @xmath350 replacing @xmath9 by @xmath351 in the previous argument , we obtain that , with probability at least @xmath27 , we have @xmath352 since @xmath353 , this implies @xmath354 .",
    "the two previous inequalities imply inequality ( [ eqtar1 ] ) ( for @xmath355 ) , showing that subexponential tails are achievable even when we only assume that the random variable admits a finite second moment ( see @xcite for more details on the robust estimation of the mean of a random variable ) .",
    "let us first recall that the kullback  leibler divergence between distributions  @xmath356 and @xmath357 defined on @xmath19 is @xmath358 , & \\quad if $ \\rho\\ll\\mu$,\\vspace*{2pt}\\cr + \\infty , & \\quad otherwise,}\\ ] ] where @xmath359 denotes as usual the density of @xmath356 w.r.t .",
    "@xmath357 . for any",
    "real - valued ( measurable ) function @xmath277 defined on @xmath19 such that @xmath360 \\pi(df)<+\\infty$ ] , we define the distribution @xmath361 on @xmath19 by its density : @xmath362}{\\int\\exp[h(f ' ) ] \\pi(df')}.\\ ] ]    the analysis of statistical inference generally relies on upper bounding the supremum of an empirical process @xmath151 indexed by the functions in a model @xmath19 .",
    "concentration inequalities appear as a central tool to obtain these bounds .",
    "an alternative approach , called the pac - bayesian one , consists in using the entropic equality @xmath363 where @xmath364 is the set of probability distributions on @xmath19 .",
    "let @xmath365 be an observable process such that , for any @xmath366 , we have @xmath367 for @xmath368 $ ] and some @xmath369 .",
    "then ( [ eqiexp ] ) leads to , for any @xmath26 , with probability at least @xmath27 , for any distribution @xmath356 on @xmath19 , we have @xmath370 the left - hand side quantity represents the expected risk with respect to the distribution @xmath356 . to get the smallest upper bound on this quantity , a  natural choice of the ( posterior ) distribution @xmath356",
    "is obtained by minimizing the right - hand side , that is , by taking @xmath371 [ with the notation introduced in  ( [ eqpih ] ) ] .",
    "this distribution concentrates on functions @xmath366 for which @xmath372 is small . without prior knowledge",
    ", one may want to choose a prior distribution @xmath373 which is rather `` flat '' ( e.g. , the one induced by the lebesgue measure in the case of a model @xmath19 defined by a bounded parameter set in some euclidean space ) .",
    "consequently , the kullback ",
    "leibler divergence @xmath374 , which should be seen as the complexity term , might be excessively large .    to overcome the lack of prior information and the resulting high complexity term",
    ", one can alternatively use a more `` localized '' prior distribution . here",
    "we use gaussian distributions centered at the function of interest ( e.g. , the function @xmath18 ) , and with covariance matrix proportional to the inverse of the gram matrix @xmath83 .",
    "the idea of using pac - bayesian inequalities with gaussian prior and posterior distributions goes back to langford and shawe - taylor  @xcite in the context of linear classification .",
    "the detailed proofs of theorems [ thhfrlam ] , [ thermom ] and [ th31 ] can be found in the supplementary material @xcite .",
    "= =             ( with the mixture noise with @xmath375 and @xmath376 ) that are not taken into account in the min  max truncated estimator ( to the extent that the estimator would not change by removing simultaneously all these points ) .",
    "max truncated estimator @xmath377 appears in dash - dot line , while @xmath378 is in solid line . in these six simulations",
    ", it outperforms the ordinary least squares estimator . ]",
    "( with the heavy - tailed noise ) that are not taken into account in the min  max truncated estimator ( to the extent that the estimator would not change by removing these points ) .",
    "max truncated estimator @xmath377 appears in dash - dot line , while @xmath378 is in solid line . in these six simulations",
    ", it outperforms the ordinary least squares estimator .",
    "note that in the last figure , it does not consider @xmath379 points among the @xmath380 training points . ]"
  ],
  "abstract_text": [
    "<S> we consider the problem of robustly predicting as well as the best linear combination of @xmath0 given functions in least squares regression , and variants of this problem including constraints on the parameters of the linear combination . </S>",
    "<S> for the ridge estimator and the ordinary least squares estimator , and their variants , we provide new risk bounds of order @xmath1 without logarithmic factor unlike some standard results , where @xmath2 is the size of the training data . </S>",
    "<S> we also provide a  new estimator with better deviations in the presence of heavy - tailed noise . </S>",
    "<S> it is based on truncating differences of losses in a min  max framework and satisfies a @xmath1 risk bound both in expectation and in deviations . </S>",
    "<S> the key common surprising factor of these results is the absence of exponential moment condition on the output distribution while achieving exponential deviations . </S>",
    "<S> all risk bounds are obtained through a pac - bayesian analysis on truncated differences of losses . </S>",
    "<S> experimental results strongly back up our truncated min  </S>",
    "<S> max estimator .    .    </S>"
  ]
}