{
  "article_text": [
    "as we enter the era of large - scale structure experiments such as lsst , wfirst and euclid , the creation of reliable mock galaxy catalogs will become increasingly more important .",
    "such catalogs are essential for correctly characterizing the expected errors in the analyses of these datasets , calibrating analysis pipelines and ultimately measuring cosmological parameters ( such as the dark energy equation of state ) from galaxy clustering ( e.g. @xcite ) . making mock catalogs for different subpopulations of galaxies ( e.g. blue versus red , high @xmath6 versus low @xmath6 , etc . )",
    "to study their clustering properties is also of utmost importance for understanding galaxy formation and evolution ( e.g. @xcite ) .",
    "although these mock catalogs can be generated relatively quickly using perturbation theory - based approaches such as that described in @xcite , it is well known that these approximations break down at small scales ( e.g. @xcite ) .",
    "alternatively , mock catalogs can be created using simulations which capture non - linear structure growth to smaller scales , and inherently include redshift - space distortions ( velocity information of the dark matter particles is known ) .",
    "ideally we would like large - volume cosmological simulations with both n - body and hydrodynamics , however , such simulations are computationally expensive .",
    "hence , the large number of mocks necessary for obtaining robust error measurements renders this approach impractical .",
    "fortunately , since pure n - body simulations are relatively inexpensive , the literature has been rife with methods for populating the dark matter halos found in these simulations with galaxies .",
    "one of the most popular methods for doing this is using halo occupation distributions ( hods ) which is an analytic model for determining the number of galaxies ( @xmath0 ) that should form in a halo given its properties ( e.g. @xcite ) .",
    "more recently , subhalo abundance matching ( sham ) has become very prevalent ( e.g. @xcite ) .",
    "this method relies on being able to correctly identify the subhalos within a halo , a very difficult problem in its own right due to resolution limitations ( see e.g. @xcite ) .",
    "it then assumes that each subhalo contains a single galaxy with a stellar mass or luminosity that is monotonically related to a subhalo property .",
    "while both methods have been shown to produce mock galaxy catalogs that match observations reasonably well , it would be progressive to attempt to eliminate the above mentioned assumptions .",
    "the sophisticated non - parametric regression algorithms that form a subcategory of `` machine learning '' ( ml ) are ideal for this purpose . to obtain the mapping from halos to @xmath0 , the only assumptions that these model - independent algorithms require are that such a relationship exists and that it is a continuous function of the halo parameters .",
    "they then proceed to construct a model from the data itself and hence do not impose any pre - supposed relationships onto the data .",
    "we note that although ml algorithms are non - parametric in the sense that we do not need to assume a known relationship between data parameters , they do often require us to assume some operational parameters such as how severely poor predictions are penalized .",
    "these , however , can be optimized as described in ",
    "[ sec : svm ] .",
    "in addition , the ml - based approaches we propose here , like hod - based methods , rely only on the properties of the parent dark matter halo .",
    "hence , we can circumvent the difficulties in subhalo identification .",
    "another point worth highlighting is that in principle , these techniques can be trivially extended to understand how halo properties map onto different subpopulations of galaxies .",
    "this provides a method for making mock catalogs for these different subpopulations as well .",
    "ml algorithms do , however , need to be trained on large , accurate datasets in order for them to learn robust mappings between halo properties and galaxy properties such as @xmath0 : a large - volume n - body plus hydro simulation with reliable galaxy formation would be ideal . at present",
    ", we have not been able to acquire such a simulation and so we use the millennium simulation with semi - analytic galaxy formation for this study . however , it is conceivable that such a simulation will become available in the near future which can be used to characterize the halo - galaxy mapping via the ml techniques discussed in this work .",
    "a final point of interest rests in the observation that most hod methods typically use the halo mass as the only parameter in their halo - to - galaxy mappings .",
    "an important topic to study is the sensitivity of @xmath0 to other halo parameters .",
    "for example , there have been investigations hinting that the environment of the halo is also an important factor in determining how many galaxies will form ( e.g. @xcite ) .",
    "this information can be gleaned using ml techniques as well , through performing a `` feature selection '' which picks out the halo properties that best predict @xmath0 .    in ",
    "[ sec : data ] we describe the dataset we use derived from the millennium simulations . in   [ sec : ml ] we describe the 2 ml techniques we employ to learn the mapping between halo properties and @xmath0 . in ",
    "[ sec : results ] we describe our results , i.e. how well our predictions match the actual values from millennium .",
    "this section also includes a discussion on using ml to make mocks for different subpopulations of galaxies .",
    "we conclude in   [ sec : theend ] .",
    "we construct our dataset from halo and galaxy catalogs at @xmath7 derived from the millennium simulation @xcite .",
    "these catalogs are obtained via querying the millennium online database @xcite .",
    "the halo catalogs were generated using a friends - of - friends ( fof ) algorithm with linking length of 0.2 and the semi - analytic galaxy prescription used to populate these halos is described in @xcite .",
    "the millennium simulation is run with @xmath8 particles in a @xmath9 box .",
    "the cosmology employed has @xmath10 , @xmath11 , @xmath12 , @xmath13 , @xmath14 and @xmath15 .    to obtain our dataset ,",
    "we search through the millennium halo catalog and extract all primary halos ( fof groups ) with mass greater than @xmath16 ( at present , we are unlikely to observe anything less massive except in the local universe ) .",
    "we then match galaxies from the semi - analytic catalog to these halos , keeping only the primary galaxies of a halo or subhalo ( i.e. those flagged 0 or 1 in the millennium database ) .",
    "hence , we emerge with a halo catalog listing the following 7 parameters : number of particles in the halo @xmath17 , @xmath1 , velocity dispersion @xmath2 , maximum circular velocity @xmath3 , half - mass radius @xmath18 , spin and number of galaxies in the halo @xmath0 .",
    "our goal is to train a machine learning algorithm to predict @xmath0 using the other 6 halo parameters .",
    "the semi - analytic model used to populate the millennium halos with galaxies is dependent on various thresholds ( such as for gas accretion and star formation ) that are also evolved through time .",
    "this quality makes millennium an adequate testing ground for ml applications because the mapping from halo parameters to @xmath0 is much more complicated than the straight - forward functions normally employed in methods such as hod and sham .",
    "the complexity of these mappings should be closer to the level we expect from actual n - body simulations with hydrodynamics .",
    "there are 395,832 halos ( with 445,983 total galaxies ) in our sample which we use for our basic tests of the ml algorithms .",
    "however , since the millennium semi - analytic model provides @xmath19 magnitudes and stellar masses for the galaxies , we can also use these same halos to learn the mapping between halo properties and @xmath0 for different subpopulations of galaxies .",
    "we perform tests of the ml algorithms after splitting the halo sample on colour and stellar mass ( a proxy for luminosity ) in  [ sec : colour ] and  [ sec : mstar ] respectively .",
    "we test 2 different machine learning ( ml ) algorithms for predicting @xmath0 from the halo parameters @xmath17 , @xmath1 , @xmath2 , @xmath3 , @xmath18 and spin .",
    "the first is a support vector machine ( svm ) and the second is a k - nearest - neighbours ( knn ) routine , both described below .",
    "they work by `` learning '' a relationship between a set of input features @xmath20 and the value we re interested in predicting @xmath21 .",
    "svm and knn are both non - parametric in the sense that we do not need to assume a model as traditional methods for populating halos with galaxies do . the mapping is constructed using information in the data itself : the data picks the model best suited to it .",
    "in addition , we avoid the messy problem of subhalo finding which is a required step in any sham - based approach ; like hod - based models , our proposed ml techniques operate on the parent halo itself .",
    "the learning process is accomplished by `` training '' the algorithm on a set of training data where @xmath21 is known .",
    "the learned mapping can then be applied to a test set of @xmath20 values with known @xmath21 to verify its accuracy .",
    "if the learned relationship appears robust , it can be applied to a set of @xmath20 with unknown @xmath21 to make predictions .    in testing the accuracy of the predicted values ,",
    "we draw on the mean - squared - error ( mse ) defined as @xmath22 where @xmath23 is the number of test data points .",
    "this effectively measures a combination of variance ( the scatter in the predicted values ) and bias ( how different from truth the predicted values are ) .",
    "support vector machines ( svms ) work by mapping the features @xmath20 to a higher dimensional - space and attempting to separate them into regions that map onto specific @xmath21 values using a set of hyperplanes @xcite . in its original form ,",
    "it is a classification scheme but can be generalized to a regression algorithm @xcite .",
    "the general idea behind svm is best illustrated through the case of a binary classifier .",
    "the binary svm is trained on a set of input features @xmath24 , where @xmath25 is the total number of features , to classify data into one of two classes @xmath26 .",
    "consider a set of @xmath23 training data , each with a corresponding column vector of features @xmath27 and a corresponding class @xmath28 .",
    "an svm attempts to separate the training data into their appropriate classes using 2 parallel hyperplanes in a high - dimensional space .",
    "these planes can be written as @xmath29 where @xmath30 is the normal vector to the hyperplane and @xmath31 is some constant scalar analogous to a @xmath32-intercept in 2d .",
    "the 2 planes must satisfy the condition that no points fall in between them , i.e. @xmath33 for @xmath27 of the first class ( i.e. @xmath34=1 ) or @xmath35 for @xmath27 of the second class .",
    "note that this can be simply re - written as @xmath36    the region bounded by these 2 planes is known as the margin , which has width @xmath37 the best classifier is obtained through maximizing this distance or minimizing @xmath38 subject to the constraint in equation ( [ eqn : linsvmcon ] ) since in this limit we will obtain the most robust separation of the datapoints . for computational purposes , we actually end up minimizing @xmath39 .",
    "the optimization can be performed using lagrange multipliers ( @xmath40 )",
    ". this leads to minimizing @xmath38 with respect to the lagrangian @xmath41 \\label{eqn : lp}\\ ] ] subject to the constraints @xmath42 . taking the derivative of this lagrangian with respect to @xmath38 yields the solution @xmath43 values for the @xmath40 can be obtained by substituting this solution back into equation ( [ eqn : lp ] ) which yields @xmath44 and maximizing @xmath45 with respect to the @xmath40 .",
    "it turns out that only a few of the @xmath40 are non - zero .",
    "these correspond to the training points that satisfy the equality condition in equation ( [ eqn : linsvmcon ] ) .",
    "such points are called the support vectors and set the value of @xmath31 , i.e. ( @xmath46 ) . in practice",
    ", @xmath31 is taken to be an average over the support vectors , that is @xmath47 where @xmath48 is the number of support vectors .",
    "it is often useful to introduce some slack ( quantified by @xmath49 below ) into the svm classifier .",
    "this amounts to replacing the constraint in equation ( [ eqn : linsvmcon ] ) with the new constraint @xmath50 the effective reduction of the margin width in the above equation allows for some misclassification of the data .",
    "we then minimize @xmath51 where @xmath52 is a parameter that determines the penalty for any misclassification .",
    "if we solve the lagrangian for this case , we will find that @xmath53 is required .",
    "in addition , one can see that equation ( [ eqn : ld ] ) contains the term @xmath54 which is just the dot product between two vectors in feature space .",
    "we can then imagine generalizing this dot product to the dot product in a space spanned by a non - linear mapping ( @xmath55 ) of the features @xcite .",
    "this mapping can be used to take the features into a higher dimensional space , making them more easily separable .",
    "the dot product @xmath56 can be thought of as a kernel function @xmath57 .",
    "commonly used kernels like the polynomial , gaussian ( or radial basis function , rbf ) and sigmoid functions are popular due to their simplicity .",
    "these have the forms @xmath58 where @xmath59 is the degree of the polynomial , @xmath60 and @xmath61 where @xmath62 and @xmath63 are parameters of the kernel .",
    "the simple binary svm described above can be generalized to support vector regression ( svr ) @xcite which is the algorithm we employ in this study .",
    "for the regression problem , we seek hyperplanes satisfying the equations @xmath64 here , @xmath65 is a tolerance parameter , i.e. there is no penalty assigned to predictions that fall within @xmath65 of the true value .",
    "@xmath66 and @xmath67 are slack variables corresponding to upper and lower constraints on the system output .",
    "the quantity the svr must minimize is @xmath68 which closely resembles equation ( [ eqn : nlsvm ] ) .",
    "the lagrangian takes the form @xmath69 \\\\ \\nonumber & & - \\sum_{i=1}^{n } \\alpha_i^ * [ \\epsilon + \\xi_i^*+y_i - ( \\mathbf{w } \\cdot \\mathbf{x}_i + b ) ] \\\\",
    "\\nonumber\\end{aligned}\\ ] ] where @xmath40 and @xmath70 are lagrange multipliers subject to the constraints @xmath71 , @xmath72 and @xmath73 .    for our analysis",
    ", we use the svr algorithm implemented in the scikit - learn python library @xcite .",
    "we use the default @xmath74 and find that changing this value does not make a noticeable difference in our predictions or the resulting mse .",
    "the algorithm takes in a value for the penalty parameter @xmath52 from equation ( [ eqn : svr ] ) and the kernel .",
    "@xmath59 , @xmath62 and @xmath63 need also be specified depending on what kernel is being used .",
    "optimal parameter values can be determined by splitting our sample of halos into three equal parts : a training set , a validation set and a test set .",
    "we can then train the svm using some pre - determined grid values : @xmath75 and @xmath62 or @xmath76 in powers of 10 , and calculate the mse of the validation set . the parameter values that give the minimum mse",
    "are chosen for our analyses , where we evaluate how well the ml predictions match truth using the test set .",
    "the k - nearest - neighbours ( knn ) algorithm is much simpler than the svm to understand and can also be used for both classification and regression .",
    "the knn routine calculates `` distances '' to the @xmath77 nearest training data points for each point @xmath27 that we are interested in predicting @xmath78 .",
    "this distance is often just a simple euclidean distance between the features @xmath20 , however , one can imagine using other definitions as well .",
    "the predicted @xmath78 is then just the average of the @xmath77 nearest training set @xmath21 values .",
    "this average can be weighted according to distance such that points further away have less impact on the predicted value .",
    "mathematically , one can represent this as @xmath79 where @xmath80 is the distance between @xmath27 and one of its @xmath77 nearest neighbours @xmath81 , and @xmath82 is the weight corresponding to that distance .",
    "we again use the scikit - learn implementation of knn @xcite .",
    "the algorithm takes in a value for @xmath77 . again",
    ", the optimal value can be found by stepping through a pre - determined set @xmath83 and picking the value with the lowest mse when the algorithm is applied to the validation set .",
    "feature selection is the process by which we select which features are the most relevant for predicting @xmath21 from @xmath20 .",
    "a simple approach to this is forward feature selection which relies on an initial comparison to the base mse , or the mse calculated by taking all @xmath84 in equation ( [ eqn : mse ] ) , i.e. @xmath85 this is just the mse one would obtain by doing the most naive thing : predicting all @xmath21 values to be the mean @xmath21 of the training set ( denoted as @xmath86 above .",
    "forward feature selection starts by training an ml algorithm to predict @xmath21 using only a single feature .",
    "we repeat this for each individual feature and calculate its mse value from a test set .",
    "if the minimum mse is less than the base mse , then we are doing better than the naive prediction , indicating that the features do contain information that is correlated with and can help us predict @xmath21 .",
    "we then `` select '' the feature that produced the minimum mse and individually add each other feature to it and repeat the train / test procedure to calculate mse values . at the end of this round , if the minimum mse is smaller than the minimum mse in the previous step , we again select the feature that produced this minimum mse and repeat the previous procedure . at each step",
    "if adding in an additional feature decreases the minimum mse further , we continue . otherwise we stop and deem the remaining features as not having much predictive power beyond the `` selected '' features .",
    "such feature selection schemes are useful for identifying the halo parameters that are most relevant to inferring @xmath0 .",
    "as described in   [ sec : data ] , our tests use a sample of 395,832 halos from the millennium simulation to assess the machine learning algorithms detailed in the preceding section .",
    "we randomly split this halo sample into three equal parts and use the first part for training , the second part for validation and the third part for testing .",
    "the base mse of the test set is @xmath87 .",
    "we first look at the results obtained through training the ml algorithms on all 6 halo features .    as mentioned in  [ sec : svm ] , we do a grid - search to find the svm training parameters ( @xmath52 , @xmath62 and kernel ) that return the minimum mse on the validation set .",
    "this procedure selected the rbf kernel with @xmath88 and @xmath89 .",
    "we then use the svm trained using these values to make predictions from the test set . for knn , we use a similar search technique ( described in  [ sec : knn ] ) and find that using @xmath90 gives the minimum mse on the validation set .",
    "the knn test set results below are derived using this value .",
    "a set of 2d histograms in @xmath91 versus @xmath92 from our test set is shown in figure [ fig : milall ] .",
    "the top panel shows the svm result and the bottom panel shows the knn result .",
    "one can see that in both cases , the mse is dramatically improved over the base mse which indicates that the ml algorithms are learning some information about @xmath0 from the input features as expected .",
    "the mse values from the 2 different methods are very similar and hence svm and knn appear to be equally good for inferring @xmath0 from halo features",
    ". however , we note that upon careful inspection of the 2d histograms , one sees that there is a slight bias towards under - predicting @xmath0 which is discussed more below .",
    "ml algorithms appear to match the expected distribution of @xmath93 as a function of @xmath0 quite well as shown in figure [ fig : mildist ] .",
    "the panels show histograms where the @xmath32-axes correspond to the fraction of halos with @xmath0 and the @xmath94-axes correspond to @xmath0 .",
    "the top panel was obtained using the svm method and the bottom panel shows the analogue for knn .",
    "we slightly overpredict the number of halos with 1 galaxy , and underpredict elsewhere , especially when using the svm .",
    "the true number of galaxies in the test set is 149,064 ; for svm we predict 140,519 and for knn we predict 144,346 .",
    "this phenomenon was pointed out previously and will be elaborated on below .",
    "the galaxy correlation function @xmath95 can also be used to test the robustness of the ml results .",
    "this test is especially interesting as @xmath95 is the principal observable for large - scale structure analyses , the key motivation behind constructing mock galaxy catalogs .",
    "we create a mock galaxy catalog using the @xmath0 values predicted by our ml algorithms .",
    "we place these galaxies randomly within their host halos according to an nfw profile in the radial direction and random point generation on a sphere in the angular directions .",
    "we calculate the correlation function for the training , true test set and predicted test set galaxies in @xmath96 bins in the range @xmath96-@xmath97 ( suitable for redshift - space distortion analyses from large - scale structure ) .",
    "figure [ fig : milxi ] shows the resulting @xmath95 using svm ( top ) and knn ( bottom ) .",
    "each plot has 2 panels , the upper panel shows the actual correlation functions and the bottom panel shows the percent difference ( i.e. @xmath98/\\xi_{true}(r)$ ] ) between the correlation function calculated from the predicted galaxies in the test set versus that calculated from the true galaxies in the test set .",
    "in the svm case we see that at small scales ( @xmath99 ) , the predicted correlation function has a lower amplitude than the true correlation function by a significant amount ( @xmath100 ) . for knn ,",
    "the difference is fairly benign ( @xmath101 ) . at these small scales ,",
    "the 1-halo term is still important and hence the fact that we underpredict the number of halos with @xmath102 as shown in figure [ fig : mildist ] will result in lower clustering amplitudes than expected .",
    "however , for analyses of large - scale structure , we are more interested in @xmath95 at @xmath103 where our predicted @xmath95 is only @xmath5 lower than the true correlation function for the most part .",
    "as the amount of underprediction is fairly constant , one can even imagine implementing a simple scaling correction for crude applications .",
    "hence , the potential for making large - scale structure mocks using svm or knn remains worthy of further investigation .",
    "+    as mentioned above , there is a slight bias towards under - predicting @xmath0 .",
    "this is likely due to the fact that the number of halos with small @xmath0 dominates the overall distribution while halos with high @xmath0 are much rarer ( see figure [ fig : mildist ] ) .",
    "then , taking knn as an example , even if a halo should have a large number of galaxies , its nearest neighbours may still be dominated by @xmath104 halos , which will lead the algorithm to underpredict .",
    "the current training set is clearly incomplete for halos with large @xmath0 .",
    "a larger training sample will have a proportionately larger number of these halos so using a larger training set should be able to partially mitigate this problem .",
    "in addition , recall that the best kernel and parameters ( @xmath52 and @xmath62 ) for the svm and number of nearest neighbours @xmath77 for the knn are selected using the validation set on the basis of minimizing mse .",
    "since the mse is a balanced measure of variance and bias , one can imagine giving a different weighting to the bias , i.e. penalizing the mse more if the bias is high .",
    "this can potentially reduce the amount of underprediction we see , however , we will pay the price of having a larger scatter in our predictions .",
    "for svm we underpredict the total number of galaxies by @xmath105 and for knn we underpredict by @xmath106 .",
    "these are both small and do not appear to significantly alter the correlation function shown in figure [ fig : milxi ] at scales relevant to large - scale structure analyses . at these scales ,",
    "the correlation function is dominated by the 2-halo term which comes mostly from halos containing a single galaxy . as discussed above ,",
    "such halos are the most abundant by far .",
    "in addition , figure [ fig : mildist ] indicates that the number of halos we predict with @xmath104 matches the true distribution reasonably well .",
    "hence , it is not surprising that our predicted @xmath95 is in fair agreement with the true @xmath95 at large @xmath63 .",
    "we can also look at the distribution of @xmath0 as a function of the various features .",
    "figure [ fig : milfeat ] shows this in scatter plot form for the knn test .",
    "the analogous plots for svm are largely the same . here",
    "we have randomly subsampled the number of points plotted to 3,000 .",
    "the black points show true @xmath0 versus features while the red points show predicted @xmath0 .",
    "one can see that overall the span of the points overlaps quite well between the true and predicted sets , again indicating the general statistical agreement between the two .",
    "however , the black points do show slightly more spread overall .",
    "this indicates that the halo properties we have chosen to use here may not fully encapsulate the mapping to @xmath0 , i.e. we are still missing some information that is relevant to the halo - galaxy relationship",
    ". this should not be surprising as most of our parameters ( @xmath17 , @xmath1 , @xmath2 @xmath3 ) are effectively mass tracers .",
    "the ratio of @xmath18 and @xmath107 ( calculated easily from @xmath1 ) can be thought of as a measure of concentration , effectively the ratio of 2 radii that are defined in the same way for all halos .",
    "concentration is known to trace mass @xcite , but has also been found to correlate with halo environment @xcite .",
    "one can imagine that in addition to mass , spin and environment , there are additional factors ( branching from the full merger history of the halo ) that might affect @xmath0 .",
    "fortunately , it appears that any other factors are not completely orthogonal to the halo properties used in this study .",
    "as shown above , our parameters seem to capture most of the halo - galaxy mapping , at least in the context of the millennium simulations .",
    "finally , we can perform a feature selection to identify , within the framework of millennium , the most predictive halo property for @xmath0 .",
    "we employ a forward feature selection algorithm using svm to do this . to ensure the stability of our results , we re - perform the feature selection 10 times with different random draws of the training , validation and test sets .",
    "key numbers are summarized in table [ tab : fs ] .",
    "the mse values quoted for each feature under the column heading `` first round '' correspond to the median mse values of the 10 trials in the first round of the forward feature selection .",
    "one can see that @xmath18 has the smallest mse in the first round of selection which indicates that it is the best predictor for @xmath0 .",
    "using @xmath18 as the only feature for training gives a median mse of 0.163 which is very close to the mse obtained using all features as shown in figure [ fig : milall ] .",
    "the other parameters yield mse values ranging from 0.189 ( @xmath1 ) to 0.275 ( spin ) .",
    "the values quoted under `` second round '' correspond to the median mse obtained by adding in another feature on top of @xmath18 .",
    "one can see that this does not significantly change / improve on the minimum mse from the first round .",
    "hence it appears that most of our constraint on @xmath0 is coming from @xmath18 in the millennium simulations .",
    "the selection of @xmath18 should not be surprising .",
    "it contains information about the halo mass and , as discussed above , can be related to halo environment through @xmath1 .",
    ".[tab : fs]mse values obtained by performing a forward feature selection using svm .",
    "the values quoted are the median mse from 10 different randomizations of the training , validation and test sets .",
    "the halo parameters we use are listed in column 1 and the median mse values obtained by using only the listed parameter are shown in column 2 ( first round ) .",
    "one can see that @xmath18 has the smallest median mse and hence it should be the best predictor of @xmath0 in the context of the millennium simulations . adding in additional parameters",
    "does not significantly change the median mse as indicated in the third column ( second round ) which lists the median mse values obtained using @xmath18 plus the parameter listed in column 1 .",
    "[ cols=\"<,^,^\",options=\"header \" , ]      the millennium semi - analytic galaxies come with @xmath19 absolute magnitudes which we can use to define colours and split galaxies into blue and red subpopulations .",
    "this allows us to study whether or not ml - based methods for learning the halo - galaxy mapping and , most importantly , making mock catalogs can be directly extended to subpopulations of galaxies that have different colours .",
    "we define blue galaxies to have @xmath108 and red galaxies to have @xmath109 .",
    "this gives 175,177 blue galaxies and 270,792 red galaxies .",
    "we again split the 395,832 halos equally into training , validation and test sets for each case ( blue or red ) and repeat the previous tests using knn .",
    "note that many of our halos now have 0 red or blue galaxies reducing the size of our effective training sample . in the case of the blue galaxies ,",
    "the mse we obtain after applying knn is 0.186 as compared to a base mse of 0.334 . for the red galaxies , we obtain an mse of 0.293 as compared to a base mse of 0.738 . in both cases ,",
    "there is a significant reduction in mse compared to the base mse which indicates that the algorithm is learning information about @xmath0 from our input features .",
    "correlation functions derived from our predictions are shown in figure [ fig : colour ] .",
    "one can see that the predicted @xmath95 agrees fairly well with truth .",
    "again at small scales @xmath99 , we see that the predicted @xmath95 has a lower clustering amplitude , especially for the red galaxies ( @xmath110 ) .",
    "if we look at the distribution of halos with @xmath0 , we again see that there is a small under - prediction in the number of halos with @xmath102 that can cause this effect .",
    "this is slightly more problematic here as the main goal of studying the dependence of clustering on colour is to understand galaxy formation and evolution mechanisms which rely on information at small scales .",
    "however , such problems can be mitigated if we had a larger set of training data .",
    "we are now also approaching the point where the number of galaxies observed is large enough for us to begin understanding the differential clustering of blue and red galaxies at large scales .",
    "the agreement between the correlation function derived from our ml - predicted galaxies and the true correlation function at these scales is better ( @xmath111 ) and hence ml - based mock catalogs can be useful for these purposes .     +      since the millennium semi - analytic galaxy models also supply us with a stellar mass , we can split our galaxies into high stellar mass @xmath6 and low stellar mass samples .",
    "this can help us understand how effectively we can extend ml - based approaches to understanding the halo - galaxy mapping as a function of stellar mass , including their potential for constructing mock catalogs for these distinct subpopulations of galaxies .",
    "we make the cut at @xmath112 which gives 71,573 high @xmath6 galaxies and 374,410 low @xmath6 galaxies .",
    "stellar mass and luminosity are correlated with each other and hence by performing this split we are effectively separating our galaxies into low and high luminosity samples .",
    "once again we split the halos randomly and equally into training , validation and test sets .",
    "we then run knn to predict @xmath0 for each of the high and low stellar mass cases .",
    "note that in the high @xmath6 case , most of our halos now contain 0 galaxies so we have reduced the effective size of the training set by a large amount here .",
    "for the high mass case , we obtain an mse of 0.119 after applying knn as compared to a base mse of 0.250 .",
    "for the low mass case , our mse is 0.214 as compared to a base mse of 0.283 .",
    "again , the mse after putting the data through a knn algorithm is smaller than the base mse , indicating that the algorithm is learning about @xmath0 from the input features .",
    "a plot of the correlation functions derived from the ml - predicted galaxies is shown in figure [ fig : mstar ] .",
    "the low @xmath6 @xmath95 agrees well with truth at large scales ( @xmath113 deviation ) , however , it is again low near @xmath96 .",
    "like in the above case where we split by colour , this is slightly troublesome since studying the luminosity dependence of clustering is also mostly focused on understanding galaxy evolution through the clustering at small scales .",
    "nonetheless , mocks produced using our predicted @xmath0 can still benefit studies of luminosity - dependent clustering at large scales .",
    "the agreement in the high @xmath6 case is poor with deviations @xmath114 .",
    "however , this is because there are very few high stellar mass galaxies . with only a small number of halos with non - zero @xmath0 , it is not surprising that the algorithm has some difficulty with the regression .",
    "looking at the overall distribution of halos with @xmath0 in this case reveals an underprediction of halos with @xmath115 ( including @xmath104 ) . having fewer galaxies that break the @xmath104 threshold effectively increases the galaxy bias and hence the clustering amplitude due to the 2-halo term which begins to become important near @xmath99 and is dominant at larger scales .",
    "we have made some preliminary investigations into using machine learning techniques to populate dark matter halos from n - body simulations with galaxies .",
    "since it is very computationally expensive to run cosmological n - body simulations with hydrodynamics , and perturbation theory approaches tend to have problems on small - scales ( such as treating redshift - space distortions correctly ) , machine learning serves as a powerful alternative for creating large numbers of mock galaxy catalogs .",
    "these are a key ingredient in large - scale structure analyses , a quickly emerging area with the advent of large galaxy surveys such as lsst , wfirst and euclid which will have effective survey volumes of @xmath116 .",
    "most importantly , machine learning brackets a subclass of non - parametric algorithms which provide a unique way to construct the mapping from halo properties to galaxies . unlike other techniques such as hod , we do not need to pre - suppose a known model for the halo - galaxy relationship .",
    "the only assumption that must be made is that a function taking halo properties to number of galaxies per halo does exist and that it is smooth .",
    "it also allows us to circumvent the problems in subhalo identification which affect sham - based approaches .",
    "we test 2 machine learning algorithms , support vector machines and k - nearest - neighbours , on the halos and semi - analytic galaxies in the millennium simulation .",
    "we use 6 halo properties : number of particles @xmath17 , @xmath1 , @xmath2 , maximum circular velocity @xmath3 , half - mass radius @xmath18 and halo spin , to characterize the mapping between halo properties and @xmath0 ( the number of galaxies that reside in the halo ) .",
    "we find that both ml algorithms give mean - squared - errors of @xmath4 for the predicted @xmath0 , which is much smaller than the base mse 0.505 .",
    "the overall distribution of number of halos as a function of @xmath0 is also matched well by the ml predictions .",
    "we use our ml - predicted galaxies to create a mock galaxy catalog and calculate the correlation function from it . while in the svm case we see a deficiency in clustering amplitude at small scales ( @xmath99 ) by @xmath100 , the predicted correlation function tracks that calculated from the true millennium galaxies to @xmath117 at the scales most relevant to large - scale structure analyses .",
    "this is very important as large - scale structure science is the key motivator behind the construction of mock galaxy catalogs .    due to the rarity of halos with high @xmath0",
    ", we do find that there is a slight bias towards under - predicting @xmath0 .",
    "this can lead to the minor deficit in power at small scales in @xmath95 as mentioned above , however , this does not appear to significantly deter our ability to make mock catalogs . as previously stated , we obtain a reasonably good matching between the predicted and test correlation functions at scales relevant for large - scale structure studies .",
    "we see that the predicted and true @xmath0 values as a function of the features are similar in spread .",
    "however , for a given @xmath0 , the spread in the features is slightly larger for the true values .",
    "this suggests that the features we have used here do not fully capture the mapping between halo properties and @xmath0 , although , they do come very close .",
    "one should not be surprised by this since our features mostly trace halo mass and environment . while key , the full merger history of the halos is likely to impact properties of the halo beyond just mass and environment .",
    "we also demonstrate a simple feature selection procedure on our halo properties .",
    "feature selection is merely the process by which we use machine learning to identify the halo property most predictive in the mapping to @xmath0 . in the context of the millennium simulations ,",
    "our feature selection algorithm identifies @xmath18 as the most relevant parameter .",
    "this is not terribly surprising as @xmath18 is germane to both halo mass and environment .",
    "finally we investigate direct extensions of our ml algorithms to understanding the halo - galaxy mapping and making mock catalogs for various subpopulations of galaxies ( i.e. blue , red , low @xmath6 and high @xmath6 ) .",
    "we find that in general the agreement between the predicted and true correlation functions is fair .",
    "however , once again we observe an underprediction of power at small scales in most cases . as studies of differential clustering in various subpopulations",
    "are aimed at understanding galaxy formation and evolution which draw on information contained in the small scales of the correlation function , this is slightly more problematic here .",
    "however , we are entering an era where the number of observed galaxies is large enough to engage in studies of differential subpopulation clustering at large scales .",
    "our ml - predicted @xmath95 matches truth to @xmath5 at these scales and hence provides an interesting alternative for creating mocks for such studies .",
    "the key advantage of ml is that it offers a method of inferring the halo - galaxy mapping in a model - independent manner .",
    "in addition , it is computationally inexpensive : for example , to train an svm on @xmath118 points as done in this study takes @xmath119 hour on a single core .",
    "if we can run a large cosmological simulation ( n - body plus hydrodynamics ) with galaxy formation calibrated against available observations , we should be able to use the ml algorithms tested here to learn the mapping from halo properties to @xmath0 very quickly .",
    "we can then create large sets of mock galaxy catalogs from pure n - body simulations which are much less computationally expensive than running a large number of these fully - armed cosmological simulations .",
    "there also exist a number of avenues for future investigation including the use of ml - based approaches to predict not only @xmath0 but also the positions and velocities of galaxies within the parent halo .",
    "this is much more complex as it requires predicting a distribution ( i.e. multiple galaxy positions @xmath120 and velocities @xmath121 ) for each halo .",
    "one can also imagine devising methods to mitigate the bias towards underprediction ( i.e. penalizing the mse more for biased predictions or giving more weight to high @xmath0 neighbours in a knn implementation ) .",
    "these will all aid in our quest to generate mock catalogs reliably and efficiently .",
    "we thank daniel eisenstein and martin white for helpful discussions .",
    "x.x . is supported by a mcwilliams center for cosmology postdoctoral fellowship made possible by the bruce and astrid mcwilliams center for cosmology .",
    "is supported in part by nsf grant ast-1109730 .",
    "is supported in part by the m. hildred blewett fellowship of the american physical society , www.aps.org .",
    "the millennium simulation databases used in this paper and the web application providing online access to them were constructed as part of the activities of the german astrophysical virtual observatory ."
  ],
  "abstract_text": [
    "<S> we investigate machine learning ( ml ) techniques for predicting the number of galaxies ( @xmath0 ) that occupy a halo , given the halo s properties . </S>",
    "<S> these types of mappings are crucial for constructing the mock galaxy catalogs necessary for analyses of large - scale structure . </S>",
    "<S> the ml techniques proposed here distinguish themselves from traditional halo occupation distribution ( hod ) modeling as they do not assume a prescribed relationship between halo properties and @xmath0 . </S>",
    "<S> in addition , our ml approaches are only dependent on parent halo properties ( like hod methods ) , which are advantageous over subhalo - based approaches as identifying subhalos correctly is difficult . </S>",
    "<S> we test 2 algorithms : support vector machines ( svm ) and k - nearest - neighbour ( knn ) regression . </S>",
    "<S> we take galaxies and halos from the millennium simulation and predict @xmath0 by training our algorithms on the following 6 halo properties : number of particles , @xmath1 , @xmath2 , @xmath3 , half - mass radius and spin . for millennium , our predicted @xmath0 values have a mean - squared - error ( mse ) of @xmath4 for both svm and knn . </S>",
    "<S> our predictions match the overall distribution of halos reasonably well and the galaxy correlation function at large scales to @xmath5 . </S>",
    "<S> in addition , we demonstrate a feature selection algorithm to isolate the halo parameters that are most predictive , a useful technique for understanding the mapping between halo properties and @xmath0 . </S>",
    "<S> lastly , we investigate these ml - based approaches in making mock catalogs for different galaxy subpopulations ( e.g. blue , red , high @xmath6 , low @xmath6 ) . given its non - parametric nature as well as its powerful predictive and feature selection capabilities , machine learning offers an interesting alternative for creating mock catalogs . </S>"
  ]
}