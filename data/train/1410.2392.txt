{
  "article_text": [
    "statistical methods are increasingly being employed to analyse complex models of physical phenomena ( e.g. in climate forecasting or simulations of molecular dynamics ; * ? ? ?",
    "* ; * ? ? ?",
    "analytic intractability of complex models has inspired the development of sophisticated monte carlo methodologies to facilitate computation @xcite . in their most basic form ,",
    "monte carlo estimators converge as the reciprocal of root-@xmath0 where @xmath0 is the number of random samples . for complex models it may only be feasible to obtain a limited number of samples ( e.g. a recent met office model for future climate simulations required the order of @xmath1 core - hours per simulation ; * ? ? ?",
    "* ) . in these situations ,",
    "root-@xmath0 convergence is too slow and leads in practice to high - variance estimation .",
    "our contribution is motivated by resolving this issue and provides novel methodology that is both formal and general .",
    "the focus of this paper is the estimation of an expectation @xmath2 , where @xmath3 is a test function of interest and @xmath4 is a probability density associated with a random variable @xmath5 .",
    "provided that @xmath6 has variance @xmath7 , the arithmetic mean estimator @xmath8 based on @xmath0 independent and identically distributed ( iid ) samples @xmath9 of the random variable , satisfies the central limit theorem and converges to @xmath10 at the rate @xmath11 , or simply at `` root-@xmath0 '' .",
    "when working with complex models , root-@xmath0 convergence can be problematic , as highlighted in e.g. @xcite .",
    "a model is considered complex when either ( i ) @xmath5 is expensive to simulate , or ( ii ) @xmath3 is expensive to evaluate , in each case relative to the required estimator precision .",
    "both situations are prevalent in scientific and engineering applications ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "this paper introduces a class of estimators that converge more quickly than root-@xmath0 .",
    "the significance of our contribution is made clear in the comparative overview below .",
    "generic approaches to reduction of variance are well - known in both statistics and numerical analysis .",
    "these include ( i ) importance sampling and its extensions @xcite ,",
    "( ii ) stratified sampling and related techniques @xcite , ( iii ) antithetic variables @xcite and more generally ( randomised ) quasi - monte carlo ( qmc / rqmc ; * ? ? ?",
    "* ) , ( iv ) rao - blackwellisation @xcite , ( v ) riemann sums @xcite , ( vi ) control variates @xcite , ( vii ) multi - level monte carlo and related techniques ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , ( viii ) bayesian monte carlo ( bmc ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , and ( ix ) a plethora of sophisticated markov chain monte carlo sampling schemes ( mcmc ; * ? ? ?",
    "classical introductions to many of the above techniques include ( * ? ? ? * chap .",
    "4 ) and ( * ? ? ?",
    "motivated by contemporary statistical applications , we state four _ desiderata _ for a variance reduction technique : ( i ) _ unbiased estimation : _ monte carlo ( mc ) methods based on iid samples produce unbiased estimators , whilst techniques such as mcmc generally produce biased estimators . ( ii ) _ compatibility with an un - normalised density @xmath4 : _ an `` un - normalised '' density is known only up to proportionality so that , for example , mcmc techniques are required for sampling .",
    "( iii ) _ super - root-@xmath0 convergence _ ( for sufficiently regular @xmath3 ) : the convergence rates of ( r)qmc are well studied and can be super - root-@xmath0 .",
    "riemann sums can also achieve super - root-@xmath0 rates and @xcite showed the same holds for bmc .",
    "( iv ) _ post - hoc schemes : _ rao - blackwellisation , riemann sums , bmc and control variates can all be conceived as _ post - hoc _ schemes ; i.e. schemes that can be applied retrospectively after samples have been obtained .",
    "in contrast , the remaining methods require modification to computer code for the sampling process itself .",
    "the former are appealing from both a theoretical and a practical perspective since they separate the challenge of sampling from the challenge of variance reduction .",
    "table [ glass ] summarises existing techniques in relation to these _ desiderata _ ; note that no technique fulfils all four criteria .",
    "in contrast , the method proposed here , called `` control functionals '' , is able to satisfy all four _ desiderata_. control functionals appear to be similar , in this sense , to riemann sums i.e. they are a super - root-@xmath0 , _ post - hoc _ approach that applies to un - normalised sampling densities .",
    "however , riemann sums are rarely used in practice due to ( i ) the fact that estimators are biased at finite sample sizes , and ( ii ) there is a prohibitive increase in methodological complexity for multi - dimensional state spaces .",
    "control functionals do not posses either of these drawbacks .",
    "the control functional method that we develop below can be intuitively considered as a non - parametric development of control variates . in control",
    "variate schemes one seeks a basis @xmath12 , @xmath13 , that have expectation @xmath14 . then a surrogate function @xmath15 is constructed such that @xmath16 and , for suitably chosen @xmath17 , a variance reduction @xmath18 is obtained ( see e.g. * ? ? ?",
    "the statistics @xmath19 are known as control variates and the variance @xmath20 can be reduced to zero if and only if there is perfect canonical correlation between @xmath3 and the basis @xmath12 . for estimation based on markov chains , control variates for the discrete state space case were provided by @xcite . for continuous state spaces , statistics relating to the chain",
    "can be used as control variates @xcite .",
    "alternatively control variates can be constructed based on gradient information @xcite .",
    "the control variates described above are solving a misspecified regression problem , since in general @xmath3 will not be a linear combination of the @xmath19 basis functions .",
    "as such they achieve at most a constant factor reduction in estimator variance .",
    "intuitively , one would like to increase the number @xmath21 of basis functions to increase in line with the number @xmath0 .",
    "@xcite explored this approach within the metropolis - hastings method .",
    "however , their solution requires the user to partition of the state space , which limits its wider appeal .",
    "this paper introduces a powerful new perspective on variance reduction that fully resolves these issues , satisfying all the _ desiderata _ described above . to realise our method we developed a gradient - based function space that leads to closed - form estimators whose",
    "convergence can be guaranteed .",
    "the functional analysis perspective works `` out of the box '' , without requiring the user to partition the state space .",
    "extensive empirical support is provided in favour of the proposed method , including applications to hierarchical models and models based on non - linear differential equations . in each case",
    "state - of - the - art estimation is achieved .",
    "all results can be reproduced using ` matlab r2015a ` code that is available to download from ` http://warwick.ac.uk/control_functionals ` .",
    "consider a random vector @xmath5 taking values in an open set @xmath22 .",
    "assume @xmath5 admits a positive density on @xmath23 with respect to @xmath24-dimensional lebesgue measure , written @xmath25 .",
    "for bounded @xmath23 with boundary @xmath26 , we assume @xmath27 is piecewise smooth ( i.e. infinitely differentiable ) .",
    "write @xmath28 for the space of measurable functions @xmath29 for which @xmath30 is finite .",
    "write @xmath31 for the space of ( measurable ) functions from @xmath23 to @xmath32 with continuous partial derivatives up to order @xmath33 .",
    "consider a test function @xmath34 of interest , assume @xmath35 and write @xmath36 , @xmath37 .",
    "denote by @xmath38 a collection of states @xmath39 . at each state",
    "@xmath40 the corresponding function values @xmath41 and gradients @xmath42 are assumed to have been pre - computed and cached .",
    "the method that we develop does not then require any further recourse to the statistical model @xmath4 , nor any further evaluations of the function @xmath3 , and is in this sense a widely - applicable _ post - hoc _ scheme .",
    "our starting point is establish a trade - off between random sampling and deterministic approximation , as suggested on several separate occasions by authors including @xcite .",
    "consider a dichotomy of available states @xmath38 into two disjoint subsets @xmath43 and @xmath44 , where @xmath45 .",
    "although @xmath21 , @xmath0 are fixed , we will be interested in the asymptotic regime where @xmath46 for some @xmath47 $ ] .",
    "consider surrogate functions of the form @xmath48 where @xmath49 is an approximation to @xmath3 , based on @xmath50 , whose expectation @xmath51 is analytically tractable . by construction @xmath52 , @xmath53 and @xmath54 .",
    "we study estimators of the form @xmath55 for theoretical purposes the second subset @xmath56 is assumed to be an iid sample from @xmath4 , statistically independent from @xmath50 .",
    "then , for @xmath57 , we have unbiasedness , i.e. @xmath58 = \\mu(f)$ ] , where the expectation here is with respect to the sampling distribution @xmath4 of the @xmath59 random variables that constitute @xmath56 , and is conditional on @xmath50 .",
    "the corresponding estimator variance , conditional on @xmath50 , is @xmath60 = ( n - m)^{-1 } \\sigma^2(f - s_{f,\\mathcal{d}_0})$ ] .",
    "this formulation encompasses control variates as the special case where @xmath61 is constrained to a finite - dimensional space .",
    "the insight required to go beyond control variates and achieve super - root-@xmath0 convergence is that we can use an infinite - dimensional space to construct an increasingly accurate approximations @xmath61 as @xmath62 .",
    "we allow for the possibility that the first subset @xmath50 are also random and write @xmath63 to denote an expectation with respect to the ( marginal ) distribution of these @xmath21 random variables .",
    "[ theo1 ] assume @xmath46 for some @xmath47 $ ] and that the expected functional approximation error ( efae ) vanishes as @xmath64 = o(m^{-\\delta } ) \\label{efae eqn}\\end{aligned}\\ ] ] for some @xmath65 .",
    "then @xmath66 = o(n^{-1-\\gamma\\delta})$ ] .",
    "all proofs are reserved for appendix [ aproofs ] .",
    "taking @xmath67 optimises the rate in prop .",
    "[ theo1 ] and we therefore assume in the sequel that @xmath68 .      to construct approximations @xmath61 whose integrals @xmath51 are analytically tractable , we make the assumption    1 .",
    "the density @xmath4 belongs to @xmath69 .",
    "denote the gradient function by @xmath70 where @xmath71^t$ ] , well - defined by ( a1 ) .",
    "we study approximations of the form @xmath72 where @xmath73 is a constant and @xmath74 .",
    "[ cfdef2 ] appears in stein s classical test for approximate normality @xcite and related to ( but simpler than ) control variates proposed by @xcite .",
    "we make the following assumption ( c.f . e.g. eqn",
    ". 9 of * ? ? ?",
    "* ) :    1 .",
    "let @xmath75 be the unit normal to the boundary @xmath27 of the state space @xmath23 .",
    "then @xmath76    ( the notation @xmath77 denotes a surface integral over @xmath27 and @xmath78 denotes the surface element at @xmath79 . ) stein s identity implies that this class of approximations has integrals that are analytically tractable :    [ div prop ] assume ( a1,2 ) .",
    "then @xmath80 and so @xmath81 .",
    "when @xmath23 is unbounded , all surface integrals are interpreted as tail conditions .",
    "i.e. ( a2 ) should be replaced with @xmath82 , where @xmath83 is the sphere of radius @xmath84 centred at the origin and @xmath75 is the unit normal to the surface of @xmath85 .",
    "the statistic @xmath86 is recognised as a control variate .",
    "these control variates were explored in the case where @xmath87 is a ( gradient of a low - degree ) polynomial by @xcite , @xcite and @xcite .",
    "this paper takes the innovative step of setting @xmath87 within a function space to enable fully non - parametric approximation .",
    "the functional approximation perspective differs fundamentally from the control variate approach , in which the estimation problem is formally mis - specified ( i.e. @xmath87 is restricted to a low dimensional parametric family that does not contain the `` true '' function ) .",
    "we emphasise this key conceptual distinction by referring to @xmath86 as a control _ functional _ ( cf ; reflecting the use of terminology from functional analysis ) .",
    "this section establishes @xmath86 as belonging to a hilbert space @xmath88 .",
    "this allows us to formulate and solve a functional approximation problem that targets the efae .",
    "specification of @xmath86 is equivalent to specification of @xmath87 .",
    "we decide to restrict each component function @xmath89 to a hilbert space @xmath90 with inner product @xmath91 .",
    "moreover we insist that @xmath92 is a reproducing kernel hilbert space .",
    "this implies that there exists a symmetric positive definite function @xmath93 such that ( i ) for all @xmath94 we have @xmath95 and ( ii ) for all @xmath94 and @xmath96 we have @xmath97 ( * ? ? ?",
    "* def . 1 , p7 , and def .",
    "2 , p10 ) .",
    "the vector - valued function @xmath98 is defined in the cartesian product space @xmath99 , itself a hilbert space with the inner product @xmath100 .",
    "we make an assumption on @xmath33 that will be enforced by construction :    1 .",
    "the kernel @xmath33 belongs to @xmath101 .",
    "now we can analyse the class of cfs induced by @xmath33 :    [ conjugate ] assume @xmath102 and ( a1,3 )",
    ". then @xmath86 belongs to @xmath103 , the reproducing kernel hilbert space with kernel @xmath104 .    to gain some intuition for @xmath103",
    "we strengthen ( a2 ) as follows :    1 .   for @xmath4-almost",
    "all @xmath94 the kernel @xmath33 satisfies @xmath105 and @xmath106    while ( a2 ) must be verified on a case - by - case basis , it can in principle always be enforced with a suitable choice of @xmath33 .",
    "[ mean ele lem ] under ( a1,2,3 ) , the gradient - based kernel @xmath107 satisfies @xmath108 for @xmath4-almost all @xmath94 .",
    "lemma [ mean ele lem ] generalises eqn . 1 of @xcite and implies that @xmath103 consists of only valid cfs , i.e. @xmath109 .",
    "these ideas are illustrated in fig .",
    "[ space ] .    1 .",
    "the gradient - based kernel @xmath107 satisfies @xmath110    [ finite variance ] under ( a1,2,3,4 ) we have @xmath88 .    in general ( a4 )",
    "must be verified on a case - by - case basis .",
    "( a4 ) is easily verified for all examples in this paper .",
    "now we establish theoretical results for consistent approximation of @xmath3 by @xmath61 .",
    "write @xmath111 for the reproducing kernel hilbert space of constant functions with kernel @xmath112 for all @xmath113 .",
    "denote the norms associated to @xmath111 and @xmath103 respectively by @xmath114 and @xmath115 .",
    "write @xmath116 for the set @xmath117 .",
    "equip @xmath118 with the structure of a vector space , with addition operator @xmath119 and multiplication operator @xmath120 , each well - defined due to uniqueness of the representation @xmath121 , @xmath122 with @xmath123 and @xmath124 .",
    "in addition , equip @xmath118 with the norm @xmath125 , again , well - defined by uniqueness of representation .",
    "it can be shown that @xmath118 is a reproducing kernel hilbert space with kernel @xmath126 ( * ? ? ?",
    "* thm . 5 , p24 ) .    for the analysis we assume a basic well - posedness condition :    1",
    "i.e. @xmath121 for some @xmath128 and @xmath129 .",
    "( a5 ) is equivalent to the existence of a solution @xmath102 to the partial differential equation @xmath130 = [ f(\\bm{x } ) - \\mu(f ) ] \\pi(\\bm{x } ) , \\end{aligned}\\ ] ] called the `` fundamental equation '' in ( * ? ? ?",
    "* eqn . 5 ;",
    "see also eqn .",
    "4 in @xcite ) . with no initial or boundary conditions to satisfy , it is easy to show that there exist infinitely many solutions to the fundamental equation , so ( a5 ) is automatically satisfied by choosing @xmath33 such that @xmath92 is big enough to contain at least one solution .    to realise the cf method we consider the regularised least - squares ( rls ) functional approximation given by @xmath131 where @xmath132 .",
    "for the special case where @xmath133 , the cf estimator can be interpreted as kernel quadrature @xcite and also as empirical interpolation @xcite .",
    "the distinguishing feature of cfs from these methods is that the stein construction is compatible with un - normalised @xmath4 .",
    "below we will establish that the rls estimate produces vanishing efae under a strengthening of ( a4 ) :    1 .",
    "@xmath134 .",
    "[ sup condition ] ( a4 ) would follow from ( a3 ) and compactness of the state space @xmath23 , but we do not assume compactness here .",
    "all experiments in this paper have at worst @xmath135 , so that ( a4 ) is automatically satisfied , for example , when we choose @xmath136 for some @xmath137 .",
    "we can now state our main result :    [ sun proof ] assume ( a1,2,3,4,5 ) and take a rls estimate with @xmath138 .",
    "when @xmath50 are iid samples from @xmath4 , the estimator @xmath139 is an unbiased estimator of @xmath10 with @xmath140 = o(n^{-7/6})$ ] .",
    "cfs based on rls therefore improve upon the monte carlo rate .",
    "the hypotheses on @xmath4 are weak , only requiring that @xmath4 be continuously differentiable .",
    "empirical evidence ( below ) indicates stronger rates hold in more regular examples .",
    "indeed we can prove sharper results under stronger conditions that include boundedness of @xmath23 .",
    "details are reserved for a future publication @xcite .",
    "importantly , the rls estimate leads to a convenient closed - form expression for the cf estimator :    [ explicit formulae ] assume ( a1,3 ) .",
    "the cf estimator based on rls is @xmath141 where @xmath142^t$ ] , @xmath143^t$ ] , @xmath144^t$ ] , @xmath145 and the vector @xmath146 contains predictions for @xmath147 based only on @xmath50 , with @xmath148 .",
    "the estimator is a weighted combination of function values @xmath149^t$ ] with weights summing to one .",
    "estimates are readily obtained using standard matrix algebra .",
    "moreover the weights are independent of the test function @xmath3 and can be re - used to estimate multiple expectations @xmath150 for a collection @xmath151 .",
    "the samples @xmath56 enter only through the term @xmath152 in eqn .",
    "[ rewrite ] , which vanishes in probability as @xmath62 .",
    "thus any randomness due to @xmath56 vanishes and this gives another perspective on the source of super - root-@xmath0 convergence of the estimator .",
    "the term @xmath153 in eqn .",
    "[ rewrite ] is algebraically equivalent to bmc based on @xmath118 .",
    "i.e. @xmath153 is the posterior mean for @xmath10 based on a gaussian process ( gp ) prior @xmath154 and data @xmath50 ( * ? ? ?",
    "* eqn . 9 ) .",
    "our general construction in lemma [ explicit formulae ] therefore `` heals '' bmc in the sense of ( i ) de - biasing the bmc estimator , ( ii ) generalising bmc to un - normalised densities and ( iii ) remaining agnostic to statistical paradigm ( e.g. frequentist vs. bayesian ) .      the naive computational complexity associated with the rls estimate",
    "is @xmath155 due to the solution of an @xmath156 linear system . in situations where @xmath5 is expensive to simulate or @xmath3 is expensive to evaluate , @xmath21 is necessarily small and this additional computational cost will be negligible relative to model - based computation . in such scenarios we are more interested in non - asymptotic behaviour :    [ preasy ]",
    "assume ( a1,2,3,5 ) .",
    "let @xmath157 to simplify presentation .",
    "then @xmath158.\\end{aligned}\\ ] ] here @xmath159 and @xmath160 .",
    "theorem [ preasy ] provides an explicit error bound for @xmath127 , mimicking the approach of ( r)qmc ( * ? ? ?",
    "this offers a principled approach to selection of the design points @xmath50 since , writing @xmath161 for the variance with respect to the joint distribution of @xmath50 and @xmath56 , we have @xmath162 \\leq \\mathbb{e}_{\\mathcal{d}_0}\\mathbb{e}_{\\mathcal{d}_1}[d(\\mathcal{d}_0,\\mathcal{d}_1 ) ] \\|f\\|_{\\mathcal{h}_+}^2.\\end{aligned}\\ ] ] in the extreme case where @xmath163 , the discrepancy @xmath164 reduces to @xmath165 and we recover the usual root-@xmath0 rate . a similar bound forms the basis for recent work on two - sample testing by @xcite .",
    "several randomly chosen splits of the samples @xmath166 into subsets @xmath50 and @xmath56 may be averaged over to reduce estimator variance .",
    "we note that a multi - splitting estimator remains unbiased . as an alternative to multi - splitting , for applications where consistency suffices and unbiased estimation is not essential",
    ", we also propose the simplified estimator @xmath153 in eqn .",
    "[ rewrite ] with @xmath133 .",
    "empirical results below show that bias is negligible for practical purposes and , to pre - empt our conclusions , we recommend this simplified estimator for use in applications due to its reduced variance compared to the multi - splitting estimator . in all cases the regularisation parameter @xmath167 was taken to be the smallest power of 10 such that the kernel matrix @xmath168 has condition number lower than @xmath169 .",
    "a kernel @xmath170 typically involves hyper - parameters @xmath171 that must be specified .",
    "selection of @xmath171 can proceed via cross - validation , under the assumption that @xmath50 are independent samples from @xmath4 .",
    "specifically , we randomly split the samples @xmath50 into @xmath172 training samples @xmath173 and @xmath174 test samples @xmath175 .",
    "then we propose to select @xmath171 to minimise @xmath176 where @xmath177 is a vector of values @xmath178 for @xmath179 , and @xmath180 are the corresponding predicted values .",
    "in this way we are targeting the efae that reflects the variance of the cf estimator .",
    "we emphasise that the cross - validated estimator does not require additional sampling and will remain unbiased provided that preliminary cross - validation is performed only using @xmath50 . in this paper we employed the kernel defined in remark",
    "[ sup condition ] , with hyper - parameters @xmath181 .",
    "full pseudocode is provided in the supplement .      to illustrate the method we begin with simple , tractable examples .",
    "consider the synthetic problem of estimating the expectation of @xmath182 where @xmath5 is a @xmath24-dimensional standard gaussian random variable . by symmetry",
    "the true expectation is @xmath183 .",
    "initially we take @xmath184 iid samples and consider the scalar case @xmath185 .",
    "cross - validation was used to select tuning parameters . specifically : ( i ) we selected the hyper - parameters @xmath186 , @xmath187 on the basis that this approximately minimised the cross - validation error ( fig .",
    "( ii ) we found that estimator variance due to sample - splitting was minimised when at least half of the samples were allocated to @xmath50 ( fig .",
    "we therefore set a conservative default @xmath188 .",
    "( iii ) empirical results showed that little additional variance reduction occurs from employing multiple splits ( fig .",
    "s1c ) , so we chose to just use a single split .",
    "( iv ) finally , we found that the bias of the simplified estimator was negligible ( @xmath189 ) compared to monte carlo error ( @xmath190 ) ( fig .",
    "this is in line with an analogous result for classical control variates , where estimator bias vanishes asymptotically with respect to monte carlo error @xcite .    in fig .",
    "[ box1 ] we summarise the sampling distribution of both the sample - splitting and simplified cf estimators as the number of samples @xmath0 is varied .",
    "the alternative approaches of the arithmetic mean , riemann sums and `` zero variance '' ( zv ) control variates are also shown , the latter being based on quadratic polynomials @xcite .",
    "it is visually apparent that cfs enjoy the lowest variance at all samples sizes considered .",
    "we note that in this synthetic example , where there are essentially no computational restrictions , the cf framework is unnecessary and gains in precision come with comparable increases in computational cost .",
    "however we emphasise that , in the serious applications that follow , the cf calculations requires negligible computational resources in comparison to simulation from the model .",
    "super - root-@xmath0 convergence could perhaps be achieved by employing polynomials of increasing degree in the zv method , but our implementation of this approach did not provide stable estimates in this example ( full details in the supplement ) .    since the performance of cf is so pronounced , in order to more clearly visualise the results for all sample sizes , in fig .",
    "[ line1 ] we plot the estimator mean square error ( mse ) scaled by @xmath0 , so that root-@xmath0 convergence corresponds to a horizontal line .",
    "empirical results here are consistent with theory , showing that the arithmetic mean and control variates all achieve a constant factor variance reduction , whereas riemann sums and cfs achieve super - root-@xmath0 convergence . in this example",
    "cfs significantly outperformed riemann sums , the latter being based on piecewise linear approximations .",
    "we plot results for both the sample - splitting cf estimator and the simplified cf estimator , observing that the latter has lower variance .    to assess the generality of our conclusions we considered going beyond the scalar case to examples with dimensions @xmath191 and @xmath192 . the analogous results in figs .",
    "s2a , s2b show that , whilst increasing dimensionality presents fundamental challenges for all the variance reduction methods , cf continues to out - perform alternatives .",
    "going further we considered a variety of alternative problems , varying both the test function @xmath3 and the density @xmath4 .",
    "these include several pathological cases , with results summarised in table s1 .",
    "the results marked ( b ) echo the conclusions of @xcite , that zv control variates are effective in many cases where @xmath3 is well - approximated by a low - degree polynomial and @xmath4 is a gaussian or gamma density .",
    "however , when @xmath3 is not well - approximated by a low - degree polynomial , or when @xmath4 takes a more complex form , as in cases marked ( c ) , zv control variates can be outperformed by cfs , which have the potential to decrease variance dramatically .",
    "we then investigated how cfs can fail when theoretical assumptions are violated ( see examples marked `` cf @xmath193 '' ) .",
    "as expected , violation of ( a2 ) and ( a5 ) in ( e ) , ( g ) respectively led to poor performance of the cf estimator .",
    "interestingly , violation of differentiability in example ( f ) did not lead to poor estimation , though this may be because @xmath4 was only non - differentiable at a single point .",
    "we have not reported computational times for these experiments .",
    "our work is motivated by settings in which either simulation from @xmath4 or evaluation of @xmath3 ( or both ) are computationally prohibitive , so that additional effort required to implement cfs is negligible by comparison ; we illustrate this with two realistic applications the next section .",
    "two applications are considered that together present many of the challenges associated with complex models .",
    "firstly we consider marginalisation of hyper - parameters in hierarchical models , focussing on a non - parametric prediction problem . here",
    "evaluation of @xmath3 forms a computational bottleneck due to the required inversion of a large matrix . for this problem",
    ", cfs are shown to offer significant computational savings .",
    "secondly we consider computation of normalising constants for models based on non - linear ordinary differential equations ( odes ) .",
    "evaluation of the likelihood function requires numerical integration of a system of odes and dominates computational expenditure in both sampling from @xmath4 and evaluation of @xmath3 . here",
    "cfs combine with gradient - based population mcmc and thermodynamic integration in order to deliver a state - of - the - art technique for low - variance estimation of normalising constants .",
    "a fully bayesian treatment of hierarchical models aims to marginalise over hyper - parameters , but this often entails a prohibitive level of computation . here we explore the efficacy of cfs in such situations .",
    "the marginalisation of hyper - parameters is a common problem in spatial statistics and bayesian statistics in general @xcite .",
    "here we consider one such model that is based on @xmath194-dimensional gp regression .",
    "denote by @xmath195 a measured response variable at state @xmath196 , assumed to satisfy @xmath197 where @xmath198 are independent for @xmath199 and @xmath200 will be assumed known . in order to use training data",
    "@xmath201 to make predictions regarding an unseen test point @xmath202 , we place a gp prior @xmath203 where @xmath204 . here",
    "@xmath205 are hyper - parameters that control how training samples are used to predict the response at a new test point . in the fully - bayesian framework",
    "these are assigned hyper - priors , say @xmath206 , @xmath207 in the shape / scale parametrisation , which we write jointly as @xmath208 .",
    "we are interested in predicting the value of the response @xmath209 corresponding to an unseen state vector @xmath202 .",
    "our estimator will be the bayesian posterior mean given by @xmath210 = \\int \\mathbb{e}[y_*|\\bm{y},\\bm{\\theta } ] \\pi(\\bm{\\theta } ) \\mathrm{d}\\bm{\\theta } , \\label{robot target}\\end{aligned}\\ ] ] where we implicitly condition on the covariates @xmath211 .",
    "[ robot target ] is unavailable in closed form and we therefore naive a monte carlo estimate by sampling @xmath212 independently from the prior @xmath208 ( more efficient qmc estimates are considered later ) .",
    "phrasing in terms of our previous notation , the function of interest is @xmath213 = \\bm{c}_{*,n } ( \\bm{c}_n + \\sigma^2 \\bm{i}_{n \\times n})^{-1 } \\bm{y}\\end{aligned}\\ ] ] where @xmath214 and @xmath215 and the underlying distribution is @xmath208 .",
    "each evaluation of the integrand @xmath216 requires @xmath217 operations due to the matrix inversion ; this can be reduced by employing a `` subset of regressors '' approximation @xmath218 where @xmath219 denotes a subset of the full data ( see sec .",
    "8.3.1 of * ? ?",
    "* for full details ) . to facilitate the illustration below , which investigates the sampling distribution of estimators , we take a random subset of @xmath220 training points and a subset of regressors approximation with @xmath221",
    ". however we emphasise that evaluation of eqn .",
    "[ sor approx ] will typically be based on much larger @xmath222 and @xmath223 and will be extremely expensive in general . in applications we would therefore have to proceed with monte carlo estimation based on only a small number @xmath0 of these function evaluations .",
    "we used the hierarchical gp model in sec .",
    "[ hgp ] to estimate the inverse dynamics of a seven degrees - of - freedom sarcos anthropomorphic robot arm .",
    "the task , as described in ( * ? ? ? * sec .",
    "8.3.1 ) , is to map from a 21-dimensional input space ( 7 positions , 7 velocities , 7 accelerations ) to the corresponding 7 joint torques using the hierarchical gp model described in sec .",
    "[ hierarchical gp ] . following @xcite we present results below on just one of the mappings , from the 21 input variables to the first of the seven torques .",
    "the dataset consists of 48,933 input - output pairs , of which 44,484 were used as a training set and the remaining 4,449 were used as a test set .",
    "the inputs were translated and scaled to have mean zero and unit variance on the training set .",
    "the outputs were centred so as to have mean zero on the training set . here",
    "@xmath224 , @xmath225 , @xmath226 , so that each hyper - parameter @xmath227 has a prior mean of @xmath228 and a prior standard deviation of @xmath229 .",
    "for each test point @xmath202 we estimated the sampling standard deviation of @xmath230 over 10 independent realisations of the monte carlo sampling procedure .",
    "for cf we took default hyper - parameters @xmath186 , @xmath231 , the latter reflecting the fact that the training data were standardised .",
    "the estimator standard deviations were estimated in this way for all 4,449 test samples and the full results are shown in fig .",
    "[ robot ] .",
    "note that each test sample corresponds to a different function @xmath3 and thus these results are quite objective , encompassing thousands of different monte carlo integration problems .",
    "results show that , for the vast majority of integration problems , cf achieves a lower estimator variance compared with both the arithmetic mean estimator and zv control variates . here",
    "the cost of post - processing the monte carlo samples ( using either zv control variates or cf ) is negligible in comparison to the cost of evaluating the function @xmath3 , even once .",
    "indeed , cf requires that we invert a @xmath232 matrix once , where @xmath0 is no larger than @xmath223 in this example .    in the supplement",
    "we investigate an extension that draws design points @xmath50 using rqmc .",
    "results show that cfs+rqmc outperforms rqmc alone .      our second application concerns the estimation of normalising constants for non - linear ode models ( e.g. * ?",
    "recent empirical investigations recommend thermodynamic integration ( ti ) for this task ( e.g. * ? ? ?",
    "the control variate method of @xcite was recently applied to ti by @xcite , who found that this `` controlled thermodynamic integral '' ( cti ) was extremely effective for standard regression models , but only moderately effective in complex models including non - linear odes due to poor approximation by low - degree polynomials .",
    "below we study the application of cfs to ti in this setting where cti is less effective .",
    "conditional on an inverse temperature parameter @xmath233 , the `` power posterior '' for parameters @xmath234 given data @xmath235 is defined as @xmath236 @xcite .",
    "varying @xmath237 $ ] produces a continuous path between the prior @xmath238 and the posterior @xmath239 and it is assumed here that all intermediate distributions exist and are well - defined .",
    "the standard thermodynamic identity is @xmath240 \\mathrm{d}t , \\label{ml}\\end{aligned}\\ ] ] where the expectation in the integrand is with respect to the power posterior . in ti ,",
    "the one - dimensional integral in eqn .",
    "[ ml ] is evaluated numerically using a quadrature approximation over a discrete temperature ladder @xmath241 .",
    "here we use the second - order quadrature recommended by @xcite : @xmath242 where @xmath243 , @xmath244 are monte carlo estimates of the posterior mean and variance respectively of @xmath245 when @xmath234 arises from @xmath246 .",
    "cti uses zv control variates to reduce the variance of these estimates .",
    "however , in complex models @xmath245 will be poorly approximated by a low - degree polynomial and @xmath247 will be non - gaussian ; this explains the mediocre performance of cti in these cases .",
    "in contrast , cfs should still be able to deliver gains in estimation .",
    "the approach is illustrated by computing the marginal likelihood for a non - linear ode model ( the van der pol oscillator ) , described in full in the supplement .",
    "for ti , a temperature schedule @xmath248 was used , following the recommendation by @xcite .",
    "the power posterior is not available in closed form , precluding the straight - forward generation of iid samples .",
    "instead , samples from each of the power posteriors @xmath246 were obtained using population mcmc , involving both ( i ) `` within - temperature '' proposals produced by the ( simplified ) m - mala algorithm of @xcite , and ( ii ) `` between - temperature '' proposals , as described previously by @xcite .",
    "gradient information is thus pre - computed in the sampling scheme and can be leveraged `` for free '' , as noted by @xcite .",
    "we denote the number of samples by @xmath0 , such that for each of the 31 temperatures we obtained @xmath0 samples ( a total of @xmath249 occasions where the system of odes was integrated numerically ) .",
    "both sampling and evaluation of the integrand are computationally expensive , requiring the numerical solution of a system of odes .",
    "results in fig .",
    "[ ode ] show that the cti estimator improves upon the standard ti estimator , but a more substantial reduction in estimator variance results from using the cf method . for the cf computation",
    "we have used the simplified but biased cf estimator , since ti in any case produces a biased estimate for the normalising constant due to numerical quadrature .",
    "the hyper - parameters @xmath186 , @xmath250 were selected on the basis of cross - validation .",
    "the additional cost of using cf is essentially zero relative to running the population mcmc sampler , the latter requiring repeated solution of the ode system .",
    "this paper developed a novel and general approach to integration that achieves super - root-@xmath0 convergence .",
    "an important feature of cfs is that variance reduction is formulated as a _ post - hoc _ step .",
    "this has several advantages : ( i ) no modification is required to existing computer code associated with either the sampling process or the model itself .",
    "( ii ) specific implementational choices , e.g. for the kernel , can be made _",
    "after _ performing expensive simulations . through exploitation of recent results in functional analysis",
    "we were able to realise our general framework and construct estimators with an analytic form .",
    "empirical results evidenced the practical utility of cf estimators in settings where gradient information is available and the dimensionality of the problem is not too large ( e.g. @xmath251 ) .",
    "the paper concludes below by suggesting directions for further research .    in terms of methodology :",
    "( i ) the estimates we presented here are not parameterisation - invariant .",
    "likewise the specification of @xmath3 and @xmath4 is not unique , as we can employ an importance sampling transformation @xmath252 , @xmath253",
    ". it would therefore be interesting to elicit effective parametrisations as an additional _ post - hoc _ step .",
    "( ii ) the version of cfs presented here is limited in terms of the dimension of the problems for which it is effective .",
    "techniques for high - dimensional functional approximation should be applicable in the context of cfs ( e.g. * ? ? ? * ) and this forms part of our ongoing research .    in terms of theory",
    ": ( i ) for bounded @xmath23 , sharper asymptotics are provided in a sequel , @xcite . these account for various levels of smoothness of both @xmath3 and @xmath4 and",
    "help to explain the strong empirical results presented here .",
    "however the case of unbounded @xmath23 seems considerably more challenging to characterise .",
    "( ii ) for problems involving un - normalised densities @xmath4 , sampling is naturally facilitated by mcmc .",
    "the analysis of cfs is carried out in @xcite under a uniform ergodicity assumption . for unbounded @xmath23",
    "this condition is too strong and future work will aim to relax this constraint .    in terms of application : ( i ) our methods were motivated by the un - normalised densities arising in bayesian computation .",
    "an extension should be possible to models with unknown , parameter - dependent normalising constants , which include e.g. markov random fields @xcite and random network models @xcite .",
    "( ii ) an interesting direction would be to use the discrepancy @xmath164 as a tool for assessment of mcmc convergence , providing a reproducing kernel hilbert space alternative to @xcite .",
    "finally we note that @xcite provide a complementary study of cf strategies in the qmc setting .    [ [ acknowledgements ] ] acknowledgements : + + + + + + + + + + + + + + + + +    the authors are grateful to the editor and referees , whose valuable feedback helped to improve the paper .",
    "the authors benefited from discussions with sergios agapiou , michel caffarel , adam johansen , christian robert , daniel simpson and tim sullivan .",
    "cjo was supported by epsrc [ ep / d002060/1 ] and the arc centre for excellence in mathematical and statistical frontiers .",
    "mg was supported by epsrc [ ep / j016934/1 ] , eu [ eu/259348 ] , an epsrc established career fellowship and a royal society wolfson research merit award .",
    "nc was supported by the anr ( agence nationale de la recherche ) grant labex ecodec anr [ 11-labex-0047 ] .",
    "we exploit the unbiasedness property @xmath254 = 0 $ ] to show that @xmath255 & = & \\mathbb{e}_{\\mathcal{d}_0 } \\mathbb{v}_{\\mathcal{d}_1 } [ \\hat{\\mu}(\\mathcal{d}_0,\\mathcal{d}_1;f ) ] \\\\ & = & ( n - m)^{-1 } \\mathbb{e}_{\\mathcal{d}_0 } [ \\sigma^2(f - s_{f,\\mathcal{d}_0})]\\end{aligned}\\ ] ] where @xmath256 and by hypothesis @xmath257 = o(m^{-\\delta})$ ] . thus using @xmath258 produces an overall rate @xmath259 , as required .",
    "( a1 ) ensures @xmath86 is well - defined .",
    "since @xmath27 is piecewise smooth we can apply the divergence theorem ( e.g. * ? ? ?",
    "* ) to obtain @xmath260 \\mathrm{d}\\bm{x } = \\oint_{\\partial\\omega } \\pi(\\bm{x } ) \\bm{\\phi}(\\bm{x } ) \\cdot \\bm{n}(\\bm{x } ) s(\\mathrm{d}\\bm{x } ) , \\end{aligned}\\ ] ] which is zero by ( a2 ) .",
    "the use of this identity in statistical applications is often attributed to @xcite .",
    "thus @xmath261 , as required .",
    "_ stage 1 : _ we begin by defining the set of cfs @xmath103 . given a reproducing kernel @xmath93 for the reproducing kernel hilbert space ( rkhs ) @xmath92 ,",
    "define the canonical feature map @xmath262 by @xmath263 . under ( a1 )",
    "the gradient function @xmath264 is well - defined . under ( a3 )",
    "@xmath33 has mixed first order partial derivatives ; it follows that all elements @xmath265 are differentiable and thus @xmath266 is well - defined ( * ? ? ?",
    "* cor . 4.36 , p131 ) .",
    "we then have that @xmath267 where we have used the notation @xmath268 .",
    "write @xmath269 for the derived feature map with @xmath270th component @xmath271 .",
    "define the set of all cfs @xmath86 of this form as @xmath272 clearly @xmath103 is a vector space with addition and multiplication defined pointwise ; @xmath273 .",
    "_ stage 2 : _ we now show that @xmath103 can be endowed with the structure of a rkhs . to this end , define a norm on @xmath103 by @xmath274 theorem 4.21 ( p121 ) of @xcite immediately gives that the normed space @xmath275 is a rkhs whose kernel @xmath107 satisfies @xmath276 .",
    "thus we can directly calculate @xmath277 where the interchange of derivative and inner product is justified by ( a3 ) and lemma 4.34 ( p130 ) in @xcite .",
    "this completes the proof .",
    "( a1,3 ) ensure the kernel @xmath107 is well - defined",
    ". then @xmath278 \\pi(\\bm{x } ' ) \\mathrm{d}\\bm{x } ' + \\int_{\\omega } [ \\bm{u}(\\bm{x } ) \\cdot \\nabla_{\\bm{x } ' } k(\\bm{x},\\bm{x } ' ) ] \\pi(\\bm{x } ' ) \\mathrm{d}\\bm{x } ' \\\\ & & + \\int_{\\omega } [ \\bm{u}(\\bm{x } ' ) \\cdot \\nabla_{\\bm{x } } k(\\bm{x},\\bm{x } ' ) ] \\pi(\\bm{x } ' ) \\mathrm{d}\\bm{x } ' + \\int_{\\omega } [ \\bm{u}(\\bm{x } ) \\cdot \\bm{u}(\\bm{x } ' ) k(\\bm{x},\\bm{x } ' ) ] \\pi(\\bm{x } ' ) \\mathrm{d}\\bm{x } ' \\\\ & = & \\int_{\\omega } [ \\nabla_{\\bm{x } ' } \\cdot \\nabla_{\\bm{x } } k(\\bm{x},\\bm{x } ' ) ] \\pi(\\bm{x } ' ) + [ \\nabla_{\\bm{x } } k(\\bm{x},\\bm{x } ' ) ] \\cdot [ \\nabla_{\\bm{x } ' } \\pi(\\bm{x } ' ) ] \\mathrm{d}\\bm{x } ' \\\\ & & + \\bm{u}(\\bm{x } ) \\cdot \\int_{\\omega } [ \\nabla_{\\bm{x } ' } k(\\bm{x},\\bm{x } ' ) ] \\pi(\\bm{x } ' ) + k(\\bm{x},\\bm{x } ' ) [ \\nabla_{\\bm{x } ' } \\pi(\\bm{x } ' ) ] \\mathrm{d}\\bm{x } ' \\\\ & = & \\int_{\\omega } \\nabla_{\\bm{x } ' } \\cdot \\left\\ { [ \\nabla_{\\bm{x } } k(\\bm{x},\\bm{x } ' ) ] \\pi(\\bm{x } ' ) \\right\\ } \\mathrm{d}\\bm{x } ' + \\bm{u}(\\bm{x } ) \\cdot \\int_{\\omega } \\nabla_{\\bm{x } ' } \\left\\ { k(\\bm{x},\\bm{x } ' ) \\pi(\\bm{x } ) \\right\\ } \\mathrm{d}\\bm{x}'.\\end{aligned}\\ ] ] now using the divergence theorem @xcite we obtain @xmath279 proving the claim .    from theorem [ conjugate ] ,",
    "( a1,3 ) ensure @xmath103 is well - defined .",
    "moreover , from lemma [ mean ele lem ] , ( a1,2,3 ) imply that @xmath80 and thus @xmath280 now , given @xmath129 , we need to show @xmath281 . by the reproducing property followed by the cauchy - schwarz inequality , we have @xmath282 and it follows from ( a4 ) that @xmath283 as required .    from theorem [ conjugate ] ,",
    "( a1,3 ) ensure @xmath118 is well - defined .",
    "unbiasedness follows from ( a1,2,3 ) and lemma [ finite variance ] .",
    "below we employ the standard notation @xmath284 and denote the standard norm on this space by @xmath285 .    for the remainder we appeal to the relatively recent work of @xcite , who considered convergence in a general setting where ( i ) @xmath286 is not required to be compact in @xmath287 , and ( ii )",
    "only weak assumptions are required on the kernel @xmath288 , which can be easily satisfied in our setting . to this end , define the integral operator @xmath289 in the well - posed setting of ( a5 ) , theorem 1.1 of @xcite establishes that if ( i ) @xmath290 and ( ii ) @xmath291 , then with a rls estimator based on @xmath138 we have @xmath292 = o(m^{-1/6})$ ] . inserting this rate into proposition [ theo1 ] with @xmath67",
    ", @xmath293 would produce a mse @xmath66 = o(n^{-1-\\gamma\\delta } ) = o(n^{-7/6})$ ] .",
    "it therefore remains to prove requirements ( i ) and ( ii ) above are satisfied . for ( i )",
    "we have that @xmath294 , where the second term is finite by ( a4 ) . for ( ii ) ,",
    "3.3 of @xcite ( which does not depend on theorem 1.1 of the same paper ) shows that , when ( i ) holds , we have @xmath295 for all @xmath296 . since @xmath127 by ( a5 )",
    "we thus have @xmath297 and so @xmath298 , as required .    from theorem [ conjugate ] , ( a1,3 ) ensure @xmath103 is well - defined .",
    "the interpolation problem is equivalently expressed as @xmath299 where @xmath300 for fixed @xmath128 , the representer theorem ( * ? ? ?",
    "5.5 , p168 ) tells us that the solution @xmath301 takes the form @xmath302 where , due to the reproducing property , @xmath303 . thus writing @xmath304^t$ ] reduces the problem to @xmath305 differentiating with respect to @xmath306 and @xmath307 leads , via the woodbury matrix inversion identity , to the solution @xmath308 and associated fitted values @xmath309 at the points @xmath56 .",
    "putting this together , we have @xmath310 this completes the proof .    from theorem [ conjugate ] ,",
    "( a1,3 ) ensure @xmath103 is well - defined .",
    "the cf estimator takes the form @xmath311 where , by lemma [ explicit formulae ] , the vector of weights @xmath312^t$ ] is given by @xmath313 \\label{weights vector}\\end{aligned}\\ ] ] and satisfies @xmath314 .",
    "using the reproducing property , the estimation error is @xmath315 it follows from the cauchy - schwarz inequality that @xmath316 the first term satisfies @xmath317 and , from the reproducing property , the second term satisfies @xmath318 .",
    "\\label{final form}\\end{aligned}\\ ] ] finally , upon substituting eqn .",
    "[ weights vector ] into eqn .",
    "[ final form ] we obtain the required result with @xmath319 .",
    "the special case @xmath320 is reported in the statement of the theorem .",
    "angelikopoulos , p. , papadimitriou , c. and koumoutsakos , p. ( 2012 ) bayesian uncertainty quantification and propagation in molecular dynamics simulations : a high performance computing framework .",
    "_ j. chem .",
    "_ , * 137 * , 144103 .",
    "ghosh , j. and clyde , m. a. ( 2011 ) rao - blackwellization for bayesian variable selection and model averaging in linear and binary regression : a novel data augmentation approach .",
    "_ , * 106*(495 ) , 1041 - 1052 .",
    "higdon , d. , mcdonnell , j. d. , schunck , n. , sarich , j. and wild , s. m. ( 2015 ) a bayesian approach for parameter estimation and prediction using a computationally intensive model .",
    "_ j. phys .",
    "_ , * 42 * , 034009 .",
    "kohlhoff , k. j. , shukla , d. , lawrenz , m. , bowman , g. r. , konerding , d. e. , belov , d. , altman , r. b. and pande , v. s. ( 2014 ) cloud - based simulations on google exacycle reveal ligand modulation of gpcr activation pathways .",
    "_ , * 6*(1 ) , 15 - 21 .",
    "mizielinski , m. s. , roberts , m. j. , vidale , p. l. , schiemann , r. , demory , m. e. , strachan , j. , edwards , t. , stephens , a. , lawrence , b. n. , pritchard , m. , chiu , p. , iwi , a. , churchill , j. , del cano novales , c. , kettleborough , j. , roseblade , w. , selwood , p. , foster , m. , glover , m. and malcolm , a. ( 2014 ) high - resolution global climate modelling : the upscale project , a large - simulation campaign .",
    "model dev .",
    "_ , * 7*(4 ) , 1629 - 1640 .",
    "slingo , j. , bates , k. , nikiforakis , n. , piggott , m. , roberts , m. , shaffrey , l. , stevens , l. , vidale , p. l. and weller , h. ( 2009 ) developing the next - generation climate system models : challenges and achievements .",
    "t. r. soc .",
    "a _ , * 367 * , 815 - 831 ."
  ],
  "abstract_text": [
    "<S> a non - parametric extension of control variates is presented . </S>",
    "<S> these leverage gradient information on the sampling density to achieve substantial variance reduction . </S>",
    "<S> it is not required that the sampling density be normalised . </S>",
    "<S> the novel contribution of this work is based on two important insights ; ( i ) a trade - off between random sampling and deterministic approximation and ( ii ) a new gradient - based function space derived from stein s identity . unlike classical control variates , </S>",
    "<S> our estimators achieve super - root-@xmath0 convergence , often requiring orders of magnitude fewer simulations to achieve a fixed level of precision . </S>",
    "<S> theoretical and empirical results are presented , the latter focusing on integration problems arising in hierarchical models and models based on non - linear ordinary differential equations .    </S>",
    "<S> _ keywords : _ control variates , non - parametric , reproducing kernel , stein s identity , variance reduction </S>"
  ]
}