{
  "article_text": [
    "this section contains the proofs of our three theorems .",
    "our proofs are constructive in nature , based on a procedure that constructs pair of matrices @xmath27 and @xmath28 .",
    "the goal of the construction is to show that matrix @xmath29 is an optimal primal solution to the convex program  , and that the matrix @xmath30 is a corresponding dual - optimal solution , meaning that it belongs to the sub - differential of the @xmath31-norm ( see lemma  [ lemsubdiff ] ) , evaluated at @xmath29 .",
    "if the construction succeeds , then the pair @xmath32 acts as a witness for the success of the convex program   in recovering the correct signed support  in particular , success of the primal - dual witness procedure implies that @xmath29 is the unique optimal solution of the convex program  , with its row support contained with @xmath33 . to be clear",
    ", the procedure for constructing this candidate primal - dual solution is _ not _ a practical algorithm ( as it exploits knowledge of the true support sets ) , but rather a proof technique for certifying the correctness of the block - regularized program .",
    "we begin by providing some background on the sub - differential of the @xmath4 norm ; we refer the reader to the books  @xcite for more background on convex analysis .",
    "the sub - differential of a convex function @xmath34 at a point @xmath35 is the set of all vectors @xmath36 such that @xmath37 for all @xmath38 .",
    "see the standard references  @xcite for background on subdifferentials and their properties .",
    "we state for future reference a characterization of the sub - differential of the @xmath4 block norm :    [ lemsubdiff ] the matrix @xmath39 belongs to the sub - differential @xmath40 if and only if the following conditions hold for each @xmath41 .    1 .",
    "if @xmath42 for at least one index @xmath43 , then @xmath44 where @xmath45 such that @xmath46 .",
    "2 .   if @xmath47 for all @xmath48 , then we require @xmath49 .",
    "we now describe our method for constructing the matrix pair @xmath50 .",
    "recalling that denotes the union of supports of the true regression vectors , let @xmath51 denote the complement of @xmath52",
    ". with this notation , figure  [ figwitness ] provides the four steps of the primal - dual witness construction .",
    "the following lemma summarizes the utility of the primal - dual witness method :    [ lemkey ] suppose that for each @xmath53 , the @xmath54 sub - matrix @xmath55 is invertible . then for any @xmath56 , we have the following correspondences :    1 .   if steps ( a ) through ( c ) of the primal - dual construction succeed , then @xmath57 is the unique optimal solution of the original convex program  .",
    "conversely , suppose that there is a solution @xmath58 to the convex program   with support contained within @xmath33",
    ". then steps ( a ) through ( c ) of the primal - dual witness construction succeed .",
    "we provide the proof of lemma  [ lemkey ] in appendix  [ applemkey ] .",
    "it is convex - analytic in nature , based on exploiting the subgradient optimality conditions associated with both the restricted convex program   and the original program  , and performing some algebra to characterize when the convex program recovers the correct signed support .",
    "lemma  [ lemkey ] lies at the heart of all three of our theorems .",
    "in particular , the positive results of theorem  [ thmdetdesign ] , theorem  [ thmgauss ] and theorem  [ thmphase](a ) are based on claims ( i ) and ( iii ) , which show that it is sufficient to verify that the primal - dual witness construction succeeds with high probability .",
    "the negative result of theorem  [ thmphase](b ) , in contrast , is based on part ( ii ) , which can be restated as asserting that if the primal - dual witness construction fails , then no solution has support contained with @xmath33 .    before proceeding to the proofs themselves ,",
    "we introduce some additional notation and develop some auxiliary results concerning the primal - dual witness procedure , to be used in subsequent development .",
    "with reference to steps ( a ) and ( b ) , we show in appendix  [ applemkey ] that unique solution @xmath59 has the form @xmath60 where the matrix @xmath61 has columns @xmath62 , \\qquad \\mbox{for $ { \\ensuremath{i}}= 1 , \\ldots , { \\ensuremath{r}}$,}\\end{aligned}\\ ] ] and @xmath63 is the @xmath64 column of the sub - gradient matrix @xmath65 .    with reference to step ( c ) ,",
    "we obtain the candidate dual solution @xmath66 as follows . for each @xmath53 , let @xmath67 denote the orthogonal projection onto the range of @xmath68 . using the sub - matrix @xmath69 obtained from step ( b ) ,",
    "we define column @xmath70 of the matrix @xmath71 as follows : @xmath72 see the end of appendix  [ applemkey ] for derivation of this condition .",
    "finally , in order to further simplify notation in our proofs , for each @xmath73 , we define the random variable @xmath74 with this notation , the strict dual feasibility condition   is equivalent to the event",
    "we begin by establishing a set of sufficient conditions for deterministic design matrices , as stated in theorem  [ thmdetdesign ] .",
    "we begin by obtaining control on the probability of the event @xmath75 , so as to show that step ( c ) of the primal - dual witness construction succeeds . recall that @xmath67 denotes the orthogonal projection onto the range space of @xmath68 , and the definition   of the incoherence parameter @xmath76 $ ] . by the mutual incoherence condition",
    ", we have @xmath77 where we have used the fact that @xmath78 for each @xmath79 . recalling that and using the definition",
    ", we have by triangle inequality @xmath80 \\ ; \\leq \\ ; { { \\ensuremath{\\mathbb{p}}}}[{\\ensuremath{\\mathbb{a}}}({\\ensuremath{\\gamma}})],\\ ] ] where we have defined the event @xmath81    to analyze this remaining probability , for each index @xmath82 and @xmath73 , define the random variable @xmath83 since the elements of the @xmath84-vector @xmath85 follow a @xmath86 distribution , the variable @xmath87 is zero - mean gaussian with variance @xmath88 . since @xmath89 by assumption and @xmath90 is an orthogonal projection matrix , the variance of each @xmath87 is upper bounded by @xmath91 .",
    "consequently , for any choice of sign vector @xmath92 , the variance of the zero - mean gaussian @xmath93 is upper bounded by @xmath94 .",
    "consequently , by taking the union bound over all sign vectors and over indices @xmath73 , we have @xmath95 \\ ; = \\ ; { \\ensuremath{\\mathbb{p}}}\\big[\\max_{{\\ensuremath{k}}\\in { \\ensuremath{{u^{c } } } } }       \\max_{{\\ensuremath{b}}\\in \\{-1,+1\\}^{\\ensuremath{r } } } \\sum_{{\\ensuremath{i}}=1}^{\\ensuremath{r}}{\\ensuremath{b}}_{\\ensuremath{i}}{\\ensuremath{w^{{\\ensuremath{i}}}_{{\\ensuremath{k } } } } } > { \\ensuremath{\\gamma}}\\big ] & \\leq & 2 \\ ,       \\exp \\big ( -\\frac{{\\lambda_n}^2 { \\ensuremath{n}}{\\ensuremath{\\gamma}}^2}{4 { \\ensuremath{r}}\\sigma^2 } + { \\ensuremath{r}}+ \\log { \\ensuremath{p}}\\big).\\end{aligned}\\ ] ] with the choice @xmath96 for some @xmath97 , we conclude that @xmath98 & \\geq & 1 - 2 \\exp(-({\\ensuremath{\\xi}}-1 ) [ { \\ensuremath{r}}+ \\log { \\ensuremath{p } } ] ) \\ ; \\rightarrow \\ ; 1.\\end{aligned}\\ ] ] by lemma  [ lemkey](i ) , this event implies the uniqueness of the solution @xmath99 , and moreover the inclusion of the supports @xmath100 , as claimed .",
    "we now turn to establishing the claimed @xmath5-bound   on the difference @xmath101 .",
    "we have already shown that this difference is exactly zero for rows in @xmath51 ; it remains to analyze the difference @xmath102 .",
    "it suffices to prove the @xmath5 bound for the columns @xmath103 separately , for each @xmath104 .",
    "we split the analysis of the random variable @xmath105 from equation  , one involving the dual variables @xmath63 , and the other involving the observation noise @xmath85 , as follows : @xmath106 the second term is easy to control : from the characterization of the subdifferential ( lemma  [ lemsubdiff ] ) , we have @xmath107 , so that @xmath108 .    turning to the first term @xmath109",
    ", we note that since @xmath68 is fixed , the @xmath110-dimensional random vector @xmath111 is zero - mean gaussian , with covariance @xmath112 .",
    "therefore , we have @xmath113 , and can use this in standard gaussian tail bounds . by applying the union bound twice , first over @xmath114 , and then over @xmath115 , we obtain @xmath116 & \\leq & 2 \\exp(-t^2 { \\ensuremath{n}}{\\ensuremath{c_{\\min}}}/(2 ) + \\log ( { \\ensuremath{r}}{\\ensuremath{s } } ) + \\log { \\ensuremath{r}}),\\end{aligned}\\ ] ] where we have used the fact that @xmath117 . setting @xmath118 yields that @xmath119 with probability greater than @xmath120 , as claimed .",
    "finally , to establish support recovery , recall that we proved above that @xmath121 is bounded by @xmath122 .",
    "hence , as long as @xmath123 , then we are guaranteed that if @xmath124 , then @xmath125 .",
    "we now turn to the proof of theorem  [ thmgauss ] , providing sufficient conditions for general gaussian ensembles .",
    "recall that for @xmath126 , each @xmath127 is a random design matrix , with rows drawn i.i.d . from a zero - mean gaussian with @xmath128 covariance matrix @xmath129 .",
    "recalling that and using the definition  , we have the decomposition @xmath130 in order to show that @xmath131 with high probability , we deal with each of these two terms in turn , showing that @xmath132 , and @xmath133 , both with high probability .    in order to bound @xmath134",
    ", we require the following condition on the columns of the design matrices :    [ lemcolcontrol ] let @xmath135 . for @xmath136 , each column of the design matrices",
    "@xmath137 has controlled @xmath138-norm : @xmath139 & \\leq & 2 \\exp \\big ( -\\frac{{\\ensuremath{n}}}{2 } + \\log ( { \\ensuremath{p}}{\\ensuremath{r } } ) \\big ) \\ ; \\rightarrow 0.\\end{aligned}\\ ] ]    this claim follows immediately by union bound and concentration results for @xmath140-variates ; in particular , the bound   in appendix  [ applargedev ] .    under the condition of lemma  [ lemcolcontrol ] , each variable @xmath141 is zero - gaussian , with variance at most @xmath142 .",
    "consequently , for any choice of signs @xmath92 , the vector @xmath93 is zero - mean gaussian , with variance at most @xmath143 .",
    "therefore , for any @xmath144 , we have @xmath145 & = & { { \\ensuremath{\\mathbb{p}}}}[\\max_{{\\ensuremath{k}}\\in      { \\ensuremath{{u^{c } } } } } \\max_{{\\ensuremath{b}}\\in \\{-1,+1\\}^{\\ensuremath{r } } }    \\sum_{{\\ensuremath{i}}=1}^{\\ensuremath{r}}{\\ensuremath{b}}_{\\ensuremath{i}}{\\ensuremath{w^{{\\ensuremath{i}}}_{{\\ensuremath{k } } } } } \\geq t ] \\\\ & \\leq & 2 \\ ; \\exp \\big ( -\\frac{{\\lambda_n}^2 { \\ensuremath{n}}}{4 \\sigma^2 { \\ensuremath{r } } } t^2 + { \\ensuremath{r}}+ \\log { \\ensuremath{p}}\\big)\\end{aligned}\\ ] ] setting @xmath146 yields that @xmath147 & \\leq & 2 \\ ; \\exp \\big ( -\\frac{{\\lambda_n}^2 { \\ensuremath{n}}}{16 \\sigma^2 { \\ensuremath{r } } } { \\ensuremath{\\gamma}}^2 + { \\ensuremath{r}}+ \\log { \\ensuremath{p}}\\big).\\end{aligned}\\ ] ]    [ lempopmutinco ] suppose that the design covariance matrices @xmath148 satisfy the mutual incoherence condition",
    ". then we have @xmath149 where each random vector @xmath150 has i.i.d .",
    "@xmath151 entries , and is independent of @xmath85 and @xmath68 .",
    "see appendix  [ applempopmutinco ] for the proof of this claim .",
    "+ it remains to show that the random variable @xmath152 defined in equation   is upper bounded by @xmath153 with high probability .",
    "conditioning on @xmath68 and @xmath85 , the scalar random variable @xmath154 is zero - mean gaussian , with variance upper bounded as @xmath155 recalling that @xmath78 , for any choice of signs @xmath92 , the variable @xmath156 is zero - mean gaussian , with variance at most @xmath157 .",
    "therefore , we have @xmath158 & \\leq & { { \\ensuremath{\\mathbb{p}}}}\\big[\\max_{{\\ensuremath{k}}\\in { \\ensuremath{{u^{c } } } } } \\max_{{\\ensuremath{b}}\\in \\{-1,+1 \\}^{\\ensuremath{r } } }    { \\ensuremath{\\big\\langle { \\ensuremath{y^{{\\ensuremath{i}}}_{{\\ensuremath{k } } } } } , \\ , { \\ensuremath{x^{{\\ensuremath{i}}}_{{\\ensuremath{u } } } } } ( { \\ensuremath{\\langle \\frac{1}{{\\ensuremath{n } } } { \\ensuremath{x^{{\\ensuremath{i}}}_{{\\ensuremath{u } } } } } , \\ , { \\ensuremath{x^{{\\ensuremath{i}}}_{{\\ensuremath{u } } } } } \\rangle}})^{-1 } { \\ensuremath{\\widetilde{z}^{\\,{\\ensuremath{i}}}_{{\\ensuremath{u } } } } } \\big\\rangle}}| \\geq { \\ensuremath{\\gamma}}/2 \\big ] \\\\ & \\leq & 2 \\exp ( -\\frac{{\\ensuremath{c_{\\min}}}{\\ensuremath{n}}}{8 { \\ensuremath{r}}{\\ensuremath{s } } }   { \\ensuremath{\\gamma}}^2 + { \\ensuremath{r}}+ \\log { \\ensuremath{p}}).\\end{aligned}\\ ] ]    this probability vanishes faster than @xmath159 , as long as @xmath160      we now turn to establishing the claimed @xmath5-bound   on the difference @xmath101 . as in the analogous portion of the proof of theorem  [ thmdetdesign ] , we use the decomposition @xmath161 in the setting of random design matrices , a bit more work is required to control these terms .",
    "beginning with the second term , by triangle inequality , we have @xmath162 \\|_\\infty + \\big \\|      \\big({\\ensuremath{\\sigma^{{\\ensuremath{i}}}_{{\\ensuremath{u}}{\\ensuremath{u}}}}})^{-1 } { \\lambda_n}{\\ensuremath{\\widetilde{z}^{\\,{\\ensuremath{i}}}_{{\\ensuremath{u } } } } } \\big \\|_\\infty \\\\ & \\leq & { |\\!|\\!| \\big[\\big ( { \\ensuremath{\\langle \\frac{1}{{\\ensuremath{n } } }      { \\ensuremath{x^{{\\ensuremath{i}}}_{{\\ensuremath{u } } } } } , \\ , { \\ensuremath{x^{{\\ensuremath{i}}}_{{\\ensuremath{u } } } } } \\rangle } } \\big ) ^{-1 } -      ( { \\ensuremath{\\sigma^{{\\ensuremath{i}}}_{{\\ensuremath{u}}{\\ensuremath{u}}}}})^{-1 } \\big ) | \\ ! | \\!|_{{2 } } } { \\lambda_n}\\sqrt{{\\ensuremath{s } } } + { \\ensuremath{d_{\\max}}}{\\lambda_n}\\end{aligned}\\ ] ] where we have used the facts that @xmath163 , since @xmath63 belongs to the sub - differential of the block @xmath4 norm ( see lemma  [ lemsubdiff ] ) so that @xmath164 for all @xmath79 . by , concentration bounds for eigenvalues of gaussian random matrices ( see equation   in appendix  [ applargedev ] ) ,",
    "we conclude that @xmath165.\\end{aligned}\\ ] ]    now consider the first term @xmath109 : if we condition on @xmath68 , then the @xmath110-dimensional random vector @xmath166 is zero - mean gaussian , with covariance @xmath167 . by concentration bounds for eigenvalues of gaussian random matrices ( see equation   in appendix  [ applargedev ] )",
    ", we have    @xmath168    since @xmath169 .",
    "therefore , we have shown that the variance of each element of @xmath170 is upper bounded by @xmath171 , so that we can apply standard gaussian tail bounds . by applying the union bound twice , first over @xmath114 , and then over @xmath172",
    ", we obtain @xmath116 & \\leq & 2 \\exp(-t^2 { \\ensuremath{n}}{\\ensuremath{c_{\\min}}}/(50 ) + \\log |{\\ensuremath{u}}| + \\log { \\ensuremath{r}}).\\end{aligned}\\ ] ] setting @xmath173 yields that @xmath116 & \\leq & 2 \\exp \\big \\ { -2 { \\ensuremath{\\xi}}^2 \\log ( { \\ensuremath{r}}{\\ensuremath{s } } ) + \\log ( { \\ensuremath{r}}{\\ensuremath{s } } ) + \\log { \\ensuremath{r}}\\big \\ } \\\\ & \\leq & 2 \\exp \\big \\ { -2 ( { \\ensuremath{\\xi}}^2 -1)\\log ( { \\ensuremath{r}}{\\ensuremath{s } } ) \\big \\},\\end{aligned}\\ ] ] where we have used the fact that @xmath117 . combining the pieces ,",
    "we conclude that @xmath174,\\end{aligned}\\ ] ] with probability greater than @xmath175 as claimed .",
    "we now turn to the proof of the phase transition predicted by theorem  [ thmphase ] , which applies to random design matrices @xmath176 and @xmath177 drawn from the standard gaussian ensemble .",
    "this proof requires significantly more technical work than the preceding two proofs , since we need to control all the constants exactly , and to establish both necessary and sufficient conditions on the sample size .",
    "we begin with the achievability result .",
    "our proof parallels that of theorems  [ thmdetdesign ] and  [ thmgauss ] , in that we first establish strict dual feasibility , and then turn to proving @xmath5 bounds and exact support recovery .      recalling that @xmath178",
    ", we have @xmath179 where the random variables @xmath134 and @xmath180 were defined at the start of section  [ subsecgaussrand ] . in order to prove that @xmath131 with high probability for the values of @xmath84 , @xmath181 , and @xmath1 , we will first establish that @xmath182 and @xmath183 for an appropriately chosen value of @xmath184 .    by the results from the previous section",
    ", we have @xmath185 with probability @xmath186 & \\leq & 2 \\ ; \\exp \\big ( -\\frac{{\\lambda_n}^2 { \\ensuremath{n}}\\epsilon^2}{32 \\sigma^2 }   + 2   + \\log { \\ensuremath{p}}\\big)\\end{aligned}\\ ] ]    recall that @xmath187 and that @xmath188 is independent of @xmath68 and @xmath85 .",
    "we will show that @xmath183 with high probability by using results on gaussian extrema .",
    "conditioning on @xmath189 , the random variable @xmath190 is zero - mean with variance upper - bounded as @xmath191 under the given conditioning , the random variables @xmath192 and @xmath193 are independent and for any sign vector @xmath194 , the random variable @xmath195 is gaussian , zero - mean with variance upper bounded as @xmath196    by lemma  [ lemrandmat ] , @xmath197 with probability at least @xmath198 for sufficiently large @xmath199 and @xmath6 under the given scaling for each @xmath70 .",
    "hence , @xmath195 is normal , zero - mean , with variance upper bounded as @xmath200    recall that @xmath65 was obtained from step ( b ) of the prima - dual witness construction .",
    "the next lemma provides control over @xmath201 .",
    "[ lemvarbound ] under the assumptions of theorem  [ thmphase ] and corollary  [ gapcor ] , if @xmath202 and @xmath203 , then @xmath204 is concentrated : for all @xmath11 , we have that for sufficiently large @xmath181 and @xmath84    @xmath205 &    \\rightarrow & 0 , \\qquad \\mbox{and } \\\\",
    "\\label{eqnvarupper } { { \\ensuremath{\\mathbb{p}}}}\\big [ \\|{{\\ensuremath{\\widetilde{z}^{\\,1}}}}_{\\ensuremath{u}}\\|^2_2 + \\|{{\\ensuremath{\\widetilde{z}^{\\,2}}}}_{\\ensuremath{u}}\\|^2_2 \\geq    ( 1+\\delta ) \\frac{s}{2 } \\big \\{(4 - 3 { \\alpha } ) + \\frac{1}{{\\lambda_n}^2    { \\ensuremath{s } } } { \\| { \\ensuremath{{\\ensuremath{\\widebar{b}}}_{\\operatorname{diff}}}}\\|}_2 ^ 2 \\big \\ } \\big ] & \\leq & c_1 \\exp (    -c_2 { \\ensuremath{n}}),\\end{aligned}\\ ] ]    see appendix  [ applemvarbound ] for the proof of this claim .    now , by applying the union bound and using gaussian tail bounds , we obtain that the probability @xmath206 $ ] is upper bounded by @xmath207 + \\log({\\ensuremath{p}}- ( 2-{\\alpha } )    { \\ensuremath{s } } ) \\big),\\ ] ] which goes to @xmath14 as @xmath208 under the condition @xmath209/(1-\\epsilon)^2 \\log({\\ensuremath{p}}- ( 2-{\\alpha } ) { \\ensuremath{s}}).\\end{aligned}\\ ] ]      we now turn to the proof of the converse claim in theorem  [ thmphase ] .",
    "we establish the claim by contradiction .",
    "we show that if a solution @xmath99 exists such that @xmath210 , then under the stated upper bound on the sample size @xmath84 , there exists some @xmath211 such that @xmath212 $ ] converges to one . from the definition",
    ", we see that conditioned on @xmath213 , the variables @xmath214 are i.i.d .",
    "zero - mean gaussians , with variance given by @xmath215 by orthogonality , we have @xmath216 , so that ( using the idempotency of projection operators ) , we have @xmath217 note that @xmath218 is a scalar random variable , but fixed under the conditioning . turning to the variables @xmath219 , a similar argument shows that have @xmath220 , where @xmath221 is the analogous random variable .    for @xmath73 ,",
    "let @xmath222 and @xmath223 .",
    "we then have @xmath224 & \\stackrel{(a)}{\\geq } &      { \\ensuremath{\\mathbb{p}}}[\\max_{{\\ensuremath{k}}\\in { \\ensuremath{{u^{c } } } } } { |{\\ensuremath{\\widetilde{z}^{\\,1}_{{\\ensuremath{k}}}}}| } +      { |{\\ensuremath{\\widetilde{z}^{\\,2}_{{\\ensuremath{k}}}}}| } > 1 + \\epsilon ] \\\\    & \\stackrel{}{\\geq } & { \\ensuremath{\\mathbb{p}}}[\\max_{{\\ensuremath{k}}\\in { \\ensuremath{{u^{c } } } } }      ( { \\ensuremath{\\widetilde{z}^{\\,1}_{{\\ensuremath{k } } } } } + { \\ensuremath{\\widetilde{z}^{\\,2}_{{\\ensuremath{k } } } } } ) > 1+\\epsilon ] \\\\    & \\stackrel{(b)}{= } & { \\ensuremath{\\mathbb{p}}}[\\max_{{\\ensuremath{k}}\\in { \\ensuremath{{u^{c } } } } } z_{\\ensuremath{k } } > 1 +      \\epsilon],\\end{aligned}\\ ] ] where @xmath225 .",
    "here inequality ( a ) follows because @xmath226 and @xmath227 are lower bounds on the variances of @xmath228 and @xmath229 respectively , and equality ( b ) follows since @xmath230 and @xmath231 are independent zero - mean gaussians with variances @xmath226 and @xmath227 , respectively .    to simplify notation ,",
    "let @xmath232 . by standard results for gaussian maxima  @xcite , for any @xmath11",
    ", there exists an integer @xmath233 such that for all @xmath234 , @xmath235 & \\geq & ( 1-\\delta ) \\sqrt{2 ( { \\ensuremath{\\sigma^2}}+ { \\ensuremath{\\widetilde{\\sigma}^2 } } ) \\log n}.\\end{aligned}\\ ] ] moreover , the maximum function is lipschitz , so that by gaussian concentration for lipschitz functions  @xcite , for any @xmath236 , we have @xmath237 -\\eta \\big ] & \\leq & \\exp \\big(-\\frac{\\eta^2}{2 ( { \\ensuremath{\\sigma^2}}+ { \\ensuremath{\\widetilde{\\sigma}^2 } } ) } \\big).\\end{aligned}\\ ] ] combining these two statements yields that for all @xmath234 , we have @xmath238 & \\leq & \\exp \\big(-\\frac{\\eta^2}{2 ( { \\ensuremath{\\sigma^2}}+ { \\ensuremath{\\widetilde{\\sigma}^2 } } ) } \\big).\\end{aligned}\\ ] ] it remains to show that there exists some @xmath211 such that converges to zero .",
    "+ * case 1 : * first suppose that @xmath239 . in this case",
    ", we have @xmath240 . with probability greater than @xmath241 ,",
    "this quantity is lower bounded by a constant , using concentration for @xmath140-variates . in this case , @xmath242 w.h.p .",
    ", so that the result follows trivially .",
    "+ * case 2 : * otherwise , we must have @xmath202 . under this condition , we now establish a lower bound on @xmath226 that holds with high probability ; it will be seen that a similar lower bound holds for @xmath227 .",
    "we begin by noting the lower bound @xmath243 . to control the minimum eigenvalue , define the event @xmath244 by standard random matrix concentration arguments ( see appendix  [ applargedev ] ) , for some fixed @xmath245 , we are guaranteed that consequently , conditioned on @xmath246 , we have @xmath247    from lemma  [ lemvarbound ] , we note that if @xmath248 , then for any @xmath11 , we have the lower bound @xmath249    the following result is the final step in the proof of theorem  [ thmphase](b ) .    [ lemcaseone ] suppose that @xmath202 . under this condition :    1 .   if @xmath250 , then @xmath251 \\rightarrow 0 $ ] .",
    "2 .   if @xmath252 , then there exists some @xmath211 such that @xmath253 \\rightarrow 0 $ ] .",
    "\\(a ) if @xmath254 is bounded below by some constant @xmath255 , then we have @xmath256 which implies that @xmath257 .",
    "thus , setting @xmath258 and @xmath259 in equation   yields that ( for @xmath260 sufficiently large ) : @xmath261 & = & { \\ensuremath{\\mathbb{p}}}\\big[\\max_{k \\in { \\ensuremath{{u^{c } } } } } z_k \\leq \\frac{1}{4 } \\ ; \\sqrt{2    ( { \\ensuremath{\\sigma^2}}+ { \\ensuremath{\\widetilde{\\sigma}^2 } } ) \\log n } \\big ] \\\\ & \\leq &   \\exp",
    "\\big(-\\frac{\\log n}{4 } \\big ) \\rightarrow 0.\\end{aligned}\\ ] ] since @xmath262 for @xmath260 large enough , the claim follows .",
    "\\(b ) in this case , we may apply the lower bound  , so that , for any @xmath11 , we have @xmath263 with high probability .",
    "since @xmath264 { \\ensuremath{s}}\\log n$ ] by assumption , we have @xmath265 consequently , from equation  , for any @xmath236 and @xmath11 , we have for all @xmath234 , @xmath266 & \\leq & \\exp \\big(-\\frac{\\eta^2}{2 ( { \\ensuremath{\\sigma^2}}+ { \\ensuremath{\\widetilde{\\sigma}^2 } } ) } \\big).\\end{aligned}\\ ] ] since @xmath267 , we may choose @xmath268 sufficiently small so that for sufficiently large choices of @xmath269 , we have @xmath270 for some @xmath211 . since from lemma  [ lemvarbound ] , the condition @xmath271 implies that @xmath272 w.h.p , we thus conclude that , using these choices of @xmath273 and @xmath274 , we have @xmath275 & \\leq & o(1 ) + \\exp \\big(-\\frac{\\eta^2}{2 ( { \\ensuremath{\\sigma^2}}+ { \\ensuremath{\\widetilde{\\sigma}^2 } } ) } \\big ) \\ ; \\rightarrow \\ ; 0,\\end{aligned}\\ ] ] as claimed .",
    "in this paper , we provided a number of theoretical results that provide a sharp characterization of when , and if so by how much the use of block @xmath4 regularization actually leads improvements in statistical efficiency in the problem of multivariate regression . as suggested in a body of past work , the use of block @xmath4 regularization is well - motivated in many application contexts .",
    "however , since it involves greater computational cost than more naive approaches , the question of whether this greater computational price yields statistical gains is an important one .",
    "this paper assessed statistical efficiency in terms of the number of samples required to recover the support exactly ; however , one could imagine studying the same issue for related loss functions ( e.g. , @xmath138-loss or prediction loss ) , and it would be interesting to see if the results were qualitatively similar or not .",
    "our results demonstrate that some care needs to be exercised in the application of @xmath4 regularization .",
    "indeed , it can yield improved statistical efficiency when the regression matrix exhibits structured sparsity , with high overlaps among the sets of active coefficients within each column .",
    "however , our analysis shows that these improvements are quite sensitive to the exact structure of the regression matrix , and how well it aligns with the regularizing norm .",
    "when this alignment is not high enough , then the use of @xmath4 can actually impair performance relative to more naive ( and less computationally intensive ) schemes based on @xmath276-regularization , such as the lasso . moreover , whether or not the @xmath4 yields statistical improvements is very sensitive to the actual magnitudes of the different regression problems . in comparison to related results obtained by obozinski et al .",
    "@xcite on block @xmath277 regularization , the block @xmath4 exhibits some fragility , in that the conditions under which it actually improves statistical efficiency are delicate and easily violated .",
    "an interesting open direction is study whether or not it is possible to develop computationally efficient methods that are _ fully adaptive _ to the sparsity overlap  namely , methods that behave like ordinary @xmath276-regularization when there is no or little shared sparsity , and behave like block regularization schemes in the presence of shared sparsity .",
    "in this appendix , we discuss some issues associated with recovering individual signed supports .",
    "we begin by observing that once the support union @xmath33 has been recovered , one can restrict the regression problem to this subset @xmath33 , and then apply lasso to each problem separately ( with substantially lower cost , since each problem is now low - dimensional ) in order to recover the individual signed supports .",
    "if one is not willing to perform some extra computation in this way , then the the interpretation of theorems  [ thmdetdesign ] and  [ thmgauss]in terms of recovering the individual signed supports  requires a more delicate treatment , which we discuss in this appendix .",
    "interestingly , the structure of the block @xmath4 norm permits two ways in which to recover the individual signed supports .",
    "[ [ ell_1ell_infty - primal - recovery ] ] @xmath4 primal recovery : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    solve the block - regularized program  , thereby obtaining a ( primal ) optimal solution @xmath278 . estimate the support union via @xmath279 , and and estimate the signed support vectors via @xmath280_{\\ensuremath{k } } & { \\ensuremath { : \\ , = } } & { \\operatorname{sign}}({\\ensuremath{\\widehat{\\beta}^{\\,{\\ensuremath{i}}}}}_{\\ensuremath{k}}).\\end{aligned}\\ ] ]    [ [ ell_1ell_infty - dual - recovery ] ] @xmath4 dual recovery : + +",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    solve the block - regularized program  , thereby obtaining an primal solution @xmath278 . for each row @xmath41 ,",
    "compute the set @xmath281 . estimate the support union via @xmath282 , and estimate the signed support vectors @xmath283",
    "& = & \\begin{cases } { \\operatorname{sign}}({\\ensuremath{\\widehat{\\beta}^{\\,{\\ensuremath{i}}}}}_{\\ensuremath{k } } ) & \\mbox{if $ { \\ensuremath{i}}\\in { \\ensuremath{\\mathbb{m}}}_{\\ensuremath{k}}$ } \\\\ 0 & \\mbox{otherwise . }",
    "\\end{cases}\\end{aligned}\\ ] ] the procedure   corresponds to estimating the signed support on the basis of a dual optimal solution associated with the optimal primal solution .",
    "the dual signed support recovery method   is more conservative in estimating the individual support sets .",
    "in particular , for any given @xmath43 , it only allows an index @xmath284 to enter the signed support estimate @xmath285 when @xmath286 achieves the maximum magnitude ( possibly non - unique ) across all indices @xmath82 .",
    "consequently , unlike the primal estimator  , a corollary of theorem  [ thmdetdesign ] guarantees that the dual signed support method   never suffers from false inclusions in the signed support set . on the other hand , unlike the primal estimator , it may incorrectly exclude indices of some supports  that is , it may exhibit false exclusions .    to provide a concrete illustration of this distinction ,",
    "suppose that @xmath287 and @xmath288 , and that the true matrix @xmath289 and estimate take the following form : @xmath290 consistent with the claims of theorem  [ thmdetdesign ] , the estimate @xmath99 correctly recovers the support union  viz . @xmath291 .",
    "the primal   and dual   methods return the following estimates of the individual signed supports : @xmath292 consequently , the primal estimate includes false non - zeros in positions @xmath293 and @xmath294 , whereas the dual estimate includes false zeros in positions @xmath295 and @xmath296 .",
    "we note that it is possible to ensure that under some conditions that the dual support method   will correctly recover each of the individual signed supports , without any incorrect exclusions .",
    "however , as illustrated by theorem  [ thmphase ] and corollary  [ gapcor ] , doing so requires additional assumptions on the size of the gap @xmath297 for indices @xmath298 .",
    "note that conditioned @xmath299 , the rows of the random matrix @xmath188 are i.i.d .",
    "gaussian random vectors with mean @xmath300 and covariance @xmath301 @xmath302 where @xmath303 .    using these expressions and triangle inequality",
    ", we obtain that @xmath304 is upper bounded by @xmath305 applying the mutual incoherence assumption  , we obtain @xmath306 as claimed .",
    "recall that @xmath307 , @xmath308 , and that @xmath309 is the set where @xmath310 is concentrated . if @xmath311 , then the claim is trivial , so that we may assume that @xmath312 .",
    "recall that @xmath313^{-1 } { \\ensuremath{f}}^1 - { \\ensuremath{m}}^1 \\big[{\\ensuremath{m}}^2      + { \\ensuremath{m}}^1 \\big]^{-1 } { \\ensuremath{f}}^2 \\big \\ } + { \\ensuremath{m}}^1 \\big [ { \\ensuremath{m}}^1 +      { \\ensuremath{m}}^2 \\big]^{-1 } \\vec{1 } - \\\\ \\frac{1}{{\\lambda_n } } [ ( { \\ensuremath{m}}^1)^{-1 }      + ( { \\ensuremath{m}}^2)^{-1}]^{-1}{\\ensuremath{{\\ensuremath{\\widebar{b}}}_{\\operatorname{diff}}}}.\\end{gathered}\\ ] ]    using @xmath314 to denote the spectral norm , we first claim that as long as @xmath203 , then the following events hold with probability greater than @xmath315 :    [ allmatcontrol ] @xmath316^{-1 } - i/2 | \\ !",
    "| \\!|_{{2 } } } & = & { \\ensuremath{{\\ensuremath{\\mathcal{o}}}(\\sqrt{{\\ensuremath{s}}/{\\ensuremath{n } } } ) } } , \\qquad \\mbox{and } \\\\",
    "\\label{eqnratcontrol }      { |\\!|\\!| { \\ensuremath{m}}^1 \\big[{\\ensuremath{m}}^1 + { \\ensuremath{m}}^2 \\big]^{-1 } - i/2 | \\",
    "! | \\!|_{{2 } } } & = &      { \\ensuremath{{\\ensuremath{\\mathcal{o}}}(\\sqrt{{\\ensuremath{s}}/{\\ensuremath{n } } } ) } } ,    \\end{aligned}\\ ] ]    as well as the analogous events with @xmath317 and @xmath318 interchanged .    to verify the bound  , we first diagonalize the projection matrix .",
    "all of its eigenvalues are @xmath14 or @xmath12 , and it has rank @xmath319 w.p .",
    "one , so that we may write @xmath320 for some orthogonal matrix @xmath321 , and the diagonal matrix @xmath322 , @xmath323 but the projection @xmath324 is independent of @xmath325 , which implies that the random rotation matrix @xmath321 is independent of @xmath325 , and hence @xmath326 . since @xmath327 is diagonal with @xmath319 ones and @xmath181 zeros , @xmath328 , where @xmath329 is a standard gaussian random matrix .",
    "consequently , we have @xmath330 since @xmath331 , and @xmath332 using concentration arguments for random matrices ( see lemma  [ lemrandmat ] in appendix  [ applargedev ] ) .    for we may use the triangle inequality and the submultiplicativity of the norm so that @xmath333^{-1 } - i/2 | \\ !",
    "| \\!|_{{2 } } } & = &    { |\\!|\\!| [ { \\ensuremath{m}}^{-1 } + { \\ensuremath{\\widetilde{{\\ensuremath{m}}}}}^{-1}]^{-1}(i - [ { \\ensuremath{m}}^{-1 } +    { \\ensuremath{\\widetilde{{\\ensuremath{m}}}}}^{-1}]/2 ) | \\ ! | \\!|_{{2 } } } \\\\ & \\leq & { |\\!|\\!| [ { \\ensuremath{m}}^{-1 } + { \\ensuremath{\\widetilde{{\\ensuremath{m}}}}}^{-1}]^{-1 } | \\ ! | \\!|_{{2 } } } \\ ;    { |\\!|\\!| i - [ { \\ensuremath{m}}^{-1 } + { \\ensuremath{\\widetilde{{\\ensuremath{m}}}}}^{-1}]/2 | \\ ! | \\!|_{{2 } } } \\\\   & \\leq & \\frac{1}{2 } \\big \\ { { |\\!|\\!| i/2 - { \\ensuremath{m}}^{-1}/2 | \\ !",
    "| \\!|_{{2 } } } +    { |\\!|\\!| i/2 - { \\ensuremath{\\widetilde{{\\ensuremath{m}}}}}^{-1}/2 | \\ !",
    "| \\!|_{{2 } } } \\big \\ } { |\\!|\\!| [ { \\ensuremath{m}}^{-1 } +    { \\ensuremath{\\widetilde{{\\ensuremath{m}}}}}^{-1}]^{-1 } | \\ ! | \\!|_{{2 } } } \\\\ & = & { |\\!|\\!| [ { \\ensuremath{m}}^{-1 } + { \\ensuremath{\\widetilde{{\\ensuremath{m}}}}}^{-1}]^{-1 } | \\ ! | \\!|_{{2 } } } \\ ; { \\ensuremath{{\\ensuremath{\\mathcal{o}}}(\\sqrt{{\\ensuremath{s}}/{\\ensuremath{n}}})}},\\end{aligned}\\ ] ] finally , since @xmath334^{-1 } | \\ ! | \\!|_{{2 } } } = { \\ensuremath{\\mathcal{o}}}(1)$ ] , equation   is valid .    in order to establish the bound",
    ", we have @xmath335^{-1 } - i/2 | \\ ! | \\!|_{{2 } } } & = & { |\\!|\\!| ( { \\ensuremath{m}}/2 - { \\ensuremath{\\widetilde{{\\ensuremath{m}}}}}/2 ) [ { \\ensuremath{m}}+ { \\ensuremath{\\widetilde{{\\ensuremath{m}}}}}]^{-1 } | \\ ! | \\!|_{{2 } } } \\\\ & \\leq & \\frac{1}{2}\\big \\ { { |\\!|\\!| { \\ensuremath{m}}- i | \\ ! | \\!|_{{2 } } } \\ ; + { |\\!|\\!| { \\ensuremath{\\widetilde{{\\ensuremath{m}}}}}-i | \\ ! | \\!|_{{2 } } } \\big \\ } \\ ; { |\\!|\\!| [ { \\ensuremath{m}}+ { \\ensuremath{\\widetilde{{\\ensuremath{m}}}}}]^{-1 } | \\ ! | \\!|_{{2 } } } \\\\ & = & { |\\!|\\!| [ { \\ensuremath{m}}+ { \\ensuremath{\\widetilde{{\\ensuremath{m}}}}}]^{-1 } | \\ ! | \\!|_{{2 } } } \\ ; { \\ensuremath{{\\ensuremath{\\mathcal{o}}}(\\sqrt{{\\ensuremath{s}}/{\\ensuremath{n}}})}}.\\end{aligned}\\ ] ] since @xmath336 - 2i | \\ !",
    "| \\!|_{{2 } } } = { \\ensuremath{{\\ensuremath{\\mathcal{o}}}(\\sqrt{{\\ensuremath{s}}/{\\ensuremath{n}}})}}\\rightarrow 0 $ ] , we have @xmath336^{-1 } | \\ ! | \\!|_{{2 } } } = { \\ensuremath{\\mathcal{o}}}(1)$ ] , which establishes the claim  .",
    "we are now ready to establish the claims of the lemma . from the representation  ,",
    "we apply triangle inequality and our bounds on spectral norms , thereby obtaining @xmath337 with probability greater than @xmath338 , where @xmath339 . by the decomposition of @xmath340 in equation   and applying bounds   @xmath341 \\big    \\}\\ ] ] since @xmath271 , in order to establish the upper bound   it suffices to show that @xmath342 w.h.p . similarly , in the other direction , we have @xmath343 following the same line of reasoning , in order to prove the lower bound  , it suffices to show that @xmath344 w.h.p .",
    "since @xmath345 and @xmath346 behave similarly , it suffices to show that @xmath347 . from the definition  , we see that conditioned on @xmath348 , the random vector @xmath349 is zero - mean gaussian , with i.i.d .",
    "elements with variance @xmath350 recalling that @xmath351 , we have @xmath352 by random matrix concentration ( see the discussion following lemma  [ lemrandmat ] in appendix  [ applargedev ] ) , we have @xmath353 w.h.p . , and by @xmath140 tail bounds ( see lemma  [ lemchitail ] in appendix  [ applargedev ] )",
    ", we have @xmath354 w.h.p .",
    "consequently , with high probability , we have @xmath355 .",
    "since the gaussian random vector @xmath349 has length @xmath356 , again by concentration for @xmath140 random variables , we have ( with probability greater than @xmath357 ) , @xmath358 . combining the pieces ,",
    "we conclude that w.h.p .",
    "@xmath359 \\big ) \\ ; = \\ ; o({\\lambda_n}^2 { \\ensuremath{s}}),\\end{aligned}\\ ] ] where the final equality follows since @xmath271 and @xmath360 .",
    "this section is devoted to the development of various properties of the optimal solution(s ) of the block @xmath4-regularized problem  .      by standard conditions for optimality in convex programs  @xcite",
    ", the zero - vector must belong to the subdifferential of the objective function in the convex program  , or equivalently , we must have for each @xmath361 @xmath362 where @xmath39 must be an element of the subdifferential @xmath363 . substituting the relation @xmath364 , we obtain @xmath365      we begin with the proof of part ( i ) : suppose that steps ( a ) through ( c ) of the primal - witness construction succeed",
    ". by definition , it outputs a primal pair , of the form @xmath366 , along with a candidate dual optimal solution @xmath367 .",
    "note that the conditions defining the @xmath4 subdifferential apply in an elementwise manner , to each index @xmath368 . since the sub - vector @xmath369 was chosen from the subdifferential of the restricted optimal solution , it is dual feasible .",
    "moreover , since the strict dual feasibility condition   holds , the matrix @xmath370 constructed in step ( c ) is dual feasible for the zero - solution in the sub - block @xmath51 .",
    "therefore , we conclude that @xmath366 is a primal optimal solution for the full block - regularized program  .",
    "it remains to establish uniqueness of this solution .",
    "define the ball @xmath371 and observe that we have the variational representation @xmath372 where @xmath373 denotes the euclidean inner product . with this notation ,",
    "the block - regularized program   is equivalent to the saddle - point problem @xmath374 since this saddle - point problem is strictly feasible and convex - concave , it has a value .",
    "moreover , given any dual optimal solution  in particular , @xmath30 from the primal - dual construction ",
    "any optimal primal solution @xmath99 must satisfy the saddle point condition @xmath375 but this condition can only hold if @xmath376 , @xmath377 for any index @xmath378 such that @xmath379 .",
    "therefore , any optimal primal solution must satisfy @xmath380 , so that solving the original program   is equivalent to solving the restricted program  .",
    "lastly , if the matrices @xmath381 are invertible for each @xmath382 , then the restricted problem   is strictly convex , and so has a unique solution , thereby completing the proof of lemma  [ lemkey](i ) .",
    "we now prove part ( ii ) of lemma  [ lemkey ] .",
    "suppose that we are given an estimate @xmath99 of the true parameters @xmath289 by solving the convex program   such that @xmath383 .",
    "since @xmath99 is an optimal solution to the convex program  , the the optimality conditions of equation  , must be satified .",
    "we may rewrite those conditions as @xmath384 where @xmath385 . recalling that @xmath386",
    ", we obtain    @xmath387    again , by standard conditions for optimality in convex programs  @xcite , the first of these two equations is exactly the condition that must be satisfied by an optimal solution of the restricted program  .",
    "however , we have already shown that the candidate solution @xmath388 satisfies this condition , so that it must also be an optimal solution of the convex program  .",
    "additionally , the value of @xmath65 that satisfies equation   for each @xmath389 is an element of @xmath390 .",
    "we have thus shown that steps ( b ) and ( c ) of the primal - witness construction succeed .",
    "it remains to establish uniqueness in part ( a ) .",
    "however , we note that @xmath391 is invertible for each @xmath70 . hence , for any solution @xmath99 such that @xmath380 , @xmath392\\ ] ] is well - defined and unique , noting that @xmath393 .",
    "thus , we have established the equality   and that @xmath388 is unique .",
    "therefore , @xmath99 gives solutions to steps ( a ) and ( b ) when solving the restricted convex program over the set @xmath33 .",
    "finally , we derive the form of the dual solution @xmath394 , as a function of @xmath391 , @xmath369 , and @xmath395 . recall that",
    "@xmath391 is invertible , @xmath369 is an element of the subdifferential of @xmath396 , and @xmath397 . from equation",
    ", we have @xmath398 the claimed form of the dual solution follows by substituting equation   into equation  .      in this section , we focus on the specific form of the dual variables @xmath399 .",
    "our approach is to construct a candidate set of dual variables , and then show that they are valid .",
    "we begin by defining the sets @xmath400 , corresponding to the intersection of the supports , and the set @xmath401 corresponding to elements in one ( but not both ) of the supports . for @xmath402 ,",
    "we let @xmath403 is a diagonal matrix whose diagonal entries correspond to @xmath404 . in addition",
    ", we define the vectors @xmath405 and matrices @xmath406 via    @xmath407\\\\      { \\ensuremath{m}}^{\\ensuremath{i } } & { \\mathrel{\\mathop:}= } & \\frac{1}{{\\ensuremath{n } } } { \\ensuremath{s}}^{\\ensuremath{i}}{\\ensuremath{\\big\\langle { \\ensuremath{x^{{\\ensuremath{i}}}_{{\\ensuremath{b } } } } } , \\ , ( i -       { \\pi}_{({\\ensuremath{x^{{\\ensuremath{i}}}_{{\\ensuremath{{\\ensuremath{{b^{c}}}}}}}}})}){\\ensuremath{x^{{\\ensuremath{i}}}_{{\\ensuremath{b } } } } } \\big\\rangle } }       { \\ensuremath{s}}^{\\ensuremath{i}}.    \\end{aligned}\\ ] ]    given these definitions , we have the following lemma :    [ lemmaboth ] assume that @xmath7 , and that latexmath:[$|{\\ensuremath{\\widehat{\\beta}^{\\,1}}}_{\\ensuremath{b}}| =    @xmath386 , then the dual variable @xmath409 satisfies the relation @xmath410^{-1 } { \\ensuremath{f}}^1 - { \\ensuremath{m}}^1 \\big[{\\ensuremath{m}}^2 + { \\ensuremath{m}}^1 \\big]^{-1 } { \\ensuremath{f}}^2 \\big \\ } + { \\ensuremath{m}}^1 \\big [ { \\ensuremath{m}}^1 + { \\ensuremath{m}}^2 \\big]^{-1 } \\vec{1 } - \\\\      \\frac{1}{{\\lambda_n } } [ ( { \\ensuremath{m}}^1)^{-1 } +        ( { \\ensuremath{m}}^2)^{-1}]^{-1 } { \\ensuremath{{\\ensuremath{\\widebar{b}}}_{\\operatorname{diff}}}}\\end{gathered}\\ ] ] and @xmath411 , with analogous results holding for @xmath412 .",
    "given these forms for @xmath413 and @xmath414 , it remains to show that the relation @xmath415 holds under the conditions of theorem  [ thmphase](a ) .",
    "intuitively , this condition should hold since under the conditions of theorem  [ thmphase](a ) , the matrix @xmath416 is approximately the identity , and the vector @xmath417 is approaching @xmath14 .",
    "finally , we expect that @xmath418 is very small , hence the final term is also very small .",
    "therefore , on the set @xmath309 , both @xmath413 and @xmath419 are approximately equal to @xmath420 .",
    "we formalize this rough intuition in the following lemma :    [ lembound ] under the assumptions of theorem  [ thmphase](a ) each of the following conditions hold for sufficiently large @xmath84 , @xmath181 , and @xmath1 with probability greater than @xmath421 :    @xmath422^{-1}({\\ensuremath{{\\ensuremath{\\widebar{b}}}_{\\operatorname{diff } } } } ) \\|}_{\\infty } & \\leq & \\epsilon \\\\",
    "\\label{lembound2 }        \\| \\frac{1}{{\\lambda_n } } \\big \\ { { \\ensuremath{m}}^2 \\big[{\\ensuremath{m}}^1 +      { \\ensuremath{m}}^2 \\big]^{-1 } { \\ensuremath{f}}^1 - { \\ensuremath{m}}^1 \\big[{\\ensuremath{m}}^2 + { \\ensuremath{m}}^1      \\big]^{-1 } { \\ensuremath{f}}^2 \\big \\ } \\|_{\\infty } & \\leq & \\epsilon \\\\        \\label{lembound3 }        \\| { \\ensuremath{m}}^1 \\big [ { \\ensuremath{m}}^1 + { \\ensuremath{m}}^2 \\big]^{-1 } \\vec{1 } - \\frac{1}{2 } \\|_{\\infty } & \\leq & \\frac{1}{2 } - 3 \\epsilon .",
    "\\end{aligned}\\ ] ]    given lemmas  [ lemmaboth ] and  [ lembound ] , we can conclude that the definition for the dual variables on the support is valid .",
    "the remaining subsections in this appendix are dedicated to verifying the above results : in particular , we prove lemma  [ lemmaboth ] in appendix  [ applemboth ] and lemma  [ lembound ] in appendix  [ applembound ] .",
    "we now proceed to establish the validity of the closed form expressions for @xmath423 and @xmath424 . from equation",
    "we have that @xmath425 +    ( \\frac{1}{{\\ensuremath{n } } }    { \\ensuremath{\\big\\langle { \\ensuremath{x^{{\\ensuremath{i}}}_{{\\ensuremath{{\\ensuremath{{b^{c } } } } } } } } } , \\ , { \\ensuremath{x^{{\\ensuremath{i}}}_{{\\ensuremath{{\\ensuremath{{b^{c } } } } } } } } } \\big\\rangle}})^{-1 }    { \\ensuremath{(x^{1}_{{\\ensuremath{{\\ensuremath{{b^{c}}}}}}})^t } } { \\ensuremath{w^{1}}}\\ ] ] substituting back into @xmath426 so that we obtain    @xmath427    recall that by assumption that @xmath428 , and @xmath429 .    subtracting @xmath430 and @xmath431 from equations   and    @xmath432    applying the fact that @xmath433 .",
    "@xmath434 where @xmath435 . then solving for @xmath436 letting @xmath437 and substituting back into equation   @xmath438 ^{-1 } { \\lambda_n}{\\ensuremath{\\vec{1}}}- [ ( { \\ensuremath{m}}^1)^{-1 } +      ( { \\ensuremath{m}}^2)^{-1}]^{-1}({\\ensuremath{{\\ensuremath{\\widebar{b}}}_{\\operatorname{diff } } } } ) \\\\    + { \\ensuremath{m}}^2 \\big [ { \\ensuremath{m}}^1 + { \\ensuremath{m}}^2 \\big ] ^{-1 } { \\ensuremath{f}}^1 - { \\ensuremath{m}}^1    \\big [ { \\ensuremath{m}}^1 + { \\ensuremath{m}}^2 \\big ] ^{-1 } { \\ensuremath{f}}^2.\\end{gathered}\\ ] ]      the first term @xmath439^{-1 } \\ , { \\ensuremath{{\\ensuremath{\\widebar{b}}}_{\\operatorname{diff}}}}$ ] can be decomposed as @xmath440^{-1}{\\ensuremath{{\\ensuremath{\\widebar{b}}}_{\\operatorname{diff } } } } & = &    \\underbrace{\\frac{1}{{\\lambda_n } } ( [ ( { \\ensuremath{m}}^1)^{-1 } +    ( { \\ensuremath{m}}^2)^{-1}]^{-1 } - i/2 ) { \\ensuremath{{\\ensuremath{\\widebar{b}}}_{\\operatorname{diff}}}}}_{\\mbox{\\large $ { t}_1 $ } } +    \\underbrace{\\frac{{\\ensuremath{{\\ensuremath{\\widebar{b}}}_{\\operatorname{diff}}}}}{2 { \\lambda_n}}}_{\\mbox{\\large $ { t}_2$}}\\end{aligned}\\ ] ] under the assumptions of theorem  [ thmphase](a ) , we have @xmath441 , hence , for @xmath181 large enough , @xmath442 .    in order to bound @xmath443",
    ", we note that with probability greater than @xmath241 , the spectral norm of @xmath444^{-1 } - i/2)$ ] is @xmath445 ( see the bound   from appendix  [ applemvarbound ] ) .",
    "consequently , we may decompose @xmath446^{-1 } - i/2)$ ] as @xmath447 where @xmath448 and @xmath327 are independent and @xmath448 is distributed uniformly over all orthogonal matrices , and @xmath449 . using this decomposition",
    ", the following lemma , proved in appendix  [ appbiglemmaone ] , allows us to obtain the necessary control on the quantity @xmath450 :    [ biglemma1 ] let @xmath451 be a matrix chosen uniformly at random from the space of all orthogonal matrices . consider a second random matrix @xmath452 , independent of @xmath453 .",
    "if @xmath271 , then for any fixed vector @xmath454 and fixed @xmath211 , we have :    1 .",
    "[ lem9:parta ] if @xmath455 , then @xmath456      & \\leq & c_1 \\exp \\big ( -c_2 \\epsilon^2 \\frac{{\\ensuremath{n}}}{{\\ensuremath{s}}\\|x\\|_\\infty^2 } + \\log({\\ensuremath{s } } ) \\big ) .",
    "\\end{aligned}\\ ] ] 2 .",
    "[ lem9:partb ] if @xmath457 , then @xmath456      & \\leq & c_1 \\exp \\big ( -c_2 \\epsilon^2 \\frac{{\\ensuremath{n}}^2}{{\\ensuremath{s}}^2        \\|x\\|_\\infty^2 } + \\log({\\ensuremath{s } } ) \\big ) .",
    "\\end{aligned}\\ ] ]    with reference to the problem of bounding @xmath458 , we may apply part ( a ) of this lemma with @xmath459 and @xmath460 to conclude that @xmath461 with high probability , thereby establishing the bound  .",
    "we now turn the proving the bound  .",
    "we begin by decomposing the terms involved in this equation as @xmath462^{-1 } { \\ensuremath{f}}^1 & = & \\frac{1}{{\\lambda_n } } \\biggr [ { \\ensuremath{m}}^2 \\big [ { \\ensuremath{m}}^1 + { \\ensuremath{m}}^2 \\big]^{-1 } - \\frac{i}{2 } \\biggr ] { \\ensuremath{f}}^1 + \\frac{{\\ensuremath{f}}^1}{2 { \\lambda_n } } \\\\ \\frac{1}{{\\lambda_n } } { \\ensuremath{m}}^1 \\big[{\\ensuremath{m}}^1 + { \\ensuremath{m}}^2 \\big]^{-1 } { \\ensuremath{f}}^2 & = & \\frac{1}{{\\lambda_n } } \\biggr [ { \\ensuremath{m}}^1 \\big [ { \\ensuremath{m}}^1 + { \\ensuremath{m}}^2 \\big]^{-1 } - \\frac{i}{2 } \\biggr ] { \\ensuremath{f}}^2 + \\frac{{\\ensuremath{f}}^2}{2 { \\lambda_n}}\\end{aligned}\\ ] ] recalling the form of @xmath405 , conditioned on @xmath463 and @xmath85 , we have @xmath464 however , by lemmas  [ lemchitail ] and  [ lemrandmat ] ( see appendix  [ applargedev ] ) , as well as the fact that @xmath465 , for @xmath84 and @xmath181 large enough , the variance term is bounded by @xmath466 with probability greater than @xmath467 . hence , by standard gaussian tail bounds , the inequalities @xmath468 and @xmath469 both hold with probability greater than @xmath470 .    now to bound the first term in the decomposition we begin by diagonalizing @xmath471 .",
    "note that @xmath448 is independent of @xmath176 and @xmath472 and by symmetry @xmath473 .",
    "following some algebra , we find that @xmath474^{-1 }    - \\frac{i}{2 } \\biggr ] { \\ensuremath{f}}^1 = \\frac{1}{{\\lambda_n } } { \\ensuremath{q}}^t \\biggr    [ d \\big [ { \\ensuremath{q}}{\\ensuremath{m}}^1 { \\ensuremath{q}}^t + d \\big]^{-1 } - \\frac{i}{2 } \\biggr    ] { \\ensuremath{q}}{\\ensuremath{f}}^1\\ ] ] the random vector @xmath475 is independent of @xmath453 and @xmath476 is independent of @xmath453 by symmetry .",
    "hence , the vector @xmath477 { \\ensuremath{q}}\\frac{1}{{\\lambda_n } } { \\ensuremath{f}}^1 $ ] is independent of @xmath453 . for",
    "a given constant @xmath478 , let us define the event @xmath479 \\big \\}.\\ ] ] we can then write @xmath480 & \\leq & { \\ensuremath{\\mathbb{p}}}\\big [ { \\| { \\ensuremath{q}}^t v \\|}_{\\infty } \\geq \\epsilon \\ , \\mid \\ , { \\ensuremath{\\mathcal{s}}}\\big ]    + { \\ensuremath{\\mathbb{p}}}[{\\ensuremath{\\mathcal{s}}}^c].\\end{aligned}\\ ] ] note that we may consider the event that @xmath481 and @xmath482 = { \\ensuremath{{\\ensuremath{\\mathcal{o}}}(\\sqrt{{\\ensuremath{s}}/{\\ensuremath{n}}})}}$ ] . we claim that each of these events happens with high probability .",
    "note that the former event occurs with high probability by lemma  [ lemrandmat ] .",
    "the latter event holds with high probability since , @xmath483 =    [ 2d((d + q { \\ensuremath{m}}^1 q^t)^{-1}-i/2 ) + d - i].\\ ] ] and , both @xmath484 and @xmath485 by equation  .",
    "thus , the sum of the two random matrices is also @xmath445 .",
    "recall the bound on the variance of each component of @xmath475 from equation   and note that each component is independent . applying the concentration results from lemma  [ lemchitail ] for @xmath486-squared random variables",
    "yields that @xmath487 with high probability .",
    "hence , under the above conditions @xmath488    { \\ensuremath{q}}\\frac{1}{{\\lambda_n } } { \\ensuremath{f}}^1\\|_2 ^ 2 & \\leq & { |\\!|\\!| \\| \\frac{1}{2 } [ 2 { \\ensuremath{d}}({\\ensuremath{d}}+ { \\ensuremath{q}}{\\ensuremath{m}}^1 { \\ensuremath{q}}^t)^{-1 } - i ] | \\ ! | \\!|_{{2}}}^2 \\| { \\ensuremath{q}}\\frac{1}{{\\lambda_n } } { \\ensuremath{f}}^1\\|_2 ^ 2 \\\\    & \\leq & c_3 ^ 2 \\ ; \\frac{{\\ensuremath{s}}^2}{{\\ensuremath{n } } } \\big [ \\frac{{\\ensuremath{s}}}{{\\ensuremath{n } } } +    \\frac{1}{{\\lambda_n}^2 { \\ensuremath{n } } } \\big ] , \\end{aligned}\\ ] ] with high probabilty , which implies that @xmath489 holds with high probability as well .",
    "therefore , it immediately follows then that @xmath490 \\leq c_1 \\exp(-c_2 { \\ensuremath{s}})$ ] .",
    "it remains to control the first term .",
    "we do so using the following lemma , which is proved in appendix  [ appminilemmaone ] :    [ minilemma1 ] let @xmath491 be a matrix chosen uniformly at random from the space of orthogonal matrices .",
    "let @xmath492 be a random vector independent of @xmath453 , such that @xmath493 with probability one .",
    "then we have @xmath494 \\ ; = \\ ; o(1).\\ ] ]    we now apply this lemma to the random vector @xmath495 with @xmath496 , and @xmath497 .",
    "note that @xmath498 from which the second claim   in lemma  [ lembound ] follows .",
    "finally , we turn to proving the third claim   in lemma  [ lembound ] . following some algebra",
    ", we obtain @xmath499^{-1 } \\vec{1 } - \\frac{1}{2 }    \\|_{\\infty } \\ , = \\ , \\frac{1}{4 } ( { \\ensuremath{m}}^1 - { \\ensuremath{m}}^2 ) { \\ensuremath{\\vec{1}}}+    \\frac{1}{2 } ( { \\ensuremath{m}}^1 - { \\ensuremath{m}}^2)(i/2 - ( { \\ensuremath{m}}^1 + { \\ensuremath{m}}^2)^{-1 } )    { \\ensuremath{\\vec{1}}}.\\ ] ] we diagonalize the matrix @xmath500 , where @xmath472 is diagonal . since the random matrix @xmath317 has a spherically symmetric distribution , the matrix @xmath453 has a uniform distribution over the space of orthogonal matrices and is independent of @xmath472 . using this decomposition , we can rewrite the second term in equation   as @xmath501 where @xmath502 .",
    "we note that @xmath503 is independent of @xmath453 , because @xmath472 and @xmath318 are independent of @xmath453 .",
    "this independence follows from the spherical symmetry of @xmath318 and the fact that @xmath504 .    defining the event",
    ", we claim that @xmath505 \\ ; \\leq \\ ; c_1 \\exp(-c_2 { \\ensuremath{n } } ) \\ ; \\rightarrow \\ ; 0.\\ ] ] in order to establish this claim , we note that sub - multiplicativity and triangle inequality imply that @xmath506 since @xmath507 with probability greater than @xmath508 , from the discussion following lemma  [ lemrandmat ] .",
    "similarly , from this same result , we have @xmath509 , so that the claim   follows .",
    "using the decomposition   and the tail bound  , we have @xmath510 & = & { { \\ensuremath{\\mathbb{p } } } } [ { \\| { \\ensuremath{q}}^t { \\ensuremath{r}}{\\ensuremath{q}}1 \\|}_{\\infty } \\geq \\epsilon \\ ; \\mid \\ ; { \\mathcal{t } } ] + { { \\ensuremath{\\mathbb{p}}}}[{\\mathcal{t}}^c ] \\\\ & \\leq & { \\ensuremath{\\mathcal{o}}}\\big ( \\ , \\frac{1}{s } \\ , \\big ) + { \\ensuremath{\\mathcal{o}}}(\\exp(-c(\\epsilon ) { \\ensuremath{n}})),\\end{aligned}\\ ] ] where lemma  [ biglemma1 ] ( proved in appendix  [ appbiglemmaone ] ) provides control on the first term in the inequality .",
    "we provide the proof for part ( a ) of the lemma and note that part ( b ) is analogous .    by union bound",
    ", we have @xmath511 & \\leq & { \\ensuremath{s}}\\ ; \\max_{i=1 , \\ldots , { \\ensuremath{s } } } { { \\ensuremath{\\mathbb{p } } } } [ | e_i^t { \\ensuremath{q}}^t { \\ensuremath{a}}{\\ensuremath{q}}x| \\geq \\epsilon].\\end{aligned}\\ ] ] we will derive a bound on the probability @xmath512 $ ] that holds for all @xmath513 . we write @xmath514 , where @xmath515 denotes the first column of @xmath453 , and @xmath516 denotes the weighted sum of the remaining @xmath517 columns of @xmath453 . since @xmath453 is orthogonal , the vector @xmath515 has unit norm @xmath518 , the vector @xmath519 is orthogonal to @xmath515 , and moreover @xmath520 .",
    "owing to the bound on the spectral norm of @xmath452 , we have @xmath521 which is less than @xmath522 for @xmath269 sufficiently large , since @xmath271 .",
    "we now turn to the second term .",
    "note that conditioned on @xmath519 , the vector @xmath515 is uniformly distributed over an @xmath523-dimensional unit sphere , contained within the subspace orthogonal to @xmath519 . still conditioning on @xmath519 ,",
    "consider the function @xmath524 .",
    "for any pair of vectors @xmath525 on the unit sphere , we have @xmath526,\\end{aligned}\\ ] ] where @xmath527 is the geodesic distance . using the inequality @xmath528 , valid for @xmath529 $ ] , and the assumption @xmath530 , and taking square roots , we obtain @xmath531 so that @xmath532 is a lipschitz constant on the unit sphere ( with dimension @xmath533 ) with constant @xmath534 .",
    "consequently , by levy s theorem  @xcite , for any @xmath211 , we have @xmath535 & \\leq & 2 \\exp(- ( { \\ensuremath{s}}-2 ) \\ ; { \\frac{{\\ensuremath{n}}}{\\|x\\|_\\infty^2 \\ ; { \\ensuremath{s}}({\\ensuremath{s}}-1 ) } } \\ ; \\epsilon^2 ) \\ ; \\leq \\ ; 2 \\ , \\exp \\big ( -c_1 \\ , \\frac{{\\ensuremath{n}}}{\\|x\\|_\\infty^2 \\ ; { \\ensuremath{s } } } \\epsilon^2 \\big).\\end{aligned}\\ ] ]    as a final side remark , we note that under the scaling of theorem  [ thmphase](b ) , we have as @xmath208 , so that the probability in question vanishes .      by union bound and symmetry of the distribution @xmath453 , for any @xmath536 , we have @xmath537 & \\leq & { \\ensuremath{m}}\\ ; { \\ensuremath{\\mathbb{p}}}\\big [ { |e_1^t { \\ensuremath{q}}^t { \\ensuremath{v}}| } \\ , \\geq \\ , t \\big ] \\\\ & = &   { \\ensuremath{m}}\\ ; { \\ensuremath{\\mathbb{p}}}\\big [ { |{\\ensuremath{q}}_1^t { \\ensuremath{v}}| } \\ , \\geq \\ , t \\big],\\end{aligned}\\ ] ] where @xmath538 is the first column of @xmath453 .",
    "note that @xmath538 is a random vector distributed uniformly over the unit sphere @xmath539 in @xmath540 dimensions .",
    "viewing the vector @xmath541 as fixed , consider the function @xmath542 defined over @xmath539 .",
    "as in lemma  [ biglemma1 ] , some calculation shows the lipschitz constant of @xmath543 over @xmath539 is at most applying levy s theorem  @xcite , we conclude that for any @xmath211 , @xmath544 & \\leq & 2 \\exp \\big ( -({\\ensuremath{m}}- 1 ) \\",
    ", \\frac{t^2 } { 2 \\|v\\|_2 ^ 2 } + \\log { \\ensuremath{m}}\\big).\\end{aligned}\\ ] ] since @xmath493 by assumption , it suffices to set @xmath545 .",
    "in this appendix , we state some known large deviation bounds for the gausssian variates , @xmath140-variates , as well as the eigenvalues of random matrices .",
    "the following gaussian tail bound is standard :                letting @xmath553 in equation  , we have @xmath554 \\ ; \\geq \\ ; { \\ensuremath{\\mathbb{p } } } [ \\frac{x}{d } \\geq ( 1 + t)^2],\\end{aligned}\\ ] ] thereby establishing  . with the same choice of @xmath555 , equation   implies the bound   immediately .",
    "[ lemrandmat ] let @xmath556 be a random matrix from the standard gaussian ensemble ( i.e. , @xmath557 , i.i.d ) .",
    "then with probability greater than @xmath241 , for any @xmath11 , its eigenspectrum satisfies the bounds @xmath558 ^ 2 \\ ; \\leq      \\ ; \\lambda_{\\min } \\big ( \\frac{x^t x}{{\\ensuremath{n } } } \\big ) \\leq",
    "\\lambda_{\\max } \\big ( \\frac{x^t x}{{\\ensuremath{n } } } \\big ) \\ ; \\leq \\ ;      ( 1+\\delta ) \\ ; \\big[1 + \\sqrt{\\frac{{\\ensuremath{s}}}{{\\ensuremath{n}}}}\\big]^2.\\ ] ]    note that this lemma implies similar bounds for eigenvalues of the inverse : @xmath559 ^ 2}\\ ; \\leq \\ ; \\lambda_{\\min } \\big ( ( \\frac{x^t x}{{\\ensuremath{n}}})^{-1 } \\big ) \\leq \\lambda_{\\max } \\big ( ( \\frac{x^t x}{{\\ensuremath{n}}})^{-1 } \\big ) \\ ; \\leq \\ ; \\frac{1}{(1-\\delta ) \\ ; \\big[1- \\sqrt{\\frac{{\\ensuremath{s}}}{{\\ensuremath{n}}}}\\big]^2}.\\ ] ]                          d.  l. donoho and j.  m. tanner . counting faces of randomly - projected polytopes when the projection radically lowers dimension .",
    "technical report , stanford university , 2006 . submitted to journal of the ams .",
    "e.  p. simoncelli .",
    "denoising of visual images in the wavelet domain . in p.",
    "mller and b.  vidakovic , editors , _ bayesian inference in wavelet based models _ , chapter  18 , pages 291308 .",
    "springer - verlag , new york , june 1999 .",
    "lecture notes in statistics , vol .",
    "141 .",
    "m.  j. wainwright .",
    "information - theoretic bounds for sparsity recovery in the high - dimensional and noisy setting .",
    "technical report 725 , department of statistics , uc berkeley , january 2007 . posted as arxiv : math.st/0702301 ; presented at international symposium on information theory ,",
    "june 2007 .",
    "m.  j. wainwright .",
    "sharp thresholds for high - dimensional and noisy recovery of sparsity using using @xmath276-constrained quadratic programs .",
    ". appeared as tech .",
    "report 709 , department of statistics , uc berkeley .",
    "may 2006 .",
    "w.  wang , m.  j. wainwright , and k.  ramchandran .",
    "information - theoretic limits on sparse signal recovery : dense versus sparse measurement matrices",
    ". technical report arxiv:0806.0604 , uc berkeley , june 2008 . presented at isit 2008 , toronto , canada .",
    "zhao , g.  rocha , and b.  yu . grouped and hierarchical model selection through composite absolute penalties .",
    "technical report , statistics department , uc berkeley , 2007 . to appear in annals of statistics ."
  ],
  "abstract_text": [
    "<S> given a collection of @xmath0 linear regression problems in @xmath1 dimensions , suppose that the regression coefficients share partially common supports . </S>",
    "<S> this set - up suggests the use of @xmath2-regularized regression for joint estimation of the @xmath3 matrix of regression coefficients . </S>",
    "<S> we analyze the high - dimensional scaling of @xmath4-regularized quadratic programming , considering both consistency rates in @xmath5-norm , and also how the minimal sample size @xmath6 required for performing variable selection grows as a function of the model dimension , sparsity , and overlap between the supports . </S>",
    "<S> we begin by establishing bounds on the @xmath5-error as well sufficient conditions for exact variable selection for fixed design matrices , as well as designs drawn randomly from general gaussian matrices . </S>",
    "<S> our second set of results applies to @xmath7 linear regression problems with standard gaussian designs whose supports overlap in a fraction @xmath8 $ ] of their entries : for this problem class , we prove that the @xmath2-regularized method undergoes a phase transition  that is , a sharp change from failure to success  characterized by the rescaled sample size @xmath9 . </S>",
    "<S> more precisely , given sequences of problems specified by @xmath10 , for any @xmath11 , the probability of successfully recovering both supports converges to @xmath12 if @xmath13 , and converges to @xmath14 for problem sequences for which @xmath15 . </S>",
    "<S> an implication of this threshold is that use of @xmath16-regularization yields improved statistical efficiency if the overlap parameter is large enough ( @xmath17 ) , but has _ </S>",
    "<S> worse _ statistical efficiency than a naive lasso - based approach for moderate to small overlap ( @xmath18 ) . </S>",
    "<S> empirical simulations illustrate the close agreement between these theoretical predictions , and the actual behavior in practice . </S>",
    "<S> these results indicate that some caution needs to be exercised in the application of @xmath4 block regularization : if the data does not match its structure closely enough , it can impair statistical performance relative to computationally less expensive schemes .    * *    [ cols=\"^ \" , ]     figure  [ figreleff ] provides an alternative perspective on the data , where we have plotted how the sample size required by block regression changes as a function of the overlap parameter @xmath19 $ ] . </S>",
    "<S> each set of data points plots a scaled form of the sample size required to hit @xmath20 success , for a range of overlaps , and the straight line @xmath21 that is predicted by theorem  [ thmphase ] note the excellent agreement between the experimental results , for all three problem sizes for @xmath22 , and the full range of overlaps . </S>",
    "<S> the line @xmath21 also characterizes the relative efficiency @xmath23 of block regularization versus the naive lasso - based method , as described in corollary  [ corscaling ] . for overlaps </S>",
    "<S> @xmath24 , this parameter @xmath23 drops below @xmath12 . on the other hand , for overlaps @xmath25 , we have @xmath26 , so that applying the joint optimization problem actually decreases statistical efficiency . intuitively , although there is still some fraction of overlap , the regularization is misleading , in that it tries to enforce a higher degree of shared sparsity than is actually present in the data . </S>"
  ]
}