{
  "article_text": [
    "what makes some scientific or engineering problems at once interesting and challenging is that often , one has fewer equations than unknowns . when the equations are linear , one would like to determine an object @xmath1 from data @xmath2 , where @xmath3 is an @xmath4 matrix with fewer rows than columns ; i.e. , @xmath5 .",
    "the problem is of course that a system with fewer equations than unknowns usually has infinitely many solutions and thus , it is apparently impossible to identify which of these candidate solutions is indeed the `` correct '' one without some additional information .    in many instances ,",
    "however , the object we wish to recover is known to be structured in the sense that it is sparse or compressible .",
    "this means that the unknown object depends upon a smaller number of unknown parameters . in a biological experiment",
    ", one could measure changes of expression in 30,000 genes and expect at most a couple hundred genes with a different expression level . in signal processing , one could sample or sense signals which are known to be sparse ( or approximately so ) when expressed in the correct basis .",
    "this premise radically changes the problem , making the search for solutions feasible since the simplest solution now tends to be the right one .    mathematically speaking and under sparsity assumptions",
    ", one would want to recover a signal @xmath1 , e.g. ,  the coefficient sequence of the signal in the appropriate basis , by solving the combinatorial optimization problem @xmath6 where @xmath7 .",
    "this is a common sense approach which simply seeks the simplest explanation fitting the data .",
    "in fact , this method can recover sparse solutions even in situations in which @xmath8 .",
    "suppose for example that all sets of @xmath9 columns of @xmath3 are in general position .",
    "then the program ( @xmath10 perfectly recovers all sparse signals @xmath11 obeying @xmath12 .",
    "this is of little practical use , however , since the optimization problem is nonconvex and generally impossible to solve as its solution usually requires an intractable combinatorial search .",
    "a common alternative is to consider the convex problem @xmath13 where @xmath14 . unlike ( @xmath15 ) , this problem is convex  it can actually be recast as a linear program  and is solved efficiently @xcite .",
    "the programs ( @xmath15 ) and ( @xmath16 ) differ only in the choice of objective function , with the latter using an @xmath0 norm as a proxy for the literal @xmath17 sparsity count .",
    "as summarized below , a recent body of work has shown that perhaps surprisingly , there are conditions guaranteeing a formal equivalence between the combinatorial problem ( @xmath15 ) and its relaxation @xmath18 .",
    "the use of the @xmath0 norm as a sparsity - promoting functional traces back several decades .",
    "a leading early application was reflection seismology , in which a sparse reflection function ( indicating meaningful changes between subsurface layers ) was sought from bandlimited data . in 1973 ,",
    "claerbout and muir  @xcite first proposed the use of @xmath0 to deconvolve seismic traces . over the next decade",
    "this idea was refined to better handle observation noise  @xcite , and the sparsity - promoting nature of @xmath0 minimization was empirically confirmed .",
    "rigorous results began to appear in the late-1980 s , with donoho and stark  @xcite and donoho and logan  @xcite quantifying the ability to recover sparse reflectivity functions .",
    "the application areas for @xmath0 minimization began to broaden in the mid-1990 s , as the lasso algorithm  @xcite was proposed as a method in statistics for sparse model selection , basis pursuit  @xcite was proposed in computational harmonic analysis for extracting a sparse signal representation from highly overcomplete dictionaries , and a related technique known as total variation minimization was proposed in image processing  @xcite .",
    "some examples of @xmath0 type methods for sparse design in engineering include vandenberghe et al .",
    "@xcite for designing sparse interconnect wiring , and hassibi et al .",
    "@xcite for designing sparse control system feedback gains . in  @xcite ,",
    "dahleh and diaz - bobillo solve controller synthesis problems with an @xmath0 criterion , and observe that the optimal closed - loop responses are sparse .",
    "lobo et al .  used @xmath0 techniques to find sparse trades in portfolio optimization with fixed transaction costs in  @xcite . in  @xcite , ghosh and boyd used @xmath0 methods to design well connected sparse graphs ; in  @xcite , sun et al .",
    "observe that optimizing the rates of a markov process on a graph leads to sparsity . in  (",
    "* ,  11.4.1 ) , boyd and vandenberghe describe several problems involving @xmath0 methods for sparse solutions , including finding small subsets of mutually infeasible inequalities , and points that violate few constraints . in a recent paper ,",
    "koh et al.used these ideas to carry out piecewise - linear trend analysis  @xcite .",
    "over the last decade , the applications and understanding of @xmath0 minimization have continued to increase dramatically .",
    "donoho and huo  @xcite provided a more rigorous analysis of basis pursuit , and this work was extended and refined in subsequent years , see @xcite .",
    "much of the recent focus on @xmath0 minimization , however , has come in the emerging field of compressive sensing  @xcite .",
    "this is a setting where one wishes to recover a signal @xmath11 from a small number of compressive measurements @xmath2 .",
    "it has been shown that @xmath0 minimization allows recovery of sparse signals from remarkably few measurements  @xcite : supposing @xmath3 is chosen randomly from a suitable distribution , then with very high probability , all sparse signals @xmath11 for which @xmath19 with @xmath20 can be _ perfectly _ recovered by using @xmath18 .",
    "moreover , it has been established @xcite that compressive sensing is robust in the sense that @xmath0 minimization can deal very effectively ( a ) with only approximately sparse signals and ( b ) with measurement noise .",
    "the implications of these facts are quite far - reaching , with potential applications in data compression  @xcite , digital photography  @xcite , medical imaging  @xcite , error correction  @xcite , analog - to - digital conversion  @xcite , sensor networks  @xcite , and so on .",
    "( we will touch on some more concrete examples in section  [ sec : exp ] . )",
    "the use of @xmath0 regularization has become so widespread that it could arguably be considered the `` modern least squares '' .",
    "this raises the question of whether we can improve upon @xmath0 minimization ?",
    "it is natural to ask , for example , whether a different ( but perhaps again convex ) alternative to @xmath17 minimization might also find the correct solution , but with a lower measurement requirement than @xmath0 minimization .    in this paper",
    ", we consider one such alternative , which aims to help rectify a key difference between the @xmath0 and @xmath17 norms , namely , the dependence on magnitude : larger coefficients are penalized more heavily in the @xmath0 norm than smaller coefficients , unlike the more democratic penalization of the @xmath17 norm . to address this imbalance , we propose a weighted formulation of @xmath0 minimization designed to more democratically penalize nonzero coefficients . in section  [ sec : overview ] , we discuss an iterative algorithm for constructing the appropriate weights , in which each iteration of the algorithm solves a convex optimization problem , whereas the overall algorithm does not . instead , this iterative algorithm attempts to find a local minimum of a concave penalty function that more closely resembles the @xmath17 norm .",
    "finally , we would like to draw attention to the fact that each iteration of this algorithm simply requires solving one @xmath0 minimization problem , and so the method can be implemented readily using existing software .    in section  [ sec : exp ] , we present a series of experiments demonstrating the superior performance and broad applicability of this algorithm , not only for recovery of sparse signals , but also pertaining to compressible signals , noisy measurements , error correction , and image processing . this section doubles as a brief tour of the applications of compressive sensing . in section  [ sec : l1analysis ] , we demonstrate the promise of this method for efficient data acquisition .",
    "finally , we conclude in section  [ sec : discussion ] with a final discussion of related work and future directions .",
    "consider the `` weighted '' @xmath0 minimization problem @xmath21 where @xmath22 are positive weights . just like its `` unweighted '' counterpart ( @xmath16 )",
    ", this convex problem can be recast as a linear program . in the sequel",
    ", it will be convenient to denote the objective functional by @xmath23 where @xmath24 is the diagonal matrix with @xmath25 on the diagonal and zeros elsewhere .",
    "the weighted @xmath0 minimization ( @xmath26 ) can be viewed as a relaxation of a weighted @xmath17 minimization problem @xmath27 whenever the solution to ( @xmath15 ) is unique , it is also the unique solution to ( @xmath28 ) provided that the weights do not vanish .",
    "however , the corresponding @xmath0 relaxations ( @xmath16 ) and ( @xmath26 ) will have different solutions in general .",
    "hence , one may think of the weights @xmath29 as free parameters in the convex relaxation , whose values  if set wisely  could improve the signal reconstruction .",
    "this raises the immediate question : what values for the weights will improve signal reconstruction ?",
    "one possible use for the weights could be to counteract the influence of the signal magnitude on the @xmath0 penalty function .",
    "suppose , for example , that the weights were inversely proportional to the true signal magnitude , i.e. , that @xmath30 if the true signal @xmath11 is @xmath31-sparse , i.e. ,  obeys @xmath32 , then ( @xmath26 ) is guaranteed to find the correct solution with this choice of weights , assuming only that @xmath33 and that just as before , the columns of @xmath3 are in general position .",
    "the large ( actually infinite ) entries in @xmath34 force the solution @xmath35 to concentrate on the indices where @xmath34 is small ( actually finite ) , and by construction these correspond precisely to the indices where @xmath11 is nonzero .",
    "it is of course impossible to construct the precise weights without knowing the signal @xmath11 itself , but this suggests more generally that large weights could be used to discourage nonzero entries in the recovered signal , while small weights could be used to encourage nonzero entries .    for the sake of illustration ,",
    "consider the simple 3-d example in figure  [ fig : pinched ] , where @xmath36^t$ ] and @xmath37.\\ ] ] we wish to recover @xmath11 from @xmath38^t$ ] .",
    "figure  [ fig : pinched](a ) shows the original signal @xmath11 , the set of points @xmath39 obeying @xmath40 , and the @xmath0 ball of radius 1 centered at the origin .",
    "the interior of the @xmath0 ball intersects the feasible set @xmath41 , and thus ( @xmath16 ) finds an incorrect solution , namely , @xmath42^t \\neq \\true$ ] ( see figure  [ fig : pinched](b ) ) .",
    "[ cols=\"^,^,^ \" , ]",
    "in summary , reweighted @xmath0 minimization outperforms plain @xmath0 minimization in a variety of setups .",
    "therefore , this technique might be of interest to researchers in the field of compressed sensing and/or statistical estimation as it might help to improve the quality of reconstructions and/or estimations .",
    "further , this technique is easy to deploy as ( 1 ) it can be built on top of existing @xmath0 solvers and ( 2 ) the number of iterations is typically very low so that the additional computational cost is not prohibitive .",
    "we conclude this paper by discussing related work and possible future directions .      whereas we have focused on modifying the @xmath0 norm ,",
    "a number of algorithms been have proposed that involve successively reweighting alternative penalty functions .",
    "in addition to irls ( see section  [ sec : irls ] ) , several such algorithms deserve mention .",
    "gorodnitsky and rao  @xcite propose focuss as an iterative method for finding sparse solutions to underdetermined systems . at each iteration",
    ", focuss solves a reweighted @xmath43 minimization with weights @xmath44 for @xmath45 . for nonzero signal coefficients",
    ", it is shown that each step of focuss is equivalent to a step of the modified newton s method for minimizing the function @xmath46 subject to @xmath47 .",
    "as the iterations proceed , it is suggested to identify those coefficients apparently converging to zero , remove them from subsequent iterations , and constrain them instead to be identically zero .    in a small series of experiments ,",
    "we have observed that reweighted @xmath0 minimization recovers sparse signals with lower error ( or from fewer measurements ) than the focuss algorithm .",
    "we attribute this fact , for one , to the natural tendency of unweighted @xmath0 minimization to encourage sparsity ( while unweighted @xmath43 minimization does not ) .",
    "we have also experimented with an @xmath48-regularization to the reweighting function ( [ eq : wfocuss ] ) that is analogous to ( [ eq : rwrule ] ) .",
    "however we have found that this formulation fails to encourage strictly sparse solutions .",
    "( sparse solutions can be encouraged by letting @xmath49 as the iterations proceed , but the overall performance remains inferior to reweighted @xmath0 minimization with fixed @xmath48 . )",
    "harikumar and bresler  @xcite propose an iterative algorithm that can be viewed as a generalization of focuss . at each stage , the algorithm solves a convex optimization problem with a reweighted @xmath43 cost function that encourages sparse solutions .",
    "the algorithm allows for different reweighting rules ; for a given choice of reweighting rule , the algorithm converges to a local minimum of some concave objective function ( analogous to the log - sum penalty function in  ( [ eq : logsum ] ) ) .",
    "these methods build upon @xmath43 minimization rather than @xmath0 minimization .",
    "delaney and bresler  @xcite also propose a general algorithm for minimizing functionals having concave regularization penalties , again by solving a sequence of reweighted convex optimization problems ( though not necessarily @xmath43 problems ) with weights that decrease as a function of the prior estimate . with the particular choice of a log - sum regularization penalty ,",
    "the algorithm resembles the noise - aware reweighted @xmath0 minimization discussed in section  [ sec : qcl1 ] .    finally , in a slightly different vein",
    ", chartrand  @xcite has recently proposed an iterative algorithm to minimize the concave objective @xmath50 with @xmath51 .",
    "( the algorithm alternates between gradient descent and projection onto the constraint set @xmath52 . )",
    "while a global optimum can not be guaranteed , experiments suggests that a local minimum may be found  when initializing with the minimum @xmath43 solution  that is often quite sparse .",
    "this algorithm seems to outperform @xmath18 in a number of instances and offers further support for the utility of nonconvex penalties in sparse signal recovery . to reiterate",
    ", a major advantage of reweighted @xmath0 minimization in this thrust is that ( 1 ) it can be implemented in a variety of settings ( see sections  [ sec : exp ] and  [ sec : l1analysis ] ) on top of existing and mature linear programming solvers and ( 2 ) it typically converges in very few steps .",
    "the log - sum penalty is also more @xmath17-like and as we discuss in section  [ sec : variations ] , additional concave penalty functions can be considered simply by adapting the reweighting rule .",
    "* under what conditions does the algorithm converge ? that is , when do the successive iterates @xmath53 have a limit @xmath54 ? * as shown in section [ sec : overview ] , when there is a sparse solution and the reweighted algorithm finds it , convergence may occur in just very few steps .",
    "it would be of interest to understand this phenomenon more precisely . * what are smart and robust rules for selecting the parameter @xmath48 ?",
    "that is , rules that would automatically adapt to the dynamic range and the sparsity of the object under study as to ensure reliable performance across a broad array of signals . of interest are ways of updating @xmath48 as the algorithm progresses towards a solution . of course ,",
    "@xmath48 does not need to be uniform across all coordinates .",
    "* we mentioned the use of other functionals and reweighting rules .",
    "how do they compare ?",
    "* finally , any result quantifying the improvement of the reweighted algorithm for special classes of sparse or nearly sparse signals would be significant .",
    "e.  c. was partially supported by a national science foundation grant ccf-515362 , by the 2006 waterman award ( nsf ) and by a grant from darpa .",
    "this work was performed while m.  w. was an nsf postdoctoral fellow ( nsf dms-0603606 ) in the department of applied and computational mathematics at caltech .",
    "s.  b. was partially supported by nsf award 0529426 , nasa award nnx07aeiia , and afosr awards fa9550 - 06 - 1 - 0514 and fa9550 - 06 - 1 - 0312 .",
    "we would like to thank nathaniel braun and peter stobbe for fruitful discussions about this project .",
    "parts of this work were presented at the fourth ieee international symposium on biomedical imaging ( isbi  07 ) held april 1215 , 2007 and at the von neumann symposium on sparse representation and high - dimensional geometry held july 812 , 2007 .",
    "related work was first developed as lecture notes for the course _ ee364b : convex optimization ii _",
    ", given at stanford winter quarter 2006 - 07 @xcite ."
  ],
  "abstract_text": [
    "<S> it is now well understood that ( 1 ) it is possible to reconstruct sparse signals exactly from what appear to be highly incomplete sets of linear measurements and ( 2 ) that this can be done by constrained @xmath0 minimization . in this paper , we study a novel method for sparse signal recovery that in many situations outperforms @xmath0 minimization in the sense that substantially fewer measurements are needed for exact recovery . </S>",
    "<S> the algorithm consists of solving a sequence of weighted @xmath0-minimization problems where the weights used for the next iteration are computed from the value of the current solution . </S>",
    "<S> we present a series of experiments demonstrating the remarkable performance and broad applicability of this algorithm in the areas of sparse signal recovery , statistical estimation , error correction and image processing . </S>",
    "<S> interestingly , superior gains are also achieved when our method is applied to recover signals with assumed near - sparsity in overcomplete representations  not by reweighting the @xmath0 norm of the coefficient sequence as is common , but by reweighting the @xmath0 norm of the transformed object . </S>",
    "<S> an immediate consequence is the possibility of highly efficient data acquisition protocols by improving on a technique known as compressed sensing </S>",
    "<S> .    * keywords . * </S>",
    "<S> @xmath0-minimization , iterative reweighting , underdetermined systems of linear equations , compressed sensing , the dantzig selector , sparsity , focuss . </S>"
  ]
}