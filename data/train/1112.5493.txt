{
  "article_text": [
    "in contrast to information - losing or lossy data compression , the lossless compression of data , the central problem of information theory , was essentially opened and closed by claude shannon in a 1948 paper@xcite .",
    "shannon showed that the entropy formula ( introduced earlier by gibbs in the context of statistical mechanics ) establishes a lower bound on the compression of data communicated across some channel - no algorithm can produce a code whose average codeword length is less than the shannon information entropy .",
    "if the probability of codeword symbol @xmath0 is @xmath1 : @xmath2    this quantity is the amount of information needed to invoke the axiom of choice and sample an element from a distribution or set with measure ; any linear measure of choice must have its analytic form of expected log - probability@xcite .",
    "this relies on the knowledge of a probability distribution over the possible codewords . without a detailed knowledge of the process producing the data , or enough data to build a histogram",
    ", the entropy may not be easy to estimate .",
    "in many practical cases , entropy is most readily measured by using a general - purpose data compression algorithm whose output length tends toward the entropy , such as lempel - ziv . when the distribution is uniform , the shannon / gibbs entropy reduces to the boltzmann entropy function of classical thermodynamics ; this is simply the logarithm of the number of states .",
    "the entropy limit for data compression established by shannon applies to the exact ( lossless ) compression of any type of data . as such",
    ", shannon entropy corresponds more directly to written language , where each symbol is presumably equally important , than to raw numerical data , where leading digits typically have more weight than trailing digits . in general , an infinite number of trailing decimal points must be truncated from a real number in order to obtain a finite , rational measurement .",
    "since some bits have much higher value than others , numerical data is naturally amenable to information - losing ( lossy ) data compression techniques , and such algorithms have become routine in the digital communication of multimedia data . for the case of a finite - precision numerical datum , rather than",
    "the shannon entropy , a more applicable complexity measure might be chaitin s algorithmic prefix complexity@xcite which measures the irreducible complexity of the leading digits from an infinite series of bits .",
    "the algorithmic prefix complexity is an example of a kolmogorov complexity@xcite , the measure of minimal descriptive complexity playing a central role in kolmogorov s formalization of probability theory .",
    "prior to the twentieth century , this basic notion of a probability distribution function ( pdf ) had not changed significantly since the time of gauss .",
    "after analysis of the brownian motion by einstein and others , building on the earlier work of markov , the stochastic process became a popular idea .",
    "stochastic processes represent the fundamental , often microscopic , actions which lead to frequencies tending , in the limit , to a probability density .",
    "stochastic partial differential equations ( for example , the fokker - planck equation ) generate a pdf as their solution , as do the master equations from whence they are derived ; such pdfs may describe , for instance , the evolution of probabilities over time .",
    "they were used notably by langevin to separate dynamical systems into a deterministic classical part and a random stochastic component or statistical model . given empirical data from such a system , the langevin approach may be combined with the maximum likelihood method@xcite or bayesian inference ( maximum posterior method ) to identify the most likely parameters for an unknown noise function .    in practice",
    ", langevin s approach either posits the form of a noise function or fits it to data ; it does not address whether or not data is stochastic in the first place .",
    "kolmogorov addressed this issue , refining the notion of stochastic processes and probability in general .",
    "some objects , a solid black image , for example , are almost entirely regular .",
    "other data seem totally random ; for instance , geiger counters recording radioactive decays .",
    "stochastic objects lay between these two extremes ; as such , they exhibit both deterministic and random behavior .",
    "kolmogorov introduced a technique for separating a message into random and nonrandom components .",
    "first , however , he defined the kolmogorov complexity , @xmath3 .",
    "@xmath3 is the minimum amount of information needed to completely reconstruct some object , represented as a binary string of symbols , x@xcite . @xmath4",
    "the recursive function f may be regarded as a particular computer and p is a program running on that computer .",
    "the kolmogorov complexity is the length of the shortest computer program that terminates with x as output on computer f. in this way , it symbolizes perfect data compression . for various reasons ( such as the non - halting of certain programs )",
    ", it is usually impossible to prove that non - trivial representations are minimal . on the other hand",
    ", a halting program always exists , the original string , so a minimal halting program also exists , even if its identity ca nt be verified . in practice , the kolmogorov complexity asymptotically approaches the shannon entropy@xcite , and the complexity of typical objects may be readily approximated using the length of an entropic code .",
    "often , a variant of kolmogorov complexity is used - chaitin s algorithmic prefix complexity @xmath5@xcite which considers only self - delimiting programs that do not use stop symbols . since",
    "a program may be self - delimiting by iteratively prefixing code lengths , @xmath6@xcite    returning to the separation of signal and noise , we now define stochasticity as it relates to the kolmogorov structure function . for natural numbers @xmath7 and",
    "@xmath8 , we say that a string x is @xmath9-stochastic if and only if there exists a finite set @xmath10 such that : @xmath11    the deviation from randomness , @xmath8 , indicates whether x is a typical or atypical member of a. this is minimized through the kolmogorov structure function , @xmath12 : @xmath13    the minimal set @xmath14 minimizes the deviation from randomness , @xmath8 , and is referred to as the kolmogorov minimal sufficient statistic for x given n. the kolmogorov structure function specifies the bits of additional entropy ( shannon entropy reduces to the logarithm function for a uniform distribution ) necessary to select the element x from a set described with k or fewer bits . for a regular object ,",
    "the structure function has a slope less than -1 .",
    "specifying another bit of k reduces the entropy requirement by more than a bit , resulting in compression . beyond a critical threshold , corresponding to the minimal sufficient statistic , stochastic objects become random . beyond this point , specifying another bit of k increases the entropy by exactly one bit , so the slope of the structure function reaches its maximum value of -1 . for this reason ,",
    "kolmogorov identified the point at which the slope reaches -1 as the minimal sufficient statistic .",
    "the kolmogorov minimal sufficient statistic represents the amount of information needed to capture all the regular patterns in the string x without literally specifying the value of random noise .    while conceptually appealing",
    ", there are practical obstacles to calculating the kolmogorov minimal sufficient statistic .",
    "first , since the kolmogorov complexity is not directly calculable , neither is this statistic .",
    "approximations may be made , however , and when using certain common data compression algorithms , the point having slope 1 is actually a reasonable estimate of the onset of noise . when certain data are compressed more fully , however , this point may not exist .",
    "for example , consider a color photograph of black and white static on an analog tv set .",
    "the pattern of visible pixels emerges from nearly incompressible entropy ; chaos resulting from the machine s attempt to choose values from a nonexistent signal .",
    "since a color photograph has three channels , and the static is essentially monochromatic ; the channels are correlated to one another and hence contain compressible mutual information . as such",
    ", the noise in the color photograph , though emergent from essentially pure entropy , is intrinsically compressible - hence , the compression ratio never reaches 1:1 and the kolmogorov minimal sufficient statistic does not exist .    instead of the parameter value where the compression ratio reaches 1:1 , which may not exist , one often seeks the parameter value which provides the most information about the object under consideration .",
    "the problem of determining the most informative parameters in a model was famously addressed by the statistician r.a .",
    "fisher@xcite .",
    "the fisher information quantifies the amount of information expected to be inferred in a local neighborhood of a continuously parameterizable probability distribution .",
    "the fisher information quantifies information at specific values of the parameters - it quantifies the informative - ness of a local observation .",
    "if the probability density of x is parameterized along some path by t , f(x;t ) , then the fisher information metric at some value of t is the expectation of the variance of the hartley information@xcite , also known as the score : @xmath15\\ ] ]    the fisher information quantifies the convexity ( the curvature ) of an entropy function at a specific point in parameter space , provided sufficient regularity and differentiability .    in the case of multiple parameters",
    ", the fisher information becomes the fisher information metric ( or fisher information matrix , fim ) , the expected covariance of the score : @xmath16\\ ] ]    the fisher - rao metric is simply the average of the metric implied by hartley information over a parameterized path .",
    "the space described by this metric has distances that represent differences in information or entropy .",
    "the differential geometry of this metric is sometimes called _",
    "information geometry_. we seek the parameter values maximizing the fisher - rao metric , for variations in these values lead to the largest possible motions in the metric space of information .",
    "if we take @xmath17 to be the universal probability of obtaining a string x from a randomly generated program on a universal computer , this probability is typically dominated by the shortest possible program , implying that @xmath18 , where we have used @xmath5 instead of @xmath3 so the sum over all x will convergence to unit probility@xcite . if we make the string @xmath19 a function of some parameter t , @xmath20 , then @xmath21 , the hartley information is @xmath22 times @xmath23 , and its associated fisher - rao metric is : @xmath24\\ ] ]    since the spaces we consider are generally discrete , we will consider paths from one parameter value to the next and evaluate partial differences in place of the partial derivatives .",
    "the one - dimensional fisher information of a path from @xmath25 to @xmath26 is , replacing the continuous differentials with finite differences , and ignoring the expectation operator , which becomes the identity operator since the expectation covers only one point : @xmath27    maximizing this quantity is equivalent to maximizing @xmath28 , which is also the denominator in the slope of the kolmogorov structure function . for incompressible data ,",
    "the numerator @xmath29 ( which is the number of additional bits erased by the uncompressed @xmath30 beyond those erased by the more descriptive @xmath31 ) also takes on this value .",
    "since the parameter in the kolmogorov structure function corresponds to bits of description length , the literal description corresponding to each subsequent parameter value differs in length by a constant , minimizing the slope of the kolmogorov structure function is equivalent to maximizing @xmath28 and the fisher information .",
    "the minimal parameter that maximizes the fisher information is the kolmogorov minimal sufficient statistic .    sometimes , rather than considering the point at which a phase transition is complete , we wish to consider the critical point at which it proceeds most rapidly .",
    "for this , we use the expectation of the hessian of the hartley information : @xmath32\\ ] ] this is in contrast to the expectation of the hartley information , the entropy , or the expected curvature of the hartley information , the fisher information .",
    "when this function is maximized , the fisher information ( or slope of the kolmogorov structure function ) is changing as rapidly as possible .",
    "this means that the phase transition of interest is at its critical point and proceeding at its maximum rate . beyond this point , the marginal utility of each additional bit decreases as the phase transition proceeds past criticality to completion at the minimal sufficient statistic .",
    "the derivatives in the fisher information were approximated using backwards differences in complexity ; however , a forward second difference may applied subsequently to complete the hessian , the net result of this is a central difference approximation to the second derivative of complexity : @xmath33 .",
    "the maximum resulting from this approximation is between the maximum and minimum values of the parameter , exclusively .    in practice , since we ca nt calculate @xmath5 exactly , it is helpful to treat any value of the fisher information ( or the slope of the kolmogorov structure function ) within some tolerance of the maximum @xmath34 as being a member of a nearly - maximum set , and select the element of this set having the fewest bits .",
    "usually , the representation having the lowest complexity is the one with the lowest bit depth or resolution , but not always - when lossless compression is applied to highly regular objects , the lossless representation may be simpler than any 2-part or purely lossy code .",
    "this statistic the represents all the bits of signal that can be described before additional bits require a nearly maximal description - it quantifies the minimum complexity needed to complete a phase transition from a low - complexity periodic signal to a high - complexity chaotic one .",
    "this also applies to the maximum of the second derivative , as considered above .",
    "any value of the hessian that is within some tolerance of the maximal @xmath35 is considered part of a nearly - maximal set , and the simplest element of this set is selected as the critical point .",
    "the sufficiency of a statistic was also defined by fisher in 1921@xcite . if a statistic is sufficient , then no other statistic provides any additional information about the underlying distribution .",
    "fisher also demonstrated the relationship between maximum likelihood and sufficient statistics .",
    "the fisher - neyman factorization theorem says that for a sufficient statistic t(x ) , the probability density @xmath36 factors into terms dependent and independent of the parameter : @xmath37 .",
    "the maximum likelihood function for the parameter @xmath38 depends only the sufficient statistic @xmath39 . as a result ,",
    "a sufficient statistic is ideal for determining the parameters of a distribution using the popular method of maximum likelihood estimation ( mle ) .",
    "the most efficient possible articulation of a sufficient statistic is a minimal sufficient statistic .",
    "a sufficient statistic @xmath40 is minimal if and only if , for all sufficient statistics @xmath39 , there exists a function f such that @xmath41 .",
    "partitioning the information content of a string into the complexity of its signal and the entropy of its noise is a nuanced idea that takes on several important forms , another is the algorithmic entropy@xcite @xmath42 of a string .",
    "this is defined in its most basic form as : @xmath43    in this context , @xmath44 is a description of a macroscopic observation constructed by truncating a microscopic state x to a bit string of length m. k(z ) is the algorithmic prefix complexity@xcite of this representation , and @xmath45 is the boltzmann entropy divided by its usual mulplicative constant , k , the boltzmann constant , and @xmath46 , since we are using bits .",
    "n is the logarithm of the multiplicity or volume of truncated states , having universal recursive measure @xmath47 , so @xmath48 and the algorithmic entropy is @xmath49 .",
    "this function is also known as the martin lf universal randomness test and plays a central role in the theory of random numbers .",
    "the partitioning of a string into signal and noise also allows the determination of the limit to its lossy compression@xcite , relative to a particular observer .",
    "if p(x ) is the set of strings which some macroscopic observer p can not distinguish from string x , then the simplest string from this set is the minimal description equivalent to x : @xmath50    we refer to this complexity as the macrostate complexity@xcite or macrocomplexity since its criterion of indistinguishability corresponds to the definition of a macrostate in classical thermodynamics ; a macrostate is a set of indistinguishable microstates .",
    "likewise , its entropy function has the form ( logarithm of cardinality ) of the boltzmann entropy ; it may be shown that if the probability @xmath51 of the class @xmath52 is dominated by the shortest program in the class such that @xmath53 , the macrocomplexity @xmath54 is approximately : @xmath55    this first - order approximation to macrocomplexity is close to the effective complexity of gell - mann and lloyd@xcite . the effective complexity , y , is summed with the shannon entropy , i , or an even more general entropy measure , such as rnyi entropy , to define an information measure , the total information @xmath56 , that is typically within a few bits of k(x)@xcite .",
    "critical data compression codes the most significant bits of an array of data losslessly , since they are typically redundant , and either fits a statistical model to the remaining bits or compresses them using lossy data compression techniques . upon decompression ,",
    "the significant bits are decoded and added to a noise function which may be either sampled from a noise model or decompressed from a lossy code .",
    "this results in a representation of data similar to the original .",
    "this type of scheme is well - suited for the representation of noisy or stochastic data .    attempting to find short representations of the specific states of a system which has high",
    "entropy or randomness is generally futile , as chaotic data is incompressible . as a result ,",
    "any operation significantly reducing the size of chaotic data must discard information , and this is why such a process is colloquially referred to as lossy data compression .",
    "today , lossy compression is conventionally accomplished by optionally preprocessing and/or partitioning data and then decomposing data blocks onto basis functions .",
    "this procedure , canonicalized by the fourier transform , is generally accomplished by an inner product transformation projecting the signal vector onto a set of basis vectors . however , this is not an appropriate mathematical operation for stochastic data .",
    "stochastic variables are not generally square - integrable , meaning that their inner products do not technically exist .",
    "though a discrete fourier transform may be applied to a stochastic time series sampled at some frequency , the resulting spectrum of the sample will not generally be the spectrum of the process , as parseval s theorem need not apply in the absence of square - integrability .",
    "worse , fourier transforms such as the discrete cosine transform do not properly describe the behavior of light emitted from complex geometries .",
    "a photograph is literally a graph of a cross - section of a solution to maxwell s equations .",
    "the first photographs were created by the absorption of photons on silver chloride surface , for instance .",
    "while it is true that the solution to maxwell s equations in a vacuum take the form of sinusoidal waves propagating in free space , a photograph of a vacuum would not generally be very interesting and , furthermore , the resolution of macroscopic photographic devices is nowhere close to the sampling frequency needed to resolve individual waves of visible light , which typically have wavelengths of a few hundred nanometers . in a limited number of circumstances ,",
    "this is appropriate - for example , a discrete cosine transformation would be ideal to encode photons emerging from a diffraction grating with well - defined spatial frequencies . in general , however , most photographs are sampled well below the nyquist rate necessary to reconstruct the underlying signal , meaning that microscopic detail is being lost to the resolution of the optical device used .",
    "if a photographic scene contains multiple electrons or other charged particles , the resulting wavefront will no longer be sinusoidal , instead being a function of the geometry of charges .",
    "though the sine and cosine functions are orthogonal , they are complete in the sense that they may be used as a basis to express any other function . however , since sinusoids do not generally solve maxwell s equations in the presence of boundary conditions , the coefficients of such an expansion do not correspond to the true modes that carry energy through the electromagnetic field .",
    "the correct set of normal modes - which solve maxwell s equations and encode the resulting light - will be eigenfunctions or green s functions of the geometry and acceleration of charges@xcite .",
    "for example , when designing waveguides ( for example , fiber optics ) the choice of a circular or rectangular cross - section is crucially important as this geometry determines whether the electric or the magnetic field is allowed to propagate along its transverse dimension . calculating the fourier cosine transform of these fields produces a noisy spectrum ; however , expanding over transverse electric and magnetic modes could produce an idealized spectrum that has all of its intensity focused into a single mode and no amplitude over the other modes .",
    "the proper , clean spectrum is appropriate for information - losing approximations - since the ( infinite ) spectrum contains no energy beyond the modes under consideration , it can be truncated without compromising accuracy . for the complex electronic geometries and motions that comprise interesting real - world photograph , these modes may be difficult to calculate , but they still exist as solutions of maxwell s equations . attempting to describe them using sinusoids that do nt solve maxwell s equations leads to incompressible noise and spectra that ca nt be approximated accurately .    for audio , however , we note that the situation is somewhat different .",
    "audio signals have much lower frequency than visible light , so they are sampled above the nyquist rate .",
    "44,100hz is a typical sampling rate which faithfully reconstructs sinusoids having frequency components less than 22,050hz , which includes the vast majority of audible frequencies .",
    "auditory neurons will phase - lock directly to sinusoidal stimuli , making audio perception amenable to fourier - domain signal processing .",
    "if compressed in the time domain , the leading bits ( which exhibit large , rapid oscillations ) often appear more random to resource - limited data compressors than the leading bits of a fourier spectrum . at the same time ,",
    "the less - important trailing bits are often redundant , given these leading bits , owing to vibrational modes which vary slowly compared to the sampling rate .",
    "this reverses the trend observed in most images - their most significant bits are usually smoother and more redundant than trailing bits . in the strictly positive domain of fourier - transformed audio ,",
    "however , the leading bits become smoother , due the use of an appropriate sampling rate . for macroscopic photographic content ,",
    "however , individual waves can not be resolved , making fourier - domain optical processing less effective .",
    "nonetheless , scientists and engineers in all disciplines all over the world successfully calculate fourier transforms of all sorts of noisy data , and a large fraction ( if not the vast majority ) of all communication bandwidth is devoted to their transmission .",
    "jpeg images use discrete cosine transforms , a form of discrete fourier transform , as does mp3 audio and most video codecs .",
    "other general - purpose transformations , such as wavelets , are closely related to the fourier transform and still suffer from the basic problem of projecting stochastic data onto a basis - the inner products do nt technically exist , resulting in a noisy spectrum . furthermore",
    ", since the basis used does nt solve generally solve maxwell s equations , finite - order approximations that truncate the spectrum will not translate into results that are accurate to the desired order .",
    "as such , we seek an alternative to expanding functions over a generic basis set .",
    "the limit of lossy data compression is the kolmogorov complexity of the macrostate perceived by the observer@xcite . explicitly describing the macrostates perceptually coded by a human observer",
    "is prohibitively difficult , which makes optimal compression for a human observer intractable , even if the complexity were exactly calculable .",
    "however , the truncation of data which appears in the definition of prefix complexity provides a very natural means of separating 2-part codes , the prefix complexity appears in the definition of the algorithmic entropy@xcite , which is a special case of macrostate complexity .",
    "truncation of amplitude data provides a simple but universal model of data observation - an observer should regard the most significant bits of a datum as being more important than its least significant bits .",
    "the codes described in this paper are the sum of a truncated macrostate , @xmath44 , which we call the signal , as well as a lossy approximation of the bits that were truncated from this signal , which we will refer to as that signal s residual noise function .",
    "this is in contrast to the algorithmic entropy , which combines a truncated macrostate with all the information needed to recover its microstate .",
    "if @xmath25 samples are truncated from @xmath57 bits to @xmath58 , the boltzmann entropy is proportional to @xmath59 and the algorithmic entropy is @xmath60 , however , since we only store @xmath61 bits using lossless compression , the savings resulting from a two - part code ( compared to a lossless entropic code ) could approach the boltzmann entropy @xmath62 in the case of a highly compressed lossy representation .",
    "first , the bits of datum @xmath63 are reordered in the string @xmath19 , from most significant to least significant .",
    "this simplifies truncation and its correspondence to the conditional prefix complexity .",
    "the resulting string is truncated to various depths , and the compressibility of the resulting string is evaluated .",
    "the point beyond which the object has attains maximum incompressibility also maximizes the fisher information associated with the distribution @xmath64 .",
    "as discussed in the previous section , this phase transition proceeds at its maximum rate when the expected hessian of the hartley information , @xmath65 , is maximal .    since the phase transition between periodicity and chaos is generally somewhat gradual , several possible signal depths could be used , to varying effect .",
    "following ehrenfest s categorization of critical points by the derivative which is discontinuous , we will also refer to critical points by the order of derivatives . due to the discrete nature of our analysis",
    ", our difference approximations never become infinite , instead , we seek the maxima of various derivatives of the information function .",
    "in the first - order approximation to the universal probability , the hartley information is simply the complexity @xmath66 , therefore , multiplying the problem by -1 , we classify critical points by the derivatives of complexity which have minima there .",
    "the first of these , @xmath67 , corresponds to the fisher - rao metric , and its maxima correspond to sufficient statistics .",
    "if the object is chaotic beyond some level of description , then this level is also the kolmologorov minimal sufficient statistic .",
    "the second order matrix , @xmath68 , is the point at which the fisher information increases most rapidly and hence the point beyond which descriptional complexity results in diminishing returns .",
    "higher - order critical points may be considered as well , but become progressively more difficult to determine reliably in the presence of imperfect complexity estimates , so we will analyze only the first two orders .    the choice of a first order critical point ( @xmath69 ) or a second order critical point ( @xmath70 ) as a cutoff for data compression will reflect a preference for fidelity or economy , respectively .",
    "other considerations may lead to alternative signal depths - the mean squared errors of images having various cutoffs are considered in the examples section , for instance .",
    "regardless of the critical point chosen , the redundant , compressible signal component defined by the selected cutoff point is isolated and compressed using a lossless code .",
    "ideally , an accurate statistical model of the underlying phenomenon , possibly incorporating psychological or other factors , would be fit to the noise component using maximum likelihood estimation to determine the most likely values of the parameters of its distribution . instead of literally storing incompressible noise ,",
    "the parameters of the statistical model are stored .",
    "when the code is decompressed , the lossless signal is decompressed , while the noise is simulated by sampling from the distribution of the statistical model . the signal and simulated noise",
    "are summed , resulting in an image whose underlying signal and statistical properties agree with the original image .    since statistical modeling of general multimedia data may be impractical , lossy data compression methods may be applied to the noise function .",
    "a successful lossy representation may be regarded as an alternate microstate of the perceived noise macrostate ; it is effectively another sample drawn from the set of data macroscopically equivalent to the observed datum@xcite . as such , in the absence of an appropriate model , the noise function is compressed using lossy methods , normalized such that the resulting intensities do not exceed the maximum possible amplitude .",
    "this representation will be decompressed and added to the decompressed signal to reconstruct the original datum .",
    "this has several advantages .",
    "first , the signal is relatively free of spurious artifacts , such as ringing , which interfere with the extraction of useful inferences from this signal . artifacts from lossy compression can not exceed the amplitude of the noise floor , and higher levels of lossy compression may be used as a result of this fact .",
    "furthermore , lossy compression algorithms tend to compress high frequencies at a higher level than lower frequencies .",
    "the eyes and ears tend to sense trends that exhibit change over broader regions of space or time , as opposed to high - frequency oscillations .",
    "the compressibility of signal and noise leads to an information - theoretic reason for this phenomenon - the former naturally requires less of the nervous system s communication bandwidth than the latter .",
    "the compression ratios afforded by such a scheme can be dramatic for noisy data . as a trivial example , consider a string containing only random noise , such as the result of @xmath25 independently distributed bernoulli trials having probability @xmath71 , such as a coin flip .",
    "lossless entropic compression can not effectively compress such a string .",
    "decomposing such a string into basis functions , such as the fourier amplitudes or wavelets used in the jpeg algorithms , inevitably results in a mess of spurious artifacts with little resemblance to the original string .",
    "the critical compression scheme described , however , easily succeeds in reproducing noise that is statistically indistinguishable from ( though not identical to ) the original string .",
    "furthermore , all that needs to be stored to sample from this distribution is the probability @xmath71 of the bernoulli trial , which has complexity o(1 ) .",
    "the observer for which this scheme is optimal makes statistical inferences of amplitude in a manner similar to a physical measurement .",
    "the observer records the statistics of the data , e.g. mean , variance , etc .",
    ", rather than encoding particular data , which could introduce bias .",
    "if the data is a waveform sampled at a frequency exceeding its effective nyquist rate , such as an audio recording sampled at more than twice the frequency of a listener s ear , then its spectrum may be analyzed rather than its time series .",
    "this will make the data smoother and non - negative , resulting in better compression for the leading bits . in practical terms , this means that we may compress audio by compressing an one - dimensional image which is a graph of its spectrum , or the spectrum of some portion of the time series .",
    "hence , we will develop the method using images as a canonical example , with the understanding that audio may be compressed , for example , using 1-d images of fourier transforms , and that video may be compressed using an array having the third dimension of time , or by embedding information into lower - dimensional arrays .      for many photographic and video applications ,",
    "it is conventional to rotate a pixel s rgb color space to a color space , @xmath72 , which more naturally reflects the eye s increased sensitivity to brightness as compared to variations in color .",
    "this is normally done in such a way that takes into account the perceived variations in brightness between different phosphors , inks , or other media used to represent color data .",
    "the @xmath63 or luma channel is a black - and - white version of the original image which contains most of the useful information about the image , both in terms of human perception and measurements of numerical error .",
    "the luma channel could be said to be the principal component ( or factor ) of an image with respect to perceptual models .",
    "the blue and red chroma channels ( @xmath73 and @xmath74 , respectively ) effectively blue - shift and/or red - shift white light of a particular brightness ; they are signed values encoding what are typically slight color variations from the luma channel .",
    "it is conventional for the luma channel to receive a greater share of bandwidth than the less important chroma channels , which are often downsampled or compressed at a lower bitrate .    as an alternative to consistently using a perceptual model optimized for the output of , for example , particular types of monitors or printers",
    ", one could use a similar approach to determine the principal components of color data as encoded rather than perceived .",
    "principal components analysis , also called factor analysis , determines linear combinations of inputs which have the most influence over the output . in principal components analysis , @xmath25 samples of @xmath58-channel sample data",
    "are placed in the columns of an @xmath58-by-@xmath25 matrix a and the matrix product @xmath75 is constructed to obtain an @xmath58-by-@xmath58 matrix .",
    "the eigenvectors of @xmath75 having the largest eigenvalues are the most influential linear combinations of data , the magnitude of these eigenvalues ( sometimes called factor weights ) reflects the importance of a particular combination .",
    "the result of applying principal components analysis to photographic content leads to a customized color space whose principal component is a luma channel whose channel weights correspond to the eigenvector having the largest eigenvalue .",
    "this channel is best communicated at a higher bitrate than the secondary and tertiary components , which are effectively chroma channels . in the appendix",
    ", we will compare the results of critically compressing photographs in rgb format against compression using a critical luma channel with lossy chroma channels .",
    "for most of these photographs , a critically compressed luma channel leads to more efficient representations than using only lossy wavelet transformations",
    ".    in general , perceived output may be optimized by analyzing the principal components of perceived rather than raw data .",
    "in contrast , directly applying principal component analysis ( or factor analysis ) to the raw data leads to a universal coordinate system for sample space which has improved compressibility , albeit optimized for a particular instance of data rather than the perceived output of a particular medium .",
    "in addition to improved compression , another advantage of this approach is that it applies to a wide variety of numerical data and this facilitates a general approach to lossy data compression .",
    "the critical bit depth determines the level of compressible content of a signal .",
    "we now determine expressions for the first and second order critical depths .",
    "this will allow us to separate signal from noise for audio , image , and video data by determining a bit depth that effectively separates signal from noise .",
    "if higher compression ratios are desired , a supercritical signal may be used , meaning that more bits may be truncated , at the cost of destroying compressible information and potentially impeding inference . on the other hand",
    ", a signal retaining nearly all of its bits would necessarily be similar to the original .    for a string which encodes the outcome of a series of independent bernoulli trials ( coin flips , for instance ) as zeros and ones",
    ", each bit constitutes the same amount of information - one bit is one sample . for a string comprised of a series of numeric samples at a bit depth greater than one ,",
    "this is not usually the case . in the traditional representation of numerals ,",
    "leading bits are generally more informative than trailing bits , so an effective lossy data compression scheme should encode leading bits at a higher rate . from the viewpoint of compressibility , on the other hand , the smooth , redundant leading bits of a typical stochastic processes are more compressible than its trailing bits . since the leading bits of multi - bit samples are often more compressible and more significant than the trailing bits , they are candidates for exact preservation using lossless data compression",
    ". since the trailing bits are generally less important and also less compressible , lossy compression can greatly reduce their descriptive complexity without perceptible loss .      since images will be easy to illustrate in this medium , and provide a middle ground as compared to one or three dimensions for audio or video , respectively",
    ", we will treat the two - dimensional case first .",
    "we will then generalize to data having any number of dimensions .",
    "we will refer to the matrices ( rank-2 tensors ) as images , since this is a canonical and intuitive case , but these expressions apply generally to all two - dimensional arrays of binary numbers .",
    "let @xmath76 represent a tensor of rank three ( a tensor of rank",
    "n is an n - dimensional array ) representing one channel of a bitmap image .",
    "subscript indices @xmath0 and @xmath77 represent @xmath78 and @xmath79 coordinates in the image , and the superscript @xmath57 indexes the bits encoding the amplitude of pixel ( i , j ) in the channel , ordered from most significant to least significant .",
    "let the set @xmath80 contain all the images whose n leading bits agree with those of @xmath76 : @xmath81    this set can be represented as the original image channel with bit depth truncated from @xmath57 to @xmath25 .",
    "the implied observer sees n significant bits of ( learnable ) signal and @xmath82 insignificant bits of ( non - learnable ) noise .",
    "for reference , the algorithmic entropy of the truncated string is : @xmath83    the literal length of the reduced image is @xmath84 , and most of this will be saved in a critical compression scheme , as noise can be coded at a high loss rate .",
    "if @xmath85 is the lossy representation , the complexity of the critically compressed representation is :    @xmath86    we may now consider the fisher information ( and hence the minimal sufficient statistic ) of @xmath80",
    ". the fisher information of a path from @xmath25 to @xmath26 is , replacing the continuous differentials with finite differences , and ignoring the expectation operator ( which becomes equivalent to the identity operator ) : @xmath87    the first order bit depth @xmath88 parameterizing the minimal sufficient statistic @xmath89 is the argument n that maximizes the change in complexity , @xmath90 : @xmath91    the first order bit depth of the image channel represented by @xmath76 is @xmath88 .",
    "that is , the first @xmath88 most significant bits in each amplitude encode the signal ; the remaining bits are noise .",
    "the noise floor of the image is @xmath88 .",
    "the second order depth , on the other hand , determines the point of diminishing returns beyond which further description has diminished utility .",
    "it is the maximum of the expected hessian of hartley information , @xmath92 $ ] , so it becomes : @xmath93    this minimizes @xmath94 .",
    "the signal having @xmath95 bits per sample has the high - value bits and the residual noise function contains the bits determined to have diminishing utility .      when considering multiple channels at once , which allows data compression to utilize correlations between these channels , we simply consider a superset of @xmath80 that is the union of the @xmath80 for each channel @xmath96 , @xmath97 .",
    "if all the channels have the same bit depth , for instance , this superset becomes : @xmath98    its corresponding representation is the union of the truncated channels , traditionally , an image would have three of these .",
    "given this new definition , the calculation of first - order depth proceeds as before .",
    "its fisher information is still @xmath99 , which takes its maximum at the minimal sufficient statistic , the first order depth maximizing @xmath90 .",
    "the second order depth , as before , maximizes @xmath94 .",
    "it is also possible to take the union of channels having different bit depths .",
    "the first - order critical parameters ( bit depths ) are best determined by the maximum ( or a near - maximum ) of the fisher - rao metric .",
    "the second - order critical parameters are determined by the maximum ( or a near - maximum ) of the hessian of hartley information .",
    "if stochastic data is not ergodic , that is , if different regions of data have different statistical properties , these regions may have different critical bit depths .",
    "such data can be compressed by separating regions having different bit depths .",
    "this phenomenon occurs frequently in photographs since brighter regions tend to be encoded using more significant bits , requiring fewer leading bits .",
    "brighter regions thus tend to have lower critical depth than darker regions whose signal is encoded using less significant bits .",
    "darker regions require a greater number of leading bits , but their leading zeros are highly compressible .",
    "the simplest way to accomplish this separation , perhaps , is to divide the original data into rectangular blocks ( see the example ) and evaluate each block s critical depth separately .",
    "this is suboptimal for a couple of reasons - one , regions of complex data having different bit depths are rarely perfect rectangles ; two , normalization or other phenomena can lead to perceptible boundary effects at the junctions of blocks .    for this reason",
    ", we develop a means of masking less intense signals for subsequent encoding at a higher bit depth . in this way , the notion of bit depth will be refined - by ignoring leading zeros , the critical bit depth of the data becomes the measure of an optimal number of significant figures ( of the binary fraction @xmath100 ) for sampled amplitudes .",
    "ideally , we would like to encode the low - amplitude signals at a higher bit depth ( given their higher compressibility ) while we make a lossy approximation of the fourier transform of the periodic noise .",
    "if a statistical model is available for this approximation , we use this model for the lossy coding , otherwise , a lossy data compression algorithm is employed .",
    "given the original data , it is easy to distinguish low - amplitude signals from truncated noise - if the original amplitude is greater than the noise floor , a pixel falls into the latter category , otherwise , the former .",
    "this allows us to create a binary mask function @xmath101 associated with bit depth d , it is 0 if the amplitude of the original data sample exceeds the noise floor , and 1 otherwise : @xmath102    this mask acts like a diagonal matrix that left - multiplies a column vector representation of the image @xmath103 .",
    "the resulting signal @xmath104 preserves regions of non - truncated low - intensity signal while zeroing all all other amplitudes .",
    "its complement , not @xmath105 acts on the noise function to preserve regions of periodic truncated noise while zeroing the low - intensity signal .",
    "it is also helpful to consider the literal description length of the samples contained in this region , its bit depth times the number of ones appearing in @xmath106 : @xmath107 , as well as the complexity of the entire signal , @xmath108 , which includes the shape of the region , since this is additional information that needs to be represented .",
    "we may now describe an algorithm that calculates critical depths while separating regions of low - intensity signal .",
    "this procedure truncates some number ( the bit depth ) of trailing digits from the original signal , separates truncated regions having only leading zeros , and calculates the complexity of the reduced and masked signal plus the complexity ( calculated via recursion ) of the excised low - intensity signal at its critical depth .",
    "once this is done , it proceeds to the next bit depth , provided that the maximum depth has not yet been reached . starting with shallow representations having only the most significant bits ( n=0 or 1 , typically ) , we truncate the data to depth n , resulting in the truncated representation of the signal @xmath80 , as well as its truncated residual ( previously noise ) function , which we will call @xmath85 . at this point , the initial truncated signal @xmath80 is compressed using lossless methods , while the mask and its complement are applied to @xmath85 .",
    "this results in a residual signal , @xmath109 ( for these pixels , @xmath85 agrees with the original data @xmath110 ) as well as a complementary residual periodic noise function @xmath111 .",
    "since it contains only residual noise , taken modulo some power of two , the noise @xmath112 is compressed using lossy methods that are typically based on fourier analysis .",
    "the residual signal @xmath113 becomes input for the next iteration .",
    "the procedure iterates using a new @xmath114 that is a truncation of the masked signal @xmath113 , as opposed to the original image .",
    "let the notation @xmath115 represent an operator that truncates amplitudes to bit depth n. in this notation , @xmath116 , and its residual function is @xmath117 .",
    "a new mask @xmath118 is determined from @xmath114 .",
    "using the new mask , we produce a new residual signal , @xmath119 , and a new residual noise , @xmath120 .",
    "@xmath114 is compressed and stored using lossless methods , while @xmath121 is compressed and stored using lossy methods , and the procedure iterates to the next value of n , using @xmath122 as the new signal provided that additional bits exist .",
    "if the maximum bit depth has been reached , there can be no further iteration , so @xmath85 is stored using lossy methods .",
    "though the separation of signal and noise is now iterative , the criterion for critical depth has not changed , only the @xmath80 that appears in their definition .",
    "if @xmath123 is nearly maximal - its largest possible value is the literal length , @xmath124 - the first - order depth has been reached ; if @xmath125 is nearly maximal , the second - order critical depth has been reached .",
    "once the desired depth is reached , the iteration may break , discarding @xmath122 and @xmath121 and storing @xmath85 using lossy methods .    if higher compression ratios are desired , more bits can be truncated and modeled statistically or with lossy methods .",
    "however , the signal introduced to the noise function in such a manner might not fit simple statistical models , and the loss of compressible signal tends to interfere with inference .",
    "since it relates an image s most significant bits to important theoretical notions such as kolmogorov minimal sufficient statistic and algorithmic entropy , critical bit depth is the canonical example of critical data representation .",
    "one could reorder the image data to define a truncated critical scale , simply sampling every nth point , however , this discards significant bits , which tends to introduce aliasing artifacts dependent on the sampling frequency and the frequency of the underlying signal .",
    "these considerations are the topic of the celebrated nyquist - shannon sampling theorem - essentially , the sampling frequency must be at least twice the highest frequency in the signal .",
    "this is known as the nyquist rate , as it was stated by nyquist in 1928@xcite before finally being proved by shannon in 1949@xcite .    as a result of the nyquist - shannon theorem",
    ", a downsampling operation should incorporate a low - pass filter to remove elements of the signal that would exceed the new nyquist rate .",
    "this should occur prior to the sampling operation in order to satisfy the sampling theorem . to complicate matters further ,",
    "idealized filters ca nt be attained in practice , and a real filter will lose some amount of energy due to the leakage of high frequencies . a low - pass filter based on a discrete fourier transform will exhibit more high - frequency leakage than one based on , for example , polyphase filters . since sampling is not the topic of this paper , we will simply refer to an idealized sampling operator @xmath126 that applies the appropriate low - pass filters to resample data in one or more dimensions .",
    "the ability to perform complexity - based inference on a common scale is important since it allows the identification of similar objects , for instance , at different levels of magnification .",
    "critical scale is useful as it provides another degree of freedom along which critical points may be optimized , analogous to phase transitions in matter that depend on both temperature and pressure .",
    "occasionally , the two objectives may be optimized simultaneously at a triple point that is critical for both bit depth and scale .",
    "we now define the critical scale , which we define in terms of operators which resample the image instead of truncating it . for",
    "some data , a minimal sufficient statistic for scale can not be reliably selected or interpreted , and hence a critical scale ca nt be determined .",
    "consider the critical scale of an image at a particular bit depth @xmath57 , which may or may not be the original bit depth of the image . let the linear operator @xmath127 represent a resampling operation , as described above , applied to an image with a spatial period of r in the x dimension and a period of s in the y dimension .",
    "it s action on @xmath76 is a @xmath128 by @xmath129 matrix of resampled amplitudes .",
    "this operator gives us two possible approaches to scaling .",
    "on one hand , given divisibility of the appropriate dimensions , we may vary @xmath130 and @xmath131 to resample the image linearly . on the other hand",
    ", we may also apply the operator repeatedly to resample the image geometrically , given divisibility of the dimensions by powers or r and s. the former may identify the scale of vertical and horizontal components separately , and the latter identifies the overall scale of the image .",
    "we will consider first overall scale , using the iterated operator , and then the horizontal and vertical components .",
    "@xmath132 , then , is the result of applying this operator n times .",
    "let the set used by the structure function , @xmath80 , contain all the images whose m leading bits agree with those of @xmath133 : @xmath134    note that in this case , unlike the critical bit depth , @xmath80 is an averaging which is not necessarily a prefix of the original image .",
    "the original image has @xmath135 bits .",
    "the reduced image has @xmath136 bits .",
    "we may now write the first - order critical scale @xmath88 parameterizing the minimal sufficient statistic @xmath89 , an expression unchanged from the previous case : @xmath137    if higher compression ratios are needed , additional signal may be discarded , as described previously .",
    "the expression for the second - order critical scale is also unchanged : @xmath138    as mentioned previously , the repeated application of averaging operators is not always appropriate or possible .",
    "we will consider linear scaling parameterized along the horizontal axis , with the understanding that the same operations may be applied to the vertical axis , or to any other index .",
    "as such , we equate the parameter @xmath25 with the horizontal factor @xmath130 .",
    "the set @xmath80 then contain all the images whose m leading bits agree with those of @xmath139 : @xmath140    note that this set may not be defined for all n. given this set , the expressions for the maximum fisher information ( the minimal sufficient statistic ) and the maximum of the hessian of hartley information ( the critical point ) do not change .    if a signal is to compressed at some scale other than its original scale , then it will need to be resampled to its original scale before being added to its decompressed ( lossy ) noise function .",
    "note than in this case , the resampled signal is not generally lossless .",
    "this smoothing of data may be acceptable , however , since it enables analysis at a common scale .",
    "we now consider critical bit depths and scales of multidimensional data . instead of a two dimensional array containing the amplitudes of an image , we consider an array with an arbitrary number of dimensions .",
    "as noted earlier , monophonic audio may be represented as a one dimensional array of scalar amplitudes , and video data may be represented as a three dimensional array which also has three channels .",
    "this generalizes the results of the previous two sections , which used the case of a two - dimensional image for illustrative purposes .",
    "let @xmath141 represent a tensor of rank @xmath142 .",
    "its subscripts index coordinates in an @xmath58-index array whose values are @xmath143-dimensional vectors . each vector component is a @xmath144-bit number .",
    "the superscripts index the bits in these numbers , ordered from most significant to least significant .",
    "we will first determine its critical bit depths and then its critical scales .",
    "let the set @xmath145 contain all possible tensors @xmath146 whose whose @xmath147 leading bits agree with those of channel @xmath0 in the original tensor @xmath141 : @xmath148    since there are multiple parameters involved in determining the first - order bit depth , a general tensor requires the use of the full fisher - rao metric , rather than a fisher information : @xmath149    where the expectation value in the definition of i(t ) becomes the identity , as before . for any particular parameter @xmath150",
    ", @xmath151 takes on a maximum with respect to some value that parameter .",
    "this value is the critical depth of channel @xmath7 .",
    "however , this does not necessarily indicate that the set of critical depths globally maximizes i(t ) .",
    "the global maximum occurs at some vector of parameter values @xmath152 : @xmath153    if channels having different depths are inconvenient , a single depth may be selected as before , using the one - parameter fisher information of @xmath64 : @xmath154    the second - order critical depth proceeds in a similar manner .",
    "the expected hessian of hartley information @xmath155 becomes @xmath156 times @xmath157    again , the maximum occurs at a vector of parameter values @xmath158 : @xmath159    we will now consider the critical scale of @xmath160 . let the linear operator @xmath161 represent an idealized resampling of the tensor by a factor of @xmath162 along each dimension @xmath7 .",
    "this operator applies a low - pass filter to eliminate frequency components that would exceed twice the new sampling frequency ( the nyquist rate @xmath163 ) prior to sampling with frequency @xmath164 .",
    "let the set used by the structure function , @xmath145 , contain all the tensors which are preimages of @xmath165 , the rescaled tensor : @xmath166    given this new definition of a , the definition of first and second order critical depth do not change .",
    "the first - order critical scales maximize ( or approximately maximize ) the fisher information : @xmath167    likewise , the second - order critical scales maximize the expected hessian of hartley information : @xmath168    alternately , a single scale parameter could be chosen in each case : @xmath169 @xmath170    hence , we see that the definition of first and second order critical scale of a general tensor is identical to their definition for rank two images .",
    "the above relations are idealized , assuming that k can be evaluated using perfect data compression .",
    "since this is not generally the case in practice , as discussed previously , the maxima of i and j may be discounted by some tolerance factor to produce the threshold of a set of effectively maximal parameter values .",
    "the maximum or minimum values within this set may be chosen as critical parameters .",
    "in addition to multimedia data such as audio ( m=1 ) , images ( m=2 ) , and video ( m=3 ) , these relations enable critical compression and decompression , pattern recognition , and forecasting for many types of data .",
    "we now have an approach to separate significant signals from noise .",
    "encoding the resulting signal follows traditional information theory and lossless compression algorithms . encoding the noise is a separate problem of statistical inference that could assume several variants depending on the type of data involved as well as its observer .",
    "regardless of the details of the implementation , the program is as follows : a statistical model is fit to the noise , its parameters are compressed and stored , and upon decompression , a sample is taken from the model .",
    "a lossless data compression scheme must encode both a signal and its associated noise . however , noise is presumed ergodic and hence no more likely or representative than any other noise sampled from the same probability distribution .",
    "hence , a lossy perceptual code is free to encode a statistical model ( presuming that one exists ) for the noisy bits , as their particular values do nt matter .",
    "this dramatically reduces the complexity of storing incompressible noise ; its effective compression ratio may approach 100% by encoding relatively simple statistical models .",
    "the most significant bits of the signal are compressed using a lossless entropic code ; when the image is decompressed , samples are taken from the model distribution to produce an equivalent instance of noise ; this sampled noise is then added to the signal to produce an equivalent image .",
    "as noted in the introduction , maximum likelihood estimates correspond to sufficient statistics@xcite .",
    "maximum likelihood estimation ( mle ) has been one of the most celebrated approaches to statistical inference in recent years .",
    "there are a wide variety of probability distribution functions and stochastic processes to fit to data , and many specialized algorithms have been developed to optimize the likelihood .",
    "a full discussion of mle is beyond the scope of this paper , but the basic idea is quite simple . having computed a minimal sufficient statistic , we wish to fit a statistical model having some finite number of parameters ( such as moments of the distribution ) to the noisy data .",
    "the parameter values leading to the most probable data are selected to produce the most likely noise model .",
    "our data compression scheme stores these parameter values , compressed losslessly if their length is significant , in order to sample from the statistical model when decompression occurs .    since different regions of data may have different statistical properties , rather than fitting a complex model to the entire noisy string",
    ", it may be advantageous to fit simpler models to localized regions of data .",
    "the description length of optimal parameters tends to be proportional to the number of parameters stored .",
    "if a model contains too many parameters , the size of their representation can approach the size of the literal noise function , reducing the advantage of critical compression .",
    "when the image is decompressed , a sample is drawn from the statistical model stored earlier .",
    "the problem of sampling has also received considerable attention in recent years , due to its importance to so - called monte carlo methods .",
    "the original monte carlo problem , first solved by metropolis and hastings , dealt with the estimation of numerical integrals via sampling .",
    "since then , monte carlo has become somewhat of a colloquial term , frequently referring to any algorithm that samples from a distribution . due to the importance of obtaining a random sample in such algorithms ,",
    "monte carlo sampling has become a relatively developed field .",
    "box - muller is a simple option for sampling from the uniform distibution@xcite , which may be transformed into any other distribution with a known cumulative distribution function .",
    "the ziggurat algorithm@xcite is a popular sampling algorithm for arbitrary distributions .",
    "this algorithm provides reasonably high - performance sampling since the sequence of samples produced is known to repeat only after a very large number of iterations .",
    "psychological modeling should be incorporated into the statistical models of the noise component rather than the signal , since this is where the loss occurs in the compression algorithm . since noise ca",
    "nt be learned efficiently , different instances of noise from the same ergodic source will typically appear indistinguishable to a macroscopic observer who makes statistical inferences . furthermore",
    ", certain distinct ergodic sources will appear indistinguishable .",
    "a good psychological model will contain parameters relevant to the perception of the observer and allow irrelevant quantities to vary freely .",
    "it may be advantageous to transform the noisy data to an orthogonal basis , for example , fourier amplitudes , and fit parameters to a model using this basis . the particular model used will depend on the type of data being observed and , ultimately , the nature of the observer .",
    "for example , a psychoacoustic model might describe only noise having certain frequency characteristics .",
    "the lossy nature of the noise function also provides a medium for other applications , such as watermarking .",
    "maximum likelihood estimation applies only when an analytic model of noise is available . in the absence of a model , the noise function",
    "may be compressed using lossy methods , as described previously , and added to the decompressed signal to reconstruct the original datum .",
    "the most obvious obstacle to this procedure is the fact that lossy data compression algorithms do not necessarily respect the intensity levels present in an image .",
    "for example , high levels of jpeg compression produces blocking artifacts resembling the basis functions of its underlying discrete cosine transformation . fitting these functions to data",
    "may result in spurious minima and maxima , often at the edges or corners of blocks , which frequently exceeds the maximum intensity of the original noise function by nearly a factor of two . with",
    "the wavelet transforms used in jpeg2000@xcite , the spurious artifacts are greatly diminished compared to the discrete cosine transforms of the original jpeg standard , but still present , so a direct summation will potentially exceed the ceiling value allowed by the image s bit depth .    in order to use the lossy representation , the spurious extrema must be avoided .",
    "it is not appropriate to simply truncate the lossy representation at the noise floor , as this leads to clipping effects - false regularities in the noise function that take the form of artificial plateaus . directly normalizing",
    "the noise function does not perform well , either , as globally scaling intensities below the noise floor leads to an overall dimming of the noise function relative to the reconstructed signal .",
    "a better solution is to upsample the noise function , normalizing it to the maximum amplitude , and storing the maximum and minimum intensities of the uncompressed noise . upon decompression ,",
    "the noise function image is de - normalized ( downsampled ) to its original maximum and minimum intensities before being summed with the signal .",
    "this process preserves the relative intensities between the signal and noise components .",
    "therefore , when constructing codes with both lossless and lossy components , a normalized ( upsampled ) noise function is compressed with lossy methods , encoded along with its original bit depth . upon decompression , the noise function is decompressed and re - normalized ( downsampled ) back to its original bit depth before being added to the decompressed signal .",
    "last , but not least , it is worth noting that separating a signal from noise generally improves machine learning and pattern recognition .",
    "this is especially true in compression - based inference .",
    "complexity theory provides a unified framework for inference problems in artificial intelligence@xcite , that is , data compression and machine learning are essentially the same problem@xcite of knowledge representation .",
    "these sorts of considerations have been fundamental to information theory since its inception . in recent years , compression - based inference experienced a revival following rissanen s 1986 formalization@xcite of minimal description length ( mdl ) inference .",
    "though true kolmogorov complexities ( or algorithmic prefix complexities ) ca nt be calculated in any provable manner , the length of a string after entropic compression has been demonstrated as a proxy sufficient for statistical inference . today , data compression is the central component of many working data mining systems .",
    "though it has historically been used to increase effective network bandwidth , data compression hardware has improved in recent years to meet the high - throughput needs of data mining .",
    "hardware solutions capable of 1 gbps or more of lempel - ziv compression throughput can be implemented using today s fpga architectures . in spite of its utility in text analysis ,",
    "compression - based inference remains largely restricted to highly compressible data such as text .",
    "the noise intrinsic to stochastic real - world sources is , by definition , incompressible , and this tends to confound compression - based inference .",
    "essentially , incompressible data is non - learnable data@xcite ; removing this data can improve inference beyond the information - theoretic limits associated with the original string . by isolating the compressible signal from its",
    "associated noise , we have removed the obstacle to inference using the complexity of stochastic data . given improved compressibility ,",
    "the standard methods of complexity - based inference and minimum description length may be applied to greater effect .    by comparing the compressibility of signal components to the compressibility of their concatenations",
    ", we may identify commonalities between signals .",
    "a full treatment of complexity - based inference is beyond the scope of this paper ( the reader is referred to the literature , particularly the book by li and vitnyi@xcite ) but we reproduce for completeness some useful definitions . given signal components a and b , and their concatenation ab , we may define the conditional prefix complexity @xmath171 .",
    "this quantity is analogous to the kullback - leibler divergence or relative entropy , and this allows us to measure a ( non - symmetric ) distance between two strings .",
    "closely related to the problem of inference is the problem of induction or forecasting addressed by solomonoff@xcite .",
    "briefly , if the complexity is k(x ) , then the universal prior probability @xmath172 is typically dominated by the shortest program , implying @xmath173 .",
    "if x is , for example , a time series , then the relative frequency of two subsequent values may be expressed as a ratio of their universal probabilities .",
    "for example , the relative probability that the next bit is a 1 rather than 0 is @xmath174 .",
    "clearly , the evaluation of universal probabilities is crucially dependent on compressibility . as such",
    ", separating signal from noise in the manner described also improves the forecasting of stochastic time series .",
    "sometimes , one wishes to consider not only the complexity of transforming a to b , but also the difficulty of transforming b to a. in the example presented , an image search algorithm , this is not the case - the asymmetric distances produce better and more intuitive results . however , if one wishes to consider the symmetrized distances , the most obvious might be symmetrized sum of conditional complexities @xmath175 and",
    "@xmath176 , @xmath177",
    ". this averaging loses useful information , though , and many authors@xcite suggest using the max - distance or picture distance @xmath178 . when implementing information measures , the desire for the mathematical convenience of symmetric distance functions should be carefully checked against the nature of the application .",
    "for example , scrambling an egg requires significantly fewer interactions than unscrambling and subsequently reassembling that egg , and a reasonable complexity measure should reflect this . for these considerations , as well as",
    "its many convenient mathematical properties@xcite the conditional prefix complexity is often the best measure of the similarity or difference of compressible signals .",
    "empirical evidence suggests that conditional prefix complexity outperforms either the symmetrized mutual information or the max - distance for machine vision .",
    "the reverse transformation should not usually be considered since this tends to overweight low - complexity signals .",
    "we will demonstrate an example sliding - window algorithm which calculates conditional prefix complexities @xmath175 between a search texture , a , and elements b from a search space .",
    "the measure @xmath179 is closely related to the universal log - probability that a will occur in a sequence given that b has already been observed .",
    "its minimum is the most similar search element .",
    "it is invariant with respect to the size of elements in the search space and does not need to be normalized . estimating the complexities of signal components from the search space and the complexities of their concatenations with signal components from the texture , we arrive at an estimate of @xmath175 which may be effectively utilized in a wide variety of artificial intelligence applications .    the first - order critical point represents the level at which all the useful information is present in a signal .",
    "at this point , the transition from order to chaos is essentially complete . for the purposes of artificial intelligence ,",
    "we want enough redundancy to facilitate compression - based inference , but alto to retain enough specific detail to differentiate between objects .",
    "the second - order critical point targets the middle of the phase transition between order and chaos , where the transition is proceeding most rapidly and both of these objectives can be met . in the examples section",
    ", we will see that visual inference at the second - order critical depth outperforms inference at other depths . for this reason ,",
    "second - order critically compressed representations perform well in artificial intelligence applications .",
    "having described the method , we now apply it to some illustrative examples",
    ". we will start with trivial models and household data compression algorithms , proceeding to more sophisticated implementations that demonstrate the method s power and utility .",
    "these examples should be regarded as illustrations of a few of the many possible ways to implement and utilize two - part codes , rather than specific limitations of this method .",
    "one caveat to the application of complexity estimates is the fact that real - world data compression produces finite sizes for strings which contain no data , as a result of file headers , etc .",
    "we will treat these headers as part of the complexity of compressed representations when the objective of the compression is data compression , as they are needed to reconstruct the data . when calculating derivatives of complexity to determine critical points , this complexity constant does not affect the result , as the derivative of a constant is zero .",
    "it does not affect the conditional prefix complexity , either , canceled by taking a difference of complexities .    for certain applications in artificial intelligence , however",
    ", small fluctuations in complexity estimates can lead to large difference in estimates of probabilities .",
    "when the quality of complexity estimates are important , as is the case for inference problems , we will first compress empty data to determine the additive constant associated with the compression overhead , and this complexity constant is subtracted from each estimate of k(x ) .",
    "formally , the data compression algorithm is regarded as a computer and its overhead is absorbed into this computer s turing equivalence constant .",
    "this results in more useful estimates of complexity which improves our ability to resolve low - complexity objects under certain complexity measures .",
    "first - order critical data compression represents a level of detail at which a signal is essentially indistinguishable from an original when viewed at a macroscopic scale .",
    "we will show that second - order critical data compression produces representations which are typically slightly more lossy but significantly more compact .",
    "as mentioned earlier , any depth could potentially be used , splitting a two - part code subject to the constraints of the intended application .",
    "since images may be displayed in a paper more readily than video or audio , we first consider the second - order critical depth of a simple geometric image with superimposed noise .",
    "this is a 256x256 pixel grayscale image whose pixels have a bit depth of 8 .",
    "the signal consists of a 128x128 pixel square having intensity 15 which is centered on a background of intensity 239 . to this signal",
    "we add , pixel by pixel , a noise function whose intensity is one of 32 values uniformly sampled between -15 and + 16 .",
    "starting from this image , we take the n most significant bits of each pixel s amplitude to produce the images @xmath80 , where n runs from 0 to the bit depth , 8 .",
    "these images are visible in figure 1 , and the noise functions that have been truncated from these images are showcased in figure 2 .    to estimate @xmath180 , which is needed to evaluate critical depth , we will compress the signal @xmath80 using the fast and popular gzip compression algorithm and compress its residual noise function into the ubiquitous jpeg format .",
    "we will then progress to more accurate estimates using slower but more powerful compression algorithms , namely , paq8 and jpeg2000 .",
    "the results are tabulated below , with n on the left and the size of gzip s representation , the estimate of @xmath180 , in the right column .    [ cols=\"^,^ \" , ]     if we allow a @xmath181 tolerance factor within the maximum set , as before , we see that the first - order critical point is at depth 6 and the second order critical point is at depth 4 .",
    "we will see that inference at depth 4 outperforms inference at lower or higher bit depths , as predicted .",
    "a sliding window is applied to the image to produce string b , and the conditional prefix complexity @xmath179 is calculated .",
    "this is done for signals having bit depths of 1 through 8 . for visibility ,",
    "the resulting filter is applied to the image four times and the resulting image is normalized to @xmath182 intensity .",
    "the result follows .",
    "+   + depths 1 and 2 .",
    "+   +   + depths 3 and 4 .",
    "+   +   + depths 5 and 6 . +   +   + depths 7 and 8 .",
    "+    figures 24 - 31 : pattern search , depths 1 - 8 .",
    "depth 4 has fewer false positives than other depths as its greater economy translates into superior inference . at lower depths ,",
    "more false matches occur , since more of the image looks similar at this depth .",
    "the full image at depth 8 has strong false matches and inferior performance even though it contains all available information , giving too much weight to bits which do not contain useful information about the signal .",
    "this tends to overweight , for instance , short and possibly irrelevant literal matches .",
    "signals at depths 5 - 8 also exhibit this phenomenon to a lesser extent .",
    "a critical depth ( or other parameter , such as scale ) represents critical data in two senses of the word : on one hand , it measures the critical point of a phase transition between noise and smoothness , on the other , it also quantifies the essential information content of noisy data .",
    "such a point separates lossless signals from residual noise , which is compressed using lossy methods .",
    "the basic theory of using such critical points to compress numeric data has now been developed .",
    "this theory applies to arrays of any dimension , so it applies to audio , video , and images , as well as many other types of data .",
    "furthermore , we have demonstrated that this hybridization of lossless and lossy coding produces competitive compression performance for all types of image data tested . whereas lossy transformation standards such as jpeg2000 sometimes include options for separate lossless coding modes , a two - part code adapts to the data and smoothly transitions between the two types of codes . in this way",
    "two - part codes are somewhat unique in being efficient for compressing both low - entropy and high - entropy sources .",
    "the optional integration of maximum likelihood models and monte - carlo - type sampling is a significant departure from deterministic algorithms for data compression and decompression .",
    "if sampling is employed , the decompression algorithm becomes stochastic and non - deterministic , potentially producing a different result each time decompression occurs .",
    "the integration of statistical modeling into such an algorithm enables two - part codes which are engineered for specific applications .",
    "this can lead to much higher levels of application - specific compression than can be achieved using general - purpose compression , as has been illustrated using a simple image corrupted by noise .",
    "the test images presented use a bit depth of 8 bits per channel , as is standard in most of today s consumer display technology .",
    "however , having more bits per sample ( as in the proposed hdr image standard , for instance ) means that the most significant bits represent a smaller fraction of the total data . as such , the utility of a two - part code is increased at the higher bit depth , since more of the less - significant bits can be highly compressed using a lossy code , while the significant bits still use lossless compression .",
    "likewise , high - contrast applications will benefit from the edge - preserving nature of a two - part code .",
    "frequency - based methods suffer from the gibbs phenomenon , or ringing, which tends to blur high - contrast edges . in the approach",
    "described , this phenomenon is mitigated by limited use of such methods .",
    "a two - part code should perform well in applications in which the fidelity of high - contrast regions is important .",
    "as suggested previously , a two - part code can significantly outperform lossy transforms by many orders of magnitude for computer - generated artwork , cartoons , and most types of animation .",
    "the low algorithmic complexity intrinsic to such sources leads to efficiently coded signals .    in most test cases",
    ", critical compression also outperforms jpeg2000 by orders of magnitude for black - and - white images . without the advantage of separating color data into chroma subspaces ,",
    "the jpeg algorithms seem much less efficient . for this reason ,",
    "two - part codes seem to outperform jpeg coding in many monochrome applications .",
    "jpeg2000 does perform well for its intended purpose - generating a highly compressed representation of color photographs . for most of the color test photographs ,",
    "a two - part code overtakes jpeg2000 at high quality levels .",
    "the point at which this occurs ( if it does ) varies by photograph . at relatively low bitrates ,",
    "jpeg2000 usually outperforms a two - part code , but usually by less than an order of magnitude .",
    "all examples presented to this point were directly coded in the rgb color space . since the theory of two - part codes applies to any array of n - bit integers , we could have just as easily performed analysis in the @xmath183 color space , like the jpeg algorithms , which often improves the redundancy apparent in color data . in the second part of the appendix ,",
    "@xmath184-space critical compression will be compared to @xmath183-space critical compression for color photographs .",
    "one unique aspect of two - part data compression is its ability to code efficiently over a wide variety of data .",
    "it can efficiently code both algorithmically generated regular signals and stochastic signals from empirical data .",
    "the former tends to be periodic , and the random aspects of the latter tend to exhibit varying degrees of quasiperiodicity or chaos .",
    "however , the creation of periodicity or redundancy is essential to the comparison operation - the prefix complexity involves concatenation , which becomes similar to repetition if the concatenated objects are similar .",
    "concatenation can create periodicity from similarities , even if the objects being concatenated have no significant periodicity within themselves , as may be the case with data altered by noise .",
    "the inferential power of critical compression derives from its ability to compress periodicity which would otherwise be obscured by noise .    in spite of its ultimately incalculable theoretical underpinnings",
    ", the human eye intuitively recognizes a critical bit depth from a set of truncated images .",
    "the mind s eye intuitively recognizes the difference between noisy , photographic , `` real world '' signals and smooth , cartoon - like , artificial ones .",
    "human visual intelligence can also identify the effective depth from the noisy bits - it is the depth beyond which features of the original image can be discerned in the noise function .",
    "conversely , given a computer to calculate the critical point of an image , we can determine its critical information content .",
    "since noise ca nt be coded efficiently due to its entropy , an effective learner , human or otherwise , will tend to preferentially encode the critical content .",
    "this leads directly to more robust artificial intelligence systems which encode complex signals in a manner more appropriate for learning .",
    "this work was funded entirely by the author , who would like to acknowledge his sister , elizabeth scoville , and his parents , john and lawana scoville .",
    "a patent related to this work is pending .",
    "10    t.  acharya and p.  tsai , _",
    "jpeg2000 standard for image compression _ , wiley , hoboken , nj , 2005 .",
    "chaitin , _ algorithmic information theory _ , cambridge university press , 1987 .",
    "cover , _ elements of information theory _ , wiley , 1991 .",
    "fisher , _ on the mathematical foundations of theoretical statistics _ , phil .",
    "trans . of the royal society of london * series a 222 * ( 1921 ) , 309368 .",
    "m.  gell - mann and s.  lloyd , _ information measures , effective complexity , and total information _ , complexity * 2/1 * ( 1996 ) , 4452 .",
    "r.  v.  l. hartley , _ transmission of information _ , bell system technical journal ( july 1928 ) , 535 ?",
    "j.  jackson , _ classical electrodynamics _",
    ", third ed . , john wiley and sons , new york , 1999 .",
    "d.  knuth , _ the art of computer programming , vol 2 . _ , addison - wesley , reading , ma , 1998 .",
    "m.  li and p.  vitnyi , _ an introduction to kolmolgorov complexity and its applications _ , second ed .",
    ", springer - verlag , new york , 1997 .",
    "g.  marsaglia and w.w .",
    "tsang , _ the ziggurat method for generating random variables _ , journal of statistical software * 5 * ( 2000 ) .",
    "h.  nyquist , _ certain topics in telegraph transmission theory _ , trans .",
    "aiee * 47 * ( 1928 ) , 617644 .",
    "j.  scoville , _ on macroscopic complexity and perceptual coding _ , arxiv:1005.1684 .",
    "shannon , _ the mathematical theory of communication .",
    "_ , bell labs tech .",
    "j. * 27 * ( 1948 ) , 379423,623656 .",
    "to3em , _ communication in the presence of noise _ , proc .",
    "institute of radio engineers * 37 * ( 1949 ) , 1021 .    w.h .",
    "zurek , _ algorithmic randomness and physical entropy .",
    "_ , physical review , ser . a * 40(8 ) * ( 1989 ) , 47314751 .",
    "in the following plots , each solid line represents two - part codes having various lossy bitrates at a particular signal depth .",
    "the dotted lines show the error level at various bitrates of jpeg2000 coding , these are also two - part codes at signal depth zero .      the image repository maintained by the university of waterloo s fractal coding and analysis group contains 32 test images .",
    "the collection includes a wide variety of content , with photographic and computer generated content in both color and black and white .",
    "two part coding dominates direct lossy image coding for the majority of these images , demonstrating the power and versatility of critical data compression using two - part codes .",
    "the images which perform better with direct lossy coding are generally color photographs , with jpeg2000 having its greatest advantage at low quality levels .",
    "two - part codes seem to have an advantage for the other images , sometimes by multiple orders of magnitude .",
    "the method described was applied to 24 uncompressed 24-bit photographic images from a sample kodak photo cd .",
    "we compare critical compression at various bitdepths in the @xmath184 color space using paq8l and jpeg2000 , as before , against a ycbcr - space encoding which critically compresses a luma ( @xmath63 ) channel at various bitdepths using paq8l and the chroma channels ( @xmath185 and @xmath186 ) using jpeg2000 . for this transformation , the chroma parameters @xmath187 and",
    "@xmath188 are both equal to @xmath189 , making the @xmath63 channel a simple average of the corresponding red , green , and blue color values . the results ( with @xmath184 above and",
    "@xmath190 below ) show that while jpeg2000 retains the advantage at low to moderate quality levels , the critical luma / lossy chroma @xmath190 scheme is usually more efficient at moderate to high quality levels than direct jpeg2000 coding or critical compression in @xmath184 ."
  ],
  "abstract_text": [
    "<S> a new approach to data compression is developed and applied to multimedia content . </S>",
    "<S> this method separates messages into components suitable for both lossless coding and lossy or statistical coding techniques , compressing complex objects by separately encoding signals and noise . </S>",
    "<S> this is demonstrated by compressing the most significant bits of data exactly , since they are typically redundant and compressible , and either fitting a maximally likely noise function to the residual bits or compressing them using lossy methods . upon decompression , </S>",
    "<S> the significant bits are decoded and added to a noise function , whether sampled from a noise model or decompressed from a lossy code . </S>",
    "<S> this results in compressed data similar to the original . </S>",
    "<S> signals may be separated from noisy bits by considering derivatives of complexity in a manner akin to kolmogorov s approach or by empirical testing . </S>",
    "<S> the critical point separating the two represents the level beyond which compression using exact methods becomes impractical . since redundant signals are compressed and stored efficiently using lossless codes , while noise is incompressible and practically indistinguishable from similar noise , such a scheme can enable high levels of compression for a wide variety of data while retaining the statistical properties of the original . for many test images , a two - part image code using jpeg2000 for lossy compression and paq8l for lossless coding produces less mean - squared error than an equal length of jpeg2000 . for highly regular images , </S>",
    "<S> the advantage of such a scheme can be tremendous . </S>",
    "<S> computer - generated images typically compress better using this method than through direct lossy coding , as do many black and white photographs and most color photographs at sufficiently high quality levels . </S>",
    "<S> examples applying the method to audio and video coding are also demonstrated . </S>",
    "<S> since two - part codes are efficient for both periodic and chaotic data , concatenations of roughly similar objects may be encoded efficiently , which leads to improved inference . </S>",
    "<S> such codes enable complexity - based inference in data for which lossless coding performs poorly , enabling a simple but powerful minimal - description based approach audio , visual , and abstract pattern recognition . </S>",
    "<S> applications to artificial intelligence are demonstrated , showing that signals using an economical lossless code have a critical level of redundancy which leads to better description - based inference than signals which encode either insufficient data or too much detail . </S>"
  ]
}