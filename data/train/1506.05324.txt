{
  "article_text": [
    "the recovery of sparse signals of high dimensions on the basis of noisy linear measurements is an important problem in the field of signal acquisition and processing .",
    "when the number of linear observations is significantly lower than the dimension of the signal to be recovered , the signal recovery may exploit the property of sparsity to deliver correct results .",
    "the field of research that studies such problems is often referred to as _ compressed sensing _ or _ compressive sensing _ ( cs ) @xcite .",
    "+ several computationally tractable methods to address cs problems have been developed in the last two decades @xcite . among them , greedy methods prove to be valuable choices as their complexity is significantly lower than that of algorithms based on @xmath0-minimization @xcite . + while many cs problems involve only one sparse signal and the corresponding _ measurement vector _ , _",
    "i.e. _ , the vector gathering all the linear observations of this signal , some applications either require or at least benefit from the presence of several sparse signals and measurement vectors .",
    "examples of such applications are available in section  [ subsec : applications ] .",
    "models involving one measurement vector are referred to as single measurement vector ( smv ) models while multiple measurement vector ( mmv ) models involve at least two measurement vectors @xcite .",
    "+ when the supports of the sparse signals are similar , it is possible to improve the reliability of the recovery by making joint decisions to determine the estimated support @xcite .",
    "thereby , all the measurement vectors intervene in the estimation of the support and the final support is common to all the sparse vectors .",
    "algorithms performing joint recovery are also capable to weaken the influence of additive measurement noise on the performance provided that the noise signals are statistically independent and exhibit some degree of isotropy . + orthogonal matching pursuit ( omp ) is one of the most extensively used greedy algorithm designed to solve smv problems @xcite . among several greedy algorithms",
    "conceived to deal with multiple measurement vectors , the extension of omp to the mmv paradigm , referred to as simultaneous orthogonal matching pursuit ( somp ) , is of great interest as it remains simple , both conceptually and algorithmically @xcite .",
    "the classical somp algorithm does not account for the possibly different measurement vector noise levels . in some sense",
    ", all the measurement vectors are considered equally worthy .",
    "however , it is clear that an optimal joint support recovery method should necessarily take into account the noise levels by accordingly weighting the impact of each measurement vector on the decisions that are taken .",
    "the first aim of this paper is to extend somp by gifting it with weighting capabilities .",
    "the new algorithm will be referred to as somp with noise stabilization ( somp - ns ) and basically extends the decision metric of somp to weight the impact of each measurement vector onto the decisions that are taken .",
    "+ the second objective is to provide theoretical and numerical evidence that the proposed algorithm indeed enables one to achieve higher performance than the other greedy alternatives when the noise levels , or more generally the signal - to - noise ratios ( snr ) , vary from one measurement vector to another .",
    "we study partial and full support recovery guarantees of somp - ns for a mmv signal model incorporating arbitrary sparse signals to be recovered and statistically independent additive gaussian noise vectors exhibiting diagonal covariance matrices , _",
    "i.e. _ , the entries within each vector are statistically independent . it is assumed that the variances of the entries within each noise vector are identical although they may be different for each measurement vector . the signal model is thoroughly detailed in section  [ subsec : signalmodel ] .",
    "+ our first contribution is the proposal of somp - ns which generalizes somp by weighting the measurement vectors .",
    "the second contribution is a novel theoretical analysis of somp and somp - ns in the presence of additive gaussian noise on the measurements . to the best of the authors knowledge",
    ", the theoretical analysis in this paper has never been proposed , neither for somp nor for somp - ns .",
    "+ finally , numerical simulations will show that the weighting capabilities of somp - ns enable one to improve the performance with regards to somp when the noise vectors exhibit different powers .",
    "the numerical results will also provide evidence that the theoretical analysis accurately depicts key characteristics of somp - ns .",
    "in particular , closed - form formulas for the optimal weights will be derived from the theoretical analysis and will be compared to the simulation results .",
    "several authors have worked on similar problems .",
    "the study of full support recovery guarantees for omp with @xmath1 or @xmath2-bounded noises as well as with gaussian noises has been performed in @xcite .",
    "the authors of @xcite also provided conditions on the stopping criterion to ensure that omp stops after having picked all the correct atoms .",
    "+ our analysis is similar to that performed by tropp in @xcite for convex programming methods in a smv setting .",
    "together with gilbert @xcite , they analyzed the probability of full support recovery by means of omp for gaussian measurement matrices in the noiseless case .",
    "their result has subsequently been refined by fletcher and rangan in @xcite to account for additive measurement noise by means of a high - snr analysis , _",
    "i.e. _ , it is assumed that the signal - to - noise ratio scales to infinity .",
    "all of the papers discussed so far only focus on the smv framework .",
    "+ the theoretical analysis of our paper is partially inspired from @xcite and has been generalized to the mmv framework .",
    "it is worth pointing out that our analysis does not require the high snr assumption of @xcite , properly captures the benefits provided by multiple measurement vectors but nevertheless presents some inaccuracies that are to be discussed at the end of this paper .",
    "+ gribonval _ et al . _ have performed an analysis of somp for a problem similar to ours in @xcite .",
    "they were interested in providing a lower bound on the probability of correct support recovery when the signal to be estimated is sparse and its non - zero entries are statistically independent mean - zero gaussian random variables exhibiting possibly different variances . +",
    "while our statistical analysis considers the additive measurement noise as a random variable and the sparse signals to be recovered as deterministic quantities , the results obtained in @xcite use the opposite approach , _",
    "i.e. _ , the sparse signals are random and the noise is deterministic .",
    "thus , the problem addressed in our paper differs from that presented in @xcite but both papers use similar mathematical tools and the criteria to ensure full support recovery with high probability share analogous properties .",
    "this last remark will be further discussed in section  [ subsec : gribonvalrelatedthm ] .",
    "first of all , section  [ sec : sigmodel ] progressively introduces the context , provides a detailed description of the signal model and depicts an associated application .",
    "afterwards , section  [ sec : sompandsompns ] provides descriptions of somp and somp - ns . + before deriving the theoretical analysis , section  [ sec : background ] introduces the mathematical tools necessary for its execution .",
    "section  [ sec : section3 ] then provides general theoretical results on the proper recovery of sparse vectors by means of somp - ns . on the basis of the results from section  [ sec : section3 ] ,",
    "we show in section  [ sec : recovguarantees ] that , for gaussian noises , the probability of failure of somp - ns decreases exponentially with regards to the number of measurement vectors . + in section  [ sec :",
    "numresults ] , extensive numerical simulations show that adequate weighting strategies enable somp - ns to outperform somp whenever the noise variances for each measurement vector are different . also , a closed - form weighting strategy is derived from the theoretical analysis of the previous sections and these weights are compared to the optimal ones obtained by simulation .",
    "finally , the simulation results show which aspects of the behavior of somp - ns are properly captured by the proposed theoretical analysis .",
    "the reasons why our analysis fails to capture some characteristics of somp are discussed and potential workarounds are proposed for investigation .",
    "we find preferable to introduce here the common notations used in this paper .",
    "first of all , @xmath3 : = \\left\\lbrace 1 , 2 , \\dots , n \\right\\rbrace$ ] . for any set @xmath4",
    ", @xmath5 refers to its cardinality .",
    "matrices are denoted by upper case bold letters while vectors are written in lower case bold letters .",
    "the tranpose operation is denoted by the superscript @xmath6 . for @xmath7",
    ", @xmath8 denotes the @xmath9-th component of @xmath10 .",
    "similarly , the @xmath11-th column vector of any matrix is denoted by the corresponding lower case bold letter with subscript @xmath11 , _",
    "e.g. _ , the @xmath11-th column of @xmath12 is @xmath13 .",
    "the @xmath14-th entry of a matrix is denoted by the upper case letter with subscript @xmath15 , _",
    "e.g. _ , the @xmath14-th entry of @xmath12 is written @xmath16 . the @xmath17 norm ( @xmath18 ) of @xmath19",
    "is defined as @xmath20 .",
    "moreover , the @xmath2 norm of vector @xmath19 is defined as @xmath21 } |\\phi_i|$ ] . for any matrix @xmath22",
    ", @xmath23 denotes the space spanned by its column vectors .",
    "also , the trace of @xmath12 is @xmath24 and the frobenius norm of @xmath12 is denoted by @xmath25 . for @xmath26 , @xmath27 denotes the submatrix of @xmath12 that comprises its columns indexed by @xmath28 .",
    "likewise , @xmath29 is the subvector of @xmath10 comprising only the components indexed within @xmath28 .",
    "the moore - penrose pseudoinverse of any matrix @xmath12 is denoted by @xmath30 .",
    "the orthogonal complement of a vector subspace @xmath4 is given by @xmath31 .",
    "for any random variable @xmath32 , its cumulative density function ( cdf ) is denoted by @xmath33 while its probability density function ( pdf ) is written @xmath34 ( when it exists ) .",
    "similarly , the joint cdf and pdf of the random variables @xmath35 are written as @xmath36 and @xmath37 respectively .",
    "the probability measure is given by @xmath38 while the mathematical expectation is denoted by @xmath39 .",
    "the statistical independence symbol is written @xmath40 .",
    "we define the support of a vector @xmath10 as @xmath41 .",
    "the @xmath42 `` norm '' of a vector @xmath10 is defined as @xmath43 . loosely speaking",
    ", we say that @xmath7 is sparse whenever @xmath44 .",
    "moreover , @xmath10 is said to be @xmath45-sparse whenever @xmath46 .",
    "+ let us consider a collection of @xmath47 signals @xmath48 ( @xmath49 ) that are sparse in an orthonormal basis , _",
    "i.e. _ , @xmath50 where @xmath51 represents the orthonormal basis and @xmath52 is the sparse coefficient vector of @xmath53 expressed in @xmath54 .",
    "+ we now consider a unique linear measurement process to recover each one of the @xmath47 sparse coefficient vector @xmath55 .",
    "moreover , the measurement process is assumed to deliver a number of observations @xmath56 significantly lower than @xmath57 .",
    "additive measurement noise vectors @xmath58 are also accounted for .",
    "formally , the latter statements rewrite @xmath59 where the _ measurement matrix _",
    "@xmath22 denotes the linear measurement process and the _ measurement vectors _ @xmath60 gather the observations .",
    "+ since the number of observations @xmath56 is lower than @xmath57 , arbitrary vectors @xmath55 can not be recovered from @xmath61 , even in the noiseless case . however , in the noiseless case , it has been shown @xcite that @xmath55 can be recovered provided that it is sparse enough , _",
    "i.e. _ , the cardinality of its support is below a certain threshold , and that the measurement matrix @xmath12 exhibits specific properties such as the restricted isometry property that is described afterwards .",
    "+ the orthonormal basis @xmath54 is often assumed to be the canonical basis .",
    "the reason that explains this simplification relies on the fact the the measurement matrix @xmath12 is usually generated as a realization of a subgaussian random matrix .",
    "it can be shown that such random matrices are well - condtionned for sparse support recovery , even when multiplied by orthonormal matrices , _",
    "i.e. _ , @xmath62 remains a subgaussian random matrix .",
    "this phenomenon is more thoroughly discussed in ( * ? ? ?",
    "* section 9.1 ) .",
    "thereby , the signal model will be simplified to @xmath63    if several sparse signals @xmath55 share similar supports , then it is interesting to simultaneously recover their joint support @xmath64 instead of performing @xmath47 independent and possibly different estimations of the support of every vector @xmath55 .",
    "the reason that explains why such a strategy yields performance improvements is twofold :    1 .",
    "it has been shown @xcite that when the sparse vectors @xmath55 share a similar support whose associated entries are highly variable from one vector to another , then the probability of correct support recovery increases with the number of available measurement vectors @xmath47 .",
    "2 .   in the noisy setting , the vectors @xmath65 are often statistically independent and isotropic which suggests that a joint support recovery procedure could be capable of filtering them",
    ". this property is part of what the theoretical results and simulations of this paper establish .",
    "once the joint support has been recovered , @xmath55 is easily recovered by solving a least squares problem involving only the non - zero entries of @xmath55 and the associated column vectors from @xmath12 .",
    "we wish to provide here a formal and precise statement of the signal model to be used in the rest of this paper . +",
    "as mentioned previously , we consider @xmath47 measurement vectors @xmath61 that are generated on the basis of @xmath47 sparse signals @xmath55 whose supports are similar .",
    "we consider the following mmv model :    @xmath66    where @xmath67 is composed of the @xmath47 measurement vectors @xmath68 .",
    "@xmath69 comprises the @xmath47 sparse signals @xmath70 .",
    "finally , the columns of @xmath71 correspond to measurement errors .",
    "each error vector @xmath65 is distributed as @xmath72 and all of them are statistically independent .",
    "the purpose of ( [ eq : mmvsignalmodel ] ) is to aggregate the @xmath47 equations defined in ( [ eq : introsigmodel2 ] ) into a single relation .",
    "this representation will be preferred throughout the rest of this paper . + before going further , we will point out that the mathematical problem of the joint support recovery is equivalent to finding the columns of @xmath12 that enable one to fully express @xmath73 .",
    "thereby , @xmath12 may be seen as a _",
    "dictionary matrix _ whose columns @xmath74 are the _ atoms _ of the associated dictionary .",
    "the problem of joint support recovery then boils down to determining which atoms to choose to simultaneously express the @xmath47 measurement vectors as their linear combinations .",
    "although viewing @xmath12 as a measurement process is well suited to the description of typical applications , the dictionary approach is more appropriate for the sake of presenting the mathematical results and will thus be adopted for the rest of this paper .",
    "however , the term _ measurement vector _ will not be replaced to stay consistent with the standard mmv terminology .",
    "+ the dictionary matrix @xmath12 is assumed to satisfy the restricted isometry property of an order equal to or higher than the cardinality of the joint support @xmath75 .",
    "moreover , it will be assumed that each column of this matrix exhibits a unit @xmath1 norm .",
    "we briefly review two procedures to obtain a dictionary matrix that satisfies both properties above with high probability :    1 .",
    "generate a random gaussian matrix and then normalize the @xmath1 norm of its columns .",
    "in such a way , the atoms are uniformly distributed on the unit hypersphere @xmath76 of dimension @xmath57 .",
    "it is possible to show that this class of random matrices satisfies the restricted isometry property with high probability ( see @xcite for example ) .",
    "2 .   generate a matrix whose entries are statistically independent rademacher random variables .",
    "each column of the resulting matrix is then normalized by multiplying the matrix by @xmath77 to obtain atoms exhibiting unit @xmath1 norms .",
    "the typical scenario associated with signal models ( [ eq : introsigmodel2 ] ) and ( [ eq : mmvsignalmodel ] ) is depicted in figure  [ fig : firstscenario ] .",
    "the idea is to observe a physical quantity , _",
    "e.g. _ , a chemical composition , a wireless signal , etc . at different locations and/or time instants by means of @xmath47",
    "_ sensing nodes _ @xmath78 whose only purpose is to acquire observations , _",
    "i.e. _ , measurement vectors , and repatriate them to a central node ( cn ) that will simultaneously process all the data .",
    "in such a configuration , the sensing nodes are generally cheap and exhibit very limited computational capabilities and power consumption while the central node is more costly because it has to deliver higher performance",
    ". +     nodes with different noise levels generate @xmath47 measurement vectors on which joint estimation of the sparse support @xmath79 can be performed ]    although several applications of the signal model ( [ eq : introsigmodel2 ] ) are presented in ( * ? ? ?",
    "* section 3.3 ) , we propose to take as an example the spectrum sensing problem .",
    "spectrum sensing aims at scanning multi - gigahertz electromagnetic ( em ) spectrums at a rate that is below that of nyquist .",
    "the reason that motivates this objective is that , although most of the available frequency bands are licensed to specific users and thus costly to acquire , it has been observed that the spectrum occupancy is limited , _",
    "i.e. _ , the spectrum is sparse in the frequency domain at a given time instant and location .",
    "therefore , it would be interesting to observe this spectrum through an appropriate linear measurement process , as described in equation  ( [ eq : introsigmodel2 ] ) , and then use algorithms tailored for cs problems to determine , in real - time , which frequency bands are free and can be used to transmit information .",
    "+ in this application , each entry of the sparse signals @xmath55 represents the power of a given frequency band .",
    "since most of the spectrum is assumed to be unused at a given time instant and location , the vectors @xmath55 are expected to be sparse . although the nodes should ideally be exposed to the same spectrum , this is not the case in practice because of the rayleigh fading that strongly attenuates some frequency bands , thus being invisible to some nodes . in multiple input multiple output ( mimo )",
    "wireless communications , this issue is circumvented by placing the receivers sufficiently far away from one another so that the fading becomes statistically independent for each node and thus highly unlikely to strongly attenuate the same frequency band for every node . in a likewise fashion",
    ", the same solution will work for our framework since occasional `` holes '' will reveal to be a nonissue when performing joint decisions .",
    "+ finally , the different nodes may exhibit different noise levels because of discrepancies in the fabrication process or because the hardware components ( _ e.g. _ , amplifiers , multipliers , filters ) of each node are different .",
    "this last remark justifies the multiple noise variances hypothesis of this paper .",
    "the two next subsections present two methods for addressing the problem of joint support recovery envisioned in section  [ subsec : signalmodel ] .",
    "the first method , somp , is standard in the literature but does not include noise stabilization .",
    "the second method , our contribution , generalizes the first one by multiplying each measurement vector @xmath61 ( @xmath49 ) by a weight @xmath80 .",
    "the original omp algorithm @xcite has been generalized in several ways to deal with matrix signals @xmath81 , _ i.e. _ , mmv problems",
    ". simultaneous orthogonal matching pursuit ( somp ) is a possible generalization of omp @xcite .",
    "+ somp is a greedy algorithm that provides approximate solutions to the joint support recovery problem by successively picking atoms from @xmath12 to simultaneously approximate the @xmath47 measurement vectors @xmath82 .",
    "somp is described in algorithm [ alg : somp ] .",
    "+    algorithm [ alg : somp ] : + simulatenous orthogonal matching pursuit ( somp ) +    @xmath81 , @xmath22 , @xmath83 initialization : @xmath84 and @xmath85 @xmath86 determine the atom of @xmath12 to be included in the support : + @xmath87 update the support : @xmath88 projection of each measurement vector onto @xmath89 : + @xmath90 projection of each measurement vector onto @xmath91  : + @xmath92 @xmath93 @xmath94    we now explain how somp proceeds .",
    "the residual at iteration @xmath95 , denoted by @xmath96 , consists of the projection of each one of the original signals @xmath61 onto the orthogonal complement of @xmath97 .",
    "in such a way , the residual is orthogonal to every atom that has been chosen so far .",
    "initially , the residual is chosen equal to the original signal . the decision on which atom to choose ( step 4 ) is based on the sum of the inner products of every atom @xmath98 with each residual measurement vector @xmath99 ( where @xmath99 refers to the @xmath100-th column of @xmath96 ) since @xmath101    the index of the atom maximizing the @xmath0 norm is included in the support ( step 5 ) .",
    "then , the original signal @xmath102 is projected onto the orthogonal complement of @xmath89 ( steps 6 and 7 ) .",
    "+ in this setting , somp stops after exactly @xmath45 iterations . however , it is worth mentioning that the stopping criterion usually comprises a criterion based on the number of iterations as well as another one relying on the norm of the residual , _",
    "i.e. _ , if the norm of the residual is below a certain threshold , then omp stops .",
    "different norms can be used for the second criterion but these considerations will not be further discussed in this paper .",
    "the interested reader can consult @xcite for related matters .",
    "+ furthermore , maximizing the @xmath0 norm in step @xmath103 is not the unique choice .",
    "other authors have investigated different norms , _",
    "e.g. _ , the @xmath1 and @xmath2 norms .",
    "nevertheless , some numerical simulations reveal that the choice of the norm has very little effect on the performance ( see ( * ? ? ?",
    "* figure 3 ) ) .",
    "we now present the development of a noise stabilization strategy to be used in conjunction with somp that has low computational requirements .",
    "the equivalent new algorithm is referred to as somp - ns where ns stands for _ noise stabilization_. algorithm  [ alg : sompns ] describes the first form of somp - ns . +",
    "this novel algorithm is a generalization of somp that weights the impact of each measurement vector within matrix @xmath81 on the decisions performed at each iteration .",
    "+    algorithm [ alg : sompns ] : + somp with noise stabilization ( somp - ns )  first form +    @xmath81 , @xmath22 , @xmath104 , @xmath83 @xmath84 and @xmath85 @xmath86 determine the atom of @xmath12 to be included in the support : + @xmath105 @xmath88 projection of each measurement vector onto @xmath89 : + @xmath90 projection of each measurement vector onto @xmath91 : + @xmath92 @xmath93 @xmath94    somp - ns is actually very close to somp .",
    "both algorithms decide on which atom to pick on the basis of a sum of absolute values of inner products , each term in the sum depending only on one measurement vector .",
    "somp gives the same importance to each measurement vector whereas its weighted counterpart introduces weights @xmath80 ( @xmath49 ) so as to give more or less importance to each measurement vector .",
    "+ a second form of somp - ns that is more computationally efficient is available in algorithm [ alg : sompnsv2 ] . in the second form , * somp*(@xmath106 )",
    "refers to the regular somp algorithm described in algorithm [ alg : somp ] .",
    "algorithm [ alg : sompnsv2 ] : + somp with noise stabilization ( somp - ns )  second form +    @xmath81 , @xmath22 , @xmath104 , @xmath83 @xmath107 the columns of @xmath102 are weighted beforehand : @xmath108 apply the regular somp algorithm : @xmath109 * somp*(@xmath106 ) @xmath28    both forms are equivalent since @xmath110 and @xmath111 .",
    "the last equality holds true because the orthogonal projector @xmath112 is applied to each column of @xmath113 separately .",
    "although the residual matrix @xmath96 is different for the two forms of somp - ns , the difference only consists in a multiplicative term @xmath114 for each column @xmath100 of @xmath96 and it does not modify the atoms added to the estimated support .",
    "this section briefly explains the mathematical tools needed to conduct the theoretical analysis .",
    "let @xmath22 be a matrix composed of @xmath57 column vectors @xmath115 .",
    "moreover , @xmath116 $ ] , @xmath117 . the coherence @xmath118 of @xmath12 is defined as    @xmath119    similarly , one can define the cumulative coherence function ( also referred to as the babel function ) of @xmath12 as @xmath120 trivially , @xmath121 . for the sake of brevity , @xmath118 and @xmath122",
    "will be respectively denoted as @xmath123 and @xmath124 from now on . + it is also frequent to use the bound @xmath125      if @xmath126 where @xmath127 ( @xmath128 ) , then @xmath129 }   { \\text{supp}}({\\boldsymbol{u}}_j)$ ] .",
    "the definition above extends the notion of support to matrices , _",
    "i.e. _ , the support of a matrix is the union of the supports of its columns .",
    "similarly to the vector case , if @xmath130 , then @xmath131 } { \\text{supp}}({\\boldsymbol{u}}_j ) \\right|$ ] .",
    "+ we define @xmath132 } |\\phi_i|$ ] which is not a norm .",
    "furthermore , for @xmath22 , @xmath133 is defined as ( * ? ? ?",
    "* equation a.8 ) @xmath134 where @xmath135 . for the sake of simplifying the notations",
    ", we will adopt the convention @xmath136 .",
    "it can be shown that , for @xmath22 , ( * ? ? ?",
    "* lemma a.5 ) @xmath137 } \\sum_{j=1}^{n } \\left| \\phi_{i , j } \\right| = \\|{\\boldsymbol{\\phi}}^{\\mathrm{t } } \\|_{1}.   \\ ] ]      the sparse rank @xcite of @xmath22 , denoted by @xmath138 , is given by @xmath139 @xmath138 is thus the smallest number of linearly dependent columns of @xmath12 .",
    "equivalently , it means that , if @xmath140 , then , for every support @xmath28 such that @xmath141 , the columns of @xmath27 are linearly independent , _",
    "i.e. _ , @xmath27 has full column rank .",
    "+ note that computing @xmath138 for a given matrix @xmath12 is not computationally tractable as this problem is even harder to solve than a @xmath42 norm minimization problem which is known to be np - hard @xcite .",
    "the matrix @xmath22 satisfies the so - called restricted isometry property ( rip ) @xcite of order @xmath45 if there exists a constant @xmath142 such that @xmath143 for all @xmath45-sparse vectors @xmath144 .",
    "the smallest @xmath145 that satisfies equation  ( [ eq : ripdef ] ) is called the rectricted isometry constant ( ric ) of order @xmath45 .",
    "the ric of order @xmath45 can theoretically be computed by considering @xmath146 \\\\",
    "|s| = s } } \\lambda_{\\mathrm{max}}({\\boldsymbol{\\phi}}_{s}^{\\mathrm{t } } { \\boldsymbol{\\phi}}_{s } ) - 1 \\\\",
    "l_s = 1 - \\min_{\\substack{s \\subseteq \\left[n \\right ] \\\\",
    "|s| = s } } \\lambda_{\\mathrm{min}}({\\boldsymbol{\\phi}}_{s}^{\\mathrm{t } } { \\boldsymbol{\\phi}}_{s})\\end{aligned}\\ ] ] where @xmath147 and @xmath148 denote the smallest and largest eigenvalues respectively . then",
    ", @xmath149    evaluating @xmath150 and @xmath151 is not computationally tractable as it requires to determine the smallest and largest eigenvalues of @xmath152 matrices of size @xmath153 .",
    "in particular , this problem has been shown to be np - hard in the general case @xcite .",
    "it is therefore interesting to find an upper bound on @xmath145 that can be easily computed .",
    "[ lem : riclambda ] if @xmath154 , then @xmath155 \\\\ |s| = s } } \\lambda_{\\mathrm{max}}({\\boldsymbol{\\phi}}_{s}^{\\mathrm{t } } { \\boldsymbol{\\phi}}_{s } ) \\leq 1 + \\mu_1(s-1 ) \\leq 1 + ( s-1 ) \\mu\\\\",
    "\\min_{\\substack{s \\subseteq \\left[m \\right ] \\\\",
    "|s| = s } } \\lambda_{\\mathrm{min}}({\\boldsymbol{\\phi}}_{s}^{\\mathrm{t } } { \\boldsymbol{\\phi}}_{s } ) \\geq 1 - \\mu_1(s-1 ) \\geq 1 - ( s-1 ) \\mu.\\end{aligned}\\ ] ] the first inequality of each line holds if @xmath156 .",
    "this result is obtained in @xcite .",
    "a consequence of lemma [ lem : riclambda ] is that , if @xmath154 , then @xmath157 it is worth noticing that if @xmath142 , then @xmath140 .",
    "the reason is that @xmath142 implies that @xmath158 , |s| = s } \\lambda_{\\mathrm{min}}({\\boldsymbol{\\phi}}_{s}^{\\mathrm{t } } { \\boldsymbol{\\phi}}_{s } ) > 0 $ ] which in turn implies that , for every support @xmath28 of cardinality equal to or less than @xmath45 , @xmath159 has full column rank .",
    "a lipschitz function @xmath160 with regards to metric @xmath1 is a function that satisfies @xmath161 the constant @xmath162 is called the lipschitz constant .",
    "we now wish to derive basic theoretical results needed to analyze the performance of somp - ns in both the noiseless and noisy cases .",
    "the theoretical framework developed hereafter will be used in section  [ sec : recovguarantees ] to give lower bounds on the probability of partial or full recovery using somp - ns when the additive noise is gaussian .",
    "we now wish to build an exact recovery criterion ( erc ) that guarantees , for dictionary matrix @xmath12 , the recovery of every sparse signal of support @xmath28 by means of somp - ns .",
    "the result that will be obtained is similar to an earlier result of j.  a.  tropp , sometimes referred to as tropp s erc @xcite .",
    "+ first of all , it is worth noticing that the maximum correlation in step 4 of algorithm [ alg : sompns ] can be written as @xmath163 } \\left ( \\sum_{k=1}^{k } \\left| \\left\\langle { \\boldsymbol{r}}^{(t)}_k , { \\boldsymbol{\\phi}}_{j } \\right\\rangle   \\right| q_k \\right ) = \\left\\|{\\boldsymbol{\\phi}}^{\\mathrm{t } } { \\boldsymbol{r}}^{(t ) } { \\boldsymbol{q } } \\right\\|_{\\infty}\\ ] ] where @xmath164 is the diagonal matrix that contains the weights to be applied to each column vector of @xmath96 .",
    "it is assumed that @xmath80 ( @xmath49 ) .",
    "+ the next result allows to state the erc for somp - ns .",
    "it will also be used to develop the exact recovery criterion for the noisy case later .",
    "[ lem : erc ] let @xmath22 where @xmath159 has full column rank for some support @xmath165 .",
    "moreover , @xmath166 denotes the orthogonal projector onto @xmath97 where @xmath167 .",
    "let @xmath168 , @xmath81 and @xmath169 where @xmath170 .",
    "it is assumed that @xmath171 . under these conditions ,",
    "the following inequality holds true @xmath172 where @xmath173 denotes the relative complement of @xmath28 with respect to @xmath174 $ ] .    following the steps of ( * ? ? ?",
    "* theorem 4.5 ) , the lemma is easily obtained .",
    "each column of the matrix @xmath96 can be expressed as a linear combination of the columns of @xmath159 .",
    "the reason that explains this last statement is that @xmath175 and @xmath176 .",
    "+ moreover , @xmath159 is guaranteed to have full column rank which implies that the moore - penrose pseudoinverse @xmath177 is equal to @xmath178 and consequently that @xmath179 . furthermore , it is easily established ( * ? ? ?",
    "* lemma 4.4 ) that for two matrices @xmath180 and @xmath181 , @xmath182 .    combining the results above yields",
    "@xmath183 finally , using equation  ( [ eq : inftytoonenorm ] ) shows that @xmath184 and concludes the proof .",
    "we are now ready to provide an erc for somp - ns .",
    "[ thm : erc ] let @xmath22 and @xmath169 .",
    "if @xmath185 and @xmath159 has full column rank , then a sufficient condition for somp - ns to properly retrieve the support @xmath28 of @xmath186 after exactly @xmath187 iterations is @xmath188 where @xmath173 is the relative complement of @xmath28 with respect to @xmath174 $ ] .",
    "let us assume that somp - ns has made correct decisions before iteration @xmath95 .",
    "the greedy selection ratio ( at iteration @xmath95 ) is defined as : @xmath189 clearly , @xmath190 ensures that somp - ns performs a correct decision at iteration @xmath95 since it implies that the largest sum of inner products is obtained for one of the atoms belonging to the support @xmath28 , _",
    "i.e. _ , @xmath191 .",
    "since it is assumed that somp - ns has only made correct decisions so far , lemma [ lem : erc ] shows that @xmath190 holds true whenever @xmath192 .",
    "+ furthermore , @xmath192 ensures that a correct atom is picked during iteration @xmath193 . by induction ,",
    "the hypothesis that correct decisions have been made so far will be satisfied for every iteration and full support recovery at iteration @xmath194 is thus guaranteed .",
    "it can be shown that theorem  [ thm : erc ] is sharp in the sense that if the inequality @xmath192 is not satisfied for some support @xmath28 , it is always possible to find a @xmath195 whose support is @xmath28 for which somp - ns identifies an incorrect atom at the first iteration .",
    "the sharpness property for somp - ns directly derives from ( * ? ? ?",
    "* theorem 3.10 ) which provides an equivalent property in the smv case for omp .",
    "indeed , if omp fails to choose a correct atom with vector @xmath196 , then somp - ns also fails with @xmath197 as , in this particular case , omp with signal @xmath198 and somp - ns with signal @xmath199 make the same decisions .",
    "+ moreover , the theorem above is generally of theoretical use only since computing @xmath200 requires to know the support beforehand .",
    "also , computing @xmath200 for all the possible supports of a given size is not computationally tractable .",
    "+ however , it is shown in @xcite that if @xmath201 , then @xmath202 for all the supports of size @xmath187 .",
    "it is worth noticing that @xmath27 is full rank for all supports @xmath28 of size @xmath45 if and only if @xmath140 . + although theorem  [ thm : erc ] proves to be interesting for @xmath203 in signal model ( [ eq : mmvsignalmodel ] ) , it should be extended to the noisy case which is the purpose of the next section .",
    "we develop here an erc for the noisy case which generalizes theorem  [ thm : erc ] to this context . to reach this result ,",
    "let us assume that somp - ns has made correct decisions before iteration @xmath95 .",
    "first of all , we separate the contribution of the noise @xmath204 and that of the useful signal @xmath186 : @xmath205 then , the somp - ns correlation is lower bounded by @xmath206 moreover , the triangle inequality yields @xmath207 since somp - ns makes a correct decision at step @xmath95 if @xmath208 , the inequalities above show that this condition is always satisfied whenever @xmath209 it is worth noticing that , due to lemma [ lem : erc ] , the following relationship holds true : @xmath210 note that the assumption that correct decisions have been so far is crucial for this result to be true .",
    "furthermore , as @xmath159 and @xmath211 are column submatrices of @xmath12 , one easily obtains @xmath212 a sufficient condition for ( [ eq : sompanalysis1 ] ) to hold is thereby @xmath213 theorem  [ thm : theoframework1 ] summarizes the previous discussion by explicitly stating the erc in the noisy case .",
    "[ thm : theoframework1 ] let @xmath186 be the sparse matrix to be retrieved and let @xmath171 denote its support .",
    "let @xmath159 exhibit a full column rank .",
    "let us assume that only correct atoms have been picked before iteration @xmath214 and that the reduced dictionary matrix @xmath159 has full column rank .",
    "somp - ns with dictionary matrix @xmath12 and signal @xmath215 is guaranteed to make a correct decision at iteration @xmath95 whenever @xmath216 where @xmath217 , @xmath218 and @xmath219 .",
    "noticeably a necessary condition for satisfying ( [ eq : sompanalysis2 ] ) reads @xmath220 which is precisely the erc obtained before for the noiseless case .",
    "moreover , low values of @xmath200 imply a better robustness against the noise as the condition hereabove is then more easily satisfied .",
    "this observation is not surprising since the robustness in the noiseless case determines the amplitude of the noise we can apply without ruining the support recovery .",
    "+ theorem  [ thm : theoframework1 ] is the cornerstone of the theoretical analysis conducted in this paper .",
    "however , as done hereafter , it remains desirable to find a lower bound for @xmath221 that expresses in a simpler manner the impact of the signal to be estimated , the weights and the dictionary matrix .",
    "also , the term @xmath222 will be dealt with by means of a statistical analysis presented in section  [ sec : recovguarantees ] when the noise is gaussian .",
    "we now focus on deriving a lower bound for @xmath223 that can be easily evaluated .",
    "verifying equation  ( [ eq : sompanalysis2 ] ) with this computable lower bound will provide a sufficient condition for the erc to hold .",
    "in particular , we desire to obtain a lower bound that does not rely on the knowledge of the particular support @xmath28 that is chosen . + to reach this goal , we chose here to extend the method proposed in @xcite to mmv problems .",
    "the following theorem mainly has a theoretical interest and is afterwards particularized in corollary [ corr : ripbasedbtbound ] that will used in the rest of the paper .",
    "corollary  [ corr : mubasedbtbound ] will provide a variant of corollary  [ corr : ripbasedbtbound ] relying on both the coherence of the dictionary ( instead of the rip ) and on the quantity @xmath200 .",
    "+    [ thm : firstboundgeneral ] let @xmath224 $ ] and let @xmath225 denote the indexes of the atoms chosen by somp - ns at iteration @xmath95 .",
    "it is assumed that @xmath226 , _",
    "i.e. _ , only correct decisions have been made before iteration @xmath95 .",
    "@xmath227 contains the indexes of the correct atoms yet to be selected at iteration @xmath95 .",
    "let @xmath228 where @xmath166 denotes the orthogonal projector onto @xmath97 .",
    "then , for any @xmath229 ( @xmath49 ) , @xmath230 where @xmath231 .",
    "moreover , if @xmath12 satisfies the rip with @xmath232-th restricted isometry constant @xmath233 , then @xmath234    denoting the @xmath100-th column of @xmath235 by @xmath236 , we first observe that @xmath237 the maximum being taken over @xmath238 since @xmath236 is orthogonal to @xmath97 because of the orthogonal projector @xmath239 . since @xmath240 , @xmath241 which implies @xmath242 the triangle inequality yields @xmath243 thus , @xmath244 the first inequality results from the observation that , for any vector @xmath245 , we have @xmath246 .",
    "the inequality @xmath247 is available in ( * ? ? ?",
    "* lemma 5 ) .",
    "the first part of the theorem is now proved . + if @xmath12 satisfies the rip with ric @xmath233 , then equation  ( [ eq : ricasmax ] ) yields @xmath248 , which proves the second part of the theorem .",
    "the result above shows that the decision metric in corresponding to the correct atoms , _",
    "i.e. _ , @xmath221 , is closely related to the @xmath1 norm of the signal to be recovered and to the singular values of @xmath249 .",
    "it is clear that the ability of @xmath12 to conserve the norm of sparse vectors is necessary to ensure that the measurement noise @xmath250 does not absorb @xmath235 .",
    "+ through the following corollary , we wish to obtain a simple term that replaces @xmath251 .",
    "[ corr : ripbasedbtbound ] let @xmath224 $ ] and let @xmath225 denote the indexes of the atoms chosen by somp - ns at iteration @xmath95 .",
    "it is assumed that @xmath226 , _",
    "i.e. _ , only correct decisions have been made before iteration @xmath95 .",
    "@xmath252 contains the indexes of the correct atoms yet to be selected at iteration @xmath95 .",
    "let @xmath228 where @xmath166 denotes the orthogonal projector onto @xmath97 .",
    "if @xmath12 satisfies the rip with @xmath232-th restricted isometry constant @xmath233 , then @xmath253    it is easy to show that for any @xmath254 @xmath255 it implies that @xmath256 in particular , the choice of the @xmath257 is arbitrary so that the best lower bound is given by @xmath258 one easily notices that @xmath259 . if @xmath260 , then choosing @xmath261 is optimal , _ i.e. _ , @xmath262 . together with theorem",
    "[ thm : firstboundgeneral ] , this result shows that the first inequality holds true .",
    "the last inequality is then trivially obtained .    both inequalities in the result above explicitly emphasize the impact of the weights , the ric and the amplitude of the coefficients to be recovered . however , regarding the weights , it is clear that they will also impact the value of @xmath222 .",
    "moreover , equation  ( [ eq : firstboundcorr1_2 ] ) suggests that , in order to reliably retrieve an atom , the sum of the absolute values of the associated coefficients in @xmath186 should be high enough .",
    "+ as already mentioned in the introduction , theorem  [ thm : firstboundgeneral ] appears to be a new result while corollary [ corr : ripbasedbtbound ] has already been obtained in the literature ( * ? ? ?",
    "* theorem 5 ) .",
    "+ it is now possible to use the coherence - based inequalities provided by lemma [ lem : riclambda ] so as to derive a new bound on the basis of the previous one .",
    "we obtain corollary [ corr : mubasedbtbound ] which should be understood as the coherence counterpart of corollary [ corr : ripbasedbtbound ] .",
    "[ corr : mubasedbtbound ] let @xmath224 $ ] and let @xmath225 denote the indexes of the atoms chosen by somp - ns at iteration @xmath95 .",
    "it is assumed that @xmath226 , _",
    "i.e. _ , only correct decisions have been made before iteration @xmath95 .",
    "@xmath252 contains the indexes of the correct atoms yet to be selected at iteration @xmath95 .",
    "let @xmath228 where @xmath166 denotes the orthogonal projector onto @xmath97 .",
    "if @xmath263 , then @xmath264 moreover , if @xmath265 , then both equation  ( [ eq : firstboundvmu1 ] ) and ( [ eq : firstboundvmu ] ) hold true . @xmath266    using lemma [ lem : riclambda ] and the equality @xmath267 , one obtains @xmath268 the first inequality makes sense only if @xmath263 while the last inequality requires @xmath265 .",
    "although corollary  [ corr : mubasedbtbound ] is less powerful and general than corollary  [ corr : ripbasedbtbound ] , it provides an interesting insight into how the coherence of the dictionary influences @xmath221 .",
    "the previous section provided a non - probabilistic analysis of the quantity @xmath221 by deriving lower bounds that are more simple to evaluate than the original quantity . regarding the noise - related quantity @xmath222 , we will in this section",
    "perform a stochastic analysis to derive a lower bound on the probability that it does not exceed a threshold @xmath269 for gaussian noises . + as shown by theorem  [ thm : theoframework1 ] , it is possible to examine whether somp - ns succeeds in choosing a correct atom at step @xmath95 by evaluating separately quantities linked to the sparse signal to be estimated and the noise vectors , @xmath221 and @xmath222 respectively . since several simple lower bounds for @xmath221 have been found , it becomes possible to evaluate a lower bound on @xmath270\\ ] ] _ i.e. _ , a lower bound on the probability that somp - ns makes correct decisions for signal model ( [ eq : mmvsignalmodel ] ) according to equation  ( [ eq : sompanalysis2 ] ) of theorem  [ thm : theoframework1 ] .",
    "corollaries  [ corr : ripbasedbtbound ] and [ corr : mubasedbtbound ] yield @xmath271 \\\\",
    "\\geq \\mathbb{p}\\left [ \\|{\\boldsymbol{\\phi}}^{\\mathrm{t } } { \\boldsymbol{e}}^{(t ) } { \\boldsymbol{q } } \\|_{\\infty } < 0.5 \\left(1 - \\|{\\boldsymbol{\\phi}}_s^+ { \\boldsymbol{\\phi}}_{\\overline{s } } \\|_{1 } \\right ) \\|{\\boldsymbol{\\phi}}_s^{\\mathrm{t } } { \\boldsymbol{z}}^{(t ) } { \\boldsymbol{q } } \\|_{\\infty}^{\\mathrm{(rip ) } }   \\right ] \\\\",
    "\\geq \\mathbb{p}\\left [ \\|{\\boldsymbol{\\phi}}^{\\mathrm{t } } { \\boldsymbol{e}}^{(t ) } { \\boldsymbol{q } } \\|_{\\infty } < 0.5 \\left(1 - \\|{\\boldsymbol{\\phi}}_s^+ { \\boldsymbol{\\phi}}_{\\overline{s } } \\|_{1 } \\right ) \\|{\\boldsymbol{\\phi}}_s^{\\mathrm{t } } { \\boldsymbol{z}}^{(t ) } { \\boldsymbol{q } } \\|_{\\infty}^{(\\mu ) }   \\right ] \\end{aligned}\\ ] ] where @xmath272    a statistical analysis of @xmath222 is proposed when @xmath273 and @xmath274 for @xmath275 .",
    "the advantage of our approach is to take into account the isotropic nature of statistically independent gaussian random vectors .",
    "our main result is theorem  [ thm : finalgaussianthm ] , which shows that the probability of making incorrect decisions from iteration @xmath193 to iteration @xmath276 included decreases exponentially with regards to a certain number of parameters .",
    "this theorem is then particularized so as to make use of the coherence @xmath123 of @xmath12 instead of the rip and the erc .",
    "+ first of all , the statistical properties of @xmath277 for a single and arbitrary atom @xmath13 are investigated in section  [ subsec : oneatom ] .",
    "these properties are then extended to @xmath278 } \\left ( \\sum_{k=1}^{k } \\left| \\left\\langle { \\boldsymbol{\\phi}}_j , { \\boldsymbol{e}}_{k }   \\right\\rangle \\right| q_k \\right)$ ] in section  [ subsec : natoms ] . on the basis of this last result ,",
    "theorem  [ thm : finalgaussianthm ] is obtained in section  [ subsec : fullrecovery ] . a particular case of the signal model",
    "is then examined in order to ease , in section  [ sec : numresults ] , the comparison of the results provided by means of the theoretical bound and those obtained by simulation .      in this section",
    ", we restate some assumptions and define quantities used later on .",
    "+ it is assumed that the entries of each @xmath65 ( @xmath49 ) are i.i.d .",
    "mean - zero gaussian random variables of variance @xmath279 , _",
    "i.e. _ , @xmath273 .",
    "furthermore , the noise vectors @xmath65 are statistically independent .",
    "finally , the columns of @xmath159 are assumed to be linearly independent . as already mentioned in section  [ subsec : ripdef ]",
    ", the latter condition is true for all supports @xmath28 of cardinality @xmath45 whenever @xmath280 .",
    "+ we define @xmath281    before going on further , we also define @xmath282 and @xmath283    corollary [ corr : ripbasedbtbound ] shows that a sufficient condition for to choose a correct atom at iteration @xmath193 is given by    @xmath284 } \\left ( \\sum_{k=1}^{k } \\left| \\left\\langle { \\boldsymbol{\\phi}}_j , { \\boldsymbol{e}}_{k }   \\right\\rangle \\right| q_k \\right ) <    0.5 \\left(1 - \\|{\\boldsymbol{\\phi}}_s^+ { \\boldsymbol{\\phi}}_{\\overline{s } } \\|_{1 } \\right )     ( 1-\\delta_{|s| } )   \\min_{j",
    "\\in s } \\sum_{k=1}^{k } | x_{j , k } | q_k . \\end{aligned}\\ ] ]    using the upper bound @xmath285 is a recurrent solution in the literature and provides satisfactory performance in a smv setting ( see @xcite ) .",
    "however , using such an approach in a mmv setting does not properly capture the performance gains obtained whenever @xmath47 increases as this upper bound assumes that all the error vectors @xmath286 are aligned with a single atom at each iteration .",
    "while this approximation is acceptable whenever only one measurement vector is available , it proves to be highly pessimistic as soon as one considers many independent ( and usually isotropic ) error vectors .",
    "+ this observation motivates an in - depth analysis of the statistical properties of @xmath287 } \\left ( \\sum_{k=1}^{k } \\left| \\left\\langle { \\boldsymbol{\\phi}}_j , { \\boldsymbol{e}}_{k }   \\right\\rangle \\right| q_k \\right)$ ] .",
    "the analysis conducted hereafter mainly relies on the notion of lipschitz functions and the related concentration inequalities .      in this section ,",
    "we are interested in providing a lower bound for @xmath288 for an arbitrary atom @xmath13 .",
    "the main result of this section is lemma  [ lem : finalboundindivprob ] .",
    "[ thm : concentrationineqlipschitz ] ( * ? ? ? * theorem 8.40 . )",
    "let @xmath289 be a lipschitz function ( with regards to the metric @xmath1 ) with lipschitz constant @xmath162 .",
    "let @xmath290 be a vector of independent standard gaussian random variables .",
    "then , for all @xmath291 @xmath292 and consequently @xmath293    the theorem above shows that lipschitz functions @xmath294 tend to concentrate around their expectations when @xmath295 is distributed as a standard gaussian random vector .",
    "moreover , the concentration gets better as the lipschitz constant @xmath162 decreases .",
    "+ this theorem is intended to be used in conjunction with the function @xmath296 where @xmath297 ( @xmath49 ) .",
    "this function will be shown to be equivalent to @xmath277 when @xmath65 is gaussian .",
    "+ we now wish to establish that @xmath298 is a lipschitz function , compute the associated lipschitz constant and determine its expectation .",
    "let @xmath299 , then , using the reverse triangle inequality and the cauchy - schwarz inequality , @xmath300 therefore , a valid lipschitz constant @xmath162 of @xmath298 is equal to @xmath301 .",
    "this is the best lipschitz constant since , for @xmath302 and @xmath303 , @xmath304 .",
    "+ using the concentration inequalities for lipschitz functions requires to know the value of @xmath305 $ ] . using the linearity of the expectation and the fact that for @xmath306 , @xmath307 , one easily obtains @xmath308 = \\sqrt{\\dfrac{2}{\\pi } } \\| { \\boldsymbol{q}}^{({\\boldsymbol{\\sigma}})}\\|_1 = b({\\boldsymbol{q } } , { \\boldsymbol{\\sigma}}).\\ ] ] by using theorem  [ thm : concentrationineqlipschitz ]",
    ", it is now possible to conclude that , for @xmath291 , @xmath309 the final step of the development is to prove that the function @xmath298 defined above is distributed as @xmath294 for @xmath310 .",
    "indeed , @xmath311 and @xmath312 .",
    "the following lemma summarizes the discussion above and will be used to establish the theorem of the next section .",
    "[ lem : finalboundindivprob ] let @xmath313 where @xmath314 ( @xmath128 ) .",
    "let @xmath65 ( @xmath49 ) be independent random variables respectively distributed as @xmath72 where @xmath297 .",
    "it is also assumed that @xmath80 .",
    "then , for @xmath291 , @xmath315    it is important to highlight that lemma [ lem : finalboundindivprob ] provides an upper bound of the probability that @xmath277 is higher than @xmath316 only if @xmath317 is higher than @xmath318 .",
    "the next problem to be tackled is that we would like to estimate an upper bound of the probability that @xmath287 } ( \\sum_{k=1}^{k } | \\langle { \\boldsymbol{\\phi}}_j , { \\boldsymbol{e}}_{k }   \\rangle | q_k ) $ ] is higher than a threshold @xmath317 instead of the same probability obtained for @xmath319 .",
    "the issue we are facing is that the @xmath57 random variables @xmath319 ( for @xmath128 ) are not statistically independent which implies that the probability of upper bounding all these @xmath57 random variables simultaneously is not equal to the product of the probability to upper bound them separately .",
    "however , it remains possible to find a workaround using the union bound , as demonstrated by the following theorem .",
    "[ thm : finalboundmanyatomsprob ] let @xmath313 where @xmath314 ( @xmath128 ) .",
    "let @xmath65 ( @xmath49 ) be independent random variables respectively distributed as @xmath72 where @xmath297 .",
    "it is also assumed that @xmath80 .",
    "then , for @xmath291 , @xmath320 \\leq n \\exp \\left ( - \\kappa({\\boldsymbol{q } } , { \\boldsymbol{\\sigma } } )   \\varepsilon^2 \\right).\\ ] ] equivalently , @xmath321 \\geq 1 - n \\exp \\left ( - \\kappa({\\boldsymbol{q } } , { \\boldsymbol{\\sigma } } ) \\varepsilon^2 \\right).\\ ] ]    first of all , we observe that @xmath322 } \\left ( \\sum_{k=1}^{k } \\left| \\left\\langle { \\boldsymbol{\\phi}}_j , { \\boldsymbol{e}}_{k }   \\right\\rangle \\right| q_k \\right)$ ] .",
    "then , by union bound , @xmath323 } \\left ( \\sum_{k=1}^{k } \\left| \\left\\langle { \\boldsymbol{\\phi}}_j , { \\boldsymbol{e}}_{k }   \\right\\rangle \\right| q_k \\right ) \\geq b({\\boldsymbol{q } } , { \\boldsymbol{\\sigma } } ) + \\varepsilon \\right ]   \\\\   = & \\mathbb{p } \\left [ \\bigcup_{j=1}^{n } \\left [   \\left ( \\sum_{k=1}^{k } \\left| \\left\\langle { \\boldsymbol{\\phi}}_j , { \\boldsymbol{e}}_{k }   \\right\\rangle \\right| q_k \\right ) \\geq b({\\boldsymbol{q } } , { \\boldsymbol{\\sigma } } ) + \\varepsilon \\right ] \\right ] \\\\",
    "\\leq & \\sum_{j=1}^{n } \\mathbb{p } \\left [ \\left ( \\sum_{k=1}^{k } \\left| \\left\\langle { \\boldsymbol{\\phi}}_j , { \\boldsymbol{e}}_{k }   \\right\\rangle \\right| q_k \\right ) \\geq b({\\boldsymbol{q } } , { \\boldsymbol{\\sigma } } ) + \\varepsilon \\right ] \\\\",
    "\\leq & n \\exp \\left ( - \\kappa({\\boldsymbol{q } } , { \\boldsymbol{\\sigma } } ) \\varepsilon^2 \\right).\\end{aligned}\\ ] ] the first inequality results from the union bound while the second inequality holds because of lemma [ lem : finalboundindivprob ] .",
    "theorem  [ thm : finalboundmanyatomsprob ] implicitly provides a lower bound on the probability of making correct decisions during iteration @xmath193 .",
    "it is indeed possible to use either equation  ( [ eq : eqripbound ] ) or ( [ eq : eqmubound ] ) in conjunction with equation  ( [ eq : probinequalities ] ) and theorem  [ thm : finalboundmanyatomsprob ] to obtain the desired lower bound .",
    "nevertheless , a lower bound on the probability that somp - ns makes correct decisions from iteration @xmath193 to iteration @xmath276 remains to be found .",
    "this is the purpose of this section .",
    "+ before establishing the main result , one needs lemmas  [ lem : fullsupportlem1 ] and [ lem : fullsupportlem2 ] . +    [",
    "lem : fullsupportlem1 ] if @xmath286 is distributed as @xmath72 and @xmath324 is a fixed orthogonal projector matrix , then @xmath325 is distributed as @xmath326 where @xmath327 .",
    "we have @xmath328 an auxiliary atom @xmath329 can be defined for index @xmath11 , @xmath330 .",
    "this atom satisfies the inequality @xmath331 .",
    "let us now observe that if the entries of @xmath65 are i.i.d .",
    "random variables distributed as @xmath332 , then @xmath333",
    ". the result immediately follows .",
    "lemma @xmath334 basically establishes that , for a fixed projection matrix @xmath324 , @xmath325 is distributed as a mean - zero gaussian random variable whose variance is always lower than that of the entries of @xmath335 .",
    "this result will enable us to apply a statistical analysis similar to that of the previous section by keeping the gaussian hypothesis .",
    "[ lem : fullsupportlem2 ] we consider the random variables @xmath32 , @xmath336 and @xmath337 where @xmath338 .",
    "it is assumed that @xmath339 and @xmath340 .",
    "then , for all @xmath341 , @xmath342 \\leq \\mathbb{p } \\left [ x + |y_2| \\leq \\varepsilon   \\right].\\ ] ]    we know that @xmath343 and @xmath344 where @xmath345    since @xmath346 is a monotonically increasing function , one obtains @xmath347 .",
    "thus , @xmath348 & = \\int_{-\\infty}^\\varepsilon \\mathbb{p } \\big [ |y_1| \\leq \\varepsilon - x \\big ] f_x(x ) \\mathrm{d}x \\\\   & \\leq \\int_{-\\infty}^\\varepsilon \\mathbb{p } \\big [ |y_2| \\leq \\varepsilon - x \\big ] f_x(x ) \\mathrm{d}x\\\\   & = \\mathbb{p } \\left[x + |y_2| \\leq \\varepsilon   \\right ] .",
    "\\qedhere\\end{aligned}\\ ] ]    in the rest of this paper , the random variable @xmath32 of lemma  [ lem : fullsupportlem2 ] will be replaced with a sum of @xmath349 independent random variables exhibiting half - normal distributions , _",
    "i.e. _ , @xmath350 where the @xmath351 ( @xmath352 ) are independent and @xmath353 .",
    "thereby , an immediate corollary of the lemma above is that the probability of upper bounding by @xmath291 the sum of statistically independent random variables exhibiting half - normal distributions is always decreased whenever the variance of at least one of the random variables is increased .",
    "+ let us now state the key theoretical result of this paper , which provides a lower bound on the probability that somp - ns selects correct atoms during the first @xmath354 iterations .",
    "[ thm : finalgaussianthm ] let @xmath65 ( @xmath49 ) be statistically independent gaussian random vectors respectively distributed as @xmath72 .",
    "let @xmath355 .",
    "let @xmath356 be the signal matrix whose support is @xmath28 .",
    "let also @xmath357 be unit - norm vectors in @xmath358 and @xmath359 be the corresponding matrix which is assumed to satisfy the rip with @xmath187-th ric @xmath360 .",
    "let @xmath361 . then",
    ", somp - ns with dictionary matrix @xmath12 , weights @xmath114 ( @xmath49 ) and signal @xmath215 is ensured to make correct decisions during the first @xmath354 iterations , _",
    "i.e. _ , from iteration @xmath193 to iteration @xmath45 included , with probability higher than @xmath362 whenever @xmath363 where @xmath364 @xmath365 and @xmath318 being defined in equation  ( [ eq : defkappa ] ) and ( [ eq : defb ] ) , respectively .",
    "first , we observe that @xmath366 where it is convenient to define the @xmath11-th auxiliary atom at iteration @xmath95 as @xmath367 . according to ( [ eq : probinequalities ] ) , at iteration @xmath95 ( for @xmath368 ) , a sufficient condition for to make a correct decision at iteration @xmath95 is given by @xmath284 } \\left ( \\sum_{k=1}^{k } \\left| \\left\\langle { \\boldsymbol{\\phi}}_j^{(t ) } , { \\boldsymbol{e}}_{k }   \\right\\rangle \\right| q_k \\right ) \\leq   0.5 ( 1 - \\|{\\boldsymbol{\\phi}}_s^+ { \\boldsymbol{\\phi}}_{\\overline{s } } \\|_{1 } )    ( 1-\\delta_{|\\mathcal{j}_t| } ) \\min_{j \\in s } \\sum_{k=1}^{k } | x_{j , k } | q_k . \\end{aligned}\\ ] ] since @xmath369 implies @xmath370 , then @xmath371 which shows that a less tight sufficient condition for somp - ns to make a correct decision at iteration @xmath95 is @xmath372 } \\left ( \\sum_{k=1}^{k } \\left| \\left\\langle { \\boldsymbol{\\phi}}_j^{(t ) } , { \\boldsymbol{e}}_{k }   \\right\\rangle \\right| q_k \\right ) \\leq \\varepsilon ( { \\boldsymbol{q } } ) = 0.5 ( 1 - \\|{\\boldsymbol{\\phi}}_s^+ { \\boldsymbol{\\phi}}_{\\overline{s } } \\|_{1 } )   ( 1-\\delta_{|s| } )   \\min_{j \\in s } \\sum_{k=1}^{k } | x_{j , k } | q_k .",
    "\\end{aligned}\\ ] ] therefore , if the condition above holds for @xmath373 , somp - ns is guaranteed to pick a correct atom at iteration @xmath193 . at iteration @xmath374",
    ", the orthogonal projector @xmath375 can take @xmath187 different values ( each value of @xmath375 corresponds to a specific support @xmath376 ) .",
    "therefore , if ( [ eq : suffcondwsompstept ] ) holds for every possible projector matrix at iteration @xmath374 , somp - ns is guaranteed to pick a correct atom at iteration @xmath374 . thus , we need to satisfy @xmath377 equations of the form @xmath378 ( where @xmath379 ) to ensure that somp - ns picks correct atoms during the first two iterations .",
    "+ extending the previous train of thought from iteration @xmath193 to iteration @xmath45 , one easily comes to the conclusion that @xmath380 equations of the form @xmath378 ( where @xmath379 ) should be satisfied since , at iteration @xmath95 , there exist @xmath381 possible realizations of the orthogonal projector @xmath382 . using",
    "the union bound as in the proof of theorem  [ thm : finalboundmanyatomsprob ] shows that the probability of satisfying the @xmath383 equations is lower bounded by @xmath384\\ ] ] where @xmath385 is the @xmath9-th possible realization of the orthogonal projector at iteration @xmath95 assuming that only correct atoms have been picked before iteration @xmath95 .",
    "+ lemmas [ lem : fullsupportlem1 ] and [ lem : fullsupportlem2 ] imply that @xmath386 \\leq    \\mathbb{p } \\left [ \\sum_{k=1}^{k } \\left| \\left\\langle { \\boldsymbol{{\\boldsymbol{\\phi}}_j } } , { \\boldsymbol{e}}_{k }   \\right\\rangle \\right| q_k \\geq \\varepsilon({\\boldsymbol{q } } ) \\right ] .",
    "\\end{aligned}\\ ] ] furthermore , since @xmath387 , lemma [ lem : finalboundindivprob ] shows that @xmath388 \\leq \\exp \\left ( - \\kappa({\\boldsymbol{q } } , { \\boldsymbol{\\sigma } } )   \\overline{\\varepsilon}({\\boldsymbol{q } } , { \\boldsymbol{\\sigma}})^2 \\right).\\ ] ] whenever @xmath389 .",
    "finally , the probability that somp - ns chooses correct atoms from iteration @xmath193 to iteration @xmath45 ( let us denote this event by @xmath390 ) satisfies @xmath391 \\geq 1 - n \\mathcal{c}_s \\exp \\left ( - \\kappa({\\boldsymbol{q } } , { \\boldsymbol{\\sigma } } ) \\overline{\\varepsilon}({\\boldsymbol{q } } , { \\boldsymbol{\\sigma}})^2 \\right ) .",
    "\\qedhere\\ ] ]    the theorem above translates several intuitive realities .",
    "first , by examining the expression of @xmath269 , one concludes that several quantities influence the probability of correct recovery :    * the expression @xmath392 quantifies the robustness of the recovery in the noiseless case .",
    "it is evident that the margin of error when deciding which atom to choose in the noiseless case will contribute to determine the admissible noise level in the noisy case .",
    "* the term @xmath393 translates the ability of the dictionary matrix @xmath12 to maintain the @xmath1 norm of @xmath187-sparse signals , which ensures that the norm of the columns of @xmath235 remain sufficiently high to avoid being absorbed by the noise matrix @xmath250 . *",
    "the minimized sum @xmath394 depicts the idea that the weighted sum of the coefficients associated with every atom should be high enough to allow somp - ns to identify them when noise is added to the measurements .",
    "this term simultaneously captures the influence of the signal to be recovered as well as that of the weights .",
    "moreover , @xmath365 indicates that increasing the noise variances decreases the probability of support recovery .",
    "also , increasing the weights naturally augments the power of the noise signal @xmath250 .",
    "nevertheless , it should be expected that this effect is counterbalanced by @xmath395 as the latter variable is also a function of the weights .",
    "finally , @xmath57 translates the fact that the noise affects every atom while @xmath396 takes into account the existence of @xmath45 iterations .",
    "+ it is worth noticing that @xmath397 so that it indicates that the probability of full support recovery is lower bounded by @xmath398    on the basis of past results in the literature , it will be suggested in section  [ subsec : gribonvalrelatedthm ] that @xmath396 is an artifact of our proof and should ideally be replaced with a function that increases more slowly as @xmath187 is augmented .",
    "+ also , as a last remark , theorem  [ thm : finalgaussianthm ] can be rephrased by stating that the probability of failure of somp - ns from iteration @xmath193 to iteration @xmath45 , _",
    "i.e. _ , at least one wrong atom is chosen during the first @xmath354 iterations , admits the upper bound @xmath399",
    "theorem  [ thm : finalgaussianthm ] is now to be particularized using the coherence of @xmath12 instead of the rip and the erc .",
    "[ thm : finalgaussianthmcoherence ] let @xmath65 ( @xmath49 ) be statistically independent gaussian random vectors respectively distributed as @xmath72 .",
    "let @xmath400 .",
    "let also @xmath356 be the signal matrix whose support is @xmath28 .",
    "let also @xmath357 be unit - norm vectors in @xmath358 and @xmath359 be the corresponding matrix whose coherence @xmath123 is assumed to satisfy @xmath201 .",
    "let @xmath361 .",
    "then , somp - ns with dictionary matrix @xmath12 , weights @xmath114 ( @xmath49 ) and signal @xmath215 is ensured to make correct decisions during the first @xmath354 iterations , _",
    "i.e. _ , from iteration @xmath193 to iteration @xmath45 included , with probability higher than @xmath401 whenever @xmath402 , where @xmath403 @xmath365 and @xmath318 being defined in equation  ( [ eq : defkappa ] ) and ( [ eq : defb ] ) , respectively .    as it has already been pointed out in section  [ subsec : ercnoiseless ] , if @xmath201 , then @xmath202 for all the supports of size @xmath187 . moreover , equation  ( [ eq : rictocoherence ] ) shows that @xmath404 whenever @xmath405 thus , using the two inequalities above in conjunction with theorem  [ thm : finalgaussianthm ] yields @xmath406 elementary algebraic manipulations show that the expression above simplifies to @xmath407    a similar although stronger result can be obtained by means of the cumulative coherence function @xmath408 . however , the resulting expression of @xmath409 is more complicated .",
    "moreover , coherence - based bounds only have a theoretical interest as they prove to be pessimistic for practical cases , even when using the cumulative coherence function .",
    "a result similar to theorem  [ thm : finalgaussianthm ] has already been obtained in @xcite .",
    "the striking similarities between our result and that obtained in @xcite motivate this section .",
    "they prove the following theorem .",
    "* theorem 7 ) [ thm : gribonval ] let @xmath410 with @xmath411 a @xmath412 matrix of standard gaussian random variables , @xmath413 and @xmath204 an error term orthogonal to the atoms in @xmath187 .",
    "assume that the dictionary matrix @xmath12 satisfies the restricted isometry property ( rip ) with restricted isometry constant ( ric ) ( of order @xmath414 ) @xmath415 and @xmath416 then , the probability that @xmath187 iterations of somp fail to exactly recover the support @xmath28 on the basis of @xmath102 does not exceed @xmath417 with @xmath47 the number of measurement vectors , @xmath57 the number of atoms and @xmath418    in @xcite , the statistical analysis has been focused on the sparse signal to be estimated , _",
    "i.e. _ , @xmath186 , while no particular statistical distribution is assumed for @xmath204 .",
    "since the noise vectors are assumed to be orthogonal to the columns of @xmath186 , their purpose is to model an approximation noise , _",
    "i.e. _ , the part of the signal @xmath102 that can not be mapped by @xmath73 .",
    "conversely , the noise signals envisioned in our paper represent additive measurement noises that can not be assumed to be orthogonal to any vector subspace . + in the noiseless case , a result similar to theorem  [ thm : gribonval ] has been obtained in ( * ? ? ?",
    "* theorem 6.2 ) for a variant of somp entitled 2-somp , _",
    "i.e. _ , a somp algorithm where the decision on which atom to pick is performed on the basis of the maximization of a @xmath1 norm instead of a @xmath0 norm . +",
    "although the problem addressed in this paper is different from that of theorem  [ thm : gribonval ] , the authors of @xcite have used approaches similar to ours and they also noticed that the parasitic term @xmath396 seems difficult to remove .",
    "the rationale of this discussion is thus that it is difficult to avoid the suboptimal term @xmath396 when conducting developments similar to that presented in this paper .",
    "theorem  [ thm : finalgaussianthm ] provides interesting insights into how successful somp - ns is whenever some parameters are modified .",
    "however , the theoretical developments presented in this paper do not properly capture all the characteristics of somp - ns .",
    "+ first of all , regarding @xmath419 , the value of @xmath57 should be lowered in practice .",
    "the reason why @xmath57 should be replaced by @xmath420 is because our analysis assumes that all the atoms not to be picked ( of index @xmath11 ) are such that @xmath421 while , in practice , only a few atoms are likely to exhibit a significant value of @xmath422 . + we conjecture that @xmath423 may be replaced by a linear function of @xmath187 .",
    "the reason why we failed at obtaining such a result is probably linked to the proof of theorem [ thm : finalgaussianthm ] . all the possible supports at each iteration are considered and it is ensured that the sufficient condition for making a correct decision is satisfied for each support .",
    "this is very pessimistic as only one support out of all the numerous possibilities actually matters .",
    "as indicated in section  [ subsec : gribonvalrelatedthm ] , other researchers working on similar problems have stumbled upon this issue and no solution has been found so far to the best of the authors knowledge .",
    "the simulation results presented in the end of this paper will however not address this problem .",
    "+ furthermore , as it will be explained in section  [ subsec : summarynumres ] , the bias @xmath318 should be removed in order to deliver results compatible with what has been observed in our numerical simulations .",
    "this term is most likely an artifact linked to equation  ( [ eq : firstbigapprox ] ) and equation  ( [ eq : secondbigapprox ] ) .",
    "equation  ( [ eq : firstbigapprox ] ) basically assumes that @xmath424 and @xmath425 always have opposite signs for atoms @xmath13 whose indexes @xmath11 belong to the support @xmath28 .",
    "conversely , equation  ( [ eq : secondbigapprox ] ) assumes that @xmath424 and @xmath425 always have identical signs for atoms whose indexes do not belong to the support @xmath28 .",
    "thus , a statistical analysis directly performed on @xmath426 may prevent the bias term from appearing .",
    "+ therefore , considering all the conjectures above , a better bound may be given by @xmath427 where @xmath57 has been replaced with @xmath420 , @xmath428 has been substituted to @xmath396 and @xmath318 has been canceled out . + finally , it is worth pointing out that equation  ( [ eq : b1 ] ) will likely not deliver perfect results . indeed ,",
    "equation  ( [ eq : trueequationmodel ] ) suggests that the probability of sparse support recovery success is actually a sum of exponential functions , each function corresponding to a single atom , iteration and support .",
    "moreover , possibly different values of @xmath429 should be chosen for each exponential function .",
    "it is also probably suboptimal to make use of the union bound .",
    "this section aims at demonstrating that :    1 .",
    "somp - ns provides significant performance improvements when compared to somp provided that the noise vectors exhibit different variances and that the weights are properly chosen .",
    "2 .   depending on the signal model that is chosen , it is possible to accurately estimate the optimal weights by using simple closed - formed formulas derived from equation  ( [ eq : b1 ] ) .",
    "3 .   equation  ( [ eq : b1 ] ) properly predicts the performance improvements obtained when the number of measurement vectors increases .",
    "the purpose of the first point is to demonstrate numerically that the gains provided by somp - ns are significant .",
    "the last two points rather focus on the numerical validation of the theoretical analysis presented in this paper . with these goals in mind",
    ", a particular signal model will be chosen .",
    "in particular , this signal model will be sufficiently simple to allow the computation of the optimal weights when the model  ( [ eq : b1 ] ) is assumed to be correct .",
    "the objective of this section is to particularize theorem  [ thm : finalgaussianthm ] to models for which @xmath430 ( @xmath49 , @xmath431 ) and @xmath432 ( @xmath49 , @xmath433 ) where the terms @xmath434 denote rademacher random variables , _",
    "i.e. _ , random variables that return either @xmath374 or @xmath435 with probability @xmath436 for both outcomes .",
    "+ two different sign patterns will be distinguished .",
    "sign pattern 1 refers to the case where the sign pattern is identical for all the sparse vectors @xmath55 to be recovered , _",
    "i.e. _ , @xmath437 for all @xmath438 , @xmath431 and @xmath439 whenever @xmath440 .",
    "sign pattern 2 corresponds to the situation where the sign pattern is independent for each and within each @xmath55 , _",
    "i.e. _ , @xmath441 whenever @xmath440 and/or @xmath275 .",
    "+ in both cases , it is worth mentioning that the absolute values of the entries of each @xmath55 are equal to @xmath442 . thus , @xmath443 and , according to theorem  [ thm : finalgaussianthm ] , the probability of full support recovery is always higher than @xmath444 where @xmath445 .",
    "also , the bound above only holds if @xmath446 . + by using the conjecture about the elimination of the bias term @xmath318 described in section  [ subsec : conjectures ] ,",
    "one obtains equation  ( [ eq : modelconjecture ] ) , which is reminiscent of equation  ( [ eq : b1 ] ) , except that ( [ eq : modelconjecture ] ) does not include the conjectures linked to the term @xmath419 . if our only objective is to derive the optimal weights according the theoretical model , only the argument of the exponential matters so that adjusting the term @xmath419 has no effect .",
    "@xmath447    let @xmath448 denote the arithmetic mean of the entries of vector @xmath10 .",
    "then , @xmath449 and @xmath450 .",
    "thus , the probability of full support recovery is always higher than @xmath451 as a particular case , if @xmath452 ( @xmath453 ) , the latter probability also rewrites @xmath454 }   \\rangle } \\varepsilon'^2 \\right).\\ ] ] let us now focus our attention on the weights that maximize equation  ( [ eq : b2 ] ) or , equivalently , @xmath455 .",
    "we can restrict our attention to the maximization of @xmath456 we define @xmath457 so that the expression above also reads @xmath458 the quantity @xmath459 represents the direction of @xmath460 so that we know that a global maximizer is obtained whenever @xmath460 and @xmath461 have the same direction .",
    "it means that a global maximizer is obtained if and only if @xmath462 where @xmath463 .",
    "this is equivalent to requiring @xmath464 since @xmath457 . by choosing @xmath465 ,",
    "one concludes that @xmath466 provides an optimal weighting strategy according to ( [ eq : b2 ] ) .",
    "now that the optimal weighting strategy for the signal models envisioned in section  [ subsec : specialsigmodel ] has been derived , it becomes possible to determine whether the bound ( [ eq : b2 ] ) properly predicts the impact of the weights on the performance that is achieved .",
    "+ our matlab simulation software is available in @xcite .",
    "all the scripts needed to generate the figures presented in this section are also available . the reader should know that every simulation result exposed hereafter has been performed by using single precision floating point representations .",
    "the reason for such a choice is that single precision arithmetic is faster and thus preferred for algorithms intended to run on real - time platform such as somp - ns .",
    "for the same reason , the simulation results are obtained faster when using the single precision format . + it is assumed that @xmath467 and @xmath468 .",
    "the simulation framework consists of a fixed dictionary matrix @xmath469 whose entries were generated on the basis of independent and identically distributed gaussian random variables and then normalized in such a way that each column of the matrix exhibits a unit @xmath1 norm .",
    "this matrix is fixed for all the simulations and is available in @xcite .",
    "+ two simulation frameworks have been envisioned to demonstrate the three points introduced at the very beginning of section  [ sec : numresults ] .",
    "the first framework consists of simulations for the case @xmath470 and addresses the first two objectives while the last framework examines how the probability of successful support recovery evolves as @xmath47 increases .",
    "first of all , it is worth pointing out that the performance achieved by is invariant if the weight vector @xmath471 is multiplied by a positive constant .",
    "therefore , only the angle @xmath472 is investigated .",
    "the weights are thus generated on the basis of the polar coordinate system @xmath473 , @xmath474 where @xmath475 . in practice ,",
    "the grid of weighting angles @xmath476 will consist of @xmath477 uniformly spaced angles from @xmath478 to @xmath479 .",
    "+ the noise standard deviation vector @xmath480 is generated on the basis of the polar coordinate system @xmath481 where @xmath482 describes the orientation of the noise vector .",
    "the grid of values for @xmath482 is composed of @xmath477 uniformly spaced angles ranging from @xmath483 to @xmath484 .",
    "extremely high or low angles have been avoided because they correspond to situations for which the noise concentrates essentially on one measurement vector .",
    "therefore , appropriate weighting strategies would be able to cancel most of the noise and would lead to probabilities of correct support recovery that are too high to be reliably estimated on the basis of a limited number of monte carlo cases . + a total of six simulation configurations have been run .",
    "each configuration corresponds to one of the two sign patterns described in section  [ subsec : specialsigmodel ] , to a support size @xmath187 and to a value of @xmath442 .",
    "simulation cases have been generated for each value of @xmath482 belonging to the grid defined beforehand .",
    "once the support size is fixed , the actual support is randomly and uniformly chosen among all the possibilities .",
    "the support that is simulated is independent for each case .",
    "+ although @xmath485 , the input signal - to - noise ratio ( snr ) , referred to as @xmath486 and defined in equation  ( [ eq : defsnrinput ] ) , is to be modified by means of the quantity @xmath442 .",
    "+ @xmath487    table [ tab : configssimk2 ] describes all the configurations that have been investigated numerically .",
    "the values of @xmath442 have been chosen in such a way that the probability of full support recovery when @xmath488 is approximately equal to @xmath489 and @xmath490 for the sign pattern 1 and sign pattern 2 respectively .",
    "these probabilities have been chosen in such a way that the probability of successful full support recovery over the grid defined for @xmath491 never reaches values so high that it can not be reliably estimated on the basis of a limited number of simulation experiments .",
    "the lower value of the probability of successful recovery for sign pattern @xmath374 is linked to the fact that having a sign pattern independent for each measurement vector , as for sign pattern 2 , provides performance improvements .",
    "this observation is actually reminiscent to what has been established in theorem  [ thm : gribonval ] .",
    "+ in table [ tab : configssimk2 ] , the input snr , _",
    "i.e. _ , @xmath486 , is estimated by generating @xmath492 cases for the configuration of interest , then the input snr ( in db ) is computed for each case and the results are finally averaged .",
    "the matlab script implementing this estimation is available in @xcite .",
    ".simulation configurations | @xmath470 [ cols= \" < , < , < , < , < , < \" , ]     as an example , figure  [ fig : simanglesplotk2conf2 ] depicts the probability of full support recovery for configuration @xmath493 as a function of @xmath491 .",
    "+    ) for configuration @xmath493 ( sign pattern @xmath374 , @xmath494 , @xmath495 and @xmath496 db )  probability of full support recovery as a function of @xmath476 and @xmath482  the black curve refers to the optimal weights derived from equation  ( [ eq : b2 ] )  each pixel of the figure has been computed on the basis of @xmath497 simulation cases , width=491 ]    the first objective we would like to fulfill is demonstrating that somp - ns is capable to outperform somp whenever the noise standard deviations are not identical for each measurement vector . to do so , we will examinate , for each configuration and for each value of @xmath482 , what is the ratio of the probability of failure obtained for @xmath498 , _",
    "i.e. _ , the weights corresponding to somp , to the lowest probability of failure , _",
    "i.e. _ , that obtained for the truly optimal weights . figure  [ fig : simanglessompvsoptimal ] plots the aforementioned quantity for the all the configurations of table [ tab : configssimk2 ] .",
    "note that the optimal weights are determined on the basis of the numerical results , the formula @xmath499 obtained on the basis of equation  ( [ eq : b2 ] ) is not used .",
    "+    it is observed that the gains provided by the proper application of somp - ns are significant , especially for low values of the support size .",
    "the only case for which the gain is almost nonexistent is for sign pattern @xmath374 , @xmath500 and @xmath501 , _ i.e. _ , configuration @xmath502 .",
    "this may actually be a consequence of the normalization procedure of @xmath442 described previously which ensures that the probability of successful support recovery for @xmath488 is equal to @xmath489 .",
    "the value of @xmath442 had to be chosen significantly higher than that of the other cases to attain the @xmath489 goal , which limits the impact of the noise and thus hinders the improvement of the performance by modifying the weights .",
    "+ let us now attack the second objective of section  [ sec : numresults ] .",
    "we wish to show that the formula @xmath499 obtained in section  [ subsec : specialsigmodel ] delivers reliable estimates of the optimal weights .",
    "figure  [ fig : simanglesoptimalvstheory ] summarizes the results .",
    "first of all , it is observed that , for the first sign pattern , the solution @xmath499 always corresponds to the truly optimal weights while this is not true for the second sign pattern . in particular , the discrepancy between the numerical results and the theoretical formula ( [ eq : b2 ] ) increases as the size of the support augments .",
    "+    the first observation is explained by the different sign patterns for each measurement vector .",
    "although the weights have been introduced to better filter the influence of the noise , they also have an impact on the relative importance of each @xmath55 in the decisions that are taken . given that the sparse vectors @xmath55 to be recovered have identical distributions , it is to be expected that , without noise , the optimal weights are obtained by choosing @xmath498 for symmetry reasons .",
    "figure  [ fig : simanglesplotk2conf6nonoise ] displays the simulation results obtained for a configuration identical to configuration @xmath503 except that @xmath504 db , _",
    "i.e. _ , the influence of the noise is negligible .",
    "it is observed that the interaction of the weights and the sparse vectors to be recovered exists and that the optimal weighting angle is equal to @xmath505 .",
    "a possible interpretation of the observations above is that the optimal weighting strategy is a mixture of the strategy that optimizes the support recovery in the noiseless case and of that which minimizes the impact of the noise on the decisions that are taken , _",
    "i.e. _ , @xmath499 .",
    "nevertheless , further theoretical developments should be conducted to assess whether the proposed interpretation is correct .",
    "finally , it is worth pointing out that the phenomenon described above is not observed for sign pattern @xmath374 because @xmath506 and @xmath507 are identical in this case .",
    "+ the reasons that explain why the optimal weights get closer to @xmath498 when the support size increases , as shown in figure  [ subfig : sp2optangles ] , is not clear and would require additional theoretical investigations that fall outside of the framework of this paper . +    ) for configuration @xmath503 with  probability of full support recovery as a function of @xmath476 and @xmath482  sign pattern @xmath493  @xmath501  @xmath508  # cases @xmath509,width=491 ]      the final question of whether the proposed theoretical analysis properly conveys the properties of somp - ns whenever @xmath47 increases is to be discussed in this section .",
    "our main objective is to show that , as predicted by equation  ( [ eq : b2 ] ) , the probability of failure of somp - ns decreases linearly with @xmath47 when it is plotted is semi - logarithmic axes . indeed ,",
    "equation  ( [ eq : b2 ] ) yields @xmath510 where @xmath511 denotes the probability of failure of correct support recovery . + the simulation framework consists of a fixed weighting strategy for which all the weights are equal , _",
    "i.e. _ , the weighting strategy corresponds to somp . for each value of @xmath47 ,",
    "the noise vector is given by @xmath512 so that it is reminiscent of the noise vector defined in section  [ subsec : simresk2 ] for @xmath470 .",
    "the results are plotted in figure  [ fig : simplotk ] .",
    "the configurations that have been chosen are directly inspired of those presented in table  [ tab : configssimk2 ] .",
    "the number of cases simulated for each curve and each value of @xmath47 is equal to @xmath513 .",
    "some configurations have been discarded because they do not exhibit a probability of failure equal to @xmath193 without noise , which is incompatible with the implicit assumption of our theoretical model that no errors are committed in the noiseless case . indeed , if @xmath514 , _",
    "i.e. _ , the erc is the noiseless case is not satisfied , then equation  ( [ eq : sompanalysis2 ] ) can not hold .",
    "for example , figure  [ fig : simanglesplotk2conf6nonoise ] shows that configuration @xmath503 from table [ tab : configssimk2 ] exhibits a non - zero probability of failure without noise .",
    "+      sp refers to sign pattern , width=491 ]    the principal observation for figure  [ fig : simplotk ] is that the slope of @xmath511 in semi - logarithmic axes is linear with regards to @xmath47 .",
    "this observation provides evidence that the theoretical model conveys the behavior of somp - ns when @xmath47 increases .",
    "the numerical results have revealed the following interesting facts :    1 .",
    "somp - ns provides significant performance improvements when compared to somp provided that the weights are properly chosen and that the noise variances are different for each measurement vector .",
    "the formula @xmath499 derived from equation  ( [ eq : b2 ] ) corresponds to the truly optimal weights whenever * the sparse vectors @xmath55 to be recovered are identical . *",
    "the support size @xmath187 is low enough .",
    "+ the exact reason why the formula @xmath499 gets less accurate as the size of the support increases remains an open question .",
    "the theoretical analysis properly predicts the characteristics of the decrease of the probability of failure of somp - ns whenever the number of measurement vectors increases .    besides the three points above , which answer the three questions enumerated at the beginning of section  [ sec : numresults ] , the close fitting of @xmath499 and the numerical results for sign pattern @xmath374 suggests that the bias term @xmath318 is indeed an artifact of our developments as adding it would change the theoretically optimal weights and we would then observe a mismatch between them and those obtained by simulation .",
    "as suggested in section  [ subsec : conjectures ] , the bias @xmath318 could be removed by avoiding to make use of the inequalities ( [ eq : firstbigapprox ] ) and ( [ eq : secondbigapprox ] ) . using a more subtle approach than the use of the union bound could also yield performance improvements .",
    "+ although it appears to be difficult , replacing the term @xmath423 by a function that depends linearly on @xmath187 would be of great interest , especially since it would close a hole in the literature regarding the performance of the well - known somp algorithm that is a particular case of ours .",
    "+ finally , extending the presented analysis by performing a joint statistical analysis of both the noise and the sparse signals to be recovered would be of great interest . to begin with",
    ", it would provide a theoretical model that predicts the truly optimal weights by simultaneously taking into account how they impact the sparse signals to be recovered and the noise vectors .",
    "next , it would also enable one to comprehend why the discrepancy between the formula @xmath499 and the truly optimal weights increases as the support size augments ( see figure  [ fig : simanglesoptimalvstheory ] ) .",
    "finally , the statistical analysis of the sparse signals could replace @xmath419 by @xmath515 ( where @xmath516 is significantly lower than @xmath57 ) as conjectured in section  [ subsec : conjectures ] .",
    "a novel algorithm entitled somp - ns that generalizes somp by associating weights with each measurement vector has been proposed . a theoretical framework to analyze",
    "this algorithm has been built .",
    "lower bounds on the probability of full support recovery by means of somp - ns have been developed in the case where the noise corrupting the measurements is gaussian .",
    "numerical simulations have revealed that the developed theoretical results accurately depict key components of the behavior of somp - ns while they also fail to capture some of its properties . in particular , it has been shown that , under the right circumstances , the weights of somp - ns can be efficiently optimized on the basis of the proposed theoretical bounds . finally , the reasons that explain why some characteristics of somp - ns are not properly conveyed by the theoretical analysis have been discussed and potential workarounds to be investigated have been suggested .",
    "r.  adamczak , a.  e.  litvak , a.  pajor , and n.  tomczak - jaegermann , `` restricted isometry property of matrices with independent columns and neighborly polytopes by random sampling , '' _ constructive approximation _ , vol .",
    "1 , pp . 6188 , 2011 , springer .",
    "e.  j.  cands , j.  romberg , and t.  tao , `` robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information , '' _ information theory , ieee transactions on _ , vol .",
    "2 , pp . 489509 , 2006 .",
    "j.  f.  determe , j.  louveaux , l.  jacques , and f.  horlin , `` somp - ns software package , '' version 0.3.2 , available online at https://opera-wireless.ulb.ac.be/owncloud/index.php/s/lhkmncgou66mtes or http://bit.ly/1qg0y1g .",
    "d.  l.  donoho , and m.  elad `` optimally sparse representation in general ( nonorthogonal ) dictionaries via @xmath5171 minimization , '' _ proceedings of the national academy of sciences _ , vol . 100 , no .",
    "5 , pp . 21972202 , 2003 .",
    "d.  l.  donoho , m.  elad , and v.  n.  temlyakov `` stable recovery of sparse overcomplete representations in the presence of noise , '' _ information theory , ieee transactions on _ , vol .",
    "1 , pp . 618 , 2006 .",
    "r.  gribonval , h.  rauhut , k.  schnass and p.  vandergheynst , `` atoms of all channels , unite ! average case analysis of multi - channel sparse recovery using greedy algorithms , '' _ journal of fourier analysis and applications _ , vol .",
    "14 , no . 5 - 6 , pp .",
    "655687 , 2008 .",
    "d.  needell , and r.  vershynin , `` signal recovery from incomplete and inaccurate measurements via regularized orthogonal matching pursuit , '' _ selected topics in signal processing , ieee journal of _ , vol .",
    "2 , pp . 310316 , 2010 .",
    "j.  a.  tropp , and a.  c.  gilbert , `` simultaneous sparse approximation via greedy pursuit , '' _ acoustics , speech , and signal processing , 2005 .",
    "proceedings.(icassp05 ) .",
    "ieee international conference on _ , vol . 5 ,",
    "pp . 721724 , 2005 .",
    "a.  m.  tillmann , and m.  e.  pfetsch , `` the computational complexity of the restricted isometry property , the nullspace property , and related concepts in compressed sensing , '' _ information theory , ieee transactions on _ , vol .",
    "2 , pp . 12481259 , 2013 ."
  ],
  "abstract_text": [
    "<S> this paper studies the joint support recovery of similar sparse vectors on the basis of a limited number of noisy linear measurements , _ </S>",
    "<S> i.e. _ , in a multiple measurement vector ( mmv ) model . the additive noise signals on each measurement vector </S>",
    "<S> are assumed to be gaussian and to exhibit different variances . the simultaneous orthogonal matching pursuit ( somp ) algorithm is generalized to weight the impact of each measurement vector on the choice of the atoms to be picked according to their noise levels . </S>",
    "<S> the new algorithm is referred to as somp - ns where ns stands for noise stabilization .    to begin with , a theoretical framework to analyze the performance of the proposed algorithm is developed . </S>",
    "<S> this framework is then used to build conservative lower bounds on the probability of partial or full joint support recovery . </S>",
    "<S> numerical simulations show that the proposed algorithm outperforms somp and that the theoretical lower bound provides a great insight into how somp - ns behaves when the weighting strategy is modified . </S>"
  ]
}