{
  "article_text": [
    "critical to understanding how information is processed in the brain is the form of the neural coding that underlies the storage and recall of memories .",
    "is there a local , or gnostic @xcite , code  colloquially called a grandmother - cell ( gm - cell ) representation  in which the firing of a single neuron ( or group of neurons ) exclusively codes recognition of a particular object , person or memory ? or is the code much more distributed ?",
    "although it is generally accepted [ e.g. , @xcite ] that gm representations are not used in reality , experiments @xcite often find localist responses by individual neurons .",
    "most dramatically , @xcite have recently found many neurons in humans that , within the limits of the measurements , behave like classic gm cells .    in this paper , we therefore quantitatively re - examine the viability of gm - cell representations , with the outcome that we refute the standard quantitative arguments against them , both theoretical and phenomenological .",
    "the information - theoretic argument is that gm representations need far too many neurons for the information coded @xcite .",
    "we show that this argument fails when one examines the information storage capacity of the synapses rather than the representational capacity of neurons for input stimuli .",
    "the standard efficiency argument applies only to the input representation , needed to represent any of the myriad possible stimuli . for storage",
    ", a gm representation can be optimally efficient .",
    "the phenomenological argument is that gm cells should fire in response to a much smaller fraction of stimuli than has been deduced from measurements of neural responses @xcite .",
    "a gm cell can be regarded as a categorizer , and the data appear to imply that any apparent gm cell responds to many categories of stimuli rather than to one category .",
    "however , our information theoretic argument shows that associated with any gm - cell population , with its ultra - low sparsity , is a more conventional population with a much higher sparsity .",
    "this two - population property , always a part of the gm - cell idea @xcite , was not allowed for in older analyses , including that @xcite by the group responsible for the new data @xcite .",
    "we devise a very general method of analyzing neural systems with multiple sparsities , and apply it to the data of @xcite .",
    "it enables us to quantify the biases against experimental detection of gm - like cells , most of which simply appear as unreported silent cells , and whose estimated numbers @xcite  may be a factor of 30 more than the reported cells .",
    "we find that the two - population property holds , and that less than @xmath1 of _ detected _ cells are in the distributed - code population : the vast majority of the cells can be gm - like .",
    "then we find that the number of categories coded by the gm - like cells can be @xmath0 or more .",
    "uncertainties are minor relative to the orders of magnitude involved .",
    "the data of @xcite therefore appear in strong quantitative agreement with the gm - cell hypothesis .",
    "the biases against detecting gm cells are enough to allow consistency with previous @xcite measurements and analyses that use a single - population model and that quantitatively argued against gm cells .",
    "we will examine other arguments against gm representations in the discussion section .",
    "inappropriate or excessively rigid definitions can exclude biologically interesting cases .",
    "for example , @xcite define a local representation as one where `` all the information that a particular stimulus or event occurred is provided by the activity of one of the neurons '' .",
    "the word `` all '' appears to exclude a system where a local representation codes the result of testing a distributed representation of a stimulus against remembered items .",
    "not all the available information is in the firing of the neurons in the local output representation .",
    "therefore in this section we present our definitions , and explain important features and consequences of the definitions needed for later sections .",
    "we define a set of cells to form a local or gm - like system when nodes of the system can be divided into groups of one or more nodes , and , to a good approximation , each group corresponds to one particular meaningful and distinct property of the stimulus input to the system .",
    "each group we call a gm group .",
    "typically we treat the properties corresponding to different gm groups as being mutually exclusive .",
    "we will usually identify the nodes with actual neurons , so that measurement in one of a gm group s cells of firing above a suitable threshold is strong evidence that the stimulus is associated with the corresponding property .",
    "but it is also possible that the nodes could be , for example , part of a dendritic tree .",
    "then the correspondence between neural firing and local coding might not be direct .",
    "in any case we can treat the system as a categorization system .",
    "in contrast , a distributed representation is formed by a set of cells where the categorization can only be determined from the activity of multiple cells / nodes and where the patterns of activity overlap between distinct properties , even when the properties themselves are mutually exclusive .",
    "[ [ classic - gm - cells - and - generalizations ] ] classic gm cells and generalizations + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the classic example of gm system is a facial identification system , where the firing of a particular gm group mediates the `` unitary perception '' @xcite of a retinal image as corresponding to a particular individual person . a characteristic property of a _ classic _ gm system is therefore that firing is exclusive between different gm groups  i.e. , only one gm group is active at a time .",
    "this corresponds to the fact that the individuals associated with the different stimuli are themselves completely distinct .",
    "of course , there will be situations where the gm group firing is not completely exclusive ; for example , if a particular stimulus is ambiguous , or if the picture of a face of one person is artificially morphed into that of another person .    our definition , however , was worded to allow certain natural generalizations from the case of classic gm cells .",
    "in particular , we will apply the terminology to declarative memories in general ( episodes , facts , etc ) .",
    "thus we could have a gm cell or gm group corresponding to each episodic memory . in this case",
    "it is evidently of practical importance for a memory to be recalled from a stimulus containing a few components of the original memory .",
    "since the same components could be part of other memories , the pattern of recognition firing need not be exclusive between memories .",
    "let us regard these patterns as priming the recall of the memories",
    ". full conscious recall of one particular memory requires some extra cues and modulation . with a gm - like memory system , non",
    "- exclusive priming recall would be at a relatively low level of firing above some threshold , with full recall involving exclusive firing at a much higher level .    in this case",
    "the exclusivity is not between the actual firing of different gm groups , but between the concepts corresponding to the groups .",
    "of course , the situation is a little more complicated for episodic memory , since episodes are happenings along a continuum in time .",
    "a memory cell for an episode corresponds to a small range in time .",
    "the exclusivity between different gm groups is between well - separated episodes , of which there are evidently a very large number .",
    "another natural generalization of the gm - cell concept is to local coding for output , with strong experimental evidence in the work of @xcite .",
    "[ [ high-level-gm-systems-v.low-level-local-coding ] ] high - level gm systems v.  low - level local coding + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we choose to restrict our use of the `` gm - cell '' terminology to the higher levels of neural processing . for lower levels , we will use the broader term `` local coding '' ; by this we mean coding by a set of cells each of which is responsive to some patch of stimulus space ( with fall off at the edges naturally ) .",
    "when the relevant properties of the stimulus are in a low - dimensional space  as for color or position in an environment  the collection of patches can give complete coverage .",
    "but when the relevant stimulus space is high dimensional , one can only expect local coverage of a minute fraction of possible stimuli  for example , to correspond to linguistic phonemes out of all possible auditory stimuli .",
    "there is considerable evidence that can be interpreted as supporting some kind of local coding below the highest perceptual levels .",
    "the key question for us is whether local coding is also used at the highest levels .",
    "[ [ internal - stimuli ] ] internal stimuli + + + + + + + + + + + + + + + +    input to a memory system need not correspond to actual physical events external to the brain .",
    "some input can be completely generated internally , as with an author planning a novel .",
    "memories of people and episodes in the novel have the same neural status as memories of real people and events .",
    "one big computational advantage of a gm - like system is the simplicity of information flow , fig .",
    "[ fig : gm - struct ] , so that the representation is easy to construct and manipulate  e.g. , @xcite .",
    "new memories are formed with new or unallocated neurons , so that they interfere minimally with old memories @xcite .",
    "the input to gm cells is from a distributed representation of the stimuli .",
    "output to downstream neurons using the categorization can be simply taken from a single gm cell .",
    "above - threshold firing of a gm - cell models a property of the cause of the stimulus .",
    "thus when firing is exclusive between different gm groups , this corresponds to exclusivity between the modeled properties of the associated external stimuli , like the identity of a person .",
    "it is therefore tempting to use exclusivity of firing between different gm groups as the primary measurable criterion for characterizing gm - like systems .",
    "but natural generalizations to declarative memory motivate us to relax this criterion .",
    "for example , with episodic memories , a stimulus may cause a response in nodes for those episodes having important commonalities with the stimulus .",
    "in addition , there can be multiple categorization systems , and there is no requirement of exclusivity between different systems .",
    "this is particularly clear at low levels in the processing hierarchy , where we could have separate local representations , for example , of the color and shape of an object .",
    "a collection of local representations of such relatively low - level features then forms a distributed representation of the whole object , suitable for input to the next level of hierarchical processing .",
    "note that we prefer to use the terminology `` gm - like system '' rather than `` gm representation '' to emphasize two aspects : the first is that if the gm - cell idea applies in its classic sense of recognition of individual people , it is likely to apply much more generally to all declarative memories .",
    "the second is that we wish to treat the firing of a gm cell as coding the result of a _ recognition computation _ from a stimulus .",
    "but the use of the word `` representation '' for a gm - like system would carry the connotation of representing the stimulus , which is not generally appropriate . if nothing else , local coding typically dramatically fails to cover the stimulus space . for example , consider a distributed input representation on 100 binary neurons , a small number compared with real sensory systems .",
    "there are @xmath2 distinct stimuli , many orders of magnitude larger than the total number of neurons in any brain .",
    "a practical local representation can only apply to a minute fraction of stimuli , presumably ones that are especially salient .",
    "a local representation can only provide full coverage for a stimulus space of very low dimension , like that for color .",
    "practical experiments only involve a limited number of stimuli and cells , and definitely do not give the detailed synaptic information that determines all possible causes of a cell s firing .",
    "thus it is non - trivial to distinguish a gm system from a distributed representation , when the distributed representation is sparse , and when we allow natural generalizations of the gm - cell idea .",
    "we illustrate the issues by comparing two very different computational memory models , one by @xcite , and one by @xcite ( bmw ) .",
    "[ [ hopfields - model ] ] hopfield s model + + + + + + + + + + + + + + + +    in hopfield s particular example , the input stimulus concerns properties of people , and the input representation is carried by a set of 1000 binary neurons .",
    "these are divided into 50 sets of 20 neurons .",
    "the 20 neurons in each set give a local representation of 20 possible values of a property of the stimulus .",
    "for example , to input the name of a person , one of a set of 20 name - coding neurons would be active .",
    "binary synapses connect every neuron to every other neuron .",
    "each stored memory is considered as the set of 50 property values for a particular person , and is coded in the state of the synapses .",
    "the synaptic strengths are set by a hebbian - like rule on presentation of stimuli .",
    "recall of a memory is caused by a stimulus that consists of partial data about an individual , i.e. , values for a subset of the 50 properties .",
    "memory retrieval results in completion of a partial stimulus to the full set of properties for the corresponding individual .",
    "there is no corresponding gm cell ; the model is a fully distributed memory system .",
    "[ [ bmw - model ] ] bmw model + + + + + + + + +    the basic bmw model is the simplest possible form of a gm system : a feed - forward perceptron with one intermediate memory - cell layer , fig.[fig : bmw ] .",
    "the memory cells are arranged to respond in a gm - cell style to recognize inputs that correspond to stored memories .",
    "the model gains its power by applying it to the situation that the input forms a sparse binary representation of stimuli .",
    "this would typically be obtained from the top of a processing hierarchy , as in fig.[fig : gm - struct ] .",
    "improvements in the model can be made , for example , by adding inhibitory interneurons to enforce a winner - take - all action in the memory - cell layer , but the simplest form of the model is sufficiently robust for our illustration .        to solve the same pattern - completion task as hopfield s model",
    ", we make the output cells identical to the input cells , thereby specializing from a heteroassociative memory system to a homoassociative system .",
    "memories are created by presenting a full stimulus to the system , and arranging for some unallocated memory neuron @xmath3 to match its synaptic strengths to the stimulus .",
    "the chosen neuron changes to a state of being allocated , with correspondingly greatly reduced synaptic plasticity .",
    "let @xmath4 denote the prototype pattern for memory neuron @xmath3 .",
    "then the strengths of the neuron s input synapses are set to a constant times the input pattern : @xmath5 , and similarly for the output .",
    "the constant @xmath6 can be scaled out of all of our formulae , but its use assists in relating the formulae to properties of real neurons . when another stimulus @xmath7 is presented , the response of neuron @xmath3 is obtained from a thresholded sum over its inputs :",
    "@xmath8 this formulation is for artificial analog neurons , but it is readily generalized to realistic spiking neurons , and the model is robust enough that minor variations do not affect the principles governing its performance .",
    "we now review how standard properties @xcite of sparse binary codes show that the system implements the pattern completion task if the threshold is set suitably .",
    "let a stimulus @xmath7 be presented ; it may be some full pattern , in which @xmath1 of the input neurons are active , or it may be a partial pattern .",
    "the input to neuron @xmath3 is @xmath9 , which is @xmath6 times the overlap between @xmath7 and the prototype pattern for memory @xmath3 , i.e. , it measures the number of common on - bits .    if the new stimulus is unrelated to memory @xmath3 , then its on - bits are random relative to those for memory @xmath3 .",
    "thus for a full pattern the typical overlap is @xmath1 of the total number of on - bits in a full patten , i.e. , @xmath10 , and less for a partial pattern .",
    "we therefore set the threshold intermediate between the input for the full prototype pattern , i.e. , @xmath11 , and the typical input for a full unrelated pattern , i.e. , @xmath12 .",
    "then unrelated patterns almost never cause neuron @xmath3 to fire . thus with sparse input patterns we have the well - known automatic orthogonalization against unrelated stimuli .",
    "but now consider an input corresponding to a subset of the features in memory @xmath3 s prototype pattern .",
    "for example , a quarter of the features could have been detected . in that case , the input to neuron @xmath3 would be @xmath13 .    hence with an appropriate threshold setting , we get firing of neuron @xmath3 in response to a subset of the features in its prototype pattern .",
    "this then causes firing of the whole set of output neurons that correspond to the prototype , i.e. , we have pattern completion .",
    "because the input to a memory cell is much bigger for patterns related to its prototype than for unrelated patterns , the operation of the system is robust against changes in the exact equations describing its operation .",
    "[ [ measurements ] ] measurements + + + + + + + + + + + +    to mimic a biological experiment , one could measure the responses of a sample of model neurons to a sample of stimuli . in hopfield",
    "s model if there are fewer than about 10 or 20 stimuli , the firing of many of the individual neurons would correspond to single people .",
    "thus the distributed nature of the memory representation would not be immediately apparent .",
    "but it is not necessary to enlarge the stimulus set to test this .",
    "data from a sample of neurons and stimuli give the fraction of stimuli to which neurons respond .",
    "then by using knowledge of the capacity of the system , which in this case is several hundred memories , one can statistically extrapolate from the sample to show that each neuron is responsive to multiple unrelated stimuli .",
    "as for the bmw model , the input / output cells would have the same kind response characteristics as all the cells of the hopfield model , responding to @xmath1 of the stimuli .",
    "but there are also gm memory cells , which respond much more rarely .",
    "if the sample data include at least a few responsive gm neurons , one can detect the different properties of the input and gm cells , and thereby distinguish the system from the hopfield model .    in this paper",
    ", we will construct a method to extrapolate in general from data with a sample of cells and stimuli to the whole system .",
    "the method enables us to compute which of the following properties of a set of putative gm cells is consistent with data : ( a ) the cells actually could code for single properties .",
    "( b ) each cell is expected to respond to multiple _ unrelated _ properties , after an extrapolation to the full set of possible stimuli .",
    "one confounding issue is that if one detects a response from a gm - like cell it is easy to misidentify the property corresponding to the cell s firing .",
    "if a cell only responds , within limited data , to a particular individual person , the cell could indeed be a classic gm cell corresponding to that person .",
    "but it could also be , for example , a gm cell for an episodic memory that contains the person . in that case",
    "it would respond to stimuli containing other components of the episode .",
    "the distinguishing feature of the most general kind of gm - like cell , but one that can be hard to test , is that apparently different stimuli that cause it to respond are in fact related .",
    "in contrast , with a purely distributed representation at the highest level , there is no high - level relation between , for example , the multiple people causing a particular cell to respond .",
    "the only relation is an identity of lower - level features in the input representation .",
    "the locality is at the feature level , not at the high level .",
    "we now quantify the information requirements for a memory or categorization system .",
    "first we distinguish the activity state and the storage state .",
    "the activity state is the pattern of firing of the neurons , and the storage state @xcite is the pattern of synaptic connectivity and strengths .",
    "furthermore , within the activity state of a memory system , we distinguish an input representation and a recognition representation .",
    "the input representation is of the current input stimulus , like a visual scene , while the recognition representation concerns which stored pattern ( if any ) corresponds to the current input .",
    "the immediate input to a memory system must be able to represent the relevant features of any possible stimulus , and not just those previously encountered stimuli for which there is a memory trace . here",
    "the standard arguments for distributed representations apply unambiguously , and a gm representation is not possible .",
    "the argument is simply that @xmath14 neurons code for at most @xmath14 exclusive properties in a gm system , but that they code for exponentially more with a distributed representation , for example @xmath15 with a simple binary code .    in general the immediate input to a memory or categorization system",
    "is not the raw sensory input but a highly processed representation of those high - level features that are relevant for the system s particular task .",
    "for example , input for face recognition could involve neurons coding for the presence , shape and position of eyes , nose and mouth , etc ; individual features could well be coded locally , but the collection of feature representations would always form a distributed representation .",
    "each neuron has a range of distinguishable firing rates , so that the raw information capacity in the activity of @xmath14 neurons is a few times @xmath14 bits .",
    "but robustness requires a certain amount of redundancy , and firing is often sparse , both of which reduce the information per neuron .",
    "measurements @xcite show that in hippocampal neurons about @xmath16 bits of independent information are coded per neuron and suggest that a few hundred neurons suffice for coding possible faces .",
    "for example , 300 neurons code about 100 bits , sufficient for about @xmath17 different categories of face , an entirely satisfactory number",
    ".    note that once an input representation is sufficiently small , pure representation efficiency is not a dominant consideration .",
    "issues of processing speed , metabolic efficiency , and algorithmic robustness can be more important .",
    "for example , sparse distributed representations appear to be favored @xcite , since they can give weak interference between memories during synaptic plasticity .",
    "the information capacity in the storage state , i.e. , in the synapses , was estimated by @xcite and by @xcite .",
    "there are two contributions , from the synaptic topology and from the synaptic strengths , giving a total of 5 to 10 bits of raw storage capacity per synapse .",
    "we divide by 10 , to provide a plausible allowance for redundancy .",
    "thus we need about one synapse for each bit of storage information .",
    "this calculation uses only very basic physical information about synapses and neural processing , so it is certainly accurate at the order of magnitude level . since measures of information in units of bits",
    "are independent of the physical implementation , the numbers are directly compared with those for ordinary digital computers , and experience with data processing and storage can be used to derive minimum numbers of synapses for a task .",
    "a human brain has around @xmath18 neurons and @xmath19 synapses per neuron .",
    "so its synapses store about @xmath20 bits , i.e. , about @xmath21{gbyte}$ ] , up to a factor of 10 or so .",
    "suppose the system has a repertoire of @xmath22 stored memories .",
    "each is an arbitrary association of a category to stimulus features .",
    "so we attribute to each memory @xmath23 bits of association information , which we term its semantics .",
    "this includes both input and output information .",
    "( e.g. , facial structure and name for a person , and , in fact , _ all remembered information about the person_. ) it is important to include output semantics in the associations , since they are what allow the retrieval of a particular memory to cause memory - specific behavior .",
    "these are quite arbitrary associations , for example of a linguistic name to a specific person , and without such output associations there is no purpose to storing a memory . the output associations do need to be stored , and they therefore contribute to the calculation of the minimum size of the system just like the input associations .",
    "the total association information is @xmath24 bits , so that approximately this number of synapses is needed .",
    "for example , with a repertoire of @xmath25 and an average of @xmath26 bits of information per item , we need about @xmath27 synapses .",
    "the measured number of synapses per neuron is around @xmath28 , thereby implicating about @xmath29 neurons , i.e. , a number proportional to the size of the system s repertoire ( and the information per memory trace ) .",
    "hence if the system uses @xmath22 gm cells for a recognition representation , this is at most a constant factor beyond the neurons needed to carry the storage synapses .",
    "moreover , if @xmath23 is larger than about @xmath28 , there is not even any overhead at all in using gm cells .",
    "we can characterize this by saying that if the memories are semantically rich , then a gm strategy for the recognition output can be optimally efficient , as in the model of @xcite .",
    "in contrast , if one ignored the storage requirement , one would assert that @xmath14 recognition neurons can code exponentially more recognized categories , e.g. , @xmath15 .",
    "it is important that in quantifying the information the term `` bit '' is used in the strict information theoretic sense .",
    "this means that if each memory were coded as an actual bit pattern , each of the @xmath30 possible patterns would be equiprobable .",
    "thus , on a computer , the bit count refers to an optimally compressed representation .",
    "however , only certain features of the input are relevant for tasks like face identification : a minimalist line drawing often suffices for unambiguous identification .",
    "so in computer terms the association bits are with respect to a representation that is both lossy and compressed . in the opposite direction",
    ", it can be difficult to perform computations on optimally compressed representations , and it is also difficult to measure accurately the probabilities of occurrences of different kinds of stimuli , since the number of possible stimuli far exceeds the number actually experienced .",
    "moreover redundancy ( in the information theoretic sense of using more than the minimum necessary number of bits ) is useful in giving robustness to a system .",
    "for all these reasons , we must expect the physical capacity of a system needed to code memories to be a substantial factor large than a minimal physical implementation of @xmath24 bits .",
    "nevertheless this measure is important in quantifying information in an implementation - independent way .",
    "it also enables us to estimate the information storage requirements by examining implementations of related tasks on a digital computer .",
    "well - known examples of simple line drawings and pictures of artificially low resolution show that the information to identify faces could be quite modest , if a suitable representation is used .",
    "but the synaptic size of the system also depends on the remaining association information , treated as output",
    ". this can be much larger in size , effectively amounting to a biography of the individual concerned in each memory .",
    "a number of objections to our bounds and ideas for evading them have been proposed , which we now answer .",
    "the general answer is simply that the information theoretic bound represents an absolute physical limit that it is impossible to exceed .",
    "all that is required is that we count the bits of information in the strictly correct information theoretic sense , and that we have identified the correct physical location of memory in the synapses .",
    "[ [ counting - memories ] ] counting memories + + + + + + + + + + + + + + + + +    does not the idea of quantifying memories as discrete items that can be counted carry the implication that we use a gm system ? are there not difficulties in counting memories in distributed memory systems ?",
    "in fact the classical kinds of distributed memory , e.g. , @xcite , are regarded as storing patterns , which can be counted .",
    "what we do have in mind is declarative , or explicit memory , the kind considered as prototypically hippocampal .",
    "here it is reasonably clear what is meant by a single memory : a picture , a scene , or the meaning of a word ; all of these are discrete .",
    "even with episodic memory , where there is a continuous time variable , we can observe that there is a correlation time within a continuous series of events . as regards storage requirements",
    ", we simply identify a single memory with the happenings within a correlation time , which is evidently of the order of seconds or minutes .",
    "of course , only a small fraction of these are stored in long term memory .",
    "we will not require great precision here .    in the contrasting case of implicit procedural memory",
    ", it is much less obvious what should be defined as a single memory item .",
    "but we are not concerned with this case .",
    "[ [ multiplexing ] ] multiplexing + + + + + + + + + + + +    could not one gain by allowing a neuron to respond to multiple different stimuli ?",
    "could not a single face - identification cell respond to either george bush , jennifer aniston or john hopfield , for example ?",
    "this multiplexing is just going in the direction of a distributed representation .",
    "our argument so far does not rule that out ; all it says is that this does not provide a way of reducing the synapse count .",
    "it is our statistical argument in later sections that enables us to estimate the degree of multiplexing .    now",
    "if a neuron responds to multiple categories , then there is interference at the neural level between different memory traces .",
    "an unambiguous categorization then requires the use of the firing information from more than one neuron . in the case of lightweight memories ,",
    "i.e. , with substantially less than 10000 bits of associations per memory , multiplexing of memories can indeed reduce the neuron count , and is allowed by our general argument . but with richer memories , there is no gain .    [",
    "[ representation-v.memory ] ] representation v.  memory + + + + + + + + + + + + + + + + + + + + + + + +    the memories we have discussed are typified by hippocampal memories .",
    "consider instead visual area v1 ; the number @xmath31 of distinguishable activation patterns is exponential in the number @xmath14 of neurons , e.g. , @xmath32 .",
    "an outside observer could identify different faces from the different activation patterns .",
    "why should we not regard this as memory , if we are to regard observation of ( much simpler ) activation patterns in gm cells as identifying faces , and as in fact part of a memory system ?",
    "the difference is not in the outside observations , but in the use the organism itself makes of the information .",
    "a memory is not useful unless it produces some consequence when recalled .",
    "what we mean , for example , is that seeing john hopfield on the other side of a street might induce us to cross the street and greet him by name .",
    "for the billions of other possible people , there would be either be a different response or no response at all .",
    "this is why we defined the associations for a memory to include output as well as input .",
    "there must be sufficient information stored to enable the relevant computation to be done from the activation pattern corresponding to the stimulus .    without being concerned with storage ,",
    "it is perfectly possible to compare one activation pattern with a previous one , to identify whether or not it has changed . but to compare it with patterns of activation for all the people one remembers , and to take appropriate actions , one needs appropriate storage .",
    "it is simply not possible to evade the fundamental physical necessities given by information theory .",
    "[ [ coding - commonalities ] ] coding commonalities + + + + + + + + + + + + + + + + + + + +    many remembered faces have features in common .",
    "can not this be used to reduce the number of synapses and neurons needed , by coding the common features in a special subsystem for the common features ?",
    "suppose we had a set of faces characterized by very short , dark , and curly hair .",
    "would there not be a gain by allocating a neuron to this combination of characteristics and using it instead of separate neurons for hair length , color and curliness ?",
    "if the different hair features were equiprobable and uncorrelated , the general argument would prevent any gain .",
    "as an example , suppose each of the three hair characteristics has 8 equiprobable values , for @xmath33 distinguishable combinations .",
    "if all the combinations were equiprobable , we would allocate 9 bits for the information content .",
    "probability here refers to a prior probability of occurrence , i.e. , before the creation of a memory trace .",
    "but if instead all the characteristics were perfectly correlated , then there would only be 8 combinations , which could be represented in 3 bits .",
    "when we construct memories , this gives a gain of a factor of 3 in storage if we only represent the combinations that actually occur in the input representation rather than all possible combinations .",
    "but this is exactly what is meant by using a correct measure of information to compute the minimum number of synapses .",
    "note that the task carried out by a memory system is not merely to identify the best fit to a current stimulus among stored memories ; for that very few bits are needed .",
    "for example , if a stimulus is represented by 100 bits , but only 8 memories are stored , then only @xmath34 suitably coded bits are needed to identify the stimulus , if it is assumed that the stimulus corresponds to one of the memories . but it is also necessary to identify the case that the stimulus fails to correspond to a stored memory , so that it is a candidate for a new memory .",
    "it is for this that the other 97 bits are needed .",
    "although our argument was used to show that gm coding can be optimally efficient in the use of synapses and neurons , the bounds on synapse and neuron number are independent of the coding method .",
    "it is therefore useful to verify that the bounds are obeyed by the polar opposite of gm systems , i.e. , by conventional distributed memory systems .",
    "calculations of the capacity of such systems have been made , e.g. , @xcite , and it can be checked that they do obey our bounds .",
    "but what appears to have been missed is that the capacity limit also removes the argument against using gm cells .",
    "hopfield s recent model @xcite provides an excellent example . in this model ,",
    "each stored memory corresponds to the values of each of 50 categories , with 20 possible values per category .",
    "this gives a total of @xmath35 bits of information .",
    "retrieval of a memory results in a neural representation of this information in the firing of 50 out of 1000 binary neurons , for a biologically realistic sparsity of @xmath1 .",
    "storage is in binary synapses with all - to - all connectivity on the 1000 neurons , for a total of slightly under @xmath36 bits of storage capacity .",
    "this implies that the system can store at most @xmath37 separate memories .",
    "in fact , the simulations in @xcite show that , with the algorithms used in that paper , performance noticeably degrades when about 250 memories , i.e. , a factor of 20 below the physical limit .",
    "thus the information - theoretic bounds are obeyed .",
    "the problem is that the memories are stored on the synapses connecting the active neurons in a particular memory .",
    "synapses overlap between memory traces , which causes interference if too many memories are stored .",
    "consider in contrast the bmw model @xcite , which , as observed by its authors , is optimal in its number of synapses .",
    "suppose for input and output we use the same 1000 neurons as in the distributed model . then to store @xmath14 memories we add @xmath14 memory neurons and @xmath38 synapses ( for input and output ) .",
    "we also may use a relatively few extra interneurons and synapses to implement winner - take - all dynamics .",
    "the number of synapses is double our minimum estimate , because we treat input and output semantics separately : they could be different .",
    "this is much more efficient than the distributed model .",
    "there is also an extra neurons for each memory .",
    "but we can readily increase the capacity for stored memories by increasing the number of memory neurons . with @xmath39",
    ", we would have the same number of synapses as in hopfield s model , @xmath40 more neurons , but double the capacity .    with the distributed model ,",
    "the capacity can be increased only either by simply duplicating the system , which is always a possibility , or by a change in architecture .",
    "an appropriate change in architecture would be to use the original input neurons to feed a separate layer which codes the same information more sparsely , after the style of a support vector machine .",
    "note that the bmw model reliably performs the same pattern completion task as hopfield s model .",
    "that is , a stimulus consisting of a few values is completed to the values of all categories for the corresponding memory .",
    "the bmw model essentially performs a comparison of the active bits in the sparse input pattern with the on - bits in the stored pattern . because of the @xmath1 sparseness of the input , the probability that an on - bit in the stimulus coincides with an on - bit in an unrelated stored pattern is also @xmath1",
    "once more than a few bits are examined , the probability of a chance coincidence is extremely small , with a correspondingly small misidentification probability .",
    "we have seen that the efficiency argument against gm cells disappears , especially for semantically rich objects , like most people s grandmothers . but",
    "the efficiency analysis for the input representation shows that the gm cell population is necessarily accompanied by a population of cells carrying a distributed code .",
    "experimental characterizations of the different kinds of coding can be made by measuring the sparsity of neural responses to stimuli .    by sparsity we mean , for each cell , the fraction of stimuli to which it responds .",
    "we assume that , as in @xcite , some threshold criterion is defined cell - by - cell as to whether a cell responds or not . thus the neuron is treated as binary .",
    "other definitions involving analog firing rates are possible , but we will not use them . note that",
    "with our definition and when all cells have the same sparsity , both the population and the lifetime sparsity are equal , unlike the case @xcite with other definitions of sparsity .    for a system with fully distributed coding",
    ", we expect to measure sparsities characteristic of the input and output representations .",
    "for example , in hopfield s model , the sparsity is exactly @xmath1 .",
    "more realistically , there will be a range of sparsity .",
    "the input and output cells of a gm system will have naturally have similar sparsities to those of all the cells in a distributed - memory system .",
    "but the gm cells must respond much more rarely .",
    "so with a gm system we expect there to be two very different populations of cells distinguished by one population having a dramatically smaller sparsity than the other . whether or not the two populations are in the same area of the brain is not determined by general arguments .",
    "but we will find that , in fact , the putative gm cells of @xcite do have an accompanying distributed - code population . for our purposes it will be unimportant whether the detected distributed - code population is the one that provides the actual input and output for the detected gm - like cells .",
    "expectations for the sparsity of gm cells can be provided in terms of the repertoire size of a system , which has a connection to behavioral data .",
    "there are two somewhat different kinds of memory system we will consider .",
    "one is typified by face recognition , where a recognized input is categorized into one of @xmath22 categories , corresponding to distinct persons ; recognition is exclusive between categories .",
    "then for a random sample of faces in the repertoire of the system the sparsity of the gm cells is @xmath41 , if we assume that each person is allocated the same number of cells .",
    "the second case is for declarative memory ( episodes , facts , etc ) .",
    "recall is by a stimulus containing a few components of the original memory .",
    "since the same components could be part of other memories , the pattern of recognition firing need not be exclusive between memories .",
    "let us regard these patterns as priming the recall of the memories",
    ". full conscious recall of one particular memory requires some extra cues and modulation . with a gm system ,",
    "non - exclusive priming recall would be at a relatively low level of firing above some threshold , with full recall involving exclusive firing at a much higher level .",
    "in a memory system , a typical stimulus can evoke multiple memories .",
    "if we let @xmath42 typify the number of memories evoked , then a given gm neuron is caused to fire by a fraction @xmath43 of stimuli .",
    "it is therefore convenient to define an effective repertoire size @xmath44 , so that the typical sparsity is @xmath45 .    from standard psychological data ,",
    "we envisage that @xmath46 is thousands to at least millions for interesting cases .",
    "in contrast , the distributed - code cells fire much more frequently ; this is known from data , and is necessary in order that this population can represent a sufficiently large number of stimuli .",
    "measurements of single - cell responses concern only a small fraction of cells and of all possible stimuli .",
    "so we will treat data as being from a sample over cells and stimuli , and deduce properties of the whole system : e.g. , the relative sizes of the cell populations and their sparsities , and hence the number of categories coded for by the gm population . in doing this",
    ", we will quantify , and hence compensate for , the strong biases against detection of gm cells .",
    "suppose we present a sample of @xmath47 stimuli that are randomly chosen from some broad class ( e.g. , pictures of famous people , images concerning movies that the subject has watched , pictures of buildings ) .",
    "any particular cell @xmath48 responds to some fraction of these , called the cell s ( lifetime ) sparsity @xmath49 .",
    "the number @xmath50 of stimuli that evoke a response by the cell is taken from a binomial distribution of mean @xmath51 : @xmath52 this simply corresponds to the probability that @xmath53 of the stimuli are in the response - causing class and @xmath54 are in the non - response - causing class , as regards cell number @xmath48 .",
    "these two classes of stimuli form fractions @xmath49 and @xmath55 of the whole set of stimuli .",
    "we now consider a sample of cells , thereby sampling the distribution of sparsity over cells , @xmath56 .",
    "this means that the fraction of cells with sparsity @xmath3 to @xmath57 is @xmath58 .",
    "then the probability of getting @xmath53 responses to the @xmath47 stimuli in some random chosen cell is obtained by integrating the single cell response with the sparsity distribution : @xmath59 this is a general result .",
    "the only necessary assumption is that the cells are randomly chosen out of some more global set of neurons ( e.g. , hippocampus ) and that the stimuli are randomly chosen out of some global class .",
    "the value of @xmath3 for a cell and the distribution @xmath56 depend both on the choice of stimulus class and on the choice of the threshold for a response . changing either",
    "will naturally affect the distribution .",
    "for the data we analyze , the response criterion is given in @xcite .",
    "if multiple sessions and multiple subjects are considered , eq.([eq : binomial.combo ] ) continues to apply , with @xmath56 being the distribution averaged over subjects .",
    "so this form is amenable for the analysis of aggregated data .",
    "observe also that the derivation of the formula does not require any assumption about the independence of the firing of different neurons : the formula is simply an average over all neurons in whatever area is being sampled .",
    "this allows the formula to be completely general , in contrast to the model of @xcite , which requires that neuron - neuron correlations be neglected .",
    "a common ansatz , as in @xcite , is to assume a fixed sparsity @xmath60 , i.e. , to set @xmath61 .",
    "such a model we term a single - population model .",
    "but for an analysis of a possible gm population , we must allow for at least two populations .    from a mathematical point of view , eq .",
    "( [ eq : binomial.combo ] ) expands @xmath62 in basis functions , with expansion coefficients @xmath56 .",
    "the significance to its use is four fold : ( 1 ) it relates the distributions for different pattern numbers @xmath47 via a common set of expansion coefficients @xmath56 .",
    "( 2 ) the expansion coefficients are non - negative .",
    "( 3 ) for a distributed - code population to represent all possible stimuli , efficiency is important , i.e , using the minimum of neurons .",
    "this will tend to maximize the sparsity subject to other constraints like keeping relatively low the metabolic costs @xcite of action - potential generation .",
    "thus we should expect the sparsity of the distributed - code population to vary over a fairly narrow range .",
    "( 4 ) any gm population has an extremely small sparsity , so that it populates just the bins with @xmath63 and @xmath64 responses to the @xmath47 stimuli .",
    "therefore the two population property leads to the qualitative expectation for @xmath56 that is shown in fig .  [",
    "fig : d](a ) .",
    "the distribution of responses @xmath62 can be regarded as a smeared version of the sparsity distribution @xmath56 with @xmath65 .",
    "given this smearing , a useful approximation is to replace the distributed - code peak by a delta function at some fixed typical sparsity , fig.[fig : d](b ) .    [ cols=\"^,^ \" , ]     a final piece of session - averaged data given in @xcite is the fraction of stimuli that produced a ( simultaneous ) response in at least two neurons . in in app .",
    "[ sec : evocative ] , we derive a formula for this quantity . as with the number of of evocative stimuli",
    ", a neglect of neuron - neuron correlations is needed .",
    "the results are shown in the last line of table [ table : comparison.session ] . as already observed in @xcite , the one - population model with their preferred value @xmath66",
    "gives a fraction @xmath67 that is rather below the data ( @xmath68 ) .",
    "a comparably bad fit is obtained by our two - population model .",
    "in fact , the bad fit happens quite generally .",
    "the formulae for both @xmath69 and @xmath70 given them in terms of a single property of the model , the cell - averaged sparsity @xmath71 . we show in app .",
    "[ sec : evocative ] , that when @xmath72 is not two large , the two quantities obey an approximate relation @xmath73 this relation is obeyed to useful accuracy in the model calculations in the last two lines in table [ table : comparison.session ] , but it is violated by a factor of two by the data .",
    "the derivation is not affected by adding yet more populations of different sparsities , but only by including neuron - neuron correlations .",
    "there are in fact two simple ways to overcome this problem .",
    "one is simply that one fraction is proportional to the number of detected units in a session , and the other is proportional to its square .",
    "since this number varied quite widely @xcite between sessions ( 18 to 74 ) , the session average of @xmath74 can not be replaced by the square of the average of @xmath14 .",
    "this could easily account for the factor of two mismatch .",
    "in contrast , the fraction of evocative stimuli is amenable to a simple average over sessions .",
    "the second possibility is from neuron - neuron correlations , to which @xmath70 is much more sensitive than the other observables .",
    "if there were a small fraction of nearby neurons that always fired in pairs , these would disproportionately contribute to @xmath70 , but not nearly as much to @xmath69 .    of course",
    ", both these suggestions can be tested by a closer examination of the data .",
    "we also examine how well the one - population model , with the parameters from @xcite , agrees with the data we used from the earlier paper @xcite .",
    "this is shown in table [ table : comparison.global ] .",
    "it can be seen that the observables we used are particularly sensitive to the differences between the models .",
    "a low _ average _ sparsity is necessary to keep the number of responsive neurons down to the experimental value . but with a one - population model this",
    "also implies that neurons with @xmath75 responses are many fewer than those with @xmath64 responses",
    ". moreover the number of cells with even more responses than 2 is minute , so that @xmath76 is close to its minimum value of 2 , whereas the data is much higher .",
    "this is a clear indication of the need for two populations of cells with very different sparsities .",
    "we conclude that working with the distribution of the number of responses by individual neurons ( or units ) , as we do , is preferable to the other distributions .",
    "the distribution @xmath77 is easy to work with , and data can be usefully aggregated over a whole experiment , given only that the number of stimuli is approximately the same in each session and that the stimuli are chosen at random in some large class .",
    "these conditions can be imposed by an experimenter .",
    "the model can always be systematically improved by changing the sparsity distribution , e.g. , by adding extra components .",
    "working with aggregate data keeps the sampling errors usefully low , and parameters for a model can be computed simply from properties of the aggregate data .",
    "other analyses of data , e.g. , @xcite , have reported that hippocampal facially responsive cells carry a distributed code as opposed to a gm - type code .",
    "see also the recent work of @xcite .",
    "these analyses might appear to contradict our calculation that distributed - code cells are a very small fraction , perhaps less than @xmath78 of the total .",
    "however , there is a strong bias against actually detecting gm - like cells . in this section ,",
    "we use our fit to the more recent data to quantify this bias , at least roughly , to determine whether there is consistency between our results and the earlier data .",
    "the primary issue is that experiments typically only report those cells that are actually detected to respond to at least one of the stimuli used .",
    "for example , in a paper documenting place cell @xcite state `` the electrode assemblies were advanced until one or more hippocampal complex - spike cells were isolated extracellularly . ''",
    "then they observe that cells that do not produce any detectable spikes `` are excluded from analysis here due to our lack of ability to detect them '' .",
    "since gm - like cells respond to a very small fraction of stimuli , the ones that respond to no presented stimulus , i.e. , the vast majority , are typically omitted from an analysis .",
    "the resulting bias can be seen in the data that we analyzed earlier .",
    "a minority of the detected cells in @xcite are in the gm - like class ( 43 out of 132 ) , even though we have shown that the gm - like cells can be in the vast majority ( @xmath79 or more ) .    with fewer stimuli ,",
    "the bias becomes even stronger , as in the data used by @xcite .",
    "they used 20 face stimuli , and the total number of facially responsive neurons was 14 .",
    "a rather higher sparsity was reported than our result .",
    "but this is partly because a different definition of sparsity was used , applied to the spike numbers rather than to a binary response criterion .",
    "furthermore the cells have considerably larger background firing rates than those in the new data .    despite the differences in cells and species ,",
    "we blindly apply our model to give a rough test of consistency . in our two - population model ,",
    "the fraction of gm cells in the _ detected _ cells is @xmath80 this is the probability that a cell is a gm - like cell _ conditional _ on the cell producing a detected response to one or more of @xmath47 stimuli .",
    "notice that the silent - cell ratio @xmath81 cancels in this formula ; we have a relation between numbers of different kinds of detected cell under different experimental conditions .",
    "we have estimates for the parameters of the model , so substituting @xmath82 predicts a detected gm - cell fraction of @xmath83 , i.e. , about 2 cells , in the set of cells investigated in @xcite .",
    "in fact , @xcite did report that two of their cells had a gm - like response .",
    "there is , of course , no significance to the fact that this number is exactly the value predicted : there are expected statistical fluctuations , and the measurements were done with different methods and in a different species than in @xcite .",
    "nevertheless it is very important that the previous report , viewed as evidence in favor of distributed representations , is completely compatible with gm cells being in the vast majority , with the parameters we have determined .",
    "the undetected gm cells simply appear to be silent within the experiment and are therefore classed as not facially responsive .",
    "statements about the neural representation being distributed apply only to ( most of ) those cells that the measurements actually detected , not to all cells in the relevant region of the brain .",
    "the results of @xcite clearly suggest the detection of grandmother cells in the classic sense .",
    "many other experiments have detected individual cells with strikingly specific responses ( e.g. , @xcite ) .",
    "therefore it is useful to hypothesize that some of these cells are indeed gm - like cells , even though the concept of gm - cell may need to be extended and modified .",
    "a purely experimental direct test of the idea needs too many stimuli to be practical , cf .",
    "so other arguments must be brought in , of which we have provided two .",
    "one uses an estimate of the actual storage requirements for a memory system .",
    "we showed that gm systems can be optimally efficient in the use of synapses and neurons .",
    "the usual efficiency argument applies only to the input representation , but now carries the implication that in a gm - like system there must be two populations of cells with widely different sparsities .",
    "our second argument is a method to analyze neural responses .",
    "a particular aim is to measure whether they are quantitatively consistent there being separate neurons coding for each recognized person , or , alternatively , for each individual declarative memory .",
    "our method enables one to determine whether or not individual cells necessarily code for multiple persons or memories .",
    "we derived a general formula eq .",
    "( [ eq : binomial.combo ] ) for the neural responses in terms of an underlying distribution of sparsity .",
    "our expansion is a new result and is applicable independently of any detailed theory or model of neural function .    in effect",
    ", the formula enables us to extrapolate from limited data to obtain the fraction to stimuli to which cells respond .",
    "it also allows us to compensate for the strong biases involved in detecting cells when sparsities differ by very large factors .",
    "thus we obtain valid estimates of the numbers of cells of different kinds .",
    "we thereby solve some of the issues raised by @xcite concerning the publication of data only about responsive cells .",
    "one primary remaining bias is that different neurons may have different electrical characteristics , with a consequent different maximum distance from the electrodes for detectability of spikes .",
    "but this is presumably a milder effect than that caused by orders of magnitude differences in sparsities .      from the data we find indeed that the two - population property is obeyed .",
    "not only does the ultra - low - sparsity population comprise the vast majority of cells in the brain regions concerned ( hippocampus , etc ) , but its sparsity can be in a range compatible with the hypothesis of a gm - like system : roughly @xmath84 with a repertoire of @xmath0 .    an important role is played by the many silent cells .",
    "it is obviously unreasonable to assume they have no function .",
    "but on the gm - cell hypothesis they naturally are to be interpreted as the majority of gm cells that are not relevant to the particular stimuli used in an experiment .",
    "the large number of these cells is what enables one to overcome the strong biases against detecting a response of any one gm cell to a limited set of stimuli .",
    "now the group responsible for the analyzed data argue @xcite that their data do not support the gm cell idea . in @xcite , they say `` if we assume that a typical adult recognizes between 10,000 and 30,000 discrete objects ( biederman , 1987 ) , @xmath85 implies that each neuron fires in response to 50  150 distinct representations . '' [ @xmath86 should be replaced by @xmath60 in the notation of the present paper . ]",
    "however their analysis assumed a single value of sparsity .",
    "while this is a suitable approximation for conventional mechanisms of distributed memory , it is very bad for gm - like systems .",
    "even though the explicit aim of @xcite was to test the gm - cell hypothesis , the use of a single sparsity in effect imposed an assumption that the hypothesis is wrong .",
    "we showed that the single - population hypothesis is a bad fit to the data .",
    "since our expansion ( [ eq : binomial.combo ] ) is very general , the fault is in the single - population hypothesis not in any assumption about neural properties . the rather low value of sparsity given by waydo et al .",
    "is merely a compromise between the widely different sparsities of the two populations .",
    "our results are consistent with an even higher number of recognized objects than in the estimates of @xcite . indeed ,",
    "even _ without _ allowing for the silent cell correction , our fits allow a gm - cell population with a sparsity of @xmath87 corresponding to a number of objects not far from the lower edge of biederman s range .",
    "note that our basic estimate of the number of categories , @xmath0 , assumes that the cells are classic gm cells , each responding to a single individual person .",
    "but the number of categories could be substantially higher .",
    "if the cells are general memory cells , in the style of the model of @xcite , they could respond to images of several people .",
    "it could also be that more familiar stimuli , with richer associations , have more cells . in that case measurements with familiar stimuli , as is the case in the data , would be biased towards these memories with unusually large numbers of cells , with a corresponding reduction in our estimate of the number of categories compared with the true number .      in gm systems , like the model of @xcite ,",
    "the number of gm cells is very much larger than that of the input cells , as is consistent with the numbers we have deduced .",
    "an immediate implication is that each gm cell receives input from a modest number of input cells , but that each input cell sends output to a much larger number of memory cells .",
    "given also our finding that the gm cells in the relevant regions are in the vast majority , there are some striking anatomical implications .",
    "in fact striking disparities in synapse number are well known in the hippocampus @xcite : for example , each ca3 pyramidal cell gets about 50 input synapses from dentate granule cells , while other connections have tens of thousands of synapses .",
    "note that hippocampal neurogenesis results in dentate granule cells , highly appropriate if they are gm - like . however , general - purpose memories need a wider variety of ( processed ) input than does a face recognition system , and hippocampal - related regions are sufficiently complex that the real picture is undoubtedly much more complicated .",
    "even so , a careful analysis of the disparities in synapse number should provide critical information on neural function and the viability of gm - like systems .",
    "other less quantitative arguments have been advanced against the reality of gm systems , e.g. , @xcite .",
    "for example , distributed memory systems are said to be robust against partial destruction , since there is no single location for a single memories .",
    "but we do know that memories disappear .",
    "if there are multiple gm cells for a memory in different places , then we can overcome the robustness argument by simple redundancy .",
    "moreover memories form a network of knowledge , so that individual items of semantic memory can be readily reconstructed from other knowledge .",
    "episodic memory is really an ordered sequence of individual episodes , not necessarily remembered at all precisely .",
    "any one episode that disappears can be approximately filled in from neighboring episodes .    distributed memory systems are also said to be good at filling in missing parts of input data , as in reconstructing a full remembered image from a stimulus containing only a part of the image . but",
    "this property can also be true for gm - like systems .",
    "for example , when the bmw architecture @xcite is used with a sparse input representation and suitable dynamics for its gm cells , it also performs pattern completion ; the completion property is actually associated with properties of sparse representations used for input data .",
    "it has been said that new memories are harder to construct in gm systems than in distributed - memory systems .",
    "but now that adult neurogenesis in the hippocampus is well established , it may well be that there is actually a pool of new neurons available for at least some uses that could include being gm - like cells for new memories .",
    "the new neuron rate may however be excessively small .",
    "in addition , it is possible that the gm nodes are on dendritic tree rather than being whole neurons .",
    "it is known that there can be substantial changes in dendritic topology , which could easily include the formation of new nodes . here",
    "the fundamental mode of operation is of a gm - like system while the neural code of memory neurons takes on some of the aspects of distributed memory . in any case , there are potential realistic mechanisms for the formation of new gm nodes , so that there is no insuperable obstacle here .",
    "jcc is supported in part by the u.s .",
    "he also thanks larry abbott and the volen center at brandeis university for hospitality during the initial stages that led to this work .",
    "dzj is supported by the alfred p. sloan fellowship and by the huck institute of life sciences at penn state .",
    "we thank larry abbott , jayanth banavar , gong chen , anne graybiel , john lisman , and michael wenger for useful conversations .",
    "our model for the statistics of neural firing has two cell populations : one that uses a conventional distributed code with a single sparsity @xmath86 and a second gm - cell population , as illustrated in fig .",
    "[ fig : gm - model ] .",
    "these from fractions @xmath88 and @xmath89 of the total number of cells .",
    "a remaining population of cells does not respond to any stimuli at all in the class used in the experiment .",
    "we let @xmath90 be the fraction of the images used which have stored representations in the gm population , we let @xmath22 be the repertoire of the gm cells , and we let @xmath42 be the typical number of categories ( or gm groups ) evoked by a stimulus in the system s repertoire . as before ,",
    "we let @xmath44 .",
    "suppose first we record from some random cell known to be in the distributed population .",
    "we have seen that when we present @xmath47 images , the probability of getting @xmath53 responses is approximately the poisson distribution in eq .",
    "( [ eq : single.cell ] ) with @xmath91 .",
    "if , instead , we pick a gm cell , then for each individual image it has a probability @xmath92 of responding .",
    "therefore over a set of @xmath93 unrelated images it has a probability @xmath94 of responding exactly once . there is a negligible probability of @xmath75 for such a cell .",
    "finally , if the cell is outside the above two populations , it is silent in the experiment and always gives @xmath63 .",
    "summing over the distributions of @xmath53 for cells of the different kinds , weighted by their fractional population size , gives @xmath95         \\dfrac { f_{\\rm gm } \\ , k \\ ,",
    "p } { r_{\\text{eff } } }         + f_{\\rm d } \\,pa   \\ , e^ { -pa }           & \\text{if $ n=1 $ , }       \\\\[2 mm ]           f_{\\rm d } ( pa)^n e^ { - pa } \\dfrac { 1 } { n ! }           & \\text{if $ n\\geq2$. }       \\end{cases}\\ ] ] this is of the form of the general distribution eq.([eq : binomial.combo ] ) with @xmath96 and with the poisson approximation approximation for the distributed - code population and with the gm approximation that the gm - like cells fire so rarely that their responses for @xmath75 can be neglected .",
    "there are several meaningful parameters for the gm population , but only one combination affects the distribution @xmath62 .",
    "so we define @xmath97 , to find @xmath98         \\dfrac { p } { \\hat{r } }         + f_{\\rm d } \\,pa   \\ , e^ { -pa }           & \\text{if $ n=1 $ , }       \\\\[2 mm ]           f_{\\rm d } ( pa)^n e^ { - pa } \\dfrac { 1 } { n ! }           & \\text{if $ n\\geq2$. }       \\end{cases}\\ ] ] the parameter @xmath99 has the meaning that if a cell is outside the distributed - code population then it responds to a stimulus in the chosen global class with probability @xmath100 \\simeq 1/\\hat{r}$ ] , where the last approximate equality applies in the realistic case that @xmath101 is small , according to our fit .",
    "we wish to extract the properties of the distributed - code cells without contamination from the gm cells . for",
    "that we need properties of the distribution for @xmath75 , for which we use the probability and the mean number of responses .",
    "the probability of @xmath75 is @xmath102 .\\ ] ] the mean number of responses , in cells with @xmath75 , is @xmath103 these last two equations suffice to determine @xmath86 and @xmath88 from the data in @xcite  see eqs .",
    "( [ eq : fd ] ) and ( [ eq : a ] ) .",
    "several further observables are considered by @xcite .",
    "these observables refer to a session in which @xmath104 stimuli are presented to @xmath105 cells or @xmath14 units .",
    "one observable is the number of neurons @xmath106 that respond to at least one stimulus .",
    "its average is just the number of cells or units times the probability that one cell or unit responds , which in our notation is @xmath107 , in the notation of eq.([eq : binomial.combo ] ) .",
    "hence the number of responsive units in one - population model of @xcite is @xmath108 in our two - population model it is @xmath109 we are not quite sure how many cells correspond to each unit in the new data , so in table [ table : comparison.session ] we gave results for several choices of the ratio of cells to units , @xmath110 : 2 ( as we estimated for the earlier data ) , 2.5 , and 3 .",
    "a second observable is the number @xmath69 of stimuli that evoked a response in at least one neuron in the session . to derive this from the response distributions requires a further assumption that correlation between the firing of different detected neurons can be neglected .",
    "now the probability of one stimulus evoking no response in any of @xmath14 independent cells ( or units ) is @xmath111 , where @xmath112 is the probability of no response in one cell / unit on presentation of 1 stimulus .",
    "hence the average number of evocative stimuli in a session is @xmath113.\\ ] ] from eq .",
    "( [ eq : binomial.combo ] ) , we find that in general @xmath114 , where @xmath71 is the sparsity averaged over cells . in the one - population model of @xcite",
    "we therefore get @xmath115,\\ ] ] while in our two - population model it is @xmath116.\\ ] ] the appearance of @xmath81 in this last formula is misleading : the dependence on @xmath81 of @xmath99 and @xmath101 in our fit cancels the explicit factor of @xmath81 .",
    "we have used the number of cells @xmath105 in this formula rather than the number of units @xmath14 , since our fit is made with respect to cells .",
    "a final observable we consider is the number of stimuli in a session that evoked responses in 2 or more cells / units .",
    "this quantity , denoted @xmath70 can be obtained from the distribution of the number of neurone responding to a single stimulus : @xmath117^{n_{\\text{r } } } } { n_{\\text{r } } } e^{-n\\,(1-\\bar{\\alpha})},\\end{aligned}\\ ] ] it follows that on average @xmath118.\\ ] ]    from eqs .",
    "( [ eq : s.r ] ) and ( [ eq : s.r.nr.ge.2 ] ) , we get a relation between @xmath69 and @xmath70 valid when @xmath119 is less than about unity and @xmath14 is substantially larger than unity .",
    "we expand the powers of @xmath120 for small @xmath71 to obtain @xmath121 and then @xmath122        d.g .",
    "amaral , n. ishizuka , and b. claiborne , `` neurons , numbers and the hippocampal network '' , in progress in brain research vol .",
    "83 , ed .",
    "j. storm - mathisen , j. zimmer and o.p ottersen , pp .",
    "111 ( elsevier , 1970 ) .      c.a .",
    "barnes , b.l .",
    "mcnaughton , s.j.y .",
    "mizumori , b.w .",
    "leonard , and l .- h .",
    "lin , `` comparison of spatial and temporal characteristics of neuronal activity in sequential stages of hippocampal processing '' , progress in brain research vol .",
    "83 , ed .  j. storm - mathisen , j. zimmer and o.p ottersen , pp . 287300 ( elsevier , 1970 ) .",
    "henze , z. borhegyi , j. csicsvari , a. mamiya , k.d .",
    "harris , and g. buzski , `` intracellular features predicted by extracellular recordings in the hippocampus in vivo '' , j. neurosci .  * 84 * , 390 ( 2000 ) .",
    "olshausen and d.j . field , `` how close are we to understanding v1 ? '' , neural computation * 17 * , 16651699 ( 2005 ) ; `` what is the other 85% of v1 doing ? '' , in `` 23 problems in systems neuroscience '' , j.l .",
    "van hemmen and t.j .",
    "sejnowski ( eds . ) ( oxford university press , 2006 ) ."
  ],
  "abstract_text": [
    "<S> quian quiroga et al .  [ nature * 435 * , 1102 ( 2005 ) ] have recently discovered neurons that appear to have the characteristics of grandmother ( gm ) cells . </S>",
    "<S> here we quantitatively assess the compatibility of their data with the gm - cell hypothesis . </S>",
    "<S> we show that , contrary to the general impression , a gm - cell representation can be information - theoretically efficient , but that it must be accompanied by cells giving a distributed coding of the input . </S>",
    "<S> we present a general method to deduce the sparsity distribution of the whole neuronal population from a sample , and use it to show there are two populations of cells : a distributed - code population of less than about 5% of the cells , and a much more sparsely responding population of putative gm cells . with an allowance for the number of undetected silent cells , </S>",
    "<S> we find that the putative gm cells can code for @xmath0 or more categories , sufficient for them to be classic gm cells , or to be gm - like cells coding for memories . </S>",
    "<S> we quantify the strong biases against detection of gm cells , and show consistency of our results with previous measurements that find only distributed coding . </S>",
    "<S> we discuss the consequences for the architecture of neural systems and synaptic connectivity , and for the statistics of neural firing .    </S>",
    "<S> and    neural representations ; grandmother cells ; distributed coding ; </S>"
  ]
}