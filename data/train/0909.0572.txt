{
  "article_text": [
    "hits ( _ hypertext induced topic search _ ) is a ranking algorithm introduced by jon kleinberg in 1998 @xcite that utilizes web graph s hyperlink structure to create two metrics associated with every page .",
    "the first metric , authority , determines page s popularity , and the second metric , hub , is used to find portal pages , pages that link to popular ( thus useful ) pages . because it is easy to create many hyperlinks on a page to boost its hub score ( thus can increase authority scores of other pages that are pointed to by it ) , hits is susceptible to _ link spamming _ problem .",
    "hits is usually being compared to pagerank @xcite , a popular ranking algorithm used by google that also uses the hyperlink structure to create a popularity measure .",
    "both algorithms were breakthrough achievements at that time because unlike previous methods which usually use page s contents , these algorithms take different approach by utilizing the hyperlink structure to measure page s values . however , there are two main differences that should be enlisted here . _",
    "first _ , while hits produces two metrics , pagerank only produces one , the popularity measure .",
    "_ second _ , unlike pagerank , hits is _ query - dependent _ ; for every incoming query the algorithm first finds relevant pages ( usually by matching terms in the query with the contents ) , builds neighborhood graph , and then calculates authority and hub scores for every page in the graph .",
    "the neighborhood graph is built by not only taking the relevant pages as the vertices , but also other pages that either point to or are being pointed to by the relevant pages .",
    "this expansion step allows semantic association to be made and usually solves synonym problem @xcite .",
    "unfortunately it also creates famous problem associated with hits ; _ topic drift _ , authoritative yet irrelevant pages are likely to be also included @xcite .",
    "the link spamming problem can be alleviated by giving only fractional weights to edges from mutually reinforcement hosts @xcite .",
    "the topic drift can be mitigated by computing the relevancy between the query and the pages in the neighborhood graph @xcite ; the more similar the pages to the query , the more weights they have .",
    "so the influence of less relevant pages can be reduced .",
    "the query - dependence is considered to be the most problematic aspect of hits because authority and hub vectors have to be calculated online and real time for every incoming query , thus consuming too much computational , memory , and network resources .",
    "this problem can be handled by modifying hits to be query - independent ; taking the entire web graph as the neighborhood graph and calculating a global authority and a global hub vector @xcite .",
    "however , some crucial problems faced by pagerank in the early development like storage issues , memory management , tasks division , parallelization strategies , and computational methods must be addressed before this task becomes possible .",
    "fortunately , mathematically query - independent hits ( qi - hits ) resembles pagerank @xcite , so it can be expected that infrastructures and methods built for pagerank can be adopted to qi - hits .",
    "the challenge of accelerating qi - hits is not a trivial problem .",
    "there are several good reasons to put some efforts on it .",
    "_ first _ , qi - hits has some nice properties : ( 1 ) like pagerank , it can be calculated offline so the system does nt have to deal with every incoming query and some resources can be saved .",
    "( 2 ) unlike pagerank , it gives two measures ; authority scores for finding popular pages and hub scores for finding portal pages . and",
    "( 3 ) it solves completely the topic drift problem and slightly reduces the link spamming problem .",
    "_ second _ , as the web graph is growing rapidly the needs for faster methods are inevitable .",
    "for example to keep the freshness of web indices , to save the resources , and to build personalized and topic - sensitive schemes as in the pagerank case @xcite , among others .",
    "yet to the best of our knowledge , the researches on accelerating hits are hardly known , partly because hits is originally query - dependent ; the sizes of the neighborhood graph ( generally about 1000 - 5000 pages in 1998 @xcite ) are much smaller than the size of the web graph .",
    "thus , faster and more memory - intensive methods based on matrix inversion or decomposition , especially for sparse symmetric systems can be used @xcite . and",
    "for qi - hits ( where the problem s scale is as enormous as the pagerank s ) , some techniques to accelerate pagerank computations like extrapolation methods @xcite , blockrank algorithm @xcite , gauss - seidel method @xcite , and reordering methods @xcite can be adopted because both algorithms involving solving dense vector @xmath0 sparse matrix operations @xcite .    in this paper",
    ", we propose a different approach to accelerate the hits algorithm .",
    "unlike other methods where the acceleration is gained by using techniques borrowed from linear algebra ( extrapolation and gauss - seidel ) , exploiting adjacency matrix sparseness ( reordering ) , or utilizing nested block structure of the web graph ( blockrank ) , this method makes use of the hits definition itself .",
    "so , it can be called _ definition - based acceleration method_.    the proposed algorithm introduces two diagonal matrices , @xmath1 and @xmath2 , which contain constants associated with authority and hub scores respectively for every page that act as weights to make authority pages more authoritative and hub pages more hubby . because in the web graph authoritative pages tend to be pointed to by hubby pages , and hubby pages tend to point to authoritative pages , the constants will make these pages collect their scores faster and the stationary distributions ( authority and hub vectors ) can be reached with less iteration steps . because this approach makes use of the authority and hub pages , it can be expected that its performance will be better in a graph with considerable proportions of authority and hub pages than in a graph with uniform degree distributions .",
    "as shown in previous works @xcite , the web graph does have power law distributions for both indegrees and outdegrees , so authority and hub pages exist .",
    "note that we concern only the performance gained when the approach is applied in conjunction with the power method because in the scale of web graph ( 1 trillion unique urls in july 2008 based on google report ) only matrix - free iterative methods like the power method , jacobi , or gauss - seidel are feasible to be implemented . out of these methods , the power method is preferable because it is the simplest , needs less memory , and is linearly scalable to the problem s size .",
    "further , some promising techniques like reordering and blockrank are based on the power method .",
    "some methods to accelerate the pagerank computations that can also be implemented in qi - hits with some modifications are discussed in this section .",
    "haveliwala @xcite suggests using induced ordering from the pagerank vector rather than the residual as the stopping criterion , and shows in 24-million - page stanford webbase archive dataset the ordering induced by only 10@xmath3 iteration agrees fairly well with the ordering induced by 100@xmath3 iteration for query specific case . and",
    "in the case of global ordering only 25 iterations are needed .",
    "arasu et .",
    "@xcite propose using gauss - seidel method instead of the power method .",
    "this method immediately updates the entries of the current iteration vector as they become available , thus it clearly converges faster than the power method .",
    "a nice thing about this method is its formulation resembles the power method s , so it can easily be implemented with small modifications to the system .",
    "kamvar et .",
    "introduce two extrapolation methods to accelerate the pagerank computations @xcite .",
    "the methods assume the pagerank vector can be written as linear combination of the eigenvectors of the google matrix , a stochastic and primitive version of the adjacency matrix induced from the web graph .",
    "because the vector is the principal eigenvector @xcite of the google matrix , the pagerank convergence can be speeded up by subtracting some subdominant eigenvectors from the current iteration vector .",
    "the first method , aitken extrapolation , uses successive intermediate vectors to estimate the second eigenvector , and subtracting it from the current iteration vector .",
    "the second method , quadratic extrapolation , estimates not only the second but also the third eigenvector , and subtracting them from the current iteration vector .",
    "it has been shown that the quadratic extrapolation is better than the aitken extrapolation not only because this method substracts more error , but also there are cases where the second and the third eigenvectors are the repeated vectors , so significant improvement can only be achieved by subtracting both vectors from the current iteration vector .    in their next work ,",
    "kamvar et .",
    "al .  propose a very promising aggregation method called blockrank that speeds up the computation of pagerank by a factor of two times in realistic scenarios @xcite .",
    "this method works well because the web graph has a nested block structure ; most pages within a host intralink to other pages within the host , and only a few are interhost links .",
    "the method first calculates local pagerank vector for each host by ignoring the interhost links . and a global pagerank vector of hostgraph ,",
    "a graph created by taking the hosts as the vertices and the interhost links as the edges , is calculated .",
    "the global pagerank vector is then used to weight the corresponding local pagerank vectors .",
    "the result is taken as a starting vector for the standard pagerank algorithm .",
    "because of this locality approach , blockrank favors parallelization scheme , thus is very suitable to be implemented in the real condition .",
    "reordering @xcite is another very promising method to speed up the pagerank calculations , both in the costs per iteration and the number of iterations .",
    "the improvement achieved by this method can not be worse than the original algorithm , and in some datasets , the speedup can reach a factor of 5 times @xcite .",
    "this method works by exploiting dangling pages , pages with no outlink that usually make up over 80% of the webpages . in the adjacency matrix representation",
    "the dangling pages are the zero rows , so they look alike and can be lumped together into a teleportation state .",
    "consequently , the problem turns into solving pagerank for nondangling pages @xcite .",
    "more recent work by langville and meyer @xcite provides linear algebra approaches to this problem .",
    "they suggest reordering adjacency matrix so that the rows corresponding to the dangling pages are placed at the bottom of the matrix , then the pagerank vector is computed only for nondangling portion .",
    "the scores of the dangling pages are recovered by using the vector of nondangling pages and forward substitution .",
    "[ table1 ]    we will first review the definition and mathematical model of hits before deriving the proposed algorithm formulation .",
    "hits is defined with the following statement : _ authority score of a page is the sum of hub scores of others that point to it , and hub score of a page is the sum of authority scores of others that are pointed to by it _",
    "this is a circular statement , the authority scores depend on the hub scores and vice versa . to solve it",
    ", every page must be given initial scores , and final scores are computed by successively repeating the summing processes with normalization until a predefined criterion is satisfied .",
    "the following equation gives the formulation of hits : @xmath4 for @xmath5 , where @xmath6 denotes the final iteration where the predefined criterion is satisfied , @xmath7 and @xmath8 denote the authority and the hub score of page @xmath9 at iteration @xmath10 , @xmath11 is the set of pages that point to @xmath9 , and @xmath12 is the set of pages that are pointed to by @xmath9 .    as shown in eq .",
    "[ eq1 ] , hits simply calculates the authority ( hub ) score of a page by adding hub ( authority ) scores of other pages that point to ( are pointed to by ) it .",
    "this original formulation misses an underlying important aspect of the preferential attachment in the web graph ; _ the portal pages ( pages with many outlinks ) tend to point to the popular pages ( pages with many inlinks ) , and the popular pages tend to get many new inlinks_. this preferential attachment is the main reason behind the skewed distributions of indegrees and oudegrees as reported in many experiments @xcite , and because the authority ( hub ) score of a page is correlated to its indegree ( outdegree ) @xcite , it can be expected that the stationary distributions of the authority and the hub scores are also skewed .",
    "we confirm this fact empirically by calculating the similarities between authority vs.  indegree vectors and hub vs.  outlink vectors . table [ table1 ] shows the results , while cosine criterion measures the distance between two vectors , spearman correlation measures the similarity between orderings induced from the vectors ( see section [ datasets ] for details about the datasets ) .        usually , a uniform distribution is used as the starting vector .",
    "thus , the distances between initial and final scores are not uniform . for some very authoritative and hubby pages ,",
    "it takes more iteration steps to reach the final scores .",
    "this is also true for pages that have very low final authority or hub scores .",
    "[ fig1 ] describes such condition ; the distances between initial and final scores of pages that ordered in the top and bottom are greater than pages in the middle positions .",
    "the proposed algorithm is formulated to deal with this problem .",
    "it measures the distances between initial and final scores , and sets the convergence velocities proportional to the distances .",
    "as stated earlier the final authority and hub scores can be roughly approximated by using indegree and outdegree distributions , so it will be utilized to create constants that determine the convergence velocities .",
    "let @xmath13 and @xmath14 be the constants associated with the authority and hub score of page @xmath9 , and make some observations before writing down the formulations .",
    "clearly , @xmath13 must be bigger than @xmath14 if @xmath9 has many inlinks than outlinks , @xmath14 must be bigger than @xmath13 if @xmath9 has many outlinks than inlinks , and @xmath13 must be equal to @xmath14 if @xmath9 has the same number of inlinks and outlinks . also , the addition of a new link to pages with small number of links should have greater impact than highly connected pages . by using these observations",
    ", we define the constants as : @xmath15 where indeg@xmath16 , outdeg@xmath16 , and deg@xmath16 denote the indegree , outdegree , and degree of @xmath9 respectively , and @xmath17 denotes the absolute value of @xmath18 . and the proposed algorithm is defined with the following equation : @xmath19    as shown above , @xmath20 and @xmath21 are weighted with @xmath22 and @xmath23 respectively . because @xmath23 and @xmath22 are proportional to indeg@xmath16 and outdeg@xmath16 which in turn are likely to be proportional to the distances between final and initial authority and hub vectors , these constants tend to make the portal and popular pages and also the pages located in the bottom positions collect their scores faster as the iteration steps progress . thus , it can be expected that the proposed algorithm will converge faster than hits .    the proposed algorithm will be represented in matrix notation for some reasons : ( 1 ) to simplify the formulation , ( 2 ) to allow graph properties being seen in linear algebra perspective , ( 3 ) to compare its formulation to the hits s and pagerank s , ( 4 ) to allow other acceleration methods stated previously be applied with ease , and ( 5 ) to analyze the convergence property ( see section [ convergenceanalysis ] ) .",
    "let @xmath24 be the adjacency matrix of the web graph where @xmath25 is 1 if there is a link from @xmath9 to @xmath26 , and 0 otherwise , @xmath27 , @xmath28 , and @xmath29 is the number of pages in the web graph .",
    "thus , the proposed algorithm can be rewritten as : @xmath30 where @xmath31 is 1@xmath32 authority vector and @xmath33 is 1@xmath32 hub vector .",
    "algorithm 1 , 2 , and 3 are used to calculate qi - hits , the proposed algorithm , and pagerank respectively , where @xmath34 denotes 1-norm of @xmath18 , @xmath35 denotes the desired residual level , @xmath36 denotes the 1@xmath32 pagerank vector , @xmath37 denotes the diagonal outdegree matrix , @xmath38 denotes @xmath391 dangling vector where its @xmath40 ( @xmath41 ) entry is 1 if @xmath42 is a dangling page and 0 otherwise , and @xmath43 denotes a scalar that controls the proportion of time a random surfer follows the hyperlinks as opposed to teleporting .",
    "algorithm 3 is adopted from works by langville .",
    "detail discussions can be found in @xcite .      in qi - hits , there are two dense vector @xmath0 sparse matrix operations for each iteration step . because @xmath24 contains only either 1 or 0 ,",
    "the cost of each multiplication is _",
    "nnz_(@xmath24 ) additions , where _ nnz_(@xmath24 ) denotes the number of nonzero of @xmath24 .",
    "and the normalization step needs @xmath29 multiplications .",
    "thus , qi - hits needs @xmath29 multiplications and 2__nnz__(@xmath24 ) additions per iteration .    in the proposed algorithm , @xmath44 and @xmath45",
    "are multiplied by * ca * and * ch * respectively , so there are additional 2@xmath29 multiplications .",
    "thus , the proposed algorithm needs 3@xmath29 multiplications and 2__nnz__(@xmath24 ) additions per iteration .    in pagerank , @xmath46 is multiplied by @xmath47 which needs @xmath29 multiplications .",
    "then the result is multiplied by @xmath24 , which needs _",
    "nnz_(@xmath24 ) additions .",
    "further there are also additional adjustments ( stochasticity and primitivity ) to ensure the convergence of the result , which need @xmath48nd@xmath48 multiplications and ( @xmath29 + @xmath48nd@xmath48 ) additions , where @xmath48nd@xmath48 denotes the number of nondangling pages . because there is no need to do normalization in pagerank , the costs are @xmath29 + @xmath48nd@xmath48 multiplications and ( _ nnz_(@xmath24 ) + @xmath29 + @xmath48nd@xmath48 ) additions per iteration .",
    "table [ table2 ] summarizes the costs .",
    "the memory requirements for @xmath24 is _",
    "nnz_(@xmath24 ) booleans ; @xmath49 , @xmath50 , @xmath51 , @xmath1 , and @xmath2 is @xmath29 doubles each ; @xmath47 is @xmath29 integers ; and @xmath38 is @xmath52 booleans .",
    "table [ table3 ] summarizes the required memory .",
    "[ table2 ]    [ table3 ]      the dangling pages can cause computational issues both for pagerank and hits . in the pagerank case",
    ", the score of a page is originally defined as the proportion of time the random surfer spends on the page after following the hyperlink structure of the web infinitely @xcite .",
    "if the web graph contains the dangling pages , the random surfer will not be able to traverse all pages because it will be trapped in a dangling page if encountering it .",
    "consequently , not only the scores can only be defined for the pages that have been visited , but also due to the finite time of the observation , these scores will not reflect the real values of the pages .    in linear algebra perspective",
    ", the dangling pages make the web graph not strongly connected ; the adjacency matrix induced from the web graph is reducible .",
    "and by perron theorem for nonnegative matrices @xcite , the dominant eigenvector of a nonnegative reducible matrix exists but is not necessarily positive and unique .",
    "thus , there is no guarantee a unique and positive pagerank vector exists for the original pagerank problem ; finding the dominant eigenvector of @xmath53 @xcite . the same condition goes for hits , both authority matrix @xmath54 ( @xmath55 ) and hub matrix @xmath56 ( @xmath57 ) are nonnegative .",
    "thus by perron theorem for nonnegative matrices , @xmath31 and @xmath33 exist but there is no guarantee of the uniqueness @xcite .",
    "in addition to the non - uniqueness , there is also problem related to the final distributions as in the pagerank case .",
    "the dangling pages receive scores from others that point to them but do not share their scores ( because they have no outdegree ) , so they will always become authoritative and have no hub scores . consequently , the authority and hub distributions will be more skewed as the number of dangling pages increases , and the average distances between initial distributions ( usually uniform distributions ) and final stationary distributions in the web graph with many dangling pages are greater than in the web graph without dangling page .",
    "thus , the convergence rates tend to become slower as the number of the dangling pages increases .",
    "[ table4 ]    [ table5 ]    our experiments confirm this effect . as shown in fig .",
    "[ fig2 ] , the pagerank convergence rates are almost the same or better than hits , which do not agree with previous experiments where the researchers usually remove the dangling pages from the datasets @xcite . and in the datasets without the dangling page ( fig .  [ fig3 ] )",
    ", the results agree with the previous experiments ; hits usually converges faster than pagerank .    to deal with the dangling pages , instead of removing them , we prefer to use the back button model @xcite .",
    "this is because _ first _ , web graph datasets usually have high percentages of the dangling pages , so great portion of useful data will be lost if they are removed .",
    "_ , some of the dangling pages are the important pages @xcite , so removing them can bias the results .",
    "and _ third _ , most users usually go back to the previous page when encountering a dangling page , so this is a natural way in modeling the web graph .",
    "mathematically , the back button model rewrites * l * into @xmath58 , where * m * is @xmath59 matrix with row _ i _ is equal to column _ i _ of * l * if _ i _ is a dangling page , and @xmath60 otherwise .",
    "table [ table6 ] shows the average fractions of authoritative and hubby pages in the original and the back button datasets , where _",
    "fi _ denotes the average fraction of indegree and _ fo _ denotes the average fraction of outdegree .",
    "the original datasets in average have authoritative pages more than 93% ( _ fi _ @xmath61 0.6 ) and this percentage is almost unchanged for very authoritative pages ( _ fi _ @xmath61 0.9 ) .",
    "as shown in table [ table1 ] , the average percentage of the dangling pages is 92.9% , so almost all authoritative pages are the dangling pages .",
    "conversely , the number of hubby pages is only about 6% and is also almost unchange for very hubby pages .",
    "[ table6 ]    when the back button model is applied , the percentages of authoritative pages drop significantly and are comparable to the percentages of hubby pages . because this model turns the dangling pages into nondangling ones , the remaining pages are the real authority ; pages with many indegrees than outdegrees , not pages that have only indegrees .",
    "the costs and memory requirement of qi - hits , the proposed algorithm , and pagerank for back button datasets are shown in table [ table4 ] and [ table5 ] .      to analyze the convergence property of the proposed algorithm , the eq .",
    "[ eq5 ] will be rewritten in one vector @xmath0 matrix representation instead of two .",
    "let @xmath62 , the authority vector of the proposed algorithm can be rewritten into : @xmath63 and after the power method applied to the above equation has converged , the hub vector can be revived by calculating @xmath64 . consequently , the problem of finding authority and hub vectors can be reduced into only calculating dominant eigenvector of @xmath65 ( note that hits can also be rewritten into this style @xcite ) .    because @xmath65 is nonnegative",
    ", it always has a nonnegative dominant eigenvalue @xmath66 such that the moduli of all other eigenvalues \\{@xmath67 } do not exceed @xmath66 , and a dominant eigenvector corresponding to @xmath66 can be chosen so that every entry is nonnegative ( see theorem 3.6 in @xcite ) .",
    "thus , the existence of the authority vector of the proposed algorithm is guaranteed . but depending on the initialization , it may be not unique since @xmath66 may be repeated @xcite .    to guarantee the uniqueness of the proposed algorithm ,",
    "the matrix @xmath68 must be modified into a positive matrix .",
    "the positive version of @xmath68 can be written as : @xmath69 , where @xmath70 is a constant that should be set near to @xmath71 to preserve the hyperlink structure information . and",
    "the proposed algorithm can be rewritten as : @xmath72 because @xmath73 is not stochastic , @xmath31 must be normalized for each iteration step .    by perron theorem for positive matrices",
    "@xcite , a unique and positive principal eigenvector of @xmath74 ( the authority vector of the proposed algorithm ) is guaranteed to exist . and by ensuring the starting vector not in the range @xmath75 , the power method applied to eq .",
    "[ eq7 ] is guaranteed to converge to this vector @xcite .",
    "in particular , all positive vectors satisfy the requirement as the starting vector @xcite .",
    "note that in addition to guaranteeing the uniqueness , this modification also tackles the second less obvious problem associated with the proposed algorithm ( and also hits ) ; producing ranking vectors that inappropriately assign zero scores to some pages @xcite .    in practice however , as with the hits case , usually the proposed algorithm can be used without this modification because the scores from the link structure ranking algorithms will be combined with other scores like contents and hypertext scores , making the final scores less sensitive to the ranking vectors . and actually based on our experiments , the hits and",
    "the proposed algorithm do converge to unique nonnegative vectors for all datasets ( including the back button datasets ) with @xmath76 as the starting vector .",
    "in this section the performance of the proposed algorithm is evaluated by comparing its convergence rates and processing times to reach the same corresponding residual level with the results of qi - hits and pagerank .",
    "the suitability of the proposed algorithm in approximating hits is confirmed in section [ similarity ] . and",
    "the examples of the top pages returned by the algorithms are given in section [ toppages ] .",
    "the experiments are conducted by using a notebook with 1.86 ghz intel processor and 2 gb ram .",
    "the codes are written in python by extensively using database to store lists of adjacency matrix , score vectors , and other related data in harddisk , and open sourced as the part of our work in developing a simple web search engine for research purposes @xcite .",
    "there are 8 datasets used in the experiments that consist around 10 thousands to 225 thousands pages with average degrees from around 4 to 47 . except wikipedia @xcite",
    ", all datasets were crawled by using our crawling system @xcite .",
    "all datasets , but britannica , have a typical web graph s average degree , around 4 to 15 @xcite . however , the percentages of the dangling pages are quite higher here than in a typical dataset ( around 70% to 85% ) due to the high number of downloaded but unexplored pages .",
    "table [ table7 ] summarizes the datasets , where % dp denotes percentage of the dangling pages , and ad denotes average degree .",
    "[ table7 ]    [ table8 ]      fig .",
    "[ britannica]-[yahoo ] and [ britannicabb]-[yahoobb ] show the convergence rates for the original and the back button datasets respectively .",
    "the horizontal axis is the number of iteration and the vertical axis is the residual . in the original datasets",
    ", the proposed algorithm converges faster than hits ( except for yahoo dataset , where the percentage of the dangling pages is too high ) , but generally still can not beat pagerank . in the back button model , where the dangling pages are forced to have outdegree , the proposed algorithm gives very promising results , faster than both hits and pagerank for all datasets .",
    "further , in the back button model",
    "generally hits converges faster than pagerank , which agrees with the previous works @xcite .",
    "[ barchart ] and [ barchartbb ] show the processing times ( in second ) to achieve the same corresponding residual level for the original and the back button datasets respectively . in the original datasets , in average pagerank",
    "is the fastest and hits is the slowest to converge . and in the back button model , the proposed algorithm becomes the fastest in five out of eight cases , pagerank is the slowest in six out of eight cases , and hits gives moderate performances .",
    "the performances of the back button model also agree with the previous works @xcite where hits needs less processing time than pagerank .",
    "+   +     +   +      the similarity measures between the authority and hub vectors of qi - hits and the proposed algorithm are shown in table [ table8 ] .",
    "the purpose of the measurements is to confirm the suitability of the proposed algorithm in approximating the results of qi - hits .",
    "as shown there , the proposed algorithm gives good approximations to the authority vectors , and very good ones to the hub vectors .",
    "the measures are the best in the hub vectors of the original datasets .",
    "because spearman correlation gives about 0.999 , it can be conferred that the proposed algorithm returns ( almost ) exactly the same ordering as qi - hits .",
    "this is not surprising because in the original datasets more than 90% pages are the dangling pages that have no hub scores .",
    "table [ table9 ] and [ table10 ] give examples of the results returned by the authority vectors of the three algorithms for wikipedia dataset without query and with query `` programming '' respectively .",
    "note that for brevity only file names are displayed .",
    "to get full urls , each name has to be prefixed with `` http://en.wikipedia.org/wiki/ '' .",
    "[ table9 ]    [ table10 ]",
    "the proposed algorithm which makes use the definition of authority and hub can be used to accelerate the hits algorithm . while in original datasets it converges only faster than hits , in back button datasets it converges faster than both pagerank and hits .",
    "further , generally there are also some improvements in the processing times , especially for the back button datasets .    the non - uniqueness problem due to the reducibility of the authority matrix @xmath68",
    "can be eliminated by forcing the matrix into a positive matrix @xmath73 .",
    "this modification not only guarantees the uniqueness , but also tackles the second less obvious problem ; producing ranking vectors that inappropriately assign zero scores to some pages .",
    "based on the similarity measurements , it can be concluded that the vectors produced by the proposed algorithm can be used to approximate the qi - hits s vectors . and",
    "if the qi - hits vectors are desired instead , the qi - hits algorithm can be run by using these vectors as the starting vectors for a few last iteration steps . in the case of qi - hits where the problem involving calculating the stationary vectors of the enormous adjacency matrix of the web graph , even a few number of iteration steps are worth many resources because it takes days to finish the calculations",
    ".    there are some interesting future researches related to this work . _",
    "first _ , as stated in section [ introduction ] and [ relatedworks ] , the researches on accelerating the qi - hits computations are hardly known .",
    "the remarkable similarity between the pagerank and qi - hits formulation @xcite implies that qi - hits can be accelerated by utilizing the methods discussed in section [ relatedworks ] in conjunction to the proposed algorithm .    and _ second _ , as shown in eq",
    ".  [ eq6 ] , the hits algorithm can be accelerated by introducing @xmath1 and @xmath2 into the original authority matrix @xmath54 . while we calculate the entries of @xmath1 and @xmath2 based on the hits definition of authority and hub scores , other schemes like dynamically updating the entries by using the differences between the vectors of current and previous iteration or using previously calculated ranking vectors as the entries probably",
    "can also be used @xcite .",
    "kleinberg , authoritative sources in a hyperlink environment , journal of the acm vol .",
    "46 , pp .",
    "604 - 632 ( 1999 ) .",
    "l.  page , s.  brin , r.  motwani , t.  winograd , the pagerank citation ranking : bringing order to the web , stanford digital libraries working paper ( 1998 ) .",
    "langville , c.d .",
    "meyer , google s pagerank and beyond : the science of search engine rankings , princeton university press ( 2006 ) .",
    "k.  bharat , m.r .",
    "henzinger , improved algorithms for topic distillation in hyperlinked environment , proceeding 21st international acm sigir conference on research and development in information retrieval , acm press , pp .",
    "104 - 111 ( 1998 ) .",
    "haveliwala , topic sensitive pagerank : a context - sensitive ranking algorithm for web search , ieee transactions on knowledge and data engineering , vol .",
    "15 no .  4 , pp .  784 - 796 ( 2003 )",
    "parlett , the symmetric eigenvalue problem , siam ( 1998 ) .",
    "kamvar , t.h .",
    "haveliwala , c.d .  manning , g.h .",
    "golub , extrapolation methods for accelerating pagerank computations , proceeding 12th international world wide web conference , pp .",
    "261 - 270 , acm press ( 2003 ) .",
    "kamvar , t.h .",
    "haveliwala , c.d .  manning , g.h .",
    "golub , exploiting the block structure of the web for computing pagerank , technical report 2003 - 17 , stanford university ( 2003 ) .",
    "a.  arasu , j.  novak , a.  tomkins , j.  tomlin , pagerank computation and the structure of the web : experiments and algorithms , proceeding of the 11th international world wide web conference , poster track , acm press ( 2002 ) .",
    "lee , g.h .",
    "golub , s.a .",
    "zenios , a fast two - stage algorithm for computing pagerank and its extensions , technical report sccm-2003 - 15 , stanford university ( 2003 ) .",
    "langville , c.d .",
    "meyer , a reordering for the pagerank problem , siam journal on scientific computing vol . 27 no . 6 , pp .  2112 - 2120 ( 2006 ) .",
    "kleinberg , r.  kumar , p.  raghavan , s.  rajagopalan , a.  tomkins , the web as a graph : measurements , models , and methods , cocoon , pp .  1 - 17 ( 1999 ) .",
    "r.  albert , h.  jeong , a.l .",
    "barabasi , diameter of the word - wide web , nature vol .",
    "401 , pp .  130 ( 1999 ) .",
    "a.  broder , r.  kumar , f.  maghout , p.  raghavan , s.  rajagopalan , r.  stata , a.  tomkins , j.  wiener , graph structure in the web , computer networks vol .",
    "33 , pp .",
    "309 - 320 ( 2000 ) .",
    "haveliwala , efficient computation of pagerank , technical report 1999 - 31 , computer science department , stanford university ( 1999 ) . c.  ding , h.  zha , x.  he , p.  husbands , h.  simon , link analysis : hub and authorities on the world wide web .",
    "siam review , vol .",
    "2 , pp .  256 - 268 ( 2004 ) . c.  ding , x.  he , h.  zha , h.  simon , pagerank , hits and a unified framework for the link analysis , proceeding of the 25th acm sigir conference , pp .  353 - 354 ( 2002 ) .",
    "langville , c.d .",
    "meyer , deeper inside pagerank , internet mathematic journal , vol . 1 , no",
    ". 3 , pp .  335 - 380 ( 2005 ) .",
    "a.  farahat , t.  lofaro , j.c .",
    "miller , g.  rae , l.a .",
    "ward , authority rankings from hits , pagerank , and salsa : existence , uniqueness , and effect of initialization , siam journal of scientific computing , vol .",
    "27 , no .  4 , pp .",
    "1181 - 1201 ( 2006 ) .",
    "ng , a.x .  zheng , m.i .",
    "jordan , link analysis , eigenvectors , and stability , the seventh international joint conference on artificial intelligence ( 2001 ) .",
    "ng , a.x .  zheng , m.i .",
    "jordan , stable algorithms for link analysis , proceeding of the 24th annual international acm sigir conference ( 2001 ) .",
    "r.  fagin , a.r .",
    "karlin , j.  kleinberg , p.  raghavan , s.  rajagopalan , r.  rubinfeld , m.  sudan , a.  tomkins , random walks with back buttons , proceeding of the 32nd acm symposium on theory of computing ( 2001 ) .",
    "f.  mathieu , m.  bouklit , the effect of the back button in a random walk : application for pagerank , proceeding of the 13th international world wide web conference , pp .",
    "370 - 371 ( 2004 ) .",
    "m.  sydow , random surfer with back step , proceeding of the 13th international world wide web conference , pp .  352 - 353 ( 2004 ) .",
    "n.  eiron , k.s .",
    "mccurley , j.a .",
    "tomlin , ranking the web frontier , 13th international word wide web conference , acm press ( 2004 ) .",
    "s.  chakrabarti , b.e .",
    "dom , s.r .",
    "kumar , p.  raghavan , s.  rajagopalan , a.  tomkins , d.  gibson , j.m .",
    "kleinberg , mining the link structure of the world wide web , ieee computer magazine , vol .",
    "32 , pp .",
    "60 - 67 ( 1999 ) .",
    "a.  mirzal , pythinsearch : a simple web search engine , proceeding international conference on complex , intelligent and software intensive systems , pp .  1 - 8 , ieee comp . soc .",
    "t.  segaran , http://kiwitobes.com/db/searchindex.db .",
    "pagerank and qi - hits can be written in one vector @xmath0 matrix operation , so the tasks of finding the ranking vectors of these algorithms turn to be the problem of calculating dominant eigenvectors of the corresponding matrices . in vector @xmath0 matrix operation",
    "the pagerank formulation is @xmath77 @xcite and the qi - hits formulation is @xmath55 . by modifying the authority matrix @xmath54 into a positive matrix @xcite ,",
    "the qi - hits problem becomes equivalent to the pagerank problem ; finding the principal eigenvector of a positive matrix . and after the iteration converges , the hub vector can be revived by calculating @xmath78 we use `` principal eigenvector '' term if the eigenvalue associated with this vector is the largest , simple and the only eigenvalue on the spectral circle ( the matrix is positive ) , and `` dominant eigenvector '' if the eigenvalue is the largest but may be repeated ( the matrix is nonnegative and reducible ) or the largest but needs not the only eigenvalue on the spectral circle ( the matrix is nonnegative and irreducible ) . to ensure a unique and positive pagerank vector exists , the matrix @xmath79 needs to be modified into a positive matrix @xcite .",
    "a proven efficient way of doing this is to adjust the matrix into a stochastic and primitive ( irreducibility and aperiodicity imply primitivity ) matrix @xcite .",
    "stochasticity adjustment is done by replacing every zero row ( rows associated with the dangling pages ) with a probability teleportation vector . and primitivity adjustment is done by replacing every zero entry with a small positive real number . the stochastic and primitive version of @xmath79 is the positive google matrix @xmath80 .",
    "further , the power method applied to @xmath81 is guaranteed to converge to the pagerank vector @xmath46 as long as the starting vector is not in the range @xmath82 @xcite . to ensure the uniqueness of the authority and hub vectors , the authority and hub matrices",
    "must be modified into positive matrices @xcite .",
    "let @xmath83 and @xmath84 be the positive version of the authority matrix and hub matrix respectively .",
    "we can define them as @xmath85 , and @xmath86 , where @xmath70 is a constant that should be set near to @xmath71 to preserve the hyperlink structure information .",
    "we are grateful for this insightful idea suggested by the anonymous reviewer ."
  ],
  "abstract_text": [
    "<S> abstract . </S>",
    "<S> we present a new method to accelerate the hits algorithm by exploiting hyperlink structure of the web graph . </S>",
    "<S> the proposed algorithm extends the idea of authority and hub scores from hits by introducing two diagonal matrices which contain constants that act as weights to make authority pages more authoritative and hub pages more hubby . </S>",
    "<S> this method works because in the web graph good authorities are pointed to by good hubs and good hubs point to good authorities . </S>",
    "<S> consequently , these pages will collect their scores faster under the proposed algorithm than under the standard hits . </S>",
    "<S> we show that the authority and hub vectors of the proposed algorithm exist but are not necessarily be unique , and then give a treatment to ensure the uniqueness property of the vectors . </S>",
    "<S> the experimental results show that the proposed algorithm can improve hits computations , especially for back button datasets .    </S>",
    "<S> acceleration method , hits , hyperlink structure , power method , web graph </S>"
  ]
}