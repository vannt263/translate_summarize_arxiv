{
  "article_text": [
    "undirected graphical models , also known as markov random fields or markov networks , have become a part of the mainstream of statistical theory and application in recent years .",
    "these models use graphs to represent conditional independences among sets of random variables . in these graphs",
    ", the absence of an edge between two vertices means the corresponding random variables are conditionally independent , given the other variables . learning the structure of a graph is equivalent to learning if there exists an edge between every pair of nodes in the graph . + in the past decade , significant progress has been made on designing efficient algorithms to learn undirected graphs from high - dimensional observational datasets .",
    "most of these methods are based on either the penalized maximum - likelihood estimation or penalized regression methods .",
    "works has focused on the problem of estimating the graph in this high dimensional setting , which becomes feasible if graph is sparse .",
    "@xcite develop an efficient algorithm for computing the estimator with excellent theoretical properties using a graphical version of the lasso .",
    "+ in high dimensional problems normality assumption is the main constraint in methods .",
    "but we can replace the gaussian constraint with a semi - parametric gaussian copula , as discussed in @xcite .",
    "this method use a semi - parametric gaussian copula or non - paranormal approach by replacing linear functions with a set of one - dimensional smooth functions , for high dimensional inference .",
    "the non - paranormal extends the normal by transforming the variables by smooth functions .",
    "+ the gaussian distribution is almost always used in study for this scope , because the gaussian distribution represents at most second - order relationships , it automatically encodes a pairwise markov graph .",
    "the gaussian distribution has the property that if the @xmath0 component of inverse covariance matrix is zero , then variables i and j are conditionally independent , given the other variables .",
    "+   + distance correlation is a measure of dependence between random vectors introduced by @xcite .",
    "for all distributions with finite first moments distance correlation , it is zero if and only if the random vectors are independent .",
    "we use this properties to construct our method .",
    "+ in this paper , we have discussed the structure learning of markov graphs with large - dimensional covariance matrices where the number of variables is not small compared to the sample size .",
    "it is well - known that in such situations the usual estimator , the sample covariance matrix , may not be invertible .",
    "the approach suggested is to use distance covariance matrix towards the identity this matrix .",
    "in this paper we are concerned with the task of estimating the graph structure of a markov random field over a random vector @xmath1 , given n independent and identically distributed samples . +",
    "it was shown that the distance covariance is zero if and only if the two vectors were independent , we use this property .",
    "+ the main idea is to create a matrix from each pair of distance correlation .",
    "then use it to construct an adjacency matrix of conditional independents between each node pair .",
    "+ the distance dependence statistics in @xcite are defined as follows . for a random sample @xmath2 of n i.i.d .",
    "random vectors ( x , y ) from the joint distribution of random vectors x in @xmath3 and y in @xmath4 , compute the euclidean distance matrices @xmath5 and @xmath6 . + define @xmath7 + where @xmath8 similarly define @xmath9 + then sample distance correlation @xmath10 are defined by , @xmath11 where @xmath12 is the distance covariance .",
    "+ also , distance covariance has simple computing formula , the computations would appear to be @xmath13 , which can be burdensome for large n. + this estimator is distribution - free and has a simple explicit formula that is easy to compute and interpret .",
    "+ in our study this formula become more easier , because here @xmath14 , for example in procedure to form distance correlation matrix , @xmath15,@xmath16 for each vector only one time computed , furthermore @xmath17 is changed to @xmath18 and requires less computing time . in appendix [ app1 ] , we show a function coded in r , that calculate this . + the distance correlation , is implemented in the r package energy @xcite .",
    "+ we construct a matrix r of sample distance correlation ( @xmath19 ) between each pair of nodes , so the element @xmath20 in r is equal to @xmath21 , + @xmath22 we call the matrix r defined above the distance correlation matrix . +   + from the property that @xmath23 , obviously r is a symmetric matrix .",
    "+ when the matrix dimension @xmath24 is larger than the number @xmath25 of observations available , the ordinary sample covariance matrix is not invertible .",
    "but distance covariance or distance correlation matrix , has better performance .",
    "+ we generate three different random data in 2 to 100 dimensions , and compute average of determinant of the correlation and distance correlation matrix in each dimension .",
    "as we see these results in figure [ fig4 ] , distance correlation is more invertible . and",
    "computationally , is non - singular enough .",
    "+   +        [ fig4 ]    now we construct the partial correlation matrix . when @xmath26 be the partial correlation between the variables @xmath27 and @xmath28 , given all the remaining variables , and @xmath29 , the inverse of the correlation matrix [ whittaker ( 1990 ) ] ; is given by , + @xmath30^{\\frac{1}{2}}}.\\end{aligned}\\ ] ] +   +   + we can calculate matrix p , by simple computations . as in partial.cor function in r package rcmdr@xcite implemented , following code in r language programming do this .    ....",
    "-solve(r )              # ri is the inverse of r   d < - 1/sqrt(diag(ri ) )        p < - -ri * ( d % o% d )       # % o% is the outer product operator   diag(p ) < - 0    ....    finally @xmath31 is the sparse structure matrix of the graph .",
    "+ we can compare each element of @xmath32 to a tuning parameter ( forming the paths ) and derive desired adjacency matrix .",
    "in this simulation , we demonstrate the performance of the proposed approach on finding the sparse structures of random markov networks , by generating erds - rnyi random graphs .",
    "+ the erds - rnyi random graph @xmath33 is a graph on @xmath24 nodes in which the probability of an edge being in the graph is @xmath34 and the edges are generated independently . in this random graph ,",
    "the average degree of a node is @xmath35 .",
    "+   + given the precision matrix for a zero - mean gaussian distribution , it is easy to sample data from the distribution .",
    "but we do not know the distribution .",
    "so we randomly constructed precision matrices , and set random linear relationships with white noise between the columns of data sample matrices .",
    "+ in a similar manner to @xcite , we simulated erds - rnyi random graphs in two types of sparse structures ( or precision matrices ) : 1 ) 50 nodes with averagely 3 neighbours per node , 2 ) 200 nodes with averagely 4 neighbours per node .",
    "+ now , the goal here is to see how well our approach recovers the sparse structures of those precision matrices given different numbers of sampled data .",
    "+ figure [ fig5 ] illustrates the performance of our approach in recovering the structures of different markov networks in comparison with the non - paranormal approach as discussed in @xcite .",
    "the performance is evaluated by hamming distance , the number of disagreeing edges between an estimated network and the ground truth , in an equal number of edges .",
    "in this section , we are compared our algorithm to that of non - paranormal method as discussed in @xcite , using the function huge.npn ( ) implemented in the r package huge @xcite for estimating a semi - parametric gaussian copula model by truncated normal or normal score .",
    "+        [ fig1 ]        [ fig2 ]    . ]    the example is based on a stock market data which is contributed to the huge package that shows closing prices from all stocks in the @xmath36 500 for all days that the market was open between january 1 , 2003 and january 1 , 2008 .",
    "this gave us 1258 samples for the 452 stocks that remained in the @xmath37 500 during the entire time period .",
    "here for convenience and more visibility , we select only first 20 parameters .",
    "also and instead of force - based graph drawing layout of fruchterman - reingold that utilized in plot function in huge , we use r package ggm@xcite for visualize graphs more distinctly . + the output of huge package graph estimation using the transformed data method and also preprocessing step that mentioned in@xcite",
    "is shown in figure [ fig1 ] .",
    "+ data have been transformed by calculating the log - ratio of the price at time @xmath38 to price at time @xmath39 , and then standardized by subtracting the mean and adjusting the variance to one .",
    "+ in order to more distinctly , we sending above output estimated data to r package ggm @xcite , and plot the graph again , in two different layout , circle and fruchterman - reingold .",
    "figure [ fig2 ] shows the results .",
    "+    j. friedman , t. hastie , and r. tibshirani .",
    "sparse inverse covariance estimation with the graphical lasso",
    ". biostatistics , 9(3):432441 , 2007b .",
    "szekely , g. and rizzo , m. ( 2009 ) .",
    "brownian distance covariance .",
    "the annals of applied statistics , 3 ( 4):12361265 .",
    "liu , h. , lafferty , j. and wasserman , l. ( 2009 ) .",
    "the nonparanormal : semiparametric estimation of high dimensional undirected graphs .",
    "journal of machine learning research 10 22952328 .",
    "t. zhao , h. liu , k. roeder , j. lafferty and l. wasserman .",
    "the huge package for high - dimensional undirected graph .",
    "journal of machine learning research 13 , 1059 - 1062    yuanqing lin , shenghuo zhu , daniel d. lee , ben taskar : learning sparse markov network structure via ensemble - of - trees models .",
    "journal of machine learning research - proceedings track 5 : 360 - 367 ( 2009 ) j. fox , et al .",
    "r package ` rcmdr ' , ( version 1 .",
    "8 - 4 ) m. rizzo , et al .",
    "r package ` energy ' , ( version 1 . 4 - 0 ) giovanni m , et al .",
    "r package ` ggm ' , ( version 1 .",
    "* simplified distance correlation * + the following r code , calculate simplified distance correlation between two vectors x , y .    ....",
    "dcor2<-function(x , y ) {    n < - length(x ) ;   if ( n ! = length(y ) ) { stop(\"sample sizes must be equal \" ) }   u < - matrix(0,2,n+1 ) ;   w<-0    for(i in 1:n ) {      for(j in 1:n ) {      if(i < j ) {      w<-abs(x[i]-x[j ] ) ;           u[1,i]<-u[1,i]+w ;        u[1,j]<-u[1,j]+w ;      }      else if(i > j ) {         w<-abs(y[i]-y[j ] ) ;           u[2,i]<-u[2,i]+w ;            u[2,j]<-u[2,j]+w ;         }          }    }   u < -u / n   u[1,n+1]<-mean(u[1,][1:n ] )    u[2,n+1]<-mean(u[2,][1:n ] )     r<-0 ;    rx<-0 ;   ry<-0 ;   for(i in 1:n ) {       for(j in 1:n ) {      r<-   r + ( abs(x[i]-x[j])-u[1,i]-u[1,j]+u[1,n+1])*(abs(y[i]-y[j])-u[2,i]-u[2,j]+u[2,n+1 ] )           rx<- rx+(abs(x[i]-x[j])-u[1,i]-u[1,j]+u[1,n+1])*(abs(x[i]-x[j])-u[2,i]-u[2,j]+u[2,n+1 ] )           ry<- ry+(abs(y[i]-y[j])-u[1,i]-u[1,j]+u[1,n+1])*(abs(y[i]-y[j])-u[2,i]-u[2,j]+u[2,n+1 ] )                   }               }   rx<-sqrt(rx)/n   ry<-sqrt(ry)/n   r<-sqrt(r)/n      r/(sqrt(rx*ry ) )   } ...."
  ],
  "abstract_text": [
    "<S> in this paper , we present a simple non - parametric method for learning the structure of undirected graphs from data that drawn from an underlying unknown distribution . </S>",
    "<S> we propose to use brownian distance covariance to estimate the conditional independences between the random variables and encodes pairwise markov graph . </S>",
    "<S> this framework can be applied in high - dimensional setting , where the number of parameters much be larger than the sample size . </S>"
  ]
}