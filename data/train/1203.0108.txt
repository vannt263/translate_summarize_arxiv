{
  "article_text": [
    "this paper considers the problem of matrix recovery from a small set of noisy observations .",
    "suppose that we observe a small set of entries of a matrix .",
    "the problem of inferring the many missing entries from this set of observations is the _ matrix completion _ problem . a usual assumption that allows to succeed such",
    "a completion is to suppose that the unknown matrix has low rank or has approximately low rank .",
    "the problem of matrix completion comes up in many areas including collaborative filtering , multi - class learning in data analysis , system identification in control , global positioning from partial distance information and computer vision , to mention some of them .",
    "for instance , in computer vision , this problem arises as many pixels may be missing in digital images . in collaborative filtering",
    ", one wants to make automatic predictions about the preferences of a user by collecting information from many users .",
    "so , we have a data matrix where rows are users and columns are items . for each user",
    ", we have a partial list of his preferences .",
    "we would like to predict the missing rates in order to be able to recommend items that may interest each user .",
    "the noiseless setting was first studied by cands and recht @xcite using nuclear norm minimization .",
    "a tighter analysis of the same convex relaxation was carried out in @xcite . for a simpler approach ,",
    "see more recent papers of recht @xcite and gross @xcite .",
    "an alternative line of work was developed by keshavan _",
    "et al . _ in @xcite .",
    "a more common situation in applications corresponds to the noisy setting in which the few available entries are corrupted by noise .",
    "this problem has been extensively studied recently .",
    "the most popular methods rely on nuclear norm minimization ( see , e.g. , @xcite ) .",
    "one can also use rank penalization as it was done by bunea _",
    "_ @xcite and klopp @xcite . typically , in the matrix completion problem",
    ", the sampling scheme is supposed to be uniform .",
    "however , in practice , the observed entries are not guaranteed to follow the uniform scheme and its distribution is not known exactly .    in the present paper , we consider nuclear norm penalized estimators and study the corresponding estimation error in frobenius norm .",
    "we consider both cases when the variance of the noise is known or not .",
    "our methods allow us to consider quite general sampling distribution : we only assume that the sampling distribution satisfies some mild `` regularity '' conditions ( see assumptions  [ l ] and [ asspi ] ) .",
    "let @xmath0 be the unknown matrix .",
    "our main results , theorems [ thmu3 ] and [ thm3 ] , show the following bound on the normalized frobenius error of the estimators @xmath1 that we propose in this paper : with high probability @xmath2 where the symbol @xmath3 means that the inequality holds up to a multiplicative numerical constant .",
    "this theorem guarantees , that the prediction error of our estimator is small whenever @xmath4 .",
    "this quantifies the sample size necessary for successful matrix completion .",
    "note that , when @xmath5 is small , this is considerably smaller than @xmath6 , the total number of entries . for large @xmath7 and small @xmath8 ,",
    "this is also quite close to the degree of freedom of a rank @xmath8 matrix , which is @xmath9 .",
    "an important feature of our estimator is that its construction requires only an upper bound on the maximum absolute value of the entries of @xmath10 .",
    "this condition is very mild .",
    "a  bound on the maximum of the elements is often known in applications .",
    "for instance , if the entries of @xmath10 are some user s ratings it corresponds to the maximal rating .",
    "previously , the estimators proposed by koltchinskii _",
    "_ @xcite and by klopp @xcite also require a bound on the maximum of the elements of the unknown matrix but their constructions use the uniform sampling and additionally require the knowledge of an upper bound on the variance of the noise .",
    "other works on matrix completion require more involved conditions on the unknown matrix . for more details ,",
    "see section [ completionknown ] .",
    "sampling schemes more general than the uniform one were previously considered in .",
    "lounici @xcite considers a different estimator and measures the prediction error in the spectral norm . in @xcite",
    "the authors consider penalization using a weighted trace - norm , which was first introduced by srebro _",
    "_ in @xcite assume that the sampling distribution is a product distribution , that is , the row index and the column index of the observed entries are selected independently .",
    "this assumption does not seem realistic in many cases ( see discussion in  @xcite ) .",
    "an important advantage of our method is that the sampling distribution does not need to be equal to a product distribution .",
    "_ in @xcite propose a method based on the `` smoothing '' of the sampling distribution .",
    "this procedure may be applied to an arbitrary sampling distribution but requires a priori information on the rank of the unknown matrix . moreover , unlike in the present paper , in @xcite the prediction performances of the estimator are evaluated through a bound on the expected @xmath11-lipschitz loss ( where the expectation is taken with respect to the sampling distribution ) .",
    "the weighted trace - norm , used in @xcite , corrects a specific situation where the standard trace - norm fails .",
    "this situation corresponds to a non - uniform distribution where the row / column marginal distribution is such that some columns or rows are sampled with very high probability ( for a more thorough discussion see @xcite ) . unlike @xcite",
    ", we use the standard trace - norm penalization and our assumption on the sampling distribution ( assumption [ l ] ) guarantees that no row or column is sampled with very high probability .",
    "most of the existing methods of matrix completion rely on the knowledge or a pre - estimation of the standard deviation of the noise .",
    "the matrix completion problem with unknown variance of the noise was previously considered in @xcite using a different estimator which requires uniform sampling .",
    "note also that in @xcite the bound on the prediction error is obtained under some additional condition on the rank and the `` spikiness ratio '' of the matrix .",
    "the construction of the present paper is valid for more general sampling distributions and does not require such an extra condition .",
    "the remainder of this paper is organized as follows . in section [ preliminaries ]",
    ", we introduce our model and the assumptions on the sampling scheme . for the reader",
    "s convenience , we also collect notation which we use throughout the paper . in section [ completionknown ]",
    "we consider matrix completion in the case of known variance of the noise .",
    "we define our estimator and prove theorem [ thm2 ] which gives a general bound on its frobenius error conditionally on bounds for the stochastic terms .",
    "theorem  [ thm3 ] , provides bounds on the frobenius error of our estimator in closed form .",
    "therefore , we use bounds on the stochastic terms that we derive in section [ stochastic ] . to obtain such bounds , we use a non - commutative extension of the classical bernstein inequality .    in section [ completionunknown ]",
    ", we consider the case when the variance of the noise is unknown .",
    "our construction uses the idea of `` square - root '' estimators , first introduced by belloni _",
    "_ @xcite in the case of the square - root lasso estimator .",
    "theorem [ thmu3 ] , shows that our estimator has the same performances as previously considered estimators which require the knowledge of the standard deviation of the noise and of the sampling distribution .",
    "let @xmath12 be an unknown matrix , and consider the observations @xmath13 satisfying the trace regression model @xmath14 the noise variables @xmath15 are independent , with @xmath16 and @xmath17 ; @xmath18 are random matrices of dimension @xmath19 and @xmath20 denotes the trace of the matrix @xmath21 .",
    "assume that the design matrices @xmath22 are i.i.d .",
    "copies of a random matrix @xmath23 having distribution @xmath24 on the set @xmath25 where @xmath26 are the canonical basis vectors in @xmath27 .",
    "then , the problem of estimating @xmath10 coincides with the problem of matrix completion with random sampling distribution @xmath24 .",
    "one of the particular settings of this problem is the uniform sampling at random ( usr ) matrix completion which corresponds to the uniform distribution @xmath24 .",
    "we consider a more general weighted sampling model .",
    "more precisely , let @xmath28 be the probability to observe the @xmath29th entry .",
    "let us denote by @xmath30 the probability to observe an element from the @xmath31th column and by @xmath32 the probability to observe an element from the @xmath33th row .",
    "observe that @xmath34 .",
    "as it was shown in @xcite , the trace - norm penalization fails in the specific situation when the row / column marginal distribution is such that some columns or rows are sampled with very high probability ( for more details , see @xcite ) .",
    "to avoid such a situation , we need the following assumption on the sampling distribution :    [ l ] there exists a positive constant @xmath35 such that @xmath36    in order to get bounds in the frobenius norm , we suppose that each element is sampled with positive probability :    [ asspi ] there exists a positive constant @xmath37 such that @xmath38    in the case of uniform distribution @xmath39 .",
    "let us set @xmath40 .",
    "assumption  [ asspi ] implies that @xmath41      we provide a brief summary of the notation used throughout this paper .",
    "let @xmath42 be matrices in @xmath43 .",
    "* we define the _ scalar product _ @xmath44 . * for @xmath45 the _ schatten - q _ ( _ quasi-_)_norm _ of the matrix @xmath21 is defined by @xmath46 where @xmath47 are the singular values of @xmath21 ordered decreasingly .",
    "* @xmath48 where @xmath49 .",
    "* let @xmath50 be the probability to observe the @xmath51th element .",
    "* for @xmath52 , @xmath53 and for @xmath54 , @xmath55 .",
    "* @xmath56 and @xmath57 .",
    "* let @xmath58 , @xmath59 and @xmath60 . * @xmath61 .",
    "* let @xmath62 be an i.i.d .",
    "rademacher sequence and we define @xmath63 * define the observation operator @xmath64 as @xmath65 . * @xmath66 .",
    "in this section , we consider the matrix completion problem when the variance of the noise is known . we define the following estimator of @xmath10 : @xmath67 where @xmath68 is a regularization parameter and",
    "@xmath69 is an upper bound on @xmath70 .",
    "this is a restricted version of the matrix lasso estimator .",
    "the matrix lasso estimator is based on a trade - off between fitting the target matrix to the data using least squares and minimizing the nuclear norm and it has been studied by a number of authors ( see , e.g. , @xcite ) .    a restricted version of a slightly different estimator , penalised by a weighted nuclear norm @xmath71 , was first considered by negahban and wainwright in @xcite .",
    "here @xmath72 and @xmath73 are diagonal matrices with diagonal entries @xmath74 and @xmath75 , respectively . in @xcite ,",
    "the domain of optimization is the following one @xmath76 where @xmath77 is a bound on the `` spikiness ratio '' @xmath78 of the unknown matrix  @xmath10 . here",
    "@xmath79 and @xmath80 .",
    "in the particular setting of the uniform sampling ( [ domainwainwrite ] ) gives @xmath81 where @xmath82 is an upper bound on the `` spikiness ratio '' @xmath83 .",
    "the following theorem gives a general upper bound on the prediction error of estimator @xmath1 given by ( [ estimator ] ) .",
    "its proof is given in appendix [ proof - thm2 ] .",
    "the stochastic terms @xmath84 and @xmath85 play a key role in what follows .    [ thm2 ]",
    "let @xmath22 be i.i.d . with distribution @xmath24 on @xmath86 which satisfies assumptions [ l ] and [ asspi ] and @xmath87 .",
    "assume that @xmath88 for some constant @xmath69 .",
    "then , there exist numerical constants @xmath89 such that @xmath90 with probability at least @xmath91 , where @xmath60 .    in order to get a bound in a closed form",
    ", we need to obtain suitable upper bounds on @xmath92 and , with probability close to @xmath93 , on @xmath94 .",
    "we will obtain such bounds in the case of _ sub - exponential noise _ , that is , under the following assumption :    [ noise ] @xmath95    let @xmath96 be a constant such that @xmath97 . the following two lemmas give bounds on @xmath94 and @xmath92 .",
    "we prove them in section [ stochastic ] using the non - commutative bernstein inequality .",
    "[ delta ] let @xmath22 be i.i.d . with distribution @xmath24 on @xmath86 which satisfies assumptions [ l ] and  [ asspi ] .",
    "assume that @xmath98 are independent with @xmath99 , @xmath100 and satisfy assumption  [ noise ] .",
    "then , there exists an absolute constant @xmath101 that depends only on @xmath102 and such that , for all @xmath103 with probability at least @xmath104 we have @xmath105 where @xmath60 .",
    "[ edelta ] let @xmath22 be i.i.d . with distribution @xmath24 on @xmath86 which satisfies assumptions [ l ] and  [ asspi ] .",
    "assume that @xmath98 are independent with @xmath99 , @xmath100 and satisfy assumption  [ noise ] .",
    "then , for @xmath106 , there exists an absolute constant @xmath101 such that @xmath107 where @xmath60 .",
    "an optimal choice of the parameter @xmath108 in these lemmas is @xmath109 .",
    "larger @xmath108 leads to a slower rate of convergence and a smaller @xmath108 does not improve the rate but makes the concentration probability smaller . with this choice of @xmath108 the second terms in the maximum in ( [ max ] ) is negligible for @xmath110 where @xmath111 .",
    "then , we can choose @xmath112 where @xmath113 is an absolute numerical constant which depends only on @xmath102 .",
    "if @xmath114 are @xmath115 , then we can take @xmath116 ( see lemma 4 in @xcite ) . with this choice of @xmath117",
    ", we obtain the following theorem .    [ thm3 ]",
    "let @xmath22 be i.i.d . with distribution @xmath24 on @xmath86 which satisfies assumptions [ l ] and [ asspi ] .",
    "assume that @xmath88 for some constant @xmath69 and that assumption [ noise ] holds .",
    "consider the regularization parameter @xmath117 satisfying ( [ lambda ] ) .",
    "then , there exist a numerical constant @xmath118 , that depends only on @xmath102 , such that @xmath119 with probability greater than @xmath120 .",
    "_ comparison to other works _ :",
    "an important feature of our estimator is that its construction requires only an upper bound on the maximum absolute value of the entries of @xmath10 ( and an upper bound on the variance of the noise ) .",
    "this condition is very mild .",
    "let us compare this matrix condition and the bound we obtain with some of the previous works on noisy matrix completion .",
    "we will start with the paper of keshavan _ et al .",
    "their method requires a priori information on the rank of the unknown matrix as well as a matrix incoherence assumption ( which is stated in terms of the singular vectors of @xmath10 ) . under a sampling scheme different from ours ( uniform sampling without replacement ) and sub - gaussian errors , the estimator proposed in @xcite satisfies , with high probability , the following bound @xmath121 the symbol @xmath3 means that the inequality holds up to multiplicative numerical constants , @xmath122 is the condition number and @xmath123 is the aspect ratio . comparing ( [ keshavan ] ) and ( [ revisionthm ] ) , we see that our bound is better : it does not involve the multiplicative coefficient @xmath124 which can be big .",
    "_ in @xcite propose an estimator which uses a priori information on the `` spikiness ratio '' @xmath125 of @xmath10 .",
    "this method requires @xmath126 bounded by a constant , say  @xmath127 , in which case the estimator proposed in @xcite satisfies the following bound @xmath128 in the case of uniform sampling and bounded `` spikiness ratio '' this bound coincides with the bound given by theorem [ thm3 ] .",
    "an important advantage of our method is that the sampling distribution does not need to be equal to a product distribution ( i.e. , @xmath129 need not be equal to @xmath130 ) as is required in @xcite .",
    "the methods proposed in @xcite use the uniform sampling . similarly to our construction , an a priori bound on @xmath131 is required .",
    "an important difference is that , in these papers , the bound on @xmath131 is used in the choice of the regularization parameter @xmath117 .",
    "this implies that the convex functional which is minimized in order to obtain @xmath1 depends on @xmath69 .",
    "a too large bound may jeopardize the exactness of the estimation . in our construction",
    ", @xmath69 determines the ball over which we are minimizing our convex functional , which itself is independent of @xmath69 .",
    "our estimator achieves the same bound as the estimators proposed in these papers .",
    "_ minimax optimality _ : if we consider the matrix completion setting ( i.e. , @xmath132 ) , then , the maximum in ( [ revisionthm ] ) is given by its first therm . in the case of gaussian errors and under the additional assumption that @xmath133 for some constant @xmath134 this rate of convergence is minimax optimal ( cf .",
    "theorem 5 of @xcite ) .",
    "this optimality holds for the class of matrices @xmath135 defined as follows : for given @xmath8 and @xmath136 @xmath137 if and only if the rank of @xmath10 is not larger than @xmath8 and all the entries of @xmath10 are bounded in absolute value by @xmath136 .",
    "_ possible extensions _ : the techniques developed in this paper may also be used to analyse weighted trace norm penalty similar to one used in @xcite .",
    "in this section , we propose a new estimator for the matrix completion problem in the case when the variance of the noise @xmath138 is unknown .",
    "our construction is inspired by the square - root lasso estimator proposed in @xcite .",
    "we define the following estimator of @xmath10 : @xmath139 where @xmath68 is a regularization parameter and @xmath69 is an upper bound on @xmath70 .",
    "note that the first term of this estimator is the square root of the data - dependent term of the estimator that we considered in section [ completionknown ] .",
    "this is similar to the principle used to define the square - root lasso estimator for the usual vector regression model .",
    "let us set @xmath140 .",
    "the following theorem gives a general upper bound on the prediction error of the estimator @xmath141 .",
    "its proof is given in appendix [ proof - thmu1 ] .",
    "[ thmu1 ] let @xmath22 be i.i.d . with distribution @xmath24 on @xmath86 which satisfies assumptions [ l ] and [ asspi ] .",
    "assume that @xmath88 for some constant @xmath69 and @xmath142 .",
    "then , there exist numerical constants @xmath143 , that depends only on @xmath102 , such that with probability at least @xmath91 @xmath144 where @xmath145 .    in order to get a bound on the prediction risk in a closed form",
    ", we use the bounds on @xmath94 and @xmath92 given by lemmas [ delta ] and [ edelta ] taking @xmath109 .",
    "it remains to bound @xmath146 .",
    "we consider the case of sub - gaussian noise :    [ subg ] there exists a constant @xmath102 such that @xmath147\\leq\\exp \\bigl(t^{2}/2k \\bigr)\\ ] ] for all @xmath103 .",
    "note that condition @xmath148 implies that @xmath149 . under assumption",
    "[ subg ] , @xmath150 are sub - exponential random variables .",
    "then , the bernstein inequality for sub - exponential random variables implies that , there exists a numerical constant @xmath151 such that , with probability at least @xmath152 , one has @xmath153 using lemma [ delta ] and the right - hand side of ( [ bq ] ) , for @xmath154 , we can take @xmath155 note that @xmath117 _ does not depend _ on @xmath138 and satisfies the two conditions required in theorem [ thmu1 ] .",
    "we have that @xmath156 with probability greater then @xmath157 and @xmath158 for @xmath159 large enough , more precisely , for @xmath159 such that @xmath160 where @xmath161 .",
    "we obtain the following theorem .",
    "[ thmu3 ] let @xmath22 be i.i.d . with distribution @xmath24 on @xmath86 which satisfies assumptions [ l ] and [ asspi ] .",
    "assume that @xmath162 for some constant @xmath69 and that assumption [ subg ] holds .",
    "consider the regularization parameter @xmath117 satisfying ( [ lambdaun ] ) and @xmath159 satisfying ( [ nun ] ) .",
    "then , there exist numerical constants @xmath163 such that , @xmath164 with probability greater than @xmath165 .",
    "note that condition ( [ nun ] ) is not restrictive : indeed the sampling sizes @xmath159 satisfying condition ( [ nun ] ) are of the same order of magnitude as those for which the normalized frobenius error of our estimator is small .",
    "thus , theorem [ thmu3 ] shows , that @xmath166 has the same prediction performances as previously proposed estimators which rely on the knowledge of the standard deviation of the noise and of the sampling distribution .",
    "in this section , we will obtain the upper bounds for the stochastic errors @xmath167 and @xmath92 defined in ( [ stoch1 ] ) . in order to obtain such bounds",
    ", we use the matrix version of bernstein s inequality .",
    "the following proposition is obtained by an extension of theorem 4 in @xcite to rectangular matrices via self - adjoint dilation ( cf .",
    ", for example , 2.6 in @xcite ) .",
    "let @xmath168 be independent random matrices with dimensions @xmath169 .",
    "define @xmath170 and @xmath171    [ pr1 ] let @xmath168 be independent random matrices with dimensions @xmath169 that satisfy @xmath172 .",
    "suppose that @xmath173 for some constant @xmath174 and all @xmath175 .",
    "then , there exists an absolute constant @xmath176 , such that , for all @xmath103 , with probability at least @xmath104 we have @xmath177 where @xmath60 .",
    "we apply proposition [ pr1 ] to @xmath178 .",
    "we first estimate @xmath179 and @xmath174 .",
    "note that @xmath180 is a zero - mean random matrix which satisfies @xmath181 then , assumption [ noise ] implies that there exists a constant @xmath102 such that @xmath182 for all @xmath175 .",
    "we compute @xmath183 where @xmath73 ( resp . , @xmath72 )",
    "is the diagonal matrix with @xmath184 ( resp .",
    ", @xmath185 ) on the diagonal .",
    "this and the fact that the @xmath22 are i.i.d .",
    "imply that @xmath186 note that @xmath187 which implies that @xmath188 and the statement of lemma [ delta ] follows .",
    "the proof follows the lines of the proof of lemma 7 in @xcite . for sake of completeness",
    ", we give it here . set @xmath189 .",
    "@xmath190 is the value of @xmath108 such that the two terms in ( [ max ] ) are equal .",
    "note that lemma [ delta ] implies that @xmath191 and @xmath192 we set @xmath193 , @xmath194 . by hlder s inequality ,",
    "we get @xmath195 the inequalities ( [ proba1 ] ) and ( [ proba2 ] ) imply that @xmath196\\\\[-8pt ] & & \\quad\\leq \\biggl(d \\int^{+\\infty}_{0}\\exp\\bigl \\{-t^{1/\\log ( d)}\\nu_1\\bigr\\}\\,\\mathrm{d}t+d \\int ^{+\\infty}_{0}\\exp\\bigl\\{-t^{1/(2\\log ( d ) } \\nu_2\\bigr\\}\\,\\mathrm{d}t \\biggr)^{1/2\\log(d ) } \\nonumber\\\\ & & \\quad\\leq \\sqrt{e } \\bigl(\\log(d)\\nu_1^{-\\log(d)}\\gamma \\bigl(\\log(d)\\bigr)+2\\log(d ) \\nu_2^{-2\\log(d)}\\gamma\\bigl(2\\log(d ) \\bigr ) \\bigr)^{1/(2\\log(d))}. \\nonumber\\end{aligned}\\ ] ] the gamma - function satisfies the following bound : @xmath197 ( see , e.g. , @xcite ) . plugging this into ( [ estem ] ) , we compute @xmath198 observe that @xmath110 implies @xmath199 and we obtain @xmath200 we conclude the proof by plugging @xmath201 into ( [ estem-1 ] ) .",
    "it follows from the definition of the estimator @xmath1 that @xmath202 which , using ( [ model ] ) , implies @xmath203 hence , @xmath204 where @xmath205 .",
    "then , by the duality between the nuclear and the operator norms , we obtain    @xmath206    let @xmath207 be the projector on the linear vector subspace @xmath208 and let @xmath209 be the orthogonal complement of @xmath208 .",
    "let @xmath210 and @xmath211 denote , respectively , the _ left _ and _ right _ orthonormal _ singular vectors _ of @xmath21 .",
    "@xmath212 is the linear span of @xmath213 , @xmath214 is the linear span of @xmath215 .",
    "we set @xmath216    by definition of @xmath217 , for any matrix @xmath218 , the singular vectors of @xmath219 are orthogonal to the space spanned by the singular vectors of @xmath10 .",
    "this implies that @xmath220 .",
    "then @xmath221 note that from ( [ ineq ] ) , we get @xmath222 this , the triangle inequality and @xmath223 lead to @xmath224\\\\[-8pt ] & \\leq & \\frac{5}{3}\\lambda\\bigl{\\vert}\\mathbf p_{a_0 } ( a_0-\\hat a ) \\bigr{\\vert}_1 . \\nonumber\\end{aligned}\\ ] ] since @xmath225 and @xmath226 we have that @xmath227 . from ( [ 2 ] ) ,",
    "we compute @xmath228    for a @xmath229 , we consider the following constrain set @xmath230 note that the condition @xmath231 is satisfied if @xmath232 .",
    "the following lemma shows that for matrices @xmath233 the observation operator @xmath234 satisfies some approximative restricted isometry . its proof is given in appendix [ proof - thm1 ] .",
    "[ thm1 ] let @xmath22 be i.i.d . with distribution @xmath24 on @xmath86 which satisfies assumptions [ l ] and [ asspi ] .",
    "then , for all @xmath235 @xmath236 with probability at least @xmath91",
    ".    we need the following auxiliary lemma which is proven in appendix [ pl2 ] .",
    "[ l2 ] if @xmath237 @xmath238    lemma [ l2 ] implies that @xmath239 set @xmath240 . by definition of @xmath1 , we have that @xmath241 .",
    "we now consider two cases , depending on whether the matrix @xmath242 belongs to the set @xmath243 or not .",
    "_ case _ 1 : suppose first that @xmath244 , then ( [ ass1 ] ) implies that @xmath245 and we get the statement of theorem [ thm2 ] in this case .    _ case _ 2 : it remains to consider the case @xmath246 .",
    "then ( [ revisioncondition ] ) implies that @xmath247 and we can apply lemma [ thm1 ] . from lemma [ thm1 ] and ( [ 3 ] ) , we obtain that with probability at least @xmath91 one has @xmath248 now ( [ ass1 ] ) and @xmath241 imply that , there exist numerical constants @xmath249 such that @xmath250 which , together with ( [ o1 ] ) , leads to the statement of the theorem [ thm2 ] .",
    "the main lines of this proof are close to those of the proof of theorem 1 in @xcite . set @xmath251",
    ". we will show that the probability of the following `` bad '' event is small @xmath252 note that @xmath253 contains the complement of the event that we are interested in .    in order to estimate the probability of @xmath254 we use a standard peeling argument .",
    "let @xmath255 and @xmath256",
    ". for @xmath257 set @xmath258 if the event @xmath253 holds for some matrix @xmath259 , then @xmath21 belongs to some @xmath260 and    @xmath261    for each @xmath262 consider the following set of matrices @xmath263 and the following event @xmath264 note that @xmath265 implies that @xmath266 .",
    "then ( [ bl ] ) implies that @xmath267 holds and we get @xmath268 .",
    "thus , it is enough to estimate the probability of the simpler event @xmath267 and then apply the union bound . such an estimation is given by the following lemma . its proof is given in appendix [ pl1 ] .",
    "let @xmath269    [ l1 ] let @xmath22 be i.i.d . with distribution @xmath24 on @xmath86 which satisfies assumptions [ l ] and [ asspi ] .",
    "then , @xmath270 where @xmath271 .",
    "lemma [ l1 ] implies that @xmath272 .",
    "using the union bound , we obtain @xmath273 where we used @xmath274 .",
    "we finally compute for @xmath275 @xmath276 this completes the proof of lemma [ thm1 ] .    as we mentioned in the beginning ,",
    "the main lines of this proof are close to those of the proof of theorem 1 in @xcite .",
    "let us briefly discuss the main differences between these two proofs .",
    "similarly to theorem 1 in @xcite we prove a kind of `` restricted strong convexity '' on a constrain set . however , our constrain set defined by ( [ constrain ] ) is quite different from the one introduced in @xcite : @xmath277 the present proof is also less involved ( e.g. , we do not need use the covering argument used in @xcite ) . one important ingredient of our proof is a more efficient control of @xmath278 given by lemma [ edelta ] ( compare with lemma 6 in @xcite ) .",
    "our approach is standard : first we show that @xmath279 concentrates around its expectation and then we upper bound the expectation .    by definition , @xmath280 .",
    "massart s concentration inequality ( see , e.g. , @xcite , theorem 14.2 ) implies that    @xmath281    where @xmath271 .",
    "next , we bound the expectation @xmath282 . using a standard symmetrization argument ( see , e.g. , @xcite , theorem 2.1 )",
    ", we obtain @xmath283 where @xmath62 is an i.i.d .",
    "rademacher sequence .",
    "the assumption @xmath284 implies @xmath285 .",
    "then , the contraction inequality ( see , e.g. , @xcite ) yields @xmath286 where @xmath287 . for @xmath288 , we have that @xmath289 where we have used ( [ ass1 ] ) .",
    "then , by the duality between nuclear and operator norms , we compute @xmath290 finally , using @xmath291 and the concentration bound ( [ concentration ] ) , we obtain that @xmath292 with @xmath271 as stated .",
    "let us set @xmath293 and @xmath294 .",
    "we have that @xmath295 where @xmath205 .",
    "this implies    @xmath296    we need the following auxiliary lemma which is proven in appendix [ plu1 ] ( @xmath217 and @xmath297 are defined in ( [ projector ] ) ) .",
    "[ lu1 ] if @xmath298 , then @xmath299 where @xmath300 .    note that from ( [ ineq ] ) we get @xmath301 the definition of @xmath141 and ( [ un2 ] ) imply that @xmath302\\\\[-8pt ] & \\leq & 2q(a_0)+\\lambda \\bigl(\\bigl{\\vert}\\mathbf p_{a_0 } ( \\delta)\\bigr{\\vert}_1-\\bigl{\\vert}\\mathbf p_{a_0}^{\\bot } ( \\delta)\\bigr{\\vert}_1 \\bigr ) \\nonumber\\end{aligned}\\ ] ] and @xmath303 lemma [ lu1 ] implies that @xmath304 . from ( [ un3 ] ) and ( [ un4 ] ) , we compute @xmath305\\\\[-8pt ] & & \\quad=\\lambda q(a_0 ) \\bigl{\\vert}\\mathbf p_{a_0 } ( \\delta)\\bigr{\\vert}_1 - 2\\lambda q(a_0 ) \\bigl{\\vert}\\mathbf",
    "p_{a_0}^{\\bot}(\\delta)\\bigr{\\vert}_1 \\nonumber\\\\ & & \\qquad{}+2\\lambda^{2 } \\bigl{\\vert}\\mathbf p_{a_0 } ( \\delta)\\bigr{\\vert}^{2}_1+\\lambda^{2 } \\bigl { \\vert}\\mathbf p_{a_0}^{\\bot}(\\delta)\\bigr{\\vert}^{2}_1 - 3 \\lambda^{2 } \\bigl{\\vert}\\mathbf p_{a_0}(\\delta)\\bigr { \\vert}_{1}\\bigl{\\vert}\\mathbf p_{a_0}^{\\bot}(\\delta ) \\bigr{\\vert}_1 . \\nonumber\\end{aligned}\\ ] ] lemma [ lu1 ] implies that @xmath306 and we obtain from ( [ un5 ] ) @xmath307\\\\[-8pt ] & & \\quad\\leq 4\\lambda q(a_0 ) \\bigl{\\vert}\\mathbf p_{a_0}(\\delta)\\bigr { \\vert}_1 -2\\lambda q(a_0 ) \\bigl{\\vert}\\mathbf p_{a_0}^{\\bot}(\\delta)\\bigr{\\vert}_1 + 2 \\lambda^{2 } \\bigl{\\vert}\\mathbf p_{a_0}(\\delta)\\bigr { \\vert}^{2}_1 .",
    "\\nonumber\\end{aligned}\\ ] ] plugging ( [ un6 ] ) into ( [ un1 ] ) , we get @xmath308 then , by the duality between the nuclear and the operator norms , we obtain @xmath309 using @xmath310 we compute @xmath311 which leads to @xmath312 the condition @xmath313 implies that @xmath314    set @xmath315 . by the definition of @xmath141 we have that @xmath241 .",
    "we now consider two cases , depending on whether the matrix @xmath316 belongs or not to the set @xmath317 .",
    "_ case _ 1 : suppose first that @xmath318 , then ( [ ass1 ] ) implies that @xmath319 and we get the statement of the theorem [ thmu1 ] in this case .",
    "_ case _ 2 : it remains to consider the case @xmath320 .",
    "lemma [ lu1 ] implies that @xmath321 and we can apply lemma [ thm1 ] . from lemma [ thm1 ] ,",
    "( [ ass1 ] ) and ( [ un7 ] ) we obtain that , with probability at least @xmath91 one has @xmath322 a simple calculation yields @xmath323 and @xmath324\\\\[-8pt ] & & { } + \\sqrt{792 a^{2}\\mu m_1m_2\\rank(a_0 ) \\bigl ( \\be \\bigl ( { \\vert}\\sigma_r{\\vert}\\bigr ) \\bigr)^{2}}.\\nonumber\\end{aligned}\\ ] ] this and @xmath241 imply that , there exist numerical constant @xmath325 such that @xmath326 which , together with ( [ un8 ] ) , leads to the statement of the theorem [ thmu1 ] .",
    "by the convexity of @xmath327 and using @xmath328 we have @xmath329 using the definition of @xmath1 , we compute @xmath330    this and ( [ ineq ] ) implies that @xmath331 as stated .",
    "by the convexity of @xmath332 , we have @xmath333 using the definition of @xmath141 , we compute @xmath334 then ( [ ineq ] ) and the triangle inequality imply @xmath335 and the statement of lemma [ lu1 ] follows .",
    "i would like to thank miao weimin for his interesting comment ."
  ],
  "abstract_text": [
    "<S> in the present paper , we consider the problem of matrix completion with noise . unlike previous works , we consider quite general sampling distribution and we do not need to know or to estimate the variance of the noise . </S>",
    "<S> two new nuclear - norm penalized estimators are proposed , one of them of `` square - root '' type . </S>",
    "<S> we analyse their performance under high - dimensional scaling and provide non - asymptotic bounds on the frobenius norm error . </S>",
    "<S> up to a logarithmic factor , these performance guarantees are minimax optimal in a number of circumstances . </S>"
  ]
}