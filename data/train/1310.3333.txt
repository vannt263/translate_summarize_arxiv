{
  "article_text": [
    "visualizing data which is in the form of a bag of vectors is useful in various domains in identifying and understanding relationship between entities involved .",
    "the goal of the visualization is to help the user identify similarities and differences between data by means of a visual aid ( usually a graph where nodes depict entities involved and the edges correspond to relationship between the entites ) . but",
    "data belonging to different domains have different characteristics .",
    "hence a targeted approach is needed to visualize data for different domains . in this paper",
    "the focus is on a set of authors each of whom has a set of papers published .",
    "+   + the data used for visualization in this paper consists of a set of authors and their publications .",
    "visualization results are provided for different author - paper datasets which are defined in * sec .",
    "[ sec : experimental - results]*. +   + each author can be represented as bag of vectors , where each vector corresponds to a paper published by that author represented as a vector of word counts in that document .",
    "but this representation of the data would lead to high dimensional data points to be visualized since the average number of distinct words in any scientific paper is quite _",
    "large_. hence dimensionality reduction techniques are necessary and one such technique is * mve * ( minimum volume embedding ) * [ 4 ] * , which is used for the visualization of the data , in this paper .",
    "+   + * mve * is an algorithm that facilitates non - linear dimensionality reduction of data .",
    "the reduction consists of * sdp * ( semi - definite programming ) and matrix factorization to find a low - dimensional embedding that preserves local distances between points while representing the dataset in many fewer dimensions .",
    "it has been shown in that mve improves upon * sde * ( semi - definite embedding ) , which is the pre - existing technique , in terms of the resulting eigenspectrum producing better visualizations for different datasets .",
    "+   + while dimensionality reduction of the data is crucial , the representation of the data provided as an input to * mve * mechanism , is equally important .",
    "usually datasets have to be preprocessed to be presented in a form _ conducive _ to visualization .",
    "since * mve * entails techniques to preserve local distances or similarities between the data , transforming the original dataset to help calculate the similarity between different data points will aid the visualization process .",
    "one such method of computing similarity scores is using a bhattarcharya kernel * [ 6][7 ] * ( based on the bhattacharya distance ) which is used to compute similarities between probability distributions .",
    "the vectors of word counts for each author can be converted to a vector of word probabilites by normalization , making it suitable for applying the bhattacharya kernel .",
    "also probability distributions can be inferred by using generative models using the author and paper information which is the main focus of this paper .",
    "+   + this paper is organized as follows : - * sec .",
    "[ sec : bag - of - words ] . * discusses the representation of each author as a single vector based on word counts aggregated over all the papers published by that author .",
    "this is followed by * sec .",
    "[ sec : generative - models - for ] . * which introduces the concept of generative models , especially the * a - t model * that is used to infer the underlying probability distributions which can be used to represent the data . in * sec .",
    "[ sec : experimental - results ] . * the actual datasets as well as the visualization results using the above mentioned representations are presented . in * sec .",
    "[ sec : hierarchical - author - topic - model ] . * an extension to the a - t model is presented .",
    "this is followed by summing up the paper with * conclusion * and * future work * sections .",
    "in the author - paper dataset , for each author , a set of papers published by that author constitute the initial dataset . each paper can be represented as a vector of word counts .",
    "each dimension of this vector represents a unique word in the dictionary consisting of unique words from all the papers of all authors .",
    "* eg . * a paper @xmath0 written by author @xmath1 is represented as @xmath2 , where @xmath3 denotes the frequency of @xmath4 word in the @xmath5 paper author @xmath1 written by in the vocabulary of all words * v , * whose size is denoted by @xmath6 .",
    "we can arrive at an aggregated word count for each author by aggregating word frequency vectors for each paper written by that author , * i.e. * @xmath7 where @xmath8 , where @xmath9 represents the set of papers under author @xmath1 .",
    "but using just the word counts can not appropriately capture the similarities between authors , whereas a normalized vector might be more suitable for comparison since the number of words in a paper is not constant .",
    "this transforms the aggregated word frequency vector to a probability vector thus rendering the use of the bhattacharya kernel possible .",
    "now @xmath10 where @xmath11 .",
    "this is the transformation used on the inital dataset to be used for visualization using * mve . *",
    "generative models are used to infer the underlying joint probability distribution of variables defined by graphical models , from which the conditionals can be derived .",
    "these conditionals can be used to determine an expression like @xmath12 where @xmath13 refers to the @xmath5 author , and _ features _ represents the characteristics of the data",
    ".      latent dirichlet allocation ( * lda)[1 ] * is used to infer the distribution of the data in terms of a set of hidden topics or categories . using the generative paradigm ,",
    "@xmath14 can be inferred , where @xmath15 refers to the @xmath16 hidden category and @xmath17 refers to the @xmath5 document .",
    "here we do not use the authorship information , hence there is no basis for comparison between authors by inferring this probability distribution as it is more of a relation between a vector of words over another vector of words .",
    "this problem gets magnified when there are multiple authors per document .",
    "this intuition is further supported by the visualization depicted in * sec .",
    "[ sec : experimental - results ] .",
    "*      the a - t model**[8 ] * * builds on * lda * by including the authorship information in influencing the topics assigned to words . since the authorship information is also included in the joint of the entire model",
    ", we can arrive at the topic distributions for each author - @xmath18 where @xmath19 refers to the @xmath16 hidden topic and @xmath13 refers to the @xmath5 author .",
    "thus each author can be represented by his / her topic distribution vector which can now be used to compute similarities and provided as input for the * mve . *",
    "the main goal of the experiments is to compare the quality of the visualization between the * aggregated bag of words model * and the * a - t model*. multiple datasets are used to faciliate the comparison .",
    "the a - t model was derived from the open source * topic - toobox * software * [ 2 ] . *",
    "apart from the comparison , experiments were conducted to generate visualizations on a new model proposed in * sec .",
    "[ sec : hierarchical - author - topic - model ] .",
    "*      the datasets used for the visualization experiments include : -    * a set of 106 authors with 5 paper abstracts each from the set of members listed at * a set of 68 authors with 10 full papers each from the same source as above    the datasets were collected manually from      the visualizations are provided in the form of figures . in some figures",
    "the entire network of connected authors is provided for the reader to compare and contrast with:- + * * + * note : * please magnify the document ( electronic version ) to observe the `` subnetwork '' relationships with greater clarity . +   +",
    "the original picture files are provided along with the report as well in .eps format .",
    "+   + the fidelity scores for all the visualizations were in the range [ 0.97 - 0.99 ] .",
    "+     _ figure 1 .",
    "visualization of aggregated bag of words model - 106 authors - 5 abstracts _",
    "the above figure shows the visualization using the aggregated bag of words model on the abstracts .",
    "there are a few relationships which are captured , as shown in the diagram below , which tally with the expected behavior . however apart from a few such `` subnetworks '' there are many deviations from expected behavior that were observed as well .",
    "+   +     _ figure 2 .",
    "sub - network of figure 1 showing people from machine learning , statistics and natural language processing _",
    "_ figure 3 .",
    "sub - network of figure 1 .",
    "showing un - expected connections _",
    "the above figure depicts shows a few similarities which do not tally with expected behavior .",
    "this can be explained by limiting the content of the data just to the abstracts .",
    "vital information pertaining to the specific field of study of each author has more chance of appearning more frequently if the entire paper is considered instead of just the abstracts .",
    "+   +     _ figure 4 .",
    "aggregated bag of words model - 68 authors - 10 papers each _    the above figure depicts the aggregated bag of words models .",
    "the color assignments for each author represent the most frequent topic derived from the topic distributions generated by the author topic model with similar model parameters ( with number of topics = 20 ) .",
    "+   +     _ figure 5 . magnified version of aggregated bag of words model - 68 authors - 10 papers each _",
    "the above figure depicts the magnified visualization for a different dataset .",
    "the similarity between machine learners and statisticians is apparent in the figure .",
    "more similarities amiong other authors can be observed in this figure as well * eg .",
    "* complexity theory , algorithms .",
    "also we can see that this model is more often than not in agreement with the a - t model due to the high frequency of similar colors of the nodes connected to each other .",
    "+   +     _ figure 6 .",
    "the a - t model - 68 authors - 10 papers each , 20 topics _    the above figure depicts the a - t model visualization .",
    "+   +     _ figure 7 .",
    "the a - t model - machine learning - staistics - neuroscience sub - network _",
    "this `` subnetwork '' shows the inter connections between authors of machine learning and statistics although there are a few inconsistencies in the topic allocations as can be see from the color of the labels .",
    "+   +     _ figure 8 .",
    "the a - t model - natural language processing sub - network _        _ figure 9 .",
    "aggregated lda model - 68 authors - 10 papers each , 20 topics _    the above figure depicts the visualization using lda to determine the topic distributions of each document and aggregating over the set of documents for each author to arrive at a pseudo - topic distribution .",
    "the aggregated topic distributions do not tally with the expected behavior- as was expected .",
    "from the a - t model , a natural extension that is possible is to extend it for data that can be naturally generated from a hierarchical categorical model .",
    "if the data belonged to such a framework , using the information from different layers of categories might help in distinguishing the data more accurately than just using a single layer of topics as done by the * a - t model . *",
    "any such discriminating feature will help in achieving a more accurate similarity score between authors , which can help improve the visualization .",
    "+   + also the hierarchy of categories is prevalent in most data including the author - papers dataset where each document / author can be segregated starting from a broad range of topics ( eg . departments , fields ) to the most specific ( eg . machine learning , complexity theory ) .",
    "also various other datasets can be naturally classified into a hierarchy of classes eg .",
    "kingdom - genus - species ( biology ) , compounds in organic chemistry etc .",
    "thus this model can be used for visualizing datasets conforming to this scenario .",
    "_ figure 10 . the hierarchical a - t graphical model _",
    "the graphical model presented above represents the hierarchical a - t model .",
    "each of the @xmath20 are the dirichlet priors of the @xmath21 which correspond to the multivariate distribution at the @xmath16 level .",
    "each of the @xmath22 are the @xmath16 level topic assignments , with @xmath23 corresponding to topic assignment at the leaf level ( * i.e. * the most specific topic assignment ) .",
    "these leaf topics being the most specific are part of the word - topic probability matrix @xmath24 .",
    "each of the @xmath22 depend on @xmath25 .",
    "inference of the parameters @xmath26 and @xmath24 can be solved by using e - m . but",
    "this paper uses the technique of collapsed gibbs sampling * [ 3 ] * to arrive at the expresssion @xmath27 by integrating out the correspondin parameters . + * * + * note : * the complete derivation is provided in the supplementary paper provided .",
    "the dataset consisting of 68 authors with 10 papers for each author was visualized using the hierarchical at - model with 2 layers of hierarchy ( 2 divisions at the root , 20 divisions at the leaf - > 20 topics in total ) .",
    "the visualizations are provided below:-        _ _    using the hierarchical at - model on the above dataset gives a rather unique visualization compared to the the visualization derived from using the a - t model .",
    "two `` subnetworks '' circled in the above diagram , show us that both relationships present in the previous visualizations are preserved and also improved upon .",
    "one such sub - network is shown in the next figure .",
    "+     _ _    one such improvement in the visualization is observed between the authors whose primary focus is in the area of natural language processing .",
    "the authors who have primarily published from the ccls ( http://www.ccls.columbia.edu ) are more similar compared to others whose publications are not primarily from ccls , proving that more complex sub - relations between similar entities can be captured by the hat model .",
    "firstly , it was observed that using the full paper information for each author over just the abstracts gave better visualization with the * aggregated bag of words model*. this can be attributed to higher frequency of certain domain - specific words in the papers compared to the abstracts .",
    "certain abstracts just constitute a sentence while others might be equivalent to around 10 sentences .",
    "this might overestimate certain word probabilities or might lead to sparse word probability vectors .",
    "+   + it can be observed that the * a - t model * considerably provides better visualization than the aggregated bag of words model .",
    "this can be attributed to the hidden topic information inferred from the data which provides more information about the class to which the authors belong facilitating more accurate similarity score measurements . also using * lda",
    "* to infer the topic distributions of the documents and aggregating over the authors did not provide a reasonable visualization as was expected generally .",
    "+   + the visualization generated by the * hierarchical a - t model * seems to be on par with the a - t model in preserving the relationships , but in a few cases provides an improvement by taking advantage of inherent hierarchy of classifications by providing more consistent and accurate topic assignments .",
    "also in the case of authors - papers dataset , we can represent each author with topic distribution from different layers , giving rise to different kernels ( including different combinations of two or more individual layer kernels ) for computing the similarity score w.r.t * mve*. +   + logistically , to achieve an improvement on the visualization from the a - t model , the right set of parameters have to be tuned into the model in particular the number of layers in the hierarchy of classification of the data as well as the number of divisions at each layer . in case of the author - paper dataset",
    ", it can be inferred logically that 2 layers of hierarchy are required - the first layer broadly categorizes topics based on department ( * e.g. * engineering , physics , biology , law , business etc . ) , and the second layer provides a more specific classification conditioned on the previous layer s topic . in practice the number of divisions of the top layer pertaining to the author - paper dataset can be expected to lie between 2 and 6 .",
    "the number of leaf topics is decided via cross - validation as is the case for a - t model .",
    "+   + in cases where just a single layer of topics exist , the hierarchical model can handle this case by setting the number of divisions at the root to be equal to 1 and the number of layers to 2 with the same number of leap topics .",
    "the resulting visualizations were also found to be equivalent for the dataset with 68 authors - 10 papers as well further supporting the credibility of the hierarchical a - t model .",
    "the dataset consisting of 68 authors - 10 papers each was chosen due to convenience - validating the visualization with the expected behavior is easier since the authors of this paper are aware of the research interests and works of the authors of this dataset .",
    "more datasets belonging to different domains will be used to further test the merits of the proposed model * e.g. * a set of 2037 authors with 1740 papers in total from the nips archives ( 1987 - 1999 ) .",
    "moreover we can also experiment with certain modifications to the existing graphical model in addition to modifying the kernel based on different layers ."
  ],
  "abstract_text": [
    "<S> the motivation of this paper is two - fold - a ) to compare between two different modes of visualizing data that exists in a bag of vectors format b ) to propose a theoretical model that supports a new mode of visualizing data . visualizing high dimensional data </S>",
    "<S> can be achieved using minimum volume embedding , but the data has to exist in a format suitable for computing similarities while preserving local distances . </S>",
    "<S> this paper compares the visualization between two methods of representing data and also proposes a new method providing sample visualizations for that method . </S>"
  ]
}