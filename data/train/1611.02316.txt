{
  "article_text": [
    "reduced order models ( roms ) continue to play a critically enabling role in modern , large - scale scientific computing applications  @xcite .",
    "the rom architecture is being exploited in many simulation based physics and engineering systems in order to render tractable many high - dimensional simulations .",
    "fundamentally , the rom algorithmic structure is designed to construct low - dimensional subspaces , typically computed with singular value decomposition ( svd ) , where the evolution dynamics can be embedded using galerkin projection .",
    "thus instead of solving a high - dimensional system of differential equations ( e.g. millions or billions of degrees of freedom ) , a rank @xmath0 model can be constructed in a principled way .",
    "three steps are required for this low - rank approximation : ( i ) numerical solutions of the original high - dimensional system , ( ii ) dimensionality - reduction of this solution data typically produced with an svd , and ( iii ) galerkin projection of the dynamics on the low - rank subspace .",
    "the first two steps are often called the _ offline _ stage of the rom architecture whereas the third step is known as the _ online _ stage .",
    "offline stages are exceptionally expensive , but enable the ( cheap ) online stage to potentially run in real time . in this manuscript , we integrate recent innovations in randomized linear algebra methods  @xcite , particularly as it relates to the singular value decomposition , and compressive sampling in order to ( i ) improve the computational efficiency of the second step of the rom architecture , namely the building of low - rank subspaces used for galerkin projection , and ( ii ) provide a rapid evaluation of the nonlinear terms in the rom model using compressive sampling of the dynamic mode decomposition .",
    "randomized methods for matrix computations provide an efficient computation of low - rank structures in data matrices , which is a foundational aspect of machine learning and big data applications .",
    "such algorithms exploit the fact that the target rank of interest , @xmath0 , is significantly smaller than the high - dimensional data under consideration . in the case of roms",
    ", there may only be a couple hundred modes of interest ( @xmath1 ) whereas the numerical solution of the original high - dimensional system may have millions or billions of degrees of freedom .",
    "roms allow one to simulate this system with a differential equation of dimension @xmath0 , thus greatly reducing computational time .",
    "randomized techniques circumvent the challenge of traditional ( deterministic ) svd reduction which requires significant memory and processing resources for the high - dimensional data generated from full state simulations .",
    "randomized techniques are robust , reliable and computationally efficient and can be used to construct a smaller ( compressed ) matrix , which accurately approximates a high - dimensional data matrix  @xcite . there",
    "exist several strategies for obtaining the compressed matrix , and using random projections is certainly the most robust off - the - shelf approach .",
    "randomized algorithms have been in particular studied for computing the near - optimal low - rank singular value decomposition by  @xcite , @xcite , @xcite and @xcite .",
    "the seminal contribution  @xcite extends and surveys this work .",
    "in addition to randomized techniques , compressive sampling strategies are of growing interest for matrix computations as they also allow for the approximation of decompositions with few measurements .",
    "much like randomized algorithms , compressive sampling takes advantage of the inherent sparsity of the spatio - temporal dynamics in an appropriate basis .",
    "thus the target rank @xmath0 determines the number of sample points required .",
    "compressive sampling can be used with the dynamic mode decomposition ( dmd )  @xcite to enact a _ compressive dmd _",
    "@xcite approximation for the galerkin projected dynamics  @xcite .",
    "the dmd method is an attractive alternative to the standard pod ",
    "galerkin reduction which also uses sparse sampling through the gappy pod and/or deim / eim architecture .",
    "ultimately , the low - rank structure inherent in roms allows the community to exploit sparse measurements to reconstruct an accurate approximation of the high - dimensional system . in this work",
    ", two new innovations are introduced that leverage our current computational capabilities , namely compressive sampling for enhancing the dmd method for roms and randomized singular value decompositions for constructing efficient pod basis elements .",
    "the structure of the paper is as follows . in section [ section2 ]",
    "we review model reduction techniques based upon the proper orthogonal decomposition ( pod ) method , the discrete empirical interpolation method ( deim ) and the dynamic mode decomposition ( dmd ) applied to general nonlinear dynamical systems . in section 3",
    "we highlight innovations of randomized techniques for matrix computations , which is the building block for our new approach .",
    "section [ section4 ] focuses on the application of compressed matrix decompositions in model order reduction .",
    "finally , numerical tests are presented in section [ section5 ] . throughout the paper",
    "we use the following notation : all matrices and vectors are in bold letters .",
    "the basis functions are denoted by the matrix @xmath2 with different superscripts denoting how we computed the basis , e.g. @xmath3 represents the basis functions from the pod method .",
    "the rank of the pod basis functions is @xmath0 , the rank of the nonlinear term is @xmath4 , whereas @xmath5 is the number of measurements utilized in the compressed techniques .",
    "we consider the general system of high - dimensional , ordinary differential equations : @xmath6,\\\\ { \\bf y}(0)={\\bf y_0},\\\\ \\end{array } \\right.\\ ] ] where @xmath7 is a given initial data , @xmath8 given matrices and @xmath9\\times{\\mathbb{r}}^n\\rightarrow{\\mathbb{r}}^n$ ] a continuous function in both arguments and locally lipschitz - type with respect to the second variable .",
    "it is well  known that under these assumptions there exists a unique solution for .",
    "this class of problems arises in a wide range of applications , but especially from the numerical approximation of partial differential equations . in such cases ,",
    "the dimension of the problem @xmath10 is the number of spatial grid points used from discretization and it typically is very large",
    ". the numerical solution of system may be very expensive to compute and therefore it is often useful to simplify the complexity of the problem by means of reduced order models .",
    "the model reduction approach is based on projecting the nonlinear dynamics onto a low dimensional manifold utilizing projectors that contain information from the full , high - dimensional system .",
    "let us assume that we have computed some basis functions @xmath11 of rank @xmath0 for .",
    "we can project the dynamics onto the low - rank basis functions using : @xmath12 where @xmath13 are functions on @xmath14 and defined on the time interval from @xmath15 $ ] .",
    "we note that we are working with a galerkin - type projection where we consider only a few basis functions whose support is non - local , unlike finite element basis functions . the reduced solution @xmath16 where @xmath17 .    inserting the projection assumption into the full model , and making use of the orthogonality of the basis functions , the reduced model takes the following form : @xmath18,\\\\ { \\bf y}^\\ell(0)={\\bf y_0^\\ell } , \\end{array}\\right.\\ ] ] where @xmath19 and @xmath20 .",
    "we also note that @xmath21 .",
    "the system ( [ pod_sys ] ) is achieved following a galerkin projection .",
    "if the dimension of the system is @xmath22 , then a significant dimensionality reduction is accomplished . +",
    "this section focuses on several model order reduction techniques as they constitute the building blocks of the proposed method .",
    "in particular , we recall three key innovations for model reduction : pod , deim and dmd .",
    "these techniques provide an efficient projector for the reduction of the complexity of the problem under consideration .",
    "one popular method for reducing the complexity of the system is the so - called proper orthogonal decomposition ( pod ) .",
    "the idea was proposed by sirovich  @xcite and is detailed here for completeness .",
    "we build an equidistant grid in time with constant step size @xmath23 .",
    "let @xmath24 with @xmath25 .",
    "let us assume we know the exact solution of ( [ ode ] ) on the time grid points @xmath26 , @xmath27 .",
    "our aim is to determine a pod basis of rank @xmath22 to optimally describe the set of data collected in time by solving the following minimization problem : @xmath28 where the coefficients @xmath29 are non - negative and @xmath30 are the so called _ snapshots _ , e.g. the solution of at a given time @xmath26 .",
    "additionally , we assume @xmath31 for a suitable hilbert space @xmath32 .",
    "the norm , here and in the sequel of the section , can be interpreted as the weighted norm such that @xmath33 and @xmath34 .    solving ( [ pbmin ] )",
    "we look for an orthonormal basis @xmath35 which minimizes the distance between the sequence @xmath30 with respect to its projection onto this unknown basis .",
    "the matrix @xmath36 contains the collection of snapshots @xmath30 as columns .",
    "it is useful to look for @xmath37 in order to reduce the dimension of the problem considered .",
    "the solution of ( [ pbmin ] ) is given by the singular value decomposition of the snapshots matrix @xmath38 , where we consider the first @xmath39 columns @xmath40 of the orthogonal matrix @xmath41 and set @xmath42 .",
    "to concretely apply the pod method , the choice of the truncation parameter @xmath0 plays a critical role .",
    "there are no a - priori estimates which guarantee the ability to build a coherent reduced model , but one can focus on heuristic considerations , introduced by sirovich @xcite , so as to have the following ratio close to one : @xmath43 this indicator is motivated by the fact that the error in is given by the singular values we neglect : @xmath44 where @xmath45 is the rank of the snapshot matrix @xmath36 . we note that the error is strictly related to the computation of the snapshots and it is not related to the reduced dynamical system . more recently ,",
    "gavish and donoho  @xcite have introduced a hard - thresholding technique for determining the truncation of the svd when the data contains a low - rank signal with noise .",
    "this method provides a principled approach to rank selection .      for a review of deim ,",
    "we closely follow the presentation in @xcite .",
    "the rom introduced in ( [ pod_sys ] ) is a nonlinear system where the significant challenge with the pod ",
    "galerkin approach is the computational complexity associated with the evaluation of the nonlinearity . to illustrate this issue",
    ", we consider the nonlinearity in ( [ pod_sys ] ) : @xmath46 to compute this inner product , the variable @xmath47 is first expanded to an @xmath48dimensional vector @xmath49 , then the nonlinearity @xmath50 is evaluated and , at the end , we return back to the reduced - order model .",
    "this is computationally expensive since it implies that the evaluation of the nonlinear term requires computing the full , high - dimensional model , and therefore the reduced model is not independent of the full dimension @xmath51 to avoid this computationally expensive , high - dimensional evaluation , the gappy pod method was introduced  @xcite . in its original formulation ,",
    "random and sparse sampling was proposed for computing the required nonlinear inner products .",
    "advances in gappy methods have led to the state - of - the - art _ empirical interpolation method _",
    "( eim , @xcite ) and _ discrete empirical interpolation method _",
    "( deim , @xcite ) methods which are now broadly used in the roms community .",
    "the computation of the pod basis functions for the nonlinear part are related to the set of the snapshots @xmath52 where @xmath30 is already computed from .",
    "we denote with @xmath53 the pod basis function of rank @xmath4 of the nonlinear part .",
    "the deim approximation of @xmath54 is as follows @xmath55 where @xmath56 and @xmath57 .",
    "the matrix @xmath58 is the interpolation point where the nonlinearity is evaluated and the selection of its points is made according to an lu decomposition algorithm with pivoting  @xcite , or following the qr decomposition with pivoting  @xcite .",
    "the error between @xmath59 and its deim approximation @xmath60 is given by @xmath61 where different error performance is achieved depending on the selection of the interpolation points in @xmath62 as shown in @xcite .",
    "dmd is an _ equation - free _ , data - driven method capable of providing accurate assessments of the spatio - temporal coherent structures in a given complex system , or short - time future estimates of such a systems .",
    "it traces its origins to pioneering work of bernard koopman in 1931  @xcite , whose work was revived in a set of papers starting in 2004  @xcite .",
    "the dmd provides the eigenvalues and eigenvectors of the best fit linear system relating a snapshot matrix and a time shifter version of the snapshot matrix at some later time .",
    "consider the following data snapshot matrices @xmath63 with @xmath30 an initial condition to and @xmath64 its corresponding output after some prescribed evolution time @xmath65 with there being @xmath66 initial conditions considered .",
    "the dmd involves the decomposition of the best - fit linear operator @xmath67 relating the matrices above : @xmath68 where @xmath69 is unknown .",
    "the exact dmd algorithm proceeds as follows @xcite : first , we collect data *",
    "y , y * and compute the reduced singular value decomposition of * y * : @xmath70 then , compute the least - squares fit @xmath67 that satisfies @xmath71 and project onto pod modes @xmath72 @xmath73 and compute the eigen - decomposition of @xmath74 @xmath75 where @xmath76 are the dmd eigenvalues .",
    "the dmd modes @xmath77 are given by : @xmath78 the data @xmath79 may come from a nonlinear system @xmath80 in which case the dmd modes are related to eigenvectors of the infinite - dimensional koopman operator .",
    "more details can be found in  @xcite .",
    "we may interpret dmd as a model reduction technique if data is acquired from a high - dimensional model , or a method of system identification if the data comes from measurements of an unknown system .",
    "for the purpose of this work we consider the dmd - galerkin method , where the assumption hold true for @xmath81 given by .",
    "we note that the techniques we provide aim to speed up the computation of the offline stage , whereas the online stage will present the same cost as the standard methods .",
    "randomized linear algebra is of growing importance for the analysis of high - dimensional data  @xcite . specifically , randomized techniques attempt to construct low - rank matrix decompositions that are computationally efficient and accurate approximations of the standard matrix decompositions such as qr and svd .",
    "randomized algorithms can be parallelized and distributed for large matrices and there are several implementations of the randomized techniques in _ matlab _ or _ r _ that are now available via open source @xcite .",
    "the algorithms that result from using randomized sampling techniques are not only computationally efficient , but are also simple to implement as they rely on standard matrix - matrix multiplication and unpivoted qr factorization .",
    "consider a randomized algorithm to compute the low - rank matrix approximation  @xcite @xmath82 where @xmath0 denotes the target - rank and is assumed to be @xmath83 .",
    "random matrix theory provides a simple and elegant solution for computing the low - rank approximation by creating a random sampling matrix @xmath84 where the entries are drawn from , for example , a gaussian distribution .",
    "then , a sampled matrix @xmath85 is computed as @xmath86 if the matrix @xmath87 has exact rank @xmath0 , then the sampled matrix @xmath88 spans , with high probability , a basis for the column space . however , most data matrices in practice are only dominated by rank-@xmath0 features since the singular values @xmath89 are non - zero .",
    "thus , instead of just using @xmath0 samples , it is favorable to slightly oversample @xmath90 , were @xmath5 denotes the number of additional samples . in practice ,",
    "small values of @xmath91 are sufficient to obtain a good basis that is comparable to the best possible basis  @xcite .",
    "an orthonormal basis @xmath92 is then obtained via the qr - decomposition @xmath93 , such that @xmath94 finally , @xmath87 is projected to this low - dimensional space @xmath95 where @xmath96 .",
    "the matrix @xmath97 can then be used to efficiently compute the matrix decomposition of interest such as the svd .",
    "the oversampling @xmath5 allows one to control the approximation error@xcite .",
    "the algorithm is summarized in algorithm [ alg : rsvd ] . in figure",
    "[ fig : rsvd ] we show the decay of the singular values for different level of the randomized svd . as expected increasing the number of sampling we obtain more accurate approximation .",
    "matrix @xmath98 draw a gaussian random matrix @xmath99 form the sample matrix @xmath100 compute the qr decomposition of @xmath101 set * b*=@xmath102 compute svd of @xmath103 set @xmath104    . we consider the full matrix ( red ) , @xmath105 ( blue ) , @xmath106 ( black ) , @xmath107 ( green ) where @xmath45 is the rank of matrix . ]",
    "model order reduction techniques are usually based on snapshots that collect data on the underlying dynamical system .",
    "the svd decomposition of the date matrix @xmath98 provides a low - dimension projector operator that allows one to obtain surrogate models .",
    "however , the svd may be computationally expensive and , for this reason , we propose the use of algorithm [ alg : rsvd ] to reduced the offline cost of the method .",
    "the main idea is to consider basis functions not from the full set of measurements but from a few spatially incoherent measurements .",
    "we introduce the measurement matrix @xmath108 which produces the compressed matrix @xmath109 such that : @xmath110 here , we consider sparse measurements of the snapshots matrix in order compute pod and dmd from this new compressed snapshot matrix . in this paper",
    "we assume that snapshots matrix is almost square , e.g. @xmath111 , and one can imagine this is a realistic situation working with an explicit time scheme or in a many - query context . in the following subsections we provide further details about compressed pod , compressed pod - deim , and compressed dmd .",
    "the compressed pod method works , as pod , starting with a snapshot matrix with the aim to compute solutions of the problem in a fast and reliable way .",
    "as discussed before , the solution of the minimization problem leads to an expensive singular value decomposition problem . here , the idea is to apply the randomized svd technique in section 2 for the approximation of .",
    "the method works as follows : ( i ) we collect the snapshot set and ( ii ) we solve the optimization problem .",
    "we make use of the optimality conditions in @xcite in order to take advantage of the randomized svd . in this way we are able to compute the compressed pod basis functions in a significantly faster way . clearly the number of samples point plays a crucial role .",
    "the algorithm is summarized in algorithm [ alg : cpod ] .",
    "snapshot matrix @xmath98 , @xmath0 number of basis functions .",
    ", @xmath5 number of measurements .",
    "compute the randomized svd ( see algorithm [ alg : rsvd ] ) , @xmath112=rsvd({\\bf y})$ ] set @xmath113 for @xmath114 .",
    "the error in the minimization problem is now associated with subsampling of the randomized svd ( @xcite ) : @xmath115 where @xmath116 is the rank of the snapshot matrix @xmath36 and @xmath5 is the number of the samples in the compressed technique .",
    "we note that we consider the expectation value of the error due to the random measurements we consider .",
    "the error is now related to the computation of the set of snapshots and the number of samples @xmath5 . we note that if the singular values of the snapshot matrix decay rapidly a minimal amount of samples drives the error close to the theoretically minimum value . however , if the singular values do not decay rapidly we can lose accuracy .",
    "we refer to @xcite and the reference therein for more details about the error of the randomized svd .",
    "finally , we note that the pod basis functions for the snapshot matrix @xmath98 can be also computed from the eigenvalue problem for the matrix @xmath117 or @xmath118 .",
    "interested readers can see ref .",
    "@xcite for a more comprehensive description .",
    "regardless , if the dimension of the matrix @xmath36 is such that @xmath111 , then the computation of the eigenvalue problem will not lead to a faster approximation than the svd .",
    "this further motivates our approach through the rsvd .",
    "similarly to the compressed pod method we aim to apply the rsvd to the deim approach .",
    "the deim method considers the computation of the svd for both the snapshots of the solution and snapshots of the nonlinear term .",
    "we note that , although the online stage benefits from a sparse evaluation of the nonlinearity , the offline stage is even more expensive than pod itself .",
    "the goal is to substitute the full dimensional svd with the much smaller randomized svd . in this way we can highly reduce the cost of the computational costs and , at the same time ,",
    "obtain accurate results .",
    "we can also combine ideas from compressive sampling to compute the dynamic mode decomposition from a few measurements of the data .",
    "this method was already introduced in @xcite . here ,",
    "it is applied in as galerkin projection method .",
    "it is possible to either collect data @xmath79 or projected data @xmath119 where @xmath120 , @xmath121 and @xmath122 is the measurement matrix .",
    "we will call the matrices @xmath123 the output - projected snapshot matrices .",
    "similar to equation above , @xmath124 and @xmath125 are related by @xmath126 the goal , as in dmd , is to compute eigenvalues and eigenvectors of the unknown matrix @xmath127 .",
    "the method differs from the standard dmd since we are using sparse measurements . under general assumptions it is possible to prove the convergence of the method when the number of measurements @xmath5 increases .",
    "cleary the cdmd method is computationally more efficient , and the method is summarized in algorithm [ alg_cdmd ] .",
    "snapshots @xmath128 , @xmath122 set @xmath129 $ ] and @xmath130 $ ] , @xmath120 , @xmath121 compute the svd of @xmath124 , @xmath131 define @xmath132 compute eigenvalues and eigenvectors of @xmath133 . set @xmath134 +    once the dmd basis functions",
    "@xmath77 are computed we utilize assumption and obtained a surrogate model of the form .",
    "in this section we present our numerical tests using our three proposed compressed / randomized svd strategies of the last section . in our numerical computations",
    "we use the finite difference method to reduce a partial differential equation into the form and integrate the system with a semi - implicit scheme in the first example and newton method in the second .",
    "all the numerical simulations reported in this paper are performed on a macbook pro with an intel core i5 , 2.2ghz and 8 gb ram using matlab r2013a .    in the following numerical examples we build different surrogate models , such as pod , compressed pod ( cpod ) , pod - deim , compressed pod - deim ( cpod - cdeim ) ,",
    "dmd and compressed dmd ( cdmd ) and compare their performance in terms of cpu time and the error with respect to a reference solution computed by a high - fidelity , finite - difference approximation .",
    "we select two numerical examples , the first one considers a time - dependent semi - linear pdes whereas the second studies a semi  linear elliptic parametric equations .",
    "both examples lead to the same conclusions . in the numerical tests , the number samples utilized for the compression of the snapshot matrix",
    "is twice the rank .",
    "as shown in figure [ fig : rsvd ] , this turns out to be very efficient for both accuracy and computational cost .",
    "let us consider the following semi linear parabolic equation : @xmath135,\\\\ y(x,0)&=y_0(x ) , & \\qquad x\\in \\omega,\\\\ y(\\cdot , t)&=0 , & \\quad x\\in\\partial\\omega , t\\in [ 0,t ] , \\end{aligned}\\right.\\ ] ] where @xmath136\\times[0,1 ] , t=5 , \\theta=0.1 , \\mu=1 , x=(x_1,x_2 ) , y_0(x)=0.1 $ ] if @xmath137 and @xmath138 elsewhere .",
    "the pod basis vectors are built upon 10000 equidistant snapshots .",
    "the fd discretization yields a system of odes of the same form as with @xmath139 .",
    "the solution of this equation generates a stationary solution @xmath140 for large @xmath141 as shown in figure [ test1:sol ] .",
    "( top ) and @xmath142 ( bottom).,title=\"fig : \" ]   ( top ) and @xmath142 ( bottom).,title=\"fig : \" ] +   ( top ) and @xmath142 ( bottom).,title=\"fig : \" ]   ( top ) and @xmath142 ( bottom).,title=\"fig : \" ] +    [ test1:sol ]    the complexity of problem is reduced by model order reduction . when dealing with model order reduction , it is relevant to consider the cpu time of the simulation and the error . in general",
    "it is important to have a trade - off between the two quantities .",
    "figure [ test1:an ] considers the cpu time on the left panel .",
    "as we can see the compressed techniques are faster than the standard reduction techniques .",
    "we note here that for the cpu time we consider both offline and online stages .",
    "although we do not aim at and improvement of the online stage , in this work one might also consider a further speed up as suggested in @xcite .",
    "it is somehow clear that the compressed dmd provide the fastest approximation since it does not require the computation of the randomized svd .",
    "however , we show in figure [ test1:an ] the relative error computed with respect to the frobenius norm . as we can see , pod and cpod , such as pod - deim and cpod - cdeim , perform exactly the same results .",
    "slightly different are the results from dmd and cdmd .",
    "however , all these techniques perform with very high accuracy .",
    "as expected , the pod - deim and its related compressed technique is less accurate since we do not evaluate the nonlinearity for the full state .        another important feature to investigate when dealing with compressed techniques",
    "is how the cpu time scales with different dimensions of the snapshot matrix .",
    "the computation of the svd is , computationally , the most expensive part of the method and its cost varies according to the dimension of the snapshot set .",
    "here we consider a square matrix .",
    "as we can see in figure [ test1:scale ] in the left panel , the cpu time scale shows that we gain more than 2 order of magnitude in speed up as the dimension increases .",
    "thus , it provides a powerful technique that allows one to significantly reduce the computational costs in the offline stage . in the right panel",
    "we can see the relative error for 10 basis functions .",
    "the second numerical example concern a parametric elliptic equation .",
    "this example follows closely from @xcite .",
    "let the dynamics given by : @xmath143 where the spatial variable @xmath144 and the parameters are @xmath145 ^ 2\\subset\\mathbb{r}^2 $ ] with a homogeneous dirichlet boundary condition and nonlinearity @xmath146 and source term @xmath147 we numerically solve the system applying newton s method to the nonlinear equations resulting from a fd discretization .",
    "the full dimension of the discretized problem is @xmath148 .",
    "the solution of is shown in figure [ test2:sol ] .",
    "note that different choice of the parameter configuration leads different solutions .",
    "+         for the sake of completeness we show the decay of the singular values of the snapshot matrix on the left panel of figure [ test2:svd ] .",
    "we compare the singular values computed with the standard svd and the randomized svd as we increase the sampling points of the original matrix .",
    "as expected , it leads to improved approximations and , at the same time , faster approximations of the problem .",
    "a similar behavior comes from the nonlinear term ( right panel ) .",
    "finally , we show the cpu time for all the methods studied in the left panel of figure [ test2:cpu ] and the error behavior in the right panel .",
    "they provide a similar analysis discussed in the previous test .",
    "model order reduction is a successful and commonly used technique that projects nonlinear high dimensional dynamical systems and pdes into low dimensional surrogate models using optimal basis functions computed from information of the system .",
    "although the solution of the surrogate model is computationally efficient , the computation of the basis functions remains computationally expensive . in this paper",
    "we have demonstrated through several examples that compressed ( randomized ) techniques are a promising approach to circumventing expensive offline stages in model order reduction . in particular ,",
    "when dealing with large snapshot matrices we suggest the use of randomized singular valued decomposition for the proper orthogonal decomposition and compressed dynamic mode decomposition .",
    "they both provide very accurate solutions and promise significant computational savings in the offline stage , which turns out to be the most expensive part of the building block for the surrogate model .",
    "critical for enacting these computational enhancements is the advent of randomized linear algebra techniques .",
    "randomized linear algebra methods have been recently surveyed in @xcite .",
    "indeed , the methods are continuing to mature and have many critical error bounds associated with their proposed matrix factorizations .",
    "these efficient matrix decompositions are tremendously important for analyzing high dimensional data sets and/or for producing the low - dimensional subspaces required for roms .",
    "randomized techniques have continued to experience modifications that increase their efficiency and broaden the range of applicability of the methods .",
    "more broadly , randomized methods have application to classical ( non - randomized ) techniques for solving the same problems such as , e.g. , krylov methods , subspace iteration , and rank - revealing qr factorizations .    ultimately , roms are primarily concerned with producing rapid evaluation of surrogate models that represent the original high - dimensional system with a given accuracy .",
    "given the significant computational bottleneck for evaluating the low - dimensional projection , it is surprising the randomized linear algebra techniques have yet to penetrate the roms community .",
    "we have explicitly demonstrated that such randomized techniques can be a significant enhancement of the roms architecture .",
    "it should be used whenever possible given the current maturity of the technique and the error bounds available .",
    "m. barrault , y. maday , n.c .",
    "nguyen , a.t .",
    "patera , an empirical interpolation method : application to efficient reduced - basis discretization of partial differential equations comptes rendus mathematique , * 339 * , 2004 , 667672 .",
    "s. voronin and p .-",
    "martinsson , _ rsvdpack : subroutines for computing partial singular value decompositions via randomized sampling on single core , multi core , and gpu architectures , _ 2015 , arxiv:1502.05366 ."
  ],
  "abstract_text": [
    "<S> singular value decomposition ( svd ) has a crucial role in model order reduction . </S>",
    "<S> it is often utilized in the offline stage to compute basis functions that project the high - dimensional nonlinear problem into a low - dimensionsl model which is , then , evaluated cheaply . </S>",
    "<S> it constitutes a building block for many techniques such as e.g. proper orthogonal decomposition and dynamic mode decomposition . </S>",
    "<S> + the aim of this work is to provide efficient computation of the basis functions via randomized matrix decompositions . </S>",
    "<S> this is possible due to the randomized singular value decomposition ( rsvd ) which is a fast and accurate alternative of the svd . </S>",
    "<S> although this is considered as offline stage , this computation may be extremely expensive and therefore the use of compressed techniques drastically reduce its cost . </S>",
    "<S> numerical examples show the effectiveness of the method for both proper orthogonal decomposition ( pod ) and dynamic mode decomposition ( dmd ) .    </S>",
    "<S> nonlinear dynamical systems , proper orthogonal decomposition , dynamic mode decomposition , randomized linear algebra </S>"
  ]
}