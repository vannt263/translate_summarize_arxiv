{
  "article_text": [
    "consider the gaussian relay problem shown in figure  [ fig : gaussian ] .",
    "suppose the receiver @xmath1 and the relay @xmath5 each receive information about the transmitted signal @xmath0 of power @xmath6 .",
    "specifically , let @xmath7 where @xmath8 have correlation coefficient @xmath9 and are jointly gaussian with zero mean and equal variance @xmath10 .",
    "what should the relay @xmath5 say to the ultimate receiver @xmath1 ?",
    "if the relay sends information at rate @xmath3 , what is the capacity @xmath11 of the resulting relay channel ?",
    "we first note that the capacity from @xmath0 to @xmath1 , ignoring the relay , is @xmath12 the channel from the relay @xmath5 to the ultimate receiver @xmath1 has capacity @xmath3 .",
    "this relay information is sent on a side channel that does not affect the distribution of @xmath1 , and the information becomes freely available to @xmath1 as long as it does nt exceed rate @xmath3 .",
    "we focus on three cases for the noise correlation @xmath9 : @xmath13 and @xmath14 .    if @xmath15 , then @xmath16 , the relay is useless , and the capacity of the relay channel is @xmath17 for all @xmath18 .",
    "now consider @xmath19 , i.e. , the noises @xmath20 and @xmath21 are independent .",
    "then the relay @xmath5 has no more information about @xmath0 than does @xmath1 , but the relay furnishes an independent look at @xmath0 .",
    "what should the relay say to @xmath1 ?",
    "this capacity @xmath11 , mentioned in @xcite , remains unsolved and typifies the primary open problem of the relay channel . as a partial converse ,",
    "zhang  @xcite obtained the strict inequality @xmath22 for all @xmath23 .",
    "how about the case @xmath24 ? this is the problem that we solve and generalize in this note . here",
    "the relay , while having no more information than the receiver @xmath1 , has much to say , since knowledge of @xmath1 and @xmath5 allows the perfect determination of @xmath0 .",
    "however , the relay is limited to communication at rate @xmath3 .",
    "thus , by a simple cut - set argument , the total received information is limited to @xmath25 bits per transmission .",
    "we argue that this rate can actually be achieved .",
    "since it is obviously the best possible rate , the capacity for @xmath24 is given as @xmath26 ( see figure  [ fig : graph ] . )    every bit sent by the relay counts as one bit of information , despite the fact that the relay does nt know what it is doing .",
    "we present two distinct methods of achieving the capacity .",
    "our first coding scheme consists of hashing @xmath27 into @xmath28 bits , then checking the @xmath29 codewords @xmath30 , @xmath31 , one by one , with respect to the ultimate receiver s output @xmath32 and the hash check of @xmath27 .",
    "more specifically , we check whether the corresponding estimated noise @xmath33 is typical , and then check whether the resulting @xmath34 satisfies the hash of the observed @xmath27 . since the typicality check reduces the uncertainty in @xmath30 by a factor of @xmath35 while the hash check reduces the uncertainty by a factor of @xmath36 , we can achieve the capacity @xmath37 .",
    "it turns out hashing is not the unique way of achieving @xmath38 .",
    "we can compress @xmath27 into @xmath39 using @xmath40 bits with @xmath32 as side information in the same manner as in wyner ",
    "ziv source coding  @xcite , which requires @xmath41 thus , @xmath28 bits are sufficient to reveal @xmath39 to the ultimate receiver @xmath32",
    ". then , based upon the observation @xmath42 , the decoder can distinguish @xmath43 messages if @xmath44    for this scheme , we now choose the appropriate distribution of @xmath45 given @xmath5 .",
    "letting @xmath46 where @xmath47 is independent of @xmath48 , we can obtain the following parametric expression of @xmath49 over all @xmath50 : @xmath51 setting @xmath52 in , solving for @xmath53 , and inserting it in , we find the achievable rate is given by @xmath54 so `` compress - and - forward '' also achieves the capacity .    inspecting what it is about this problem that allows this solution",
    ", we see that the critical ingredient is that the relay output @xmath55 is a deterministic function of the input @xmath0 and the receiver output @xmath1 .",
    "this leads to the more general result stated in theorem  [ thm : main ] in the next section .",
    "we consider the following relay channel with a noiseless link as depicted in figure  [ fig : det - relay ] .",
    "we define a _ relay channel with a noiseless link _ @xmath56 as the channel where the input signal @xmath0 is received by the relay @xmath5 and the receiver @xmath1 through a channel @xmath57 , and the relay can communicate to the receiver over a separate noiseless link of rate @xmath3 .",
    "we wish to communicate a message index @xmath58 = \\{1,2,\\ldots , 2^{nr}\\}$ ] reliably over this relay channel with a noiseless link.$ ] is interpreted to mean @xmath59 . ]",
    "we specify a @xmath60 code with an encoding function @xmath61 \\to \\mathcal{x}^n$ ] , a relay function @xmath62 $ ] , and the decoding function @xmath63 \\to [ 2^{nr}]$ ] .",
    "the probability of error is defined by @xmath64 with the message @xmath65 distributed uniformly over @xmath66 $ ] .",
    "the capacity @xmath11 is the supremum of the rates @xmath67 for which @xmath68 can be made to tend to zero as @xmath69 .",
    "we state our main result .    for the relay channel @xmath70 with a noiseless link of rate @xmath3 from the relay to the receiver ,",
    "if the relay output @xmath71 is a deterministic function of the input @xmath0 and the receiver output @xmath1 , then the capacity is given by @xmath72 [ thm : main ]    the converse is immediate from the simple application of the max - flow min - cut theorem on information flow  ( * ? ? ?",
    "* section 15.10 ) .",
    "the achievability has several interesting features .",
    "first , as we will show in the next section , a novel application of random binning achieves the cut - set bound . in this coding scheme",
    ", the relay simply sends the hash index of its received output @xmath27 .",
    "what is perhaps more interesting is that the same capacity can be achieved also via the well - known `` compress - and - forward '' coding scheme of cover and el gamal  @xcite . in this coding scheme",
    ", the relay compresses its received output @xmath27 as in wyner ",
    "ziv source coding with the ultimate receiver output @xmath32 as side information .    in both coding schemes ,",
    "every bit of relay information carries one bit of information about the channel input , although the relay does not know the channel input . and",
    "the relay information can be summarized in a manner completely independent of geometry ( random binning ) or completely dependent on geometry ( random covering ) .    more surprisingly , we can partition the relay space using both random binning and random covering .",
    "thus , a combination of `` hash - and - forward '' and `` compress - and - forward '' achieves the capacity .",
    "the next section proves the achievability using the `` hash - and - forward '' coding scheme .",
    "the `` compress - and - forward '' scheme is deferred to section  [ sec : second ] and the combination will be discussed in sections  [ sec : discuss ] and [ sec : third ] .",
    "we combine the usual random codebook generation with list decoding and random binning of the relay output sequences :    _ codebook generation .",
    "_ generate @xmath43 independent codewords @xmath73 of length @xmath74 according to @xmath75 . independently , assign all possible relay output sequences in @xmath76 into @xmath77 bins uniformly at random .    _",
    "_ to send the message index @xmath78 $ ] , the transmitter sends the codeword @xmath73 . upon receiving the output sequence @xmath27",
    ", the relay sends the bin index @xmath79 to the receiver .    _ decoding .",
    "_ let @xmath80  ( * ? ? ?",
    "* section 7.6 ) denote the set of jointly typical sequences @xmath81 under the distribution @xmath82 .",
    "the receiver constructs a list @xmath83 , ( x^n(w ) , y^n ) \\in a_{\\epsilon}^{(n)}\\}\\ ] ] of codewords @xmath73 that are jointly typical with @xmath32 . since the relay output @xmath5 is a deterministic function of @xmath84 , then for each codeword @xmath73 in @xmath85 , we can determine the corresponding relay output @xmath86 exactly .",
    "the receiver declares @xmath87 was sent if there exists a unique codeword @xmath73 with the corresponding relay bin index @xmath88 matching the true bin index @xmath79 received from the relay .",
    "_ analysis of the probability of error . _ without loss of generality , assume @xmath89 was sent .",
    "the sources of error are as follows ( see figure  [ fig : scheme1 ] ) :    1 .",
    "the pair @xmath90 is not typical .",
    "the probability of this event vanishes as @xmath74 tends to infinity .",
    "the pair @xmath90 is typical , but there is more than one relay output sequence @xmath86 with the observed bin index , i.e. , @xmath91 . by markov",
    "s inequality , the probability of this event is upper bounded by the expected number of codewords in @xmath85 with the corresponding relay bin index equal to the true bin index @xmath92 . since the bin index",
    "is assigned independently and uniformly , this is bounded by @xmath93 which vanishes asymptotically as @xmath94 if @xmath95 .",
    "the pair @xmath90 is typical and there is exactly one @xmath96 matching the true relay bin index , but there is more than one codeword @xmath73 that is jointly typical with @xmath32 and corresponds to the same relay output @xmath27 , i.e. , @xmath97 .",
    "the probability of this kind of error is upper bounded by @xmath98 which vanishes asymptotically if @xmath99 .",
    "the general relay channel was introduced by van der meulen  @xcite .",
    "we refer the readers to cover and el gamal  @xcite for the history and the definition of the general relay channel . for recent progress , refer to kramer et al .",
    "@xcite , el gamal et al .",
    "@xcite , and the references therein .",
    "we recall the following achievable rate for the general relay channel investigated in  @xcite .    for any relay channel @xmath100 , the capacity @xmath101 is lower bounded by @xmath102 where the supremum is taken over all joint probability distributions of the form @xmath103 subject to the constraint @xmath104 [ thm : ceg ]    roughly speaking , the achievability of the rate in theorem  [ thm : ceg ] is based on a superposition of `` decode - and - forward '' ( in which the relay decodes the message and sends it to the receiver ) and `` compress - and - forward '' ( in which the relay compresses its own received signal without decoding and sends it to the receiver ) .",
    "this coding scheme turns out to be optimal for many special cases ; theorem  [ thm : ceg ] reduces to the capacity when the relay channel is degraded or reversely degraded  @xcite and when there is feedback from the receiver to the relay  @xcite .",
    "furthermore , for the semideterministic relay channel with the sender @xmath0 , the relay sender @xmath105 , the relay receiver @xmath106 and the receiver @xmath1 , el gamal and aref  @xcite showed that theorem  [ thm : ceg ] reduces to the capacity given by @xmath107 although this setup looks similar to ours , we note that neither nor theorem  [ thm : main ] implies the other . in a sense ,",
    "our model is more deterministic in the relay - to - receiver link , while the el gamal  aref model is more deterministic in the transmitter - to - relay link .",
    "a natural question arises whether our theorem  [ thm : main ] follows from theorem  [ thm : ceg ] as a special case .",
    "we first note that in the coding scheme described in section  [ sec : main ] , the relay does neither `` decode '' nor `` compress '' , but instead `` hashes '' its received output .",
    "indeed , as a coding scheme , this `` hash - and - forward '' appears to be a novel method of summarizing the relay s information .",
    "however , `` hash - and - forward '' is not the unique coding scheme achieving the capacity @xmath72 in the next section , we show that `` compress - and - forward '' can achieve the same rate .",
    "theorem  [ thm : main ] was proved using `` hash - and - forward '' in section  [ sec : first ] . here",
    "we argue that the capacity in theorem  [ thm : main ] can also be achieved by `` compress - and - forward '' .",
    "we start with a special case of theorem  [ thm : ceg ] .",
    "the `` compress - and - forward '' part ( cf .",
    "* theorem  6 ) ) , combined with the relay - to - receiver communication of rate @xmath3 , gives the achievable rate @xmath108 where the supremum is over all joint distributions of the form @xmath109 satisfying @xmath110 here the inequality comes from the wyner ",
    "ziv compression  @xcite of the relay s output @xmath27 based on the side information @xmath32 .",
    "the achievable rate captures the idea of decoding @xmath111 based on the receiver s output @xmath32 and the compressed version @xmath39 of the relay s output @xmath27 .",
    "we now derive the achievability of the capacity @xmath112 from an algebraic reduction of the achievable rate given by and .",
    "first observe that , because of the deterministic relationship @xmath113 , we have @xmath114 also note that , for any triple @xmath115 , if @xmath116 , there exists a distribution @xmath117 such that @xmath118 and @xmath119 .    henceforth , maximums are taken over joint distributions of the form @xmath120 with @xmath71 .",
    "we have @xmath121 on the other hand , @xmath122 thus , we have @xmath123 in words , `` compress - and - forward '' achieves the capacity .",
    "it is rather surprising that both `` hash - and - forward '' and `` compress - and - forward '' optimally convey the relay information to the receiver , especially because of the dual nature of compression ( random covering ) and hashing ( random binning ) .",
    "( and the hashing in `` hash - and - forward '' should be distinguished from the hashing in wyner  ziv source coding . ) the example in figure  [ fig : bsc ] illuminates the difference between the two coding schemes .    here",
    "the binary input @xmath124 is sent over a binary symmetric channel with cross - over probability @xmath125 , or equivalently , the channel output @xmath126 is given as @xmath127 where the binary additive noise @xmath128 is independent of the input @xmath0 . with no information on @xmath129 available at the transmitter or the receiver , the capacity is @xmath130    now suppose there is an intermediate node which observes @xmath129 and `` relays '' that information to the decoder through a side channel of rate @xmath3 . since @xmath131 is a deterministic function of @xmath132 , theorem  [ thm : main ] applies and we have @xmath133 for @xmath134    there are two ways of achieving the capacity .",
    "first , hashing .",
    "the relay hashes the entire binary @xmath135 into @xmath77 bins , then sends the bin index @xmath136 of @xmath137 to the decoder .",
    "the decoder checks whether a specific codeword @xmath73 is typical with the received output @xmath32 and then whether @xmath138 matches the bin index .",
    "next , covering . the relay compresses the state sequence @xmath137 using the binary lossy source code with rate @xmath3 .",
    "more specifically , we use the standard backward channel for the binary rate distortion problem ( see figure  [ fig : dist ] ) : @xmath139    here @xmath140 is the reconstruction symbol and @xmath141 is independent of @xmath142 ( and @xmath0 ) with parameter @xmath143 satisfying @xmath144 thus , using @xmath40 bits , the ultimate receiver can reconstruct @xmath145 .",
    "finally , decoding @xmath146 based on @xmath147 , we can achieve the rate @xmath148    in summary , the optimal relay can partition its received signal space into either random bins or hamming spheres .",
    "the situation is somewhat reminiscent of that of lossless block source coding .",
    "suppose @xmath149 is independent and identically distributed ( i.i.d . )",
    "@xmath150 . here",
    "are two basic methods of compressing @xmath111 into @xmath151 bits with asymptotically negligible error",
    ".    1 .   _ hashing . _",
    "the encoder simply hashes @xmath111 into one of @xmath152 indices . with high probability",
    ", there is a unique typical sequence with matching hash index .",
    "enumeration _",
    "the encoder enumerates @xmath153 typical sequences .",
    "then @xmath154 bits are required to give the enumeration index of the observed typical sequence . with high probability , the given sequence",
    "@xmath111 is typical .",
    "while these two schemes are apparently unrelated , they are both extreme cases of the following coding scheme .    1 .   _ covering with hashing .",
    "_ by fixing @xmath155 and generating independent sequences @xmath156 @xmath157 each i.i.d .",
    "@xmath158 , we can induce a set of @xmath159 coverings for the space of typical @xmath111 s .",
    "for each cover @xmath160 , there are @xmath161 sequences that are jointly typical with @xmath160 .",
    "therefore , by hashing @xmath111 into one of @xmath162 hash indices and sending it along the cover index , we can recover a typical @xmath111 with high probability .",
    "this scheme requires @xmath163 bits .",
    "now if we take @xmath164 independent of @xmath0 , then we have the case of hashing only .",
    "on the other hand , if we take @xmath165 , then we have enumeration only , in which case the covers are hamming spheres of radius zero .",
    "it is interesting to note that the combination scheme works under any @xmath155 .",
    "thus motivated , we combine `` hash - and - forward '' with `` compress - and - forward '' in the next section .",
    "here we show that a combination of `` compress - and - forward '' and `` hash - and - forward '' can achieve the capacity @xmath166 for the setup in theorem  [ thm : main ] .",
    "we first fix an _ arbitrary _",
    "conditional distribution @xmath117 and generate @xmath167 sequences @xmath168 @xmath169 each i.i.d . @xmath170 . then , with high probability , a typical @xmath27 has a jointly typical cover @xmath171 .",
    "( if there is more than one , pick the one with the smallest index .",
    "if there is none , assign @xmath172 . )",
    "there are two cases to consider , depending on our choice of @xmath117 ( and the input codebook distribution @xmath173 ) .",
    "first suppose @xmath174 if we treat @xmath171 as the relay output , @xmath39 is a deterministic function of @xmath27 and thus of @xmath175 .",
    "therefore , we can use `` hash - and - forward '' on @xmath39 sequences .",
    "( markov lemma  @xcite justifies treating @xmath171 as the output of the memoryless channel @xmath176 . )",
    "this implies that we can achieve @xmath177 but from and the functional relationship between @xmath5 and @xmath84 , we have @xmath178 therefore , @xmath179 which is achieved by the above `` compress - hash - and - forward '' scheme with @xmath173 and @xmath117 satisfying .    alternatively ,",
    "suppose @xmath180 then , we can easily achieve the rate @xmath181 by the `` compress - and - forward '' scheme .",
    "the rate @xmath182 suffices to convey @xmath39 to the ultimate receiver .",
    "but we can do better by using the remaining @xmath183 bits to further hash @xmath27 itself .",
    "( this hashing of @xmath27 should be distinguished from that of wyner  ziv coding which bins @xmath39 codewords . ) by treating @xmath184 as a new ultimate receiver output and @xmath5 as the relay output , `` hash - and - forward '' on top of `` compress - and - forward '' can achieve @xmath185 since @xmath186 and @xmath187 the achievable rate in reduces to @xmath188 thus , by maximizing over input distributions @xmath173 , we can achieve the capacity for either case or .",
    "it should be stressed that our combined `` compress - hash - and - forward '' is optimal , regardless of the covering distribution @xmath117 . in other words ,",
    "any covering ( geometric partitioning ) of @xmath27 space achieves the capacity if properly combined with hashing ( nongeometric partitioning ) of the same space .",
    "in particular , taking @xmath189 leads to `` hash - and - forward '' while taking the optimal covering distribution @xmath190 for and in section  [ sec : second ] leads to `` compress - and - forward '' .",
    "in this section , we show that theorem  [ thm : main ] confirms the following conjecture by ahlswede and han  @xcite on the capacity of channels with rate - limited state information at the receiver , for the special case in which the state is a deterministic function of the channel input and the output .",
    "first , we discuss the general setup considered by ahlswede and han , as shown in figure  [ fig : ah ] .    here",
    "we assume that the channel @xmath191 has independent and identically distributed state @xmath137 and the decoder can be informed about the outcome of @xmath137 via a separate communication channel at a fixed rate @xmath3 .",
    "ahlswede and han offered the following conjecture on the capacity of this channel .",
    "the capacity of the state - dependent channel @xmath191 as depicted in figure  [ fig : ah ] with rate - limited state information available at the receiver via a separate communication link of rate @xmath3 is given by @xmath192 where the maximum is over all joint distributions of the form @xmath193 such that @xmath194 and the auxiliary random variable @xmath142 has cardinality @xmath195 .",
    "it is immediately seen that this problem is a special case of a relay channel with a noiseless link ( figure  [ fig : det - relay ] ) . indeed",
    ", we can identify the relay output @xmath5 with the channel state @xmath129 and identify the relay channel @xmath196 with the state - dependent channel @xmath197 .",
    "thus , the channel with rate - limited state information at the receiver is a relay channel in which the relay channel output @xmath5 is independent of the input @xmath0 .",
    "the binary symmetric channel example in section  [ sec : discuss ] corresponds to this setup .",
    "now when the channel state @xmath129 is a deterministic function of @xmath198 , for example , @xmath131 as in the binary example in section  [ sec : discuss ] , theorem  [ thm : main ] proves the following .    for the state - dependent channel @xmath191 with state information available at the decoder via a separate communication link of rate @xmath3 ,",
    "if the state @xmath129 is a deterministic function of the channel input @xmath0 and the channel output @xmath1 , then the capacity is given by @xmath199    our analysis of `` compress - and - forward '' coding scheme in section  [ sec : second ] shows that reduces to , confirming the ahlswede ",
    "han conjecture when @xmath129 is a function of @xmath132 . on the other hand , our proof of achievability ( section  [ sec : first ] ) shows that `` hash - and - forward '' is equally efficient for informing the decoder of the state information .",
    "even a completely oblivious relay can boost the capacity to the cut set bound , if the relay reception is fully recoverable from the channel input and the ultimate receiver output . and there are two basic alternatives for the optimal relay function",
    " one can either compress the relay information as in the traditional method of `` compress - and - forward , '' or simply hash the relay information .",
    "in fact , infinitely many relaying schemes that combine hashing and compression can achieve the capacity .",
    "while this development depends heavily on the deterministic nature of the channel , it reveals an interesting role of hashing in communication .",
    "r.  ahlswede and t.  s. han , `` on source coding with side information via a multiple - access channel and related problems in multi - user information theory , '' _ ieee trans .",
    "inform . theory _",
    "it-29 , no .  3 , pp .",
    "396412 , 1983 ."
  ],
  "abstract_text": [
    "<S> the capacity of a class of deterministic relay channels with the transmitter input @xmath0 , the receiver output @xmath1 , the relay output @xmath2 , and a separate communication link from the relay to the receiver with capacity @xmath3 , is shown to be @xmath4 thus every bit from the relay is worth exactly one bit to the receiver . </S>",
    "<S> two alternative coding schemes are presented that achieve this capacity . the first scheme , `` hash - and - forward '' , is based on a simple yet novel use of random binning on the space of relay outputs , while the second scheme uses the usual `` compress - and - forward '' . in fact </S>",
    "<S> , these two schemes can be combined together to give a class of optimal coding schemes . as a corollary , this relay capacity result confirms a conjecture by ahlswede and han on the capacity of a channel with rate - limited state information at the decoder in the special case when the channel state is recoverable from the channel input and the output . </S>"
  ]
}