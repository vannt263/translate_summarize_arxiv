{
  "article_text": [
    "this paper studies a generalized linear inverse problem [ @xcite ] , called the _ unfolding problem _ [ @xcite ] , arising in data analysis at the large hadron collider ( lhc ) at cern , the european organization for nuclear research .",
    "the lhc is the world s largest and most powerful particle accelerator .",
    "it collides two beams of protons in order to study the properties and interactions of elementary particles produced in such collisions .",
    "the trajectories and energies of these particles are recorded using gigantic underground particle detectors and the vast amounts of data produced by these experiments are analyzed in order to draw conclusions about fundamental laws of physics . due to their complex structure and huge quantity ,",
    "the analysis of these data poses significant statistical and computational challenges .",
    "experimental high energy physicists use the term `` unfolding '' to refer to correcting the distributions measured at the lhc for the limited resolution of the particle detectors .",
    "let @xmath1 be some physical quantity of interest measured in the detector .",
    "this could , for example , be the energy , mass or production angle of a particle . due to the noise induced by the detector ,",
    "we are only able to observe a stochastically _ smeared _ or _ folded _",
    "version @xmath2 of this quantity . as a result ,",
    "the observed distribution of @xmath2 is a `` blurred '' version of the true , physical distribution of @xmath1 and the task is to use the observed values of @xmath2 to estimate the distribution of  @xmath1 .",
    "each year , the experimental collaborations working with lhc data produce dozens of physics results that make use of unfolding .",
    "recent examples include studies of the characteristics of jets [ @xcite ] , the transverse momentum distribution of @xmath3 bosons [ @xcite ] and charge asymmetry in top - quark pair production [ @xcite ] , to name a few .",
    "the main challenge in unfolding is the ill - posedness of the problem in the sense that a simple inversion of the forward mapping from the true space into the smeared space is unstable with respect to small perturbations of the data [ @xcite ] . as such",
    ", the trivial maximum likelihood solution of the problem often exhibits spurious high - frequency oscillations .",
    "these oscillations can be tamed by regularizing the problem , which is done by taking advantage of additional a priori knowledge about plausible solutions .",
    "an additional complication is the non - gaussianity of the data which follows from the fact that both the true and the smeared observations are realizations of two interrelated poisson point processes , which we denote by @xmath4 and @xmath5 , respectively . as such , unfolding is an example of a _ poisson inverse problem _ [ @xcite ] , where the intensity function @xmath6 of the true process @xmath4 is related to the intensity function @xmath7 of the smeared process @xmath5 via a fredholm integral operator @xmath8 , that is , @xmath9 , where @xmath8 represent the response of the detector .",
    "the task at hand is then to estimate and make inferences about the true intensity @xmath6 given a single observation of the smeared process @xmath5 . due to the poisson nature of the data , many standard techniques based on a gaussian likelihood function , such as tikhonov regularization ,",
    "are only approximately valid for unfolding .",
    "furthermore , estimators properly taking into account the poisson distribution of the observations are rarely available in a closed form , making the problem computationally challenging .    at present , the unfolding methodology used in lhc data analysis is not well established [ @xcite ] .",
    "the two main approaches are the expectation  maximization ( em ) algorithm with an early stopping [ @xcite ] and a certain variant of tikhonov regularization [ @xcite ] . in high energy physics ( hep )",
    "terminology , the former is called the _ dagostini iteration _ and the latter , somewhat misleadingly , _ svd unfolding _",
    "( with svd referring to the singular value decomposition ) .",
    "in addition , a hep - specific heuristic , called _ bin - by - bin unfolding _ , which provably accounts for smearing effects incorrectly through a multiplicative efficiency correction , has been widely used .",
    "recently , @xcite proposed a bayesian solution to the problem , but this seems to have seldom been used in practice thus far .",
    "the main problem with the dagostini iteration is that it is difficult to give a physical interpretation to the regularization imposed by early stopping of the iteration .",
    "svd unfolding , on the other hand , ignores the poisson nature of the observations and does not enforce the positivity of the solution .",
    "furthermore , both of these methods suffer from not dealing with two significant issues satisfactorily : ( 1 ) the choice of the regularization strength and ( 2 ) quantification of the uncertainty of the solution .",
    "the delicate problem of choosing the regularization strength is handled in most lhc analyses using nonstandard heuristics or , in the worst - case scenario , by simply fixing some value `` by hand . '' when quantifying the uncertainty of the unfolded spectrum , the analysts form approximate frequentist confidence intervals using simple error propagation , but little is known about the coverage properties of these intervals .    in this paper , we propose new statistical methodology aimed at addressing the two above - mentioned issues in a more satisfactory manner .",
    "our main methodological contributions are as follows :    empirical bayes selection of the regularization parameter using a monte carlo expectation ",
    "maximization algorithm [ @xcite , geman and mcclure ( @xcite ) , @xcite ] ;    frequentist uncertainty quantification using a combination of an iterative bias - correction procedure [ @xcite ] and bootstrap percentile intervals [ @xcite ] .    to the best of our knowledge ,",
    "neither of these techniques has been previously used to solve the hep unfolding problem .",
    "our framework also properly takes into account the poisson distribution of the observations , enforces the positivity constraint of the unfolded spectrum and imposes a curvature penalty on the solution with a clear physical interpretation .",
    "it is helpful to think of the unfolding problem as consisting of two separate , but related , subproblems : one of point estimation and the other of uncertainty quantification .",
    "we follow the standard practice of first constructing a point estimate of the unknown intensity and then using the variability of this point estimate to form frequentist confidence intervals . for the point estimation part ,",
    "the main challenge is to decide how to regularize the ill - posed problem and , in particular , how to choose the regularization strength .",
    "classical , well - known techniques for making this choice include the morozov discrepancy principle [ @xcite ] and cross - validation [ @xcite ] .",
    "@xcite study these techniques in the context of poisson inverse problems , while @xcite provide an alternative approach based on statistical hypothesis testing . from a bayesian perspective",
    ", the problem can be addressed using a bayesian hierarchical model [ @xcite ] which necessitates the choice of a hyperprior for the regularization parameter . on the other hand , empirical bayes selection of the regularization parameter using the marginal likelihood , which has the key advantage of not requiring the specification of a hyperprior , has received relatively less attention in the inverse problems literature . however , in many other fields of statistical inference , such as semiparametric regression [ @xcite , section  5.2 ] , neural networks [ @xcite , sections  3.5 and 5.7.2 ] and gaussian processes [ @xcite , chapter  5 ] ,",
    "the use of empirical bayes techniques has become part of standard practice .",
    "the approach we follow bears similarities to that of @xcite , where the marginal maximum likelihood estimator is used to select the regularization parameter in tomographic image reconstruction with poisson data .",
    "once we have formed an empirical bayes point estimate of the unknown intensity function , we would like to use the variability of this point estimator to quantify the uncertainty of the solution . in high energy physics , frequentist confidence statements are generally preferred over bayesian alternatives [ @xcite ] and we would hence like to obtain confidence intervals with good frequentist coverage properties . to achieve this ,",
    "the main challenge comes from the bias that is present in the point estimate in order to regularize the otherwise ill - posed problem .",
    "we show using simulations that this bias can seriously degrade the coverage probability of both bayesian credible intervals and standard bootstrap confidence intervals .",
    "we propose to remedy this problem by employing an iterative bias - correction technique [ @xcite ] and then using the variability of the bias - corrected point estimate to form the confidence intervals .",
    "quite remarkably , our simulation results indicate that such a technique achieves close - to - nominal coverage with only a modest increase in the length of the intervals .",
    "the paper is structured as follows .",
    "section  [ sec : physics ] gives a brief overview of the lhc detectors and explains the role that unfolding plays in these experiments .",
    "we then formulate in section  [ sec : formulation ] a forward model for the unfolding problem using poisson point processes .",
    "the proposed statistical methodology is explained in detail in section  [ sec : ebu ] which forms the backbone of this paper .",
    "this is followed by simulation studies in section  [ sec : simulations ] and a real - world data analysis scenario in section  [ sec : zboson ] which consists of unfolding of the @xmath0 boson invariant mass spectrum measured in the cms experiment at the lhc .",
    "we close the paper with some concluding remarks in section  [ sec : discconc ] .",
    "we also invite the reader to consult the online supplement [ @xcite ] which provides further simulation results and some technical details .",
    "the large hadron collider is a27  km long circular proton  proton collider located in an underground tunnel at cern in geneva , switzerland . with proton ",
    "proton collisions of up to13  tev , is the customary unit of energy used in particle physics , @xmath10",
    ". ] center - of - mass energy , the lhc is the world s most powerful particle accelerator .",
    "the protons are accelerated in bunches of billions of particles and bunches moving in opposite directions are led to collide at the center of four gigantic particle detectors called alice , atlas , cms and lhcb . in the lhc run 1 configuration , these bunches collided every 50 ns at the heart of the detectors , resulting in some 20 million collisions per second in each detector , out of which the few hundred most interesting ones were stored for further analysis . in lhc",
    "run 2 , which started in june 2015 , the collision rate is likely to be even higher .    out of the four detectors , atlas and cms",
    "are multipurpose experiments capable of performing a large variety of physics analyses ranging from the discovery of the higgs boson to precision studies of quantum chromodynamics .",
    "the other two detectors , alice and lhcb , specialize in studies of lead - ion collisions and @xmath11-hadrons , respectively . in what follows ,",
    "we focus on describing the data collection and analysis in the cms experiment , which is also the source of the data of our unfolding demonstration in section  [ sec : zboson ] , but similar principles also apply to atlas and , to some extent , to other high energy physics experiments .",
    "the cms experiment [ @xcite ] , an acronym for compact muon solenoid , is situated in an underground cavern along the lhc ring near the village of cessy , france .",
    "the detector , weighing a total of 12,500  tons , has a cylindrical shape with a diameter of 14.6  m and a length of 21.6  m. the construction , operation and data analysis of the experiment is conducted by an international collaboration of over 4000 scientists , engineers and technicians .",
    "when two protons collide at the center of cms , their energy is transformed into matter in the form of new particles . a small fraction of these particles are exotic , short - lived particles , such as the higgs boson or the top quark , which are at the center of the scientific interest of the high energy physics community . such particles decay almost instantly into more familiar , stable particles , such as electrons , muons or photons . using various subdetectors , the energies and trajectories of these particles are recorded in order to study the properties and interactions of the exotic particles created in the collision .    .",
    "each type of particle leaves its characteristic trace in the various subdetectors of the experiment .",
    "this enables identification of different particles as well as the measurement of their energies and trajectories .",
    "copyright : cern , for the benefit of the cms collaboration . ]    the layout of the cms detector is illustrated in figure  [ fig : cms_slice ] .",
    "the detector is immersed in a 3.8  t magnetic field created using a superconducting solenoid magnet .",
    "this magnetic field bends the trajectory of any charged particle traversing the detector .",
    "this enables the measurement of the particle s momentum , since the higher the momentum , the less the particle s trajectory is bent .",
    "cms consists of three layers of subdetectors : the tracker , the calorimeters and the muon detectors .",
    "the innermost detector is the silicon tracker , which consists of an inner layer of pixel detectors and an outer layer of microstrip detectors .",
    "when a charged particle passes through these semiconducting detectors , it leaves behind electron  hole pairs , and hence creates an electric signal .",
    "these signals are combined into a particle track using a kalman filter in order to reconstruct the trajectory of the particle .",
    "the next layer of detectors are the calorimeters , which are devices for measuring the energies of particles .",
    "the cms calorimeter system is divided into an electromagnetic calorimeter ( ecal ) and a hadron calorimeter ( hcal ) .",
    "both of these devices are based on the same general principle : they are made of extremely dense materials with the aim of stopping the particles passing through . in the process",
    ", a portion of the energy of these particles is converted into light in a scintillating material and the amount of light , which depends on the energy of the incoming particle , is measured using photodetectors inside the calorimeters .",
    "the ecal measures the energy of particles that interact mostly via the electromagnetic interaction , in other words , electrons , positrons and photons .",
    "the hcal , on the other hand , measures the energies of hadrons , that is , particles composed of quarks .",
    "these include , for example , protons , neutrons and pions .",
    "the hcal is also instrumental in measuring the energies of jets , that is , collimated streams of hadrons produced by quarks and gluons , and in detecting the so - called missing transverse energy , an energy imbalance caused by noninteracting particles , such as neutrinos , escaping the detector .",
    "the outermost layer of cms consists of muon detectors , whose task is to identify muons and measure their momenta .",
    "accurate detection of muons was of central importance in the design of cms since muons provide a clean signature for many exciting physics processes .",
    "this is because there is a very low probability for other particles , with the exception of noninteracting neutrinos , to penetrate through the cms calorimeter system .",
    "for example , the four - muon decay channel played an important role in the discovery of the higgs boson at cms [ @xcite ] .",
    "the information of all cms subdetectors is combined [ @xcite ] to identify the stable particles , that is , muons , electrons , positrons , photons and various types of hadrons , produced in each collision event ; see figure  [ fig : cms_slice ] .",
    "for example , a muon will leave a track in both the silicon tracker and the muon chamber , while a photon produces a signal in the ecal without an associated track in the tracker .",
    "the information of these individual particles is then used to reconstruct higher - level physics objects , such as jets or missing transverse energy .",
    "the need for unfolding arises because any quantity measured at the lhc detectors is corrupted by stochastic noise .",
    "for example , let @xmath12 be the true energy of an electron hitting the cms ecal",
    ". then the observed value of the energy follows to a good approximation the gaussian distribution @xmath13 , where the variance satisfies [ @xcite ] @xmath14 where @xmath15 , @xmath5 and @xmath16 are fixed constants .",
    "the measurement noise is not always additive .",
    "furthermore , for more sophisticated measurements , such as the ones combining information from several subdetectors or more than one particle , the distribution of the response is usually not available in a closed form . indeed",
    ", most analyses rely on detector simulations or auxiliary measurements to determine the detector response .",
    "it should be pointed out that not all lhc physics analyses directly rely on unfolding .",
    "the common factor between the examples given in section  [ sec : intro ] is that these are _ measurement _ analyses and not _ discovery _ analyses , meaning that these are analyses studying in detail the properties of some already known phenomenon .",
    "in such a case , the experimental interest often lies in the detailed particle - level shape of some distribution which can be obtained using unfolding , while discovery analyses are almost exclusively carried out in the smeared space . unfolding nevertheless plays an indirect role in attempts to discover new physics at the lhc .",
    "namely , discovery analyses often use unfolded results as inputs to their analysis chain .",
    "the need to unfold the measurements usually arises for the purposes of the following :    * _ comparison of experiments with different responses : _ the only direct way of comparing the spectra measured in two different experiments is to compare the unfolded measurements . *",
    "_ input to a subsequent analysis : _ certain tasks , such as the estimation of parton distributions functions or the fine - tuning of monte carlo event generators , typically require unfolded input spectra . * _ comparison with future theories : _ when unfolded spectra are published , theorists can directly use them to compare with any new theoretical predictions which might not have existed at the time of the original measurement .",
    "this justification is sometimes considered controversial since , alternatively , one could publish the response of the detector and the theorists could use it to smear their new predictions . *",
    "_ exploratory data analysis : _ the unfolded spectrum could reveal hidden features and structure in the data which are not considered in any of the existing theoretical predictions .",
    "according to the cern document server ( https://cds.cern.ch/ ) , the cms experiment published in 2012 a total of 103 papers out of which 16 made direct use of unfolding and many more indirectly relied on unfolded results .",
    "that year , unfolding was most often used in studies of quantum chromodynamics ( 4  papers ) , forward physics ( 4 ) and properties of the top quark ( 3 ) .",
    "most of these results relied on the questionable bin - by - bin heuristic ( 8) , while the em algorithm ( 3 ) and various forms of penalization ( 6 ) were also used .",
    "we expect similar statistics to also hold for the other lhc experiments .",
    "in most situations in high energy physics , the data generation mechanism can be modeled as a _ poisson point process _ [ see , e.g. , @xcite ] .",
    "let @xmath17 be a compact interval on @xmath18 , @xmath6 a nonnegative function in @xmath19 and @xmath4 a discrete random measure on @xmath17 .",
    "then @xmath4 is a poisson point process on state space @xmath17 with intensity function @xmath6 if and only  if :    1 .",
    "@xmath20 with @xmath21 for every borel set @xmath22 ; 2 .",
    "@xmath23 are independent for pairwise disjoint borel sets @xmath24 .    in other words ,",
    "the number of points @xmath25 observed in the set @xmath26 is poisson distributed with mean @xmath27 and the number of points in disjoint sets are independent random variables .",
    "for the problem at hand , the poisson process @xmath4 represents the true , particle - level spectrum of events .",
    "the smeared , detector - level spectrum is represented by another poisson process @xmath5 .",
    "the process @xmath5 is assumed to have a state space @xmath28 , which is a compact interval on @xmath18 , and a nonnegative intensity function @xmath29 .",
    "the intensities of the two processes are related by a bounded linear operator @xmath30 so that @xmath9 . in what follows ,",
    "we assume @xmath8 to be a fredholm integral operator , that is , @xmath31 where the kernel @xmath32 .",
    "for the purposes of this paper , we assume that @xmath33 is known , although in reality there is usually an uncertainty associated with it ; see section  [ sec : discconc ] .",
    "the unfolding problem is then to estimate the true intensity @xmath6 given a single observation of the smeared poisson process  @xmath5 .",
    "this poisson inverse problem [ @xcite ] is ill - posed in the sense that in virtually all practical cases the pseudoinverse @xmath34 of the forward operator @xmath8 is an unbounded , and hence discontinuous , linear operator [ @xcite ] .",
    "this means that the nave approach of first estimating @xmath7 using , for example , a kernel density estimate @xmath35 and then estimating @xmath6 using @xmath36 is unstable with respect to fluctuations of @xmath35 .    to better understand the physical meaning of the kernel @xmath33 ,",
    "let us consider the unfolding problem at the point level .",
    "denoting by @xmath37 the true observables , the poisson point process @xmath4 can be written as @xmath38 , where @xmath39 is the dirac measure at @xmath40 and @xmath41 are independent random variables such that @xmath42 and the @xmath37 are identically distributed with probability density @xmath43 , where @xmath44 .",
    "when the particle corresponding to @xmath37 traverses the detector , the first thing that can happen is that it might not be observed at all due to the limited efficiency and acceptance of the device .",
    "mathematically , this corresponds to _ thinning _ of the poisson process .",
    "let @xmath45 be an indicator variable showing whether the point @xmath37 is observed ( @xmath46 ) or not ( @xmath47 ) .",
    "we assume that @xmath48 are independent and that the pairs @xmath49 are identically distributed",
    ". then the thinned true process is given by @xmath50 , where @xmath51 and the @xmath52 are the true points with @xmath46 .",
    "the thinned process @xmath53 is a poisson point process with intensity function @xmath54 , where @xmath55 is the efficiency of the detector for a true observation at @xmath56 .",
    "for each observed point @xmath57 , the detector measures a noisy value @xmath58 .",
    "we assume that the smeared observations @xmath59 are i.i.d . with probability density @xmath60 from this",
    ", it follows that the smeared observations @xmath59 constitute a poisson point process @xmath61 whose intensity function @xmath7 is given by @xmath62 we hence identify that the kernel @xmath33 in equation ( [ eq : fredinteq ] ) is given by @xmath63 notice that in the special case where @xmath64 , unfolding becomes a deconvolution problem [ @xcite ] for poisson point process observations .",
    "in this section we propose a novel combination of statistical methods for solving the high energy physics unfolding problem formalized in section  [ sec : formulation ] .",
    "the proposed methodology is based on the following key ingredients :    1 .",
    "discretization : a.   the smeared observations are discretized using a histogram .",
    "b.   the unknown particle - level intensity is modeled using a b - spline , that is , @xmath65 , where @xmath66 , are the b - spline basis functions . 2 .",
    "point estimation : 1 .   posterior mean estimation of the unknown basis coefficients @xmath67 ^{\\mathrm{t}}$ ] using a single - component metropolis",
    " hastings sampler .",
    "empirical bayes estimation of the scale @xmath68 of the regularizing smoothness prior @xmath69 using a monte carlo em algorithm .",
    "uncertainty quantification : 1 .",
    "iterative bias - correction of the point estimate @xmath70 .",
    "2 .   use of bootstrap percentile intervals of the bias - corrected intensity function to form pointwise frequentist confidence bands for  @xmath6 .",
    "this methodology enables a principled solution of the unfolding problem , including the choice of the regularization strength and frequentist uncertainty quantification .",
    "we explain below each of these steps in detail and argue why this particular choice of techniques provides a natural framework for solving the problem .      in applied situations , poisson inverse problems",
    "are almost exclusively studied in a form where both the observable process @xmath5 and the unobservable process @xmath4 are discretized .",
    "usually the first step is to discretize the observable process using a histogram . in most applications , this has to be done due to the discrete nature of the detector . in our case ,",
    "the observations are , at least in principle , continuous , but we still carry out the discretization due to computational reasons",
    ". indeed , in many analyses , there can be millions of observed collision events and treating each of these individually would not be computationally feasible .    in order to discretize the smeared process @xmath5 ,",
    "let @xmath71 be a partition of the smeared space @xmath28 into @xmath72 ordered intervals and let @xmath73 denote the number of points falling on interval @xmath74 , that is , @xmath75 .",
    "this can be seen as recording the observed points in a histogram with bin contents @xmath76 ^{\\mathrm{t}}}$ ] and is indeed the form of discretization most often employed in hep .",
    "this discretization is convenient since it now follows from @xmath5 being a poisson process that the @xmath73 are independent and poisson distributed with means @xmath77    in the true space @xmath17 , there is no need to settle only for histograms .",
    "instead , we consider a basis expansion of the true intensity @xmath6 , that is , @xmath78 , where @xmath79 is a sufficiently large dictionary of basis functions .",
    "substituting the basis expansion into equation  ( [ eq : smearedmean ] ) , we find that the means @xmath80 are given by @xmath81 where we have denoted @xmath82 consequently , unfolding reduces to estimating @xmath83 in the poisson regression problem @xmath84 for an ill - conditioned matrix @xmath85 .    since spectra in high energy physics are typically smooth functions , splines [ @xcite ] provide a particularly attractive way of representing the unknown intensity @xmath6 .",
    "let @xmath86 be a sequence of knots in the true space @xmath17 .",
    "then an order-@xmath87 spline with knots @xmath88 , is a piecewise polynomial whose restriction to each interval @xmath89 , is an order-@xmath87 polynomial ( i.e. , a polynomial of degree @xmath90 ) and which has @xmath91 continuous derivatives at each interior knot @xmath92 .",
    "an order-@xmath87 spline with @xmath93 interior knots has @xmath94 degrees of freedom . in this work ,",
    "we use exclusively order-4 cubic splines which consist of third degree polynomials and are twice continuously differentiable .",
    "note also that an order-1 spline yields a histogram representation of  @xmath6 .",
    "there exist various bases @xmath95 for expressing splines of arbitrary order .",
    "we use b - splines @xmath96 , that is , spline basis functions of minimal local support , because of their good numerical properties and conceptual simplicity .",
    "@xcite ( @xcite ) was among the first authors to use regularized b - spline estimators in statistical applications , with the approach later popularized by @xcite . in the hep unfolding literature , penalized",
    "maximum likelihood estimation with b - splines goes back to the work of @xcite and recent contributions using similar methodology include @xcite and @xcite .",
    "we use the matlab curve fitting toolbox to efficiently evaluate and perform basic operations on b - splines .",
    "these algorithms rely on the recursive use of lower - order b - spline basis functions ; for details , see @xcite .",
    "the nonnegativity of the intensity function @xmath6 is enforced by constraining @xmath83 to be in @xmath97 . since each of the b - spline basis functions @xmath96 , is nonnegative ,",
    "this is a sufficient condition for the nonnegativity of @xmath6 .",
    "it should be noted , however , that generally this is not a necessary condition for the nonnegativity of @xmath6 ( except for order-1 and order-2 b - splines ) . that is , when imposing @xmath98 , we are restricting ourselves to a proper subset of the set of positive splines which may incur a slight , but not restrictive , reduction in the versatility of the family of functions available to us [ @xcite ] .",
    "in contrast to most work on unfolding , we take a bayesian approach to estimation of the spline coefficients @xmath83 .",
    "that is , we estimate @xmath83 using the bayesian posterior=1 @xmath99=0 where the likelihood is given by the poisson regression model ( [ eq : poissonregressionproblem ] ) , @xmath100 the prior @xmath69 , which regularizes the otherwise ill - posed problem , depends on a hyperparameter @xmath68 , which controls the concentration of the prior and is analogous to the regularization parameter in the classical methods for solving inverse problems .",
    "we decided to use the bayesian approach for two reasons .",
    "first , it provides a natural interpretation for the regularization via the prior density @xmath69 , which should be chosen in such a way that most of its probability mass lies in physically plausible regions of the parameter space @xmath101 .",
    "second , the bayesian framework enables a straightforward , data - driven way of choosing the regularization strength @xmath68 using empirical bayes estimation as explained below in section  [ sec : empiricalbayes ] .    in order to regularize the problem ,",
    "let @xmath102 , and @xmath103 , and consider the truncated gaussian smoothness prior @xmath104 where the elements of the @xmath105 matrix @xmath106 are given by @xmath107 .",
    "the interpretation of this prior is that the total curvature of @xmath6 , characterized by @xmath108 , should be small .",
    "in other words , @xmath6 should be a relatively smooth function .",
    "the strength of the regularization is controlled by the hyperparameter @xmath68the larger the value of @xmath68 , the smoother @xmath6 is required to  be .",
    "the prior as defined by equation ( [ eq : smoothnessprior ] ) does not enforce any boundary conditions for the unknown intensity @xmath6 . as a result",
    ", the matrix @xmath106 has rank @xmath109 , and hence the prior is potentially improper ( this depends on the orientation of the null space of @xmath106 ) .",
    "although the posterior would still be a proper probability density , the rank deficiency of @xmath106 is undesirable since the empirical bayes approach requires a proper prior distribution .",
    "furthermore , without any boundary constraints , the unfolded intensity has an unnecessarily large variance near the boundaries .",
    "to address these issues , we use _ aristotelian boundary conditions _",
    "[ @xcite ] , where the idea is to condition the smoothness penalty on the boundary values @xmath110 and @xmath111 and then introduce additional hyperpriors for these values . since @xmath112 and @xmath113 , we can equivalently condition on @xmath114 . as a result ,",
    "the prior model becomes @xmath115 where @xmath116 .",
    "we model the boundaries using once again truncated gaussians : @xmath117 where @xmath118 are fixed constants .",
    "the full prior can then be written as @xmath119 where the elements of the @xmath105 matrix @xmath120 are given by @xmath121 the augmented matrix @xmath120 is positive definite , and hence equation ( [ eq : aristotelianprior ] ) defines a proper probability density .",
    "once the hyperparameter @xmath68 has been estimated using empirical bayes ( see section  [ sec : empiricalbayes ] ) , we plug its estimate @xmath122 into bayes rule ( [ eq : bayesrule ] ) to obtain the empirical bayes posterior @xmath123 .",
    "we then use the mean of this posterior as a point estimator @xmath124 of the spline coefficients @xmath83 , that is , @xmath125 , yielding the estimator @xmath126 of the unknown intensity @xmath6 .    of course , in practice",
    ", the posterior @xmath127 is not available in a closed form because of the intractable integral in the denominator of bayes rule  ( [ eq : bayesrule ] ) .",
    "hence , we need to resort to markov chain monte carlo ( mcmc ) [ @xcite ] sampling from the posterior and the posterior mean is then computed as the empirical mean of the monte carlo sample .",
    "unfortunately , the most elementary mcmc samplers are not well - suited for solving the unfolding problem : gibbs sampling is not computationally tractable since the full posterior conditionals do not belong to any of the standard families of probability distributions and the metropolis ",
    "hastings sampler with multivariate proposals is difficult to implement since the posterior can have very different scales for different components of @xmath128 .    to be able to efficiently sample from the posterior",
    ", we adopt the single - component metropolis ",
    "hastings sampler ( also known as the metropolis - within - gibbs sampler ) proposed by @xcite . denoting @xmath129 ^{\\mathrm{t}}}$",
    "] , the basic idea of the sampler is to approximate the full posterior conditionals @xmath130 of the gibbs sampler using a more tractable density [ @xcite ] .",
    "one then samples from this approximate full conditional and performs a metropolis ",
    "hastings acceptance step to correct for the approximation error . in our case , we take a second - order taylor expansion of the log full conditional , resulting in a gaussian approximation of this density . when the mean of the gaussian is nonnegative , we sample from its truncation to the nonnegative real line , and if the mean is negative , we replace the gaussian tail by an exponential distribution .",
    "further details on the mcmc sampler can be found in section  iii.c of @xcite , while the online supplement [ @xcite ] provides details on the convergence and mixing checks that were performed for the sampler .",
    "thebayesian approach to solving inverse problems is particularly attractive since it admits selection of the regularization strength @xmath68 using marginal maximum likelihood estimation . for a comprehensive introduction to this and related empirical bayes methods ,",
    "see , for example , chapter  5 of @xcite .",
    "the main idea in empirical bayes is to regard the marginal distribution @xmath131 appearing in the denominator of bayes rule ( [ eq : bayesrule ] ) as a parametric model for the data @xmath132 and then use standard frequentist point estimation techniques to estimate the hyperparameter @xmath68 .",
    "the _ marginal maximum likelihood estimator _ ( mmle ) of the hyperparameter @xmath68 is defined as the maximizer of @xmath131 with respect to @xmath68 .",
    "that is , we estimate @xmath68 using @xmath133 computing the mmle is nontrivial since we can not evaluate the high - dimensional integral in ( [ eq : defmmle ] ) either in a closed form or using standard numerical integration methods .",
    "monte carlo integration , where one samples @xmath134 from the prior @xmath69 and then approximates @xmath135 is also out of question .",
    "this is because , in the high - dimensional parameter space , most of the @xmath136 s fall on regions where the likelihood @xmath137 is numerically zero .",
    "hence , we would need an enormous sample size @xmath15 to get even a rough idea of the marginal likelihood @xmath138 .",
    "luckily , it is possible to circumvent these issues by using the expectation  maximization ( em ) algorithm [ @xcite ] to find the mmle . in the context of poisson inverse problems ,",
    "this approach was originally proposed by @xcite ( @xcite ) for tomographic image reconstruction and later studied and extended by @xcite , but has received little attention since then . when applied to the unfolding problem , the standard em prescription reads as follows .",
    "let @xmath139 be the complete data , in which case the complete - data log - likelihood is given by @xmath140 where we have used @xmath141 . in the e - step of the algorithm , one computes the expectation of the complete - data log - likelihood over the unknown spline coefficients @xmath83 conditional on the observations @xmath142 and the current hyperparameter @xmath143 : @xmath144 where the constant does not depend on @xmath68 .",
    "in the subsequent m - step , one maximizes the expected complete - data log - likelihood @xmath145 with respect to the hyperparameter @xmath68 .",
    "this maximizer is then used as the hyperparameter estimate on the next step of the algorithm : @xmath146 by theorem  1 of @xcite , each step of this iteration is guaranteed to increase the incomplete - data likelihood @xmath131 , that is , @xmath147 with this construction , the incomplete - data likelihood conveniently coincides with the marginal likelihood and , hence , the em algorithm enables us find the mmle @xmath148 of the hyperparameter  @xmath68 .",
    "the expectation in equation ( [ eq : mstep ] ) , @xmath149 again involves an intractable integral , but this time can be computed using monte carlo integration .",
    "we simply need to sample @xmath150 from the posterior @xmath151 and then replace the expectation by its monte carlo approximation : @xmath152 this monte carlo approximation is better behaved than that of equation  ( [ eq : naivemcintegration ] ) due to the appearance of the logarithm and due to the fact that the sampling is from the posterior instead of the prior .",
    "the posterior sample can be obtained using the single - component metropolis ",
    "hastings sampler described in section  [ sec : estimation ] .",
    "the resulting variant of the em  algorithm is called a _",
    "monte carlo expectation  maximization _",
    "( mcem ) _ algorithm _ [ @xcite ] .    to summarize , the mcem algorithm for finding the mmle of the hyperparameter @xmath68 iterates between the following two steps :    sample @xmath153 from the posterior @xmath154 and compute @xmath155    set @xmath156 .",
    "this algorithm has a rather intuitive interpretation .",
    "first , on the e - step , we use the current iterate @xmath143 to produce a sample of @xmath83 s from the posterior .",
    "since this sample summarizes our current best understanding of @xmath83 , we then tune the prior by varying @xmath68 on the m - step to match this sample as well as possible , and the value of @xmath68 that matches the posterior sample the best will then become the next iterate  @xmath157 .",
    "when @xmath69 is given by the aristotelian smoothness prior ( [ eq : aristotelianprior ] ) , the m - step of the mcem algorithm is available in a closed form .",
    "taking normalization into account , the prior density is given by @xmath158 where the normalization constant @xmath159 depends on the hyperparameter @xmath68 and satisfies @xmath160 .",
    "hence , @xmath161 where the constant does not depend on @xmath68 .",
    "plugging this into equation ( [ eq : qtilde ] ) , we find that the maximizer on the m - step is given by @xmath162    @xmath132observed data ; @xmath163initial guess ; @xmath164number of mcem iterations ; @xmath15size of the mcmc sample ; @xmath165starting point for the mcmc sampler ; @xmath148mmle of the hyperparameter @xmath68 ; set @xmath166 ; sample @xmath167 starting from @xmath168 using the single - component metropolis ",
    "hastings sampler of @xcite ; set @xmath169 compute @xmath170 ; @xmath171 .",
    "the resulting iteration for finding the mmle @xmath148 is summarized in algorithm  [ alg : mcem ] .",
    "the mcmc sampler is started from the empirical mean of the posterior sample of the previous iteration in order to facilitate the convergence of the markov chain . in this work ,",
    "we run the mcem algorithm for a fixed number of steps @xmath164 , but one could easily devise more elaborate stopping rules for the algorithm .",
    "note , however , that the optimal choice of this stopping rule and the mcmc sample size @xmath15 are , to a large extent , open problems [ @xcite ] .",
    "frequentist uncertainty quantification in nonparametric inference is generally considered to be a very challenging problem ; see , for example , chapter  6 of @xcite for an overview of some of the issues involved .",
    "common approaches considered in the literature include bootstrapping [ @xcite ] and various bayesian constructions [ see , e.g. , @xcite , @xcite and @xcite ] . in the case of classical nonparametric regression , bayesian intervals",
    "are often argued to have good frequentist properties based on the results of @xcite , which guarantee that the coverage probability _ averaged over the design points _ is close to the nominal value . such average coverage , however , provides no guarantees about pointwise coverage and the intervals can suffer from significant pointwise under- or overcoverage as demonstrated by @xcite .",
    "the main problem in building confidence intervals for the unknown intensity @xmath6 is the bias that is present in the estimator @xmath172 in order to regularize the problem .",
    "we show using simulations in section  [ sec : simulations ] that this bias is a major hurdle for both bayesian and bootstrap confidence intervals , resulting in major frequentist undercoverage in regions of sizable bias . to overcome this issue , we propose attacking the problem from a different perspective : instead of directly using the variability of @xmath172 to construct confidence intervals , we first iteratively reduce the bias of @xmath172 and then use the variability of the bias - corrected estimator @xmath173 to construct confidence intervals .",
    "this approach has similarities with the recent work of @xcite on uncertainty quantification in high - dimensional regression by de - biasing the estimator , but our problem setting and bias - correction method are different from theirs",
    ".    it may at first seem counterintuitive that reducing the bias of @xmath174 enables us to form improved confidence intervals  it is after all the bias that regularizes the otherwise ill - posed problem .",
    "it is indeed true that the iterative bias - correction described below increases the variance of the point estimator @xmath175 ; but at the same time the coverage performance of the intervals formed using @xmath175 improves at each iteration of the procedure .",
    "what is more , our simulations reported in section  [ sec : simulations ] indicate that , by stopping the bias - correction iteration early enough , it is possible to attain nearly nominal coverage with only a modest increase in interval length . in other words , one can use the iterative bias - correction to remove so much of the bias that the interval coverage probability is close to its nominal value , but the small amount of residual bias that remains in enough to regularize the interval length .",
    "this phenomenon is consistent with what @xcite observe in debiased @xmath176-regularized regression .",
    "our bias - correction approach is based on an iterative use of the bootstrap to estimate the bias of the point estimate @xmath177 .",
    "a similar approach has been previously used by @xcite and @xcite to debias estimators in generalized linear mixed models , but , to the best of our knowledge , this procedure has not been previously employed to improve the frequentist coverage of confidence intervals in ill - posed nonparametric inverse problems , such as the one studied here .",
    "@xmath178observed value of the estimator @xmath179 ; @xmath148estimated value of the hyperparameter @xmath68 ; @xmath180number of bias - correction iterations ; @xmath181size of the bootstrap sample ; @xmath15size of the mcmc sample ; @xmath182bias - corrected point estimate ; sample @xmath183 ; for each @xmath184 , compute @xmath185 by sampling @xmath15 observations from the posterior using the single - component metropolis ",
    "hastings sampler of @xcite ; compute @xmath186 ; set @xmath187 ; set @xmath188 , where @xmath189 denotes the element - wise product ; @xmath190 .    by definition ,",
    "the bias of @xmath124 is given by @xmath191 .",
    "the standard bootstrap estimate of this bias [ see , e.g. , @xcite ] replaces the actual value of @xmath83 by the observed value of the estimator @xmath124 which we denote by @xmath178 . in other words ,",
    "the bootstrap estimate of the bias is @xmath192 where in practice the expectation is usually replaced by its empirical version obtained using simulations .",
    "the estimated bias can then be subtracted from @xmath178 to obtain a bias - corrected estimator @xmath193 now , ignoring the bootstrap sampling error , the reason @xmath194 is not a perfectly unbiased estimator of @xmath83 is the fact that in equation ( [ eq : biashat0 ] ) the actual value of @xmath195 was replaced by an estimate @xmath178 .",
    "but since @xmath196 should be better than @xmath197 at estimating @xmath83 ( at least in the sense that @xmath194 should be less biased than  @xmath178 ) , we can replace @xmath178 in equation ( [ eq : biashat0 ] ) by @xmath196 to obtain a new estimate of the bias @xmath198 which gives rise to a new bias - corrected estimator @xmath199 the estimator @xmath200 can then be used to replace @xmath196 in equation ( [ eq : biashat1 ] ) which naturally leads us to the following _ iterative bias - correction _ procedure :    1 .",
    "estimate the bias : @xmath201 .",
    "2 .   compute the bias - corrected estimate : @xmath202 .",
    "the details of this procedure , when the expectation @xmath203 is replaced by its empirical version and the positivity constraint of @xmath83 is enforced by setting any negative entries to zero , are given in algorithm [ alg : bsbc ] .",
    "note that for computational reasons we keep the hyperparameter @xmath68 fixed to its estimated value @xmath148 instead of resampling it .      the bias - corrected spline coefficients @xmath182 are associated with a bias - corrected intensity function estimate @xmath204 . as the bias - corrected estimator @xmath175 has a smaller bias than the original estimator @xmath172 , the variability of @xmath173 can be used to construct confidence intervals for @xmath6 that have better coverage properties than the intervals based on the variability of @xmath172 .",
    "the basic approach we follow is to use the bootstrap to probe the sampling distribution of @xmath175 in order to construct pointwise _ pointwise _ frequentist confidence bands , that is , collections of random intervals @xmath205 $ ] that for any @xmath56 , @xmath206 and intensity function  @xmath6 aim to satisfy @xmath207 .",
    "this is in contrast with @xmath208 _ simultaneous _ confidence bands , which would satisfy for any @xmath206 and intensity function  @xmath6 the property @xmath209 ; see also the discussion in section  [ sec : discconc ] . ]",
    "confidence bands for @xmath6 .    to obtain the bootstrap sample ,",
    "we generate @xmath210 i.i.d .",
    "observations from the @xmath211 distribution , where @xmath212 is the maximum likelihood estimator of @xmath213 . for each resampled observation @xmath214 , we compute a resampled point estimate @xmath215 . again , for computational reasons , the hyperparameter @xmath216 is kept fixed to its estimated value @xmath148 for the observed datum  @xmath132 . the bias - correction procedure described in algorithm [ alg : bsbc ]",
    "is then run with the resampled point estimate @xmath217 to obtain a resampled bias - corrected point estimate @xmath218 , leading to a resampled bias - corrected intensity function @xmath219 .",
    "the sample @xmath220 is a bootstrap representation of the sampling distribution of  @xmath175 and can be used to construct various forms of approximate confidence intervals for @xmath6 [ @xcite ] .",
    "the simplest approach is to simply use the pointwise standard deviations of the bootstrap sample to form standard error intervals for @xmath6 .",
    "however , it was concluded using simulations that the standard error intervals suffer from a slight reduction in coverage due to the skewness of the sampling distribution induced by the positivity constraint , especially in areas of low intensity values . to account for this effect ,",
    "we use instead the bootstrap percentile intervals .",
    "more specifically , for every @xmath56 , an approximate @xmath208 confidence interval for @xmath221 is given by @xmath222 $ ] , where @xmath223 denotes the @xmath224-quantile of the bias - corrected bootstrap sample @xmath225 evaluated at point  @xmath56 .",
    "the formal justification for these intervals comes from an implicit use of a transformation that ( approximately ) normalizes the sampling distribution of @xmath226 ; see section  13.3 of @xcite .",
    "we first demonstrate the proposed unfoldingmethodology using simulated data .",
    "the data were generated using a two - component gaussian mixture model on top of a uniform background and smeared by convolving the particle - level intensity with a gaussian density .",
    "specifically , the true process @xmath4 had the intensity @xmath227 where @xmath228 is the expected number of true observations , @xmath229 denotes the lebesgue measure of @xmath17 and the mixing proportions @xmath230 sum up to one and were set to @xmath231 , @xmath232 and @xmath233 .",
    "we consider the sample sizes @xmath234 , @xmath235 and @xmath236 , which we refer to as the small , medium and large sample size experiments , respectively . the true space @xmath17 and the smeared space @xmath28 were both taken to be the interval @xmath237 $ ] .",
    "the true points @xmath37 were smeared with additive gaussian noise of zero mean and unit variance .",
    "points smeared beyond the boundaries of @xmath28 were discarded from further analysis . with this setup , the smeared intensity is given by the convolution @xmath238 note that this setup corresponds to the classically most difficult class of deconvolution problems since the gaussian error has a supersmooth probability density [ @xcite ] .",
    "the smeared space @xmath28 was discretized using @xmath239 histogram bins of uniform size , while the true space @xmath17 was discretized using order-4 b - splines with @xmath240 uniformly placed interior knots , resulting in @xmath241 unknown basis coefficients . with these choices ,",
    "the condition number of the smearing matrix @xmath242 was @xmath243 , indicating that the problem is severely ill - posed .",
    "the boundary hyperparameters were set to @xmath244 .",
    "all experiments reported in this paper were implemented in matlab and the computations were carried out on a desktop computer with a quad - core 2.7  ghz intel core i5 processor .",
    "the outer bootstrap loop was parallelized to the four cores of this setup .    with the exception of the number of bias - correction iterations , all the algorithmic parameters were fixed to the same values for the three different sample sizes .",
    "the mcem algorithm was started using the initial hyperparameter @xmath245 and was run for 30 iterations .",
    "the mcmc sampler was started from the nonnegative least - squares spline fit to the smeared data , that is , @xmath246 , where the elements of @xmath247 are given by equation ( [ eq : kij ] ) with the smearing kernel @xmath248 , where @xmath249 denotes the dirac delta function .",
    "the sampler was then used to obtain 1000 post - burn - in observations from the posterior .",
    "the number of bias - correction iterations was set to @xmath250 for the small sample size experiment , while @xmath251 iterations was used with the medium and the large sample size . in each case , @xmath252 bootstrap observations were used to obtain the bias estimates and the bias - corrected percentile intervals were obtained using @xmath253 bootstrap  observations .        for each sample size , 30 mcem iterations sufficed for convergence to a stable point estimate of the hyperparameter  @xmath68 , with faster convergence for larger sample sizes . in each case",
    ", there was little monte carlo variation in the hyperparameter estimates .",
    "the estimated hyperparameters were @xmath254 , @xmath255 and @xmath256 in the small , medium and large sample size cases , respectively . a figure illustrating the convergence of the mcem iteration is given in the online supplement [ @xcite , figure  3(a ) ] .",
    "in each case , the running time to obtain @xmath148 , and thence the point estimate @xmath172 , was approximately 8 minutes , while the time it took to form the confidence intervals was approximately 14 hours for the medium and large sample sizes and 39 hours for the small sample size , where three times as many bias - correction iterations were performed .",
    "while these are fairly long computations on a desktop setup , it should be noted that it is easy to further parallelize the bootstrap computations and the running time could be cut down substantially on a modern massively parallel cloud computing platform .",
    "figure  [ fig : gmmunfolded ] shows the true intensities @xmath6 , the unfolded intensities @xmath172 and the bias - corrected unfolded intensities @xmath175 for the three sample sizes .",
    "the confidence bands consist of 95% pointwise percentile intervals obtained by bootstrapping the bias - corrected point estimate as described in section  [ sec : uq ] . in each case",
    ", the unfolded intensity beautifully captures the two - humped shape of the true intensity , and , unsurprisingly , the more data is available , the better the quality of the estimate .",
    "it is also apparent that the estimators are biased near the peaks and the trough of the true intensity , and the relative size of this bias is larger for smaller sample sizes . in each case",
    ", the bias can be reduced using the iterative bias - correction procedure , but at the cost of an increased variance of the point estimator .",
    "@c@    ,  @xmath235 and @xmath257 .",
    "the confidence intervals are the 95% pointwise percentile intervals of the bias - corrected estimator .",
    "the number of bias - correction iterations was 5 in cases and and 15 in case .,title=\"fig : \" ] +   + ,  @xmath235 and @xmath257 .",
    "the confidence intervals are the 95% pointwise percentile intervals of the bias - corrected estimator .",
    "the number of bias - correction iterations was 5 in cases and and 15 in case .,title=\"fig : \" ] +   + ,  @xmath235 and @xmath257 .",
    "the confidence intervals are the 95% pointwise percentile intervals of the bias - corrected estimator .",
    "the number of bias - correction iterations was 5 in cases and and 15 in case .,title=\"fig : \" ] +    for each sample size , the confidence intervals perform well at covering the true intensity without being excessively long or wiggly . for these particular realizations ,",
    "the intervals cover the truth at every point @xmath56 in the large and small sample size cases , while with the medium sample size the intervals cover everywhere except for a short section near @xmath258 and another one near @xmath259 .",
    "we study the coverage performance of the bias - corrected intervals shown in figure  [ fig : gmmunfolded ] using a gaussian approximation to the poisson likelihood described in section  2.2 of the online supplement [ @xcite ] . with this approximation ,",
    "finding of the point estimator @xmath124 effectively reduces to a ridge regression ( i.e. ,  tikhonov regularization ) problem for which the solution can be computed in a fraction of the time required for the posterior mean with the full poisson likelihood .",
    "this enables us to perform an otherwise computationally infeasible empirical coverage study for the bias - corrected intervals .",
    "the gaussian approximated intervals look similar to the full intervals ( see section  3.2 of the supplement [ @xcite ] for a detailed comparison ) , and we expect the coverage of the full intervals to be similar to or better than that of the gaussian intervals .    the coverage of the bias - corrected intervals is compared to alternative bootstrap and empirical bayes intervals .",
    "the empirical bayes intervals we consider are the 95% equal - tailed credible intervals induced by the empirical bayes posterior @xmath260 .",
    "the bootstrap intervals that we compare with are the standard percentile intervals without bias - correction , which are obtained from our procedure by setting the number of bias - correction iterations @xmath261 to zero .",
    "we also consider the bootstrap basic intervals which for point @xmath56 are given by @xmath262 $ ] , where @xmath263 is the @xmath224-quantile of the bootstrap sample of nonbias - corrected unfolded intensities obtained using a fixed hyperparameter  @xmath148 and the resampling scheme @xmath264 with @xmath265 .",
    "the bootstrap intervals are formed using the gaussian approximation , while the empirical bayes intervals are for the full poisson likelihood .",
    "figure   compares the empirical coverage of the iteratively bias - corrected intervals with 5 bias - correction iterations to that of empirical bayes credible intervals as well as the nonbias - corrected bootstrap percentile and basic intervals .",
    "figure   shows the empirical coverage of the bias - corrected intervals as the number of bias - correction iterations is varied between 0 and  50 .",
    "all intervals are formed for 95% nominal coverage shown by the horizontal line.,title=\"fig : \" ] +   + .",
    "figure   compares the empirical coverage of the iteratively bias - corrected intervals with 5 bias - correction iterations to that of empirical bayes credible intervals as well as the nonbias - corrected bootstrap percentile and basic intervals .",
    "figure   shows the empirical coverage of the bias - corrected intervals as the number of bias - correction iterations is varied between 0 and  50 .",
    "all intervals are formed for 95% nominal coverage shown by the horizontal line.,title=\"fig : \" ] +    figure  [ fig : gmm10000coverage](a ) shows the empirical coverage of the bias - corrected intervals obtained using the gaussian approximation and the various alternatives in the medium sample size case for 1000 repeated observations from the smeared process @xmath5 .",
    "the coverage of the bias - corrected intervals is close to the nominal value of 95% throughout the spectrum ( average coverage 94.6% ) , although a minor reduction in coverage due to the residual bias can be observed near the peak at @xmath266 , where the lowest coverage is 91.7% .",
    "a very different behavior is observed with the empirical bayes intervals : these intervals overcover for most parts of the spectrum , but at regions of significant bias they undercover .",
    "in particular , near @xmath267 , the worst - case coverage of these intervals is 74.5% .",
    "the standard bootstrap intervals , on the other hand , consistently undercover ( average coverage 81.6% for the percentile and 79.2% for the basic intervals ) and the nonbias - corrected percentile intervals in particular fail to cover near the peak at @xmath266 , with empirical coverage of just 33.1% .    to gain further insight into the improvement in coverage provided by the bias - correction , we plot in figure  [ fig : gmm10000coverage](b ) the empirical coverage for various numbers of bias - correction iterations . for @xmath268 ,",
    "the coverage is simply that of the standard percentile intervals .",
    "we see that a single bias - correction iteration already improves the coverage significantly , with further iterations always improving the performance .",
    "in fact , increasing @xmath180 from 5 , we are able to do away with the dip in figure  [ fig : gmm10000coverage](a ) around @xmath266 .",
    "however , we preferred settling with @xmath251 , as increasing the number of iterations produced increasingly wiggly intervals .",
    "interestingly , the coverage of the bias - corrected intervals does not seem to suffer from the fact that all the bootstrap computations were performed without resampling the hyperparameter estimate  @xmath148 , which suggests that the uncertainty of @xmath148 plays only a minor role in the overall uncertainty of @xmath172 .",
    "the coverage patterns reported here for @xmath269 repeat themselves in the small and large sample size situations , with starker differences between the methods with the small sample size and milder differences with the large sample size . in the large sample size case , the bias - corrected intervals effectively attain nominal coverage ( average coverage 94.7% ) , with no major deviations from the nominal due to residual bias .",
    "the bootstrap intervals , on the other hand , undercover , while the empirical bayes intervals overcover , except for the peak at @xmath270 where slight undercoverage is observed .",
    "in the most difficult small sample size case , both the bootstrap and the empirical bayes intervals have major problems with undercoverage , while the bias - corrected intervals achieve close - to - nominal coverage on most parts of the spectrum except for some undercoverage at @xmath266 , where the worst - case coverage is 85.0% .",
    "see the supplement [ @xcite ] for detailed coverage plots for the small and large sample size experiments as well as plots showing a realization of the intervals for the different methods and numbers of bias - correction iterations .",
    "the supplement also includes a comparison with hierarchical bayes intervals which are observed to provide qualitatively similar coverage performance as the empirical bayes intervals , although with improved worst - case coverage for some choices of the hyperprior .",
    "in this section we illustrate the proposed unfolding framework using real data from the cms experiment at the large hadron collider . in particular , we unfold the @xmath0 boson invariant mass spectrum published in @xcite .",
    "the @xmath0  boson , which is produced in copious quantities at the lhc , is a mediator of the weak interaction .",
    "the particle is very short - lived and decays almost instantly into other elementary particles .",
    "the decay mode considered here is the decay of a @xmath0 boson into a positron and an electron , @xmath271 .",
    "the original purpose of these data was to calibrate and measure the resolution of the cms electromagnetic calorimeter , but they also serve as an excellent testbed for unfolding since the true mass spectrum of the @xmath0 boson is known with great precision from previous measurements .    the electron and the positron produced in the decay of the @xmath0 boson are first detected in the cms silicon tracker after which their energies  @xmath272 , are measured by stopping the particles at the ecal ; see section  [ sec : physicsdetectors ] . from this information",
    ", one can compute the _ invariant mass _ @xmath3 of the electron  positron system defined by the equation @xmath273 where @xmath274 , are the momenta of the two particles and the equation is written using the natural units where the speed of light @xmath275 . since @xmath276 , where @xmath277 is the rest mass of the electron , one can reconstruct the invariant mass @xmath3 using only the ecal energy deposits @xmath278 and the opening angle between the two tracks in the silicon tracker .",
    "the invariant mass @xmath3 is preserved in particle decays .",
    "furthermore , it is invariant under lorentz transformations and has therefore the same value in every frame of reference .",
    "this means that the invariant mass of the @xmath0  boson , which is simply its rest mass @xmath87 , is equal to the invariant mass of the electron ",
    "positron system , @xmath279 .",
    "it follows that measurement of the invariant mass spectrum of the electron ",
    "positron pair enables us to measure the mass spectrum of the @xmath0 boson itself .    due to the time - energy uncertainty principle",
    ", the @xmath0 boson does not have a unique rest mass @xmath87 .",
    "instead , the mass follows the cauchy distribution , also known in particle physics as the _ breit  wigner distribution _",
    ", with density @xmath280 where @xmath281 is the mode of the distribution ( often simply called _ the _ mass of the @xmath0 boson ) and @xmath282 is the full width of the distribution at half maximum [ @xcite ] . since the contribution of background processes to the electron ",
    "positron channel near the @xmath0 peak is negligible [ @xcite ] , the underlying true intensity @xmath283 is proportional to @xmath284 .",
    "the dominant source of smearing in measuring the @xmath0 boson invariant mass  @xmath87 is the measurement of the energy deposits @xmath278 in the ecal .",
    "the resolution of these energy deposits is in principle described by equation  ( [ eq : ecal_res ] ) . however , when working on a small enough invariant mass interval around the @xmath0  peak , one can in practice assume that the response is given by a convolution with a fixed - width gaussian kernel .",
    "moreover , the left tail of the kernel is typically replaced with a more slowly decaying tail function in order to account for energy losses in the ecal .",
    "it is therefore customary to model the response using the so - called _ crystal ball _ (",
    "cb ) _ function _ [ @xcite ] given by @xmath285 \\\\[-8pt ] \\nonumber & & \\qquad = \\cases { c e^{-{(m-\\delta m)^2}/{(2\\sigma^2 ) } } , & \\quad $ \\displaystyle\\frac{m-\\delta m}{\\sigma } > -\\alpha$ , \\vspace*{2pt } \\cr \\displaystyle c \\biggl ( \\frac{\\gamma}{\\alpha } \\biggr)^\\gamma e^{-{\\alpha^2}/{2 } } \\biggl ( \\frac{\\gamma}{\\alpha } - \\alpha- \\frac{m - \\delta m}{\\sigma } \\biggr)^{-\\gamma } , & \\quad $ \\displaystyle\\frac{m - \\delta m}{\\sigma } \\leq- \\alpha$ , } \\ ] ] where @xmath286 , @xmath287 and @xmath16 is a normalization constant chosen so that the function is a probability density .",
    "this function is a gaussian density with mean @xmath288 and variance @xmath289 , where the left tail is replaced with a power - law function .",
    "the parameter @xmath224 controls the location of the transition from exponential decay into power - law decay and the parameter @xmath290 controls the decay rate of the power - law tail .",
    "the forward mapping @xmath8 in equation ( [ eq : fredinteq ] ) is then given by a convolution where the kernel is the cb function , that is , @xmath291 .",
    "the data set we use is a digitized version of the lower left - hand plot of figure  11 in @xcite .",
    "these data correspond to an integrated luminosity .",
    "] of @xmath292 collected at the lhc in 2011 at the @xmath293 center - of - mass energy and include 67 778 electron  positron events with the measured invariant mass between 65  gev and 115  gev .",
    "the data are discretized using a histogram with 100 bins of uniform width . for details on the event selection ,",
    "see @xcite and the references therein .    in order to estimate the parameters of the crystal ball function",
    ", we divided the data set into two independent samples by drawing a binomial random variable independently for each bin with the number of trials equal to the observed bin contents .",
    "consequently , the bins of the resulting two histograms are marginally mutually independent and poisson distributed .",
    "each observed event had a 70% probability of belonging to the sample @xmath132 used for the unfolding demonstration and a 30% probability of belonging to the sample used for cb parameter estimation .",
    "the cb parameters @xmath294 were estimated using maximum likelihood with the subsampled data on the full invariant mass range 65115  gev assuming that the true intensity is proportional to the breit  wigner distribution of equation  ( [ eq : breitwigner ] ) .",
    "the unknown proportionality constant of the true intensity was also estimated as part of the maximum likelihood fit .",
    "the maximum likelihood estimates of the cb parameters were @xmath295 indicating that the measured invariant mass is on average 0.56 gev too high and has an experimental resolution of approximately 1 gev .",
    "as a cross - check of the fit , the estimated crystal ball function was used to smear the true intensity to obtain the corresponding expected smeared histogram , which was found to be in good agreement with the observations .      to carry out the unfolding of the @xmath0 boson invariant mass",
    ", we used the subsampled @xmath296 bins on the interval @xmath297 $ ] .",
    "the resulting histogram @xmath132 had a total of 42,475 electron  positron events . to account for events that are smeared into the observed interval @xmath28 from the outside",
    ", we let the true space @xmath298 $ ] , that is , we extended it by approximately @xmath299 on both sides with respect to @xmath28 .",
    "the true space @xmath17 was discretized using order-4 b - splines with @xmath300 uniformly placed interior knots , resulting in @xmath301 unknown spline coefficients .",
    "it was found out that such overparameterization with @xmath302 facilitated the mixing of the mcmc sampler . with these choices ,",
    "the condition number of the smearing matrix was @xmath303 .",
    "the boundary hyperparameters were set to @xmath304 .",
    "boson invariant mass spectrum .",
    "the confidence band consists of the 95% pointwise percentile intervals of the bias - corrected estimator obtained using 5  bias - correction iterations .",
    "the points show a histogram estimate of the smeared intensity . ]",
    "the parameters of the unfolding algorithm were set to the same values as in the medium and large sample size experiments of section  [ sec : simulations ] .",
    "in particular , the number of bias - correction iterations was set to @xmath251 .",
    "the mcem algorithm converged after approximately 10 iterations to the hyperparameter estimate  @xmath305 with little monte carlo variation . finding",
    "the hyperparameter @xmath148 followed by the point estimate @xmath174 took 10 minutes , while the running time to obtain the bias - corrected confidence intervals was approximately 17 hours .    in figure",
    "[ fig : zee ] , the unfolded intensity @xmath172 and the 95% pointwise percentile intervals of the bias - corrected intensity @xmath173 are compared with the breit  wigner shape of the @xmath0 boson mass peak .",
    "the proportionality constant of the true intensity was obtained from the maximum likelihood fit described in section  [ sec : zbosonintro ] .",
    "the figure also shows a histogram estimate of the smeared intensity given by the observed event counts @xmath132 divided by the 0.5 gev bin width . the unfolding algorithm is able to correctly reconstruct the location and width of the @xmath0 mass peak which are both distorted by the smearing in the ecal .",
    "the intensity is also estimated reasonably well in the 1 gev regions in the tails of the intensity where no smeared observations were available .",
    "more importantly , the 95% bias - corrected confidence intervals cover the true mass peak across the whole invariant mass spectrum .",
    "we also observe that the bias - correction is particularly important near the top of the mass peak , where the original point estimate @xmath172 appears to be somewhat  biased .",
    "we have studied a novel approach to solving the high energy physics unfolding problem involving empirical bayes selection of the regularization strength and frequentist uncertainty quantification using an iterative bias - correction .",
    "we have shown that empirical bayes provides a straightforward , fully data - driven way of choosing the hyperparameter  @xmath68 and demonstrated that the method provides good estimates of the regularization strength with both simulated and real data . given the good performance of the approach , we anticipate empirical bayes methods to also be valuable in solving other inverse problems beyond the unfolding  problem .",
    "the natural fully bayesian alternative to empirical bayes consists of using a bayesian hierarchical model which necessitates the choice of a hyperprior for @xmath68 .",
    "unfortunately , the resulting estimates are known to be sensitive to this nontrivial choice [ @xcite ] .",
    "we provide in the online supplement [ @xcite ] a comparison of empirical bayes and hierarchical bayes for a number of uninformative hyperpriors .",
    "our results indicate that the relative point estimation performance of the methods depends strongly on the choice of the hyperprior , especially when there is only a limited amount of data .",
    "for example , in the small sample size experiment of section  [ sec : simulations ] , the integrated squared error of hierarchical bayes ranges from 17% better to 30% worse than that of empirical bayes depending on the choice of the hyperprior .",
    "unfortunately , there are no guarantees that the uninformative hyperprior that worked the best in this example would always provide the best performance .",
    "these results also indicate that hyperpriors which were designed to be uninformative are actually fairly informative in hierarchical bayes unfolding and have an undesirably large influence on the unfolded results , while empirical bayes achieves comparable point estimation performance without the need to make any arbitrary distributional assumptions on @xmath68 .",
    "the hierarchical bayes credible intervals also suffer from similar coverage issues as the empirical bayes credible intervals .",
    "the other main component of our approach is frequentist uncertainty quantification using an iterative bias - correction technique .",
    "we have shown that the bias - correction is crucial for establishing close - to - nominal frequentist coverage and that , at least within the context of our simulation study , the approach outperforms existing techniques based on bootstrap resampling and bayesian credible intervals .",
    "the good performance of this approach raises many interesting questions for future work .",
    "for example , there seems to be a trade - off between the coverage and the length of the bias - corrected intervals and this trade - off is governed by the number of bias - correction iterations @xmath180 .",
    "that is , it appears that one is able to obtain consistently better coverage by running more bias - correction iterations at the expense of increased interval length . by studying the theoretical properties of the iteration",
    ", one might be able to provide guidance on how to choose @xmath180 in such a way that it optimizes the length of the intervals while maintaining the coverage within a preset threshold of the nominal value .    in this work ,",
    "we have focused on the most basic inferential tool in nonparametric statistics , that is , we have aimed at building pointwise confidence bands for the unknown intensity @xmath6 with good frequentist coverage properties .",
    "these bands can be directly used for making pointwise inferential statements , such as testing if the data are consistent with some theoretical prediction of @xmath6 at a single point @xmath306 chosen before seeing the data .",
    "they also form the basis for making more complicated inferences .",
    "for example , one can envisage using the bands for testing whether the data are consistent with a theoretical prediction at several locations or over the whole spectrum by making a multiple testing correction .",
    "the appropriate form of the inferential statement depends , however , on the original purpose of the unfolding operation and each purpose listed in section  [ sec : physicsunfolding ] involves different types of inferential goals , including the comparison and combination of several unfolded measurements .",
    "extensions of the tools studied in this paper to such more complicated situations constitutes an important topic for future work .",
    "it should also be noted that throughout this paper the confidence intervals are computed for a fixed forward mapping @xmath8 , while in reality there is usually a systematic uncertainty associated with @xmath8 stemming from the fact that @xmath8 is typically estimated using either simulations or some auxiliary measurements . in cases",
    "where the detector response can be modeled using a parametric model , such as the @xmath0 boson example of section  [ sec : zboson ] , the unknown parameters can be estimated using maximum likelihood on data generated by a known true intensity .",
    "more generally , however , one may need to consider nonparametric estimates of @xmath8 .",
    "once an estimate of @xmath8 has been obtained , it is likely that its uncertainty can be incorporated into the outer bootstrap loop of the bias - corrected confidence intervals",
    ". a  detailed study of these ideas will be the subject of future work by the authors .",
    "finally , it should be pointed out that it is possible to find situations where the proposed unfolding methodology will not yield good reconstructions .",
    "this happens when the smoothness penalty , that is , penalizing for large values of @xmath108 , is not the appropriate way of regularizing the problem .",
    "for instance , if the true intensity @xmath6 contains sharp peaks or rapid oscillations , the solution would potentially be biased to an extent where the iterative bias - correction would be unable to adequately improve the situation . in particular",
    ", the penalty would be unlikely to adapt well to situations where the magnitude of the second derivative varies significantly across the spectrum .",
    "this is the case , for example , when the intensity contains features at very different spatial scales or when it corresponds to a steeply falling spectrum that varies over many orders of magnitude .",
    "naturally , in these cases , a more suitable choice of the family of regularizing priors @xmath307 should fix the problem . for example , in the case of a steeply falling spectrum , one could consider penalizing for the second derivative of @xmath308 instead of @xmath6 or using a spatially adaptive smoothness penalty [ @xcite ] .",
    "these considerations highlight the fact that all the inferences considered here are contingent on the chosen family of regularizing priors and should be interpreted with this in mind .",
    "we wish to warmly thank bob cousins , anthony davison , tommaso dorigo , louis lyons and mikko voutilainen for insightful discussions and their encouragement in the course of this work .",
    "we are also grateful to the associate editor and the two anonymous reviewers for their exceptionally detailed and constructive feedback ."
  ],
  "abstract_text": [
    "<S> we consider the high energy physics unfolding problem where the goal is to estimate the spectrum of elementary particles given observations distorted by the limited resolution of a particle detector . </S>",
    "<S> this important statistical inverse problem arising in data analysis at the large hadron collider at cern consists in estimating the intensity function of an indirectly observed poisson point process . </S>",
    "<S> unfolding typically proceeds in two steps : one first produces a regularized point estimate of the unknown intensity and then uses the variability of this estimator to form frequentist confidence intervals that quantify the uncertainty of the solution . in this paper </S>",
    "<S> , we propose forming the point estimate using empirical bayes estimation which enables a data - driven choice of the regularization strength through marginal maximum likelihood estimation . observing that neither bayesian credible intervals nor standard bootstrap confidence intervals succeed in achieving good frequentist coverage in this problem due to the inherent bias of the regularized point estimate </S>",
    "<S> , we introduce an iteratively bias - corrected bootstrap technique for constructing improved confidence intervals . </S>",
    "<S> we show using simulations that this enables us to achieve nearly nominal frequentist coverage with only a modest increase in interval length . </S>",
    "<S> the proposed methodology is applied to unfolding the @xmath0  boson invariant mass spectrum as measured in the cms experiment at the large hadron collider .    </S>",
    "<S> ./style / arxiv - general.cfg </S>"
  ]
}