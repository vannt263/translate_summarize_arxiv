{
  "article_text": [
    "high - impact , real - world events often unfold in an unexpected way , dynamically involving a variety of entities .",
    "this is especially true for long - running events when full information about the event and its development becomes available only in the course of days after the happening , as in the case of a recent germanwings airplane crash . in this paper , we study event timeline summarization and present a novel method which shows key entities at different time points of an event thus capturing this dynamic event unfolding .",
    "in contrast to other work in event summarization @xcite , our _ entity timelines _ use entities instead of sentences as main units of summarization as depicted in the case of the 2015 germanwings plan crash ( figure [ fig:4u9525 ] ) .",
    "such summaries can be easily digested and used both as starting points for personalized exploration of event details , and for retrospective revisiting .",
    "the latter can be triggered by a new similar event , or by a new twist in the story .",
    "for example , the testimonial of the captain in the costa concordia trial in late 2014 triggered a revisiting of the disaster in 2012 . from a cognitive perspective , for event revisiting , we rather create  memory cues \" to support remembering the unfolded events than summaries for rehearsing the details .",
    "in fact , memory cues can be regarded as  a circumstance or piece of information which aids the memory in retrieving details not re - called spontaneously \" ( oxford online dictionary , 2015 ) . in this sense ,",
    "our work is related to the idea of designing or creating memory cues for real - life remembering  @xcite .",
    "entities such as persons and locations have been identified as very effective external memory cues  @xcite .",
    "in addition , the importance of entities in event summarization has also been shown in recent work  @xcite .        for creating an entity timeline , the entities to be used in the summary have to be chosen carefully .",
    "they should 1 )  be characteristic for a respective time point of the event , 2 )  not be repetitive ( if nothing new happened with respect to the entities ) , 3 )  be associated to relevant event information , and 4 )  be interesting to the reader .",
    "for this purpose , we propose an approach to entity summarization , which dynamically combines _ entity salience _ with the _ informativeness _ of entities at a considered point in time .",
    "entity salience , on the one hand , considers the property of being in the focus of attention in a document has been studied in previous work  @xcite . in  @xcite , boguraev and kennedy use salient text phrases for the creation of so - called _ capsule overviews _ , whereas recently methods for the identification of salient entities , e.g. , in web pages  @xcite and news articles @xcite , have been developed .",
    "informativeness , on the other hand , assesses the level of new information associated with an entity in a text and can be computationally measured using features derived from statistical and linguistic information  @xcite .    in more detail , we aim at optimizing a trade - off between the in - document salience of entities and the informativeness of entities across documents describing the unfolding of an event .",
    "our contributions are :    * we are the first to propose an entity timeline for novel user experience in event exploration and digestion . *",
    "we propose a novel entity ranking model that dynamically learns a trade - off between entity salience and informativeness . * for training our ranking model ,",
    "we propose a method for gathering soft labels ( or ground truths ) by mining collective attention in wikipedia .",
    "we validate our contributions with experiments on a large real - world news dataset covering various types of events .",
    "the rest of the chapter is organized as follows .",
    "section 2 discusses the related work .",
    "sections 3,4 details our proposed method .",
    "section 5 discusses features used in learning framework .",
    "section 6 presents the experiments and results .",
    "finally , section 7 concludes our paper .",
    "* news summarization . *",
    "the task of news summarization has been already studied in various contexts , which range from focusing on multi - document summarization  @xcite to generating a timeline summary for a specific news story  @xcite .",
    "news stories can be complex , having a non - linear structure and associated to multiple aspects .",
    "shahaf et al .",
    "@xcite propose a novel method for summarizing complex stories using metro maps that explicitly capture the relations among different aspects and display a temporal development of the stories . instead of using documents or sentences as a information units , we provide a set of entities , as _ memory cues _",
    ", for supporting event exploration and digestion at each individual point in time .",
    "to the best of our knowledge , none of the previous work provides a trade - off solution that balances between content - based and collective attention - based approaches , in supporting the entity - centric summarization .",
    "* entity ranking and entity salience . * in recent years , entity retrieval and ranking have gained interest from the ir community moving beyond traditional document retrieval to more specific information about entities in response to a query , e.g. ,  @xcite .",
    "although a lot of interesting work on entity ranking has already been proposed , most of previous works have focused on static collections , thus ignoring the temporal dynamics of queries and documents .",
    "the relevant work closest to ours in this respect is by demartini et al .",
    "@xcite , where the task tackled is to identify the entities that best describe the documents for a given query .",
    "the entities are identified by analyzing the top - k retrieved documents at the time when the query was issued as well as relevant documents in the past . in ir , salient entities in news documents help improving the performance of document retrieval  @xcite . there is a body of work in identifying salient entities in general web  @xcite and in news domain  @xcite , but the existing work does not take into account the time dimension .",
    "our work targets a different question , using entities as a pivot , and it considers the global context ( timeline of the event ) as a new dimension of entity salience determination .    *",
    "learning to rank multiple criteria . * in our work",
    ", we apply a learning to rank ( l2r ) technique to generate timeline summaries in a similar way as done in  @xcite .",
    "in contrast to the previous work , we propose a novel joint learning method , which optimizes different aspects of an event . the model is able to distinguish the aspects semantically .",
    "we also incorporate the time effect ( i.e. , decay ) into the joint model , making our learning to rank time - sensitive and suitable for timeline summarization .",
    "* long - running event . * following @xcite",
    ", we define a long - running event as `` a newsworthy happening in the world that is significant enough to be reported on over multiple days '' . each event",
    "is represented by a short textual description or a set of keywords @xmath0 , where we will use @xmath0 to denote the event in the rest of this paper .",
    "for example , the bombing incident during boston marathon in april 2013 can be described by the terms `` boston marathon bombing '' .",
    "we assume that the relevant time frame is split into a sequence of consecutive non - overlapping , equal - sized time intervals @xmath1 in a chronological order , in our case individual days .",
    "furthermore , for a given event @xmath0 , there is a set of timestamped documents @xmath2 ( each with a publication date ) reporting on the event .",
    "we define a _ reporting timeline _ @xmath3 as an ordered list of ( not necessarily consecutive ) those time periods @xmath4 in @xmath5 , which contain the publication date of at least document in @xmath2 .",
    "finally , we denote the set of all documents about @xmath0 published within a time period @xmath4 as @xmath6 .",
    "* we are interested in named entities mentioned in documents , namely , persons , organizations , locations .",
    "an entity @xmath7 can be identified by a canonical name , and can have multiple terms or phrases associated with @xmath7 , called * labels * , which refer to the entity .",
    "we call an appearance of an entity label in a document , a * mention * of @xmath7 , denoted by @xmath8 .",
    "we define @xmath9 as the set of all entities mentioned in @xmath2 .",
    "furthermore , we define the text snippet surrounding @xmath8 ( e.g. , sentence or phrases ) as the * mention context * , denoted @xmath10 .",
    "* entity salience and informativeness . * similarly to  @xcite , we define the entity salience as the quality of `` being in the focus of attention '' in the corresponding documents .",
    "another relevant aspect considered for selecting entities to be included in an event timeline is _ informativeness _",
    "@xcite , which imposes that selected entities in an evolving event should also deliver novel information when compared to the past information .",
    "for example , although the airline `` germanwings '' stays relevant for many articles reporting on the plane crash , it will only be considered as informative , if new information about the airline becomes available .",
    "* problem . * given a long - running event @xmath0 , a time interval @xmath11 in its reporting timeline @xmath12 , and the set of entities @xmath9 , we aim to identify the top-@xmath13 salient and informative entities for supporting the exploration and digestion of @xmath0 at @xmath11 .",
    "we tackle this problem by learning a function for ranking entities , which is aimed at optimizing the trade - off between in - document entity salience and the informativeness of entities across documents : @xmath14    where @xmath15 is the vector of ranking scores for entities in @xmath9 at time interval @xmath16 , @xmath17 is a matrix composed from feature vectors of entities in @xmath9 extracted from their mention contexts .",
    "@xmath18 are the unknown parameter vectors for ranking entities based on salience and informativeness , respectively .    in our work , a ranking function is based on a learning - to - rank technique  @xcite .",
    "a general approach for learning - to - rank is to optimize a defined loss function @xmath19 given manual annotation or judgments @xmath20 of entities for a set of training events @xmath21 within the time intervals @xmath12 : @xmath22    two major challenges must be taken into account when learning a ranking function defined in equation [ eq : l2r ] .",
    "first , we need a reliable source for building judgments ( ground truths ) for annotating entities by considering their salience with respect to a given event .",
    "in addition , the judgments must be dynamically adapted to the evolving of entities along the unfolding event , i.e. , bearing of novel information .",
    "second , the models of our two aspects @xmath23 must be unified to produce a joint learned function for ranking entities . in the subsequent sections",
    ", we will explain our proposed method for these challenges in more detail .",
    "figure [ fig : arch ] gives an overview of our entity ranking framework covering both the training and the application / testing phase .",
    "given one event @xmath0 , its reporting timeline , and the set of documents @xmath2 ( in practice , @xmath2 can be given a priori , or can be retrieved using different retrieval models ) we identify the entity set @xmath9 using our entity extraction , which consists of named entity recognition , co - reference and context extraction ( section  [ sec : entity - extraction ] ) .",
    "when the event is used for training ( training phase ) , we link a subset of @xmath9 to wikipedia concepts , which comprises the popular and emerging entities of the event . to facilitate the learning process , these entities are softly labeled using view statistics from wikipedia ( section  [ sec : soft - labelling ] ) , serving as training instances .",
    "although we use popular entities for training , we design the features such that it can be generalized to arbitrary entities , independent from wikipedia .    the next component in our framework is the adaptive learning that jointly learns the salience and informativeness models , taking into account the diverse nature of events and their evolution .",
    "( section  [ sec : multi - crit - learning ] ) . in the application phase , entity and feature extraction",
    "are applied the same as in training phase .",
    "first , the input event and time interval is examined against the joint models to return the adaptive scores ( details in section  [ sec : adaptive ] ) .",
    "then , entities are put into an ensemble ranking , using the adapted models , to produce the final ranks for the summary .",
    "first , we discuss how we can extract entities , the key units in our framework . for each document in the set , ,",
    "we employ a named entity recognizer to identify mentions of entities in three categories ( persons , locations and organizations ) . for each mention",
    ", we include the containing sentences as the context .",
    "mentions that do not contain any alphabetical characters or only stop words are removed .",
    "we additionally use intra- and cross - document co - reference to track mentions pertaining to the same entity .",
    "first , an intra - document co - reference system is employed to identify all co - reference chains for entity mentions within a document .",
    "we include each reference in the chain together with its sentence to the set of mention contexts of the entity .",
    "second , to identify mentions that potentially refer to the same real - world entity across documents , we adapt the state - of - the - art cross - document co - reference method proposed by lee et al.@xcite .",
    "this method first clusters documents based on their content using an expectation maximization ( em ) algorithm , then iteratively merges references ( verbal and nominal ) to entities and events in each cluster . *",
    "speeding up co - reference resolution : * to speed up the computation , we do not use em clustering as @xcite , but employ a set of heuristics , which have proven to be effective in practice .",
    "first , we only consider cross - document co - references from documents of the same day .",
    "second , instead of clustering an entire document set , we use mentions with their contextual sentences ( kept in the order of their appearance in the original documents ) as `` pseudo - documents '' for the clustering .",
    "third , we assume that mentions to the same entity have similar labels .",
    "hence , we represent entity mention labels as vectors using two - grams ( for instance , `` obama '' becomes `` ob '' , `` ba '' , `` am '' , `` ma '' ) and apply lsh clustering  @xcite to group similar mentions .",
    "lsh has been proven to perform well in entity disambiguation tasks  @xcite , and it is much faster than the standard em - based clustering . finally , for each entity mention , we merge its contextual sentences with those of all other references of the same co - reference chain to obtain the event - level context for an entity , which will be used as inputs for constructing the entity features .",
    "we note that while we are aware of other methods to increase the quality of entity extraction by linking them to a knowledge base such as yago or dbpedia , we choose not to limit our entities to such knowledge bases , so as to be able to identify and rank more entities in the `` long tail '' .      in the following ,",
    "we explain how the training data is generated .",
    "specifically , given an event @xmath0 , an interval @xmath16 and an entity @xmath7 , we aim to automatically generate the score @xmath24 such that @xmath25 if the entity @xmath7 is more prominent than the entity @xmath26 with respect to the event @xmath0 at time @xmath16 .",
    "this score can be used as a soft labelling to learn the ranking functions mentioned in equation [ eq : l2r ] .",
    "the use of soft labeling for entities salience has already been proposed in @xcite , where user click behaviour in query logs is used as an indicator for entity salience scores .",
    "dunnietz et al .",
    "@xcite proposed treating entities in news headlines as salient , and propagate those salience scores to other entities via the pagerank algorithm .",
    "the limitation of these measures is that they restrict the assessment of salience to the scope of individual documents , and do not consider the temporal dimension .",
    "in contrast , our soft labels are evolving , i.e. , an entity can have different labels for one event at different time intervals .",
    "the soft labeling is based on the assumption that for globally trending news event , prominence of related entities can be observed by the collective attention paid to resources representing the entities .",
    "for instance , during the boston marathon bombing , wikipedia pages about the bomber _ tsarnaev _ were created and viewed 15,000 times after one day , indicating their strong salience driven by the event . for soft labeling , we exploit the page view statistic of wikipedia articles , which reflects the interest of users in an entity : most obviously , wikipedia articles are viewed for currently popular entities indicating entity salience .",
    "however , taking the encyclopedic character of wikipedia into account , wikipedia articles are also viewed in expectation of new information about an entity indicating its ( expected ) informativeness , especially in the context of an ongoing event .",
    "actually , wikipedia has gained attention in recent years as a source of temporal event information  @xcite .",
    "event - triggered bursts in page views , as they are for example used in  @xcite for event detection , are thus a good proxy for the interestingness of an event - related entity at a considered point in time , which is influenced both by the salience and the informativeness of the event .",
    "therefore , we propose a new metric called _ view outlier ratio _ or to approximate the soft labels for a combined measure of entity salience and informativeness as follows . for each entity @xmath7 and for a given time interval @xmath11 , we first construct the time series of view count from the corresponding wikipedia page of @xmath7 in the window of size @xmath27 : @xmath28\\ ] ] where each @xmath29 is the view count of the wikipedia page of @xmath7 at @xmath30 . from @xmath31",
    "we calculate the median @xmath32 and define as follows .",
    "the view outlier ratio is the ratio of difference between the entity view and the median : @xmath33 where @xmath34 is a minimum threshold to regularize entities with too low view activity .",
    "we now turn our attention to defining the ranking function in equation [ eq : problem ] .",
    "the intuition is that for each event @xmath0 and time @xmath11 , we rank an entity @xmath7 higher than others if : ( 1 ) @xmath7 is more relevant to the central parts of documents in @xmath6 where it appears ( salience ) ; and ( 2 ) the context of @xmath7 is more diverse to other contexts ( of @xmath7 or other entities ) at @xmath35 .",
    "moreover , these two criteria should be unified in an adaptive way that depends on the query , that is , event and time .",
    "for example , users interested in a festival might wish to know more about salient entities of the event , while those that follow a breaking story prefer entities with more fresh information . even for one event",
    ", the importance of salience and informativeness might vary over time .",
    "for instance , informativeness is more important at the beginning when the event is updated frequently .",
    "based on this intuition , we propose the following ranking function : @xmath36    where @xmath37 is the @xmath38 matrix representing the @xmath39 dimensional feature vectors of entities used to learn the salience score ( @xmath39 is the number of salience features ) , and @xmath40 is the @xmath41 is the matrix of @xmath42 dimensional informativeness feature vectors ( @xmath42 is the number of informativeness features ) . @xmath43 and @xmath44 represent the scores of salience and informativeness tendency for an event @xmath0 at @xmath16 . here",
    "we introduce another factor , @xmath45 , which is the decay function of time @xmath16 , controlling how much the informativeness should have impact on the overall ranking .",
    "the rationale of @xmath46 is that when the distance between two time intervals @xmath11 and @xmath47 is long , informativeness has less impact on the overall ranking .",
    "for example , if there are only reports about the news after one year ( anniversary of a past event ) , the changes of entities in that long time period should not contribute much to the informativeness criterion .",
    "we now discuss how to learn the above ranking function using equation [ eq : l2r ] .",
    "a straightforward way is to learn the two models @xmath48 and @xmath49 separately , and assign values to @xmath50 in a pre - defined manner , then aggregate into equation [ eq : rankfunc ] .",
    "however , this is not desirable , since it requires to build two sets of training data for salience and informativeness at the same time , which is expensive .",
    "secondly , previous work has pointed out that a `` hard '' classification of query based on intent itself is a difficult problem , and can harm the ranking performance @xcite . in this work , we exploit the _ divide and conquer _",
    "( dac ) learning framework in @xcite as follows .",
    "we define @xmath51 as the @xmath52 matrix of @xmath53 dimensional _ extension vectors _ from the corresponding vectors of @xmath54 and @xmath55 matrices .",
    "similarly , we define @xmath56 as the @xmath53 extension vectors of zero vector @xmath57 , and the vectors @xmath48 , and @xmath58 as the @xmath59 extension vector of @xmath49 and @xmath57 . with this transformation , equation [ eq : rankfunc ] can be written as :    @xmath60    where @xmath61 is a linear function .",
    "incorporating equations [ eq : l2r ] and [ eq : rankfunc1 ] , we can co - learn the models @xmath62 ( and thus @xmath63 ) simultaneously , using any loss functions .",
    "for instance , if we use hinge loss as in  @xcite , we can then adapt the ranksvm  @xcite , an algorithm that seeks to learn the linear function @xmath64 in ( [ eq : rankfunc1 ] ) by minimizing the number of misordered document pairs . for completeness , we describe here the traditional objective function of ranksvm :    @xmath65    where @xmath66 implies that entity @xmath67 is ranked higher than entity @xmath68 for the event @xmath0 at time @xmath16 , @xmath69 denotes slack variables , and @xmath70 sets the trade - off between the training error and model complexity .",
    "if we change the linear function @xmath64 to @xmath71 , we can adapt ( [ eq : l2r ] ) into ( [ eq : ranksvm ] ) to obtain the following objective function :    @xmath72      the adaptive scores @xmath73 and the decay function @xmath45 is critical to adapting the salience and informativeness models .",
    "a nave supervised approach to pre - define the categories for event ( @xmath50 ) is impractical and detrimental to ranking performance if the training data is biased . instead",
    ", previous work on query - dependent ranking  @xcite often exploit the `` locality property '' of query spaces , i.e. , features of queries of the same category are more similar than those of different categories .",
    "bian et al.@xcite constructed query features using top - retrieved documents , and clustered them via a mixture model .",
    "however , the feature setting is the same for all clusters , making it hard to infer the semantics of the query categories .    in this work ,",
    "we inherit and adjust the approach in  @xcite as follows .",
    "for each event @xmath0 and time @xmath16 , we obtain all entities appearing in @xmath74 to build the `` pseudo - feedback '' for the query @xmath75 .",
    "we then build the query features from the pseudo - feedback as follows . from each matrix @xmath76",
    ", we take the _",
    "mean _ and _ variance _ of the feature values of all entities in the pseudo feedback . as a result",
    ", each pair @xmath75 is mapped into two feature vectors ( with @xmath77 and @xmath78 dimensions ) corresponding to the salience and informativeness spaces . in each space",
    ", we use gaussian mixture model to calculate the centroid of the training queries , and use the distance of the query feature vector to the centroid as its corresponding adaptive score :    @xmath79    where @xmath80 indicates the event categories , @xmath81 is the query feature in the feature space of @xmath82 , and @xmath83 is the centroid of feature vectors in training set @xmath21 in the corresponding space .",
    "the scores are scaled between @xmath84 and @xmath85 .",
    "* decayed informativeness . *",
    "the decay function @xmath86 adjusts the contribution of informativeness into the adaptive model and is defined by :    @xmath87    where @xmath88 are parameters ( @xmath89 ) , and @xmath90 is the interval unit distance .",
    "equation  [ eq : gamma ] represents the time impact onto the informativeness of entities : when the time lag between two intervals is high , the difference in contexts of entities between them is less likely to correlate with the informativeness quality of entities .",
    "we now discuss the salience and informativeness features for entity ranking .",
    "ranking features are extracted from event documents where the entity appears as follows .",
    "these features , called _ individual features _",
    ", are extracted two different levels . first , at * context * level , features are extracted independently from each mention and its contexts .",
    "features of this level include mention word offset , context length , or importance scores of the context within the document using summarization algorithms ( sumbasic or sumfocus features ) .",
    "second , at * label * level , features are extracted from all mentions , for instance aggregated term ( document ) frequencies of mentions .    based on the individual features ,",
    "the entity features are constructed as follows . for each entity and feature dimension",
    ", we have the list of feature values @xmath91 , where @xmath92 is the individual feature of label or mention categories . for label level",
    ", we simply take the average of @xmath92 s over all entity labels . for mention level , each @xmath92 is weighted by the the confidence score of the document containing the corresponding mention and context .",
    "such confidence score can be calculated by several ways , for instance by a reference model ( e.g. , bm25 ) when retrieving the document , or by calculating the authority score of the document in the collection ( e.g. , using pagerank algorithm ) . for all features",
    ", we apply quantile normalization , such that all individual features ( and thus entity aggregated features ) are scaled between @xmath93 $ ] . below we describe the most important features .",
    "[ cols= \" < , < , < , < \" , ]",
    "in this paper , we have presented a novel approach for timeline summarization of high - impact news events , using entities as the main unit of summary .",
    "we propose to dynamically adapt between entity salience and informativeness for improving the user experience of the summary .",
    "furthermore , we introduce an adaptive learning to rank framework that jointly learns the salience and informativeness features in a unified manner . to scale the learning , we exploit wikipedia page views as an implicit signal of user interest towards entities related to high - impact events . our experiments have shown that the introduced methods considerably improve the entity selection performance , using both small - scale expert - based and large - scale crowdsourced assessments .",
    "the evaluation also confirms that integrating salience and informativeness can significantly improve the user experience of finding surprisingly interesting results .",
    "as the problem discussed in the paper is new , there are several promising directions to explore for future work .",
    "we aim to investigate further the impact of adaptation in different types of events , using larger and more diverse sets of events .",
    "furthermore , we plan to study more advanced ways to mine wikipedia temporal information as signals of collective attention towards public events .",
    "we are planning to use this for further improving our vor measure for the soft labeling approach .",
    "other direction includes investigating deeper models to improve the performances of current entity timeline summarization systems , which are quite low .",
    "* acknowledgements .",
    "* this work was partially funded by the european commission for the fp7 project forgetit ( under grant no .",
    "600826 ) and the erc advanced grant alexandria ( under grant no .",
    "339233 ) .",
    "j.  p. kincaid , r.  p. fishburne  jr , r.  l. rogers , and b.  s. chissom .",
    "derivation of new readability formulas ( automated readability index , fog count and flesch reading ease formula ) for navy enlisted personnel .",
    "technical report , dtic document , 1975 ."
  ],
  "abstract_text": [
    "<S> long - running , high - impact events such as the boston marathon bombing often develop through many stages and involve a large number of entities in their unfolding . </S>",
    "<S> timeline summarization of an event by key sentences eases story digestion , but does not distinguish between what a user remembers and what she might want to re - check . in this work , we present a novel approach for timeline summarization of high - impact events , which uses entities instead of sentences for summarizing the event at each individual point in time </S>",
    "<S> . such entity summaries can serve as both ( 1 )  important memory cues in a retrospective event consideration and ( 2 )  pointers for personalized event exploration . in order to automatically create such summaries , </S>",
    "<S> it is crucial to identify the  right \" entities for inclusion . </S>",
    "<S> we propose to learn a ranking function for entities , with a dynamically adapted trade - off between the in - document salience of entities and the informativeness of entities across documents , i.e. , the level of new information associated with an entity for a time point under consideration . </S>",
    "<S> furthermore , for capturing collective attention for an entity we use an innovative soft labeling approach based on wikipedia . </S>",
    "<S> our experiments on a real large news datasets confirm the effectiveness of the proposed methods . </S>"
  ]
}