{
  "article_text": [
    "this note relates the number of hypotheses falsified by a learning algorithm to the expected future performance of the predictor it outputs .",
    "it does so by reformulating two basic results from statistical learning theory information - theoretically .",
    "suppose we wish to predict an unknown physical process @xmath0 occurring in nature after observing its outputs @xmath1 on sample @xmath2 of its inputs , where inputs arise according to unknown distribution @xmath3 .",
    "one method is to take a repertoire @xmath4 of functions from @xmath5 and choose the predictor @xmath6 that best approximates @xmath7 on the observed data .",
    "how confident can we be in @xmath8 s future performance on unseen data ?",
    "statistical learning theory provides bounds on @xmath8 s expected future performance by quantifying a tradeoff implicit in the choice of repertoire @xmath4 . at first glance , the bigger the repertoire the better since the best approximation to @xmath7 in @xmath4 can only improve as more more functions are added to @xmath4 .",
    "however , increasing @xmath4 , and improving the approximation on observed data , can _ reduce _ future performance due to overfitting . as a result ,",
    "the bounds depend on both the accuracy with which @xmath8 approximates @xmath7 on the observed data and the capacity of repertoire @xmath4 , see theorems  [ t : vc ] and [ t : rademacher ] .",
    "we wish to connect statistical learning theory with popper s ideas about falsification .",
    "popper argued that no amount of positive evidence confirms a theory @xcite .",
    "rather , theories should be judged on the basis of how many hypotheses they falsify .",
    "a theory is _",
    "falsifiable _ if there are possible hypotheses about the world ( i.e. data ) that are not consistent with the theory . a bold theory falsifies ( disagrees with ) many potential hypotheses about observed data . testing a bold theory , by checking that the hypotheses it disagrees with are in fact false , provides corroborating evidence",
    ". if a theory has been thoroughly tested then ( perhaps ) we can have confidence in its predictions .",
    "popper s criticism of positive confirmation was devastating .",
    "however , and hence the `` perhaps '' , he failed to provide a rationale for trusting the predictions of severely tested theories .    to understand how falsifying hypotheses affects future performance we reformulate learning as a kind of _ measurement_. before doing so , we need to describe precisely what we mean by measurement .    given physical system @xmath9 with state space @xmath10 ,",
    "a classical measurement is a function @xmath11 .",
    "for example a thermometer @xmath12 maps configurations ( positions and momenta ) of particles in the atmosphere to real numbers .",
    "when the thermometer outputs @xmath13 it generates information by specifying that atmospheric particles were in a configuration in @xmath14 .",
    "the information generated by the thermometer is a brute physical fact depending on how the thermometer is built and its output .",
    "we quantify the information , see ",
    "[ s : meas ] , by comparing the size of the total configuration space @xmath10 with the size of the pre - image @xmath15 .",
    "the smaller the pre - image , the more informative the measurement , see  [ s : meas ] for details .",
    "more generally , any ( classical ) physical process @xmath16 can be thought of as performing measurements by taking inputs in @xmath17 to outputs in @xmath18 .",
    "section   [ s : lim ] introduces an important example , the _ min - risk _ @xmath19 , which outputs the minimum value of the empirical risk over repertoire @xmath4 on a hypothesis space @xmath20 . finding the min - risk is a necessary step in finding the best approximation @xmath8 to @xmath7 in @xmath4 . since computing",
    "the min - risk requires actually implementing it as a physical process somehow or other , the measurements it performs and the effective information it generates are brute physical facts , no different in kind than the information generated by a thermometer .",
    "it turns out that the min - risk categorizes hypotheses in @xmath21 according to how well they are approximated by predictors in repertoire @xmath4 .",
    "proposition  [ t : ei - vc ] shows that the effective information generated by the min - risk is ( essentially ) the empirical vc - entropy .",
    "moreover , the effective information generated by the min - risk `` counts '' the number of hypotheses about @xmath22 that @xmath4 falsifies , see eq .  . as a consequence ,",
    "corollary  [ t : ei - vcb ] , we obtain that the future performance of predictor @xmath8 is controlled by ( i ) how well @xmath8 fits the observed data ; ( ii ) how many hypotheses about the data the min - risk rules out and ( iii ) a confidence term .",
    "it follows that , assuming the assumptions of the theorems below hold , bounds on future performance are brute physical facts resulting from the act of minimizing empirical risk , and so falsifying potential hypotheses , on observed data .",
    "a consequence of our results , corollary [ t : mml ] , is that empirical vc - entropy is essentially the minimal length of the true hypothesis under the optimal code for the actual repertoire ( a distribution depending on the min - risk ) .",
    "this suggests there may be interesting connections between vc - theory and the minimum message length ( mml ) approach to induction proposed by wallace and boulton @xcite .",
    "finally , section ",
    "[ s : rrad ] reformulates empirical rademacher complexity via falsification . here",
    "we build on solomonoff s probability distribution introduced in @xcite . in short ,",
    "we take solomonoff s definition and substitute the _ min - risk _ in place of the universal turing machine , thereby obtaining what we refer to as the rademacher distribution  a _ non - universal _ analog of solomonoff s distribution .",
    "rademacher complexity is then computed using the expectation of the min - risk over the rademacher distribution , see proposition [ t : mr - rad ] .",
    "the min - risk thus provides a bridge that not only connects vc - theory to a computable analog of solomonoff s seminal distribution , but also sheds light on how falsification provides guarantees on future performance .",
    "* related work . *",
    "the connection between popper s ideas on falsifiability and statistical learning theory was pointed out in @xcite . however , these works focus on vc - dimension , which does not relate to falsification as directly as vc - entropy and rademacher complexity which we consider here .",
    "further , vc - entropy is a more fundamental concept in statistical learning theory than vc - dimension since vc - dimension is defined in terms of the limit behavior of the growth function , which is an upper bound on vc - entropy @xcite . for more details on the link between mml and algorithmic probability ,",
    "see @xcite .",
    "* acknowledgements . *",
    "i thank david dowe and samory kpotufe for useful comments on an earlier version of this paper .",
    "we consider a toy universe containing probabilistic mechanisms ( input / output devices ) of the following form    given finite sets @xmath17 and @xmath18 , a * mechanism * is a markov matrix @xmath23 defined by conditional probability distribution @xmath24 .",
    "mechanisms generate information about their inputs by assigning them to outputs @xcite .",
    "the * actual repertoire * ( or * measurement * ) specified by @xmath23 outputting @xmath25 is the probability distribution @xmath26 where @xmath27 is the uniform distribution .",
    "the * effective information * generated by the measurement is @xmath28,\\ ] ] where @xmath29=\\sum_i p_i\\log_2\\frac{p_i}{q_i}$ ] is kullback - leibler divergence .",
    "the kullback - leibler divergence @xmath29 $ ] can be interpreted informally as the number of y / n questions needed to get from distribution @xmath30 to distribution @xmath31 . however , as pointed out in @xcite , kullback - leibler divergence is invariant with respect to the `` framing of the problem ''  the ordering and structure of the questions  suggesting it is a suitable measure of information - theoretic `` effort '' .",
    "the definition of measurement is motivated by the special case where @xmath32 assigns probabilities that are either 0 or 1 ; in other words , when it corresponds to a set - valued function @xmath16 .",
    "the measurement performed by @xmath12 is @xmath33 where @xmath34 denotes cardinality .",
    "the support of @xmath35 is the preimage @xmath36 .",
    "all elements of the support are assigned equal probability  they are treated as an undifferentiated list .",
    "the measurement @xmath37 therefore generalizes the notion of preimage to the probabilistic setting .",
    "the effective information generated by @xmath12 outputting @xmath25 is @xmath38 : @xmath39 where inputs are counted in bits ( after logarithming ) .",
    "effective information is maximal ( @xmath40 bits ) when a single input leads to @xmath25 , and is minimal ( 0 bits ) when _ all _ inputs lead to @xmath25 .",
    "in the first case , observing @xmath12 output @xmath25 tells us exactly what the input was , and in the latter case , it tells us nothing at all .",
    "next we consider two approaches to characterizing the meaning of measurements .",
    "the first relates to possible world semantics @xcite . here",
    ", the meaning of a sentence is given by the set of possible worlds in which it is true .",
    "meaning is thus determined by considering all counterfactuals .",
    "for example , the meaning of `` that car is 10 years old '' is the set of possible worlds where the speaker is pointing to a car manufactured 10 years previously .",
    "since the set of contains cars of many different colors , we see that color is irrelevant to the meaning of the sentence .",
    "more precisely , the meaning of sentence @xmath41 is a map from possible worlds @xmath42 to truth values @xmath43 .",
    "equivalently , the meaning of a sentence is @xmath44 inspired by possible world semantics , we propose    [ d : mgs ] the * meaning * of output @xmath25 by mechanism @xmath23 is @xmath45 for a deterministic function this reduces to @xmath46 .    grounding meanings in mechanisms yields four advantages over the possible worlds approach .",
    "first , it replaces the difficult to define notion of a possible world with the concrete set of inputs the mechanism is physically capable of receiving .",
    "second , in possible world semantics the work of determining whether or not a sentence is true is performed somewhat mysteriously offstage , whereas the meaning of a measurement is determined via bayes rule .",
    "third , the approach generalizes to probabilistic mechanisms .",
    "finally , we can compute the effective information generated by a measurement , whereas there is no way to quantify the information content of a sentence in possible world semantics .",
    "the second , pragmatic notion of meaning characterizes usefulness .",
    "we consider a special case , well studied in statistical learning theory , where usefulness relates to predictions @xcite .",
    "let @xmath47 be the set of all functions ( deterministic mechanisms ) mapping @xmath17 to @xmath48 .",
    "we will often write @xmath21 for short .",
    "suppose there is a random variable @xmath9 taking values in @xmath17 with unknown distribution @xmath3 and an unknown mechanism @xmath49 , the _ supervisor _ , who assigns labels to elements of @xmath17 .",
    "the * risk * quantifies how well mechanism @xmath12 approximates an unknown or partially known mechanism @xmath7 : @xmath50\\cdot p(x).\\ ] ] it is the probability that @xmath12 and @xmath7 disagree on elements of @xmath17 .",
    "unfortunately , the risk can not be computed since @xmath3 and @xmath7 are unknown .",
    "given a finite sample @xmath51 with labels @xmath52 , the * empirical risk * of @xmath53 @xmath54\\ ] ] is the fraction of the data @xmath22 on which @xmath12 and @xmath7 disagree .",
    "the empirical risk provides a computable approximation to the ( true ) risk .",
    "[ r : finite ] note that in this paper , sets @xmath17 and @xmath18 are both finite .",
    "similarly , the training data @xmath55 and labels @xmath56 also live in finite sets .",
    "suppose we wish to predict the unknown supervisor @xmath7 based on its behavior on labeled data @xmath57 . a simple way to find a mechanism in repertoire @xmath58 that approximates @xmath7 well is to minimize the empirical risk .",
    "given repertoire @xmath59 and unlabeled data @xmath60 , define * learning algorithm * @xmath61 which finds the mechanism in @xmath4 that minimizes empirical risk .",
    "learning algorithm @xmath62 finds the mechanism in @xmath4 that appears , based on the empirical risk , to best approximate @xmath7 .",
    "empirical risk stays constant or decreases as @xmath4 is enlarged , suggesting that the larger the repertoire the better .",
    "this is not true in general since minimizing risk  and _ not _ empirical risk  is the goal .",
    "there is a tradeoff : increasing the size of @xmath4 leads to overfitting the data which can increase risk even as empirical risk is reduced .",
    "the tendency of a repertoire to overfit data depends on its size or capacity .",
    "we recall two measures of capacity that are used to bound risk : empirical vc - entropy @xcite and empirical rademacher complexity @xcite .    given unlabeled data @xmath60 and repertoire @xmath59 let @xmath63 the empirical * * vc - entropy * * rather than @xmath64 .",
    "] of @xmath4 on @xmath22 is @xmath65 , where @xmath66 is the number of distinct points in the image of @xmath67 .",
    "the empirical * rademacher complexity * of @xmath4 on @xmath22 is @xmath68 .",
    "\\label{e : rademacher}\\ ] ]    vc - entropy `` counts '' how many labelings of @xmath22 the classifiers in @xmath4 fit perfectly .",
    "rademacher complexity is a weighted count of how many labelings of @xmath22 functions in @xmath4 fit well .    the following theorems are shown in @xcite and @xcite respectively :    [ t : vc]@xmath69 +   with probability @xmath70 , the expected risk is bounded by @xmath71 for all @xmath72 , where the constants are @xmath73 and @xmath74 .",
    "[ t : rademacher]@xmath69 +   for all @xmath75 , with probability at least @xmath70 , @xmath76 for all @xmath72 , where @xmath77 .",
    "the tradeoff between empirical risk and capacity is visible in the first two terms on the right - hand sides of the bounds .",
    "the left - hand sides of eqs   and can not be computed since @xmath3 and @xmath7 are unknown .",
    "remarkably , the right - hand sides depend only on mechanism @xmath12 chosen from repertoire @xmath4 , labeled data @xmath57 and desired confidence @xmath78 .",
    "the theorems assume data is drawn _ i.i.d . _",
    "according to @xmath3 and labeled according to @xmath7 ; it make no assumptions about the distribution @xmath3 on @xmath17 or supervisor @xmath7 , except that they are _",
    "this section reformulates the results from statistical learning theory to show how the past falsifications performed by a learning algorithm control future performance .",
    "we show that the empirical vc - entropies and rademacher complexities admit interpretations as `` counting '' ( in senses made precise below ) the number of hypotheses falsified by a particular measurement performed when learning .",
    "we start by introducing a special mechanism , the min - risk , which is used implicitly in learning algorithm @xmath62 .",
    "as we will see , the structure of the measurements performed by the min - risk determine the capacity of the learning algorithm .",
    "[ d : error ] given repertoire @xmath59 and unlabeled data @xmath60 , define the * min - risk * as the minimum of the empirical risk on @xmath4 : @xmath79    the min - risk is a mechanism mapping supervisors @xmath80 in @xmath21 to the empirical risk of their best approximations @xmath81 in @xmath4 , see fig .",
    "[ f : mrisk ] .",
    "note that inputs to the min - risk are themselves mechanisms .",
    "we suggestively interpret the setup as follows .",
    "suppose a scientist studies a universe where inputs in @xmath17 appear according to distribution @xmath3 , and are assigned labels in @xmath18 by unknown physical process @xmath7 .",
    "the _ hypothesis space _ is @xmath20 , the set of all possible ( deterministic ) physical processes that take @xmath17 to @xmath18 .",
    "the scientist s goal is to learn to predict physical process @xmath7 , on the basis of a small sample of labeled data @xmath57 .",
    "she has a _ theory _ , repertoire @xmath4 , and a method , @xmath62 , which she uses to fit some particular @xmath6 given @xmath82 .",
    "the most important question for the scientist is : how reliable are predictions made by @xmath8 on _ new _ data ?",
    "we will show that @xmath8 s reliability depends on the measurements performed by the min - risk ",
    "i.e. on the work done by the scientist when she applies method @xmath62 to find @xmath8 .          empirical vc - entropy is , essentially , the effective information generated by the min - risk when it outputs a perfect fit :    [ t : ei - vc]@xmath69 +   empirical vc entropy is @xmath83    proof : let @xmath84 and @xmath85",
    ". then @xmath86 . by definition @xmath87 with @xmath88 .",
    "it remains to show that @xmath89 .",
    "points in the image of @xmath67 correspond to labelings @xmath80 of the data by functions in @xmath4 .",
    "thus , @xmath66 counts distinct labelings of @xmath22 that @xmath4 fits perfectly .",
    "these occur with multiplicity @xmath90 in the pre - image by the product decomposition of @xmath21 above .",
    "@xmath91    we interpret the result as follows .",
    "suppose the scientist applies theory @xmath4 to explain her labeled data and perfectly fits function @xmath92 with risk @xmath93 .    by definition  [ d : mgs ]",
    ", the meaning of her work is @xmath94 : the set of mechanisms that her theory @xmath4 fits perfectly .",
    "the effective information generated by her work is @xmath95 where hypotheses are counted in bits ( after logarithming ) .",
    "a theory is informative if it rules out many potential hypotheses @xcite .    _",
    "the number of hypotheses the scientist falsifies when using theory @xmath4 to fit @xmath8 has implications for its future performance _ :    [ t : ei - vcb]@xmath69 +   with probability @xmath70 , the risk of predictor @xmath92 outputted by learning algorithm @xmath96 is bounded by @xmath97    proof : by theorem  [ t : vc ] and proposition  [ t : ei - vc ] . @xmath91",
    "the corollary states that minimizing empirical risk embeds expectations about the future into predictors .",
    "so long as the corollary s assumptions hold , future performance by @xmath8 is controlled by : ( i ) the output of the min - risk , i.e. the fraction @xmath98 of the data that @xmath8 fits ; ( ii ) the effective information generated by the min - risk , i.e. the number ( in bits ) of hypotheses the learning algorithm falsifies if it fits perfectly ; and ( iii ) a confidence term .",
    "the only assumption made by the corollary is that @xmath3 and @xmath7 are _",
    "the theorem provides no guarantees on the future performance of a theory that `` explains everything '' , i.e. @xmath99 , no matter how well it fits the data .",
    "this follows since effective information is zero when @xmath99 , and so the second term on the right - hand side of eq .   is @xmath100 .    reformulating the above result in terms of code lengths suggests a connection between vc - theory and minimum message length ( mml ) , see @xcite and  6.6 of @xcite .",
    "recall that , given probability distribution @xmath101 , the message length of event @xmath102 in an optimal binary code is @xmath103 .",
    "[ t : mml]@xmath69 +   denote the min - risk by @xmath104 .",
    "the length of the true hypothesis @xmath105 in the optimal code for the actual repertoire specified by the min - risk , @xmath106 , is @xmath107    proof : by proposition [ t : ei - vc ] we have @xmath108 .",
    "@xmath91    the length of the message describing the true hypothesis in the actual repertoire s optimal code is the empirical vc - entropy plus a term , @xmath109 , that decreases as the amount of training data increases . the shorter the message , the better the predictor s expected performance ( for fixed empirical risk ) .",
    "vc - entropy only considers hypotheses that theory @xmath4 fits perfectly .",
    "rademacher complexity is an alternate capacity measure that considers the distribution of risk across the entire hypothesis space .",
    "this section explains rademacher complexity via an analogy with solomonoff probability @xcite .",
    "we first recall solomonoff s definition . given universal turing machine @xmath110 , define ( unnormalized ) * solomonoff probability * @xmath111 where the sum is over strings should output @xmath112 . ] @xmath113 that cause @xmath110 to output @xmath112 as a prefix , and @xmath114 is the length of @xmath113 .",
    "we adapt eq .   by replacing turing machine @xmath110 with min - risk @xmath115 .    equipping hypothesis space with the uniform distribution @xmath116 , all hypotheses have length @xmath117 in the optimal code .",
    "set the * rademacher distribution * for the min - risk @xmath104 as @xmath118    the rademacher distribution is constructed following solomonoff s approach after substituting the min - risk as a `` special - purpose turing machine '' that only accepts hypotheses in finite set @xmath21 as inputs .",
    "it tracks the fraction of hypotheses in @xmath21 that yield risk @xmath98 .",
    "the rademacher distribution arises naturally as the denominator when using bayes rule to compute the actual repertoire @xmath119 : @xmath120    [ t : mr - rad]@xmath69 +    @xmath121.\\ ] ]    proof : we refer to @xmath122 $ ] as the expected min - risk . from eq .  , @xmath123.\\ ] ]",
    "observe that @xmath124 .",
    "it follows that @xmath125 , which implies @xmath126    rademacher complexity is low if the expected min - risk is high .",
    "the expected min - risk admits an interesting interpretation . for any hypothesis @xmath127 the classifier @xmath128 outputted by the learning algorithm yields incorrect answers on fraction @xmath129 $ ] of the data .",
    "it follows that @xmath130      \\\\      & = & \\sum_\\epsilon & \\big(\\mbox{fraction of hypotheses falsified}\\big)&\\cdot & \\big(\\mbox{on fraction } \\epsilon\\mbox { of the data}\\big ) .",
    "\\end{matrix}\\ ] ]    a bold theory @xmath4 is one for which @xmath131 $ ] is high , meaning that its predictors ( the classifiers it tries to fit to data ) are sufficiently narrow that it would falsify most hypotheses on most of the data .    _ when a bold theory happens to fit labeled data well , it is guaranteed to perform well in future : _    [",
    "t : ei - radb]@xmath69 +   with probability @xmath70 , the risk of predictor @xmath132 outputted by learning machine @xmath96 is bounded by @xmath133              + c_3\\sqrt{\\frac{1-\\log_2\\delta}{l}}\\ ] ]    proof : by proposition  [ t : mr - rad ] and definition of effective information we have @xmath134 the result follows by theorem  [ t : rademacher ] .",
    "@xmath91    rademacher complexity is low if the min - risk s sharp measurements ( high @xmath135 ) are accurate ( low @xmath98 ) , and conversely .",
    "analogously to corollary  [ t : ei - vcb ] , the rademacher bound implies the future performance of a classifier depends on : ( i ) the fraction @xmath98 of the data that @xmath8 fits ; ( ii ) the weighted ( by the fraction @xmath98 of data that falsifies them ) sum of the fraction of hypotheses falsified ; and ( iii ) a confidence term .",
    "once again , the only assumption is that @xmath3 and @xmath7 are _",
    "learning according to algorithm @xmath62 entails computing the min - risk , which classifies hypotheses about @xmath22 according to how well they are approximated by predictors in repertoire @xmath4 .",
    "repertoires that rule out many hypotheses when they fit labeled data @xmath57 generate more effective information than repertoires that `` approximate everything '' . as a consequence ,",
    "when and if an informative repertoire fits labeled data well , corollary  [ t : ei - vcb ] implies we can be confident in future predictions on unseen data .",
    "a pleasing consequence of reformulating empirical vc - entropy and empirical rademacher complexity in terms of falsifying hypotheses is that it directly connects popper s intuition about falsifiable theories to statistical learning theory , thereby providing a rigorous justification for the former .",
    "our motivation for reformulating learning theory information - theoretically arises from a desire to better understand the role of information in biology .",
    "although shannon information has been heavily and successfully applied to biological questions , it has been argued that it does not fully capture what biologists mean by information since it is not semantic .",
    "for example , maynard smith states that `` in biology , the statement that a carries information about b implies that a has the form it does because it carries that information '' @xcite .",
    "shannon information was invented to study communication across prespecified channels , and lacks any semantic content .",
    "maynard smith therefore argues that a different notion of information is needed to understand in what sense evolution and development embed information into an organism .",
    "it may be fruitful to apply statistical learning theory to models of development .",
    "one possible approach is to consider analogs of repertoire @xmath4 .",
    "for example , @xmath4 may correspond to the repertoire of possible adult forms a zygote could develop into .",
    "the particular adult form chosen , @xmath6 , depends on the historical interactions @xmath57 between the organism and its environment , assuming these can be suitably formalized .",
    "the information generated by the organism s development would then have implications for its future interactions with its environment .",
    "more speculatively , a similar tactic could be applied to quantify the information embedded in populations by inheritance and natural selection .",
    "bousquet , o. , boucheron , s. , lugosi , g. : introduction to statistical learning theory . in : bousquet , o. ,",
    "von luxburg , u. , rtsch , g. ( eds . ) advanced lectures on machine learning , pp .",
    "springer ( 2004 )    corfield , d. , schlkopf , b. , vapnik , v. : falsification and statistical learning theory : comparing the popper and vapnik - chervonenkis dimensions .",
    "journal for general philosophy of science 40(1 ) , 5158 ( 2009 )    dowe , d.l . :",
    "handbook of the philosophy of science .",
    "volume 7 : philosophy of statistics , chap .",
    "mml , hybrid bayesian network graphical models , statistical consistency , invariance and uniqueness , pp .",
    "901982 . elsevier ( 2011 )"
  ],
  "abstract_text": [
    "<S> we information - theoretically reformulate two measures of capacity from statistical learning theory : empirical vc - entropy and empirical rademacher complexity . </S>",
    "<S> we show these capacity measures count the number of hypotheses about a dataset that a learning algorithm _ falsifies _ when it finds the classifier in its repertoire minimizing empirical risk . </S>",
    "<S> it then follows from that the future performance of predictors on _ unseen _ data is controlled in part by how many hypotheses the learner falsifies . as a corollary we show that empirical vc - entropy quantifies the message length of the true hypothesis in the optimal code of a particular probability distribution , the so - called actual repertoire . </S>"
  ]
}