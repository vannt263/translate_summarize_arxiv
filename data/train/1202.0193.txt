{
  "article_text": [
    "consider a continuous random variable @xmath0 with probability density function ( pdf ) @xmath1 and a selection of @xmath2 values @xmath3 , @xmath4 of @xmath5 .",
    "we assume @xmath1 to be of class @xmath6 everywhere .",
    "the purpose of the described method is to computationally estimate @xmath1 using this selection of @xmath5 . for this , @xmath1 is restricted to the interval @xmath7 $ ] , where @xmath8 , @xmath9 , and label @xmath10 .",
    "next the pdf is discretised on @xmath11 equidistant points @xmath12 $ ] , @xmath13 of the form @xmath14 , generating a probability distribution @xmath15 , @xmath13 .",
    "the values of @xmath16 are computed from the selection @xmath17 as presented below .",
    "next , the pdf @xmath1 is estimated on each @xmath18 as @xmath19 .    in order to apply the maximum entropy principle @xcite ,",
    "some conditions are imposed .",
    "these are constructed with a function @xmath20 , @xmath21 , @xmath22 .",
    "these functions are inspired by the kernel density estimation method @xcite .",
    "we center the function @xmath23 on different points in the domain of values of @xmath5 . for this ,",
    "take @xmath24 equidistant values @xmath25 $ ] of the form @xmath26 and parameters @xmath27 , @xmath28 . for each @xmath29 a function @xmath30",
    "is built as @xmath31 .",
    "next , two averages of @xmath30 are computed : an empirical average given by selection values : @xmath32 and a simulated average given by estimated probabilities @xmath16 : @xmath33 the probability distribution @xmath34 is determined by maximising the shannon entropy of @xmath34 : @xmath35 with conditions @xmath36 , @xmath28 and @xmath37 . for ease of computation , @xmath15 is obtained by minimising a cost function of the form : @xmath38 where @xmath39 .",
    "this relaxes the above conditions .",
    "the unit sum of @xmath15 is imposed at the end by dividing each obtained @xmath16 to @xmath40 .",
    "the value of @xmath41 regulates the smoothness of the estimated pdf .",
    "this is necessary because the empirical averages @xmath42 contain statistical noise , which is transmitted to the estimated pdf . by maximising the entropy",
    "the noise is reduced .",
    "nevertheless , too great an importance given to entropy maximisation may lead to smoothing real features of the pdf .",
    "the parameters @xmath43 control the locality and the smoothness of the estimated pdf , _",
    "i.e. _ a smaller value of @xmath43 define conditions on a smaller neighbourhood of @xmath29 , allowing a better local approximation of @xmath44 . in practice , the minimisation was done on a computer with the simulated annealing ( sa ) algorithm @xcite . because of the finite number of steps of this algorithm , some more noise is introduced in the final pdf estimation .",
    "this is eliminated by applying a moving average on the final result .",
    "we determine the errors of the pdf estimation and calculate optimal values of @xmath43 .",
    "we work in a small perturbations setting .",
    "experimentally , the algorithm was observed to reproduce the theoretical pdf well for @xmath45 selection values , which supports this assumption .",
    "the algorithm converges to a perturbed pdf @xmath46 with respect to the theoretical pdf @xmath1 , where @xmath47 , @xmath48 .",
    "the perturbation of the pdf is @xmath49 .",
    "since both @xmath50 and @xmath51 have unit integral , it follows that @xmath52 .",
    "consider the case where there is only one condition given by a function @xmath53 centered at @xmath54 with parameter @xmath55 .",
    "we are interested in the behaviour of the estimated pdf in a neighbourhood of @xmath56 .",
    "replace @xmath57 by the theoretical average @xmath58 and @xmath59 by the perturbed average @xmath60 .",
    "the perturbation of the average is @xmath61 .",
    "then the cost function is perturbed to @xmath62 .",
    "here we take the continuous shannon entropy @xmath63 .",
    "we consider that @xmath64 , @xmath65 .",
    "then we have from the taylor series : @xmath66 as @xmath67 .",
    "we compute the entropy perturbation to the second degree with respect to @xmath68.the perturbation of the cost function becomes : @xmath69 the condition @xmath70 reduces to @xmath71 , since @xmath72 is fixed by @xmath50 .",
    "then , by the method of lagrange multipliers , @xmath68 can be determined by minimising a functional @xmath73 , @xmath74 . for a small variation @xmath75 of @xmath68",
    "there corresponds a small variation @xmath76 of @xmath77 and the condition of minimum reduces to @xmath78 , _",
    "i.e. _ : @xmath79 it holds for any @xmath75 if and only if the paranthesis under the integral is zero .",
    "the multiplier @xmath80 is obtained from @xmath52 .",
    "the perturbation of the pdf becomes : @xmath81 .\\ ] ]    we label the variation of @xmath50 around @xmath56 @xmath82 .",
    "now consider the interval @xmath83 is chosen such that @xmath84 , @xmath85 .",
    "this assumption is not very restrictive , since @xmath83 can be taken as a subset of the interval of values of @xmath5 in order to study @xmath1 locally .",
    "we label @xmath86",
    ". then @xmath87 can be further expressed by taylor series development of @xmath88 as @xmath89 and @xmath90 , such that ( [ sol ] ) becomes : @xmath91 + o \\left ( \\delta \\rho \\right ) ^2 .\\ ] ] here @xmath68 is identified with the error of the estimation of pdf , while @xmath92 represents the error of the average of @xmath93 .",
    "the free term @xmath94 is an error given by the local variation of the pdf around @xmath95 .",
    "we have @xmath96 and @xmath97 .    the total error of conditions @xmath98 has two sources : the computation error of @xmath99 , which gives a term @xmath100 and the pdf estimation process , which gives a term @xmath101 .",
    "the term @xmath100 can be estimated by considering that @xmath99 is computed with a large enough number @xmath2 of values from the selection @xmath17 , such that @xmath99 is approximately normally distributed . from the law of large numbers",
    "@xmath102 is a random variable with approximately normal distribution with average @xmath103 and variance @xmath104 , where @xmath105 is the variance of @xmath93 : @xmath106 , with @xmath107 .",
    "we identify @xmath108 .",
    "label @xmath109 and @xmath110 .",
    "then the expression of @xmath68 becomes : @xmath111 .\\ ] ] putting this expression into the integral form of @xmath92 , we get : @xmath112 if @xmath84 , @xmath85 , by the taylor series of @xmath88 we have @xmath113 and the error of the pdf estimation becomes : @xmath114 it follows that , for large enough @xmath2 , @xmath115 is an approximately normally distributed random variable with parameters : @xmath116 in the same way , the total error of conditions @xmath98 is a normally distributed random variable @xmath117 with parameters : @xmath118    take @xmath119 $ ] and @xmath120 , @xmath121 . with the gaussian function @xmath53 we have : @xmath122 where @xmath123 $ ] are functions of @xmath124 .",
    "if @xmath125 then @xmath126 , @xmath127 .",
    "also @xmath128 and @xmath129 .",
    "we want to find the value of @xmath124 that minimises @xmath130 . putting the condition @xmath131",
    "one obtains a sixth degree equation in @xmath124 , which can be solved numerically only .",
    "nevertheless , this condition can be relaxed for small @xmath41 and @xmath132 to cancellation of the first term in @xmath133 .",
    "the average error of approximation will be then @xmath134 .",
    "we put the condition @xmath135 , which leads to the third - degree equation , which can be solved exactly @xcite : @xmath136 its discriminant is @xmath137 .",
    "the parenthesis cancels for @xmath138 .",
    "if @xmath139 $ ] then @xmath140 and the equation has 3 real solutions ; otherwise , @xmath141 and the equation has 1 real and 2 complex solutions . if @xmath142 there is only one solution : @xmath143 for @xmath144 and @xmath141 label @xmath145 ^ { 1/3 } $ ] .",
    "then the three solutions are : @xmath146    the first solution is always real . it can be approximated as : @xmath147 for small @xmath148 .",
    "the 3 solutions are represented in figure 1 .",
    "only @xmath149 is meaningful .",
    "another way to find optimal @xmath124 is to put the condition @xmath150 or equivalently @xmath151 . for @xmath83 centered in @xmath56 as above ,",
    "this yields the equation : @xmath152 = 0 .\\ ] ] its only positive solution is : @xmath153 this solution increases with @xmath154 and has a vertical asymptote for @xmath155 .",
    "in practice the conditions were given by gaussian functions .",
    "the width @xmath124 was chosen the same for all @xmath29 .",
    "gaussian conditions allowed measuring the error of estimation locally through the relative error of conditions : @xmath156 we used @xmath157 conditions and @xmath158 .",
    "the pdf was estimated on @xmath159 points and the final pdf was further smoothed with a 10 - point moving average .",
    "tests were carried on the pdf @xmath160 $ ] , @xmath161 $ ] , with @xmath162 such that @xmath163 . computed",
    "values of @xmath124 vary from 0 up to @xmath164 for @xmath165 and up to @xmath166 for @xmath167 .",
    "results are shown in figure 1 .",
    "for @xmath168 the choice of @xmath124 is important for the quality of the estimation .",
    "generally , @xmath167 can be used around pdf maxima , while @xmath165 can be used between maxima . for small @xmath169 @xmath170",
    "may be used . for @xmath171",
    "the estimation is quite good .",
    "the error @xmath172 is small except where @xmath173 .",
    "compared to the classical maximum entropy method using power law conditions , the presented method has some advantages : ( i ) local conditions allow improving the estimation locally and also measuring its quality with @xmath172 ; ( ii ) the discretised pdf is well suited for a heuristic optimisation approach such as the sa , even for high @xmath11 , because there are no intrinsic parameters to be determined ( the lagrange multipliers of the classical approach ) , which the sa has difficulty finding ; ( iii ) the pdf estimate lies between a uniform pdf , obtained for @xmath174 and the experimental values pdf , obtained for @xmath175 and @xmath176 ; a bad pdf estimate obtained for too small @xmath41 or @xmath124 can be improved by smoothing ; ( iv ) for large @xmath2 the method works well for @xmath124 far from optimal .",
    "the method was applied to the study of some asteroid parameters @xcite and solar cycles @xcite .",
    "abramowitz m. abramowitz , i. a. stegun ( eds . ) , handbook of mathematical functions with formulas , graphs , and mathematical tables , 9th printing , dover , new york , 1972 , p. 17 .",
    "jaynes e.t .",
    "jaynes , information theory and statistical mechanics , phys .",
    "106 ( 1957 ) 620 - 630 .",
    "kirk s. kirkpatrick , c.d .",
    "gelatt jr . , m.p .",
    "vecchi , optimization by simulated annealing , science 220 ( 1983 ) 671 - 680 .",
    "parzen e. parzen , on estimation of a probability density function and mode , ann .",
    "33 ( 1962 ) 1065 - 1076 .",
    "pop , statistical distribution of some asteroid parameters , bull . of the transilvania university of braov",
    "iii 4(53 ) ( 2011 ) 139 - 146 .",
    "pop , distribution of the daily sunspot number variation for the last 14 solar cycles , solar physics 276 ( 2012 ) 351 - 361 .",
    "press w.h . press ,",
    "teukolsky , w.t .",
    "vetterling , b.p .",
    "flannery , numerical recipes : the art of scientific computing , third edition , cambridge university press , 2007 , pp .",
    "228 - 229 .",
    "rosenblatt m. rosenblatt , remarks on some nonparametric estimates of a density function , ann .",
    "statist . 27",
    "( 1956 ) 832 - 837 ."
  ],
  "abstract_text": [
    "<S> we describe a method to computationally estimate the probability density function of a univariate random variable by applying the maximum entropy principle with some local conditions given by gaussian functions . the estimation errors and optimal values of parameters are determined . </S>",
    "<S> experimental results are presented . </S>",
    "<S> the method estimates the distribution well if a large enough selection is used , typically at least 1 000 values . </S>",
    "<S> compared to the classical approach of entropy maximisation , local conditions allow improving estimation locally . </S>",
    "<S> the method is well suited for a heuristic optimisation approach .    </S>",
    "<S> * keywords : * maximum entropy method , probability distribution estimation , gaussian function , simulated annealing </S>"
  ]
}