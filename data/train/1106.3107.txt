{
  "article_text": [
    "experimental distributions of specific variables in high - energy physics are altered by detector effects .",
    "this can be due to limited acceptance , finite resolution , or other systematic effects producing a transfer of events between different regions of the spectra .",
    "provided that they are well controlled experimentally , all these effects can be included in the monte carlo simulation  ( mc ) of the detector response , which can be used to correct the data .",
    "we will not concentrate on the correction of acceptance effects .",
    "it is straightforward to perform it on the distribution corrected for the effects resulting in a transfer of events between different bins of the spectrum .",
    "the detector response is encoded in a transfer matrix connecting the measured and true variables under study . however , as the transfer matrix used in the unfolding is obtained from the simulation of a given physical process , one must perform background subtraction and data / mc corrections for acceptance effects before the unfolding .",
    "several deconvolution methods for data affected by detector effects were described in the past ( see for example  @xcite ) .",
    "we present an unfolding method  ( described into detail in  @xcite ) allowing one to obtain a data distribution as close as possible to the `` real '' one for rather difficult , yet realistic , examples .",
    "this method is based on the idea that if two conditions are satisfied , namely the mc simulation provides a relatively good description of the data and of the detector response , one can use the transfer matrix to compute a matrix of unfolding probabilities . if the first condition is not fulfilled one can iteratively improve the transfer matrix .",
    "our method uses a first step , providing a good result if the difference between data and normalized reconstructed mc is relatively small on the entire spectrum .",
    "if this is not the case , one should proceed with a series of iterations .",
    "the regularization of the method is dynamical , coming from the way the data - mc differences are treated in each bin , at each step of the unfolding method .",
    "this method is to be applied on binned , one dimensional data .",
    "it can be directly generalized to multidimensional problems .",
    "we use a regularization function @xmath0 to dynamically reduce fluctuations and prevent the transfer of events which could be due to fluctuations , particularly in the subtracted background .",
    "this function provides information on the significance of the absolute deviation @xmath1 between data and simulation in a given bin , with respect to the corresponding error @xmath2 .",
    "it is a smooth monotone function going from 0 , when @xmath3 , to 1 , when @xmath4 .",
    "@xmath5 is a scaling factor , used as a regularization parameter .",
    "as we will see in the following , changing the regularization function used in our method will change the way we discriminate between significant deviations and statistical fluctuations .",
    "for the unfolding procedure , we can consider several functions of the relevant variable @xmath6 ( see fig .",
    "[ fig : functions ] ) .     with respect to @xmath6.,width=529 ]    in general , we will use different @xmath5 parameters for the regularization function for each componenent of the unfolding procedure described in the following .",
    "we will see however that some of these parameters can be unified  ( i.e. assigned identical values ) or even dropped  ( when a trivial value is assigned to them ) .      a rather tricky point is the way the unfolding deals with new structures not considered in the mc generator but present in the data .",
    "these structures are affected by the detector effects , and hence they need to be corrected .",
    "it seems that the singular value decomposition ( svd )  @xcite and the iterative  @xcite methods provide a natural way of performing this correction .",
    "however , if the new structures in the data contain a relatively important number of events , they could also affect the normalization of mc spectra with respect to the data  ( needed in the unfolding procedure , as we ll see in the following ) .",
    "for the unfolding procedure described here , we introduce a comparison method between data and mc spectra which is able to distinguish significant shape differences . exploiting the regularization function introduced before",
    ", it counts the events in data  ( @xmath7 ) without including those corresponding to significant new structures . dividing @xmath7 by the number of events in the mc  ( @xmath8 ) , one obtains the data / mc normalization factor .",
    "this procedure is especially useful when the differences between the two spectra consist of relatively narrow structures .",
    "our normalization method allows a meaningful comparison of data and mc spectra and improves the convergence of the algorithm in this case .",
    "if the differences are widely distributed , they have smaller impact on the normalization factor , and the sensitivity of our method is weaker too .",
    "experimental spectra are generally obtained after background subtraction ; this operation  ( performed before the unfolding ) results in an increase in errors for the corresponding data points . due to bin - to - bin or correlated fluctuations of the subtracted background",
    ", these points can fluctuate within their errors .",
    "these fluctuations can be important especially on distribution tails or dips , where the signal is weak and the background subtraction relatively large . actually , it is only when the background subtraction produces a large increase in the uncertainties of the data points  ( well beyond their original statistical errors ) , that these fluctuations become a potential problem .",
    "the problematic regions of the spectrum can be identified even before going to the unfolding . when computing the corrected distribution , the unfolding procedure has to take into account the size of the experimental errors , including those from background subtraction . at this step",
    "we identify large but not significant data - mc deviations .",
    "not doing so could result in propagation of large fluctuations and uncertainties to more precisely known regions of the spectrum .",
    "such an effect of the procedure is to be avoided , and we treat this problem carefully . to our knowledge ,",
    "none of the previous methods aim at dealing with this second type of problem , and at distinguishing it from the previous one .      in the mc simulation of the detector one",
    "can directly determine the number of events which were generated in the bin @xmath9 and reconstructed in the bin @xmath10 ( @xmath11 ) .",
    "provided that the transfer matrix @xmath12 gives a good description of the detector effects , it is straightforward to compute the corresponding folding and unfolding matrix : @xmath13 . here , @xmath14 is the number of bins in data  ( and reconstructed mc ) , while @xmath15 is the one in the unfolding result  ( and true mc ) . the folding probability matrix , as estimated from the mc simulation",
    ", @xmath16 gives the probability for an event generated in the bin @xmath9 to be reconstructed in the bin @xmath10 .",
    "the unfolding probability matrix @xmath17 corresponds to the probability for the `` source '' of an event reconstructed in the bin @xmath10 to be situated in the bin @xmath9 .",
    "the folding matrix describes the detector effects , and one can only rely on the simulation in order to compute it .",
    "the quality of this simulation must be the subject of dedicated studies within the analysis , and generally the transfer matrix can be improved before the unfolding .",
    "systematic errors can be estimated for it and they are propagated to the unfolding result .",
    "the unfolding matrix depends not only on the description of detector effects but also on the quality of the model which was used for the true mc distribution .",
    "it is actually this model which can ( and will ) be iteratively improved , using the comparison of the true mc and unfolded distributions .    in order to perform the unfolding",
    ", one must first use the iterative procedure described in section  [ sec : mcnorm ] to determine the mc normalization coefficient  ( @xmath18 ) .",
    "one can then proceed to the unfolding , where , in the case of identical initial and final binnings , the result for @xmath19 $ ] is given by : @xmath20 with @xmath21 . here , for a given bin k , @xmath22 is the number of true mc events , while @xmath23 is the uncertainty to be used for the comparison of the data  ( @xmath24 ) and the reconstructed mc  ( @xmath25 ) .",
    "@xmath26 is the ( estimated )  vector of the number of events in the data distribution which are associated to a fluctuation in the background subtraction . in the case of different binnings for the data and the unfolding ,",
    "the kroneker symbol @xmath27 must be replaced by a rebinning transformation @xmath28 .",
    "the first two contributions to the unfolded spectrum are given by the normalized true mc and the events potentially due to a fluctuation in background subtraction , which we do not transfer from one bin to another .",
    "then one adds the number of events in the data minus the estimated effect from background fluctuations , minus the normalized reconstructed mc .",
    "a fraction @xmath29 of these events are unfolded using the estimate of the unfolding probability matrix @xmath30 , and the rest are left in the original bin . with the description of the regularization functions given in section [ sec : regf ] , it is clear that reducing @xmath5 would result in increasing the fraction of unfolded events , and reducing the fraction of events left in the original bin .",
    "choosing an appropriate value for this coefficient provides one with a dynamical attenuation of spurious fluctuations , without reducing the performance of the unfolding itself .      as explained in the introduction ,",
    "if the initial true mc distribution does not contain or badly describes some structures which are present in the data , one can iteratively improve it , and hence the transfer matrix .",
    "this can be done by using a better  ( weighted ) true mc distribution , with the same folding matrix describing the physics of the detector , which will yield an improved unfolding matrix .",
    "the improvement is performed for one bin @xmath9 at the time , exploiting the difference between an intermediate unfolding result and the true mc  ( @xmath31 ) : @xmath32 here , @xmath33  ( for modification ) stands for the regularization parameter used when modifying the matrix .",
    "increasing @xmath33 would reduce the fraction of events in @xmath31 used to improve the transfer matrix .",
    "this method allows an efficient improvement of the folding matrix , without introducing spurious fluctuations .",
    "actually , the amplification of small fluctuations can be prevented at this step of the procedure too .",
    "in this section we describe a general unfolding strategy , based on the elements presented before .",
    "it works for situations presenting all the difficulties listed before , even in a simplified form , where some parameters are dropped and the corresponding steps get trivial .",
    "the strategy can be simplified even more , for less complex problems .",
    "one will start with a null estimate of the fluctuations from background subtraction . a first unfolding , as described in section [ sec : foldandunf ] ,",
    "is performed , with a relatively large value of @xmath34 .",
    "this step will not produce any important transfer of events from the regions with potential remaining background fluctuations  ( provided that @xmath35 is large enough ) , while the correction of resolution effects for the new structures in data will be limited too .    at this level",
    "one can start the iterations : + 1 ) estimation of the fluctuations in background subtraction + an estimate of the fluctuations in background subtraction can be obtained using the procedure described in section [ sec : estbkg ] .",
    "the parameter @xmath36 used here must be large enough , in order not to underestimate them .",
    "@xmath36 can however not be arbitrary large , as this operation must not bias initially unknown structures , by not allowing their unfolding .",
    "+ 2 ) improvement of the unfolding probability matrix + using the method described in section [ sec : impmatrix ] , one can improve the folding matrix @xmath12 .",
    "a parameter @xmath33 , small enough for an efficient improvement of the matrix , yet large enough not to propagate spurious fluctuations  ( if not eliminated at another step ) , must be used at this step . +",
    "3 ) an improved unfolding + a parameter @xmath37 will then be used to perform an unfolding following section [ sec : foldandunf ] , exploiting the improvements done at the previous step .",
    "it must be small enough to provide an efficient unfolding , but yet large enough to avoid spurious fluctuations ( if not eliminated elsewhere ) .",
    "+ these three steps will be repeated until one gets a good agreement between data and reconstructed mc plus the estimate of fluctuations in background subtraction .",
    "another way of proceeding  ( providing similar results ) could consist in stopping the iterations when the improvement brought by the last one on the intermediate result is relatively small .",
    "the values of the @xmath5 parameters are to be obtained from ( realistic )  toy simulations , and some of these can be dropped by using trivial  ( null ) values for them .",
    "the needed number of steps can also be estimated using these simulations .",
    "in the following we briefly present a rather complex , yet realistic test , proving the robustness of the method .",
    "it exhibits all the features discussed previously , which are simultaneously taken into account by the unfolding . for the clarity of the presentation ,",
    "the structures and dips of the spectrum are separated .",
    "the structure around the bin 130  ( see fig .  [",
    "fig : toyunfoldingn1 ] ) is present both in data and simulation , while the ones at 90 and 170 are only in data .",
    "the dip around 40 is affected by large fluctuations due to background subtraction in data .",
    "all the structures are affected by resolution and a systematic transfer of events , from high to lower bin numbers .",
    "the first unfolding step was performed with a very large value for @xmath35 and it corrects all the elements of the spectrum which are simulated in the mc , for both kinds of transfer effects  ( in spite of the fact that they are relatively important ) . the final unfolding result  ( after iterations ) reconstructs well all the structures in the data model , without introducing important systematic effects due to the fluctuations in background subtraction  ( see fig .  [",
    "fig : toyunfoldingn1 ] ) .",
    "the errors of the unfolding result(s ) were estimated using 100 mc toys , with fluctuated data and transfer matrix for the unfolding procedure .    , solid line ) and the true mc model plus the new structures  ( @xmath38 , dashed line).,width=604 ]    another example for the use of the ids unfolding method , with less statistics available in the spectra , has been presented in  @xcite .",
    "we have described a new iterative method of data unfolding , using a dynamical regularization .",
    "it allows us to treat several problems , like the effects of new structures in data and the large fluctuations from background subtraction , which were not considered before .",
    "this method has been tested for complex examples , and it was able to treat correctly all the effects mentioned before",
    ".    99 a.  hocker and v.  kartvelishvili , nucl .",
    "instrum .  meth .",
    "a * 372 * , 469 ( 1996 ) [ arxiv : hep - ph/9509307 ] .",
    "v.  blobel , desy 84 - 118 , [ arxiv : hep - ex/0208022 ] .",
    "a.  kondor , jinr - e11 - 82 - 853 .",
    "l.  lindemann and g.  zech , nucl .",
    "instrum .",
    "a * 354 * , 516 ( 1995 ) .",
    "g.  dagostini , nucl .",
    "instrum .",
    "a * 362 * , 487 ( 1995 ) .",
    "d.  acton _ et al . _",
    "[ opal collaboration ] , z.  phys .",
    "c * 59 * , 1 ( 1993 ) .",
    "b.  malaescu ,",
    "arxiv:0907.3791 [ physics.data-an ] . k.  tackmann s talk at this workshop ."
  ],
  "abstract_text": [
    "<S> we describe an iterative unfolding method for experimental data , making use of a regularization function . </S>",
    "<S> the use of this function allows one to build an improved normalization procedure for monte carlo spectra , unbiased by the presence of possible new structures in data . </S>",
    "<S> we unfold , in a dynamically stable way , data spectra which can be strongly affected by fluctuations in the background subtraction and simultaneously reconstruct structures which were not initially simulated . </S>"
  ]
}