{
  "article_text": [
    "planned observations of the cosmic microwave background ( cmb ) will have sufficient angular resolution to probe the cmb power spectrum up to multipoles @xmath3 or more ( for a general review of forthcoming observations see @xcite ) .",
    "if we are able to extract the multipole amplitudes @xmath4 from the data sufficiently accurately we will be able to obtain the values of the fundamental cosmological parameters to unprecedented accuracy .",
    "the cmb will then have lived up to its promise of being the most powerful discriminant between cosmological models @xcite .",
    "extracting the power spectrum is conceptually simple  the raw data is cleaned and converted into a time - ordered dataset .",
    "this is then converted to a sky temperature map , which in turn is analysed to find the location of , and curvature at , the maximum of the likelihood function of the power spectrum . in practice",
    "as the size of the dataset increases the problem rapidly becomes intractable by conventional methods .",
    "this is particularly true of the final step  the likelihood analysis of any reasonably general sky temperature map .",
    "an observation of the cmb contains both signal and noise @xmath5 at each of @xmath0 pixels . for independent , zero - mean , signal and noise the covariance matrix of the data @xmath6 is symmetric , positive definite and dense . for any theoretical power spectrum @xmath4 we can construct the corresponding signal covariance matrix @xmath7 ; knowing the noise covariance matrix @xmath8 for the experiment we now know the observation covariance matrix for that power spectrum @xmath9 .",
    "the probability of the observation given the assumed power spectrum is then @xmath10 assuming a uniform prior , so that @xmath11 we can restrict our attention to evaluating the right hand side of equation ( [ eq.lf ] ) . unfortunately unless the noise covariance matrix is unrealistically simple ( eg . diagonal ) both direct evaluation @xcite and quadratic estimation @xcite of the likelihood function scale as at best @xmath1 @xcite , making them impractical for the forthcoming @xmath12  @xmath13 pixel datasets .",
    "instead of the expensive exact evaluation of the likelihood function , here we implement a much cheaper bounding algorithm due to golub et al @xcite .",
    "this method determines bounds @xmath14 for an @xmath15-vector @xmath16 , symmetric positive definite @xmath17 matrix @xmath18 and smooth function @xmath19 defined on the spectrum of @xmath18 .",
    "the underlying idea is to rewrite the problem as a riemann - stieltjes integral which is then approximated using gaussian quadrature .",
    "since @xmath18 is symmetric it can be written in eigendecomposition as @xmath20 where @xmath21 is the orthogonal matrix of normalised eigenvectors and @xmath22 is the diagonal matrix of increasing eigenvalues @xmath23 .",
    "writing @xmath24 we have @xmath25 where the measure @xmath26 is a piecewise constant function defined by @xmath27 the integral @xmath28 can now be bounded above and below using gauss - radau quadrature @xcite @xmath29 with weights @xmath30 and @xmath31 , nodes @xmath32 and @xmath33 , with opposite - signed remainders @xmath34 .",
    "given the resulting bounds on @xmath28 we can increase the number of nodes @xmath35 until some convergence criterion , such as a maximum relative error , @xmath36 is met . calculating such bounds scales as @xmath37 due to the @xmath38 matrix - vector multiplication in using the lanczos algorithm @xcite to calculate each of the @xmath35 nodes and their weights .",
    "rewriting the likelihood function of equation ( [ eq.lf ] ) as @xmath39 \\right ) \\right)\\ ] ] leaves @xmath40 and @xmath41 $ ] to be evaluated .",
    "the former is already in the required form and can be bounded immediately . for the latter we note that @xmath42\\ ] ] where @xmath43 is a random vector whose elements are @xmath44 with equal probability .",
    "generating @xmath45 realisations of @xmath43 we can calculate the bounds @xmath46 for each , from which we want to derive bounds on the expectation value @xmath47\\ ] ] for each realisation the estimator @xmath48 the sum of the expectation value @xmath49 , a random sample error @xmath50 , and a systematic bound - width error @xmath51 , where @xmath52 ( note that this is an absolute , not relative , error measure ) .",
    "given the variance of the systematic - free data @xmath53 and assuming that @xmath45 is large enough for the central limit theorem to apply , our estimator @xmath54 has student s t - distribution with @xmath55 degrees of freedom , and we can take bounds @xmath56 with @xmath57 confidence . although the systematics prevent us from determining @xmath58 itself we can calculate a stochastically larger quantity as follows : taking the sample variance of the midpoints @xmath59 where we have used the fact that @xmath60 thus , defining @xmath61 we have @xmath62 and hence the bound @xmath63    if the number of terms @xmath35 required to evaluate each of the @xmath45 bound pairs is approximately constant , we have a method to bound the likelihood function which scales as @xmath64 overall .",
    "the viability of this approach depends crucially on the way that the number of nodes in the gauss - radau quadrature ( @xmath35 ) and the number of trace estimates ( @xmath45 ) depend on the size of the dataset ( @xmath15 ) and the required tightness of the bounds ( @xmath65 ) . to examine these dependencies",
    "we have applied the algorithm to subsets of the cobe data and a standard cdm target power spectrum .",
    "the number of nodes required to achieve a given accuracy clearly depends on that accuracy . at the extrema @xmath66 figure 1 shows the dependence of the number of nodes on size of the dataset for typical useful values @xmath67 , @xmath68 , @xmath69 , @xmath70 , and @xmath71 .",
    "the points are from numerical experiments , evaluating bounds on @xmath72 and averaging over 100 realisations of @xmath43 .",
    "the solid lines are power law fits @xmath73 ( giving overall scaling as @xmath74 ) with @xmath75    figure 2 shows the normalised 99% confidence bounds achieved on @xmath76 $ ] for a 1000 pixel dataset as the number of estimates @xmath45 increases , with covergence set at @xmath67 , @xmath68 , @xmath69 , @xmath70 and @xmath71 .",
    "not surprisingly , the 10% bounds on each estimate give a rather poor overall constraint .",
    "however with the 1% and tighter bounds we can determine the logarithm of the likelihood function to 2  3% with 99% confidence in as few as 20 realisations . note that below the 1% level the bounds are dominated by sample error , and become essentially independent of the bound width .",
    "figure 3 shows the 99% confidence bounds achieved after 20 realisations at @xmath77 , showing no systematic variation as the size of the dataset increases .",
    "we have presented an algorithm to calculate probabilistic bounds on the power spectrum likelihood function from an @xmath0-pixel cmb map using gaussian quadrature which scales as between @xmath2 and @xmath78  a very significant advance on existing algorithms for the exact calculation which scale as @xmath1 . since lowering the convergence constraint below the 1% level",
    "gains us only marginally tighter final bounds at the expense of increasing the scaling power , it is not recommended for the forthcoming @xmath79  @xmath80 pixel cmb maps .",
    "our final algorithm of choice therefore gives better than 3% bounds on the logarithm of the likelihood function with @xmath2 operations with 99% confidence .",
    "since this algorithm gives no information about the local curvature of the likelihood function it is not as well suited as quadratic estimator techniques for searching a large multi - dimensional parameter space for its likelihood maximum .",
    "however for direct estimation of a small set of cosmological parameters this technique is certainly viable and fast .",
    "moreover , even when the parameters are taken to be the multipole moments ( individually or in bins ) , quadratic estimation , being a newton - raphson iteration , requires a starting point ` sufficiently close ' to the maximum to guarantee convergence ; the algorithm presented here is then well suited to provide a coarse overall mapping of the likelihood function from which to select a starting point for more refined techniques .",
    "this work was supported by the laboratory directed research and development program of lawrence berkeley national laboratory under the u.s .",
    "department of energy , contract no .",
    "de - aco3 - 76sf00098 , and used resources of the national energy research scientific computing center , which is supported by the office of energy research of the u.s .",
    "department of energy .",
    "this work is also part of the combat project supported by nasa aisrp grant nag5 - 3941 .",
    "the author wishes to thank jim demmel and phil stark for many helpful discussions .",
    "_ figure 1 _ + a log - log plot of the scaling in the number of quadrature nodes @xmath35 required to achieve bounds with a given relative error convergence criterion @xmath65 with the size of the cmb dataset @xmath15 .",
    "the points are obtained from numerical experiments ; the lines are power law fits @xmath73 . from the bottom of the figure reading upwards",
    "the relative errors are @xmath81 and @xmath82 , with corresponding power laws @xmath83 and @xmath84 .    _ figure 2 _ + a plot of the normalised 99% confidence upper and lower bounds achieved on @xmath85 $ ] for a 1000 pixel dataset against the estimator sample size @xmath45 . from the outer limits reading inwards the line - pairs correspond to the relative error covergence criterion being set at @xmath86 and @xmath71 .",
    "_ figure 3 _ + a plot of the variation in the normalised 99% confidence upper and lower bounds achieved on @xmath85 $ ] with the relative error convergence criterion set at @xmath77 after @xmath87 estimates as the size of the cmb dataset @xmath15 increases ."
  ],
  "abstract_text": [
    "<S> as the cosmic microwave background ( cmb ) radiation is observed to higher and higher angular resolution the size of the resulting datasets becomes a serious constraint on their analysis . </S>",
    "<S> in particular current algorithms to determine the location of , and curvature at , the peak of the power spectrum likelihood function from a general @xmath0-pixel cmb sky map scale as @xmath1 . moreover the current best algorithm </S>",
    "<S>  the quadratic estimator  is a newton - raphson iterative scheme and so requires a ` sufficiently good ' starting point to guarantee convergence to the true maximum . here </S>",
    "<S> we present an algorithm to calculate bounds on the likelihood function at any point in parameter space using gaussian quadrature and show that , judiciously applied , it scales as only @xmath2 . although it provides no direct curvature information </S>",
    "<S> we show how this approach is well - suited both to estimating cosmological parameters directly and to providing a coarse map of the power spectrum likelihood function from which to select the starting point for more refined techniques .    8.75 in 6.5 in -30pt    </S>",
    "<S> cfpa 98-th-05 + astro - ph / xxyyzzz + ( submitted to * physical review * ) +     +   + center for particle astrophysics , university of california , berkeley , ca 94720 + and + national energy research scientific computing center , lawrence berkeley national laboratory , university of california , berkeley , ca 94720 +    pacs numbers : 98.80 , 98.70.v , 02.70 + </S>"
  ]
}