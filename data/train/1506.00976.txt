{
  "article_text": [
    "machine learning on time series is a booming field and as such plenty of representations , transformations , normalizations , metrics and other divergences are thrown at disposal to the practitioner .",
    "a further consequence of the recent advances in time series mining is that it is difficult to have a sober look at the state of the art since many papers state contradictory claims as described in @xcite . to be fair , we should mention that when data , pre - processing steps , distances and algorithms are combined together , they have an intricate behaviour making it difficult to draw unanimous conclusions especially in a fast - paced environment . restricting the scope of time series to independent and identically distributed ( i.i.d . )",
    "stochastic processes , we propound a method which , on the contrary to many of its counterparts , is mathematically grounded with respect to the clustering task defined in subsection  [ motivation ] . the representation we present in section [ copula_repr_section ] exploits a property similar to the seminal result of copula theory , namely sklar s theorem @xcite .",
    "this approach leverages the specificities of random variables and this way solves several shortcomings of more classical data pre - processing and distances that will be detailed in subsection [ shortcomings_section ] .",
    "section [ exp_section ] is dedicated to experiments on synthetic and real datasets to illustrate the benefits of our method which relies on the hypothesis of i.i.d .",
    "sampling of the random variables .",
    "synthetic time series are generated by a simple model yielding correlated random variables following different distributions .",
    "the presented approach is also applied to financial time series from the credit default swaps market whose prices dynamics are usually modelled by random walks according to the efficient - market hypothesis @xcite .",
    "this dataset seems more interesting than stocks as credit default swaps are often considered as a gauge of investors fear , thus time series are subject to more violent moves and may provide more distributional information than the ones from the stock market .",
    "we have made our detailed experiments ( cf .",
    "machine tree on the website www.datagrapple.com ) and python code available ( www.datagrapple.com/tech ) for reproducible research .",
    "finally , we conclude the paper with a discussion on the method and we propound future research directions .",
    "machine learning methodology usually consists in several pre - processing steps aiming at cleaning data and preparing them for being fed to a battery of algorithms .",
    "data scientists have the daunting mission to choose the best possible combination of pre - processing , dissimilarity measure and algorithm to solve the task at hand among a profuse literature . in this article",
    ", we provide both a pre - processing and a distance for studying i.i.d .",
    "random processes which are compatible with basic machine learning algorithms .",
    "many statistical distances exist to measure the dissimilarity of two random variables , and therefore two i.i.d . random processes .",
    "such distances can be roughly classified in two families :    1 .",
    "distributional distances , for instance @xcite , @xcite and @xcite , which focus on dissimilarity between probability distributions and quantify divergences in marginal behaviours , 2 .",
    "dependence distances , such as the distance correlation or copula - based kernel dependency measures @xcite , which focus on the joint behaviours of random variables , generally ignoring their distribution properties .",
    "however , we may want to be able to discriminate random variables both on distribution and dependence .",
    "this can be motivated , for instance , from the study of financial assets returns : are two perfectly correlated random variables ( assets returns ) , but one being normally distributed and the other one following a heavy - tailed distribution , similar ? from risk perspective , the answer is no @xcite , hence the propounded distance of this article .",
    "we illustrate its benefits through clustering , a machine learning task which primarily relies on the metric space considered ( data representation and associated distance ) . besides clustering has found application in finance , e.g. @xcite , which gives us a framework for benchmarking on real data .",
    "our objective is therefore to obtain a good clustering of random variables based on an appropriate and simple enough distance for being used with basic clustering algorithms , e.g. ward hierarchical clustering @xcite , @xmath0-means++ @xcite , affinity propagation @xcite .    by clustering",
    "we mean the task of grouping sets of objects in such a way that objects in the same cluster are more similar to each other than those in different clusters .",
    "more specifically , a cluster of random variables should gather random variables with common dependence between them and with a common distribution .",
    "two clusters should differ either in the dependency between their random variables or in their distributions .",
    "a good clustering is a partition of the data that must be stable to small perturbations of the dataset .",
    " stability of some kind is clearly a desirable property of clustering methods \" @xcite . in the case of random variables",
    ", these small perturbations can be obtained from resampling @xcite , @xcite , @xcite in the spirit of the bootstrap method since it preserves the statistical properties of the initial sample @xcite .",
    "yet , practitioners and researchers pinpoint that state - of - the - art results of clustering methodology applied to financial times series are very sensitive to perturbations @xcite .",
    "the observed unstability may result from a poor representation of these time series , and thus clusters may not capture all the underlying information .      a naive but often used distance between random variables to measure similarity and to perform clustering",
    "is the @xmath1 distance @xmath2 $ ] . yet",
    ", this distance is not suited to our task .",
    "let @xmath3 be a bivariate gaussian vector , with @xmath4 , @xmath5 and whose correlation is @xmath6 $ ] .",
    "we obtain @xmath2 = ( \\mu_x - \\mu_y)^2 + ( \\sigma_x - \\sigma_y)^2 + 2\\sigma_x \\sigma_y ( 1 - \\rho(x , y ) ) $ ] .",
    "now , consider the following values for correlation :    * @xmath7 , so @xmath2 = ( \\mu_x - \\mu_y)^2 + \\sigma_x^2 + \\sigma_y^2 $ ] .",
    "the two variables are independent ( since uncorrelated and jointly normally distributed ) , thus we must discriminate on distribution information .",
    "assume @xmath8 and @xmath9 . for @xmath10",
    ", we obtain @xmath2 \\gg 1 $ ] instead of the distance @xmath11 , expected from comparing two equal gaussians .",
    "* @xmath12 , so @xmath2 = ( \\mu_x - \\mu_y)^2 + ( \\sigma_x - \\sigma_y)^2 $ ] .",
    "since the variables are perfectly correlated , we must discriminate on distributions .",
    "we actually compare them with a @xmath1 metric on the mean @xmath13 standard deviation half - plane .",
    "however , this is not an appropriate geometry for comparing two gaussians @xcite .",
    "for instance , if @xmath14 , we find @xmath2 = ( \\mu_x - \\mu_y)^2 $ ] for any values of @xmath15 .",
    "as @xmath15 grows , probability attached by the two gaussians to a given interval grows similar ( cf .",
    "[ fig : equidist_gaussians ] ) , yet this increasing similarity is not taken into account by this @xmath1 distance .",
    "+ 0.2 in +   and @xmath16 ( in green ) , gaussians @xmath17 and @xmath18 ( in red ) , and gaussians @xmath19 and @xmath20 ( in blue ) .",
    "green , red and blue gaussians are equidistant using @xmath1 geometry on the parameter space @xmath21 . ] + -0.2 in    @xmath22 $ ] considers both dependence and distribution information of the random variables , but not in a relevant way with respect to our task .",
    "yet , we will benchmark against this distance since other more sophisticated distances on time series such as dynamic time warping @xcite and representations such as wavelets @xcite or sax @xcite were explicitly designed to handle temporal patterns which are inexistant in i.i.d . random processes .",
    "our purpose is to introduce a new data representation and a suitable distance which takes into account both distributional proximities and joint behaviours .",
    "[ thm]property      let @xmath23 be a probability space .",
    "@xmath24 is the sample space , @xmath25 is the @xmath15-algebra of events , and @xmath26 is the probability measure .",
    "let @xmath27 be the space of all continuous real - valued random variables defined on @xmath23 .",
    "let @xmath28 be the space of random variables following a uniform distribution on @xmath29 $ ] and @xmath30 be the space of absolutely continuous cumulative distribution functions ( cdf ) .",
    "let @xmath31 be a random vector with cdfs @xmath32 .",
    "the random vector @xmath33 is known as the copula transform .",
    "@xmath34 , @xmath35 , are uniformly distributed on @xmath29 $ ] .",
    "@xmath36 .",
    "we define the following representation of random vectors that actually splits the joint behaviours of the marginal variables from their distributional information .",
    "[ gnpr ] let @xmath37 be a mapping which transforms @xmath38 into its generic representation , an element of @xmath39 representing @xmath40 , defined as follow @xmath41    [ bij ] @xmath37 is a bijection .",
    "@xmath37 is surjective as any element @xmath42 has the fiber @xmath43 .",
    "@xmath37 is injective as @xmath44 _ a.s . _ in @xmath45 implies that they have the same cdf @xmath46 and since @xmath47 _ a.s .",
    "_ , it follows that @xmath48 _ a.s . _",
    "this result replicates the seminal result of copula theory , namely sklar s theorem @xcite , which asserts one can split the dependency and distribution apart without losing any information .",
    "[ gnpr_projection ] illustrates this projection for @xmath49 .    0.2 in                -0.2 in      we leverage the propounded representation to build a suitable yet simple distance between random variables which is invariant under diffeomorphism .",
    "let @xmath50 $ ] .",
    "let @xmath51 .",
    "let @xmath52 , where @xmath53 and @xmath54 are respectively @xmath40 and @xmath55 marginal cdfs .",
    "we define the following distance @xmath56 where @xmath57,\\end{aligned}\\ ] ] and @xmath58    in particular , @xmath59 is the hellinger distance related to the bhattacharyya ( 1/2-chernoff ) coefficient @xmath60 upper bounding the bayes classification error . to quantify distribution dissimilarity ,",
    "@xmath61 is used rather than the more general @xmath62-chernoff divergences since it satisfies the properties [ propa ] , [ propb ] , [ diffeo ] ( significant for practitioners ) .",
    "in addition , @xmath63 can thus be efficiently implemented as a scalar product .",
    "@xmath64 is a distance correlation measuring statistical dependence between two random variables , where @xmath65 is the spearman s correlation between @xmath40 and @xmath55 .",
    "notice that @xmath66 can be expressed by using the copula @xmath67 ^ 2 \\rightarrow [ 0,1]$ ] implicitly defined by the relation @xmath68 since @xmath69 @xcite .",
    "let @xmath3 be a bivariate gaussian vector , with @xmath4 , @xmath5 and @xmath70 .",
    "we obtain , @xmath71    remember that for perfectly correlated gaussians ( @xmath72 ) , we want to discriminate on their distributions .",
    "we can observe that    * for @xmath73 , then @xmath74 , it alleviates a main shortcoming of the basic @xmath1 distance which is diverging to @xmath75 in this case ; * if @xmath76 , for @xmath77 , then @xmath78 , its maximum value , i.e. it means that two gaussians can not be more remote from each other than two different dirac delta functions .",
    "we will refer to the use of this distance as the generic parametric representation ( gpr ) approach .",
    "gpr distance is a fast and good proxy for distance @xmath63 when the first two moments @xmath79 and @xmath15 predominate .",
    "nonetheless , for datasets which contain heavy - tailed distributions , gpr fails to capture this information .",
    "[ propa ] let @xmath50 $ ] .",
    "the distance @xmath63 verifies @xmath80 .",
    "let @xmath50 $ ] .",
    "we have    a.   @xmath81 , property of the hellinger distance ; b.   @xmath82 , since @xmath83 .    finally , by convex combination , @xmath84 .    [ propb ] for @xmath85 , @xmath63 is a metric .",
    "let @xmath51 . for @xmath85 , @xmath63 is a metric , and the only non - trivial property to verify is the separation axiom    a.   @xmath86 _ a.s .",
    "_ @xmath87 + @xmath86 _ a.s .",
    "_ @xmath88 , and thus @xmath89 , b.   @xmath90 _ a.s . _ + @xmath91 and @xmath92 @xmath93 _ a.s . _ and @xmath94 .",
    "since @xmath95 is absolutely continuous , it follows @xmath86 _ a.s .",
    "_    notice that for @xmath96 , this property does not hold .",
    "let @xmath97 , @xmath98 $ ] . @xmath99",
    "but @xmath100 . let @xmath101 .",
    "@xmath102 but @xmath103",
    ".    [ diffeo ] diffeomorphism invariance .",
    "let @xmath104 be a diffeomorphism .",
    "let @xmath105 .",
    "distance @xmath106 is invariant under diffeomorphism , i.e. @xmath107    from definition , we have @xmath108 and since @xmath109 we obtain @xmath110 in addition , @xmath111 , we have @xmath112 \\\\   & = \\left\\ {          \\begin{split }           \\mathbb{p}\\left[x\\le x\\right ] & = g_{x}(x),\\mathrm{~if~}h\\mathrm{~increasing } \\\\          1 - \\mathbb{p}\\left[x\\le x\\right ] & = 1 - g_{x}(x ) , \\mathrm{~otherwise }          \\end{split }      \\right .   \\end{split}\\ ] ] which implies that @xmath113 \\\\   & =3 \\mathbb{e}\\left[|g_{x}(x)-g_{y}(y)|^{2}\\right ] \\\\ & = d_{1}^{2}(x , y ) .",
    "\\end{split}\\ ] ] finally , we obtain property [ diffeo ] by definition of @xmath106 .",
    "thus , @xmath63 is invariant under monotonic transformations , a desirable property as it ensures to be insensitive to scaling ( e.g. choice of units ) or measurement scheme ( e.g. device , mathematical modelling ) of the underlying phenomenon .      to apply the propounded distance @xmath63 on sampled data without parametric assumptions , we have to define its statistical estimate @xmath114 working on realizations of the i.i.d .",
    "random variables .",
    "distance @xmath66 working with continuous uniform distributions can be approximated by normalized rank statistics yielding to discrete uniform distributions , in fact coordinates of the multivariate empirical copula @xcite which is a non - parametric estimate converging uniformly toward the underlying copula @xcite .",
    "distance @xmath61 working with densities can be approximated by using its discrete form working on histogram density estimates .",
    "let @xmath115 , @xmath116 , be @xmath117 observations from a random vector @xmath38 with continuous margins @xmath118 . since one can not directly obtain the corresponding copula observations @xmath119 without knowing a priori @xmath53 , one can instead estimate the @xmath120 empirical margins @xmath121 to obtain @xmath117 empirical observations @xmath122 which are thus related to normalized rank statistics as @xmath123 , where @xmath124 denotes the rank of observation @xmath125 .",
    "let @xmath126 and @xmath127 be @xmath117 realizations of real - valued random variables @xmath128 respectively .",
    "an empirical distance between realizations of random variables can be defined by @xmath129 where @xmath130 and @xmath131 @xmath132 being here a suitable bandwidth , and @xmath133 being a density histogram estimating pdf @xmath134 from @xmath126 , @xmath117 realizations of random variable @xmath135 .",
    "we will refer henceforth to this distance and its use as the generic non - parametric representation ( gnpr ) approach .",
    "to use effectively @xmath106 and its statistical estimate , it boils down to select a particular value for @xmath136 .",
    "we suggest here an exploratory approach where one can test ( i ) distribution information ( @xmath137 ) , ( ii ) dependence information ( @xmath138 ) , and ( iii ) a mix of both information ( @xmath139 ) .",
    "ideally , @xmath136 should reflect the balance of dependence and distribution information in the data . in",
    "a supervised setting , one could select an estimate @xmath140 of the right balance @xmath141 optimizing some loss function by techniques such as cross - validation . yet",
    ", the lack of a clear loss function makes the estimation of @xmath141 difficult in an unsupervised setting . for clustering , many authors @xcite , @xcite , @xcite , @xcite suggest stability as a tool for parameter selection .",
    "but , @xcite warn against its irrelevant use for this purpose .",
    "besides , we already use stability for clustering validation and we want to avoid overfitting . finally , we think that finding an optimal trade - off @xmath141 is important for accelerating the rate of convergence toward the underlying ground truth when working with finite and possibly small samples , but ultimately lose its importance asymptotically as soon as @xmath85 .",
    "we propose the following model for testing the efficiency of the gnpr approach : @xmath120 time series of length @xmath117 which are subdivided into @xmath142 correlation clusters themselves subdivided into @xmath143 distribution clusters .",
    "let @xmath144 , be @xmath142 i.i.d .",
    "random variables .",
    "let @xmath145 .",
    "let @xmath146 .",
    "let @xmath147 , @xmath35 , be independent random variables . for @xmath35 , we define @xmath148 where    a.   @xmath149 , if @xmath150 ( mod @xmath143 ) , @xmath11 otherwise ; b.   @xmath151 $ ] , c.   @xmath152 , if @xmath153 , @xmath11 otherwise .    @xmath154 are partitioned into @xmath155 clusters of @xmath156 random variables each . playing with the model parameters , we define in table @xmath157 some interesting test case datasets to study distribution clustering , dependence clustering or a mix of both .",
    "we use the following notations as a shorthand : @xmath158 and @xmath159-distribution@xmath160 .",
    "since @xmath161 and @xmath162 have both a mean of @xmath11 and a variance of @xmath163 , gpr can not find any difference between them , but gnpr can discriminate on higher moments as it can be seen in fig .",
    "[ fig : dist_comp_gpr_gnpr ] .    ) , but only gnpr finds the 2 distributions ( @xmath162 and @xmath161 ) subdividing them ( @xmath137 ) . finally , by combining both information gnpr ( @xmath139 ) can highlight the 10 original clusters , while gpr ( @xmath139 ) simply adds noise on the correlation distance matrix it recovers.,title=\"fig : \" ] gpr @xmath137    ) , but only gnpr finds the 2 distributions ( @xmath162 and @xmath161 ) subdividing them ( @xmath137 ) . finally , by combining both information gnpr ( @xmath139 ) can highlight the 10 original clusters , while gpr ( @xmath139 ) simply adds noise on the correlation distance matrix it recovers.,title=\"fig : \" ] gpr @xmath138    ) , but only gnpr finds the 2 distributions ( @xmath162 and @xmath161 ) subdividing them ( @xmath137 ) .",
    "finally , by combining both information gnpr ( @xmath139 ) can highlight the 10 original clusters , while gpr ( @xmath139 ) simply adds noise on the correlation distance matrix it recovers.,title=\"fig : \" ] gpr @xmath139    ) , but only gnpr finds the 2 distributions ( @xmath162 and @xmath161 ) subdividing them ( @xmath137 ) . finally , by combining both information gnpr ( @xmath139 ) can highlight the 10 original clusters , while gpr ( @xmath139 ) simply adds noise on the correlation distance matrix it recovers.,title=\"fig : \" ] gnpr @xmath137    ) , but only gnpr finds the 2 distributions ( @xmath162 and @xmath161 ) subdividing them ( @xmath137 ) .",
    "finally , by combining both information gnpr ( @xmath139 ) can highlight the 10 original clusters , while gpr ( @xmath139 ) simply adds noise on the correlation distance matrix it recovers.,title=\"fig : \" ] gnpr @xmath138    ) , but only gnpr finds the 2 distributions ( @xmath162 and @xmath161 ) subdividing them ( @xmath137 ) . finally , by combining both information gnpr ( @xmath139 ) can highlight the 10 original clusters , while gpr ( @xmath139 ) simply adds noise on the correlation distance matrix it recovers.,title=\"fig : \" ] gnpr @xmath139     @xmath117 & @xmath164 & @xmath142 & @xmath165 & @xmath166 & @xmath167 & @xmath168 & @xmath169 & @xmath170 + & a & 200 & 5000 & 4 & 1 & 0 & @xmath171 & @xmath171 & @xmath161 & @xmath162 & @xmath172 + & b & 200 & 5000 & 10 & 10 & 0.1 & @xmath162 & @xmath162 & @xmath162 & @xmath162 & @xmath162 + & c & 200 & 5000 & 10 & 5 & 0.1 & @xmath171 & @xmath171 & @xmath162 & @xmath171 & @xmath162 + & g & @xmath173 & @xmath174 & 32 & 8 & 0.1 & @xmath171 & @xmath171 & @xmath172 & @xmath161 & @xmath162 +      we empirically show that the gnpr approach achieves better results than others using common distances regardless of the algorithm used on the defined test cases a , b and c described in table [ tab2 ] .",
    "test case a illustrates datasets containing only distribution information : there are 4 clusters of distributions .",
    "test case b illustrates datasets containing only dependence information : there are 10 clusters of correlation between random variables which are heavy - tailed .",
    "test case c illustrates datasets containing both information : it consists in 10 clusters composed of 5 correlation clusters and each of them is divided into 2 distribution clusters . using scikit - learn implementation @xcite",
    ", we apply @xmath175 clustering algorithms with different paradigms : a hierarchical clustering using average linkage ( hc - al ) , @xmath0-means++ ( km++ ) , and affinity propagation ( ap ) .",
    "experiment results are reported in table [ tab1 ] .",
    "gnpr performance is due to its proper representation ( cf .",
    "[ fig : dist_comp ] ) . finally , we have noticed increasing precision of clustering using gnpr as time @xmath117 grows to infinity , all other parameters being fixed .",
    "the number of time series @xmath120 seems rather uninformative as illustrated in fig .",
    "[ fig : nt_conv ] ( left ) which plots ari @xcite between computed clustering and ground - truth of dataset g as an heatmap for varying @xmath120 and @xmath117 .",
    "[ fig : nt_conv ] ( right ) shows the convergence to the true clustering as a function of @xmath117 .",
    "distance , gpr and gnpr .",
    "none but gnpr highlights the 10 original clusters which appear on its diagonal.,title=\"fig : \" ] @xmath176     distance , gpr and gnpr .",
    "none but gnpr highlights the 10 original clusters which appear on its diagonal.,title=\"fig : \" ] @xmath1     distance , gpr and gnpr . none but",
    "gnpr highlights the 10 original clusters which appear on its diagonal.,title=\"fig : \" ] gpr @xmath139     distance , gpr and gnpr .",
    "none but gnpr highlights the 10 original clusters which appear on its diagonal.,title=\"fig : \" ] gnpr @xmath139     algo . &",
    "distance & a & b & c + & @xmath177 & 0.00 & 0.99 & 0.56 + & & 0.00 & 0.09 & 0.55 + & @xmath137 & 0.34 & 0.01 & 0.06 + & @xmath138 & 0.00 & 0.99 & 0.56 + & @xmath178 & 0.34 & 0.59 & 0.57 + & @xmath179 & * 1 * & 0.00 & 0.17 + & @xmath180 & 0.00 & * 1 * & 0.57 + & @xmath181 & 0.99 & 0.25 & * 0.95 * + & @xmath177 & 0.00 & 0.60 & 0.46 + & & 0.00 & 0.34 & 0.48 + & @xmath137 & 0.41 & 0.01 & 0.06 + & @xmath138 & 0.00 & 0.45 & 0.43 + & @xmath178 & 0.27 & 0.51 & 0.48 + & @xmath179 & * 0.96 * & 0.00 & 0.14 + & @xmath180 & 0.00 & * 0.65 * & 0.53 + & @xmath181 & 0.72 & 0.21 & * 0.64 * + & @xmath177 & 0.00 & 0.99 & 0.48 + & & 0.14 & 0.94 & 0.59 + & @xmath137 & 0.25 & 0.01 & 0.05 + & @xmath138 & 0.00 & 0.99 & 0.48 + & @xmath178 & 0.06 & 0.80 & 0.52 + & @xmath179 & * 1 * & 0.00 & 0.18 + & @xmath180 & 0.00 & * 1 * & 0.59 + & @xmath181 & 0.39 & 0.39 & * 1 * +    ]    ]        it has been noticed that straightfoward approaches automatically discover sector and industries @xcite .",
    "since detected patterns are blatantly correlation - flavoured , it prompted econophysicists to focus on correlations , hierarchies and networks @xcite from the minimum spanning tree and its associated clustering algorithm the single linkage to the state of the art @xcite exploiting the topological properties of the planar maximally filtered graph @xcite and its associated algorithm the directed bubble hierarchical tree ( dbht ) technique @xcite . in practice , econophysicists consider the assets log returns and compute their correlation matrix .",
    "the correlation matrix is then filtered thanks to a clustering of the correlation - network @xcite built using similarity and dissimilarity matrices which are derived from the correlation one by convenient _",
    "ad hoc _ transformations .",
    "clustering these correlation - based networks @xcite aims at filtering the correlation matrix for standard portfolio optimization @xcite . yet , adopting similar approaches only allow to retrieve information given by assets co - movements and nothing about the specificities of their returns behaviour , whereas we may also want to distinguish assets by their returns distribution .",
    "for example , we are interested to know whether they undergo fat tails , and to which extent .",
    "we apply the gnpr approach on financial time series , namely daily credit default swap @xcite ( cds ) prices .",
    "we consider the @xmath182 most actively traded cds according to dtcc ( http://www.dtcc.com/ ) .",
    "for each cds , we have @xmath183 observations corresponding to historical daily prices over the last 9 years , amounting for more than one million data points . since credit default swaps are traded over - the - counter , closing time for fixing prices can be arbitrarily chosen , here 5 pm gmt , i.e. after the london stock exchange trading session .",
    "this synchronous fixing of cds prices avoids spurious correlations arising from different closing times .",
    "for example , the use of close - to - close stock prices artificially overestimates intra - market correlation and underestimates inter - market dependence since they have different trading hours @xcite .",
    "these cds time series can be consulted on the web portal http://www.datagrapple.com/.    assuming that cds prices @xmath184 follow random walks , their increments @xmath185 are i.i.d .",
    "random variables , and therefore the gnpr approach can be applied to the time series of prices variations , i.e. on data @xmath186 , @xmath116 .",
    "thus , for aggregating cds prices time series , we use a clustering algorithm ( for instance , ward s method @xcite ) based on the gnpr distance matrices between their variations .    using gnpr @xmath137 ,",
    "we look for distribution information in our cds dataset .",
    "we observe that clustering based on the gnpr @xmath137 distance matrix yields 4 clusters which fit precisely the multi - modal empirical distribution of standard deviations as can be seen in fig .",
    "[ fig : histo_vol ] . for gnpr @xmath138",
    ", we display in fig .",
    "[ fig : correl_mat ] the rank correlation distance matrix obtained .",
    "we can notice its hierarchical structure already described in many papers , e.g. @xcite , @xcite , focusing on stock markets .",
    "there is information in distribution and in correlation , thus taking into account both information , i.e. using gnpr @xmath139 , should lead to a meaningful clustering .",
    "we verify this claim by using stability as a criterion for validation . practically , we consider even and odd trading days and perform two independent clusterings , one on even days and the other one on odd days .",
    "we should obtain the same partitions . in fig .",
    "[ fig : stability_gnpr_cds ] , we display the partitions obtained using the gnpr @xmath139 approach next to the ones obtained by applying a @xmath1 distance on prices returns .",
    "we find that gnpr clustering is more stable than @xmath1 on returns clustering .",
    "moreover , clusters obtained from gnpr are more homogeneous in size .",
    "clusters found using gnpr @xmath137 represented by the 4 colors fit precisely the multi - modal distribution of standard deviations . ]",
    "exhibits a hierarchical structure of correlations : first level consists in europe , japan and us ; second level corresponds to credit quality ( investment grade or high yield ) ; third level to industrial sectors . ]        to conclude on the experiments , we have highlighted through clustering that the presented approach leveraging dependence and distribution information leads to better results : finer partitions on synthetic test cases and more stable partitions on financial time series .",
    "in this paper , we have exposed a novel representation of random variables which could lead to improvements in applying machine learning techniques on time series describing underlying i.i.d .",
    "stochastic processes .",
    "we have empirically shown its relevance to deal with random walks and financial time series .",
    "we have led a large scale experiment on the credit derivatives market notorious for not having gaussian but heavy - tailed returns , first results are available on website www.datagrapple.com .",
    "we also intend to lead such clustering experiments for testing applicability of the method to areas outside finance . on the theoretical side",
    ", we plan to improve the aggregation of the correlation and distribution part by using elements of information geometry theory and to study the consistency property of our method .",
    "we thank frank nielsen , the anonymous reviewers , and our colleagues at hellebore capital management for giving feedback and proofreading the article .",
    "28 natexlab#1#1[1]`#1 ` [ 2]#2 [ 1]#1 [ 1]http://dx.doi.org/#1 [ ] [ 1]pmid:#1 [ ] [ 2]#2 , , . .",
    ". , . . , . , . . , . , , ,",
    ". . . , , , , ,",
    ". . , . , ,",
    ". . , . , ,",
    ". . , . , , ,",
    ". , . . , , .",
    ", , , , . , . , , , ,",
    ", , , , , . . ,",
    ", , , , . . ,",
    ". , . . , . , ,",
    ". . , . , ,",
    ". . , . , , , ,",
    ", , , . . ,",
    ", , , , , , , , , , , , , , , , . . ,",
    ", , , . . ,",
    ", , , . . ,",
    ", , , . . ,",
    ", , , , . . ,"
  ],
  "abstract_text": [
    "<S> this paper presents a pre - processing and a distance which improve the performance of machine learning algorithms working on independent and identically distributed stochastic processes . </S>",
    "<S> we introduce a novel non - parametric approach to represent random variables which splits apart dependency and distribution without losing any information . </S>",
    "<S> we also propound an associated metric leveraging this representation and its statistical estimate . besides experiments on synthetic datasets , the benefits of our contribution is illustrated through the example of clustering financial time series , for instance prices from the credit default swaps market . </S>",
    "<S> results are available on the website www.datagrapple.com and an ipython notebook tutorial is available at www.datagrapple.com/tech for reproducible research . </S>"
  ]
}