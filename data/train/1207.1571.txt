{
  "article_text": [
    "as the silicon technology approaches subsequent physical barriers , keeping the exponential growth rate of computational power of computers poses numerous scientific and technological challenges .",
    "today , most of performance improvements comes from increased parallelism .",
    "however , since a vast majority of the existing technologies for writing parallel applications were designed for coarse - grained concurrency and rely on bulk - synchronous algorithms , further progress requires new computer architectures , algorithms and programming models aimed at fine - grained on - chip parallelism @xcite a promising attempt in this direction is the massively parallel architecture of modern graphics processor units ( gpus ) . in just a few years",
    "gpus evolved into versatile programmable computing devices , whose peak computational performance matches that of the most powerful supercomputers of a decade ago . for this reason gpus are used as numerical accelerators on a vast variety of systems , from laptops to many of today s @xcite and tomorrow s @xcite petaflop supercomputers .",
    "many computational fluid dynamics ( cfd ) algorithms are inherently data - parallel , and hence suitable for gpu acceleration .",
    "there are , however , several obstacles to reach this goal .",
    "first , existing industry - level cfd algorithms and data structures were mostly developed for sequential or coarse - grained parallel architectures .",
    "second , the massively parallel architecture of gpus imposes stiff conditions on the software to exploit the hardware efficiently .",
    "finally , redesigning existing applications and porting them to gpus using new programming paradigms requires a considerable time and effort .",
    "in @xcite we accelerated a standard finite volume cfd solver ( openfoam ) by replacing its linear system solvers and sparse matrix vector product ( smvp ) kernels with the corresponding gpu implementations .",
    "a significant speedup was achieved only for steady state problems , and we attributed marginal acceleration of transient problems to frequent data format conversions and additional data transfers . here",
    "we tackle the problem of porting of a complete finite volume solver to the gpu .",
    "we chose to implement pressure - implicit with splitting of operators ( piso ) and semi - implicit method for pressure - linked equations ( simple ) solvers @xcite and examine whether eliminating intermediate data transfers through a narrow cpu - gpu bus and adjusting the internal data format to the needs of the gpu is sufficient to significantly accelerate the time to solution .",
    "we also perform a more detailed analysis of the conditions necessary to obtain high acceleration rates .",
    "the literature devoted to porting cfd algorithms to gpus is ample , but a large part of the research has focused on cfd solvers based on structured , regular grids .",
    "while this approach facilitates coalescing of device memory accesses and leads to a gpu - accelerated code that was reported as several @xcite , tens @xcite or even hundreds @xcite times faster than the corresponding cpu - only solution , the usability of such cfd programs is limited to simple geometries or applications in computer graphics @xcite .",
    "for example , cohen and molemaker @xcite accelerated a solver for the boussinesq approximation of the navier - stokes equations on a regular three - dimensional ( 3d ) grid , and found an 8-fold speedup versus the corresponding multithreaded fortran code running on an 8-core dual - socket intel xeon processor .",
    "another example is the work of tlke and krafczyk @xcite , who found their gpu implementation of a 3d lattice - boltzmann method for flows through porous media up to two orders of magnitude faster than a corresponding single - core cpu code . while reports on two- or even three - digit speed gains should be interpreted cautiously @xcite , they show a great potential of gpu accelerators .",
    "our gpu implementation works on _ unstructured _ grids , which is necessary to make it applicable for realistically complex geometries .",
    "so far , optimized gpu implementations of cfd solvers based on unstructured grids have been relatively rare , mostly because of stringent requirements for efficient utilization of the gpu . for example , klckner et  al .",
    "@xcite implemented discontinuous galerkin methods over unstructured grids and found that the highest acceleration is achieved for higher - order cases due to their larger arithmetic intensity which helps hide indirect addressing latencies .",
    "alternative optimization techniques were recently used by corrigan et  al .",
    "@xcite to show that a tesla 10 series gpu can accelerate a finite - volume solver for an inviscid , compressible flow over an unstructured grid by almost @xmath0 times relative to an openmp - parallelized cpu code .",
    "there are also several reports on accelerating existing cfd software with gpus .",
    "for example , corrigan et  al .",
    "@xcite used a python script to semi - automatically translate openmp loops to gpu kernels in a large - scale cfd code ( nearly a million lines of fortran 77 code ) , feflo .",
    "another strategy is to port complete solvers .",
    "this approach was used , for example , by elsen et  al .",
    "@xcite , who accelerated the navier stokes stanford university solver ( nssus ) and found the speedup of over @xmath1 times for complex geometries containing up to @xmath2 million grid points .",
    "porting of the mbflo2 multi - block turbulent flow solver was described by phillips et  al .",
    "@xcite , who found a 9-fold speedup over the original cpu implementation . both feflo and mbflo2 libraries",
    "are , however , based on structured grids .",
    "while there are several libraries aimed at gpu - acceleration of existing unstructured - grid cfd libraries , e.g.cufflink ( http://cufflink-library.googlecode.com ) and ofgpu ( http://www.symscape.com/gpu-0-2-openfoam ) , their design adheres to the `` partial acceleration '' rather than `` full port '' strategy .",
    "in particular , they typically use gpus only to accelerate some well - defined , time - consuming and data - parallel tasks , like solving large sparse linear systems .",
    "this approach introduces some artificial overhead related to frequent cpu @xmath3 gpu @xmath3 cpu data transfers as well as data format conversions .",
    "therefore our aim is to go a step further and develop a gpu - only implementation of a cfd solver that would have selected features of industrial solvers : support for arbitrary meshes ( orthogonal or nonorthogonal ) and time - dependent boundary conditions .",
    "piso and simple are standard cfd solvers for incompressible flows and we followed @xcite in their implementation .",
    "we used the finite volume method ( fvm ) to discretize navier - stokes equations which are then solved iteratively until convergence .",
    "this iterative procedure consists of a sequence of well - defined steps .",
    "for example , piso starts with the solution of the momentum equation followed by a series of solutions of the pressure equation and explicit velocity corrections . the simple model works on similar principles , but is optimized for steady - state flows . thus , at such a coarse - grain level of description , both algorithms are essentially sequential and can not be parallelized .",
    "fortunately , individual steps of these algorithms can be parallelized using gpu .    to port the solvers to the gpu architecture , we used cuda , the computer architecture and software development tools for modern nvidia accelerators @xcite .",
    "one of the most distinguishing features of the cuda programming model is the hierarchical organization of the memory .",
    "thus , one of major contributions of our paper is reorganization of solver data structures to enhance the efficiency of internal data transfers in a gpu device .",
    "in particular , we focused on enhancing efficiency of the solution of large sparse linear systems , the most time - consuming operation in the piso and simple solvers .",
    "this nontrivial problem is the subject of intensive research @xcite , as many advanced techniques , like lu - based preconditioning , contain large serial components .",
    "here we focus on conjugate gradient ( cg ) and biconjugate gradient stabilized ( bicgstab ) iterative solvers with jacobi preconditioning @xcite , two methods known to be amenable to effective fine - grained parallelization .",
    "since gpus not only execute programs in parallel , but also access their memory in parallel , choosing right data structures is of highest importance .",
    "the data processed in fvm - based cfd solvers come from discretization of space , time and flow equations .",
    "space is divided into a mesh of @xmath4 cells .",
    "cells are polyhedrons with flat faces , and each face belongs to exactly two polyhedrons or is a boundary face .",
    "pressure and velocity are defined at centroids of the cells . since partial differential equations are local in space and time",
    ", their discretization leads to nonlinear algebraic equations relating the velocity and pressure at each polyhedron with their values at adjacent cells only .",
    "after linearization , these equations reduce to a linear system @xmath5 where @xmath6 and @xmath7 are vectors of length @xmath4 and @xmath8 is a sparse matrix such that @xmath9 if and only if cells @xmath10 and @xmath11 have a common face or @xmath12 .",
    "the value of @xmath13 can depend on the current and previous values of the pressure and velocity at @xmath10 , @xmath11 as well as on some face - specific parameters , e.g.area of the face .",
    "matrix @xmath8 must be assembled many times and then used in a linear solver . during these operations @xmath8",
    "is accessed in rows or columns as if in sparse matrix - vector and sparse transposed matrix - vector products ( stmvp ) .",
    "although the highest priority must be granted to the optimization of smvp , @xmath8 must be at the same time stored in a way enabling a reasonably efficient implementation of stmvp .",
    "several formats designed for efficient implementation of the smvp kernel on modern gpus were investigated by bell and garland @xcite and the data format implemented in our simple and piso solvers is similar to their hybrid format . in the original hybrid format @xmath8",
    "is split into two disjoint parts : @xmath14 , where @xmath15 is stored in the ell format , whereas @xmath16 is stored in the coo format .",
    "consider the following example : @xmath17 in ell , this matrix would be represented in two 2d arrays : ` v ` and i @xcite , @xmath18 in general , dimensions of both of these arrays are @xmath19 , where @xmath4 is the number of cells , @xmath20 is a small integer , and the entries in ` i ` are integers between 0 and @xmath21 . while this format gives excellent memory bandwidth when the matrix is accessed by rows , it is completely inadequate for accessing it by columns .",
    "we can , however , take advantage of the fact that @xmath8 is structurally symmetric ( @xmath22 iff @xmath23 ) and extend the ell format with an additional @xmath19 integer array ` j ` defined indirectly as follows .",
    "for the sake of clarity assume that @xmath15 is also structurally symmetric . by definition of ell , for any @xmath24 and @xmath25 , @xmath26 with @xmath11 = @xmath27 .",
    "the corresponding entry in the transposed matrix , @xmath28 , is stored in ` v ` in row @xmath11 and some column @xmath29 ( @xmath30 ) .",
    "the value of @xmath31 is defined as @xmath29 .",
    "evaluation of one entry of ` j ` for the exemplary matrix @xmath15 is illustrated in fig .",
    "[ fig : j ] ,    [ 2 ] ` for the examplary matrix described in the text .",
    "first it is checked ` v[1][2 ] ` hold @xmath32 .",
    "then the indices of @xmath32 are transposed to get @xmath33 and since the latter is stored in ` v ` in column 0 , ` j[1][2 ] ` @xmath34 . ]    and its full form reads @xmath35    to access @xmath15 by rows one can simply use the ell format , i.e. , arrays ` v ` and ` i ` . to access a column @xmath36 of @xmath15 ,",
    "one first reads the entries in row @xmath36 of array ` i ` , which in this case identify the row numbers of nonzero entries in this column .",
    "these row numbers are then used as row indices into ` v ` , with column indices read from ` j ` .    if @xmath15 is not structurally symmetric , i.e. if @xmath37 and @xmath38 for some @xmath39 , then the value of @xmath40 is stored in @xmath41 @xmath42 and @xmath13 is stored at @xmath43 with some @xmath44 .",
    "this can be indicated in the computer representation by assigning a negative value to @xmath31 . in a similar way the computer representation of @xmath41 stores the information that @xmath13 is to be found in @xmath15 .",
    "note that while arrays ` i ` and ` j ` facilitate column accesses to a matrix , these accesses rely on indirect addressing , which results in uncoalesced , inefficient memory transfers .",
    "the only way to improve this would be to store a transposed matrix besides the original one , but this would require not only extra storage , but also a costly matrix transposition to be repeated every time a new matrix is assembled .",
    "in contrast , arrays ` i ` and ` j ` can be initialized only once in the program and be shared by all data related to the faces .",
    "note also that arrays ` i ` and ` j ` could be combined into a single integer array , ` q ` , to reduce memory requirements of the program .",
    "for example , @xmath45 and @xmath46 can be encoded in @xmath47 as @xmath48`i`@xmath49 ` j`@xmath50 or @xmath51`j`@xmath49 ` i`@xmath50 , with trivial decoding of @xmath45 and @xmath46 through integer division and remainder operations .",
    "while in the original hybrid format @xmath16 is represented in the coo format , we used compressed row sparse ( crs ) instead to further reduce the memory usage .",
    "however , this particular choice does not seem to influence the overall performance significantly .",
    "most of the polyhedrons forming the mesh are of the same simple shape , e.g. , they are tetrahedra or parallelepipeds .",
    "this means that most cells have the same number of neighbours , typically 4 or 6 .",
    "this , in turn , implies that most of the entries in @xmath8 can fit into @xmath15 , with @xmath16 containing only a small fraction of nonzero entries in @xmath8 .",
    "in particular , @xmath16 usually vanishes altogether for structured meshes .",
    "to validate the code and evaluate its performance we solved three cfd problems : a steady flow in a 3d lid - driven cavity @xcite , the transient poiseuille flow @xcite in two dimensions , and the steady flow through the human left coronary artery ( lca ) ( see fig .  [",
    "fig : res ] a  c ) .",
    "the cavity was a cube of side length @xmath52  m. a constant velocity @xmath53  m / s was imposed at its top face and the kinematic viscosity @xmath54  m@xmath55/s was assumed ( reynolds number @xmath56 ) . to evaluate how the solver performance depends on the problem size",
    ", we varied the regular mesh resolution from @xmath57 to @xmath58 cells ( see tab .",
    "[ tab_test_cases ] ) .        .basic parameters of the meshes [ cols=\"<,>,>,^\",options=\"header \" , ]     on can see that for orthogonal meshes the matrix assembly time is dominated by the gradient .",
    "this is a gpu - specific phenomenon , a consequence of the fact that in this operator the faces are accessed in a different order than in smvp , which leads to uncoalesced memory accesses and significant overall performance degradation . for nonorthogonal",
    "meshes the laplacian requires additional , costly corrections and dominates the matrix assembly time .",
    "the largest problem we were able to run on a 6  gb device , ` cav223 ^ 3 ` , occupied 5.9  gb for the piso and 5.7  gb for the simple solver , respectively . in each test case",
    "the device memory footprint was similar for piso and simple and , for problems of size @xmath59 , equaled to @xmath60 bytes per cell .",
    "our results show that a gpu can outperform a six - core server - class cpu running algorithmically equivalent implementations of popular cfd solvers , simple or piso , by a factor slightly exceeding 4 .",
    "this value is consistent with the ratio 4.5 of the theoretical memory bandwidths of the two processors used in our test , 144 gb / s ( gpu ) and 32 gb / s ( cpu ) .",
    "we show , however , that despite its huge computational power , the gpu is still inferior to the cpu if the latter uses the most efficient algorithms .",
    "this is because the gpu is not flexible enough to allow both direct and efficient implementation of many procedures optimized for cpus .",
    "for example , some of the most efficient preconditioners for the cg sparse linear solver are based on incomplete lu decomposition , which , unfortunately , has a large serial component .",
    "this issue has attracted a lot of interest .",
    "recently , naumov showed a @xmath61 speedup over a quad - core cpu in the incomplete - lu preconditioned iterative methods @xcite , helfenstein and koko @xcite reported a 10-fold acceleration over a single - threaded cpu code , and nvidia included ilu0-class preconditioners in its forthcoming cusparse 5.0 gpu library @xcite .",
    "even better results for the pressure solver performance can often be achieved with multigrid methods and the effort to produce efficient gpu implementation of such solvers is also intensive , see for example geveler et  al .",
    "@xcite and references therein .",
    "in particular , geveler et  al .",
    "reported a 5@xmath62 average speedup over a multithreaded cpu code .",
    "these achievements are mostly concentrated on accelerating a particular , rather narrow aspect of a cfd solver . by replacing in our code the jacobi - peconditioned cg with one of the above - mentioned solutions",
    ", it should be possible to produce a fully functional cfd solver that would at least match the speed of standard cfd software running on cpus , which is our plan for the future .",
    "if a faster pressure solver is used in future cfd software on gpus , the role of the remaining solver components will increase dramatically .",
    "moreover , since the gap between the computational power of processors and the speed of the bus connecting gpu with cpu is expected to be still widening , porting of complete software rather than accelerating only its parts will become more and more desirable .",
    "we implemented piso and simple solvers on gpus and investigated their main properties .",
    "the implementations are fully functional , execute completely on the gpu using double precision and support time - dependent boundary conditions and arbitrary meshes . to facilitate a complete gpu port , we proposed a generic data format for internal data storage which helps implement elementary cfd solver operations like smvp , gradient or laplacian .",
    "if gpu and cpu execute essentially the same algorithms , a gpu ( tesla c2070 ) can outperform a server - class , 6-core cpu ( intel xeon x5670 ) by up to about 4.2 times .",
    "we also investigated how the acceleration scales with the problem size and estimate that the minimum problem size for which gpu can outperform cpu is @xmath63 . since our gpu implementation exploits a simple pressure solver , we compared our results against the cpu running simple or piso with a state - of - the art multigrid ( gamg ) pressure solver and found that a better pressure solver is needed for serious cfd applications on gpus .",
    "we also carried out a detailed , coarse- and fine - grained profiling of our solvers , finding that their implementation is close to optimal , which confirmed again that the only way for gpus to match the efficiency of cpus in piso and simple kernels is a better pressure solver .",
    "tt and kz prepared this publication as part of the project of the city of wrocaw , entitled `` green transfer ''  academia - to - business knowledge transfer project co - financed by the european union under the european social fund , under the operational programme human capital ( op hc ) : sub - measure 8.2.1 .",
    "zk and mm acknowledge support from polish ministry of science and higher education grant no .",
    "n n519 437939 .",
    "we kindly acknowledge f.  rikhtegar and v.  kurtcuoglu ( eth zurich ) for providing the artery data and fruitful discussions . this research was supported in part by pl - grid infrastructure . a tesla c2070 gpu donation from nvidia is also gratefully acknowledged .",
    "nathan bell and michael garland . implementing sparse matrix - vector multiplication on throughput - oriented processors . in _",
    "sc 09 : proceedings of the conference on high performance computing networking , storage and analysis _ , pages 111 , new york , ny , usa , 2009 .",
    "acm .",
    "m.  geveler , d.  ribbrock , d.  gddeke , p.  zajac , and s.  turek . towards a complete fem - based simulation toolkit on gpus : unstructured grid finite element geometric multigrid solvers with strong smoothers based on sparse approximate inverses . ,",
    "pages   , 2012 . in press .",
    "v.  w. lee , ch .",
    "kim , j.  chhugani , m.  deisher , d.  kim , a.  d. nguyen , n.  satish , m.  smelyanskiy , s.  chennupaty , p.  hammarlund , r.  singhal , and p.  dubey . debunking the 100x gpu vs. cpu",
    "myth : an evaluation of throughput computing on cpu and gpu .",
    ", 38(3):451460 , june 2010 .",
    "phillips , y.  zhang , r.l .",
    "davis , and j.d .",
    "rapid aerodynamic performance prediction on a cluster of graphics processing units . in _",
    "47th aiaa aerospace sciences meeting _",
    ", pages paper no : aiaa 2009565 , 2009 .",
    "everett  h. phillips , roger  l. davis , and john  d. owens .",
    "unsteady turbulent simulations on a cluster of graphics processors . in _ proceedings of the 40th aiaa fluid dynamics conference _ , number aiaa 2010 - 5036 , june 2010 ."
  ],
  "abstract_text": [
    "<S> we implemented the pressure - implicit with splitting of operators ( piso ) and semi - implicit method for pressure - linked equations ( simple ) solvers of the navier - stokes equations on fermi - class graphics processing units ( gpus ) using the cuda technology . </S>",
    "<S> we also introduced a new format of sparse matrices optimized for performing elementary cfd operations , like gradient or divergence discretization , on gpus . </S>",
    "<S> we verified the validity of the implementation on several standard , steady and unsteady problems . </S>",
    "<S> computational efficiency of the gpu implementation was examined by comparing its double precision run times with those of essentially the same algorithms implemented in openfoam . </S>",
    "<S> the results show that a gpu ( tesla c2070 ) can outperform a server - class 6-core , 12-thread cpu ( intel xeon x5670 ) by a factor of 4.2 .    </S>",
    "<S> cfd , gpu , piso , simple </S>"
  ]
}