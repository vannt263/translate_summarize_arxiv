{
  "article_text": [
    "_ undirected graphical models _ , also known as _ markov random fields _ , are important tools for summarizing dependency relationships between random variables and have found application in many fields , including bioinformatics , language and speech processing , and digital communications .",
    "each such model is associated to an undirected graph @xmath1 , with vertex set @xmath2 and edge set @xmath3 . for a random vector @xmath4 indexed by the nodes of @xmath5 ,",
    "the graphical model given by @xmath5 requires that @xmath6 and @xmath7 be conditionally independent given all other variables whenever nodes @xmath8 and @xmath9 are not joined by an edge in @xmath5 @xcite . if @xmath5 is the smallest graph such that @xmath10 satisfies this requirement , we term @xmath5 the _ conditional independence graph _ of @xmath10 . in this case ,",
    "@xmath6 and @xmath7 are conditionally independent given all other variables if and only if @xmath8 and @xmath9 are non - adjacent in @xmath5 .",
    "we will always take the vertex set to be @xmath11 , so @xmath12 is the number of observed variables in @xmath10 .",
    "specific models are obtained from additional distributional assumptions .",
    "particularly , an assumption of multivariate normality gives gaussian graphical models , for which estimation of conditional independence graphs is equivalent to _ covariance selection _ @xcite .",
    "if @xmath10 is jointly multivariate normal with mean vector @xmath13 and covariance matrix @xmath14in symbols , @xmath15then the conditional independences among the random variables , and hence edges between nodes in the graph , are determined by the entries of the inverse covariance , or concentration matrix @xmath16 .",
    "more precisely , @xmath17 for @xmath18 if and only if @xmath6 and @xmath7 are independent given all other variables .",
    "there is a large literature on selection of conditional independence graphs ; see the references in ( * ? ? ? * chap .",
    "6 ) or @xcite . in the last decade",
    ", attention has shifted to high - dimensional settings with the number of variables @xmath19 comparable to or larger than the sample size @xmath20 .",
    "this scenario arises , for instance , in microarray experiments .",
    "fortunately , high - dimensional problems may remain tractable in the presence of structural constraints such as _ sparsity _ , i.e. , if each node in the graph is incident to a small number of edges .",
    "this is of interest for microarray data as gene regulatory networks are intrinsically sparse @xcite .",
    "gaussian models have been the primary tool for graphical modeling of data comprising continuous variables , such as gene expression data , and a large number of methods have been proposed for statistical estimation in high - dimensional gaussian graphical models .",
    "a common strategy involves augmenting a loss function with a sparsity - inducing penalty such as an @xmath0 , or lasso penalty .",
    "two widely - used approaches are the _ graphical lasso _ or _ glasso _ @xcite and _ neighborhood selection _ @xcite . in glasso , an @xmath0 penalty on the entries of the inverse covariance matrix",
    "is added to the negative gaussian log - likelihood .",
    "neighborhood selection , on the other hand , is an @xmath0-penalized pseudo - likelihood approach that leverages the fact that the node - wise full conditional distributions from a gaussian graphical model form @xmath19 linear regression models .",
    "@xcite treat these separate regression models as having their parameters unrelated , but as we discuss below , methods that account for the symmetry in a concentration matrix have been proposed in subsequent work .",
    "methods for high - dimensional data have also been developed for non - gaussian settings .",
    "@xcite , @xcite , @xcite and @xcite address robustness to outliers .",
    "@xcite , @xcite and @xcite treat gaussian copula models .",
    "neighborhood selection / pseudo - likelihood procedures can also be applied to models for categorical models where the node - wise regression is logistic or multinomial @xcite . @xcite and",
    "@xcite discuss extensions using node - wise generalized linear models , and semi-/nonparametric methods were proposed by @xcite and @xcite .    in this paper , we propose a different approach to high - dimensional graphical model selection . addressing the case of _ continuous _ but not necessarily gaussian observations , the proposed method is based on the _ score matching _ loss , first introduced by @xcite in the setting of image analysis . recently ,",
    "@xcite studied score matching in gaussian graphical models with symmetry constraints , and demonstrated that , when the number of variables @xmath19 is fixed , the estimators derived from the score matching loss are asymptotically efficient in some special cases , but not in general .",
    "our focus is instead on the use of score matching in high - dimensional problems , for which we consider regularization with an @xmath0 penalty .",
    "we will refer to this graphical model selection technique as _",
    "regularized score matching_. regularized score matching is computationally very convenient for any exponential family comprising continuous distributions .",
    "indeed , the score matching loss is a positive semi - definite quadratic function .",
    "it follows that the solution path for the regularized score matching problem is piecewise linear and can be computed in entirety .",
    "moreover , theoretical analysis can be based on familiar techniques .",
    "most importantly , as we demonstrate for gaussian graphical models , regularized score matching exhibits state - of - the - art statistical efficiency in high - dimensional settings .",
    "the method also performs well in our applications to non - gaussian models , which include models that seem rather difficult to handle via other methods .    in the gaussian",
    "setting , regularized score matching is structurally closest to pseudo - likelihood methods with symmetry constraints , such as _ space _ @xcite , _ symmetric lasso _ @xcite and",
    "_ splice _ @xcite . a thorough discussion of these different methods is given by @xcite who also reformulate the space objective function to ensure convergence of coordinate descent algorithms .",
    "they abbreviate their method as _ concord_. for brevity , we refer to these algorithms collectively as space .",
    "we note that in contrast to regularized score matching , the space methods do not have piecewise linear solution paths . furthermore ,",
    "as remarked before , the computational convenience of regularized score matching carries over to non - gaussian settings .",
    "a limitation of the original score matching introduced by @xcite is that it requires the data to be generated from a distribution whose density is twice differentiable on @xmath21 .",
    "@xcite proposed a generalization of the approach to the important case of non - negative data . for exponential families ,",
    "the non - negative score matching loss is again a semidefinite quadratic function .",
    "we explore regularization of the non - negative score matching loss as a tool for estimation of conditional independence graphs from high - dimensional non - negative data , and we establish consistency of the method .    the remainder of the paper is organized as follows .",
    "section  [ background ] provides the needed background on score matching and its applications . in section  [ sec : rsme ] , we describe the proposed method , _ regularized _ score matching .",
    "implementation details are given in appendix a. in section  [ simulations ] , we present results of numerical experiments to compare the performance of the procedure with existing approaches .",
    "an application to rnaseq data is given in section  [ real ] .",
    "section  [ theory ] provides sparsistency theory for both basic and non - negative regularized score matching .",
    "proofs are given in section  [ sec : proofs ] with details deferred to appendix b and c. we end with a discussion in section  [ discussion ] .",
    "the following notational conventions are used throughout the paper :    1 .",
    "random variables / vectors are denoted by upper case letters ; lower case letters are used for observed values .",
    "so , @xmath22 is an observed value of the random vector @xmath10 .",
    "similarly , @xmath23 is a matrix of observed values , which will typically hold the realizations of @xmath20 i.i.d .",
    "copies of @xmath10 in its rows .",
    "we index the columns of a matrix with subscripts , so @xmath24 refers to the @xmath8th column of @xmath25 .",
    "superscripts in parentheses are used to refer to the rows of a matrix , so @xmath26 is the @xmath27th row of @xmath25 .",
    "2 .   for a matrix @xmath28",
    ", we denote the vectorization obtained by stacking columns by @xmath29 3 .",
    "let @xmath30 $ ] .",
    "we denote the @xmath31 norm of a vector @xmath32 by @xmath33 and write @xmath34 for the @xmath35 operator norm of a matrix @xmath36 .",
    "we let @xmath37 and @xmath38 .",
    "we begin with an overview of hyvrinen s score matching , discussing first random vectors supported on all of @xmath21 and then random vectors supported on the nonnegative orthant .",
    "we also review the convenient form of the score matching estimating equations in exponential families .",
    "suppose @xmath10 is a continuous random vector taking values in @xmath21 , with joint distribution @xmath39 .",
    "suppose further that @xmath39 belongs to the family @xmath40 that comprises all probability distributions with support equal to @xmath21 and a twice differentiable density with respect to lebesgue measure .",
    "we emphasize that in a statistical context the differentiability requirement is with respect to data .",
    "we write @xmath41 to denote the density of @xmath39 and adopt the usual notation for the gradient and laplacian @xmath42 of a function @xmath43 .    for a distribution @xmath44 with density @xmath45 , define the divergence function @xmath46 \\,dx\\ ] ] as the expected squared distance between the gradients of the log - densities of the two distributions @xmath47 and @xmath39 .",
    "by choosing @xmath47 to minimize ( [ divergence ] ) , we are matching ` scores ' with respect to the data vector @xmath48 .",
    "hence , ( [ divergence ] ) has been referred to as the _ score matching loss_. it is evident from ( [ divergence ] ) that the score matching loss is uniquely minimized when @xmath49",
    ".    upon initial inspection , optimization of @xmath50 seems to require knowledge of @xmath39 in an important way .",
    "however , @xcite showed that , under mild regularity conditions , the score matching loss ( [ divergence ] ) can be rewritten as : @xmath51dx \\ ; + \\ ; \\mbox{const},\\ ] ] where ` const ' refers to a term independent of @xmath47 .",
    "the key term in the integrand in  ( [ scorematch ] ) is the so - called hyvrinen scoring rule @xmath52 the integral in  ( [ scorematch ] ) admits an empirical version in which the integration with respect to @xmath39 is replaced by an average over an observed sample , which we arrange into a data matrix @xmath53 .",
    "this leads to the _ empirical score matching loss _ @xmath54 and the _ score matching estimator _ ( sme ) @xmath55    the score matching loss @xmath50 was motivated by problems involving models whose distributions have an intractable normalization constant . indeed , evaluating ( [ scorematch ] ) and computing the sme @xmath56 requires no knowledge of the normalization constant , which is eliminated upon taking logarithmic derivatives with respect to @xmath48 . besides the imaging problems considered by @xcite ,",
    "score matching has been applied to spatial statistics @xcite and neural networks @xcite .",
    "the statistical properties of smes in classical large sample settings have been investigated by @xcite and @xcite .",
    "in particular , it has been shown that , under the usual regularity conditions , smes are asymptotically consistent and normal in large - sample theory .",
    "however , smes are not necessarily asymptotically efficient .",
    "the partial integration arguments underlying ( [ scorematch ] ) may fail to apply when considering distributions @xmath47 that are not supported on all of @xmath21 .",
    "in particular , when @xmath47 is taken to be from @xmath57 , i.e. the family of distributions that are supported on @xmath58 with lebesgue densities that are twice differentiable on @xmath59 , then partial integration may not be possible due to discontinuities at points with zero coordinates .",
    "we thus consider the non - negative score matching loss , @xmath60dx,\\ ] ] as proposed in @xcite . here",
    ", ` @xmath61 ' stands for the hadamard product , that is , element - wise multiplication .    the score matching loss ( [ divergence ] ) can be thought of as a function of the euclidean distance between the gradients of the model density @xmath45 and true density @xmath41 with respect to a hypothetical location parameter @xmath13 , evaluated at @xmath62 .",
    "that is , we may write ( [ divergence ] ) as @xmath63dx.\\ ] ] likewise , the non - negative score matching loss compares the gradient of the model density @xmath45 and true density @xmath41 with respect to a hypothetical scale parameter @xmath64 evaluated at @xmath65 , @xmath66dx.\\ ] ] under suitably adjusted regularity conditions , @xcite showed that the non - negative score matching loss from ( [ nndivergence ] ) can be simplified into @xmath67 with scoring rule @xmath68}.\\ ] ] for a data matrix @xmath53 , one obtains the _ empirical non - negative score matching loss _ @xmath69 and the _ non - negative score matching estimator _ ( @xmath70 ) @xmath71 again , under the usual regularity conditions , the estimator @xmath72 is asymptotically consistent and normal in traditional large - sample theory .",
    "@xcite and @xcite have shown that the sme has a convenient closed form as a rational function of the data when @xmath40 is an exponential family .",
    "@xcite showed the same for @xmath70 for the example of truncated normal distributions . as they provide the basis for our later work , we revisit these results for both sme and @xmath70 .",
    "let @xmath73 be an exponential family with natural parameter space @xmath74 .",
    "suppose that the distributions @xmath75 have their common support equal to either @xmath76 or @xmath77 , and that @xmath40 is dominated by lebesgue measure on @xmath21 .",
    "assuming that the sufficient statistics @xmath78 take values in @xmath79 , the log - densities of the distributions @xmath75 have the form @xmath80 and @xmath81    [ quadraticlemma ] let @xmath53 be a data matrix , and suppose @xmath73 is an exponential family characterized by ( [ exponentialfamily ] ) and ( [ exponentialpar ] ) .",
    "if @xmath40 has support @xmath76 , then the empirical score matching loss @xmath82 is a quadratic function in @xmath83 with @xmath84 where @xmath85 is a positive semidefinite @xmath86 matrix , and @xmath87 is an @xmath88-vector . the same is true for @xmath89 when @xmath40 has support @xmath90 .    for @xmath91 and @xmath92 , define the @xmath88-vectors @xmath93 it then follows from  ( [ exponentialfamily ] ) that @xmath82 can be expressed in the claimed form with @xmath94 for non - negative score matching , @xmath89 admits the claimed form with @xmath95 where the @xmath96 are the entries of the @xmath97 data matrix @xmath25 .",
    "lemma [ quadraticlemma ] implies that , when working with exponential families , both score matching objectives are quadratic functions of the unknown parameter vector @xmath83 .",
    "a score matching estimator @xmath98 thus satisfies a set of _ linear _ estimating equations @xmath99      the most basic class of exponential families that appear in graphical modeling are pairwise interaction models with log - densities @xmath100 here , the @xmath101 are sufficient statistics that depend only on the @xmath8th and @xmath9th coordinate of @xmath48 , and the @xmath102 are interaction parameters .",
    "if @xmath75 denotes the distribution with density given by  ( [ expfam : pairwise ] ) , then the hammersley - clifford theorem implies that an edge between nodes @xmath8 and @xmath9 exists in the conditional independence graph of @xmath75 if and only if @xmath102 is nonzero .",
    "the specific models we consider later either exactly have the form in  ( [ expfam : pairwise ] ) or are closely related extensions with log - densities @xmath103 where pairwise interactions may be of @xmath104 different types and we also include @xmath105 sets of sufficient statistics @xmath106 depending on the individual coordinates .",
    "the latter appear , for instance , when allowing distributions to vary in location .",
    "the distribution @xmath75 defined by  ( [ expfam : pairwise : general ] ) has no edge between @xmath8 and @xmath9 in its conditional independence graph if and only if @xmath107 .    in our study of score matching methods for models of the type  ( [ expfam : pairwise ] ) or  ( [ expfam : pairwise : general ] )",
    ", it will be convenient to introduce the symmetric @xmath108 interaction matrix @xmath109 with entries @xmath110    [ blockform ] let @xmath40 to be the pairwise interaction model given by  ( [ expfam : pairwise ] ) with symmetric @xmath111 interaction matrix @xmath109 .",
    "if @xmath40 has support @xmath76 , then the empirical score matching loss @xmath82 equals @xmath112 for a symmetric @xmath113 matrix @xmath85 that is block - diagonal , with all blocks of size @xmath111 .",
    "the same is true for @xmath89 when @xmath40 has support @xmath90 .    by  ( [ gammaform ] ) and  ( [ gammaform+ ] ) , it suffices to show that there exists a block - diagonal matrix @xmath114 such that @xmath115 where @xmath116 .",
    "now , @xmath117 define a vector @xmath118 , indexed by pairs @xmath119 with @xmath120 , by setting the entries to @xmath121 then @xmath122 and ( [ eq : going - to - vec ] ) holds with @xmath123 , which is block - diagonal as it is zero with the exception of the @xmath111 block indexed by pairs @xmath119 with @xmath124 .",
    "[ rem : blockform - gen ] when @xmath40 is a model as specified in  ( [ expfam : pairwise : general ] ) , then the empirical ( non - negative ) score matching loss may still be represented as an explicit quadratic form with a block - diagonal symmetric matrix @xmath85 as in  ( [ quadratic - block ] ) .",
    "however , @xmath85 is then of size @xmath125 , and its @xmath12 diagonal blocks are of size @xmath126 .",
    "the @xmath8th block has its rows and columns corresponding to the @xmath8th columns of each of @xmath127 as well @xmath128 .",
    "[ example1 ] if the exponential family is taken to be the family of centered multivariate normal distributions with precision matrix @xmath129 , then the support is @xmath130 and @xmath131 with @xmath132 and dropping a term that is constant in @xmath133 , the empirical score matching loss from ( [ scorematch ] ) takes the form @xmath134 where @xmath135 is the empirical covariance matrix ( under knowledge of zero mean ) .",
    "lemma  [ blockform ] applies with @xmath136 , in which case the matrix @xmath114 constructed in the proof of the lemma does not depend on @xmath8 , other than through the location of the nonzero block .",
    "indeed ,  ( [ quadratic - block ] ) holds with @xmath137 and @xmath138 , where @xmath139 is the @xmath108 identity matrix .",
    "clearly , @xmath85 is positive definite if and only if @xmath140 is as well .",
    "if @xmath140 is invertible then sme of @xmath133 is @xmath141 and coincides with the maximum likelihood estimator .",
    "[ example2 ] consider truncated normal densities of the form @xmath142 using @xmath143 to denote the @xmath8th column of @xmath133 , it can be shown that the empirical non - negative score matching objective is @xmath144 the loss can be written as in  ( [ quadratic ] ) with @xmath85 a block diagonal @xmath145 matrix , whose @xmath8th block is given by @xmath146 moreover , @xmath147 , where @xmath148 and @xmath149 .",
    "the maximum likelihood estimator for @xmath133 has no closed form due to intractable normalizing constants .",
    "[ example2.5 ] finally , consider the family of distributions with densities of the form @xmath150 here , @xmath151 is an @xmath19-vector , and @xmath152 and @xmath153 are symmetric @xmath154 interaction matrices , the latter having a zero diagonal .",
    "this family is a class of distributions with normal conditionals , with densities that need not be unimodal @xcite .",
    "this family is intriguing from the perspective of graphical modeling as , in contrast to the gaussian case , conditional dependence may also express itself in the variances .",
    "for conditional independence of @xmath6 and @xmath7 both @xmath155 and @xmath156 need to vanish .    by remark",
    "[ rem : blockform - gen ] , the empirical score matching loss for the family from  ( [ condnorm ] ) can be written as a quadratic function with the quadratic term given by block - diagonal matrix @xmath85 of size @xmath157 .",
    "the blocks are of size @xmath158 , and the @xmath8th block has its rows and columns corresponding to the @xmath8th columns of @xmath159 and @xmath160 and the @xmath8th entry in @xmath161 .",
    "in this section , we propose the use of _ regularized score matching _ for graphical model selection in the setting of high - dimensional sparse graphical models .",
    "we begin by discussing the proposed method and its implementation .",
    "later sections show that , despite the fact that smes need not be asymptotically efficient in the sense of traditional large - sample theory , regularized score matching achieves state - of - the - art statistical performance in high - dimensional problems , all the while allowing seemingly complicated non - gaussian graphical models to be treated in a computationally efficient manner .      building on the ideas underlying methods such as glasso , neighborhood selection and space , we augment the score matching loss with a sparsity - promoting penalty .",
    "our focus is on the most basic case of an @xmath0 penalty but other regularization schemes could be considered instead ; see also example  [ example2.5 ] below",
    ".    using the generic representation given in lemma  [ quadraticlemma ] , for an exponential family , the proposed method is based on minimizing the objective @xmath162 where @xmath85 is positive semidefinite and @xmath163 is a tuning parameter that controls the sparsity level .",
    "larger values of @xmath164 yield sparser solutions , and @xmath165 gives the unregularized sme . since @xmath85 is positive semidefinite , the function @xmath166 is convex but in the settings of interest here @xmath85 will be singular and @xmath166 will not be strictly convex .",
    "the regularized score matching objective from  ( [ regquadratic ] ) is similar to the lasso objective in linear regression @xcite , where the function to be minimized takes the special form @xmath167 for a ` response vector ' @xmath168 and a ` design matrix ' @xmath10 . in the applications we have in mind  ( [ regquadratic ] )",
    "can not be written exactly as in ( [ lassoreg ] ) because the vector @xmath169 is generally not in the column span of @xmath85 .",
    "however , we may adapt existing optimization methods for lasso to solve the regularized score matching problem .",
    "implementation details are given in appendix [ implementation ] .",
    "if the considered exponential family is supported on @xmath76 and we use the loss from  ( [ eq : emp - sm - loss ] ) , then we call the minimizer of ( [ regquadratic ] ) the regularized score matching estimator ( rsme ) .",
    "if @xmath90 and we use the loss from  ( [ eq : emp - sm - loss+ ] ) , then we abbreviate to @xmath170 . in specific instances of graphical models , we may apply the @xmath0 penalty only to those coordinates of @xmath83 whose vanishing corresponds to absence of edges in a conditional independence graph .",
    "if the subset @xmath171 holds the relevant coordinates then we use the penalty @xmath172 , the target of estimation is the symmetric precision matrix @xmath133 .",
    "the conditional independence graph corresponds to the pattern of zeros in the off - diagonal entries of @xmath133 and the rsme is @xmath173 where @xmath174 is the empirical covariance matrix and @xmath175 penalizes only the off - diagonal entries indexed by @xmath176 .",
    "we emphasize that while in this example the natural parameter space is the positive definite cone , we propose minimizing simply over the entire space of symmetric @xmath108 matrices , denoted by @xmath177 . as our interest",
    "is primarily in graph selection , we do not enforce positive definiteness of @xmath178 , which is in line with methods such as space or neighborhood selection ; compare @xcite .",
    "we remark that evaluating the function from  ( [ eq : regscore ] ) at a nonsymmetric matrix @xmath133 as well as its transpose @xmath179 gives the same value . by convexity , minimizing over all @xmath154 matrices gives a solution in @xmath177 , which then must equal @xmath178 .",
    "example2 in the truncated normal family from example  [ example2 ] , the conditional independence graph corresponds again to the zero pattern in the off - diagonal entries of the positive definite interaction matrix @xmath133 .",
    "proceeding in analogy to the gaussian case , we define the @xmath180 as the minimizer @xmath181 of the objective given by ( [ nngaussiansme ] ) with the penalty @xmath182 added on .",
    "again , we ignore the positive definiteness requirement and minimize the penalized non - negative score matching loss with respect to @xmath183 .",
    "example2.5 for the family of distributions with normal conditionals from example [ example2.5 ] , we would like a penalty to induce joint sparsity in the two symmetric interaction matrices @xmath159 and @xmath160 , because an edge between nodes @xmath8 and @xmath9 is absent from the conditional independence graph if and only both @xmath159 and @xmath160 have their @xmath184 entries zero . for this purpose",
    ", it is natural to adopt the group lasso penalty @xcite .",
    "the rsme is then obtained by minimizing the empirical score matching loss augmented by the penalty @xmath185 ignoring again any refined constraints from the natural parameter space of the family , we propose minimizing the penalized loss with respect to @xmath186 and @xmath187 . since the group lasso is applied with small groups ( of size 2 ) , the problem would be suitable for application of exact block - coordinate descent as discussed in @xcite .      in the setup from lemma  [ quadraticlemma ] ,",
    "we may write @xmath188 for an @xmath189 matrix @xmath190 ; recall  ( [ gammaform ] ) and  ( [ gammaform+ ] ) . based on the arguments leading to lemmas 3 and 5 in @xcite , the function @xmath166 from  ( [ regquadratic ] )",
    "has a unique minimizer @xmath98 as long as @xmath191 and the columns of @xmath190 are in _",
    "general position_. to clarify , suppose that @xmath192 is a collection of @xmath193 vectors .",
    "then @xmath194 is in general position if for all @xmath195 , all choices of vectors @xmath196 and signs @xmath197 , the affine span of @xmath198 does not contain any vector @xmath199 or @xmath200 for @xmath201 .",
    "the graphical models we are interested in are pairwise interaction models that have additional special structure in that the matrix @xmath85 is block - diagonal with @xmath12 blocks of equal size ; recall lemma  [ blockform ] and remark  [ rem : blockform - gen ] . denote the diagonal blocks by @xmath202 , which in the setup from  ( [ expfam : pairwise : general ] ) are of size @xmath203 .",
    "each block is the sum of @xmath20 symmetric rank one matrices and we have the decomposition @xmath204 the @xmath20 columns of each of the matrices @xmath205 were specified in  ( [ eq : barh ] ) .",
    "it now holds that the regularized score matching problem from  ( [ regquadratic ] ) has a unique minimizer provided each one of the @xmath206 blocks @xmath207 defined in  ( [ eq : hj ] ) has its columns in general position .",
    "example1 in the gaussian case , @xmath208 . by the lemma in @xcite , the set of matrices @xmath25 that fail to be in general position has measure zero .",
    "the rsme @xmath178 is unique almost surely when data are generated from a continuous joint distribution .",
    "example2 in the truncated normal case , @xmath205 is equal to the matrix obtained from @xmath25 by multiplying each column element - wise with @xmath24 , the @xmath8th column of @xmath25 .",
    "the lemma in @xcite implies that the rsme@xmath209 is unique almost surely .    for the normal conditionals model from example  [ example2.5 ] ,",
    "almost sure uniqueness would have to be derived by appealing to results on uniqueness of group lasso @xcite .",
    "the rsme depends on the regularization parameter @xmath164 . in this section",
    "we make this explicit and denote it by @xmath210 . adopting standard language , we refer to the set of @xmath210 obtained by varying @xmath164 as the _ solution path _ and call this path _ piecewise linear _ if there exists @xmath211 and @xmath212 such that @xmath213 for @xmath214 $ ] .",
    "piecewise linear solution paths have the appeal that the entire solution path can be found by calculating the change points @xmath215 and associated slopes @xmath216 .",
    "the next lemma is a consequence of the quadratic nature of the score matching objective for exponential families , and holds for the lasso problem as well .",
    "the solution path @xmath210 for the regularized score matching problem from  ( [ regquadratic ] ) is piecewise linear .",
    "an @xmath88-vector @xmath217 belongs to @xmath218 , the subdifferential of the @xmath0 norm , if @xmath219 & \\mbox{if $ \\theta_j = 0$}.    \\end{cases}\\ ] ] the karush - kuhn - tucker ( kkt ) conditions characterizing optimality in ( [ regquadratic ] ) are @xmath220 the linear relationship between @xmath98 and @xmath164 ( for `` fixed '' @xmath221 ) implies the claim .    while straightforward to show , the property of piecewise linear paths is special to the score matching method we propose .",
    "other methods that give symmetric estimates of precision matrices in gaussian graphical models , such as glasso or the space - type methods discussed in @xcite do not have piecewise linear solution paths .",
    "this said , piecewise linear paths also arise in neighborhood selection @xcite , which , however , is a formulation without symmetry .",
    "note also that when using a group lasso penalty as suggested for example  [ example2.5 ] , rsme solution paths are no longer piecewise linear .    0.22 0.2 in    0.6 0.2 in -0.1 in    example1 in the gaussian model , the kkt conditions state that @xmath178 is a solution to ( [ regquadratic ] ) if and only if @xmath222 for @xmath223 , which in slight abuse of notation , we take to mean that @xmath224        & \\mbox{if } \\",
    "{ \\hat{\\kappa}}_{jk } = 0 \\ \\mbox{and } \\",
    "j\\not=          k.    \\end{cases}\\ ] ] the first case accounts for the fact that the objective is smooth in the diagonal entries of the precision matrix , which are not penalized . combining ( [ gaussiankkt ] ) and ( [ subdiffoff ] ) , we have that @xmath225 a gaussian solution path is shown in figure  [ linear ] , with the horizontal axis transformed to @xmath226 .",
    "the data were drawn from a multivariate normal distribution with the conditional independence graph from figure  [ graph ] , with sample size @xmath227 .",
    "we note that , as one would hope , the coefficient that last enters the solution corresponds to the absent edge @xmath228 .",
    "a number of methods have been proposed for selecting the regularization parameter @xmath164 in @xmath0 penalization methods and can be applied in our context . on the one hand ,",
    "a predictive assessment as in cross - validation can be considered , but the selected graphs are typically too dense .",
    "other possibilities include generalized cross validation ( gcv ) @xcite , akaike s information criterion ( aic ) , approaches based on stability under resampling @xcite , the bayesian information criterion ( bic ) @xcite as well as extensions of bic proposed to cope with large model spaces @xcite .",
    "the latter come with some consistency guarantees . as a demonstration ,",
    "for the gaussian case from example [ example1 ] , we may consider an extended bic criterion based on the basic score matching loss ( [ scorematch ] ) , defined as @xmath229 where @xmath230 and @xmath231 is typically taken to be @xmath232 or @xmath65 .",
    "alternatively , we could refit , that is , replace @xmath233 by an unregularized sme computed in the submodel given by constraining all @xmath234 with @xmath235 to be zero . in either case , we choose @xmath164 to minimize ( [ ebic ] ) .",
    "we perform numerical experiments comparing regularized score matching to existing methods when data is simulated from ( i ) a multivariate normal distribution , ( ii ) a multivariate truncated normal distribution , and ( iii ) a distribution with normal conditionals .",
    "the comparison is made against three methods for estimation of gaussian graphical models , namely , glasso , neighborhood selection ( both implemented in the ` r ` packages ` huge ` ) and space ( in its concord formulation , with ` r ` package ` gconcord ` ) .",
    "in addition , we consider the _ nonparanormal skeptic _ , which applies glasso to a matrix of rank correlations ( kendall s @xmath236 or spearman s @xmath237 ) and can be motivated by a gaussian copula model @xcite .",
    "we utilize the version based on kendall s @xmath236 .",
    "finally , we compare to _ spacejam _",
    "@xcite , which is based on additive modeling of conditional means and implemented in the ` r ` package ` spacejam ` .",
    "we conclude this section with brief investigations on the robustness of regularized score matching when data is not generated under the assumed model .",
    "all results in this section are based on averaging over 100 independently generated datasets .",
    "we consider a graph with @xmath238 nodes , composed of @xmath239 connected components , each @xmath240 nodes in size and structured as a @xmath241 2-d lattice ( 4 nearest neighbors ) .",
    "each connected component also features three hubs with node degree 20 , randomly selected from the subset of nodes in the component .",
    "we follow a procedure similar to the one from @xcite to convert the adjacency matrix of the graph into a sparse diagonally dominant partial correlation matrix . for each non - zero element of the adjacency matrix , we sample a draw from a uniform distribution on @xmath242 $ ] . each row of this new matrix",
    "is then rescaled by 1.5 times the sum of the absolute values of the off - diagonal entries in the row .",
    "we average this matrix with its transpose to ensure symmetry , and set its diagonal elements to 1 .",
    "this matrix is inverted and converted into a correlation matrix to form @xmath243 .",
    "data is then generated from a multivariate normal distribution with mean zero and a covariance matrix @xmath243 .",
    "we choose sample size @xmath244 and @xmath245 .",
    "the setup agrees with that in @xcite , except that the number of nodes has been scaled up .",
    "figure [ gauss ] shows the roc curves obtained under both sample sizes . since the truth is gaussian , we do not report results for skeptic or spacejam . for both sample sizes , the curve for regularized score matching almost perfectly aligns with those for neighborhood selection , space , and glasso .",
    "the results indicate that regularized score matching estimators achieves state - of - the - art statistical efficiency in gaussian models .",
    "0.45 [ gauss1 ] ) , neighborhood selection ( [ mbcolour ] ) , glasso ( [ glassocolour ] ) , and space ( [ spacecolour ] ) . the curves are almost perfectly aligned.,title=\"fig : \" ]    0.45 [ gauss2 ] ) , neighborhood selection ( [ mbcolour ] ) , glasso ( [ glassocolour ] ) , and space ( [ spacecolour ] ) . the curves are almost perfectly aligned.,title=\"fig : \" ]      glasso , space , neighborhood selection and skeptic all presume some form of underlying gaussianity . in this and the next subsection",
    ", we demonstrate the application of regularized score matching in scenarios where these assumptions do not hold to highlight the versatility of the proposed appraoch .",
    "similar to the gaussian setting , we consider a graph with @xmath246 nodes , composed of 10 disconnected subgraphs with equal number of nodes .",
    "using the lower triangular elements adjacency matrix of each @xmath239 node subgraph , we construct ten matrices , where in each matrix , the element is drawn independently to be 0 with probability 0.2 , and from a uniform distribution on @xmath242 $ ] with probability 0.8 .",
    "the matrices , after symmetrization , are combined into a @xmath247 block matrix .",
    "the diagonal elements are set to a common positive number such that the minimum eigenvalue is 0.1 to form the precision matrix of the pre - truncated normal , @xmath248 .",
    "data was then generated from a truncated centered multivariate normal , left - truncated at @xmath62 and with @xmath249 as normal covariance .",
    "we used the gibbs sampler from the ` tmvtnorm ` package in ` r ` with a burnin period of 100 samples .",
    "we thinned out the remaining samples , keeping one in ten .",
    "the sample size @xmath20 is taken to be either @xmath250 or @xmath251 .",
    "the need for a larger sample size is explained by our theoretical findings in section [ theory ] , specifically corollary [ th2 ] .",
    "the roc curves are shown in figure  [ nngauss ] , where regularized score matching outperforms all competitors considered . the closest competitor to regularized",
    "score matching are skeptic and spacejam , both of which , objectively , perform well , being capable of capturing some of the non - gaussianity in the data .",
    "0.45 [ gauss1a ] ) , glasso ( [ glassocolour ] ) , space ( [ spacecolour ] ) , skeptic ( [ skepticcolour ] ) , and spacejam ( [ spacejamcolour]).,title=\"fig : \" ]    0.45 [ gauss2a ] ) , glasso ( [ glassocolour ] ) , space ( [ spacecolour ] ) , skeptic ( [ skepticcolour ] ) , and spacejam ( [ spacejamcolour]).,title=\"fig : \" ]    we emphasize that here score matching was applied in its non - negative version from section  [ sec : extens - non - negat ] . the basic score matching procedure from section  [ sec : basic - score - matching ] is far less efficient based on experiments not reported here .",
    "next , we take the data - generating distribution to have a density from the class @xmath252 where @xmath253 is a symmetric matrix with diagonal entries @xmath62 .",
    "this family is a special case of the distributions with normal conditionals from example  [ example2.5 ] .",
    "we consider the case @xmath254 , with the graph being a @xmath255 2-d lattice ( 4 nearest neighbors ) .",
    "the true interaction matrix @xmath256 is constructed by multiplying the adjacency matrix by @xmath257 .",
    "the coefficients for the terms @xmath258 are all set equal to @xmath259 and those for the @xmath24 all equal to @xmath260 , which makes the marginal distributions deviate noticeably from gaussianity .",
    "data can be generated by gibbs sampling using the gaussian full conditionals .",
    "we discard the first 100 samples and thin out the remaining samples , keeping one in ten , as in section  [ sec : non - negat - gauss ] .",
    "0.45 [ conditional1 ] ) , glasso ( [ glassocolour ] ) , space ( [ spacecolour ] ) , skeptic ( [ skepticcolour ] ) , and spacejam ( [ spacejamcolour ] ) . the curve for glasso overlaps with the curve for space.,title=\"fig : \" ]    0.45 [ conditional2 ] ) , glasso ( [ glassocolour ] ) , space ( [ spacecolour ] ) , skeptic ( [ skepticcolour ] ) , and spacejam ( [ spacejamcolour ] ) . the curve for glasso overlaps with the curve for space.,title=\"fig : \" ]    we plot the roc curves for conditional normal data in figure [ conditional ] . regularized score matching outperforms its competitors by a clear margin .",
    "this is not surprising , as both glasso and space are derived under normality .",
    "a gaussian copula model as underlying skeptic is of little help .",
    "spacejam does best among the competitors but can not fully extract the available signal about the edge structure as the conditional means are non - additive and the conditional variances are not constant .",
    "it is of interest to see how score matching performs when the data - generating mechanism is misspecified .",
    "we consider two scenarios .",
    "first , we apply the gaussian score matching to a contaminated gaussian setting similar to that explored in @xcite . that is , a random subset of gaussian observations is replaced with gaussian noise . in the second example",
    ", we investigate the performance of the regularized gaussian score matching when the observations are not gaussian but rather drawn from a multivariate @xmath261-distribution .",
    "we mimic the setup used in the numerical experiments in @xcite , who consider these settings to test the robustness of their _",
    "tlasso_. fixing @xmath262 , we construct a sparse precision matrix @xmath263 according to the following steps : ( 1 ) choose each ( strictly ) lower triangular element of @xmath263 to be independently -1 , 0 , 1 with probability 0.01 , 0.98 and 0.01 respectively , ( 2 ) symmetrize the matrix ( 3 ) for each row , i.e. for @xmath264 , set @xmath265 where @xmath266 refers to the @xmath8th row of @xmath263 with the diagonal element in that row removed . to strengthen partial correlations ,",
    "the diagonal elements are scaled down by a common positive factor such that the minimum eigenvalue of the resulting matrix is approximately 0.6 ( close to 0.62 in our setup ) .",
    "the covariance matrix @xmath267 is obtained by inverting @xmath263 .",
    "we generate either @xmath268 or @xmath269 observations from a multivariate normal distribution with mean zero and a covariance matrix @xmath243 .",
    "we then corrupt 2% of the observations , substituting them with i.i.d .",
    "@xmath270 draws .",
    "the corrupted observations can not easily be differentiated from normal observations , and this elevates the difficulty of the estimation problem .",
    "0.45 [ contaminate1 ] ) , neighborhood selection ( [ mbcolour ] ) , glasso ( [ glassocolour ] ) , space ( [ spacecolour ] ) , skeptic ( [ skepticcolour ] ) , and spacejam ( [ spacejamcolour]).,title=\"fig : \" ]    0.45 [ contaminate2 ] ) , neighborhood selection ( [ mbcolour ] ) , glasso ( [ glassocolour ] ) , space ( [ spacecolour ] ) , skeptic ( [ skepticcolour ] ) , and spacejam ( [ spacejamcolour]).,title=\"fig : \" ]    we present the roc curves in figure [ contaminated ] .",
    "interestingly , score matching performs reasonably well , on par with skeptic and neighborhood selection . for both sample sizes ,",
    "the differences , which are subtle , are most apparent in the regime where the number of false positives detected is small : score matching falls slightly short of neighborhood selection , but it also appears to slightly outperform skeptic . surprisingly , there is a clear margin of difference between the performances of regularized score matching and space , the former outperforming the latter , despite their noted structural similarities .",
    "glasso , which utilizes the full gaussian likelihood , performs the worst .",
    "overall , we conclude that regularized score matching is competitively robust when compared to its alternatives in the contaminated gaussian setting .",
    "in this section , we apply regularized gaussian score matching to observations arising from a multivariate @xmath261-distribution with mean @xmath62 and covariance matrix @xmath267 .",
    "this corresponds to testing the robustness of regularized score matching under model misspecification .",
    "like in the previous section , we consider the case when @xmath262 . to set up @xmath267",
    ", we construct a @xmath271 adjacency matrix based on an erds - rnyi graph with the probability of drawing an edge between any two arbitrary nodes set to 0.01 .",
    "we then convert the adjacency matrix into @xmath267 using the same procedure as in section [ gaussiancase ] .",
    "samples were drawn from a multivariate @xmath261-distribution with covariance matrix @xmath267 and three degrees of freedom .",
    "0.45 [ tdist1 ] -distributed case .",
    "the dashed line represents random selection of edges .",
    "the color to method correspondence is as follows : regularized score matching ( [ scorecolour ] ) , neighborhood selection ( [ mbcolour ] ) , glasso ( [ glassocolour ] ) , space ( [ spacecolour ] ) , skeptic ( [ skepticcolour ] ) , and spacejam ( [ spacejamcolour]).,title=\"fig : \" ]    0.45 [ tdist2 ] -distributed case .",
    "the dashed line represents random selection of edges .",
    "the color to method correspondence is as follows : regularized score matching ( [ scorecolour ] ) , neighborhood selection ( [ mbcolour ] ) , glasso ( [ glassocolour ] ) , space ( [ spacecolour ] ) , skeptic ( [ skepticcolour ] ) , and spacejam ( [ spacejamcolour]).,title=\"fig : \" ]    the roc curves are plotted in figure [ tdist ] for @xmath272 and @xmath273 .",
    "as expected , skeptic outperforms all others , owing to its flexibility to accommodate outliers , as previously demonstrated in @xcite .",
    "in fact , for elliptical distributions , such as the multivariate @xmath261-distribution , kendall s @xmath236 allows for consistent estimation of @xmath267 , so skeptic should perform optimally @xcite . nonetheless , regularized score matching is reasonably robust under this setting : its performance is comparable to that of spacejam  only falling slightly short  space , and neighborhood selection .",
    "again , glasso yields the poorest results .",
    "the american cancer society estimates that in 2015 there will be 220,800 new cases of prostate cancer and 27,540 deaths . to understand how the cancer develops , as well as how it may be treated , it is necessary to decipher the genetic machinery which drives it .",
    "since cancer is such a complex disease , it is insufficient to study a single gene at a time , as genes may interact with one another in many ways .",
    "graphical modeling of gene expression data has the potential to aid in discovery of such interactions .",
    "rnaseq data from next - generation sequencing technology can be used to identify genes that are activated / transcribed or suppressed at the time of measurement . however , rnaseq data are non - negative and have skewed marginals , which presents a challenge for existing methodologies .",
    "graphical models based on truncated gaussian models are interesting alternatives to existing approaches that primarily consist of applying gaussian methods after transformations .",
    "whether truncation models are truly useful scientifically deserves a fuller exploration ; here we simply illustrate how different estimates can be obtained from the proposed methodology .    our case study is based on the rnaseq data from 487 prostate adenocarcinoma samples available in the cancer genome atlas dataset .",
    "we focus on 350 genes that belong to `` known '' cancer pathways in the kyoto encyclopedia of genes and genomes . removing genes with more than 10% missing values",
    ", we obtained a dataset with @xmath274 genes .",
    "remaining missing values were simply set to zero , adding to the challenge .",
    "( we will comment on the issue of missing data in the discussion . ) in illustration of the regularized score matching methodology , we consider an exponential family of truncated normal distributions with density @xmath275 this generalizes the family of distributions considered in example  [ example2 ] by allowing the truncated normal distribution to have nonzero mean .",
    "we compare regularized non - negative score matching , space ( using concord formulation ) , glasso , skeptic and spacejam .",
    "we apply space and glasso directly to the standardized data .",
    "we do not consider any marginal transformations as they are naturally accounted for when comparing to the rank correlation - based skeptic .",
    "for each method , we tune the regularization parameter @xmath164 in order to obtain @xmath276 ( or @xmath277 ) edges .",
    "figure  [ topology ] depicts the estimated networks , with isolated nodes removed , in layouts optimized for each graph . to allow for easier comparison",
    ", we also show the estimated networks in fixed layouts in figure  [ topology2 ] .",
    "node degree distributions are plotted in figure  [ degdist ] .",
    "0.3   or @xmath277 edges for all considered methods .",
    "the layout has been optimized for each graph .",
    "isolated nodes are not shown .",
    "red colored nodes have degree greater or equal to 10.,title=\"fig : \" ]    0.3   or @xmath277 edges for all considered methods .",
    "the layout has been optimized for each graph .",
    "isolated nodes are not shown .",
    "red colored nodes have degree greater or equal to 10.,title=\"fig : \" ]    0.3   or @xmath277 edges for all considered methods .",
    "the layout has been optimized for each graph .",
    "isolated nodes are not shown .",
    "red colored nodes have degree greater or equal to 10.,title=\"fig : \" ]    0.3   or @xmath277 edges for all considered methods .",
    "the layout has been optimized for each graph .",
    "isolated nodes are not shown .",
    "red colored nodes have degree greater or equal to 10.,title=\"fig : \" ]    0.3   or @xmath277 edges for all considered methods .",
    "the layout has been optimized for each graph .",
    "isolated nodes are not shown .",
    "red colored nodes have degree greater or equal to 10.,title=\"fig : \" ]    0.3   or @xmath277 edges for all considered methods .",
    "layout of nodes is fixed across graph estimates and was optimized for the space estimate .",
    "isolated nodes have now been included .",
    "red colored nodes have degree greater or equal to 10.,title=\"fig : \" ]    0.3   or @xmath277 edges for all considered methods .",
    "layout of nodes is fixed across graph estimates and was optimized for the space estimate .",
    "isolated nodes have now been included .",
    "red colored nodes have degree greater or equal to 10.,title=\"fig : \" ]    0.3   or @xmath277 edges for all considered methods .",
    "layout of nodes is fixed across graph estimates and was optimized for the space estimate .",
    "isolated nodes have now been included .",
    "red colored nodes have degree greater or equal to 10.,title=\"fig : \" ]    0.3   or @xmath277 edges for all considered methods .",
    "layout of nodes is fixed across graph estimates and was optimized for the space estimate .",
    "isolated nodes have now been included .",
    "red colored nodes have degree greater or equal to 10.,title=\"fig : \" ]    0.3   or @xmath277 edges for all considered methods .",
    "layout of nodes is fixed across graph estimates and was optimized for the space estimate .",
    "isolated nodes have now been included .",
    "red colored nodes have degree greater or equal to 10.,title=\"fig : \" ]    -0.2 in    -0.2 in    by visual inspection , glasso and skeptic give similar topologies , which can be explained by the fact that both are derived from the full gaussian likelihood .",
    "interestingly , we observe that spacejam and space likewise yield similar graphs , which reinforces findings from @xcite .",
    "regularized non - negative score matching yields a graph that is fairly different from the rest .",
    ".the most densely connected genes according to the estimated graphs generated via nonnegative regularized score matching , glasso , skeptic , space and spacejam .",
    "the number in parenthesis corresponds to the estimated degree of the gene . [ cols=\"<,<,<,<,<\",options=\"header \" , ]     while the usefulness of these models remains to be further explored , our case study demonstrates that regularized score matching can provide estimates that differ in interesting ways to the estimates generated by other methods .",
    "we compile a list of the top ten most highly connected genes in each of the estimated graphs in table [ table : topten ] ( some lists have more than ten genes due to ties ) , as there is strong evidence that highly connected nodes play important roles in biological networks @xcite",
    ". there are slight overlaps between the lists .",
    "upon further inspection , we observe that six of the ten genes listed under regularized score matching have been previously linked to prostate cancer , five of which have not been identified by the competing methods :    * _ ccne2 _ ( cyclin e2 ) : a protein which is required for transition of the @xmath278 to @xmath279 phase of the cell cycle , which determines cell division .",
    "regulated by pten , a tumor suppressor , it is over - expressed in metastatic prostate tumor cells @xcite . *",
    "_ brca2 _ ( breast cancer 2 ) : mutations in the brca2 gene have been associated with early - onset prostate cancer in men ; men carrying mutations have a predisposition to more aggressive phenotypes @xcite . *",
    "_ birc5 _ ( survivin ) : a protein which prevents cell death , or apoptosis , and regulates cell division .",
    "heightened expression has been found to be associated with higher final gleason score , i.e. , more aggressive cancer and worse prognosis @xcite . * _ skp2 _ ( s - phase kinase - associated protein 2 , e3 ubiquitin protein ligase ) : a positive regulator of the @xmath278 to @xmath279 phase of the cell cycle , which determines cell division .",
    "skp2 labelling frequency in cancer was positively correlated with the gleason score , and shown to be a significant predictor of reduced recurrence - free survival time after radical prostatectomy @xcite .",
    "it has been proposed elsewhere as a promising therapeutic target for prostate cancer @xcite .",
    "* _ stat5b _ ( signal transducer and activator of transcription 5b ) : a transcription factor that encourages metastatic behavior of human prostate cancer cells .",
    "its inhibition has been shown to induce apoptosis in human prostate cancer cells @xcite .    furthermore , via the kolmogorov - smirnov test",
    ", we fail to reject the hypothesis that the degrees of the nodes for the regularized score matching graph estimate follow a power law distribution , with significance level of 0.05 . on the other hand",
    ", we reject this hypothesis for all other generated estimates at the same significance level .",
    "there is evidence that genetic networks are ` scale - free ' , which implies that their degree distribution can be approximated by a power law distribution @xcite . in this aspect ,",
    "the topology of regularized score matching estimate is most similar to the hypothesized structure of gene networks .",
    "finally , we would like to emphasize that we do not intend to claim that regularized score matching provides the _ best _ estimate of the underlying gene network , as the truth is unknown to us .",
    "what we can posit is that truncated gaussian may be a useful model that provides potentially valid targets for therapy which may be missed by other methods .",
    "this section establishes high - dimensional model selection consistency ( sparsistency ) of regularized score matching .",
    "we focus on pairwise interaction models as in ( [ expfam : pairwise ] ) , although our results could be extended to more general models .",
    "theorem  [ generalthm ] below identifies general deterministic conditions on data that yield sparsistency of regularized ( non - negative ) score matching .",
    "two subsequent corollaries make probabilistic statements about sparsistency in the gaussian and the non - negative gaussian case .",
    "proofs are given in section  [ sec : proofs ] .",
    "experiments that corroborate the theoretical findings are shown in appendix  [ sec : experiments ] .    before stating the main results",
    ", we describe a key assumption for model selection consistency of @xmath0-penalized estimators , the irrepresentability assumption , and highlight differences between various estimators of gaussian graphical models with respect to this assumption .",
    "we consider a continuous pairwise interaction model as given by  ( [ expfam : pairwise ] ) with symmetric @xmath111 interaction matrix @xmath280 .",
    "we let @xmath281 .",
    "then the regularized score matching estimator , in its basic or non - negative version , is @xmath282 by lemma  [ blockform ] , @xmath85 is a symmetric @xmath283 matrix that is block - diagonal , with blocks of size @xmath108 . for notational convenience",
    ", we drop the explicit reference to the data matrix @xmath25 and denote @xmath85 and @xmath87 as @xmath284 and @xmath285 .",
    "the true data - generating distribution is assumed to belong to the considered model .",
    "we denote the true interaction matrix by @xmath286 and its vectorization by @xmath287 .",
    "we define @xmath288 and @xmath289 to be the expected values of @xmath284 and @xmath285 . the support of @xmath287 , that is , @xmath290 is the edge set of the true conditional independence graph .",
    "similarly , @xmath291 determines the graph inferred by regularized score matching . finally , we write @xmath292 for the maximum degree of the @xmath19 nodes of the conditional independence graph . in other words , @xmath292 is the maximum number of nonzero off - diagonal entries in any row ( or column ) of @xmath293 .",
    "we say that the irrepresentability ( or mutual incoherence ) condition holds with incoherence parameter @xmath294 if the following assumption holds .",
    "[ assumption ] there exists an @xmath295 $ ] such that @xmath296    irrepresentability conditions play a key role in the analysis of @xmath0 regularization techniques @xcite . for neighborhood selection in gaussian graphical models",
    ", it has been formulated in terms of the covariance matrix @xmath243 @xcite . in the theoretical analysis of the glasso",
    ", the constraint is placed on the hessian of the log - determinant of the precision matrix @xmath248 , i.e. , @xmath297 @xcite . in order to highlight the differences in conditions required for sparsistency of glasso , neighborhood selection , space and regularized score matching , we revisit the gaussian graphical model example in @xcite .",
    "let @xmath298 , and let @xmath299 be the @xmath300 covariance matrix with ones along the diagonal , @xmath301 , @xmath302 and all other off - diagonal entries equal to @xmath237 .",
    "the precision matrix @xmath303 then has @xmath304 . the conditional independence graph @xmath5 is as in figure  [ graph ] .",
    "meinshausen showed that for samples drawn from @xmath305 , glasso can consistently recover @xmath5 only if @xmath306 . for neighborhood selection ,",
    "the corresponding necessary condition is @xmath307 .",
    "if these conditions fail , then for large sample size , the probability of erroneously including the edge @xmath308 , i.e. , @xmath309 can be shown to be at least 0.5 .",
    "it turns out that for regularized score matching , the analogous necessary condition gives a bound that falls in between 0.23 and 0.5 , specifically , @xmath310 .",
    "we observe that glasso , which yields positive definite estimates , requires the most stringent condition .",
    "when working with symmetric matrices as in regularized score matching , the condition is markedly relaxed . allowing non - symmetric matrices in neighborhood selection leads to further relaxation of the condition .",
    "interestingly , the pseudo - likelihood methods classified under space have the same necessary condition as score matching .",
    "assumption  [ assumption ] should be seen as sufficient for consistency of regularized score matching . for meinshausen",
    "s example , it can be shown to amount to @xmath311 .",
    "the analogous sufficient condition for glasso from @xcite requires that @xmath312 . for neighborhood selection ,",
    "the condition is @xmath313 .",
    "we define @xmath314 moreover , let @xmath315 such that the kkt conditions from  ( [ generalkkt ] ) can be written as @xmath316    [ generalthm ] assume that @xmath317 is invertible and the irrepresentability condition holds with incoherence parameter @xmath295 $ ] ( assumption [ assumption ] ) .",
    "furthermore , assume that @xmath318 with @xmath319 .",
    "if @xmath320 then the following statements hold :    1 .",
    "the rsme @xmath98 is unique , has its support included in the true support ( @xmath321 ) , and satisfies @xmath322 2 .",
    "if @xmath323 then @xmath324 and @xmath325 for all @xmath326 .    theorem [ generalthm ] imposes deterministic conditions on the data , namely , the bounds in ( [ bigassumptions2 ] ) . in the following corollaries",
    ", we will consider specific distributional assumptions and impose population conditions that imply bounds of the form ( [ bigassumptions2 ] ) with high probability .",
    "first , we provide a result for regularized score matching for the gaussian case ( example [ example1 ] ) , which has @xmath327 with @xmath140 being the sample covariance matrix , and @xmath328 . when the data is generated from a normal distribution with covariance matrix @xmath243 then @xmath329 and , of course , @xmath330 .",
    "[ th1 ] suppose the data is generated from a normal distribution @xmath331 such that @xmath317 is invertible and irrepresentability holds for @xmath295 $ ] .",
    "let @xmath332 , @xmath333 take any @xmath334 . if the sample size satisfies @xmath335 and the regularization parameter is @xmath336 then the following statements hold with probability @xmath337 :    1 .   the rsme @xmath178 from  ( [ eq : regscore ] ) is unique , has its support included in the true support ( @xmath321 ) , and satisfies @xmath338 2 .   if @xmath339 then @xmath324 and @xmath340 for all @xmath326 .",
    "the corollary is proven in appendix  [ sec : proof - coroll - refth1 ] .",
    "numerical experiments reported in appendix  [ sec : experiments ] suggest that the sample size @xmath20 indeed needs to scale at least @xmath341 for sparsistency .    from theorem  [ generalthm ]",
    ", we can also derive an analogous result for regularized non - negative score matching for the truncated gaussian case ( example [ example2 ] ) .",
    "the result requires the sample size to be larger than in the gaussian case , due to the need to control higher order moments .",
    "recall that here , @xmath85 a block diagonal @xmath145 matrix , with the @xmath8th block given by @xmath342 and @xmath343 , where @xmath344 and @xmath345 .",
    "[ th2 ] suppose the data is generated from a non - negative gaussian distribution with parameter @xmath248 , i.e. , @xmath346 is truncated to @xmath347 .",
    "suppose further that @xmath317 is invertible and irrepresentability holds for @xmath295 $ ] .",
    "let @xmath348 } , \\left(\\frac{l}{2}\\right)^2\\sqrt{\\max_{j } \\operatorname{var}[x_j^2 ] } \\right\\ }   \\quad\\text{and}\\quad c_2 = \\frac{6}{\\alpha}c_{\\boldsymbol{\\gamma}^*}\\ ] ] where @xmath349 is an absolute constant . take any @xmath350 .",
    "if the sample size satisfies @xmath351 and the regularization parameter is @xmath352 then the following statements hold with probability @xmath353 :    1 .   the rsme @xmath181 based on penalizing  ( [ nngaussiansme ] ) with @xmath182 is unique , has its support included in the true support ( @xmath321 ) , and satisfies @xmath354 2 .   if @xmath339 then @xmath324 and @xmath355 for all @xmath326 .",
    "the proof of the corollary , which is given in section  [ sec : proof - coroll - refth2 ] , uses general tail bounds that apply to log - concave measures .",
    "the lower bound for @xmath20 given in  ( [ nnn ] ) could well be suboptimal and a lower power of @xmath356 may be sufficient for sparsistency . however , the experiments in appendix  [ sec : experiments ] suggest that the exponent for @xmath357 can not be taken too much smaller than 8 .",
    "we also compared the lower bound we obtained for the non - negative gaussian case to a result implied by the work of @xcite who treat consistency of neighborhood selection in a general framework that allows node - wise conditional distributions to arise from exponential families .",
    "interestingly , when working out what their general theorem would say about the above non - negative gaussian model we found that the sample size @xmath20 would also be required to be at least @xmath358 .",
    "our result from corollary  [ th2 ] is thus at least comparable to existing results in the literature .",
    "first , we note that claim ( b ) is an immediate consequence of claim ( a ) . to show ( a ) , we apply the primal - dual witness method ( pdw ) from @xcite . as explained in detail below , pdw entails construction of a pair @xmath359 , with @xmath360 and @xmath361 , that satisfies the kkt optimality conditions from  ( [ largekkt ] ) and has the support of @xmath362 included in @xmath279 .",
    "if the construction is successful then it ensures that the rsme problem admits a unique solution such that the rsme @xmath98 is equal to @xmath362 and inherits all the properties the latter has by definition .",
    "these properties include the @xmath363 bound on estimation error in addition to the claim about the support .",
    "replacing @xmath284 by @xmath288 and @xmath364 by @xmath365 in the empirical ( basic or non - negative ) score matching loss recovers the population loss which , in the present exponential family context , is quadratic and minimized when @xmath366 .",
    "( recall that the score matching loss is consistent . )",
    "it follows that @xmath367 from  ( [ eq : r1r2r3 ] ) is zero as it is the gradient of the population loss . in block form , ( [ largekkt ] )",
    "becomes @xmath368 we construct the pdw pair @xmath359 according to the following steps :    1 .",
    "take @xmath369 to be the unique solution to the support - restricted problem , that is , @xmath370 2 .",
    "choose @xmath371 3 .   solving ( [ blockkkt ] ) ,",
    "set @xmath372 .",
    "\\end{aligned}\\ ] ] 4 .",
    "check the _ strict dual feasibility _ condition that @xmath373    by step ( i ) , @xmath362 has support contained in @xmath279 . by step ( iii )",
    ", @xmath374 is guaranteed to fulfill the equations from  ( [ blockkkt ] ) . by step ( ii )",
    ", the @xmath279-coordinates of @xmath375 satisfy ` their part ' of the subgradient condition .",
    "thus , if the strict dual feasibility from step ( iv ) holds , then @xmath374 satisfies the kkt conditions from  ( [ largekkt ] ) .",
    "having a strict inequality in  ( [ eq : strict - dual - feas ] ) ensures that every solution to the original rsme problem has support contained in the true support @xmath279 and since @xmath317 is assumed invertible , there is then only one solution ( * ? ? ?",
    "* lemma 1 ) .",
    "the invertibility of @xmath317 is also what guarantees the uniqueness in step ( i ) .",
    "if the pdw construction is successful , that is , if the strict dual feasibility condition can be established , then we may conclude the rsme @xmath98 possesses all the desired properties .",
    "indeed , @xmath98 equals @xmath362 which has these properties by construction .",
    "let @xmath376 , where @xmath362 is the solution to the support - restricted regularized score matching problem from  ( [ restricted ] ) . by definition ,",
    "furthermore , by step ( iii ) in the pdw construction , @xmath378\\\\ + \\boldsymbol{\\gamma}^*_{s^cs}(\\boldsymbol{\\gamma}_{ss}^*)^{-1}\\tilde{z}_s.\\end{gathered}\\ ] ] by assumption [ assumption ] , and the triangle inequality for the @xmath363 norm , @xmath379 + ( 1-\\alpha)\\nonumber\\\\ & \\leq \\frac{(2-\\alpha)}{\\lambda } \\bigg[\\|\\mathbf{r}_{1,\\cdot s}(\\theta^*_s + \\delta_s)\\|_\\infty + \\|r_2\\|_\\infty \\bigg ] + ( 1 -\\alpha ) \\nonumber\\\\ & =   \\frac{(2-\\alpha)}{\\lambda } \\bigg[\\|\\mathbf{r}_{1}\\theta^ * + \\mathbf{r}_{1 , \\cdot s}\\delta_s\\|_\\infty + \\|r_2\\|_\\infty \\bigg ] + ( 1 -\\alpha ) \\nonumber\\\\ & \\leq \\underbrace{\\frac{(2-\\alpha)}{\\lambda } \\ ,    \\|\\mathbf{r}_1\\theta^*\\|_\\infty}_{= g_1 } +    \\underbrace{\\frac{(2-\\alpha)}{\\lambda } \\,{{\\left\\vert\\kern-0.25ex\\left\\vert\\kern-0.25ex\\left\\vert \\mathbf{r}_{1 , \\cdot    s }       \\right\\vert\\kern-0.25ex\\right\\vert\\kern-0.25ex\\right\\vert}}_\\infty\\|\\delta_s\\|_\\infty}_{= g_2 } +    \\underbrace{\\frac{(2-\\alpha)}{\\lambda}\\|r_2\\|_\\infty}_{= g_3 } +    ( 1-\\alpha ) ,    \\nonumber \\ ] ] where the equality in the second to last line follows from the fact that @xmath380 .",
    "we observe that @xmath381 where @xmath382 is an @xmath383 matrix whose diagonal blocks are given by the rows of the the interaction matrix @xmath293 , each row being replicated @xmath19 times .",
    "moreover , @xmath384 refers to the vectorization of the @xmath19 diagonal blocks of @xmath385 that are each of size @xmath108 ; recall lemma [ blockform ] .",
    "more precisely , if @xmath386 are the diagonal blocks of @xmath385 , then @xmath384 is obtained by concatenating @xmath387 in that order .",
    "equation  ( [ eq : wide ] ) is the only argument relying on the block - diagonality of @xmath284 and @xmath385 .    from  ( [ eq : wide ] ) , we obtain that @xmath388 since we have assumed that @xmath389 . by construction , @xmath390 .",
    "it follows , from our choice of @xmath164 that @xmath391 .    by the assumption that @xmath392 , we have @xmath393 and it remains to similarly bound @xmath394 .",
    "we treat @xmath395 and @xmath396 separately .",
    "we note that the rows of @xmath397 have at most @xmath292 non - zero elements .",
    "it follows that @xmath398 , where the last inequality holds by assumption .",
    "since @xmath399 is assumed invertible , we have from the top block of equations in ( [ blockkkt ] ) that @xmath400 note that by assumption , @xmath399 is invertible .",
    "we obtain that @xmath401 \\nonumber \\\\ & <    { { \\left\\vert\\kern-0.25ex\\left\\vert\\kern-0.25ex\\left\\vert ( \\boldsymbol{\\gamma}_{ss})^{-1 }       \\right\\vert\\kern-0.25ex\\right\\vert\\kern-0.25ex\\right\\vert}}_\\infty\\bigg [ { { \\left\\vert\\kern-0.25ex\\left\\vert\\kern-0.25ex\\left\\vert \\boldsymbol{\\theta}^*_{\\mbox{\\tiny{wide } } }       \\right\\vert\\kern-0.25ex\\right\\vert\\kern-0.25ex\\right\\vert}}_\\infty\\|\\mbox{vec}(\\mathbf{r}_1)\\|_\\infty    +    \\|r_2\\|_\\infty + \\lambda \\bigg ] \\nonumber\\\\ & \\leq { { \\left\\vert\\kern-0.25ex\\left\\vert\\kern-0.25ex\\left\\vert ( \\boldsymbol{\\gamma}_{ss})^{-1 }       \\right\\vert\\kern-0.25ex\\right\\vert\\kern-0.25ex\\right\\vert}}_\\infty \\times \\frac{(6    - \\alpha)}{3(2 - \\alpha)}\\lambda .\\label{delta }   \\end{aligned}\\ ] ] since @xmath402 , we have @xmath403 .",
    "this implies that @xmath404 which gives us the following bound in the error in the inverse in the matrix @xmath363 norm , @xmath405 application of the triangle inequality , along with our definition of @xmath406 , yields @xmath407 where the last inequality uses the assumption that @xmath408 . substituting  ( [ gammainverse ] ) into ( [ delta ] ) , it is straightforward to show that @xmath409 . therefore , @xmath410 , which yields that @xmath411 .    along the way we have also proven the second part of the claim .",
    "indeed , from ( [ delta ] ) and ( [ gammainverse ] ) , we have @xmath412      we need to show that the conditions in theorem [ generalthm ] , specifically those in  ( [ bigassumptions2 ] ) , hold with the claimed probability .",
    "since @xmath413 , the second inequality in  ( [ bigassumptions2 ] ) can be trivially satisfied with any @xmath414 .",
    "thus , we only need to show that we can bound @xmath415 by some suitable @xmath416 with sufficiently large probability .",
    "to do so , we apply a bernstein - type concentration inequality for the entries of @xmath174 that is also used by @xcite .",
    "lemma  [ lem : ravi - concentration ] below states the inequality , as given in their paper .",
    "the matrix @xmath385 features only entries in @xmath417 . by taking a union bound over the @xmath418 entries of @xmath140 , plugging in our lower bound for @xmath20 and observing that @xmath419 in the gaussian case , lemma  [ lem : ravi - concentration ] yields that @xmath420   \\leq \\exp\\left\\{-\\log { m}^{\\tau_1 }   +     2\\log { m}\\right\\ } = \\frac{1}{{m}^{\\tau_1 - 2 } } .\\ ] ] in addition , each row in @xmath421 features at most @xmath292 entries from the matrix @xmath417 .",
    "hence , it follows from another union bound , and choosing @xmath20 at least @xmath422 where @xmath423 and @xmath424 are defined in the corollary statement , that @xmath425   \\leq \\frac{1}{{m}^{\\tau_1 - 2}}.\\ ] ] thus , applying theorem  [ generalthm ] with @xmath426 shows that our choices for @xmath164 and @xmath20 give the high probability statement in corollary [ th1 ] .",
    "when looking back at the proof of theorem [ generalthm ] , we see that as a consequence of having @xmath427 , we need only be concerned with bounding terms @xmath278 and @xmath394 .",
    "we may thus bound @xmath278 and @xmath394 each by @xmath428 instead of @xmath429 and ignore the @xmath430 term entirely , as it is @xmath62 .",
    "this leads us to having @xmath431 , as opposed to the expected @xmath432 .",
    "we proceed as for the proof of corollary [ th1 ] and use concentration results to satisfy the bounds from  ( [ bigassumptions2 ] ) in theorem [ generalthm ] . however , we now bound @xmath415 and @xmath433 using concentration inequalities for general log - concave measures ( any truncated multivariate normal density is log - concave ) .",
    "let @xmath434 be i.i.d .  according to @xmath346 with truncation to @xmath347 .",
    "take @xmath435 ^ 4}{\\sqrt{n}}\\sqrt{\\underset{j}{\\max}~\\operatorname{var}[x_j^4 ] } , \\label{e1}\\\\ \\epsilon_2 & = \\frac{\\left[\\left(\\frac{l}{2}\\right)(\\log { m}^{\\tau_2}+ \\log 2)\\right]^2}{\\sqrt{n}}\\sqrt{\\underset{j}{\\max}~\\operatorname{var}[x_j^2 ] } \\label{e2}. \\end{aligned}\\ ] ] from lemma [ polynomiallemma ] below , we know that for the absolute constant @xmath105 specified in lemma [ markovlemma ] , we have , @xmath436\\right| > \\epsilon_1\\right ] & < \\exp\\left\\ { - \\frac{2}{l}\\bigg(\\frac{\\sqrt{n}\\epsilon_1}{\\sqrt{\\underset{j , k , \\ell}{\\max}~\\operatorname{var}[x_jx_kx_\\ell^2]}}\\bigg)^{\\frac{1}{4 } } \\right\\ } , \\\\",
    "\\pr\\left[\\left|\\frac{1}{n}\\sum_{i=1}^n x_{ij}x_{ik } - e[x_jx_k]\\right| > \\epsilon_2\\right ] & < \\exp\\left\\ { - \\frac{2}{l}\\bigg(\\frac{\\sqrt{n}\\epsilon_2}{\\sqrt{\\underset{j , k , \\ell}{\\max}~\\operatorname{var}[x_jx_k]}}\\bigg)^{\\frac{1}{2 } } \\right\\}\\end{aligned}\\ ] ] for all @xmath437 . by a union bound over",
    "no more than @xmath438 events , we have both @xmath402 and @xmath439 with probability at least @xmath440 as @xmath441 . applying theorem  [ generalthm ] with the chosen",
    "@xmath416 and @xmath442 thus shows that our choices for @xmath164 and @xmath20 lead to the claim in corollary [ th2 ] .",
    "this paper proposes the use of regularized score matching for estimation of conditional independence graphs in high dimensions .",
    "the focus is on modifying the score matching loss of @xcite with an @xmath0 penalty to accommodate underlying sparsity , which is in the spirit of popular existing methods such as glasso and neighborhood selection .",
    "this said , any other regularization scheme can be considered instead . for instance , the method from @xcite can be applied to encourage hub structure in the inferred graph .",
    "our study of the gaussian example of @xcite suggests that @xmath0-regularized score matching falls in between neighborhood selection and glasso in terms of conditions for required for graph selection consistency . here",
    ", the glasso requires the most stringent conditions , and the score matching approach appears to be similar to pseudo - likelihood methods that work with symmetric estimates of precision matrices , such as space @xcite and subsequent reformulations such as concord @xcite . however , regularized score matching is particularly convenient in that the score matching loss is a quadratic function , even for non - gaussian exponential families .",
    "this brings about piecewise linear solution paths and allows for a simple theoretical analysis .",
    "we anticipate that the simple structure of score matching will lead to further advances in graphical modeling , such as computationally efficient techniques to deal with corrupted or missing data , in the spirit of @xcite , or new methods to tune regularization parameters , as in @xcite .",
    "regularized score matching is an interesting method for gaussian models , as we showed empirically and theoretically . in particular , for consistency ( under the usual irrepresentability conditions ) , the sample @xmath20 must be on the order @xmath443 , which matches the conditions for the existing methods mentioned above .",
    "however , as our simulation study shows , regularized score matching really shines in the context of non - gaussian models , where it eliminates the need to deal with computationally intractable normalization constants in a way that the loss continues to be a quadratic function of parameters .",
    "this opens a lot of new possibilities for graphical modeling such as the truncated normal model we applied to rnaseq data .",
    "score matching applies to continuous data . while @xcite discusses a ratio matching method for discrete data , it is not as computationally convenient as its continuous counterpart . a different approach of adding gaussian noise to discrete data was proposed for imaging problems by @xcite .",
    "exploring the merits of their approach for graphical modeling , and supplying supporting theory , would be an interesting problem for future work .",
    "the piecewise linear solution path for regularized score matching can be computed using algorithm  [ alg1 ] , which is an adaptation of the lars - lasso algorithm for linear regression @xcite .",
    "it is also a special case of the algorithm found in @xcite . in our pseudocode",
    ", @xmath444 is the current active set , i.e. , @xmath445 for the currently relevant value of the regularization parameter @xmath164 .",
    "_ initialize _ @xmath446 _ initialize _ @xmath447 _ initialize _ @xmath448 _ initialize _",
    "@xmath449 @xmath450 .",
    "@xmath453 add variable that attains equality to @xmath444 . remove variable that attains @xmath62 from @xmath444 .",
    "@xmath454    in the gaussian and truncated gaussian case , the algorithm stops when the active set has size @xmath455 . for larger active sets",
    "the matrix @xmath456 is not invertible .",
    "finding the step size in algorithm  [ alg1 ] requires @xmath457 operations , while the inversion step is at its worst @xmath458 .",
    "overall , the complexity of algorithm  [ alg1 ] can be found to be @xmath459 ; the heaviest cost comes from the matrix inversion step .    for large - scale problems , lars - type algorithms may be slow and coordinate - descent methods are popular alternatives ( see e.g. * ? ? ?",
    "algorithm  [ alg2 ] describes a coordinate - descent algorithm to minimize the regularized score matching objective from  ( [ regquadratic ] ) .",
    "it entails updating one coordinate , or one element in the parameter vector / matrix , such that it minimizes the objective function while holding all others as constant , until a convergence criterion is satisfied .",
    "results in @xcite ensure convergence of algorithm  [ alg2 ] .",
    "initial estimate @xmath460 @xmath461 , maximum number of iterations @xmath462 , the maximal tolerance level _ initialize _ @xmath463 _ initialize _ @xmath464 ( @xmath465 stands for convergence criteria ) @xmath466 @xmath467 .",
    "@xmath468 @xmath469    example1 for the gaussian case , the coordinate descent procedure alternates between updating the diagonal entries and off - diagonal entries , by manipulating the estimating equations ( [ diag ] ) and ( [ offdiag ] ) accordingly .",
    "the updates are of the form @xmath470 for @xmath471 .",
    "the computational complexity of this scheme can be shown to be @xmath472 , which is the same as for the methods classified under space ; the complexity of glasso is @xmath473 .",
    "we do not prove this fact , as it follows directly from reasoning elaborated on in @xcite .",
    "corollaries [ th1 ] and [ th2 ] make use of the following concentration results . the first lemma is used to prove corollary [ th1 ] while the latter two ( one is derived from the other ) are used to prove corollary [ th2 ] .    [",
    "lem : ravi - concentration ] if @xmath474 is a zero - mean random vector with covariance matrix @xmath243 such that @xmath475 is sub - gaussian with scale parameter @xmath64 , then the sample covariance matrix @xmath140 , for @xmath20 i.i.d .",
    "samples , satisfies the bound @xmath476 & \\leq 4\\exp\\left\\{-\\frac{n\\delta^2}{128(1 + 4\\sigma^2)^2\\underset{j = 1,\\ldots , { m}}{\\max}{(\\boldsymbol{\\sigma}^*_{jj})^2 } } \\right\\ } \\end{aligned}\\ ] ] for any fixed choice of two indices @xmath477 and for all @xmath478 .",
    "[ markovlemma ] let @xmath479 be a banach space , and let @xmath480 be a polynomial of degree at most @xmath217 .",
    "suppose @xmath481 and @xmath13 is a log - concave probability measure on @xmath21 .",
    "then @xmath482 where @xmath483 is an absolute constant .    from this lemma",
    "we may derive the following concentration result .",
    "after proving the lemma , we comment on how it is used in the proof of corollary  [ th2 ] .",
    "[ polynomiallemma ] consider a degree @xmath217 polynomial @xmath484 , where @xmath485 are possibly dependent random variables with log - concave joint distribution on @xmath21 .",
    "let @xmath349 be the constant from lemma  [ markovlemma ] .",
    "then , for all @xmath486 such that @xmath487 } } \\right)^{1/z } \\ge 2,\\ ] ] we have , @xmath488| > \\delta ] \\;\\leq\\ ; \\exp\\left\\{-\\frac{2}{l}\\left(\\frac{\\delta}{\\sqrt{\\operatorname{var}[f(x)]}}\\right)^{1/z } \\right\\}.\\ ] ]    choosing @xmath489 and @xmath490 in lemma  [ markovlemma ] , we have @xmath491|^{k}]^{\\frac{1}{k } }       & \\;\\leq\\ ; \\left(\\frac{lk}{2}\\right)^z\\sqrt{\\operatorname{var}[f(x)]}.     \\end{aligned}\\ ] ] hence , by markov s inequality , for any @xmath486 satisfying ( [ kcondition ] ) , @xmath492| > \\delta ]       & \\;\\leq\\ ;   \\frac{e[|f(x ) -        e[f(x)]|^{k}]}{\\delta^{k}}\\\\      & \\;\\leq\\ ;          \\left [        \\left(\\frac{lk}{2}\\right)^z\\frac{\\sqrt{\\operatorname{var}[f(x)]}}{\\delta }        \\right]^k   \\\\       & \\;=\\ ;         \\mbox{exp}\\{-k\\ } \\\\        & \\;=\\ ; \\mbox{exp}\\left\\{-\\frac{2}{l}\\left(\\frac{\\delta}{\\sqrt{\\operatorname{var}[f(x)]}}\\right)^{\\frac{1}{z } } \\right\\ } ,    \\end{aligned}\\ ] ] and",
    "the proof is complete .    in the proof of corollary [ th2 ]",
    ", we apply lemma  [ polynomiallemma ] with @xmath493 from  ( [ e1 ] ) and with @xmath494 from  ( [ e2 ] ) .",
    "it thus needs to be checked that condition ( [ kcondition ] ) holds in these two cases .",
    "indeed , the condition holds as long as @xmath495 to see this , we substitute @xmath416 and @xmath442 for @xmath486 in ( [ kcondition ] ) , take @xmath496 and @xmath497 respectively , to find a term that is lower bounded by @xmath498 . here ,",
    "the @xmath499 factor in @xmath416 and @xmath442 cancels out with the @xmath499 term generated by the @xmath500}$ ] term in the denominator .",
    "( recall that in our scenario @xmath501 is an empirical average ) .",
    "the more stringent condition on @xmath19 comes from @xmath442 and is stated in ( [ mmm ] ) .",
    "thus , if  ( [ mmm ] ) holds , ( [ kcondition ] ) is satisfied . since @xmath502 , the right - hand side of  ( [ mmm ] ) never exceeds @xmath503 hence , in our application of lemma  [ polynomiallemma ] , the condition from  ( [ kcondition ] ) holds for @xmath504 .",
    "we perform experiments , similar to those found in related work , that give empirical support for corollary [ th1 ] .",
    "this corollary treats gaussian graphical models for which the sample size @xmath20 ought to be of order @xmath505 .",
    "we experiment by varying the number of variables @xmath19 , the degree @xmath292 , and the minimum signal strength .",
    "following @xcite , we define the ` model complexity ' to be @xmath506 in addition , we investigate how the sample size @xmath20 required for sparsistency for non - negative gaussian graphical models needs to depend on @xmath19 .",
    "all reported results are based on averaging over 100 trials .",
    "we conduct our experiments using three graph structures : ( a ) a chain , ( b ) a 2-d lattice with 4 nearest neighbors , and ( c ) a star .",
    "we consider ( a ) and ( b ) when varying the number of variables @xmath19 , in which case we vary the length of the chain and the number of nodes in the lattice .",
    "this keeps the degree @xmath292 constant .",
    "the effect that @xmath292 has on the sample complexity is investigated using stars .",
    "we let the regularization parameter @xmath164 scale with @xmath507 , a choice corroborated by corollary [ th1 ] .",
    "consider first the case where the underlying conditional independence graph is a chain of length @xmath508 .",
    "the degree @xmath292 is always 2 , and we choose the tridiagonal precision matrix @xmath248 to have entries @xmath509 if @xmath510 and @xmath511 for @xmath512 . here ,",
    "@xmath294 , @xmath513 and @xmath514 are constant across all @xmath19 .",
    "figure [ chain ] shows the probability of correct signed support recovery plotted against the sample size @xmath20 , with different curves corresponding to different @xmath19 .",
    "as expected , we see from figure [ chain](a ) that successful support recovery requires @xmath20 to grow with @xmath19 .",
    "however , upon rescaling @xmath20 by @xmath515 , the curves overlap as seen in figure [ chain](b ) .",
    "we repeat the experiment with the 2-d lattice graph with @xmath516 nodes .",
    "each node is connected to four nearest neighbors such that the degree @xmath292 is always 4 .",
    "we choose @xmath248 with @xmath517 for @xmath510 and @xmath511 for @xmath512 .",
    "again , @xmath294 , @xmath513 and @xmath514 are constant across all @xmath19 .",
    "the results are presented in figure [ lattice ] , which shows curves of recovery probabilities that stack on top of one another when @xmath20 by @xmath515 .",
    "we now fix the number of nodes to @xmath518 and vary @xmath292 .",
    "we consider a star graphs with varying hub node degree @xmath519 .",
    "the precision matrix @xmath248 is chosen such that @xmath520 for @xmath510 , and @xmath521 for @xmath512 .",
    "now , @xmath294 , @xmath513 and @xmath514 are constant across all @xmath292 .",
    "figure [ star ] shows the probability of correct signed support recovery plotted against @xmath20 .",
    "the left panel demonstrates that correct recovery is more difficult with increasing @xmath292 .",
    "larger @xmath20 is needed to attain the same success rate . upon rescaling @xmath20 by @xmath522 in the right panel ,",
    "the three curves align .",
    "this validates corollary [ th1 ] in that for fixed @xmath19 , @xmath294 , @xmath513 and @xmath514 , the sample size @xmath20 needs to scale with @xmath523 to ensure sign consistency .",
    "we return to the chain - structured graphs considered earlier in this section .",
    "this time , however , we fix @xmath524 and @xmath525 while changing the edge strengths @xmath526 for @xmath510 , which alters @xmath465 from  ( [ eq : model - complexity ] ) .",
    "we plot the probability of correct signed support recovery against @xmath20 for varying @xmath465 . in the resulting figure [ chaincomplex ] ,",
    "the curves shift right as @xmath465 becomes larger so a larger @xmath20 is needed to attain the same probability of correct signed support recovery when @xmath465 grows .",
    "this is again consistent with the implications of corollary [ th1 ] .",
    "we do not believe that the lower bound we found for @xmath20 is sharp enough in terms of its dependence on @xmath294 , @xmath513 and @xmath514 to determine the rescaling we must perform on @xmath20 to align the curves .        finally",
    ", we experiment with regularized non - negative score matching for normal observations truncated to the positive orthant .",
    "according to corollary [ th2 ] , a sample size of @xmath527 is sufficient for signed support recovery .",
    "the aim of our experiments is to explore to what extent this scaling is necessary .",
    "specifically , we will consider exponents other than 8 for @xmath356 .",
    "for our experiments , we revisit the chain - structured graphs from section [ numbernodes ] and choose a triangular matrix @xmath248 with @xmath509 if @xmath510 and and @xmath511 for @xmath512 .",
    "the degree @xmath292 is fixed at 2 and we only vary @xmath528 .",
    "we let the regularization parameter @xmath164 to scale with @xmath529 .",
    "figure [ chain22 ] plots the probability of correct signed support recovery against @xmath20 , with different curves for the different values of @xmath19 .",
    "panel ( a ) in figure [ chain22 ] illustrates that , larger @xmath20 is needed account for larger @xmath19 .",
    "the other three panels have the @xmath48-axis rescaled to @xmath530 for exponents @xmath531 .",
    "panel ( b ) suggests that @xmath20 scaling with @xmath532 is not sufficient for support recovery .",
    "comparing panels ( c ) and ( d ) , @xmath533 seems more than what is necessary .",
    "it thus appears that the scaling of the sample size we assumed in corollary [ th2 ] is suboptimal but not drastically so ."
  ],
  "abstract_text": [
    "<S> graphical models are widely used to model stochastic dependences among large collections of variables . </S>",
    "<S> we introduce a new method of estimating undirected conditional independence graphs based on the score matching loss , introduced by @xcite , and subsequently extended in @xcite . </S>",
    "<S> the _ </S>",
    "<S> regularized score matching _ method </S>",
    "<S> we propose applies to settings with continuous observations and allows for computationally efficient treatment of possibly non - gaussian exponential family models . in the well - explored gaussian </S>",
    "<S> setting , regularized score matching avoids issues of asymmetry that arise when applying the technique of neighborhood selection , and compared to existing methods that directly yield symmetric estimates , the score matching approach has the advantage that the considered loss is quadratic and gives piecewise linear solution paths under @xmath0 regularization . under suitable irrepresentability conditions , we show that @xmath0-regularized score matching is consistent for graph estimation in sparse high - dimensional settings . through numerical experiments and an application to rnaseq data , we confirm that regularized score matching achieves state - of - the - art performance in the gaussian case and provides a valuable tool for computationally efficient estimation in non - gaussian graphical models .    , </S>"
  ]
}