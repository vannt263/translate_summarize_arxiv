{
  "article_text": [
    "deep convolutional neural networks ( dcnns ) have brought about breakthroughs in many domains , and their application to online handwritten chinese character recognition ( hccr ) has persistently yielded state - of - the - art results in recent years @xcite .",
    "these previous work have effectively improved hccr performance , e.g. application of the path signature theory @xcite , usage of comprehensive domain knowledge @xcite and more advanced training methods @xcite , etc .",
    "however , several challenges still need to be addressed .",
    "the main challenge presented by hccr results from varied handwriting styles and the large number of chinese character classes .",
    "a large - scale dataset is vital to hccr performance , especially when using a dcnn .",
    "however , data acquisition with high quality ground - truth is a tedious job .",
    "one possible solution to this problem may be to apply character distortion to generate artificial samples @xcite .",
    "such kind of approach enables the generation of a large number of training samples to improve the performance .",
    "however , in previous studies , character distortion is usually applied at a fixed degree throughout the entire training process .",
    "even though a fixed high - degree distortion reduces overfitting by generating varied samples , it may lead to a distribution that deviates from the underlying data distribution , whereas a fixed low - degree distortion would achieve the opposite .",
    "hence , more proper distortion strategies should be investigated .",
    "another challenge is to find a good feature representation for online characters .",
    "although dcnn is good at capturing visual concepts from raw inputs , prior knowledge can be encoded into the inputs for a dcnn to improve the performance@xcite . in online hccr ,",
    "8-directional features @xcite , or path signature @xcite can be regarded as prior knowledge that enhances a dcnn , but it needs further study to determine how these feature representations could be improved .",
    "rather than incorporating dynamic information into dcnn , @xcite used recurrent neural network ( rnn ) to directly recognize and draw online chinese characters , and showed that rnn is also able to learn complex representation of online characters .    in this paper , we focus on dcnn based models and explore several techniques to address these challenges .",
    "motivated by the great performance of the path signature in hccr @xcite , we utilize path signature as features for online characters .",
    "different from previous usage of path signature features for hccr , we add a time dimension to the online characters in accordance with the writing order to enable extraction of more expressive features .",
    "our dcnn architecture is carefully designed to enable more effective learning from the signature features , with spatial stochastic max - pooling layers performing feature map distortion and model averaging .",
    "the _ dropdistortion _ training strategy , which gradually lowers the character distortion degree during training , is proposed to address the drawbacks of a fixed degree of distortion .",
    "1 pictorially illustrates this concept . in early training epochs , a high degree of distortion",
    "provides improved generalization , whereas in later epochs , a decreasing degree of distortion gradually reveals more genuine data distribution .",
    "experiments on the casia - olhwdb 1.0 , casia - olhwdb 1.1 , and the icdar2013 online hccr competition dataset achieve state - of - the - art accuracies of 97.67% , 97.30% and 97.99% , respectively .     is lowered after certain epochs at the training stage , in order to gradually reveal the genuine data distribution . ]    the remaining part of this paper is organized as follows .",
    "section provides a detailed analysis of the proposed _",
    "dropdistortion _ method .",
    "section introduces the path signature .",
    "section describes our dcnn architecture and design pipeline .",
    "section presents the experimental results and its detailed analysis . finally , section concludes the paper .",
    "character distortion is widely used in hccr to generate artificial training samples@xcite .",
    "the basic structures of the distorted samples are the same as those of the originals .",
    "2 illustrates some rotated samples of a chinese character . in previous studies ,",
    "however , character distortion is applied upto a certain extent in an empirical way . in this paper , we study the influence of character distortion in detail and propose a simple but novel method , namely _ dropdistortion _ , to help enhance hccr performance .",
    "we use affine transformation to distort input characters .",
    "let @xmath0 denote the degree of character distortion , _",
    "denote a random number drawn from uniform distribution _",
    "u(-@xmath0 , @xmath0 ) _ , and [ _ * x * _ _ * y * _ ] denote the coordinate series of a character stroke , a matrix of size _ _ n__@xmath22 where n indicates the number of data points",
    ". then we can apply affine transformation to distort online handwritten chinese characters : @xmath3\\leftarrow[\\emph{\\textbf{x}}\\ \\emph{\\textbf{y}}]\\cdot\\left [                                                                                         \\begin{array}{cc }                                                                                           1+\\emph{$\\xi_x$ } & 0 \\\\                                                                                           0 & 1+\\emph{$\\xi_y$ } \\\\",
    "\\end{array }                                                                                       \\right],\\ ] ] @xmath4\\leftarrow[\\emph{\\textbf{x}}\\ \\emph{\\textbf{y}}]\\cdot\\left [                                                                                         \\begin{array}{cc }                                                                                           1 & \\emph{$\\xi$ } \\\\                                                                                           0 & 1 \\\\                                                                                         \\end{array }                                                                                       \\right],\\ ] ] @xmath5\\leftarrow[\\emph{\\textbf{x}}\\ \\emph{\\textbf{y}}]\\cdot\\left [                                                                                         \\begin{array}{cc }                                                                                           1 & 0 \\\\                                                                                           \\emph{$\\xi$ } & 1 \\\\                                                                                         \\end{array }                                                                                       \\right],\\ ] ] @xmath6\\leftarrow[\\emph{\\textbf{x}}\\ \\emph{\\textbf{y}}]\\cdot\\left [                                                                                         \\begin{array}{cc }                                                                                           \\cos(\\emph{$\\xi$ } ) & -\\sin(\\emph{$\\xi$ } ) \\\\                                                                                           \\sin(\\emph{$\\xi$ } ) & \\cos(\\emph{$\\xi$ } ) \\\\",
    "\\end{array }                                                                                       \\right],\\ ] ] where ( 1 ) stretches or shrinks the stroke , ( 2 ) and ( 3 ) slant the stroke and ( 4 ) performs rotational distortion . a character is distorted by simply applying one or more of the above equations to all its strokes , with _",
    "@xmath1 _ in the same equation fixed within a character .",
    "we also randomly translate the characters to achieve a better distortion diversity .",
    "first , consider a simple example with two handwritten characters , the arabic numeral `` 1 '' and the chinese character``one '' ( fig .",
    "we use _ a _ and _ b _ to denote them respectively for convenience .",
    "when written by hand _",
    "a _ and _ b _ look quite similar except for their orientations .",
    "in other words , they have the same topological structure . let _",
    "@xmath7 _ denote the angle between their orientations and the horizontal direction , and assume _",
    "@xmath7 _ obeys a gaussian distribution : _ _",
    "@xmath8__@xmath9_n(@xmath10/2,@xmath11 ) _ , _ _ @xmath12__@xmath9_n(0,@xmath11)_.    then from the bayesian perspective , we have @xmath13 @xmath14 where _",
    "f(a@xmath15@xmath7 ) _ and _ f(b@xmath15@xmath7 ) _ are probabilities predicted by a classifier , _",
    "f(@xmath7@xmath15a ) _ and _ f(@xmath7@xmath15b ) _ model the orientation information , _",
    "f(a ) _ and _ f(b ) _ model the topological structure information , and _",
    "f(@xmath7 ) _ is a normalized constant determined by the training set and can be safely removed : @xmath16 @xmath17 if the topological structure of _ a _ and _ b _ is nearly the same , we have @xmath18 although _ _",
    "f(a)__@xmath19_f(b ) _ , the classifier is still able to correctly classify these two characters according to _ @xmath7 _ due to the difference between _",
    "f(@xmath7@xmath15a ) _ and _ f(@xmath7@xmath15b)_.    if rotational distortion is applied to _ a _ and _ b _ with @xmath1 drawn from the uniform distribution _ u(-@xmath0 , @xmath0 ) _ , then the distribution of",
    "_ @xmath7 _ is shown in fig .",
    "4 . if rotational distortion is applied to an extreme , i.e. , @xmath0=@xmath20 , then _ @xmath7 _ obeys a uniform distribution as well . under these circumstances , the classifier is unable to distinguish between _ a _ and _ b _ because _ _ f(@xmath7@xmath15a)__=_f(@xmath7@xmath15b ) _ and _ _ f(a)__@xmath19_f(b)_.    however , when it comes to hccr , the situation is somewhat different .",
    "a rotated chinese character will rarely resemble other chinese characters .",
    "a high degree of rotational distortion actually removes the conditional term _",
    "f(@xmath7@xmath15c ) _ and forces the classifier to predict _",
    "f(c ) _ more accurately , i.e. , to achieve improved learning of the topological structure of the character _ c_. by preventing co - adaptation of _",
    "f(c ) _ and _ f(@xmath7@xmath15c ) _ , a high degree of rotational distortion reduces overfitting .",
    "other distortions such as stretching can be analyzed in the same way . hence using character distortion to generate artificial samples",
    "is a reasonable and effective way to enhance hccr performance .",
    "although effective in improving hccr performance , character distortion changes the data distribution .",
    "specially , it confuses some similar characters inevitably .",
    "for example , the chinese characters `` the sun '' ( fig .",
    "3 ( c ) ) and `` say '' ( fig .",
    "3 ( d ) ) would be indistinguishable if they were to be highly stretched .",
    "the change of distribution should be considered to give further improvements , but no such efforts have been reported in previous studies @xcite , where the character distortion is carried out upto a fixed degree during the entire training process . the proposed _ dropdistortion _",
    "method is a novel strategy that is designed to take the change of distribution into consideration .",
    "it s based on a simple idea that the dcnn should be fine - tuned with more genuine samples , i.e. , low - degree or non distorted samples . the proposed _",
    "algorithm is given in algorithm 1 .",
    "* input : * training set @xmath21=(@xmath22 , @xmath23 ) , i=1, ... ,m of k classes . +",
    "* initialize : * index @xmath24 ; distortion degree @xmath25 . + * output : * dcnn parameters @xmath26 . +",
    "* begin : *    @xmath27 sample distortion at degree @xmath28 update w through back propagation algorithm @xmath29    [ alg_lirnn ]    in the proposed method , a high degree of distortion is used in the early training epochs to generate varied samples to help the dcnn learn effective features and reduce the risk of overfitting . in the later epochs ,",
    "the degree of distortion decreases gradually to allow a subsequent finer adjustment of the dcnn with more genuine samples .",
    "the only extra complexity _ dropdistortion _ introduces is to monitor the training loss and decrease the distortion rate if a certain condition is fulfilled . in practice ,",
    "_ dropdistortion _ can be simply implemented in a multi - step way , and in this paper it is implemented in a three - step way as explained in fig . 1 .",
    "in mathematics , a path signature is a collection of iterated integrals @xcite of a path .",
    "t_]@xmath30__@xmath31 _ _ denote a continuous path of bounded variation , mapping from time interval [ 0 , _ t _ ] to space _ @xmath31_. then the _ k_-th iterated integral of path _ x _ is @xmath32 where @xmath33 represents the tensor product . by convention",
    ", @xmath34 is the number one .",
    "the signature of path _ x _ is the collection of all the iterated integrals of _ x _ , denoted by _",
    "s(x)_. since this is an infinite series , in practice one often considers the first _",
    "_ m__th - order integrals , namely truncated path signature : @xmath35 as is often the case , _ x _ is sampled and approximated by a set of discrete points",
    ". then the iterated integrals can be approximated by using some simple tensor algebra @xcite .",
    "online handwriting can be seen as a path mapping from time interval [ 0 , _ t _ ] to @xmath36 .",
    "graham @xcite first introduced truncated path signature features to online handwritten character recognition as inputs for a dcnn and achieved remarkable performance . some previous work @xcite experimented with truncated level _ m _ and found that the accuracy increases with the increase of _ m _ and gradually saturates . in our work , _ m _ is empirically set as 4 because integrals of a higher order typically characterize more trivial details of a path and do not lead to further improvement .",
    "the signature is invariant to translations and time re - parameterization of the path , and uniquely characterizes the path if it contains no part that exactly retraces itself @xcite .",
    "however , in handwritten characters , some local parts do retrace themselves due to joined - up writing .",
    "for example , path ( [ 0,0 ] , [ 1.5,2.5 ] , [ 3,3 ] , [ 1.5,2.5 ] ) has the same signature with path ( [ 0,0 ] , [ 1.5,2.5 ] ) because ( [ 1.5,2.5 ] , [ 3,3 ] , [ 1.5,2.5 ] ) exactly retraces itself . to handle this problem",
    ", we introduce a monotone time dimension to the original handwriting sequences , i.e. , the _ _ n__@xmath22 matrix [ _ * x * _ _ * y * _ ] is appended by a column vector _ _",
    "* t*__=@xmath37^t$ ] .",
    "this concept is demonstrated in fig .",
    "5 . the time dimension ensures the uniqueness for the path signature , hence features extracted from the _ _ n__@xmath23 matrix [ _ * t * _ _ * x * _ _ * y * _ ] would be more expressive .    in practice , if _ _",
    "m__=4 , then _ _",
    "n__=31 for a 2d character and _ _",
    "n__=121 for a 3d character , where _ n _ denotes the number of input channels .",
    "deep convolutional neural networks ( dcnns ) have shown great success in computer vision and pattern recognition , and different architectures of dcnn have been explored @xcite .",
    "however , these designs did not evaluate the max - pooling ( mp ) operation , which incorporates a degree of invariance with respect to translations and elastic distortions into the dcnn @xcite .",
    "max - pooling operates in a sliding window method , and conventionally the pooling region slides with a fixed integer stride .",
    "usually the stride is two , hence the size of feature map is reduced by a factor of two .      compared to traditional mp layers , fractional max - pooling ( fmp ) @xcite layers reduce the feature map size by a factor of @xmath38 with @xmath38 as a fraction . if 1@xmath39@xmath38@xmath392 , then fmp reduces the size in a slower manner than mp . as @xmath38 is a fraction ,",
    "the pooling stride can not be a fixed integer . in this case , the pooling region slides with a pre - calculated stride series .",
    "we prefer to call fmp as spatial stochastic max - pooling ( ssmp ) , which describes the pooling process more precisely .    let _",
    "n@xmath40 _ and _ n@xmath41 _ denote the input and output feature map size respectively",
    ". then @xmath42 @xmath43 @xmath38 is renewed above due to the rounding effect .",
    "the stride series is a @xmath44 vector for each dimension of the output image .",
    "then for the i - th ( i=0,1, ...",
    "@xmath452 ) position of the stride series , @xmath46 @xmath47 where @xmath48 is the stride series , @xmath49 is the accumulated stride series with @xmath50 and @xmath51 ( the pooling size is 2@xmath22 ) , and @xmath52 is a randomly drawn threshold to round @xmath49 up or down . in practice , @xmath53 can be set independently at different position i , or set only once and shared across different positions as a constant .",
    "we denote these two strategies as ssmp@xmath54 and ssmp@xmath55 respectively .",
    "different to @xcite which employs ssmp@xmath54 , we propose a new strategy that @xmath53 is set independently at each position i in early training epochs and set only once in the last epochs .",
    "we call this strategy as ssmp@xmath56 .    during the pooling process",
    ", ssmp introduces a certain degree of randomness into the pooling regions by a random choice of the feasible stride series . fig .",
    "6 illustrates this concept vividly .",
    "the colored 2@xmath22 squares are the pooling regions chosen . by setting @xmath38 as 1.5 ,",
    "the feature map size is reduced from 6@xmath26 to 4@xmath24 with multiple feasible pooling choices ; hence every forward path may have a slightly different output .",
    "moreover , ssmp achieves elastic distortion @xcite of the feature maps of the previous layer , which implicitly distorts the input characters and improves the generalization ability of a dcnn . in test phase , running the network for multiple times and averaging the outputs achieve the same effect as an ensemble of similar networks .      in our dcnn",
    ", we use 3@xmath23 or 2@xmath22 convolutional filters @xcite , which also makes the network deeper compared to larger filter size .",
    "the path signature features are extracted from online handwriting data sequences , and then padded into _ _",
    "n__@xmath250@xmath250 bitmaps embedded in larger grids ( pad zeros around the characters ) .",
    "the first pooling layer is max - pooling rather than ssmp , as we find max - pooling works better than ssmp when dealing with the input , partly due to the input noise introduced by padding a continuous handwriting path onto a discrete bitmap .",
    "we apply the leaky relu activation function @xcite with _ _",
    "a__=0.333 , which outperforms relu @xcite in terms of convergence speed and representation capability @xcite .",
    "the datasets we used are casia - olhwdb 1.0 ( db 1.0 ) , casia - olhwdb 1.1 ( db 1.1 ) @xcite and icdar2013 competition dataset @xcite . db 1.0",
    "contains 3740 chinese character classes in standard level-1 set of gb2312 - 80 ( gb1 ) and is obtained from 420 writers ( 336@xmath23740 samples for training , 84@xmath23740 for testing ) .",
    "db 1.1 contains 3755 classes in gb1 and is obtained from 300 writers ( 240@xmath23755 samples for training , 60@xmath23755 for testing ) .",
    "the database for the icdar2013 online hccr competition consists of three datasets containing isolated chinese characters , namely , casia - olhwdb 1.0@xmath91.2 ( db 1.0@xmath91.2 ) .",
    "the test set was released after the competition , which contains 3755 classes in gb1 .      in the experiments , we use nesterov momentum with momentum @xmath57=0.9 .",
    "the learning rate is set as 0.003 initially , and halved after the first iteration , and then exponentially decreases to 0.00001 .",
    "the training mini - batch size is 96 .",
    "we trained the dcnn for 70 epochs .      ''",
    "denotes the reduction of feature map size .",
    "the value following  drop \" means the dropout ratio . ]",
    "we first conducted experiment on db1.1 to evaluate the effectiveness of our proposed method .",
    "i.e , _ dropdistortion _ , ssmp@xmath56 and 3d signature .",
    "we designed a baseline dcnn with simple 0 - 1 bitmaps as input , and fixed the distortion rate @xmath58 .",
    "we evaluated the three techniques individually through the variable - controlling approach .",
    "the architecture of the baseline dcnn is input - 32c3 - mp2 - 64c3 - 96c3 - mp2 - 128c3 - 160c3 - mp2 - 192c3 - 224c3 - mp2 - 256c3 - output .    in order to evaluate the _ dropdistortion _",
    ", the baseline dcnn is trained with _",
    "dropdistortion_.    in order to evaluate the 3d signature features ,",
    "the baseline dcnn is trained with 2d signature and 3d signature .    for the evaluation of the ssmp and the @xmath53 drawing methods as described in section 4.1 , i.e. ssmp@xmath54 , ssmp@xmath55 and ssmp@xmath56 , two mp layers in the baseline dcnn is replaced with four ssmp layers with @xmath38=1.5 .",
    "refer to table 1 .",
    "the last convolutional layer is 256c2 instead of 256c3 because @xmath59 . in the test phase , repeatedly running the network produce a slightly different output each time because of ssmp .",
    "averaging these outputs can improve the accuracy .",
    "m0.4 cm < m0.4 cm < m0.4 cm < m0.4 cm < m0.8 cm < m0.4 cm < m0.8 cm < m0.4 cm < m0.8 cm < m0.4 cm < m0.8 cm < m0.4 cm < m0.4 cm < m0.4 cm",
    "< dimension&32&&64&96&&128&&160&&192&&224&&256 + baseline dcnn & c3 & mp2 & c3 & c3 & * mp2 * & c3 & ( ) & c3 & * mp2 * & c3 & ( ) & c3 & * mp2 * & c3 + ssmp dcnn & c3 & mp2 & c3 & c3 & * ssmp1.5 * & c3 & * ssmp1.5 * & c3 & * ssmp1.5 * & c3 & * ssmp1.5 * & c3 & * mp2 * & c2 +    the experimental results are presented in table 2 . the employed three techniques , namely _ dropdistortion _",
    ", 3d signature and ssmp@xmath56 , all improve the performance over the baseline and their corresponding counterparts .",
    "the 2d signature substantially improves the performance , while the 3d signature gives further improvement .",
    "ssmp greatly decreases the error rates by averaging 10 test scores , and the ssmp@xmath56 we proposed is better than ssmp@xmath54 and ssmp@xmath55 .",
    "_ dropdistortion _ decreases the error rate with little extra cost . as _ dropdistortion _ and ssmp can be used jointly with path signature , we conducted further experiments to evaluate the joint effects .",
    ".evaluation of the proposed three techniques : _ dropdistortion _ , ssmp and path signature ( test error rates , % ) [ cols=\"^,^,^\",options=\"header \" , ]      to achieve state - of - art results , we extended the baseline model to be deeper and wider . the overall architecture is shown in fig .",
    "the second ( 48c2 ) , fourth ( 64c2 ) and sixth layer ( 96c2 ) reduce the dimension in lower layers and act as regularization .",
    "the experimental results are presented in table 3 , where we can see _ dropdistortion",
    "_ always achieves the best performance , and a higher degree distortion is usually better than a lower degree distortion .",
    "_ dropdistortion _ brings about an obvious improvement when using bitmaps as inputs , as bitmaps only contains structural information and _ dropdistortion _ helps to learn the structure .",
    "compared to rendering online characters as bitmaps , the path signature greatly improves the recognition accuracy , which proves its effectiveness in information extraction . and",
    "the 3d signature also gives consistent improvement over the 2d signature .",
    "the ssmp averaging results are given in table 4 ( _ dropdistortion _ + 3d signature + ssmp@xmath56 ) . by averaging 10 test scores of the network ,",
    "the test error rates decrease significantly on both db1.1 and icdar2013 competition dataset .",
    "the best results for db1.1 and icdar2013 competition dataset are both produced by jointly using _ dropdistortion _ , 3d signature and ssmp , which results in relative error reduction of 36.3% ( = ( 4.24 - 2.70)/4.24 * 100% ) and 37.2% ( = ( 3.20 - 2.01)/3.20 * 100% ) respectively .",
    "m2.4 cm < m2.5 cm < m2.5 cm < m2.5 cm < & & + & & fixed 0.1&fixed 0.3&dropdistortion : 0.3@xmath300.2@xmath300.1 + & bitmap&4.24&4.18&4.08 + db 1.1&2d signature&3.10&3.02&2.99 + & 3d signature&3.01&2.95&2.92 + & bitmap&3.20&3.25&3.13 + icdar2013&2d signature&2.32&2.27&2.24 + & 3d signature&2.30&2.26&2.21 +    m3 cm < m1 cm < m1 cm < m1 cm < m1 cm < m1 cm < m1 cm < m1 cm < m1 cm < m1 cm < m1.2 cm < database&1 test&2 tests&3 tests&4 tests&5 tests&6 tests&7 tests&8 tests&9 tests&10 tests + db 1.1 & 2.92 & 2.82 & 2.78 & 2.75 & 2.75 & 2.73 & 2.71 & 2.70 & 2.70 & 2.70 + icdar2013 & 2.21 & 2.11 & 2.09 & 2.08 & 2.05 & 2.05 & 2.03 & 2.03 & 2.03 & 2.01 +      we conducted further experiments on db 1.0 , and in table 5 we compared our method with some state - of - the - art achieved for hccr in previous studies . it can be seen from table 5 that our method has achieved the highest recognition accuracies for all the three datasets , showing the effectiveness of the proposed approach .    our dcnn architecture in fig . 7",
    "follows that of deepcnet and fmp network , e.g. 2@xmath22 convolutional kernels , increasing kernel numbers and stacked ssmp layers , but it is no deeper than the network in @xcite",
    ". domain knowledge and _ dropsample _ can be adapted into our approach to give further improvement , but this is beyond the scope of this paper .",
    "8-directmap contains directional information , which is equivalent to the first level of path signature@xcite .",
    "higher levels of path signature we used here contain richer information and can further improve the performance .",
    "8 demonstrates some randomly chosen misclassified samples , from which we can discern that the misclassified samples are usually illegible .",
    "some are even mislabeled or wrongly written .",
    "m3 cm < c c m2 cm <",
    "methods&db 1.0&db 1.1&icdar2013 competition + deepcnet & & & + @xcite&[0pt]&[0pt]96.42&[0pt]97.39 + domain knowledge & & & + @xcite&[0pt]97.20&[0pt]96.87&[0pt]97.20 + fmp ensemble & & & + @xcite&[0pt]&[0pt]97.03&[0pt] + dropsample & & & + @xcite&[0pt]97.33&[0pt]97.06&[0pt]97.51 + directmap+convnet & & & + @xcite&[0pt]&[0pt]&[0pt]97.55 + our method 1 test&97.5&97.08&97.79 + & & & + [ 0pt]our method 10 tests&[0pt]*97.67*&[0pt]*97.30*&[0pt]*97.99 * +",
    "this paper presents several new techniques for online hccr that effectively boost the recognition accuracy .",
    "we proposed a simple but effective character distortion method called _ dropdistortion _ , which improves the recognition accuracy with little additional computational cost .",
    "path signature acts as an effective feature representation for online characters , and the ssmp layers in our dcnn perform feature map distortion and model averaging .",
    "experiments on casia - olhwdb 1.0 , casia - olhwdb 1.1 and the icdar2013 online hccr competition dataset achieved new state - of - the - art accuracies of 97.67% , 97.30% and 97.99% , respectively . compared with previous best result @xcite on the icdar2013 competition dataset , our method has achieved an error reduction of 17.9% , showing the effectiveness of the proposed approach .",
    "although we mainly focus on online hccr , the proposed _",
    "method is expected to serve as a general technique in many other tasks in machine learning , such as image classification .",
    "the _ dropdistortion _",
    "training strategy was applied in a simple three - step way in this paper , and in future work it is of great worth to investigate self - adaptive _ dropdistortion _ , where the distortion degree could be automatically adjusted .",
    "25 natexlab#1#1[1]`#1 ` [ 2]#2 [ 1]#1 [ 1]http://dx.doi.org/#1 [ ] [ 1]pmid:#1 [ ] [ 2]#2 , , . , in : , pp . .",
    ", , , , . .",
    ", , , , . .",
    ", , , , . . ,",
    ", , , . , in : , pp . .",
    ", , , , . . ,",
    ", , . , in : , pp . .",
    ", , , , . , in : , pp . .",
    ", , , . , in : , p.  .",
    ", , . , in : , pp . .",
    ", , , , , , , , , . , in : , pp . .",
    ", , , , . .",
    ", , , a. , in : , pp . .",
    ", , , a. . , .",
    ", , , , , b. . , .",
    ", , , , b. , in : , pp . . , , , ,",
    ", in : , pp . . , , , , , . .",
    ", , , . . ,"
  ],
  "abstract_text": [
    "<S> this paper presents an investigation of several techniques that increase the accuracy of online handwritten chinese character recognition ( hccr ) . </S>",
    "<S> we propose a new training strategy named _ dropdistortion _ to train a deep convolutional neural network ( dcnn ) with distorted samples . </S>",
    "<S> _ dropdistortion _ gradually lowers the degree of character distortion during training , which allows the dcnn to better generalize . </S>",
    "<S> path signature is used to extract effective features for online characters . </S>",
    "<S> further improvement is achieved by employing spatial stochastic max - pooling as a method of feature map distortion and model averaging . </S>",
    "<S> experiments were carried out on three publicly available datasets , namely casia - olhwdb 1.0 , casia - olhwdb 1.1 , and the icdar2013 online hccr competition dataset . </S>",
    "<S> the proposed techniques yield state - of - the - art recognition accuracies of 97.67% , 97.30% , and 97.99% , respectively . </S>",
    "<S> +    online handwritten chinese character recognition , deep convolutional neural network , spatial stochastic max - pooling , character distortion , path signature , </S>"
  ]
}