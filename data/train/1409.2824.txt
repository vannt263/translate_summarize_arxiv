{
  "article_text": [
    "we wish to model the occurrence of pairs of discrete symbols @xmath0 from a finite set , or predict the occurrence of symbol @xmath1 given that the other symbol is @xmath2 . these pairs might be tuples of @xmath3 purchase events , or a stream of @xmath4 gameplay events .",
    "from such a model , a recommender system can be tailored around the conditional probability of item or game @xmath1 , given user @xmath2 .",
    "alternatively , these tuples might be @xmath5 bigrams in a simple language model .",
    "if there are @xmath6 and @xmath7 of each symbol , their discrete density can be fully modelled as a multinomial distribution with @xmath8 normalized counts , one for each pair . in practice , data is typically sparse compared to large symbol vocabulary sizes , with @xmath9 and @xmath10 in tasks considered in this paper , and this prevents the full multinomial from generalizing : from user @xmath2 watching only one movie @xmath1 , we would like to infer the odds of her viewing other movies @xmath11 .",
    "this necessitates more compactly parameterized models , which commonly associate real - valued vectors @xmath12 and @xmath13 ( where @xmath14 ) with user @xmath2 and item @xmath1 , and draws on an energy @xmath15 to couple them .",
    "this paper proposes a new approach to modelling the occurrence of pairs of symbols , and makes two main contributions .",
    "first , pairwise data is modelled through a simple selection process followed by a preference function that censors the data : in the generative process , pairs @xmath0 are presented to a censor at a basic rate , which then chooses to include them in the stream of data with odds that depend only on the coupling energy @xmath15 .",
    "inference is based on the well - founded principle of variational bounding .",
    "second , we show how a scalable procedure can be obtained by using novel looser site - independent bounds .    to see why scalability is a challenge , consider the bilinear softmax distribution @xmath16 whose normalizing constant sums over all @xmath8 discrete options .",
    "when @xmath2 is given , @xmath17 defines softmax regression , the multi - class extension of logistic regression .",
    "the bilinear softmax function poses a practical difficulty : the large sums from the normalizing constant appear in the likelihood gradient through @xmath18 , where @xmath19 requires a sum over all @xmath20 pairs in its normalizer . on observing a pair @xmath0 ,",
    "the likelihood is increased by pulling @xmath21 towards @xmath22 , while simultaneously pushing it further from all other @xmath23 .",
    "there were recent approaches to using the softmax function at scale .",
    "mnih and teh @xcite used noise contrastive estimation @xcite to estimate the expensive softmax gradients when training neural probabilistic language models , which improves on using importance sampling for gradient estimation @xcite . in a different approach , the normalization problem can be addressed by redefining @xmath17 as a tree - based hierarchy of smaller softmax functions ; this has a direct application to implicit - feedback collaborative filtering @xcite .",
    "alternatively , modelling can be done by formulating a simpler objective function based on a classification likelihood , and including stochastically `` negative sampled '' pairs during optimization .",
    "this was done for skip - gram models that consider @xmath5 pairs @xcite , and for @xmath3 pairs @xcite , where the latter work assumed that each pair can appear at most once .",
    "there additionally exists a large body of tangential work , which models an i.i.d .",
    "observation _ given _ @xmath2 and @xmath1 , instead of doing density estimation as described above .",
    "these include the stochastic block model and its extensions for binary matrices or graphs @xcite and the family of `` probabilistic matrix factorization '' models for a variety of likelihood functions for the observation @xcite .",
    "the restriction of each pair to appearing at most once places us in the domain of one - class matrix completion @xcite , where modelling is typically done by formulating different loss functions over the absent pairs ( or missing values in the matrix ) . in these set - ups ,",
    "a cost value is typically associated with each possible pair .",
    "these can be predefined @xcite or optimized for @xcite .",
    "this paper has large - scale collaborative filtering and recommender systems in mind , and places two requirements on the model and inference procedure that do not coexist in other work .",
    "crucially , _ inference should scale with @xmath24 _ , the size of the dataset , i.e.  the number of observed pairs , and not with @xmath20 , the number of possible pairs .",
    "we prefer a bayesian approach that incorporates parameter uncertainty in our inference .",
    "this is particularly useful when data is scarce ; if game @xmath1 was played by a handful of users , its lack of usage should be reflected in the posterior estimate of parameters associated with @xmath1 .",
    "to fulfil these requirements , we borrow an unorthodox idea from @xcite , which views the stream of data as a censored one .",
    "their perspective is that of a one - class model , which contrasts the observations against an unobserved _ `` negative background '' _ , although unlike @xcite , a pair @xmath0 can repeatedly be observed . in section [ sec :",
    "scalable ] , this `` negative background '' will be employed in various caches as part of the inference pipeline .",
    "our approach practically improves on that of @xcite , where the data set size was effectively doubled as the non - revealed stream was stochastically resampled and averaged over .",
    "as the `` non - revealed half '' continually changed due to resampling , the inference procedure also did not comfortably map to a distributed architecture . because exact inference in our model is not possible , we resort to approximating the parameter posterior via a variational lower bound . in this",
    "variational bayes setting , with a fully factorized posterior approximation , the bound is iteratively maximized through closed - form updates of each factor .",
    "the updates are in terms of natural gradients , and are embarrassingly parallel . empirically , our approach achieves state of the art results on two large - scale recommendation problems ( section [ sec : eval ] ) .",
    "a pair @xmath0 will be represented as a pair @xmath25 of binary indicator vectors , where only bits @xmath2 and @xmath1 are `` on '' in @xmath26 and @xmath27 respectively .",
    "we shall model the data stream by appending a binary variable @xmath28 ( true ) to each pair : we _ did _ observe that symbols @xmath2 and @xmath1 co - occurred , user @xmath2 played game @xmath1 today , and so on . we therefore observe a stream of @xmath24 pairs , which takes the form @xmath29 .",
    "the censored approach assumes that there were a number of pairs that did not surface in the data stream , such that @xmath30 ( false ) .",
    "_ we do not know which pairs and how many they were _ , but in practice we will allow the length of the censored stream be specified as a hyperparameter @xmath31 , and assume that @xmath32 is additionally provided .",
    "let data @xmath33 denote all observations .",
    "the ratio @xmath34 can be seen as a pre - specified positive to negative class ratio ; various settings of @xmath35 in @xmath36 are investigated in section [ sec : eval ] .",
    "the censored stream constitutes the `` negative background '' against which the energy @xmath15 will be fit , and it plays a role similar to that of the softmax normalizer in the gradient of @xmath37 from ( [ eq : softmax ] ) : on observing a pair @xmath0 , @xmath21 is pulled towards @xmath22 and pushed further from all other @xmath23 .",
    "we additionally associate real - valued biases @xmath38 ( and @xmath39 ) with each user and item , modifying the energy to @xmath40 .",
    "they play a useful interpretive role in distinguishing between polarizing and non - polarizing content in a recommender system : content that appeals to a wide range of tastes is described by a @xmath22 with smaller norm , and their appeal is modelled by a positive taste - independent bias .",
    "polarizing content is described by a large - normed @xmath22 and a negative taste - independent bias ; it is only enjoyed by a narrow sliver of tastes .",
    "we propose a model which combines popularity - based selection with a personalized preference function to model @xmath0 .",
    "* 1 . *  in a _ selection step _ a user @xmath2 is chosen with probability @xmath41 , and an item @xmath1 is chosen with probability @xmath42 .",
    "* 2 . *  in a _ censoring step _",
    "the pair @xmath0 is observed with probability @xmath43 and censored with probability @xmath44 , where @xmath45 is the logistic function .",
    "= 1    = [ circle , thick , draw = black!100 , fill = black!20 , minimum size = 7 mm ] = [ circle , thick , draw = black!100 , fill = red!0 , minimum size = 7 mm ] = [ circle , draw , fill = black!100 , minimum width = 3pt , inner sep = 0pt ] = [ black!100 ]    \\(t ) @xmath46 ; ( f ) [ right of = t ] @xmath47 ; ( zo ) [ above of = t ] @xmath48 edge [ post ] ( t ) ; ( yo ) [ left of = zo ] @xmath49 edge [ post ] ( t ) ; ( yh ) [ above of = f ] @xmath50 edge [ post ] ( f ) ; ( zh ) [ right of = yh ] @xmath51 edge [ post ] ( f ) ; ( pi ) [ above of = zo ] @xmath52 ; ( pi ) to[out=220,in=90 ] ( yo ) ; ( pi ) to[out=320,in=90 ] ( yh ) ; ( psi ) [ above of = yh ] @xmath53 ; ( psi ) to[out=220,in=90 ] ( zo ) ; ( psi ) to[out=320,in=90 ] ( zh ) ; ( bu ) [ below of = t ] @xmath38 edge [ post ] ( t ) edge [ post ] ( f ) ; ( -1.5,-1.5 ) .. controls ( -1.5,-0.5 ) and ( 0,-1.3 ) .. ( 0,-0.6 ) ; ( -1.5,-1.5 ) .. controls ( -1.5,-0.5 ) and ( 0,-1.3 ) .. ( 0.9,-0.6 ) ; ( u ) [ left of = bu ] @xmath21 ; ( v ) [ below of = f ] @xmath22 edge [ post ] ( f ) edge [ post ] ( t ) ; ( 3.0,-1.5 ) .. controls ( 3.0,-0.5 ) and ( 1.5,-1.3 ) .. ( 1.5,-0.6 ) ; ( 3.0,-1.5 ) .. controls ( 3.0,-0.5 ) and ( 1.5,-1.3 ) .. ( 0.6,-0.6 ) ; ( bv ) [ right of = v ] @xmath39 ; ( -2.0,-0.3 ) node @xmath24 ; ( 3.5,-0.3 ) node @xmath31 ; ( -2.0,-2.0 ) node @xmath6 ; ( 3.5,-2.0 ) node @xmath7 ;    background ( -2.3cm,2.3 cm ) rectangle ( 0.6cm,-0.6 cm ) ( 0.9cm,2.3 cm ) rectangle ( 3.8cm,-0.6 cm ) ( -2.3cm,-0.9 cm ) rectangle ( 0.6cm,-2.3 cm ) ( 0.9cm,-0.9 cm ) rectangle ( 3.8cm,-2.3 cm ) ;    _ figure here _",
    "let @xmath54 and @xmath55 denote all bilinear parameters and @xmath56 denote biases , with @xmath57 .",
    "lastly @xmath58 includes multinomial parameters @xmath52 and @xmath53 .",
    "the generative process is illustrated in figure [ fig : graphicalmodel ] , and is as follows : draw parameters @xmath59 from their prior distributions ( given explicitly below ) .",
    "repeat drawing pairs @xmath0 with indexes drawn from @xmath60 and @xmath61 and observe the pairs with probability @xmath43 .",
    "@xmath24 such pairs are seen , while we assume that @xmath31 , the number of censored data points , is specified as a hyperparameter .",
    "the density of an uncensored data point @xmath62 is therefore @xmath63^{y_{di } z_{dj } } , \\ ] ] while @xmath64 is the odds of censoring pair @xmath65 if its indexes were known .",
    "the censored indexes @xmath50 and @xmath51 are unknown ; by including their prior and marginalizing over them , @xmath66 is a mixture of @xmath67 components .    the joint density of @xmath68 and the unobserved variables @xmath69 depends on further priors on @xmath59 , for which we choose dirichlet priors for @xmath70 and @xmath71 .",
    "the other priors are fully factorized gaussians , with @xmath72 and @xmath73 and , with some overloaded notation , @xmath74 .",
    "the hierarchical model could be extended further with gamma hyperpriors on the various gaussian precisions @xmath75 , or normal - wishart hyperpriors on both of the gaussian parameters @xcite .",
    "if the symbols @xmath2 and @xmath1 were accompanied by meta - data tags , these could also be incorporated into the bayesian model @xcite . for the sake of clarity ,",
    "we omit these additions in this paper .",
    "the joint density decomposes as @xmath76^{\\sum_{d ' } y_{d ' i } z_{d ' j } } \\nonumber \\\\   & \\quad \\cdot \\prod_i \\pi_i^{c_i + \\sum_{d ' } y_{d ' i } } \\cdot \\prod_j \\psi_j^{c_j + \\sum_{d ' } z_{d ' j } } \\cdot p(\\u ) \\ , p(\\v ) \\ , p(\\b ) \\ ,",
    "p(\\bpi ) \\ , p(\\bpsi ) \\ , \\label{eq : joint}\\end{aligned}\\ ] ] where the uncensored data likelihood was regrouped using observation counts @xmath77 for each pair @xmath0 , and marginal counts @xmath78 and @xmath79 .",
    "note that @xmath80 .",
    "marginalizing @xmath81 over @xmath82 gives a mixture of @xmath83 components , each representing a different way of assigning @xmath31 indistinguishable @xmath47 s to @xmath67 distinguishable bins , or assigning nonnegative counts @xmath84 with @xmath85 to a `` negative class count matrix '' . at first glance of ( [ eq : joint ] ) , it would seem as if inference would still scale with @xmath67 , and that we have done nothing more than match the bilinear softmax s @xmath86 computational burden through the introduction of @xmath31 .",
    "the following sections are devoted to developing a variational approximation , and with it a practically scalable inference scheme that relies on various `` negative background '' caches .",
    "to find a scalable yet bayesian inference procedure , we approximate @xmath87 with a factorized surrogate density @xmath88 , found by maximizing a variational lower bound to @xmath89 @xcite .",
    "first , we lower - bound each logistic function in ( [ eq : joint ] ) by associating a parameter @xmath90 with it @xcite . dropping subscripts , each bound would be @xmath91 , where the lower bound on @xmath92 is that of @xmath93 above .",
    "the bound depends on the deterministic function @xmath94 $ ] .",
    "let @xmath95 denote the set of logistic variational parameters , and substitute the bound into ( [ eq : joint ] ) to get @xmath96 .",
    "our variational objective @xmath97 $ ] , as a function of @xmath98 and functional of @xmath99 , follows from @xmath100 \\",
    ", \\label{eq : l}\\ ] ] which will be maximized with respect to @xmath99 and @xmath98 .",
    "the factorization of @xmath99 employed in this paper is @xmath101 the factors approximating each symbol s features in @xmath102 , @xmath103 , and @xmath104 are chosen to be a gaussian , for example @xmath105 .",
    "the approximating factors @xmath106 and @xmath107 are both dirichlet , for example @xmath108 .",
    "the bound in ( [ eq : l ] ) is stated fully in appendix [ sec : bounddetails ] .    for the purpose of obtaining a scalable algorithm , the most important parameterizations are for the categorical ( discrete ) factors @xmath109 and @xmath110 .",
    "we shall argue and show in sections [ sec : scalable ] and [ sec : eval ] that choosing @xmath111 is desired , and as @xmath31 is potentially large , the _ parameters of @xmath109 will be tied_. _ this tying of parameters is the key to achieving a scalable algorithm . _ we let all @xmath109 share the same parameter vector @xmath112 on the probability simplex , such that @xmath113 for all @xmath65 .",
    "similarly , all @xmath110 share probability vector @xmath114 , such that @xmath115 for all @xmath65 .    [ [ making - and - trading - predictions ] ] making and trading predictions + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    our original desideratum was to infer the probability of symbol @xmath1 , conditional on the other symbol being @xmath2 , and the observed data .",
    "bayesian marginalization requires us to average the predictions over the model parameter posterior distribution .",
    "here it is an analytically intractable task , which we approximate by using @xmath99 as a surrogate for the true posterior .",
    "firstly , @xmath116 if @xmath117 . the random variable @xmath118 was defined as @xmath119 , with its density approximated with its first two moments under @xmath99 , i.e.  @xmath120 $ ] and @xmath121 $ ] . with @xmath122 , the final approximation of a logistic gaussian integral follows from @xcite . again using @xmath99 ,",
    "the posterior density of symbol @xmath1 , provided that the first symbol is @xmath2 , is approximately proportional to ( writing `` @xmath46 '' for `` @xmath28 '' for brevity ) @xmath123 \\   .",
    "\\label{eq : prediction}\\ ] ] hence @xmath124 \\big/ \\sum_{j ' } \\ebb_q [ \\psi_{j ' } ] \\ ,",
    "\\sigma ( x_{ij'})$ ] , normalizing to one .",
    "a scalable update procedure for the factors of @xmath88 is presented in this section , culminating in algorithm [ alg ] .",
    "the algorithm optimizes over @xmath125 loops , but can also be run until complete convergence as the evidence lower bound @xmath126 from ( [ eq : l ] ) can be explicitly calculated .",
    "we use * _ _ p__for * to indicate embarrassingly parallel loops , although the updates for @xmath112 , @xmath114 , and @xmath127 also make extensive use of parallelization .",
    "let graph @xmath128 be the sparse set of all observed pair indexes .",
    "as there are @xmath20 logistic variational parameters @xmath90 , we shall divide them into two sets , those with indexes in @xmath129 , and those without .",
    "therefore @xmath90 shall be optimized for when @xmath130 , while the @xmath90 s shall _ share the same parameter value @xmath127 _ for @xmath131 . even though the form of ( [ eq : joint ] ) suggests that we would need two versions of @xmath90 , one for the bounded @xmath132-term , and one its opposite , this is not required , as the optimization of the bound is symmetric",
    "when @xmath90 maximizes @xmath126 on the bounded @xmath132-term , it simultaneously maximizes @xmath126 on the bounded @xmath133-term .",
    "we ll use the shorthand @xmath134 for @xmath130 ; similarly , @xmath135 denotes @xmath136 when @xmath131 .",
    "the updates for symbols @xmath2 and @xmath1 s parameters mirror each other , and only the `` user updates '' are laid out in this section .    * input : * @xmath68 ( or @xmath31 and all non - zero @xmath137 ) , @xmath138 , @xmath139 , @xmath140 , @xmath141 , @xmath142 * initialize : * @xmath143 , @xmath144 $ ] , @xmath145 $ ]    [ [ gaussian - updates - for - qu_ik ] ] gaussian updates for @xmath146 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we will present here a bulk update of @xmath147 , which is faster than sequentially maximizing @xmath97 $ ] with respect to each of them in turn .",
    "we first solve for the maximum of @xmath126 with respect to a full gaussian ( not factorized ) approximation @xmath148 .",
    "the fully factorized @xmath146 can then be recovered from the intermediate approximation @xmath149 as those that minimize the kullback - leibler divergence @xmath150 : this is achieved when the means of @xmath146 match that of @xmath149 , while their precisions match the diagonal precision of @xmath149 .",
    "the validity of the intermediate bound in proved in appendix [ sec : traitupdates ] .",
    "the updates rely on careful caching , which we ll first illustrate through @xmath151 s precision matrix .",
    "@xmath126 is maximized when @xmath149 has as natural parameters a precision matrix @xmath152 + \\ , \\overbrace{\\sum_{d ' } \\sum_{j } \\ebb_q[y_{d ' i } \\ , z_{d ' j } ] \\cdot   2 \\lambda_{ij } \\cdot \\ebb_{q } \\big [ \\v_j \\v_j^t \\big]}^{\\mathrm{(a ) } } \\ , + \\",
    ", \\tau_u \\i\\ ] ] and a mean - times - precision vector @xmath153 , which we will state later . looking at @xmath154 in ( [ eq : pi ] ) , an undesirable sum over all @xmath65 and @xmath1",
    "is required in @xmath155 .",
    "we endeavoured that the update would be _ sparse _ , and only sum over observed indexes in @xmath156 .",
    "the benefit of the shared variational parameters now becomes apparent .",
    "with @xmath157 = s_i t_j$ ] and @xmath158 when @xmath131 , the sum in @xmath155 decomposes as @xmath159 + s_i d ' \\cdot 2 \\lambda^ { * } \\cdot \\overbrace{\\sum_j t_j \\ebb_{q } \\big [ \\v_j \\v_j^t \\big]}^{\\textrm{negative background $ \\p_{\\ominus}$ } }   \\ .\\ ] ] barring the `` negative background '' term , only a sparse sum that involves observed pairs is required .",
    "this background term is rolled up into a global _ item_-background cache , which is computed once before updating all @xmath146 . throughout the paper",
    ", the @xmath160 symbol will denote an item - background cache .",
    "the cache @xmath161 $ ] is used in each precision matrix update , for example @xmath162 + \\tau_u \\i \\ . \\ ] ] we ve deliberately laboured the above decomposition of an expensive update into a background cache and a sparse sum over actual observations , as it serves as a template for other parameter updates to come . turning to the mean - times - precision vector @xmath163 of @xmath149 , we find that @xmath164 .",
    "\\label{eq : mp}\\ ] ] there is a subtle link between ( [ eq : mp ] ) and the gradients of the bilinear soft - max likelihood , which we ll explore in the next paragraph . to find @xmath153 ,",
    "two additional caches are added to the item - background cache , and are computed once before any @xmath146 updates .",
    "they are @xmath165 \\ebb_{q } [ \\v_j ] $ ] and @xmath166 $ ] .",
    "the final mean - times - precision update is @xmath167 \\right ) \\m_{\\ominus}^{\\ddagger }   - 2 \\lambda^ { * } \\m_{\\ominus}^{\\dagger } \\right ] \\nonumber \\\\ & \\qquad + \\sum_{j \\in \\gcal(i ) } \\big ( c_{ij } \\left ( \\tfrac{1}{2 } - 2 \\lambda_{ij } \\",
    ", \\ebb_{q } \\big [ b_i   + b_j \\big ] \\right )   - s_i t_j d ' \\cdot 2 ( \\lambda_{ij } - \\lambda^ * ) \\ , \\ebb_{q } \\big [ b_i+ b_j \\big ] \\big ) \\ebb_{q } \\big [ \\v_j \\big ] \\",
    ", \\label{eq : mtp - update}\\end{aligned}\\ ] ] and again only sums over @xmath168 and not all @xmath7 .",
    "there are of course additional variational parameters @xmath90 , and they are computed and discarded when needed according to ( [ eq : locallogistic ] ) .",
    "[ [ bilinear - softmax - gradients ] ] bilinear softmax gradients + + + + + + + + + + + + + + + + + + + + + + + + + +    the connection between this model and a bilinear softmax model can be seen when the biases are ignored .",
    "consider the gradient of @xmath126 with respect to _",
    "parameter @xmath169 , @xmath170 - d ' \\sum_{j } s_i t_j \\ebb_q \\big[\\v_j \\big ]",
    "\\bigg ) \\ .\\ ] ] the gradient @xmath171 is zero at ( [ eq : mp ] ) , which was stated , together with ( [ eq : pi ] ) , in terms of _ natural _ parameters .",
    "as @xmath172 is quadratic , it can be exactly maximized ; furthermore , the maximum with respect to @xmath154 is attained at the negative hessian @xmath173 , given in ( [ eq : pi ] ) .",
    "the curvature of the bound , as a function of @xmath169 , directly translates into our posterior approximation s uncertainty of @xmath21 .",
    "the log likelihood of a softmax model would be @xmath174 , with the likelihood of each pair defined by ( [ eq : softmax ] ) .",
    "the gradient of the log likelihood is therefore @xmath175 with weights @xmath176 that sum to one over all @xmath67 options . the weights in ( [ eq : this - vs - softmax ] )",
    "were simply @xmath177 , and also sum to one over all options .",
    "the difference between ( [ eq : this - vs - softmax ] ) and ( [ eq : softmaxgradfull ] ) is that @xmath177 is used as a _",
    "factorized substitute _ for @xmath178 .",
    "this simplification allows the convenience that none of the updates described in section [ sec : scalable ] need to be stochastic , and substitute functions , as employed by noise contrastive divergence to maximize @xmath179 , are not required .",
    "( the hessian @xmath180 contains a _",
    "double - sum _ over indexes @xmath1 . ) considering the two equations above , one might expect to set hyperparameter @xmath31 to @xmath181 , and in section [ sec : eval ] we show that this is a reasonable choice .    [",
    "[ gaussian - updates - for - qb_i ] ] gaussian updates for @xmath182 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the maximum of @xmath126 with respect to @xmath182 _ re - uses _ cache @xmath183 , but requires the additional cache @xmath184 $ ] to be precomputed .",
    "gaussian @xmath182 has a mean - times - precision parameter @xmath185",
    "\\m_{\\ominus}^{\\ddagger } ) )   + \\sum_{j \\in \\gcal(i ) } \\big ( c_{ij } ( \\frac{1}{2 } - 2 \\lambda_{ij } \\ ,",
    "\\ebb_{q } [ \\u_i^t \\v_j   + b_j ] )   -   s_i t_j d ' \\cdot 2 ( \\lambda_{ij } - \\lambda^ * ) \\ , \\ebb_{q } [ \\u_i^t \\v_j +   b_j ] \\big ) $ ] , and its precision parameter @xmath186 follows a similar form .",
    "[ [ logistic - bound - parameter - updates ] ] logistic bound parameter updates + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    as discussed above , the logistic bound parameters @xmath90 associated with observations @xmath130 are treated individually whilst the remainder are shared and denoted by @xmath127 . the individually optimized bound @xmath187\\ ]",
    "] can be used anytime during the updates and then discarded ( we always use the positive root for @xmath90 ) .",
    "the shared parameter can be written in terms of cached quantities and a sum that scales with @xmath24 ( the user - background cache is denoted with a @xmath188 symbol , and mirrors the item - background cache ) : @xmath189 where @xmath190 .",
    "cache @xmath191 $ ] also plays a role in the categorical updates .",
    "[ [ dirichlet - updates ] ] dirichlet updates + + + + + + + + + + + + + + + + +    as the multinomial distribution is conjugate to a dirichlet , its updates have a particularly simple form .",
    "@xmath106 is dirichlet @xmath192 with parameters @xmath193 .",
    "each pseudo - count adds @xmath194 , the number of views for user @xmath2 , to the expected number of views that were censored and not made .    [",
    "[ categorical - updates ] ] categorical updates + + + + + + + + + + + + + + + + + + +    there are @xmath31 categorical ( discrete ) factors @xmath109 , and the key to finding a scalable inference procedure lies in tying all their parameters together in @xmath112 , with @xmath195 . looking at the second line of ( [ eq : joint ] ) , the factors depend on the expected bounded logistic functions @xmath196 - \\xi_{ij}^2 \\big )   - \\frac{\\xi_{ij}}{2 } - \\frac{1}{2 } \\ebb_q[\\u_i^t \\v_j + b_i + b_j ] \\ .\\end{aligned}\\ ] ] the categorical parameters are , if we solve for all the @xmath31 tied distributions @xmath109 _ jointly _ , @xmath197 + \\sum_j t_j \\omega_{ij } \\big ) }   \\ .\\ ] ] in practice , each entry @xmath198 can be computed in parallel ; afterwards , they are renormalized to give @xmath112 . to find @xmath112 ,",
    "an efficient way is needed to determine @xmath199 , and this can again be done with careful bookkeeping .",
    "the observed terms @xmath200 are treated differently from the rest . for observed terms we can use the optimal logistic parameters in ( [ eq : locallogistic ] ) to simplify @xmath201 $ ] . by denoting @xmath202 evaluated with the shared parameter @xmath127 by @xmath203 ,",
    "we can write @xmath204 . the first term scales with @xmath24 and the second term can be written using cached quantities : @xmath205 \\p_{\\ominus } + 2 \\ebb_q [ b_i \\u_i^t ] \\m_{\\ominus}^{\\ddagger }    + 2 \\ebb_q [ \\u_i^t ] \\m_{\\ominus}^{\\dagger } + \\ebb_q[b_i^2 ] + 2 \\ebb_q[b_i ] \\nu_{\\ominus } + \\varkappa_{\\ominus } ) + \\log \\sigma(\\xi^ * )   + \\frac { ( \\xi^*)^2}{2 } \\lambda^ *    - \\frac{\\xi^*}{2 } - \\frac{1}{2 } ( \\ebb_q[\\u_i^t ] \\m_{\\ominus}^{\\ddagger } + \\ebb_q[b_i ] + \\nu_{\\ominus } ) $ ] .",
    "[ t ]    = 1   in ( [ eq : rank ] ) , averaged over users and grouped logarithmically by @xmath194 .",
    "the _ top _",
    "evaluation is on the _ xbox movies _",
    "sample , while the _ bottom _ one is on the `` implicit feedback '' _ netflix ( 4 and 5 stars ) _ set .",
    ", title=\"fig:\",scaledwidth=48.0% ]   in ( [ eq : rank ] ) , averaged over users and grouped logarithmically by @xmath194 .",
    "the _ top _",
    "evaluation is on the _ xbox movies _ sample , while the _ bottom _ one is on the `` implicit feedback '' _ netflix ( 4 and 5 stars ) _ set . ,",
    "title=\"fig:\",scaledwidth=48.0% ] _ figure here _    a key application for modelling paired @xmath3 symbols is large - scale recommendation systems , and we evaluate the predictions obtained by ( [ eq : prediction ] ) on two large data sets .. ] the _ xbox movies _ data is a sample of @xmath206 views for @xmath207 users on a sub - catalogue of around @xmath208 movies @xcite . to evaluate on data known in the machine learning community",
    ", the four- and five - starred ratings from the netflix prize data set were used to simulate a stream of `` implicit feedback '' @xmath3 pairs in the _ netflix ( 4 and 5 stars ) _ data .",
    "we refer the reader to @xcite for a complete data set description . for each user , one item",
    "was randomly removed to create a test set . to mimic a real scenario in the simplest possible way ,",
    "each user s non - viewed items were ranked , and the position of the test item noted .",
    "we are interested in the rank of held out item @xmath209 for user @xmath2 on a @xmath210 $ ] scale , @xmath211 \\ , \\ ] ] where @xmath212 indicates the score given by ( [ eq : prediction ] ) or any alternative algorithm .    in figure",
    "[ fig : averagerank ] , we facet the average rank by @xmath194 , the number of movie views per user .",
    "as the evaluation is over 6 million users , this gives a more representative perspective than reporting a single average .",
    "apart from ranking by _ popularity _",
    "@xmath213 , which would be akin to only factorizing with @xmath177 , we compare against two other baselines .",
    "_ bpr - uniform _ and _ bpr - pop _ represent different versions of the bayesian personalized ranking algorithm @xcite , which optimizes a rank metric directly against either the data distribution of items ( _ bpr - uniform _ , with missing items are sampled uniformly during stochastic optimization ) , or a tilted distribution aimed at personalizing recommendations regardless of an item s popularity ( _ bpr - pop _ , with missing items sampled proportional to their popularity ) .",
    "their hyperparameters were set using cross - validation .",
    "for the random graph model @xcite , rankings are shown with pure personalization ( _ rg - like _ ) and with an item popularity rate factored in ( _ rg - pop*like _ ) . the comparison in figure [ fig : averagerank ]",
    "is drawn using @xmath214 dimensions , @xmath181 and hyperparameters set to one . for _ xbox movies _ , the model outperforms all alternatives that we compared against .",
    "_ bpr - uniform _ , optimizing ( [ eq : rank ] ) directly , performs slightly better on the less sparse netflix set ( the xbox usage sample is much sparser , as it is easier to rate many movies than to view as many ) . for _ xbox movies _ , updating all item - related parameters in algorithm [ alg ] took 69 seconds on a 24-core ( intel xeon 2.93ghz ) machine , and updating all user - related parameters took 83 seconds .    [ t ! ]    = 1   in ( [ eq : rank ] ) , grouped logarithmically by @xmath194 , for varying values of @xmath35 in @xmath215 .",
    ", title=\"fig:\",scaledwidth=48.0% ]   in ( [ eq : rank ] ) , grouped logarithmically by @xmath194 , for varying values of @xmath35 in @xmath215 . , title=\"fig:\",scaledwidth=48.0% ] _ figure here",
    "_    [ t ]    = 1      _ figure here _    we surmised in section [ sec : scalable ] that @xmath181 is a reasonable hyperparameter setting , and figure [ fig : d - values ] validates this claim .",
    "the figure shows the average held - out rank on the _ netflix ( 4 and 5 ) _",
    "set for various settings of @xmath31 through @xmath216 .",
    "the average rank improves beyond @xmath217 , but empirically slowly decreases beyond @xmath218 . to provide insight into the _",
    "`` censoring '' _ step , figure [ fig : sigma ] accompanies figure [ fig : d - values ] , and shows the empirical density of the bernoulli variable @xmath219 for held - out items @xmath209 .",
    "we break the empirical density down over users that appear in @xmath220 pairs . given",
    "that the held - out pairs were observed , the bernoulli variable should be _ true _ , and the density of @xmath219 shifts right as @xmath194 becomes bigger .",
    "the effect of having to explain less ( @xmath221 ) or more ( @xmath222 ) censored pairs is also visible in the figure .",
    "there is also a slight benefit in increasing @xmath223 .",
    "the average rank @xmath224 for @xmath214 is 0.9649 , using @xmath217 .",
    "an increased latent dimensionality gives @xmath225 , @xmath226 , and @xmath227 .",
    "in this paper we presented a novel model for pairs of symbols , and showed state of the art results on a large - scale movies recommendation task .",
    "scalability was achieved by factorizing the popularity or selection step via @xmath228 , and employing `` site - independent '' variational bounds through careful parameter tying .",
    "this approach might be too simplistic ; an extension would be to use a @xmath229-component mixture model to select pairs with odds @xmath230 , and perform inference with gibbs sampling .",
    "it is worth noting that bhning @xcite and bouchard @xcite provide lower bounds to the logarithm of ( [ eq : softmax ] ) .",
    "we originally embarked on a variational approximation to a posterior with ( [ eq : softmax ] ) as likelihood using bouchard s bound , for which bookkeeping like section [ sec : scalable ] s was done .",
    "however , with realistically large @xmath6 and @xmath7 , solutions were trivial , as the means of the variational posterior approximations for @xmath21 and @xmath22 were zero .",
    "we leave bhning s bound to future work .",
    "the joint density in ( [ eq : joint ] ) follows from combining the data likelihood @xmath231^{y_{di } z_{dj } } \\prod_{d ' } \\prod_{i , j } ( 1 - \\sigma ( \\u_i^t \\v_j+ b_i + b_j ) ) ^{y_{d'i } z_{d'j}}\\end{aligned}\\ ] ] with a prior on the unobserved variables @xmath232 , and rewriting the expression using observation counts @xmath233 for each pair @xmath0 , and marginal counts @xmath78 and @xmath79 .",
    "the joint density is shown in figure [ fig : graphicalmodel ] .",
    "for the sake of later derivations , it is worthwhile to explicitly write @xmath97 $ ] as it appears in ( [ eq : l ] ) .",
    "it is @xmath234 & = \\sum_{i , j } c_{ij } \\ , \\ebb_q \\bigg [ \\log \\sigma(\\xi_{ij } ) -    \\lambda(\\xi_{ij } ) \\big ( ( \\u_i^t \\v_j + b_i + b_j ) ^2 - \\xi_{ij}^2 \\big )    + \\frac{1}{2 } ( \\u_i^t \\v_j + b_i + b_j ) - \\frac{\\xi_{ij}}{2 } \\bigg ] \\nonumber \\\\ & \\quad + \\sum_{i , j } \\sum_{d ' } \\ebb_q[y_{d ' i } z_{d ' j } ] \\ , \\ebb_q \\bigg [ \\log \\sigma(\\xi_{ij } )    - \\lambda(\\xi_{ij})\\big ( ( \\u_i^t \\v_j + b_i + b_j ) ^2 - \\xi_{ij}^2 \\big ) \\nonumber \\\\ & \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad    - \\frac{1}{2 } ( \\u_i^t \\v_j + b_i + b_j ) - \\frac{\\xi_{ij}}{2 } \\bigg ] \\nonumber \\\\ & \\quad + \\sum_i \\left ( c_i + \\sum_{d ' } \\ebb[y_{d'i } ] \\right ) \\ebb_q[\\log \\pi_i ]    + \\sum_j \\left ( c_j + \\sum_{d ' } \\ebb[z_{d'j } ] \\right ) \\ebb_q[\\log \\psi_j ] \\nonumber \\\\ & \\quad + \\sum_i \\sum_k \\ebb_q[\\log p(u_{ik } ) ] + \\sum_j \\sum_k \\ebb_q[\\log p(v_{jk } ) ]    + \\sum_i \\ebb_q[\\log p(b_i ) ] + \\sum_j \\ebb_q[\\log p(b_j ) ] \\nonumber \\\\ & \\quad + \\ebb_q [ \\log p(\\bpi ) ] + \\ebb_q [ \\log p(\\bpsi ) ] \\nonumber \\\\ & \\quad - \\sum_i \\sum_k \\ebb_q[\\log p(u_{ik } ) ] - \\sum_j \\sum_k \\ebb_q[\\log p(v_{jk } ) ]    - \\sum_i \\ebb_q[\\log p(b_i ) ] - \\sum_j \\ebb_q[\\log p(b_j ) ] \\nonumber \\\\ & \\quad - \\ebb_q [ \\log q(\\bpi ) ] - \\ebb_q [ \\log q(\\bpsi ) ] - \\sum_{d ' } \\ebb_q[\\log p(\\y_{d ' } ) ] - \\sum_{d ' } \\ebb_q[\\log p(\\z_{d ' } ) ] \\ .",
    "\\label{eq : thefulljoint}\\end{aligned}\\ ] ] all expectations are taken under @xmath88 defined in ( [ eq : factorizing ] ) .",
    "the scalable parameter updates in section [ sec : scalable ] rely on a number of cached quantities , which we state here together for completeness : @xmath235 & \\m_{\\ominus}^{\\dagger } & \\defined \\sum_{j } t_j \\ebb_{q } [ b_j ] \\ebb_{q } [ \\v_j ] &   \\m_{\\ominus}^{\\ddagger } & \\defined \\sum_{j } t_j \\ebb_{q } [ \\v_j ] \\\\ \\nu_{\\ominus } & \\defined \\sum_{j } t_j \\ebb_{q } \\big [ b_j \\big ] & \\varkappa_{\\ominus } & \\defined \\sum_j t_j \\ebb_{q } [ b_j^2 ] \\end{aligned } \\right\\ } \\text{\\emph{item}-background } \\\\ \\left.\\begin{aligned } \\p_{\\oplus } & \\defined \\sum_{i } s_i \\ , \\ebb_{q } [ \\u_i \\u_i^t ] & \\m_{\\oplus}^{\\dagger } & \\defined \\sum_i s_i \\ebb_{q } [ b_i ] \\ebb_{q } [ \\u_i ] &   \\m_{\\oplus}^{\\ddagger } & \\defined \\sum_i s_i \\ebb_{q } [ \\u_i ] \\\\ \\nu_{\\oplus } & \\defined \\sum_i s_i \\ebb_{q } \\big [ b_i \\big ] & \\varkappa_{\\oplus } & \\defined \\sum_i s_i \\ebb_{q } [ b_i^2 ] \\end{aligned } \\right\\ } \\text{\\emph{user}-background}\\end{aligned}\\ ] ]      we stated @xmath88 in terms of the factorized gaussian @xmath147 , and will solve for @xmath147 by first maximizing an intermediate lower bound with respect to the full - covariance gaussian @xmath236 .",
    "once @xmath149 is found , a lower bound to it is maximized to find @xmath147 .",
    "let @xmath134 .",
    "the variational bound in ( [ eq : thefulljoint ] ) , as a function of the full - covariance gaussian @xmath149 , is @xmath237 & = - \\frac{1}{2 } \\sum_{j } c_{ij } \\bigg ( 2 \\lambda_{ij } \\tr \\left ( \\ebb_{\\tilde{q } } \\big [ \\u_i \\u_i^t \\big ] \\ebb_q \\big [ \\v_j \\v_j^t \\big ] \\right )   - 2 \\ebb_{\\tilde{q } } [ \\u_i ] ^t \\left ( \\frac{1}{2 } - 2 \\lambda_{ij } \\ebb_q[b_i + b_j ] \\right ) \\ebb_q [ \\v_j ] \\bigg ) \\nonumber \\\\ & \\quad - \\frac{1}{2 } \\sum_{j } \\sum_{d ' } \\ebb_q[y_{d ' i } ] \\ , \\ebb_q [ z_{d ' j } ] \\bigg ( 2 \\lambda_{ij } \\tr \\left ( \\ebb_{\\tilde{q } } \\big [ \\u_i \\u_i^t \\big ] \\ebb_q \\big [ \\v_j \\v_j^t \\big ] \\right ) \\nonumber \\\\ &",
    "\\qquad\\qquad - 2 \\ebb_{\\tilde{q } } [ \\u_i ] ^t \\left ( - \\frac{1}{2 } - 2 \\lambda_{ij } \\ebb_q[b_i + b_j ] \\right ) \\ebb_q [ \\v_j ] \\bigg ) - \\frac{1}{2 } \\tr \\left ( \\ebb_q \\big [ \\u_i \\u_i^t \\big ] \\tau_u \\i \\right )   - \\ebb_{\\tilde{q } } [ \\log \\tilde{q}(\\u_i ) ] \\nonumber \\\\ & =   - \\frac{1}{2 } \\tr \\ebb_{\\tilde{q } } \\big [ \\u_i \\u_i^t \\big ] \\left(\\tau_u \\i +   \\sum_{j } 2 \\lambda_{ij } \\left ( c_{ij } +   \\sum_{d ' } \\ebb_q[y_{d ' i } ] \\ , \\ebb_q [ z_{d ' j } ] \\right ) \\ebb_q \\big [ \\v_j \\v_j^t \\big ] \\right ) \\nonumber \\\\ & \\quad + \\ebb_{\\tilde{q } } [ \\u_i ] ^t   \\sum_{j } \\left ( c_{ij } \\left ( \\frac{1}{2 } - 2 \\lambda_{ij } \\ebb_q[b_i + b_j ] \\right ) \\phantom{\\sum_j } \\right . \\nonumber \\\\ & \\qquad\\qquad + \\left .",
    "\\sum_{d ' } \\ebb_q[y_{d ' i } ] \\ , \\ebb_q [ z_{d ' j } ] \\left ( - \\frac{1}{2 } - 2 \\lambda_{ij } \\ebb_q[b_i + b_j ] \\right ) \\right ) \\ebb_q [ \\v_j ] - \\ebb_{\\tilde{q } } [ \\log \\tilde{q}(\\u_i ) ] \\ , \\label{eq : intermediatebound}\\end{aligned}\\ ] ] where @xmath238 denotes the @xmath239 operator .",
    "@xmath240 $ ] is maximized when @xmath149 is a gaussian density @xmath241 whose natural parameters @xmath154 and @xmath163 are given by ( [ eq : pi ] ) and ( [ eq : mp ] ) ; they accompany @xmath242 $ ] and @xmath243 $ ] in the quadratic and linear terms above .",
    "the above expression contains a sum over @xmath244 and a further inner sum over @xmath245 .",
    "the scalable evaluation for @xmath154 and @xmath153 in section [ sec : scalable ] incorporates caches @xmath246 , @xmath247 , and @xmath248 , and only requires a sparse sum over @xmath168 .",
    "the simplification is obtained by using    1 .",
    "@xmath249 ( and hence @xmath250 ) for all @xmath251 ; 2 .",
    "@xmath252 for all @xmath251 ; 3 .",
    "@xmath253 = s_i$ ] for all @xmath245 ; 4 .",
    "@xmath254 = t_j$ ] for all @xmath245 .",
    "the bound @xmath240 $ ] is maximized at @xmath148 . with @xmath255 being the minimizer of the kullback - leibler divergence @xmath256",
    "we now show that @xmath240 $ ] serves as a temporary or _",
    "lower bound to @xmath89 : @xmath257 \\ge \\lcal \\left [ \\prod_k q'(u_{ik } ) \\right ] \\ .\\ ] ] the bound in ( [ eq : intermediate ] ) follows by substituting @xmath258 = \\bmu_i \\bmu_i^t + \\p_i^{-1}$ ] in ( [ eq : intermediatebound ] ) : @xmath259 & = -\\frac{1}{2 } \\tr \\ebb_{\\tilde{q } } \\big [ \\u_i \\u_i^t \\big ] \\p_i + \\ebb_{\\tilde{q } } [ \\u_i ] ^t \\p_i \\bmu_i - \\ebb_{\\tilde{q}}\\big[\\log \\tilde{q}(\\u_i)\\big ] \\\\ & = - \\frac{k}{2 } + \\frac{1}{2 } \\bmu_i^t \\p_i \\bmu_i - \\left ( - \\frac{k}{2 } \\log(2 \\pi \\erm ) + \\frac{1}{2 } \\log |\\p_i| \\right ) \\ .\\end{aligned}\\ ] ] let @xmath260 indicate the @xmath223-by-@xmath223 matrix that contains only the diagonal of @xmath154 . as @xmath261 = \\bmu_i \\bmu_i^t + \\diag(\\p_i)^{-1}$ ] and @xmath262 , the second bound expands as @xmath263 & = -\\frac{1}{2 } \\tr \\ebb_{q ' } \\big [ \\u_i \\u_i^t \\big ] \\p_i + \\ebb_{q ' } [ \\u_i ] ^t \\p_i \\bmu_i - \\ebb_{q'}\\left[\\sum_k \\log q'(u_{ik } ) \\right ] \\\\ & = - \\frac{k}{2 } + \\frac{1}{2 } \\bmu_i^t \\p_i \\bmu_i - \\left(- \\frac{k}{2 } \\log(2 \\pi \\erm ) + \\frac{1}{2 } \\log \\big|",
    "\\diag(\\p_i ) \\big| \\right ) \\ .\\end{aligned}\\ ] ] finally , ( [ eq : intermediate ] ) follows from the identity @xmath264 as @xmath154 is positive definite .      by first solving for @xmath149 , the updates in ( [ eq : pi ] ) and",
    "( [ eq : mp ] ) require _ one _ sum over @xmath168 , and an @xmath265 matrix inverse to obtain @xmath266 and @xmath147 . on the other hand",
    ", one may solve for each @xmath146 for @xmath267 in turn .",
    "each of these @xmath223 updates require a sum over @xmath168 , but does not require the matrix inverse .",
    "there is therefore a computational trade - off between these two options .",
    "the trade - off depends on @xmath268 and @xmath223 , and was nt investigated further in the paper .",
    "all the @xmath90 parameters are tied to @xmath269 for @xmath131 , and we write ( [ eq : thefulljoint ] ) as a function of @xmath269 as @xmath270 \\\\ & \\quad \\quad + \\left ( \\lambda(\\xi^ { * } ) \\ , { \\xi^{*}}^2 - \\frac{\\xi_{ij}}{2 } \\right ) \\sum_{(i , j ) \\notin \\gcal } d ' s_i t_j \\ .\\end{aligned}\\ ] ] ( notice that for @xmath131 we have @xmath252 , and @xmath137 does not explicitly occur in the above expression . )",
    "recalling that @xmath94 $ ] and that @xmath271 , the above derivative is @xmath272 \\ .\\ ] ] as the bound is symmetric around @xmath273 and as @xmath274 is a monotonic function of @xmath269 for @xmath275 , the derivative is zero when @xmath276 \\ .\\ ] ] unfortunately ( [ eq : xi ] ) requires a sum over @xmath131 .",
    "however , ( [ eq : locallogistic ] ) states that @xmath277 $ ] can be computed and discarded for @xmath130 if required , and hence the required sum can be written in terms of cached quantities through @xmath278 =   \\sum_{i , j } s_i t_j",
    "\\ebb_q \\big [   ( \\u_i^t \\v_j + b_i + b_j ) ^2 \\big ] -   \\sum_{(i , j ) \\in \\gcal } s_i t_j \\xi_{ij}^2 \\ , \\ ] ] and using @xmath279 = \\tr \\p_{\\oplus } \\p_{\\ominus } + 2 \\m_{\\oplus}^{\\ddagger t } \\m_{\\ominus}^{\\dagger } + 2 \\m_{\\oplus}^{\\dagger t } \\m_{\\ominus}^{\\ddagger } + 2 \\nu_{\\oplus } \\nu_{\\ominus } + \\varkappa_{\\oplus } + \\varkappa_{\\ominus } \\ .\\ ] ]      we want to find @xmath280 which is a categorical distribution parameterized by @xmath112 .",
    "using the notation @xmath196 - \\xi_{ij}^2 \\big )   - \\frac{\\xi_{ij}}{2 } - \\frac{1}{2 } \\ebb_q[\\u_i^t \\v_j + b_i + b_j]\\end{aligned}\\ ] ] from section [ sec : scalable ] , substitute @xmath253 = s_i$ ] into ( [ eq : thefulljoint ] ) to obtain @xmath97 $ ] as a function of @xmath112 : @xmath281 - d ' \\sum_i s_i \\log s_i + l\\left ( \\sum_i s_i - 1 \\right ) \\ .\\ ] ] the above function includes a lagrange multiplier @xmath282 as @xmath283 normalizes to one .",
    "the gradient of @xmath284 with respect to @xmath285 is zero when @xmath286 + \\sum_j t_j \\omega_{ij } + \\frac{l}{d ' } - 1 \\ , \\ ] ] while the lagrange multiplier gives the normalizer so that @xmath287 + \\sum_j t_j \\omega_{ij } } } { \\sum_{i ' = 1}^{i } \\erm^ { \\ebb_q[\\log \\pi_{i ' } ] + \\sum_j t_j \\omega_{i'j } } } \\ .\\ ] ]      evaluating @xmath199 in ( [ eq : multinomial ] ) for every @xmath288 again leaves us with an undesirable @xmath289 complexity . here , too",
    ", we shall make heavy use of cached quantities to simplify this computation .",
    "first note that @xmath290 - ( \\xi^{*})^2 \\big )   - \\frac{\\xi^{*}}{2 } - \\frac{1}{2 } \\ebb_q[\\u_i^t \\v_j + b_i + b_j ] \\right)\\ ] ] where @xmath291 , and that @xmath292 .",
    "we therefore compute the full sum @xmath293 using caches , and then only loop over the sparse set @xmath168 to incorporate the difference .",
    "that is , @xmath294 \\p_{\\ominus } + 2 \\ebb_q [ b_i \\u_i^t ] \\m_{\\ominus}^{\\ddagger }    + 2 \\ebb_q [ \\u_i^t ] \\m_{\\ominus}^{\\dagger }   + \\ebb_q[b_i^2 ] + 2 \\ebb_q[b_i ] \\nu_{\\ominus } + \\varkappa_{\\ominus } \\big ) \\\\ & \\qquad \\qquad + \\log \\sigma(\\xi^ * )   + \\frac { ( \\xi^*)^2}{2 } \\lambda^ *    - \\frac{\\xi^*}{2 } - \\frac{1}{2 } \\left ( \\ebb_q[\\u_i^t ] \\m_{\\ominus}^{\\ddagger } + \\ebb_q[b_i ] + \\nu_{\\ominus } \\right)\\end{aligned}\\ ] ] is computed using bookkeeping , and",
    "finally @xmath295 then relies on a sparse sum .",
    "[ t ! ]    = 1   and @xmath107 from a model with @xmath296 windows 8 phone app install signals as feedback pairs . with `` variational model pruning '' of small components , a result that is more useful in a real system can be created by using constant messages with @xmath297 = 0 $ ] when optimizing for @xmath109 and @xmath110 . , title=\"fig:\",scaledwidth=48.0% ]   and @xmath107 from a model with @xmath296 windows 8 phone app install signals as feedback pairs . with `` variational model pruning '' of small components , a result that is more useful in a real system",
    "can be created by using constant messages with @xmath297 = 0 $ ] when optimizing for @xmath109 and @xmath110 . , title=\"fig:\",scaledwidth=48.0% ] _ figure here",
    "_    `` variational model pruning '' @xcite can be observed on the fully optimized @xmath109 and @xmath110 factors . in figure",
    "[ fig : practical ] , one sees the average @xmath298 $ ] tailing roughly where @xmath299 .",
    "the net effect of disproportionately decreasing the expected appearance probability is that @xmath137 is explained by a much larger bias @xmath39 .    in the context of the large - scale online system in which this model is deployed , we ve found it beneficial to substitute a constant @xmath297 = 0 $ ] when optimizing for @xmath109 and @xmath110 .",
    "[ t ]    = 1 -axis by the softmax map point estimate .",
    "the map estimate overfits with sparse data , as is evident in the ( truncated blue ) tail that approaches @xmath300 , and does not assign high odds to out - of - sample co - author k.  obermayer , for example .",
    ", title=\"fig:\",scaledwidth=65.0% ] _ figure here _    as we do not directly maximize the softmax likelihood in ( [ eq : softmax ] ) , we are additionally interested in how the model s predictions differ from those obtained from a full softmax model .",
    "this is evaluated on a much smaller scale here .",
    "this paper s starting point was the bilinear softmax likelihood function in ( [ eq : softmax ] ) .",
    "we will now turn to examine how much the approximation to @xmath301 in ( [ eq : prediction ] ) deviates from that of a maximum a posteriori ( map ) solution to the softmax likelihood . as discussed earlier , the softmax map estimate is expensive to find , we thus use the relatively smaller nips 112 co - authorship dataset ( even though it is not naturally bipartite data ) .",
    "we removed all single - authors which left us with @xmath302 authors , and treat co - authorship as _ symmetric _ counts in @xmath68 .",
    "biases were included in ( [ eq : softmax ] ) , and excluded from our model , so that with @xmath303 both models have the same number of parameters .",
    "we had to add the additional constraint @xmath304 for all @xmath2 to enforce the softmax point estimate to be symmetric .",
    "this was not required for algorithm [ alg ] , which found a symmetric solution with and without such a constraint .",
    "figure [ fig : smola ] shows the predicted co - authors for a.  smola , with the top 25 predictions labelled for each model . this is a density estimation problem with scarce data and an abundance of parameters , and with no shrinkage there are many singularities in the likelihood function . with shrinkage ( @xmath305 )",
    "the smallest softmax odds are @xmath300 in figure [ fig : smola ] , and the small data set is memorized by the map solution , which might not generalize .",
    "this result underscores the need for a bayesian approach .",
    "although the most probable predictions are still anecdotally interpretable , we note that a truer comparison would be against posterior predictions that are estimated using markov chain monte carlo samples with ( [ eq : softmax ] ) as likelihood , but leave this research to future work .",
    "n.  koenigstein and u.  paquet .",
    "xbox movies recommendations : variational bayes matrix factorization with embedded feature selection . in _ proceedings of the 7th acm conference on recommender systems _ , pages 129136 , 2013 .",
    "t.  mikolov , i.  sutskever , k.  cheni , g.  s. corrado , and j.  dean . distributed representations of words and phrases and their compositionality . in _ advances in neural information processing systems 26 _ , pages 31113119 .",
    "2013 .",
    "r.  pan and m.  scholz .",
    "mind the gaps : weighting the unknown in large - scale one - class collaborative filtering . in _ proceedings of the 15th acm sigkdd international conference on knowledge discovery and data mining _ , pages 667675 , 2009 .",
    "r.  salakhutdinov and a.  mnih .",
    "bayesian probabilistic matrix factorization using markov chain monte carlo . in _ proceedings of the 25th international conference on machine learning _ , pages 880887 , 2008 ."
  ],
  "abstract_text": [
    "<S> we present a novel , scalable and bayesian approach to modelling the occurrence of pairs of symbols @xmath0 drawn from a large vocabulary . </S>",
    "<S> observed pairs are assumed to be generated by a simple popularity based selection process followed by censoring using a preference function . by basing inference on the well - founded principle of variational bounding , and using new site - independent bounds , </S>",
    "<S> we show how a scalable inference procedure can be obtained for large data sets . </S>",
    "<S> state of the art results are presented on real - world movie viewing data . </S>"
  ]
}