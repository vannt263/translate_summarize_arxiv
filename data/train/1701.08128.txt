{
  "article_text": [
    "we will discuss here some preliminary observations and assumptions .",
    "first of all , we observe that we need a set of graphs that satisfies the following points :    * they should be finite and should not be multigraphs ; * they should be undirected ; * they should have weighted edges ; * the weights on the edges should be integers ; since the graph is finite , it is enough to show that there exist @xmath1 such that it is the maximum weight on the edges of the graph ; * they might contain self - loops ; * they have to be connected ( the graph has only one connected component ) ; * they should be represented with adjacency lists ; * they should be represented in the same manner ( we need an unvarying file format ) .    unfortunately , the graphs and their representations , which can easily be found on the internet , do nt accomplish all of this requirements at the same time , although many standards for the file format are available .    given this observation , our choice was to use randomly generated graphs , hence to implement our own graphs generator .",
    "this gives us the opportunity to generate a wide set of connected graphs , with tunable parameters , carefully chosen looking forward to the tests ; these parameters include the number of nodes , the number of edges and the edges weight , nonetheless the distribution law for the edges .",
    "the edges between the nodes are step - by - step randomly constructed , respecting the connection requirement . the different types of graphs that we use in our experimental evaluation are presented afterwards .    after studying the paper we made some assumptions .",
    "one of the problem we encountered is that the theoretical algorithm assumes to have as input only graph @xmath2 and to have direct access to the family of graphs @xmath3 is an induced subgraph of @xmath2 such that the maximum weight on his edges is @xmath4 ( e.g. @xmath5 is exactly @xmath2 ) ] ; with `` direct '' we intend that no computation is required to extract @xmath3 , which is not true .",
    "in fact , we can show easily that a lower bound for the extraction of all the family is , at least , @xmath6 ( i.e. is linear on the number of edges ) .",
    "a nave approach would cost @xmath6 for the extraction of @xmath3 for a given @xmath4 , hence @xmath7 for the whole family ; a better approach could order the edges in @xmath8 and build the family in a single pass on the edges , achieving @xmath9 . having as input only @xmath2 , it would seem that the algorithm is responsible for the extraction of the family , but this is not desirable due to this lower bound that would sabotage the overall performance .",
    "finally we decided to consider this cost as a part of the construction of the data structure of the graph , to be done prior to the call of the algorithm .",
    "as mentioned before , our choice is to implement our own graphs generator .",
    "the aim is to generate _ connected random graphs _ with a specific set of parameters , like the number of nodes , the number of edges , the maximum edges weight and the average degree .",
    "moreover , we want to test how our algorithm behaves in different environments , so we want to control the distribution law of the edges .",
    "keep in mind that the graph has to satisfy all the points of the previous section , among which the connectivity requirement . given a desired number of nodes @xmath10 and",
    "@xmath11 edges , the key - concepts under the implementation of our connected graphs generator are the following :    * we begin by generating a random permutation of the vertices @xmath12 * we generate a random spanning tree by iteratively adding edges in this vector in a uniform random manner ; suppose we have added @xmath13 vertices to the tree @xmath14 , we randomly select a vertex @xmath15 in the set @xmath16 and add a new edge @xmath17 .",
    "at the end we will have an acyclic graph with @xmath18 edges . * following a certain probability law , we add the remaining @xmath19 edges    note that every time we add an edge , a weight is associated to it , with a value uniformly choosen in @xmath20 $ ] .",
    "we even decided to use a custom file format to save the data , which is a ` .ssv ` file , standing for _ space separated values _ :",
    "every line of the file corresponds to two edges , reporting the values @xmath21 that means source and target nodes , edge weight .",
    "being the graph undirected , it is implicit that the edge @xmath22 also exists .",
    "we implemented two versions of the algorithm : the first time using the well - known _ boost _ s bgl for the graphs data structures and efficient implementations of kruskal s and prim s algorithms ; the latter instead embodies our own implementation of both the structures and all the side algorithms .",
    "we decided to do so in order to obtain more control over the code for testing purposes and because , at the moment ( version ` 1.61.0 ` ) , the bgl subgraphs framework presents some still unfixed bug .",
    "unfortunately , our kruskal algorithm , although using _ union - find _ structures with path compression , but can be improved to @xmath23 using union find structures with some euristic conveniently applied ] , is not as fine - tuned as that proposed by the boost s libraries ; we did nt take further time on this because on the other side , our version of prim s algorithm shows the _ same exact performances _ of the boost version and it counts as a theoretical lower bound , being an optimal algorithm and way better than the kruskal s solution .",
    "our data structures have been called fastgraph for their optimization over the operation required for the test . in any way our data structures _ can _ nor _ do _ `` help '' the crt algorithm in improving his time complexity .      in the implementation of the algorithm ,",
    "the graph stored in the text file is read into a ` fastgraph ` structure , which is of course a representation by adjacency lists .",
    "we want to compare the performances of crt algorithm with a standard mst algorithm that in addition computes the total weight .",
    "+ we want to emphasize now that the crt algorithm is based on probabilistic assumptions ; some parameters , that depend asymptotically on @xmath24 should be selected carefully in order to provide a good approximation very fast .",
    "these includes :    * @xmath25 , the number of vertices uniformly choosen in the `` approx - number - connected - components '' .",
    "this number is critical for the performances , as it directly determines the running time of the entire algorithm .",
    "* @xmath26 , the large costant that we use to pick the number of vertices determinants for the application of the original paper s lemma 4 .",
    "both these values largely affect the overall performances because they undirectly decide how many bfs will be carried out by the crt algorithm ; the bfses represent the fundamental cost driver of the entire run .",
    "initially in our opinion , the choice of these parameters must depend on the number of vertices in a way that they are dynamic , keeping their dependency on @xmath24 .",
    "we tried to untie this bond to @xmath10 but having static values for this parameters showed poor performances and a relative error exploding too fast as we grow the input instance .",
    "as the paper says , the only requirement for @xmath25 is that it is @xmath27 and @xmath28 .",
    "the demonstration reported on the original paper bound these values in a way that helped us on choosing the right function to compute them ; in fact , we have that @xmath29 hence we choose    @xmath30 @xmath31      another problem we encountered was the random selection of @xmath32 distinct vertices out of @xmath10 with @xmath33 ; after some research , we found that this kind of problem can not be solved in sublinear time on the number of vertices , which ca nt be done for the same reasons exposed in the introduction about the subgraph family .",
    "the problem here is that we ca nt extract _ with no reinsertion _",
    "@xmath32 values without using an external data structure ; this structure has a linear cost for the maintainance that depends on @xmath10 .",
    "a special case is that in which @xmath10 is a prime that fulfills certain properties ; the key idea is to make use of the properties of the quadratic residues of @xmath10 . that permits us to extract a non - repeating random value in constant time in the range @xmath34 ; sadly , this solution is not affordable here because we need a dynamic range for each call , so that we can not fulfill such constraints for @xmath10 .",
    "the solution we found is to use fisher - yates sequences which permits us to prepare in advance the sequences and then get different , distinct values at each call in constant time and with dynamic bounds .",
    "the cost of the preparation of those sequences is linear and is not considered in the total cost .",
    "we choose to implement the algorithm in c++ because we wanted a language that offers the advantages of an oop language , not least a faster development , that had a standard library and sufficiently `` near '' the machine to have a tighter control over the performances and the memory occupation ( e.g. performances high variance due to overhead of a vm or an interpreter ) .",
    "we considered that absolute performances are not important here , whereas relative performances of the crt algorithm and other mst algorithms are the focus , so finally the choice of the language is something not of the greatest importance .",
    "the ide tool used for the implementation is jetbrains clion .",
    "we also used github , as our code versioning control system .",
    "also , as already mentioned , we did use the boost library to extend the stl that c++ already offers ; we did implement fastgraph in place of bgl also to address memory issues in relation to the method boost uses to store subgraphs of a graph ; we in fact used an advanced method to store the family of subgraphs @xmath35 that simply store the difference of vertices between them , @xmath36 .",
    "it is always possibile to reconstruct every @xmath3 because this family only has _ induced _",
    "subgraphs , and this cost is not taken into account in the computation of the total time .",
    "+ the main function of the implementation is in the `` algoweb.cpp '' file . launching the program from this file",
    "allows us to run either the crt algorithm or the prim algorithm , or the kruskal algorithm , and to view either the running time or the computed weight of the minimum spanning tree .",
    "it takes some argument in input , namely the file containing the graph , a suitable value for @xmath24 and the path where to save the results of the single run .",
    "besides , it is possible to run the random graph generator by the ` grandom ` utility we developed apart .",
    "it takes many arguments in input , that you can find just by calling    ....   $ > grandom -h|--help ....      in order to give exhaustive results , we designed ` grandom ` to produce 4 classes of random graphs :    * erds - rnyi model , that builds random graphs using a uniform distribution of the edges over the vertices ; his average degree @xmath37 ; * gaussian model , that builds random graphs with a `` cluster zone '' where the edges are more frequent , hence the gaussian shape of the degree distribution ; we have still @xmath37 ; * barabsi - albert model , that builds random _ scale - free _ graphs .",
    "the average degree is @xmath38 ; * watts - strogatz model , that builds random graphs with _ small - world _ properties and @xmath37 .",
    "[ ddd ] we want to emphasize here the fact that for the barabsi - albert model the average degree results to be different respect to the other models ; this is due to the algorithm the scientific literature offers to build such graphs .",
    "for the other models is always possible to add an arbitrary number of edges keeping the theoretical properties valid ; having , on the contrary , for the barabsi - albert model a certain probability @xmath39 at each step to successfully add an edge , it is not possible to build a graph with an arbitrary number of edges ; the user can solely give the number of vertices .",
    "but then again , the theory states that if the algorithm starts with a complete graph of @xmath40 vertices ( hence @xmath41 edges ) , it will produce a barabsi - albert graph whose average degree is scaled by this quantity .",
    "our initial complete graph has @xmath42 vertices , so we will have @xmath43 . a little insight on",
    "the proof is the following : the distribution of the degree in the barabsi - albert model is a power law with a cubic exponent .",
    "fortunately , in that case this distribution has a well defined mean . applying a little of arithmetic",
    "we can easily see the truthfulness of what stated previously .",
    "this difference in the models has to be bore in mind when reading the test results , because when comparing the performances over a barabsi - albert graph with @xmath10 vertices and @xmath44 edges and any other one of a different model with same number of vertices and edges , we will have different average degrees .",
    "since the crt algorithm is designed to only depend on the latter and not on @xmath44 nor @xmath10 , a multiplicative factor of @xmath45 is to be taken into account .",
    "our implementation provide a _ _ memory - aware _ _ subroutine that calculates the exact value of @xmath46 at each run ; the values obtained so far agree with the statements above .",
    "the following section reports the results we had launching the algorithm over a variegate dataset of graphs , as described in the next paragraph .      for each random graph model listed in the previous section we decided to produce a dataset ; each dataset has a `` family '' of graphs that differs one from each other following a pattern for the parameters .",
    "precisely , we composed the dataset grouping sets of graphs based on the value of the parameters , following the rules :    * build one set of graphs for each model ( ) ; * every set contains in turn other sets , one for each selected value of @xmath10 , i.e. the number of nodes ( @xmath47 , @xmath48 , @xmath49 , @xmath50 , @xmath51 , @xmath52 , @xmath53 , @xmath54 , @xmath55 , @xmath56 ) ; * then again , for each selected value of @xmath46 , i.e. the average degree ( @xmath57 ) ; this way we also determine the desired value for @xmath44 , since we have , for our models and for the fact that @xmath58 , a proportion between the two .",
    "it s easy to see that @xmath59 , so we have the selected values for @xmath44 ( @xmath60 times @xmath10 ) , so @xmath46 is a random variable with mean @xmath61 ; * finally , a set for each selected value of @xmath62 , i.e. the weight ( @xmath63 ) .",
    "in conclusion we have @xmath64 @xmath64 @xmath64 for a total of @xmath65 graphs .",
    "this dataset , made of plain text files as already described , is quite heavy : @xmath66 gib of data .      using the dataset just described , we used a pattern for the runs , in order to have a complete view of the behavior of the algorithm in the domain of the various parameters",
    "; every single result consists of three plots , and we obtained , inspired by the structure of the dataset :    * a set of results for each model ; * this set containing in turn a set for each selected value of @xmath24 ( @xmath67 ) ; * then a set for each selected value of @xmath68 ( @xmath60 ) ; * finally a set for each selected value of @xmath62 .",
    "the first two plots report the absolute and relative error of the crt result compared to the correct one , calculated with prim s algorithm ; the third report the trend of the used time .",
    "as we mentioned , we indeed used kruskal too but it s not reported here for it was not faster respect to prim s times and unuseful for the computation of the mst weight , since already done with prim s method .",
    "we had this way a total of @xmath69 plots , or better @xmath70 different cases of study . a comparison between those cases we consider meaningful finally concludes our work , as reported below in section  [ sec : results ] .    as mentioned before",
    ", the parameters should be selected carefully in order to provide a good approximation very fast . in this sense @xmath24",
    "plays a crucial role here : since the crt algorithm is a probabilistic algorithm and @xmath24 is indeed the driver parameter either for performances and for the accuracy of the estimate mst weight , it does not make sense to choose values too small for this parameter , as the performances could dramatically degrades ( as , indeed , is expected ) .",
    "so , although it could be of interest to study the right limit of @xmath71 , we empirically noted that values of @xmath24 below @xmath72 shows undesired behaviour of the crt algorithm , either for the computed mst weights and for the running times .",
    "this is not unusual dealing with theoretical algorithms that shows _ asymptotical _ properties ; the class this algorithm belongs to is known as _ property testing algorithms _ , whose requirement is to have a query complexity much smaller than the instance size of the problem . given that and the fact that the algorithm is not required to compute an _ exact _ value , but to _ estimate _ a value _ probabilistically near _ to the correct one , we are not surprised that if the instance of the problem is small , the algorithm shows worse performances respect to a `` deterministic '' method . because of this , results for @xmath73 were not comprehensive nor interesting and are not reported .",
    "given the intrinsically probabilistic nature of the crt algorithm , we had to launch several runs on the same graph to have a good estimate of the crt running time .",
    "for this purpose we decided to launch 10 runs for each graph of the dataset , and to take the average running time as the estimate of the running time ; the amount of runs has been decided as a compromise between having a low variance between execution time s mode and mean values , and keeping a restrained amount of tests to do over the whole dataset . for the output value of the approximated mst weight we instead took one over the ones computed , randomly , to preserve the information on the tolerance of the estimate .",
    "following there are considerations about the meaningful line charts regarding the results .",
    "we know that the crt time complexity is @xmath74 , where @xmath46 is the average degree of a graph ; on the other hand , we know that the accuracy depends on @xmath24 also , so we expect an inversely proportional relation with the running time .",
    "therefore what we expect is that :    * by increasing one or more of the parameters @xmath46 , @xmath62 , there should be a worsening of the average running time ; * keeping all the other parameters unchanged , if we consider an increase in the value of @xmath24 there must be an improvement of the running time ( as well as a worsening of the result , though ) ; * viceversa we expect the opposite behaviour if we decrease @xmath46 and/or @xmath62 or decrease @xmath24 with the other parameters unchanged .",
    "let us call the above _",
    "crucial parameters_.    what we still need to know is what happens to the error ; we should expect a direct proportion with the running times , so the above considerations could be also valid for the error . on the contrary , we ll see that this is not exactly respected .    first thing to show",
    "is that the crt algorithm is sublinear in the number of edges , hence is better than any other _ exact _ algorithm .",
    "this can be easily seen in figures from  [ u_03_50_40_time_kruskal ] to  [ u_03_50_40_rel ] . for the rest of the plots listed from now on",
    "it is possibile to see , above each graph , the values of the parameters for the presented run .",
    "it is interesting to note that the correct value , computed with prim s algorithm , it s linear in the number of edges ( figure  [ u_03_50_40_abs ] ) ; we want to stress the fact that this is not the general case , and that this trend is due to the uniform distribution of the weights over the edges .",
    "we will dissect this point more in section  [ rtn ] .",
    "+    given that the crt algorithm respects the sub - linearity constraint , let s now see the variations that occur when selectively changing other parameters .",
    "for the sake of completeness , in figures  [ is_sublin ] informations about kruskal s runs are reported , yet we wo nt report them in the charts that follow .",
    "let us now see the behaviour for the variation of @xmath46 ; we will initially concentrate on the running time and solely for the model .",
    "the selected values of @xmath24 and @xmath62 are respectivey @xmath75 and @xmath76 .",
    "+   +    as we can see in figures  [ u_03_10_40_time ] to  [ u_03_100_40_time ] , there is a worsening in the performance _ for small instances _ : an increase of the average degree @xmath46 is somewhat correlated to a loss of performances to the point that our property testing algorithm needs more time that the deterministic one ; still , that seems to be true _ under _ a certain dimension of the instance of the graph , so that we do nt lose the truthfulness of the theoretical time complexity because for a fixed @xmath77 it will always be possibile to find empirically a certain number of edges @xmath78 beyond which the running time function is always below @xmath79 for a certain @xmath26 .",
    "we want here to highlight the crucial fact that the algorithm behaves better on _ big _ instances , where with `` _ _ big",
    "_ _ '' we refer to the parameters the performances depend on , namely @xmath46 , @xmath62 .",
    "almost all the trends reported in this paper , in fact , show this initial `` bad '' curve and , from a certain value onward , a sublinear behaviour .",
    "we will discuss further about this in section  .",
    "+   +    we would speculate that the error could be related to this .",
    "indeed in figure  [ d_increase_rel ] we can see that to an increase of @xmath46 corresponds a dramatic annihilation of the error ; to explain this point we use a simplified example .",
    "let us consider a minimal complete graph ] ; the algorithm launches some bfses on a _ strict subset _ of the graph , in more steps that could be interrupted according to certain rules , based in part on a stochastic process .",
    "it is easily provable that in this kind of graphs we have a worst case of @xmath18 hops between to vertices @xmath80 and @xmath81 ; if we add a random edge to this graph , the length of this path decrease _ at least _ of one hop . by induction",
    "we can hence prove that the _ diameter _ of the graph decreases as @xmath82 grows .",
    "in other words having a stronger connection within the graph ( i.e. a greater probability to visit a vertex @xmath81 from @xmath80 in @xmath32 hops of a random walk ) increases the probability to have a complete view of the graph , that is more _ information _ about the mst weight .",
    "moreover , we saw in our study of all the results showed in this paper , that the performances of the crt are completely untied from the number of vertices @xmath10 and from the number of edges @xmath44 of the input graph ; this suggests us also that the error is in turn driven solely by the parameters responsible of the algorithm s complexity , as the results that follow are in fact going to prove .    in figure  [ d_increase_time2 ]",
    "we summarize the results for the gaussian and small - world models , noticing that they equate the one we showed about the uniform model .",
    "this suggests that the algorithm complexity does not depend on the dimension and clustering coefficient of the graphs , being those the main differences from one model to another .",
    "+   +    in figure  [ d_increase_rel2 ] we summarize instead the trend of the relative error ; we see here a slightly different evolution .",
    "we can not conclude , as we did for the time complexity , that the error does nt suffer from the different graph model .",
    "the error in fact depends on the clustering coefficient , because it is going to grow dependently on the number of not accomplished bfses : each one of them cause in fact a loss of information .",
    "the algorithm , as well explained in  @xcite , during the bfses phase avoid to explore nodes that shows a high degree and even stops when encounters hubs .",
    "in other words , having equals values of @xmath46 in two different runs of the algorithm , we see that its time complexity trend remain the same ; assuming that @xmath83 is the sample mean of the vertices degree , this tells us that the time complexity is bound to the _ average _ of @xmath83 and , since we have a growth of the error on graphs that contains hubs , we also conclude that the relative error is bound to the _ variance _ of @xmath83 .",
    "+   +      here we will manipulate the value of @xmath62 , similarly to what we have already done with the average degree .",
    "figures  [ w_increase_time ] and  [ w_increase_rel ] are hereafter proposed ; this time the other fixed values are @xmath84 , @xmath85 .",
    "still this graphs has been build using a uniform model .",
    "+    this time we see the error growing as @xmath62 increase .",
    "so we see here a _ direct _ proportion of the execution time with the maximum weight , unlike the _ inverse _ proportion it had with @xmath46 .",
    "this is due to the fact that every iteration of the subroutine ` approx - number - connected - components ` that the reader can find in the original paper and remembered in pseudocode  [ alg ] , adds a further approximation to the final result , because approximates the addend @xmath86 that contributes to the total approximation , that is , the final error .",
    "we see also that the dimension of the initial curve described here so far , grows proportionally to the worsening of the excution time s trend .",
    "this evidence also is observable in all the trends studied .",
    "+      we will test our algorithm for the values of @xmath87 over uniform generated graphs .",
    "as already explained , no values below @xmath72 are investigated .",
    "we see in figures  [ e_increase_time ] and  [ e_increase_rel ] the trends .",
    "+    as expected , we do note that the time trends tend to decrease as @xmath24 increases , since we tolerate a higher error for the computed value , so the algorithm is less aggressive on computation and takes less time .",
    "+   +    for the error trend instead we note an increase , still as expected ; figures from  [ u_02_50_40_rel ] to  [ u_049999_50_40_rel ] has to be read with attention because every function graph has a different scale .",
    "the reader must not confuse the apparent lowering of the error since it is not the case .",
    "looking at the other graphs about the absolute error , from  [ u_02_50_40_abs ] to  [ u_049999_50_40_abs ] , we see an expansion of the tolerance cone ( light blue and ochre yellow lines ) as increasing @xmath24 means admitting an higher deviation from the correct mst weight value . here",
    "too the different scaling must not confuse about the increasing trend of the error .",
    "we see that the result is coherent with the theoretical model as the error increases with @xmath24 , but his variation is , after all , contained .      as a last comparison , instead of varying one by one the crucial parameters and see the time and error trends , we fix all of them and try to change the model the graph belongs to .",
    "figures   show those results .",
    "we see here that both uniform and small - world models keep the same trend for the error , but the small - world one behaves slightly better on small instances . on the contrary ,",
    "the gaussian show the same time trend respect to the uniform case , but his error has a higher growth curve .",
    "the scale - free model seems to be the worse case both regarding time and error trend , but we might remember , as observed earlier at  , that a real term of comparison requires to considerate a scale factor of @xmath45 as done in figure  .",
    "we see in fact that , compared to the uniform case , the scale - free model has even a better behaviour , looking carefully at the function graph scale : but after all , both of them show sublinear complexity for instances of more than @xmath88 edges , so have both the same transients as the same steady state trend .",
    "another thing we note is that a bad trend in execution times are always bond to an explosion of the error , as we can see in the charts so far .",
    "this means that using more time to compute the value does nt mean it will be nearer to the correct one .",
    "at this point , all of our graphs show a bad initial curve every time we burden the input instance ; in a specific case of study this anomalous curve was so persistent that all the function graph was more than linear ; we decided to deepen this special case of study , performing a longer run to see the tail of this particular trend .",
    "the results are reported on figure  .",
    "+    we can see that the original case of study hinted a sublinear trend beyond 20 millions edges instances , but we considered to investigate further , and figure  [ bg_time ] confirms the sublinearity .",
    "we have a good behaviour on the error trend ( figure  [ bg_err ] ) .",
    "as expected , a probabilistic algorithm like the crt allows us to compute an approximation of the minimum spanning tree weight in sublinear time on the number of edges , under certain conditions .",
    "tunable parameters , that depends on @xmath24 , allows us to perform either a better or a worse approximation , implying respectively a very slow and a very fast computation .",
    "the choice of a small value of @xmath24 can lead to terrible running times , and for these values it does not make sense to compare the crt algorithm with any other deterministic algorithm . + for other @xmath24 values , instead , we prove the good performances of the crt .",
    "the reader can easily view the better performances of crt algorithm versus prim algorithm or kruskal algorithm watching the line charts in the previous section of this paper .",
    "more in general , we see that execution time and error depend on the number of bfses successfully completed during the computation the more bfses are completed , the more information the algorithm has to predict a correct value for the mst weight , but on the other hand , completing the bfses takes time .",
    "if instead we have a lot of interrupted bfses , we waste a large amount of time without gathering information , hence resulting in both high execution time and error .",
    "we considered so far different theoretical graph models , and we conclude that a high _ clustering coefficient _ tends to increase the probability to have interrupted bfses .",
    "this because one of the reasons the algorithm has to interrupt the search is having encountered a hub , i.e. a vertex with a high degree .",
    "we saw in fact that when changing the model there is a slight perturbation of the trend , although it remains sublinear .",
    "the key concept is the `` distance '' from the hubs of the graph of the root vertex from which our bfs starts .",
    "generally we also concluded that increasing the average degree let our algorithm gather more information , because there is a growing of the probability to visit the generic node @xmath80 of our graph . on the other side , increasing the maximum weight correspond to an increase in the number of iterations our algorithm does , that leads to summing more intermediate approximated results that imply a higher final approximation .",
    "we observe that the crt algorithm lends itself very well to a parallel implementation .",
    "indeed the majority of the algorithm s code is organized into independent sections , and in most cases they do nt need to comunicate to each other .",
    "we also observe that three levels of paralellism can be achieved within the code . in the first level",
    "we parallelize each of the @xmath62 independent calls to ` approx - number - connected - components ` , as depicted in pseudocode  [ alg ] , below ; every of this calls internally performs @xmath25 independent bfses from @xmath25 different roots , that could in turn run in parallel , achieving a second level of parallelism . moreover , considering that in the academic world there already exist different parallel implementations of the bfs algorithm , we can use one of them to perform an additional third level of paralellism .",
    "@xmath89 @xmath90    @xmath91    @xmath92    to make it even more simpler , the number of different flows is known a priori , so a static pre - instantiation and a smart scheduling of the threads can be performed . at the first and second levels",
    "a _ master - slave _ model can be used in a _ fork - join _ structure , while in the third level a shared variable is needed between the different bfses . as a final remark ,",
    "parallelizing the bfses could have too much overhead given that the algorithm is optimized to run them very fast and to stop the ones that seem to cost too much .",
    "during an e - mail exchange with one of the original authors of @xcite , dr .",
    "ronitt rubinfeld , another topic of discussion and study has emerged , about the distribution of the weight on the edges .    in our code , the generation of random graphs only assume a uniform distribution of the weights on the edges , i.e. a given edge can have an integer weight @xmath93 $ ] with probability @xmath94 .",
    "that implies a linear growth of the dimension of @xmath95 $ ] , namely the set of @xmath3 s edges ; this is well depicted in figure  [ same_gap ] , where , as @xmath4 grows , the size of @xmath96 increases at each step of a quantity `` near '' @xmath97 , and it is more true as @xmath82 is big for the law of large numbers . on the other side ,",
    "having a generic law of distribution for the edges weight implies having a different behavior as depicted on  [ diff_gap ] .",
    "this difference could be of interest because it means that the input of different subsequent iterations of ` approx - number - connected - components ` will have a non regularly increasing size , or even the same size for some calls ; it can be easily shown indeed that the function in  [ gaps ] is nondecreasing .",
    "since the cost of those calls finally determines the overall cost , we might argue that this could lead to a minor difference , but at the same time think that only with another set of tests we could conclude something relevant about this observation ."
  ],
  "abstract_text": [
    "<S> we present an implementation and an experimental evaluation of an algorithm that , given a connected graph g ( represented by adjacency lists ) , estimates in sublinear time , with a relative error @xmath0 , the minimum spanning tree weight of g ( see  @xcite for a theoretical exposure of the algorithm ) . since the theoretical performances have already been shown and demonstrated in the above - mentioned paper of chazelle et al . </S>",
    "<S> our goal is , exclusively , to experimental evaluate the algorithm and at last to present the results . </S>",
    "<S> some technical insights are given on the implementation of the algorithm and on the dataset used in the test phase , hence to show how the experiment has been carried out even for reproducibility purposes ; the results are then evaluated empirically and widely discussed , comparing these with the performances of the prim algorithm and the kruskal algorithm , launching several runs on a heterogeneous set of graphs and different theoretical models for them .    </S>",
    "<S> we assume hereafter that the reader has knowledge about the cited paper as we will just recap the theoretical results .    minimum spanning tree , sublinear time algorithms , randomized algorithm , approximation algorithm , minimum spanning tree weight , experimental evaluation    68w20 , 68w25 , 68r10 </S>"
  ]
}