{
  "article_text": [
    "linear systems arising from high  dimensional problems usually can not be solved by standard numerical algorithms .",
    "if the equation is considered in @xmath0 dimensions on a @xmath1 grid , the number of unknowns @xmath2 scales exponentially with @xmath3 and even for moderate dimension @xmath0 and mode sizes @xmath4 the numerical complexity lays far beyond the technical possibilities of modern workstations and parallel systems . to make the problem tractable ,",
    "different approximations are proposed , including sparse grids  @xcite and tensor product methods  @xcite . in this paper",
    "we consider the linear system @xmath5 where the matrix @xmath6 and right - hand - side @xmath7 are given and approximate solution @xmath8 is sought in the _ tensor train _ ( tt ) format .",
    "methods based on the tt format , also known as a _ linear tensor network _",
    ", are novel and particularly interesting among all tensor product methods due to their robustness and simplicity .",
    "the numerical optimization on tensor networks was first considered in quantum physics community by s.  white  @xcite , who introduces the _ matrix product states _ ( mps ) formalism to represent the ground state of a spin system together with the _ density matrix renormalization group _",
    "( dmrg ) optimization scheme . the tensor train format and some computational methods were",
    "independently re - discovered in the papers of oseledets and tyrtyshnikov ( see @xcite and references therein ) until the results of white  et .",
    "were popularized in the numerical mathematics community by r.  schneider  @xcite .",
    "the questions concerning the convergence properties of alternating schemes for different tensor product formats were immediately raised and studied .",
    "the experimental results from quantum physics show the notably fast convergence of dmrg for the ground state problem , i.e. , finding the minimal eigenstate of a system , but give no theoretical justification for this observation .",
    "the _ alternating least squares _ ( als ) algorithm was used in multilinear analysis for the computation of _ canonical _ tensor decomposition since early results of hitchcock  @xcite and was known for its monotone but very slow convergence . for als",
    "there is also a lack of convergence estimates both in the classical papers  @xcite , and in the recent ones , where als was applied to the tucker model  @xcite , tensor trains  @xcite , hierarchical tucker format  @xcite and high  dimensional interpolation  @xcite .    in recent papers by uschmajew  @xcite the local convergence of als",
    "is proven for the canonical and tensor train decompositions .",
    "this is a major theoretical breakthrough , which unfortunately does not immediately lead to practical algorithms due to the local character of convergence studied , unjustified assumptions on the structure of the hessian , and very strong requirements on the accuracy of the initial guess .",
    "the convergence rate of als is difficult to estimate partly due to the complex geometrical structure of manifolds defined by tensor networks .",
    "this problem is now approached from several directions , and we might expect new results soon  @xcite .",
    "in contrast to als schemes which operate on manifolds of fixed dimension , the dmrg algorithm changes the ranks of a tensor format .",
    "this allows to choose the ranks adaptively to the desired error threshold or the accuracy of the result and develop more practical algorithms which do not rely on a priori choice of ranks .",
    "the dmrg was adopted for novel tensor formats ( see references above ) and new problems , including adaptive high  dimensional interpolation  @xcite and solution of linear systems  @xcite . the geometrical analysis , eg the convergence of the nonlinear gauss  seidel method , is however even more difficult when the dimensions of underlying manifolds are not fixed .",
    "apart of working with the tensor format structure directly , like als and dmrg do , standard algorithms from numerical linear algebra can be applied with tensor approximations and other tensor arithmetics . following this paradigm ,",
    "the solution of linear problems in tensor product formats was addressed in @xcite .",
    "the usual considerations of linear algebra can be used in this case to analyze the convergence .",
    "a first notable example is the method of conjugate  gradient type for the rayleigh quotient minimization in higher dimensions , for which the global convergence was proven by o.  lebedeva  @xcite .",
    "we develop a framework which combines the als optimization steps ( ranks are fixed , convergence estimates not yet possible ) with the steps when the tensor subspaces are increased and the ranks of a tensor format grow . choosing the new vectors in accordance with standard linear algebra algorithms",
    ", we recast the classical convergence estimates for the proposed algorithm in higher dimensions . in this paper",
    "we consider the case of symmetrical positive definite ( spd ) matrices and analyze the convergence in the @xmath9norm , i.e. minimize the _",
    "energy function_. the _ basis enrichment _ choice follows the steepest descent ( sd ) algorithm and the convergence of the resulted method is analyzed with respect to the one of steepest descent .",
    "we show that the basis enrichment step combined with the als step can be seen as a certain computationally cheap approximation of the dmrg step .",
    "the complexity of the resulted method is equal to the one of als and is linear in the mode size and dimension .",
    "our choice of the basis enrichment appears to be very good for practical computations , and for the considered numerical examples the proposed methods converge almost as fast as the dmrg algorithm .    summarizing the above , the proposed algorithms have ( 1 ) proven geometrical convergence with the estimated rate , ( 2 ) practical convergence compared to the one of dmrg , ( 3 ) numerical complexity compared to the one of als .",
    "the paper is organized as follows .    in section",
    "[ sec : tt ] we introduce the tensor train notation and necessary definitions .    in section  [ sec : als ]",
    "we introduce the basic notation for als and dmrg schemes .",
    "we also study how the modification of one tt ",
    "block affects the als problem for its neighbor and describe this in terms of the galerkin correction method .    in section  [ sec : sd ]",
    "we develop the family of steepest descent methods for the problems in one , two and many dimensions .",
    "the proposed methods have an inner  outer structure , i.e. , a steepest descent step in @xmath0 dimensions",
    "is followed by a steepest descent step in @xmath10 dimension , etc , cf .",
    "the interpolation algorithms  @xcite .",
    "the convergence of the recursive algorithms in higher dimensions is analyzed using the galerkin correction framework .",
    "the effect of roundoff / approximation errors is also studied .",
    "since we make no assumptions on the tt  ranks of the solution , the ranks of the vectors in the proposed algorithms can grow at each iteration and make the algorithm inefficient . in section  [ sec : prac ]",
    "we discuss the implementation details , in particular the steps when the tensor approximation is required to reduce the ranks .    in section  [ sec : num ] the model numerical experiments",
    "demonstrate the efficiency of the method proposed and compare it with other algorithms mentioned in the paper .",
    "the tensor train ( tt ) representation of a @xmath0-dimensional tensor @xmath11 $ ] is written as the following multilinear map ( cf .",
    "@xcite ) @xmath12 where @xmath13 are the _ mode _ ( physical ) indices , @xmath14 are the _ rank _ indices , @xmath15 are the tensor train _ cores _ ( tt  cores ) and @xmath16 denote the whole tensor train . here and later we use the einstein summation convention  @xcite , which assumes a summation over every pair of repeated indices .",
    "therefore , in eq .",
    "we assume the summation over all rank indices @xmath17 @xmath18 we also imply the _ closed boundary conditions _",
    "@xmath19 to make the right  hand side a scalar for each @xmath20 eq .",
    "is written in the elementwise form , i.e. , the equation is assumed over all free ( unpaired ) indices .",
    "it is often convenient in higher dimensions and will be used throughout the paper .",
    "the indices can be written either in the subscript @xmath21 or in brackets @xmath22 .",
    "for the summation , there is no difference .",
    "the subscripted indices are usually considered as _ row and column _ indices of a matrix , while the indices in brackets are seen as _",
    "parameters_. for example , each tt  core @xmath15 is considered as a parameter - dependent on @xmath23 matrix with the row index @xmath24 and the column index @xmath25 as follows @xmath26 \\in \\c^{r_{k-1}\\times n_k \\times r_k } , \\qquad x^{(k)}(i_k ) \\in \\c^{r_{k-1}\\times r_k}.\\ ] ] in our notation @xmath27 is a matrix , for which standard algorithms like orthogonalization ( qr ) and singular value decomposition ( svd ) can be applied .",
    "we will freely transfer indices from subscripts to brackets in order to make the equations easier to read or to emphasize a certain transposition of elements in tensors .",
    "it brings the notations in consistence with previous papers on the numerical tensor methods , e.g. @xcite and others .",
    "we will reshape arrays into matrices and vectors by using the _ index grouping _ , i.e. , combining two or more indices @xmath28 in a single multi - index @xmath29 following  @xcite we define _ interface _ matrices @xmath30 and @xmath31 as follows @xmath32 and similarly for symbols @xmath33 and @xmath34 using the @xmath35 notation defined in   we can write @xmath36 for a tensor @xmath37 $ ] we also define the _ unfolding matrix _ , which consists of the entries of the original tensor as follows @xmath38    for @xmath8 in the tt  format   it holds @xmath39 and therefore @xmath40 in  @xcite the reverse is proven : for any tensor @xmath8 there exists the representation   with tt  ranks @xmath41 this gives the term _ tt  rank _ the definite algebraic meaning . as a result , the tensor train representation of fixed tt  ranks yields a closed manifold , and the rank-@xmath42 approximation problem is well  posed .",
    "we can also approximate a given tensor by a tensor train with quasi ",
    "optimal ranks using a simple and robust approximation ( rank truncation , or _ tensor rounding _ ) algorithm  @xcite .",
    "this is the case for all tensor networks without cycles , eg .",
    "tucker  @xcite , ht  @xcite , qtt - tucker  @xcite , etc .",
    "in contrast , the mps formalism originally assumes the _ periodic boundary conditions _ @xmath43 and sum over these indices , which leads to @xmath44 where all matrices can be shifted in cycle under the trace .",
    "the optimization in such type of _ tensor networks _ is difficult , because they form unclosed manifolds and the best approximation does not always exist .",
    "the tensor train representation of the matrix is made similarly with the tt ",
    "cores depending on two parameters @xmath45 hence , @xmath46 is sought in the form   and @xmath6 and @xmath7 given in the tt  format as follows @xmath47    for @xmath6 and @xmath8 given in the tt ",
    "format , the matrix - vector product @xmath48 is also a tt  format computed as follows @xmath49 where @xmath50 denotes the tensor ( kronecker ) product of two matrices defined as follows @xmath51 we refer to @xcite for more details on basic tensor operations in the tt  format .    in this paper",
    "we will use standard @xmath52 scalar product @xmath53 and the @xmath9scalar product @xmath54 defined by a symmetrical positive definite ( spd ) matrix @xmath9 as follows @xmath55 for a given nonsingular matrix @xmath56 we define the @xmath9orthogonal projector @xmath57 as follows : for all @xmath58 and all @xmath59 it holds @xmath60    we will use vector notations for mode indices @xmath61 and rank indices @xmath62 we also denote the subspace of tensor trains @xmath63 with tensor ranks @xmath64 as @xmath65",
    "the mps formalism was proposed in quantum physics , where the representation   was used for the minimization of the rayleigh quotient @xmath66 similarly , the solution of a linear system @xmath67 with @xmath68 can be sought through the minimization of an _ energy function _",
    "@xmath69 where @xmath70 denotes the exact solution .",
    "we consider the hermitian matrix @xmath68 and the right - hand side @xmath7 given in the tt ",
    "format  , and solve the minimization problem with @xmath8 sought in the tt  format   with fixed tt  ranks @xmath71 i.e. , @xmath72this heavy nonlinear minimization problem can hardly be solved unless a ( very ) accurate initial guess is available ( see , eg .",
    "@xcite ) . to make it tractable",
    ", we can use the alternating linear optimization framework and substitute the global minimization over the tensor train @xmath73 by the linear minimization over all cores @xmath74 subsequently in a cycle . solving the _ local _ problem",
    "we assume that all cores but @xmath75th of the current tensor train @xmath76 are ` frozen ' , and the minimization is done over @xmath15 as follows @xmath77 clearly , the energy function does not grow during the sequence of als updates and the solution will converge to a local minimum .    to write each als step as a linear problem , let us stretch all entries of the tt  core @xmath15 in the vector @xmath78 from   we see that @xmath79 where @xmath80 is the @xmath81 matrix defined as follows @xmath82 where @xmath83 is the kronecker symbol , i.e. , @xmath84 if @xmath85 and @xmath86 elsewhere . if @xmath87 is considered as a function of @xmath88 it is also the second - order energy function @xmath89 where the gradient w.r.t .",
    "@xmath90 is zero when , for the detailed derivation see  @xcite . ]",
    "@xmath91 the solution of the local minimization problem   is therefore equivalent to the solution of the original system @xmath92 in the _ reduced basis _",
    "@xmath93 defined by  .",
    "the tensor train representation   is non - unique .",
    "indeed , two representations @xmath94 and @xmath95 map to one tensor @xmath96 as soon as @xmath97 where @xmath98 and @xmath99 @xmath100 are arbitrary nonsingular matrices . given a vector in the tt ",
    "format @xmath101 any transformation @xmath102 does not change the energy level since @xmath103 but gives us some flexibility for the choice of the reduced basis since @xmath104 the proper choice of the _ representation _ @xmath94 essentially defines the reduced basis and affects the properties of the _ local problem _  . a prominent transformation @xmath105 is the tt  orthogonalization algorithm proposed in  @xcite .",
    "it chooses matrices @xmath106 applying the qr factorization to the reshaped tt ",
    "cores , i.e. , matrices of size @xmath107 and/or @xmath108 the transformation @xmath105 given by the tt ",
    "orthogonalization implies the left ",
    "orthogonality constrains on tt ",
    "cores @xmath109 and right ",
    "orthogonality on @xmath110 which results in the orthogonality of the interfaces @xmath111 and @xmath112 and hence the reduced basis @xmath113 such a _ normalization _ step will be assumed in many algorithms throughout the paper ; in most cases we will do this without introduction of a new representation @xmath95 just by ` claiming ' the necessary orthogonalization pattern of the tt representation we use .",
    "if the reduced basis method is applied and such a representation @xmath94 is chosen so that @xmath114 is orthogonal , the spectrum of the reduced matrix @xmath115 lies between the minimum and maximum eigenvalues of the matrix @xmath116 indeed , using the rayleigh quotient  @xcite , we write @xmath117 and similarly for the maximum values .",
    "it follows that the reduced matrix is conditioned not worse than the original , @xmath118 therefore , the orthogonality of tt  cores ensures the stability of local problems and we will silently assume this for all reduced problems in this paper .    to conclude this part ,",
    "let us calculate the complexity of the local problem  .",
    "as was pointed out in @xcite , either a direct elimination , or an iterative linear solver with fast matrix - by - vector products ( _ matvecs _ ) may be applied .",
    "if the direct solution method is used , the costs which are required to form the @xmath119 matrix of the local problem   are smaller than the complexity of the gaussian elimination , i.e. , the overall cost is @xmath120 and @xmath121 in the complexity estimates . ]",
    "if an iterative method is used to solve the local problem , one multiplication @xmath122 requires @xmath123 operations , where @xmath124 and @xmath125 denote the tt ",
    "rank of the current solution @xmath8 and the matrix @xmath6 , respectively .",
    "careful implementation of the matvec is essential to reach this complexity , see  @xcite for details .",
    "the complexity of the normalization step is only @xmath126 operations and can be neglected .      in practical numerical work",
    "the tt  ranks of the solution are usually not known in advance , which puts a restriction on the use of the methods with fixed tt",
    "the underestimation of tt  ranks leads to a low accuracy of the solution , while the overestimation results in a large computational overhead .",
    "this motivates the development of methods which can choose and modify the tt ",
    "ranks on  the  fly adaptively to the desired accuracy level .",
    "a prominent example of such method is the density matrix renormalization group ( dmrg ) algorithm  @xcite , developed in the quantum physics community for the solution of a ground state problem .",
    "dmrg performs similarly to the als but at each step combines two succeeding blocks @xmath15 and @xmath127 into one _ superblock _ @xmath128 and make the minimization over @xmath129 classical dmrg minimizes the rayleigh quotient , our version minimizes the energy function @xmath130 see  @xcite . similarly to  , we write the local dmrg problem @xmath131 as follows @xmath132 when the @xmath133 is computed , new tt  blocks are obtained by the low  rank decomposition , i.e. the right - hand side of   is computed and the @xmath75-th rank is updated adaptively to the chosen accuracy .",
    "the minimization over @xmath134 components of @xmath133 leads to complexity @xmath135 , and seriously increases the computational time for systems with large mode sizes .",
    "suppose that we have just solved   and updated the tt - block @xmath15 .",
    "before we move to the next step , we would like to improve the reduced basis @xmath136 by adding a few vectors to it .",
    "denote the current solution vector by @xmath137 and suppose we add a step @xmath138 then the updated solution @xmath139 has the tt  representation @xmath46 defined as follows @xmath140 where @xmath141 we will denote this tensor train as @xmath142 the considered update affects the solution process in two ways : first , naturally , adds a certain correction to the solution , and second , enlarge the reduced basis that we will use at the _ next step _ of the als minimization .",
    "indeed , it can easily be seen from definition   that @xmath143 from   we conclude that @xmath144 and hence @xmath145 the clever choice of @xmath146 allows to add the essential vectors to @xmath147 and therefore improve the convergence of als .    a random choice of @xmath148 with some small tt  ranks @xmath64 ( cf .",
    "_ random kick _ proposed in  @xcite ) may lead to a slow convergence .",
    "it also introduces an unwanted perturbation of the solution .",
    "a more robust idea is to choose @xmath149 in accordance to some one - step iterative method , for instance , take @xmath150 and construct a steepest descent or minimal residual method with approximations .",
    "this choice allows to derive the convergence estimate similarly to the classical one and will be discussed in sec .",
    "[ sec : sd ] .    to stay within methods of linear complexity ,",
    "we restrict ourselves to zero shifts @xmath151 with a simple tt  structure @xmath152 the tensor train @xmath153 has the following structure for the _ backward _ sweep the construction is done analogously . ]",
    "@xmath154 and @xmath155 for other @xmath156 note that since @xmath157 the enrichment step does not affect the energy @xmath158 therefore , we can choose @xmath159 freely and develop ( probably , heuristic ) approaches to improve the convergence of our scheme .",
    "the reduced basis @xmath160 depends on the choice of @xmath159 as follows ( cf .  )",
    "@xmath161 where @xmath162 is a column of @xmath33 and @xmath163 is the @xmath164 matrix which is the _ slice _ of 3-tensor @xmath165 $ ] corresponding to the fixed @xmath166 and similarly for @xmath167 and @xmath168 below we will write the local system   at the step @xmath169 and see how it is affected by the choice of @xmath170    the two - dimensional system defined by   is shown by gray boxes in the fig .",
    "[ fig : reduced ] . it appears here as the local problem in the dmrg method , but in the same framework we may consider the whole initial system with @xmath171 , and @xmath172 , depending on what type of analysis we would like to perform .",
    "now the reduced system for the elements of @xmath173 writes @xmath174 where the following multi - indices are introduced for brevity of notation @xmath175 and @xmath176 @xmath177 the system   has @xmath178 unknowns . at the same time it is the reduction of a 2d system @xmath179 which has @xmath180 unknowns .",
    "therefore , the choice of the enrichment @xmath159 ( as a part of @xmath15 ) can be considered as a cheaper approximation of the 2d system solution .",
    "taking into account the structure of @xmath15 from   we rewrite   as follows @xmath181    the system is difficult to analyze .",
    "however , we may propose a certain approximation to its solution , and estimate the quality of the solution to the whole system via the properties of the approximation .",
    "namely , let us consider the zero  padded tt ",
    "core @xmath127 in   as the  _ initial guess _ ,",
    "i.e. , some information about the solution @xmath182 that we want to use .",
    "for instance , we can apply the block gauss ",
    "seidel step , restricting the unknown block to the form @xmath183 then   writes as the following overdetermined system @xmath184 and following the gauss ",
    "seidel step we solve it considering only the lower part @xmath185    equation   is a galerkin reduction method with the basis @xmath186 applied to the system @xmath179 with the initial guess  , and tt ",
    "cores @xmath15 and @xmath127 defined by  .",
    "after   is solved , the updated superblock @xmath187 writes as follows @xmath188 which allows to consider the proposed method as a solver for the 2d system , which performs the  _ low  rank correction _ for the superblock rather than recompute it from scratch .",
    "equations and can be considered as certain approximate approaches to the solution of the 2d system .",
    "different such approaches can be collected into table [ tab : dmrg ] , sorted from the highest to the lowest accuracy .    .",
    "comparison of different solution methods for a two  dimensional system   with blocks given by",
    "keep _ the block from the previous iteration , _ choose _ it arbitrary ( eg . , using quasi  optimal or heuristic choice ) or _ optimize _ solving the reduced system . in the complexity estimates",
    ", @xmath124 is typical rank of @xmath94 and @xmath189 is typical rank of @xmath190 [ cols=\"<,^,^,^,^,^ \" , ]",
    "given the initial guess @xmath191 the _ steepest descent _ ( sd )",
    "step minimizes the energy function   over vectors @xmath192 where the step is chosen as follows @xmath193 the solution after the sd step satisfies the so - called _ galerkin condition _",
    "@xmath194 the progress of the sd step can be analyzed in terms of @xmath9norms of errors @xmath195 and @xmath196 as follows @xmath197 this gives interpretation in terms of projections and proves the monotone decrease of the energy function @xmath198 to estimate the convergence rate , we write @xmath199 the convergence rate @xmath200 is therefore a square root of the rayleigh quotient for @xmath201 in the @xmath9scalar product .",
    "it can be bounded using the kantorovich inequality  @xcite as follows @xmath202 where @xmath203 and @xmath204 denote the largest and smallest eigenvalues of @xmath205 respectively .",
    "the residual @xmath206 of the steepest descent method can not be computed exactly for high  dimensional problems .",
    "suppose that it is approximated by @xmath207 and the perturbed sd step is applied as follows @xmath208 where @xmath209 we further restrict ourselves to the perturbations of the following form @xmath210 which will appear naturally in our algorithms for higher dimensions . for such perturbations",
    "the second term vanishes , @xmath211 and the perturbation of the sd step writes through the perturbation of @xmath9orthogonal projectors as follows @xmath212 a comprehensive overview of the perturbation theory for projections , pseudo  inverses and least square problems can be found in  @xcite . rather than adapting their results to the case of @xmath9orthogonal projectors , we will develop a more accurate estimate for @xmath213 using specifically the perturbations  .",
    "[ thm : sd1 ] for @xmath207 given by   the progress of the perturbed sd step   writes as follows @xmath214 where @xmath200 is the progress of the unperturbed sd step given by  .",
    "for @xmath215 the following simple identity can be verified from definition @xmath216 the perturbation of the sd step @xmath217 writes @xmath218 where @xmath219 and @xmath220 obviously , @xmath221 to estimate the @xmath9norm of the second term , we write @xmath222 where @xmath223 then @xmath224 and @xmath225 since @xmath226 and @xmath207 are @xmath9orthogonal , we write @xmath227 finally , we estimate @xmath228 where the last inequality is based on @xmath229 since @xmath230 we obtain the statement of the theorem .",
    "if @xmath231 there exists @xmath232 such that for all @xmath233 it holds @xmath234 this critical value @xmath235 is the real positive root of the cubic equation @xmath236 where @xmath237 and @xmath238 act as parameters .",
    "the minimal value of @xmath235 for @xmath239 and @xmath240 behaves as @xmath241      consider the two  dimensional linear system @xmath242 written in the elementwise notation as follows and @xmath243 as vectors and at the same time as two - dimensional arrays @xmath244 $ ] and @xmath245 $ ] with the same entries .",
    "we will switch freely between these representations without change of a notation . ] @xmath246 as previously , we assume @xmath9 and @xmath243 to be given , and @xmath247 to be sought in the following low - rank decomposition format @xmath248",
    "\\in \\c^{n_p \\times n_p \\times r_a } , \\\\    y(\\overline{i_1i_2 } ) & = y^{(1)}_{\\beta}(i_1 ) y^{(2)}_{\\beta}(i_2 ) ,   \\quad y^{(p ) } = [ y^{(p)}_{\\beta}(i_p ) ] \\in \\c^{n_p \\times r_y } , \\\\    x(\\overline{j_1j_2 } ) & = x^{(1)}_{\\alpha}(j_1 ) x^{(2)}_{\\alpha}(j_2 ) ,   \\quad x^{(p ) } = [ x^{(p)}_{\\alpha}(j_p ) ] \\in \\c^{n_p \\times r_x } ,   \\end{split}\\ ] ] where @xmath249 given the initial guess @xmath250 in the same format , we compute the low ",
    "rank approximation of the residual @xmath251 as follows @xmath252 \\in",
    "\\c^{n_1 \\times r_z } , \\quad    z^{(2 ) } = [ z^{(2)}_{\\zeta}(i_2 ) ] \\in \\c^{n_2 \\times r_z}.\\ ] ] following the perturbed sd algorithm , we can write the updated solution @xmath253 in a form @xmath254 and optimize by the step size @xmath255 recalling the considerations from section [ sec : gal ] , we can consider more efficient optimization steps listed in table  [ tab : dmrg ] .",
    "for example , the solution of dmrg system corresponds to the exact solution of the considered 2d system .",
    "we will particularly consider the galerkin correction framework , i.e. , will optimize over the bottom block of @xmath256 denoted as @xmath257 in  .",
    "this is the cheapest method in table  [ tab : dmrg ] , and all other methods have better convergence properties .    in the proposed method",
    "we choose the step @xmath258 where @xmath259 and without the loss of generality assume the orthogonality of @xmath260 minimization of the energy function @xmath261 over @xmath58 leads to the set of galerkin conditions @xmath262 and the step writes as follows @xmath263 note that if we restrict ourselves to the perturbations @xmath264 such that @xmath265 it holds @xmath266 then the accuracy of the proposed method can be estimated similarly to the standard sd step @xmath267 and the progress of this step writes @xmath268 since @xmath269 it follows that @xmath270 i.e. , the convergence of the proposed method   is not slower than the one of the perturbed sd step   estimated in thm .",
    "[ thm : sd1 ] .    when @xmath271 we converge in one iteration , i.e. @xmath272 for large @xmath273 s.t .",
    "@xmath274 we can expect @xmath275 in general , however , the inequality @xmath276 is sharp . to show this ,",
    "consider @xmath277 with @xmath278 it is easy to show that @xmath279 which proves @xmath280 however , the ratio can be equal to one when @xmath281 and @xmath282 simultaneously .",
    "it can happen , eg .",
    "if @xmath283 is an eigenvector of @xmath116 similarly , if there is a @xmath75dimensional invariant subspace of @xmath9 which is orthogonal to @xmath284 we can form @xmath285 from the basis vectors of this subspace and have the same convergence @xmath286 as the sd step does .    to find the correction term @xmath58 we have to solve the reduced linear system size @xmath287 which writes as follows @xmath288 suppose that @xmath289 is still too large for the system to be solved exactly and we find the approximate solution @xmath290 the simplest idea is to solve the reduced problem by the standard sd method .",
    "the following theorem estimates the progress of such ` lazy ' approach .",
    "[ thm : sd2 ] consider the system @xmath242 with the initial guess @xmath250 and error @xmath291 after one outer step of sd   and one inner step of sd applied to the reduced problem  , the error @xmath196 writes as follows @xmath292 where @xmath293 is the @xmath294orthogonal projector on @xmath295 and @xmath296    if @xmath58 is the obtained ( approximate ) solution of  , the progress of the step   is @xmath297 where in the last line we use the @xmath9orthogonality of the two terms .",
    "the initial guess for @xmath58 is zero , and after one step of the sd applied to   the error is @xmath298 the first line of the theorem now follows by the definition of @xmath299 to prove the second line it is enough to note that @xmath300 and @xmath301 the progress of the inner sd step is @xmath302 where @xmath303 substituting these estimates to   we obtain the second claim of the theorem .",
    "now we prove that @xmath296 similarly to   we have @xmath304 since @xmath273 is orthogonal , @xmath305 it also holds that @xmath306 finally we show that @xmath307 which completes the proof .    the second term of   can be written also as follows @xmath308 which gives @xmath309 this shows that the combination of one outer and one inner sd step is equivalent to the sd step with perturbation  .",
    "this is also easily seen from the structure of our inner ",
    "outer method itself .",
    "indeed , in the outer step we add components @xmath310 to the basis set and in the inner step we add components of the inner residual @xmath311 where @xmath312 contains the elements of @xmath313 stretched into one vector .",
    "therefore , the described inner ",
    "outer scheme is equivalent to one ` global ' sd step .",
    "the idea behind theorem  [ thm : sd2 ] is of course not to prove a slightly worse estimate in a more complicated way . in the recursive algorithm the second term in",
    "will be obtained by the sd step followed by further optimization which will decrease the error of the reduced problem and consequently the total error .",
    "the sd step is therefore required as an initial guess for which we can provide a theoretical estimate of convergence .",
    "the practical convergence that we expect is of course better than the upper estimate in  .",
    "regarding the spectrum of reduced problems , the following two  side inequality is proved in  @xcite @xmath314 where @xmath56 is unitary matrix and @xmath315 means that @xmath316 is positive definite .",
    "the last inequality used in theorem [ thm : sd2 ] follows from the left part of this inequality ( which is itself rather elementary ) .",
    "system @xmath242 and initial guess @xmath250 in the tt ",
    "format  , approximate residual @xmath317 updated solution @xmath318 @xmath319 find @xmath320 @xmath321    in higher dimensions we can further improve the steepest descent step by an als cycle over the step vector , as shown by alg .",
    "[ alg : talsz ] .",
    "this algorithm searches for @xmath322 using the als optimization and therefore can be considered as a _",
    "greedy _ algorithm .",
    "the application of greedy algorithms to optimization in tensor formats was rigorously studied in  @xcite .",
    "[ alg : talsz ] starts from the sd step with perturbation , and then the energy function is additionally improved by an alternative minimization cycle .",
    "the combined progress is therefore not worse than the one of the sd step , @xmath323 given by thm .",
    "[ thm : sd1 ] .",
    "another estimate is proven by the following theorem .",
    "[ thm : sdd ] consider the system @xmath242 with the initial guess @xmath250 and error @xmath291 the step described by alg .",
    "[ alg : talsz ] returns the solution @xmath324 such that the error @xmath196 is bounded as follows @xmath325    in 2d the statement of the theorem reads @xmath326 it is easy to see that the als update over @xmath313 gives exactly the two  dimensional sd step   with the progress @xmath327 given by  .",
    "the als update over @xmath310 further improves the energy function by the factor @xmath328 which proves the statement of the theorem for @xmath329 the base of the recursion is proved .",
    "after a _ microstep _ when @xmath330 is optimized and becomes @xmath331 , the solution writes as follows @xmath332 where @xmath333 i.e. @xmath334 and @xmath335 this equation is similar to the two  dimensional sd step   and allows to estimate the progress of alg .",
    "[ alg : talsz ] using the result of thm .",
    "[ thm : sd2 ] recursively .",
    "following  , the progress can be written as follows @xmath336 where @xmath337 @xmath338 and @xmath339 is the exact solution of the reduced problem @xmath340 note that @xmath341 , so the inner sd steps will share the tt ",
    "factors of the same residual @xmath342    to prove the recursion step , assume that the theorem holds in the dimension @xmath343 write   with @xmath172 and apply   for the second term as follows @xmath344 where @xmath345 @xmath346 @xmath347 @xmath348 is the exact solution of @xmath349 @xmath350 is the @xmath294orthogonal projector on @xmath351 and @xmath352 is defined for @xmath353 similarly to  . since @xmath354 , and",
    "@xmath355 we have @xmath356 and @xmath357 similarly @xmath358 now defines the progress of the als microstep over the components of @xmath359 updating @xmath310 by the als step we reduce the error by the factor @xmath360 and write the total progress as follows @xmath361 which completes the proof .",
    "[ rem : d-1 ] under the conditions of the theorem it holds @xmath362 indeed , after the first als microstep the solution has the form @xmath363 see  . comparing this to the steepest descent in 2d   we follow   and claim the convergence rate @xmath364 for @xmath365 and consequently for the result of alg .",
    "[ alg : talsz ] due to the monotone convergence of the als .",
    "[ rem : nu=1 ] if als steps occasionally give no progress , i.e. @xmath366 the progress @xmath238 of alg .",
    "[ alg : talsz ] given by   satisfies @xmath367 it follows that in this case @xmath368 , and the convergence estimate given by the previous remark is better than the one given by the theorem .",
    "if a sensible estimates for @xmath369 are available , we can plug them in   to estimate the combined progress of the sd and als steps .",
    "[ alg : talsz ] is a greedy  type algorithm .",
    "such algorithms are likely to have a slow convergence or stagnate at some error level . to improve the practical convergence",
    "we can apply the als optimization to the whole solution vector @xmath370 as shown by alg .",
    "[ alg : alstz ] .    just like alg .",
    "[ alg : talsz ] , the non - greedy alg .",
    "[ alg : alstz ] starts from the steepest descent step and then improves the energy function by a number of als updates .",
    "therefore , the progress of alg .",
    "[ alg : alstz ] is estimated by the one of the sd algorithm , @xmath371 the better estimate of remark  [ rem : d-1 ] also applies to alg .",
    "[ alg : alstz ] , i.e. @xmath372 this follows from the fact that the optimization over @xmath373 gives better energy function than the optimization over the lower part of this tt ",
    "block @xmath374 , performed in greedy alg .  [ alg : talsz ] .",
    "however , we can not generalize the result of thm .",
    "[ thm : sdd ] for alg .",
    "[ alg : alstz ] , since the non - greedy als update destroys the @xmath375 structure of the interfaces .",
    "the practically observed convergence of this method is nevertheless much better than that of the greedy descent method .",
    "more rigorous analysis of the convergence of als schemes can probably provide much better estimates for the convergence rate of the proposed algorithm .    in the sequel we will develop a version of the algorithm which mixes the als and sd steps , following  , cf . line ` amen ' in table  [ tab : dmrg ] .",
    "for this algorithm it is possible to analyze the convergence recurrently similarly to theorem  .",
    "the mixed amen version also has better convergence properties for the practical problems considered in  @xcite .",
    "set @xmath376 find @xmath377 @xmath378",
    "throughout the paper , we considered vectors , perturbed due to the tensor approximation .",
    "now we highlight the practical features of this operation .",
    "the tt  rounding procedure @xcite performs the recursive svd - based truncations , which reduce the tt  ranks .",
    "the truncation of the @xmath75-th unfolding writes as follows , @xmath379 where matrices @xmath56 and @xmath257 are orthogonal .",
    "the approximation algorithm returns @xmath380 where @xmath381 contains the @xmath124 first ( dominant ) vectors of @xmath56 .",
    "it follows by the construction of the tt ",
    "svd algorithm that @xmath382 , and therefore @xmath383 .",
    "we rely on this property for the residual approximation in the accuracy analysis of the perturbed steepest descent method , see theorem [ thm : sd1 ] .",
    "the block version of the same orthogonality condition is used in the derivation of the two - dimensional steepest descent progress .",
    "the svd algorithm truncates a vector in the frobenius norm , i.e. chooses the approximation rank considering a sum of squared smallest singular values . to satisfy the accuracy assumption in we need to perform the accuracy control in the @xmath9-norms , @xmath384 .",
    "an optimal approximation in the @xmath9-norms is a difficult problem .",
    "we can either truncate in the frobenius norm and rely on the norm equivalence @xmath385 , or follow the cheap heuristic strategy proposed in @xcite . in the inner steps of the tt - rounding procedure ,",
    "after the svd is computed , we throw away the smallest singular values one by one , while the _ local _ error / residual is below the tolerance , i.e. @xmath386    the basis enrichment step developed in our paper can only increase the tt  ranks of the solution . to make the procedure computationally feasible , we need to introduce a truncation step , which will reduce the solution ranks . to do this",
    ", we apply the tt  rounding procedure between the iterations , which perturbs the solution and can increase the energy function .",
    "therefore , the truncation accuracy has to be chosen accurately to provide the convergence of the methods with approximation .",
    "assume that a step of the proposed method has the following progress , @xmath387 the progress after the approximation @xmath388 reads @xmath389 while the energy function is large , the first term dominates for sufficiently small @xmath390 . in the end of the process , the perturbation error is comparable to the progress of the method , and the algorithm stagnates .",
    "we will see this in numerical examples .",
    "let us verify the methods proposed on a model example of symmetric positive definite system : @xmath391^d , \\qquad \\left .",
    "x\\right|_{\\partial\\omega}=0,\\ ] ] where @xmath392 is the standard finite difference laplacian discretization on a uniform grid with the mode size @xmath393 in each direction , i.e. , the linear system has @xmath394 unknowns .",
    "the right  hand side @xmath395 is the vector of all ones .",
    "such a system arises naturally in the heat transfer simulation , or to precondition more complex elliptic problems .",
    "note that the matrix and the right  hand side have exact low ",
    "rank representations , see  @xcite .    for different @xmath0",
    "we compare the following methods in fig .",
    "[ fig : lp ] :    * the dmrg method presented in @xcite ( `` dmrg '' ) ; * the 2d sd method in a form @xmath396 ( `` @xmath397 '' ) ; * the greedy algorithm [ alg : talsz ] ( `` @xmath398 '' ) ; * the non - greedy algorithm [ alg : alstz ] ( `` @xmath399 '' ) ; * wherever possible , the standard ( vectorized ) steepest descend ( `` sd '' ) .    the tt ",
    "rank of the enrichment @xmath207 was chosen @xmath400 , and the solution after each step was approximated with the relative truncation tolerance @xmath401 in the frobenius norm .",
    "the convergence of the considered methods is compared in fig .",
    "[ fig : lp ] .",
    "a one  dimensional sweep is considered as one iteration , the progress of micro - iterations is also shown whenever possible .",
    "we can make the following remarks based on the experimental results .",
    "* als steps sufficiently improves the convergence of all considered methods , i.e. the pessimistic assumptions of remark  [ rem : nu=1 ] do not hold . a refined analysis of als convergence rates @xmath369 is still an open question .",
    "* the convergence of non - greedy alg .",
    "[ alg : alstz ] is comparable to the one of the dmrg iteration - wise .",
    "however , the complexity of each dmrg iteration is cubic in the mode size , while the proposed methods have linear complexity .",
    "this is clearly demonstrated in the right column , where the convergence is shown w.r.t . the computational time .",
    "the proposed methods time  wise are up to @xmath402 times faster than the dmrg for this problem . *",
    "the one - step steepest descend method shows the slowest convergence , which is a direct consequence of the narrow ( one vector ) direction subspace .",
    "this indicates that the upper bounds of the convergence rate established in the paper might be seriously overestimated .",
    "in this paper we equip the als scheme with a basis enrichment step , which is chosen in accordance with the steepest descent algorithm .",
    "the resulted method demonstrates the convergence almost as good as the one of dmrg , while has the linear in the mode size and dimension complexity of als .",
    "moreover , the global convergence rate is established similarly to the one of the steepest descent . up to the best of our knowledge ,",
    "this is the first result on the global convergence of a numerically efficient solution method for linear systems in higher dimensions .",
    "the proposed algorithm combines the advances of optimization methods in tensor formats ( als , dmrg ) with the ones of classical methods of numerical analysis .",
    "the proposed family of methods includes the algorithm with greedy  type step , for which the theoretical results obtained in the framework of greedy algorithms can be applied . however , other algorithms developed in the non - greedy style also have proven convergence rate and manifest much better convergence in numerical experiments .",
    "the results of this paper can be developed in the following directions .",
    "first , the analysis for the non  symmetric systems can be made similarly to this paper , substituting the steepest descent algorithm by the minimal residual method .",
    "the second krylov vector is required in minres  type algorithms , which have to be approximated and",
    "the convergence of perturbed method should be discussed similarly to the theorem  [ thm : sd1 ] .",
    "second , the complexity of the proposed methods w.r.t .",
    "tensor ranks should be studied and improved using faster ( eg , cross ) approximation schemes .",
    "finally , we will develop and analyze the amen method for which the enrichment steps are mixed with als optimization , i.e. , there is no explicit steepest descent step ."
  ],
  "abstract_text": [
    "<S> we introduce a family of numerical algorithms for the solution of linear system in higher dimensions with the matrix and right hand side given and the solution sought in the tensor train format . </S>",
    "<S> the proposed methods are rank  adaptive and follow the alternating directions framework , but in contrast to als methods , in each iteration a tensor subspace is enlarged by a set of vectors chosen similarly to the steepest descent algorithm . </S>",
    "<S> the convergence is analysed in the presence of approximation errors and the geometrical convergence rate is estimated and related to the one of the steepest descent . </S>",
    "<S> the complexity of the presented algorithms is linear in the mode size and dimension and the convergence demonstrated in the numerical experiments is comparable to the one of the dmrg  type algorithm .    _ </S>",
    "<S> keywords : _ high  dimensional problems , tensor train format , als , dmrg , steepest descent , convergence rate , superfast algorithms . </S>"
  ]
}