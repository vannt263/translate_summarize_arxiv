{
  "article_text": [
    "game strategy and playcalling are an integral part of american football . especially at the professional level coaches",
    "spend most of their time either analyzing the opponent s past games to find out about weaknesses or teaching their players how to take advantage of the opponent s weaknesses .",
    "even though american football is a very data - driven sports coaches have the final say over playcalling .",
    "this often leads to controversy over whether coaches made good or bad decisions .",
    "a prime example is this year s superbowl where the seahawks went with a pass instead of a run and ended up getting intercepted and loosing the game .",
    "we argue that computers could assist coaches or even completely take over playcalling duties by predicting the outcome of specific plays .",
    "+ for such an application we build the foundation by providing predictions for the outcome of a play based on the game situation and the play description . as a next step coaches could vary the play description using plays from their own playbook and see which play has the best chance of succeeding .",
    "+ this report is structured as follows : section  [ sec : related ] looks at related work on play prediction . in section  [ sec : dataset ] we describe the data set we used and how we obtained it .",
    "we examine the features of the data set in greater detail in section  [ sec : features ] . in section  [ sec : targets ] we identify preferable targets for our predictions , namely a binary success variable , the number of yards gained , and a newly defined real - valued progress measure .",
    "furthermore , we describe the various methods we used for classification and regression on the outcome of plays in section  [ sec : methods ] before evaluating and comparing the results in section  [ sec : eval ] .",
    "once the foundation is established , we give an example of how this work could be used in section  [ sec : applications ] .",
    "finally , section  [ sec : conclusion ] concludes with a summary and future outlook .",
    "stilling and critchfield  [ 6 ] used generalized matching equations to analyze the relationship between play selection and yards gained .",
    "they found that undermatching , which may result from the tendency of coaches to `` mix up '' plays , a bias towards rushing plays , and that the generalized matching equation accounted for a majority of the variability in play selection . using specialized equations for each down revealed that first down is biased towards rushing , and later downs are biased towards passing .",
    "it was also found that rushing was preferred when fewer than 4 yards remained to first down , and that passing was preferred when more than 10 yards remained .",
    "kovash and levitt  [ 7 ] analyzed whether in american football and baseball decisions conform with minimax . by studying 125,000 nfl plays from 2001 - 2005",
    "they found negative serial correlation and thus predictable tendencies of coaches in calling run or pass plays .",
    "specifically , their results indicate that playcalling should focus more on passing - a conclusion that is inconsistent with minimax theory and that at the same time is the basis for the main application of this work .",
    "mitchell  [ 8 ] enhanced the matching pennies model by kovash and levitt  [ 7 ] by adding the notion of investment to playcalling .",
    "this helps explain the commonly accepted ideas of `` running to wear down a defense '' and `` running to set up the pass '' .",
    "reed , critchfield and martens  [ 9 ] analyzed playcalling with the generalized matching law , a mathematical model of operant choice .",
    "they discovered a bias for calling rushing plays and undermatching in the sense of imperfect playcalling .",
    "mcgarrity and linnen  [ 5 ] developed a game theoretic model to analyze how a team changes its play calling when the starting quarterback is replaced by its backup .",
    "replacement quarterbacks are usually less - experienced which means passing the ball is of higher risk .",
    "the paper refers to standard optimization theories which suggest that the team should run more often since in contrast to the quarterback the productivity of the running back has not changed .",
    "however , their findings say teams will not change their play calling because the defense will expect more run plays and will consequently play more often against them .",
    "that is , quarterback substitutions have less impact than expected .",
    "that supports our approach of considering only team - based features and disregarding features based on individuals .",
    "+ molineaux , aha and sukthankar  [ 11 ] used plan recognition to identify the defensive strategy and thus improve the results of their case - based q - learning algorithm .",
    "they evaluated this on the open - source american football simulator rush 2008 . while this is an abstraction from real football at the moment they are working on making it more realistic in the future for their purposes .",
    "+ lutz  [ 21 ] already identified related work some of which we refer to in the following .",
    "most research in sports prediction focuses on predicting the winner of a match instead of predicting individual plays .",
    "min et al .",
    "[ 15 ] used bayesian inference and rule - based reasoning to predict the result of american football matches .",
    "their work is based on the idea that sports is both highly probabilistic and at the same time based on rules because of team strategies .",
    "sierra , fosco and fierro  [ 14 ] used classification methods to predict the outcome of an american football match based on a limited number of game statistics excluding most scoring categories . in their experiments , linear support vector machines had by far the most accurate predictions and ranked above all expert predictions they compared with .",
    "similarly , harville  [ 16 ] used a linear model for the same task . a neural network approach to predicting the winner of college football games was proposed by pardee  [ 17 ] .",
    "his accuracy of @xmath0 improved on expert predictions .",
    "purucker  [ 13 ] and kahn  [ 12 ] applied neural networks to nfl game predictions and mostly improved on existing accuracies .",
    "a problem of both their results is the small test set since they only evaluated on a total of two weeks of nfl games .",
    "stefani  [ 18 ] used an improved least squares rating system to predict the winner for nearly 9000 games from college and pro football and even other sports .",
    "fokoue and foehrenbach  [ 19 ] have analyzed important factors for nfl teams with data mining .",
    "their results are especially useful for this work because the first step for predicting plays involves identifying important features .",
    "the api _ nflgame _  [ 2 ] provides access to data from all nfl games of the last six years ( 2009 - 2014 ) on a play - by - play basis . from this data we extracted a total of 12 features and filtered out irrelevant plays which left us with 177245 plays .",
    "the following sections describe this process and the features in greater detail .",
    "for each play we receive a data structure from the api .",
    "this structure contains basic information about the current situation of the game like the game clock , the field position , the possessing team , etc .",
    "it also contains a string which describes what happened on the field .",
    "apart from the actual play this string can describe injuries , timeouts , quarterback substitutions , penalties , or other information .",
    "since we are only interested in the actual plays we need to filter out all the irrelevant information .",
    "that is , we need to find the sentence which describes the actual play . in cases of penalties we reject the entire string because a penalty impacts the course of the play .",
    "furthermore , there are plays which are of no interest for us which will be also ignored .",
    "these include field goals , punts , sacks , fumbles , etc .",
    "the final string that describes the current play will then be parsed to extract the features .",
    "table  [ tab : feature_table ] lists and describes all the features we use .",
    "| l | l | x | * feature * & * type * & * description * + team & categorical & the name of the team which has the ball + opponent & categorical & the name of the team which is defending + half & continuous & describes whether it is the first or the second half of the game + time & continuous & remaining time ( in seconds ) in the current half + field position & continuous & distance to the opponent s end zone ( in yards ) + down & continuous & the current down ( 1 , 2 , 3 or 4 ) + to - go & continuous & the remaining distance to the next first down ( in yards ) + shotgun & binary & flag , whether the team starts the play in the shotgun formation + pass & binary & flag , whether the quarterback passes the ball + side & categorical & side to which the quarterback passes or the rusher runs ( left , middle , or right ) + pass length & categorical & in case of a pass it describes whether it is a short or a deep pass .",
    "otherwise it is 0 .",
    "+ qb run & binary & flag , whether the quarterback runs himself +      table  [ tab : feature_examples ] in the appendix shows some example play descriptions and the extracted features and labels .",
    "omitted features have value 0 .",
    "note that not all features are extracted from the description string .",
    "some features are directly obtained from the data structure obtained from the api .",
    "the table also lists the different labels which are explained in section  [ sec : targets ] .",
    "some of our features have categorical values .",
    "for example either of the features @xmath1 and @xmath2 take on a value representing one of the 32 teams . simply numbering the teams from 1 to 32",
    "would not represent the real situation because team # 1 is not closer to team # 2 than it is to team # 32 .",
    "consequently , we need a more sophisticated encoding . for that reason ,",
    "we have chosen one - hot encoding for our categorical features .",
    "that is , each categorical feature is replaced by @xmath3 binary features where @xmath3 is the number of possible values .",
    "for example , the feature `` team '' is replaced by 32 features like @xmath4 , @xmath5 , and so on . in each sample",
    "only one of these features has the value `` 1 '' while all others are `` 0 '' .",
    "encoding all our categorical features of our data set expands the size from 12 to 77 dimensions .      in order to analyze the distribution of data we performed a principal component analysis ( pca ) .",
    "figure  [ fig : variance ] shows a plot of the variance ratios of the resulting components .",
    "the first component already covers more than 99.7% of the variance .",
    "figure  [ fig:2dproj ] shows a projection of the entire data set on the two dimnsions that represent most of the variance .",
    "red points represent successful plays and blue points are failure plays .",
    "the distribution of the plays shows some sort of pattern or structure .",
    "noticeable are the two red clusters close to the left and the right edge . however , the data is far too noisy to separate the two classes .",
    "note that pca relies on continuous - valued variables while our data consists of a mixture of continuous and binary values .",
    "as discussed by yu and tresp [ 20 ] there are some issues when applying pca to mixed types of data .",
    "hence , in our case pca helps us to explore the data but it is not suitable for a dimensionality reduction . a different approach of reducing the dimensionality is to just remove features which have only very little impact on the classification .",
    "one method is to calculate the analysis of variance ( anova ) f - test statistic and rank the features according to their score .",
    "the anova f - test statistic is the ratio of the average variability among groups to the average variability within groups .",
    "that is , it determines whether the means between the two groups are significantly different .",
    "table  [ tab : feature_scores ] in the appendix shows the computed f - values of each feature .",
    "it can be seen that the feature for the remaining distance to the next first - down ( @xmath6 ) has by far the highest score .",
    "an interesting observation is the wide range of scores of team - specific features . for example",
    "the feature for the new england patriots ( @xmath5 ) has a score which is around a million times higher than for the miami dolphins ( @xmath7 ) .",
    "this gives evidence that some teams might be more predictable than others .",
    "apart from the features we also have to extract the ground truth labels of each play . since we pursue multiple approaches we also have to extract multiple labels .",
    "table  [ tab : label_table ] lists and describes all the labels we use .",
    "| l | l | x | * label * & * type * & * description * + success & binary & indicates whether a play resulted in a first down or a touchdown .",
    "+ yards & continuous & indicates how many yards a team has gained through this play . + progress & continuous & a newly defined measure for progress based on the current down , achieved yards and yards to go .",
    "+    while the prediction of _ success _ is a classification task , _ yards _ and _ progress _ are real numbers requiring regression methods .",
    "we examine various methods for classification and regression in section  [ sec : methods ] .",
    "+ when we use the binary value _ success _ we define successful plays as plays which either obtain a first down or score a touchdown . in all other cases",
    "the play will be classified as failure . with this target definition",
    "our data set is quite imbalanced since it contains about 70% unsuccessful and only 30% successful samples .",
    "that imbalance needs to be taken into account at the different classification methods or else a classifier that classifies each sample as `` failure '' would achieve a classification accuracy of 70% which is actually quite good .",
    "for that reason we do not only evaluate the accuracy but also the recall and the precision .",
    "those are two error measures which evaluate the quality of the classification .",
    "the recall tells how many of the successful samples were classified correctly , while the precision tells how many of the _ success _ predictions were actually correct . reconsidering the case",
    "were all samples are predicted as `` failure '' , this would yield a precision and recall of 0% .",
    "so , although the accuracy is quite high the quality of the classification is pretty poor . a good way to combine recall and precision is the @xmath8 score which is the harmonic mean of those two values .",
    "the @xmath8 score is calculated as shown in the following equation : @xmath9 still , we argue that both _ success _ and _ yards _ are unable to capture whether the result of a play is actually desirable .",
    "for example , a 9 yard gain on 1st down and 10 yards to go is given the same label ( 0 ) as a 10 yard loss .",
    "_ success _ can not represent this difference .",
    "_ yards _ has similar issues .",
    "imagine a team has a fourth down and 28 yards to go , but achieves only 27 yards .",
    "in most situations 27 yards is a great result for a play , but not here since the play would result in a turnover on downs .",
    "_ yards _ would still get the very good value of 27 . + as a consequence of these observations we invented a completely new success measure which we call",
    "_ progress_. the special thing about this label is that also takes the current down into account .",
    "it is calculated as follows with @xmath10 representing the current down , @xmath6 being the remaining yards to go for a first down and @xmath11 representing the yards gained on the play : @xmath12        the idea behind this is the following : if a new first down is not reached on third or fourth down it is ( in most cases ) a failure on the side of the offense and will result directly in a turnover on downs ( in case of fourth down ) or a punt ( in case of third down ) .",
    "we neglect the case when teams are in field goal range and can still score because the ultimate goal is always to have a touchdown .",
    "these cases are all combined and labeled as 0 which represents failure .",
    "any real number larger than zero represents success to some extent .",
    "the larger the number the better . a first down",
    "is still represented as 1 . since 0",
    "is a failure and 1 is a first down , it intuitively makes sense to label progress towards a first down with a number between 0 and 1 .",
    "progress even after the first down does not get a larger value .",
    "if the first down was not reached , the value is dependent on the ratio of the remaining yards and the yards that were gained .",
    "depending on the down this ratio can be squared .",
    "the idea behind this is as follows : imagine the following two situations : ( 1 ) 5 yards were gained on first down .",
    "( 2 ) 5 yards were gained on second down ( after no gain on first down ) .",
    "even ( 2 ) is solid and leaves only 5 yards for third down .",
    "the ratio values are therefore @xmath13 and @xmath13 for first and second down , respectively .",
    "however , the second value is as large as the first even though the remaining attempts are less . in order to fix this issue",
    ", we argue that whichever gain is achieved on second down should be penalized in some way since only one additional attempt is available assuming fourth down is used for a punt or field goal .",
    "after first down , two more attempts are left which puts the offense in a more comfortable situation .",
    "the actual _ progress _ scores using a quadratic function for second down which are therefore @xmath14 and @xmath15 .",
    "figure  [ fig : progress ] illustrates the idea . while first down progress scores are awarded linearly , second down progress scores are calculated by a quadratic function . the closer one gets to a first down , the closer the progress values of the two get .",
    "+ our first idea of a progress measure increased linearly even after the first down for additional yards , but with the larger numbers even the average error increased dramatically .",
    "for example , a prediction of 1.0 on first down - or in other words a 10 yard gain - would result in an error of 8 if the play was actually a 90 yard play ( 9.0 real score ) whereas an incomplete pass would only result in an error of 1 ( 0.0 real score ) .",
    "this is even when the play was actually a 10 yard pass and the receiver managed to break free .",
    "intuitively , the positive prediction should get a relatively low error which is not the case .",
    "we argue that it s more about getting a first down or not instead of whether we make some yards more or less after the first down since it s the first down that keeps the offense on the field . due to this reasoning we decided for a constant value of 1 for all play that reach a new first down .",
    "this section shortly explains all methods we use for classification and regression .",
    "classification trees  [ 10 ] are binary trees where each internal node defines a rule for one of the input features which divides the data into two subsets aiming for the best split . in order to avoid over - fitting",
    "it is useful to define a maximum depth for the tree .",
    "we used balanced class weighting to handle the imbalanced data .",
    "that is , the weighting factors for the classes are automatically adjusted to their inverse frequencies in the input data .",
    "figure  [ fig : dectree ] shows a simple classification tree trained on our data set .",
    "it contains only a single decision rule which divides the data set into two subsets .",
    "the only decision rule here is whether the next first down is more than 7.5 yards away .",
    "that is , all plays where the team has 7.5 yards or less to go for their next first down are classified as `` success '' and all others as `` failure '' .",
    "this simple rule already gives a surprisingly good accuracy of 69.2% with 47.9% precision and 50.0 % recall .",
    "table  [ tab : tree_results ] in the appendix shows the classification performance of classification trees with different depth limitations .",
    "the classification tree with just one rule achieves actually the best accuracy but its precision and recall are quite bad .",
    "the tree with a maximum depth of six achieves the highest f1 score .",
    "regression trees are pretty similar to classification trees as they also use a set of rules to split the data into subsets .",
    "however , each final remaining set gets labeled with a real value .",
    "figure  [ fig : regtree ] shows a simple regression tree trained on our data set for predicting the gained distance on a play .",
    "it contains only a single decision rule which divides the data set into two subsets .",
    "the only decision rule here is whether a deep pass has been played .",
    "that is , the first subset contains all plays with either a short pass or a run play and the second subset contains all plays with a deep pass .",
    "the first subset gets labeled with 5.3 yards and the second subset with 11.1 yards . with this decision rule",
    "we achieve a mae of 5.7 yards and rmse of 8.4 yards .",
    "table  [ tab : regtree_results ] in the appendix shows the performance of the regression tree with different depth limitations .",
    "it can be seen that the performance stays quite constant . however , using no depth limitation causes the algorithm to overfit the training data which results in poor performance .",
    "for nearest centroid classification we first calculate the centroids of the success and failure labels of the feature vectors in our training set .",
    "then to predict the classification of a given feature vector we simply assign it to the class with the nearest centroid .",
    "this method provided approximately 50% accuracy , precision , and recall after we undersampled the training set to have equal amounts of success and failure labels , so overall it is no better than a random assignment of the labels .",
    "this suggests that the classifications are not divided into two distinct clusters some distance apart , but are positioned in a way that makes distance from the centroids a poor indicator of class .      for our linear discriminant analysis classification we used scikit learn s ldaclassifer .",
    "we then compared the results between using singular value decomposition , least squares , and eigenvalue decomposition to solve the lda problem before classification .",
    "both the least squares and eigenvalue decomposition solutions also used shrinkage to try and further improve the accuracy on our high dimensional feature vectors .",
    "all three solutions gave similar results with around 66% classification accuracy and f1 scores .",
    "for support vector machines we first tried a simple linear decision boundary on an undersampled training set , and after performing a search over varying penalty values using 5-fold validation to compare performance .",
    "our best result was 57.72% accuracy at @xmath16 .",
    "we then tried using a gaussian radial basis function as our kernel and performed a grid search over @xmath17\\}$ ] and @xmath18\\}$ ] .",
    "we achieved our best accuracy of 66.65% at @xmath19 and @xmath20 .",
    "this accuracy is similar to what we achieved using the lda classifiers , but not an improvement .      for regression we considered two different metrics : _ yards _ to go , and _ progress _ , which are discussed in the section  [ sec : targets ] . once again we grid searched over values of @xmath17\\}$ ] and @xmath18\\}$ ] to try and find good hyperparameters for support vector regression .",
    "our best results were found at @xmath21 and @xmath20 with a mean error of 5.207 yards and root mean squared error of 8.977 yards .",
    "we were left dissatisfied by the performance in this regard as most frequently a first down is less than 10 yards away , so an error of 5 yards could be very significant . for this reason we continued using the progress metric , and obtained a mean error of 0.1351 and root mean squared error of 0.2332 . as a rough estimate",
    "you can consider 0.13 progress to be approximately 13% of the distance to a first down .",
    "so with the progress metric we obtained a much more useful prediction in terms of mean error , although the root mean squared error of 0.2332 shows that there is still quite a bit of variance in our results .",
    "our artificial neural network implementation uses the pybrain  [ 3 ] library .",
    "pybrain allows the user to specify a number of parameters including the number of hidden layers , the number of units per hidden layer , the type of units in the hidden layers and the maximum number of epochs for training the network .",
    "+ it is worth mentioning that the training of the neural networks took considerably longer than model fitting for any other method which is why less configurations were explored .",
    "moreover , this could be a disadvantage for a scenario when the network has to be retrained in real - time between plays or drives .",
    "for our experiments it meant that re - training the networks after taking into account the imbalanced data set for success classification was not possible . without that , the neural networks classified almost everything ( or nothing ) as a failure .",
    "+ we only present values that were reasonable . especially the networks with linear units in the hidden layers performed badly and",
    "were not used for further experiments .",
    "the results are shown in tables  [ tab : neuralnet_progress_results ] , [ tab : neuralnet_yards_results ] and  [ tab : neuralnet_success_results ] in the appendix . the only measure where the neural networks performed equally well compared to other regression methods was yards with a mae of 5.524 and a rmse of 8.751 .",
    "the complete result tables are in the appendix . tables  [ tab : classifier_results ] , [ tab : regression_results_yards ] and [ tab : regression_results_progress ] contain a compressed version of the results for the _ success _ , _ yards _ and _ progress _ measures , respectively .",
    "we report only the data of the configuration with the best results for each of the approaches . +    .8 | x | c | c | c | c | * method * & * accuracy * & * precision * & * recall * & * f1 * + linear svm & 57.72% & 58.74% & 51.59% & 54.93% + rbf svm & 66.65% & * 67.62*% & 63.75% & 65.63% + svd lda & 66.91% & 67.20% & 65.05% & * 66.11*%",
    "+ least squares lda & 66.70% & 67.24% & * 64.97*% & 66.08% + eigenvalue lda & 66.69% & 67.40% & 64.48% & 65.91% + nearest centroid & 50.74% & 50.68% & 50.98% & 50.83% + decision trees & * 67.14% * & 46.08% & 66.90% & 54.57% +    when comparing the results for _ success _ accuracy is not the main criterion . consider the real - world scenario where a team uses an application based on ones of these methods to select the next play .",
    "we have to make sure that plays that would result in a failure are classified as such , even if it is at the cost of classifying successful plays as failures .",
    "this is represented by precision or , in other words , the portion of true positives among all positives .",
    "high recall and accuracy , on the other hand , are nice to have but not as important . based on this observation support vector machines ( svm ) using a radial basis function ( rbf ) kernel is the best method with a precision of 67.62% . in other words , in two thirds of the cases where this method predicts a play to result in a first down or touchdown the play actually is successful .",
    "other methods perform similarly well and have a slightly higher recall and accuracy .",
    "still , for applications this is most likely not enough .",
    "+    .5 | x | c | c | * method * & * mae * & * rmse * + rbf svr & * 5.207 * & 8.977 + linear regression & 5.569 & 8.309 + regression trees & 5.491 & * 8.299 * + neural network & 5.524 & 8.751 +    as an alternative to _ success _ we also considered _ yards_. as expected , _ yards _ is not a useful measure of success since it is prone to have large errors even for comparably good predictions , e.g. a 80-yard gain on a play with a predicted 30-yard gain results in a huge error .",
    "this is confirmed by the results in table  [ tab : regression_results_yards ] .",
    "the method with the lowest mean absolute error of 5.207 ( mae ) is the rbf svr . considering that the distance to a first down is initially 10 yards , an error of more than 5 yards is quite substantial .",
    "the same applies to the root mean squared error ( rmse ) of more than 8 for all methods .",
    "we conclude that _ yards _ can not be used for a good application due to the high errors .",
    "+    .5 | x | c | c | * method * & * mae * & * rmse * + rbf svr & * 0.1351 * & 0.2332 + linear regression & 0.1412 & 0.2283 + regression trees & 0.1424 & * 0.2131 * + neural networks & 0.1575 & 0.2449 +    finally , let us evaluate the results of the regression models using the _ progress _ measure that we defined in section  [ sec : targets ] . apart from the regression trees all of the other three methods performed fairly well .",
    "the best results were achieved by the rbf svr with a mae of 0.1351 and a rmse of 0.2332 . in order to understand the intuition",
    "let us consider the following example .",
    "for a typical 1st and 10 situation where no first down is achieved this would mean that our prediction is on average only 1.3 yards away from the real gain . for real predictions",
    "this is a very promising result .",
    "it is even more encouraging when we consider that third and fourth down situations still have potentially high errors since it is about predicting a binary variable . for these experiments ,",
    "we did not round the third and fourth down predictions to 0 or 1 which might reduce the error even further .",
    "given that our methods evaluate the success of a play in a certain game situation one can go ahead and use all different plays a team has learnt . by that , the coach can determine which play is suited best in this particular game situation .",
    "unfortunately , it is impossible to evaluate the effectiveness of this method based on our data set .",
    "we only have the result of the one play that was chosen , not the results of all plays that are in the team s playbook . the only way to measure whether it is indeed useful is to have actual teams use the method for a while and not use it for another time period and compare the success .",
    "even then , one could argue that a number of factors other than the playcalling play a role including injuries , form , temperature , weather , etc .",
    "in this work we aimed at providing predictions for the outcome of a specific play in a particular game situation in the nfl . this is a novel approach that could be used to identify the best play in real games by finding the available play with the maximum success before the play is actually executed .",
    "+ in order to define a proper measure of success of a play we have come up with a new measure called _ progress _ that is both intuitive and more accurately predictable than other measures at the same time .",
    "the accuracy we achieved by using a number of different regression models is promising for our intended application . especially with more comprehensive data ( formations , routes , players , etc . ) , more samples , advanced regression methods to get higher accuracy , and possibly by considering series of plays instead of single plays , the actual use in real - world playcalling is viable .",
    "+ furthermore , it could be useful to train classification or regression methods on plays of a specific team .",
    "the anova statistic has shown that some teams are probably more predictable than others .",
    "this approach seems also more relevant for a real - world application since a coach is only interested in finding the best play for his own team",
    ". the same approach could be used for a certain opponent .",
    "for example , when a team has a winning streak and seems to be unbeatable everyone is interested in finding out how to win against this team . +",
    "all of our source code is on github  [ 1 ] .",
    "we explicitly encourage others to try using , modifying and extending it .",
    "feedback and ideas for improvement are most welcome . + finally , we would like to give credit to three open - source projects without which this work would not have been possible : scikit - learn  [ 4 ] , pybrain  [ 3 ] and nflgame  [ 2 ] .",
    "| x | l | l | * description * & * features * & * labels * +               | x | r | * feature * & * f - value * + togo & 16146.92 + down & 6690.42 + pass & 2927.44 + passlen = short & 1256.44 + passlen = deep & 867.02 + shotgun & 642.94 + position & 295.36 + qbrun & 87.01 + time & 56.66 + team = no & 54.67 + team = ne & 48.01 + side = left & 32.54 + team = cle & 23.23 + team = dal & 22.78 + opponent = nyj & 22.67 + team = stl & 21.45 + opponent = sf & 20.35 + team = gb & 19.44 + team = sd & 18.26 + team",
    "= buf & 15.45 + team = ten & 14.05 + team = nyj & 13.90 + team = tb & 13.64 + side = middle & 13.63 + team = pit & 13.57 + opponent = bal & 13.15 + team = oak & 13.12 + team = jac & 13.11 + team = ari & 12.61 + team = phi & 10.11 + team = den & 8.76 + opponent = tb & 8.72 + opponent = ne & 8.56 + team = kc & 7.81 + opponent = sea & 7.45 + opponent = pit & 6.60 + opponent = dal & 6.11 + opponent = ari & 5.65 + team = hou & 4.80 +     |",
    "x | r | * feature * & * f - value * + team = sf & 4.43 + opponent = ind & 3.87 + opponent = atl & 3.79 + opponent = cin & 3.67 + half & 3.50 + side = right & 3.48 + opponent = was & 3.23 + team = bal & 2.64 + team = cin & 2.63 + opponent = ten & 2.56 + opponent = nyg & 2.29 + opponent = min & 2.24 + team = atl & 2.21 + opponent = den & 1.97 + opponent = stl & 1.92 + opponent = chi & 1.87 + opponent = jac & 1.83 + team = sea & 1.80 + opponent = sd & 1.67 + opponent = no & 1.63 + opponent = hou & 1.57 + team = ind & 1.17 + team = chi & 0.87 + opponent = gb & 0.63 + team = was & 0.57 + team = nyg & 0.49 + team = car & 0.44 + opponent = kc & 0.44 + opponent = car & 0.38 + opponent = mia & 0.36 + opponent = buf & 0.10 + opponent = cle & 0.09 + opponent = phi & 0.09 + team = det & 0.06 + team = min & 0.06 + opponent = det & 0.02 + opponent = oak & 0.001 + team = mia & 0.00005 +    0.8 | > x | c | c | c | c | * maximum tree - depth * & * accuracy * & * precision * & * recall * & * @xmath8 * + 1 & * 69.196% * & * 47.891% * & 49.965% & 48.906% + 2 & * 69.196% * & * 47.891% * & 49.965% & 48.906% + 4 & 68.940% & 47.847% & 58.558% & 52.663% + 6 & 67.135% & 46.079% & * 66.904% * & * 54.573% * + 8 & 67.589% & 46.521% & 65.851% & 54.524% + 10 & 67.164% & 46.054% & 65.868% & 54.207% + 12 & 69.66% & 45.798% & 65.166% & 53.792% + 15 & 65.957% & 44.703% & 64.887% & 52.936% + no limit & 64.321% & 40.143% & 42.607% & 41.338% +        1 & 5.7434 yds & 8.4103 yds & 0.1667 & 0.2264 + 2 & 5.6762 yds & 8.3468 yds & 0.1508 & 0.2174 + 4 & 5.5134 yds & 8.2730 yds & 0.1451 & 0.2141 + 6 & 5.4943 yds & * 8.2682 yds * & 0.1429 & * 0.2131 * + 8 & * 5.4906 yds * & 8.2991 yds & * 0.1424 * & * 0.2131 * + 10 & 5.5137 yds & 8.3870 yds & * 0.1424 * & 0.2138 + 12 & 5.5520 yds & 8.5435 yds & 0.1427 & 0.2153 + 15 & 5.6350 yds & 8.8265 yds & 0.1436 & 0.2195 + no limit & 7.2699 yds & 11.7828 yds & 0.1727 & 0.3026 +       & * rmse * & * mae * + 1 & 10 & sigmoid & * 8.595409 * & 5.780438 + 1 & 10 & tanh & 8.595958 & 5.768907 + 1 & 50 & sigmoid & 8.599206 & 5.761803 + 1 & 50 & tanh & 8.621712 & 5.709222 + 1 & 100 & sigmoid & 8.611599 & 5.663897 + 1 & 100 & tanh & 8.650681 & 5.660227 + 10 & 10 & sigmoid & 8.595773 & 5.774482 + 10 & 10 & tanh & 8.599356 & 5.759645 + 10 & 50 & sigmoid & 8.595777 & 5.769012 + 10 & 50 & tanh & 8.618312 & 5.882702 + 10 & 100 & sigmoid & 8.599489 & 5.778361 + 10 & 100 & tanh & 8.751191 & * 5.523861 * +     | > x | c | c | c | c | c | c | * hidden layers * & * hidden units * & * hidden class * & * accuracy * & * precision * & * recall * & * @xmath8 * + 1 & 10 & sigmoid & 0.704923 & 0.250000 & 0.000038 & 0.000076 + 1 & 10 & tanh & 0.704911 & 0.312500 & 0.000096 & 0.000191 + 1 & 10 & linear & 0.295055 & 0.295055 & * 1.000000 * & * 0.455664 * + 1 & 50 & sigmoid & 0.704911 & 0.486957 & 0.002142 & 0.004264 + 1 & 50 & tanh & 0.703112 & 0.363789 & 0.008299 & 0.016227 + 1 & 50 & linear & 0.295055 & 0.295055 & * 1.000000 * & * 0.455664 * + 1 & 100 & sigmoid & 0.704889 & 0.477477 & 0.002027 & 0.004037 + 1 & 100 & tanh & 0.702113 & 0.345062 & 0.010689 & 0.020736 + 1 & 100 & linear & 0.295055 & 0.295055 & * 1.000000 * & * 0.455664",
    "* + 5 & 100 & sigmoid & * 0.706598 * & 0.593610 & 0.017764 & 0.034496 + 5 & 100 & tanh & 0.704945 & 0.500000 & 0.000057 & 0.000115 + 10 & 10 & sigmoid & 0.704945 & 0.000000 & 0.000000 & 0.000000 + 10 & 10 & tanh & 0.704939 & 0.000000 & 0.000000 & 0.000000 + 10 & 10 & linear & 0.295055 & 0.295055 & * 1.000000 * & * 0.455664 * + 10 & 50 & sigmoid & 0.705430 & * 0.643333 * & 0.003690 & 0.007339 + 10 & 50 & tanh & 0.704945 & 0.000000 & 0.000000 & 0.000000 + 10 & 50 & linear & 0.295055 & 0.295055 & * 1.000000 * & * 0.455664 * + 10 & 100 & sigmoid & 0.705199 & 0.574751 & 0.003308 & 0.006578 + 10 & 100 & tanh & 0.704945 & 0.000000 & 0.000000 & 0.000000 + 10 & 100 & linear & 0.295055 & 0.295055 & * 1.000000 * & * 0.455664 * +"
  ],
  "abstract_text": [
    "<S> based on nfl game data we try to predict the outcome of a play in multiple different ways including decision and classification trees , nearest neighbors , naive bayes , linear discriminant analysis , support vector machines and regression , and artificial neural networks . </S>",
    "<S> an application of this is the following : by plugging in various play options one could determine the best play for a given situation in real time . while the outcome of a play can be described in many ways we had the most promising results with a newly defined measure that we call _ </S>",
    "<S> progress_. we see this work as a first step to include predictive analysis into nfl playcalling . </S>"
  ]
}