{
  "article_text": [
    "large graphs with _ billions _ of nodes and edges are increasingly common in many domains and applications , such as in studies of social networks , transportation route networks , citation networks , and many others . distributed frameworks ( find a thorough review in the work of lu _ et al . _",
    "@xcite ) have become popular choices for analyzing these large graphs .",
    "however , distributed approaches may not always be the best option , because they can be expensive to build @xcite , and hard to maintain and optimize .",
    "these potential challenges prompted researchers to create single - machine , billion - scale graph computation frameworks that are well - suited to essential graph algorithms , such as eigensolver , pagerank , connected components and many others .",
    "examples are graphchi  @xcite and turbograph  @xcite",
    ". frameworks in this category define sophisticated processing schemes to overcome challenges induced by limited main memory and poor locality of memory access observed in many graph algorithms .",
    "however , when studying previous works , we noticed that despite their sophisticated schemes and novel programming models , they do not optimize for disk operations and data locality , which are the core of performance in graph processing frameworks .    in the context of _",
    "single - node _ , _ billion - scale _ , graph processing frameworks , we present * m - flash * , a novel scalable framework that overcomes critical issues found in existing works . the innovation of m - flashconfers it a performance many times faster than the state of the art .",
    "more specifically , our contributions include :    1 .",
    "* m - flashframework & methodology : * we propose a novel framework named m - flashthat achieves fast and scalable graph computation .",
    "m - flash(https://github.com / m - flash ) introduces the bimodal block processing , which significantly boosts computation speed and reduces disk accesses by dividing a graph and its node data into blocks ( dense and sparse ) to minimize the cost of i / o .",
    "* programming model : * m - flashprovides a flexible and simple programming model , which supports popular and essential graph algorithms , e.g. , pagerank , connected components , and the _ first _ single - machine eigensolver over billion - node graphs , to name a few .",
    "* extensive experimental evaluation : * we compared m - flashwith state - of - the - art frameworks using large graphs , the largest one having 6.6 billion edges ( yahooweb https://webscope.sandbox.yahoo.com ) .",
    "m - flashwas consistently and significantly faster than graphchi @xcite , x - stream @xcite , turbograph @xcite , mmap @xcite , and gridgraph @xcite across all graph sizes .",
    "furthermore , it sustained high speed even when memory was severely constrained , e.g. , 6.4x faster than x - stream , when using 4 gb of random access memory ( ram ) .",
    "a typical approach to scalable graph processing is to develop a distributed framework .",
    "this is the case of gbase @xcite , powergraph , pregel , and others @xcite . among these approaches ,",
    "gbase is the most similar to m - flash .",
    "despite the fact that gbase and m - flashuse a block model , gbase is distributed and lacks an adaptive edge processing scheme to optimize its performance .",
    "such scheme is the greatest innovation of m - flash , conferring to it the highest performance among existing approaches , as demonstrated in section [ sec : evaluation ] .    among the existing works designed for single - node processing , some of them",
    "are restricted to ssds .",
    "these works rely on the remarkable low - latency and improved i / o of ssds compared to magnetic disks .",
    "this is the case of turbograph @xcite , which relies on random accesses to the edges  not well supported over magnetic disks .",
    "our proposal , m - flash , avoids this drawback by avoiding random accesses .",
    "graphchi @xcite was one of the first single - node approaches to avoid random disk / edge accesses , improving the performance over mechanical disks .",
    "graphchi partitions the graph on disk into units called _ shards _ , requiring a preprocessing step to sort the data by source vertex .",
    "graphchi uses a vertex - centric approach that requires a shard to fit entirely in memory , including both the vertices in the shard and all their edges ( in and out ) .",
    "as we demonstrate , this fact makes graphchi less efficient when compared to our work .",
    "m - flashrequires only a subset of the vertex data to be stored in memory .",
    "mmap @xcite introduced an interesting approach based on os - supported mapping of disk data into memory ( virtual memory ) .",
    "it allows graph data to be accessed as if they were stored in unlimited memory , avoiding the need to manage data buffering .",
    "our framework uses memory mapping when processing edge blocks but , with an improved engineering , m - flashconsistently outperforms mmap , as we demonstrate .",
    "gridgraph @xcite divides the graphs into blocks and processes the edges reusing the vertices values loaded in main memory ( in - vertices and out - vertices ) .",
    "furthermore , it uses a two - level hierarchical partitioning to increase the performance , dividing the blocks into small regions that fit in cache . when comparing gridgraph with m - flash , both divide the graph using a similar approach with a two - level hierarchical optimization to boost computation . however , m - flashadds a bimodal partition model over the block scheme to optimize even more the computation for sparse blocks in the graph .",
    "graphtwist @xcite introduces a 3d cube representation of the graph to add support for multigraphs .",
    "the cube representation divides the edges using three partitioning levels : slice , strip , and dice .",
    "these representations are equivalent to the block representation ( 2d ) of gridgraph and m - flash , with the difference that it adds one more dimension ( slice ) to organize the edge metadata for multigraphs .",
    "the slice dimension filters the edges metadata optimizing performance when not all the metadata is required for computation .",
    "additionally , graphtwist introduces pruning techniques to remove some slices and vertices that they do not consider relevant in the computation .",
    "m - flashalso draws inspiration from the edge streaming approach introduced by x - stream s processing model @xcite@xcite , improving it with fewer i / o operations for dense regions of the graph .",
    "edge streaming is a sort of stream processing referring to unrestricted data flows over a bounded amount of buffering . as we demonstrate",
    ", this leads to optimized data transfer by means of less i / o and more processing per data transfer .    ) ; @xmath0 is an edge block with source vertices in interval @xmath1 and destination vertices in interval @xmath2 ; @xmath3 is a _ source - partition _ containing all blocks with source vertices in interval @xmath1;@xmath4 is a _ destination - partition _ containing all blocks with destination vertices in interval @xmath2 .",
    "* vertices ( right ) : * the data of the vertices as @xmath5 vectors ( @xmath6 ... @xmath7 ) , each one divided into @xmath8 logical segments . ,",
    "the design of m - flashconsiders the fact that real graphs have a varying density of edges ; that is , a given graph contains dense regions with many more edges than other regions that are sparse . in the development of m - flash , and through experimentation with existing works",
    ", we noticed that these dense and sparse regions could not be processed in the same way .",
    "we also noticed that this was the reason why existing works failed to achieve superior performance . to cope with this issue , we designed m - flashto work according to two distinct processing schemes : dense block processing ( dbp ) and streaming partition processing ( spp ) . for full performance ,",
    "m - flashuses a theoretical i / o cost - based scheme to decide the kind of processing to use in face of a given block , which can be dense or sparse . the final approach , which combines dbp and spp , was named bimodal block processing ( bbp ) .      a graph in m - flashis a directed graph @xmath9 with vertices @xmath10 labeled with integers from 1 to @xmath11 , and edges @xmath12 , @xmath13 .",
    "each vertex has a set of attributes @xmath14 ; edges also might have attributes for specific processings .",
    "+ * _ blocks _ in m - flash * : given a graph @xmath15 , we divide its vertices @xmath16 into @xmath8 intervals denoted by @xmath1 , where @xmath17 .",
    "note that @xmath18 for @xmath19 , and @xmath20 . consequently , as shown in figure [ fig : blocks ] , the edges are divided into @xmath21 _",
    "blocks_. each block @xmath0 has a _ source node interval _ @xmath22 and a _ destination node interval _ @xmath23 , where @xmath24 . in figure",
    "[ fig : blocks ] , for example , @xmath25 is the block that contains edges with source vertices in the interval @xmath26 and destination vertices in the interval @xmath27 .",
    "we call this on - disk organization as _ partitioning_. since m - flashworks by alternating one entire block in memory for each running thread , the value of @xmath8 is automatically determined by the following equation : @xmath28 where the constant @xmath29 refers to the need of one buffer to store the input vertex values that are shared between threads ( read - only ) , @xmath30 is the amount of data to represent each vertex , @xmath31 is the number of threads , @xmath11 is the number of vertices , and @xmath32 is the available ram .",
    "for example , 4 bytes of data per node , 2 threads , a graph with 2 billion nodes , and for 1 gb ram , @xmath33 , thus requiring @xmath34 blocks .",
    "the number of threads enters the equation because all the threads access the same block to avoid multiple seeks on disk , and they use an exclusive memory buffer to store the vertex data processed ( one buffer per thread ) , so to prevent `` race '' conditions .     to @xmath35 , m - flashreuses source node interval @xmath36 ) , which reduces data transfer from disk to memory .",
    ", width=230 ]      this section presents our proposed processing model .",
    "we first describe the two strategies targeted at processing dense or sparse blocks .",
    "next , we present the novel cost - based optimization used to determine the best processing strategy .    * dense block processing ( dbp ) * : figure  [ fig : blockreading ] illustrates the dbp ; notice that vertex intervals are represented by vertical ( source i ) and horizontal ( destination i ) vectors . after partitioning the graph into _ blocks",
    "_ , we process them in a vertical zigzag order , as illustrated",
    ". there are three reasons for this order : ( 1 ) we store the computation results in the destination vertices ; so , we can `` pin '' a destination interval ( e.g. , @xmath27 ) and process all the vertices that are sources to this destination interval ( see the red vertical arrow ) ; ( 2 ) using this order leads to fewer reads because the attributes of the destination vertices ( horizontal vectors in the illustration ) only need to be read once , regardless of the number of source intervals .",
    "( 3 ) after reading all the blocks in a column , we take a `` u turn '' ( see the orange arrow ) to benefit from the fact that the data associated with the previously - read source interval is already in memory .    within a block , besides loading the attributes of the source and destination intervals of vertices into ram , the corresponding edges @xmath37 are sequentially read from disk , as explained in figure  [ fig : approach1 ] .",
    "these edges , then , are processed using a user - defined function so to achieve the desired computation . after all blocks in a column are processed , the updated attributes of the destination vertices are written to disk .",
    "+    .,width=325 ]    * streaming partition processing ( spp ) * : the performance of dbp decreases for graphs with sparse blocks ; this is because , for a given block , we have to read more data from the source intervals of vertices than from the very blocks of edges . in such cases",
    ", spp processes the graph using partitions instead of blocks .",
    "a graph _ partition _ is a set of _ blocks _ sharing the same _ source node interval _  a line in the logical partitioning , or , similarly , a set of _ blocks _ sharing the same _ destination node interval _  a column in the logical partitioning .",
    "formally , a _ source - partition @xmath38 _ contains all the blocks with edges having source vertices in the interval @xmath1 ; a _ destination - partition @xmath39 _ contains all the blocks with edges having destination vertices in the interval @xmath2 .",
    "for example , in figure [ fig : blocks ] , @xmath40 is the union of blocks @xmath41 , @xmath42 , and @xmath43 ; meanwhile , @xmath44 is the union of blocks @xmath43 , @xmath45 , and @xmath46 . in a graph , hence , there are @xmath8 _ source - partitions _ and @xmath8 _ destination - partitions_.     and @xmath47 as ilustrative examples .",
    "step 1 : the edges of _ source - partition _",
    "@xmath48 are sequentially read and combined with the values of their source vertices from @xmath49 .",
    "next , edges are grouped by destination , and written to @xmath8 files , one for each _ destination partition_. step 2 : the files corresponding to _ destination - partition _",
    "@xmath47 are sequentially processed according to a given desired computation , with results written to destination vertices in @xmath50.,width=325 ]    considering the graph organized into partitions , spp takes two steps ( see figure  [ fig : approach2 ] ) . in the first step , for",
    "a given _ source - partition _",
    "@xmath3 , it loads the values of the vertices of the corresponding interval @xmath1 ; next , it reads the edges of the partition @xmath3 sequentially from disk , storing them in a buffer together with their source - vertex values . at this point , it sorts the buffer in memory , grouping the edges by destination . finally , it stores the edges on disk into @xmath8 files , one for each of the @xmath8 _ destination - partitions_. this processing is performed for each _ source - partition _",
    "@xmath3 , @xmath17 , so to iteratively build the @xmath8 _ destination - partitions_.    in the second step , after processing the @xmath8 _ source - partitions _",
    "( each with @xmath8 blocks ) , it is possible to read the @xmath8 files according to their destinations , so to logically `` build '' @xmath8 _ destination - partitions _",
    "@xmath51 , @xmath52 , each one containing edges together with their source - vertex values . for each _ destination - partition _",
    "@xmath51 , we read the vertices of interval @xmath2 ; next , we sequentially read the edges , processing their values through a user - defined function .",
    "this function uses the properties of the vertices and of the edges to perform specific computations whose results will update the vertices . finally , spp stores the updated vertices of interval @xmath2 back on disk . +",
    "* bimodal block processing ( bbp ) * : schemes _ dbp _ and _ spp _ improve the performance in complementary circumstances .",
    "but , _ how can we decide which processing scheme to use when we are given a graph block to process ? _ to answer this question , we join dbp and spp into a single scheme  the bimodal block processing ( bbp ) .",
    "the combined scheme uses the theoretical i / o cost model proposed by aggarwal and vitter @xcite to decide for _ sbp _ or _",
    "spp_. in this model , the i / o cost of an algorithm is equal to the number of _ disk blocks _ with size @xmath53 transferred between disk and memory plus the number of non - sequential reads ( seeks ) .",
    "since we use this model to choose the scheme with the smaller cost , we need to algebraically determine the cost of each scheme , as follows .    for processing a graph",
    "@xmath54 , _ dbp _ performs the following operations : it reads the @xmath55 vertices @xmath8 times and it writes the @xmath55 vertices once ; it also reads the @xmath56 edges once  _ disk blocks _ of size @xmath53 , vertices and edges with constant sizes omitted from the equation for simplification .",
    "@xmath21 seeks are necessary because the edges are read sequentially .",
    "hence , the i / o cost for _ dbp _ is given by : @xmath57 in turn , _ spp _ initially reads the @xmath55 source vertices and the @xmath56 edges ; then , still in its first step , it sorts ( simple shuffling ) the @xmath56 edges grouping them by destination into a set of edges and vertices @xmath58 , writing them to disk . in its second step , it reads the @xmath59 edges / vertices to perform the update operation , writing @xmath55 destination vertices back to disk . the i / o cost for _ spp _ comes to be : @xmath60 equations [ eq : dbp ] and [ eq : spp ] define the i / o cost for one processing iteration over the whole graph @xmath15 .",
    "however , in order to decide in relation to the graph blocks , we are interested in the costs of equations [ eq : dbp ] and [ eq : spp ] divided by the number of graph blocks @xmath21 .",
    "the result , after the appropriate algebra , reduces to equations [ eq : dbpb ] and [ eq : sppb ] .",
    "@xmath61 where @xmath62 is the number of edges in @xmath0 , @xmath63 is the number of vertices in the interval , and @xmath30 and @xmath64 are , respectively , the number of bytes to represent a vertex and an edge @xmath65 .",
    "once we have the costs per graph block of dbp and spp , we can decide between one and the other by simply analyzing the ratio spp / dbp : +   @xmath66 \\right )                      \\label{eq : ratio }                  \\end{aligned}\\ ] ] this ratio leads to the final decision equation : @xmath67 we apply equation [ eq : ratio ] to select the best option according to equation [ eq : blocktype ] . with this scheme ,",
    "bbp is able to select the best processing scheme for each graph block . in section [ sec : evaluation ] , we demonstrate that this procedure yields a performance superior than the current state - of - the - art frameworks .      the m - flash s computational model , which we named _ malgorithm _ ( short for _ matrix algorithm interface _ ) is shown in algorithm [ alg : malgorithm ] . since _",
    "malgorithm _ is a vertex - centric model , it stores computation results in the destination vertices , allowing for a vast set of iterative computations , such as pagerank , random walk with restart , weakly connected components , sparse matrix vector multiplication , eigensolver , and diameter estimation .    _ * initialize * _ ( vertex v ) _ * process * _ ( vertex u , vertex v , edgedata data ) _ * gather * _ ( accum v_1 , accum v_2 , accum v_out ) _ * apply * _ ( vertex v )    the _ malgorithm _ interface has four operations : * initialize * , * process * , * gather * , and * apply*. the _ initialize _ operation , optionally , loads the initial value of each destination vertex ; the _ process _ operation receives and processes the data from incoming edges ( neighbors )  this is where the desired processing occurs ; the _ gather _ operation joins the results from the multiple threads so to consolidate a single result ; finally , the _ apply _ operation is able to perform finalizing operations , such as normalization",
    " apply is optional .    _ * degree(v ) : * _ = out degree for vertex v _ * initialize(v ) * _ : v.value = 0 _ * process(u , v , data ) * _ : v.value + = u.value/ _",
    "degree(u ) _ _ * gather(v_1 , v_2 , v_out ) * _ : v_out = v_1 + v_2 _ * apply(v ) * _ : v.value = 0.15 + 0.85 * v.value      m - flashstarts by preprocessing an input graph dividing the edges into @xmath8 partitions and counting the number of edges per logical block ( @xmath21 blocks ) , at the same time that the blocks are classified as sparse or dense using equation [ eq : blocktype ] .",
    "note that m - flashdoes _ _ sort the edges during preprocessing , it simply divides them into @xmath21 blocks , @xmath68 . in a second preprocessing ,",
    "m - flashprocesses the graph according to the organization given by the concept of _ source - partition _ as seen in section [ section : model ] . at this point , blocks are only a logical organization , while partitions are physical .",
    "the _ source - partitions _ are read and , whenever a dense block is found , the corresponding edges are extracted from the partition and a file is created for this block in preparation for dbp ; the remaining edges in the _ source - partition _ will be ready for processing using spp . notice that , after the second preprocessing , the logical blocks classified as dense , are materialized into physical files .",
    "the total i / o cost for preprocessing is @xmath69 , where @xmath53 is the size of each block transferred between disk and memory .",
    "algorithm [ alg : mflash ] shows the pseudo - code of m - flash .",
    "graph @xmath70 and vertex attributes @xmath71 user - defined _ malgorithm _ program memory size @xmath32 and number of iterations _ iter _ vector @xmath72 with vertex results set @xmath30 from @xmath71 attributes , and",
    "@xmath8 using equation [ eq : beta ] . @xmath73",
    "execute graph preprocessing and _ partitioning _ execute the first step of _ spp _ ( figure [ fig : approach2 ] ) to process the sparse source - partitions load vertex values of destination interval @xmath2 initialize @xmath2 of @xmath72 using _",
    "malgorithm_.initialize * for each *",
    "edge invoke _ malgorithm_.process storing results in vector @xmath72 @xmath74 @xmath75 load vertex values of source interval @xmath1 * for each * edge in @xmath0 invoke _ malgorithm_.process storing results in vector @xmath72 invoke _ malgorithm_.gather for @xmath2 of @xmath72 invoke _",
    "malgorithm_.apply for @xmath2 of @xmath72 store interval @xmath2 of vector @xmath72",
    "we compare m - flash(https://github.com / m - flash ) with multiple state - of - the - art approaches : graphchi , turbograph , x - stream , mmap , and gridgraph . for a fair comparison , we used the experimental setups recommended by the respective authors .",
    "gridgraph did not publish nor share its code , so the comparison is based on the results reported in its publication .",
    "we omit the comparison with graphtwist because it is not accessible and its published results are based on a hardware that is less powerful than ours .",
    "we use four graphs at different scales ( see table [ tab : datasets ] ) , and we compare the runtimes of all approaches for two well - known essential algorithms pagerank ( subsection  [ sec : pagerank ] ) and weakly connected components ( subsection [ sec : cc ] ) . to demonstrate how m - flashgeneralizes to more algorithms , we implemented the lanczos algorithm ( with _ selective orthogonalization _ ) , which is one of the most computationally efficient approaches to computing eigenvalues and eigenvectors  @xcite ( subsection  [ sec : spectral ] ) .",
    "to the best of our knowledge , m - flashprovides the * first design and implementation * of lanczos that can handle graphs with more than one billion nodes .",
    "next , in subsection  [ sec : memory ] , we show that m - flashmaintains its high speed even when the machine has little ram ( including extreme cases , like 4 gb ) , in contrast to the other methods .",
    "finally , through a theoretical analysis of i / o , we show the reasons for the performance increase using the bbp strategy ( subsection  [ subsec : ta ] ) .",
    "all experiments ran on a standard personal computer equipped with a four - core intel i7 - 4500u cpu ( 3 ghz ) , 16 gb ram , and 1 tb 540-mb / s ( max ) ssd disk .",
    "note that m - flashdoes _ _ require an ssd to run , which is not the case for all frameworks , like turbograph .",
    "we used an ssd , nevertheless , to make sure that all methods can perform at their best .",
    "table [ tab : datasets ] shows the datasets used in our experiments .",
    "graphchi , x - stream , mmap , and m - flash ran on linux ubuntu 14.04 ( x64 ) .",
    "turbograph ran on windows ( x64 ) .",
    "all the reported times correspond to the average time of three * cold * runs , that is , with all caches and buffers purged between runs to avoid any potential advantage due to caching or buffering .",
    "[ tab : datasets ]      table [ tab : processing ] presents the pagerank runtime of all the methods , as discussed next .",
    "* livejournal * ( small graph ) : since the whole graph fits in ram , all approaches finish in seconds . still , m - flashwas the fastest , up to 6x faster than graphchi , 3x than mmap , and 2x than x - stream .    * twitter * ( medium graph ) : the edges of this graph do not fit in ram ( it requires 11.3 gb ) but its node vectors do .",
    "m - flashhad a similar performance if compared to mmap , however , mmap is not a generic framework , rather it is based on dedicated implementations , one for each algorithm . still , m - flashwas faster . in comparison to graphchi and x - stream , the related works that offer generic programming models ,",
    "m - flashwas the fastest , 5.5x and 7x faster , respectively .",
    "* yahooweb * ( large graph ) : for this billion - node graph , neither its edges nor its node vectors fit in ram ; this challenging situation is where m - flashhas notably outperformed the other methods .",
    "the results of table [ tab : processing ] confirm this claim , showing that m - flashprovides a speed that is 3x to 6.3x faster that those of the other approaches .",
    "r - mat ( synthetic large graph ) * : for our big graph , we compared only graphchi , x - stream , and m - flashbecause turbograph and mmap require indexes or auxiliary files that exceed our current disk capacity .",
    "gridgraph was not considered in the comparison because its paper does not provide information about r - mat graphs with a similar scale .",
    "table [ tab : processing ] shows that m - flashis 2x and 3x faster that x - stream and graphchi respectively .",
    "when there is enough memory to store all the vertex data , the _ union find _",
    "algorithm @xcite is the best option to find all the wccs in one single iteration .",
    "otherwise , with memory limitations , an iterative algorithm produces identical solutions .",
    "hence , in this round of experiments , we use algorithm _ union find _ to solve wcc for the small and medium graphs , whose vertices fit in memory ; and we use an iterative algorithm for the yahooweb graph .",
    "table [ tab : processing ] shows the runtimes for the livejournal and twitter graphs with 8 gb ram ; all approaches use union find , except x - stream .",
    "this is because of the way that x - stream is implemented , which handles only iterative algorithms .    in the wcc problem ,",
    "m - flashis again the fastest method with respect to the entire experiment : for the livejournal graph , m - flashis 3x faster than graphchi , 4.3x than x - stream , 3.3x than turbograph , and 8.2x than mmap . for the twitter graph ,",
    "m - flash s speed is 2.8x faster than graphchi , 41x than x - stream , 5x than turbograph , 2x than mmap , and 11.5x than gridgraph .    in the results of the yahooweb graph , one can see that m - flashwas significantly faster than graphchi , and x - stream .",
    "similarly to the pagerank results , m - flashis pronouncedly faster : 5.3x faster than graphchi , and 7.1x than x - stream .",
    "eigenvalues and eigenvectors are at the heart of numerous algorithms , such as singular value decomposition ( svd ) @xcite , spectral clustering , triangle counting @xcite , and tensor decomposition @xcite .",
    "hence , due to its importance , we demonstrate m - flashover the _ lanczos algorithm _ , a state - of - the - art method for eigen computation .",
    "we implemented it using method _ selective orthogonalization _ ( _ lso _ ) . to the best of our knowledge , m - flashprovides the * first design and implementation * that can handle lanczos for graphs with more than one billion nodes .",
    "different from the competing works , m - flashprovides functions for basic vector operations using secondary memory .",
    "therefore , for the yahooweb graph , we are not able to compare it with the other frameworks using only 8 gb of memory .    to compute the top 20 eigenvectors and eigenvalues of the yahooweb graph , one iteration of _ lso _ over m - flashtakes 737s when using 8 gb of ram .",
    "for a comparative panorama , to the best of our knowledge , the closest comparable result of this computation comes from the heigen system  @xcite , at 150s for one iteration ; note however that , it was for a much smaller graph with 282 million edges ( 23x fewer edges ) , using a _ _ hadoop cluster , while our experiment with m - flashused a single personal computer and a much larger graph .      since the amount of memory strongly affects the computation speed of single - node graph processing frameworks , here , we study the effect of memory size . figure [ fig : scalability ]",
    "summarizes how all approaches perform under 4 gb , 8 gb , and 16 gb of ram when running one iteration of pagerank over the yahooweb graph . m - flashcontinues to run at the highest speed even when the machine has very little ram , 4 gb in this case . other methods tend to slow down .",
    "in special , mmap does not perform well due to _ thrashing _",
    ", a situation when the machine spends a lot of time on mapping disk - resident data to ram or unmapping data from ram , slowing down the overall computation . for 8 gb and 16 gb , respectively , m - flashoutperforms all the competitors for the most challenging graph , the yahooweb .",
    "notice that all the methods , but for m - flashand x - stream , are strongly influenced by restrictions in memory size ; according to our analyses , this is due to the higher number of data transfers needed by the other methods when not all the data fit in the memory . despite that x - stream worked efficiently for any memory setting , it still has worse performance if compared to m - flashbecause it demands three full disk scans in every case  actually , the innovations of m - flash , as presented in section [ sec : mflash ] , were designed to overcome such problem .",
    "following , we show the theoretical scalability of m - flashwhen we reduce the available memory at the same time that we demonstrate why the performance of m - flashimproves when we combine dbp and spp into bbp , instead of using dbp or spp alone . here",
    ", we use a measure that we named _",
    "t - cost _ ; 1 unit of t - cost corresponds to three operations , one reading of the vertices , one writing of the vertices , and one reading of the edges . in terms of computational complexity",
    ", t - cost is defined as follows : @xmath76        notice that this cost considers that reading and writing the vertices have the same cost ; this is because the evaluation is given in terms of computational complexity . for more details ,",
    "please refer to the work of mcsherry _ et al . _",
    "@xcite , who draws the basis of this kind of analysis .",
    "we measure the t - cost metric to analyze the theoretical scalability for processing schemes @xmath77 only , @xmath78 only , and @xmath79 .",
    "we perform these analyses using matlab simulations that were validated empirically .",
    "we considered the characteristics of the three datasets used so far , livejournal , twitter , and yahooweb . for each case , we calculated the t - cost ( y - axis ) as a function of the available memory ( x - axis ) , which , as we have seen , is the main constraint for graph processing frameworks .",
    "figure [ fig : graphram ] shows that , for all the graphs , dbp - only processing is the least efficient when memory is reduced ; however , when we combine dbp ( for dense region processing ) and spp ( for sparse region processing ) into bbp , we benefit from the best of both worlds .",
    "the result corresponds to the best performance , as seen in the charts .",
    "figure [ fig : density ] shows the same simulated analysis ",
    "t - cost ( y - axis ) in function of the available memory ( x - axis ) , but now with an extra variable : the density of hypothetical graphs , which is assumed to be uniform in each analysis .",
    "each plot , from ( a ) to ( d ) considers a different density in terms of average vertex degree , respectively , 3 , 5 , 10 , and 30 . in each plot , there are two curves , one corresponding to dbp - only , and one for spp - only ; and , in dark blue , we depict the behavior of m - flashaccording to combination bbp . notice that , as the amount of memory increases",
    ", so does the performance of dbp , which takes less and less time to process the whole graph ( decreasing curve ) .",
    "spp , in turn , has a steady performance , as it is not affected by the amount of memory ( light blue line ) . in dark blue",
    ", one can see the performance of bbp ; that is , which kind of processing will be chosen by equation [ eq : blocktype ] at each circumstance . for sparse graphs , figures [ fig : density](a ) and [ fig : density](b ) , spp answers for the greater amount of processing ; while the opposite is observed in denser graphs , figures [ fig : density](c ) and [ fig : density](d ) , when dbp defines almost the entire dark blue line of the plot .",
    "these results show that the graph processing must take into account the density of the graph at each moment ( block ) so to choose the best strategy . it also explains why m - flashimproves the state of the art .",
    "it is _ important _ to note that no former algorithm considered the fact that most graphs present varying density of edges ( dense regions with many more edges than other regions that are sparse ) . ignoring this fact leads to a decreased performance in the form of a higher number of data transfers between memory and disk , as we empirically verified in the former sections .        , where @xmath80 , and varying amount of memory , width=280 ]      table [ tab : preprocessing ] shows the preprocessing times for each graph using 8 gb of ram .",
    "as one can see , m - flashhas a competitive preprocessing runtime .",
    "it reads and writes two times the entire graph on disk , which is the third best performance , after mmap and x - stream .",
    "gridgraph and graphtwist , in turn , demand a preprocessing that divides the graph using blocks in a way similar to m - flash .",
    "we did not compare preprocessing with these frameworks because , as already discussed , we do not have their source code . despite the extra preprocessing time required by m - flash  if compared to mmap and x - stream , the total processing time ( preprocessing + _ _ ) for algorithms pagerank and wcc over the yahooweb graph , is of @xmath81s and @xmath82s , still , 29% and 4% better than the total time of mmap and x - stream respectively .",
    "note that the algorithms are iterative and m - flashneeds only one iteration to overcome its competitors .",
    "[ tab : preprocessing ]",
    "we proposed m - flash , a _ single - machine _ , _ billion - scale _ graph computation framework that uses a block partition model to optimize the disk i / o .",
    "m - flash uses an innovative design that takes into account the variable density of edges observed in the different blocks of a graph .",
    "its design uses dense block processing ( dbp ) when the block is dense , and streaming partition processing ( spp ) when the block is sparse . in order to take advantage of both worlds",
    ", it uses the combination of dbp and spp according to the bimodal block processing ( bbp ) scheme , which is able to analytically determine whether a block is dense or sparse , so to trigger the appropriate processing . to date",
    ", our proposal is the first framework that considers a bimodal approach for i / o minimization , a fact that , as we demonstrated , granted m - flashthe best performance compared to the state of the art ( graphchi , x - stream , turbograph , mmap , and gridgraph ) ; notably , even when memory is severely limited .",
    "the findings observed in the design of m - flash are a step further in determining an ultimate graph processing paradigm .",
    "we expect the research in this field to consider the criterion of block density as a mandatory feature in any such framework , consistently advancing the research on high - performance processing .",
    "this work received support from brazilian agencies cnpq ( grant 444985/2014 - 0 ) , fapesp ( grants 2016/02557 - 0 , 2014/21483 - 2 ) , and capes ; from usa agencies nsf ( grants iis-1563816 , twc-1526254 , iis-1217559 ) , and grfp ( grant dge-1148903 ) ; and korean ( msip ) agency iitp ( grant r0190 - 15 - 2012 ) ."
  ],
  "abstract_text": [
    "<S> recent graph computation approaches have demonstrated that a single pc can perform efficiently on billion - scale graphs . </S>",
    "<S> while these approaches achieve scalability by optimizing i / o operations , they do not fully exploit the capabilities of modern hard drives and processors . to overcome their performance , in this work , we introduce the bimodal block processing ( _ bbp _ ) , an innovation that is able to boost the graph computation by minimizing the i / o cost even further . with this strategy </S>",
    "<S> , we achieved the following contributions : ( 1 ) m - flash , the fastest graph computation framework to date ; ( 2 ) a flexible and simple programming model to easily implement popular and essential graph algorithms , including the _ first _ single - machine billion - scale eigensolver ; and ( 3 ) extensive experiments on real graphs with up to 6.6 billion edges , demonstrating m - flash s consistent and significant speedup .    algorithms , graph processing , graph mining , complex networks </S>"
  ]
}