{
  "article_text": [
    "recently , demand for large - scale complex optimization is increasing in computational science , engineering and many of other fields . in that kind of problems",
    ", there are many difficulties caused by noise in function evaluation , many tuning parameters and high computation cost . in such cases ,",
    "derivatives of the objective function are unavailable or computationally infeasible .",
    "these problems can be treated by the derivative - free optimization ( dfo ) methods .",
    "dfo is the tool for optimization without derivative information of the objective function and constraints , and it has been widely studied for decades  @xcite .",
    "dfo algorithms include gradient descent methods with a finite difference gradient estimation  @xcite , some direct search methods using only function values  @xcite , and trust - region methods  @xcite .",
    "there is , however , a more restricted setting in which not only derivatives but also values of the objective function are unavailable or computationally infeasible . in such a situation , the so - called pairwise comparison oracle , that tells us an order of function values on two evaluation points , is used instead of derivatives and function evaluation  @xcite .",
    "for example , the pairwise comparison is used in learning to rank to collect training samples to estimate the preference function of the ranking problems  @xcite . in decision making , finding the most preferred feasible solution from among the set of many alternatives is an important application of ranking methods using the pairwise comparison .",
    "also , other type of information such as stochastic gradient - sign oracle has been studied  @xcite .",
    "now , let us introduce two dfo methods , i.e. , the nelder - mead method  @xcite and stochastic coordinate descent algorithm  @xcite .",
    "they are closely related to our work . in both methods ,",
    "the pairwise comparison of function values is used as a building block in optimization algorithms .",
    "nelder and mead s downhill simplex method  @xcite was proposed in early study of algorithms based on the pairwise comparison of function values . in each iteration of the algorithm , a simplex that approximates the objective function",
    "is constructed according to ranking of function values on sampled points .",
    "then , the simplex receives four operations , namely , reflection , expansion , contraction and reduction in order to get close to the optimal solution .",
    "unfortunately , the convergence of the nelder - mead algorithm is theoretically guaranteed only in low - dimension problems  @xcite . in high dimensional problems ,",
    "the nelder - mead algorithm works poorly as shown in @xcite .    the stochastic coordinate descent algorithm using only the noisy pairwise comparison was proposed in @xcite .",
    "lower and upper bounds of the convergence rate were also presented in terms of the number of pairwise comparison of function values , i.e. , query complexity .",
    "the algorithm iteratively solves one dimensional optimization problems like the coordinate descent method .",
    "however , practical performance of the optimization algorithm was not studied in that work .    in this paper , we focus on optimization algorithms using the pairwise comparison oracle . in our algorithm ,",
    "the convergence to the optimal solution is guaranteed , when the number of pairwise comparison tends to infinity .",
    "our algorithm is regarded as a block coordinate descent method consisting of two steps : the direction estimate step and search step . in the direction estimate step , the search direction is determined . in the search step , the current solution is updated along the search direction with an appropriate step length . in our algorithm , the direction estimate step is easily parallelized .",
    "therefore , it is expected that our algorithm effectively works even in large - scale optimization problems .",
    "let us summarize the contributions presented in this paper .    1 .",
    "we propose a block coordinate descent algorithm based on the pairwise comparison oracle , and point out that the algorithm is easily parallelized .",
    "we derive an upper bound of the convergence rate in terms of the number of pairwise comparison of function values , i.e. , query complexity .",
    "we show a practical efficiency of our algorithm through numerical experiments .    the rest of the paper is organized as follows . in section  [ pre ] , we explain the problem setup and give some definitions",
    ". section  [ mai ] is devoted to the main results .",
    "the convergence properties and query complexity of our algorithm are shown in the section . in section  [ num ] ,",
    "numerical examples are reported . finally in section  [ con ]",
    ", we conclude the paper with the discussion on future works .",
    "all proofs of theoretical results are found in appendix .",
    "in this section , we introduce the problem setup and prepare some definitions and notations used throughout the paper .",
    "a function @xmath0 is said to be @xmath1-strongly convex on @xmath2 for a positive constant @xmath1 , if for all @xmath3 , the inequality @xmath4 holds , where @xmath5 and @xmath6 denote the gradient of @xmath7 at @xmath8 and the euclidean norm , respectively .",
    "the function @xmath7 is @xmath9-strongly smooth for a positive constant @xmath9 , if @xmath10 holds for all @xmath11 .",
    "the gradient @xmath12 of the @xmath9-strongly smooth function @xmath7 is referred to as @xmath9-lipschitz gradient .",
    "the class of @xmath1-strongly convex and @xmath9-strongly smooth functions on @xmath13 is denoted as @xmath14 . in the convergence analysis , mainly we focus on the optimization of objective functions in @xmath14 .",
    "we consider the following pairwise comparison oracle defined in @xcite .",
    "the stochastic pairwise comparison ( pc ) oracle is a binary valued random variable @xmath15 defined as @xmath16 \\ge \\frac{1}{2 }   + \\min \\{\\delta_0 , \\mu |f(\\y ) - f(\\x)|^{\\kappa-1 } \\ } ,   \\label{eqp3}\\ ] ] where @xmath17 , @xmath18 and @xmath19 . for @xmath20 , without loss of generality @xmath21 is assumed .",
    "when the equality @xmath22 = 1\\ ] ] is satisfied for all @xmath8 and @xmath23 , we call @xmath24 the deterministic pc oracle .    for @xmath20 ,",
    "the probability in ( [ eqp3 ] ) is not affected by the difference @xmath25 , meaning that the probability for the output of the pc oracle is not changed under any monotone transformation of @xmath7 . in  @xcite ,",
    "jamieson et al . derived lower and upper bounds of convergence rate of an optimization algorithm using the stochastic pc oracle .",
    "the algorithm is referred to as the original pc algorithm in the present paper . under above preparations ,",
    "our purpose is to find the minimizer @xmath26 of the objective function @xmath27 in @xmath28 by using pc oracle . in the following section , we provide a dfo algorithm in order to solve the optimization problem and consider the convergence properties including query complexity .",
    "initial point @xmath29 , and accuracy in line search @xmath30 . set @xmath31 .",
    "choose @xmath32 coordinates @xmath33 out of @xmath34 coordinates according to the uniform distribution .",
    "solve the one - dimension optimization problems @xmath35 within the accuracy @xmath36 using the pc - based line search algorithm shown in algorithm  [ alg2 ] , where @xmath37 denotes the @xmath38-th unit basis vector .",
    "then , obtain the numerical solutions @xmath39 .",
    "set @xmath40 .",
    "if @xmath41 is the zero vector , add @xmath36 to @xmath42 .",
    "apply algorithm  [ alg2 ] to obtain a numerical solution @xmath43 of @xmath44 within the accuracy @xmath45 . @xmath46 .",
    "@xmath47    in algorithm  [ alg1 ] , we propose a dfo algorithm based on the pc oracle . in our algorithm",
    ", @xmath32 coordinates out of @xmath34 elements are updated in each iteration to efficiently cope with high dimensional problems .",
    "algorithm  [ alg1 ] is referred to as @xmath48 $ ] .",
    "the original pc algorithm is recovered by setting @xmath49 .",
    "the pc oracle is used in the line search algorithm to solve one - dimensional optimization problems ; the detailed line search algorithm is shown in algorithm  [ alg2 ] .    for @xmath50 , the search direction @xmath41 in algorithm  [ alg1 ]",
    "approximates that of a modified newton method  ( * ? ? ?",
    "* chap.10 ) , as shown below . in step",
    "d-1 of the algorithm , one - dimensional optimization problems are solved .",
    "let @xmath51 be the optimal solution of with @xmath52 . then , @xmath51 will be close to the numerical solution @xmath53 .",
    "the taylor expansion of the objective function leads to @xmath54 where @xmath55 is the hessian matrix of @xmath7 at @xmath56 .",
    "when the point @xmath56 is close to the optimal solution of @xmath27 , the optimal parameter @xmath57 will be close to zero , implying that the higher order term @xmath58 in the above is negligible .",
    "hence , @xmath53 is approximated by the optimal solution of the quadratic approximation , i.e. , @xmath59 . as a result , the search direction in @xmath60 $ ] is approximated by @xmath61 , where @xmath62 denotes the diagonal matrix , the diagonal elements of which are those of the square matrix  @xmath63 . in the modified newton method , the hessian matrix in the newton method is replaced with a positive definite matrix to reduce the computation cost . using only the diagonal part of the hessian matrix is a popular choice in the modified newton method .",
    "current solution @xmath64 , search direction @xmath65 and accuracy in line search @xmath30 .",
    "set @xmath66 , @xmath67 , @xmath68 , @xmath69 .",
    "@xmath70 @xmath71 ( double - sign corresponds ) @xmath72 , @xmath73 @xmath74 , @xmath75 , @xmath76 @xmath77 , @xmath78 , @xmath79 ( double - sign corresponds ) @xmath80 , @xmath81 @xmath82    figure  [ fg1 ] demonstrates an example of the optimization process of both the original pc algorithm and our algorithm .",
    "the original pc algorithm updates the numerical solution along a randomly chosen coordinate in each iteration .",
    "hence , many iterations are required to get close to the optimal solution . on the other hand , in our algorithm",
    ", a solution can move along a oblique direction . therefore",
    ", our algorithm can get close to the optimal solution with less iterations than the original pc algorithm .     with same initialization .",
    "left panel : jamieson et al.s original pc algorithm .",
    "right panel : proposed algorithm . , title=\"fig : \" ]   with same initialization . left panel : jamieson et al.s original pc algorithm .",
    "right panel : proposed algorithm . , title=\"fig : \" ]      we now provide an upper bound of the convergence rate of our algorithm using the deterministic pc oracle .",
    "let us denote the minimizer of @xmath7 as @xmath26 .",
    "[ upper ] suppose @xmath83 , and define @xmath84 and @xmath85 be @xmath86 let us define @xmath87 be @xmath88 for @xmath89 , we have @xmath90\\leq{}\\varepsilon$ ] , where the expectation is taken with respect to the random choice of coordinates @xmath91 to be updated in @xmath48 $ ] .",
    "the proof of theorem  [ upper ] is given in  [ proof.upper ] .",
    "note that any monotone transformation of the objective function does not affect the output of the deterministic pc oracle .",
    "hence , the theorem above holds even for the function @xmath27 such that the composite function with a monotone function is included in @xmath14 .",
    "let @xmath92 be the output of blockcd@xmath93 $ ] after @xmath94 pairwise comparison queries .",
    "to solve the one dimension optimization problem within the accuracy @xmath36 , the sufficient number of the call of pc - oracle is @xmath95 as shown in @xcite .",
    "hence , if the inequality @xmath96 holds , theorem  [ upper ] assures that the numerical solution @xmath92 based on @xmath94 queries satisfies @xmath97\\leq\\varepsilon.\\ ] ] when @xmath98 is sufficiently large , we have @xmath99 where @xmath100 is a constant depending on @xmath101 and @xmath102 .",
    "the last inequality holds if @xmath103 and @xmath104 are both greater than @xmath105 .",
    "eventually we have @xmath106\\leq { }   \\exp\\left\\{-\\frac{c}{\\log{n}}\\sqrt{\\frac{q}{n}}\\right\\ } ,      \\ ] ] where @xmath107 .",
    "the above bound is of the same order of the convergence rate for the original pc algorithm up to polylog factors . on the other hand ,",
    "a lower bound presented in  @xcite is of order @xmath108 with a positive constant @xmath109 up to polylog factors , when the pc oracle with @xmath110 is used .    in theorem",
    "[ upper ] , it is assumed that the objective function is strongly convex and gradient lipschitz . in a realistic situation",
    ", we usually do not have the knowledge of the class parameters @xmath1 and @xmath9 of the unknown objective function . moreover , strong convexity and gradient lipschitzness on the whole space @xmath111 is too strong . in the following corollary",
    ", we relax the assumption in theorem [ upper ] and prove the convergence property of our algorithm without strong convexity and strong smoothness .    [ convergence ]",
    "let @xmath112 be a twice continuously differentiable convex function with non - degenerate hessian on @xmath111 and @xmath26 be a minimizer of @xmath7 .",
    "then , there is a constant @xmath109 such that the output @xmath113 of blockcd[n , m ] satisfies ( [ eq : quecom ] ) .",
    "the proof of corollary  [ convergence ] is given in  [ proof.convergence ] .",
    "@xmath3 , @xmath114 $ ] , @xmath115 set @xmath116 and toss the coin with probability @xmath117 of heads once .",
    "@xmath118 @xmath119    in stochastic pc oracle , one needs to ensure that the correct information is obtained in high probability . in algorithm",
    "[ alg3 ] , the query @xmath120 is repeated under the stochastic pc oracle .",
    "the reliability of line search algorithm based on stochastic pc oracle was investigated by @xcite .",
    "[ @xcite ] [ sto_line ] for any @xmath3 with @xmath114 $ ] , the repeated querying subroutine in algorithm  [ alg3 ] correctly identifies the sign of @xmath121 $ ] with probability @xmath122 , and requests no more than @xmath123 queries .",
    "it should be noted here that , in this paper , @xmath124\\}= { \\rm sign } \\{f(y)-f(x)\\}$ ] always holds because @xmath125 from ( [ eqp3 ] ) . in @xcite , a modified line search algorithm using a ternary search instead of bisection search",
    "was proposed to lower bound @xmath126 in lemma [ sto_line ] for arbitrary @xmath3 .",
    "then , one can find that the total number of queries required by a repeated querying subroutine algorithm is at most @xmath127 , where @xmath45 is an accuracy of line search .",
    "the query complexity of the stochastic pc oracle is obtained from that of the deterministic pc oracle .",
    "suppose that one has @xmath128 responses from the deterministic pc oracle . to obtain the same responses from the stochastic pc oracle with probability more than @xmath129 , one needs more than @xmath130 queries . from the above discussion",
    ", we have the following upper bounds for stochastic setting : @xmath131 \\le     \\begin{cases }     \\displaystyle     \\exp \\left \\{-\\frac{c_{1}}{\\log{n } } \\sqrt{\\frac{q}{n } } \\right \\ } , & \\kappa = 1 ,   \\vspace*{2 mm } \\\\",
    "\\displaystyle     c_{2 } \\displaystyle \\frac{n^2}{m }   \\left(\\frac{n}{q } \\right)^{1/(2\\kappa - 2 ) } , & \\kappa > 1 ,     \\end{cases }    \\label{eq : sto_compl}\\ ] ] where @xmath132 and @xmath133 are constant depending on @xmath101 and @xmath134 as well as @xmath135 poly - logarithmically .",
    "if @xmath32 and @xmath34 are of the same order in the case of @xmath136 , the bound coincides with that shown in theorem  2 of @xcite .",
    "in this section , we present numerical experiments in which the proposed method in algorithm  [ alg1 ] was mainly compared with the nelder - mead algorithm  @xcite and the original pc algorithm  @xcite , i.e. , blockcd@xmath137 $ ] of algorithm  [ alg1 ] . here , the pc oracle was used in all the optimization algorithms . in blockcd@xmath93 $ ] with @xmath138",
    ", one can execute the line search algorithm to each axis separately .",
    "hence , the parallel computation is directly available to find the components of the search direction @xmath41 . also , we investigated the computation efficiency of the parallel implementation of our method .",
    "the numerical experiments were conducted on amd opteron processor 6176 ( 2.3ghz ) with 48 cores , running cent os linux release 6.4 .",
    "we used the r language  @xcite with snow library for parallel statistical computing .",
    "it is well - known that the nelder - mead method efficiently works in low dimensional problems .",
    "indeed , in our preliminary experiments for two dimensional optimization problems , the nelder - mead method showed a good convergence property compared to the other methods such as blockcd@xmath139 $ ] with @xmath140 .",
    "numerical results are presented in figure  [ fig:2_dim_problems ] .",
    "we tested optimization methods on the quadratic function @xmath141 , and two - dimension rosenbrock function , @xmath142 , where the matrix @xmath63 was a randomly generated 2 by 2 positive definite matrix . in two dimension problems",
    ", we do not use the parallel implementation of our method , since clearly the parallel computation is not efficient that much .",
    "the efficiency of parallel computation is canceled by the communication overhead . in our method , the accuracy of the line search is fixed to a small positive number @xmath45 .",
    "hence , the optimization process stops on the way to the optimal solution , as shown in the left panel of fig .",
    "[ fig:2_dim_problems ] . on the other hand , the nelder - mead method tends to converge to the optimal solution in high accuracy . in terms of the convergence speed for the optimization of two - dimensional quadratic function , there is no difference between the nelder - mead method and blockcd method , until the latter stops due to the limitation of the numerical accuracy . even in non - convex rosenbrock function ,",
    "the nelder - mead method works well compared to the pc - based blockcd algorithm .    [ cols=\"^,^ \" , ]",
    "in this paper , we proposed a block coordinate descent algorithm for unconstrained optimization problems using the pairwise comparison of function values .",
    "our algorithm consists of two steps : the direction estimate step and search step .",
    "the direction estimate step can easily be parallelized . hence",
    ", our algorithm is effectively applicable to large - scale optimization problems .",
    "theoretically , we obtained an upper bound of the convergence rate and query complexity , when the deterministic and stochastic pairwise comparison oracles were used .",
    "practically , our algorithm is simple and easy to implement . in addition",
    ", numerical experiments showed that the parallel implementation of our algorithm outperformed the other methods .",
    "an extension of our algorithm to constrained optimization problems is an important future work .",
    "other interesting research directions include pursuing the relation between pairwise comparison oracle and other kind of oracles such as gradient - sign oracle  @xcite .",
    "10    charles audet and john  e dennis  jr .",
    "analysis of generalized pattern searches .",
    ", 13(3):889903 , 2002 .",
    "stephen  poythress boyd and lieven vandenberghe .",
    ". cambridge university press , 2004 .    a  andrew  r conn , katya scheinberg , and luis  n vicente . ,",
    "volume  8 .",
    "siam , 2009 .",
    "andrew  r conn , nicholas  i m gould , and ph  l toint .",
    ", volume  1 .",
    "siam , 2000 .",
    "a.  d. flaxman , a.  t. kalai , and h.  b. mcmahan .",
    "online convex optimization in the bandit setting : gradient descent without a gradient . in",
    "_ proceedings of the sixteenth annual acm - siam symposium on discrete algorithms _ , soda 05 , pages 385394 , philadelphia , pa , usa , 2005 .",
    "society for industrial and applied mathematics .",
    "m.  c. fu .",
    "gradient estimation . in s.  g. henderson and b.  l. nelson , editors , _ handbooks in operations",
    "research and management science : simulation _ , chapter  19 .",
    "elservier amsterdam , 2006 .",
    "fuchang gao and lixing han .",
    "implementing the nelder - mead simplex algorithm with adaptive parameters .",
    ", 51(1):259277 , 2012 .",
    "k.  g. jamieson , r.  d. nowak , and b.  recht .",
    "query complexity of derivative - free optimization . in _ nips _ , pages 26812689 , 2012 .",
    "matti kriinen .",
    "active learning in the non - realizable case . in _ algorithmic learning theory _ , pages 6377 .",
    "springer , 2006 .",
    "jeffrey  c. lagarias , james  a. reeds , margaret  h. wright , and paul  e. wright .",
    "convergence properties of the nelder - mead simplex method in low dimensions .",
    ", 9:112147 , 1998 .",
    "d.  luenberger and y.  ye . .",
    "springer , 2008 .",
    "mehryar mohri , afshin rostamizadeh , and ameet talwalkar . .",
    "mit press , 2012 .",
    "j.  a. nelder and r.  mead . a simplex method for function minimization . , 7(4):308313 , 1965 .    .",
    "r foundation for statistical computing , vienna , austria , 2014 .",
    "aaditya ramdas and aarti singh .",
    "algorithmic connections between active learning and stochastic convex optimization . in _ algorithmic learning theory _ , pages 339353 .",
    "springer , 2013 .",
    "aaditya ramdas and aarti singh .",
    "algorithmic connections between active learning and stochastic convex optimization . in sanjay jain , rmi munos , frank stephan , and thomas zeugmann , editors , _ alt _ , volume 8139 of _ lecture notes in computer science _ , pages 339353 .",
    "springer , 2013 .",
    "luis  miguel rios and nikolaos  v sahinidis .",
    "derivative - free optimization : a review of algorithms and comparison of software implementations .",
    ", 56(3):12471293 , 2013 .",
    "the optimal solution of @xmath7 is denoted as @xmath26 .",
    "let us define @xmath143 be @xmath144 . if @xmath145 holds in the algorithm , we obtain @xmath146 , since the function value is non - increasing in each iteration of the algorithm is assured by a minor modification of pc - oracle in  @xcite . ] .",
    "next , we assume @xmath147 .",
    "the assumption leads to @xmath148 in which the second inequality is derived from ( 9.9 ) in @xcite . in the following ,",
    "we use the inequality @xmath149 that is proved in @xcite . for",
    "the @xmath38-th coordinate , let us define the functions @xmath150 and @xmath151 as @xmath152 then , we have @xmath153 let @xmath154 and @xmath155 be the minimum solution of @xmath156 and @xmath157 , respectively . then , we obtain @xmath158 the inequality @xmath159 yields that @xmath155 lies between @xmath160 and @xmath161 , where @xmath100 and @xmath132 are defined as @xmath162 here , @xmath163 holds .",
    "each component of the search direction @xmath164 in algorithm  [ alg1 ] satisfies @xmath165 if @xmath166 and otherwise @xmath167 . for @xmath168 ,",
    "let @xmath169 of the vector @xmath170 be @xmath171 .",
    "then , the triangle inequality leads to @xmath172 the assumption @xmath147 and the inequalities @xmath173 lead to @xmath174 hence , we obtain @xmath175_+ , \\end{aligned}\\ ] ] where @xmath176_+=\\max\\{0,x\\}$ ] for @xmath177 .",
    "let @xmath178 be a non - negative valued random variable defined from the random set @xmath179 , and define the non - negative value @xmath180 as @xmath181 .",
    "a lower bound of the expectation of @xmath182 with respect to the distribution of @xmath179 is given as @xmath183 & \\geq   \\ebb_i\\left [   \\left (   \\frac { \\left [   c_0\\|\\nabla{f}(\\x)\\|_{i}^2-\\frac{c_0}{2}\\sqrt{\\frac{m}{n}}\\|\\nabla{f}(\\x_t)\\|\\|\\nabla{f}(\\x_t)\\|_i   \\right]_+ }   { c_1\\|\\nabla{f}(\\x_t)\\|_i + \\frac{c_0}{2}\\sqrt{\\frac{m}{n}}\\|\\nabla{f(\\x_t)}\\| }   \\right)^2   \\right ] \\\\ & =   k^2\\frac{m}{n}\\|\\nabla{f(\\x_t)}\\|^2\\ebb_i\\left[{z^2 } \\frac{[z-1/2]_+^2}{(z+k/2)^2}\\right]\\\\ & \\geq   k^2\\frac{m}{n}\\|\\nabla{f(\\x_t)}\\|^2 \\ebb_i\\left[{z^2 }   \\frac{[z-1/2]_+^2}{(z+1/2)^2}\\right ] .",
    "\\end{aligned}\\ ] ] the random variable @xmath184 is non - negative , and @xmath185=1 $ ] holds .",
    "thus , lemma  [ eqn : lemma_bound_expz ] in the below leads to @xmath183   & \\geq   \\frac{k^2}{53}\\frac{m}{n}\\|\\nabla{f}(\\x_t)\\|^2 . \\end{aligned}\\ ] ] eventually , if @xmath147 , the conditional expectation of @xmath186 for given @xmath187 is given as @xmath188 & \\leq   f(\\x_t)-f(\\x^*)- \\frac{k^2}{106l}\\frac{m}{n}\\|\\nabla{f}(\\x_t)\\|^2 + \\frac{l\\eta^2}{2}\\\\ & \\leq   \\left(1-\\frac{m}{n}\\gamma\\right)(f(\\x_t)-f(\\x^ * ) ) + \\frac{l\\eta^2}{2}. \\end{aligned}\\ ] ] combining the above inequality with the case of @xmath145 , we obtain @xmath189\\\\   & \\leq   \\1[f(\\x_{t})-f(\\x^*)\\geq\\varepsilon']\\cdot \\left [    \\left(1-\\frac{m}{n}\\gamma\\right)(f(\\x_{t})-f(\\x^ * ) ) + \\frac{l\\eta^2}{2}\\right ]   + \\1[f(\\x_{t})-f(\\x^*)<\\varepsilon ' ] \\cdot\\varepsilon ' . \\end{aligned}\\ ] ] the expectation with respect to all @xmath190 yields @xmath191 & \\leq   \\left(1-\\frac{m}{n}\\gamma\\right )   \\ebb[\\1[f(\\x_{t})-f(\\x^*)\\geq\\varepsilon'](f(\\x_{t})-f(\\x^ * ) ) ] \\\\ & \\phantom{\\leq }   + \\ebb[\\1[f(\\x_{t})-f(\\x^*)\\geq\\varepsilon']]\\frac{l\\eta^2}{2 }   + \\ebb[\\1[f(\\x_{t})-f(\\x^*)<\\varepsilon ' ] ] \\varepsilon ' \\\\ &",
    "\\leq   \\left(1-\\frac{m}{n}\\gamma\\right ) \\ebb[f(\\x_{t})-f(\\x^ * ) ]   + \\max\\left\\ { \\frac{l\\eta^2}{2},\\,\\varepsilon ' \\right\\}.\\end{aligned}\\ ] ] since @xmath192 and @xmath193 hold , for @xmath194 $ ] we have @xmath195 when @xmath196 is greater than @xmath87 in , we obtain @xmath197 and @xmath198 let us consider the accuracy of the numerical solution @xmath199 . as shown in ( * ? ? ? * chap .",
    "9 ) , the inequality @xmath200 holds .",
    "thus , for @xmath89 , we have @xmath201 ^ 2\\leq   \\mathbb{e}[\\|\\x_t-\\x^*\\|^2]\\leq\\frac{8l}{\\sigma^2}\\varepsilon   =   64n\\left(\\frac{l}{\\sigma}\\right)^3\\left(1+\\frac{n}{m\\gamma}\\right)\\eta^2 . \\end{aligned}\\ ] ]    [ eqn : lemma_bound_expz ] let @xmath184 be a non - negative random variable satisfying @xmath202=1 $ ] .",
    "then , we have @xmath203_+^2}{(z+1/2)^2}\\right]\\geq\\frac{1}{53}.   \\end{aligned}\\ ] ]    for @xmath204 and @xmath205 , we have the inequality @xmath206_+^2}{(z+1/2)^2 }   \\geq   \\frac{\\delta^2}{(1+\\delta)^2}\\1[z\\geq1/2+\\delta ] . \\end{aligned}\\ ] ] then ,",
    "we get @xmath207_+^2}{(z+1/2)^2}\\right ]   & \\geq   \\frac{\\delta^2}{(1+\\delta)^2}\\ebb[z^2 \\1[z\\geq1/2+\\delta]]\\\\   & =   \\frac{\\delta^2}{(1+\\delta)^2}\\ebb[z^2(1-\\1[z<1/2+\\delta])]\\\\   & =    \\frac{\\delta^2}{(1+\\delta)^2 }   \\left ( 1-\\ebb[z^2\\1[z<1/2+\\delta ] ] \\right)\\\\   & \\geq   \\frac{\\delta^2}{(1+\\delta)^2 }   \\left (   1-(1/2+\\delta)^2\\pr(z<1/2+\\delta )   \\right)\\\\ & \\geq   \\frac{\\delta^2}{(1+\\delta)^2 } \\left ( 1-(1/2+\\delta)^2 \\right ) . \\end{aligned}\\ ] ] by setting @xmath208 appropriately , we obtain @xmath209_+^2}{(z+1/2)^2}\\right]\\geq\\frac{1}{53}.\\end{aligned}\\ ] ]",
    "for the output @xmath210 of blockcd@xmath93 $ ] , @xmath211 holds , and thus , the sequence @xmath212 is included in @xmath213 since @xmath7 is convex and continuous , @xmath214 is convex and closed . moreover , since @xmath7 is convex and it has non - degenerate hessian , the hessian is positive definite , and thus , @xmath7 is strictly convex .",
    "then @xmath214 is bounded as follows .",
    "we set the minimul directional derivative along the radial direction from @xmath26 over the unit sphere around @xmath26 as @xmath215 then , @xmath216 is strictly positive and the following holds for any @xmath217 such that @xmath218 , @xmath219 thus we have @xmath220 since the right hand side of ( [ include ] ) is a bounded ball , @xmath214 is also bounded .",
    "thus , @xmath214 is a convex compact set .    since @xmath7 is twice continuously differentiable , the hessian matrix @xmath221 is continuous with respect to @xmath222 . by the positive definiteness of the hessian matrix ,",
    "the minimum and maximum eigenvalues @xmath223 and @xmath224 of @xmath221 are continuous and positive",
    ". therefore , there are the positive minimum value @xmath1 of @xmath223 and maximum value @xmath9 of @xmath224 on the compact set @xmath214 .",
    "it means that @xmath7 is @xmath1-strongly convex and @xmath9-lipschitz on @xmath214 .",
    "thus , the same argument to obtain ( [ eq : quecom ] ) can be applied for @xmath7 ."
  ],
  "abstract_text": [
    "<S> this paper provides a block coordinate descent algorithm to solve unconstrained optimization problems . in our algorithm , </S>",
    "<S> computation of function values or gradients is not required . instead , pairwise comparison of function values is used . </S>",
    "<S> our algorithm consists of two steps ; one is the direction estimate step and the other is the search step . </S>",
    "<S> both steps require only pairwise comparison of function values , which tells us only the order of function values over two points . in the direction estimate step , </S>",
    "<S> a newton type search direction is estimated . a computation method like block </S>",
    "<S> coordinate descent methods is used with the pairwise comparison . in the search step , a numerical solution is updated along the estimated direction . the computation in the direction estimate step can be easily parallelized , and thus , the algorithm works efficiently to find the minimizer of the objective function . also , we show an upper bound of the convergence rate . in numerical experiments </S>",
    "<S> , we show that our method efficiently finds the optimal solution compared to some existing methods based on the pairwise comparison . </S>"
  ]
}