{
  "article_text": [
    "birnbaum s theorem @xcite is arguably the most controversial result in statistics .",
    "the theorem s conclusion is that a framework for statistical inference that satisfies two natural conditions , namely , the sufficiency principle ( sp ) and the conditionality principle ( cp ) , must also satisfy an exclusive condition , the likelihood principle ( lp ) .",
    "the controversy lies in the fact that lp excludes all those standard methods taught in stat 101 courses .",
    "professor mayo successfully refutes birnbaum s claim , showing that violations of lp need not imply violations of sp or cp .",
    "the key to mayo s argument is a correct formulation of cp ; see also @xcite .",
    "her demonstration resolves the controversy around birnbaum and lp , helping to put the statisticians house in order .",
    "the controversy and confusion surrounding birnbaum s claim has perhaps discouraged researchers from considering questions about the foundations of statistics .",
    "we view professor mayo s paper as an invitation for statisticians to revisit these fundamental questions , and we are grateful for the opportunity to contribute to this discussion .",
    "though lp no longer constrains the frequentist approach , this does not mean that pure frequentism is necessarily correct .",
    "for example , reproducibility issues in large - scale studies is an indication that the frequentist techniques that have been successful in classical problems may not be appropriate for today s high - dimensional problems .",
    "we contend that something more than the basic sampling model is required for valid statistical inference , and appropriate conditioning is one aspect of this . here",
    "we consider what a new framework , called _ inferential models _",
    "( ims ) , has to say concerning the foundations of statistical inference , with a focus on something more fundamental than cp and sp .",
    "for this , we begin in section  [ s : valid ] with a discussion of valid probabilistic inference as motivation for the i m framework .",
    "a general efficiency principle is presented in section  [ s : drp ] and we give an im - based dimension reduction strategy that accomplishes what the classical sp and cp set out to do .",
    "section  [ s : discuss ] gives some concluding remarks .",
    "the sampling model for observable data @xmath0 , depending on an unknown parameter @xmath1 , is the familiar starting point .",
    "our claim is that the sampling model alone is not sufficient for valid probabilistic inference . by `` probabilistic inference ''",
    "we mean a framework whereby any assertion / hypothesis about the unknown parameter has a ( predictive ) probability attached to it after data is observed .",
    "the sampling model facilitates a comparison of the chances of the event @xmath2 on different probability spaces . for probabilistic inference",
    ", there must be a known distribution available , after data is observed .",
    "the random element that corresponds to this distribution shall be called a predictable quantity , and probabilistic inference is obtained by predicting this predictable quantity after seeing data .",
    "the probabilistic inference is `` valid '' if the predictive probabilities are suitably calibrated or , equivalently , have a fixed and known scale for meaningful interpretation . without risk of confusion , we shall call the prediction of the predictable quantity valid if it admits valid probabilistic inference .",
    "we summarize this in the following _ validity principle _ ( vp ) .",
    "probabilistic inference requires associating an unobservable but predictable quantity with the observable data and unknown parameter .",
    "probabilities to be used for inference are obtained by valid prediction of the predictable quantity .",
    "the frequentist approach aims at developing procedures , such as confidence intervals and testing rules , having long - run frequency properties .",
    "expressions like `` 95% confidence '' have no predictive probability interpretation after data is observed , so frequentist methods are not probabilistic in our sense .",
    "nevertheless , certain frequentist quantities , such as @xmath3-values , may be justifiable from a valid probabilistic inference point of view @xcite .",
    "when genuine prior information is available and can be summarized as a usual probability model , the corresponding bayesian inference is both probabilistic and valid [ see @xcite , remark  4 ] .",
    "when no genuine prior information is available , and a default prior distribution is used , the validity property is questionable .",
    "probability matching priors , bernstein ",
    "von mises theorems , etc . , are efforts to make posterior inference valid , in the sense above , at least approximately .",
    "the standard interpretation of these results is , for example , that bayesian credible intervals have the nominal frequentist coverage probability asymptotically ; see @xcite . in that case , the remarks above concerning frequentist methods apply .",
    "fiducial inference was introduced by @xcite to avoid using artificial priors in scientific inference .",
    "subsequent work includes structural inference @xcite , the dempster  shafer theory of belief functions ( @xcite ; @xcite ) , generalized inference ( @xcite ; @xcite ) and generalized fiducial inference ( @xcite , @xcite , @xcite ) .",
    "fiducial distributions are defined by expressing the parameter as a data - dependent function of a pivotal quantity .",
    "this results in a bona fide posterior distribution only in fraser s structural models and , in those cases , it corresponds to a bayesian posterior ( @xcite ; @xcite ) .",
    "therefore , the fiducial distribution is meaningful when the corresponding bayesian prior is meaningful in the sense above .",
    "more on fiducial from the i m perspective is given below .",
    "the i m framework , proposed recently by @xcite , has its roots in fiducial and dempster  shafer theory ; see also @xcite . at a fundamental level ,",
    "the i m approach is driven by vp .",
    "here is a quick overview .",
    "write the sampling model / data - generating mechanism as @xmath4 where @xmath5 is the observable data , @xmath6 is the unknown parameter , and @xmath7 is an unobservable auxiliary variable with known distribution @xmath8 . following vp , the goal is to use the data @xmath0 and the distribution for @xmath9 for meaningful probabilistic inference on @xmath1 without assuming a prior .",
    "the following three steps describe the i m construction .",
    "associate the observed data @xmath2 , the parameter and the auxiliary variable via ( [ eq : assoc ] ) and construct the set - valued mapping , given by @xmath10 the fiducial approach considers the distribution of @xmath11 as a function of @xmath12 .",
    "the i m framework , on the other hand , predicts the unobserved @xmath9 using a random set .",
    "predict the unobservable @xmath9 with a random set @xmath13 .",
    "the distribution @xmath14 of @xmath13 is required to be valid in the sense that @xmath15 where @xmath16 and @xmath17 means `` stochastically no smaller than . ''",
    "combine @xmath18 and @xmath13 as @xmath19 \\bigcup_{u \\in{\\mathcal{s } } } \\theta_x(u)$ ] . for a given assertion @xmath20 , evaluate the evidence in @xmath21 for and not against the claim `` @xmath22 '' via the belief and plausibility functions : @xmath23 in cases where @xmath24 is empty with positive @xmath14-probability , some adjustments to the above formulas are needed ; see @xcite .",
    "the i m output is the pair of functions @xmath25 and , when applied to an assertion @xmath26 about the parameter @xmath1 of interest , these provide a ( personal ) probabilistic summary of the evidence in data @xmath2 supporting the truthfulness of @xmath26 .",
    "property ( [ eq : valid ] ) guarantees that the i m output is valid .",
    "our focus in the remainder of the discussion will be on the a - step and , in particular , auxiliary variable dimension reduction .",
    "such concerns about dimensionality are essential for efficient inference on @xmath1 .",
    "these are also closely tied to the classical ideas of sufficiency and conditionality .",
    "we end this section with a few remarks on fiducial . if one takes the predictive random set @xmath13 in the p - step as a singleton , that is , @xmath27 , where @xmath12 , then @xmath28 and @xmath29 are equal and equal to the fiducial distribution . in this sense ,",
    "fiducial provides probabilistic inference .",
    "however , the singleton predictive random set is not valid in the sense of ( [ eq : valid ] ) , so fiducial inference is generally not valid , violating the second part of vp .",
    "one can also reconstruct the fiducial distribution by choosing a valid predictive random set @xmath13 so that @xmath30 equals the fiducial probability of @xmath26 for all suitable @xmath20 .",
    "but this would generally require that @xmath13 depend on the observed @xmath2 , and the resulting inference suffers from a selection bias , or double - use of the data , resulting in unjustifiably large belief probabilities .",
    "it is natural to strive for efficient statistical inference . in the context of ims",
    ", we want @xmath31 to be as stochastically small as possible , as a function of @xmath0 , when the assertion @xmath26 about @xmath1 is false . to connect this to classical efficiency",
    ", @xmath31 can be interpreted like the @xmath3-value for testing @xmath32 , so stochastically small plausibility when @xmath26 is false corresponds to the high power of the test .",
    "we state the following _ efficiency principle _ ( ep ) .",
    "subject to the validity constraint , probabilistic inference should be made as efficient as possible .",
    "ep is purposefully vague : it allows for a variety of techniques to be employed to increase efficiency .",
    "the next section discusses one important technique related to auxiliary variable dimension reduction .      in the classical",
    "setting , sufficiency reduces the data to a good summary statistic . in the i m context , however , the dimension of the auxiliary variable , not the data , is of primary concern .",
    "for example , in the case of iid sampling , the dimension of @xmath9 is the same as that of @xmath0 , which is usually greater than that of @xmath1 . in such cases ,",
    "it is inefficient to predict a high - dimensional auxiliary variable for inference on a lower dimensional parameter . the idea is to reduce the dimension of @xmath9 to that of @xmath1 .",
    "this auxiliary variable dimension reduction will indirectly result in some transformation of the data .    how to reduce the dimension of @xmath9 ?",
    "we seek a new auxiliary variable @xmath33 , of the same dimension of @xmath1 , such that the baseline association ( [ eq : assoc ] ) can be rewritten as @xmath34 for some functions @xmath35 and @xmath36 , and distribution @xmath37 . here",
    "@xmath37 may actually depend on some features of the data @xmath0 .",
    "such a dimension reduction is general , but @xcite consider an important case , which we summarize here .",
    "suppose we have two one - to - one mappings , @xmath38 and @xmath39 , with the requirement that @xmath40 .",
    "since @xmath41 is observable , so too must be the feature @xmath42 of @xmath9 .",
    "this point has two important consequences : first , a feature of @xmath9 that is observed need not be predicted , hence a dimension reduction ; second , the feature of @xmath9 that is observed naturally provides some information about the part that remains unobserved , so conditioning should help improve prediction . by construction ,",
    "the baseline association ( [ eq : assoc ] ) is equivalent to @xmath43 and this suggests an association of the form ( [ eq : assoc1 ] ) , where @xmath44 and @xmath37 is the conditional distribution of @xmath45 given @xmath40 .",
    "it remains to discuss how the dimension reduction strategy described above related to ep .",
    "the following theorem gives one relatively simple illustration of the improved efficiency via dimension reduction .",
    "[ thm : drp ] suppose that the baseline association ( [ eq : assoc ] ) can be rewritten as ( [ eq : assoc2 ] ) and that @xmath46 and @xmath42 are independent .",
    "then inference based on @xmath47 alone , by a valid prediction of @xmath46 , ignoring @xmath42 , is at least as efficient as inference from ( [ eq : assoc2 ] ) by a valid prediction of @xmath48 .",
    "see the corollary to proposition  1 in martin and liu ( @xcite , full - length version ) ; see also @xcite .",
    "therefore , reducing the dimension of the auxiliary variable can not make inference less efficient .",
    "the point in that paper is that reducing the dimension actually improves efficiency , hence ep .    in the standard examples , for example",
    ", regular exponential families , our dimension reduction above corresponds to classical sufficiency ; see example  [ ex : normal ] . outside the standard examples , the i",
    "m dimension reduction gives something different from sufficiency , in particular , the former often leads directly to further dimension reduction compared to the latter ; see example  [ ex : nile ] . that the i m dimension reduction naturally contains",
    "some form of conditioning is an advantage .",
    "the absence of conditioning in the standard definition of sufficiency is one possible reason why conditional inference has yet to become part of the mainstream .",
    "the i m framework also has advantages beyond dimension reduction and conditioning . in particular , the i m output gives valid prior - free probabilistic inference on @xmath1 .",
    "this section draws some connections between the i m dimension reduction above and sp and cp .",
    "first , it is clear that following the auxiliary variable dimension reduction strategy described above entails cp . in the cox example",
    ", the randomization that determines which measurement instrument will be used corresponds to an auxiliary variable whose value is observed completely .",
    "so , our auxiliary variable dimension reduction strategy implies conditioning on the actual instrument used , hence cp . for sp , theorem  [ thm : drp ]",
    "gives some insight .",
    "that is , when a sufficient statistic has dimension the same as @xmath1 , one can take @xmath49 as that sufficient statistic and select independent @xmath46 and @xmath42 .",
    "in general , our dimension reduction and efficiency considerations are more meaningful than sufficiency and sp .",
    "the examples below illustrate this point further .",
    "[ ex : normal ] suppose @xmath50 are independent @xmath51 samples , and write the association as @xmath52 , where @xmath53 are independent standard normal . in this case , there are lots of candidate mappings @xmath54 to rewrite the baseline association in the form ( [ eq : assoc2 ] ) .",
    "two choices are @xmath55 and @xmath56 . at first look , the second choice , corresponding to sufficiency , seems better than the first .",
    "however , the dimension - reduced associations ( [ eq : assoc1 ] ) based on these two choices are exactly the same .",
    "this means , first , there is nothing special about sufficiency in light of proper conditioning ( @xcite ; @xcite ) .",
    "second , it suggests that , at least in simple problems , the dimension - reduced association ( [ eq : assoc1 ] ) does not depend on the choice of @xmath54 , that is , it only depends on the sufficient statistic , hence sp .",
    "the message here holds more generally , though a rigorous formulation remains to be worked out .",
    "[ ex : nile ] consider independent exponential random variables @xmath50 , the first with mean @xmath1 and the second with mean @xmath57 . in this case , the minimal sufficient statistic , @xmath58 , is two - dimensional while the parameter is one - dimensional . @xcite",
    "take the baseline association as @xmath59 and @xmath60 , where @xmath53 are independent standard exponential .",
    "they employ a novel partial differential equations technique to identify a function @xmath61 of @xmath62 whose value is fully observed , so that only a scalar auxiliary variable needs to be predicted .",
    "their solution is equivalent to that based on the conditional distribution of the maximum likelihood estimate given an ancillary statistic ( @xcite ; @xcite ) .",
    "the message here is that the im - based auxiliary variable dimension reduction strategy does something similar to the classical strategy of conditioning on ancillary statistics , but it does so in a mostly automatic way .",
    "professor mayo is to be congratulated for her contribution . besides resolving the controversy surrounding birnbaum s theorem ,",
    "her paper is an invitation for a fresh discussion on the foundations of statistical inference .",
    "though lp no longer constrains the frequentist approach , we have argued here that something more than the basic sampling model is required for valid statistical inference .",
    "the i m framework features the prediction of unobserved auxiliary variables as this `` something more , '' and the idea of reducing the dimension of the auxiliary variable before prediction leads to improved efficiency , accomplishing what sp and cp are meant to do .",
    "we expect further developments for and from ims in years to come .",
    "supported in part by nsf grants dms-10 - 07678 , dms-12 - 08833 and dms-12 - 08841 ."
  ],
  "abstract_text": [
    "<S> this is an invited contribution to the discussion on professor deborah mayo s paper , `` on the birnbaum argument for the strong likelihood principle , '' to appear in _ </S>",
    "<S> statistical science_. mayo clearly demonstrates that statistical methods violating the likelihood principle need not violate either the sufficiency or conditionality principle , thus refuting birnbaum s claim . with the constraints of birnbaum s theorem lifted </S>",
    "<S> , we revisit the foundations of statistical inference , focusing on some new foundational principles , the inferential model framework , and connections with sufficiency and conditioning . </S>"
  ]
}