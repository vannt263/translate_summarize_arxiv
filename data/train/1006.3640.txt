{
  "article_text": [
    "a gp @xmath1 is a probabilistic map parametrised by a covariance @xmath2 and a mean @xmath3 .",
    "we use @xmath4 and automatic relevance determination ( ard ) @xmath5 in the following . here ,",
    "@xmath6 and @xmath7 denote the signal and noise variance , respectively and the diagonal matrix @xmath8 contains the squared length scales .",
    "since a gp is a distribution over functions @xmath9 , the output @xmath10 is random even though the input @xmath11 is deterministic . in gp regression , a gp prior",
    "is combined with training data @xmath12 into a gp posterior conditioned on the training data with mean @xmath13 and covariance @xmath14 where @xmath15@xmath16^{\\top}$ ] , @xmath17_{ij=1 .. n}$ ] , @xmath18_{ij=1 .. n}$ ] and @xmath19\\mathbf{k}^{-1}$ ] .",
    "deterministic inputs @xmath11 lead to gaussian outputs and gaussian inputs lead to non - gaussian outputs whose first two moments can be computed analytically @xcite for ard covariance .",
    "multivariate deterministic inputs @xmath11 lead to spherical gaussian outputs @xmath20 and gaussian inputs @xmath11 lead to non - gaussian outputs @xmath20 whose moments @xmath21 are given by :    here , @xmath22^{\\top}=[\\mathbf{z}^{1}, .. ,\\mathbf{z}^{n}]\\mathbf{k}^{-1}$ ] and the quantities @xmath23 $ ] and @xmath24 $ ] denote expectations of @xmath25^{\\top}$ ] w.r.t .",
    "the gaussian input distribution @xmath26 that can readily be evaluated in closed form @xcite as detailed in the appendix . in the limit of @xmath27",
    "we recover the deterministic case as @xmath28 , @xmath29 and @xmath30 .",
    "non - zero input variance @xmath31 results in full non - spherical output covariance @xmath32 , even for independent gps because all the gps are driven by the same ( uncertain ) input .",
    "a gplvm @xcite is a successful and popular non - parametric bayesian tool for high dimensional nonlinear data modeling taking into account the data s manifold structure based on a low - dimensional representation .",
    "high dimensional data points @xmath33 , @xmath34 $ ] , are represented by corresponding latent points @xmath35 $ ] from a low - dimensional latent space @xmath36 mapped into @xmath37 by @xmath38 independent gps @xmath39 ",
    "one for each component @xmath40 of the data .",
    "all the gps @xmath39 are conditioned on @xmath41 and share the same covariance and mean functions .",
    "the model is trained by maximising the sum of the log marginal likelihoods over the @xmath38 independent regression problems with respect to the latent points @xmath41 .",
    "the high dimensional ( @xmath42 ) density of the data points @xmath43 in panel ( c ) is modelled by a mixture of gaussians @xmath44 shown in panel ( b , d ) where the means @xmath45 and variances @xmath46 are given by the predictive means and covariances of a set of independent gaussian processes @xmath47 conditioned on low - dimensional ( @xmath48 ) latent locations @xmath49 . a latent dirac mixture ( a )",
    "yields a spherical gaussian mixture with varying widths ( b ) and a latent gaussian mixture ( e ) results in a fully coupled mixture model ( d ) smoothly sharing covariances across mixture components.,scaledwidth=100.0% ]    while most often applied to nonlinear dimensionality reduction , the gplvm can also be used as a tractable and flexible density model in high dimensional spaces as illustrated in figure [ fig : dgplvm ] .",
    "the basic idea is to interpret the latent points @xmath41 as centres of a mixture of either dirac ( figure [ fig : dgplvm]a ) or gaussian ( figure [ fig : dgplvm]e ) distributions in the latent space @xmath50 that are _ projected forward _ by the gp to produce a high dimensional gaussian mixture @xmath44 in the observed space @xmath37 .",
    "depending on the kind of latent mixture , the density model @xmath51 will either be a mixture of spherical ( figure [ fig : dgplvm]b ) or full - covariance gaussians ( figure [ fig : dgplvm]d ) . by that mechanism",
    ", we get a tractable high dimensional density model @xmath51 : a set of low - dimensional coordinates in conjunction with a probabilistic map @xmath1 yield a mixture of high dimensional gaussians whose covariance matrices are smoothly shared between components .",
    "as shown in figure [ fig : dgplvm](d ) , the model is able to capture high dimensional covariance structure along the data manifold by relatively few parameters ( compared to @xmath0 ) , namely the latent coordinates @xmath52 and the hyperparameters @xmath53\\in\\mathbb{r}_{+}^{2d+2}$ ] of the gp .",
    "the role of the latent coordinates @xmath41 is twofold : they both _ define the gp _ , mapping the latent points into the observed space , and they _ serve as centres _ of the mixture density in the latent space .",
    "if the latent density is a mixture of gaussians , the centres of these gaussians are used to define the gp map , but the full gaussians ( with covariance @xmath31 ) are projected forward by the gp map .",
    "learning or model fitting is done by minimising a loss function @xmath54 w.r.t .",
    "the latent coordinates @xmath41 and the hyperparameters @xmath55 . in the following",
    ", we will discuss the usual gplvm objective function , make clear that it is not suited for density estimation and use leave - out estimation to avoid overfitting .",
    "a gplvm @xcite is trained by setting the latent coordinates @xmath41 and the hyperparameters @xmath55 to maximise the probability of the data @xmath56 + that is the product of the marginal likelihoods of @xmath38 independent regression problems . using @xmath57 , conjugate gradients optimisation at a cost of @xmath58 per step is straightforward but suffers from local optima .",
    "however , optimisation of @xmath59 does not encourage the gplvm to be a good density model . only indirectly",
    ", we expect the predictive variance to be small ( implying high density ) in regions supported by many data points .",
    "the main focus of @xmath59 is on faithfully predicting @xmath60 from @xmath41 ( as implemented by the fidelity trace term ) while using a relatively smooth function ( as favoured by the log determinant term ) .",
    "therefore , we propose a different cost function .      density estimation @xcite constructs parametrised estimators @xmath61 from iid data @xmath62 .",
    "we use the kullback - leibler divergence @xmath63 to the underlying density and its empirical estimate @xmath64 as quality measure where @xmath65 emphasises that the full dataset has been used for training .",
    "this estimator , is prone to overfitting if used to adjust the parameters via @xmath66 .",
    "therefore , estimators based on @xmath67 subsets of the data @xmath68 are used .",
    "two well known instances are @xmath67-fold cross - validation ( cv ) and leave - one - out ( loo ) estimation .",
    "the subsets for cv are @xmath69 and @xmath70 for loo .",
    "both of them can be used to optimise @xmath55 .",
    "there are two reasons why training a gplvm with the log likelihood of the data @xmath59 ( eq . [ eq : gplvm - lik ] ) is not optimal in the setting of density estimation : firstly , it treats the task as regression , and does nt explicitly worry about how the density is spread in the observation space .",
    "secondly , our empirical results ( see section [ sec : experiments ] ) indicate , that the test set performance is simply not good .",
    "therefore , we propose to train the model using the leave - out density @xmath71 + this objective is very different from the gplvm criterion as it measures how well a data point is explained under the mixture models resulting from projecting each of the latent mixture components forward ; the leave - out aspect enforces that the point @xmath43 gets assigned a high density even though the mixture component @xmath72 has been removed from the mixture .",
    "the leave - one - out idea is trivial to apply in a mixture setting by just removing the contribution in the sum over components , and is motivated by the desire to avoid overfitting .",
    "evaluation of @xmath73 requires @xmath58 assuming @xmath74 .",
    "however , removing the mixture component is not enough since the latent point @xmath49 is still present in the gp . using rank",
    "one updates to compute inverses and determinants of covariance matrices @xmath75 with row and column @xmath76 removed , it is possible to evaluate eq .",
    "[ eq : dgplvm - lpo ] for mixture components @xmath77 with latent point @xmath49 removed from the mean prediction @xmath78  which is what we do in the experiments .",
    "unfortunately , going further by removing @xmath49 also from the covariance @xmath79 increases the computational burden to @xmath80 because we need to compute rank one corrections to all matrices @xmath81 .",
    "since @xmath82 is only slightly smaller than @xmath79 , we refrain from computing it in the experiments .    in the original gplvm",
    ", there is a clear one - to - one relationship between latent points @xmath49 and data points @xmath43  they are inextricably tied together .",
    "however , the leave - one - out ( loo ) density @xmath73 does not impose any constraint of that sort . the number of mixture components does not need to be @xmath83 , in fact we can choose any number we like .",
    "only the data visible to the gp @xmath84 is tied together .",
    "the actual latent mixture centres @xmath41 are not necessarily in correspondence with any actual data point @xmath43 .",
    "however , we can choose @xmath85 to be a subset of @xmath60 .",
    "this is reasonable because any mixture centre @xmath86 ( corresponding to the latent centre @xmath87 ) lies in the span of @xmath85 , hence @xmath85 should approximately span @xmath60 . in our experiments , we enforce @xmath88 .",
    "overfitting in density estimation means that very high densities are assigned to training points , whereas very low densities remain for the test points . despite its success in parametric models , the leave - one - out idea alone , is not sufficient to prevent overfitting in our model . when optimising @xmath73 w.r.t .",
    "@xmath89 using conjugate gradients , we observe the following behaviour : the model circumvents the loo objective by arranging the latent centres in pairs that take care of each other . more generally , the model partitions the data @xmath90 into groups of points lying in a subspace of dimension @xmath91 and adjusts @xmath89 such that it produces a gaussian with very small variance @xmath92 in the orthogonal complement of that subspace .",
    "by scaling @xmath92 to tiny values , @xmath73 can be made almost arbitrarily large .",
    "it is understood that the hyperparameters of the underlying gp take very extreme values : the noise variance @xmath7 and some length scales @xmath93 become tiny . in @xmath59",
    ", this is penalised by the @xmath94 term , but @xmath73 is happy with very improbable gps . in our initial experiments , we observed this `` cheating behaviour '' on several of datasets .",
    "we conclude that even though the loo objective ( eq . [ eq : dgplvm - lpo ] ) is the standard tool to set kde kernel widths @xcite , it breaks down for too complex models .",
    "we counterbalance this behaviour by leaving out not only one point but rather @xmath95 points at a time .",
    "this renders cheating tremendously difficult . in our experiments we use the leave-@xmath95-out ( lpo ) objective @xmath96 + ideally , one would sum over all @xmath97 subsets @xmath98 of size @xmath99 .",
    "however , the number of terms @xmath67 soon becomes huge : @xmath100 for @xmath101 .",
    "therefore , we use an approximation where we set @xmath102 and @xmath103 contains the indices @xmath104 that currently have the smallest value @xmath105 .    all gradients @xmath106 and @xmath107 can be computed in @xmath58 when using @xmath78 .",
    "however , the expressions take several pages .",
    "we use a conjugate gradient optimiser to find the best parameters @xmath41 and @xmath55 .",
    "in the experimental section , we show that the gplvm trained with @xmath59 ( eq . [ eq : gplvm - lik ] ) does not lead to a good density model in general . using our @xmath108 training procedure ( section [ sub : overfit ] ,",
    "[ eq : dgplvm - lpo ] ) , we can turn it into a competitive density model .",
    "we demonstrate that a latent variance @xmath109 improves the results even further in some cases and that on some datasets , our density model training procedure performs better than all the baselines .",
    "we consider @xmath110 data sets , frequently used in machine learning .",
    "the data sets differ in their domain of application , their dimension @xmath38 , their number of instances @xmath83 and come from regression and classification . in our experiments",
    ", we do not use the labels .",
    "we do not only want to demonstrate that our training procedure yields better test densities for the gplvm .",
    "we are rather interested in a fair assessment of how competitive the gplvm is in density estimation compared to other techniques . as baseline methods",
    ", we concentrate on three standard algorithms : penalised fitting of a mixture of full gaussians ( ` gm ` ) , kernel density estimation ( ` kde ` ) and manifold parzen windows @xcite ` ( mp ) ` .",
    "we run these algorithms for three different type of preprocessing : raw data ( r ) , data scaled to unit variance ( ` s ` ) and whitened data ( ` w ` ) . we explored a large number of parameter settings and report the best results in table [ tab : baselines ] .      in order to speed up em computations , we partition the dataset into @xmath67 disjoint subsets using the @xmath67-means algorithm `` .",
    "we fitted a penalised gaussian to each subset and combined them using the relative cluster size as weight @xmath111 .",
    "every single gaussian @xmath112 has the form @xmath113 where @xmath114 and @xmath115 equal the sample mean and covariance of the particular cluster , respectively .",
    "the global ridge parameter @xmath116 prevents singular covariances and is chosen to maximise the loo log density @xmath117 .",
    "we use simple gradient descent to find the best parameter @xmath118 .",
    "the kernel density estimation procedure fits a mixture model by centring one mixture component at each data point @xmath43 .",
    "we use independent multi - variate gaussians : @xmath119 , where the diagonal widths @xmath120 are chosen to maximise the loo density @xmath121 .",
    "we employ a newton - scheme to find the best parameters @xmath122 .",
    "the manifold parzen window estimator @xcite tries to capture locality by means of a kernel @xmath123 .",
    "it is a mixture of @xmath83 full gaussians where the covariance @xmath124 of each mixture component is only computed based on neighbouring data points .",
    "as proposed by the authors , we use the @xmath125-nearest neighbour kernel and do not store full covariance matrices @xmath126 but a low rank approximation @xmath127 with @xmath128 . as in the other baselines ,",
    "the ridge parameter @xmath116 is set to maximise the loo density .",
    "the results of the baseline density estimators can be found in table [ tab : baselines ] .",
    "they clearly show three things : ( i ) more data yields better performance , ( ii ) penalised mixture of gaussians is clearly and consistently the best method and ( iii ) manifold parzen windows @xcite offer only little benefit .",
    "the absolute values can only be compared within datasets since linearly transforming the data @xmath60 by @xmath129 results in a constant offset @xmath130 in the log test probabilities .",
    "we keep the experimental schedule and setting of the previous section in terms of the @xmath110 datasets , the @xmath131 fold averaging procedure and the maximal training set size @xmath132 .",
    "we use the gplvm log likelihood of the data @xmath59 , the lpo log density with deterministic latent centres ( @xmath133 ) and the lpo log density using a gaussian latent centres @xmath134 to optimise the latent centres @xmath41 and the hyperparameters @xmath55 .",
    "our numerical results include @xmath135 different latent dimensions @xmath136 , @xmath135 preprocessing procedures and @xmath137 different numbers of leave - out points @xmath95 . optimisation is done using @xmath138 conjugate gradient steps alternating between @xmath41 and @xmath55 . in order to compress the big amount of numbers ,",
    "we report the method with highest test density as shown in figure [ fig : test_density ] , only .",
    "each panel displays the log test density averaged over @xmath131 random splits for three different gplvm training procedures and the _ best _ out of 41 baselines ( penalised mixture @xmath139 , diag.+isotropic kde , manifold parzen windows with 36 different parameter settings ) as well as various mixture of factor analysers ( mfa ) settings as a function of the number of training data points @xmath140 . we report the maximum value across latent dimension @xmath141 , three preprocessing methods ( raw , scaled to unit variance , whitened ) and @xmath142 leave - out points . the gplvm training procedures are the following : @xmath108-rd : stochastic leave-@xmath95-out density ( eq . [ eq : dgplvm - lpo ] with latent gaussians , @xmath109 ) , @xmath108-det : deterministic leave-@xmath95-out density ( eq . [ eq : dgplvm - lpo ] with latent diracs , @xmath143 ) and @xmath144 : marginal likelihood ( eq . [ eq : gplvm - lik]).,scaledwidth=100.0% ]    the most obvious conclusion , we can draw from the numerical experiments , is the bad performance of @xmath59 as a training procedure for gplvm in the context of density modeling .",
    "this finding is consistent over all datasets and numbers of training points .",
    "we get another conclusive result in terms of how the latent variance @xmath31 influences the final test densities could be fixed to @xmath145 because its scale can be modelled by @xmath41 . ] . only in the ` bodyfat",
    "` data set it is not beneficial to allow for latent variance .",
    "it is clear that this is an intrinsic property of the dataset itself , whether it prefers to be modelled by a spherical gaussian mixture or by a full gaussian mixture .",
    "an important issue , namely how well a fancy density model performs compared to very simple models , has in the literature either been ignored @xcite or only done in a very limited way @xcite . experimentally",
    ", we can conclude that on some datasets e.g. ` diabetes , sonar , abalone ` our procedure can not compete with a plain ` gm ` model .",
    "however note , that the baseline numbers were obtained as the maximum over a wide ( 41 in total ) range of parameters and methods .",
    "for example , in the ` usps ` case , our elaborate density estimation procedure outperforms a single penalised gaussian only for training set sizes @xmath146 .",
    "however , the margin in terms of density is quite big : on @xmath147 prewhitened data points @xmath148 with deterministic latents yields @xmath149 at @xmath150 , whereas full @xmath148 reaches @xmath151 at @xmath152 which is significantly above @xmath153 as obtained by the ` gm ` method  since we work on a logarithmic scale , this corresponds to factor of @xmath154 in terms of density .      while the baseline methods such as ` gm , ` ` kde ` and ` mp ` run in a couple of minutes for the ` usps ` dataset , training a gplvm with either @xmath155 or @xmath15@xmath59 takes considerably longer since a lot of cubic covariance matrix operations need to be computed during the joint optimisation of @xmath89 .",
    "the gplvm computations scale cubically in the number of data points @xmath140 used by the gp forward map and quadratically in the dimension of the observed space @xmath38 .",
    "the major computational gap is the transition from @xmath143 to @xmath109 because in the latter case , covariance matrices of size @xmath0 have to be evaluated which cause the optimisation to last in the order of a couple of hours . to provide concrete timing results , we picked @xmath147 , @xmath150 , averaged over the @xmath110 datasets and show times relative to @xmath144 .",
    "note that the methods @xmath108 are run in a conservative fail - proof black box mode with @xmath138 gradient steps .",
    "we observe good densities after considerably less gradient steps already .",
    "another straightforward speedup can be obtained by carefully pruning the number of inputs to the @xmath108 models .",
    "we have discussed how the basic gplvm is not in itself a good density model , and results on several datasets have shown , that it does not generalise well .",
    "we have discussed two alternatives based on explicitly projecting forward a mixture model from the latent space .",
    "experiments show that such density models are generally superior to the simple gplvm .    among the two alternative ways of defining the latent densities ,",
    "the simplest is a mixture of delta functions , which  due to the stochasticity of the gp map  results in a smooth predictive distribution .",
    "however , the resulting mixture of gaussians , has only axis aligned components .",
    "if instead the latent distribution is a mixture of gaussians , the dimensions of the observations become correlated .",
    "this allows the learnt densities to faithfully follow the underlying manifold .",
    "although the presented model has attractive properties , some problems remain : the learning algorithm needs a good initialisation and the computational demand of the method is considerable .",
    "however , we have pointed out that in contrast to the gplvm , the number of latent points need not match the number of observations allowing for alternative sparse methods ."
  ],
  "abstract_text": [
    "<S> density modeling is notoriously difficult for high dimensional data . </S>",
    "<S> one approach to the problem is to search for a lower dimensional manifold which captures the main characteristics of the data . </S>",
    "<S> recently , the gaussian process latent variable model ( gplvm ) has successfully been used to find low dimensional manifolds in a variety of complex data . </S>",
    "<S> the gplvm consists of a set of points in a low dimensional latent space , and a stochastic map to the observed space . </S>",
    "<S> we show how it can be interpreted as a density model in the observed space . </S>",
    "<S> however , the gplvm is not trained as a density model and therefore yields bad density estimates . </S>",
    "<S> we propose a new training strategy and obtain improved generalisation performance and better density estimates in comparative evaluations on several benchmark data sets .    </S>",
    "<S> modeling of densities , aka unsupervised learning , is one of the central problems in machine learning . despite its long history @xcite , </S>",
    "<S> density modeling remains a challenging task especially in high dimensional spaces . </S>",
    "<S> for example , the generative approach to classification requires density models for each class , and training such models well is generally considered more difficult than the alternative discriminative approach . </S>",
    "<S> classical approaches to density modeling include both parametric and non parametric methods . in general , </S>",
    "<S> simple parametric approaches have limited utility , as the assumptions might be too restrictive . </S>",
    "<S> mixture models , typically trained using the em algorithm , are more flexible , but e.g. gaussian mixture models are hard to fit in high dimensions , as each component is either diagonal or has in the order of @xmath0 parameters , although the mixtures of factor analyzers algorithm @xcite may be able to strike a good balance . </S>",
    "<S> methods based on kernel density estimation @xcite are another approach , where bandwidths may be set using cross validation @xcite .    </S>",
    "<S> the methods mentioned so far have two main shortcomings : 1 ) they typically do not perform well in high dimensions , and 2 ) they do not provide an intuitive or generative understanding of the data . </S>",
    "<S> generally , we can only succeed if the data has some regular structure , the model can discover and exploit . </S>",
    "<S> one attempt to do this is to assume that the data points in the high dimensional space lie on  or close to  some smooth underlying lower dimensional manifold . </S>",
    "<S> models based on this idea can be divided into models based on _ implicit _ or _ explicit _ representations of the manifold . </S>",
    "<S> an implicit representation is used by @xcite in a non - parametric gaussian mixture with adaptive covariance to every data point . </S>",
    "<S> explicit representations are used in the generative topographic map @xcite and by @xcite . within the explicit camp , </S>",
    "<S> models contain two separate parts , a lower dimensional latent space equipped with a density , and a function which maps points from the low dimensional latent space to the high dimensional space where the observations lie . </S>",
    "<S> advantages of this type of model include the ability to understand the structure of the data in a more intuitive way using the latent representation , as well as the technical advantage that the density in the observed space is automatically properly normalised by construction .    the gaussian process latent variable model ( gplvm ) @xcite uses a gaussian process ( gp ) @xcite to define a ( stochastic ) map between a lower dimensional latent space and the observation space . </S>",
    "<S> however , the gplvm does not include a density in the latent space . in this paper </S>",
    "<S> , we explore extensions to the gplvm based on densities in the latent space . </S>",
    "<S> one might assume that this can trivially be done , by thinking of the latent points learnt by the gplvm as representing a mixture of delta functions in the latent space . </S>",
    "<S> since the gp based map is stochastic , it induces a proper mixture in the observed space . </S>",
    "<S> however , this formulation is unsatisfactory , because the resulting model is not trained as a density model . </S>",
    "<S> consequently , our experiments show poor density estimation performance .    </S>",
    "<S> mixtures of gaussians form the basis of the vast majority of density estimation algorithms . </S>",
    "<S> whereas kernel smoothing techniques can be seen as introducing a mixture component for each data point , infinite mixture models @xcite explore the limit as the number of components increases and mixtures of factor analysers impose constraints on the covariance of individual components . the algorithm presented in this paper </S>",
    "<S> can be understood as a method for stitching together gaussian mixture components in a way reminiscent of @xcite using the gplvm map from the lower dimensional manifold to induce factor analysis like constraints in the observation space . in a nutshell </S>",
    "<S> , we propose a density model in high dimensions by transforming a set of low - dimensional gaussians with a gp .    </S>",
    "<S> we begin by a short introduction to the gplvm and show how it can be used to define density models . in section [ sec : learning ] , we introduce a principled learning algorithm , and experimentally evaluate our approach in section [ sec : experiments ] . </S>"
  ]
}