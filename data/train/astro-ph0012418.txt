{
  "article_text": [
    "the growing size and complexity of the available cmb data sets poses a difficult challenge for the data analysis ( borrill ( 1999 ) , borrill ( 2000 ) ) .",
    "the largest already existing data sets ( , ) consist of up to many million of the time measurements sampling areas of the sky of up to many thousands of the beam - size pixels .",
    "the analysis of such a big data set is usually divided into three distinct stages focusing in turn on the time - ordered data , maps and power spectra . on each stage",
    "the size of the data item to be manipulated is significantly reduced .",
    "however , a successful implementation of such a program requires that the compression performed on each stage should should not compromise the cosmological information contained in the data .",
    "the algorithms presented in the following are designed to be optimal  or nearly optimal  in a maximum - likelihood sense under the assumptions of the gaussianity and stationarity of the instrumental noise .",
    "we start from the description of the time - ordered data manipulation techniques which prove to be necessary to ensure the efficiency and applicability of the map making algorithms",
    ". then we briefly review a variety of map making algorithms highlighting involved assumptions and assessing their efficiency .",
    "in the usual map - making methodology the noise power spectra are taken to be given with an arbitrary high precision .",
    "the circumstances in which actual cmb data are collected are specific enough to require the noise estimation to be evaluated directly from the time - ordered data .",
    "the robust noise - estimation procedure therefore needs to be capable of efficient dealing with the undesirable effects present in the real data .",
    "the common feature of the realistic time streams are short breaks ( gaps ) in their continuity caused by the cosmic rays or other transients in the experimental apparatus .",
    "their presence not only prevents a straightforward application of the fast fourier techniques ( fft ) to the noise estimation but also introduces a whole suite of subtle problems related to their treatment on the level of the time - ordered data manipulation .",
    "they also appear to be one of major obstacles in an implementation of the efficient and fast map making algorithms",
    ". therefore restoration of the continuity of the time - ordered data  in a statistically correct way  stands out as one of the important goals of the data analysis on the time domain stage .",
    "a required algorithm needs simultaneously to estimate the noise power spectrum and fill the gaps with pure gaussian noise realization without compromising its correlations throughout the length of the time stream . as described in detail in stompor ( 2001 ) that can be achieved iteratively when on each step of the iterations the gaps are filled with the constrained realization ( hoffman & ribak 1991 ) of the gaussian noise ,",
    "a power spectrum of which has been determined on the preceding step of the iterations using standard fft methods .",
    "the noise estimation procedure attempts to estimate a noise ensemble average power spectrum using just one realization of the time stream . that is clearly not sufficient . as a result of the above",
    "sketched algorithm one usually ends up with the reliable estimation of the ensemble average noise power spectrum in time domain at sufficiently high frequencies ( for  for @xmath0hz ) , but the low frequency end is a subject to a non - negligible sampling error . to minimize the effect of the low frequencies we marginalize over the low frequency part of the spectrum .",
    "no significant dependence on the assumed low frequency cut - off ( in the range from @xmath1hz up to @xmath2hz ) has been found in the  case .",
    "adding randomly drawn signal to the data introduces an extra freedom and may undermine the uniqueness of the results .",
    "though depending on the particular noise realization the results may somewhat differ , all of them are statistically equivalent by construction , and no bias is introduced .",
    "moreover , that extra randomness can be minimized by introducing a fictitious gap pixel to the recovered map . as a result",
    "we find that the final maps and their power spectra for  are robust and not affected by that procedure .",
    "so far , for simplicity we have been assuming that the time - ordered data are noise dominated , however , an extension of the described algorithm for the more general case is straightforward ( ferreira & jaffe 2000 ) .",
    "the general algebra for the maximum - likelihood map making under the assumption of the gaussian correlated noise can be found in tegmark ( 1997 ) . in short ,",
    "if the time stream data @xmath3 can be modeled as @xmath4 where @xmath3 is the vector of the measurements for the given chunk of the time stream , @xmath5 a pointing matrix assigning each of the time samples to an appropriate pixel in the sky and @xmath6 - a time stream noise - is a vector of the random gaussian variables with correlations as given by @xmath7 - a time domain correlation matrix , then a ( maximum - likelihood ) map @xmath8 and its noise - correlation matrix in pixel domain ( @xmath9 ) are given by , @xmath10 here @xmath11 is a positive definite symmetric matrix .",
    "@xmath12 is a time domain representation of the filters applied to the time stream on the previous stages ( a prewhitening filter ) assumed to be orthogonal @xmath13 .    if @xmath14 then @xmath8 is a minimum variance map . the other choices can sacrifice the optimality but being computationally faster . in particular tegmark ( 1997 ) proposed to choose as @xmath11 only a circulant part of the @xmath7 matrix . in the following",
    "we discuss a number of approaches and their applications to a realistic -like data set .",
    "the implementation of the exact minimum variance estimator of the map seems a daunting task .",
    "the time stream length ( @xmath15 ) may easily reach many millions of the time samples making an inversion of the time domain noise correlation matrix  an `` @xmath15-cubed '' process  prohibitive . however , for a  like experiment with the time stream chunks of the length reaching `` only '' up to @xmath16 time samples we found that the implementation is feasible even on a moderately large workstation .    at the heart of that implementation",
    "lies a simple observation that the time domain correlation matrix of the stationary and continues time stream is toeplitz @xmath17 . the inversion of the the toeplitz matrix can be performed in as few as @xmath18 operations . a clearly feasible task for @xmath19 time samples .",
    "( an even faster algorithm exists ( golub & van loan 1983 ) bringing that number down to @xmath20 . ) an extra gain in a number of operations can be achieved if the noise - correlation length is shorter than the time stream length and the noise - correlation matrix band - diagonal .",
    "the actual time streams are commonly strewn with the gaps and a toeplitz character of the correlation matrix lost .",
    "however the gap filling algorithm , presented earlier , reconstructs the continuity of the time stream and its stationarity . to minimize the entirely spurious content of the gaps being added to the map all the time samples from the gaps",
    "are directed to an extra fictitious pixel ( `` a gap pixel '' ) which is subsequently rejected ( marginalized over ) from the map and the pixel domain noise - correlation matrix .",
    "the computational effort for the case when @xmath21 scale with the number of pixels @xmath22 and the number of time samples @xmath15 as :    * noise inverse in time domain : @xmath23 : @xmath24 ; * noise inverse in pixel domain : @xmath25 : @xmath24 ; * noise weighted map : @xmath26 : @xmath24 ; * noise matrix in pixel domain : @xmath27 : @xmath28 ; * final map : @xmath29 @xmath30 .    in the case of the first three items from the list the substantial savings can be made if the fact that the inverse noise - correlation matrix is assumed to be band - diagonal  an approximation usually well fulfilled for an inverse of a band - diagonal noise - correlation matrix .",
    "if the noise - correlation matrix is sparse then the additional savings can also be made .",
    "memory - wise clearly there is only need to keep a first row of the @xmath7 matrix and in a typical situation the major requirement would be then set by the size of the @xmath31 matrix which is @xmath32 and the size of the noise correlation for the entire map @xmath30 .",
    "note that the first of these limits can be alleviated at the expense of the operation counts ( there is no need to keep all @xmath33 matrix at the same time but then some of the parts may have to be recomputed a multiple number of times ) .      the way to speed up map making but still aiming at the optimal minimum variance map",
    "was proposed by ferreira & jaffe ( 2000).the approximation those authors favor assumes that , @xmath34 here @xmath35 and @xmath36 are the time chunk and correlation lengths respectively , and a subscript @xmath37 denotes a circulant part of the noise correlation matrix defined as @xmath38 for @xmath39 .    this approach is designed to perform the @xmath7 inversion with the `` fft '' speed providing at the same time a good approximation to the exact solution of the previous section . by construction , however , it can be only exact in the absence of the noise correlations .",
    "the operation count changes only for two , but the most time consuming , items from the list in the previous paragraph which now read ,    * noise inverse in time domain : @xmath23 : @xmath40 . *",
    "noise weighted map : @xmath41 : @xmath42 ;    the scaling of the noise weighted map computation is given assuming using fft and a toeplitz matrix property ( golub & van loan 1983 ) .",
    "the required memory is set by the size of the noise matrix in pixel domain @xmath30 .",
    "it is apparent that if the efficient fast ( `` @xmath43 '' ) implementation of the toeplitz matrix inversion is available then the major computational advantage of the approximate method over the exact one would vanish and only the memory requirement would remain as its main asset .",
    "this approximation is implemented in the very successful , publicly available package called madcap by julian borrill ( borrill 2000 ) .      in this case @xmath44 and the noise - correlation matrix in pixel domain can be expressed as follows ( tegmark 1997 ) , @xmath45 where we decomposed explicitly a noise - correlation matrix in time domain into its circulant ( @xmath46 ) and sparse parts ( @xmath47 ) , @xmath48 .",
    "the sparse part of the matrix corrects for the presence of `` corners '' of the circulant matrix and the gaps in the time stream .",
    "as advocated by tegmark ( 1997 ) such a matrix should be really sparse and a number of the required operation greatly decreased over the exact approach described above . in the implementation of that author the @xmath12 filters",
    "are additionally used to enhance sparseness .    the price to pay for it is not only a non - optimal map as a result ( the loss of the precision is hoped not to be substantial here ) , but also definitely more complicated algebra to be implemented especially in a presence of large number of gaps in the time stream .",
    "if no assumption about the band - diagonality is made then the computational costs scale as follows :    noise inverse in time domain : @xmath23 : @xmath40 ;    noise inverse in pixel domain :    circulant part : @xmath25 : @xmath24 ;    sparse part : @xmath49 : @xmath50 ;    noise weighted map : @xmath26 : @xmath42 ;    noise matrix in pixel domain : @xmath27 : @xmath28 .    we have omitted the filter matrix @xmath12 in above expressions assuming that both @xmath11 and @xmath12 are circulant and therefore their product can be computed with the `` fft speed '' .",
    "the effort is dominated usually by the sparse part computation ( @xmath51 ) though the above scaling includes only the fastest growing term which can not be suppressed if only the band diagonality of the @xmath7 matrix is assumed .",
    "memory required for performing that task is @xmath32 .",
    "clearly if no more approximations are made this approach is far from competitive with any of those discussed above .",
    "one possible way of improving on that is to recognize the fact that if @xmath46 is band - diagonal ( plus of course non - zero corners to fulfill the circulancy criterion ) then also @xmath52 , @xmath53 and @xmath54 are usually approximately band - diagonal .",
    "another possible approximation , referred to hereafter , is to neglect completely the sparse correction in the expression for the noise - correlation matrix in the pixel domain .",
    "clearly the operational cost goes down to @xmath18 or @xmath55  whichever larger  and is usually @xmath28 especially if a band diagonal character of the inverse of @xmath56 is assumed .",
    "in such a case the method achieves the performance comparable with that of ferreira & jaffe ( 2000 ) .      in the paper by stompor ( 2001 ) we show that though none of discussed above approximations reproduces perfectly the map and the noise - correlation matrix in the pixel domain ( @xmath57 ) , it preserves the statistical information contained in the data ",
    "2-point correlation properties  correctly .",
    "in fact we find that both the approximations , as described above , fare very well in recovering the anisotropy power spectrum with no apparent systematic bias .",
    "clearly the performance of these approximations needs to be further tested if other statistics are to be applied to the maps .    out of the two exact methods the minimum variance implementation described here performs generally much better .",
    "the very fact that the exact maximum - likelihood method can be applied to the data sets of the  size is of importance for the further development , tests and validation of the approximate algorithms .",
    "the simple operation counts and memory requirements are clearly great assets of those methods and major stumbling blocks for the exact approach to go far beyond the size of the current data sets .",
    "it is worth noticing that the gaps filling procedure described in the beginning is important for all the described methods : in a case of the exact methods ( optimal and circulant ) it facilitates the practical feasibility of the implementation . for the approximate methods it improves the accuracy and robustness of the involved approximations .",
    "we briefly have discussed here the making of the cmb temperature anisotropy maps from the time - ordered data . we have argued that for the data set with up to a few millions of time samples producing the maps of up to a few thousands of the pixels is readily possible without need for sacrificing the optimal character of the final map .",
    "moreover accurate and efficient approximate methods also exist .",
    "those have been tested through the direct comparison with the exact method on the realistic simulations and the  data set and can become a starting point for developing algorithms capable of coping with still bigger and more complicated data sets .",
    "it seems likely that the hard problem of this kind of data analysis will be accounting on all kinds of imperfectness of a real data set in the statistically sound way and without the substantial increase of the operational count , rather than solely a sheer size of a data set itself . as an example here",
    "we have demonstrated how to incorporate the presence of short discontinuities in the time - ordered data .",
    "the other common problems are to be discussed in stompor ( 2001 ) .",
    "rs acknowledges support of nasa grant nag5 - 3941 and help of polish state committee for scientific research grant no.@xmath58 2p03d01719 .",
    "ahj and jhpw acknowledge support from nasa ltsa grant no.@xmath58 nag5 - 6552 and nsf kdi grant no.@xmath58 9872979 .",
    "pgf acknowledges support from the rs .",
    "br and cdw acknowledge support from nasa gsrp grants no.@xmath58 s00-gsrp-032 and s00-gsrp-031 ."
  ],
  "abstract_text": [
    "<S> we review selected methods of the cosmic microwave background data analysis appropriate for the analysis of the largest currently available data sets . </S>",
    "<S> we focus on techniques of the time - ordered data manipulation and map making algorithms based on the maximum - likelihood approach . </S>",
    "<S> the presented methods have been applied to the  data analysis ( hanany 2000 ) and the description of the algorithms is illustrated with the examples drawn from that experience . </S>",
    "<S> the more extensive presentation of the here - mentioned issues will be given in the forthcoming paper ( stompor 2001 ) . </S>"
  ]
}