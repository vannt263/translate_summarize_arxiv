{
  "article_text": [
    "the vision of fully autonomous and intelligent systems that learn by themselves has influenced ai and robotics research for many decades .",
    "to devise fully autonomous systems , it is necessary to ( 1 ) process perceptual data ( e.g. , images ) to summarize knowledge about the surrounding environment and the system s behavior in this environment , ( 2 ) make decisions based on uncertain and incomplete information , ( 3 ) take new information into account for learning and adaptation . effectively , any fully autonomous system has to close this perception - action - learning loop without relying on specific human expert knowledge .",
    "the _ pixels to torques problem _",
    "@xcite identifies key aspects of an autonomous system : autonomous thinking and decision making using sensor measurements only , intelligent exploration and learning from mistakes .",
    "we consider the problem of learning closed - loop policies ( `` torques '' ) from pixel information end - to - end .",
    "a possible scenario is a scene in which a robot is moving about .",
    "the only available sensor information is provided by a camera , i.e. , no direct information of the robot s joint configuration is available .",
    "the objective is to learn a continuous - valued policy that allows the robotic agent to solve a task in this continuous environment in a data - efficient way , i.e. , we want to keep the number of trials small . to date",
    ", there is no fully autonomous system that convincingly closes the perception - action - learning loop and solves the pixels to torques problem in continuous state - action spaces , the natural domains in robotics .    a promising approach toward solving the pixels to torques problem is reinforcement learning ( rl ) @xcite , a principled mathematical framework that deals with fully autonomous learning from trial and error .",
    "however , one practical shortcoming of many existing rl algorithms is that they require many trials to learn good policies , which is prohibitive when working with real - world mechanical plants or robots .    one way of using data efficiently ( and",
    "therefore keep the number of experiments small ) is to learn forward models of the underlying dynamical system , which are then used for internal simulations and policy learning .",
    "these ideas have been successfully applied to rl , control and robotics in  @xcite , for instance . however",
    ", these methods use heuristic or engineered low - dimensional features , and they do not easily scale to data - efficient rl using pixel information only because even `` small '' images possess thousands of dimensions .    a common way of dealing with high - dimensional data is to learn low - dimensional feature representations .",
    "deep learning architectures , such as deep neural networks  @xcite , stacked auto - encoders  @xcite , or convolutional neural networks  @xcite , are the current state of the art in learning parsimonious representations of high - dimensional data .",
    "deep learning has been successfully applied to image , text and speech data in commercial products , e.g. , by google , amazon and facebook .",
    "deep learning has been used to produce first promising results in the context of model - free rl on images : for instance ,  @xcite present an approach based on deep - q - learning , in which human - level game strategies are learned autonomously , purely based on pixel information .",
    "moreover , @xcite presented an approach that learns good discrete actions to control a slot car based on raw images , employing deep architectures for finding compact low - dimensional representations .",
    "other examples of deep learning in the context of rl on image data include  @xcite . these approaches have in common that they try to estimate the value function from which the policy is derived .",
    "however , neither of these algorithms learns a predictive model and are , therefore , prone to data inefficiency , either requiring data collection from millions of experiments or relying on discretization and very low - dimensional feature spaces , limiting their applicability to mechanical systems .    to increase data efficiency , we therefore introduce a model - based approach to learning from pixels to torques . in particular , exploit results from  @xcite and",
    "jointly learn a lower - dimensional embedding of images and a transition function in this lower - dimensional space that we can use for internal simulation of the dynamical system .",
    "for this purpose , we employ deep auto - encoders for the lower - dimensional embedding and a multi - layer feed - forward neural network for the transition function .",
    "we use this deep dynamical model to predict trajectories and apply an adaptive model - predictive - control ( mpc ) algorithm @xcite for online closed - loop control , which is practically based on pixel information only .",
    "mpc has been well explored in the control community , however , adaptive mpc has so far not received much attention in the literature @xcite . an exception is  @xcite , where the authors advocate a neural network approach similar to ours .",
    "however , they do not consider high - dimensional data but assume that they have direct access to low - dimensional measurements .",
    "our approach benefits from the application of model - based optimal control principles within a machine learning framework . along these lines",
    ", @xcite suggested to first learn a transition model and then use optimal control methods to solve rl problems . unlike these methods , our approach does not need to estimate value functions and scales to high - dimensional problems .",
    "similar to our approach ,  @xcite recently proposed model - based rl methods that learn policies directly from visual information . unlike these methods , we exploit a low - dimensional feature representation that allows for fast predictions and online control learning via mpc .      we consider a classical @xmath0-step finite - horizon rl setting in which an agent attempts to solve a particular task by trial and error .",
    "in particular , our objective is to find a closed - loop policy @xmath1 that minimizes the long - term cost @xmath2 , where @xmath3 denotes an immediate cost , @xmath4 is the continuous - valued system state and @xmath5 are continuous control inputs .",
    "the learning agent faces the following additional challenges : ( a ) the agent has no access to the true state , but perceives the environment only through high - dimensional pixel information ( images ) , ( b ) a good control policy is required in only a few trials .",
    "this setting is practically relevant , e.g. , when the agent is a robot that is monitored by a video camera based on which the robot has to learn to solve tasks fully autonomously .",
    "therefore , this setting is an instance of the pixels to torques problem .",
    "our approach to solve the pixels - to - torques problem is based on a deep dynamical model ( ddm ) , which jointly ( i ) embeds high - dimensional images in a low - dimensional feature space via deep auto - encoders and ( ii ) learns a predictive forward model in this feature space  @xcite . in particular , we consider a ddm with control inputs @xmath6 and high - dimensional observations @xmath7 .",
    "we assume that the relevant properties of @xmath7 can be compactly represented by a feature variable @xmath8 .",
    "the two components of the ddm , i.e. , the low - dimensional embedding and the prediction model , which predicts future observations @xmath9 based on past observations and control inputs , are detailed in the following . throughout this paper , @xmath10 denotes the high - dimensional measurements , @xmath11 the corresponding low - dimensional encoded features and @xmath12 the reconstructed high - dimensional measurement .",
    "further , @xmath13 and @xmath14 denote a predicted feature and measurement at time @xmath15 , respectively .",
    "/[count= ] in 1,2,3,missing,4 ( input- ) at ( 0,2.5- ) ;    in 1,missing,2 ( hiddena- ) at ( 2,2 - 1.25 ) ;    in 1,missing,2 ( hiddenb- ) at ( 4,1.5- ) ;    in 1,missing,2 ( hiddenc- ) at ( 6,2 - 1.25 ) ;    /[count= ] in 1,2,3,missing,4 ( output- ) at ( 8,2.5- ) ;    [count = i ] in 1,2,3,m ( input - i )  + +",
    "( -1.2,0 ) node [ above , midway ] @xmath16 ;    [count = i ] in 1,m at ( hiddenb-i.north ) @xmath17 ;    [count = i ] in 1,2,3,m ( output - i )  + +",
    "( 1.9,0 ) node [ above , midway ] @xmath18 ;    iin 1, ... ,4 in 1, ...",
    ",2 ( input - i )  ( hiddena- ) ;    iin 1, ... ,2 in 1, ... ,2 ( hiddena - i ) ",
    "( hiddenb- ) ; iin 1, ... ,2 in 1, ... ,2 ( hiddenb - i ) ",
    "( hiddenc- ) ;    iin 1, ... ,2 in 1, ... ,4 ( hiddenc - i ) ",
    "( output- ) ;    [count = from 0 ] in input layer + ( high - dim .",
    "data ) , , hidden layer + ( feature ) , , output layer + ( reconstructed ) at ( 2,3 )  ; [count = from 0 ] in , @xmath19 , , @xmath20 , at ( 2,-2.8 )  ;    ( u0 ) at ( -4,-0.5 ) ; ( u0b ) at ( -4,-0.5 ) @xmath21 ; ( ua ) at ( -2.5,-0.5 ) @xmath22 ; ( u1 ) at ( -1,-0.5 ) @xmath23 ; ( ya ) at ( -2.5,1.5 ) @xmath22 ; ( y0 ) at ( -4,1.5 ) ; ( y0b ) at ( -4,1.5 ) @xmath24 ; ( y1 ) at ( -1,1.5 ) @xmath25 ; ( y2 ) at ( 1,1.5 ) ; ( y2b ) at ( 1,1.5 ) ; ( d1 ) at ( -2.5,-0.5 ) ; ( d2 ) at ( -2.5,1.5 ) ; ( d3 ) at ( -2.5,3.3 ) ; ( z0 ) at ( -4,3.3 ) ; ( z0b ) at ( -4,3.3 ) @xmath26 ; ( za ) at ( -2.5,3.3 ) @xmath22 ; ( z1 ) at ( -1,3.3 ) @xmath27 ; ( z2 ) at ( 1,3.3 ) ; ( z2b ) at ( 1,3.3 ) ; at ( -4.7,3.3 ) high - dim . + observations ; at ( -4.7,1.5 ) features ; at ( -4.7,-0.5 ) control + inputs ; ( y1 )  ( y2 ) ; ( u1 )  ( y2 ) ; ( u0 )  ( y2 ) node[midway , below ] @xmath28 ; ( z1 )  ( y1 ) node[midway , left ] @xmath29 ; ( z0 )  ( y0 )",
    "node[midway , left ] @xmath29 ; ( y2 )  ( z2 ) node[midway , left ] @xmath30 ; ( y0 ) .. controls ( -1,0.7 ) .. ( y2 ) ;      we use a deep auto - encoder for embedding images in a low - dimensional feature space , where both the encoder @xmath29 and the decoder  @xmath30 are modeled with deep neural networks .",
    "each layer @xmath31 of the _ encoder _ neural network @xmath29 computes @xmath32 , where @xmath33 is a sigmoidal activation function ( we used @xmath34 ) and @xmath35 and @xmath36 are free parameters .",
    "the input to the first layer is the image , i.e. , @xmath37 .",
    "the last layer is the low - dimensional feature representation of the image @xmath38 , where @xmath39 $ ] are the parameters of all neural network layers .",
    "the _ decoder _ @xmath30 consists of the same number of layers in reverse order , see [ fig : autoencoder ] , and approximately inverts the encoder @xmath30 , such that @xmath40 is the reconstructed version of @xmath10 with an associated reconstruction error @xmath41 the main purpose of the deep auto - encoder is to keep this reconstruction error and the associated compression loss negligible , such that the features @xmath42 are a compact representation of the images @xmath43 .",
    "we now turn the static auto - encoder into a dynamical model that can predict future features @xmath44 and images @xmath45 .",
    "the encoder @xmath46 allows us to map high - dimensional observations @xmath47 onto low - dimensional features @xmath42 . for predicting we assume that _ future features _",
    "depend on an @xmath49-step history @xmath50 of past features and control inputs , i.e. , @xmath51 where @xmath28 is a nonlinear transition function , in our case a feed - forward neural network , and @xmath52 are the corresponding model parameters .",
    "this is a nonlinear autoregressive exogenous model ( narx ) @xcite .",
    "the predictive performance of the model will be important for model predictive control ( see section  [ sec : mpc ] ) and for model learning based on the prediction error  @xcite .",
    "to predict _ future observations _ @xmath53 we exploit the decoder , such that @xmath54 .",
    "the deep decoder  @xmath30 maps features  @xmath8 to high - dimensional observations @xmath7 parameterized by @xmath55 .",
    "now , we are ready to put the pieces together : with feature prediction model and the deep auto - encoder , the ddm predicts future features and images according to    [ eq : prediction ] @xmath56{\\ ( - \\)}}{n}+1},u_{{t}{\\scalebox{0.75}[1.0]{\\ ( - \\)}}{n}+1};\\theta_{\\text{p } } ) , \\notag \\\\ \\widehat{{y}}_{{t}+1 \\mid { h_n}}(\\theta_{\\text{e}},\\theta_{\\text{d}},\\theta_{\\text{p } } ) & = g(\\widehat{{z}}_{{t}+1 \\mid { h_n}};\\theta_{\\text{d } } ) , \\end{aligned}\\ ] ]    which is illustrated in [ fig : model ] . with this prediction model",
    "we define the prediction error @xmath57 where @xmath9 is the observed image at time @xmath58 .",
    "the ddm is parameterized by the encoder parameters @xmath59 , the decoder parameters @xmath55 and the prediction model parameters @xmath52 . in the ddm , we train both the prediction model and the deep auto - encoder jointly by finding parameters @xmath60 , such that    @xmath61    which minimizes the sums of squared reconstruction   and prediction   errors .",
    "we learn all model parameters @xmath62 _ jointly _ by solving  .. in a second step , the prediction model parameters @xmath52 are estimated based on these features by minimizing   conditioned on the estimated @xmath63 and @xmath64 . in our experience ,",
    "a problem with this approach is that the learned features might have a small reconstruction error , but this representation will not be ideal for learning a transition model .",
    "the supplementary material discusses this in more detail . ]",
    "the required gradients with respect to the parameters are computed efficiently by back - propagation , and the cost function is minimized by the bfgs algorithm  @xcite .",
    "note that in   it is crucial to include not only the prediction error @xmath65 , but also the reconstruction error @xmath66 . without this term",
    "the multi - step ahead prediction performance will decrease because predicted features are not consistent with features achieved from the encoder . since we consider a control problem in this paper , multi - step ahead predictive performance is crucial .",
    "[ [ initialization . ] ] initialization .",
    "+ + + + + + + + + + + + + + +    with a linear activation function the auto - encoder and pca are identical @xcite , which we exploit to initialize the parameters of the auto - encoder : the auto - encoder network is unfolded , each pair of layers in the encoder and the decoder are combined , and the corresponding pca solution is computed for each of these pairs .",
    "we start with high - dimensional image data at the top layer and use the principal components from that pair of layers as input to the next pair of layers .",
    "thereby , we recursively compute a good initialization for all parameters of the auto - encoder .",
    "similar pre - training routines are found in @xcite , in which a restricted boltzmann machine is used instead of pca .    in this section ,",
    "we have presented a ddm that facilitates fast predictions of high - dimensional observations via a low - dimensional embedded time series .",
    "the property of fast predictions will be exploited by the online feedback control strategy presented in the following .",
    "more details on the proposed model are given in @xcite .",
    "we use the ddm for learning a closed - loop policy by means of nonlinear model predictive control ( mpc ) .",
    "we start off by an introduction to classical mpc , before moving on to mpc on images in section  [ sec : mpc on images ] .",
    "mpc finds an optimal sequence of control signals that minimizes a @xmath67-step loss function , where @xmath67 is typically smaller than the full horizon . in general",
    ", mpc relies on ( a ) a reference trajectory @xmath68 ( which can be a constant reference signal ) and ( b ) a dynamics model @xmath69 which , assuming that the current state is denoted by @xmath70 , can be used to compute / predict a state trajectory @xmath71 for a given sequence @xmath72 of control signals . using",
    "the dynamics model mpc determines an optimal ( open - loop ) control sequence @xmath73 , such that the predicted trajectory @xmath74 gets as close to the reference trajectory @xmath75 as possible , such that @xmath76 where @xmath77 is a cost associated with the deviation of the predicted state trajectory @xmath78 from the reference trajectory @xmath75 , and @xmath79 penalizes the amplitude of the control signals .",
    "note that the predicted @xmath80 depends on all previous @xmath81 .",
    "when the control sequence @xmath73 is determined , the first control @xmath82 is applied to the system . after observing the next state , mpc repeats the entire optimization and turns the overall policy into a closed - loop ( feedback ) control strategy .",
    "we now turn the classical mpc procedure into mpc on images by exploiting some convenient properties of the ddm .",
    "the ddm allows us to predict _ features _",
    "@xmath83 based on a sequence of controls @xmath84 . by comparing with , we define the _ state _ @xmath70 as the present and past @xmath85 features and the past @xmath85 control inputs , such that @xmath86 .",
    "\\label{eq : state def}\\end{aligned}\\ ] ] the ddm computes the present and past features with the encoder @xmath87 , such that @xmath70 is known at the current time , which matches the mpc requirement .",
    "our objective is to control the system towards a desired reference image frame @xmath88 .",
    "this reference frame @xmath88 can also be encoded to a corresponding reference feature @xmath89 , which results in the mpc objective @xmath90 where @xmath70 , defined in  , is the current state .",
    "the gradients of the cost function with respect to the control signals @xmath84 are computed in closed form , and we use bfgs to find the optimal sequence of control signals . note that the objective function depends on @xmath84 not only via the control penalty @xmath79 but also via the feature predictions @xmath91 of the ddm via  .",
    "overall , we now have an online mpc algorithm that , given a trained ddm , works indirectly on images by exploiting their feature representation . in the following",
    ", we will now turn this into an iterative algorithm that learns predictive models from images and good controllers from scratch .",
    "we will now turn over to describe how ( adaptive ) mpc can be used together with our ddm to address the pixels to torques problem and to learn from scratch . at the core of our mpc formulation",
    "lies the ddm , which is used to predict future states   from a sequence of control inputs .",
    "the quality of the mpc controller is inherently bound to the prediction quality of the dynamical model , which is typical in model - based rl  @xcite .    to learn models and controllers from scratch",
    ", we apply a control scheme that allows us to update the ddm as new data arrives . in particular",
    ", we use the mpc controller in an adaptive fashion to gradually improve the model by collected data in the feedback loop without any specific prior knowledge of the system at hand .",
    "data collection is performed in closed - loop ( online mpc ) , and it is divided into multiple sequential trials .",
    "after each trial , we add the data of the most recent trajectory to the data set , and the model is re - trained using all data that has been collected so far .",
    "follow a random control strategy and record data update ddm with all data collected so far get state @xmath92 via auto - encoder @xmath93-greedy mpc policy using ddm prediction apply @xmath94 and record data    simply applying the mpc controller based on a randomly initialized model would make the closed - loop system very likely to converge to a point , which is far away from the desired reference value , due to the poor model that can not extrapolate well to unseen states .",
    "this would in turn imply that no data is collected in unexplored regions , including the region that we actually are interested in .",
    "there are two solutions to this problem : either we use a probabilistic dynamics model as suggested in  @xcite to explicitly account for model uncertainty and the implied natural exploration or we follow an explicit exploration strategy to ensure proper excitation of the system . in this paper",
    ", we follow the latter approach . in particular , we choose an @xmath95-greedy exploration strategy where the optimal feedback @xmath82 at each time step is selected with a probability @xmath96 , and a random action is selected with probability @xmath95 .",
    "algorithm  [ alg : proposed ] summarizes our adaptive online mpc scheme .",
    "we initialize the ddm with a random trial .",
    "we use the learned ddm to find an @xmath95-greedy policy using predicted features within mpc .",
    "this happens online .",
    "the collected data is added to the data set and the ddm is updated after each trial .",
    "in the following , we empirically assess the components of our proposed methodology for autonomous learning from high - dimensional synthetic image data : ( a ) the quality of the learned ddm and ( b ) the overall learning framework .",
    "in both cases , we consider a sequence of images ( @xmath97 pixels ) and a control input associated with these images .",
    "each pixel @xmath98 is a component of the measurement @xmath99 and assumes a continuous gray - value in the interval @xmath100 $ ] .",
    "no access to the underlying dynamics or the state ( angle @xmath101 and angular velocity @xmath102 ) was available , i.e. , we are dealing with a high - dimensional continuous state space .",
    "the challenge was to learn ( a ) a good dynamics model ( b ) a good controller from pixel information only .",
    "we used a sampling frequency of @xmath103s and a time horizon of @xmath104s , which corresponds to 100 frames per trial .",
    "the input dimension has been reduced to @xmath105 prior to model learning using pca . with these 50-dimensional inputs ,",
    "a four - layer auto - encoder network was used with dimension 50 - 25 - 12 - 6 - 2 , such that the features were of dimension @xmath106 , which is optimal to model the periodic angle of the pendulum .",
    "the order of the dynamics was selected to be @xmath107 ( i.e. , we consider two consecutive image frames ) to capture velocity information , such that @xmath108 .",
    "for the prediction model @xmath28 we used a feedforward neural network with a 6 - 4 - 2 architecture .",
    "note that the dimension of the first layer is given by @xmath109 .      to assess the predictive performance of the ddm",
    ", we took @xmath110 screenshots of a moving tile , see [ fig : results_long_term ] .",
    "the control inputs are the ( random ) increments in position in horizontal and vertical directions .",
    "[ sec : model_learning ]    we evaluate the performance of the learned ddm in terms of long - term predictions , which play a central role in mpc for autonomous learning .",
    "long - term predictions are obtained by concatenating multiple 1-step ahead predictions .",
    "+    the performance of the ddm is illustrated in [ fig : results_long_term ] on a test data set .",
    "the top row shows the ground truth images and the bottom row shows the ddm s long - term predictions .",
    "the model predicts future frames of the tile with high accuracy both for 1-step ahead and multiple steps ahead .",
    "the model yields a good predictive performance for both one - step ahead prediction and multiple - step ahead prediction . in [ fig : latent_2dtile_new ] , the feature representation of the data is displayed .",
    "the features reside on a two - dimensional manifold that encodes the two - dimensional position of the moving tile . by inspecting the decoded images we can see that each corner of the manifold corresponds to a corner position of the tile . due to this structure",
    "a relatively simple prediction model is sufficient to describe the dynamics . in case the auto - encoder and",
    "the prediction model would have been learned sequentially ( first training the auto - encoder , and then based on these features values train the prediction model ) such a structure would not have been enforced . in [ fig : latent_2dtile_sep ] the corresponding feature representation is displayed where only the auto - encoder has been trained .",
    "clearly , these features does not exhibit such a structure .      in this section ,",
    "we report results on learning a policy that moves a pendulum ( 1-link robot arm with length 1 m , weight 1 kg and friction coefficient 1nsm / rad ) from a start position @xmath111 to a target position @xmath112 .",
    "the reference signal was the screenshot of the pendulum in the target position .",
    "for the mpc controller , we used a    planning horizon of @xmath113 steps and a control penalty @xmath114 . for the @xmath95-greedy exploration strategy we used @xmath115 .",
    "we conducted 50 independent experiments with different random initializations .",
    "the learning algorithm was run for 15 trials ( plus an initial random trial ) .",
    "after each trial , we retrained the ddm using all collected data so far , where we also include the reference image while learning the auto - encoder .",
    "[ fig : pred_example ] displays the decoded images corresponding to learned latent representations in @xmath116 ^ 2 $ ] .",
    "the learned feature values of the training data ( green ) line up in a circular shape , such that a relatively simple prediction model is sufficient to describe the dynamics .",
    "if we would not have optimized for both the prediction error and reconstruction error , such an advantageous structure of the feature values would not have been obtained .",
    "the ddm extracts features that can also model the _ dynamic _ behavior compactly .",
    "the figure also shows the predictions produced by the mpc controller ( yellow ) , starting from the current time step ( cyan ) and targeting the reference feature ( red ) where the pendulum is in the target position .    to assess the controller performance after each trial ,",
    "we applied a greedy policy ( @xmath117 ) . in [ fig : control_traj ] , angle trajectories for 15 of the 50 experiments at different learning stages are displayed . in the first trial ,",
    "the controller managed only in a few cases to drive the pendulum toward the reference value @xmath118 .",
    "the control performance increased gradually with the number of trials , and after the 15th trial , it manages in most cases to get it to an upright position .    to assess the data efficiency of our approach , we compared it with the pilco rl framework  @xcite to learning closed - loop control policies for the pendulum task above",
    ". pilco is a current state - of - the art model - based rl algorithm for data - efficient learning of control policies in continuous state - control spaces . using collected data pilco learns a probabilistic model of the system dynamics , implemented as a gaussian process ( gp )  @xcite .",
    "subsequently , this model is used to compute a distribution over trajectories and the corresponding expected cost , which is used for gradient - based optimization of the controller parameters .",
    "although pilco uses data very efficiently , its computational demand makes its direct application impractical for many data points or high - dimensional ( @xmath119d ) problems , such that we had to make suitable adjustments to apply pilco to the pixels - to - torques problem .",
    "in particular , we performed the following experiments : ( 1 ) pilco applied to 20d pca features , ( 2 ) pilco applied to 2d features learned by deep auto - encoders , ( 3 ) an optimal baseline where we applied pilco to the standard rl setting with access to the `` true '' state ( @xmath120 )  @xcite .",
    "+ [ error bars/.cd , y dir = both , y explicit ] coordinates ( 0 , 0 ) + - ( 0,0 ) ( 100 , 0.91 ) + - ( 0,0.08 ) ( 200 , 1 ) + - ( 0,0 ) ( 300 , 1 ) + - ( 0,0 ) ( 400 , 1 ) + - ( 0,0 ) ( 500 , 1 ) + - ( 0,0 ) ( 600 , 1 ) + - ( 0,0 ) ( 700 , 1 ) + - ( 0,0 ) ( 800 , 1 ) + - ( 0,0 ) ( 900 , 1.0 ) + - ( 0,0 ) ( 1000 , 1 ) + - ( 0,0 ) ( 1100 , 1.0 ) + - ( 0,0 ) ( 1200 , 0.91 ) + - ( 0,0.08 ) ( 1300 , 1.0 ) + - ( 0,0 ) ( 1400 , 1.0 ) + - ( 0,0 ) ( 1500 , 0.75 ) + - ( 0,0.13 ) ; plot coordinates ( 0 , 0 ) ( 100 , 0 ) ( 200 , 0 ) ( 300 , 0 ) ( 400 , 0 ) ( 500 , 0 ) ( 600 , 0 ) ( 700 , 0 ) ( 800 , 0 ) ( 900 , 0 ) ( 1000 , 0 ) ( 1100 , 0 ) ( 1200 , 0 ) ( 1300 , 0 ) ( 1400 , 0 ) ( 1500 , 0 ) ; plot coordinates ( 0 , 0 ) ( 100 , 0 ) ( 200 , 0 ) ( 300 , 0 ) ( 400 , 0 ) ( 500 , 0 ) ( 600 , 0 ) ( 700 , 0 ) ( 800 , 0 ) ( 900 , 0 ) ( 1000 , 0 ) ( 1100 , 0 ) ( 1200 , 0 ) ( 1300 , 0 ) ( 1400 , 0 ) ( 1500 , 0 ) ; + [ error bars/.cd , y dir = both , y explicit ] coordinates ( 0 , 0 ) + - ( 0 , 0 ) ( 100 , 0.08 ) + - ( 0 , 0.038756 ) ( 200 , 0.22 ) + - ( 0 , 0.059178 ) ( 300 , 0.5 ) + - ( 0 , 0.071429 ) ( 400 , 0.5 ) + - ( 0 , 0.071429 ) ( 500 , 0.64 ) + - ( 0 , 0.068571 ) ( 600 , 0.72 ) + - ( 0 , 0.064143 ) ( 700 , 0.76 ) + - ( 0 , 0.061012 ) ( 800 , 0.68 ) + - ( 0 , 0.066639 ) ( 900 , 0.82 ) + - ( 0 , 0.054884 ) ( 1000 , 0.8 ) + - ( 0 , 0.057143 ) ( 1100 , 0.78 ) + - ( 0 , 0.059178 ) ( 1200 , 0.82 ) + - ( 0 , 0.054884 ) ( 1300 , 0.86 ) + - ( 0 , 0.04957 ) ( 1400 , 0.88 ) + - ( 0 , 0.046423 ) ( 1500 , 0.86 ) + - ( 0 , 0.04957 ) ;    [ fig : success_rate ] displays the average success rate of pilco ( including standard error ) and our proposed method using deep dynamical models together with a tailored mpc ( ddm+mpc ) .",
    "we define `` success '' if the pendulum s angle is stabilized within @xmath121 around the target state .",
    "the baseline ( pilco trained on the ground - truth 2d state ( @xmath120 ) ) is shown in blue and solves the task very quickly .",
    "the graph shows that our proposed algorithm ( black ) , which learns torques directly from pixels , is not too far behind the ground - truth rl solution , achieving a n almost 90% success rate after 15 trials ( 1500 image frames ) .",
    "however , pilco trained on the 2d auto - encoder features ( red ) and 20d pca features fail consistently in all experiments we explain pilco s failure by the fact that we trained the auto - encoder and the transition dynamics in feature space separately .",
    "the auto - encoder finds good features that minimize the reconstruction error .",
    "however , these features are not good for modeling the dynamic behavior of the system , .",
    "see supplementary material for more details .",
    "] and lead to bad long - term predictions .",
    "computation times of pilco and our method are vastly different : while pilco spends most time optimizing policy parameters , our model spends most of the time on learning the ddm .",
    "computing the optimal nonparametric mpc policy happens online and does not require significant computational overhead . to put this into context",
    ", pilco required a few days of learning time for 10 trials ( in a 20d feature space ) . in a 2d feature space ,",
    "running pilco for 10 trials and 1000 data points requires about 10 hours .",
    "overall , our ddm+mpc approach to learning closed - loop policies from high - dimensional observations exploits the learned deep dynamical model to learn good policies fairly data efficiently .",
    "we have proposed a data - efficient model - based rl algorithm that learns closed - loop policies in continuous state and action spaces directly from pixel information .",
    "the key components of our solution are ( 1 ) a deep dynamical model ( ddm ) that is used for long - term predictions in a compact feature space and ( 2 ) an mpc controller that uses the predictions of the ddm to determine optimal actions on the fly without the need for value function estimation . for the success of this rl algorithm",
    "it is crucial that the ddm learns the feature mapping and the predictive model in feature space jointly to capture dynamic behavior for high - quality long - term predictions .",
    "compared to state - of - the - art rl our algorithm learns fairly quickly , scales to high - dimensional state spaces and facilitates learning from pixels to torques .",
    "this work was supported by the swedish foundation for strategic research under the project cooperative localization and the swedish research council under the project probabilistic modeling of dynamical systems ( contract number : 621 - 2013 - 5524 ) .",
    "mpd was supported by an imperial college junior research fellowship ."
  ],
  "abstract_text": [
    "<S> data - efficient learning in continuous state - action spaces using very high - dimensional observations remains a key challenge in developing fully autonomous systems . in this paper , we consider one instance of this challenge , the pixels to torques problem , where an agent must learn a closed - loop control policy from pixel information only . </S>",
    "<S> we introduce a data - efficient , model - based reinforcement learning algorithm that learns such a closed - loop policy directly from pixel information . </S>",
    "<S> the key ingredient is a deep dynamical model that uses deep auto - encoders to learn a low - dimensional embedding of images jointly with a predictive model in this low - dimensional feature space . </S>",
    "<S> joint learning ensures that not only static but also dynamic properties of the data are accounted for . </S>",
    "<S> this is crucial for long - term predictions , which lie at the core of the adaptive model predictive control strategy that we use for closed - loop control . </S>",
    "<S> compared to state - of - the - art reinforcement learning methods for continuous states and actions , our approach learns quickly , scales to high - dimensional state spaces and is an important step toward fully autonomous learning from pixels to torques . </S>"
  ]
}