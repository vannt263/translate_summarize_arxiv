{
  "article_text": [
    "since the pioneering work of hopfield @xcite , there has been much interest in both the training and performance of attractor neural networks .",
    "training consists in encoding an appropriate synaptic matrix that enables the network to store a macroscopic number of patterns , while the performance of a network refers to the ability to retrieve one or a specific set of stored patterns @xcite .",
    "training and performance are usually thought of as separate stages in the operation of a network .",
    "the retrieval performance of an attractor network can be studied in two different scenarios @xcite .",
    "one is characterized by a _ fixed _",
    "synaptic prescription , as in the case of the hopfield model @xcite or the maximally stable network ( msn ) [ 4 - 6 ] , while in the other one , the entire space of synaptic interactions is searched for optimal performance whenever there is a change in the retrieval environment .",
    "the synapsis in the first scenario are determined in an ordinary learning stage and the performance of the network is optimized separately in a given training environment . in the second scenario one",
    "resorts to a continuously going on adaptive training process in which the network performance is optimized in an adiabatically evolving retrieving environment @xcite . for each value of the noise parameter t ( temperature of the retrieval dynamics ) , and storage ratio @xmath3",
    ", the network has a unique interaction configuration , the so - called _",
    "retriever_. this is in distinction to the retrieval performance that yields the phase diagrams for the hopfield model or the msn , in which the interaction configuration determined in the separate learning stage is the same for all t and @xmath3 @xcite .",
    "adaptive training processes seem to be biologically appealing as a mean to learn from the environment .",
    "the adaptive process in the second scenario requires training the network with noisy patterns @xcite and it is a procedure that does not separate the training process as a distinct step from the operating stage of the network .",
    "the principle of adaptation in a network of binary units consists in the search of the interaction space for the optimized network performance adjusting the training noise to be the same as the retrieval noise in each step of the adiabatically evolving retrieving environment .",
    "both noises refer to the hamming distances between the actual states of the network and the encoded patterns .",
    "training noises have been introduced in feedforward networks @xcite in order to avoid overfitting to training examples and in attractor networks with the purpose of enlarging their basins of attraction @xcite . a slightly distorted set of random patterns is presented to the network in the process of encoding the synaptic matrix by means of a stepwise updating procedure following the perceptron learning rule @xcite .",
    "the msn is generated by an infinitesimal amount of training noise and , except for low retrieval noise t and low load @xmath3 , the performance of the optimally adapted network is clearly superior to that of the msn @xcite . in particular , for low to moderate t and higher load @xmath3 , a second optimal solution in interaction space appears for each value of the training noise in the optimally adapted network .",
    "this solution is a weaker retriever which can be interpreted as an attractor of self - adaptation .",
    "the point is that the second retriever constitutes a further solution to the optimization process , with its own interaction , in a neighborhood of interaction space where there is no solution for the msn",
    ". this second , optimal solution , appears as a low performance solution in the absence or for low to moderate retrieval noise , with improving performance , up to a certain point , as the retrieval noise is raised .",
    "thus , there has to be already a certain level of retrieval noise for the weak retriever to have an interesting performance .",
    "moreover , whenever the solution exists it is only within a narrow range of @xmath3 .",
    "the principle of adaptation has been worked out , so far , only for a network of binary neurons and the purpose of the present paper is to explore the merits of an extension of the principle to a multi  state attractor network in which both the neurons and the noisy training versions of the encoded patterns can be in @xmath4 states .",
    "this adds two new dimensions to the study of the performance of the network .",
    "first , the randomly distributed noisy patterns presented to the network in the training process introduce a training activity @xmath5 .",
    "second , the firing rate of the neurons is determined by one or more thresholds , or a growth parameter in the dynamical output function .",
    "thus , in the extension considered in this work , an evolving dynamical overlap @xmath6 and a dynamical activity @xmath7 are generated at each time step @xmath8 of the neuron updating procedure .",
    "the search for the optimized network performance by means of the extended adaptation principle consists now in the adjustment of the training overlap @xmath9 and the training activity @xmath5 to be @xmath10 and @xmath11 , respectively , i.e. , the same as the retrieval overlap and dynamical activity in each step of the adiabatically evolving retrieval environment .",
    "adaptive performance in this wider sense is a self  consistent procedure in which the retrieval environment continuously optimizes the attractor performance of the network .",
    "networks of multi  state neurons have interesting features and applications .",
    "feedforward networks of such units can be used to study multi  class classification problems @xcite , while multi  state attractor networks , which are useful for the recognition of various grey  toned patterns , are networks that have interesting inferential properties , by means of which the storage capacity and the retrieval ability can be enhanced when they are trained with patterns of low activity [ 1316 ] .",
    "also , the categorization ability can be improved in a multi  state network with hierarchical patterns .",
    "there has been lately considerable interest in such networks [ 1719 ] .",
    "we consider an extremely diluted network and , for simplicity , restrict ourselves to binary unbiased encoded patterns .",
    "the main emphasis of the paper is on the storage capacity , the quality of the performance of the strong and the second retrievers and on the characterization of the various phases that can appear . with that purpose",
    "we produce explicit results for a network with @xmath1 or @xmath2 states .",
    "it will be shown that , within a finite range of a threshold parameter , there is a considerable improvement of the storage capacity and in the high performance of the second retriever solution , in the absence or for low retrieval noise , when compared with the optimally adapted network of binary neurons .",
    "in particular , we show that the second retriever may attain a fairly high retrieval overlap for small training noise in a regime where there is no solution for the optimally adapted network of binary neurons .",
    "these are important results in the search for improvement of the behavior of attractor neural networks .",
    "we restrict ourselves to finite@xmath0 state networks , in place of addressing the general ( large@xmath0 ) case .",
    "the outline of the paper is the following . in section 2",
    "we extend the training with noise procedure in the space of synaptic interactions to a @xmath0state ising network by means of a quenched optimization approach @xcite , within the replica  symmetry ansatz , introducing a smooth cost function given by an average squared hamming distance .",
    "the equations for the adaptation process in a noisy retrieval environment are formulated in that section .",
    "the explicit results for the fixed  point behavior , the storage capacity and the corresponding phase diagrams for self ",
    "adaptation for the three and the four  state models are discussed in section 3 , and compared with the msn .",
    "the domain of validity of the replica symmetric results is determined by the de  almeida  thouless lines @xcite in terms of the retrieval noise and the threshold in the dynamical updating procedure . a summary and concluding remarks are presented in section 4 .",
    "consider a network of @xmath12 nodes with a dynamical variable @xmath13 , at time step @xmath8 on node @xmath14 , that indicates the extent to which the unit on node @xmath14 fires .",
    "each unit can be in any one of @xmath0 ising states @xmath15 in the interval @xmath16 $ ] , for @xmath17 .",
    "a macroscopic set of @xmath18 binary patterns @xmath19 , with @xmath20 , is encoded in the network in the learning process , where @xmath21 is the connectivity of a node .",
    "the patterns constitute a set of independent identically distributed random variables .",
    "training consists in presenting to the network a noisy version @xmath22 of the patterns , at time @xmath8 , and in the optimization of the network output after one time step .",
    "this involves a dynamical process in the space of state configurations of the network and , to keep the dynamics simple , we restrict ourselves to an extremelly diluted network .",
    "each @xmath23 is assumed to be in one of @xmath0 states , @xmath24 , and can be thought of as an example of the pattern @xmath25 .",
    "assuming that every noisy pattern has the same overlap @xmath26 with the corresponding pattern @xmath25 , and that the activity @xmath27 is the same for all patterns in the training set , we define @xmath28 and @xmath29 where the brackets @xmath30 denote averages over the probability distribution of @xmath31 .",
    "thus , the noisy training inputs are constrained to satisfy the mean @xmath32 and variance @xmath33 .",
    "the normalized local field at node @xmath14 , due to the activity at the other nodes , is given by @xmath34 where @xmath35 is the synaptic connection between nodes @xmath14 and @xmath36 , independently in what state the dynamical variable @xmath37 is , while @xmath38 denote the nodes feeding node @xmath14 .",
    "the connections follow the spherical constraint @xmath39 , and we consider the extremely diluted network in the limit of large connectivity in which @xmath40 .",
    "the one time ",
    "step dynamics is exact in this limit .",
    "we deal in this paper with the asymptotic , equilibrium configuration @xmath41 , for the synaptic matrix elements of the learning process that follows from a langevin dynamics with a noise term .",
    "this involves an annealing temperature @xmath42 which takes care that the network does not get trapped in local minima of the free energy .",
    "the distribution of equilibrium states of the @xmath35 can then be described by a canonical ensemble with temperature @xmath42 .",
    "thus , there are two time scales in this approach : a short  time scale for the dynamical evolution of the synaptic matrix @xmath41 and a long  time scale for the dynamical evolution of the training and of the retrieval parameters .",
    "the dynamical variables are updated according to the rule @xmath43 where @xmath44 is the non  decreasing step function @xmath45   \\label{2.4}\\ ] ] shown in figure  1 for @xmath1 and @xmath2 in which @xmath46 is the unitary step function , @xmath47 , @xmath48 , @xmath49 is the threshold parameter and @xmath24 are the uniformly spaced ising states of eq .",
    "( [ 2.1 ] ) .",
    "according to eq .",
    "( [ 2.4 ] ) , there is a zero activity state whenever @xmath0 is odd and none if @xmath0 is even .    for the adapted optimization a temperature @xmath50",
    "is introduced as a noise parameter , not to be confused with the annealing temperature @xmath51 , to characterize the noisy retrieval environment .",
    "we assume a gaussian thermal noise term added to the local field to write the one  step output as @xmath52 where @xmath53 has mean zero and unit variance .",
    "the optimization , in the extremely diluted limit , consists in penalizing deviations from the minimal output error in one time step on any node which is independent of the optimization on all the other nodes .",
    "thus , it is sufficient to consider the cost function for a single node .",
    "we choose this to be @xmath54\\right\\rangle_z\\right\\rangle_r\\;,\\ ] ] where @xmath55 is the average squared hamming distance to a stored pattern in which @xmath56 denotes the average over the gaussian thermal noise .",
    "the training noise enters only through the local fields , via eqs.([2.2 ] ) and ( [ 2.9 ] ) . in the case of binary patterns ,",
    "the local field is a gaussian random variable with mean @xmath57 and variance @xmath58 , in which @xmath59 is the local field on node @xmath14 due to the pattern @xmath60 .",
    "the optimization of the hamming distance between the one  step output of the network in the noisy _ training _ environment and a given pattern in a network of binary neurons is equivalent to finding the optimal output overlap after one time step . in the case of a network of multi  state neurons , the hamming distance also depends on the activity through the local field , and our first goal is to find the optimal output hamming distance @xmath61 , after one time step , for a given training overlap _ and _ a given training activity .    for that purpose , and for later use , we need the averages @xmath62 and @xmath63 which follow from eq .",
    "( [ 2.9 ] ) , where    @xmath64    with @xmath65 and @xmath66    the quenched optimization approach @xcite requires the introduction of the partition function @xmath67 \\label{2.18}\\ ] ] to obtain first an annealed average over the space of synaptic connections @xmath35 , in which @xmath68 is the inverse annealing temperature , and @xmath69 is the squared hamming distance , for a given configuration @xmath70 of encoded patterns , averaged over thermal and training noises . its dependence on the noise parameters @xmath26 , @xmath27 and @xmath50 is left implicit .",
    "the quenched average free  energy is then obtained making use of the replica method to write @xmath71 where @xmath72 denotes the average over the set of stored patterns @xmath70 . using the standard technique in the space of synaptic interactions , with the assumption of replica symmetry @xcite , we obtain the optimal one  step output hamming distance for training @xmath73 as a function of the overlap @xmath26 and activity @xmath27 of the noisy input patterns , in which @xmath74 is a gaussian measure and @xmath75 here , @xmath76 is the squared hamming distance averaged over @xmath25 , while @xmath77 and @xmath78 for all @xmath79 is the spin  glass order parameter for the problem .",
    "the optimization in the training process amounts to take the limits @xmath80 and @xmath81 keeping @xmath82 finite . a single solution in the space of interactions is thus obtained out of the full multiplicity of solutions when @xmath81 @xcite .",
    "the minimization with respect to @xmath83 yields @xmath84 where @xmath85 is the inverse function of @xmath86 . on the other hand ,",
    "the extremum in @xmath82 gives the saddle  point equation @xmath87 ^ 2\\ ; , \\label{2.25}\\ ] ] which determines the storage capacity @xmath3 for a given training environment .    in cases where @xmath88 is a multivalued function of @xmath89 , which is the case for @xmath90",
    ", there may be one or more transitions , each with a fixed @xmath91 between an upper and a lower value @xmath92 and @xmath93 , respectively , ruled by a maxwell construction @xmath94 where @xmath95 .",
    "it turns out that the function @xmath96 is the same on both sides of the `` first  order '' transition .    the optimal output hamming distance for training becomes then @xmath97 it is convenient to introduce the distribution of the local fields due to the encoded patterns , defined as [ 35 ] @xmath98 where the ensemble average @xmath99 is performed with the partition function @xmath100 , eq .",
    "( [ 2.18 ] ) .",
    "it turns out that this distribution becomes @xmath101 and the transition between the lower and upper bonds , @xmath93 and @xmath92 respectively , implies a gap in the distribution of local fields @xmath102 whenever @xmath88 is a multivalued function of @xmath89 .",
    "the optimal one  step output hamming distance for training with noise may now be written as @xmath103 where @xmath104 is the optimized overlap between the encoded patterns and their noisy versions and @xmath105 is their optimized activity .",
    "the distribution of the local fields , @xmath106 , is a characteristic property of the training set and , as such , it depends on @xmath26 and @xmath27 .",
    "the formal results presented so far assume that replica symmetry holds in the space of interactions .",
    "the condition for local stability of the replica symmetric saddle  point can be writen as @xcite @xmath107 ^ 2\\ ; , \\label{2.39}\\ ] ] in which @xmath108 , and this is to be solved together with eq .  ( [ 2.25 ] ) . when the distribution of the local fields has a gap , @xmath109 diverges and the condition can not be satisfied .",
    "then , the network becomes unstable to replica  symmetry  breaking fluctuations .",
    "the limiting load for which eq .",
    "( [ 2.39 ] ) is still satisfied yields the de almeida  thouless ( at ) line , @xmath110 @xcite .",
    "the dependence on the retrieval noise @xmath50 comes from @xmath83 .",
    "note that the at line must lie within the one  band region or , at most , on the band  merging surface where the gap in @xmath102 disappears @xcite .",
    "this completes the formal description of the training process in itself . in order to become",
    "optimally adapted , we consider now the retriever process .",
    "the calculation of the one  step output hamming distance between _ any _ input state @xmath111 and a given encoded pattern in a noisy retrieval environment , with temperature @xmath50 , is now obtained as follows .",
    "first , the training parameters @xmath26 and @xmath27 in eqs .",
    "( [ 2.14])([2.16 ] ) are replaced by the overlap @xmath6 and the dynamical activity @xmath7 of the noisy _ retrieval _ state @xmath111 , expressed respectively as eqs .",
    "( [ 2.5 ] ) and ( [ 2.6 ] ) with @xmath111 in place of the the noisy pattern @xmath22 .",
    "the one  step output hamming distance in the _ retrieval _ environment is now given by an expression similar to eq .",
    "( [ 2.36 ] ) , depending on the pair ( @xmath112 ) through the distribution of local fields _ and _ on the pair ( @xmath113 ) through the present state of the network as given , literally , by eqs .",
    "( [ 2.37 ] ) and ( [ 2.38 ] ) .",
    "now , the training overlap @xmath26 and the training activity @xmath27 which give the optimal performance for retrieval at a fixed temperature @xmath50 , storage level @xmath3 and threshold parameter @xmath114 , are given by the adaptation principle .",
    "the optimal adaptation consists in a search in the space of interactions @xmath41 simultaneously with a search in the space of state configurations @xmath111 .",
    "the best adapted performance of the network is attained by adjusting the training noise and activity to the same level as the retrieval noise and activity . for the parallel dynamics in the extremely diluted network we are dealing with , the stable fixed point of the set of equations",
    "@xmath115 and @xmath116 gives at the same time the optimal training condition and the optimized performance .",
    "the stable fixed point for each value of the synaptic noise parameter @xmath50 , the storage ratio @xmath3 _ and _ the threshold parameter @xmath114 is a _ retriever _ , for which the network has a unique interaction configuration . in other words , in distinction to the usual phase diagrams for retrieval , every point of the phase diagrams that will be discussed next represents a different network .",
    "we present next the results for the optimally adapted retrievers .",
    "the rich structure of locally stable states and the corresponding phase diagrams for self  adaptation that arise as the threshold parameter @xmath114 is increased will be discussed now , separately for @xmath1 and @xmath2 .      to illustrate the role of the threshold parameter @xmath114 , we discuss first the fixed ",
    "point solutions for @xmath117 and @xmath118 and the corresponding phase diagram for @xmath3 vs. @xmath114 , in the absence of retrieval noise shown in figure  2 . for fixed @xmath114 within the range",
    "@xmath119 and @xmath120 , there is a perfect retriever with @xmath121 which is the only stable fixed point , and a solution with @xmath122 and either @xmath123 or @xmath124 , which is an unstable fixed point .",
    "this suggests that one can conceive a network capable of perfect retrieval operating with a limited threshold , as long as the training is with infinitesimal noise @xmath125 and almost full activity @xmath126 .",
    "the corresponding retriever is that of the msn .",
    "the line @xmath127 deserves further attention .",
    "it is the upper bound of the region where the perfect retriever is the only attractor in the retriever dynamics with a wide basin of attraction for self  adaptation . beyond that line ,",
    "the basin of attraction of this retriever is greatly reduced in the three  state network , as will be seen next .",
    "thus , for increasing @xmath114 , in the small @xmath114 regime , there is an enhancement of the associativity of the network , as long as @xmath128 is an increasing function of @xmath114 .",
    "a new pair of stable and unstable fixed points appears discontinuously at @xmath127 .",
    "the stable fixed point represents a new retriever of weaker attractor overlap and reduced activity .",
    "note , however , that for low to moderate @xmath114 ( illustrated in the inset by @xmath129 ) , there is a considerably enhanced retrieval overlap when compared with the overlap for the optimally adapted network of binary units @xcite .",
    "the second retriever has a rather wide basin of attraction for this larger retriever overlap .",
    "this higher performance can be attained through training with low  noise patterns with moderately high activity . for the threshold @xmath130 that maximizes @xmath127 ,",
    "the improvement in storage capacity with the _ same _ retrieval overlap as that of the network of binary neurons is about @xmath131 .",
    "however , as one would expect , the performance deteriorates with a further increase in the threshold @xmath114 .",
    "the second stable fixed point means that there exists a second training condition , with higher noise , which results in a network with lower , but still optimal performance when compared with other three  state networks in its vicinity of the space of interactions , for this training condition .",
    "the unstable fixed  points are repelors of the self  adaptation dynamics @xcite .",
    "the overlap of this second retriever vanishes continuously as @xmath3 increases approaching @xmath132 . for @xmath133 , the perfect retriever and a non  retriever with @xmath122 , and either a finite or no activity , are the only stable fixed  point solutions .",
    "the non  retriever state with @xmath123 appears as a self  sustained activity phase , which has been discussed first for a diluted network with a hebbian learning rule @xcite .",
    "when the activity is zero the network stops operating .",
    "the presence of a non  retriever with finite activity follows from the fixed ",
    "point solution for @xmath134 when @xmath122 is a stable fixed ",
    "the expression for @xmath135 becomes then independent of the local field @xmath136 and , hence , of @xmath82 and @xmath3 .",
    "the fixed  point values for @xmath118 are then given by the solutions of the equation @xmath137 .",
    "the solution @xmath124 is stable for all @xmath114 , when @xmath138 .",
    "there is a second stable fixed point that decreases monotonically from @xmath139 , at @xmath140 , and disappears discontinuously at @xmath141 when the value @xmath142 is reached .",
    "this is the origin of the `` tricritical '' point in the phase diagram for @xmath3 vs. @xmath114 , where the line of continuous transitions for the overlap becomes discontinuous .",
    "we come back to this point below .",
    "it is important to remark that the term `` transition '' here only means that the network changes from one retriever state to another one .",
    "we remind that it is not meant as an usual thermodynamic phase transition , since each point of the phase diagram corresponds to a different network .    finally , when @xmath3 reaches the critical storage capacity @xmath143 , given by @xmath144 the perfect retriever is destabilized .",
    "consider next the case where @xmath145 .",
    "for @xmath146 , there is again a perfect retriever which is a stable fixed  point solution .",
    "in addition , a pair of stable and unstable fixed points appears .",
    "the stable fixed point is a non  retriever with @xmath122 and either @xmath123 or @xmath124 .",
    "a new pair of stable and unstable fixed points appears discontinuously at @xmath127 .",
    "the stable fixed point is , again , a retriever of weaker attractor overlap and reduced activity .",
    "however , as @xmath3 approaches @xmath132 , this second retriever vanishes _ discontinuously _ and , thus , there is a changeover from the line of continuous transitions @xmath132 when @xmath114 increases and reaches a tricritical point at @xmath141 .",
    "when @xmath3 increases beyond @xmath132 , the perfect retriever and the non  retriever are , again , stable fixed ",
    "point solutions , and the perfect retriever , which has a narrow basin of attraction , is destabilized when the critical @xmath143 is reached .",
    "when @xmath114 is increased , the retriever of weaker attractor overlap disappears at @xmath147 , and beyond this point the perfect retriever is the only stable fixed point with finite overlap for @xmath148 .",
    "now we discuss the stability of the replica symmetric solution .",
    "first , the strong retriever state is always stable to replica  symmetry  breaking fluctuations below @xmath149 .",
    "thus , at most the weak retriever can become unstable . in view of this , we mapped out the region of the phase diagram where the stability condition , eq .",
    "( [ 2.39 ] ) , is not satisfied for the weak retriever state , and this is shown as the shaded area in figure  2 , the dash  dotted line being the at line .",
    "furthermore , we found that this line corresponds to the appearing of a gap in the distribution of local fields .",
    "the phase diagram also yields the optimal basin boundary of the self  adaptation dynamics for a given @xmath3 .",
    "thus , as @xmath3 increases for @xmath150 the strong retriever is a `` wide '' retriever for @xmath151 , since it is the only attractor in the self  adaptation dynamics . for @xmath152 ,",
    "the strong retriever becomes a `` narrow '' retriever which coexists with the weak retriever .",
    "finally , for @xmath153 , the strong retriever is a narrow retriever that coexists with the non  retriever state for all @xmath154 .",
    "we consider next the results in the presence of retrieval noise @xmath50 . in the case of a small to moderate threshold where the strong and weak retriever coexist ,",
    "say , for @xmath129 , the phase diagram for @xmath50 vs. @xmath3 is not very different from the phase diagram for the network of binary units .",
    "the strong and the weak retriever coexist now over a wider range of @xmath3 but the strong retriever disappears , as one would expect , for a lower @xmath50 .",
    "more interesting are the results for the phase diagram and the underlying fixed  point solutions for the overlap and the activity when @xmath155 , shown in figure  3 .",
    "this threshold is typical of an optimally adapted network that has a perfect retriever as the only stable fixed point with non ",
    "zero overlap at @xmath138 . for fixed and low @xmath156",
    ", there is a strong retriever with rapidly decreasing @xmath117 and @xmath118 when @xmath3 comes close to the line @xmath157 , where both parameters vanish discontinuously .",
    "there is a second stable fixed point with @xmath122 and @xmath158 , for all @xmath159 , and this non  retriever is the only stable solution for @xmath160 .",
    "there is also an unstable fixed point for @xmath117 and @xmath118 throughout the range @xmath161 that separates the basin of attraction for self  adaptation of the two stable fixed points , and indicates that the strong retriever is a narrow retriever in this interval .",
    "an increase in retrieval noise can be of use for the enhancement of the performance of the single , strong retriever , with a moderately large threshold , as in the present case of @xmath155 .",
    "indeed , for @xmath162 , the non  retriever becomes an _",
    "un_stable fixed point for @xmath3 below the line @xmath163 , leaving the strong retriever as a wide retriever .",
    "the overlap and the activity change discontinuously as @xmath3 goes through @xmath163 . for @xmath164 ,",
    "the overlap of the wide retriever vanishes continuously as @xmath3 approaches @xmath157 .",
    "the results shown here confirm the general expectation that one can not attain the best retriever overlap ( as we have here for the narrow retriever ) together with the best associativity , as for the wide retriever , in the same network except at the phase boundary .",
    "the at line coinciding with the locus where the gap closes down is also shown in figure  3 , and the region to the right of the line up to the @xmath149 line is stable to replica  symmetry  breaking fluctuations .",
    "thus , it seems that the part of the discontinuous transition line @xmath157 that is close to the tricritical point where the changeover to the line of continuous transitions takes place , is marginally stable .",
    "we also argue that for low @xmath50 the line @xmath157 may be almost correct , since @xmath165 is the critical capacity of the msn , which corresponds to a stable point . note that the line @xmath157 of discontinuous transitions has an upper part of infinite slope which should also be correct since one would not expect a reentrant behavior for @xmath157 .",
    "finally , for comparision , we also show the phase boundaries for the msn and conclude that the optimally adapted network with three  state neurons has an improved performance in the presence of retrieval noise .",
    "to see now the effects of the threshold in the optimally adapted four  state network , we present first the results for the fixed ",
    "point solutions for the overlap and the activity in figure  4 .",
    "depending on the value of @xmath114 there may be a domain in the values of @xmath3 in which there are up to three stable fixed ",
    "point solutions with non  zero @xmath117 , one for a perfect retriever and the other ones for weaker retrievers .",
    "the perfect retriever exists up to a critical @xmath143 , given by @xmath166 it turns out that there is a load @xmath127 for all @xmath114 , where a weak retriever , which may or may not be the only one , appears discontinuously as @xmath3 attains that point . for @xmath167 , it is the only weak retriever , as can be seen in the phase diagram for @xmath3 vs. @xmath114 shown in figure  5 . note that , also for the four state network , @xmath128 increases with @xmath114 in the small @xmath114 regime with a considerable enhancement of the strong retriever as a wide retriever .",
    "the perfect and the weak retriever coexist with increasing @xmath3 until either the weak retriever disappears continuously at @xmath132 , which is the case for @xmath168 , or the strong retriever ends at @xmath143 for @xmath169 . in the latter case ,",
    "the weak retriever of non  zero overlap remains as the only attractor of self  adaptation up to @xmath170 .",
    "on the other hand , for @xmath114 well above @xmath171 , a second weak retriever ( @xmath172 ) appears discontinuously as @xmath3 attains the line @xmath173 while the first weak retriever ( @xmath174 ) extends up to a quite higher load @xmath175 , where the state of the network changes discontinuously to the non  retriever state .",
    "the overlap of the @xmath172 vanishes continuously as @xmath3 approaches @xmath132 .",
    "the two weak retrievers coexist for @xmath176 .",
    "note that both the line where the first weak retriever disappears and the domain of @xmath3 where the second weak retriever exists may lie well above the critical capacity @xmath143 for the existence of the perfect retriever .",
    "the situation can become more involved for intermediate values of @xmath114 , shown by the inset in figure  5 . around the endpoint c of the wedge of discontinuous transitions lines",
    ", the second weak retriever can be reached continuously from the first one .",
    "it is interesting to note that , for large @xmath114 , the @xmath174 state has an asymptotic overlap and activity @xmath177 and @xmath178 , respectively .",
    "these correspond to the storage of binary patterns in a network with only the microscopic states @xmath179 being activated .",
    "these are , practically , the only states favoured in the high@xmath114 regime , since the states @xmath180 can only become active by means of high local fields which are extremely unlikely in the absence of retrieval noise .",
    "indeed , we found that the line @xmath175 goes to the critical value @xmath181 for the optimal network of binary units with increasingly large @xmath114 .",
    "thus , as expected , the behavior of the network in the large@xmath114 limit should become that of the msn with reduced overlap and activity .",
    "the phase diagram in figure  5 also provides the optimal basin boundary of attraction , for a given @xmath3 and @xmath114 . for @xmath155 , say , the strong retriever is a wide retriever for @xmath151 , and a narrow retriever when @xmath182 . on the other hand , in the interval @xmath183 ,",
    "the weak attractor with higher overlap is a wide retriever , since it is the only attractor for the self  adapting dynamics in this interval . in distinction , in the interval @xmath184 that weak retriever is a narrow retriever , that coexists with @xmath185 if @xmath186 and with the non  retriever state otherwise .    to discuss the validity of the replica symmetric results note that , whenever two weak retrievers coexist in the phase diagram , each one has to be analyzed separately since they refer to different levels of training noise , such that one may correspond to a gapless local field distribution and the other may not",
    "the at line is the dash  dotted line shown in figure  5 , that starts on the boundary @xmath127 where the single weak retriever appears for small @xmath114 and it merges with @xmath173 around @xmath187 . that retriever is stable to replica  symmetry  breaking fluctuations above the at line .",
    "the @xmath172 is unstable around c and is stable in the strip @xmath176 , whereas the @xmath174 is unstable everywhere below and at the boundary @xmath175 .",
    "the left part of the boundary @xmath127 is marginally stable , as well as the boundary @xmath143 for the perfect retriever .",
    "the principle of adaptation , formulated earlier for a network of binary neurons , has been extended in this work to study the training and performance of optimally adapted attractor neural networks of multi  state neurons trained with noisy inputs in the presence of a noisy retrieval environment .",
    "explicit results where obtained for the optimal attractor overlap and the optimal dynamical activity as functions of the retrieval noise @xmath50 , the load @xmath3 and the threshold @xmath114 , for a network with dilute connectivity .",
    "the maximum storage capacity was also obtained as a function of @xmath114 and @xmath50 and explicit retriever phase diagrams of performance and associativity of the retrievers are exhibited for a network of three or four  state neurons .",
    "these are phase diagrams for _ self  adaptation _ , in distinction to phase diagrams for attraction , as pointed out in ref .",
    "we remind the reader that , as pointed out by wong and sherrington , coexisting retrievers are solutions for different networks , which should correspond to distinct synaptic interactions .",
    "an important issue of this work concerns the improvement in the associativity of multi  state networks , when the width @xmath114 of the intermediate states increases , in the small @xmath114 regime .",
    "the enhanced performance of the second retrievers has also been emphasized .",
    "this is important because they are optimal _ retriever _ solutions on their own , rather than weaker _ retrieval _",
    "solutions for the optimal network configuration , if such solutions exist @xcite .",
    "we have shown that an improvement of the performance of the second retriever in the optimally adapted network with multi  state units can be attained with relatively small training noise and large  activity input patterns . in practical terms , this may be a more accessible situation than training with an infinitesimal amount of noise and almost full activity .",
    "furthermore , we have shown that the storage capacity of the second retriever is a non  monotonic function of the threshold @xmath114 with an increasing capacity for small @xmath114 . with a moderately large threshold , as in the case of @xmath155 for the three  state network , an increase in retrieval noise @xmath50",
    "may help to enlarge the basin of attraction of the single , strong retriever .",
    "this can be understood noting that the increase in the noise should aid to overcome the large gap in the local field in firing the units when the network has been trained with a moderate training noise .",
    "these are important results in the search for improvement of the behaviour of attractor neural networks .",
    "the work presented here is restricted , for simplicity , to binary encoded patterns . on the basis of results we obtained for three or four  state patterns",
    ", we argue that this should not be a serious restriction .",
    "what is important is that the states of the noisy training set @xmath22 have the same degrees of freedom as the arbitrary input set @xmath111 for retrieval .",
    "this requires the introduction of a training activity @xmath27 in the noisy inputs , in order to optimize both the training and the adaptation process in the @xmath0state network .",
    "we have found , in accordance with earlier works , that networks are specialized @xcite .",
    "indeed , one can not attain the best storage capacity for all @xmath50 and @xmath114 in a single network .",
    "even if @xmath114 is fixed the storage capacity of the strong retriever will be that of the msn only at very low @xmath50 and it will become that of the hopfield model at high @xmath50 .",
    "all the results were obtained with the assumption of replica symmetry in the space of synaptic interactions and the limit of validity of this assumption has been established finding the de  almeida  thouless lines @xmath188 at @xmath138 and @xmath110 for a given @xmath114 .",
    "these lines coincide with the band  merging lines for the distribution of the local field . due to the presence of optimal solutions for small  to ",
    "moderate training noise , there are gaps in the distribution of the local fields over sizeable domains of the phase diagram which are not stable to replica  symmetry  breaking fluctuations . nevertheless , interesting phase boundaries and domains of the phase diagrams are stable or , at worst , marginally stable , confirming the validity of our results . indeed , the enhancement of the line @xmath127 , where the second retriever appears for small training noise and large activity , both for @xmath1 and @xmath2 , lies on the replica symmetric side of the at line .",
    "furthermore , the interesting weak retriever lies completely on this side .",
    "that is also the case for the tricritical point and the first ",
    "order transition line , @xmath132 , for the three  state network , which at worst becomes marginally stable .",
    "furthermore , the phase diagram for @xmath189 reveals that the line @xmath190 of continuous transitions is stable to replica  symmetry  breaking fluctuations , for both @xmath1 and @xmath2 and all @xmath114 . in view of these results",
    ", it does not seem worthwhile to pursue a calculation beyond the replica  symmetry ansatz .    a closer look at our results reveals that although the critical capacity @xmath149 , where the strong retriever terminates , decreases faster with increasing @xmath114 for the four  state than for the three  state network , the trend is opposite for the lower and upper critical storage ratio @xmath128 and @xmath190 respectively , for the presence of a second retriever in the low@xmath114 regime .",
    "this suggests that the role of the threshold could become even more important in optimally adapted higher q  state networks .",
    "the extended principle of adaptation of the present work assumes that both , the training overlap and the training activity become continuously adapted to the noisy retrieval environment . in particular",
    ", the training activity follows the changes in the dynamical activity characteristic of the @xmath0 states of the units , and this makes difficult the study of the optimally adapted network for general @xmath0 . it may be possible to study a weaker version of the extended adaptation principle for the graded response network in which the training activity remains fixed",
    "this , and other questions , will be considered in future work .",
    "we thank j. f. fontanari and d. boll for critical comments , and one of us ( wkt ) thanks the kind hospitality of the institute for theoretical physics of the catholic university of leuven , belgium , where part of the work was written .",
    "the research of one of us ( wkt ) was supported by cnpq ( conselho nacional de desenvolvimento cientfico e tecnolgico , brazil ) , and the work was supported in part by finep ( financiadora de estudos e projetos , brazil ) . a grant from cnpq on a neural network project is gratefully acknowledged .",
    "99 j. j. hopfield , _ proc .",
    "usa _ * 79 * , 2544 ( 1982 ) ; j. a. hertz , a. krogh and r. g. palmer , _ introduction to the theory of neural computation _",
    "( addison  wesley , redwood city , ca , 1991 ) . k. y. m. wong and d. sherrington , _ j. phys .",
    "* 23 * , 4659 ( 1990 ) . t. b. kepler and l. f. abbott , _ j. physique _ * 49 * , 1657 ( 1988 ) .",
    "w. krauth , j.p .",
    "nadal and m. mzard , _ j. phys .",
    "* 21 * , 2995 ( 1988 ) .",
    "e. gardner , _ j. phys .",
    "a _ * 22 * , 1969 ( 1989 ) .",
    "d. j. amit , m. evans , h. horner and k. y. m. wong , _ j. phys .",
    "* 23 * , 3361 ( 1990 ) .",
    "e. gardner , n. stroud and d. wallace , _ j. phys .",
    "* 22 * , 2019 ( 1989 ) .",
    "k. y. m. wong and d. sherrington , _",
    "j. phys . a _",
    "* 23 * , l175 ( 1990 ) .",
    "g. gyrgyi and n. tishby , _ workshop on neural networks and spin glasses _",
    ", w. k. theumann and r. koeberle ( eds . ) ( world scientific , singapore , 1990 ) .",
    "g. gyrgyi , _ phys .",
    "rev . a _ * 41 * , 7079 ( 1990 ) . t. l. h. watkin , a. rau , d. boll and j. van mourik , _ j. phys .",
    "i france _ * 2 * , 167 ( 1992 ) . c. meunier , d. hansel and a. verga , _",
    ". phys _ * 55 * , 859 .",
    "j. s. yedidia , _ j. phys .",
    "* 22 * , 2265 ( 1989 ) .",
    "d. r. c. dominguez and w. k. theumann , _ j. phys . a _ * 29 * , 749 ( 1996 ) ; _ j. phys",
    "* 30 * , 1403 ( 1997 ) .",
    "h. rieger , _",
    "a _ * 23 * , l1273 ( 1990 ) .",
    "m. bouten and a. engel , _ phys .",
    "e _ * 47 * , 1397 ( 1993 ) .",
    "d. boll , g. m. shim and v. a. zagrebnov , _ j. stat phys . _ * 74 * , 565 ( 1994 ) .",
    "d. boll , h. rieger and g. m. shim , _ j. phys .",
    "* 27 * , 3411 ( 1994 ) .",
    "k. y. m. wong and d. sherrington , _ phys .",
    "e _ * 47 * , 4465 ( 1993 ) .",
    "j. r. l. de almeida and d. thouless , _ j. phys .",
    "a _ * 11 * , 983 ( 1978 ) .",
    "e. gardner , _",
    "a _ * 21 * , 257 ( 1988 ) .",
    "e. gardner and b. derrida , _ j. phys .",
    "* 21 * , 271 ( 1988 ) .",
    "m. bouten , _ j. phys",
    "* 27 * , 6021 ( 1994 ) .",
    "* figure 1 : * the non  decreasing step function @xmath191 for @xmath1 ( a ) and @xmath2 ( b ) . + * figure 2 : * phase diagram for the load @xmath3 as a function of the threshold @xmath114 for @xmath1 , at @xmath138 , and the corresponding optimal overlap @xmath117 ( solid lines ) and activity @xmath118 ( dashed lines ) for @xmath129 ( right ) , @xmath192 ( center ) and @xmath193 ( left ) , in the inset .",
    "unstable fixed ",
    "point solutions are shown in light lines .",
    "sr and wr are strong and weak retrievers , respectively .",
    "the sr is a wide retriever at the left of the light dotted line and below @xmath127 .",
    "solid lines in the phase diagram indicate discontinuous transitions and a dashed line a continuous transition .",
    "the dash  dotted line is the de  almeida  thouless line ( cf .",
    "the text ) .",
    "the wr is unstable to replica  symmetry  breaking in the shaded area . + * figure 3 : * phase diagram , for @xmath50 vs. @xmath3 , for @xmath1 and @xmath155 . in the inset",
    "are shown the optimal overlap ( solid lines ) and activity ( dashed lines ) for @xmath138 , @xmath194 and @xmath195 ; the unstable solutions for @xmath117 and @xmath118 are in light lines . in @xmath196",
    "the strong retriever is a narrow ( wide ) retriever .",
    "nr is the non  retriever phase .",
    "the dash  dotted line is the de  almeida  thouless line . + * figure 4 : * optimal overlap ( solid lines ) and activity ( dashed lines ) for @xmath2 , at @xmath138 , for @xmath197 and @xmath187 . the various @xmath3 indicate the loads for which the optimal solutions appear or disappear , for @xmath187 , and wr , @xmath174 and @xmath172 are weak retrievers .",
    "+ * figure 5 : * phase diagram for @xmath3 as a function of @xmath114 , for @xmath2 at @xmath138 , described in the text .",
    "the amplified central part is shown separately .",
    "the retrievers and the nature ( continuous or discontinuous ) of the phase boundaries are as in previous figures .",
    "the sr , @xmath174 and @xmath172 coexist in the shaded area of the inset .",
    "the de  almeida  thouless line is the dash  dotted line ."
  ],
  "abstract_text": [
    "<S> the principle of adaptation in a noisy retrieval environment is extended here to a diluted attractor neural network of @xmath0-state neurons trained with noisy data . </S>",
    "<S> the network is adapted to an appropriate noisy training overlap and training activity which are determined self - consistently by the optimized retrieval attractor overlap and activity . the optimized storage capacity and the corresponding _ retriever _ overlap are considerably enhanced by an adequate threshold in the states . </S>",
    "<S> explicit results for improved optimal performance and new retriever phase diagrams are obtained for @xmath1 and @xmath2 , with coexisting phases over a wide range of thresholds . </S>",
    "<S> most of the interesting results are stable to replica - symmetry - breaking fluctuations . </S>",
    "<S> +    pacs numbers : 87.10.+e ; 64.60.cn </S>"
  ]
}