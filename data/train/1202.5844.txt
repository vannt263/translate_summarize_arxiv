{
  "article_text": [
    "in recent years , the problem of low - rank matrix factorization ( lrmf ) has attracted much attention due to its wide range of applications in computer vision and pattern recognition , such as structure from motion @xcite , face recognition @xcite , shape from varying illumination @xcite , and object tracking @xcite . representing the measurements or the observation data as a @xmath1 matrix @xmath2 ,",
    "whose columns @xmath3s correspond to the @xmath4-dimensional input measurements and @xmath5 is the number of input items , the aim of the lrmf can be mathematically described as solving the following optimization problem : @xmath6 where @xmath7 , @xmath8 and @xmath9 . to deal with the real lrmf problems in the presence of missing data , the optimization ( [ e1 ] ) is also reformulated as @xmath10 where @xmath11 denotes the component - wise multiplication ( i.e. , the hadamard product ) , and the element @xmath12 of the denotation matrix @xmath13 is @xmath14 if the corresponding element of @xmath15 is known , and @xmath16 otherwise @xcite . here",
    "@xmath17 is some form of the matrix norm .",
    "the global minimum of the optimization problem ( [ e1 ] ) with @xmath18 matrix norm ( i.e. , the frobenius norm ) can easily be solved by the well known singular value decomposition ( svd , @xcite ) method . to handle missing data , some methods , such as the wiberg algorithm @xcite and the weighted low - rank approximation method ( wlra , @xcite ) ,",
    "have further been proposed to solve the optimization ( [ e2 ] ) with @xmath18 matrix norm . the performance of these techniques , however , is sensitive to the presence of outliers or noises , which often happen in real measurements , because the influence of outliers or noises with a large norm tends to be considerably exaggerated by the use of the @xmath18 norm @xcite . to alleviate this robustness problem ,",
    "the often used approach is to replace the @xmath18 matrix norm with the @xmath0 norm in the objective functions of ( [ e1 ] ) and ( [ e2 ] ) @xcite .",
    "the models are then expressed in the following forms : @xmath19 and @xmath20 unfortunately , it turns out that replacing the @xmath18 norm with @xmath0 norm in the optimizations makes the problem significantly more difficult @xcite .",
    "first , both ( [ e11 ] ) and ( [ e12 ] ) are non - convex problems , so their global optimality are in general difficult to obtain .",
    "second , both optimizations are non - smooth problems conducted by the @xmath0 matrix norm , so it is hard to attain an easy closed - form iteration formula to efficiently approximate their solutions by standard optimization tools .",
    "third , in real applications , both optimizations can also be very computationally demanding problems to solve , which always limit their availability in large - scale practice .    in this paper , by employing an important algorithm design paradigm ,",
    "namely divide and conquer , we formulate efficient algorithms against the optimization models ( [ e11 ] ) and ( [ e12 ] ) , respectively . the core idea underlying",
    "the new algorithms is to break the original optimizations into a series of smallest possible problems and recursively solve them .",
    "each of these small problems is convex and has closed - form solution , which enables the new algorithms to avoid using a time - consuming numerical optimization as an inner loop .",
    "the proposed algorithms are thus easy to implement .",
    "especially , it is theoretically evaluated that the computational speeds of the proposed algorithms are approximately linear in both data size and dimensionality , which allows them to handle large - scale @xmath0 norm matrix factorization problems . the efficiency and robustness of the proposed algorithms",
    "have also been substantiated by a series of experiments implemented on synthetic and real data .    throughout the paper ,",
    "we denote matrices , vectors , and scalars by the upper - case letters , lower case bold - faced letters , and lower - case non - bold - faced letters , respectively .",
    "various approaches have recently been proposed to deal with the optimizations ( [ e11 ] ) and ( [ e12 ] ) to achieve robust low - rank matrix factorization results . for the @xmath0 norm model ( [ e11 ] ) ,",
    "the iteratively re - weighted least - squares approach introduced by torre and black is one of the first attempts @xcite .",
    "its main idea is to iteratively assign a weight to each element in the measurements .",
    "the method , however , is generally very sensitive to initialization ( @xcite ) . instead of the @xmath0 matrix norm , ding et al . utilized the rotational invariant @xmath21 norm , as defined by @xmath22 for the objective function of ( [ e11 ] ) ( icml06 , @xcite ) . like the @xmath0 norm ,",
    "the @xmath21 norm so defined is also capable of softening the contributions from outliers . by substituting the maximization of the @xmath0 dispersion of data , @xmath23 , for the minimization of the original @xmath0 objective , @xmath24 , kwak presented another approach for the problem ( pami08 , @xcite ) .",
    "the method is also able to suppress the negative effects of outliers to a certain extent .",
    "the most predominance of this method is its fast computational speed , which is linear in both measurement size and dimensionality .",
    "two methods represent the current state of the art of solving the model ( [ e12 ] ) .",
    "the first method is presented by ke and kanade , who formulated the robust @xmath0 norm matrix factorization objective as alternative convex programs ( cvpr05 , @xcite ) .",
    "the programs can then be efficiently solved by linear or quadratic programming .",
    "the second method is designed by eriksson and hengel , which represents a generalization of the traditional wiberg algorithm ( cvpr10 , @xcite ) .",
    "the method has been empirically proved to perform well on some synthetic and real world problems , such as the structure from motion ( sfm ) applications .",
    "it should be noted that both methods can also be employed to solve ( [ e11 ] ) by setting all elements of the missing data denotation matrix @xmath25 to be @xmath14s .",
    "unlike the previous methods for robust matrix factorization , the proposed divide - and - conquer ( d&c in brief ) method chooses to solve the smallest possible sub - problems of ( [ e11 ] ) and ( [ e12 ] ) at every step ( involving only one scalar parameter of @xmath26 or @xmath27 ) .",
    "the advantage of the new method lies in the fact that each small sub - problem so attained can be solved analytically .",
    "thus , complicated numerical optimization techniques are entirely avoided , and the overall problem can thus be efficiently solved .",
    "we introduce our method and its theoretical fundament as follows .",
    "we first consider the optimization model ( [ e11 ] ) .",
    "since the @xmath28-rank matrix @xmath29 can be partitioned into the sum of @xmath28 @xmath14-rank matrices , i.e. , @xmath30 , ( [ e11 ] ) can thus be equivalently reformulated as @xmath31 the original @xmath28-rank matrix factorization problem can then be decomposed into a series of recursive @xmath14-rank sub - problems : @xmath32 where @xmath33 .",
    "we can naturally approximate the solution of ( [ e3 ] ) by sequentially solving ( [ e4 ] ) with respect to @xmath34 for @xmath35 , with all other @xmath36s ( @xmath37 ) fixed .    solving ( [ e4 ] ) can further be simplified to alteratively optimizing @xmath38 or @xmath39 while letting the other fixed .",
    "since @xmath38 and @xmath39 can be solved in a completely symmetrical way ( in the sense that @xmath40 ) , we only need to consider how to efficiently solve @xmath41    by reformulating ( [ e5 ] ) to its decoupling form : @xmath42 where @xmath43 is the @xmath44-th element of the vector @xmath39 , the problem can then be further divided into @xmath5 small sub - optimizations with the following expression ( for @xmath45 ) : @xmath46    from ( [ e3 ] ) to ( [ e7 ] ) , we have broken the original large optimization ( [ e11 ] ) , with respect to @xmath26 and @xmath27 , into a series of smallest possible optimization problems , each with respect to only one scalar parameter of @xmath26 or @xmath27 . by utilizing the similar strategy , it is also easy to decompose the large optimization ( [ e12 ] ) into a series of small optimizations , expressed as : @xmath47 where @xmath48 is the @xmath44-th column of the denotation matrix @xmath25 .",
    "it is very fortunate that both small optimizations ( [ e7 ] ) and ( [ e7s ] ) are not only convex , but also have closed form solutions .",
    "this implies that it is possible to construct fast algorithms for ( [ e11 ] ) and ( [ e12 ] ) , as introduced in the following discussion .",
    "we first formalize ( [ e7 ] ) as : @xmath49where both @xmath50 and @xmath51 are @xmath4-dimensional vectors , and denote their @xmath52-th elements as @xmath53 and @xmath54 , respectively .",
    "the following theorem shows the convexity of this optimization problem ( the proofs of all involved theorems are moved to the supplementary material due to the page limitation ) .    @xmath55 as defined in ( [ e75 ] ) is a convex function with respect to @xmath56 .",
    "theorem 1 implies that it is hopeful to find the global optimum of ( [ e75 ] ) .",
    "we first clarify the case when all elements of @xmath51 are positive in the following lemma .    for ( [ e75 ] )",
    ", assuming each element @xmath54 of @xmath51 is positive ( @xmath57 ) , denote    * the label set @xmath58 : the permutation of @xmath59 based on the ascending order of @xmath60 ; * the sequence @xmath61 : @xmath62 @xmath63 @xmath64 ; * the label @xmath65 : the label of the first non - negative element of @xmath66 ;    and the following closed form expression provides a global optimum of ( [ e75 ] ) : @xmath67    [ cols=\"<\",options=\"header \" , ]     _ structure from motion experiments e9,e10,e11 _ : the structure from motion ( sfm ) problem can be posed as a typical low - rank matrix approximation task @xcite . in this series of experiments , we employ two well known sfm data sequence , the dinosaur sequence , available at http://www.robots.ox.ac.uk/~vgg/ , and the pingpong ball sequence , available at http://vasc.ri.cmu.edu/idb/ , for substantiation .",
    "the entire dinosaur and pingpong sequence contain projections of @xmath68 and @xmath69 points tracked over @xmath70 and @xmath71 frames , respectively , composing @xmath72 and @xmath73 sfm matrices correspondingly .",
    "each matrix contains more than @xmath74 missing data due to occlusions or tracking failures . as considering robust approximation in this work ,",
    "we further include outliers uniformly generated from @xmath75 $ ] in @xmath76 components of two matrices to form the input data of the experiments e10 and e11 , respectively$ ] . ] .",
    "since some other robust matrix factorization methods can not be made available at such data scales ( see table [ t2 ] ) , we further picked up @xmath77 points from the dinosaur sequence to form a smaller @xmath78 matrix , and also added @xmath76 outliers to it to compose the input measurements of the experiment e9 .    as the experiments e1-e6 , the original un - corrupted matrix , denoted as @xmath15 , is saved as ground truth in each experiment for comparison purpose .",
    "four current low - rank matrix factorization methods were employed for comparison .",
    "they include the wlra @xcite and wiberg methods @xcite , which are typical methods designed for the @xmath18 norm model ( [ e2 ] ) , the cvpr05 @xcite and cvpr10 @xcite methods , which are current state - of - the - art methods for solving the @xmath0 norm model ( [ e12 ] ) .",
    "all of the utilized methods adopted similar initialization for each of the involved experiments .",
    "the performances of these methods are compared in table [ t2 ] ( that of e7 is depicted as the average result over @xmath79 experiments ) .",
    "the advantage of the proposed d&c method is evident based on table [ t2 ] , in terms of both computational speed and accuracy . on one hand , our algorithm always attains the most accurate reconstruction of the original data matrix by the product of the obtained low - rank matrices @xmath80 and @xmath81 , and on the other hand , the computation cost of the proposed algorithm is the smallest of all employed methods in most experiments ( except being the second smallest in e9 ) .",
    "it is very impressive that the computational speed of the proposed algorithm is even faster than the wlra and the wiberg methods , which are constructed for @xmath18 norm matrix factorization model , in most cases ( except slower than wlra in e9 ) .",
    "considering the difficulty of solving the @xmath0 model due to its non - convexity and non - smoothness , the efficiency of the proposed method is more prominent .",
    "in this paper we have tried a new methodology , the divide and conquer technique , for solving the @xmath0 norm low - rank matrix factorization problems ( [ e11 ] ) and ( [ e12 ] ) .",
    "the main idea is to break the original large problems into smallest possible sub - problems , each involving only one unique scalar parameter .",
    "we have proved that these sub - problems are convex , and have closed form solutions .",
    "inspired by this theoretical result , fast algorithms have been constructed to handle the original large problems , entirely avoiding the complicated numerical optimization for the inner loops of the iteration . in specific , we have proved that the computational complexity of the new algorithms is approximately linear in both the size and dimensionality of the input data , which enables the possible utilization of the new algorithms in large - scale @xmath0 norm matrix factorization problems .",
    "the convergence of the new algorithms have also been theoretically validated .",
    "based on the experimental results on a series of synthetic and real data sets , it has been substantiated that the proposed algorithms attain very robust performance on data with outliers and missing components . as compared with the current state - of - the - art methods , our algorithms exhibit notable advantages in both computational speed and accuracy .",
    "the experimental results also illuminate the potential usefulness of the proposed algorithms on large - scale face recognition and sfm applications ."
  ],
  "abstract_text": [
    "<S> the low - rank matrix factorization as a @xmath0 norm minimization problem has recently attracted much attention due to its intrinsic robustness to the presence of outliers and missing data . in this paper </S>",
    "<S> , we propose a new method , called the divide - and - conquer method , for solving this problem . </S>",
    "<S> the main idea is to break the original problem into a series of smallest possible sub - problems , each involving only unique scalar parameter . </S>",
    "<S> each of these sub - problems is proved to be convex and has closed - form solution . by recursively optimizing these small problems in an analytical way , efficient algorithm , entirely avoiding the time - consuming numerical optimization as an inner loop , for solving the original problem </S>",
    "<S> can naturally be constructed . </S>",
    "<S> the computational complexity of the proposed algorithm is approximately linear in both data size and dimensionality , making it possible to handle large - scale @xmath0 norm matrix factorization problems . </S>",
    "<S> the algorithm is also theoretically proved to be convergent . </S>",
    "<S> based on a series of experiment results , it is substantiated that our method always achieves better results than the current state - of - the - art methods on @xmath0 matrix factorization calculation in both computational time and accuracy , especially on large - scale applications such as face recognition and structure from motion .    </S>",
    "<S> low - rank matrix factorization , robustness , divide - and - conquer , face recognition , structure from motion . </S>"
  ]
}