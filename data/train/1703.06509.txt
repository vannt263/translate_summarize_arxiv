{
  "article_text": [
    "numerical methods for approximating variational problems or partial differential equations ( pdes ) with solutions defined on surfaces or manifolds are of growing interests over the last decades .",
    "finite element methods , as one of the main streams in numerical simulations , are well established for those problems .",
    "a starting point can be traced back to @xcite , which is the first to investigate a finite element method for solving elliptic pdes on surfaces . since then , there have been a lot of extensions both in analysis and in algorithms , see for instance @xcite and the references therein . in the literature , most of the works consider the _ a priori _ error analysis of various surface finite element methods , and only a few works , up to our best knowledge , take into account the _ a posteriori _ error analysis and superconvergence of finite element methods in a surface setting , see @xcite .",
    "recently , there is an approach proposed in @xcite which merges the two types of analysis to develop a higher order finite element method on an approximated surface , where a gradient recovery scheme plays a key role .",
    "gradient recovery techniques , which are important in _ post processing _ solutions or data for improving the accuracy of numerical simulations , have been widely studied and applied in many aspects of numerical analysis . in particular for planar problems ,",
    "the study of gradient recovery methods has reached already a mature stage , and there is a massive of works in the literature , to name but only a few @xcite .",
    "we point out some significant methods among them , like the classical zienkiewicz  zhu ( @xmath0 ) patch recovery method @xcite , and a later method called polynomial preserving recovery ( ppr ) @xcite .",
    "the two approaches work with different philosophies in methodology .",
    "the former method first locates positions of certain points in the given mesh , and then recovers the gradients themselves at those points to achieve a higher order approximation accuracy , while the latter one first recovers the function values by polynomial interpolations in a local patch at each nodal points , and then takes gradients at the nodal points from the previously recovered functions .",
    "both the methods can produce comparable superconvergence results , but do not require the same assumptions on the discretized meshes .",
    "gradient recovery methods for data defined on curved spaces have only recently been investigated . in @xcite ,",
    "several gradient recovery methods have been adapted to a general surface setting for linear finite element solutions which are defined on polyhedrons by triangulation .",
    "there a surface is concerned to be a zero level set of a smooth function defined in a higher dimensional space , which is from the point of view of an ambient space of the surface .",
    "it has been shown that most of the properties of the gradient recovery schemes for planar problems are maintained in their counterparts for surface problems .",
    "in particular , in their implementation and analysis , the methods ask for exact knowledge of the surface , e.g. the nodal points are located on the exact surface , and the tangent spaces or in another word the normal vector field are given . however , this information is usually not available in reality , where we have only the approximations of surfaces , for instance , polyhedrons , splines or polynomial surfaces . on the other hand , the generalized @xmath0 scheme for gradient recovery with surface elements gives the most competitive results in @xcite , including several other methods ,",
    "their superconvergence are proved with the assumption that the local patch is @xmath1symmetric on the discretized surfaces , which is restrictive in applications . in the planar case",
    ", the @xmath1symmetric condition is also asked for the superconvergence by these methods which have been generalized to a surface setting in @xcite , but it is not necessary for the ppr method .",
    "this difference gives us the motivation to generalize the ppr method for problems with data defined on manifolds .",
    "a follow - up question would be what are the polynomials in the domains of curved manifolds .",
    "using the idea from the literature , e.g. @xcite , one could consider polynomials locally on the tangent spaces of the manifolds .",
    "obviously , a direct generalization of ppr to a manifold setting based on tangent spaces will again fall into the awkward situation : the exact manifold and its tangent spaces are unknown .    to overcome these difficulties , we go back to the original definition of a manifold .",
    "we take the manifold as patches locally parametrized by euclidean planar domains , but not necessarily by their tangent spaces .",
    "this has no interruption for us to define patch - wise polynomials in such planar parameter domains . in this manner",
    ", we are able to recover the unknown surfaces from the given sampling points in these local domains , as well as the finite element solutions iso - parametrically .",
    "our proposed method is thus called parametric polynomial preserving recovery ( pppr ) which _ does not _",
    "rely on the @xmath1 symmetric condition for the superconvergence , just like its genetic father ppr . to this end",
    ", it will be revealed that pppr is particularly useful to _ address the issue of unavailable tangent spaces _ , and thus it enables us to solve the open issues in @xcite .",
    "another benefit of the pppr method for data on a surface is that it is relatively _ curvature stable _ in comparing with the methods proposed in @xcite .",
    "this is verified by our numerical examples , but a quantitative analysis will be open in the paper . moreover , the original ppr method @xcite does not preserve the function values at the nodal points in its pre - recovery step . in this paper , we provide an alternative method which can achieve this goal . with this option , the pppr can not only preserve _ parametric polynomial _ , but also preserve the _ surface sampling points _ and the _ function values _ at the given points simultaneously .",
    "that means the given data is invariant during the recovery by using the pppr method .",
    "the rest of the paper is organized as follows : section [ sec : background ] gives a preliminary account on relevant differential geometry concepts and an analytic pde problem .",
    "section [ sec : spaces ] introduces discretized function spaces and collects some geometric notations used in the paper .",
    "section [ sec : pppr ] presents the new algorithms especially the pppr for gradient recovery on manifolds .",
    "there we make remarks on the comparison of algorithms and the idea of preserving function values , and provide an argument for its curvature stable property .",
    "section [ sec : analysis ] gives a brief analysis of the superconvergence properties of the proposed method .",
    "section [ sec : estimator ] analyze the recovery - based _ a posteriori _ estimator using the new gradient recovery operators . finally , we present some numerical results and the comparisons with existing methods in section [ sec : numerics ] .",
    "we have a proof of a basic lemma in appendix [ appendix ] .",
    "we will only show some basic concepts which are relevant to our paper . for a more general overview on the topic of riemannian geometry or differential geometry",
    ", one could refer to for instance @xcite . in the context of the paper",
    ", we shall consider @xmath2 as an oriented , connected , @xmath3 smooth and compact riemannian manifold without boundary , where @xmath4 denotes the riemann metric tensor .",
    "the idea we are going to work should be no restriction for general @xmath5 dimensional manifolds , but we will focus on the case of two dimensional ones , which are also called surfaces , in the later applications and numerical examples .",
    "our concerns are some quantities @xmath6 which are scalar functions defined on manifolds .",
    "first , let us mention the differentiation of a function @xmath7 in a manifold setting , which is called covariant derivatives in general .",
    "it is defined as the directional derivatives of the function @xmath7 along an arbitrarily selected path @xmath8 on the manifold @xmath9 here @xmath10 is a tangential vector field .",
    "the gradient then is an operator such that @xmath11 it is not harm to think of the gradient as a tangent vector field on the manifold @xmath12 . in a local coordinate",
    ", the gradient has the form @xmath13 where @xmath14 is the entries of the inverse of the metric tensor @xmath4 , and @xmath15 denotes the tangential basis .",
    "let @xmath16 be a local geometric mapping , then we can rewrite into a matrix form with this local parametrization , that is @xmath17 in , @xmath18 is the pull back of function @xmath7 to the local planar parameter domain @xmath19 , @xmath20 denotes the gradient on the planar domain @xmath19 , @xmath21 is the jacobian matrix of @xmath22 , and @xmath23 on this patch .",
    "[ rem : surface_gradient ] @xmath22 is not specified here , and we will make it clear when it becomes necessary later .",
    "we actually have a relation that @xmath24 where @xmath25 denotes the moore - penrose inverse of @xmath21 .",
    "see ( * ? ? ?",
    "* appendix ) for a detailed explanation .",
    "note that the parametrization map @xmath22 is not unique , typical ones can be constructed through function graphs which will be used in our later algorithms .",
    "we have the following lemma of which the proof is given in appendix [ appendix ] .",
    "[ lem : invariant ] the gradient is invariant under different chosen of regular isomorphic parametrization functions @xmath22 .    let @xmath26 be the volume form on @xmath12 , and @xmath27 be the tangential bases . for every tangent vector field @xmath28 , @xmath29",
    ", we have a @xmath30 form defined by the interior product of @xmath31 and the volume form @xmath32 through the following way @xmath33 where @xmath34 are @xmath30 indexes with @xmath35 taking out from @xmath36 .",
    "the divergence of the vector field @xmath31 then satisfies @xmath37 where @xmath38 denotes the exterior derivative . since both the left hand side and the right hand side of are @xmath5 forms",
    ", @xmath39 is a scalar field . using the local coordinates ,",
    "we can write the volume form explicitly @xmath40 applying equation , the divergence of the vector field @xmath41 can be computed by @xmath42 it is revealed that the divergence operator is actually the dual of the gradient operator . with the above preparation",
    ", we can now given the definition of the laplace - beltrami operator , which is denoted by @xmath43 in our paper , as the divergence of the gradient , that is @xmath44    we mention that if the manifold @xmath12 is a hyper - surface , that is @xmath45 which has co - dimension @xmath46 .",
    "the gradient and divergence of the function @xmath7 can be equally calculated through projecting the gradient and divergence of an extended function in ambient space @xmath47 to the tangent spaces of @xmath12 respectively .",
    "this type of definitions has been applied in many references which consider problems in an ambient space setting , i.e. @xmath48 where @xmath49 and @xmath50 are the extended scalar and vector fields defined in the ambient space of the hypersurface , which satisfies @xmath51 and @xmath52 for all @xmath53 .",
    "note that @xmath54 is the gradient operator defined in the ambient euclidean space @xmath47 , @xmath55 is the tangential projection operator @xmath56 and @xmath57 is a unit normal vector field of @xmath12 . it can be showed that the gradient and divergence by projections are independent of the way of the extension of the scalar or vector fields , and they are equivalent to the former definitions .    with the generalized notions of the differentiation on manifolds , the function spaces based on manifold domains can be studied analogously to euclidean domains .",
    "sobolev spaces on manifolds @xcite are one of the mostly investigated spaces , which provide a breeding ground to study pdes .",
    "we are interested in numerically approximating pdes of which the solutions are defined on @xmath12 . even though our methods are _ problem independent _ , in this paper , our analysis will be mainly based on the laplace - beltrami operator , and its generated pdes . for the purpose of both analysis and applications",
    ", we consider an exemplary problem @xcite the _ laplace - beltrami equation _ , that is for a given @xmath58 satisfying @xmath59 to solve the equation @xmath60 where @xmath61 denotes the manifold volume measure .",
    "the discretization of a smooth manifold @xmath12 has been widely studied in many settings , especially in terms of surfaces @xcite .",
    "a discretized surface , in most cases , is a piecewise polynomial surface .",
    "one of the most simple case is the polygonal approximation to a given smooth surface , especially with triangulations .",
    "finite element methods for triangulated meshes on surfaces have firstly been studied in @xcite by using linear elements . in @xcite , a generalization of @xcite to high order finite element method",
    "is proposed based on triangulated surfaces . in order to have an optimal convergence rates , it is showed that the geometric approximation error and the function approximation error has to be compatible with each other . in fact",
    ", the balance of geometric approximation errors and function approximation errors is also the key point in the development of our recovery algorithm .    in this paper",
    ", we will denote @xmath62 the triangulated surface , where @xmath63 is the set of triangles , and @xmath64 is the maximum diameter .",
    "we restrict ourselves to the first order finite element methods , thus the nodes consist of simply the vertices of @xmath65 , which we denote by @xmath66 .    in the following , we define transform operators between the function spaces on @xmath12 and function spaces on @xmath67 , where @xmath67 denotes some perturbation of @xmath12 . @xmath68 and its inverse @xmath69 where @xmath70 is a continuous and bijective projection map from @xmath67 to @xmath12 .",
    "we have the following lemma with triangulated approximation @xmath71 .",
    "[ lem : transform ] for @xmath72 @xmath73 , the transform operators @xmath74 are uniformly bounded between the spaces @xmath75 and @xmath76 as long as the space @xmath76 is compatible with the smoothness of @xmath65 .    for every @xmath77 , denote @xmath78 .",
    "each triangular faces @xmath79 of @xmath65 corresponding to a curved triangle faces on @xmath12 , and we denote it as @xmath80 .",
    "if @xmath81 , every function @xmath82 and its derivatives are uniformly bounded on @xmath12 , then we can always find constants @xmath83 and @xmath84 such that @xmath85 for @xmath86 , using the results in @xcite , we have the equivalence of @xmath87 and @xmath88 .",
    "that is there exists positive and uniformly bounded constants @xmath89 and @xmath90 , such that @xmath91 holds on each pair of the triangular faces .",
    "since @xmath92 we have then the estimate @xmath93 which gives the conclusion .",
    "[ rem : transform ] the statement of lemma [ lem : transform ] hold also for higher order continuous piece - wise polynomial approximation of @xmath12 .",
    "we give here an assumption on the triangulations of surfaces , which is a common condition to have the so - called supercloseness .",
    "[ ass : irregular ] @xmath65 is a quasi - uniform and shape regular triangulation of @xmath12 , and it satisfies the @xmath94 irregular condition ( cf .",
    "* definition 2.4 ) , or ( * ? ? ?",
    "* definition 3.2 ) ) .",
    "+ for convenience , table [ tab : geometry ] collects some notations in the paper .",
    ".notations [ cols=\"<,<\",options=\"header \" , ]      in the example , we consider a benchmark problem for adaptive finite element method for laplace - beltrami equation on the sphere @xcite .",
    "we choose the right hand side function @xmath58 such that he exact solution in spherical coordinate is given by @xmath95 in case of @xmath96 , it easy to see that the solution @xmath7 has two singularity points at north and south poles and the solution @xmath7 is barely in @xmath97 .",
    "in fact , @xmath98 .     [ fig : sphere_init ]     [ fig : sphere_adaptive ]     [ fig : sphere_err ]     [ fig : sphere_idx ]    to obtain optimal convergence rate , adaptive finite element method ( afem ) is used .",
    "different from existing methods in the literature , recovery - based _ a posteriori _ error estimator is adopted .",
    "we start with the initial mesh given as in fig [ fig : sphere_init ] . the mesh is adaptively refined using the dorfler @xcite marking strategy with parameter equal to @xmath99 .",
    "fig [ fig : sphere_adaptive ] plots the mesh after the 18 adaptive refinement steps .",
    "clearly , the mesh successfully resolves the singularities .",
    "the numerical errors are displayed in fig [ fig : sphere_err ] . as expected , optimal convergence rate for @xmath100 error can be observed .",
    "in addition , we observe that the recovery is superconvergent to the exact gradient at a rate of @xmath101 .    to test the performance of our new",
    "recovery - based _ a posterior _ error estimator for laplace - beltrami problem , the effectivity index @xmath102 is used to measure the quality of an error estimator @xcite , which is defined by the ratio between the estimated error and the true error @xmath103 the effectivity index is plotted in fig [ fig : sphere_idx ] .",
    "we see that @xmath102 converges asymptotically to @xmath46 which indicates the posteriori error estimator or is asymptotically exact .      in this example , we consider the following laplace - beltrami type equation on dziuk surface as in @xcite : @xmath104 where @xmath105 .",
    "@xmath58 is chosen to fit the exact solution @xmath106 note that the solution has an exponential peak . to track this phenomena ,",
    "we adopt afem with an initial mesh graphed in fig [ fig : dziuk_init ] .",
    "fig [ fig : dziuk_adaptive ] shows the adaptive refined mesh .",
    "we would like to point out that the mesh is refined not only around the exponential peak but also at the high curvature areas .",
    "fig [ fig : dziuk_err ] displays the numerical errors .",
    "it demonstrates the optimal convergence rate in @xmath100 norm and a superconvergence rate for the recovered gradient .",
    "the effective index is showed in fig [ fig : dziuk_idx ] , which converges to 1 quickly after the first few iterations .",
    "again , it indicates the error estimator ( or ) is asymptotically exact .",
    "[ fig : dziuk_init ]     [ fig : dziuk_adaptive ]     [ fig : dziuk_err ]     [ fig : dziuk_idx ]",
    "in this paper , we have proposed a curvature stable gradient recovery method ( pppr ) for data defined on manifolds . in comparing with existing methods for data on surfaces in the literature , cf .",
    "@xcite , the proposed method has several improvements : the first highlight is that it does not require the exact surfaces , which makes it a realistic and robust method for practical problems ; second , it does not need the element patch to be @xmath101 symmetric to achieve superconvergence .",
    "third , it is the most curvature stable methods in comparing with the existing methods . aside from that",
    ", we have evolved the traditional ppr method ( for planar problems ) to function value preserving at the mean time . by testing with some benchmark examples ,",
    "it is quite evident that the proposed method numerically performs better than the methods in the state of the art .",
    "we have also shown the capability of the recovery operator for constructing _ a posterior _ error estimator . even though we only develop the methods for linear finite element methods on triangulated meshes , the idea should be applicable to higher order fem on more accurate approximations of surfaces , e.g. piece - wise polynomial surfaces , b - splines or nurbus .",
    "we leave this as a potential work for future .",
    "gradient recovery has other applications , like enhancing eigenvalues , pre - processing data in image science , simplifying higher order discretization of pdes , or even designing new numerical methods for higher order pdes , and so on .",
    "it would also be interesting to further investigate the full usage of the pppr method for problems with solutions defined on a manifold domain .",
    "the authors thank dr .",
    "pravin madhavan , dr .",
    "bjorn stinner and dr .",
    "andreas dedner for their kind help and discussions on a numerical example .",
    "gd acknowledges support from the austrian science fund ( fwf ) : geometry and simulation , project s11704 .",
    "hg acknowledges support from the center for scientific computing from the cnsi , mrl : an nsf mrsec ( dmr-1121053 ) and nsf cns-0960316 for providing computing resources .",
    "in general , there are infinitely many isomorphic parametrizations for a given patch @xmath107 .",
    "let us pick arbitrarily two of them , which are denoted by @xmath108 respectively , where @xmath19 and @xmath109 are planar parameter domains , then there exist @xmath110 to be a bijective , differentiable mapping , such that @xmath111 .",
    "that means for an arbitrary but fixed position @xmath112 , we have @xmath113 and @xmath114 , such that @xmath115 then we have @xmath116 and consequently , for every function @xmath117 @xmath118 we have @xmath119 using chain rule on both sides of the former equation of , then we get @xmath120 which gives the latter equation in since @xmath121 is non - degenerate . using the same process but consider @xmath122 , we can show the reverse implication .",
    "thus , we have shown that any two arbitrary parameterizations @xmath22 and @xmath123 lead to the same gradient values at same positions .        , _ best constants in the sobolev imbedding theorem : the yamabe problem _ , in seminar on differential geometry , vol .  102 of ann . of math .",
    "stud . , princeton univ . press , princeton , n.j .",
    ", 1982 , pp .  173184 .                                , _ finite elements for the beltrami operator on arbitrary surfaces _ , in partial differential equations and calculus of variations , vol .",
    "1357 of lecture notes in math . ,",
    "springer , berlin , 1988 , pp .",
    "142155 .",
    ", _ nonlinear analysis on manifolds : sobolev spaces and inequalities _ , vol .  5 of courant lecture notes in mathematics , new york university , courant institute of mathematical sciences , new york ; american mathematical society , providence , ri , 1999 .",
    "height 2pt depth -1.6pt width 23pt , _ the superconvergent patch recovery and a posteriori error estimates .",
    "error estimates and adaptivity _ , internat .",
    "methods engrg . , 33 ( 1992 ) ,",
    ".  13651382 ."
  ],
  "abstract_text": [
    "<S> this paper investigates gradient recovery schemes for data defined on discretized manifolds . the proposed method , parametric polynomial preserving recovery ( pppr ) , does not ask for the tangent spaces of the exact manifolds which have been assumed for some significant gradient recovery methods in the literature . </S>",
    "<S> another advantage of the proposed method is that it removes the symmetric requirement from the existing methods for the superconvergence . </S>",
    "<S> these properties make it a prime method when meshes are arbitrarily structured or generated from high curvature surfaces . as an application , </S>",
    "<S> we show that the recovery operator is capable of constructing an asymptotically exact posteriori error estimator . </S>",
    "<S> several numerical examples on 2dimensional surfaces are presented to support the theoretical results and make comparisons with methods in the state of the art , which show evidence that the pppr method outperforms the existing methods . </S>",
    "<S> .3 cm * ams subject classifications . *  primary 65n50 , 65n30 ; secondary 65n15 , 53c99 .3 cm    * key words . </S>",
    "<S> *  gradient recovery , manifolds , superconvergence , parametric polynomial preserving , function value preserving , curvature stable . </S>"
  ]
}