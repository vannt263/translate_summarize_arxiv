{
  "article_text": [
    "the majority of statistical literature on regression analysis focuses on inferring the mean function of the response @xmath0 given a predictor @xmath1 .",
    "there are also a large body of work devoted to making inference for the quantiles of @xmath0 given @xmath1 in the regression setting ( e.g. , * ? ? ?",
    "recently , researchers started to investigate methods to infer local modes of @xmath0 given @xmath1 ( see * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* among others ) .",
    "these researchers pointed out valuable information about the association between the response and the predictor that conditional modes can provide yet the conditional mean / quantiles can miss .",
    "advantages of modal regression compared to mean or quantile regression have been well appreciated in analyzing speed - flow data in traffic engineering @xcite , studying temperature patterns @xcite , investigating galaxy properties conditioning on a given environment @xcite , and in economics @xcite for instance . to address the practical issue of error - contaminated covariates ,",
    "methods accounting for measurement error in mean regression @xcite and methods for quantile regression in the presence of measurement error @xcite have been developed .",
    "in contrast , there is no existing research on modal regression when @xmath1 is prone to measurement error even though this issue often arises in the aforementioned applications , and ignoring measurement error in modal regression typically results in misleading inference .",
    "we tackle this important problem in our study .",
    "we propose two nonparametric methods for estimating conditional modes of a response variable @xmath0 given an error - prone covariate @xmath1 .",
    "the first method exploits a kernel density estimator of the joint density of @xmath3 that accounts for measurement error in @xmath1 .",
    "the second method is based on a local linear estimator of the conditional density of @xmath0 given @xmath2 .",
    "these methods are elaborated in section  [ s : methods ] .",
    "asymptotic properties of the mode estimators resulting from these methods are presented in section  [ s : asymptotics ] .",
    "we provide a data - driven method for bandwidth selection to facilitate the implementation of the proposed methods in section  [ s : implementation ] .",
    "numerical studies are presented in section  [ s : empirical ] , which include simulation experiments and an application to dietary data .",
    "section  [ s : discussion ] provides a discussion on follow - up open research questions .",
    "suppose one wishes to collect data for a response @xmath0 and a covariate @xmath1 , @xmath4 + @xmath5 , consisting of @xmath6 independent pairs from a bivariate distribution specified by the joint probability density function ( pdf ) , @xmath7 . however , the reality is that @xmath8 can not be observed directly due to error contamination , and @xmath9 are observed instead .",
    "more specifically , the observed covariate @xmath10 relates to the underlying true covariate @xmath1 via an additive measurement error model given by @xmath11 where @xmath12 are nondifferential measurement errors ( * ? ? ?",
    "* section 2.5 ) , meaning that @xmath13 and @xmath14 .",
    "we assume a known distribution of @xmath15 specified by its pdf @xmath16 in this study . estimating the distribution of @xmath15",
    "requires either replicate measures of each underlying @xmath17 or external validation data .",
    "for instance , if replicate measures are available and one assumes @xmath16 known up to some parameters , such as the variance parameter , then one can follow equation ( 4.3 ) in @xcite to estimate the measurement error variance consistently .",
    "one may also estimate the characteristic function of @xmath15 as proposed in @xcite , which is all our proposed inference methods need in terms of information regarding the measurement error distribution .",
    "the focal point of statistical inference presented in this study lies in local modes of the conditional pdf of @xmath0 given @xmath2 , @xmath18 .",
    "denote by @xmath19 the support of @xmath1 and by @xmath20 the support of @xmath0 .",
    "for a generic bivariate function @xmath21 , @xmath22 refers to @xmath23 evaluated at @xmath24 , and notations for higher - order partial derivatives of @xmath21 are similarly defined .",
    "given @xmath25 , a mode of @xmath18 , denoted by @xmath26 , is a value in @xmath20 such that @xmath27 and @xmath28 .",
    "equivalently , @xmath26 satisfies @xmath29 and @xmath30 .",
    "this latter viewpoint motivates our first proposed estimator of @xmath26 described next .",
    "it is possible that @xmath18 is multimodal at a given @xmath31 , producing a mode set @xmath32 .",
    "although multi - modality brings in certain challenges in the actual implementation of mode seeking , it adds little complication in asymptotics analyses of our proposed mode estimators . to avoid unnecessarily tedious notations , we assume a unimodal @xmath18 for the methodology development and theoretical analysis in the main article .",
    "we repeat the key part of the preliminary theoretical development with the uni - modality assumption relaxed in appendix i for illustration purposes .",
    "in addition , multimodal @xmath18 is considered when illustrating the implementation of the proposed methods in section  [ s : empirical ] .    even though the conditional mode set @xmath33 is formulated above in terms of the joint pdf @xmath34",
    ", we shall point out that , as @xmath31 moves in @xmath19 , the resultant conditional mode curve(s ) , @xmath35 , characterize the mode structure of the conditional density of @xmath0 given @xmath1 .",
    "hence , they typically differ from density ridges @xcite and principal curves @xcite , which focus on certain structures of the joint pdf .",
    "* section 8) provided detailed explanations on the distinction between conditional mode curves and density ridges , which are also helpful for one to see how they differ from principal curves .",
    "we first consider an estimator of @xmath26 , denoted by @xmath36 , as the solution to @xmath37 , where @xmath38 is a kernel - based estimator of @xmath39 . the construction of @xmath38 traces back to the kernel density estimator of @xmath7 in the absence of measurement error considered in @xcite , @xmath40 where @xmath41 and @xmath42 are bandwidths , and @xmath43 and @xmath44 are kernels . in the presence of measurement error , with @xmath45 observed in place of @xmath46",
    ", we follow the idea of deconvoluting kernel @xcite and propose an estimator of @xmath7 that accounts for measurement error as follows , @xmath47 where @xmath48 is the deconvoluting kernel , in which @xmath49 is the imaginary unit , @xmath50 is the fourier transform of @xmath51 , and @xmath52 is the characteristic function of @xmath15 , i.e. , the fourier transform of @xmath53 . in this article ,",
    "all integrations are over the entire real line unless otherwise specified .",
    "the estimator @xmath54 is motivated by the property of the deconvoluting kernel that @xmath55=k_1\\{(x - x)/h_1\\}$ ]",
    ". a direct implication of this property is that @xmath56 .    differentiating ( [ eq : jointpdfest ] ) with respect to ( w.r.t . ) @xmath57 yields an estimator of @xmath39 based on @xmath58 , @xmath59 where @xmath60 . to facilitate mode seeking , we choose @xmath44 to be a radially symmetric kernel ,",
    "that is , @xmath61 , in which @xmath62 is a nonnegative - valued function , referred to as the profile of @xmath44 , and @xmath63 is a positive constant serving as a normalization constant so that @xmath44 integrates to one . furthermore , we choose @xmath44 such that its profile @xmath62 relates to the profile of another radially symmetric kernel @xmath64 via @xmath65 .",
    "the epanechnikov kernel and the normal kernel are examples where a kernel possesses the above desirable features for @xmath44 . using the so - chosen @xmath44 , ( [ eq : py1 ] )",
    "can be further elaborated as @xmath66 for illustration purposes , throughout the article , we set @xmath67 . then , with @xmath68 and @xmath69 , one has @xmath70 , with @xmath71 and @xmath72 , and the above estimator becomes @xmath73    based on @xmath38 in ( [ eq : py3 ] ) , an estimator of @xmath26 is the solution to the equation @xmath74 rearranging terms in this equation yields a variant of the equation leading to the following updating formula that one evaluates iteratively until convergence in order to find a solution to it , @xmath75 where @xmath76 is the value resulting from the @xmath77th iteration as an update of the value from the previous iteration , @xmath78 , in search for @xmath36 , for @xmath79 .",
    "one may view @xmath80 as a weight associated with the @xmath81th data point @xmath82 , for @xmath83 , of which the magnitude depends on the proximity of this data point to @xmath84 . following this viewpoint",
    ", one can see that the right - hand side of ( [ eq : iterate0 ] ) is a weighted average of @xmath85 , and one may interpret this updating formula as updating @xmath78 to @xmath76 using the weighted mean of the responses surrounding @xmath84 .",
    "in fact , this algorithm of finding an estimated mode is in line with the mean - shift algorithm used to search for modes of a distribution @xcite . compared to the existing mean - shift algorithm and its application ,",
    "the complication caused by measurement error is that the weight @xmath86 can be negative because the deconvoluting kernel @xmath87 is not guaranteed to be nonnegative @xcite .",
    "however , with careful choices of the bandwidths and starting values for the algorithm , as to be elaborated in sections  [ s : implementation ] and [ s : empirical ] , our mean - shift algorithm can converge and produce a mode estimate @xmath36 .",
    "we call the so - obtained estimator @xmath36 a local constant estimator of the mode because of the construction of @xmath54 , which in nature is a local constant estimator of @xmath7 .",
    "this interpretation of @xmath54 is made clearer when compared to the way we estimate @xmath18 in order to estimate the mode .",
    "the second estimator of @xmath26 we propose , denoted by @xmath88 , is a solution to @xmath89 , where @xmath90 is an estimator of @xmath91 obtained as follows .",
    "we start from evoking the local linear estimator of @xmath18 in the absence of measurement error proposed by @xcite given by @xmath92 where @xmath93 , @xmath94^\\t,\\ ] ] in which @xmath95 this estimator is motivated by the property that @xmath96\\approx p(y|x)$ ] as @xmath97 , and hence @xmath18 can be approximately viewed as the mean function when regressing @xmath98 on @xmath1 . adopting this standpoint",
    ", one can employ the general strategy of estimating a mean function via local polynomial estimators @xcite to estimate @xmath18 , with @xmath99 being the response data and @xmath100 as the covariate data .",
    "in particular , the local linear estimator of @xmath18 is as in ( [ eq : cpdfnome ] ) .    in the presence of measurement error",
    ", we adjust @xmath101 to account for measurement error in @xmath1 and propose the following local linear estimator of @xmath18 , @xmath102 where @xmath103^\\t,\\ ] ] in which @xmath104 and , with @xmath105 denoting the @xmath106-th derivative of @xmath107 , @xmath108 the transform of @xmath51 in ( [ eq : ku1 ] ) is a generalization of the transform in ( [ eq : ku0 ] ) derived in @xcite .",
    "this generalization leads to a generalized deconvoluting kernel @xmath109 possessing the property that @xmath110=\\{(x - x)/h_1\\}^\\ell k_1\\{(x - x)/h_1\\}$ ] , @xmath111 .",
    "thanks to this property , given @xmath112 , ( [ eq : sme ] ) and ( [ eq : tme ] ) are unbiased estimators of their counterparts in the absence of measurement error in ( [ eq : snome ] ) and ( [ eq : tnome ] ) , respectively .",
    "hence , @xmath113 is a sensible counterpart estimator of @xmath101 in the presence of measurement error .    using ( [ eq : cpdfme ] )",
    ", an estimator of @xmath91 follows immediately by differentiating @xmath113 w.r.t .",
    "this gives @xmath114 where @xmath115^\\t$ ] , in which , for @xmath116 , @xmath117 setting @xmath89 gives an equation to which the solution is the mode estimator @xmath88 . elaborating ( [ eq : pyhat ] ) reveals that @xmath88 solves @xmath118 this suggests the following updating formula one may use iteratively until convergence to find the solution to the equation , @xmath119 like the previous updating formula , the right - hand side of this updating formula can also be viewed as a weighted average of @xmath85 , and thus this algorithm of searching for an estimated mode is also in the spirit of the mean - shift algorithm .    the common theme",
    "the above two proposed mode estimation methods follow is to solve an equation of the form @xmath120 , where @xmath121 an estimator of @xmath122 , and @xmath123 is a function such that @xmath124 and @xmath125 . when @xmath126 , the solution is @xmath36 ; and , when @xmath127 , solving the equation yields @xmath88 .",
    "furthermore , this common theme closely relates to the idea of corrected score @xcite .",
    "more specifically , since @xmath128 , one has @xmath129 . hence , the estimating equation one solves to obtain @xmath36 , i.e. , @xmath37 , is the corrected score estimating equation corresponding to @xmath130 , which is the equation one solves to estimate modes in the absence of measurement error .",
    "although , given @xmath131 , @xmath90 is not an unbiased estimator of @xmath132 , the building blocks in the former are unbiased scores of those in the latter , i.e. , @xmath133 and @xmath134 . when the solution to an equation associated with a method is not unique , the method leads to an estimated mode set , denoted by @xmath135 and @xmath136 for the first and the second method , respectively . in what follows",
    ", we present asymptotic properties of these mode estimators . for notational simplicity , we assume @xmath137 and @xmath138 in the next section . also , @xmath139 is often used in place of @xmath26 for brevity in the sequel , and @xmath140 is used to refer to a mode estimator generically when we do not distinguish between @xmath36 and @xmath88 .",
    "we focus on convergence rates of three forms of error associated with @xmath140 in this section , first , the pointwise error defined by @xmath141 ; second , the mean integrated squared error ( mise ) , @xmath142 ; and third , the uniform error , @xmath143 . we show next that the convergence rate of @xmath144 hinges on the bias and variance of @xmath145 .",
    "given the pointwise error rate , the convergence rate of @xmath146 follows straightforwardly . under regularity conditions",
    "pointed out along the way , the uniform error rate can be established using existing results regarding the uniform consistency of kernel - based estimators @xcite .    because @xmath147 , by the mean - value theorem , one has @xmath148 , where @xmath149 lies between @xmath139 and @xmath140 .",
    "thus , @xmath150 provided that @xmath151 and @xmath152 are bounded away from zero , one has @xmath153 where @xmath154 .",
    "then ( [ eq : diff ] ) implies that @xmath155 it follows that @xmath156 , or , equivalently , @xmath157 under conditions ( ck2 ) , ( ck5 ) and ( ck9 ) in appendix a , by lemma 10 in @xcite , @xmath158 converges to zero in probability .",
    "therefore , under these conditions , ( [ eq : delta2 ] ) suggests that @xmath144 can be approximated by @xmath159 , and thus the convergence rate of @xmath144 can be revealed through studying the convergence rate of @xmath160 .",
    "once we turn to studying the convergence rate of @xmath160 , the bias and variance of @xmath161 become highly relevant .",
    "this connection can be explained by first noting that @xmath162 , and thus one has @xmath163+|\\textrm{bias}\\{\\hat g_y(x , y_{\\hbox { \\tiny $ m$}})\\}| .",
    "\\label{eq : keyconnection}\\end{aligned}\\ ] ]    the above preliminary asymptotic analysis leads to the road map we follow to study the error rates associated with @xmath140 , which is to , first , derive the asymptotic bias and variance of @xmath161 , which leads to the pointwise error rate by ( [ eq : keyconnection ] ) ; second , establish the convergence rate of @xmath146 ; third , provide the uniform error rate .",
    "this is also the order in which we present our theoretical findings for each of the proposed mode estimator in the next two subsections .",
    "supporting materials of these findings are provided in the appendices . in particular , all coded conditions referenced henceforth are in appendix a.      with @xmath164 , we establish the following asymptotic bias result for @xmath165 , with the proof given in appendix c.    [ l : bias0 ] under conditions ( cp1 ) and ( ck1)(ck4 ) , @xmath166 where @xmath167 , @xmath168 , and @xmath169 .",
    "this bias result coincides with the result in @xcite , where kernel - based estimators of derivatives of a multivariate joint pdf are considered in the absence of measurement error .",
    "this is expected since @xmath170 , as pointed out in section  [ s : localcon ] .",
    "the variance of @xmath165 depends on the smoothness of the measurement error distribution .",
    "there are two levels of smoothness of @xmath16 considered in measurement error literature @xcite , ordinary smooth and super smooth .",
    "their definitions are given below .",
    "[ df : ordsm ] the distribution of @xmath15 is ordinary smooth of order @xmath171 if @xmath172 for some positive constants @xmath171 and @xmath173 .",
    "[ df : supsm ] the distribution of @xmath15 is super smooth of order @xmath171 if @xmath174 for some positive constants @xmath175 , @xmath176 , @xmath177 , @xmath171 , @xmath178 and @xmath179 .    in appendix d",
    ", we derive the asymptotic variance of @xmath165 and the results are given in the next lemma .",
    "[ l : var0 ] assume conditions required for lemma  [ l : bias0 ] hold . when @xmath15 is ordinary smooth of order @xmath171 , under conditions ( cu2 ) and ( ck5)(ck7 ) , if @xmath180 , then @xmath181 where @xmath182 , and @xmath183 is the joint pdf of @xmath184 .    when @xmath15 is super smooth of order @xmath171 , under conditions ( ck5 ) and ( ck8 ) , if @xmath185 , then @xmath186 where @xmath187 is some finite positive constant and @xmath188 .",
    "putting results in lemmas  [ l : bias0 ] and [ l : var0 ] together , one has the convergence rate of @xmath189 , which leads to the pointwise error rates summarized in the following theorem .",
    "[ thm : deltanx ] under the conditions in lemma  [ l : bias0 ] , lemma  [ l : var0 ] , and ( ck9 ) , when @xmath15 is ordinary smooth , @xmath190 and , when @xmath15 is super smooth , @xmath191    @xcite estimated local modes based on @xmath192 in ( [ eq : pdfest ] ) with @xmath193 , and they showed that the pointwise error rate of the resultant mode estimator is of order @xmath194 . comparing this with ( [ eq : pwe0ord ] ) and ( [ eq : pwe0sup ] ) , one can see that the pointwise error tends to zero much slower with the added complication of measurement error , especially when it is super smooth .",
    "hence , mode estimation in the presence of measurement error is substantially more challenging , as one would expect with noisier data .",
    "the convergence rate of @xmath195 can also be deduced from lemmas  [ l : bias0 ] and [ l : var0 ] . to see this more clearly , first note that , assuming interchangeability of expectation and integration , one has @xmath196 . as elaborated in appendix e , the dominating terms in the integrated squared bias of @xmath197 , @xmath198 ,",
    "can be easily derived based on @xmath199 ; and the dominating terms in the integrated variance of @xmath197 , @xmath200 , can be deduced from @xmath201 .",
    "combining these dominating terms , we reach the following conclusion regarding @xmath195 .",
    "[ thm : mise0 ] under conditions in theorem  [ thm : deltanx ] , and assume that @xmath202 and @xmath203 are square integrable , then , when @xmath15 is ordinary smooth , @xmath204 when @xmath15 is super smooth , @xmath205    moving to the uniform error , @xmath206 , it is helpful to note that , by ( [ eq : delta2 ] ) ,",
    "( [ eq : keyconnection ] ) , and lemma  [ l : bias0 ] , one has @xmath207",
    "+ o_{\\hbox { \\tiny $ p$}}(1).\\ ] ] thus @xmath208 following similar methods in @xcite and @xcite , one can establish the following result regarding @xmath206 .",
    "[ thm : uniferr0 ] under conditions in theorem  [ thm : deltanx ] and ( cp2 ) , for ordinary smooth @xmath15 , @xmath209 and , for super smooth @xmath15 , @xmath210    the uniform error rate associated with the mode estimator with @xmath193 considered in @xcite in the absence of measurement error is @xmath211 .",
    "compared with their result , theorem  [ thm : uniferr0 ] suggests the inevitable compromise in convergence rate due to measurement error .",
    "with @xmath127 , we present in appendix f the bias analysis and in appendix g the variance analysis of @xmath212 .",
    "results from these analyses are summarized in the following two lemmas .",
    "[ l : bias1 ] when @xmath15 is ordinary smooth , assume ( ck4 ) , ( ck5 ) , ( cx1 ) , and @xmath213 ; when @xmath15 is super smooth , assume ( ck4 ) , ( ck6 ) , ( cx1 ) , ( cp1 ) , and @xmath214 then one has @xmath215",
    "+ o(h_1 ^ 4h_2^{-1}+h_2 ^ 3),\\label{eq : bias1}\\end{aligned}\\ ] ] where @xmath216 is the pdf of @xmath1 and @xmath217 .",
    "[ l : var1 ] under the conditions in lemma  [ l : bias1 ] , when @xmath15 is ordinary smooth , if @xmath180 , then @xmath218 and , when @xmath15 is super smooth , if @xmath219 , then @xmath220 where @xmath187 is some finite positive constant .",
    "although the order of @xmath221 in ( [ eq : bias1 ] ) and that of @xmath199 in ( [ eq : bias0 ] ) are both @xmath222 , the dependence of the dominating term on @xmath223 shown in ( [ eq : bias1 ] ) indicates that estimating @xmath212 at the ( diminishing ) tail of the @xmath1-distribution can be challenging . the residual term in ( [ eq : bias1 ] )",
    "suggests that we need @xmath224 in order for @xmath225 , and thus for @xmath226 , to be consistent .",
    "a sufficient condition for @xmath224 is to have @xmath227 faster than @xmath97 as @xmath228 .",
    "this may indicate that a sensible bandwidth selection procedure tends to choose @xmath229 , which is indeed observed in our simulation study when we apply the data - driven bandwidth selection method described in section  [ s : implementation ] .    comparing ( [ eq : var1ord ] ) with ( [ eq : var0ord ] ) , one can see that the dominating variance of @xmath212 can be higher than that of @xmath165 , and the former can be large at the ( diminishing ) tail of @xmath216 .",
    "this suggests that estimating @xmath230 via @xmath212 , and thus estimating @xmath26 via @xmath88 , will be subject to high uncertainty if data surrounding @xmath31 are scarce .",
    "based on these bias and variance results , we establish the following theorem regarding the pointwise error rate of @xmath88 .",
    "[ thm : pwe1 ] under conditions in lemma  [ l : bias1 ] , lemma  [ l : var1 ] , ( ck2 ) and ( ck9 ) , when @xmath15 is ordinary smooth , @xmath231 and , when @xmath15 is super smooth , @xmath232    following the same line of arguments as those leading to @xmath233 in section  [ s : rate0 ] , we show the same convergence rate for @xmath234 as those stated in theorem  [ thm : mise0 ] under slightly different conditions .",
    "now we need to assume all conditions stated in lemmas  [ l : bias1 ] and [ l : var1 ] , ( cp2 ) , and that @xmath235 , @xmath236 , and @xmath237 are square integrable .",
    "finally , for the uniform error @xmath206 associated with @xmath226 , using the elaboration of @xmath212 in appendix f ( section f.1 to be specific ) , one can see that the dominating term of @xmath212 is simply @xmath165 divided by @xmath216 .",
    "hence , the convergence rate of @xmath206 associated with @xmath226 is the same as that of @xmath206 associated with @xmath197 stated in theorem  [ thm : uniferr0 ] under the same set of conditions , in addition to the assumption that @xmath216 is bounded away from zero over the range of @xmath31 of interest , @xmath238 $ ] .      with the asymptotic error rates of the proposed mode estimators provided in sections  [",
    "s : rate0 ] and [ s : rate1 ] , the asymptotic optimal ( in some sense ) bandwidths are readily available . in particular , taking mise as the metric to optimize w.r.t .",
    "@xmath239 , we show that , for both proposed mode estimators , the optimal rate of mise(@xmath140 ) is of order @xmath240 for both ordinary smooth @xmath15 and super smooth @xmath15 .",
    "the orders of the asymptotic optimal @xmath41 and @xmath42 ( as @xmath241 ) are given in a corollary next , where  @xmath242 \" refers to  tending to zero or infinity at the same rate . \"",
    "note that explicit expressions of the asymptotic optimal @xmath243 are not available except for @xmath197 when @xmath15 is ordinary smooth .",
    "[ cor : optimalh ] under conditions in theorem  [ thm : mise0 ] , when @xmath15 is ordinary smooth of order @xmath171 , the asymptotic optimal bandwidths for @xmath197 satisfy @xmath244 and @xmath245 , where @xmath246 in which @xmath247 when @xmath15 is super smooth of order @xmath171 , the asymptotic optimal bandwidths for @xmath197 satisfy @xmath248 and @xmath249 .",
    "the rates of the asymptotic optimal @xmath41 and @xmath42 for @xmath226 are the same as those for @xmath197 under each type of @xmath15 .",
    "the corresponding optimal rate of mise(@xmath140 ) is of order @xmath240 under each type of @xmath15 for both @xmath197 and @xmath226 .",
    "these rates of the asymptotic optimal bandwidths for mode estimators are also the rates of the asymptotic optimal bandwidths for the corresponding density derivative estimators .",
    "this is not surprising considering the connection between the proposed mode estimators and density derivative estimators pointed out in section  [ s : prelim ] .",
    "for instance , when @xmath15 follows a laplace distribution , which is ordinary smooth of order @xmath250 , the asymptotic optimal @xmath41 and @xmath42 are of order @xmath251 for @xmath252 , and the corresponding optimal mise of @xmath252 is of order @xmath253 . in the absence of measurement error , ( * ? ? ?",
    "* theorem 3 ) showed that the asymptotic optimal bandwidths for the kernel - based estimator of @xmath254 is of order @xmath255 , and the corresponding optimal mise of the density derivative estimator is of order @xmath256 .",
    "this comparison highlights that measurement error inflate the optimal mise rate of density derivative estimators , and also lead to much larger optimal bandwidths .",
    "the choice of bandwidths can noticeably affect finite sample performance of almost all kernel - based estimators .",
    "the explicit expression of the asymptotic optimal @xmath243 for @xmath197 in corollary  [ cor : optimalh ] is not ready to use for choosing bandwidths given a finite sample until reliable estimators of unknown quantities , such as @xmath257 , @xmath258 , and @xmath259 , are available . in this section",
    "we present a strategy of choosing @xmath243 that mostly follows the idea of incorporating cross validation ( cv ) and simulation extrapolation ( simex , * ? ? ?",
    "* ) proposed by @xcite .",
    "this strategy is based on the cv method of choosing bandwidth for estimating conditional density in the absence of measurement error developed by @xcite and @xcite .",
    "these authors constructed a cv criterion based on the weighted integrated squared error ( ise ) associated with a kernel - based estimator of @xmath18 , @xmath101 , defined by @xmath260 where @xmath261 is a nonnegative weight function used to avoid estimating @xmath18 at an @xmath31 around which data are scarce .",
    "given the observed data @xmath46 , a reasonable choice of @xmath261 is simply @xmath262)$ ] , where @xmath263 and @xmath264 are the 2.5th and 97.5th percentile of @xmath46 , respectively , and @xmath265 is the indicator function .",
    "noting that the third term in the above elaboration of ise does not depend on @xmath243 and thus can be ignored when minimizing ise with respect to @xmath243 , they proposed the following estimator of the first two terms as a cv criterion , @xmath266 where @xmath267 is the estimate of @xmath268 based on data @xmath131 excluding @xmath269 , for @xmath83 .",
    "it is worth pointing out that , if one uses the kernel density estimator of @xmath18 with the kernel associated with @xmath57 , i.e. , @xmath270 , being the standard normal pdf , then the integral in ( [ cv : density ] ) can be derived explicitly .",
    "clearly , the components in ( [ cv : density ] ) that involve @xmath17 can not be evaluated in the presence of measurement error . to account for measurement error in @xmath46 ( but not in @xmath85 ) , we first set @xmath42 at @xmath271 according to the normal reference rule @xcite , where @xmath272 is the sample standard deviation of @xmath85",
    "; then we adopt the cv - simex method to find an approximation of @xmath273 where @xmath274 denotes the estimate of @xmath18 based on @xmath275 .",
    "implementation of the cv - simex method involves the following steps .",
    "step 1 : : :    generate @xmath276 sets of further contaminated data according    to @xmath277 ,    where @xmath278 are    i.i.d . from @xmath16 , for    @xmath279 .",
    "step 2 : : :    viewing @xmath45 as the unobserved true covariate    values , and @xmath280 as the error - contaminated    surrogate of @xmath45 , find    @xmath281 where @xmath282 is the estimate of    @xmath18 based on    @xmath283 , and    @xmath284)$ ] ,    with @xmath285 and    @xmath286 being the 2.5th and 97.5th    percentile of @xmath45 , respectively .",
    "step 3 : : :    generate another @xmath276 sets of further contaminated data ,    @xmath287 ,    where @xmath288 are    i.i.d . from @xmath16 , for    @xmath279 .",
    "step 4 : : :    viewing @xmath280 as the unobserved true    covariate values , and @xmath289 as the the    error - contaminated surrogate of @xmath280 , find    @xmath290 where @xmath291 is the estimate    of @xmath18 based on    @xmath292 , and    @xmath293)$ ] ,    with @xmath294 and    @xmath295 being the 2.5th and 97.5th    percentile of @xmath280 , respectively .",
    "step 5 : : :    set @xmath296 as the final choice of    @xmath41 for estimating @xmath18 based on    @xmath275 .",
    "the rationale behind the cv - simex method is that the way @xmath297 in ( [ eq : h1 * ] ) relates to @xmath41 in ( [ eq : h1 ] ) is similar to the way @xmath298 ( [ eq : h1 * * ] ) relates to @xmath297 .",
    "in particular , @xcite showed that , as @xmath299 , @xmath300 , and thus @xmath301 , which suggests step 5 .",
    "we then set @xmath41 at @xmath302 when estimating local modes using @xmath275 .",
    "for the local constant mode estimator @xmath36 , @xmath113 in ( [ eq : h1 ] ) is @xmath303 , where @xmath54 is given in ( [ eq : jointpdfest ] ) and @xmath304 is the deconvoluting density estimator of @xmath216 as in @xcite . when considering the local linear mode estimator @xmath88 , @xmath113 in ( [ eq : h1 ] )",
    "is given by ( [ eq : cpdfme ] ) .",
    "in this section we first present simulation studies to demonstrate the performance of the two proposed mode estimators , and compare them with the mode estimator resulting from naively applying the method in @xcite to error - contaminated data .",
    "then we apply these methods to a real data example . in @xcite",
    ", it is assumed that @xmath305 .",
    "we do not impose this constraint when implementing their method for a fair comparison with our methods .      in the simulation experiment , we consider the following two true model configurations :    1 .",
    "@xmath306 \\sim 0.5n\\left(m(x)-2\\sigma(x ) , \\ , 2.5 ^ 2\\sigma^2(x)\\right)+0.5n\\left(m(x ) , \\ , 0.5 ^ 2\\sigma^2(x)\\right)$ ] , where @xmath307 , @xmath308 , @xmath309 , @xmath310 . in this case",
    ", @xmath18 is unimodal with @xmath311 , @xmath312 $ ] .",
    "2 .   @xmath306 \\sim 0.5n\\left(m_1(x ) , \\ , 0.5 ^ 2\\right)+0.5n\\left(m_2(x ) , \\ , 0.5 ^ 2\\right)$ ] , where @xmath313 , @xmath314 , @xmath309 , and @xmath310 . in this case",
    ", @xmath18 is bimodal with @xmath315 , @xmath312 $ ] .    under each true model configuration , we vary @xmath316 to achieve the reliability ratio @xmath317 equal to 0.75 , 0.85 , and 0.95 . given each of the six simulation settings , we generate @xmath318 monte carlo ( mc ) replicates , each of sample size @xmath319 , from the true model of @xmath184 .",
    "when implementing our proposed methods , we choose @xmath51 of which the fourier transform is @xmath320)$ ] , and choose @xmath270 to be the standard normal pdf . for the method in @xcite , both @xmath51 and @xmath270",
    "are the standard normal pdfs .",
    "to focus on comparing different mode estimators without being distracted by data - driven bandwidth selection , we first use approximated theoretical optimal bandwidths for each method to mitigate the confounding effect of data - driven bandwidth selection on the estimation quality .",
    "denote by @xmath321 a mode set estimator generically . given a candidate @xmath243",
    ", we obtain @xmath321 for a sequence of grid points in @xmath322 $ ] , @xmath323 , where @xmath324 is the partition resolution , and @xmath325 is the largest integer no larger than @xmath326 . then the approximated theoretical optimal @xmath243 associated with @xmath321",
    "is obtained by minimizing with respect to @xmath243 the empirical ise , @xmath327 where @xmath328 denotes the hausdorff distance between sets @xmath329 and @xmath330 , which is defined by @xmath331 , in which @xmath332 , for @xmath333 .",
    "simply put , @xmath321 and @xmath33 are close according to the hausdorff distance if and only if every point in either set is close to some point in the other set , where the closeness of two points is assessed by the euclidean distance .    for any given finite sample , besides the choice of @xmath243 , the starting values one uses in the mean - shift algorithm also influence @xmath334 .",
    "a starting mode too far from the majority of the data cloud around @xmath31 can cause numerical trouble in this iterative algorithm , and thus we suggest exercising great care in choosing starting values .",
    "one way that works well in our simulation study to set starting values is as follows .",
    "given @xmath31 at which @xmath33 is of interest , define an index set @xmath335 , where @xmath336 is a positive small value chosen so that the number of elements in @xmath337 is relatively large , say , 30 .",
    "then the starting values for estimating @xmath33 via the mean - shift algorithm are set to be the percentiles of @xmath338 equally spaced between the 10th and 90th percentiles .",
    "for example , if one chooses to start with three initial values for an @xmath31 , then one may set the starting values to be the 10th , 50th , and @xmath339th percentiles of @xmath340 . to avoid missing a mode ,",
    "the number of starting values , denoted by @xmath341 , can be slightly bigger than one s visual impression of the number of clusters of the observed data cloud .",
    "table  [ sim1laplap500:table ] shows the mc average of ise of each mode ( set ) estimate across 500 mc replicates and the associated standard error under each simulation setting , with @xmath322=[-2,2]$ ] and @xmath342 . in terms of both ise and variability , our mode estimates , @xmath135 and @xmath136 ,",
    "outperform the naive mode estimate , denoted by @xmath343 , in all six settings . and",
    "the improvement of our estimates over the naive estimate is more substantial when the error contamination is more severe ( i.e. , for smaller @xmath344 ) .",
    "in addition , the local linear estimate @xmath136 performs much better than the local constant estimate @xmath135 under ( c1 ) , while the advantage of the former is less obvious under ( c2 ) .",
    "in fact , @xmath136 deteriorates , although still outperforms @xmath343 , faster than @xmath135 does as @xmath344 decreases .",
    "the boxplots of ises and several estimated mode curves from each method are given in appendix h , which clearly show that there are more outliers for @xmath136 compared to @xmath135 .",
    "this comparison between the two proposed mode estimators indicates that the local linear estimator may be more suitable when @xmath18 is unimodal , and can be subject to more numerical instability when applying to data from a multimodal distribution .",
    "this is reminiscent of a remark in @xcite , who recommended use the local linear mode estimator ( in the absence of measurement error in their study )  only for the case of functional dependence , i.e. where the mode is unique . \"",
    "width=0.9    .averages of ise across 500 mc replicates using approximated theoretical optimal bandwidths .",
    "numbers in parentheses are ( @xmath345 standard errors ) associated with the averages[sim1laplap500:table ] [ cols=\"^,^,^,^,^,^,^,^ \" , ]     acknowledging the fact that the approximated theoretical optimal @xmath243 is not available in practice , we carry out a second round of simulation under the same six settings , with @xmath42 fixed at @xmath346 for all three methods , @xmath41 used in our estimators chosen via the cv - simex method with @xmath347 , and @xmath41 for the naive method chosen via naive cv as if @xmath45 were @xmath46 in ( [ eq : h1 ] ) .",
    "table  [ sim1laplap500simex - dens : table ] shows the mc averages of ise of the three considered estimates across 500 mc replicates , along with the corresponding standard errors , with @xmath238=[-1.8 , 1.8]$ ] .",
    "one can see that , under ( c1 ) , the proposed estimates still outperform the naive estimate . however , this is not as clear - cut under ( c2 ) .",
    "it appears that the number of modes in an estimated mode set ( for any of these methods ) can be sensitive to @xmath42 , and a smaller @xmath42 tends to give a bigger estimated mode set , creating more estimated local modes that can be far away from the true modes . as pictorial demonstration of these results , boxplots of these ises and several estimated mode curves from each method",
    "are provided in appendix h.    width=0.9    * 8c & & & + @xmath344 & 0.75 & 0.85 & 0.95 & & 0.75 & 0.85 & 0.95 + @xmath343 & 0.83 ( 0.19 ) & 0.51 ( 0.12 ) & 0.25 ( 0.06 ) & & 1.20 ( 0.51 ) & 0.51 ( 0.11 ) & 0.21 ( 0.03 ) + @xmath135 & 0.61 ( 0.15 ) & 0.42 ( 0.11 ) & 0.24 ( 0.05 ) & & 0.68 ( 0.37 ) & 0.44 ( 0.39 ) & 0.33 ( 0.40 ) + @xmath136 & 0.44 ( 0.12 ) & 0.29 ( 0.08 ) & 0.18 ( 0.07 ) & & 1.07 ( 0.54 ) & 0.53 ( 0.29 ) & 0.35 ( 0.60 ) +    besides the choice of @xmath42 , as pointed out in section  [ s : simulation ] , the starting values for the mean - shift algorithm also affect finite sample performance of these estimators .",
    "we use the data - dependent starting values described in section  [ s : simulation ] in the first round of simulations where the approximated theoretical optimal @xmath243 is used . with",
    "the data - driven @xmath243 used in the estimates as in the second round of simulations , the quality of an estimate is even more sensitive to the choice of starting values .",
    "figure  [ sim : onesample ] depicts the estimated mode curves from the three methods based on one simulated data set under each of ( c1 ) and ( c2 ) , with the data - dependent starting values highlighted in turquoise . from there",
    "one can see that , if one starts at a starting mode value far away from the truth , the mean - shift algorithm can fail to converge or / and result in an inferior mode estimate . to avoid the interaction effects between the data - driven bandwidth selection and the data - dependent starting values , allowing us to focus on assessing the performance of the data - driven bandwidth selection , we set the starting values to be @xmath348 under ( c1 ) , and @xmath349 under ( c2 ) when estimating @xmath33 in the second round of simulations that produce table  [ sim1laplap500simex - dens : table ] .",
    "for illustration purposes , we consider estimating local modes of the food frequency questionnaire ( ffq ) intake given one s long - term usual intake using dietary data .",
    "the data set to be analyzed contains the ffq intake , measured as percent calories from fat ( @xmath0 ) , and six 24-hour food recalls from @xmath350 subjects in the women s interview survey of health .",
    "the covariate of interest , the long - term usual intake ( @xmath1 ) , can not be observed directly .",
    "a common practice in epidemiology studies is to use data from 24-hour food recalls to construct a surrogate ( @xmath10 ) of the true covariate .",
    "for instance , @xcite used the average of two 24-hour food recalls from a subject as @xmath10 and studied the mean ffq intake conditioning on @xmath1 and other error - free covariates ; @xcite used the average of six 24-hour food recalls as @xmath10 and estimated conditional quantiles of the ffq intake .",
    "all intake values are on the log scale in these studies .",
    "we followed the construction of @xmath10 in @xcite , associated with which the estimated reliability ratio is 0.737 .",
    "figure  [ realdata : curve ] presents the naive estimated mode curve and the two non - naive estimated mode curves from the two proposed methods .",
    "both visual inspection of the scatter plot and the mean - shift algorithm seem to suggest a unimodal @xmath18 .",
    "empirical evidence from simulation experiments suggest that this is the scenario where the local linear mode estimator @xmath136 can substantially improve over the naive estimator @xmath343 , and the local constant mode estimator @xmath135 can also correct @xmath343 to some extent .",
    "the discrepancy between the three estimated mode curves at the lower segments in figure  [ realdata : curve ] can be due to bias correction from the two non - naive estimates , with the correction from @xmath136 more noticeable than that from @xmath135 .",
    "( blue line ) , and our proposed estimates accounting for measurement error , @xmath351 ( green line ) and @xmath136 ( red line ) .",
    "turquoise points are the data - dependent starting values for the mean - shift algorithm.,width=288,height=220 ]",
    "the study presented in this article fills in an important gap in the measurement error literature by providing mode estimation in the presence of measurement error .",
    "we rigorously study the asymptotic properties of the proposed mode estimators and develop a data - driven bandwidth selection method .",
    "this line of research leads us to more interesting open questions that we have started to investigate upon the completion of this project .",
    "an immediate extension of this research is to allow the response prone to measurement error , with or without measurement error in covariates .",
    "an easy revision of the current estimator of the conditional ( or joint ) density is to replace @xmath270 with the corresponding deconvoluting kernel .",
    "the complication then , at least from the implementation standpoint , is that one will lose the mean - shift algorithm updating formula because the deconvoluting kernel for @xmath0 is no longer radially symmetric , the key feature of @xmath270 that leads to the updating formula in all existing mean - shift algorithm applications .",
    "a different mode seeking algorithm is needed in this case .",
    "an even more involved problem arises from our development of data - driven bandwidth selection methods . in this study , we employed the cv - simex method to select the bandwidth in the @xmath31-direction , @xmath41 , with the bandwidth in the @xmath57-direction , @xmath42 , fixed at the normal reference that only depends on the response data .",
    "we conjecture that a more sensible way to choose these two bandwidths is to choose them jointly according to some cv criterion tailored for mode estimation , as opposed to choosing them in two separate steps using two different cv criteria that are designed for density estimation .",
    "it is unclear at this point how to implement such joint selection while bearing in mind that @xmath41 and @xmath42 play very different roles , with one relating to an error - prone predictor and the other corresponding to an error - free response .",
    "@xcite assumed @xmath193 and then chose @xmath352 to minimize the volume of the estimated prediction set , a statistic constructed to strive for a balance between the number of estimated local modes and the distance between the estimated mode and @xmath85 .",
    "following their idea , one may incorporate in the cv - simex method the following cv criterion defined by ( in the absence of measurement error ) @xmath353 where @xmath354 for a set @xmath355 , @xmath356 represents the estimated mode set at @xmath31 based on @xmath112 , @xmath357 is the number of distinct elements in @xmath356 , and @xmath261 is some weight function",
    ". however , theoretical justification of this cv criterion has not been established , and we did not see improvement from this bandwidth selection method over our current version of cv - simex method in the simulation study ( not shown here ) .    besides the need for a new cv criterion for the purpose of mode estimation , we also believe that mode estimation , with or without measurement error , can benefit from using bandwidths that depend on @xmath31 .",
    "we gain this intuition from simulation study with multimodal @xmath18 , where we encountered more difficulty in estimating modes when multiple true mode curves are steeper and close to each other .",
    "this difficulty is expected and can be illustrated by figure  [ f : steep ] , where two pairs of curves are shown , with the left pair flat and the right pair steep ( as functions of @xmath31 ) .",
    "fixing at an @xmath31 , the separation ( in @xmath57-direction ) between two curves within each pair is the same in this figure , and the variability of @xmath0 around each mode curve is also the same .",
    "but , within a given window ( of a fixed width ) in the @xmath31-direction , the data points ( in red in figure  [ f : steep ] ) surrounding the two steep curves are much harder to be separated into two clusters compared to the ( red ) data points around the two flat mode curves . and indeed in our simulation experiments ( not shown here ) , the @xmath243 that works well for identifying the flat pair of mode curves does poorly in revealing the steep pair of the mode curves , and vice versa .",
    "hence , if the multiple mode curves associated with @xmath18 show different steepness and different amount of separation along the @xmath57-direction over different regions along the @xmath31-direction , it seems more sensible to apply different bandwidths along @xmath19 .",
    "illustration of two pairs of mode curves , one pair being steeper than the other pair as functions of @xmath31 .",
    "the red data points fall within a window of width 0.1 in the @xmath31-direction centering at @xmath358 for the left pair of curves and @xmath359 for the right pair of curves.,width=288,height=172 ]    in light of the need for variable bandwidths , the asymptotic optimal global bandwidths provided in corollary  [ cor : optimalh ] may seem irrelevant for the purpose of bandwidth selection , although they are theoretically valuable because they lead to optimal rates of mise of the proposed mode estimators . whether or not these optimal rates reach the minimax rate ( given a well - defined class of mode estimators and a class of distributions or mode functions )",
    "remains an open problem .",
    "we conjecture that the key to solving this problem lies in the minimax convergence rate of mise associated with nonparametric density derivative estimators in the presence of measurement error , which itself is an open problem that we will tackle next .",
    "here we provide technical conditions that are needed at different parts of the theoretical development for different estimators considered in the main article .",
    "( cp1 ) : :    the joint density @xmath7 is four times continuously    differentiable with all partial derivatives bounded in absolute value    by a finite positive constant @xmath360 .",
    "( cp2 ) : :    for @xmath361 where    @xmath362 , there exists a finite positive constant    @xmath363 such that    @xmath364 .    condition ( cp1 ) is an ordinary smoothness condition . condition ( cp2 ) is a sharpness condition imposed on all critical points and implies no saddle points for @xmath34 .",
    "( ck1 ) : :    @xmath365 is an even function and    @xmath366 , for @xmath333 .",
    "( ck2 ) : :    @xmath44 is four times continuous differentiable with all    derivatives bounded in absolute value , and    @xmath367 ,    @xmath368 , for    @xmath369 .",
    "( ck3 ) : :    @xmath370    and    @xmath371 ,    for @xmath369 .",
    "( ck4 ) : :    @xmath372 ,    @xmath373 .",
    "( ck5 ) : :    @xmath374 ,    and    @xmath375 .",
    "( ck6 ) : :    @xmath376 is supported on    @xmath377 $ ] .",
    "( ck7 ) : :    @xmath378 is even and real .",
    "( ck8 ) : :    @xmath379 is not identically    0 , for @xmath369 .",
    "( ck9 ) : :    the class of functions defined by    @xmath380    is a vc - type class @xcite , that is , there exist @xmath381 ,    @xmath382 , and a constant envelop @xmath383 such that    @xmath384 ,    where @xmath385 is the    @xmath386-covering number for a semi - metric space    @xmath387 and @xmath388 is any probability measure",
    ".    define @xmath389 for @xmath79 .",
    "then ( ck1 ) implies that @xmath390 for all odd @xmath391 and @xmath392 , for @xmath333 .",
    "condition ( ck2 ) and ( ck5 ) are needed to derive the uniform error rates of the proposed mode estimator . with @xmath393 , ( ck3 )",
    "gives equation ( 1.2 ) in @xcite , the assumption necessary for a well - behaved deconvoluting kernel @xmath394 .",
    "condition ( ck3 ) with @xmath395 and ( ck8 ) are needed to derive the mean and variance of @xmath90 .",
    "conditions ( ck4 ) and ( the first half of ) ( ck5 ) are needed in lemma b.4 in @xcite , which we use to derive @xmath396 when @xmath15 is ordinary smooth .",
    "conditions ( ck4 ) and ( ck6 ) are needed in lemma b.9 in @xcite , which we evoke to derive @xmath396 when @xmath15 is super smooth .",
    "condition ( ck7 ) , along with ( cu1 ) given later , are imposed so that @xmath394 is real .",
    "finally , ( ck9 ) is needed to obtain the uniform consistency of the kernel - based estimators involved in the study .",
    "( cu1 ) : :    @xmath397 ,    @xmath398 , and it is an even function .",
    "( cu2 ) : :    @xmath399 .    condition ( cu1 ) is needed for a well - defined real - valued @xmath400 , for @xmath401 , and ( cu2 ) is imposed in lemma b.4 in @xcite .",
    "( cx1 ) : :    @xmath402 ,    @xmath403 ;    @xmath216 is twice differentiable and    @xmath404 is bounded in absolute    value by a finite positive constant @xmath405 , for    @xmath369 .",
    "this condition is needed for deriving the mean and variance of @xmath90 .",
    "in deriving @xmath396 we evoke lemma b.4 , lemma b.6 ( for ordinary smooth @xmath15 ) and lemma b.9 ( for super smooth @xmath15 ) in @xcite . for completeness , these lemmas are restated next .",
    "lemma b.4 : : :    assume that , for @xmath406 ,    @xmath407 ,    @xmath408 ,    @xmath409 ,    @xmath410 ,    and    @xmath411 ,    then , for a bounded function @xmath412 ,    @xmath413 lemma b.9 : : :    suppose that @xmath414 is supported on    @xmath377 $ ] , and , for @xmath415 and    @xmath416 ,    @xmath417 .    then    @xmath418 ,    where @xmath419 .",
    "assuming ( ck3 ) , it is shown that @xmath421=k_1\\{(x - x)/h_1\\}$ ] @xcite .",
    "hence , @xmath422 , thus @xmath423 .",
    "we next focus on deriving @xmath424 .",
    "recall that @xmath425 it follows that @xmath426 under ( cp1 ) , @xmath427 has the third - order taylor expansion around @xmath428 as follows , @xmath429 where @xmath430 , @xmath431 , @xmath432 , and @xmath433 approach zero as @xmath434 , @xmath435 , @xmath167 , and other partial derivatives are similarly denoted . given ( ck1 ) , using this third - order taylor expansion in ( [ eq : intk1k2tp ] ) leads to @xmath436 which is equal to @xmath437 when @xmath44 is the standard normal density , the choice we make in the main article .",
    "hence , @xmath438 setting @xmath439 gives lemma 3.1 in the main article .",
    "recall that @xmath440 it follows that @xmath441 ^ 2 .",
    "\\label{eq : varsummand}\\end{aligned}\\ ] ] from the bias analysis in appendix c , we have @xmath442,\\end{aligned}\\ ] ] thus @xmath443 ^ 2\\nonumber\\\\ = & h_1 ^ 2 h_2 ^ 6\\left[p_y^2(x , y)+p_y(x , y)\\left\\{p_{xxy}(x , y ) \\mu_2^{(1)}h_1",
    "^ 2+p_{yyy}(x , y ) h_2 ^ 2\\right\\}+o(h_1 ^ 2+h_2 ^ 2)\\right ] .",
    "\\label{eq : meansq}\\end{aligned}\\ ] ] this suggests that the second term in ( [ eq : varsummand ] ) is of order @xmath444 if @xmath445 , and it is of order @xmath446 if @xmath362 .",
    "we next look into the first term in ( [ eq : varsummand ] ) .    with nondifferential measurement error ,",
    "the joint pdf of @xmath184 is @xmath447 it follows that @xmath448 using the first - order taylor expansion of @xmath449 around @xmath450 in the innermost integral in ( [ eq : threelayer ] ) gives @xmath451 where @xmath452 , for @xmath453 .",
    "putting this elaboration of the innermost integral in ( [ eq : threelayer ] ) and using the first - order taylor expansion of @xmath454 around @xmath455 in ( [ eq : threelayer ] ) gives @xmath456 this reveals the dominating term of the expectation above as @xmath457    when @xmath15 is ordinary smooth of order @xmath171 , by lemma b.4 in @xcite , which is repeated under appendix b above , ( [ eq : ku02 ] ) indicates that @xmath458 where @xmath459 . because ( [ eq : osmooth ] ) tends to zero slower than @xmath460 , ( [ eq : varsummand ] ) is dominated by the first term there .",
    "hence , assuming @xmath180 , we have @xmath461 setting @xmath439 gives the first half of lemma 3.2 in the main article , where @xmath462 is replaced by @xmath463 because @xmath270 is the standard normal pdf in the main article .",
    "when @xmath15 is super smooth of order @xmath171 , by lemma b.9 in @xcite , repeated in appendix b above , ( [ eq : ku02 ] ) suggests that @xmath464 where @xmath187 is a finite positive constant .",
    "hence , if @xmath185 , ( [ eq : varsummand ] ) suggests @xmath465 setting @xmath439 and absorbing @xmath466 in @xmath187 gives the second half of lemma 3.2 in the main article .",
    "assuming interchangeability of integrations , the mean integrated squared error ( mise ) of @xmath36 can be decomposed into the sum of two parts , @xmath468 by equation ( 17 ) in the main article , one has @xmath469    to derive the integrated squared bias , using ( [ eq : biaspyjoint ] ) in ( [ eq : biasym0 ] ) , one has that , provided that @xmath470 tends to zero , @xmath471 under ( cp2 ) , and assuming @xmath202 and @xmath203 square integrable , one has @xmath472    to derive the integrated variance , using the variance analysis appendix d in ( [ eq : varym0 ] ) , one has that , when @xmath15 is ordinary smooth , @xmath473 and , when @xmath15 is super smooth , @xmath474    putting the above integrated squared bias of @xmath36 and integrated variance of @xmath36 together gives theorem 3.2 in the main article .",
    "recall that @xmath477 . denote by @xmath478 $ ] the first row of @xmath479 , then one has @xmath480 the derivations of the dominating bias of @xmath90 involve two tasks .",
    "first , revealing the dominating terms in @xmath481 . under ( cx1 ) , ( cu1 ) , and ( ck1 ) , ( ck3)(ck5 ) , ( ck8 ) , @xcite showed that @xmath482 where @xmath483 elaborating the above result yields @xmath484 this completes the first task .",
    "the second task is to reveal the dominating terms in @xmath485 according to the following decomposition , @xmath486 .",
    "\\label{eq : t'meanvar}\\ ] ] next , we first look into @xmath487 , then we study @xmath488 , and show that the former dominates that latter under certain conditions .",
    "because @xmath489 and @xmath490 we have @xmath491 inserting ( [ eq : taylorpxy ] ) in the above integrand leads to @xmath492 dsdt , \\nonumber \\\\ \\label{eq : taylor } \\end{aligned}\\ ] ] where @xmath493 , in which @xmath494 .    focusing on the case with @xmath439 , noting that @xmath495 and @xmath390 when @xmath391 is odd , for @xmath333 , ( [ eq : taylor ] ) reduces to @xmath496",
    "by ( [ eq : t ] ) , we have @xmath497 next , we elaborate the two inner integrals , one w.r.t . @xmath498 and the other w.r.t .",
    "@xmath499 , in ( [ eq : twoinner ] ) .    for the inner integral w.r.t .",
    "@xmath499 , using the second - order taylor expansion of @xmath500 around @xmath501 , one has @xmath502 setting @xmath439 , the above gives @xmath503    when it comes to the inner integral w.r.t @xmath498 in ( [ eq : twoinner ] ) , one shall distinguish between ordinary smooth @xmath15 and super smooth @xmath15 .",
    "if @xmath15 is ordinary smooth of order @xmath171 , under conditions ( ck4 ) and ( ck5 ) , lemma b.4 in @xcite implies that , @xmath504 where @xmath505 , for @xmath116 . by ( [ eq : tinner ] ) and",
    "( [ eq : sinner1 ] ) , ( [ eq : twoinner ] ) is equal to @xmath506,\\label{eq : ordtwoinner}\\end{aligned}\\ ] ] where ",
    "@xmath507 \" is the convolution operator , that is , @xmath508 . because the dominating term within the square brackets in ( [ eq : ordtwoinner ] ) is @xmath509 which is equal to @xmath510 by ( [ eq : fwy ] ) , ( [ eq : twoinner ] ) indicates that , for @xmath116 , @xmath511 it follows that , when @xmath15 is ordinary smooth , the first term in ( [ eq : tmeanvar ] ) dominates the second term there if @xmath512 .",
    "if @xmath15 is super smooth of order @xmath171 , by lemma b.9 in @xcite , under conditions ( ck4 ) and ( ck6 ) , one has @xmath513 where @xmath514 , and @xmath187 is some positive finite constant .",
    "hence , @xmath515 hence , when @xmath15 is super smooth , the first term in ( [ eq : tmeanvar ] ) dominates the second term there if @xmath516 .",
    "based on the mean and variance analysis of @xmath517 in sections  [ s : meant ] and [ s : vart ] , we now reach the conclusion that , if @xmath512 when @xmath15 ordinary smooth , or if @xmath518 when @xmath15 is super smooth , then @xmath519 this completes the second task stated in section  [ s : outline ] in order to derive @xmath476 .    using ( [ eq : twos ] ) and ( [ eq : twot ] ) in ( [ eq : pyhat2 ] ) , we have @xmath520 + o_{\\hbox { \\tiny $ p$}}\\left\\{h_2^{-1}r_4({\\mbox{\\boldmath $ h$}})\\right\\}. \\label{eq : dominating}\\end{aligned}\\ ] ] because the dominating term in ( [ eq : dominating ] ) is a non - random quantity , this dominating term is also the dominating bias of @xmath212 .",
    "this proves lemma 3.3 in the main article .",
    "by ( [ eq : twos ] ) , @xmath521 . \\end{aligned}\\ ] ] extracting the dominating terms in the above expression reveals that , to find the dominating variance of @xmath212 , it suffices to look into the variance of @xmath522 this leads us to study the following variance and covariance , @xmath523 @xmath524",
    "for @xmath116 , @xmath525 ^ 2 .",
    "\\label{eq : sqmean}\\end{aligned}\\ ] ] the expectation in ( [ eq : meansq ] ) is considered in section  [ s : vart ] ( see ( [ eq : var01 ] ) ) .",
    "from there , we have shown that , if @xmath15 is ordinary smooth of order @xmath171 , then ( relating to ( [ eq : ordvart ] ) ) @xmath526 and , if @xmath15 is super smooth of order @xmath171 , then ( relating to ( [ eq : supvart ] ) ) @xmath527 the expectation in ( [ eq : sqmean ] ) is considered in section  [ s : meant ] ( see ( [ eq : mean01 ] ) ) . by ( [ eq : twoet ] ) , this expectation is of order @xmath528 when @xmath529 , and it is of order @xmath530 when @xmath531 .",
    "hence , for both @xmath116 , ( [ eq : sqmean ] ) tends to zero faster than ( [ eq : meansqord ] ) and ( [ eq : meansqsup ] ) .",
    "it follows that that , for ordinary smooth @xmath15 , @xmath532 and , for super smooth @xmath15 , @xmath533",
    "the covariance in ( [ eq : term3 ] ) is equal to @xmath534 in section  [ s : meant ] , we have shown that the product in ( [ eq : prodmean ] ) is of order @xmath535 , which tends to zero faster than ( [ eq : meanprod ] ) , as to be revealed next .    when @xmath15 is ordinary smooth , by lemma b.4 in @xcite , @xmath536 where @xmath537 .",
    "when @xmath15 is super smooth , by lemma b.9 in @xcite , @xmath538 hence , the covariance in ( [ eq : term3 ] ) and variances in ( [ eq : term12 ] ) are of the same order .",
    "since ( [ eq : term12 ] ) and ( [ eq : term3 ] ) are of the same order , the variance of ( [ eq : keyvar ] ) with @xmath439 is dominated by @xmath539 hence , for ordinary smooth @xmath15 , by ( [ eq : term1ord ] ) with @xmath540 , @xmath541 and , for super smooth @xmath15 , by ( [ eq : term1sup ] ) , @xmath542 this proves lemma 3.4 in the main article .",
    "+   +     +   +     +   +     +   +",
    "suppose @xmath18 has @xmath543 modes , with the mode set @xmath544 , and the estimated mode set @xmath545 .",
    "then the pointwise error is @xmath546 . by the mean - value theorem , for each @xmath547",
    ", one has @xmath548 as in ( [ eq : diffym ] ) .",
    "it follows that @xmath549 and thus @xmath550 where the last equality results from assumption ( cp2 ) .",
    "hence , under the same conditions that supporting ( [ eq : delta2 ] ) , @xmath144 can be approximated by @xmath551 , and thus the convergence rate of @xmath144 is the same as that of @xmath552 . from this point on ,",
    "all arguments in section  [ s : asymptotics ] regarding @xmath161 , which is @xmath165 in section  [ s : rate0 ] and is @xmath212 in section  [ s : rate1 ] , carry over to @xmath553 for each @xmath547 . and as long as @xmath543 is finite , the convergence rate of @xmath552 is the same as that of @xmath554 for a @xmath555",
    ".    bamford , s.p . ,",
    "rojas , a.l . , genovese , c.r . , miller , c.e . , and nichol r , wasserman , l. ( 2008 ) revealing components of the galaxy population through nonparametric techniques .",
    "_ mon . not .",
    "r. astron .",
    "_ , * 391 * , 607616 ."
  ],
  "abstract_text": [
    "<S> in the context of regressing a response @xmath0 on a predictor @xmath1 , we consider estimating the local modes of the distribution of @xmath0 given @xmath2 when @xmath1 is prone to measurement error . </S>",
    "<S> we propose two nonparametric estimation methods , with one based on estimating the joint density of @xmath3 in the presence of measurement error , and the other built upon estimating the conditional density of @xmath0 given @xmath2 using error - prone data . </S>",
    "<S> we study the asymptotic properties of each proposed mode estimator , and provide implementation details including the mean - shift algorithm for mode seeking and bandwidth selection . </S>",
    "<S> numerical studies are presented to compare the proposed methods with an existing mode estimation method developed for error - free data naively applied to error - prone data . </S>"
  ]
}