{
  "article_text": [
    "streams of user - generated content from social media and microblogging systems exhibit patterns of collective attention across diverse topics , with temporal structures determined both by exogenous factors , such as driving from mass media , and endogenous factors such as viral propagation . because of the openness of social media , of the complexity of their interactions with other social and information systems , and of the aggregation that typically leads to the observable stream of posts , several concurrent signals are usually simultaneously present in the post stream , corresponding to the activity of different user communities in the context of several different topics . making sense of this information stream is an inverse problem that requires moving beyond simple frequency counts , towards the capability of teasing apart latent signals that involve complex correlations between users , topics and time intervals .",
    "the motivation for the present study is twofold . on the one hand",
    ", we want to devise techniques that can reliably solve the inverse problem of extracting latent signals of attention to specific topics based on a stream of posts from a micro - blogging system .",
    "that is , we aim at extracting the time - varying topical structure of a microblog stream such as twitter .",
    "on the other hand , we want to deploy these techniques in a context where temporal and semantic metadata about external events driving twitter are available , so that the relation between exogenous driving and time - varying topical responses can be elucidated .",
    "we do not regard this as a validation of the methods we use , because the relation between the external drivers and the response of a social system is known to be complex , with memory effects , topical selectivity , and different degrees of endogenous social amplification .",
    "rather , we regard the comparison between the time - resolved topical structure of a microblog stream and an independently available event schedule as an important step for understanding to what extent twitter can be used as a social sensor to extract high - resolution information on concurrent events happening in the real world .    here",
    "we focus on content collected by the emoto project from twitter during the london 2012 olympics , for which a daily schedule of the starting time and duration of sport events and social events is available and can be used for reference . in this context , resolving topical activity over time requires to go beyond the analysis and characterization of popularity spikes . a given topic driven by external events",
    "usually displays an extended temporal structure at the hourly scale , with multiple activity spikes or alternating periods of high and low activity .",
    "we aim at extracting signals that consists of an association of ( i ) a weighted set of terms defining the topic , ( ii ) a set of tweets that are associated to the topic , together with the corresponding users , and ( iii ) an activity profile for the topic over time , which may comprise disjoint time intervals of nonzero activity .",
    "we detect time - varying topics by using two independent methods , both based on non - negative matrix or tensor factorization . in the first case we build the full tweet - term - time tensor and use non - negative tensor factorization to extract the topics and their activity over time .",
    "we introduce an adapted factorization technique that can naturally deal with the special tensor structure arising from microblog streams . in the second case , which in principle affords on - line computation , we build tweet - term frequency matrices over consecutive time intervals of fixed duration .",
    "we apply non - negative matrix factorization to extract topics for each time interval and we track similar topics over time by means of agglomerative hierarchical clustering .",
    "we then apply both methods to the twitter dataset collected during the olympics , which reflects the attention users pay to tens of different concurrent events over the course of every day .",
    "we focus on topical dynamics at the hourly scale , and find that for those sport events in the schedule that can be semantically matched to the topics we obtain from twitter , the activity timeline of the detected topic in twitter closely matches the event timeline from the schedule .",
    "this paper is structured as follows : section  [ sec : background ] reviews the literature on collective attention , popularity , and topic detection in microblog streams .",
    "section  [ sec : data ] describes the olympics 2012 twitter dataset used for the study , the event schedule we use as an external reference , and introduces some notations and conventions used throughout the paper .",
    "section  [ sec : tf ] and section  [ sec : mf ] describe the two techniques we use to mine time - varying topical activity in the twitter stream .",
    "section  [ sec : results ] discusses the relation between the time - varying topics we obtain and the known schedule of the olympics events for one representative day , and provides some general observations on the behavior of the two methods .",
    "finally , section  [ sec : conclusions ] summarizes our findings and points to directions for further research .",
    "the dynamics of collective attention and popularity in social media has been the object of extensive investigation in the literature .",
    "attention can suddenly concentrate on a web page  @xcite , a youtube video  @xcite , a story in the news media  @xcite , or a topic in twitter  @xcite .",
    "intrinsic features of the popular item under consideration have been related to its popularity profile by means of semantic analysis and natural language processing of user - generated content  @xcite . in particular ,",
    "a great deal of research  @xcite has focused on characterizing the shape of peaks in popularity time series and in relating their properties to the popular item under consideration , to the relevant semantics , or to the process driving popularity .    within the broad context of social media ,",
    "twitter has emerged as a paradigmatic system for the vision of a `` social sensor '' that can be used to measure diverse societal processes and responses at scale  @xcite . to date , comparatively little work has been devoted to extracting signals that expose complex correlations between topics and temporal behaviors in micro - blogging systems . given the many factors driving twitter , and their highly concurrent nature , exposing such a topical - temporal structure may provide important insights in using twitter as a sensor when the social signals of interest can not be pinpointed by simply using known terms or hashtags to select the relevant content , or when the topical structure itself , and its temporal evolution , needs to be learned from the data .",
    "saha and sindhwani  @xcite adopt such as viewpoint and propose an algorithm based on non - negative matrix factorization that captures and tracks topics over time , but is evaluated at the daily temporal scale only , against events that mainly consist of single popularity peaks , without concurrency .",
    "here we aim at capturing multiple concurrent topics and their temporal evolution at the scale of hours , in order to be able to compare the extracted signals with a known schedule for several concurrent events taking place during one day .",
    "as we will discuss in detail , microblog activity can be represented using a tweet - term - time three - way tensor , and tensor factorization techniques can be used to uncover latent structures that represent time - varying topics .",
    "@xcite proposed in @xmath0 the canonical decomposition ( candecom ) , also called parallel factorization + ( parafac , @xcite ) , which can be regarded as a generalization to tensors of singular value decomposition ( svd ) . maintaining the interpretability of the factors",
    "usually requires to achieve factorization under non - negativity constraints , leading to techniques such as non - negative matrix or tensor factorization ( nmf and ntf ) .",
    "tensor factorization to detect latent structures has been extensively used in several domains such as signal processing , psychometrics , brain science , linguistics and chemometrics  @xcite .",
    "the following notations are used throughout the paper .",
    "scalars are denoted by lowercase letters , e.g. , @xmath1 , and vectors are denoted by boldface lowercase letters , e.g. , @xmath2 , where the @xmath3-th entry is @xmath4 .",
    "matrices are denoted by boldface capital letters , e.g. , @xmath5 , where the @xmath3-th column of matrix @xmath5 is @xmath6 , and the @xmath7-th entry is @xmath8 .",
    "third order tensors are denoted by calligraphic letters , e.g. , @xmath9 . the @xmath3-th slice of @xmath9 , denoted by @xmath10 , is formed by setting the last mode of the third order tensor to @xmath3 . the @xmath7-th vector of @xmath9 , denoted by @xmath11 , is formed by setting",
    "the second to last and last modes of @xmath9 to @xmath3 and @xmath12 respectively , and the @xmath13-th entry of @xmath9 is @xmath14 .",
    "the emoto dataset consists of around 14 million tweets collected during the london 2012 summer olympics using the public twitter streaming api .",
    "all tweets have at least one of 400 keywords , including common words used in the olympic games  like athlete , olympic , sports names and twitter accounts of high followed athletes and media companies .",
    "tweets were collected during all the interval of 17 days comprising the olympic games , from july 27 to august 12 2012 .      in order to investigate the relation between the extracted time - varying topics and the sport events of the olympic games , we use the schedule available on the official london 2012 olympics page , where the starting time and duration of most events is reported together with metadata about the type of event ( discipline , involved teams or countries , etc . )      for the text analysis performed in this paper , urls are removed from the original tweet content .",
    "the remaining text is used to build a vocabulary composed of the most common 30,000 terms , where each term can be a single word , a digram or a trigram .",
    "352 common words of the english language are also removed from the vocabulary . in order to localize twitter users , we examine the user profile descriptions and use an adapted version of geodict to identify , if possible , the user country . to study the relation between the extracted topical activity and the schedule of the olympic events , we focus on tweets posted by users located in the uk , only .",
    "this allows us to avoid potential confusion arising from tweets posted in countries , such as the usa , where olympics events were broadcasted with delays of several hours due to time zone differences .",
    "this selection leaves us with a still substantial amount of data ( about one third of the full dataset ) and simplifies the subsequent temporal analysis , even though it probably oversamples the attention payed to events that involved uk athletes .    for the scope of this study",
    ", we represent the data as a sparse third - order tensor @xmath15 , with @xmath16 tweets , @xmath17 terms and @xmath18 time intervals .",
    "we aggregate the tweets over 1-hour intervals , for a total of @xmath19 intervals .",
    "the tensor @xmath20 is sparse : the average number of terms ( also referred as features in the following ) for each tweet is typically no more than 10 , compared to the 30k terms of our term vocabulary .",
    "moreover , as each tweet is emitted at a given time , each interval @xmath21 has a limited number of active tweets , @xmath22 .",
    "a tensor slice @xmath23 is a sparse matrix with non - zero values only for @xmath22 rows . @xmath24",
    "represent the sparse tweet - term matrix observed at time @xmath21 .",
    "the term values @xmath25 for each tweet @xmath3 are normalized using the standard term frequency and inverse - document frequency ( tf - idf ) weighting , @xmath26 , where @xmath27 is the frequency of term @xmath12 in tweet @xmath3 , and @xmath28 where @xmath29 is the total number of tweets and @xmath30 is the number of tweets where the term @xmath12 appears .",
    "the methods that we present in this paper are able to extract topical - temporal structures from @xmath20 .",
    "such topical - temporal structures can be represented a stream matrix @xmath31 with @xmath32 topics and @xmath18 intervals .",
    "each component @xmath32 is also characterized by a term - vector @xmath33 that defines the most representative terms for that component . in order to visualize such topical - temporal structures represented as a stream matrix",
    ", we use the method described by byron and wattenberg  @xcite , which yields a layered stream - graph visualization .",
    "as explained in section [ sec : data ] , the tensor @xmath15 with @xmath16 tweets , @xmath17 terms and @xmath18 intervals is a natural way to represent the tweets and their contents with respect to the time .",
    "the tensor has the advantage to directly encompass the relationship between tweets posted at different hours and consequently between topics of the different hours .",
    "the tensor factorization as described below allows to uncover topics together with their temporal pattern .    before describing the process of factorization itself and its output",
    ", one needs to introduce the concept of canonical decomposition ( cp ) .",
    "cp in @xmath34 dimensions aims at writing a tensor @xmath15 in a factorized way that is the sum of the outer product of three vectors : @xmath35 where the smallest value of @xmath36 for which this relation exists , is the rank of the tensor @xmath37 . in other words , the tensor @xmath37 is expressed with a sum of rank-@xmath38 tensors .",
    "the set of vectors @xmath39 ( resp .",
    "@xmath40,@xmath41 ) can be re - written as a matrix @xmath42 ( resp @xmath43,@xmath44 ) where each of the @xmath36 vectors is a column of the matrix .",
    "the decomposition of eq .",
    "[ approx ] can also be represented in terms of the three matrices @xmath45 as @xmath46 . a visual representation of such a factorization , also called kruskal decomposition ,",
    "is displayed on fig .",
    "[ drawing_factorization ] .",
    "regarding the extraction of topics , the aim is not to decompose the tensor in its exact form but to approximate the tensor by a sum of rank-@xmath38 tensors with a number of terms smaller than the rank of the original tensor .",
    "this number @xmath32 corresponds to the number of topics that we want to extract ( see fig .",
    "[ drawing_factorization ] ) .",
    "such an approximation of the tensor leads to minimize the difference between @xmath37 and @xmath46 : @xmath47 where @xmath48 is the frobenius norm .",
    "we transform the @xmath34-dimensional problem ( eq .  [ 3d - problem ] ) in @xmath49-dimensional sub - problems by unfolding the tensor @xmath37 in three different ways .",
    "this process called matricization gives rise to three modes @xmath50 .",
    "the mode-@xmath51 matricization consists of linearizing all the indices of the tensor except @xmath51 .",
    "the three resulting matrices have respectively a size of @xmath52,@xmath53 and @xmath54 .",
    "each element of the matrix @xmath55 corresponds to one element of the tensor @xmath37 such that each of the mode contains all the values of the tensor . due to matricization",
    ", the factorization problem given by eq.[approx ] can be reframed in factorization of the three modes . in other terms , maximizing the likelihood between @xmath37 and @xmath46 is equivalent to minimizing the difference between each of the mode and their respective approximation in terms of @xmath45 .",
    "the factorization problem ( parafac ) in eq.[3d - problem ] is converted to the three following sub - problems where we added a condition of non - negativity of the three modes : @xmath56 where @xmath57 is the khatri - rao product which is a columnwise kronecker product , i.e. such that @xmath58 $ ] . if @xmath59 and @xmath60 , then the khatri - rao product @xmath61 . in our case of study",
    ", @xmath45 will give each access to a different information : @xmath62 allows to know at which topic belongs a tweet , @xmath63 gives the definition of the topics with respect to the features and @xmath64 gives the temporal activity of each topic .",
    "several algorithms have been developped to tackle the parafac decomposition .",
    "the two most common are one method based on the projected gradient and the alternating least square method ( als ) .",
    "the first one is convenient for its ease of implementation and is largely used in singular value decomposition ( svd ) but converges slowly . in the als method ,",
    "the modes are deduced successively by solving eq [ 2d - problem ] . in each iteration , for each of the sub - problem , two modes are kept fixed while the third one is computed .",
    "this process is repeated until convergence . in our case",
    ", we use a nonnegativity constraint to make the factorization better posed and the results meaningful .",
    "one thus uses nonnegative als ( anls @xcite ) combined with a block - coordinate - descent method in order to reach the convergence faster .",
    "each of the step of the algorithm needs to take into account the karush - kuhn - tucker ( kkt ) conditions to have a stationary point .",
    "our program is based on the algorithm implemented by @xcite .",
    "we can not directly perform the ntf on the tensor [ tweets @xmath65 features @xmath65 interval ] built as explained aboved as this tensor has a `` block - disjoint '' structure peculiar to the tweets .",
    "indeed each tweet has non - zero values only at one interval because a tweet is emitted only at a given time .",
    "each interval @xmath21 has only @xmath22 active tweets . in each slice @xmath24 of the tensor ,",
    "only @xmath22 rows have meaningful values .",
    "so , we are only interested in reproducing the tensor part which contains the meaningful values . in order to focus on these meaningful values",
    ", one needs to consider an adapted version of the tensor @xmath37 .",
    "we first consider the tensor @xmath37 built as explained above .",
    "we generate a first set of matrices @xmath45 which could approximate the tensor . at the next step ,",
    "one tries to decompose a tensor @xmath66 where the values are a combination of the values of @xmath66 and of the values of @xmath46 . more exactly",
    ", this tensor has the same size than @xmath37 and the same values than @xmath37 for the rows @xmath22 of each slice @xmath67 .",
    "the complementary values are given by @xmath46 .",
    "in other terms , at each step , the tensor that we approximate is updated by : @xmath68 where @xmath69 is the hadamard product ( element - wise product ) and @xmath70 is a binary tensor of the same size than @xmath71 with @xmath38-values only when the values of @xmath71 at this position are meaningful . the particular structure of the tensor ( disjoint blocks in time ) could be perceived as a `` missing values '' problem in the tensor , this problem has been for example tackled in @xcite .",
    "concretely , the implementation is an adaptation of a matlab program @xcite which uses the tensor toolbox @xcite .",
    "this adaptation includes the introduction of a mask ( via the tensor of weight ) as mentionned above and the rewriting of some operations to avoid memory issues .",
    "this point is not detailed here as it is not part of the main point of the paper .",
    "we calculate the strength of each topic with respect to the time by using both the information about the link between each topic and each tweet and about temporal pattern of the topics .",
    "these informations are available through @xmath62 and @xmath64 and the consequent strength of a topic @xmath72 on each interval of time @xmath21 is given by : @xmath73 where @xmath74 is a sum over the tweets indexed by @xmath3 occurring at the interval indexed by @xmath21 .",
    "the set of elements @xmath75 with @xmath76 and @xmath77 forms the stream matrix @xmath78 . each topic",
    "is then defined by a terms vector and each of this term vector is given by a column of @xmath63 .",
    "for each tensor slice @xmath24 , we compute a non - negative factorization by minimizing the following error function , @xmath79 where @xmath48 is the frobenius norm , subject to the constraint that the values in @xmath80 and @xmath81 must be non - negative . the non - negative factorization is achieved using the projected gradient method with sparseness constraints , as described in  @xcite .",
    "the factorization produces a matrix of left vectors @xmath82 and a matrix of right vectors @xmath83 , where @xmath84 is the number of components used in the decomposition .",
    "the matrix @xmath81 stores the term vectors of the extracted components at interval @xmath85 .",
    "the matrix @xmath80 is used to calculate the strength of each extracted component , which are represented in a matrix @xmath86 given by @xmath87 where @xmath88 is the strength of factor @xmath89 at interval @xmath21 .      in order to track topics over time",
    ", we need to merge components into topics depending on how similar they are .",
    "since each component is defined by a term vector , we can calculate a similarity matrix of all possible pairs of term vectors using cosine similarity .",
    "this matrix is fed to a standard agglomerative hierarchical clustering algorithm , known as upgma  @xcite , that at each step combines the two most similar clusters into a higher - level cluster .",
    "cluster similarity is defined in terms of average linkage : that is , the distance between two clusters @xmath90 and @xmath91 is defined as the average of all pair - wise distances between the children of @xmath90 and those of @xmath91 .",
    "the hierarchical clustering produces a tree that can be cut at a given depth to yield a clustering at a chosen level of detail .",
    "that is , by varying the threshold similarity we use for the cut we can go from a coarse - grained topical structure , with few clusters that may merge unrelated topics , to a fine - grained topical structure , with many clusters that may separate term vectors that otherwise could be regarded as the same continuous topic over time .",
    "the cut threshold needs to be chosen based on criteria that depends on the application at hand .",
    "each choice for the cut yields a number of clusters @xmath92 and a map function @xmath93 that associates the component index @xmath89 at time interval @xmath21 to a topic cluster @xmath72 .",
    "this function collects all components associated to cluster @xmath72 in a set @xmath94 for each interval @xmath21 .",
    "when constructing the stream matrix , the number of topics @xmath32 in the stream matrix is given by the number of clusters generated by the clustering step . in order to calculate the entries @xmath95 of the resulting stream matrix @xmath96 , we aggregate the strengths of the clustered components .",
    "we build a stream matrix @xmath31 , with @xmath32 topics and @xmath18 intervals , given by    @xmath97    finally , we extract the term vectors that are associated to each cluster .",
    "each cluster will be associated to a term vector @xmath98 that is the average of all term vectors @xmath99 associated to that cluster in the component clustering step .",
    "we now move to the analysis of the london 2012 twitter dataset and its relation with the known schedule of the games .",
    "we focus on one representative day , july 29th , during which several sport events took place at different times and concurrently .",
    "we use both topic detection methods , show the signals they extract , and check to what extent they are capable of extracting signals that we can understand in terms of the schedule .    the topic detection methods are set up as follows .",
    "for the masked ntf method , we decompose the tensor using a fixed number of components , using a tolerance value of @xmath100 for the stopping condition , and limiting the number of iterations to @xmath101 . for the agglomerative nmf method",
    ", we decompose each interval matrix using a fixed number of components .",
    "we use a tolerance value of @xmath100 for the stopping condition , and limit the number of iterations to @xmath102 .",
    "we use 250 topics for the masked ntf , and 50 components per time intervals in the agglomerative nmf .",
    "figure  [ bigstream2 ] shows a streamgraph representation of time - varying topics extracted using the two methods we have discussed .",
    "two global activity peaks are visible in both streamgraphs : the peak at about 2.30pm utc was triggered when elizabeth armistead won the silver medal in road cycle ; the peat at about 7 pm utc is driven by the bronze medal in 400 m freestyle to rebecca adlington . in the stream graphs , for clarity ,",
    "each topic is annotated using only its topmost weighted term .",
    "this makes it difficult to assess a visual correspondence between the same topics across the two representations , as the term with top weight may be different for the two term vectors even though the vectors are overall very similar ( in terms of cosine similarity ) . on closer inspection",
    ", many precise correspondences can be established between the topics extracted by the masked ntf method and those extracted by the agglomerative nmf method : for example , the topic _ armistead _ in the top streamgraph matches the topic _ congratulation _ in the bottom one .",
    "an interactive streamgraph visualization of the london 2012 twitter dataset is available at http://www.datainterfaces.org / projects / emoto/.        in order to show the possible correspondence between the extracted topics and sport events , we manually annotate the schedule collected from the official london 2012 olympics page for july 29th , 2012 . as the number of events in a day can be substantial and we want to focus on events with higher impact on social media , we retain events that are either finals or team sports match .",
    "we annotate each event with a set of at most three terms extracted from the schedule , as described in section [ sec : data ] . for a team sport",
    ", we use the sport name and the countries of the two teams , otherwise , we put the name of the sport and its characteristics , e.g. , the discipline for swimming .      for each event , we use a matching criteria to select one of the extracted topics from each of the set of topics produced by the methods . since we want to select a topic in which all event annotated terms appear with a high weight in its term vectors , we define our matching score based on the geometric average of the weights of the event annotated terms in the topic s term vectors : @xmath103{h_{w_1 r } \\ , h_{w_2 r } \\ , \\dots \\ , h_{w_n r}}\\ ] ] for masked ntf , for each event , we choose the topic with the highest corresponding geometric average @xmath104 . in the agglomerative nmf case , for each event , we choose the topic with the highest corresponding geometric average @xmath104 weighted by @xmath105 where @xmath51 is the number of components in the selected cluster .",
    "we use @xmath105 in order to favor the selection of clusters with a higher number of aggregated components , otherwise the most detailed clusters which aggregates only one component are always selected . since the agglomerative nmf method produces a tree structure in which each node agglomerates a set of components and represents topic activity , we have to calculate such matching result for each node , and select the node for which such matching result is the highest .      at this point , we have , for each event , a topic which was selected in each method , and the corresponding matching result . in figure  [ fig : distinf ] , we show the schedule events for the top @xmath102 highest matching results . in the lefthand figure",
    ", we show , for each one of the top @xmath102 matching results , the topic extracted by the masked ntf method , while in the righthand figure , we show the topic extracted by the agglomerative nmf method .",
    "the results are sorted by the corresponding matching weight .    for each event , on its top left corner , we show the manually annotated terms used for the matching . the shaded blue area shows the exact interval during which the event was occurring according to the official olympics schedule . in the same area , the solid green line represents the temporal structure of the topic with higher matching result according to our matching criteria .",
    "such values roughly represent the amount of activity for such topic and are normalized according to the peak of activity .",
    "we show the value for this peak in the top right side , along with the matching results between parenthesis . in the agglomerative nmf graph ( on the right )",
    "we show as a dotted line the activity in time for the given terms regarding the number of tweets that have such terms ( tweet count ) .",
    "we remark that by considering only the dotted line the timing of many events on the right side of the figure does not match the schedule timings , i.e. , merely counting tweets is not sufficient at this resolution level .",
    "we also measured the number of tweets where the terms are co - ocurring , and in this case the number of tweets is so small that it does not allow the detection of any structure in time .",
    "we evaluated these activity profiles using the crowdflower web - based crowdsourcing platform ( restricted to amazon mechanical turk workers ) .",
    "each work unit asks a worker to visually inspect and compare two timelines : the one to be evaluated , and a reference timeline corresponding to the known time intervals for sport events taken from the olympic schedule .",
    "each work unit looks like a row from figure  [ fig : distinf ] .",
    "our evaluation was based on 100 work units evenly distributed among 5 types : 1 ) ( nmf ) work units based on the results of agglomerative nmf ; 2 ) ( cnt - nmf ) work units with activity profiles generated by simply counting the number of tweets with the terms used in matching the nmf topics ; 3 ) ( ntf ) work units from the masked ntf approach ; 4 ) ( cnt - nmf ) same as ( cnt - nmf ) for masked ntf ; 5 ) synthetic work units ( `` gold '' units ) used to assess worker quality . for each work unit",
    ", we asked the workers whether the two timelines matched exactly ( yes ) , matched partially ( partially ) or not at all ( no ) .",
    "95% of the judgments for gold work units were correct .",
    "we only retained those users who correctly judged more than 80% of the gold units .",
    "figure  [ crowdsourced ] shows the distribution of judgements for the different types of work units .",
    "the left hand side of the figure shows the distribution obtained for all work units , while the right hand side shows the distribution restricted to work units with more than 80% of agreement across different workers .",
    "according to this evaluation , both ntf and nmf outperform the count - based methods .",
    "we see that for most of the events there is a close temporal alignment between the event schedule and the topic structure , at the scale of the hour or less .",
    "we see that such temporal alignment is much closer than when compared to the peaks of activity generated by counting tweets .",
    "we observe that the mismatches in the temporal alignment are caused by two different factors .",
    "the first one is due to a low matching results , like the event annotated with ( football , mexico , gabon ) .",
    "it means that the term vectors for the given topic does not represent with high confidence the terms used to annotate the event .",
    "the second one is due to a different behaviour in collective attention .",
    "this happens for example in the case of swimming events , where the first part of the event is related to eliminatories and the second part is related to the finals . in such cases ,",
    "the peak in activity arrives when the event finishes and the attention goes to the winner .",
    "the topic detection techniques we discussed here afford tracking the attention that a community of users devotes to multiple concurrent topics over time , teasing apart social signals that can not be disentangled by simply measuring frequencies of term or hashtags occurrences .",
    "this allows to capture the emergence of topics and to track their popularity with a high temporal resolution and a controllable semantic granularity .",
    "the comparison with an independently available schedule of real - world events shows that the response of twitter to external driving retains a great deal of temporal and topical information about the event schedule , pointing to more sophisticated uses of twitter as a social sensor .",
    "the work described here can be extended along several directions .",
    "it would be interesting to develop and characterize on - line versions of the techniques we used here , so that topic emergence and trend detection could be carried out on live microblog streams . because of its temporal segmentation",
    ", the agglomerative nmf case lends itself rather well to on - line incremental computation , whereas a dynamic version of the masked ntf technique would be more challenging to achieve .    another interesting direction for",
    "future research would be to augment the tweet - term - time tensor with a fourth dimension representing the location of the users , so that the latent signals we extract could expose correlation between topics , time intervals and locations , exposing geographical patterns of collective attention and their relation to delays , e.g. , in the seeding by mass media across different countries .",
    "the authors acknowledge the emoto project www.emoto2012.org and its partners for access to the twitter dataset on the london olympics 2012 . the authors acknowledge inspiring discussions with moritz stefaner and bruno goncalves .",
    "the authors aknowledge partial support from the lagrange project of the isi foundation funded by the crt foundation , from the q - aracne project funded by the fondazione compagnia di san paolo , and from the fet multiplex project ( eu - fet-317532 ) funded by the european commission .",
    "f.  figueiredo , f.  benevenuto , and j.  almeida .",
    "the tube over time : characterizing popularity growth of youtube videos . in _ proc .",
    "conf . on web search and data mining ( wsdm ) _ ,",
    "pages 745754 , 2011",
    ".          j.  kim and h.  park .",
    "fast nonnegative tensor factorization with an active - set - like method . in m.",
    "w. berry , k.  a. gallivan , e.  gallopoulos , a.  grama , b.  philippe , y.  saad , and f.  saied , editors , _ high - performance scientific computing _ , pages 311326 .",
    "springer london , 2012 .",
    "j.  lehmann , b.  gonalves , j.  j. ramasco , and c.  cattuto .",
    "dynamical classes of collective attention in twitter . in _ proc . of the 21st intl . conf . on world",
    "wide web _ , www 12 , pages 251260 , new york , ny , usa , 2012 .",
    "acm .",
    "a.  saha and v.  sindhwani .",
    "learning evolving and emerging topics in social media : a dynamic nmf approach with temporal regularization . in _ proc .",
    "of the fifth acm intl .",
    "conf . on web search and data mining _ , wsdm 12 , pages 693702 , new york , ny , usa , 2012 .",
    "t.  sakaki , m.  okazaki , and y.  matsuo .",
    "earthquake shakes twitter users : real - time event detection by social sensors . in _ proc . of the 19th intl .",
    "conf . on world",
    "wide web _ , www 10 , pages 851860 , new york , ny , usa , 2010 .",
    "a.  shashua and t.  hazan .",
    "non - negative tensor factorization with applications to statistics and computer vision . in _ proc . of the 22nd intl .",
    "conf . on machine learning _ , icml 05 , pages 792799 , new york , ny , usa , 2005 .",
    "j.  sun , d.  tao , and c.  faloutsos . beyond streams and graphs : dynamic tensor analysis . in _ proc . of the 12th acm sigkdd intl .",
    "conf . on knowledge discovery and data mining _ , kdd 06 , pages 374383 , new york , ny , usa , 2006 .",
    "t.  van  de cruys . a non - negative tensor factorization model for selectional preference induction .",
    "in _ proc . of the workshop on geometrical models of natural language semantics _ , gems 09 , pages 8390 , stroudsburg , pa , usa , 2009 .",
    "association for computational linguistics .",
    "y.  wang and e.  agichtein .",
    "temporal latent semantic analysis for collaboratively generated content : preliminary results . in _ proc .",
    "of the 34th intl .",
    "acm sigir conf . on research and development in information retrieval _ , sigir 11 , pages 11451146 , new york",
    ", ny , usa , 2011 ."
  ],
  "abstract_text": [
    "<S> _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ streams of user - generated content in social media exhibit patterns of collective attention across diverse topics , with temporal structures determined both by exogenous factors and endogenous factors . teasing apart different topics and resolving their individual , concurrent </S>",
    "<S> , activity timelines is a key challenge in extracting knowledge from microblog streams . </S>",
    "<S> facing this challenge requires the use of methods that expose latent signals by using term correlations across posts and over time . </S>",
    "<S> here we focus on content posted to twitter during the london 2012 olympics , for which a detailed schedule of events is independently available and can be used for reference . </S>",
    "<S> we mine the temporal structure of topical activity by using two methods based on non - negative matrix factorization . </S>",
    "<S> we show that for events in the olympics schedule that can be semantically matched to twitter topics , the extracted twitter activity timeline closely matches the known timeline from the schedule . </S>",
    "<S> our results show that , given appropriate techniques to detect latent signals , twitter can be used as a social sensor to extract topical - temporal information on real - world events at high temporal resolution . _ </S>",
    "<S> _ _ _ _ </S>",
    "<S> _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ </S>",
    "<S> _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ </S>"
  ]
}