{
  "article_text": [
    "we consider the problem of distributed mean - square - error estimation , where a set of nodes is required to collectively estimate some vector parameter of interest from noisy measurements by relying solely on in - network processing .",
    "we consider an ad - hoc network consisting of @xmath0 nodes that are distributed over some geographic region . at every time instant @xmath1 ,",
    "every node @xmath2 collects a scalar measurement @xmath3 of some random process @xmath4 and a @xmath5 regression vector @xmath6 of some random process @xmath7 with covariance matrix @xmath8 .",
    "the objective is for the nodes in the network to use the collected data @xmath9 to estimate some @xmath10 parameter vector @xmath11 in a distributed manner .",
    "there are a couple of distributed strategies that have been developed in the literature for such purposes .",
    "one typical strategy is the incremental approach @xcite-@xcite , where each node communicates only with one neighbor at a time over a cyclic path . in the incremental strategy ,",
    "information is processed in a cyclic manner across the nodes of the network until optimization is achieved .",
    "however , determining a cyclic path that covers all nodes is an np - hard problem @xcite and , in addition , cyclic trajectories are prone to link and node failures .",
    "when any of the edges along the path fails , the sharing of data through the cycle is interrupted and the algorithm stops performing . to address these difficulties ,",
    "adaptive diffusion techniques were proposed and studied in @xcite . in diffusion implementations ,",
    "the nodes exchange information locally and cooperate with each other without the need for a central processor . in this way",
    ", information is processed on the fly by all nodes and the data diffuse across the network by means of a real - time sharing mechanism .",
    "the resulting adaptive networks exploit the time and spatial - diversity of the data more fully , thus endowing networks with powerful learning and tracking abilities . in view of their robustness and adaptation properties ,",
    "diffusion networks have been applied to model a variety of self - organized behavior encountered in nature , such as birds flying in formation @xcite , fish foraging for food @xcite or bacteria motility @xcite .",
    "diffusion adaptation has also been used to solve dynamic resource allocation problems in cognitive radios @xcite , to perform robust system modeling @xcite , and to implement distributed learning over mixture models in pattern recognition applications @xcite .    in many situations ,",
    "the parameter of interest , @xmath11 , is sparse , containing only a few relatively large coefficients among many negligible ones .",
    "any prior information about the sparsity of @xmath11 can be exploited to help improve the estimation performance , as demonstrated in many recent efforts in the area of compressive sensing ( cs ) @xcite-@xcite .",
    "nevertheless , most cs efforts so far have concentrated on batch recovery methods , where the estimation of the desired vector is achieved from a collection of a fixed number of measurements . in this paper , we are instead interested in adaptive ( online ) techniques that allow the recovery of the vector @xmath11 to be pursued _ both _ recursively and distributively as new data arrive at the nodes .",
    "more importantly , we are interested in schemes that allow the recovery process to track changes in the sparsity pattern of the vector over time .",
    "such schemes are useful in several contexts such as in the analysis of prostate cancer data @xcite , @xcite , spectrum sensing in cognitive radio @xcite , and spectrum estimation in wireless sensor networks @xcite .    some of the early works that mix adaptation with sparsity - aware constructions include methods that rely on the heuristic selection of active taps @xcite-@xcite , and on sequential partial updating techniques @xcite-@xcite ; some other methods assign proportional step - sizes to different taps according to their magnitudes , such as the proportionate normalized lms ( pnlms ) algorithm and its variations @xcite-@xcite . in subsequent studies , motivated by the lasso technique @xcite and by connections with compressive sensing @xcite ,",
    "several algorithms for sparse adaptive filtering have been proposed based on lms @xcite-@xcite , rls @xcite , and projection - based methods @xcite-@xcite .",
    "a couple of distributed algorithms implementing lasso over ad - hoc networks have also been considered before , although their main purpose has been to use the network to solve a _ batch _ processing problem @xcite .",
    "one basic idea in all these previously developed sparsity - aware techniques is to introduce a convex penalty term into the cost function to favor sparsity .",
    "however , these earlier contributions did not consider the design of both _ adaptive _ and _ distributed _ solutions that are able to exploit and track sparsity while at the same time processing data in real - time and in a fully decentralized manner .",
    "doing so would endow networks with learning abilities and would allow them to learn the sparse structure from the incoming data recursively and , therefore , to _ track _ variations in the sparsity pattern of the underlying vector as well .",
    "investigations on adaptive and distributed solutions appear in @xcite,@xcite , and @xcite . in @xcite , we employed diffusion techniques that are able to identify and track sparsity over networks in a distributed manner ; the techniques relied on the use of convenient convex regularization terms . in the related work @xcite ,",
    "the authors employ projection techniques onto hyperslabs and weighted @xmath12-balls to develop a useful sparsity - aware algorithm for distributed learning over diffusion networks . in @xcite ,",
    "the authors use the same formulation of @xcite and the techniques of @xcite-@xcite to independently arrive at useful diffusion strategies except that they limit the function @xmath13 in ( [ glob_cost_function ] ) to choices of the form @xmath14 , for particular selections of @xmath15-vector norms ; they also include the regularization factor into the combination step of their algorithm rather than in the adaptation step , as done further ahead in this work .",
    "the algorithms proposed here and in @xcite are more general in a couple of respects : they allow for broader choices of the regularization function @xmath13 , they allow for sharing of both data and weight estimates among the nodes ( and not only estimates ) by allowing for the use of two sets of combinations weights @xmath16 instead of only one set , and the resulting mean - square and stability analyses are more demanding due to these generalizations ; see , e.g. , appendices a and b. we further use the results of the analysis to propose an adaptive method to adjust online the important regularization parameter @xmath17 in ( [ glob_cost_function ] ) .",
    "this is an important step in order to endow the resulting diffusion strategies with full adaptation abilities : adaptation to nonstationarities in the data and to changes in the sparsity patterns of the data .",
    "the approach we follow in this work is based on developing diffusion strategies that are stable in the mean - square - error sense , with guaranteed performance bounds .",
    "for this reason , a detailed mean - square - error analysis is carried out in order to examine the behavior of the algorithm in the presence of noisy measurements and random regression data .",
    "the analysis ends up suggesting a convenient method for adapting the regularization parameter in a distributed manner as well . doing so , enhances the sparsity - awareness of the algorithm and adds another useful layer of adaptation to the operation of the network . in summary , in this paper",
    "we extend our preliminary work in @xcite to develop adaptive networks running diffusion algorithms subject to constraints that enforce sparsity .",
    "we consider two convex regularization constraints . in one case",
    ", we consider an @xmath12-norm regularization term , which acts as a uniform zero - attractor . in another case , and in order to improve the estimation performance , we employ reweighted regularization to selectively promote sparsity on the zero elements of @xmath11 , rather than uniformly on all the elements .",
    "we provide detailed convergence analysis of the proposed methods , giving a closed form expression for the bias on the estimate due to regularization .",
    "we carry out a mean - square - error analysis , showing the conditions under which the sparse diffusion filter outperforms its unregularized version in terms of steady - state performance .",
    "it turns out that , if the system model is sufficiently sparse , it is possible to tune a single parameter to outperform the standard diffusion algorithm .",
    "then , on the basis of this result , we propose a method to adaptively choose the regularization parameter . in this way , the network is able to learn the sparse structure from the incoming data recursively and to adjust its parameters for improved tracking .",
    "the main contributions of this paper are therefore : ( a ) exploitation of sparsity for distributed estimation over adaptive networks ; ( b ) derivation of the mean - square properties of the sparse diffusion adaptive filter ; ( c ) and adaptation of the regularization parameter to enhance performance under sparsity .",
    "the paper is organized as follows . in section",
    "ii we develop the sparse diffusion algorithm for distributed adaptive estimation .",
    "section iii provides performance analysis , which includes mean stability , mean - square performance and adaptation of the regularization parameter .",
    "section iv provides simulation results in support of the theoretical analysis , and section v draws some conclusions .    *",
    "notation : * we use bold face letters to denote random variables and normal font letters to denote their realizations . matrices and vectors",
    "are respectively denoted by capital and small letters .",
    "we assume the data @xmath18 collected by the various nodes are related to an unknown sparse vector @xmath11 via the linear model : @xmath19 where @xmath20 is a zero mean random variable with variance @xmath21 , independent of @xmath22 for all @xmath23 and @xmath24 , and independent of @xmath25 for @xmath26 and @xmath27 .",
    "linear models of the form ( [ scalar_obs ] ) arise frequently in applications and are able to represent many cases of interest .",
    "the cooperative sparse estimation problem is cast as the search for the optimal estimator that minimizes in a fully distributed manner the following cost function : @xmath28 where @xmath29 denotes the expectation operator , and @xmath30 is a real - valued convex regularization function weighted by the parameter @xmath31 , which is used to enforce sparsity of the solution . the optimization problem in ( [ glob_cost_function ] )",
    "may be solved in a centralized fashion . in this approach ,",
    "the nodes send their data to a central processor , or fusion center , where all computations can be performed .",
    "centralized implementations of this type require transmitting data back and forth between the nodes and the central processor , which translates into requirements of power and bandwidth resources .",
    "additionally , centralized solutions have a serious point of failure : if the central processor fails , then the network operation is adversely affected and operation comes to a halt . for these reasons , we are interested in distributed solutions , where each node communicates with its neighboring nodes , and processing is distributed among all nodes in the network . in this way , communications are localized , and even when individual nodes fail , the network can continue to operate .",
    "we follow the approach proposed in @xcite to derive distributed strategies for the minimization of @xmath32 in ( [ glob_cost_function ] ) .",
    "we start by introducing an @xmath33 matrix @xmath34 with non - negative entries @xmath35 such that @xmath36 where @xmath37 denotes the @xmath38 vector with unit entries and @xmath39 denotes the neighborhood of node @xmath2 .",
    "each coefficient @xmath40 represents a weight value that node @xmath2 assigns to information arriving from its neighbor @xmath23 . of course , the coefficient @xmath40 is equal to zero when nodes @xmath23 and @xmath2 are not connected .",
    "furthermore , each row of @xmath34 adds up to one so that the sum of all weights leaving each node @xmath23 should be one . using the coefficients in ( [ combination_coefficients ] ) , the global cost function in ( [ glob_cost_function ] ) can be expressed as : @xmath41 where @xmath42 the function introduced in ( [ loc_cost_function ] ) is a local ( neighborhood ) cost for node @xmath2 ; it involves a weighted combination of the costs of its neighbors without considering the sparsity constraint . assuming the processes @xmath4 and @xmath7 are jointly wide sense stationary",
    ", the minimization of the local cost function ( [ loc_cost_function ] ) over @xmath43 leads to the optimal local solution : @xmath44 where @xmath45 is assumed positive - definite ( i.e. , @xmath46 ) and @xmath47 , where the operator @xmath48 denotes complex - conjugate transposition .",
    "thus , the local estimate @xmath49 is based solely on local covariance data @xmath50 from the neighborhood of node @xmath2 .",
    "if we multiply both sides of ( [ scalar_obs ] ) by @xmath51 and take expectations and then add over the neighborhood of node @xmath2 , it is easy to verify that the estimate @xmath49 from ( [ loc_solution ] ) agrees with the desired vector @xmath11 . therefore , in principle , each node @xmath2 can estimate @xmath11 _ if _ it knows the moments @xmath52 . often , in practice , these moments are not available and nodes only sense realizations of data arising from these statistical distributions . in that case , cooperation among the nodes can help them improve their estimates of @xmath11 from the data realizations . to motivate the cooperative procedure",
    ", we start by noting that a completion of squares argument shows that ( [ loc_cost_function ] ) can be rewritten in terms of @xmath49 as @xmath53 where mmse is a constant term that does not depend on @xmath43 , the notation @xmath54 , for any nonnegative definite matrix @xmath55 , and @xmath56 thus , using ( [ glob_cost_function2 ] ) , ( [ loc_cost_function ] ) and ( [ loc_cost_function2 ] ) , and dropping the constant mmse terms , we can replace the original global cost ( [ glob_cost_function ] ) with the equivalent cost : @xmath57 expression ( [ glob_cost_function3 ] ) shows how the local cost @xmath58 can be modified to approach the desired global cost ; two correction terms appear on the right - hand side : the regularization term @xmath59 and a sum involving the local estimates @xmath60 .",
    "node @xmath2 can not minimize ( [ glob_cost_function3 ] ) directly .",
    "this is because the cost in ( [ glob_cost_function3 ] ) still requires the nodes to have access to global information , namely , the local estimates @xmath60 , and the matrices @xmath61 , from all other nodes in the network . to enable a distributed and iterative procedure ,",
    "we make three adjustments to ( [ glob_cost_function3 ] ) .",
    "first , we limit the sum over @xmath26 to a sum over the neighbors of node @xmath2 , i.e. , only over @xmath62 .",
    "this step is reasonable since node @xmath2 can only rely on data that are available to it from its neighborhood .",
    "second , we replace the covariance matrices @xmath63 with constant diagonal weighting matrices of the form @xmath64 , where @xmath65 is a set of non - negative real coefficients that give different weights to different neighbors , and @xmath66 is the @xmath67 identity matrix .",
    "although the @xmath61 from its neighbors are available to node @xmath2 , this step is meant to simplify the structure of the resulting algorithm .",
    "this substitution is also reasonable in view of the fact that norms are equivalent and that each of the weighted norms in ( 9 ) can be bounded as @xmath68 substitutions of this kind are common in the stochastic approximation literature where hessian matrices , such as @xmath63 , are replaced by multiples of the identity matrix ; such approximations allow the use of simpler steepest - descent iterations in place of newton - type iterations @xcite . at this stage",
    ", we do not need to worry about the selection of the weights @xmath69 because they are going to be embedded into another set of coefficients that the designer can choose .",
    "finally , while the nodes are attempting to estimate @xmath11 , they do not know what the optimal local estimates @xmath70 are during the iterative learning process .",
    "as the ensuing discussion will reveal , each node in the resulting distributed algorithm will be working on estimating the sparse vector @xmath11 and will have access to a local estimate for @xmath11 , which we denote by @xmath71 at node @xmath23 . due to the diffusion process , this estimate will not be based solely on data from the neighborhood of node @xmath23 but also on data from across the network .",
    "we are therefore motivated to replace @xmath72 in ( [ glob_cost_function3 ] ) by @xmath71 . in this way",
    ", each node @xmath2 can instead minimize the following modified local cost function : @xmath73 the cost in ( [ glob_cost_function4 ] ) is now defined in terms of information that is available to node @xmath2 .",
    "observe that while ( [ glob_cost_function4 ] ) is a local approximation for the global cost ( [ glob_cost_function3 ] ) , it is nevertheless more general than the local cost ( [ loc_cost_function ] ) .",
    "the node @xmath2 can then proceed to optimize ( [ glob_cost_function4 ] ) by means of a steepest - descent procedure .",
    "note that all functions in ( [ glob_cost_function4 ] ) are continuously differentiable except possibly @xmath30 , which is only supposed to be convex .",
    "thus , computing the sub - gradient of ( [ glob_cost_function4 ] ) we obtain @xmath74^*=\\sum_{l\\in\\mathcal{n}_k}c_{l , k}(r_{u , l}w - r_{du , l})+\\sum_{l\\in\\mathcal{n}_k/\\{k\\}}b_{l , k}(w-\\psi_l)+\\gamma \\partial f(w)\\end{aligned}\\ ] ] where @xmath75 is the sub - gradient of the convex function @xmath30 .",
    "then , we can use ( [ subgradient ] ) to obtain a steepest descent recursion for the estimate of @xmath11 at node @xmath2 at time @xmath1 , denoted by @xmath76 , such as @xmath77 for some sufficiently small positive step - sizes @xmath78 .",
    "the update ( [ steepest_descent ] ) involves the sum of three terms and we can compute it in two steps by generating an intermediate estimate @xmath79 , as follows : @xmath80 since every node in the network will be running recursions of the form ( [ steepest_descent2_1])-([steepest_descent2_2 ] ) , then the intermediate estimate of @xmath11 that is available to each node @xmath23 at time @xmath1 is @xmath81 .",
    "therefore , we replace @xmath71 in ( [ steepest_descent2_2 ] ) by @xmath81 .",
    "moreover , since @xmath79 is an updated estimate relative to @xmath82 , as evidenced by ( [ steepest_descent2_1 ] ) , we are motivated to replace @xmath82 in ( [ steepest_descent2_2 ] ) by @xmath79 , which generally leads to enhanced performance since @xmath79 contains more information than @xmath82 .",
    "this step is reminiscent of an incremental - type substitution @xcite-@xcite . performing these substitutions in ( [ steepest_descent2_2 ] ) , we get : @xmath83",
    "if we now introduce the entries of an @xmath33 matrix @xmath84 such that @xmath85 then , we can rewrite the update in ( [ steepest_descent2_1])-([steepest_descent2_2 ] ) as : @xmath86 where the weighting coefficients @xmath16 are real , non - negative and satisfy : @xmath87 the recursion in ( [ steepest_descent3 ] ) requires knowledge of the second - order moments @xmath88 .",
    "an adaptive implementation can be obtained by replacing these second - order moments by local instantaneous approximations , say , of the lms type , as follows : @xmath89 thus , substituting the approximations ( [ inst_approx ] ) into ( [ steepest_descent3 ] ) , we arrive at the following adapt - then - combine ( atc ) strategy .",
    "we refer to the algorithm as the atc sparse diffusion algorithm .",
    "start with @xmath90 for all @xmath2 .",
    "given non - negative real coefficients @xmath16 satisfying ( [ combination_coefficients2 ] ) , for each time @xmath91 and for each node @xmath2 , repeat : @xmath92-\\mu_k\\gamma \\partial f(w_{k , i-1 } ) \\hspace{1 cm } \\hbox{(adaptation step ) } \\\\",
    "w_{k , i}=\\displaystyle\\sum_{l \\in { \\cal n}_k}a_{l , k}\\psi_{l , i } \\hspace{8.85 cm } \\hbox{(diffusion step ) } \\end{cases}\\end{aligned}\\ ] ]    the first step in ( [ atc diffusion ] ) is an adaptation step , where the coefficients @xmath40 determine which nodes @xmath93 should share their measurements @xmath94 with node @xmath2 .",
    "the second step is a diffusion step where the intermediate estimates @xmath81 , from the neighbors @xmath93 , are combined through the coefficients @xmath95 .",
    "we remark that had we reversed the steps ( [ steepest_descent2_1 ] ) and ( [ steepest_descent2_2 ] ) to implement ( [ steepest_descent ] ) , we arrive at a similar but alternative strategy , known as the combine - then - adapt ( cta ) strategy ; in this implementation , the only difference is that data aggregation is performed before adaptation ( see , e.g. , @xcite ) .",
    "start with @xmath90 for all @xmath2 .",
    "given non - negative real coefficients @xmath16 satisfying ( [ combination_coefficients2 ] ) , for each time @xmath91 and for each node @xmath2 , repeat : @xmath96-\\mu_k\\rho \\partial f(\\psi_{k , i-1 } ) \\hspace{1.2 cm } \\hbox{(adaptation step ) } \\end{cases}\\end{aligned}\\ ] ]    the complexity of the sparse diffusion schemes in ( [ atc diffusion])-([cta diffusion ] ) is @xmath97 , which is the same complexity as standard stand - alone lms adaptation .",
    "it was argued in @xcite that atc strategies generally outperform cta strategies .",
    "for this reason , we continue our discussion by focusing on the atc algorithm ( [ atc diffusion ] ) ; similar analysis applies to cta .    compared with the strategies proposed in @xcite and @xcite , the diffusion algorithm ( [ atc diffusion ] )",
    "exploits data in the neighborhood more fully ; the adaptation step aggregates data @xmath98 from the neighbors , and the diffusion step aggregates estimates @xmath99 from the same neighbors .",
    "the implementation in @xcite uses a different algorithmic structure with @xmath100 so that data @xmath98 from the neighbors are not directly used .",
    "compared with @xcite , observe that the effect of the regularization factor in ( [ atc diffusion ] ) influences the adaptation step , and not the combination step as in @xcite .",
    "observe also that the adaptation step allows for the exchange of data @xmath94 among the nodes through the use of the coefficients @xmath35 , whereas @xcite uses @xmath100 as well .      before proceeding with the discussions ,",
    "let us comment on the regularization function @xmath30 in ( [ glob_cost_function ] ) .",
    "a sparse vector @xmath11 generally contains only a few relatively large coefficients interspersed among many negligible ones and the location of the non - zero elements is often unknown beforehand .",
    "however , in some applications , we may have available some upper bound on the number of nonzero elements .",
    "thus , assume that @xmath11 satisfies @xmath101 where @xmath102 is the @xmath103-norm , denoting the number of non - zero entries of a vector , and @xmath104 is a known upper bound . since the @xmath103-norm in ( [ l0norm ] ) is not convex , we can not use it directly .",
    "thus , motivated by lasso @xcite and work on compressive sensing @xcite , we first consider the following @xmath12-norm convex choice for a regularization function : @xmath105 which amounts to the sum of the absolute entries of the vectors .",
    "the @xmath12-norm works as a surrogate approximation for the @xmath103-norm .",
    "this choice leads to an algorithm update in ( [ atc diffusion ] ) where the subgradient column vector is given by @xmath106 and the entries of the vector @xmath107 are obtained by applying the following function to each entry of @xmath43 : @xmath108 this update leads to what we shall refer to as the _ zero - attracting _",
    "( za ) diffusion algorithm .",
    "the za update uniformly shrinks all components of the vector , and does not distinguish between zero and non - zero elements @xcite . since all the elements are forced toward zero uniformly , the performance would deteriorate for systems that are not sufficiently sparse .",
    "motivated by the idea of reweighting in compressive sampling @xcite,@xcite , we also consider the following approximation : @xmath109 which , for very small positive values of @xmath110 , is a better approximation for the @xmath103-norm of a vector @xmath43 than the @xmath12-norm @xcite , thus enhancing sparse recovery by the algorithm .",
    "therefore , interpreting ( [ approx_l0norm ] ) as a weighted @xmath12-norm regularization , to update the algorithm in ( [ atc diffusion ] ) , we shall consider the following sub - gradient column vector : @xmath111 this choice leads to what we shall refer to as the _ reweighted zero - attracting _",
    "( rza ) diffusion algorithm .",
    "the update in ( [ update_rew_l1norm ] ) selectively shrinks only the components whose magnitudes are comparable to @xmath110 , and there is little effect on components satisfying @xmath112 , see , e.g. , @xcite .",
    "from now on , we view the estimates @xmath76 as realizations of a random process @xmath113 and analyze the performance of the sparse diffusion algorithm in terms of its mean - square behavior .",
    "to do so , we introduce the error quantities @xmath114 , @xmath115 , and the network vectors : @xmath116 we also introduce the block diagonal matrix @xmath117 and the extended block weighting matrices @xmath118 where @xmath119 denotes the kronecker product operation .",
    "we further introduce the random block quantities : @xmath120 then , we conclude from ( [ atc diffusion ] ) that the following relations hold for the error vectors : @xmath121+\\gamma\\mathcal{m } \\partial f(\\bw_{i-1})\\label{psi}\\\\ \\tilde{\\bw}_{i}&=&\\mathcal{a}^t\\tilde{\\boldsymbol{\\psi}}_{i } \\label{dabliu}\\end{aligned}\\ ] ] where @xmath122 we can combine ( [ psi ] ) and ( [ dabliu ] ) into a single recursion : @xmath123\\tilde{\\bw}_{i-1}-\\mathcal{a}^t\\mathcal{m}\\bg_i+\\gamma\\mathcal{a}^t\\mathcal{m}\\partial f(\\bw_{i-1})}\\end{aligned}\\ ] ] this relation tells us how the network weight - error vector evolves over time .",
    "the relation will be the launching point for our mean - square analysis .",
    "to proceed , we introduce the following independence assumption on the regression data .",
    "* assumption 1 ( independent regressors ) * _ the regressors @xmath7 are temporally white and spatially independent with @xmath124 .",
    "_    ' '' ''    it follows from assumption 1 that @xmath7 is independent of @xmath125 for all @xmath23 and @xmath126 .",
    "although not true in general , this assumption is common in the adaptive filtering literature since it helps simplify the analysis .",
    "several studies in the literature , especially on stochastic approximation theory @xcite@xcite , indicate that the performance expressions obtained using this assumption match well the actual performance of stand - alone filters for sufficiently small step - sizes .",
    "therefore , we shall also rely on the following condition .",
    "* assumption 2 ( small step - sizes ) * _ the step - sizes @xmath78 are sufficiently small so that terms that depend on higher - order powers of @xmath127 can be ignored . _    ' '' ''",
    "let @xmath128 then , taking expectations of both sides of ( [ compact_diffusion ] ) and calling upon assumption 1 , we conclude that the mean - error vector evolves according to the following dynamics : @xmath129\\mathbb{e}\\tilde{\\bw}_{i-1}+\\gamma\\mathcal{a}^t\\mathcal{m}\\mathbb{e}\\partial f(\\bw_{i-1})}\\end{aligned}\\ ] ] the following theorem guarantees the asymptotic mean stability of the diffusion strategy ( [ atc diffusion ] ) , and provides a closed form expression for the weight bias due to the use of the regularization term .",
    "* theorem 1 ( stability in the mean ) * _ assume data model ( [ scalar_obs ] ) and assumption 1 hold .",
    "then , for any initial condition and any choice of the matrices @xmath130 and @xmath34 satisfying ( [ combination_coefficients2 ] ) , the diffusion strategy ( [ atc diffusion ] ) asymptotically converges in the mean if the step - sizes are chosen to satisfy : @xmath131 where @xmath132 denotes the maximum eigenvalue of a hermitian positive semi - definite matrix @xmath133 .",
    "furthermore , as @xmath134 , the estimators across all nodes have biases that are given by the corresponding entries in the following bias vector : @xmath135 where @xmath136^{-1}{\\cal a}^t{\\cal m}.\\end{aligned}\\ ] ] moreover , it holds that @xmath137 where @xmath138 is the block maximum norm of a vector ( defined in appendix a ) , @xmath139 , @xmath140 and @xmath141 , with @xmath142 denoting the spectral radius of a matrix @xmath133 .",
    "_    see appendix a.      we now examine the behavior of the steady - state mean - square deviation , @xmath143 as @xmath134 , for any of the nodes and derive conditions under which the sparse diffusion filter outperforms its unregularized version in terms of steady - state performance .",
    "in particular , we will show that , if the vector parameter @xmath11 is sufficiently sparse , then it is possible to tune the sparsity parameter @xmath17 to achieve better performance than the standard diffusion algorithm . following the energy conservation framework of @xcite and under assumption 1 , we can establish the following variance relation : @xmath144 + 2\\gamma\\mathbb{e}\\partial f(\\bw_{i-1})^t\\mathcal{m}\\mathcal{a}\\sigma\\mathcal{a}^t\\left(i-{\\cal m}\\mathcal{d}\\right)\\tilde{\\bw}_{i-1}\\nonumber\\\\ & + & \\gamma^2\\mathbb{e}\\|\\partial f(\\bw_{i-1})\\|^2_{\\mathcal{m}\\mathcal{a}\\sigma\\mathcal{a}^t\\mathcal{m}}\\end{aligned}\\ ] ] where @xmath55 is any hermitian nonnegative - definite matrix that we are free to choose , and @xmath145 relations ( [ weighted_norm_expanded])-([sigma ] ) can be derived directly from ( [ compact_diffusion ] ) if we compute the weighted norm of both sides of the equality and use the fact that @xmath146 is independent of @xmath147 and @xmath148 .",
    "we can rewrite ( [ weighted_norm_expanded ] ) more compactly if we collect the terms depending on @xmath17 as @xmath149+\\phi_{\\sigma , i}(\\gamma)\\end{aligned}\\ ] ] with @xmath150 moreover , setting @xmath151={\\cal c}^t\\cdot{\\rm diag}\\{\\sigma_{v,1}^2r_{u,1},\\ldots,\\sigma_{v , n}^2r_{u , n}\\}\\cdot{\\cal c}\\end{aligned}\\ ] ] we can rewrite ( [ weighted_norm ] ) in the form @xmath152+\\phi_{\\sigma , i}(\\gamma)\\end{aligned}\\ ] ] where @xmath153 denotes the trace operator .",
    "let @xmath154 and @xmath155 , where the @xmath156 notation stacks the columns of @xmath55 on top of each other and @xmath157 is the inverse operation .",
    "we will use interchangeably the notation @xmath158 and @xmath159 to denote the same quantity @xmath160 . using the kronecker product property @xmath161",
    ", we can vectorize both sides of ( [ sigma ] ) and conclude that ( [ sigma ] ) can be replaced by the simpler linear vector relation : @xmath162 , where @xmath163 is the following @xmath164 matrix with block entries of size @xmath165 each : @xmath166 using the property @xmath167 we can then rewrite ( [ weighted_norm2 ] ) as follows : @xmath168^t\\sigma+\\phi_{\\sigma , i}(\\gamma)\\end{aligned}\\ ] ] the following theorem guarantees the asymptotic mean - square stability ( i.e. , convergence in the mean and mean - square sense ) of the diffusion strategy ( [ atc diffusion ] ) .",
    "* theorem 2 ( mean - square stability ) * _ assume the data model ( [ scalar_obs ] ) and assumption 1 hold .",
    "then , the sparse diffusion lms algorithm ( [ atc diffusion ] ) will be mean - square stable if the step - sizes are sufficiently small and satisfy ( [ step_sizes ] ) , and the matrix @xmath163 in ( [ matrix_f ] ) is stable . _    see appendix b.    * remark 1 * : note that the step sizes influence ( [ matrix_f ] ) through the matrix @xmath169 .",
    "since the step - sizes are sufficiently small , we can ignore terms that depend on higher - order powers of the step - sizes .",
    "then , we can approximate ( [ matrix_f ] ) as @xmath170 where @xmath171 . now , since @xmath172 is left - stochastic , it can be verified that the above @xmath163 is stable if @xmath173 is stable @xcite ; this latter condition is guaranteed by ( [ step_sizes ] ) . in summary ,",
    "sufficiently small step - sizes ensure the stability of the diffusion strategy in the mean and mean - square senses .    ' '' ''    taking the limit as @xmath174 ( assuming the step - sizes are small enough to ensure convergence to a steady - state ) , we deduce from ( [ weighted_norm3 ] ) that : @xmath175^t\\sigma + \\gamma\\beta_{\\sigma,\\infty}\\left(\\gamma-\\frac{\\alpha_{\\sigma,\\infty}}{\\beta_{\\sigma,\\infty}}\\right)}\\end{aligned}\\ ] ] where @xmath176 the limits in ( [ limits1])-([limits2 ] ) exist .",
    "indeed , first , in appendix c we show that @xmath177 converges to @xmath178 .",
    "second , we also show in appendix b that the lhs of ( [ variance_relation2 ] ) converges",
    ". therefore , the term @xmath179 also exists .",
    "expression ( [ variance_relation2 ] ) is a useful result : it allows us to derive several performance metrics through the proper selection of the free weighting parameter @xmath180 ( or @xmath55 ) , as was done in @xcite .",
    "for example , the msd for any node @xmath2 is defined as the steady - state value @xmath143 , as @xmath134 , and can be obtained by computing @xmath181 with a block weighting matrix @xmath182 that has the @xmath67 identity matrix at block @xmath183 and zeros elsewhere .",
    "then , denoting the vectorized version of the matrix @xmath182 by @xmath184 , where @xmath185 is the vector whose @xmath2-th entry is one and zeros elsewhere , and if we select @xmath180 in ( [ variance_relation2 ] ) as @xmath186 , we arrive at the msd for node @xmath2 : @xmath187^t(i-{\\cal f})^{-1}t_k + \\gamma\\beta_{\\sigma_k,\\infty}\\left(\\gamma-\\frac{\\alpha_{\\sigma_k,\\infty}}{\\beta_{\\sigma_k,\\infty}}\\right)}\\end{aligned}\\ ] ] the average network @xmath188 is given by : @xmath189 then , to obtain the network msd from ( [ variance_relation2 ] ) , the weighting matrix of @xmath190 should be chosen as @xmath191 .",
    "let @xmath192 denote the vectorized version of @xmath193 , i.e. , @xmath194 , and selecting @xmath180 in ( [ variance_relation2 ] ) as @xmath195 , the network msd is given by : @xmath196^t(i-{\\cal f})^{-1}q + \\frac{1}{n}\\gamma\\beta_{\\sigma,\\infty}\\left(\\gamma-\\frac{\\alpha_{\\sigma,\\infty}}{\\beta_{\\sigma,\\infty}}\\right)}\\end{aligned}\\ ] ]      we now examine under what conditions the sparse diffusion filter ( [ atc diffusion ] ) dominates in terms of mean - square performance its unregularized counterpart when @xmath197 . considering the msd expression ( [ msd_k ] ) at the @xmath2-th node , we notice that the first term on the rhs coincides with the msd of the standard diffusion algorithm when @xmath197 ( compare with ( 48 ) in @xcite ) , whereas the second term in ( [ msd_k ] ) is due to the regularization . then ,",
    "if @xmath198 the second term on the rhs of ( [ msd_k ] ) is negative and sparse diffusion would outperform standard diffusion .",
    "the condition @xmath199 , where @xmath177 is given by ( [ alpha ] ) , is a _ necessary _ condition to have dominance of sparse diffusion over standard diffusion .",
    "let us examine an interpretation for the condition @xmath199 in terms of the sparsity of the vector @xmath11 .",
    "since @xmath13 is a real - valued convex function , by the definition of subgradient it holds that @xmath200 then , choosing @xmath201 and @xmath202 , where @xmath203 , we get @xmath204\\end{aligned}\\ ] ] if the step - sizes are sufficiently small , we can approximate @xmath205 , neglecting the second term that depends on @xmath206 . then , we have @xmath207 at convergence , the vector @xmath148 fluctuates close to @xmath208 .",
    "now , since @xmath209 , expression ( [ bar_w_i ] ) can be interpreted as a gradient descent update minimizing the function @xmath210 , yielding for small step - sizes a vector @xmath211 that is closer to @xmath208 than @xmath148 .",
    "if @xmath208 is sparse , the non - zero elements ( nz set ) of the vector are in general much less in number than the zero elements ( z set ) .",
    "then , the gradient update in ( [ bar_w_i ] ) helps move the components of the vector @xmath211 that belong to the z set closer to zero . intuitively ,",
    "if the z set is larger than the nz set , @xmath211 will be more sparse than @xmath148 .",
    "thus , considering ( [ asigma ] ) at convergence , since the function @xmath30 measures the sparsity of the vector @xmath43 , it is expected that @xmath212 > 0\\end{aligned}\\ ] ] since @xmath213 is likely to be more sparse than @xmath148 .",
    "consequently , the condition @xmath214 is likely to be true .",
    "therefore , by properly selecting the sparsity coefficient @xmath17 to satisfy ( [ dominance_cond ] ) , the sparse diffusion algorithm will yield better msd than the standard diffusion algorithm at each node . on the other hand ,",
    "if @xmath11 is not sparse , condition ( [ asigma ] ) in general would not be true and the sparse diffusion algorithm will perform worse than standard diffusion .      to endow networks with the capability to adaptively exploit and track the sparsity of the system model ,",
    "we now propose a systematic approach to choosing the regularization parameter @xmath17 in an adaptive fashion .",
    "we thus allow the sparsity parameter to be iteration dependent , i.e. , @xmath215 .",
    "following similar steps as in section iii.b , we can replace ( [ weighted_norm2 ] ) with the conditional relation : @xmath216=\\|\\tilde{w}_{i-1}\\|^2_{\\sigma'}+{\\rm tr}[\\sigma \\mathcal{a}^t\\mathcal{m}{\\cal g}^t\\mathcal{m}\\mathcal{a}]+ \\phi_{\\sigma , i}(\\gamma_i)\\end{aligned}\\ ] ] where @xmath217 is given by ( [ sigma ] ) and @xmath218\\tilde{w}_{i-1}\\label{alpha_cond}\\end{aligned}\\ ] ] thus , letting @xmath219 and if @xmath220 , the sparse diffusion algorithm will outperform the standard diffusion algorithm in terms of the instantaneous msd .",
    "the condition @xmath220 is satisfied when @xmath221 since @xmath222 in ( [ instant_phi ] ) is quadratic in @xmath223 , we can choose the optimal parameter that minimizes ( [ instant_phi ] ) as : @xmath224 now , exploiting the small step - sizes assumption in ( [ alpha_cond ] ) , we consider the following approximation : @xmath225 an approximate expression for the sparsity parameter in ( [ opt_rho ] ) is then given by : @xmath226    * remark 2 * : the rule ( [ opt_rho_network ] ) can not be directly used due to the presence of the true parameter vector @xmath11 in @xmath227 , which is unknown to the nodes in the network .",
    "furthermore , the update ( [ opt_rho_network ] ) depends on data coming from all nodes . however , in the sequel we propose some useful approximations that allow the local computation of the regularization parameter .    ' '' ''    first , we notice that the regularization parameter ( [ opt_rho_network ] ) depends on the combination matrix @xmath228 , which influences how the nodes perform the combination step in ( [ atc diffusion ] ) .",
    "this step helps improve the quality of the node s estimate @xmath76 by reducing the effect of the measurement and gradient noises but , it generally has a marginal effect on the sparse recovery capability of the algorithm .",
    "the regularization function appears instead inside the adaptation step in ( [ atc diffusion ] ) .",
    "thus , to simplify expression ( [ opt_rho_network ] ) , we consider the case in which we want to select @xmath223 under the condition that @xmath229 , i.e. , no cooperation is performed among the nodes . in this case , the following relations hold : @xmath230 let @xmath231 and @xmath232",
    ". using ( [ subgrad_ineq ] ) , we find that @xmath233\\label{alpha_cond_approx}\\end{aligned}\\ ] ] in practice , some prior knowledge about the sparsity of the true vector @xmath11 is often available .",
    "for example , the @xmath12-norm of @xmath11 can be upper bounded by some constant value @xcite . in this work",
    ", we assume that @xmath234 for some given positive constant @xmath235 . using ( [ eta ] ) in ( [ alpha_cond_approx2 ] ) , we get @xmath236\\end{aligned}\\ ] ] and , using ( [ beta_cond_approx ] ) and ( [ alpha_cond_approx3 ] ) , the regularization parameter in ( [ opt_rho ] ) can instead be approximated as : @xmath237}{\\sum_{k=1}^n \\mu_k^2 \\|\\partial f(w_{k , i-1})\\|^2}\\right\\}\\end{aligned}\\ ] ]    * remark 3 * : the update ( [ opt_rho2 ] ) still depends on data coming from all nodes in the network",
    ". however , we can replace ( [ opt_rho2 ] ) with a local rule where each node computes its own @xmath238 from data received from its neighbors only , say , @xmath239}{\\sum_{l\\in{\\cal n}_k } \\mu_l^2 \\|\\partial f(w_{l , i-1})\\|^2}\\right\\}\\end{aligned}\\ ] ] in the simulation section , we will check the performance of the sparse diffusion strategy using ( [ opt_rho4 ] ) .    ' '' ''    we summarize below the sparse diffusion strategy with adaptive regularization .",
    "the complexity of this strategy is @xmath240 , which is the same complexity as standard stand - alone lms adaptation .",
    "start with @xmath90 for all @xmath2 .",
    "given non - negative real coefficients @xmath16 satisfying ( [ combination_coefficients2 ] ) , for each time @xmath91 and for each node @xmath2 , repeat : @xmath241}{\\sum_{l\\in{\\cal n}_k } \\mu_l^2 \\|\\partial f(w_{l , i-1})\\|^2}\\right\\ }   \\hspace{5.2 cm } \\hbox{(sparsity control ) }   \\\\",
    "\\psi_{k , i}=w_{k , i-1}+\\mu_k \\displaystyle \\sum_{l\\in \\mathcal{n}_k}c_{l , k } u^*_{l , i}[d_{l}(i)-u_{l , i}w_{k , i-1}]-\\mu_k\\gamma^o_{k , i } \\partial f(w_{k , i-1 } ) \\hspace{1 cm } \\hbox{(adaptation step ) } \\\\",
    "w_{k , i}=\\displaystyle\\sum_{l \\in { \\cal n}_k}a_{l , k}\\psi_{l , i } \\hspace{9.2 cm } \\hbox{(diffusion step ) } \\end{cases}\\end{aligned}\\ ] ]    * remark 4 * : equation ( [ opt_rho4 ] ) indicates that , in order to ensure superiority of the sparse diffusion strategy , the construction ( [ opt_rho4 ] ) is triggered only if @xmath242>0 $ ] , otherwise , @xmath243 .",
    "the performance of the sparse diffusion strategy depends on how close the upper bound @xmath235 is to the right value . in the simulation section",
    ", we will check the robustness of the regularized diffusion algorithm to misspecified values of @xmath235 .    ' '' ''",
    "in this section , we provide some numerical examples to illustrate the performance of the sparse diffusion algorithm . in the first example",
    ", we compare the performance of the sparse diffusion strategy with respect to standard diffusion , considering fixed values of the regularization parameter @xmath17 .",
    "the second example shows the benefits of adapting the sparsity parameter according to ( [ opt_rho4 ] ) .",
    "_ numerical example 1 : performance : _ we consider a connected network composed of 20 nodes .",
    "the topology of the network is shown in fig .",
    "the regressors @xmath7 have size @xmath244 and are zero - mean white gaussian distributed with covariance matrices @xmath245 , with @xmath246 shown on the top right side of fig .",
    "the background white noise power @xmath246 of each node is depicted on the bottom right side of fig .",
    "the first example aims to show the tracking and steady - state performance for the sparse diffusion algorithm . in fig .",
    "[ comparison ] , we report the learning curves in terms of network msd for 6 different adaptive filters : atc diffusion lms @xcite , za - atc diffusion described by ( [ atc diffusion ] ) and ( [ sign ] ) and rza - atc diffusion described by ( [ atc diffusion ] ) and ( [ update_rew_l1norm ] ) , and the non - cooperative approach from @xcite .    ) -([sign ] ) , rza - atc given by ( [ atc diffusion])-([update_rew_l1norm ] ) . ]    the simulations use a value of @xmath247 and the results are averaged over 100 independent experiments .",
    "the sparsity parameters are set to @xmath248 for za - lms , @xmath249 for rza - lms , @xmath250 for za - atc , @xmath251 for rza - atc , and @xmath252 . in this simulation",
    ", we consider diffusion algorithms without measurement exchange , i.e. , @xmath100 , and a combination matrix @xmath130 that simply averages the estimates from the neighborhood such that @xmath253 for all @xmath23 .",
    "initially , only one of the 50 elements of @xmath11 is set equal to one while the others are equal to zero , making the system very sparse .",
    "after 1000 iterations , 25 elements are randomly selected and set equal to 1 , making the system have a sparsity ratio of @xmath254 .",
    "after 2000 iterations , all the elements are set equal to 1 , leaving a completely non - sparse system . as we see from fig .",
    "[ comparison ] , when the system is very sparse both za - atc and rza - atc yield better steady - state performance than standard diffusion .",
    "the rza - atc outperforms za - atc thanks to reweighted regularization .",
    "when the vector @xmath11 is only half sparse , the performance of za - atc deteriorates , performing worse than standard diffusion , while rza - atc has the best performance among the three diffusion filters .",
    "when the system is completely non - sparse , the rza - atc still performs comparably to the standard diffusion filter .",
    "we also notice the gain of diffusion schemes with respect to the non - cooperative approaches from @xcite .     for za - atc diffusion lms ( left ) and for rza - atc diffusion lms ( right ) , for different degrees of system sparsity.,title=\"fig : \" ]   for za - atc diffusion lms ( left ) and for rza - atc diffusion lms ( right ) , for different degrees of system sparsity.,title=\"fig : \" ]    the theoretical derivations in section iii showed that it is possible to select the regularization parameter @xmath17 in order to have dominance in terms of msd of the atc - sd filter with respect to the unregularized diffusion algorithm .",
    "to quantify the effect of the sparsity parameter @xmath17 on the performance of the atc - sd filters with respect to different degrees of system sparsity , we consider two additional examples . in fig .",
    "[ dmsd_za ] ( left ) , we show the behavior of the difference ( in db ) between the network msd of atc - za and standard diffusion versus @xmath17 , for different sparsity degrees of @xmath11 .",
    "we consider the same settings of the previous simulation and the results are averaged over 100 independent experiments and over 100 samples after convergence . as we can see from fig .",
    "[ dmsd_za ] ( left ) , reducing the sparsity of @xmath11 , the interval of @xmath17 values that yields a gain for atc - za with respect to standard diffusion becomes smaller , until it reduces to zero when the system is not sparse enough .",
    "different update functions may affect differently the steady - state performance of the atc - sd algorithm .",
    "thus , in fig . [ dmsd_za ] ( right )",
    ", we repeat the same experiment considering the atc - rza algorithm . as we can see , thanks to the reweighted regularization in ( [ update_rew_l1norm ] ) , atc - rza gives better performance than za - atc and yields a performance loss with respect to standard diffusion , for any @xmath17 , only when the vector @xmath11 is completely non - sparse .",
    "finally , we compare our proposed sparse diffusion schemes with the sparsity promoting adaptive algorithms for distributed learning recently proposed in @xcite and in @xcite . at the best of our knowledge , the works in @xcite and @xcite are the only two present in the literature that exploit sparsity processing data both in an adaptive and distributed fashion .    ) and ( [ sign ] ) , rza - atc described by ( [ atc diffusion ] ) and ( [ update_rew_l1norm ] ) , and the sparse diffusion algorithms from @xcite .",
    "( right ) transient network msd for the diffusion techniques",
    "atc @xcite , za - atc described by ( [ atc diffusion ] ) and ( [ sign ] ) , rza - atc described by ( [ atc diffusion ] ) and ( [ update_rew_l1norm ] ) , and the projection based distributed learning technique from @xcite.,title=\"fig : \" ] ) and ( [ sign ] ) , rza - atc described by ( [ atc diffusion ] ) and ( [ update_rew_l1norm ] ) , and the sparse diffusion algorithms from @xcite .",
    "( right ) transient network msd for the diffusion techniques atc @xcite , za - atc described by ( [ atc diffusion ] ) and ( [ sign ] ) , rza - atc described by ( [ atc diffusion ] ) and ( [ update_rew_l1norm ] ) , and the projection based distributed learning technique from @xcite.,title=\"fig : \" ]    in fig .",
    "[ comparison3 ] ( left ) , we compare the steady - state performance , averaged over 100 independent simulations , of five adaptive filters : atc diffusion lms @xcite , za - atc diffusion described by ( [ atc diffusion ] ) and ( [ sign ] ) , rza - atc diffusion described by ( [ atc diffusion ] ) and ( [ update_rew_l1norm ] ) , the atc @xmath12-lms and the atc @xmath12-rwlms algorithms from @xcite . we consider a vector parameter @xmath11 with only 5 elements set equal to one , which have been randomly chosen , leading to a sparsity ratio of 5/50 .",
    "the sparsity parameters are set to @xmath250 for za - atc , @xmath255 for rza - atc , and @xmath252 , for both our methods and the algorithms from @xcite .",
    "the other settings are the same of the previous simulation , except that in this simulation the combination coefficients in ( [ atc diffusion ] ) are chosen as @xmath256 for all @xmath23 , thus leading to za - atc and rza - atc diffusion algorithms with measurement exchange . as we can notice from fig .",
    "[ comparison3 ] ( left ) , the proposed methods outperform the algorithms from @xcite in terms of steady - state msd . in fig .",
    "[ comparison3 ] ( right ) , we compare the transient network msd of four adaptive filters : atc diffusion lms @xcite , za - atc diffusion described by ( [ atc diffusion ] ) and ( [ sign ] ) , rza - atc diffusion described by ( [ atc diffusion ] ) and ( [ update_rew_l1norm ] ) , and the projection based sparse learning from @xcite .",
    "the settings of the za - atc and rza - atc diffusion algorithms are the same of the previous simulation , whereas the parameters of the algorithm from @xcite are chosen in order to have similar steady - state msd with respect to the rza - atc diffusion method .",
    "using the same notation adopted in @xcite , the parameters of the projection based filter are : @xmath257 ; @xmath258 ; the radius of the weighted @xmath12 ball is equal to @xmath259 ( i.e. , the correct sparsity level ) ; @xmath260 ; @xmath261 for @xmath262 and @xmath263 for @xmath264 ; the number of hyperslabs used per time update equals to @xmath265 . from fig .",
    "[ comparison3 ] ( right ) , it is possible to notice how the projected based method has a larger convergence rate with respect to the rza atc diffusion method .",
    "this positive feature is paid in terms of computational complexity .",
    "indeed , while our methods have an lms type complexity @xmath97 , the projection - based method from @xcite has a complexity equal to @xmath266 , due to the presence of @xmath192 projections onto the hyperslabs and 1 projection on the weighted @xmath12 ball per iteration .",
    "_ numerical example 2 - adaptation of the regularization parameter : _ in this example , we consider the same network shown in fig .",
    "[ net ] and the same setting of the previous simulation for the regression data and additive noise .",
    "the first example aims to show the tracking and steady - state performance of the atc - sd algorithm with adaptive regularization . in fig .",
    "[ comparison2 ] ( left ) , we report the learning curves in terms of network msd for 3 different adaptive filters : atc diffusion lms @xcite , za - atc diffusion described by ( [ atc diffusion ] ) and ( [ sign ] ) ) and rza - atc diffusion described by ( [ atc diffusion ] ) and ( [ update_rew_l1norm ] ) , when the regularization parameter @xmath223 is chosen locally at each node according to the adaptive rule ( [ opt_rho4 ] ) .",
    "the simulations use a value of @xmath247 and the results are averaged over 100 independent experiments .",
    "the approximation parameter for rza - atc diffusion in ( [ update_rew_l1norm ] ) is chosen equal to @xmath252 .    ) and ( [ sign ] ) , rza - atc described by ( [ atc diffusion ] ) and ( [ update_rew_l1norm ] ) with adaptive selection of the regularization parameter @xmath223 .",
    "( right ) temporal behavior of the regularization parameter @xmath223 evaluated through the adaptive relation ( [ opt_rho4 ] ) for za - atc diffusion ( solid ) and rza - atc diffusion ( dashed).,title=\"fig : \" ] ) and ( [ sign ] ) , rza - atc described by ( [ atc diffusion ] ) and ( [ update_rew_l1norm ] ) with adaptive selection of the regularization parameter @xmath223 .",
    "( right ) temporal behavior of the regularization parameter @xmath223 evaluated through the adaptive relation ( [ opt_rho4 ] ) for za - atc diffusion ( solid ) and rza - atc diffusion ( dashed).,title=\"fig : \" ]    . ]",
    "initially , only one of the 50 elements of @xmath11 is set equal to one while the others are equal to zero , making the system very sparse .",
    "after 1000 iterations , 5 elements are randomly selected and set equal to 1 , making the system have a sparsity ratio of @xmath267 .",
    "after 2000 iterations , all the elements are set equal to 1 , leaving a completely non - sparse system .",
    "the upper bound @xmath235 in ( [ eta ] ) , used to evaluate the sparsity parameter in ( [ opt_rho4 ] ) , is set to @xmath268 and varies in time according to the different choices of @xmath11 . as we can see from fig .",
    "[ comparison2 ] ( left ) , when the system is very sparse both za - atc and rza - atc yield better steady - state performance than standard diffusion .",
    "the rza - atc outperforms za - atc thanks to the reweighted regularization .",
    "when the vector @xmath11 is less sparse , the performance of za - atc deteriorates , getting closer to standard diffusion , while rza - atc still guarantees a large gain .",
    "when the system is completely non - sparse , the three filters have the same performance . to see the effect of different sparsity ratios of the vector @xmath11 on the choice of the regularization , in fig .",
    "[ comparison2 ] ( right ) we show the average behavior of the parameter @xmath269 evaluated according to ( [ opt_rho4 ] ) , for za - atc diffusion and rza - atc diffusion , averaged across nodes over 100 independent realizations .",
    "as we can see , the system reacts to different sparsity ratios of the vector @xmath11 , adjusting accordingly the regularization parameter @xmath269 in order to improve the performance of the atc - sd strategy with respect to the unregularized algorithm . from fig .",
    "[ comparison2 ] ( right ) , it is interesting to note how the regularization parameter converges close to the minimum of the differential msd plotted in fig .",
    "[ dmsd_za ] for both za - atc and rza - atc .",
    "in particular , @xmath269 is forced to zero when the vector @xmath11 is totally non - sparse , leading to the same performance of the standard diffusion algorithm .    since the adaptive update of the sparsity parameter @xmath270 in ( [ opt_rho4 ] ) depends on the selection of the trigger @xmath235 , which depends on some available prior knowledge on the sparsity level of @xmath11 , it is important to check the sensitivity of the atc - sd algorithm to misspecified values of @xmath235",
    ". thus , in fig . [ msd_error ] , we report the average behavior of the msd , for za - atc diffusion and rza - atc diffusion , versus a percentual error on the specification of the true trigger value @xmath235 .",
    "the settings are the same of the previous simulation and the results are averaged over 100 independent experiments and over 100 samples after convergence .",
    "we consider a vector parameter @xmath11 with only 5 elements set equal to one , which have been randomly chosen , leading to a sparsity ratio of 5/50 . in this case , the true value for the trigger parameter @xmath235 would be equal to @xmath271 .",
    "the regularization parameter @xmath223 is chosen locally at each node according to the adaptive rule ( [ opt_rho4 ] ) . as we can notice from fig .",
    "[ msd_error ] , the za - atc diffusion algorithm is very sensitive to misspecified values of @xmath235 , especially in the case of under - estimation of the trigger parameter .",
    "indeed , by under - estimating the value of @xmath235 , the system would try to increase the sparsity parameter @xmath223 , in order to make the solution more sparse .",
    "thus , as we notice from fig .",
    "[ msd_error ] , being the true vector @xmath11 not sparse enough with respect to the selection of the trigger @xmath235 , the system determines an increment of the bias that strongly affects the performance . on the contrary , from fig .",
    "[ msd_error ] , we notice how the rza - atc diffusion algorithm is robust to errors in the selection of the trigger parameter @xmath235 .",
    "this benefit is again due to regularization , whose presence reduces the magnitude of the bias , improving the estimation capabilities of the algorithm and relaxing the choice of the system parameters .",
    "in this paper we proposed a class of diffusion lms strategies , regularized by convex sparsifying penalties , for distributed estimation over adaptive networks .",
    "two different penalty functions have been employed : the @xmath12-norm , which uniformly attracts to zero all the vector elements , and a reweighted function , which better approximates the @xmath103-norm , selectively shrinking only the elements with small magnitude . convergence and mean - square analysis of the sparse adaptive diffusion filter show under what conditions we have dominance of the proposed method with respect to its unregularized counterpart in terms of steady - state performance .",
    "further analysis leads to a procedure to update the regularization parameter of the algorithm , in order to ensure dominance of the sparse diffusion filter with respect to its unregularized version . in this way",
    ", the network can adjust in real - time the system parameters to improve the estimation performance , according to the sparsity of the underlying vector .",
    "several numerical results show the potential benefits of using such strategies .",
    "letting @xmath272 and @xmath273 , recursion ( [ compact_diffusion_expected ] ) gives @xmath274 where @xmath275 is the initial condition .",
    "as long as we can show that both terms on the right hand side of ( [ recursion_expected ] ) converge as @xmath1 goes to infinity , then we would be able to conclude the convergence of @xmath276 .",
    "to proceed , we call upon results from @xcite",
    ". let @xmath277 denote a vector that is obtained by stacking @xmath0 subvectors of size @xmath10 each ( as is the case with @xmath278 ) .",
    "the block maximum norm of @xmath279 is defined as @xmath280 where @xmath281 denotes the euclidean norm of its vector argument .",
    "likewise , the induced block maximum norm of a block matrix @xmath282 with @xmath67 block entries is defined as : @xmath283 it is easy to check that the first term on the rhs of ( [ recursion_expected ] ) converges to zero as @xmath284 .",
    "indeed , note that @xmath285 if we can ensure that @xmath286 .",
    "this condition is actually satisfied by ( [ step_sizes ] ) . to see this",
    ", we invoke the triangle inequality of norms to note that @xmath287 since @xmath288 in view of the fact that @xmath130 is a left - stochastic matrix @xcite .",
    "therefore , to satisfy @xmath286 , it suffices to require @xmath289 now , we recall a result from @xcite on the block maximum norm of a block diagonal and hermitian matrix @xmath282 with @xmath67 blocks @xmath290 , which states that @xmath291 thus , since @xmath292 is diagonal , condition ( [ conv_cond2 ] ) will hold if the matrix @xmath293 is stable . using ( [ perf_matrices3 ] ) , we can easily verify that this condition is satisfied for any step - sizes satisfying ( [ step_sizes ] ) , as claimed before .",
    "therefore , when the step - sizes satisfy condition ( [ step_sizes ] ) , the first term on the rhs of ( [ recursion_expected ] ) will converge to zero .",
    "we will show next that condition ( [ step_sizes ] ) also implies that the second term on the rhs of ( [ recursion_expected ] ) asymptotically converges to a finite value , thus leading to the overall convergence of the recursion ( [ recursion_expected ] ) .",
    "one effective tool to prove convergence of a series is the comparison test @xcite : a series is absolutely convergent if each term of the series can be bounded by a term of an absolutely convergent series .",
    "thus , denoting by @xmath294_k$ ] the @xmath2-th entry of a vector @xmath295 , it suffices to show that the series @xmath296_k\\end{aligned}\\ ] ] converges for each @xmath297 .",
    "now , each term of the series in ( [ rhs2 ] ) can be bounded as : @xmath298_k\\leq\\left|\\left[{\\cal b}^nb_{i - n}\\right]_k\\right|\\leq\\|{\\cal b}^nb_{i - n}\\|_{b,\\infty}\\leq\\|{\\cal b}\\|^n_{b,\\infty}\\cdot\\|b_{i - n}\\|_{b,\\infty}\\leq \\delta^n \\cdot b_{\\max}\\end{aligned}\\ ] ] where @xmath299 and @xmath300 the second inequality in ( [ rhs2_bound ] ) holds because the block maximum norm of a vector is greater than or equal to the largest absolute value of its entries .",
    "the scalar @xmath301 is finite for the following reason .",
    "first , note that the subgradient vector @xmath302 has bounded entries .",
    "in particular , @xmath303 for the za update in ( [ sign ] ) , and @xmath304 for the rza update in ( [ update_rew_l1norm ] ) .",
    "we further note that @xmath305 and @xmath306 .",
    "it follows that @xmath307 now , if condition ( [ step_sizes ] ) is satisfied , then @xmath308 and @xmath309 which means that the series ( [ geom_series ] ) and , consequently , the series ( [ rhs2 ] ) , are absolutely convergent . in summary , since both first and second term on the rhs of ( [ recursion_expected ] ) asymptotically converge to finite values , we conclude that @xmath276 will converge to a steady - state value .",
    "now , taking the limit of ( [ compact_diffusion_expected ] ) as @xmath134 , it is easy to derive a closed form expression for the bias : @xmath310^{-1}\\displaystyle{\\cal a}^t{\\cal m}\\lim_{i\\rightarrow\\infty } \\mathbb{e } \\partial f(\\bw_{i-1})\\end{aligned}\\ ] ] moreover , exploiting ( [ rhs2_bound ] ) , ( [ bound_b ] ) and ( [ geom_series ] ) , we further note that @xmath311 this completes the proof of theorem 1 .",
    "from ( [ phisigma])-([alpha ] ) we have @xmath312 since , as noted in appendix a , @xmath313 is a bounded function for all @xmath1 , the term @xmath314 in ( [ beta ] ) can be upper bounded by a positive constant term @xmath315 for all @xmath1 .",
    "the term @xmath177 in ( [ alpha ] ) can be written as @xmath316 where the vector @xmath317 is again bounded for all @xmath1 .",
    "thus , we have @xmath318 where @xmath319 .",
    "as shown in appendix a , the evolution of @xmath320 is given by ( [ recursion_expected ] ) , which , for any finite initialization vector @xmath321 , converges as @xmath284 and can not diverge for all @xmath1 , if the step - sizes are chosen to satisfy ( [ step_sizes ] ) . consequently , @xmath322 can be upper bounded by some positive constant vector @xmath323 for all @xmath1 .",
    "thus , letting @xmath324 , expression ( [ weighted_norm3 ] ) can be upper bounded as @xmath325 where @xmath326 .",
    "the positive constant @xmath327 can be related to the quantity @xmath328 through some constant @xmath329 , say , @xmath330 .",
    "relation ( [ msdi2 ] ) is an inequality , which can be used to prove convergence of the sequence @xmath331 to a bounded region instead of a fixed point .",
    "alternatively , we convert ( [ msdi2 ] ) into an equality recursion as follows : @xmath332 for some coefficient @xmath333 $ ] that depends on both @xmath331 and @xmath334 .",
    "recursion ( [ msdi3 ] ) leads to : @xmath335 \\mathbb{e}\\|\\tilde{\\bw}_{0}\\|^2_{{\\cal f}^i\\sigma}+(1+\\upsilon)r^t\\sum_{l=0}^{i-1}\\left[\\prod_{n = i - l}^{i}\\theta_n\\right]{\\cal f}^l\\sigma\\end{aligned}\\ ] ] where @xmath336 is the initial condition .",
    "we first note that if @xmath163 is stable , @xmath337 as @xmath134 . in this way",
    ", the first term on the rhs of ( [ msdi4 ] ) vanishes asymptotically .",
    "now , proceeding as in appendix a , we can use the comparison test @xcite to prove that , if @xmath163 is a stable matrix , the second term on the rhs of ( [ msdi4 ] ) is an absolutely convergent series .",
    "thus , denoting again by @xmath294_k$ ] the @xmath2-th entry of a vector @xmath295 , it suffices to show the convergence of the series : @xmath338_k\\end{aligned}\\ ] ] with @xmath339 , for @xmath297 .",
    "each term of the series in ( [ rhs3 ] ) can be bounded as : @xmath340_k\\leq\\left|\\left[\\xi_l(i){\\cal f}^l\\sigma\\right]_k\\right|\\leq\\left|\\left[{\\cal f}^l\\sigma\\right]_k\\right|\\leq\\|{\\cal f}^l\\sigma\\|_{b,\\infty}\\leq\\|{\\cal f}^l\\|_{b,\\infty}\\|\\sigma\\|_{b,\\infty}\\end{aligned}\\ ] ] where the second inequality in ( [ rhs2_bound3 ] ) holds because the coefficients @xmath341 $ ] for all @xmath1 , whereas the third inequality in ( [ rhs2_bound3 ] ) holds because the block maximum norm of a vector is greater equal than the largest absolute value of its entries . a known result in matrix theory @xcite states that for every square stable matrix @xmath163 , and every @xmath342 , there exists a submultiplicative matrix norm @xmath343 such that @xmath344 since @xmath163 is stable , @xmath345 , we can choose @xmath342 such that @xmath346 .",
    "now , since in a finite dimensional space all norms are equivalent @xcite , we have @xmath347 , for some positive constant @xmath348 .",
    "thus , we have @xmath349 and , substituting ( [ boundsf ] ) into ( [ rhs2_bound3 ] ) , we get @xmath350 which means that the series ( [ geom_series2 ] ) and , consequently , the series ( [ rhs3 ] ) , are absolutely convergent . in summary , since both the first and second terms on the rhs of ( [ msdi4 ] ) asymptotically converge to finite values , we conclude that @xmath331 will converge to a steady - state value , thus completing our proof .",
    "let us consider the bounded random vector @xmath351 in ( [ ci-1 ] ) , which is independent of the noise sequence @xmath20 for all @xmath352 . letting @xmath272 and @xmath353 , from ( [ compact_diffusion ] )",
    ", we get @xmath354 where @xmath355 is the initial condition . following the same steps as in appendix a",
    ", if the step - sizes satisfy condition ( [ step_sizes ] ) , the first term on the rhs of ( [ recursion_expected_alpha ] ) will converge to zero .",
    "furthermore , since the vector sequence @xmath351 is bounded , similarly to what we have done in ( [ rhs2])-([geom_series ] ) , we can again use the comparison test @xcite to prove that the second term on the rhs of ( [ recursion_expected_alpha ] ) asymptotically converges to a finite value , thus leading to the existence of the limit in ( [ recursion_expected_alpha ] ) .",
    "l. li , j. chambers , c. lopes , and a. h. sayed , `` distributed estimation over an adaptive incremental network based on the affine projection algorithm , '' _ ieee transactions on signal processing _ ,",
    "1 , pp . 151164 , 2009 .",
    "c. g. lopes and a. h. sayed , `` diffusion least - mean squares over adaptive networks : formulation and performance analysis , '' _ ieee transactions on signal processing _",
    "56 , no . 7 , pp .",
    "31223136 , july 2008 .",
    "a. h. sayed , `` diffusion adaptation over networks , '' to appear in _ e - reference signal processing _",
    ", r. chellapa and s. theodoridis , _ editors _ , elsevier , 2013 . also available on arxiv at http://arxiv.org/abs/1205.4220 , may 2012 .",
    "n. takahashi , i. yamada , and a. h. sayed , `` diffusion least - mean squares with adaptive combiners : formulation and performance analysis , '' _ ieee transactions on signal processing _ , vol .",
    "9 , pp . 47954810 , sep",
    ". 2010 .",
    "j. chen and a. h. sayed , `` diffusion adaptation strategies for distributed optimization and learning over networks , '' _ ieee transactions on signal processing _",
    "60 , no . 8 , pp . 4289 - 4305 , august 2012 .          p.",
    "di lorenzo , s. barbarossa and a. h. sayed , `` bio - inspired swarming for dynamic radio access based on diffusion adaptation , '' _ proc .",
    "european signal processing conference ( eusipco ) _ , aug .",
    "2011 , barcelona , pp.402406 .",
    "z. towfic , j. chen and a. h. sayed , `` collaborative learning of mixture models using diffusion adaptation , '' in _ proc .",
    "ieee workshop on machine learning for signal processing _ ,",
    "beijing , china , sept .",
    "2011 , pp . 16 .",
    "s. chouvardas , k. slavakis , and s. theodoridis , `` trading off communications bandwidth with accuracy in adaptive diffusion networks , '' in _ proc .",
    "ieee international conference on acoustics speech and signal processing _ , may 2011 , prague , czeck rep . , pp .",
    "20482051 .    s. kawamura and m. hatori , `` a tap selection algorithm for adaptive filters , '' in _ proc .",
    "of ieee international conference on acoustics speech and signal processing ( icassp ) _ , 1986 , vol .",
    "11 , tokyo , japan , pp .",
    "29792982 .",
    "y. li , y. gu , and k. tang , `` parallel nlms filters with stochastic active taps and step - sizes for sparse system identification , '' _ proc . of ieee international conference on acoustics speech and signal processing ( icassp )",
    "iii , 2006 , toulouse .",
    "etter , `` identification of sparse impulse response systems using an adaptive delay filter , '' in _ pro . of ieee international conference on acoustics speech and signal processing ( icassp )",
    "_ , 1985 , tampa , florida , pp .",
    "11691172 .",
    "d. angelosante , j.a .",
    "bazerque , and g.b .",
    "giannakis , `` online adaptive estimation of sparse signals : where rls meets the @xmath356-norm , '' _ ieee transactions on signal processing _ , vol .",
    "58 , no . 7 , pp .",
    "34363447 , july , 2010 .",
    "y. kopsinis , k. slavakis , and s. theodoridis , `` online sparse system identification and signal reconstruction using projections onto weighted @xmath356 balls , '' _ ieee transactions on signal processing _ , vol .",
    "3 , pp . 936952 , march , 2010 .",
    "y. murakami , m. yamagishi , m. yukawa , and i. yamada , `` a sparse adaptive filtering using time - varying soft - thresholding techniques , '' in _ proc .",
    "ieee international conference on acoustics speech and signal processing ( icassp ) _ , dallas , usa , 2010 , pp .",
    "37343737 .        p.",
    "di lorenzo , s. barbarossa , and a. h. sayed , `` sparse diffusion lms for distributed adaptive estimation , '' _ proc .",
    "ieee international conference on acoustics , speech , and signal processing _ , kyoto , japan , march 2012 .",
    "s. chouvardas , k. slavakis , y. kopsinis , s. theodoridis , `` a sparsity - promoting adaptive algorithm for distributed learning , '' _ ieee transactions on signal processing _ , vol .",
    "10 , pp . 54125425 , oct .",
    "2012 .",
    "paolo di lorenzo ( s10 ) received the m.sc .",
    "degree in 2008 and the ph.d . in electrical engineering in 2012 , both from university of rome `` la sapienza , '' italy .",
    "he is currently a post - doc in the department of information , electronics and telecommunications , university of rome , `` la sapienza . '' during 2010 he held a visiting research appointment in the department of electrical engineering , university of california at los angeles ( ucla ) .",
    "he has participated in the european research project freedom on femtocell networks .",
    "he is currently involved in the european projects simtisys , on moving target detection through satellite constellations , and tropic , on distributed computing , storage and radio resource allocation over cooperative femtocells .",
    "his primary research interests are in statistical signal processing , distributed optimization algorithms for communication and sensor networks , graph theory , and adaptive filtering .",
    "di lorenzo received three best student paper awards , respectively at ieee spawc10 , eurasip eusipco11 , and ieee camsap11 , for works in the area of signal processing for communications and synthetic aperture radar systems .",
    "he is recipient of the 2012 gtti ( italian national group on telecommunications and information theory ) award for the best ph.d .",
    "thesis in information technologies and communications .",
    "ali h. sayed ( s90-m92-sm99-f01 ) is professor of electrical engineering with the university of california , los angeles ( ucla ) , where he leads the adaptive systems laboratory .",
    "he has published widely , in the areas of adaptation and learning , statistical signal processing , distributed processing , and bio - inspired cognition .",
    "he is coauthor of the textbook _ linear estimation _",
    "( englewood cliffs , nj : prentice - hall , 2000 ) , of the research monograph _ indefinite quadratic estimation and control _ ( philadelphia , pa : siam , 1999 ) , and co - editor of _ fast algorithms for matrices with structure _ ( philadelphia , pa : siam , 1999 ) .",
    "he is also the author of the textbooks _ fundamentals of adaptive filtering _",
    "( hoboken , nj : wiley , 2003 ) , and _ adaptive filters _ ( hoboken , nj : wiley , 2008 ) .",
    "he has contributed several encyclopedia and handbook articles .",
    "dr . sayed is a fellow of ieee for his contributions to adaptive ?",
    "ltering and estimation algorithms .",
    "he has served on the editorial boards of several publications .",
    "he has also served as the editor - in - chief of the ieee transactions on signal processing from 2003 to 2005 , and the eurasip journal on advances in signal processing from 2006 to 2007 .",
    "he has served on the publications ( 2003 - 2005 ) , awards ( 2005 ) , and conference boards ( 2007-present ) of the ieee signal processing society .",
    "he also served on the board of governors of the ieee signal processing society from 2007 to 2008 and as vice president of publications of the same society from 2009 to 2011 .",
    "his work has received several recognitions , including the 1996 ieee donald g. fink award , the 2002 best paper award from the ieee signal processing society , the 2003 kuwait prize in basic sciences , the 2005 terman award , and the 2005 young author best paper award from the ieee signal processing society .",
    "he has served as a 2005 distinguished lecturer of the ieee signal processing society and as general chairman of the ieee international conference on acoustics , speech , and signal processing ( icassp ) 2008 ."
  ],
  "abstract_text": [
    "<S> this article proposes diffusion lms strategies for distributed estimation over adaptive networks that are able to exploit sparsity in the underlying system model . </S>",
    "<S> the approach relies on convex regularization , common in compressive sensing , to enhance the detection of sparsity via a diffusive process over the network . </S>",
    "<S> the resulting algorithms endow networks with learning abilities and allow them to learn the sparse structure from the incoming data in real - time , and also to track variations in the sparsity of the model . </S>",
    "<S> we provide convergence and mean - square performance analysis of the proposed method and show under what conditions it outperforms the unregularized diffusion version . we also show how to adaptively select the regularization parameter . </S>",
    "<S> simulation results illustrate the advantage of the proposed filters for sparse data recovery .    </S>",
    "<S> diffusion lms , adaptive networks , compressive sensing , distributed estimation , sparse vector . </S>"
  ]
}