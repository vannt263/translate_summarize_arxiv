{
  "article_text": [
    "fitting simple contours ( primitives ) to experimental data is one of the basic problems in pattern recognition and computer vision .",
    "the simplest contours are linear segments and circular arcs .",
    "the need of approximating scattered points by a circle or a circular arc arises in physics @xcite , biology and medicine @xcite , archeology @xcite , industry @xcite , etc .",
    "the problem was studied since at least early sixties @xcite , and attracted much more attention in recent years due to its importance in image processing @xcite .",
    "we study the least squares fit ( lsf ) of circles and circular arcs .",
    "this method is based on minimizing the mean square distance from the circle to the data points .",
    "given @xmath0 points @xmath1 , @xmath2 , the objective function is defined by = _",
    "i=1^n d_i^2 [ fmain1 ] where @xmath3 is the euclidean ( geometric ) distance from the point @xmath1 to the circle .",
    "if the circle satisfies the equation ( x - a)^2 + ( y - b)^2 = r^2 [ abr ] where @xmath4 is its center and @xmath5 its radius , then d_i = - r [ diabr ] the minimization of ( [ fmain1 ] ) is a nonlinear problem that has no closed form solution .",
    "there is no direct algorithm for computing the minimum of @xmath6 , all known algorithms are either iterative and costly or approximative by nature .    the purpose of this paper is a general study of the problem .",
    "it consists of three parts . in the first one we address fundamental issues , such as the existence and uniqueness of a solution , the right choice of parameters to work with , and the general behavior of the objective function @xmath6 on the parameter space .",
    "these issues are rarely discussed in the literature , but they are essential for understanding of advantages and disadvantages of practical algorithms . the second part is devoted to the _ geometric fit _  the minimization of the sum of squares of geometric distances , which is given by @xmath6 . here",
    "we evaluate three most popular algorithms ( levenberg - marquardt , landau , and spth ) and develop a new approach . in the third part",
    "we discuss an _ algebraic fit _ based on approximation of @xmath6 by a simpler algebraic function that can be minimized by a direct , noniterative algorithm .",
    "we compare three such approximations and propose some more efficient numerical schemes than those published so far .",
    "additional information about this work can be found on our web site @xcite .",
    "the very first questions one has to deal with in many mathematical problems are the existence and uniqueness of a solution . in our context the questions are : does the function @xmath6 attain its minimum ? assuming that it does , is the minimum unique",
    "? these questions are not as trivial as they may appear .    *",
    "2.1 existence of lsf*. the function @xmath6 is obviously continuous in the circle parameters @xmath7 .",
    "according to a general principle , a continuous function always attains a minimum ( possibly , not unique ) if it is defined on a closed and bounded ( i.e. , compact ) domain .",
    "our function @xmath6 is defined for all @xmath8 and all @xmath9 , so its domain is not compact .",
    "for this reason the function @xmath6 fails to attain its minimum in some cases .",
    "for example , let @xmath10 distinct points lie on a straight line .",
    "then one can approximate the data by a circle arbitrarily well and make @xmath6 arbitrarily close to zero , but since no circle can interpolate @xmath10 collinear points , we will always have @xmath11 . hence ,",
    "the least squares fit by circles is , technically , impossible . for any circle one can find another circle that fits the data better .",
    "the best fit here is given by the straight line trough the data points , which yields @xmath12 .",
    "if we want the lsf to exist , we have to allow lines , as well as circles , in our model , and from now on we do this .",
    "one can prove now that the function @xmath6 defined on circles _ and _ lines always attains its minimum , and so the existence of the lsf is guaranteed .",
    "a detailed proof is available in @xcite .",
    "we should note that if the data points are generated randomly with a continuous probability distribution ( such as normal or uniform ) , then the probability that the lsf returns a line , rather than a circle , is zero .",
    "this is why lines are often ignored and one practically works with circles only . on the other hand ,",
    "if the coordinates of the data points are discrete ( such as pixels on a computer screen ) , then lines may appear with a positive probability and have to be reckoned with .    *",
    "2.2 uniqueness of lsf*. this question is not trivial either .",
    "even if @xmath6 takes its minimum on a circle , the best fitting circle may not be unique , as several other circles may minimize @xmath6 as well .",
    "we could not find such examples in the literature , so we provide our own here .",
    "_ examples of multiple minima_. let four data points @xmath13 and @xmath14 make a square centered at the origin .",
    "we place another @xmath15 points identically at the origin @xmath16 to have a total of @xmath17 points .",
    "this configuration is invariant under a rotation through the right angle about the origin .",
    "hence , if a circle minimizes @xmath6 , then by rotating that circle through @xmath18 , @xmath19 , and @xmath20 we get three other circles that minimize @xmath6 as well .",
    "thus , we get four distinct best fitting circles , unless either ( i ) the best circle is centered at the origin or ( ii ) the best fit is a line . so we need to show that neither is the case .",
    "this involves some simple calculations . if a circle has radius @xmath21 and center at @xmath16 , then @xmath22 .",
    "the minimum of this function is attained at @xmath23 , and it equals @xmath24 .",
    "also , the best fitting line passes through the origin and gives @xmath25 . on the other hand , consider the circle passing through three points @xmath16 , @xmath26 , and @xmath27 .",
    "it only misses two other points , @xmath28 and @xmath29 , and it is easy to see that it gives @xmath30 , which is less than @xmath31 and @xmath32 whenever @xmath15 .",
    "hence , the best fit is a circle not centered at the origin , and so our construction works .",
    "@xmath33    figure 1 : a data set ( left ) for which the objective function ( right ) has four minima .",
    "figure  1 illustrates our example , it gives the plot of @xmath6 with four distinct minima ( the plotting method is explained below ) . by changing the square to a rectangle",
    "one can make @xmath34 have exactly two minima . by replacing the square with a regular @xmath35-gon and increasing the number of identical points at @xmath16",
    "one can construct @xmath6 with exactly @xmath35 minima for any @xmath36 , see details in @xcite .    of course , if the data points are generated randomly with a continuous probability distribution , then the probability that the objective function @xmath6 has multiple minima is zero .",
    "in particular , small random perturbations of the data points in our example on fig .  1 will slightly change the values of @xmath6 at its minima , so that one of them will become a global ( absolute ) minimum and three others  local ( relative ) minima .",
    "we note , however , that while the cases of multiple global minima are indeed exotic , they demonstrate that the global minimum of @xmath6 may change discontinuously if one slowly moves data points .",
    "* 2.3 local minima of the objective function*. generally , local minima are undesirable , since they can trap iterative algorithms and lead to false solutions .",
    "we have investigated how frequently the function @xmath6 has local minima ( and how many ) . in our experiment ,",
    "@xmath0 data points were generated randomly with a uniform distribution in the unit square @xmath37 .",
    "then we applied the levenberg - marquard algorithm ( described below ) starting at 1000 different , randomly selected initial conditions .",
    "every point of convergence was recorded as a minimum ( local or global ) of @xmath6 .",
    "if there were more than one point of convergence , then one of them was the global minimum and the others were local minima .",
    "table  1 shows the probabilities that @xmath6 had 0,1,2 or more local minima for @xmath38 data points .",
    "[ cols=\">,^,^,^,^,^,^\",options=\"header \" , ]     table 1 .",
    "probability of 0 , 1 , 2 or more local minima of @xmath6 when @xmath38 points are randomly generated in a unit square .",
    "we see , surprisingly , that local minima only occur with probability @xmath39% .",
    "the more points are generated , the less frequently the function @xmath6 has any local minima .",
    "multiple local minima turn up with probability even less than 1% .",
    "the maximal number of local minima we observed was four , it happened only a few times in almost a million random samples we tested .",
    "generating points with a uniform distribution in a square produces completely `` chaotic '' samples without any predefined pattern .",
    "this is , in a sense , the worst case scenario . if we generate points along a circle or a circular arc ( with some noise ) , then local minima virtually never occur .",
    "for example , if @xmath40 points are sampled along a 90@xmath41 circular arc of radius @xmath42 with a gaussian noise at level @xmath43 , then the probability that @xmath6 has any local minima is as low as @xmath44 .",
    "therefore , in typical applications the objective function @xmath6 is very likely to have a unique ( global ) minimum and no local minima . does this",
    "mean that a standard iterative algorithm , such as the steepest descent or gauss - newton or levenberg - marquardt , would converge to the global minimum from any starting point ?",
    "unfortunately , this is not the case , as we demonstrate next .    * 2.4 plateaus and valleys on the graph of the objective function*. in order to examine the behavior of @xmath6 we found a way to visualize its graph . even though @xmath45 is a function of three parameters , it happens to be just a quadratic polynomial in @xmath5 when the other two variables are fixed .",
    "so it has a unique global minimum in @xmath5 that can be easily found . if we denote r_i = then the minimum of @xmath6 with respect to @xmath5 is attained at = |r : = 1n _ i=1^n r_i [ rbarr ] this allows us to eliminate @xmath5 and express @xmath6 as a function of @xmath8 : = _ i=1^n ( r_i - |r)^2 [ fab ] a function of two variables can be easily plotted .",
    "this is exactly how we did it on fig .  1 .",
    "figure 2 : a simulated data set of 50 points .    now , fig .",
    "2 presents a typical random sample of @xmath46 points generated along a circular arc ( the upper half of the unit circle @xmath47 ) with a gaussian noise added at level @xmath48 .",
    "3 shows the graph of @xmath6 plotted by maple in two different scales .",
    "one can clearly see that @xmath6 has a global minimum around @xmath49 and no local minima .",
    "4 presents a flat grey scale contour map , where darker colors correspond to deeper parts of the graph ( smaller values of @xmath6 ) .",
    "@xmath50    figure 3 : the objective function @xmath6 for the data set shown on fig .  2 : a large view ( left ) and the minimum ( right ) .",
    "3 shows that the function @xmath6 does not grow as @xmath51 .",
    "in fact , it is bounded , i.e.  @xmath52 .",
    "the boundedness of @xmath6 actually explains the appearance of large nearly flat plateaus and valleys on fig .  3 that stretch out to infinity in some directions .",
    "if an iterative algorithm starts somewhere in the middle of such a plateau or a valley or gets there by chance , it will have hard time moving at all , since the gradient of @xmath6 almost vanishes there .",
    "we indeed observed conventional algorithms getting `` stuck '' on flat plateaus or valleys in our tests . the new algorithm we propose below",
    "does not have this drawback .",
    "figure 4 : a grey - scale contour map of the objective function @xmath34 .",
    "darker colors correspond to smaller values of @xmath6 .",
    "the minimum is marked by a cross .",
    "second , there are two particularly interesting valleys that stretch roughly along the line @xmath53 on figs .  3 and 4 .",
    "one of them , corresponding to @xmath54 , has its bottom point at the minimum of @xmath6 .",
    "the function @xmath6 slowly decreases along the valley as it approaches the minimum .",
    "hence , any iterative algorithm starting in that valley or getting there by chance should , ideally , find its way downhill and arrive at the minimum of @xmath6 .",
    "the other valley corresponds to @xmath55 , it is separated from the global minimum of @xmath6 by a ridge .",
    "the function @xmath6 slowly decreases along this valley as @xmath56 grows .",
    "hence , any iterative algorithm starting in this valley or getting there `` by accident '' will be forced to move up along the @xmath56 axis , away from the minimum of @xmath6 , all the way to @xmath57 .",
    "if an iterative algorithm starts at a randomly chosen point , it may go down into either valley , and there is a good chance that it descends into the second valley and then escapes to infinity . for the sample on fig .  2",
    ", we applied the levenberg - marquardt algorithm starting at a randomly selected point in the square @xmath58 about the centroid @xmath59 , @xmath60 of the data .",
    "we found that the algorithm escaped to infinity with probability about 50%",
    ".    unfortunately , such `` escape valleys '' are inevitable .",
    "we have proved @xcite that for almost every data sample there was a pair of valleys stretching out in opposite directions , so that one valley descends to the minimum of @xmath6 , while the other valley descends toward infinity .",
    "we now summarize our observations .",
    "there are three major ways in which conventional iterative algorithms may fail to find the minimum of @xmath34 :    * when they converge to a local minimum ; * when they slow down and stall on a nearly flat plateau or in a valley ; * when they diverge to infinity along a decreasing valley .",
    "we will not attempt here to deal with local minima of @xmath6 causing the failure of type ( a ) , since local minima occur quite rarely .",
    "but the other two types of failures ( b ) and ( c ) can be drastically reduced , if not eliminated altogether , by adopting a new set of parameters , which we introduce next .    *",
    "2.5 choice of parametrization*. the trouble with the natural circle parameters @xmath7 is that they become arbitrarily large when the data are approximated by a circular arc with low curvature .",
    "not only does this lead to a catastrophic loss of accuracy when two large nearly equal quantities are subtracted in ( [ diabr ] ) , but this is also ultimately responsible for the appearance of flat plateaus and descending valleys that cause failures ( b ) and ( c ) in iterative algorithms .",
    "we now adopt a parametrization used in @xcite , in which the equation of a circle is a(x^2+y^2 ) + bx + cy + d = 0",
    "[ abcd ] note that this gives a circle when @xmath61 and a line when @xmath62 , so it conveniently combines both types of contours in our model .",
    "the parameters @xmath63 must satisfy the inequality @xmath64 in order to define a nonempty circle or a line . since the parameters only need to be defined up to a scalar multiple , we can impose a constraint b^2+c^2 - 4ad = 1 [ constr ] if we additionally require that @xmath65 , then every circle or line will correspond to a unique quadruple @xmath66 and vice versa . for technical reasons ,",
    "though , we do not restrict @xmath67 , so that circles have a duplicate parametrization , see ( [ convabcd ] ) below . the conversion formulas between the natural parameters and the new ones",
    "are a=- ,  b=- ,  r= [ convabr ] and a = ,  b=-2aa ,  c=-2ab ,  d= [ convabcd ] the distance from a point @xmath1 to the circle can be expressed , after some algebraic manipulations , as d_i = 2 [ dip ] where p_i = a(x_i^2+y_i^2)+bx_i+cy_i+d one can check that @xmath68 for all @xmath69 , see below , so that the function ( [ dip ] ) is well defined .",
    "the formula ( [ dip ] ) is somewhat more complicated than ( [ diabr ] ) , but the following advantages of the new parameters can outweigh the higher cost of computations .",
    "first , the is no need to work with arbitrarily large values of parameters anymore .",
    "one can effectively reduce the parameter space to a box \\{|a|<a_,|b|<b_,|c|<c_,|d|<d _ } [ box ] where @xmath70 , @xmath71 , @xmath72 , @xmath73 can be determined explicitly @xcite .",
    "this conclusion is based on some technical analysis , and we only outline the main steps . given a sample @xmath1 , @xmath74 , let @xmath75 denote the maximal distance between the data points .",
    "then we observe that ( i ) the distance from the best fitting line or circle to the centroid of the data @xmath76 does not exceed @xmath77 .",
    "it also follows from ( [ rbarr ] ) that ( ii ) the best fitting circle has radius @xmath78 , hence @xmath79 .",
    "thus the model can be restricted to circles and lines satisfying the two conditions ( i ) and ( ii ) . under these conditions ,",
    "the parameters @xmath63 are bounded by some constants @xmath70 , @xmath71 , @xmath72 , @xmath73 . a detailed proof of this fact is contained in @xcite .",
    "the effective boundedness of the parameters @xmath63 renders them a preferred choice for numerical calculations .",
    "in particular , when the search for a minimum is restricted to a closed box ( [ box ] ) , it can hardly run into vast nearly flat plateaus that we have seen on fig .",
    "second , the objective function is now smooth in @xmath63 on the parameter space . in particular , no singularities occur as a circular arc approaches a line .",
    "circular arcs with low curvature correspond to small values of @xmath67 , lines correspond to @xmath62 , and the objective function and all its derivatives are well defined at @xmath62 .",
    "third , recall the two valleys shown on figs .",
    "3 - 4 , which we have proved to exist for almost every data sample @xcite . in the new parameter space",
    "they become two halves of one valley stretching continuously across the hyperplane @xmath62 and descending to the minimum of @xmath6 .",
    "therefore , any iterative algorithm starting _ anywhere _ in that ( now unique ) valley would converge to the minimum of @xmath6 ( maybe crossing the plane @xmath62 on its way ) .",
    "there is no escape to infinity anymore .    as a result ,",
    "the new parametrization can effectively reduce , if not eliminate completely , the failures of types ( b ) and ( c ) .",
    "this will be confirmed experimentally in the next section .",
    "* 3.1 three popular algorithms*. the minimization of the nonlinear function @xmath6 can not be accomplished by a finite algorithm .",
    "various iterative algorithms have been applied to this end .",
    "the most successful and popular are    * the levenberg - marquardt method .",
    "* landau algorithm @xcite . *",
    "spth algorithm @xcite    here ( a ) is a short name for the classical gauss - newton method with the levenberg - marquardt correction @xcite .",
    "it can effectively solve any least squares problem of type ( [ fmain1 ] ) provided the first derivatives of @xmath3 s with respect to the parameters can be computed .",
    "the levenberg - marquardt algorithm is quite stable and reliable , and it usually converges rapidly ( if the data points are close to the fitting contour , the convergence is nearly quadratic ) .",
    "fitting circles with the levenberg - marquardt method is described in many papers , see , e.g.  @xcite .",
    "the other two methods are circle - specific .",
    "the landau algorithm employs a simple fixed - point iterative scheme , nonetheless it shows a remarkable stability and is widely used in practice . its convergence , though , is linear @xcite .",
    "the spth algorithm makes a clever use of an additional set of ( dummy ) parameters and is based on alternating minimization with respect to the dummy parameter set and the real parameter set @xmath80 . at each step ,",
    "one set of parameters is fixed , and a _ global _ minimum of the objective function @xmath6 with respect to the other parameter set is found , which guarantees ( at least theoretically ) that @xmath34 decreases at every iteration .",
    "the convergence of spth s algorithm is , however , known to be slow @xcite .",
    "the cost per iteration is about the same for all the three algorithms : the levenberg - marquardt requires @xmath81 flops per iteration , landau takes @xmath82 flops per iteration and for spth the flop count is @xmath83 per iteration ( in all cases , a prior centering of the data is assumed , i.e.  @xmath84 ) .    we have tested the performance of these three algorithms experimentally , and the results are reported in section  3.3 .    *",
    "3.2 a new algorithm for circle fitting*. in section  2.5 we introduced parameters @xmath63 subject to the constraint ( [ constr ] ) . here",
    "we show how the levenberg - marquardt scheme can be applied to minimize the function ( [ fmain1 ] ) in the parameter space @xmath66 .",
    "first , we introduce a new parameter  an angular coordinate @xmath85 defined by @xmath86 so that @xmath85 replaces @xmath87 and @xmath88 .",
    "now one can perform an unconstrained minimization of @xmath89 in the three - dimensional parameter space @xmath90 . the distance @xmath3 is expressed by ( [ dip ] ) with @xmath91 where we denote , for brevity , @xmath92 , @xmath93 , and @xmath94 .",
    "the first derivatives of @xmath3 with respect to the parameters are @xmath95 @xmath96 @xmath97 where @xmath98 and @xmath99 then one can apply the standard levenberg - maquardt scheme , see details in @xcite .",
    "the resulting algorithm is more complicated and costly than the methods described in 3.1 , it requires @xmath100 flops and one trigonometric function call per iteration .",
    "but it converges in a fewer iterations than other methods , so that its overall performance is rather good , see the next section .",
    "this approach has some pitfalls  the function @xmath6 is singular ( its derivatives are discontinuous ) when @xmath101 for some @xmath69 or when @xmath102 .",
    "we see that @xmath103 this quantity vanishes if @xmath104 and @xmath105 , i.e.   when a data point coincides with the circle s center , and this is extremely unlikely to occur .",
    "in fact , it has never happened in our tests , so that we did not use any security measures against the singularity @xmath101 .",
    "on the other hand , we see that @xmath106 which vanishes whenever @xmath49 .",
    "this singularity turns out to be more serious  when the circle s center computed iteratively approaches the origin , the algorithm often gets stuck because it persistently tries to enter the forbidden area @xmath107 .",
    "we found a simple remedy : whenever the algorithm attempts to make @xmath108 negative , we shift the origin by adding a vector @xmath109 to all the data coordinates @xmath1 , and then recompute the parameters @xmath90 .",
    "the vector @xmath109 should be of size comparable to the average distance between the data points , and its direction can be chosen randomly . the shift had to be applied only occasionally and its cost was negligible .    *",
    "remark*. another convenient parametrization of circles ( and lines ) was proposed by karimki @xcite .",
    "he uses three parameters : the signed curvature ( @xmath110 ) , the distance of closest approach ( @xmath111 ) to the origin , and the direction of propagation ( @xmath112 ) at the point of closest approach .",
    "parameters @xmath113 , @xmath111 , @xmath112 are similar to ours @xmath67 , @xmath114 , @xmath85 , and can be also used in the alternative levenberg - marquardt scheme .    * 3.3 experimental tests*.",
    "we have generated @xmath115 samples of @xmath0 points randomly with a uniform distribution in the unit square @xmath37 . for each sample",
    ", we generated @xmath116 initial guesses by selecting a random circle center @xmath4 in a square @xmath58 around the centroid @xmath76 of the sample and then computing the radius @xmath5 by ( [ rbarr ] ) .",
    "every triple @xmath80 was then used as an initial guess , and we ran all the four algorithms starting at it .    for each sample and for each algorithm , we have determined the number of runs , from 1000 random initial guesses , when the iterations converged to the global minimum of @xmath6 . dividing that number by the total number of generated initial guesses , 1000",
    ", we obtained the _ probability _ of convergence to the minimum of @xmath6 .",
    "we also computed the average number of iterations in which the convergence took place . at this stage",
    ", the samples for which the function @xmath6 had any local minima , in addition to the global minimum , were eliminated .",
    "the fraction of such samples was less than 15% , as one can see in table  1 .",
    "the remaining majority of samples were used to compute the overall characteristics of each algorithm : the grand average probability of convergence to the minimum of @xmath6 and the grand mean number of iterations the convergence took .",
    "we note that , since samples with local minima are eliminated , the probability of convergence will be a true measure of the algorithm s reliability .",
    "all failures to converge will occur when the algorithm falters and can not find any minima , which we consider as the algorithm s fault .",
    "this experiment was repeated for @xmath117 .",
    "the results are presented on figures  5 and 6 , where the algorithms are marked as follows : lan for landau , spa for spth , lmc for the canonical levenberg - maquardt in the @xmath80 parameter space , and lma for the alternative levenberg - maquardt in the @xmath90 parameter space .",
    "figure 5 : the probability of convergence to the minimum of @xmath34 starting at a random initial guess .",
    "figure  5 shows the probability of convergence to the minimum of @xmath6 , it clearly demonstrates the superiority of the lma method .",
    "figure  6 presents the average cost of convergence , in terms of flops per data point , for all the four methods .",
    "the fastest algorithm is the canonical levenberg - marquardt ( lmc ) .",
    "the alternative levenberg - marquardt ( lma ) is almost twice as slow .",
    "the spth and landau methods happen to be far more expensive , in terms of the computational cost , than both levenberg - marquardt schemes .",
    "figure 6 : the average cost of computations , in flops per data point .",
    "the poor performance of the landau and spth algorithms is illustrated on fig .",
    "it shows a typical path taken by each of the four procedures starting at the same initial point @xmath118 and converging to the same limit point @xmath119 ( the global minimum of @xmath6 ) .",
    "each dot represents one iteration . for lma and lmc ,",
    "subsequent iterations are connected by grey lines .",
    "one can see that lma ( hollow dots ) heads straight to the target and reaches it in about 5 iterations .",
    "lmc ( solid dots ) makes a few jumps back and forth but arrives at the target in about 15 steps . on the contrary , spa ( square dots ) and lan ( round dots ) advance very slowly and tend to make many short steps as they approach the target ( in this example spa took 60 steps and lan more than 300 ) .",
    "note that lan makes an inexplicable long detour around the point @xmath120 .",
    "such tendencies account for the overall high cost of computations for these two algorithms as reported on fig .",
    "@xmath121    figure 7 : paths taken by the four algorithms on the @xmath122 plane , from the initial guess at @xmath118 marked by a large cross to the minimum of @xmath123 at @xmath119 ( a small cross ) .",
    "next , we have repeated our experiment with a different rule of generating data samples .",
    "instead of selecting points randomly in a square we now sample them along a circular arc of radius @xmath42 with a gaussian noise at level @xmath48 .",
    "we set the number of points to 20 and vary the arc length from @xmath124 to @xmath125 .",
    "otherwise the experiment proceeded as before , including the random choice of initial guesses .",
    "the results are presented on figures  8 and 9 .",
    "figure 8 : the probability of convergence to the minimum of @xmath34 starting at a random initial guess .",
    "we see that the alternative levenberg - marquardt method ( lma ) is very robust ",
    "its displays a remarkable 100% convergence across the entire range of the arc length .",
    "the reliability of the other three methods is high ( up to 95% ) on full circles ( @xmath125 ) but degrades to 50% on half - circles .",
    "then the conventional levenberg - marquardt ( lmc ) stays on the 50% level for all smaller arcs down to @xmath124 .",
    "the spth method breaks down on @xmath126 arcs and the landau method breaks down even earlier , on @xmath127 arcs .",
    "figure  9 shows that the cost of computations for the lma and lmc methods remains low for relatively large arcs , but it grows sharply for very small arcs ( below @xmath128 ) .",
    "the lmc is generally cheaper than lma , but , interestingly , becomes more expensive on arcs below @xmath129 .",
    "the cost of the spth and landau methods is , predictably , higher than that of the levenberg - marquardt schemes , and it skyrockets on arcs smaller than half - circles making these two algorithms prohibitively expensive .",
    "figure 9 : the average cost of computations , in flops per data point .",
    "we emphasize , though , that our results are obtained when the initial guess is just picked randomly from a large square . in practice",
    "one can always find more sensible ways to choose an initial guess , so that the subsequent iterative schemes would perform much better than they did in our tests .",
    "we devote the next section to various choices of the initial guess and the corresponding performance of the iterative methods .",
    "generally , iterative algorithms for minimizing nonlinear functions like ( [ fmain1 ] ) are quite sensitive to the choice of the initial guess . as a rule , one needs to provide an initial guess close enough to the minimum of @xmath6 in order to ensure a rapid convergence .",
    "the selection of an initial guess requires some other , preferably fast and non - iterative , procedure . in mass data processing , where speed is a factor , one often can not afford relatively slow iterative methods , hence a non - iterative fit is the only option .      * 4.1 `` pure '' algebraic fit*. the first one , we call it af1 , is a very simple and old method , it has been known since at least 1972 , see @xcite , and then rediscovered and published independently by many people @xcite . in this method , instead of minimizing the sum of squares of the geometric distances ( [ fmain1])([diabr ] ) , one minimizes the sum of squares of _ algebraic distances _",
    "@xmath130 ^ 2      \\nonumber\\\\      & = & \\sum_{i=1}^n ( z_i + bx_i + cy_i + d)^2        \\label{f1}\\end{aligned}\\ ] ] where @xmath131 ( as before ) , @xmath132 , @xmath133 , and @xmath134 .",
    "now , differentiating @xmath135 with respect to @xmath136 yields a system of _ linear _ equations : @xmath137 where @xmath138 , etc .",
    "denote moments , for example @xmath139 , @xmath140 .",
    "solving this system ( by cholesky decomposition or another matrix method ) gives @xmath136 , and finally one computes @xmath7 .",
    "the af1 algorithm is very fast , it requires @xmath141 flops to compute @xmath7 .",
    "however , it gives an estimate of @xmath80 that is not always statistically optimal in the sense that the corresponding covariance matrix may exceed the rao - cramer lower bound , see @xcite .",
    "this happens when data points are sampled along a circular arc , rather than a full circle .",
    "moreover , when the data are sampled along a small circular arc , the af1 is very biased and tends to return absurdly small circles @xcite . despite these shortcomings ,",
    "though , af1 remains a very attractive and simple routine for supplying an initial guess for iterative algorithms .",
    "* 4.2 gradient - weighted algebraic fit ( gwaf)*. in the next subsections we will show how the `` pure '' algebraic fit can be improved at a little extra computational cost .",
    "first , we use again the circle equation ( [ abcd ] ) and note that the minimization of ( [ f1 ] ) is equivalent to that of _ 1(a ,",
    "b , c , d ) = _ i=1^n ( az_i + bx_i + cy_i + d)^2 [ f1a ] under the constraint @xmath142 .",
    "we will show that some other constraints lead to more accurate estimates .",
    "the best results are achieved with the so called gradient - weighted algebraic fit , or gwaf for brevity . in its general form",
    "it goes like this .",
    "suppose one wants to approximate scattered data with a curve described by an implicit polynomial equation @xmath143 , the coefficients of the polynomial @xmath144 playing the role of parameters .",
    "the `` pure '' algebraic fit is based on minimizing @xmath145 ^ 2\\ ] ] where one of the coefficients of @xmath146 must be set to one to avoid the trivial solution in which all the coefficients turn zero .",
    "our af1 is exactly such a scheme . on the other hand ,",
    "the gwaf is based on minimizing _",
    "g = _ i=1^n [ calfg ] here @xmath147 is the gradient of the function @xmath144 .",
    "there is no need to set any coefficient of @xmath146 to one , since both numerator and denominator in ( [ calfg ] ) are homogeneous quadratic polynomials of parameters , hence the value of @xmath148 is invariant under the multiplication of all the parameters by a scalar .",
    "the reason why @xmath149 works better than @xmath150 is that we have , by the taylor expansion , @xmath151 where @xmath3 is the geometric distance from the point @xmath1 to the curve @xmath143 .",
    "thus , the function @xmath152 is simply the first order approximation to the classical objective function @xmath123 in ( [ fmain1 ] ) .",
    "the gwaf is known since at least 1974 @xcite .",
    "it was applied specifically to quadratic curves ( ellipses and hyperbolas ) by sampson in 1982 @xcite , and recently became standard in computer vision industry @xcite .",
    "this method is well known to be statistically optimal , in the sense that the covariance matrix of the parameter estimates satisfies the rao - cramer lower bound @xcite .",
    "we plan to investigate the statistical properties of the gwaf for the circle fitting problem in a separate paper , here we focus on its numerical implementation .",
    "in the case of circles , @xmath153 and @xmath154 , hence @xmath155 and the gwaf reduces to the minimization of _ g = _ i=1^n [ calfg2 ] this is a nonlinear problem that can only be solved iteratively , see some general schemes in @xcite .",
    "however , there are two approximations to ( [ calfg2 ] ) that lead to simpler and noniterative solutions .    *",
    "4.3 pratt s approximation to gwaf*. if data points @xmath1 lie close to the circle , then @xmath156 , and we approximate ( [ calfg2 ] ) by _ 2 = _",
    "i=1^n [ calf2 ] the objective function @xmath157 was proposed by pratt in 1987 @xcite , who clearly described its advantages over the `` pure '' algebraic fit .",
    "pratt proposed to minimize @xmath157 by using matrix methods , see below , which were computationally expensive .",
    "we describe a simpler yet absolutely reliable numerical algorithm below .    converting the function",
    "@xmath157 back to the original circle parameters @xmath80 gives the minimization problem _ 2(a , b , r ) = _ i=1^n [ x_i^2+y_i^2 - 2ax_i - 2by_i + a^2+b^2-r^2]^2 [ calf2r ] in this form it was stated and solved by chernov and ososkov in 1984 @xcite .",
    "they found a stable and efficient noniterative algorithm that did not involve expensive matrix computations . since 1984 , this algorithm has been used in experimental nuclear physics .",
    "we propose an improvement of their algorithm below .",
    "the objective function ( [ calf2r ] ) was also derived by kanatani in 1998 @xcite , but he did not suggest any numerical method for minimizing it .    the minimization of ( [ calfg2 ] ) is equivalent to the minimization of the simpler function ( [ f1a ] ) subject to the constraint @xmath158 .",
    "we write the function ( [ f1a ] ) in matrix form as @xmath159 , where @xmath160 is the vector of parameters and @xmath161 is the matrix of moments : =(      ) [ mmatrix4 ] note that @xmath161 is symmetric and positive semidefinite ( actually , @xmath161 is positive definite unless the data points are interpolated by a circle or a line ) .",
    "the constraint @xmath162 can be written as @xmath163 , where =(      ) [ bmatrix ] now introducing a lagrange multiplier @xmath164 we minimize the function @xmath165 differentiating with respect to @xmath166 gives @xmath167 .",
    "hence @xmath164 is a generalized eigenvalue for the matrix pair @xmath168 .",
    "it can be found from the equation ( * m*-)=0 [ etaeqn4 ] since @xmath169 is a polynomial of the fourth degree in @xmath164 , we arrive at a quartic equation @xmath170 ( note that the leading coefficient of @xmath171 is negative ) .",
    "the matrix @xmath172 is symmetric and has four real eigenvalues @xmath173 . in the generic case ,",
    "when the matrix @xmath174 is positive definite , by sylvester s law of inertia the generalized eigenvalues of the matrix pair @xmath175 are all real and exactly three of them are positive . in the special case when @xmath161 is singular , @xmath176 is a root of ( [ etaeqn4 ] ) . to determine which root of @xmath171 corresponds to the minimum of @xmath157 we observe that @xmath177",
    ", hence the minimum of @xmath157 corresponds to the smallest nonnegative root @xmath178 .",
    "the above analysis uniquely identifies the desired root @xmath179 of ( [ etaeqn4 ] ) , but we also need a practical algorithm to compute it .",
    "pratt @xcite proposed matrix methods to extract all eigenvalues and eigenvectors of @xmath180 , but admitted that those make his method a costly alternative to the `` pure algebraic fit '' .",
    "a much simpler way to solve ( [ etaeqn4 ] ) is to apply the newton method to the corresponding polynomial equation @xmath170 starting at @xmath176 .",
    "this method is guaranteed to converge to @xmath179 by the following theoretical result :    the polynomial @xmath181 is decreasing and concave up between @xmath176 and the first nonnegative root @xmath179 of @xmath171 .",
    "therefore , the newton algorithm starting at @xmath176 will always converge to @xmath179 .",
    "the cost of af2 is @xmath182 flops , here @xmath35 is the number of steps the newton method takes to find the root of ( [ etaeqn4 ] ) . in our tests ,",
    "5 steps were enough , on the average , and never more than 12 steps were necessary .",
    "hence the average cost of af2 is @xmath183 .    *",
    "4.4 taubin s approximation to gwaf*. another way to simplify ( [ calfg2 ] ) is to average the variables in the denominator : _",
    "3 = _ i=1^n [ calf3 ] where @xmath184 this idea was first proposed by agin @xcite but became popular after a publication by taubin @xcite , and it is known now as taubin method .",
    "the minimization of ( [ calf3 ] ) is equivalent to the minimization of @xmath135 defined by ( [ f1a ] ) subject to the constraint 4a^2 m_z + 4ab m_x + 4ac m_y + b^2n + c^2n = 1 this problem can be expressed in matrix form as @xmath185 , see 4.3 , with the constraint equation @xmath186 , where =(      ) [ cmatrix ] is a symmetric and positive semidefinite matrix . introducing a lagrange multiplier , @xmath164 as in 4.3",
    ", we arrive at the equation ( * m*-)=0 [ etaeqn3 ] here is an advantage of taubin s method over af2 : unlike ( [ etaeqn4 ] ) , ( [ etaeqn3 ] ) is a cubic equation for @xmath164 , we write it as @xmath187 .",
    "it is easy to derive from sylvester s law of inertia that all the roots of @xmath188 are real and positive , unless the data points belong to a line or a circle , in which case one root is zero .",
    "as in 4.3 , the minimum of @xmath189 corresponds to the smallest nonnegative root @xmath190 .",
    "taubin @xcite used matrix methods to extract eigenvalues and eigenvectors of the matrix pair @xmath191 .",
    "a simpler way is to apply the newton method to the corresponding polynomial equation @xmath187 starting at @xmath176 .",
    "this method is guaranteed to converge to the desired root @xmath179 since @xmath188 is obviously decreasing and concave up between @xmath176 and @xmath179 ( note that the leading coefficient of @xmath188 is negative ) .",
    "we denote the resulting algorithm by af3 .",
    "the cost of af3 is @xmath192 flops , here @xmath35 is the number of steps the newton method takes to find the root of ( [ etaeqn3 ] ) . in our tests ,",
    "5 steps were enough , on the average , and never more than 13 steps were necessary .",
    "hence the average cost of af3 is @xmath193 .",
    "this is 50 flops less than the cost of af2 .    *",
    "4.5 nonalgebraic ( heuristic ) fits*. some experimenters also use various simplistic procedures to initialize an iterative scheme .",
    "for example , some pick three data points that are sufficiently far apart and find the interpolating circle @xcite .",
    "others place the initial center of the circle at the centroid of the data @xcite .",
    "even though such `` quick and dirty '' methods are generally inferior to the algebraic fits , we will include two of them in our experimental tests for comparison .",
    "we call them tri and cen :    * tri : find three data points that make the triangle of maximum area and construct the interpolating circle .",
    "* cen : put the center of the circle at the centroid of the data and then compute the radius by ( [ rbarr ] ) .",
    "we note that our tri actually requires @xmath194 flops and hence is far more expensive than any algebraic fit . in practice , though , one can often make a faster selection of three points based on the same principle @xcite .    * 4.5 experimental tests*. here",
    "we combine the fitting algorithms in pairs : first , an algebraic ( or heuristic ) algorithm prefits a circle to the data , and then an iterative algorithm uses it as the initial guess and proceeds to minimize the objective function @xmath6 . our goal here is to evaluate the performance of the iterative methods described in section  3 when they are initialized by various algebraic ( or heuristic ) prefits , thus ultimately we determine the quality of those prefits .",
    "we test all 5 initialization methods ",
    "af1 , af2 , af3 , tri , and cen  and all 4 iterative schemes  lma , lmc , spa , and lan ( in the notation of section  3 )  a total of @xmath195 pairs .",
    "we conduct two major experiments , as we did in sect",
    "first , we generate @xmath115 samples of @xmath0 points randomly with a uniform distribution in the unit square @xmath37 . for each sample , we determine the global minimum of the objective function @xmath123 by running the most reliable iterative scheme , lma , starting at 1000 random initial guesses , as we did in section  3.3 .",
    "this is a credible ( though , expensive ) way to locate the global minimum of @xmath6 .",
    "then we apply all 20 pairs of algorithms to each sample .",
    "note that no pair needs an initial guess , since the first algorithm in each pair is just designed to provide one .    after running all @xmath196 samples ,",
    "we find , for each pair @xmath197 $ ] of algorithms ( @xmath198 and @xmath199 ) , the number of samples , @xmath200 , on which that pair successfully converged to the global minimum of @xmath6 ( recall that the minimum was predetermined at an earlier stage , see above ) . the ratio @xmath201 then represents the probability of convergence to the minimum of @xmath6 for the pair @xmath197 $ ] .",
    "we also find the average number of iterations the convergence takes , for each pair @xmath197 $ ] separately .",
    "this experiment was repeated for @xmath204 , and @xmath205 data points .",
    "the results are presented on figures  10 and 11 by grey - scale diagrams .",
    "the bars of the right explain our color codes . for brevity , we numbered the algebraic / heuristic methods :            fig .",
    "10 shows the probability of convergence to the global minimum of @xmath6 .",
    "we see that all the cells are white or light grey , meaning the reliability remains close to 100% ( in fact , it is 97 - 98% for @xmath40 , 98 - 99% for @xmath206 and almost 100% for @xmath207 . )    we conclude that for completely random samples filling the unit square uniformly , all five prefits are sufficiently accurate , so that any subsequent iterative method has little trouble converging to the minimum of @xmath6 .",
    "11 shows the computational cost for all pairs of algorithms , in the case of convergence .",
    "colors represent the number of flops per data point , as coded by the bar on the far right .",
    "we see that the cost remains relatively low , in fact it never exceeds 700 flops per point ( compare this to figs .  6 and 9 ) .",
    "the highest cost here is 684 flops per point for the pair tri+lan and @xmath40 points , marked by a cross .",
    "the most economical iterative scheme is the conventional levenberg - marquardt ( lmc , column b ) , which works well in conjunction with any algebraic ( heuristic ) prefit .",
    "the alternative levenberg - marquardt ( lma ) , spth ( spa ) and landau ( lan ) methods are slower , with lma leading this group for @xmath40 and trailing it for larger @xmath0 .",
    "there is almost no visible difference between the algebraic ( heuristic ) prefits in this experiment , except the tri method ( row 4 ) performs slightly worse than others , especially for @xmath208 ( not surprisingly , since tri is only based on 3 selected points ) .",
    "one should not be deceived by the overall good performance in the above experiment .",
    "random samples with a uniform distribution are , in a sense , `` easy to fit '' . indeed , when the data points are scattered chaotically , the objective function @xmath6 has no pronounced minima or valleys , and so it changes slowly in the vicinity of its minimum .",
    "hence , even not very accurate initial guesses allow the iterative schemes to converge to the minimum rapidly .",
    "we now turn to the second experiment , where data points are sampled , as in sect .",
    "3.3 , along a circular arc of radius @xmath42 with a gaussian noise at level @xmath48 .",
    "we set the number of points to @xmath206 and vary the arc length from @xmath128 to @xmath209 . in this case",
    "the objective function has a narrow sharp minimum or a deep narrow valley , and so the iterative schemes depend on an accurate initial guess .",
    "the results of this second experiment are presented on figures  12 and 13 , in the same fashion as those on figs .",
    "10 and 11 .",
    "we see that the performance is quite diverse and generally deteriorates as the arc length decreases .",
    "the probability of convergence to the minimum of @xmath6 sometimes drops to zero ( black cells on fig .",
    "12 ) , and the cost per data point exceeds our maximum of 2000 flops ( black cells on fig .  13 ) .",
    "first , let us compare the iterative schemes . the spth and landau methods ( columns c and d ) become unreliable and too expensive on small arcs .",
    "interestingly , though , landau somewhat outperforms spth here , while in earlier experiments reported in sect .",
    "3.3 , spth fared better .",
    "both levenberg - marquartd schemes ( columns a and b ) are quite reliable and fast across the entire range of the arc length from @xmath209 to @xmath128 .",
    "this experiment clearly demonstrates the superiority of the levenberg - marquardt algorithm(s ) over fixed - point iterative schemes such as spth or landau .",
    "the latter two , even if supplied with the best possible prefits , tend to fail or become prohibitively expensive on small circular arcs .",
    "lastly , we extend this test to even smaller circular arcs ( of 5 to 15 degrees ) keeping only the two levenberg - marquardt schemes in our race .",
    "the results are presented on figure  14 .",
    "we see that , as the arc gets smaller , the conventional levenberg - marquardt ( lmc ) gradually loses its reliability but remains quite efficient , while the alternative scheme ( lma ) gradually gives in speed but remains very reliable .",
    "interestingly , both schemes take about the same number of iterations to converge , for example , on @xmath124 arcs the pair af2+lma converged in 19 iterations , on average , while the pair af2+lmc converged in 20 iterations",
    ". the higher cost of the lma seen on fig .",
    "14 is entirely due to its complexity  one iteration of lma requires @xmath100 flops compared to @xmath81 for lmc , see sect .",
    "perhaps , the new lma method can be optimized for speed , but we did not pursue this goal here . in any case , the cost of lma per data point remains moderate , it is nowhere close to our maximum of 2000 flops ( in fact , it always stays below 1000 flops ) .",
    "we finally compare the algebraic ( heuristic ) methods .",
    "clearly , af1 , tri , and cen are not very reliable , with cen ( the top row ) showing the worst performance of all .",
    "our winners are af2 and af3 ( rows 2 and 3 ) whose characteristics seem to be almost identical , in terms of both reliability and efficiency .          in any case , our experiments clearly demonstrate the superiority of the af2 and af3 prefits over other algebraic and heuristic algorithms . the slightly higher cost of these methods themselves ( compared to af1 , for example ) should not be a factor here , since this difference is well compensated for by the faster convergence of the subsequent iterative schemes .",
    "our experiments were done on pentium iv personal computers and a dell power edge workstation with 32 nodes of dual 733mhz processors at the university of alabama at birmingham .",
    "the c++ code is available on our web page @xcite .",
    "g. taubin , estimation of planar curves , surfaces and nonplanar space curves defined by implicit equations , with applications to edge and range image segmentation , _ ieee transactions on pattern analysis and machine intelligence _ * 13 * , 1991 , 11151138 ."
  ],
  "abstract_text": [
    "<S> we study theoretical and computational aspects of the least squares fit ( lsf ) of circles and circular arcs . </S>",
    "<S> first we discuss the existence and uniqueness of lsf and various parametrization schemes . </S>",
    "<S> then we evaluate several popular circle fitting algorithms and propose a new one that surpasses the existing methods in reliability . </S>",
    "<S> we also discuss and compare direct ( algebraic ) circle fits . </S>"
  ]
}