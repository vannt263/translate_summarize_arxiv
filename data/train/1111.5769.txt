{
  "article_text": [
    "the profound interrelationship between information and thermodynamics was first brought to light by j.  c.  maxwell  @xcite in his gedankenexperiment about a hypothetical being of intelligence which was later christened by william thomson as maxwell s demon . since then , numerous discussions have been spurred as to whether and how maxwell s demon is compatible with the second law of thermodynamics  @xcite .",
    "to understand the roles of maxwell s demon , let us consider a situation in which a gas is confined in a box surrounded by adiabatic walls .",
    "a barrier is inserted at the center of the box to divide it into two .",
    "the temperatures of the gases in the two boxes are assumed to be initially the same .",
    "we assume that the demon is present near the barrier and can close or open a small hole in the barrier . if a faster - than - average ( slower - than - average ) molecule comes from the right ( left ) box , the demon opens the hole .",
    "otherwise , the demon keeps it closed . by doing so over and over again ,",
    "the temperature of the left gas becomes higher than that of the right one , in apparent contradiction with the second law of thermodynamics .",
    "this example illustrates the two essential roles of the demon :    * the demon observes individual molecules , and obtains the information about their velocities . *",
    "the demon opens or closes the hole based on each measurement outcome , which is the feedback control .",
    "in general , feedback control implies that a control protocol depends on the measurement outcome , or equivalently , we control a system based on the obtained information  @xcite .",
    "the crucial point here is that the measurements are performed at the level of thermal fluctuations ( i.e. , the demon can distinguish the velocities of individual molecules ) .",
    "therefore , maxwell s demon can be characterized as a feedback controller that utilizes information about a thermodynamic system at the level of thermal fluctuations ( see also fig .  1 ) .",
    "in the nineteenth century , it was impossible to observe and control individual atoms and molecules , and therefore it was not necessary to take into account the effect of feedback control in the formulation of thermodynamics .",
    "however , due to the recent advances in manipulating microscopic systems , the effect of feedback control on thermodynamic systems has become relevant to real experiments .",
    "we can simulate the role of maxwell s demon in real experiments and can reduce the entropy of small thermodynamic systems .",
    "in this chapter , we review a general theory of thermodynamics that involves measurements and feedback control  @xcite .",
    "we generalize the second law of thermodynamics by including information contents concerning the thermodynamics of feedback control .",
    "we note that , by the `` demon , '' we mean a type of devices that perform feedback control at the level of thermal fluctuations .",
    "this chapter is organized as follows . in sec .  2",
    ", we discuss the szilard engine which is a prototypical model of maxwell s demon and examine the consistency between the demon and the second law . in sec .  3 , we review information contents that are used in the following sections . in sec .  4",
    ", we discuss a generalized second law of thermodynamics with feedback control , which is the main part of this chapter . in sec .  5",
    ", we generalize nonequilibrium equalities such as the fluctuation theorem and the jarzynski equality to the case with feedback control . in sec .  6",
    ", we discuss the energy cost ( work ) that is needed for measurement and information erasure . in sec .  7 , we conclude this chapter .",
    "in 1929 , l.  szilard proposed a simple model of maxwell s demon that illustrates the quantitative relationship between information and thermodynamics  @xcite . in this section ,",
    "we briefly review the model , which is called the szilard engine , and discuss its physical implications .",
    "the szilard engine consists of a single - particle gas that is in contact with a single heat bath at temperature @xmath0 . by a measurement ,",
    "we obtain one bit of information about the position of the particle and use that information to extract work from the engine via feedback control .",
    "while the engine eventually returns to the initial equilibrium , the total amount of the extracted work is positive .",
    "the details of the control protocol are as follows ( see fig .  2 ) .",
    "_ step 1 : initial state .",
    "_ we prepare a single - particle gas in a box of volume @xmath1 , which is at thermal equilibrium with temperature @xmath0 .",
    "_ step 2 : insertion of the barrier .",
    "_ we insert a barrier in the middle of the box , and divide it into two with equal volume @xmath2 .",
    "_ step 3 : measurement .",
    "_ we perform an error - free measurement of the position of the particle to find which box the molecule is in . because the particle will be found to be in each box with probability @xmath3 , the amount of information gained from this measurement is one bit .",
    "we note that one ( @xmath4 ) bit of information in the binary logarithm corresponds to @xmath5 nat of information in the natural logarithm .",
    "_ step 4 : feedback .",
    "_ if the particle is in the left ( right ) box , we quasi - statically expand it by moving the barrier to the rightmost ( leftmost ) position . by this process",
    ", we can extract work @xmath6 given by @xmath7 where we used @xmath8 with @xmath9 , @xmath10 , and @xmath11 respectively being the pressure , the volume and the boltzmann constant .",
    "this process corresponds to feedback control , because the direction of the expansion ( i.e. , left or right ) depends on the measurement outcome .",
    "after this expansion , the gas returns to the initial thermal equilibrium with volume @xmath1 .",
    "the extracted work is proportional to the obtained information with proportionality constant @xmath12 .",
    "this is due to the fact that the entropy of the system is effectively decreased by @xmath13 via feedback control , and the decrease in entropy leads to the increase in the free energy by @xmath14 , which is the resource of the extracted work .",
    "the szilard engine _ prima facie _ seems to contradict the second law of thermodynamics , which dictates that one can not extract positive work from a single heat bath with a thermodynamic cycle ( kelvin s principle ) .",
    "in fact , the szilard engine is consistent with the second law , due to an additional energy cost ( work ) that is needed for the measurement and information erasure of the measurement device or the demon .",
    "this additional cost compensates for the excess work extracted from the engine . in his original paper ,",
    "szilard argued that there must be an entropic cost for the measurement process  @xcite .",
    "we stress that the work performed on the demon need not be transferred to the engine ; only the information obtained by the measurement should be utilized for the feedback .",
    "this is the crucial characteristic of the information heat engine .    by utilizing the obtained information in feedback control",
    ", we can extract work from the engine without decreasing its free energy , or we can increase the engine s free energy without injecting any work to the engine directly .",
    "the resource of the work or the free energy is thermal fluctuations of the heat bath ; by utilizing information via feedback , we can rectify the thermal energy of the bath and convert it into the work or the free energy .",
    "this method allows us to control the energy balance of the engine beyond the conventional thermodynamics .",
    "we shall call such a feedback - controlled heat engine as an `` information heat engine . ''",
    "the szilard engine works as the simplest model that illustrates the quintessence of information heat engines .",
    "quantum versions of the szilard engine have also been studied  @xcite .",
    "in this section , we briefly review the shannon information and the mutual information  @xcite .",
    "in particular , the mutual information plays a crucial role in thermodynamics of information processing .",
    "let @xmath15 be a probability variable which represents a finite set of possible events .",
    "we write as @xmath16 $ ] the probability of event @xmath17 being realized .",
    "the information content that is associated with event @xmath17 is then defined as @xmath18 $ ] , which implies that the rarer an event is , the more information is associated with it .",
    "the shannon information is then given by the average of @xmath18 $ ] over all possible events : @xmath19 \\ln p[x ] .",
    "\\label{shannon}\\ ] ] the shannon information satisfies @xmath20 , where @xmath21 is the number of the possible events ( the size of set @xmath22 ) . here , the lower bound ( @xmath23 ) is achieved if @xmath16 = 1 $ ] holds for some @xmath17 ; in this case , the event is indeed deterministic , while the upper bound ( @xmath24 ) is achieved if @xmath16 = 1/n$ ] for arbitrary @xmath17 . in general , the shannon information characterizes the randomness of a probability variable ; the more random the variable , the greater the shannon information .",
    "consider , for example , a simple case in which @xmath17 takes two values : @xmath25 or @xmath26 .",
    "we set @xmath27 = : p$ ] and @xmath28 = : 1-p$ ] with @xmath29 .",
    "the shannon information is then given by @xmath30 , which takes the maximum value @xmath5 for @xmath31 and the minimal value @xmath32 for @xmath33 or @xmath34 .",
    "the mutual information characterizes the correlation between two probability variables .",
    "let @xmath15 and @xmath35 be the two probability variables , and @xmath36 $ ] be their joint distribution .",
    "the marginal distributions are given by @xmath16 = \\sum_y p[x , y]$ ] , @xmath37 = \\sum_x p[x , y]$ ] .",
    "if the two variables are statistically independent , then @xmath36 = p[x]p[y]$ ] . otherwise , they are correlated .",
    "if the two variables are perfectly correlated , the joint distribution satisfies @xmath38 = \\delta ( x , f(y))p[x ] =   \\delta ( x , f(y))p[y ] , \\label{perfect_correlation}\\ ] ] where @xmath39 is the kronecker s delta and @xmath40 is a bijection function on @xmath41 . for example , if @xmath42 = p[1,0 ] = 1/2 $ ] and @xmath43 = p[1,1 ] = 0 $ ] with @xmath44 and @xmath45 , the two variables are perfectly correlated with @xmath46 and @xmath47 . if @xmath40 is the identity function that satisfies @xmath48 for any @xmath49 , eq .",
    "( [ perfect_correlation ] ) reduces to @xmath36 = \\delta ( x , y)p[x ] =   \\delta ( x , y)p[y]$ ] .",
    "the conditional probability of @xmath17 for given @xmath49 is given by @xmath50 = p[x , y ] / p[y]$ ] .",
    "if the two probability variables are statistically independent , the conditional probability reduces to @xmath50 = p[x]$ ] .",
    "this implies that we can not obtain any information about @xmath17 from knowledge of @xmath49 . on the other hand , in the case of the perfect correlation ( [ perfect_correlation ] )",
    ", we obtain @xmath50 = \\delta ( x , f(y))$ ] .",
    "this means that we can precisely estimate @xmath17 from @xmath49 by @xmath51 .",
    "we next introduce the joint shannon information and the conditional shannon information .",
    "the shannon information for the joint probability @xmath36 $ ] is given by @xmath52 \\ln p[x , y].\\ ] ] on the other hand , the shannon information for the conditional probability @xmath53 $ ] , where @xmath17 is the relevant probability variable , is given by @xmath54 \\ln p[x|y].\\ ] ] by averaging @xmath55 over @xmath49 , we define the conditional shannon information @xmath56 h(x|y ) = - \\sum_{xy } p[x , y ] \\ln p[x|y ] .",
    "\\label{conditional1}\\ ] ] the conditional shannon information satisfies the following properties : @xmath57 by definition , @xmath58 and @xmath59 .",
    "hence @xmath60 which implies that the randomness decreases if only one of the two variables is concerned .",
    "the mutual information is defined by @xmath61 or equivalently , @xmath62",
    "\\ln \\frac{p[x , y]}{p[x]p[y]}. \\label{mutual2}\\ ] ] as shown below , the mutual information satisfies @xmath63 here , @xmath64 is achieved if @xmath22 and @xmath41 are statistically independent , i.e. , @xmath36 = p[x]p[y]$ ] . on the other hand , @xmath65 is achieved if @xmath66 , or equivalently , if @xmath67 for any @xmath49 .",
    "this condition is equivalent to the condition that , for any @xmath49 , there exists a single @xmath17 such that @xmath53 = 1 $ ] , which implies that we can estimate @xmath17 from @xmath49 with certainty .",
    "similarly , @xmath68 is achieved if @xmath69 .",
    "in particular , if the correlation between @xmath17 and @xmath49 is perfect such that eq .",
    "( [ perfect_correlation ] ) holds , @xmath70 . in general",
    ", the mutual information describes the correlation between two probability variables ; the more strongly @xmath17 and @xmath49 are correlated , the larger @xmath71 is .",
    "the proof of inequalities  ( [ inequality_mutual ] ) goes as follows . because @xmath72 for @xmath73 , where the equality is achieved if and only if @xmath74 , we obtain @xmath75 \\ln \\frac{p[x]p[y]}{p[x , y ] } \\geq \\sum_x p[x , y ] \\left ( 1 - \\frac{p[x]p[y]}{p[x , y ] } \\right ) = 0,\\ ] ] which implies @xmath76 . on the other hand , eq .",
    "( [ mutual1 ] ) leads to @xmath77 which implies @xmath78 and @xmath79 .",
    "we note that eqs .",
    "( [ conditional2 ] ) , ( [ mutual1 ] ) , and ( [ mutual3 ] ) can be illustrated by using a venn diagram shown in fig .",
    "3  @xcite .",
    "this diagram is useful to memorize the relationship among @xmath80 , @xmath81 , and @xmath71 .",
    ", width=188 ]    the mutual information can be used to characterize the effective information that can be obtained by measurements .",
    "let us consider a situation in which @xmath17 is a phase - space point of a physical system and @xmath49 is an outcome that is obtained from a measurement on the system . in the case of the szilard engine , @xmath17 specifies the location of the particle ( `` left '' or `` right '' ) , and @xmath49 is the measurement outcome . if the measurement is error - free as we assumed in sec .  2 , @xmath82 is always satisfied and the correlation between the two variables is perfect . in this case , @xmath70 holds , and therefore the obtained information can be characterized by the shannon information as is the argument in sec .  2 .",
    "in general , there exist measurement errors , and the obtained information by the measurement needs to be characterized by the mutual information .",
    "the less the amount of the measurement error is , the more the mutual information is .",
    "we next discuss the cases in which the probability variables take continuous values .",
    "the probability distributions such as @xmath36 $ ] and @xmath16 $ ] should then be interpreted as probability densities , where the corresponding probabilities are given by @xmath36dxdy$ ] and @xmath16dx$ ] with @xmath83 and @xmath84 being the integral elements .",
    "the shannon information of @xmath17 can be formally defined as @xmath85 \\ln p[x ] .",
    "\\label{shannon_continuous}\\ ] ] however , eq .  ( [ shannon_continuous ] )",
    "is not invariant under the transformation of the variable .",
    "in fact , if we change @xmath17 to @xmath86 such that @xmath16dx = p[x']dx'$ ] , eq .",
    "( [ shannon_continuous ] ) is given by @xmath87 \\ln p[x ' ] - \\int dx ' p[x ' ] \\ln \\bigl| \\frac{dx'}{dx } \\bigr|.\\ ] ] thus , that the shannon information is not uniquely defined for the case of continuous variables .",
    "only when we fix some probability variable , we can give the shannon information a unique meaning .",
    "on the other hand , the mutual information is defined as @xmath88 \\ln \\frac{p[x , y]}{p[x]p[y]},\\ ] ] which is invariant under the transformation of the variables . in this sense",
    ", the mutual information is uniquely defined for the cases of continuous variables , regardless of the choices of probability variables .",
    "we now discuss two typical examples of probability variables : discrete and continuous variables .",
    "[ [ example-1-binary - channel ] ] example 1 ( binary channel ) + + + + + + + + + + + + + + + + + + + + + + + + + +    we consider a binary channel with which at most one bit of information is sent from variable @xmath17 to @xmath49 [ see fig .  4 ( a ) ] .",
    "let @xmath89 be the sender s bit and @xmath90 be the receiver s bit .",
    "we regard this binary channel as a model of a measurement , in which @xmath17 describes the state of the measured system and @xmath49 describes the measurement outcome .",
    "we assume that the error in the communication ( or the measurement error ) is characterized by @xmath91   = 1-\\varepsilon_0 , \\",
    "p[x=0 | y=1 ]   = \\varepsilon_0 , \\\\ p[x=1 | y=0 ]   = \\varepsilon_1 , \\",
    "p[x=1 | y=1 ]   = 1-\\varepsilon_1 , \\end{split}\\ ] ] where @xmath92 and @xmath93 are the error rates for @xmath25 and @xmath26 , respectively .",
    "the crucial assumption here is that the error property is only characterized by a pair @xmath94 , which is independent of the probability distribution of @xmath17 . if @xmath95 , this model is called a binary symmetric channel .     and @xmath93 .",
    "( b ) mutual information @xmath71 versus error rate @xmath96 for a binary symmetric channel with @xmath95 and @xmath31 , which gives @xmath97.,width=377 ]    let @xmath98 = : p$ ] and @xmath99 = :",
    "1-p$ ] be the probability distribution of @xmath17 .",
    "the joint distribution of @xmath17 and @xmath49 is then given by @xmath100   = p(1-\\varepsilon_0)$ ] , @xmath101   = p\\varepsilon_0 $ ] , @xmath102   = ( 1-p)\\varepsilon_1 $ ] , @xmath103   = ( 1-p)(1-\\varepsilon_1)$ ] , and the distribution of @xmath49 is given by @xmath104 = p(1-\\varepsilon_0 ) + ( 1-p)\\varepsilon_1 = : q$ ] , @xmath105 = p \\varepsilon_0 + ( 1-p)(1-\\varepsilon_1 ) = : 1-q$ ] . by definition , we can show that the mutual information is given by @xmath106 where @xmath107 is the shannon information for @xmath41 , and we defined @xmath108 for @xmath109 and @xmath34 . from eq .",
    "( [ mutual_binary ] ) , @xmath68 holds for @xmath110 , @xmath111 , @xmath112 , or @xmath113 . for a binary symmetric channel , eq .",
    "( [ mutual_binary ] ) reduces to @xmath114 .",
    "figure 4 ( b ) shows @xmath71 versus @xmath96 for the case of @xmath31 .",
    "the mutual information takes the maximum value if @xmath115 or @xmath116 . in this case",
    ", we can precisely estimate @xmath17 from @xmath49 .",
    "we note that , if @xmath116 , we can just relabel `` @xmath32 '' and `` @xmath34 '' of @xmath49 such that @xmath82 holds .    [ [ example-2-gaussian - channel ] ] example 2 ( gaussian channel ) + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we next consider a gaussian channel with continuous variables @xmath17 and @xmath49 .",
    "let @xmath17 be the sender s variable or the `` signal '' and @xmath49 be the receiver s variable or the `` outcome . ''",
    "we can also interpret that @xmath17 describes the phase - space point of a physical system such as the position of a brownian particle , and that @xmath49 describes the outcome of the measurement on the system .",
    "we assume that the error is characterized by a gaussian noise @xmath117 = \\frac{1}{\\sqrt{2 \\pi n } } \\exp \\left ( - \\frac{(y - x)^2}{2n } \\right ) , \\label{gauss_error}\\ ] ] where @xmath21 is the variance of the noise . for simplicity",
    ", we also assume that the probability density of @xmath17 is also gaussian : @xmath118 = \\frac{1}{\\sqrt{2\\pi s } } \\exp \\left ( - \\frac{x^2}{2s } \\right ) , \\label{gauss_x}\\ ] ] where @xmath119 is the variance of @xmath17 .",
    "the joint probability density of @xmath17 and @xmath49 is then given by @xmath38 = p[y|x]p[x ] = \\frac{1}{\\sqrt{4\\pi^2 sn } } \\exp \\left ( - \\frac{x^2}{2s } - \\frac{(y - x)^2}{2n } \\right ) .",
    "\\label{gauss_xy}\\ ] ] on the other hand , the probability density of @xmath49 is given by @xmath120 =   \\frac{1}{\\sqrt{2 \\pi ( s+n ) } } \\exp \\left ( - \\frac{y^2}{2(s+n ) }",
    "\\right ) , \\label{gauss_y}\\ ] ] which implies that the variance of the outcome @xmath49 is enhanced by factor @xmath121 compared with the original variance @xmath119 of the signal @xmath17 .",
    "we can also calculate the conditional probability density as @xmath122 = \\frac{p[x , y]}{p[y ] } = \\frac{1}{\\sqrt{2 \\pi sn / ( s+n ) } } \\exp \\left ( - \\frac{s+n}{2sn } \\left ( x - \\frac{s}{s+n}y \\right)^2 \\right ) , \\label{gauss_conditional}\\ ] ] which implies that the variance of the conditional distribution of @xmath17 is suppressed compared with the original variance @xmath119 by a factor of @xmath123 .",
    "we can straightforwardly calculate the mutual information as @xmath124 which is determined by the signal - to - noise ratio @xmath125 alone . in the limit of @xmath126 , where the noise dominates the signal , the mutual information vanishes . on the other hand , in the limit of @xmath127 where the noise is negligible compared with the signal , the mutual information diverges , as the variable is continuous .",
    "in this section , we discuss a universal upper bound of the work that can be extracted from information heat engines such as the szilard engine . starting from a general argument for isothermal processes in sec .",
    "4.1 , we discuss two models with which the universal bound is achieved in sec .",
    "4.2 and 4.3 .",
    "moreover , we will discuss an experimental result that demonstrates an information heat engine in sec .",
    "we will also discuss the carnot efficiency with two heat baths in sec .",
    "in the conventional thermodynamics , we extract a work from a heat engine by changing external parameters such as the volume of a gas or the frequency of an optical tweezer .",
    "in addition to such parameter changes , we perform measurements and feedback control for the case of information heat engines .",
    "suppose that we have a thermodynamic engine that is in contact with a single heat bath at temperature @xmath0 .",
    "we perform a measurement on a thermodynamic engine , and obtain @xmath128 of mutual information .",
    "after that , we extract a positive amount of work by changing external parameters .",
    "the crucial point here is that the protocol of changing the external parameters can depend on the measurement outcome via feedback control .    in the case of the szilard engine ,",
    "we obtain @xmath5 nat of information , and extract @xmath14 of work . how much work",
    "can we extract in principle under the condition that we have @xmath128 of mutual information about the system ?",
    "the answer of this fundamental question is given by the following inequality  @xcite : @xmath129 where @xmath6 is the average of the work that is extracted from the engine , and @xmath130 is the free - energy difference of the engine between the initial and final states .",
    "inequality  ( [ second_feedback ] ) has been proved for both quantum and classical regimes  @xcite .",
    "however , the mutual information in ( [ second_feedback ] ) needs to be replaced by a quantum extension of the mutual information  @xcite for quantum cases .",
    "we will prove inequality  ( [ second_feedback ] ) for classical cases by invoking the detailed fluctuation theorem in sec .  5 .",
    "inequality  ( [ second_feedback ] ) states that we can extract an excess work up to @xmath131 if we utilize @xmath128 of mutual information obtained by the measurement . in the conventional thermodynamics , the upper bound of the extractable work",
    "is bounded only by the free - energy difference @xmath130 , which is determined by the initial and final values of external parameters . for information heat engines ,",
    "the mutual information is also needed to determine the upper bound of the extractable work . in this sense ,",
    "inequality  ( [ second_feedback ] ) is a generalization of the second law of thermodynamics for feedback - controlled processes , in which thermodynamic variables ( @xmath132 and @xmath130 ) and the information content ( @xmath128 ) are treated on an equal footing .",
    "the equality in ( [ second_feedback ] ) is achieved with the `` best '' protocol , which means that the process is quasi - static and the post - feedback state is independent of the measurement outcome , i.e. , we utilize all the obtained information .",
    "this condition is achieved by the szilard engine , as discussed above . some models that achieves the equality in ( [ second_feedback ] )",
    "have been proposed  @xcite .",
    "two of them  @xcite are discussed in secs .",
    "4.2 and 4.3 .",
    "the szilard engine , which gives @xmath133 , @xmath134 , and @xmath135 , achieves the upper bound of the extractable work , and its special role in information heat engines parallels that of the carnot cycle in conventional thermodynamics .",
    "we discuss a generalization of the szilard engine with measurement errors and imperfect feedback  @xcite , with which the equality in ( [ second_feedback ] ) is achieved .",
    "the control protocol is described as follows ( see fig .",
    "5 ) :        _ step 1 : initial state . _",
    "a single - particle gas is in thermal equilibrium , which is in contact with a single heat bath at temperature @xmath0 .",
    "_ step 2 : insertion of the barrier .",
    "_ we insert a barrier to the box and divide it into two with the volume ratio being @xmath136 .",
    "_ step 3 : measurement .",
    "_ we perform a measurement to find out which box the particle is in .",
    "the possible outcomes are `` left '' and `` right , '' which we respectively denote as `` @xmath32 '' and `` @xmath34 . ''",
    "the measurement can then be modeled by the binary channel with error rates @xmath92 and @xmath93 ( see sec .",
    "3 ) , where @xmath17 ( @xmath137 ) specifies the location of the particle and @xmath49 ( @xmath138 ) shows the measurement outcome .",
    "_ step 4 : feedback .",
    "_ we quasi - statically move the barrier depending on the measurement outcome .",
    "if the outcome is `` left '' ( @xmath139 ) , we move the barrier , so that the final ratio of the volumes of the two boxes is given by @xmath140 ( @xmath141 ) .",
    "if the outcome is `` right '' ( @xmath142 ) , we move the the barrier , so that the final ratio of the volumes of the two boxes is given by @xmath143 ( @xmath144 ) .",
    "we note that the feedback protocol is specified by @xmath145 .",
    "_ step 5 : removal of the barrier .",
    "_ we remove the barrier , and the system returns to the initial thermal equilibrium by a free expansion .",
    "we now calculate the amount of the work that is extracted in _",
    "step 4_. by using the equation of states @xmath8 , we find that the extracted work is @xmath146 $ ] for @xmath147 , @xmath148 $ ] for @xmath111 , @xmath149 $ ] for @xmath112 , and @xmath150 $ ] for @xmath113 . therefore the average work is given by @xmath151    the mutual information obtained by the measurement is given by eq .",
    "( [ mutual_binary ] ) .",
    "the upper bound of inequality ( [ second_feedback ] ) is not necessarily achieved with a general feedback protocol @xmath145 .",
    "we then maximize @xmath6 in terms of @xmath152 and @xmath153 .",
    "the optimal feedback protocol with the maximum work is determined by equations @xmath154 and @xmath155 , which lead to @xmath156 $ ] and @xmath157 $ ] .",
    "therefore , we obtain the maximum work as @xmath158 which achieves the upper bound of the generalized second law  ( [ second_feedback ] ) .",
    "we next discuss a feedback protocol for an overdamped langevin system , which also achieves the upper bound of inequality  ( [ second_feedback ] ) as shown in ref .",
    "we consider a brownian particle with a harmonic potential , which obeys the following overdamped langevin equation : @xmath159 where @xmath160 is a friction constant and @xmath161 is a gaussian white noise satisfying @xmath162 with @xmath163 being the delta function .",
    "the harmonic potential can be controlled through two external parameters @xmath164 such that @xmath165 where @xmath166 and @xmath167 respectively describe the spring constant and the center of the potential .",
    "we consider the following feedback protocol ( see fig .",
    "6 ) .",
    "_ step 1 : initial state . _",
    "the particle is initially in thermal equilibrium with initial external parameters @xmath168 and @xmath169 .",
    "_ step 2 : measurement .",
    "_ we measure the position of the particle and obtain outcome @xmath49 .",
    "the measurement error is assumed to be gaussian that is given by eq .",
    "( [ gauss_error ] ) , where @xmath119 is the variance of @xmath17 in the initial equilibrium state ( i.e. , @xmath170 ) , and @xmath21 is the variance of the noise in the measurement [ see eq .  ( [ gauss_error ] ) ] . immediately after the measurement ,",
    "the conditional probability is given by eq .",
    "( [ gauss_conditional ] ) .",
    "the obtained mutual information @xmath128 is given by eq .",
    "( [ gauss_mutual ] ) .",
    "_ step 3 : feedback . _ immediately after the measurement , we instantaneously change @xmath166 from @xmath171 to @xmath172 and @xmath167 from @xmath32 to @xmath173 . by this change ,",
    "the conditional distribution ( [ gauss_conditional ] ) becomes thermally equilibrated with new parameters @xmath174 .",
    "_ step 4 : work extraction .",
    "_ we quasi - statically expand the potential by changing @xmath166 from @xmath175 to @xmath171 thereby extracting the work .",
    "the system then get thermally equilibrated with parameters @xmath176 .",
    "we now calculate the work that can be extracted from this engine .",
    "let @xmath177 $ ] be the probability distribution of @xmath17 at time @xmath178 under the condition of @xmath49 .",
    "the average of the work for _ step 3 _ is given by @xmath179   p[x,0 | y ] p[y ]   dxdy   = 0 , \\label{work3}\\ ] ] where @xmath180 $ ] is given by @xmath50 $ ] in eq .",
    "( [ gauss_conditional ] ) , and @xmath37 $ ] is given by eq .",
    "( [ gauss_y ] ) .",
    "the work for _ step 4 _ is given by @xmath181 p[y ] \\frac{\\partial v}{\\partial \\lambda_1 } ( x , \\lambda_1 ( t ) , \\lambda_2 ( t ) ) \\\\ & = -\\frac{1}{2 } \\int_{k'}^{k } d\\lambda_1 \\int dxdy p[x , t | y]p[y ] ( x- \\mu_y)^2   =   - \\frac{1}{2 } \\int_{k'}^{k } d\\lambda_1 \\frac{k_{\\rm b}t}{\\lambda_1 } \\\\ & = \\frac{k_{\\rm b}t}{2 } \\ln \\frac{k'}{k } = \\frac{k_{\\rm b}t}{2 } \\ln \\left ( 1 + \\frac{s}{n } \\right ) , \\end{split } \\label{work4}\\ ] ] where we used the fact that the expansion is quasi - static .",
    "comparing eqs .",
    "( [ work3 ] ) and ( [ work4 ] ) with mutual information ( [ gauss_mutual ] ) , we obtain @xmath182 which achieves the upper bound of inequality  ( [ second_feedback ] ) .",
    "we next discuss a recent experiment that realized an information heat engine by using a real - time feedback control on a colloidal particle in water at room temperature  @xcite .    in the experiment , a colloidal particle with diameter",
    "300 nm is attached to the cover glass , and another particle is attached to the first one [ fig .  7 ( a ) ] .",
    "the second particle then moves around the first as a rotating brownian particle which we observe and control .",
    "an ac electric field is applied with four electrodes , and the particle undergoes an effective potential as illustrated in fig .  7 ( b ) .",
    "we note that the potential can take two configurations depending on the phases of the electric field .",
    "each configuration consists of a spatially - periodic potential and a constant slope .",
    "the slope is created by a constant torque around the circle along which the particle rotates .",
    "this potential is like spiral stairs .",
    "the depth of the periodic potential is about @xmath183 , and the gradient of the slope per angle @xmath184 is about @xmath12 .",
    "if the periodic potential without the slope was asymmetric and the two potential configurations were periodically switched , the particle would be transported in one direction as a flashing ratchet  @xcite . in the present setup ,",
    "however , the periodic potential without the slope is symmetric and is not switched periodically but switched in a manner that depends on the measured position of the particle via feedback control  @xcite .",
    "such a feedback - controlled ratchet has been experimentally realized  @xcite . in the present experiment , the work and the free energy were measured precisely for quantitatively comparing the experimental results with the theoretical bound  ( [ second_feedback ] ) .",
    "it has been pointed out  @xcite that the feedback - controlled ratchet , as well as the flashing ratchet , can be a model of biological molecular motors  @xcite .",
    "the feedback protocol in the experiment  @xcite was done as follows [ see fig .",
    "7 ( c ) ] .",
    "the position of the particle was probed every 40 ms by a microscope , a camera , and an image analyzer . only if the particle was found in the switching region described by `` s '' in fig .",
    "7 ( c ) , the potential configuration was switched after a short delay time @xmath96 . by this switching ,",
    "when the particle reached the hilltop , the potential is inverted , so that the peak of the potential changed into the bottom of the valley , and therefore the particle is transported to the right direction . without the switching , the particle would be more likely to go back to the left valley .",
    "this position - dependent switching via feedback control induces the reduction of the entropy in a manner analogous to the feedback control in the szilard engine . by performing this protocol many times",
    ", the particle is expected to be transported to the right direction by climbing up the potential slope .",
    "figure 8 ( a ) shows typical trajectories of the particle . if the feedback delay @xmath96 is sufficiently shorter than the relaxation time of the particle in each well ( @xmath185 ms ) , the particle climbs up the potential . if the feedback delay is longer than the relaxation time , the feedback does not work and the particle moves down the potential in agreement with the conventional second law of thermodynamics .",
    "figure 8 ( c ) shows the averaged velocity of the particle versus @xmath96 , which implies that the shorter the feedback delay is , the faster the average velocity is .",
    "figure 8 ( c ) shows the energy balance of this engine .",
    "the shaded region is prohibited by the conventional second law of thermodynamics @xmath186 , where @xmath130 is the free - energy difference corresponding to the height of the potential , @xmath132 is the work performed on the particle during the switching , and @xmath187 represents the ensemble average over all trajectories . by using information via feedback , however ,",
    "the shaded region is indeed achieved if @xmath96 is sufficiently small .",
    "the resource of the excess free - energy gain is thermal fluctuations of the heat bath , which are rectified by feedback control .",
    "this is an experimental realization of an information heat engine .",
    "ms , @xmath188 ms , and @xmath189 ms .",
    "( b ) the averaged velocity of the particle versus the feedback delay .",
    "( c ) energy balance of feedback control .",
    "the shaded region is prohibited by the conventional second law of thermodynamics , and can only be achieved by feedback control.,width=302 ]    for the case of @xmath190 ms , @xmath191 . on the other hand , the obtained information is given by @xmath192 , which can be calculated from the histogram of the measurement outcomes by assuming that the measurement is error - free . by comparing this experimental data with the theoretical bound ( [ second_feedback ] ) , the efficiency of this information heat engine is determined to be @xmath193 .",
    "the reason why the efficiency is less than unity is twofold : ( 1 ) the switching is not quasi - static but instantaneous , and ( 2 ) the obtained information is not utilized if the particle is found outside of the switching region .",
    "we note that various experiments that are analogous to maxwell s demon have been performed with , for example , a granular gas  @xcite , supramolecules  @xcite , and ultracold atoms  @xcite ; however , these examples do not explicitly involve the measurement and feedback , as the controlled system and the demon constitute an autonomous system in those experiments .",
    "such autonomous versions of maxwell s demon have also been theoretically studied  @xcite .",
    "in contrast , in the experiment of ref .",
    "@xcite , the demon ( the camera and the computer ) is separated from the controlled system ( the colloidal particle ) as in the case for the szilard engine . we also note that an information heat engine similar to that in ref .  @xcite has been proposed for an electron pump system in ref .",
    "@xcite .",
    "we next consider the case in which there are two heat baths and the process is a thermodynamic cycle . without feedback control , the heat efficiency is bounded by the carnot bound .",
    "if we perform measurements and feedback control on this system , the extractable work is bounded from above by  @xcite @xmath194 where @xmath195 and @xmath196 are the temperatures of the hot and cold heat baths , respectively , and @xmath197 is the heat that is absorbed by the engine from the hot heat bath .",
    "the proof of inequality ( [ carnot_feedback ] ) will be given in sec .",
    "the last term on the right - hand side ( rhs ) of ( [ carnot_feedback ] ) describes the effect of feedback .",
    "we note that the coefficient of the last term is given by the temperature of the cold bath .",
    "the equality in ( [ carnot_feedback ] ) is achieved in the following example .",
    "we consider a single - particle gas in a box , and quasi - statically control it as in the case of the usual carnot cycle .",
    "we then perform the szilard - engine - type operation consisting of measurement and feedback on the system while it is in contact with the cold bath . in this case",
    ", we have @xmath198 of excess work , and @xmath197 remains unchanged .",
    "therefore , the equality in ( [ carnot_feedback ] ) is achieved with @xmath135 .",
    "we can also achieve the equality in ( [ carnot_feedback ] ) if we perform the szilard - engine - type operation while the engine is in contact with the hot heat bath . in this case",
    ", we can extract @xmath199 of excess work , and @xmath197 is increased by @xmath199 .",
    "therefore , we again obtain the equality in ( [ carnot_feedback ] ) with @xmath135 .",
    "since the late 1990 s , a number of universal equalities have been found for nonequilibrium processes  @xcite , and they have been shown to reproduce the second law of thermodynamics and the fluctuation - dissipation theorem . the fluctuation theorem and the jarzynski equality are two prime examples of the nonequilibrium equalities . in this section ,",
    "we generalize the nonequilibrium equalities to situations in which a thermodynamic system is subject to measurements and feedback control in line with refs .  @xcite . as corollaries , we derive inequalities ( [ second_feedback ] ) and ( [ carnot_feedback ] ) .",
    "first of all , we review the nonequilibrium equalities without feedback control .",
    "we consider a stochastic thermodynamic system in contact with heat bath(s ) with inverse temperatures @xmath200 ( @xmath201 ) .",
    "let @xmath17 be the phase - space point of the system .",
    "the system is controlled through external parameters @xmath202 , which describe , for instance , the volume of a gas or the frequency of an optical tweezer .",
    "even when the initial state of the system is in thermal equilibrium , the system can be driven far from equilibrium by changing the external parameters .",
    "we consider such a stochastic dynamics of the system from time @xmath32 to @xmath203 .",
    "the state of the system stochastically evolves as @xmath204 under a deterministic protocol of the external parameters denoted collectively as @xmath205 .",
    "let @xmath206 be the trajectory of the phase - space point and @xmath207 be that of the external parameters . the heat that is absorbed by the system from heat bath `` @xmath208 '' is a trajectory - dependent quantity , which we write as @xmath209 $ ] .",
    "the work that is performed on the system is also trajectory - dependent and is denoted as @xmath210 $ ] .",
    "the first law of thermodynamics is then given by @xmath211 - h [ x(0 ) | \\lambda ( 0 ) ] = w[x_\\tau | \\lambda_\\tau ] + \\sum_m q_m [ x_\\tau | \\lambda_\\tau],\\ ] ] where @xmath212 $ ] is the hamiltonian with external parameters @xmath202 , and the work can be written as @xmath213 = \\int_0^\\tau \\frac { \\partial h}{\\partial \\lambda } [ x(t ) | \\lambda ( t ) ] \\frac{d \\lambda ( t)}{dt } dt.\\ ] ]    let @xmath214 $ ] be the probability density of trajectory @xmath215 with control protocol @xmath216 .",
    "it can be decomposed as @xmath214 = p [ x_\\tau | x(0 ) ,   \\lambda_\\tau ] p_{\\rm f}[x(0)]$ ] , where @xmath217 $ ] is the probability density under the condition that the initial state is @xmath218 , and @xmath219 $ ] is the initial distribution of the forward process .",
    "we next introduce backward processes of the system .",
    "let @xmath220 be the time - reversal of @xmath17 .",
    "for example , if @xmath221 with @xmath222 being the position and @xmath223 being the momentum , then @xmath224 .",
    "similarly , we denote the time - reversal of @xmath202 as @xmath225 .",
    "for example , if @xmath202 is the magnetic field , then @xmath226 .",
    "let @xmath227 be the time - reversed trajectory of @xmath215 defined as @xmath228 .",
    "we also write the time - reversal of control protocol @xmath216 as @xmath229 , and write as @xmath230 $ ] the probability density of the time - reversed trajectory with the time - reversed control protocol",
    ". we can decompose @xmath230 $ ] as @xmath231 = p [ x_\\tau^\\dagger | x^\\dagger ( 0 ) ,   \\lambda_\\tau ] p_{\\rm b}[x^\\dagger(0)]$ ] , where @xmath232 $ ] is the probability density under the condition that the initial state of the backward process is @xmath233 , and @xmath234 $ ] the initial distribution of the backward process .",
    "we stress that the initial distribution of the backward process @xmath234 $ ] can be set independently of the final distribution of the forward process . in experiments , we can initialize the system before we start a backward process so that its initial distribution can be chosen independently of the forward process .    the detailed fluctuation theorem ( the transient fluctuation theorem )",
    "is given by  @xcite @xmath235}{p [ x_\\tau | x(0 ) , \\lambda_\\tau ] } = e^{\\sum_m \\beta_m q_m [ x_\\tau | \\lambda_\\tau ] } .",
    "\\label{fluctuation1}\\ ] ] defining the entropy production as @xmath236 : = \\ln p_{\\rm f}[x(0 ) ] - \\ln p_{\\rm b}[x^\\dagger(0 ) ] - \\sum_m q_m[x_\\tau | \\lambda_\\tau ] , \\label{entropy_production1}\\ ] ] we obtain @xmath237}{p [ x_\\tau | \\lambda_\\tau ] } = e^{-\\sigma [ x_\\tau | \\lambda_\\tau]}. \\label{fluctuation2}\\ ] ] by taking the ensemble average of eq .",
    "( [ fluctuation2 ] ) , we have @xmath238 e^{-\\sigma [ x_\\tau | \\lambda_\\tau ] } = \\int dx_\\tau p [ x_\\tau | \\lambda_\\tau ]   \\frac{p [ x_\\tau^\\dagger | \\lambda_\\tau^\\dagger]}{p [ x_\\tau |",
    "\\lambda_\\tau]}= \\int dx_\\tau^\\dagger p [ x_\\tau^\\dagger | \\lambda_\\tau^\\dagger ]   = 1,\\ ] ] where we used @xmath239 .",
    "therefore , we obtain the integral fluctuation theorem @xmath240 by using the concavity of the exponential function , we find from eq .",
    "( [ integral_fluctuation ] ) that @xmath241 which is an expression of the second law of thermodynamics . in the following ,",
    "we discuss the physical meanings of entropy production @xmath242 for typical situations .",
    "the equality in ( [ second1 ] ) is achieved if @xmath231 = p [ x_\\tau | \\lambda_\\tau]$ ] holds for any @xmath215 , which implies the reversibility of the process .",
    "we first consider isothermal processes . in this case",
    ", we choose the initial distributions of the forward and backward processes as @xmath243 & = \\exp \\left ( \\beta ( f[\\lambda ( 0 ) ] - h [ x(0 ) | \\lambda ( 0 ) ] ) \\right ) , \\\\",
    "p_{\\rm b } [ x^\\dagger(0 ) ] & = \\exp \\left ( \\beta ( f[\\lambda^\\dagger ( 0 ) ] - h [ x^\\dagger ( 0 ) | \\lambda^\\dagger ( 0 ) ] ) \\right ) ,   \\end{split } \\label{initial}\\ ] ] where @xmath244 : = - k_{\\rm b}t \\ln \\int dx e^{-\\beta h[x | \\lambda]}$ ] is the free energy of the system .",
    "we assume that the hamiltonian has the time - reversal symmetry @xmath245 = h[x^\\ast | \\lambda^\\ast],\\ ] ] and therefore the canonical distribution satisfies @xmath246 = \\exp \\left ( \\beta ( f[\\lambda ( \\tau ) ] - h [ x ( \\tau ) | \\lambda ( \\tau ) ] ) \\right).\\ ] ] the entropy production then reduces to @xmath247 = \\beta ( w [ x_\\tau ] - \\delta f ) , \\label{entropy_production2}\\ ] ] where @xmath248 - f[\\lambda ( 0)]$ ] .",
    "thus , the integral fluctuation theorem  ( [ integral_fluctuation ] ) leads to the jarzynski equality  @xcite @xmath249 and inequality  ( [ second1 ] ) gives the second law of thermodynamics @xmath250 where @xmath251 is the work that is extracted from the system .",
    "we next consider a case with multi - heat baths , and assume that the initial distributions of the forward and the backward processes are given by the canonical distributions as in  ( [ initial ] ) with a reference inverse temperature @xmath252 .",
    "in practice , @xmath252 can be taken as one of @xmath200 s , which can be realized if the system is initially attached only to that particular heat bath .",
    "we then have @xmath247 = \\beta ( \\delta e [ x_\\tau ] -\\delta f ) - \\sum_m \\beta_m q_m [ x_\\tau ] , \\label{entropy_production3}\\ ] ] where @xmath253 : = h [ x ( \\tau ) | \\lambda ( \\tau ) ] - h [ x ( 0 ) | \\lambda ( 0)]$ ] is the difference of the internal energy of the system . inequality  ( [ second1 ] ) leads to @xmath254 in particular , if the process is a cycle such that @xmath134 and @xmath255 hold , inequality  ( [ second3 ] ) reduces to @xmath256 which is the clausius inequality .",
    "if there are two heat baths with temperatures @xmath195 and @xmath196 , ( [ clausius ] ) gives the carnot bound @xmath257 where @xmath258 is the average of the heat that is absorbed by the engine from the hot heat bath .",
    "we now formulate measurements and feedback on the thermodynamic system  @xcite .",
    "we perform measurements at time @xmath259 ( @xmath260 ) with @xmath261 .",
    "let @xmath262 be the measurement outcome at time @xmath259 . for simplicity ,",
    "we assume that the measurement is instantaneous ; the measurement error of @xmath262 can be characterized only by the conditional probability @xmath263 $ ] , which implies that only @xmath262 has the information about @xmath264 .",
    "( note , however , that this assumption can be relaxed  @xcite . )",
    "we write the sequence of the measurement outcomes as @xmath265 , and write @xmath266 : = \\prod_k p_{\\rm c}[y ( t_k ) | x(t_k ) ] .",
    "\\label{measurement_probability}\\ ] ] we then introduce the following quantity : @xmath267 : = \\ln \\frac{p_{\\rm c}[y_\\tau | x_\\tau]}{p[y_\\tau ] } , \\label{stochastic_mutual}\\ ] ] which can be interpreted as a stochastic version of the mutual information . the ensemble average of eq .",
    "( [ stochastic_mutual ] ) gives the mutual information obtained by the measurements as @xmath268 \\ln \\frac{p_{\\rm c}[y_\\tau | x_\\tau]}{p[y_\\tau]}. \\label{average_mutual}\\ ] ] we note that @xmath269 describes the correlation between @xmath215 and @xmath270 that is induced only by measurements , and not by feedback control .",
    "the suffix `` c '' represents this property of @xmath271 .",
    "we then identify @xmath269 with @xmath128 in the foregoing arguments .",
    "see ref .",
    "@xcite for details .",
    "we note that @xmath269 has been discussed and referred to as the transfer entropy in ref .",
    "the following equality also holds by definition : @xmath272    we next consider feedback control by using the obtained outcomes . the control protocol after time @xmath259 can depend on the outcome @xmath262 in the presence of feedback control .",
    "we write this dependence as @xmath273 .",
    "we can show that the joint probability of @xmath274 is given by @xmath275 = p_{\\rm c}[y_\\tau | x_\\tau ] p[x_\\tau | \\lambda ( y_\\tau ) ] , \\label{joint_probability}\\ ] ] which satisfies the normalization condition @xmath276 = 1 $ ] .",
    "equality  ( [ joint_probability ] ) is proved in appendix a ( see also ref .",
    "the probability of obtaining outcome @xmath270 in the forward process is given by the marginal distribution as @xmath277 = \\int dx_\\tau p[x_\\tau , y_\\tau ] $ ] , and the conditional probability is given by @xmath278 = p[x_\\tau , y_\\tau]/p[y_\\tau]$ ] . in the presence of measurement and feedback , the ensemble average is taken over all trajectories and all outcomes ; for an arbitrary stochastic quantity @xmath279 $ ] , its ensemble average is given by @xmath280 a[x_\\tau , y_\\tau ] .",
    "\\label{average}\\ ] ]    the detailed fluctuation theorem for a given @xmath270 can be written as @xmath281}{p [ x_\\tau | \\lambda_\\tau ( y_\\tau ) ] } = e^{-\\sigma [ x_\\tau | \\lambda_\\tau ( y_\\tau)]}. \\label{fluctuation3}\\ ] ] we note that eq .",
    "( [ fluctuation3 ] ) is valid in the presence of feedback control , because the detailed fluctuation theorem is satisfied once a control protocol is fixed .",
    "equality  ( [ fluctuation3 ] ) provides the basis for the derivations of the formulas in the following section .      in this subsection",
    ", we generalize the nonequilibrium equalities by incorporating the mutual information .",
    "first of all , from eq .",
    "( [ stochastic_mutual ] ) , we have @xmath282}{p_{\\rm c}[y_\\tau | x_\\tau ] } = e^{-i_{\\rm c}}. \\label{stochastic_mutual2}\\ ] ] by multiplying the both - hand sides of this equality by those of eq .",
    "( [ fluctuation3 ] ) , we obtain @xmath281 p[y_\\tau ] } { p[x_\\tau , y_\\tau ] } = e^{-\\sigma [ x_\\tau | \\lambda_\\tau ( y_\\tau ) ] - i_{\\rm c}[x_\\tau : y_\\tau]}. \\label{fluctuation_feedback1}\\ ] ] to measure @xmath283 p[y_\\tau ] $ ] , we follow the backward process corresponding to each forward outcome and count the occurrences of the time - reversed trajectories . by taking the ensemble average of the both - hand sides of eq .",
    "( [ fluctuation_feedback1 ] ) with formula  ( [ average ] ) , we obtain a generalized integral fluctuation theorem with feedback control : @xmath284 using the concavity of the exponential function , eq .",
    "( [ integral_fluctuation_feedback1 ] ) leads to @xmath285 inequality  ( [ second_feedback1 ] ) is a generalized second law of thermodynamics , which states that the entropy production can be decreased by feedback control , and that the lower bound of the entropy production is given by the mutual information @xmath269 . as shown below , inequalities  ( [ second_feedback ] ) and ( [ carnot_feedback ] ) in sec",
    ".  4 are special cases of inequality  ( [ second_feedback1 ] ) .",
    "the equality in ( [ second_feedback1 ] ) is achieved if @xmath283 p[y_\\tau ] = p[x_\\tau , y_\\tau]$ ] holds for any @xmath215 and @xmath270 , which implies the reversibility with feedback control as discussed in ref .",
    "@xcite .",
    "the generalized integral fluctuation theorem of the form ( [ integral_fluctuation_feedback1 ] ) was first shown in ref .",
    "@xcite for a single measurement , and eq .",
    "( [ fluctuation_feedback1 ] ) was obtained in ref .",
    "@xcite for multiple measurements .",
    "these results has also been generalized to the optimal control process with continuous measurement and the kalman filter in ref .",
    "@xcite .",
    "a generalized fluctuation theorem was also obtained in ref .",
    "@xcite , which is similar to eq .",
    "( [ integral_fluctuation_feedback1 ] ) . in ref .",
    "@xcite , feedback control is performed based on information about the continuously - monitored velocity of a langevin system . the result of ref .",
    "@xcite includes an quantity that describes the decrease in the entropy by continuous feedback control , instead of the mutual information obtained by the continuous measurement .",
    "we consider isothermal processes with a single heat bath , in which the entropy production is given by eq .",
    "( [ entropy_production2 ] ) .",
    "equality  ( [ integral_fluctuation_feedback1 ] ) then reduces to a generalized jarzynski equality @xmath286 and inequality  ( [ second_feedback1 ] ) reduces to @xmath287 which implies inequality ( [ second_feedback ] ) with identifications @xmath288 , @xmath289 , and @xmath290 .",
    "we next consider the cases in which there are two heat baths and the process is a cycle , in which the entropy production is given by the ensemble average of eq .",
    "( [ entropy_production3 ] ) with @xmath291 .",
    "the generalized second law  ( [ second_feedback1 ] ) then leads to @xmath292 which can be rewritten as @xmath293 by identifying @xmath294 , inequality  ( [ second_feedback3 ] ) implies inequality ( [ carnot_feedback ] ) .      in this subsection",
    ", we discuss another generalization of nonequilibrium equalities .",
    "we define the time - reversal of outcomes @xmath270 as @xmath295 , where @xmath296 is the time - reversal of @xmath49 , and introduce the probability that we obtain outcome @xmath297 with control protocol @xmath298 , which is given by @xmath299 = \\int dx_\\tau^\\dagger p_{\\rm c}[y_\\tau^\\dagger | x_\\tau^\\dagger ] p[x_\\tau^\\dagger | \\lambda_\\tau ( y_\\tau)^\\dagger ] .",
    "\\label{reverse_probability}\\ ] ] we stress that no feedback control is performed in the backward processes .",
    "we then assume that the measurement error has the time - reversal symmetry @xmath300   = p_{\\rm c}[y_\\tau | x_\\tau ] .",
    "\\label{assumption_measurement}\\ ] ] this assumption is satisfied if @xmath301 = p_{\\rm c}[y(\\tau - t_k)^\\ast | x(\\tau - t_k)^\\ast]$ ] holds for @xmath302 . by using eq .",
    "( [ reverse_probability ] ) and assumption ( [ assumption_measurement ] ) , we can show that @xmath303 } { p[y_\\tau ] } = \\langle e^{- \\sigma } \\rangle_{y_\\tau } , \\label{kpb}\\ ] ] where @xmath304 denotes the conditional average with condition @xmath270 such that @xmath305 } p[x_\\tau | y_\\tau].\\ ] ] equality  ( [ kpb ] ) has been shown for hamiltonian systems  @xcite and stochastic systems  @xcite . by noting that @xmath306 \\langle e^{-\\sigma } \\rangle_{y_\\tau},\\ ] ] we obtain yet another generalization of the integral fluctuation theorem  @xcite @xmath307 where @xmath308 \\label{gamma}\\ ] ] is the sum of the probabilities that we obtain the time - reversed outcomes with a time - reversed protocol .",
    "for the cases of isothermal processes , eq .",
    "( [ integral_fluctuation_feedback2 ] ) reduces to @xmath309    we note that @xmath310 characterizes the efficacy of feedback control .",
    "the more efficient the feedback protocol is , the larger the amount of @xmath310 is . without feedback control ,",
    "@xmath311 $ ] reduces to a single unconditional probability distribution , and we therefore obtain @xmath312 = 1,\\ ] ] which reproduces the integral fluctuation theorem ( [ integral_fluctuation ] ) without feedback .",
    "we note that the maximum value of @xmath310 is the number of the possible outcomes of @xmath270 .",
    "we illustrate the efficacy parameter @xmath310 for the case of the szilard engine that is described in sec .  2 .",
    "the backward control protocol of the szilard engine is as follows ( see also fig .",
    "9 )  @xcite .",
    "_ step 1 : initial state . _",
    "the single - particle gas is initially in thermal equilibrium .",
    "_ step 2 : compression of the box . _ in accordance with the measurement outcome in the forward process , which is `` @xmath32 '' ( @xmath313 `` left '' ) or `` @xmath34 '' ( @xmath313 `` right '' ) , we quasi - statically compress the box by moving the wall in the box to the center . by this compression ,",
    "the volume of the box becomes half .",
    "_ step 3 : measurement .",
    "_ we measure the position of the particle to find which box the particle is in .",
    "the outcome of this backward measurement is `` @xmath32 '' ( @xmath313 `` left '' ) or `` @xmath34 '' ( @xmath313 `` right '' ) with unit probability corresponding to forward outcome `` @xmath32 '' or `` @xmath34 , '' respectively .",
    "_ step 4 : _ we remove the barrier at the center of the box , and the engine returns to the initial state by a free expansion .    in these backward processes , the measurement outcomes in _ step 2",
    "_ satisfy @xmath314 = 1 $ ] and @xmath315 = 1 $ ] , and therefore we obtain @xmath316 + p[1 | \\lambda_\\tau ( 1)^\\dagger ] = 2 $ ] , which gives the maximum value of @xmath310 for situations in which the number of possible outcomes is two . on the other hand , since @xmath317 and @xmath134 in the absence of fluctuations , the generalized jarzynski equality ( [ jarzynski_feedback2 ] ) is satisfied as @xmath318 .    the generalized jarzynski equality ( [ jarzynski_feedback2 ] ) has been experimentally verified in the experiment described in sec .",
    "4.4 by measuring @xmath319 and @xmath310 separately in the forward and backward experiments , respectively  @xcite .",
    "equalities  ( [ jarzynski_feedback1 ] ) and ( [ jarzynski_feedback2 ] ) have been obtained in hamiltonian systems  @xcite .",
    "equality  ( [ jarzynski_feedback2 ] ) has also been generalized to quantum systems  @xcite .    while eq .",
    "( [ integral_fluctuation_feedback1 ] ) only includes the obtained mutual information and does not describe how we utilize the information via feedback , eq .",
    "( [ integral_fluctuation_feedback2 ] ) includes the term of feedback efficacy that depends on the feedback protocol .",
    "to quantitatively discuss the relationship between mutual information @xmath271 and efficacy parameter @xmath310 , we define @xmath320 : = - \\ln \\langle e^{-a } \\rangle$ ] . by noting eq .",
    "( [ fluctuation_information ] ) , we obtain @xmath321 + c[i_{\\rm",
    "c } ] - c[\\sigma + i_{\\rm c } ] = - \\ln \\gamma . \\label{i_gamma1}\\ ] ] if the joint distribution of @xmath242 and @xmath271 is gaussian , eq .",
    "( [ i_gamma1 ] ) reduces to @xmath322 equalities  ( [ i_gamma1 ] ) and ( [ i_gamma2 ] ) imply that , the more efficiently we use the obtained information to decrease the entropy production by feedback control , the larger @xmath310 is . in fact , if @xmath310 is large , the left - hand sides of eqs .",
    "( [ i_gamma1 ] ) and ( [ i_gamma2 ] ) are both small , which means that the obtained information @xmath271 has a large negative correlation with @xmath242 . without feedback control",
    ", @xmath323 holds and therefore @xmath271 is not correlated with @xmath242 . in this sense , @xmath310 characterizes the efficacy of feedback control .",
    "so far , we have discussed the energy balance of information heat engines controlled by the demon . in this section ,",
    "we discuss the energy cost that is needed for the demon itself , which has been a subject of active discussion  @xcite .",
    "suppose that the demon has a memory that can store the outcome obtained by a measurement .",
    "if the outcome is binary , the memory can be modeled by a system with a binary potential ( see fig .",
    "before the measurement , the memory is in the initial standard state @xmath32 .",
    "the memory then interacts with a measured system such as the szilard engine , and stores the measurement outcome .",
    "figure 10 illustrates a case with a binary outcome .",
    "let @xmath324 be the probability of obtaining outcome @xmath171 .",
    "after the measurement , the memory is detached from the measured system and returns to the initial standard state , which is the erasure of the obtained information .",
    "the central question in this section is how much work is needed for the demon during the measurement and the information erasure .",
    "let @xmath325 be the free energy of the memory under the condition that the outcome is `` @xmath171 . '' during the measurement process",
    ", the free energy of the memory is changed on average by @xmath326 , where @xmath327 is the free energy of the initial standard state .",
    "if @xmath325 s are the same for all @xmath171 s including @xmath328 ( i.e. , the memory s potential is symmetric ) , @xmath329 holds for every @xmath330 .",
    "it has been shown  @xcite that the averaged work @xmath331 that is performed on the memory during the measurement is bounded as @xmath332 where @xmath333 is the shannon information of the outcomes and @xmath128 is the mutual information obtained by the measurement .",
    "for the special case with @xmath329 and @xmath334 , the rhs of inequality ( [ measurement ] ) reduces to zero .    on the other hand , during the information erasure",
    ", the change of the free energy of the memory is given by @xmath335 .",
    "the averaged work @xmath336 that is needed for the erasure process is bounded as  @xcite @xmath337 if @xmath338 vanishes , inequality ( [ erasure ] ) reduces to @xmath339 which is known as the landauer principle  @xcite .",
    "the additional term @xmath340 on the rhs of ( [ erasure ] ) arises from the asymmetry of the memory . by summing up inequalities ( [ measurement ] ) and ( [ erasure ] ) ,",
    "we obtain the fundamental inequality @xmath341 which implies that the work needed for the demon is only bounded by the mutual information if we take into account both the measurement and erasure processes .",
    "we stress that , while inequality ( [ erasure ] ) is a generalized landauer principle for the information erasure , inequality ( [ total ] ) is completely different from the landauer principle .",
    "in fact , while the lower bound of the landauer principle is given by the shannon information that characterizes the randomness of the measurement outcomes , the lower bound of ( [ total ] ) is given by the mutual information that characterizes the correlation between the measured system and the measurement outcome .",
    "moreover , both terms on the rhs of ( [ erasure ] ) is exactly canceled by the first and second terms on the rhs of ( [ measurement ] ) .",
    "the reason for the cancellation lies in the fact that the dynamics of the memory during the erasure process is the time - reversal of the measurement process , except for the fact that the memory interacts with the measured system and establishes a correlation ( or equivalently , gains information ) only in the measurement process ( see also fig .",
    "the additional cost for the establishment of the correlation is given by the last term on the rhs of ( [ measurement ] ) , which also appears in the rhs of ( [ total ] ) .",
    "therefore , the mutual information term in inequality ( [ total ] ) is induced by the measurement process .",
    "historically , there has been a lot of discussions  @xcite as to what compensates for the additional work of @xmath14 which can be extracted from the szilard engine .",
    "szilard considered that an entropic cost must be needed for the measurement process  @xcite .",
    "l.  brillouin  @xcite argued that we need the work greater than @xmath14 for the measurement process , based on a specific model of measurement .",
    "later , by explicitly constructing a model of the memory does not require any work for the measurement , c. h. bennett argued that , based on the landauer principle  ( [ landauer ] ) , we always need the work of at least @xmath14 for the erasure process  @xcite .",
    "the key observation here is that the erasure process is logically irreversible while the measurement process can be logically reversible .",
    "in fact , if we assume that the shannon information of the measurement outcome equals the thermodynamic entropy of the memory , the logically irreversible erasure should be accompanied by a reduction in the thermodynamic entropy of the memory , which implies that @xmath14 of heat should be transfered to the heat bath and , therefore , the same amount of the work is needed .    however , the argument by landauer and bennett is valid only for symmetric memories with @xmath329 . as discussed in refs .",
    "@xcite , the shannon information does not equal the thermodynamic entropy of the memory in general . if the memory is asymmetric as illustrated in fig .  10 ,",
    "the lower bound of the energy cost needed for the information erasure is not given by ( [ landauer ] ) , and the landauer principle needs to be generalized to inequality  ( [ erasure ] ) for asymmetric memories .",
    "we note that the landauer principle can also be violated for symmetric memories in the quantum regime due to the initial correlation between the memory and the heat bath  @xcite .",
    "a more detailed historical review about the landauer principle is given in ref .",
    "@xcite .    as a consequence , the lower bound of the individual energy cost for measurement or erasure processes can be made arbitrarily small for asymmetric memories , while their sum  ( [ total ] ) is bounded from below by @xmath131 that originates from the measurement process .",
    "the total work given in the left - hand - side of ( [ total ] ) then compensates for @xmath342 of additional work in ( [ second_feedback ] ) that is extracted from an information heat engine by the demon .",
    "this compensation confirms the consistency between the demon and the second law of thermodynamics ; we can not extract any positive amount of work by a cycle from the total system consisting of the engine and the memory of the demon .",
    "nevertheless , feedback control is still useful for manipulating small thermodynamic systems .",
    "in fact , as discussed in sec .  2 , feedback control enables us to increase the engine s free energy without injecting energy to the engine directly . in other words ,",
    "the work ( [ total ] ) needed for the demon is not necessarily transfered to the engine , which can be energetically separated from the demon .",
    "therefore , by using information heat engines , we can control thermodynamic systems beyond the energy balance that is imposed by the conventional thermodynamics .",
    "in this chapter , we have discussed a generalized thermodynamics that can be applied to feedback - controlled systems which we call information heat engines . the szilard engine described in sec .",
    "2 is the simplest model of information heat engines .",
    "based on the information theory reviewed in sec .  3 ,",
    "we have formulated a generalized second law involving the term of the mutual information in sec .",
    "the generalized second law gives an upper bound of the work that can be extracted from a heat bath with the assistance of feedback control .",
    "we also discussed some typical examples of information heat engines including a recent experimental result  @xcite . in sec .  5 , we discussed nonequilibrium equalities with feedback control , and derived the generalized second law discussed in sec .  4 .",
    "we also discussed the energy cost that is needed for the measurement and the information erasure in sec .",
    "6 .    inequalities ( [ second_feedback ] ) , ( [ measurement ] ) , ( [ erasure ] ) , and ( [ total ] ) are the generalizations of the second law of thermodynamics , giving the fundamental bounds of the work needed for information processing . in fact , if we set the information contents to be zero ( i.e. , @xmath343 ) in these inequalities , all of them reduce to the conventional second law of thermodynamics . in this sense , these inequalities constitute the second law of `` information thermodynamics , '' which is a generalized thermodynamics for information processing .",
    "while the studies of information and thermodynamics have a long history , recent developments of nonequilibrium physics and nanotechnologies have shed new light on classic problems from the modern point of view .",
    "thermodynamics of information processing will open a fruitful research arena that enables us to quantitatively analyze the energy costs of the feedback control and information processing in small thermodynamic systems .",
    "possible applications of this new research field include designing designing and controlling nanomachines  @xcite and nanodevices .",
    "in this appendix , we prove eq .",
    "( [ joint_probability ] ) .",
    "we introduce notations @xmath344 , @xmath345 , @xmath346 , and @xmath347 .",
    "the joint probability of @xmath274 is given by @xmath348 & = p[x_{t_m < t \\leq \\tau } | x_{t_m } , \\lambda_\\tau ( y_\\tau ) ] \\\\ & \\cdot \\prod_{k=1}^m p[y(t_k)|x(t_k ) ] p[x_{t_{k-1 } < t \\leq t_k } | x_{t_{k-1 } } , \\lambda_{t_k } ( y_{t_{k-1 } } ) ] \\cdot p_{\\rm f}[x(0 ) ] , \\end{split } \\label{joint_probability0}\\ ] ] where we set @xmath349 .",
    "we note that @xmath350 depends only on @xmath351 due to the causality .",
    "we also note that @xmath352 = p[x_{t_k < t \\leq \\tau } | x_{t_k } , \\lambda_\\tau ( y_\\tau ) ] \\prod_{k=1}^m p[x_{t_{k-1 } < t",
    "\\leq t_k } | x_{t_{k-1 } } , \\lambda_{t_k } ( y_{t_{k-1 } } ) ] p_{\\rm f}[x(0 ) ] . \\label{appendix2}\\ ] ] by combining eqs .",
    "( [ measurement_probability ] ) , ( [ joint_probability0 ] ) , and ( [ appendix2 ] ) , we obtain eq .",
    "( [ joint_probability ] ) .",
    "we can confirm that the joint probability satisfies the normalization condition @xmath276 = 1 $ ] by integrating eq .",
    "( [ joint_probability0 ] ) in the order of @xmath353 @xmath354 due to the causality .",
    "the authors are grateful to shoichi toyabe , eiro muneyuki , and masaki sano for providing us the experimental data discussed in sec .",
    "this work was supported by kakenhi 22340114 , a grant - in - aid for scientific research on innovation areas `` topological quantum phenomena '' ( kakenhi 22103005 ) , a global coe program `` the physical sciences frontier '' , the photon frontier network program , and the grant - in - aid for research activity start - up ( kakenhi 11025807 ) , from mext of japan .",
    "j. c. maxwell , `` _ _ theory of heat _ _ , '' ( appleton , london , 1871 ) .",
    "_ `` maxwell s demon 2 : entropy , classical and quantum information , computing '' _ , h. s. leff and a. f. rex ( eds . ) , ( princeton university press , new jersey , 2003 ) . k. maruyama , f. nori , and v. vedral , rev . mod . phys . * 81 * , 1 ( 2009 ) . o. j. e. maroney , `` information processing and thermodynamic entropy '' , the stanford encyclopedia of philosophy ( fall 2009 edition ) , edward n. zalta ( ed . ) .",
    "j. c. doyle , b. a. francis , and a. r. tannenbaum , `` _ _ feedback control theory _ _ , '' ( macmillan , new york , 1992 ) .",
    "k. j. strom and r. m. murray , `` _ _ feedback systems : an introduction for scientists and engineers _ _ , '' ( princeton university press , 2008 ) .",
    "s. lloyd , phys .",
    "a * 39 * , 5378 ( 1989 ) . c. m. caves , phys .",
    "* 64 * , 2111 ( 1990 ) .",
    "s. lloyd , phys .",
    "a * 56 * , 3374 ( 1997 ) .",
    "m. a. nielsen , c. m. caves , b. schumacher , and h. barnum , proc .",
    "london a * 454 * , 277 ( 1998 ) .",
    "h. touchette and s. lloyd , phys .",
    "* 84 * , 1156 ( 2000 ) .",
    "w. h. zurek , arxiv : quant - ph/0301076 ( 2003 ) .",
    "t. d. kieu , phys .",
    "lett . * 93 * , 140403 ( 2004 ) .",
    "allahverdyan , r. balian , th.m .",
    "nieuwenhuizen , j. mod .",
    "optics , * 51 * , 2703 ( 2004 ) .",
    "h. touchette and s. lloyd , physica a * 331 * , 140 ( 2004 ) . h. t. quan , y. d. wang , y - x .",
    "liu , c. p. sun , and f. nori , phys .",
    "* 97 * , 180402 ( 2006 ) .",
    "f. j. cao , l. dinis , j. m. r. parrondo , phys .",
    "lett . * 93 * , 040603 ( 2004 ) .",
    "k. h. kim and h. qian , phys .",
    "lett . * 93 * , 120602 ( 2004 ) .",
    "k. h. kim and h. qian , phys .",
    "e * 75 * , 022102 ( 2007 ) .",
    "b. j. lopez , n. j. kuwada , e. m. craig , b. r. long , and h. linke , phys .",
    "* 101 * , 220601 ( 2008 ) .",
    "a. e. allahverdyan and d. b. saakian , europhys lett . * 81 * , 30003 ( 2008 ) .",
    "t. sagawa and m. ueda , phys . rev .",
    "lett . * 100 * , 080403 ( 2008 ) .",
    "k. jacobs , phys .",
    "a * 80 * , 012322 ( 2009 ) . f. j. cao and m. feito , phys .",
    "e * 79 * , 041118 ( 2009 ) .",
    "f.j . cao , m. feito , and h. touchette , physica a * 388 * , 113 ( 2009 )",
    ". t. sagawa and m. ueda , phys .",
    "lett . * 104 * , 090602 ( 2010 ) .",
    "m. ponmurugan , phys .",
    "e * 82 * , 031129 ( 2010 ) .",
    "y. fujitani and h. suzuki , j. phys .",
    "soc . jpn . *",
    "79 * , 104003 ( 2010 ) .",
    "j. m. horowitz and s. vaikuntanathan , phys .",
    "e * 82 * , 061120 ( 2010 ) .",
    "s. toyabe , t. sagawa , m. ueda , e. muneyuki , and m. sano , nature physics * 6 * , 988 ( 2010 ) .",
    "s. w. kim , t. sagawa , s. de liberato , and m. ueda , phys .",
    "* 106 * , 070401 ( 2011 ) .",
    "y. morikuni and h. tasaki , j. stat . phys . *",
    "143 * , 1 ( 2011 ) .",
    "s. ito and m. sano , phys .",
    "e * 84 * , 021123 ( 2011 ) .",
    "j. m. horowitz and j. m. r. parrondo , europhys lett .",
    "* 95 * , 10005 ( 2011 ) .",
    "d. abreu and u. seifert , europhys lett . * 94 * , 10001 ( 2011 ) .",
    "s. vaikuntanathan and c. jarzynski , phys .",
    "e * 83 * , 061120 ( 2011 ) .",
    "t. sagawa , j. phys .",
    ": conf . ser .",
    "* 297 * , 012015 ( 2011 ) .",
    "h. dong , d. z. xu , c. y. cai , and c. p. sun , phys .",
    "e * 83 * , 061108 ( 2011 ) .",
    "d. v. averin , m. mttnen , and j. p. pekola , phys .",
    "b * 84 * , 245448 ( 2011 ) . j. m. horowitz and j. m. r. parrondo , new j. phys . * 13 * , 123019 ( 2011 ) .",
    "l. granger and h. kantz , phys .",
    "e * 84 * , 061110 ( 2011 ) .",
    "s. lahiri , s. rana , and a. m. jayannavar , _",
    "j. phys . a : math .",
    "theor . _ * 45 * , 065002 ( 2012 ) .",
    "y. lu and g. l. long , phys .",
    "e * 85 * , 011125 ( 2012 ) .",
    "t. sagawa and m. ueda , phys .",
    "e * 85 * , 021104 ( 2012 ) .",
    "r. d. vale and f. oosawa , adv .",
    "* 26 * , 97 ( 1990 ) .",
    "f. julicher , a. ajdari , and j. prost , rev .",
    "phys . * 69 * , 1269 ( 1997 ) .",
    "j. m. r. parrondo and b. j. de cisneros , appl .",
    "phys . a * 75*,179 ( 2002 ) .",
    "p. reimann .",
    ". rept . * 361 * , 57 ( 2002 ) .",
    "p. hnggi and f. marchesoni , rev .",
    "phys . * 81 * , 387 ( 2009 ) .",
    "m. bier , biosystems * 88 * , 201 ( 2007 ) .",
    "m. schliwa and g. woehlke , nature * 422 * , 759 ( 2003 ) .",
    "h. j. schlichting and v. nordmeier , math .",
    "unterr . * 49 * , 323 ( 1996 ) .",
    "k. van der weele , d. van der meer , m. versluis and d. lohse , europhys . lett .",
    "* 53 * , 328 ( 2001 ) . v. serreli , c - f .",
    "lee , e. r. kay , and d. a. leigh , nature * 445 * , 523 ( 2007 ) . g. n. price , s. t. bannerman , k. viering , e. narevicius , and m. g. raizen , phys .",
    "* 100 * , 093004 ( 2008 ) .",
    "m. m. millonas , phys .",
    "lett . * 74 * , 10 ( 1995 ) .",
    "a. m. jayannavar , phys .",
    "e * 53 * , 2957 ( 1996 ) .",
    "j. eggers , phys .",
    "lett . * 83 * , 5322 ( 1999 ) .",
    "j. j. brey , f. moreno , r. garcia - rojo , and m. j. ruiz - montero , phys .",
    "e * 65 * , 011305 ( 2001 ) . c. van den broeck , p. meurs , and r. kawai , new j. phys .",
    "* 7 * , 10 ( 2005 ) .",
    "a. ruschhaupt , j. g. muga , and m. g. raize , j. phys .",
    "b : at . mol .",
    ". phys . * 39 * , 3833 ( 2006 ) .    c. jarzynski , phys .",
    "lett . * 78 * , 2690 ( 1997 ) .",
    "g. e. crooks , j. stat . phys .",
    "* 90 * , 1481 ( 1998 ) .",
    "g. e. crooks , phys .",
    "e * 60 * , 2721 ( 1999 ) . c. jarzynski , j. stat .",
    "* 98 * , 77 ( 2000 ) .",
    "u. seifert , phys .",
    "* 95 * , 040602 ( 2005 ) .",
    "r. kawai , j. m. r. parrondo , and c. van den broeck , phys .",
    "lett . * 98 * , 080602 ( 2007 ) . c. bustamante , j. liphardt , and f. ritort , physics today ,",
    "* 58 * , 43 ( 2005 ) .",
    "j. liphardt _",
    "et al . _ ,",
    "science * 296 * , 1832 ( 2002 ) .",
    "d. collin _ et al .",
    "_ , nature * 437 * , 231 ( 2005 ) .",
    "l. brillouin , j. appl . phys . * 22 * , 334 ( 1951 ) .",
    "r. landauer , ibm j. res .",
    "dev . * 5 * , 183 ( 1961 ) . c. h. bennett , int .",
    "* 21 * , 905 ( 1982 ) .",
    "w. h. zurek , nature * 341 * , 119 ( 1989 ) .",
    "w. h. zurek , phys .",
    "a * 40 * , 4731 ( 1989 ) .",
    "k. shizume , phys .",
    "e * 52 * , 3495 ( 1995 ) .",
    "h. matsueda , e. goto , and k - f .",
    "loe , rims kkyroku * 1013 * , 187 ( 1997 ) .",
    "b. piechocinska , phys .",
    "a * 61 * , 062314 ( 2000 ) . c. h. bennett , stud .",
    "phys . * 34 * , 501 ( 2003 ) .",
    "a. e. allahverdyan and t.m .",
    "nieuwenhuizen , phys .",
    "e * 64 * , 0561171 ( 2001 ) . c. horhammer and h. buttner , j. stat .",
    "133 * , 1161 ( 2008 ) .",
    "m. m. barkeshli , arxiv : cond - mat/0504323 ( 2005 ) .",
    "j. d. norton , stud .",
    "* 36 * , 375 ( 2005 ) . o. j. e. maroney , phys .",
    "e * 79 * , 031105 ( 2009 ) .",
    "s. turgut , phys .",
    "e * 79 * , 041102 ( 2009 ) .",
    "t. sagawa and m. ueda , phys .",
    "lett . * 102 * , 250602 ( 2009 ) ; * 106 * , 189901(e ) ( 2011 ) ."
  ],
  "abstract_text": [
    "<S> we review theory of information thermodynamics which incorporates effects of measurement and feedback into nonequilibrium thermodynamics of a small system , and discuss how the second law of thermodynamics should be extended for such situations . </S>",
    "<S> we address the issue of the maximum work that can be extracted from the system in the presence of a feedback controller ( maxwell s demon ) and provide a few illustrative examples . </S>",
    "<S> we also review a recent experiment that realized a maxwell s demon based on a feedback - controlled ratchet . </S>"
  ]
}