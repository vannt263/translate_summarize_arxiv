{
  "article_text": [
    "feature extraction and model selection are important for sparse high dimensional data analysis in many research areas such as genomics , genetics and machine learning . motivated by the need of robust and efficient high dimensional model selection method , we introduce a new penalized quasi - likelihood estimation for linear model with high dimensionality of parameter space .",
    "consider the estimation of the unknown parameter @xmath1 in the linear regression model @xmath2 where @xmath3 is an @xmath4-vector of response , @xmath5 is an @xmath6 matrix of independent variables with @xmath7 being its @xmath8-th row , @xmath9 is a @xmath10-vector of unknown parameters and @xmath11 is an @xmath4-vector of i.i.d .",
    "random errors with mean zero , independent of @xmath12 .",
    "when the dimension @xmath10 is high it is commonly assumed that only a small number of predictors actually contribute to the response vector @xmath13 , which leads to the sparsity pattern in the unknown parameters and thus makes variable selection crucial . in many applications such as genetic association studies and disease classifications using high - throughput data such as microarrays with gene - gene interactions ,",
    "the number of variables @xmath10 can be much larger than the sample size @xmath4 .",
    "we will refer to such problem as ultrahigh - dimensional problem and model it by assuming @xmath14 for some @xmath15 .",
    "following @xcite , we will refer to @xmath10 as a non - polynomial order or np - dimensionality for short .",
    "popular approaches such as lasso @xcite , scad @xcite , adaptive lasso @xcite and elastic - net @xcite use penalized least - square regression : @xmath16 where @xmath17 is a specific penalty function .",
    "the quadratic loss is popular for its mathematical beauty but is not robust to non - normal errors and presence of outliers .",
    "robust regressions such as the least absolute deviation and quantile regressions have recently been used in variable selection techniques when @xmath10 is finite @xcite .",
    "other possible choices of robust loss functions include huber s loss @xcite , tukey s bisquare , hampel s psi , among others .",
    "each of these loss functions performs well under a certain class of error distributions : quadratic loss is suitable for normal distributions , least absolute deviation is suitable for heavy - tail distributions and is the most efficient for double exponential distributions , huber s loss performs well for contaminated normal distributions .",
    "however , none of them is universally better than all others . how to construct an adaptive loss function that is applicable to a large collection of error distributions ?",
    "we propose a simple and yet effective quasi - likelihood function , which replaces the quadratic loss by a weighted linear combination of convex loss functions : @xmath18 where @xmath19 are convex loss functions and @xmath20 are positive constants chosen to minimize the asymptotic variance of the resulting estimator . from the point of view of nonparametric statistics , the functions",
    "@xmath21 can be viewed as a set of basis functions , not necessarily orthogonal , used to approximate the unknown log - likelihood function of the error distribution .",
    "when the set of loss functions is large , the quasi - likelihood function can well approximate the log - likelihood function and therefore yield a nearly efficient method .",
    "this kind of ideas appeared already in traditional statistical inference with finite dimensionality @xcite .",
    "we will extend it to the sparse statistical inference with np - dimensionality .",
    "the quasi - likelihood function can be directly used together with any penalty function such as @xmath22-penalty with @xmath23 @xcite , lasso i.e. @xmath0-penalty @xcite , scad @xcite , hierarchical penalty @xcite , resulting in the penalized composite quasi - likelihood problem : @xmath24 instead of using folded - concave penalty functions , we use the weighted @xmath0- penalty of the form @xmath25 for some function @xmath26 and initial estimator @xmath27 , to ameliorate the bias in @xmath0-penalization @xcite and to maintain the convexity of the problem .",
    "this leads to the following convex optimization problem : @xmath28 when @xmath29 , the derivative of the penalty function , ( [ eq5 ] ) can be regarded as the local linear approximation to problem ( [ eq4 ] ) @xcite . in particular , lasso @xcite corresponds to @xmath30 , scad reduces to @xcite @xmath31 and adaptive lasso @xcite takes @xmath32 where @xmath33 .",
    "there is a rich literature in establishing the oracle property for penalized regression methods , mostly for large but fixed @xmath10 @xcite .",
    "one of the early papers on diverging @xmath10 is the work by @xcite under conditions of @xmath34 .",
    "more recent works of the similar kind include @xcite , @xcite , @xcite , which assume that the number of non - sparse elements @xmath35 is finite .",
    "when the dimensionality @xmath10 is of polynomial order , @xcite recently gave the conditions under which the scad estimator is an oracle estimator .",
    "we would like to further address this problem when @xmath14 with @xmath15 and @xmath36 for @xmath37 , that is when the dimensionality is of exponential order .",
    "the paper is organized as follows .",
    "section [ sec2 ] introduces an easy to implement two - step computation procedure .",
    "section [ sec3 ] proves the strong oracle property of the weighted @xmath0-penalized quasi - likelihood approach with discussion on the choice of weights and corrections for convexity .",
    "section [ sec4 ] defines two specific instances of the proposed approach and compares their asymptotic efficiencies .",
    "section [ sec5 ] provides a comprehensive simulation study as well as a real data example of the snp selection for the down syndrome .",
    "section [ sec6 ] is devoted to the discussion . to facilitate the readability , all the proofs",
    "are relegated to the appendices a , b & c.",
    "we would like to describe the proposed two - step adaptive computation procedure and defer the justification of the appropriate choice of the weight vector @xmath38 to section [ sec3 ] .    in the first step",
    ", one will get the initial estimate @xmath39 using the lasso procedure , i.e : @xmath40 and estimate the residual vector @xmath41 ( for justification see discussion following condition [ cond2 ] ) .",
    "the matrix @xmath42 and vector @xmath43 are calculated as follows : @xmath44 where @xmath45 is a choice of the subgradient of @xmath46 , @xmath47 is the @xmath8-th component of @xmath48 , and @xmath49 should be considered as a consistent estimator of @xmath50 , which is the derivative of @xmath51 at @xmath52 .",
    "for example , when @xmath53 , then @xmath54 and @xmath55 .",
    "the optimal weight is then determined as @xmath56 in the second step , one calculates the quasi maximum likelihood estimator ( qmle ) using weights @xmath57 as @xmath58 * remark 1 * : note that zero is not an absorbing state in the minimization problem . those elements that are estimated as zero in the initial estimate @xmath27",
    "have a chance to escape from zero , whereas those nonvanishing elements can be estimated as zero in .",
    "* remark 2 * : the number of loss functions @xmath59 is typically small or moderate in practice .",
    "problem can be easily solved using a quadratic programming algorithm .",
    "the resulting vector @xmath57 can have vanishing components , automatically eliminating inefficient loss functions in the second step ( [ eq8 ] ) and hence learning the best approximation of the unknown log - likelihood function .",
    "this can lead to considerable computational gains .",
    "see section  [ sec4 ] for additional details .",
    "* remark 3 * : problem ( [ eq8 ] ) is a convex optimization problem when @xmath60 s are all convex and latexmath:[$\\gamma_\\lambda ( |\\hat{\\beta}^{(0)}_j    and efficient computational algorithms such as pathwise coordinate optimization @xcite and least angle regression @xcite .    one particular example is the combination of @xmath0 and @xmath62 regressions , in which @xmath63 , @xmath64 and @xmath65 . here",
    "@xmath66 denotes the median of error distribution @xmath67 .",
    "if the error distribution is symmetric , then @xmath68 .",
    "if the error distribution is completely unknown , @xmath66 is unknown and can be estimated from the residual vector @xmath69 or being regarded as an additional parameter and optimized together with @xmath1 in ( [ eq8 ] ) .",
    "another example is the combination of multiple quantile check functions , that is , @xmath70 where @xmath71 is a preselected quantile and @xmath72 is the @xmath73-quantile of the error distribution .",
    "again , when @xmath72 s are unknown , they can be estimated using the sample quantiles @xmath73 of the estimated residuals @xmath48 or along with @xmath1 in ( [ eq8 ] ) .",
    "see section [ sec4 ] for additional discussion .",
    "in this section , we plan to establish the sampling properties of estimator ( [ eq5 ] ) under the assumption that the number of parameters ( true dimensionality ) @xmath10 and the number of non - vanishing components ( effective dimensionality ) @xmath74 satisfy @xmath14 and @xmath75 for some @xmath15 and @xmath76 .",
    "particular focus will be given to the oracle property of @xcite , but we will strengthen it and prove that estimator ( [ eq5 ] ) is an oracle estimator with overwhelming probability .",
    "@xcite were among the first to discuss the oracle properties with np dimensionality using the full likelihood function in generalized linear models with a class of folded concave penalties .",
    "we work on a quasi - likelihood function and a class of weighted convex penalties .      to facilitate presentation",
    ", we relegate technical conditions and the details of proofs to the appendix .",
    "we consider more generally the weighted @xmath0-penalized estimator with nonnegative weights @xmath77 .",
    "let @xmath78 denote the penalized quasi - likelihood function .",
    "the estimator in ( [ eq5 ] ) is a particular case of and corresponds to the case with @xmath79 .    without loss of generality , assume that parameter @xmath80 can be arranged in the form of @xmath81 , with @xmath82 a vector of non - vanishing elements of @xmath80 .",
    "let us call @xmath83 the biased oracle estimator , where @xmath84 is the minimizer of @xmath85 in @xmath86 and @xmath87 is the vector of all zeros in @xmath88 . here",
    ", we suppress the dependence of @xmath89 on @xmath90 and @xmath91 . the estimator @xmath92 is called the biased oracle estimator , since the oracle knows the true submodel @xmath93 , but nevertheless applies a penalized method to estimate the non - vanishing regression coefficients .",
    "the bias becomes negligible when the weights in the first part are zero or uniformly small ( see theorem  [ thm2 ] ) . when the design matrix @xmath94 is non - degenerate , the function @xmath95 is strictly convex and the biased oracle estimator is unique , where @xmath94 is a submatrix of @xmath12 such that @xmath96 $ ] with @xmath94 and @xmath97 being @xmath98 and @xmath99 sub - matrices of @xmath12 , respectively .",
    "the following theorem shows that @xmath89 is the unique minimizer of @xmath100 on the whole space @xmath101 with an overwhelming probability . as a consequence",
    ", @xmath102 becomes the biased oracle .",
    "we establish the following theorem under conditions on the non - stochastic vector @xmath103 ( see condition [ cond2 ] ) .",
    "it is also applicable to stochastic penalty weights as in ; see the remark following condition  [ cond2 ] .",
    "[ thm1 ] under conditions [ cond1]-[cond4 ] , the estimators @xmath104 and @xmath105 exist and are unique on a set with probability tending to one .",
    "furthermore , @xmath106 for a positive constant @xmath107 .    for the previous theorem to be nontrivial",
    ", we need to impose the dimensionality restriction @xmath108 , where @xmath109 controls the rate of growth of the correlation coefficients between the matrices @xmath94 and @xmath97 , the important predictors and unimportant predictors ( see condition  [ cond5 ] ) and @xmath110 is a non - negative constant , related to the maximum absolute value of the design matrix [ see condition  [ cond4 ] ] .",
    "it can be taken as zero and is introduced to deal with the situation where @xmath111 is small or zero so that the result is trivial .",
    "the larger @xmath112 is , the more stringent restriction is imposed on the choice of @xmath113 . when the above conditions hold , the penalized composite quasi - likelihood estimator @xmath105 is equal to the biased oracle estimator @xmath89 , with probability tending to one exponentially fast .",
    "* remark  4 : * the result of theorem [ thm1 ] is stronger than the oracle property defined in @xcite once the properties of @xmath104 are established ( see theorem  [ thm2 ] ) .",
    "it was formulated by @xcite for the scad estimator with polynomial dimensionality @xmath10 .",
    "it implies not only the model selection consistency and but also sign consistency @xcite : @xmath114 in this way , the result of theorem [ thm1 ] nicely unifies the two approaches in discussing the oracle property in high dimensional spaces .",
    "let @xmath115 and @xmath116 be the first @xmath35 components and the remaining @xmath117 components of @xmath118 , respectively .",
    "according to theorem  [ thm1 ] , we have @xmath119 with probability tending to one . hence",
    ", we only need to establish the properties of @xmath120 .",
    "[ thm2 ] under conditions [ cond1]-[cond5 ] , the asymptotic bias of non - vanishing component @xmath120 is controlled by @xmath121 with @xmath122 furthermore , when @xmath123 , @xmath120 possesses asymptotic normality : @xmath124 where @xmath125 is a unit vector in @xmath126 and @xmath127}{\\left(\\sum_{k=1}^kw_ke[\\partial \\psi_k(\\varepsilon)]\\right)^2}.\\ ] ]    since the dimensionality @xmath35 depends on @xmath4 , the asymptotic normality of @xmath128 is not well defined in the conventional probability sense .",
    "the arbitrary linear combination @xmath129 is used to overcome the technical difficulty .",
    "in particular , any finite component of @xmath128 is asymptotically normal .",
    "the result in theorem  [ thm2 ] is also equivalent to the asymptotic normality of the linear combination @xmath130 stated in @xcite , where @xmath131 is a @xmath132 matrix , for any given finite number @xmath133 .",
    "this theorem relates to the results of @xcite in classical setting ( corresponding to @xmath134 ) where he established asymptotic normality of @xmath135-estimators when the dimensionality is not higher than @xmath136 .",
    "the asymptotic normality ( [ eq10 ] ) allows us to do statistical inference for non - vanishing components .",
    "this requires an estimate of the asymptotic covariance matrix of @xmath115 .",
    "let @xmath137 be the residual and @xmath138 be its @xmath8-th component .",
    "a simple substitution estimator of @xmath139 is @xmath140 see also the remark proceeding .",
    "consequently , by , the asymptotic variance - covariance matrix of @xmath120 is given by @xmath141 another possible estimator of the variance and covariance matrix is to apply the standard sandwich formula . in section [ sec5 ] , through simulation studies , we show that this formula has good properties for both @xmath10 smaller and larger than @xmath4 ( see tables 3 and 4 and comments at the end of section  [ sec5.1 ] ) .",
    "note that only the factor @xmath142 in equation depends on the choice of @xmath38 and it is invariant to the scaling of @xmath38 .",
    "thus , the optimal choice of weights for maximizing efficiency of the estimator @xmath115 is @xmath143 where @xmath144 and @xmath43 are defined in section  [ sec2 ] using an initial estimator , independent of the weighting scheme @xmath90 .",
    "* remark   5 : * the quadratic optimization problem does not have a closed form solution , but can easily be solved numerically for a moderate @xmath59 . the above efficiency gain , over the least - squares , could be better understood from the likelihood point of view .",
    "let @xmath145 denote the unknown error density .",
    "the most efficient loss function is the unknown log - likelihood function , @xmath146 .",
    "but since we have no knowledge of it , the set @xmath147 , consisting of convex combinations of @xmath148 given in , could be viewed as a collection of basis functions used to approximate it .",
    "the broader the set @xmath147 is , the better it can approximate the log - likelihood function and the more efficient the estimator @xmath149 in becomes .",
    "therefore , we refer to @xmath150 as the quasi - likelihood function .",
    "the restriction of @xmath151 guarantees the convexity of @xmath150 so that the problem becomes a convex optimization problem .",
    "however , this restriction may cause substantial loss of efficiency in estimating @xmath152 ( see table 1 ) .",
    "we propose a one - step penalized estimator to overcome this drawback while avoiding non - convex optimization .",
    "let @xmath153 be the estimator based on the convex combination of loss functions and @xmath154 be its nonvanishing components .",
    "the one - step estimator is defined as @xmath155^{-1 } \\phi_{n,{\\mathbf{w}}}(\\hat{\\mbox{\\boldmath $ \\beta$}}_1 ) , \\",
    "\\hat{{\\mbox{\\boldmath $ \\beta$}}}^{\\footnotesize{\\mbox{os}}}_{\\mathbf{w}2 } = \\mathbf{0},\\ ] ] where @xmath156 @xmath157    [ thm3 ] under conditions [ cond1]-[cond5 ] , if @xmath158 , then the one - step estimator @xmath159 enjoys the asymptotic normality : @xmath160 provided that @xmath161 , @xmath162 is lipchitz continous , and @xmath163 , where @xmath164 denote the maximum eigenvalue of a matrix and @xmath165 are defined as in theorem [ thm2 ] .",
    "the one - step estimator overcomes the convexity restriction and is always well defined , whereas is not uniquely defined when convexity of @xmath150 is ruined",
    ". note that if we remove the constraint of @xmath166 ( @xmath167 ) , the optimal weight vector in is equal to @xmath168 this can be significantly smaller than the optimal variance obtained with convexity constraint , especially for multi - modal distributions ( see table 1 ) .",
    "the above discussion prompts a further improvement of the penalized adaptive composite quasi - likelihood in section  [ sec2 ] .",
    "use to compute the new residuals and new matrix @xmath169 and vector @xmath43 .",
    "compute the optimal unconstrained weight @xmath170 and the one - step estimator .",
    "in this section , we discuss two specific examples of penalized quasi - likelihood regression .",
    "the proposed methods are complementary , in the sense that the first one is computationally easy but loses some general flexibility while the second one is computationally intensive but efficient in a broader class of error distributions .      first , we consider the combination of @xmath0 and @xmath62 loss functions , that is , @xmath171 and @xmath172 .",
    "the nuisance parameter @xmath66 is the median of the error distribution .",
    "let @xmath173 denote the corresponding penalized estimator as the solution to the minimization problem : @xmath174 if the error distribution is symmetric , then @xmath175 and the minimization problem ( [ eq17 ] ) can be recast as a penalized weighted least square regression @xmath176 which can be efficiently solved by pathwise coordinate optimization @xcite or least angle regression @xcite .",
    "if @xmath177 , the penalized least - squares problem ( [ eq17 ] ) is somewhat different from ( [ eq5 ] ) since we have an additional parameter @xmath66 . using the same arguments , and treating @xmath66 as an additional parameter for which we solve in , we can show that the conclusions of theorems [ thm2 ] and [ thm3 ] hold with the asymptotic variance equal to @xmath178 where @xmath179 $ ] and @xmath180 is the density of @xmath67 .",
    "this will hold when @xmath66 is either known or unknown .",
    "explicit optimization of is not trivial and we go through it as follows .",
    "since @xmath181 is invariant to the scale of @xmath38 , by setting @xmath182 , we have @xmath183 where @xmath184 and @xmath185 .",
    "note that @xmath186 \\leq \\sigma.\\ ] ] hence , @xmath187 and @xmath188    the optimal value of @xmath107 over @xmath189 can be easily computed .",
    "if @xmath190 , then the optimal value is obtained at @xmath191 in particular , when @xmath192 , @xmath193 , and the optimal choice is the least - squares estimator .",
    "when @xmath194 , if @xmath192 , then the minimizer is @xmath193 . in all other cases ,",
    "the minimizer is @xmath195 i.e. we are left to use @xmath0 regression alone .",
    "the above result shows the limitation of the convex combination , i.e. @xmath196 . in many cases ,",
    "we are left alone with the least - squares or least absolute deviation regression without improving efficiency .",
    "the efficiency can be gained and achieved by allowing negative weights via the one - step technique as in section  [ sec3.4 ] .",
    "let @xmath197 .",
    "the function @xmath198 has a pole at @xmath199 and a unique critical point @xmath200 provided that @xmath201 .",
    "consequently , the function @xmath198 can not have any local maximizer ( otherwise , from the local maximizer to the point @xmath199 , there must exist a local minimizer , which is also a critical point ) . hence , the minimum value is attained at @xmath202 . in other words , @xmath203 where @xmath204 since the denominator can be written as @xmath205 , we have @xmath206 , namely , it outperforms the least - squares estimator , unless @xmath207 .",
    "similarly , it can be shown that @xmath208 } \\leq \\frac{1}{4b_\\varepsilon^2},\\ ] ] namely , it outperforms the least absolute deviation estimation , unless @xmath209 .    when error distribution is symmetric unimodal , @xmath210 , according to chapter 5 of @xcite .",
    "the worst scenario for the @xmath0-regression in comparison with the @xmath62-regression is the uniform distribution ( see chapter 5 , @xcite ) , which has the relatively efficiency of merely @xmath211 . for such uniform distribution , @xmath212 and @xmath213 , @xmath214 , and @xmath215 .",
    "hence , the best @xmath0-@xmath62 is 4 times better than @xmath0 regression alone .",
    "more comparisons about the weighted @xmath0-@xmath62 combination with @xmath0 and least - squares are given in table 1(section  [ sec4.3 ] ) .",
    "the weighted composite quantile regression ( cqr ) was first studied by @xcite in classical statistical inference setting .",
    "@xcite used equally weighted cqr ( ecqr ) for penalized model selection with @xmath10 large but fixed .",
    "we show that the efficiency of ecqr can be substantially improved by properly weighting and extend the work to the case of @xmath216 .",
    "consider @xmath59 different quantiles , @xmath217 .",
    "let @xmath218 .",
    "the penalized composite quantile regression estimator @xmath219 is defined as the solution to the minimization problem @xmath220 where @xmath221 is the estimator of the nuisance parameter @xmath222 , the @xmath73-th quantile of the error distribution .",
    "note that @xmath223 are nuisance parameters and the minimization at is done with respect to them too . after some algebra we can confirm that the conclusions of theorems [ thm2 ] and [ thm3 ] continue to hold with the asymptotic variance as @xmath224 as shown in @xcite and @xcite , when @xmath225 , the optimally weighted cqr ( wcqr ) is as efficient as the maximum likelihood estimator , always more efficient than ecqr .",
    "computationally , the minimization problem in equation ( [ eq24 ] ) can be casted as a large scale linear programming problem by expanding the covariate space with new ancillary variables .",
    "thus , it is computationally intensive to use too many quantiles . in section [ sec4.3 ]",
    ", we can see that usually no more than ten quantiles are adequate for wcqr to approach the efficiency of mle , whereas determining the optimal value of @xmath59 in ecqr seems difficult since the efficiency is not necessarily an increasing function of @xmath59 ( table 2 ) .",
    "also , some of the weights in @xmath226 are zero , hence making wcqr method computationally less intensive than ecqr . from our experience in large @xmath10 and small @xmath4 situations , this reduction tends to be significant .    the optimal convex combination of quantile regression uses the weight @xmath227 where @xmath228 and @xmath169 is a @xmath229 matrix whose @xmath230-element is @xmath231 .",
    "the optimal combination of quantile regression , which is obtained by using the one - step procedure , uses the weight @xmath232 clearly , both combinations improve the efficiency of ecqr and the optimal combination is most efficient among the three ( see table 1 ) .",
    "when the error distributions are skewed or multimodal , the improvement can be substantial .      in this section",
    ", we studied the asymptotic efficiency of proposed estimators under several error distributions . for comparison",
    ", we also included @xmath0 regression , @xmath62 regression and ecqr .",
    "the error distribution ranges from the symmetric to asymmetric distributions : double exponential ( de ) , @xmath233 distribution with degree of freedoms 4 ( @xmath234 ) , normal @xmath235 , gamma @xmath236 , beta @xmath237 , a scale mixture of normals ( @xmath238 ) @xmath239 and a location mixture of normals ( @xmath240 ) @xmath241 . to keep the comparison fair and to satisfy the first assumption of mean zero error terms , we first centered the error distribution to have mean zero .",
    "table 1 shows the asymptotic relative efficiency of each estimator compared to mle . @xmath0-@xmath242 and",
    "@xmath0-@xmath62 indicate the optimal convex @xmath0-@xmath62 combination and optimal @xmath0-@xmath62 combination , respectively .",
    "while @xmath0 regression can have higher or lower efficiency than @xmath62 regression in different error distributions , @xmath0-@xmath242 and @xmath0-@xmath62 regressions are consistently more efficient than both of them .",
    "wcqr@xmath243 denote the optimal convex combination of multiple quantile regressions and wcqr represent the optimal combination . in all quantile regressions , quantiles @xmath244 were used .",
    "as shown in table 1 , wcqr@xmath243 and wcqr always outperform ecqr and the differences are more significant in double exponential distribution and asymmetric distributions such as gamma and beta",
    ". in de , @xmath234 and @xmath235 , nine quantiles are usually adequate for wcqr@xmath243 and wcqr to achieve full efficiency . in @xmath245 and @xmath237",
    ", they need 29 quantiles to achieve efficiency close to mle while the other estimators are significantly inefficient .",
    "this difference is most expressed in multimodal distributions , mn@xmath246 and mn@xmath247 , with wcqr outperforming all .",
    "one of the possible problems with ecqr is that the efficiency does not necessarily increase with @xmath59 , making the choice of @xmath59 harder .",
    "for example , for the double exponential distribution , the relative efficiency decreases with @xmath59 .",
    "this is understandable , as @xmath248 is optimal : putting more and odd number of quantiles dilutes the weights .",
    ".asymptotic relative efficiency compared to mle [ cols=\"<,>,^,^,^,^,^,^,^\",options=\"header \" , ]     the eqtls are almost all located within 500 kb upstream tss or 500 kb downstream tes ( figure 2 ) and mostly from 100 kb upstream tss to 350 kb downstream tes .",
    "[ fig : loci ]",
    "in this paper , a robust and efficient penalized quasi - likelihood approach is introduced for model selection with np - dimensionality .",
    "it is shown that such an adaptive learning technique has a strong oracle property .",
    "as specific examples , two complementary methods of penalized composite @xmath0-@xmath62 regression and weighted composite quantile regression are introduced and they are shown to possess good efficiency and model selection consistency in ultrahigh dimensional space .",
    "numerical studies show that our method is adaptive to unknown error distributions and outperforms lasso @xcite and equally weighted composite quantile regression @xcite .",
    "the penalized composite quasi - likelihood method can also be used in sure independence screening @xcite or iterated version @xcite , resulting in a robust variable screening and selection . in this case , the marginal regression coefficients or contributions will be ranked and thresholded @xcite . it can also be applied to the aggregation problems of classification @xcite where the usual @xmath62 risk function could be replaced with composite quasi - likelihood function . the idea can also be used to choose the loss functions in machine learning . for example , one can adaptively combine the hinge - loss function in the support vector machine , the exponential loss in the adaboost , and the logistic loss function in logistic regression to yield a more efficient classifier .",
    "let @xmath249 be the set of discontinuity points of @xmath45 , which is a subgradient of @xmath60 .",
    "assume that the distribution of error terms @xmath250 is smooth enough so that @xmath251 .",
    "additional regularity conditions on @xmath252 are needed , as in @xcite .",
    "[ cond1 ] the function @xmath252 satisfies @xmath253=a_kc+o(|c| ) \\mbox { as } |c| \\to 0 $ ] , for some @xmath254 . for sufficiently small @xmath255 ,",
    "@xmath256 $ ] exists and is continuous at @xmath257 , where @xmath258 .",
    "the error distribution satisfies the following cramr condition : @xmath259 , for some constants @xmath260 and @xmath59 .",
    "this condition implies that @xmath261 , which is an unbiased score function of parameter @xmath1 .",
    "it also implies that @xmath262 exists .",
    "the following two conditions are important for establishing sparsity properties of parameter @xmath105 by controlling the penalty weighting scheme @xmath103 and the regularization parameter @xmath113 .",
    "[ cond2 ] assume that @xmath263 and @xmath264 .",
    "in addition , @xmath265    the first statement is to ensure that the bias term in theorem  [ thm2 ] is negligible .",
    "it is needed to control the bias due to the convex penalty .",
    "the second requirement is to make sure that the weights @xmath103 in the second part are uniformly large so that the vanishing coefficients are estimated as zero .",
    "it can also be regarded as a normalization condition , since the actual weights in the penalty are @xmath266 .",
    "the lasso estimator will not satisfy the first requirement of condition [ cond2 ] unless @xmath113 is small and @xmath267 . nevertheless , under the sparse representation condition @xcite ,",
    "@xcite show that with probability tending to one , the lasso estimator is model selection consistent with @xmath268 , when the minimum signal @xmath269 .",
    "they also show that the same result holds for the scad - type estimators under weaker conditions .",
    "using one of them as the initial estimator , the weight @xmath270 in ( [ eq8 ] ) would satisfy condition [ cond2 ] , on a set with probability tending to one .",
    "this is due to the fact that with @xmath271 given by ( [ eq6 ] ) , for @xmath272 , @xmath273 , whereas for @xmath274 , @xmath275 , as long as @xmath276 . in other words , the results of theorems  [ thm1 ] and [ thm2 ] are applicable to the penalized estimator ( [ eq8 ] ) with data driven weights .",
    "[ cond3 ] the regularization parameter @xmath277 , where parameter @xmath109 is defined in condition [ cond5 ] and @xmath110 is a constant , bounded by the restriction in condition  [ cond4 ] .",
    "we use the following notation throughout the proof .",
    "let @xmath131 be a matrix .",
    "denote by @xmath278 and @xmath279 the minimum and maximum eigenvalue of the matrix @xmath131 when it is a square symmetric matrix .",
    "let @xmath280 be the operator norm and @xmath281 the largest absolute value of the elements in @xmath131 . as a result , @xmath282 is the euclidean norm when applied to a vector .",
    "define @xmath283 .",
    "[ cond4 ] the matrix @xmath284 satisfies @xmath285 for some positive constants @xmath286 .",
    "there exists @xmath287 such that @xmath288 where @xmath289 is the @xmath8-th row of @xmath94 .",
    "furthermore , assume that the design matrix satisfies @xmath290 and @xmath291 , where @xmath292 is the @xmath293-th column of @xmath12 .",
    "[ cond5 ] assume that @xmath294 @xmath295 where @xmath296 is an @xmath35-dimensional ball centered at @xmath297 with radius @xmath298 and @xmath299 is the diagonal matrix with @xmath8-th element equal to @xmath300 .",
    "recall that @xmath301 and @xmath302 is the true model .",
    "[ lem6.1 ] under conditions  [ cond2 ] and [ cond4 ] , the penalized quasi - likelihood @xmath100 defined by has a unique global minimizer @xmath303 , if @xmath304 @xmath305 where @xmath306 , @xmath307 and @xmath308 stand for the subvectors of @xmath103 , consisting of its first @xmath35 elements and the last @xmath117 elements respectively , and @xmath309 and @xmath310 ( the hadamard product ) in are taken coordinatewise .",
    "conversely , if @xmath311 is a global minimizer of @xmath100 , then holds and holds with strict inequality replaced with non - strict one .",
    "* proof of lemma [ lem6.1 ] : * under conditions  [ cond2 ] and [ cond4 ] , @xmath100 is strictly convex .",
    "necessary conditions and are direct consequences of the karush - kuhn - tucker conditions of optimality .",
    "the sufficient condition follows from similar arguments as those in the proof of theorem 1 in @xcite and the strict convexity of the function @xmath312 .",
    "[ lem6.2 ] under conditions [ cond1]-[cond5 ] we have that @xmath313 where @xmath314 is the subvector of @xmath103 , consisting of its first @xmath35 elements .",
    "* proof of lemma [ lem6.2 ] : * since @xmath315 , we only need to consider the sub - vector of the first @xmath35 components .",
    "let us first show the existence of the biased oracle estimator .",
    "we can restrict our attention to the @xmath35-dimensional subspace @xmath316 .",
    "our aim is to show that @xmath317 for sufficiently large @xmath318 . here",
    ", there is a minimizer inside the ball @xmath319 , with probability tending to one .",
    "using the strict convexity of @xmath100 , this minimizer is the unique global minimizer .    by the taylor expansion at @xmath320",
    ", we have @xmath321 where @xmath322 where @xmath323 $ ] . by the cauchy - schwarz inequality , @xmath324 note that for all @xmath325 , we have @xmath326 and @xmath327 which is of order @xmath328 by condition  [ cond4 ] . hence , @xmath329 uniformly in @xmath330",
    ".    finally , we deal with @xmath331 .",
    "let @xmath332 .",
    "by lemma 3.1 of @xcite , we have @xmath333 for a positive constant @xmath107 . combining all of the above results , we have with probability tending to one that @xmath334 where the right hand side is larger than 0 when @xmath335 for a sufficiently large @xmath336 .",
    "since the objective function is strictly convex , there exists a unique minimizer @xmath337 such that @xmath338    [ lem6.3 ] under the conditions of theorem [ thm2 ] , @xmath339^{-1/2}\\sum_{i=1}^n   \\psi_{\\mathbf{w}}(\\varepsilon_i )     { \\mbox{\\bf b}}^t{\\mbox{\\bf s}}_i   \\stackrel{\\mathcal{d}}{\\to } \\mathcal{n}(0,1)\\ ] ] where @xmath340 .",
    "* proof of lemma [ lem6.3 ] : * by condition  [ cond1 ] , since @xmath341 is independent of @xmath342 , we have @xmath343 , and @xmath344^{-1/2 } \\sum_{i=1}^n \\psi_{\\mathbf{w}}(\\varepsilon_i ) \\mathbf{b}^t { \\mbox{\\bf s}}_i    \\right ] = 1.\\end{aligned}\\ ] ] to complete proof of the lemma , we only need to check the lyapounov condition . by condition  [ cond1 ] , @xmath345 . furthermore , condition  [ cond4 ] implies @xmath346 for a positive constant @xmath347 . using these together with the cauchy - schwartz inequality",
    ", we have @xmath348^{-1/2 }    \\psi_{\\mathbf{w}}(\\varepsilon_i ) \\mathbf{b}^t { \\mbox{\\bf s}}_i \\right|^{2 + \\xi}\\\\ & = & o(1 )   \\sum_{i=1}^n \\left| n^{-1/2 } { \\mbox{\\bf b}}^t { \\mbox{\\bf s}}_i   \\right|^{2 + \\xi}. \\\\ & = & o(1 )   \\sum_{i=1}^n \\left| n^{-1/2 } \\|{\\mbox{\\bf s}}_i\\|   \\right|^{2 + \\xi},\\end{aligned}\\ ] ] which tends to zero by condition [ cond4 ] .",
    "this completes the proof .",
    "the following bernstein s inequality can be found in lemma 2.2.11 of + @xcite .",
    "[ lem6.4 ] let @xmath349 be independent random variables with zero mean such that @xmath350 , for every @xmath351 ( and all @xmath8 ) and some constants @xmath135 and @xmath352 .",
    "then @xmath353 for @xmath354 .",
    "then the following inequality is a consequence of previous bernstein s inequality .",
    "let @xmath355 satisfy the condition of lemma  [ lem6.4 ] with @xmath356 .",
    "for a given sequence @xmath357 , @xmath358 . a direct application of lemma  [ lem6.4 ] yields @xmath359",
    "* proof of theorem [ thm1 ] : * we only need to show that @xmath104 is the unique minimizer of @xmath312 in @xmath360 on a set @xmath361 which has a probability tending to one . since @xmath362 already satisfies , we only need to check .",
    "we now define the set @xmath361 .",
    "let @xmath363 and consider the event @xmath364 with @xmath365 being chosen later . then , by condition  [ cond1 ] and bernstein s inequality , it follows directly from that @xmath366 where @xmath367 is the @xmath293-th column of @xmath12 .",
    "taking @xmath368 , we have @xmath369 for some positive constant @xmath370 , by condition  [ cond4 ] .",
    "thus , by using the union bound , we conclude that @xmath371    we now check whether holds on the set @xmath361 .",
    "let @xmath372 be the @xmath4-dimensional vector with the @xmath8-th element @xmath373 .",
    "then , by condition  [ cond2 ] @xmath374   \\right \\|_\\infty \\nonumber \\\\   & = & o\\left (   n^{1/2 } u_n   +   \\left\\|   { \\mbox{\\bf q}}^t \\mbox{diag } ( \\partial { \\mbox{\\boldmath $ \\psi$}}_{\\mathbf{w}}({\\mbox{\\bf v } } ) ) { \\mbox{\\bf s}}(\\hat { \\mbox{\\boldmath $ \\beta$}}_{1}^o - { \\mbox{\\boldmath $ \\beta$}}_1^ * )   \\right\\|_{\\infty } \\right ) \\end{aligned}\\ ] ] where @xmath375 lies between @xmath104 and @xmath376 . by condition",
    "[ cond5 ] , the second term in is bounded by @xmath377 where the equality follows from lemma  [ lem6.2 ] . by the choice of parameters , @xmath378 by taking @xmath379 .",
    "hence , by lemma  [ lem6.1 ] , @xmath104 is the unique global minimizer .        by using , we have @xmath387 or equivalently , @xmath388 note that @xmath389 .",
    "we have for any vector @xmath330 , @xmath390 consequently , for any unit vector @xmath125 , @xmath391 by using conditions  [ cond4 ] and [ cond5 ] .",
    "this shows that the second term in ( [ eqc4 ] ) , when multiplied by the vector @xmath392 is of order @xmath393 by condition  [ cond2 ] .",
    "therefore , we need to establish the asymptotic normality of the first term in .",
    "this term is identical to the situation dealt by @xcite . using his result ,",
    "the second conclusion of theorem  [ thm2 ] follows .",
    "this completes the proof .",
    "* proof of theorem [ thm3 ] : * first of all , by taylor expansion , @xmath394 where @xmath395 lies between @xmath376 and @xmath154 .",
    "consequently , @xmath396 by the definition of the one step estimator and , we have @xmath397 where @xmath398",
    "we first deal with the remainder term .",
    "note that @xmath399 and @xmath400 where @xmath401 . by the liptchiz continuity , we have @xmath402 where @xmath403 is the liptchiz coefficient of @xmath404 .",
    "let @xmath405 be the identity matrix of order @xmath35 and @xmath406 . by ( [ eqc8 ] )",
    ", we have @xmath407 hence , all of the eigenvalues of the matrix is no larger than @xmath408 .",
    "similarly , by ( [ eqc8 ] ) , @xmath409 and all of its eigenvalue should be at least @xmath410 .",
    "consequently , @xmath411    by condition  [ cond5 ] and the assumption of @xmath154 , it follows from ( [ eqc7 ] ) that @xmath412 thus , for any unit vector @xmath125 , @xmath413 the main term in can be handled by using lemma  [ lem6.3 ] and the same method as @xcite .",
    "this completes the proof ."
  ],
  "abstract_text": [
    "<S> in high - dimensional model selection problems , penalized least - square approaches have been extensively used . </S>",
    "<S> this paper addresses the question of both robustness and efficiency of penalized model selection methods , and proposes a data - driven weighted linear combination of convex loss functions , together with weighted @xmath0-penalty . </S>",
    "<S> it is completely data - adaptive and does not require prior knowledge of the error distribution . </S>",
    "<S> the weighted @xmath0-penalty is used both to ensure the convexity of the penalty term and to ameliorate the bias caused by the @xmath0-penalty . in the setting with dimensionality much larger than the sample size </S>",
    "<S> , we establish a strong oracle property of the proposed method that possesses both the model selection consistency and estimation efficiency for the true non - zero coefficients . as specific examples , </S>",
    "<S> we introduce a robust method of composite l1-l2 , and optimal composite quantile method and evaluate their performance in both simulated and real data examples .    _ key words _ : composite qmle , lasso , model selection , np dimensionality , oracle property , robust statistics , scad </S>"
  ]
}