{
  "article_text": [
    "in the standard problems of statistical mechanics , we begin with the definition of the hamiltonian and proceed to calculate the expectation values or correlation functions of various observable quantities . in the inverse problem , we are given the expectation values and try to infer the underlying hamiltonian .",
    "the history of the inverse problems goes back ( at least ) to 1959 , when keller and zumino @xcite showed that , for a classical gas , the temperature dependence of the second virial coefficient determines the interaction potential between molecules uniquely , provided that this potential is monotonic .",
    "subsequent work on classical gases and fluids considered the connection between pair correlation functions and interaction potentials in various approximations @xcite , and more rigorous constructions of boltzmann distributions consistent with given spatial variations in density @xcite or higher order correlation functions @xcite .",
    "in fact the inverse problem of statistical mechanics arises in many different contexts , with several largely independent literatures . in computer science",
    ", there are a number of problems where we try to learn the probability distribution that describes the observed correlations among a large set of variables in terms of some ( hopefully ) simpler set of interactions .",
    "many algorithms for solving these learning problems rest on simplifications or approximations that correspond quite closely to established approximation methods in statistical mechanics @xcite . more explicitly , in the context of neural network models @xcite , a family of models referred to as ` boltzmann machines ' lean directly on the mapping of probabilistic models into statistical physics problems , identifying the parameters of the probabilistic model with the coupling constants in an ising  like hamiltonian @xcite .",
    "inverse problems in statistical mechanics have received new attention because of attempts to construct explicit network models of biological systems .",
    "physicists have long hoped that the collective behavior which emerges from statistical mechanics could provide a model for the emergence of function in biological systems , and this general class of ideas has been explored most fully for networks of neurons @xcite .",
    "recent work shows how these ideas can be linked much more directly to experiment @xcite by searching for maximum entropy models that capture some limited set of measured correlations . at a practical level , implementing this program requires us to solve a class of inverse problems for ising models with pairwise interactions among the spins , and this is the problem that we consider here .    to be concrete , we consider a network of neurons . throughout the brain ,",
    "neurons communicate by generating discrete , identical electrical pulses termed action potentials or spikes . if we look in a small window of time , each neuron either generates a spike or it does not , so that there is a natural description of the instantaneous state of the network by a collection of binary or ising variables ; @xmath0 indicates that neuron @xmath1 generates a spike , and @xmath2 indicates that neuron @xmath1 is silent . knowing the average rate at which spikes are generated by each cell is equivalent to knowing the expectation values @xmath3 for all @xmath1 .",
    "similarly , knowing the probabilities of coincident spiking ( correlations ) among all pairs of neurons is equivalent to knowing the expectation values @xmath4 .",
    "of course there are an infinite number of probability distributions @xmath5 over the states of the whole system ( @xmath6 ) that are consistent with these expectation values , but if we ask for the distribution that is as random as possible while still reproducing the data  the maximum entropy distribution  then this has the form of an ising model with pairwise interactions : @xmath7 . \\label{ising1}\\ ] ] the inverse problem is to find the `` magnetic fields '' @xmath8 and `` exchange interactions '' @xmath9 that reproduce the observed values of @xmath3 and @xmath4 .",
    "the surprising result of ref @xcite was that this ising model provides an accurate quantitative description of the combinatorial patterns of spiking and silence observed in groups of order @xmath10 neurons in the retina as it responds to natural sensory inputs , despite only taking account of pairwise interactions .",
    "the ising model allows us to understand how , in this system , weak correlations among pairs of neurons can coexist with strong collective effects at the network level , and this is even clearer as one extends the analysis to larger groups ( using real data for @xmath11 , and extrapolating to @xmath12 ) , where there is a hint that the system is poised near a critical point in its dynamics @xcite . since the initial results , a number of groups have found that the maximum entropy models provide surprisingly accurate descriptions of other neural systems @xcite and similar approaches have been used to look at biochemical and genetic networks @xcite .",
    "the promise of the maximum entropy approach to biological networks is that it builds a bridge from easily observable correlations among pairs of elements to a global view of the collective behavior that can emerge from the network as a whole .",
    "clearly this potential is greatest in the context of large networks .",
    "indeed , even for the retina , methods are emerging that make it possible to record simultaneously from hundreds of neurons @xcite , so just keeping up with the data will require methods to deal with much larger instances of the inverse problem .",
    "the essential difficulty , of course , is that once we have a large network , even checking that a given set of parameters @xmath13 reproduce the observed expectation values requires a difficult calculation . in ref @xcite we took an essentially brute force monte",
    "carlo approach to this part of the problem , and then adjusted the parameters to improve the match between observed and predicted expectation values using a relatively naive algorithm .    in this work",
    "we combine several ideas  taken both from statistical physics and from machine learning @xcite  which seem likely to help arrive at more efficient solutions of the inverse problem for the pairwise ising model .",
    "first , we adapt the histogram monte carlo method @xcite to ` recycle ' the monte carlo samples that we generate as we make small changes in the parameters of the hamiltonian .",
    "second , we use a coordinate descent method to adjust the parameters @xcite .",
    "finally , we exploit the fact that neurons use their binary states in a very asymmetric fashion , so that silence is much more common that spiking . combining these techniques ,",
    "we are able to solve the inverse problem for @xmath11 neurons in tens of minutes , rather than many days for the naive approach , holding out hope for generalization to yet larger problems .",
    "our overall goal is to build a model for the distribution @xmath5 over the states of the a system with @xmath14 elements , @xmath15 . as ingredients for determining this model , we use low order statistics computed from a set of @xmath16 samples @xmath17 } , which we can think of as samples drawn from the distribution @xmath5 .",
    "the classical idea of maximum entropy models is that we should construct @xmath5 to generate the correct values of certain average quantities ( e.g. , the energy in the case of the boltzmann distribution ) , but otherwise the distribution should be ` as random ' as possible @xcite .",
    "formally this means that we find @xmath5 as the solution of a constrained optimization problem , maximizing the entropy of the distribution subject to conditions that enforce the correct expectation values .",
    "we will refer to the quantities whose averages are constrained as `` features '' of the system , @xmath18 , where each @xmath19 is a function of the state @xmath20 , @xmath21 .",
    "one special set of average features are just the marginal distributions for subsets of the variables .",
    "thus we can construct the one  body marginals @xmath22 the two  body marginals , @xmath23 and so on for larger subsets .",
    "the maximum entropy distributions consistent with marginal distributions up to @xmath24body terms generates a hierarchy of models that capture increasingly higher  order correlations , monotonically reducing the entropy of the model as @xmath24 increases , toward the true value @xcite .",
    "let @xmath25 denote the empirical distribution @xmath26 where @xmath27 is the kronecker delta , equal to one when @xmath28 and equal to zero otherwise .",
    "the maximum - entropy problem is then @xmath29     \\text { such that }     \\langle { \\mathbf{f}}({{\\bf\\sigma}})\\rangle_p = \\langle { \\mathbf{f}}({{\\bf\\sigma}})\\rangle_{\\tilde p } \\enspace,\\ ] ] where @xmath30 $ ] denotes the entropy of the distribution @xmath31 , and @xmath32 denotes an expectation value with respect to that distribution . using the method of lagrange multipliers , the solution to the maximum entropy problem has the form of a boltzmann or gibbs distribution @xcite , @xmath33 , \\label{qtheta}\\ ] ] where as usual the partition function @xmath34 is the normalization constant ensuring that the distribution @xmath35 sums to one , and the parameters @xmath36 correspond to the lagrange multipliers .",
    "note that the expression for @xmath35 above describes the pairwise ising model , eq ( [ ising1 ] ) , with @xmath37 , and the features @xmath38 are the one  spin and two  spin combinations @xmath39 .    rather than thinking of our problem as that of maximizing the entropy subject to constraints on the expectation values",
    ", we can now think of our task as searching the space of gibbs distributions , parameterized as in eq ( [ qtheta ] ) , to find the values of the parameters @xmath40 that generate the correct expectation values .",
    "importantly , because of basic thermodynamic relationships , this search can also be formulated as an optimization problem .",
    "specifically , we recall that expectation values in statistical mechanics can be written as derivatives of the free energy , which in turn is the logarithm of the partition function ( up to factors of the temperature , which is nt relevant here ) .",
    "thus , for distribution in the form of eq ( [ qtheta ] ) , we have @xmath41 enforcing that these expectation values are equal to the expectation values computed from our empirical samples means solving the equation @xmath42 but this can be written as    @xmath43\\\\ & = & { \\partial\\over{\\partial\\theta_\\mu } } { 1\\over m}\\sum_{n=1}^m \\ln q_{{{\\mathbf\\theta } } } ( { { { \\bf\\sigma}}}^n ) .\\end{aligned}\\ ] ]    thus we see that matching the empirical expectation values is equivalent to looking for a local extremum ( which turns out to be a maximum ) of the quantity @xmath44 but if all the samples @xmath45 are drawn independently , then the total probability that the gibbs distribution with parameters @xmath40 generates the data is @xmath46 , and so the quantity in eq ( [ logq ] ) is ( up to a factor of @xmath16 ) , just @xmath47 . finding",
    "the maximum entropy distribution thus is equivalent to maximizing the probability or likelihood that our model generates the observed samples , within the class of models defined by the gibbs distribution eq ( [ qtheta ] ) .",
    "we recall that , in information theory @xcite , probability distributions implicitly define strategies for encoding data , and the shortest codes are achieved when our model of the distribution actually matches the distribution from which the data are drawn . since code lengths are related to the negative logarithm of probabilities , it is convenient to define the cost of coding or log loss @xmath48 that arises when we use the model with parameters @xmath40 to describe data drawn from the empirical distribution @xmath25 : @xmath49 comparing with eq ( [ logq ] ) , we obtain the _ dual _ formulation of the maximum entropy problem , @xmath50    why is the optimization problem in eq ( [ dual ] ) difficult ? in principle , the convexity properties of free energies should make the problem well behaved and tractable .",
    "but it remains possible for @xmath48 to have a very sensitive dependence on the parameters @xmath40 , and this can cause practical problems , especially if , as suggested in ref @xcite , the systems we want to describe are poised near a critical point . even before encountering this problem , however , we face the difficulty that computing @xmath48 or even its gradient in parameter space involves computing expectation values with respect to the distribution @xmath51 .",
    "once the space of states becomes large , it is no longer possible to do this by exact enumeration .",
    "we can try to use approximate analytical methods , or we can use monte carlo methods .      our general strategy for solving the optimization problem in eq ( [ dual ] ) will be to use standard monte carlo simulations @xcite to generate samples from the distribution @xmath52 , approximate the relevant expectation values as averages over these samples , and then use the results to propose changes in the parameters @xmath40 so as to proceed toward a minimum of @xmath48 . implemented naively , as in ref @xcite , this procedure is hugely expensive , because at each new setting of the parameters we have to generate a new set of monte carlo samples .",
    "some of this cost can be avoided using the ideas of histogram monte carlo @xcite .",
    "we recall that if we want to compute the expectation value of some function @xmath53 in the distribution @xmath54 , this can be written as @xmath55 \\\\ & = & { \\bigg\\langle } { { q_{{{\\mathbf\\theta}}'}({{{\\bf\\sigma}}})}\\over{q_{{{\\mathbf\\theta } } } ( { { { \\bf\\sigma } } } ) } } \\phi({{{\\bf\\sigma } } } ) { \\bigg\\rangle}_{\\theta}\\\\ & = &   { { \\langle \\phi({{{\\bf\\sigma } } } ) \\exp [ - ( { { { \\mathbf\\theta } } ' } - { { { \\mathbf\\theta}}}){\\bf\\cdot } { \\mathbf f}({{{\\bf\\sigma}}})]\\rangle_{{{\\mathbf\\theta } } } } \\over { \\langle \\exp [ - ( { { { \\mathbf\\theta } } ' } - { { { \\mathbf\\theta}}}){\\bf\\cdot } { \\mathbf f}({{{\\bf\\sigma}}})]\\rangle_{{{\\mathbf\\theta } } } } } , \\label{difftheta}\\end{aligned}\\ ] ] where we denote expectation values in the distribution @xmath51 by @xmath56 , and similarly for @xmath57 .",
    "we note that eq ( [ difftheta ] ) is exact .",
    "the essential step of histogram monte carlo is to use an approximation to this equation , replacing the expectation value in the distribution @xmath35 by an average over a set of samples drawn from monte carlo simulation of this distribution .",
    "consider an algorithm @xmath58 which searches for a minimum of @xmath48 . as this algorithm progresses , the values of the parameters @xmath40 change slowly",
    ". we will divide these changes into stages @xmath59 , and within each stage we will perform @xmath60 iterations . at the first iteration , we will generate , via monte carlo , @xmath61 samples of the state @xmath20 drawn out of the distribution appropriate to the current value of @xmath62 .",
    "let us refer to averages over these samples as @xmath63 , which should approximate @xmath56 . at subsequent iterations ,",
    "the parameters @xmath40 will be adjusted ( see below for details ) , but we keep the same monte carlo samples and approximate the expectation values over the distribution with parameters @xmath64 as @xmath65\\rangle_{mc{{\\mathbf\\theta } } } } \\over { \\langle \\exp [ - ( { { { \\mathbf\\theta } } ' } - { { { \\mathbf\\theta}}}){\\bf\\cdot } { \\mathbf f}({{{\\bf\\sigma}}})]\\rangle_{mc{{\\mathbf\\theta } } } } } .\\ ] ] we denote this approximation as @xmath66 .",
    "once we reach @xmath67 , we run a new monte carlo simulation appropriate to the current value of @xmath40 , and the cycle begins again at stage @xmath68 .",
    "this strategy is summarized as pseudocode in fig [ fig : alg_shell ] .",
    "* input * :  = empirical observations @xmath69 + parameters @xmath61 and @xmath70 + access to maxent inference algorithm @xmath71 + * algorithm * : +  nitialize @xmath71 + @xmath72 initial parameter vector computed by @xmath71 + for @xmath73 +  generate @xmath61 monte carlo samples using @xmath74 + for @xmath75 +  run one iteration of @xmath71 , approximating @xmath76 + @xmath77 parameter vector computed by @xmath71 on this iteration + @xmath78    once we chose the optimization algorithm @xmath71 , there are still two free parameters in our scheme : the number of samples @xmath61 provided by the monte carlo simulation at each stage , and the number of iterations per stage @xmath70 . in our experiments ,",
    "we explore how choices of these parameters influence the convergence of the resulting algorithms .      at the core of our problem",
    "is an algorithm which tries to adjust the parameters @xmath79 so as to minimize @xmath80 . in ref",
    "@xcite we used a simple gradient descent method . here",
    "we use a coordinate descent method @xcite , adapted from ref @xcite and tuned specifically for the maximum entropy problem . where gradient descent methods attempt to find a vector in parameter space along which one can achieve the maximum reduction in @xmath81 , coordinate descent methods explore in sequence the individual parameters @xmath82 .    beyond the general considerations in refs @xcite ,",
    "coordinate descent may be especially well suited to our problem because of the the sparsity of the feature vectors . in implementing any parameter adjustment algorithm ,",
    "it is useful to take account of the fact that in networks of real neurons , spikes and silences are used very asymmetrically .",
    "it thus is useful to write the basic variables as @xmath83 .",
    "this involves a slight redefinition of the parameters @xmath13 , but once this is done one finds that individual terms @xmath84 or @xmath85 are often zero , because spikes ( corresponding to @xmath86 ) are rare .",
    "this is true not just in the experimental data , but of course also in the monte carlo simulations once we have parameters that approximately reproduce the overall probability of spiking vs. silence .",
    "this sparsity can lead to substantial time savings , since all expectation values can be evaluated in time proportional to the number of non ",
    "zero elements @xcite .    as an alternative to coordinate descent method",
    ", we also used a general purpose convex optimization algorithm known as the limited memory variable metric ( lmvm )  @xcite , which is known to outperform others such general purpose algorithms on a wide variety of problems @xcite .",
    "while lmvm has the advantage of changing multiple parameters at a time , the cost of updating may outweigh this advantage , especially for very sparse data such as ours .",
    "both the coordinate descent and the lmvm schemes are initialized with parameters [ in the notation of eq ( [ ising1 ] ) ] @xmath87 and the @xmath88 chosen to exactly reproduce the expectation values @xmath3 . our monte carlo method follows the discussion of the gibbs sampler in ref @xcite .",
    "all computations were on the same computing cluster @xcite used in ref @xcite .",
    "in this section , we evaluate the speed of convergence of our algorithms as a function of @xmath61 ( the number of monte carlo samples drawn in a sampling round ) and @xmath70 ( the number of learning steps per stage ) under a fixed time budget .",
    "we begin with synthetic data , for which we know the correct answer , and then consider the problem of constructing maximum entropy models to describe real data .",
    "the maximum entropy construction is a strategy for simplifying our description of a system with many interacting elements .",
    "separate from the algorithmic question of finding the maximum entropy model is the natural science question of whether this model provides a good description of the system we are studying , making successful predictions beyond the set of low order correlations that are used to construct the model . to sharpen our focus on the algorithmic problem , we use as `` empirical data '' a set of samples generated by monte carlo simulation of an ising model as in eq ( [ ising1 ] ) . to get as close as possible to the problems we face in analyzing real data",
    ", we use the parameters of the ising model found in ref @xcite in the analysis of activity in a network of @xmath11 neurons in the retina as it responds to naturalistic movies .",
    "we note that this model has competing interactions , as in a spin glass , with multiple locally stable states ; extrapolation to larger systems suggests that the parameters are close to a critical point .",
    "approximate difference in the test log loss between the current solution of an algorithm and the exact model that generates the test data , as a function of the algorithm running time .",
    "the number of monte carlo samples at each stage is set to 320,000 .",
    "the two choices of optimization algorithm exhibit similar performance , but much greater efficiency is achieved in both cases by including multiple iterations of learning algorithm @xmath71 per stage . ]    starting with the parameters @xmath89 determined in ref @xcite , we generate @xmath90 samples by monte carlo simulation .",
    "let us call the empirical distribution over these samples @xmath91 .",
    "then we proceed to minimize @xmath92 . to monitor the progress of the algorithm ,",
    "we compute @xmath93 note that this computation requires us to estimate the log ratios of partition functions , for which we again use the histogram monte carlo approach , @xmath94 \\rangle_{{{{\\mathbf\\theta}}}_0}\\\\ & \\approx & { \\bigg\\langle } \\exp\\left [ - ( { { { \\mathbf\\theta}}}-{{{\\mathbf\\theta}}}_0){\\bf \\cdot } { \\bf f}({{{\\bf\\sigma}}})\\right ] { \\bigg\\rangle}_{mc{{{\\mathbf\\theta}}}_0 } .\\end{aligned}\\ ] ] as a check on this approximation , we can exchange the roles of @xmath95 and @xmath96 , @xmath97 { \\bigg\\rangle}_{mc{{{\\mathbf\\theta } } } } , \\ ] ] and test for consistency between the two results . finally , to compare the performance of the two optimization algorithms",
    ", we withhold some fraction of the data , here chosen to be 10% , for testing .",
    "figure  [ fig : loss_compare ] illustrates the performance of the two algorithms on the synthetic dataset , measured by @xmath98 . for simplicity",
    ", we have held the number of monte carlo samples at each stage , @xmath99 , fixed .",
    "choosing @xmath100 iteration per stage corresponds to a naive use of monte carlo , with a new simulation for each new setting of the parameters , and we see that this approach converges very slowly , as found in ref @xcite .",
    "simply increasing this number to @xmath101 produces at least an order of magnitude speed up in convergence .      here",
    "we consider the problem of constructing the maximum entropy distribution consistent with the correlations observed in real data .",
    "our example is from refs @xcite , the responses of @xmath11 retinal neurons .",
    "as noted at the outset , if we divide time into small slices of duration @xmath102 then each cell either does or does not generate an action potential , corresponding to a binary or ising variable .",
    "the experiments of ref @xcite , analyzed with @xmath103 , provide @xmath104 samples of the state @xmath20 .",
    "we note that spikes are rare , and pairwise correlations are weak , as discussed at length in ref @xcite .    as we try to find the parameters @xmath40 that describe these data ,",
    "we need a metric to monitor the progress of our algorithm ; of course we do nt have access to the true distribution .",
    "since we are searching in the space of gibbs distributions which automatically have correct functional form to maximize the entropy , we check the quality of agreement between the predicted and observed expectation values .",
    "it is straightforward to insure that the model reproduces the one  body averages @xmath3 almost exactly .",
    "thus we focus on the ( connected ) two  body averages @xmath105 .",
    "we compute these averages via monte carlo in the distribution @xmath51 , and then form the absolute difference between computed and observed values of @xmath106 , and finally average over all pairs @xmath107 .",
    "the resulting average error @xmath108 measures how close we come to solving the maximum entropy problem .",
    "note that since the observed values of @xmath106 are based on a finite set of samples , we do nt expect that they are exact , and hence we should nt identify convergence with @xmath109 . instead we divide the real data in half and measure @xmath108 between these halves , and use this as the ` finish line ' for our algorithms .    in fig",
    "[ fig : c2_compare ] we see the behavior of @xmath108 as a function of the number of iterations per stage ( @xmath70 ) , where we terminate the algorithm after 300 seconds of running time on our local cluster @xcite .",
    "as expected intuitively , both small @xmath70 and large @xmath70 perform badly , but there is a wide range @xmath110 in which the algorithm reaches the finish line within the alloted time .",
    "this performance also depends on @xmath61 , the number of monte carlo samples per stage , so that again there is a range of @xmath111 that seems to work well . in effect ,",
    "when we constrain the total run time of the algorithm there is a best way of apportioning this time between stages ( in which we run new monte carlo simulations ) and iterations ( in which we adjust parameters using estimates based on a fixed set of monte carlo samples ) . by working within a factor of two of this optimum ,",
    "we achieve substantial speed up of the algorithm , or an improvement in convergence at fixed run time .",
    "our central conclusion is that recycling of monte carlo samples , as in the histogram monte carlo method @xcite , provides a substantial speedup in the solution of the inverse ising problem . in detail",
    ", some degree of recycling speeds up the computations , while of course too much recycling renders the underlying approximations invalid , so that there is some optimal amount of recycling ; fortunately it seems from fig [ fig : c2_compare ] that this optimum is quite broad .",
    "we expect that these basic results will be true more generally for problems in which we have to learn the parameters of probabilistic models to provide the best match to a large body of data . in the specific context of ising models for networks of real neurons , the experimental state of the art is now providing data on populations of neurons which are sufficiently large that these issues of algorithmic efficiency become crucial .",
    "we thank mj berry ii , e schneidman and r segev for helpful discussions and for their contributions to the work which led to the formulation of the problems addressed here .",
    "this work was supported in part by nih grant p50 gm071508 , and by nsf grants iis0613435 and phy0650617 .",
    "99 jb keller & b zumino , determination of intermolecular potentials from thermodynamic data and the law of corresponding states .",
    "_ j chem phys _ * 30 , * 13511353 ( 1959 ) .",
    "w kunkin & hl frisch , inverse problem in classical statistical mechanics .",
    "_ phys rev _ * 177 , * 282287 ( 1969 ) .",
    "jt chayes , l chayes & e lieb , the inverse problem in classical statistical mechanics .",
    "_ commun math phys _ * 93 , * 57121 ( 1984 ) .",
    "e caglioti , t kuna , j lebowitz & e speer , point processes with specified low order correlation .",
    "_ j markov proc and related fields _ * 12 , * 257272 ( 2006 ) .",
    "js yedidia , wt freeman & y weiss , constructing free energy approximations and generalized belief propagation algorithms .",
    "_ ieee trans inf theory _ * 51 , * 22822312 ( 2005 ) .",
    "jj hopfield , neural networks and physical systems with emergent collective computational abilities . _ proc natl acad sci ( usa ) _ * 79 , * 25542558 ( 1982 ) .",
    "dj amit , _ modeling brain function : the world of attractor neural networks _ ( cambridge university press , cambridge , 1989 ) .",
    "ge hinton & tj sejnowski , learning and relearning in boltzmann machines , in _ parallel distributed processing : explorations in the microstructure of cognition , vol . 1 , _ ed .",
    "de rumelhart , jl mcclelland & the pdp research group , pp .",
    "282317 ( mit press , cambridge , 1986 ) .",
    "e schneidman , mj berry ii , r segev & w bialek , weak pairwise correlations imply strongly correlated network states in a neural population .",
    "_ nature _ * 440 , * 10071012 ( 2006 ) ; qbio.nc/0512013 .",
    "g tkaik , e schneidman , mj berry ii & w bialek , ising models for networks of real neurons .",
    "qbio.nc/0611072 ( 2006 ) .",
    "j shlens et al , the structure of multi  neuron firing patterns in primate retina .",
    "_ j neurosci _ * 26 , * 82548266 ( 2006 ) .",
    "see , for example , the presentations at the 2007 meeting on computational and systems neuroscience , http://cosyne.org/wiki/cosyne_07 : ie ohiorhenuan & jd victor , maximum entropy modeling of multi  neuron firing patterns in v1 .",
    "j shlens et al , spatial structure of large ",
    "scale synchrony in the primate retina . a tang et al , a second  order maximum entropy model predicts correlated network states , but not their evolution over time .",
    "see also the presentations at the 2007 meeting of the society for neuroscience , http://www.sfn.org/am2007/ : j shlens et al , spatial organization of large ",
    "scale concerted activity in primate retina , 176.17/jj10 .",
    "ie ohiorhenuan & jd victor , maximum - entropy analysis of multi - neuron firing patterns in primate v1 reveals stimulus - contingent patterns , 615.8/o01 .",
    "s yu , d huang , w singer & d nikoli , a small world of neural synchrony , 615.14/o07 .",
    "ma sacek , tj blanche , jk seamans & nv swindale , accounting for network states in cortex : are pairwise correlations sufficient ? , 790.1/j12 .",
    "a tang et al , a maximum entropy model applied to spatial and temporal correlations from cortical networks in vitro , 792.4/k27 .",
    "tr lezon et al , using the principle of entropy maximization to infer genetic interaction networks from gene expression patterns .",
    "_ proc natl acad sci ( usa ) _ * 103 , * 1903319038 ( 2006 ) .",
    "g tkaik , _ information flow in biological networks _ ( dissertation , princeton university , 2007 ) .",
    "am litke et al , what does the eye tell the brain ?",
    "development of a system for the large scale recording of retinal output activity .",
    "_ ieee trans nucl sci _ * 51 , * 14341440 ( 2004 ) .",
    "r segev , j goodhouse , jl puchalla & mj berry ii , recording spikes from a large fraction of the ganglion cells in a retinal patch .",
    "_ nat neurosci _ * 7 , * 11551162 ( 2004 ) . for a general discussion of monte carlo methods in machine learning , see a andrieu , n de freitas , a doucet & mi jordan , an introduction to mcmc for machine learning , _ machine learning _ * 50 , * 543 ( 2003 ) ; rm neal probabilistic inference using markov chain monte carlo methods , technical report crg  tr931 , dept of computer science , university of toronto ( 1993 ) .",
    "am ferrenberg & rh swendsen , new monte carlo technique for studying phase transitions .",
    "_ phys rev lett _ * 61 , * 26352638 ( 1988 ) .",
    "m dudk , sj phillips & re schapire , performance guarantees for regularized maximum entropy density estimation , in _ learning theory , 17th annual conference on learning theory _ , j shawe  taylor & y singer , eds , pp 472486 ( springer  verlag , berlin , 2004 ) .",
    "et jaynes , information theory and statistical mechanics .",
    "_ phys rev _ * 106 , * 6279 ( 1957 ) .",
    "e schneidman , s still , mj berry ii & w bialek , network information and connected correlations .",
    "_ phys rev lett _ * 91 , * 238701 ( 2003 ) ; physics/0307072 .",
    "tm cover & ja thomas , _ elements of information theory _",
    "( john wiley & sons , new york , 1991 ) . k binder , _ monte carlo methods in statistical physics _ ( springer  verlag , berlin , 1979 ) .",
    "mej newman & gt barkema , _ monte carlo methods in statistical physics _",
    "( oxford university press , oxford , 1999 ) . s geman & d  geman , stochastic relaxation , gibbs distributions , and the bayesian restoration of images .",
    "_ ieee trans pami _ * 6 , * 721741 ( 1984 ) .",
    "m collins , re schapire & y singer , logistic regression , adaboost and bregman distances . _ machine learning _ * 48 , * 253285 ( 2002 ) .",
    "in early stages of our work we encountered some difficulties with convergence of the coordinate descent algorithm .",
    "one possible source of the problem is that if , for example , the monte carlo sample includes no spikes from a particular neuron , then trying to match the observed expectation values can become impossible . to ease this difficulty we sometimes bias expectation values over the monte carlo samples with a small contribution from expectation values taken from the real data .",
    "further studies would be required to determine if this is essential once we have the optimal values for the parameters @xmath61 and @xmath70 .",
    "sj benson , l curfman  mcinnes , j mor & j sarich , tao user manual ( revision 1.8 ) . technical report anl / mcs  tm242 , mathematics and computer science division , argonne national laboratory ( 2005 ) ; http://www.mcs.anl.gov/tao",
    "r malouf , a comparison of algorithms for maximum entropy parameter estimation . in _ proceedings of the sixth conference on natural language learning _ , d roth & a van den bosch , eds , pp 4955 ( morgan kaufman , 2002 ) .",
    "the facility has 90 2.3 ghz apple g5 and 16 1.3 ghz g4 cpus , running under sun s grid engine .",
    "see the description of the fafner grid at http://genomics.princeton.edu / support / grids/."
  ],
  "abstract_text": [
    "<S> recent work has shown that probabilistic models based on pairwise interactions  in the simplest case , the ising model  provide surprisingly accurate descriptions of experiments on real biological networks ranging from neurons to genes . </S>",
    "<S> finding these models requires us to solve an inverse problem : given experimentally measured expectation values , what are the parameters of the underlying hamiltonian ? </S>",
    "<S> this problem sits at the intersection of statistical physics and machine learning , and we suggest that more efficient solutions are possible by merging ideas from the two fields . </S>",
    "<S> we use a combination of recent coordinate descent algorithms with an adaptation of the histogram monte carlo method , and implement these techniques to take advantage of the sparseness found in data on real neurons . </S>",
    "<S> the resulting algorithm learns the parameters of an ising model describing a network of forty neurons within a few minutes . </S>",
    "<S> this opens the possibility of analyzing much larger data sets now emerging , and thus testing hypotheses about the collective behaviors of these networks . </S>"
  ]
}