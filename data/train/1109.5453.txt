{
  "article_text": [
    "super - resolution ( sr ) is an information processing technique that makes it possible to infer a spatially high - resolution ( hr ) image of a scene from corresponding multiple low - resolution ( lr ) images that are affected by warping , blurring , and noise .",
    "sr can be applied to a variety of images ; e.g. , still images extracted from several sequential video frames .",
    "sr needs the registration of lr images in addition to the image restoration of the registered lr images . since the earliest work by tsai and huang @xcite",
    ", sr has been achieved using various methods @xcite and good overviews of these methods are given in @xcite .",
    "generally , sr is an ill - posed inverse problem because inverting the blur process without amplifying the effect of the noise is difficult @xcite . in other words ,",
    "the degrees of freedom of the hr image and pixel - wise observation noise are always higher than the dimensionality of the observed lr images , so complete determination of an hr image is impossible . therefore , the hr image is frequently inferred as the most preferable image within the framework of the probabilistic information processing , and we handle sr using this framework in this paper .",
    "the probabilistic information processing has three key features : 1 ) model , 2 ) objective function , and 3 ) optimization method . in the sr problem ,",
    "the model includes the observation model and the prior model .",
    "the observation model consists of warping , blurring , downsampling , and noise models . the prior model , necessary for the bayesian framework , mainly consists of an hr image prior , and sometimes includes both the hyperparameter prior for the hr image prior and the registration prior .",
    "the objective function evaluates how good or bad an estimator is .",
    "the estimator usually represents the inferred hr image , and sometimes includes auxiliary parameters ; e.g. , the registration parameters and edge information .",
    "the optimization method numerically maximizes / minimizes the objective function and determines the estimator .",
    "an optimization method is not necessary for simple problems in which an analytical exact solution can be obtained . in the probabilistic information processing ,",
    "sr can be categorized according to these three key features .    to deal with warping , blurring , and downsampling",
    ", a linear transformation model is frequently used @xcite .",
    "warping is usually limited with planar rotation and parallel translation .",
    "blurring is defined by using a point spread function ( psf ) ; a square or gaussian type psf is common .",
    "downsampling denotes sampling from an hr image to construct an lr image .",
    "downsampling sometimes includes anti - aliasing . since these three transformations are linear , they can be combined into a single transformation matrix . as for the noise model , pixel - independent additive white gaussian noise ( awgn )",
    "is usually employed .",
    "the bayesian framework , especially the hr image prior , is quite useful for sr .",
    "the hr image prior provides appropriate smoothness between neighboring pixel luminances . a common type of hr image prior imposes an l2-norm penalty on differences between horizontally and vertically adjacent pixel luminances ( the first derivative ) .",
    "the l1-norm of the first derivative is sometimes used , and it has the advantage of robust inference against outliers . the total variation ( tv ) prior @xcite employs the l1-norm of the gradient vector .",
    "the huber prior @xcite is a mixture prior of l1- and l2-norms .",
    "the sar model @xcite employs the response of a two - dimensional laplacian filter ( the second derivative ) .",
    "the gaussian process prior @xcite has neighboring pixels spread according to a gaussian distribution .",
    "besides the degree of smoothness between neighboring pixels , information regarding the discontinuity , or equivalently , the edges or line process , is also useful for inference .",
    "a common type of prior implementing edges is the compound markov random field ( mrf ) prior that was introduced by geman & geman @xcite and is widely used @xcite . with respect to the compound mrf",
    "@xcite prior , the normalizing constant , or equivalently , the partition function , is usually difficult to calculate because it has an exponential calculation cost with respect to the dimensionality of the line process .",
    "recently , kanemura _ et al . _",
    "@xcite confusingly introduced a `` causal '' type of gaussian mrf prior whose calculation cost is polynomial .",
    "we try to improve this prior in this paper .",
    "the sr estimator should be derived from an objective function . as the objective function",
    ", a posterior distribution has been widely employed .",
    "since the posterior distribution usually includes both the hr image and registration parameters , the joint maximum a posteriori ( map ) solution @xcite is a suitable estimator for this objective function .",
    "other than the joint map , the use of the marginalized maximum likelihood ( ml ) @xcite or marginalized map @xcite has been proposed . tipping _",
    "_ @xcite and kanemura _ et al . _",
    "@xcite determine the registration parameters by using ml inference , where the hr image is marginalized out , and determine the hr image by using map inference .",
    "@xcite determines the hr image by using map inference , wherein the registration uncertainties are marginalized out , and assumes that the registration parameters are pre - registered by using standard registration techniques .",
    "marginalized ml is also called type - ii ml , evidence approximation , or empirical bayes .",
    "marginalized ml has no registration prior , unlike marginalized map .",
    "@xcite reported that marginalized map is superior to both joint map and marginalized ml .",
    "we evaluate the accuracy of sr methods in terms of the l2-norm ( mean square error ) -based peak signal - to - noise ratio ( psnr ) .",
    "therefore , we think it is natural to employ psnr as the objective function . for this objective function ,",
    "posterior mean ( pm ) is a suitable estimator .",
    "the variational bayes @xcite approach @xcite seems to approximately determine the pm of the hr image , although the authors assume some registration parameters are known and use point - estimate model parameters obtained by ml inference . to determine the exact pm of the hr image , all parameters other than the hr image",
    "should be marginalized out over the joint posterior distribution .",
    "the type of optimization method to use is not as substantial a problem as the choice of model and objective function , but it is still important .",
    "since almost all good estimators can not be exactly determined because of difficult analytical integration or an exponential calculation cost , some approximation methods need to be introduced .",
    "also , parameter tuning is necessary in many numerical optimization methods ; e.g. , of the initial value and the step - width settings in gradient methods .",
    "specifically , in early work done on image restoration , an annealing method was used for the joint map solution @xcite . for marginalized ml and marginalized map solutions ,",
    "the scaled conjugate gradients algorithm was used @xcite . in recent work , the variational expectation - maximization ( em ) algorithm has been applied , which includes the gradient method in the m step @xcite .",
    "the variational bayes approach has also been applied @xcite .",
    "this method includes nested optimization of the majorization - minimization approach .",
    "this majorization - minimization approach seems to affect both the hr image prior and the estimator .",
    "specifically , it modifies the tv prior to include a discontinuity parameter ( called local spatial activity ) .",
    "in addition , this parameter is point - estimated when the hr image is inferred .    in this paper ,",
    "we propose a new sr method that employs a `` causal '' gaussian mrf prior and utilizes variational bayes to calculate the optimal estimator , pm , with respect to the objective function of the l2-norm - based psnr .",
    "this is a straightforward approach , but it was not proposed earlier possibly because an important limitation of variational bayes is that a conjugate prior is needed .",
    "we solve this problem through simple taylor approximations . in section",
    "ii , we define models , where we introduce a novel unified warping , blurring and downsampling model , an improved hr image prior , an improved hyperparameter prior , and a registration prior . in section iii , we employ psnr as the objective function and derive the optimal estimator , pm , from this objective function . in section iv ,",
    "we determine the pm by using variational bayes and taylor approximations . in section",
    "v , we evaluate the proposed method by comparing it with existing methods .",
    "we discuss the proposed method in section vi and conclude in section vii .",
    "first , we define the gamma , bernoulli , and gaussian distributions used in this paper : @xmath0 here , @xmath1 is the gamma function , @xmath2 denotes the determinant of a given matrix , superscript @xmath3 denotes the transpose , @xmath4 is the real number field , and @xmath5 is the dimension of @xmath6 .",
    "the logistic function and kullback - leibler ( kl ) divergence from distributions @xmath7 to @xmath8 are respectively defined as @xmath9 where the angle brackets @xmath10 denote the expectation of @xmath11 with respect to a distribution @xmath12 .",
    "additionally , @xmath13 denotes the trace of a given matrix .",
    "@xmath14 denotes a diagonal matrix .",
    "@xmath15 is an identity matrix of appropriate size .",
    "@xmath16 is a zero vector or a zero matrix of appropriate size .",
    "all the vectors in this paper are column vectors .",
    "the @xmath17 denotes the l2-norm of a given vector . at this point , these variables have absolutely nothing to do with the variables that appear later .",
    "our task is to estimate an hr grayscale image , @xmath18 , from the observed multiple lr grayscale images , @xmath19 .",
    "images @xmath20 and @xmath6 are regarded as lexicographically stacked vectors . the number of pixels for each lr image , @xmath21 , is assumed to be less than that of the hr image , @xmath22 ; i.e. , @xmath23 .",
    "we do this estimation using an sr technique whose resolution enhancement factor is @xmath24{n_\\bx / n_\\by}$ ] @xmath25 . although we define the range of a pixel luminance value as infinite , we use @xmath26 for black , @xmath27 for white , and values between @xmath26 and @xmath27 for gradual gray .    the image observation process is modeled as shown in fig .",
    "[ figprocess ] ; the hr image @xmath6 is geometrically warped , blurred , downsampled , and corrupted by noise @xmath28 to form the observed lr image @xmath20 : @xmath29 or , more strictly , @xmath30 the @xmath31 is awgn with precision ( inverse variance ) @xmath32 @xmath33 . here",
    ", @xmath34 is the @xmath35 transformation matrix that is simultaneously used for warping , blurring , and downsampling .",
    "it is defined as @xmath36 where @xmath37 represents the extent of the summation ( explained in the next paragraph ) , and the vectors @xmath38 and @xmath39 respectively denote the two - dimensional positions of the @xmath40-th pixel of the original hr image and the @xmath41-th pixel of the observed lr image .",
    "we define the center of each image as the origin and the size of each pixel is @xmath42 by @xmath42 . for example , regarding an hr image with @xmath43 pixels , each @xmath44 represents @xmath45^\\top , [ -18.5,-19.5]^\\top , ... , [ 19.5,19.5]^\\top$ ] .",
    "@xmath46 and @xmath47 represent the warping parameters of the @xmath48-th lr ) represents a gaussian psf that defines the blur , and @xmath49 @xmath33 represents its precision parameter . in this paper , we assume @xmath49 also differs for each observed image .",
    "these transformation parameters are packed into @xmath50 , which is defined as @xmath51_{k=1}^4      \\equiv [ \\theta_l , [ \\vec{o}_l]_h , [ \\vec{o}_l]_v , \\gamma_l ] ^\\top,\\end{aligned}\\ ] ] where subscripts @xmath52 and @xmath53 , respectively , denote horizontal and vertical positions on the image .    in previous works",
    "@xcite , the extent of @xmath37 was defined as the extent of the hr image . according to this definition , however , the shape of the psf is no longer gaussian .",
    "for example , at the corner of the hr image , the shape is not omnidirectional but limited in a way such as that of a quadrant . in this paper ,",
    "the extent of @xmath37 is defined as infinite , and the luminance values outside the hr image are defined as @xmath54 ( middle gray ) .",
    "this normalization term faithfully represents the gaussian psf .",
    "we also found that this normalization term is exactly given by using the elliptic theta function @xmath55 , and we can rewrite @xmath34 as @xmath56_h , \\!\\rme^{-\\frac{2\\rmpi^2}{\\gamma_l } } \\!\\right )           \\!\\vartheta_3\\ ! \\left(\\ ! \\big [ \\vec{\\chi}\\!\\big(\\!\\theta_l\\!,\\vec{o}_l\\!,\\vec{\\zeta}_j\\!,\\vec{\\xi}_i\\!\\big)\\ ! \\big]_v,\\ ! \\rme^{-\\frac{2\\rmpi^2}{\\gamma_l } } \\!\\right ) } , \\\\      & \\vartheta_3(u , q ) \\equiv 1 + 2 \\sum_{n=1}^\\infty q^{n^2 } \\cos 2 n \\rmpi u.\\end{aligned}\\ ] ] the elliptic theta function includes an infinite series , but it is easily determined numerically because the convergence is quite fast . in ( [ eqtransformationmatrixelliptictheta ] ) , the normalization term ( the denominator of the right - hand side ) seems to depend on @xmath40 because @xmath57 includes @xmath38 , but this is not true . because the elliptic theta function is a periodic function with respect to the argument @xmath58 with period @xmath42 , and @xmath57 can only take discrete values with step size @xmath42 for the horizontal and vertical directions , the normalization term has the same value with respect to @xmath40 .      here , we introduce a `` causal '' gaussian mrf prior for the hr image and additional latent variables .",
    "these latent variables are called the line process that controls the local correlation among pixel luminances .",
    "the introduction of the latent variables enables explicit expression of the possible discontinuity in the hr image .",
    "the line process , @xmath59 , consists of binary variables @xmath60 for all adjacent pixel pairs @xmath40 and @xmath41 .",
    "its size equals @xmath61 - [ \\textrm{number of hr image 's vertical pixels}]$ ] .",
    "we define the prior as @xmath62,\\end{aligned}\\ ] ] where @xmath63 here , the summation @xmath64 is taken over all pairs of adjacent pixels .",
    "the notation @xmath65 means that the @xmath40-th and @xmath41-th pixels are adjacent in the upward , downward , leftward , and rightward directions .",
    "the line process @xmath59 switches the local characteristics of the prior .",
    "it indicates whether two adjacent pixels take similar values or independent values . when @xmath66 , the @xmath40-th and the @xmath41-th pixels are strongly smoothed according to the quadratic penalty , whereas there is no smoothing when @xmath67 .",
    "the hyperparameter @xmath68 @xmath33 is an edge penalty parameter that prevents @xmath69 from excessively taking edges .",
    "note that @xmath68 is restricted to positive values because a negative @xmath68 leads to a reward rather than a penalty for taking edges .",
    "@xmath33 is a smoothness parameter that prevents the differences in adjacent pixel luminances from becoming large , and @xmath71 @xmath33 is a contrast parameter that prevents @xmath6 from taking an improperly large absolute value . on the other hand , in previous works @xcite , @xmath71",
    "is assumed to be @xmath54 , which results in an improper normalizing constant ( see discussion ) .",
    "@xmath72 is the @xmath73 precision matrix of @xmath6 .",
    "we have defined the introduced causal gaussian mrf prior in the joint distribution form of @xmath6 and @xmath59 , i.e. , @xmath74 .",
    "we call such a model `` causal '' because @xmath59 seems to cause @xmath6 .",
    "the mrf model is defined as having the property @xmath75 in this case ; i.e. , the conditional distribution of a random variable , @xmath76 , given all other variables , @xmath77 and @xmath59 , equals the conditional distribution of the random variable , @xmath76 , given its `` neighboring '' variables , @xmath78 and @xmath79 .",
    "if this conditional distribution is a gaussian distribution , such an mrf is called a gaussian mrf .",
    "the `` compound '' mrf prior is usually defined in the form of the gibbs distribution @xcite , @xmath80 which is based on some microstate energy function , or equivalently , a hamiltonian , such as @xmath81 in addition to the property of ( [ eqmrfx ] ) , a compound mrf also has the property of @xmath82 whereas the introduced `` causal '' gaussian mrf prior does not .",
    "therefore , we do not call the introduced prior a `` compound '' mrf prior , even though ( [ eqpriordistributionxeta ] ) and ( [ eqpriordistributionanotherxeta ] ) have similar forms .",
    "furthermore , the introduced `` causal '' gaussian mrf prior is a generative model , whereas the `` compound '' mrf is not .",
    "a generative model has the advantage of reducing the calculation cost ( see discussion ) .",
    "generally , prior distributions should be non - informative unless we have explicit reasons because an informative prior leads to heuristics . actually , we define the prior distributions for the hyperparameters of the hr image prior to be as non - informative as possible : @xmath83 for a gamma distribution , the number of effective prior observations in the bayesian framework is equal to two times parameter @xmath84 .",
    "as shown in the appendix , the number of observations for the hyperparameter @xmath68 is @xmath85 in this sr . also , that for @xmath70 and @xmath71 is @xmath22 , and that for @xmath32 is @xmath86 . therefore , the above settings ",
    "e.g. , @xmath87  are considered sufficiently non - informative .",
    "superscript @xmath88 is added because we use these parameters as the initial values of variational bayes later .      for the registration parameters including the blurring parameter",
    ", we also define the corresponding prior as @xmath89 , ~       \\bsigma_{\\bphi_l}^{{(\\!0\\!)}}\\equiv \\mathrm{diag}[10^{-3 } , 10^{0 } , 10^{0 } , 10^{-3}].\\end{aligned}\\ ] ] for the rotational motion parameter @xmath46 , the prior assumes @xmath90 degree ( @xmath91 ) .",
    "this assumption is considered suitable for this sr task .",
    "similarly , an assumption of @xmath92 pixels for translational motion parameters @xmath93_h$ ] and @xmath93_v$ ] is considered suitable . for blurring parameter @xmath49 , @xmath94",
    "is taken to be the value equivalent to the anti - aliasing of the scale factor @xmath95 .",
    "first , we confirm that the joint distribution of all random variables can now be explicitly given as @xmath96 , \\bphi ] , \\end{aligned}\\ ] ] once the joint distribution is obtained , we can derive all the marginal and conditional distributions ; e.g. , the posterior distribution @xmath97 and joint distribution of the hr and lr images @xmath98 .",
    "one of the most commonly used evaluation functions of the inferred image is the l2-norm ( mean square error ) -based psnr .",
    "it is defined as @xmath99 where @xmath100 is the estimator of the hr image and @xmath6 is the true hr image . since only lr images , @xmath101 , are available for the estimator , we sometimes explicitly express it as a function form , @xmath102 .",
    "now , our objective function ( functional ) to be maximized regarding the estimator is defined as @xmath103 this is because we prefer good estimator performance on average over various hr images and the corresponding lr images . here",
    ", we assume that the occurrence rate of hr and lr images exactly coincides with the model we just introduced .      using the above objective function",
    ", we can explicitly derive the best estimator of the hr image as the pm , @xmath104 here , we used the well - known fact that the pm coincides with the minimum mean square error estimator in bayesian framework .",
    "note that @xmath105 needs marginalization of all parameters other than @xmath6 over @xmath97 .",
    "if the pm of the line process or other model parameters is necessary , it can also be determined in the same manner .",
    "though we could derive the optimal estimator , we can not obtain the analytical solutions of the posterior distribution @xmath97 and marginalized posterior distribution @xmath105 .",
    "consequently , we have to rely on approximations . here",
    ", we employ variational bayes .",
    "variational bayes @xcite provides a trial distribution @xmath106 that approximates the true posterior .",
    "we impose a factorization assumption on the trial distribution , @xmath107 note that , at this moment , the distribution family of each factorized distribution is not limited .",
    "we identify the optimal trial distribution that minimizes the kl divergence between the trial and the true distributions as the best approximation of the true distribution : @xmath108 actually , the trial distribution that minimizes the kl divergence , not from @xmath106 to @xmath97 but from @xmath97 to @xmath106 coincides with the product of the exact marginal distributions as @xmath109 but this minimization is difficult to calculate .    under the factorization assumption of the trial distribution and the extremal condition of the kl divergence , each optimal trial distribution should satisfy the self - consistent equations , @xmath110 in the common style of variational bayes @xcite , this equation is solved by making repetitive updates , @xmath111 each factorized trial distribution is supposed to converge to the optimal distribution .",
    "sometimes , some @xmath112s are used instead of @xmath113s for the distribution on the right - hand side of ( [ eqrecurrencerelation ] ) .",
    "it depends on the hierarchical structure of the model .",
    "similarly , some @xmath114s may not be necessary .      although variational bayes is a widely used general framework , its application is difficult in practice because it requires a conjugate prior .",
    "the prior distributions we have introduced are not conjugate priors .",
    "however , we have found that simple taylor approximations make them conjugate and enable the analytical exact expectations in ( [ eqrecurrencerelation ] ) .    here , to simplify the notation",
    ", we define the mean values of the latent variables @xmath59 , the hyper parameters @xmath115 , and the registration parameters @xmath50 over the trial distributions in the step number @xmath116 of the updates of variational bayes as @xmath117 , @xmath118 , @xmath119 , @xmath120 , @xmath121 , @xmath122 .    specifically , we use first - order taylor approximations for three non - linear terms .",
    "@xmath34 is approximated around @xmath123 , @xmath124_k \\bw'^{{(\\!t\\!)}}_{l , k},\\end{aligned}\\ ] ] where @xmath125 similarly , @xmath126 is approximated around @xmath127 = [ \\bmu_\\bmeta^{{(\\!t\\ ! ) } } , \\ln \\mu_\\rho^{{(\\!t\\ ! ) } } , \\ln \\mu_\\kappa^{{(\\!t\\!)}}]$ ] , @xmath128.\\end{aligned}\\ ] ] we also use a similar approximation around @xmath127 = [ \\bmu_\\bmeta^{{(\\!t\\!+\\!1\\ ! ) } } , \\ln \\mu_\\rho^{{(\\!t\\ ! ) } } , \\ln \\mu_\\kappa^{{(\\!t\\!)}}]$ ] .",
    "in addition , @xmath129 is approximated around @xmath130 , @xmath131      the trial distributions are obtained from ( [ eqrecurrenceinitiation])-([eqtaylorapproximation1 ] ) , ( [ eqtaylorapproximation2 ] ) , and ( [ eqtaylorapproximation3 ] ) , as follows : @xmath132 for ( [ eqrecurrenceinitiation ] ) and ( [ eqrecurrencerelation ] ) , we update those distributions as follows .",
    "first , we compute @xmath133 using @xmath134 .",
    "second , we compute @xmath135 using @xmath136 . finally , we compute @xmath137 using @xmath138 and @xmath139 using @xmath140 . here , we simply compute only the parameters of those distributions because we can compute the expectations in ( [ eqrecurrencerelation ] ) analytically by using taylor approximations in ( [ eqtaylorapproximation1 ] ) , ( [ eqtaylorapproximation2 ] ) , and ( [ eqtaylorapproximation3 ] ) .",
    "specific update equations are described in the appendix .    for the initial parameters of the trial distributions of @xmath59 and @xmath6",
    ", we use non - informative values , @xmath141 for the initial parameters for @xmath68 , @xmath70 , @xmath71 , @xmath32 and @xmath142 , we use the same values as their prior s values .",
    "we obtain the well - approximated pm of @xmath6 as @xmath143 . realistically , instead of @xmath143 , we use @xmath144 when the following convergence conditions hold for @xmath144 and each @xmath145 , @xmath146_{k } } & < 10^{-4 } \\quad ( k=1,2,3,4),\\end{aligned}\\ ] ] where we defined @xmath147 $ ] as the scaling constant .",
    "[ cols=\"^,^,^,^,^,^ \" , ]",
    "in this paper , we proposed a bayesian image super - resolution ( sr ) method with a causal gaussian markov random field ( mrf ) prior .",
    "we improved existing models with respect to three points : 1 ) the combined transformation model through a preferable normalization term using the elliptic theta function , 2 ) the causal gaussian mrf model through introduction of a contrast parameter @xmath71 , which provides an effective normalizing constant including @xmath148 , and 3 ) the hyperparameter prior model through application of a gamma distribution for the edge penalty parameter @xmath68 , which prevents an unfavorable edge - strewn image .",
    "we then logically derived the optimal estimator , that is , not the joint maximum a posteriori ( map ) or marginalized maximum likelihood ( ml ) but the posterior mean ( pm ) , from the objective function of the l2-norm ( mean square error ) -based peak signal - to - noise ratio ( psnr ) .",
    "the estimator is numerically determined by using variational bayes .",
    "we solved the conjugate prior problem in variational bayes by introducing three taylor approximations .",
    "other than these approximations , we did not use any approximations such as ignoring the term @xmath148 .",
    "experimental results showed that the proposed method is mostly superior to existing methods in accuracy .",
    "the update equation of @xmath59 is given as @xmath151 where @xmath152^\\top + \\bsigma_\\bx^{{(\\!t\\ ! ) } } , \\\\",
    "[ \\bm_{i , j}]_{k , l }      & \\equiv      \\begin{cases }          + 1 , & ( k , l)=(i , i ) ~\\mathrm{or}~ ( j , j ) , \\\\",
    "-1 , & ( k , l)=(i , j ) ~\\mathrm{or}~ ( j , i ) , \\\\           0 , & \\mathrm{otherwise}.      \\end{cases}\\end{aligned}\\ ] ] using the taylor approximation of ( [ eqtaylorapproximation2 ] ) , we obtain the distribution of ( [ eqtrialdistributioneta ] ) at step @xmath153 with the parameter @xmath154 where @xmath155.\\end{aligned}\\ ] ]    the update equation of @xmath6 is given as @xmath156 it becomes a gaussian distribution . using the taylor approximation ( [ eqtaylorapproximation1 ] ) ,",
    "we obtain the distribution of ( [ eqtrialdistributionx ] ) at step @xmath153 with the parameters @xmath157^\\top , \\\\",
    "\\label{equpdatesigmax }      \\bsigma_\\bx^{{(\\!t\\!+\\!1\\!)}}&= \\left [   \\ba(\\bmu_\\bmeta^{{(\\!t\\!+\\!1\\ ! ) } } , \\mu_\\rho^{{(\\!t\\ ! ) } } , \\mu_\\kappa^{{(\\!t\\ ! ) } } ) + \\mu_\\beta^{{(\\!t\\!)}}\\sum_{l=1}^l \\bc'^{{(\\!t\\!)}}_{\\bw_l } \\right]^{-1},\\end{aligned}\\ ] ] where @xmath158^\\top \\bw^{{(\\!t\\!)}}_l       + \\sum_{k , k ' } [ \\bsigma_{\\bphi_l}^{{(\\!t\\!)}}]_{k , k ' } { [ \\bw'^{{(\\!t\\!)}}_{l , k}}]^\\top \\bw'^{{(\\!t\\!)}}_{l , k'}.\\end{aligned}\\ ] ]    the update equation of @xmath159 is given as @xmath160 using taylor approximations ( [ eqtaylorapproximation1 ] ) , ( [ eqtaylorapproximation2 ] ) , and ( [ eqtaylorapproximation3 ] ) , we obtain the distribution of ( [ eqtrialdistributionlambda ] ) at step @xmath153 with parameters @xmath161    the update equation of @xmath142 is given as @xmath162^\\top [ \\bsigma_{\\bphi_l}^{{(\\!0\\!)}}]^{-1 } [ \\bphi_l - \\bmu_{\\bphi_l}^{{(\\!0\\ ! ) } } ] \\nonumber\\\\      & ~~ + \\mu_\\beta^{{(\\!t\\!)}}\\left\\ { \\tr \\bc_\\bx^{{(\\!t\\!+\\!1\\!)}}\\bw(\\bphi_l)^\\top \\bw(\\bphi_l ) -2 \\by_l^\\top \\bw(\\bphi_l ) \\bmu_\\bx^{{(\\!t\\!+\\!1\\!)}}\\right\\ }      \\bigg\\ } \\bigg).\\end{aligned}\\ ] ] using the taylor approximation ( [ eqtaylorapproximation1 ] ) , we obtain the distribution of ( [ eqtrialdistributionphi ] ) at step @xmath153 with parameters @xmath163^{-1 } \\bmu_{\\bphi_l}^{{(\\!0\\!)}}+ \\mu_\\beta^{{(\\!t\\ ! ) } } [ \\bc''^{{(\\!t\\!+\\!1\\!)}}_{\\bphi_l } \\bmu_{\\bphi_l}^{{(\\!t\\!)}}- \\bc'^{{(\\!t\\!+\\!1\\!)}}_{\\bphi_l } ] \\right ] , \\\\",
    "\\label{equpdatesigmaphi }      \\bsigma_{\\bphi_l}^{{(\\!t\\!+\\!1\\!)}}&= \\left [ [ \\bsigma_{\\bphi_l}^{{(\\!0\\!)}}]^{-1 } + \\mu_\\beta^{{(\\!t\\!)}}\\bc''^{{(\\!t\\!+\\!1\\!)}}_{\\bphi_l } \\right]^{-1},\\end{aligned}\\ ] ] where @xmath164_k      & \\equiv \\frac{1}{2 } \\tr \\bc_\\bx^{{(\\!t\\!+\\!1\\!)}}\\left [ [ \\bw^{{(\\!t\\!)}}_l]^\\top \\bw'^{{(\\!t\\!)}}_{l , k } + [ \\bw'^{{(\\!t\\!)}}_{l , k}]^\\top \\bw^{{(\\!t\\!)}}_l \\right ] \\nonumber\\\\      & ~~ - \\by_l^\\top \\bw'^{{(\\!t\\!)}}_{l , k } \\bmu_\\bx^{{(\\!t\\!+\\!1\\ ! ) } } , \\\\      [ \\bc''^{{(\\!t\\!+\\!1\\!)}}_{\\bphi_l}]_{k , k ' }      & \\equiv \\tr \\bc_\\bx^{{(\\!t\\!+\\!1\\!)}}[\\bw'^{{(\\!t\\!)}}_{l , k}]^\\top \\bw'^{{(\\!t\\!)}}_{l , k'}.\\end{aligned}\\ ] ]    99 r. tsai and t. huang , `` multiframe image restoration and registration , '' in _ advances in computer vision and image processing _ , vol . 1 , no",
    ". 2 , jai press inc . , pp . 317339 , greenwich , ct , 1984 .",
    "r. hardie , k. barnard , and e. armstrong , `` joint map registration and high resolution image estimation using a sequence of undersampled images , '' _ ieee trans .",
    "image process .",
    "_ , vol . 6 , no . 12 , pp . 16211633 , 1997 . m. e. tipping , and c. m. bishop , `` bayesian image super - resolution , '' in _ advances in nips 15 _ , mit press , pp .",
    "12791286 , 2003 .",
    "r. molina , j. mateos , a. k. katsaggelos , and m. vega , `` bayesian multichannel image restoration using compound gauss - markov random fields , '' _ ieee transactions on image processing _",
    "12 , pp . 16421654 , 2003 .",
    "l. c. pickup , d. p. capel , s. j. roberts , and a. zisserman , `` bayesian image super - resolution , continued , '' in _ advances in nips 19 _ , mit press , 2007 .",
    "a. kanemura , s. maeda , and s. ishii , `` hyperparameter estimation in bayesian image superresolution with a compound markov random field prior , '' _ ieee international workshop on machine learning for signal processing ( mlsp ) _ , pp . 181186 , 2007 .",
    "p. vandewalle , l. sbaiz , j. vandewalle , and m. vetterli , `` super - resolution from unregistered and totally aliased signals using subspace methods , '' _ ieee trans .",
    "signal processing _ , vol .",
    "55 , no . 7 , part 2 , pp . 36873703 , 2007 .",
    "a. kanemura , s. maeda , and s. ishii , `` superresolution with compound markov random fields via the variational em algorithm , '' _ neural networks _",
    "22 , pp . 10251034 , 2009 . s. villena , m. vega , s.d .",
    "babacan , r. molina , and a. k. katsaggelos , `` image prior combination in super - resolution image registration @xmath165 reconstruction , '' _ ieee international workshop on machine learning for signal processing ( mlsp ) _ , pp .",
    "355360 , 2010 . s. d. babacan , r. molina , a. k. katsaggelos , `` variational bayesian super resolution , '' _ ieee transactions on image processing _",
    "4 , pp . 984999 , 2011 .",
    "s. borman and r l. stevenson , `` spatial resolution enhancement of low - resolution image sequences a comprehensive review with directions for future research , '' department of electrical engineering , university of notre dame , tech .",
    "s. c. park , m. k. park , m. g. kang , `` super - resolution image reconstruction : a technical overview , '' _ signal processing magazine _ , ieee , vol .",
    "20 , no.3 , pp . 2136 , 2003 . s. farsiu , d. robinson , m. elad , and p. milanfar , `` advances and challenges in super - resolution , '' _ int .",
    "_ , vol . 14 , no .",
    "4757 , 2004 .",
    "m. ng , t. chan , m. g. kang , and p. milanfar , `` special issue on superresolution imaging : analysis , algorithms , and applications , '' _ eurasip journal on applied signal processing _ , 2006 .",
    "katsaggelos , r. molina , and j. mateos , `` super resolution of images and video '' , synthesis lectures on image , video , and multimedia processing , morgan @xmath165 claypool , 2007 .",
    "milanfar , ed .",
    ", `` super - resolution imaging , '' crc press , 2010 .",
    "r. molina , m. vega , j. abad , and a. k. katsaggelos , `` parameter estimation in bayesian high - resolution image reconstruction with multisensors , '' _ ieee transactions on image processing _ , vol .",
    "12 , pp . 16551667 , 2003 . s. geman , and d. geman , `` stochastic relaxation , gibbs distributions , and the bayesian restoration of images , '' _ ieee trans . on pattern analysis and machine intelligence _ , vol .",
    "pami6 , no . 6 , pp . 721741 , 1984 .",
    "r. chellappa , and a.k .",
    "jain , eds .",
    ", `` markov random fields : theory and application '' , academic press , boston 1993 .",
    "jeng , and j. w. woods , `` compound gauss - markov random fields for image estimation '' , _ ieee transactions on signal processing _ , vol .",
    "3 , pp . 683697 , 1991 . h. attias , `` inferring parameters and structure of latent variable models by variational bayes , '' in _ proc .",
    "uai _ , san francisco , ca , pp .",
    "2130 , morgan kaufmann , 1999 . f. -c .",
    "jeng , and j. w. woods , `` simulated annealing in compound gaussian random fields , '' _ ieee transactions on information theory _ , vol .",
    "1 , pp . 94107 , 1990 . c. m. bishop , d. spiegelhalter ,",
    "j. winn `` vibes : a variational inference engine for bayesian networks , '' in _ advances in nips 15 _ , mit press , pp . 777784 , 2003 ."
  ],
  "abstract_text": [
    "<S> we propose a bayesian image super - resolution ( sr ) method with a causal gaussian markov random field ( mrf ) prior . </S>",
    "<S> sr is a technique to estimate a spatially high - resolution image from given multiple low - resolution images . </S>",
    "<S> an mrf model with the line process supplies a preferable prior for natural images with edges . </S>",
    "<S> we improve the existing image transformation model , the compound mrf model , and its hyperparameter prior model . </S>",
    "<S> we also derive the optimal estimator  not the joint maximum a posteriori ( map ) or marginalized maximum likelihood ( ml ) , but the posterior mean ( pm )  from the objective function of the l2-norm ( mean square error ) -based peak signal - to - noise ratio ( psnr ) . </S>",
    "<S> point estimates such as map and ml are generally not stable in ill - posed high - dimensional problems because of overfitting , while pm is a stable estimator because all the parameters in the model are evaluated as distributions . </S>",
    "<S> the estimator is numerically determined by using variational bayes . </S>",
    "<S> variational bayes is a widely used method that approximately determines a complicated posterior distribution , but it is generally hard to use because it needs the conjugate prior . </S>",
    "<S> we solve this problem with simple taylor approximations . </S>",
    "<S> experimental results have shown that the proposed method is more accurate or comparable to existing methods .    super - resolution , bayesian inference , markov random field prior , line process , posterior mean , variational bayes , taylor approximation . </S>"
  ]
}