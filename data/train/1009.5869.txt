{
  "article_text": [
    "suppose a single time series of length @xmath0 is observed for each of @xmath1 different units .",
    "two possible models for each time series are entertained : a simple autoregressive null model @xmath2 and a more complex alternative model @xmath3 .",
    "the goal is to determine which units come from the alternative model .",
    "this is a common problem in the analysis of multiple time series , and although many details will be context - dependent , certain common themes emerge . of key interest",
    "is how , in repeatedly applying a procedure used for testing a single time series , the rate of type - i errors can be controlled .",
    "model - based approaches are an attractive option , but model errors can become overwhelming in the face of massive multiplicity .",
    "one of this paper s main results is that great care must be taken in characterizing @xmath2 and @xmath3 in order to keep false positives at bay , with the suggested robustification step involving the use of nonparametric bayesian methods .    in typical hypothesis - testing scenarios involving standard parametric models @xmath2 and @xmath3 , the bayes factor @xmath4 contains an `` ockham s razor '' term [ @xcite ] that penalizes the more complex model . in most cases",
    "this is the result of needing to integrate the likelihood across a higher - dimensional prior under the more complex model , which will therefore have a more diffuse predictive distribution .",
    "this penalty for model complexity is quite different from any sort of penalty term imposed for conducting multiple hypothesis tests [ @xcite ] .",
    "but the multiple - testing framework employed here uses dirichlet - process mixtures to represent the unknown null and alternative models , which are properly thought of as being infinite - dimensional .",
    "another of this paper s main objectives is to clarify the nature and operating characteristics of this penalty for model complexity in nonparametric settings  specifically , how it interfaces with the recommended multiple - testing methodology .",
    "a running example from management theory will be used to motivate and study the proposed bayesian model .",
    "the results and discussion are problem - specific , but areas in which the methodology and lessons can be generalized will be pointed out .    our data set covers up to 50 years of annual performance ( operationalized by a common accounting measure ) for over 24,000 publicly traded american companies .",
    "the goal of the analysis is to flag firms whose performance is highly unlikely to have occurred by random chance , since these firms may have good ( or bad ) management practices that are discernible through follow - up case studies .",
    "longitudinal performance stratification is a classic topic in management theory .",
    "indeed , one of the primary aims of strategic - management research , and the conceit of many best - selling books , is to explain why some firms fail and others succeed .",
    "much academic work in this direction focuses on decomposing observed variation into market - level , industry - level and firm - level components . of the work that attempts to identify specific nonrandom performers ,",
    "much of it relies upon model - free clustering algorithms [ e.g. , @xcite ] , which contain no guarantee that the clusters found will be significantly different from one another .",
    "other approaches employ simple classical tests [ @xcite ] , often based upon ordinal time series .",
    "these have the advantage of being model - free , but are typically not based upon sufficient statistics , and suffer from the fact that available multiplicity - correction approaches ( e.g. ,  bonferroni correction ) tend toward the overly aggressive .    due to the number of firms for which public financial data is available , this problem makes an excellent testbed for the study of general - purpose multiple - testing methodology in time - series analysis .",
    "there is , however , no theoretical ideal of what an `` average - performing '' company should look like , beyond the notion that it should revert to the population - level mean even if it has some randomly good or bad years .",
    "the bayesian approach requires that suitable notions of randomness and nonrandomness be embodied in a statistical model .",
    "this model must confront an obvious multiplicity problem : many thousands of companies will be tested , and false positives will make expensive wild - goose chases out of any follow - up studies seeking to explain possible sources of competitive advantage . robustness and trustworthy type - i error characteristics are therefore crucial practical considerations , and so even though this paper s modeling approach is bayesian , it also contains much frequentist reasoning regarding type - i error rates .",
    "this section outlines a basic framework for multiple testing that , for the sake of illustration , will be purposely simplistic .",
    "nonetheless , it will provide a useful jumping - off point for the methodological developments of subsequent sections , and will show why more flexible models are typically needed in order to achieve reasonable type - i error performance .",
    "let @xmath5 be the observation for unit @xmath6 at time @xmath7 , and let @xmath8 be the vector of observations for unit @xmath6 . in the management - theory example , @xmath9 is a standardized performance metric called return on assets ( roa ) , which measures how efficiently a company s assets generate earnings .",
    "each company s roa values were regressed upon a set of covariates judged to be relevant by three experts in management theory collaborating on the project .",
    "these include the company s size , debt - to - equity ratio and market share , along with categorical variables for year and for industry membership .",
    "[ see @xcite , for a summary of the literature regarding covariate effects on observed firm performance . ]",
    "the actual values used in the following analyses were the residuals from this regression . also",
    ", since the question at issue is one of relative performance , not absolute performance , these residuals were standardized by cdf transform to follow a @xmath10 distribution .",
    "since we do not expect random gains or losses in one year to be completely erased by the following year , a model accounting for serial autocorrelation seems mandatory .",
    "management - theoretic support for this assumption in the present context can be found in @xcite and @xcite ; analogous situations in engineering , finance and biology are very common .",
    "the null hypothesis is then a stationary ar(1 ) model depending upon parameter @xmath11 : @xmath12 this assumption allows for long runs of good or bad performance due simply to chance : a large shock ( @xmath13 ) may take quite awhile to decay depending upon the value of @xmath14 , which is assumed to lie on @xmath15 .",
    "nonnull companies can then be modeled as ar(1 ) processes that revert to a nonzero mean . placing a mixture prior on this unknown mean",
    "will then encode the relevant hypothesis test : @xmath16 with @xmath17 is the vector of all ones , @xmath18 is the familiar ar(1 ) variance matrix , @xmath19 is a point mass at 0 , and @xmath20 $ ] is the prior probability of arising from the alternative hypothesis .",
    "the exchangeable normal prior on the nonzero means @xmath21 reflects the prior belief that , among firms that systematically deviate from zero , most of the deviations will be relatively small .",
    "the posterior probabilities @xmath22 then can be used to flag nonnull units . in some contexts ,",
    "these are called the posterior inclusion probabilities , reflecting inclusion in the `` nonnull '' set .",
    "the model must be completed by specifying priors for @xmath14 , @xmath23 and @xmath24 . in the example at hand ,",
    "the data have been standardized to a @xmath10 scale , so it makes sense to choose @xmath25 . in more general settings , the prior for @xmath24",
    "must be appropriately scaled by @xmath14 and @xmath23 in the absence of strong prior information , since the marginal variance of the residual autoregressive process is the only quantity that provides a default scale for the problem .",
    "the conditional likelihoods of each data vector under the two hypotheses ( versus @xmath26 ) are available in closed form : @xmath27 where @xmath28 is the matrix of all ones .",
    "the ratio of ( [ condlike2 ] ) to ( [ condlike1 ] ) gives the bayes factor , conditional upon @xmath14 , @xmath23 and @xmath24 , for testing an individual time series against the null model .",
    "multiplicity , from a bayesian perspective , is handled through careful treatment of the prior probability @xmath29 .",
    "one possibility is to choose a small value of @xmath29 , with the expectation that a prior bias in favor of the null will solve the problem .",
    "but the key intuition in using this model for multiple testing is to treat @xmath29 as just another model parameter to be estimated from the data , giving it a uniform prior .",
    "this induces an effect that is often referred to as an automatic multiple - testing penalty , with the effect being `` automatic '' in the sense that no arbitrary penalty terms must be specified .",
    "this effect can most easily be seen if one imagines repeatedly testing a fixed number of signals in the presence of an increasing number of null units .",
    "asymptotically , the posterior mass of @xmath29 will concentrate near @xmath30 , making it increasingly difficult for all units ( even the signals ) to overcome the prior belief in their irrelevance .",
    "this yields much the same effect as choosing a small value for @xmath29 after the fact , but bayesian learning about @xmath29 negates the need to make such an arbitrary choice .",
    "two caveats are in order .",
    "first , this should not be misinterpreted as saying that bayesians need never worry about multiplicities .",
    "automatic adjustment depends upon allowing the data itself to choose @xmath29 , and more generally , upon careful treatment of prior model probabilities .",
    "the adjustment itself can be seen primarily in the inclusion probabilities , and will not necessarily be incorporated into other posterior quantities of interest . in particular , the joint distribution of effect sizes , conditional on their being nonzero , will fail to adjust for multiplicity .",
    "second , it is still necessary to choose a threshold on posterior probabilities in order to get a decision rule about `` signal '' versus `` noise . ''",
    "an obvious threshold is @xmath31 , but ideally this threshold should be chosen to minimize bayesian expected loss under properly specified loss functions .",
    "perhaps , for example , the loss incurred by a false positive is constant while the loss incurred by a false negative scales according to some function of the underlying difference from @xmath30 . introducing such loss functions will complicate the analysis only slightly , without changing the fundamental need to account for multiplicity .",
    "it is important to distinguish this fully bayesian model from the empirical - bayes approach to multiplicity correction , whereby @xmath29 is estimated by maximum likelihood [ see , e.g. , @xcite ] .",
    "these two approaches are intuitively similar , since the prior inclusion probability is estimated by the data in order to yield an automatic multiple - testing penalty .",
    "yet the approaches have quite different operational and theoretical properties [ @xcite ] , with the focus here being on the fully bayesian approach .",
    "similar multiple - testing procedures have been extensively studied in many different contexts .",
    "see @xcite for theoretical development ; @xcite for genomics ; @xcite and @xcite for portfolio selection ; and @xcite and @xcite for examples in regression .",
    "the posterior inclusion probabilities @xmath32 can be computed straightforwardly using importance sampling to account for posterior uncertainty about @xmath14 , @xmath23 and @xmath29 , which will be stable as long as @xmath29 is not too close to @xmath30 or @xmath33 .",
    "the advantage of importance sampling here is that a common importance function may be used for all marginal inclusion probabilities , greatly simplifying the required calculations . after transforming all variables to have unrestricted domains ,",
    "importance sampling was used to compute the results presented in this section , with repetition and plots of the importance weights used to confirm stability .",
    "historical roa time series for 3459 publicly traded american companies between 1965 and 2004 were used to fit the above model .",
    "this encompasses almost every public company over that period for which at least 20 years of data were available .",
    "standard independent conjugate priors for @xmath14 and @xmath23 were used : @xmath34 where @xmath35 indicates that the normal prior for @xmath14 is truncated to lie on @xmath36 .",
    "these priors were chosen to reflect the expectations of the collaborating management theorists regarding the persistence and scale of random roa fluctuations from year to year . because these parameters appear in both the null and alternative models , integrals over these priors appear in both the numerator and denominator of the bayes factor comparing @xmath37 versus @xmath38 , making them not overly influential in the analysis ; see @xcite for general guidelines on choosing common hyperparameters in model - selection problems .    in many cases",
    "the results of the fit seemed reasonable .",
    "most firms were assigned to @xmath2 with high probability , while companies with obvious patterns of sustained excellence or inferiority were flagged as being from @xmath3 with very high probability .",
    "figure [ examplefirms ] contains instructive examples : two excellent companies ( wd-40 and coca - cola ) , along with one obviously poor company ( oglethorpe power ) , were assigned greater than @xmath39 probability of being nonnull .",
    "a fourth example , texas intruments , had several intermittent years of good performance but no pattern of sustained excellence , and the model gave it greater than @xmath40 probability of being from the null model .",
    "on the other hand , the model displayed two serious shortcomings :    * many firms diverged in obvious ways ( e.g. , via the appearance of long - term trends ) from the expectations of a single ar(1 ) model .",
    "two such examples are in figure [ trendexamples ] .",
    "discussion of this important issue is postponed until section  [ dpgp ] .",
    "* more subtly , the model imposed a homogeneous error structure on data that seemed rather heterogeneous .",
    "some fairly basic exploratory data analysis indicated that firms displayed differing degrees of `` stickiness '' in their trajectories .",
    "this suggested that a single value of @xmath14 for the entire data set might be unsatisfactory .",
    "likewise , some firms appeared systematically more volatile than others , making a single - variance model equally questionable .",
    "the possibility of model errors in ( [ simple1])([simple3 ] ) bring the issue of robustness to the forefront .",
    "this section describes the results of a simulation study that shows just how poorly this model can perform when a particular type of model error is encountered : deviation from the `` single @xmath14 , single @xmath23 '' approach to describing the ar(1 ) residuals of all companies in the sample .",
    "several data sets displaying different levels of heterogeneity were simulated .",
    "the homogeneous ( i.e. ,  single @xmath14 , single @xmath23 ) model was subsequently fit to each simulated data set in order to assess the robustness of the procedure s type - i error performance .",
    "each simulated data set had 3500 times series of length @xmath41 , with each time series drawn from a mixture distribution of ar(1 ) models .",
    "these distributions ranged from trivial one - component mixtures ( for which the assumed model was true ) to complex nine - component mixtures ( for which the assumed model was quite a bad approximation ) .",
    "these conditions are summarized in table [ robustnesstable ] . in the four- and nine - component models ,",
    "all components were equiprobable .",
    "since all simulated companies had @xmath26 , ideally there should be no positive flags .    for the purposes of classification ,",
    "thresholding is reported at the @xmath42 and the @xmath43 levels , where @xmath44 is the posterior inclusion probability for company  @xmath6 .",
    "the first ( @xmath45 ) threshold reflects a 01 loss function that symmetrically penalizes false positives and false negatives . the second threshold is arbitrary , but meant to reflect a more conservative approach to identifying signals .",
    "a full decision - theoretic analysis incorporating more realistic loss functions would yield a different , data - adaptive threshold .",
    "table [ robustnesstable ] supports two conclusions :    * the proposed model yields very strong control over false positives when its assumptions are met : 3 false positives and 0 false positives in the two cases investigated , out of 3500 units tested .",
    "this confirms that the theory outlined in @xcite , which concerns the much simpler normal - means testing problem , applies here as well .",
    "* this excellent type - i error profile is not at all robust to a violation of the autoregressive model s assumptions . in the most extreme case , nearly a third of units ( 1045 out of 3500 ) tested had inclusion probabilities @xmath46 , when in reality none were from the alternative model .",
    "in other less extreme cases , the false positives still numbered in the hundreds , which is clearly unsatisfactory .",
    "@lcd4.0d3.0@ * number of components : model * & @xmath47 & & + 1 : @xmath48 & 0.01 & 3 & 0 + 1 : @xmath49 & 0.02 & 0 & 0 + 4 : @xmath50 & 0.02 & 30 & 4 + 4 : @xmath51 & 0.08 & 152 & 66 + 9 : @xmath52 & 0.37 & 1045 & 560 + 9 : @xmath53 & 0.29 & 797 & 493 +    these results dramatically illustrate the effect of heterogeneity in the autoregressive profiles of each tested unit .",
    "if such heterogeneity exists but is ignored , the type - i error performance of the procedure may be severely compromised .",
    "in the previous section , a specific form of model error  different groups of companies following different ar models  was shown to be a source of overwhelming type - i errors .",
    "hence , a natural extension is to consider a more complicated autoregressive model for the residuals that accounts for the possibility of stratification .",
    "the dirichlet process [ @xcite ] offers a straightforward nonparametric technique for accommodating uncertainty about this random distribution .",
    "let @xmath54 represent the response vector for unit @xmath6 , for now ignoring any contribution due to a nonzero mean .",
    "recall that for parameter @xmath11 , @xmath55 denotes the ar(1 ) variance matrix .",
    "the dp mixture model can then be written as a hierarchical model:=1 @xmath56 where the hyperparameters @xmath57 and @xmath58 must be chosen to reflect the expected properties of the base measure @xmath59 ( which is a product of two independent distributions , a normal and an inverse - gamma ) , and where @xmath60 controls the degree of expected departure from the base measure .",
    "dirichlet - process priors for nonparametric bayesian density estimation were popularized by @xcite , @xcite , and @xcite .",
    "an example of their use in analyzing nonlinear autoregressive time series can be found in @xcite .",
    "realizations of the dirichlet process are almost surely discrete distributions , and so we expect some of the @xmath61 s to be the same across companies . this is the dp framework s primary strength here , since it will facilitate borrowing of information across time series . simply allowing each time series to have its own @xmath14 and own",
    "@xmath23 would make for a simpler model ( albeit with more parameters ) , but the dp prior reflects the subject - specific knowledge that significant clustering of autoregressive parameters should be expected .",
    "this will lead to behavior similar to that predicted by a finite mixture of ar(1 ) models , such as the kind considered by frhwirth - schnatter and kaufmann ( @xcite ) .",
    "the dirichlet - process prior , however , avoids the complicated task of directly computing marginal likelihoods for mixture models of different sizes , and so makes computation much simpler . note",
    "that since the marginal distribution of one draw from a dirichlet - process mixture depends only on the base measure , the dp acts like a mixture model that is predictively matched to a single observation .",
    "it is important to consider , of course , how choices for @xmath60 and @xmath59 affect the implied prior distributions both for the number of mixture components and for the parameters associated with each component .",
    "the marginal prior for the parameters of each mixture component is simply given by the base measure , while the prior for @xmath60 can be described in terms of @xmath62 and @xmath63 , the desired number of mixture components , using results from @xcite .",
    "@xmath64section [ basicmodel ] considered a simple constant - mean ar(1 ) model for nonnull units , and section [ dpar ] modified the ar(1 ) assumption to account for a richer autoregressive structure .",
    "this section now modifies the constant - mean assumption to allow for time - varying nonzero trajectories upon which the autoregressive residuals are superimposed .",
    "most management teams , after all , do not stay the same for 40 or 50 years , and we should not expect their performance to stay the same , either .",
    "firm performance trajectories can be viewed as continuous random functions that are observed at discrete ( in this case , annual ) intervals .",
    "this is essentially a nonparametric version of a mixed - effects model for longitudinal data [ @xcite ] .",
    "recent examples of such models include @xcite , @xcite , and , in a spatial context , @xcite .",
    "let @xmath65 be a continuous - time stochastic process for each observed unit , and let @xmath66 denote the vector of times at which each unit was observed",
    ". then the model is @xmath67 where @xmath68 is the nonparametric residual model defined in section [ dpar ] , @xmath69 represents a point mass at the zero function @xmath70 , and @xmath71 is a random distribution over a function space @xmath72 . as in section [ basicmodel ] , @xmath29 is the unknown prior probability of coming from the alternative model @xmath3 , represented in this case by the distribution @xmath71 .",
    "it is convenient to represent each hypothesis test using a model index parameter @xmath73 : @xmath74 if @xmath75 ( i.e. ,  the null model @xmath2 is true for unit @xmath6 ) , and @xmath76 otherwise .",
    "the crucial consideration in using the above model for hypothesis testing is that the space @xmath72 from which each @xmath77 is drawn must be restricted to a sufficiently small class of functions .",
    "this would be necessary even if @xmath71 were only being estimated , and not tested against a simpler model : if @xmath72 is too broad , then the alternative model itself will not be likelihood - identified , since any pattern of residuals could equally well be absorbed by the mean function .",
    "but this guideline is even more important in model - selection problems ; an over - broad class of functions will mean that the random distribution @xmath71 is vague , in the sense that the predictive distribution of observables will be diffuse . it is widely known that using vague priors for model selection can be produce very misleading results , and will typically have the unintended consequence of sending the bayes factor in favor of the simpler model to infinity .",
    "this is often known as bartlett s paradox in the simple context of testing normal means [ @xcite ] , but the same principle applies here .",
    "it may also be the case that elements of @xmath72 depend upon some parameter @xmath78 .",
    "since this parameter appears only in the alternative model , @xmath78 needs a proper prior , or else the marginal likelihoods will be defined only up to an arbitrary multiplicative constant .",
    "similar challenges occur in all model - selection problems .",
    "general approaches and guidelines for choosing priors on nonshared parameters can be found in @xcite , @xcite , @xcite , and @xcite .",
    "but very few tools of analogous generality have been developed for nonparametric problems , with most work concentrating on how to compute bayes factors for pre - specified models [ @xcite ] , or how to test a parametric null against a nonparametric alternative of a suitably restricted form [ @xcite ] .",
    "this leaves just two obvious criteria for choosing @xmath72 and @xmath71 in the face of weak prior information :    1 .",
    "elements of @xmath72 should be smooth , that is ,  slowly varying on the unit - time scale of the residual model .",
    "this will allow deconvolution of the mean process from the residual , and reflects the prior belief that the mean function will describe long - term departures from @xmath30 in the face of short - term autoregressive jitters .",
    "( indeed , these departures are precisely what the methodology is meant to detect . )",
    "@xmath71 should be centered at the null model , and should concentrate most of its mass on elements of @xmath72 that predict @xmath79 values on a scale similar to those predicted by the null model .",
    "this will avoid bartlett s paradox , and generalizes the argument made by @xcite in recommending an appropriately scaled cauchy prior for testing normal means .",
    "these criteria allow much wiggle room , but at least provide a starting point .",
    "unfortunately there is no objective solution , in this or in any model - selection problem , though the closest thing to a default approach is to simply choose the marginal variance of the alternative process to exactly match the marginal variance of the null process .",
    "best , of course , is to conduct a robustness study , where the features of the nonparametric alternative not shared by the null are varied in order to assess changes in the conclusions",
    ". this will usually be quite difficult in large multiple - testing problems , since computations for just a single version of the alternative model may be expensive .",
    "the choice of @xmath60 , the precision parameter for the residual dirichlet - process prior , is relatively free by comparison , since this parameter appears in both the null and alternative models . strictly speaking , in order to use anoninformative prior for @xmath60 , verification of the conditions in@xcite regarding group invariance is necessary , which is difficult in this case .",
    "( the issue is that a parameter does not necessarily mean the same thing in both @xmath2 and @xmath3 just because it is assigned the same symbol in each . ) in the absence of a formal justification for using a noninformative prior , the conservative approach is to elicit priors for @xmath60 in terms of the expected number of ar(1 ) mixture components in each of @xmath2 and  @xmath3 .",
    "often there will be extrinsic justification for choosing @xmath60 to be the same under both models .",
    "as an example of how tests involving ( [ npalt1])([npalt3 ] ) can be constructed , this section outlines a nonparametric model for a larger subset of the corporate - performance data containing 5498 firms .",
    "this contains every publicly traded american company between 1965 and 2005 for which at least 15 years of history were available .    the class of gaussian processes with some known covariance function is ideally suited for modeling nonzero trajectories , since the covariance function can be chosen to yield smooth functions with probability 1 , and since the prior marginal variance of the process can be controlled exactly ( so that bartlett s paradox may be easily avoided ) .",
    "gaussian processes have the added advantage of analytical tractability , which is very important in hypothesis testing because of the need to evaluate the marginal likelihood of the data under the alternative model .",
    "more general classes of functions are certainly possible , though perhaps computationally challenging in the face of massive multiplicity .",
    "one additional feature to account for is clustering , since management theorists are interested in identifying a small collection of archetypal trajectories that may correspond to different sources of competitive advantage .",
    "partitioning of firms into shared trajectories is especially relevant for advocates of the so - called `` resource - based view '' of the firm [ @xcite ] .",
    "additionally , clustering on treatment effects is known to increase power in multiple - testing problems [ @xcite ] .",
    "the approach considered here is similar to that introduced by@xcite .",
    "the distribution of nonzero random functions is modeled with a functional dirichlet process : @xmath80 the functional dirichlet process in ( [ fdpgp ] ) has precision parameter @xmath81 and is centered at a gaussian process with covariance function @xmath82 . at time @xmath7 , the value of the function @xmath77 has a dirichlet - process marginal distribution : @xmath83 , where @xmath84 . in choosing the hyperparameter @xmath85",
    ", close attention must be paid to the marginal variance of the residual model , so that variance inflation in ( [ fdpgp2 ] ) does not overwhelm the bayes factor . for greater detail on gaussian processes for nonparametric function estimation , see @xcite .",
    "this model is significantly richer than the simplistic framework developed in section [ basicmodel ] , but is similar in two crucial ways :    * since the gaussian process in ( [ fdpgp2 ] ) leads to @xmath86 .",
    "as before , it is equally likely a priori that a firm s trajectory will be predominantly negative or predominantly positive . * under the alternative model is controlled through the choice of a single hyperparameter , with @xmath87 in ( [ fdpgp2 ] ) playing the role of @xmath24 in ( [ simple3 ] ) .",
    "hence , despite the complicated nonparametric wrapper , the ockhams - razor effect upon the implied marginal likelihoods still happens in the familiar way .",
    "the model also solves both of the major problems encountered in the roa data : time - varying nonzero trajectories , and clustering both of trajectories and of company - specific parameters for the autoregressive residual .",
    "an extensive prior - elicitation process was undertaken with three experts in management theory who had originally compiled the data .",
    "for the base measure of the dirichlet - process mixture of ar(1 ) covariance models , the same hyperparameters from the parametric model in ( [ basicprior1 ] ) and ( [ basicprior2 ] ) were used .",
    "hence , the starting point for this elicitation was @xmath88 , which is the prior marginal variance of the residual ar process ( assessed by simulation ) , and @xmath89 ( on a 41-year time scale ) , which reflected the experts judgments about the long - term effects of strategic choices made by firms .",
    "the elicitees were repeatedly shown trajectories drawn from this prior and other similar priors , and soon settled upon @xmath90 and @xmath91 as values that better reflected their expectations . additionally , they chose @xmath92 , and @xmath93 on the basis of how many clusters they expected .",
    "for the actual data , a third trajectory model was also introduced : a dp mixture of constant trajectories rather than of gaussian - process trajectories .",
    "this entails only a slight complication of the analysis , in that now ( [ npalt3 ] ) is a three - component mixture rather than a two - component mixture .",
    "this is equivalent to including the limiting - linear - model framework of @xcite  whereby a flat trajectory is given nonzero probability as an explicit limiting case of the gaussian process  inside the base measure of the functional dirichlet process itself .",
    "the above model was implemented using the blocked gibbs - sampling algorithm of @xcite to draw from the nonparametric distributions @xmath71 and @xmath68 .",
    "convergence was assessed through multiple restarts from different starting points , and was judged to be satisfactory .",
    "the software is available from the journal website in a supplementary file [ scott ( @xcite ) ] .",
    "overall , 981 of 5498 firms were flagged as being from the alternative model with greater than @xmath31 probability , representing an overall discovery rate of about @xmath94 .",
    "of these , only 196 firms were from the alternative model with greater than @xmath40 probability .    to assess robustness to the hyperparameter choices @xmath87 and @xmath95 , which control the marginal variance and temporal range of of the gaussian process base measure in ( [ fdpgp2 ] )",
    ", the results were recomputed for a coarse grid of 12 pairs of values spanning @xmath96 and @xmath97 .",
    "this reflected the lower and upper ends of what the collaborating management theorists considered reasonable on the basis of observing draws from these priors .",
    "as expected , larger values of @xmath87 tended to yield fewer nonnull classifications ( due to variance inflation in the marginal likelihoods ) , while larger values of @xmath95 tended to punish firms whose peaks and valleys in performance were short - lived over the 41-year time horizon .",
    "many firms that were borderline in the original analysis ( that is , having @xmath44 just barely larger than @xmath31 ) were reclassified as `` noise '' for certain other values of @xmath98 .",
    "yet a stable cohort of @xmath99 firms were flagged as nonnull in all 12 analyses , suggesting a reasonable degree of robustness with respect to hyperparameter choice .",
    "the behavior of individual firms was then characterized by using the mcmc history from the original analysis to get a maximum - likelihood estimate of the nonparametric alternative model .",
    "the almost - sure discreteness of the dirichlet process means that this estimate is a mixture of a small number of flat and gaussian - process trajectories .",
    "the 17 highest - weight trajectories in the mle are in figure [ mletrajectories ] ; they are split into four loose categories reflecting different archetypes of company performance .",
    "-axis is given on the @xmath10 inverse - cdf scale to reflect the quantile of each firm s performance . ]",
    "@lld2.0d2.0d2.0ccc@ * gv * & * company name * & & & & & & + 11535 & winn - dixie stores & 4 & 1 & 10 & 78 & 5 & 2 + 4828 & delhaize america & 7 & 2 & 11 & 75 & 4 & 1 + 6830 & lubrizol corp & 20 & 9 & 2 & 64 & 4 & 1 + 7139 & maytag corp & 25 & 2 & 8 & 57 & 2 & 6 + 4323 & emery air freight & 5 & 2 & 29 & 57 & 4 & 3 + 7734 & national gas & oil & 18 & 13 & 11 & 53 & 3 & 2 +    this allows an mle clustering analysis : if @xmath71 in ( [ fdpgp ] ) is frozen at the mixture model in figure [ mletrajectories ] and the mcmc rerun , it is possible to flag companies that come from specific clusters .",
    "( strictly speaking , only the first 17 atoms in the stick - breaking approximation of @xmath71 were frozen ; others atoms were still considered , but they were allowed to vary . ) examples of the kinds of summaries available are in table [ fallersummaries ] and figure [ threeexamplefirms ] .",
    "some general features of the methodology are apparent from these results :    * there is substantial shrinkage of estimated mean trajectories back toward the global average ( i.e. ,  the 50th percentile ) .",
    "this is itself a form of multiplicity correction , in that extreme outcomes are quite likely to be attributed to chance even among those firms flagged as being from the alternative model .",
    "* often there is no dominant trajectory in the mle cluster set that characterizes a specific firm s history . for the three firms in figure [ threeexamplefirms ] , the probability is split among two or even three trajectories . *",
    "model - averaged predictions of future performance are available with very little extra work , since full mcmc histories of each firm s trajectory and residual model are available .",
    "* the mle clustering analysis can provide evidence for historical evolution within specific firms , which is of great interest as the subject of follow - up case studies .",
    "maytag , for example , displays markedly different performance patterns before and after 1987 , which is reflected in its high probability of being from a falling trajectory ( gp 8) .",
    "it must be emphasized that any such clustering analysis is at best an approximation , aside from the fact that the models themselves are also approximations .",
    "it relies , after all , upon a single point estimate of the trajectories composing the random distribution @xmath71 , and as such ignores uncertainty about the trajectories themselves .",
    "in the example at hand , several independent runs were conducted ; each yielded a different mle cluster set , but the same broad patterns ( e.g. ,  something like gp  8 , something like flat 3 , and so on ) emerged each time , suggesting at least some degree of robustness of the qualitative conclusions .",
    "still , the most reliable quantities are the inclusion probabilities @xmath100 computed from the full nonparametric analysis , which should form the basis of claims regarding which units are from the null and which are not .",
    "this section recapitulates the simulation study of section [ prelimrobustnessstudy ] using the more complicated models .",
    "the goal is to assess type - i error performance by applying the methodology outlined here to a simulated data set where the number of nonzero trajectories is known .",
    "the true residual model @xmath68 was constructed using a single mcmc draw from the stick - breaking representation of the nonparametric residual model for the corporate - performance data .",
    "this corresponded roughly to a 21-component mixture of ar(1 ) models ( since 39 of the 60 atoms in the stick - breaking representation of @xmath68 had trivial weights ) , with many @xmath101 pairs that differed starkly in character .",
    "to simulate the true mean trajectories , two independent sets of @xmath102 draws were taken from the prior in ( [ npalt1])([npalt3 ] ) . in the first simulated set of trajectories ,",
    "the true value of @xmath29 was fixed at @xmath103 in order to roughly approximate the fraction of discoveries on the roa data . in the second set , @xmath29",
    "was fixed at @xmath104 , to reflect a much sparser collection of signals . upon sampling , these yielded @xmath105 and @xmath106 nonzero trajectories , respectively .    in both studies ,",
    "each of these trajectories was convolved with a single independent vector of autoregressive noise drawn from the true @xmath68in other words , with a highly complex pattern of residual variation of the type that was shown to flummox the `` single ar(1 ) '' model of section [ basicmodel ] . for the sake of comparison ,",
    "the same set of @xmath102 noise vectors was used for each experiment .",
    "the results of these simulations were very encouraging for the type - i error performance of the more complicated model .",
    "in the first simulated data set , @xmath107 of the @xmath105 nonzero trajectories were flagged as nonnulls with greater than @xmath31 probability .",
    "only 24 of the 4374 null companies were falsely flagged , and of these , only @xmath108 had larger than a @xmath109 inclusion probability .    in the second simulated data set , of the @xmath106 known nonzero trajectories , @xmath110 had posterior inclusion probabilities greater than @xmath31 , and @xmath111 had inclusion probabilities greater than @xmath40 . of the @xmath112",
    "null trajectories , only @xmath113 had inclusion probabilities greater than @xmath31 ( these two were only @xmath114 and @xmath115 ) .    if @xmath42 is taken as the decision rule for a posteriori classification of a trajectory as nonnull ( which again reflects a symmetric 01 loss function ) , then the realized false - discovery rates were only @xmath116 on 26 discoveries in the sparser case , and only @xmath117 on 321 discoveries in the denser case .",
    "( the closeness of these two realized false - discovery rates may simply be a coincidence of the particular values of @xmath29 chosen ) .",
    "this suggests that a sizeable fraction of the @xmath118 firms flagged as nonnull trajectories in section [ exampleresults ] represent nonaverage performers , and are not false positives .",
    "this paper has described a framework for bayesian multiple hypothesis testing in time - series analysis .",
    "the proposed methodology requires specifying only a few key hyperparameters for the nonparametric null and alternative models , and general guidelines for choosing these quantities have been given .",
    "importantly , no ad - hoc `` correction factors '' are necessary in order to introduce a penalty for multiple testing .",
    "rather , this penalty happens quite naturally by treating the prior inclusion probability @xmath29 as an unknown quantity with a noninformative prior .",
    "nave characterizations of the null hypothesis are shown to have poor error performance , suggesting that the bayesian procedure is highly sensitive to the accuracy of the null model used to describe an `` average '' time series .",
    "yet once a sufficiently complex model for residual variation is specified , the procedure exhibits very strong control over the number of false - positive declarations , even in the face of firms with different autoregressive profiles .",
    "the difference between the results in section [ prelimrobustnessstudy ] and section [ simstudy ] highlights the effectiveness and practicality of using nonparametric methods as a general error - robustification tactic in multiple - testing problems .",
    "posterior inference for a specific time series can be summarized in at least three ways : by quoting @xmath44 ( the probability of that unit s being from the alternative model ) , by performing an mle clustering analysis as in section [ exampleresults ] , or by plotting the posterior draws of the unit s nonzero mean trajectory .",
    "plots such as figure [ threeexamplefirms ] can be quite useful for communicating inferences to nonexperts , as in the management - theory example considered throughout .",
    "the companies flagged as impressive performers , of course , can only be judged so with respect to a particular notion of randomness : the dp mixture of autoregressive models described in section [ dpar ] .",
    "inference on the nonzero trajectories can still reflect model misspecification , and can not unambiguously identify companies that have found a source of sustained competitive advantage .",
    "this dirichlet - process model , however , is a much more general statement of the null models postulated by @xcite and @xcite , suggesting that the procedure described here can identify firms that , with high posterior probability , depart from randomness in a specific way that may be interesting to researchers in strategic management .",
    "the author would like to thank mumtaz ahmed , michael raynor , lige shao , jim guszcza and jim wappler of deloitte consulting for their insight into the application discussed here , along with andy henderson of the university of texas and carlos carvalho of the university of chicago for all their helpful feedback ."
  ],
  "abstract_text": [
    "<S> this paper describes a framework for flexible multiple hypothesis testing of autoregressive time series . </S>",
    "<S> the modeling approach is bayesian , though a blend of frequentist and bayesian reasoning is used to evaluate procedures . </S>",
    "<S> nonparametric characterizations of both the null and alternative hypotheses will be shown to be the key robustification step necessary to ensure reasonable type - i error performance . </S>",
    "<S> the methodology is applied to part of a large database containing up to 50 years of corporate performance statistics on 24,157 publicly traded american companies , where the primary goal of the analysis is to flag companies whose historical performance is significantly different from that expected due to chance .    . </S>"
  ]
}