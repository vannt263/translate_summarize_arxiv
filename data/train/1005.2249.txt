{
  "article_text": [
    "consider a signal @xmath6 , and suppose that we observe its linear transformation plus measurement noise as : @xmath7 here , @xmath8 is an @xmath9 matrix .",
    "if we define an objective function @xmath10 then we may estimate the parameter @xmath11 by minimizing @xmath12 , subject to appropriate constraints .",
    "if @xmath13 , then the solution of the unconstrained optimization problem @xmath14 is not unique . in order to estimate @xmath11 , additional assumptions on @xmath11",
    "is necessary .",
    "we are specifically interested in the case where @xmath11 is sparse .",
    "that is @xmath15 , where @xmath16 it is known that under appropriate conditions , it is possible to recover @xmath11 by solving ( [ eq : quad - opt - unconstr ] ) with a sparsity constraint as follows : @xmath17 however , this optimization problem is generally np - hard .",
    "therefore one seeks computationally efficient algorithms that can approximately solve ( [ eq : opt ] ) , with the goal of recovering sparse signal @xmath11 .",
    "this paper considers the popular orthogonal matching pursuit algorithm ( omp ) , which has been widely used for this purpose ( for example , see @xcite ) .",
    "we are specifically interested in two issues : the performance of omp in terms of optimizing @xmath12 and the performance of omp in terms of recovering the sparse signal @xmath11 .",
    "our analysis considers a more general objective function @xmath12 that does not necessarily take the quadratic form in ( [ eq : quad - opt ] ) . however , we assume that @xmath12 is convex .",
    "for such a general convex objective function , we consider the fully ( or totally ) corrective greedy algorithm in figure  [ fig : forward - greedy ] , which was analyzed in @xcite .",
    "this paper refines the analysis to show that the algorithm works under the restricted isometry property ( rip ) of @xcite ( the required condition will be described later in this section ) .",
    "this algorithm is a direct generalization of omp which has been traditionally considered only for the quadratic objective function in ( [ eq : quad - opt ] ) with @xmath18 . for simplicity , we assume that the number of iterations @xmath19 is chosen a priori . the algorithm has been known in the machine learning community as a version of boosting @xcite , and has also been proposed recently in the signal processing community @xcite .",
    "input : @xmath12 defined on @xmath2 , + initial feature set @xmath20 .",
    "+ output : @xmath21 + let @xmath22 + ( default choice is @xmath18 with @xmath23 ) + * for * @xmath24 + let @xmath25 + let @xmath26 + let @xmath27 + * end *    for quadratic loss , the objective function @xmath12 is given by ( [ eq : quad - opt ] ) and its derivative is @xmath28 .",
    "therefore @xmath25 becomes @xmath29 , where @xmath30 is the @xmath31-th column of matrix @xmath8 .",
    "this , together with @xmath18 , leads to the standard omp algorithm . in order to use notation consistent with the sparse recovery literature , in the current paper",
    ", we still refer to the more general algorithm in figure  [ fig : forward - greedy ] as omp even though it applies to objective functions other than ( [ eq : quad - opt ] ) .",
    "the general problem of optimization under sparsity constraint is np hard . in order to alleviate the difficulty , we consider approximate optimization under the restricted strong convexity assumption introduced below",
    ".    given any @xmath32 , define restricted strong convexity constants @xmath33 and @xmath34 as follows : for all @xmath35 , we require @xmath36    if the objective function takes the quadratic form given by ( [ eq : quad - opt ] ) , then the above definition is equivalent to the following sparse eigenvalue condition of @xmath37 : @xmath38 such that @xmath39 , @xmath40 in this case , the constants @xmath33 and @xmath34 are closely related to the restricted isometry constant @xmath41 in @xcite , which is defined as a constant that satisfies the condition that @xmath38 such that @xmath39 : @xmath42 the restricted isometry constant was used to define the restricted isometry property ( rip ) in the analysis of @xmath43 regularization method @xcite .",
    "we employ the slightly more general restricted strong convexity constants in ( [ eq : sparse - eig - cond ] ) because our analysis only requires the ratio @xmath44 to be bounded , and this is useful for general machine learning problems where @xmath34 can be larger than @xmath45 .    in order to recover the target @xmath11",
    ", we have to assume that @xmath11 is sparse and approximately optimizes @xmath12 . if a target @xmath11 is an exact global optimal solution , then @xmath46 .",
    "however , this paper deals with approximate optimal solutions , where @xmath47 .",
    "in particular , we introduce the following definition , which is convenient to apply .",
    "[ def : grad ] given @xmath6 and @xmath48 , we define the restricted gradient optimal constant @xmath49 as the smallest non - negative value that satisfies the following condition @xmath50 for all @xmath51 such that @xmath52 .    the constant @xmath49 measures how close is @xmath53 to zero .",
    "if @xmath46 , then @xmath54 . if @xmath47 , then @xmath49 is small .",
    "moreover , similar to the definition of restricted strong convex constants , we are only interested in the value of @xmath53 in any subset of @xmath55 with @xmath56 elements .",
    "the following proposition provides some estimates of @xmath49 using quantities that are easier to understand .",
    "we have @xmath57 and @xmath58 .",
    "moreover , if @xmath59 then @xmath60 [ prop : delta ]    the first two inequalities are straight - forward . for the third inequality , we note that for @xmath52 : @xmath61 \\\\    = & q(\\bar{{{\\mathbf x } } } ) - |\\nabla q(\\bar{{{\\mathbf x}}})^\\top { { \\mathbf u}}|^2/ ( 4 \\rho_+(s ) \\|{{\\mathbf",
    "u}}\\|_2 ^ 2 ) .\\end{aligned}\\ ] ] the result follows by rearranging the above inequality .",
    "the following theorem is the main result of this paper , which shows that omp can approximately recover a sparse signal @xmath11 in 2-norm if the condition ( [ eq : rip - condition ] ) in the theorem involving strong convexity constants can be satisfied . as we shall discuss later , this condition is closely related to the rip condition for the quadratic objective ( [ eq : quad - opt ] ) .",
    "consider the omp algorithm .",
    "let @xmath6 and @xmath62 .",
    "if there exists @xmath56 such that @xmath63 then when @xmath64 , we have @xmath65 and @xmath66 [ thm : omp_rip ]    the detailed proof relies on a number of technical lemmas that are left to the appendix .    the first inequality of the theorem is a direct consequence of lemma  [ lem : omp_rip ] .",
    "the second inequality is a consequence of the first inequality and lemma  [ lem : param - est ] : @xmath67 + \\epsilon_s(\\bar{{{\\mathbf x}}})^2 /\\rho_-(s ) \\\\",
    "\\leq & 6 \\epsilon_s(\\bar{{{\\mathbf x}}})^2 /\\rho_-(s ) .\\end{aligned}\\ ] ] this implies the second inequality .",
    "note that ( [ eq : rip - condition ] ) can be satisfied as long as @xmath68 grows sub - linearly as a function of @xmath56 . with appropriate assumptions ,",
    "this allows the ratio @xmath44 to be significantly larger than @xmath69 but bounded from above ( such a condition is sometimes referred to as sparse eigenvalue condition in the statistics literature ) . in this context ,",
    "theorem  [ thm : omp_rip ] is useful for estimation problems encountered in machine learning , where @xmath44 may be large .    in compressed sensing , one can often control the ratio of @xmath44 to be not much larger than @xmath69 using random projection",
    "in this context , the following result gives a simpler interpretation of the above theorem , where the condition ( [ eq : rip - condition ] ) of the theorem is replaced by @xmath70 .",
    "consider the omp algorithm with @xmath18 .",
    "let @xmath6 and @xmath71 .",
    "if the condition @xmath70 holds , then when @xmath72 , we have @xmath65 and @xmath73 where @xmath74 .",
    "[ cor : omp - rip ]    if @xmath70 holds , then we can let @xmath74 , which implies that @xmath75 therefore @xmath76 this means that the condition ( [ eq : rip - condition ] ) holds , and the corollary follows directly from theorem  [ thm : omp_rip ] .    for the quadratic objective ( [ eq : quad - opt ] ) , the condition @xmath70 is analogous to the rip condition in @xcite . in particular ,",
    "if the matrix @xmath8 has the restricted isometry constant @xmath77 , then the condition @xmath78 and @xmath79 holds , with @xmath34 and @xmath33 defined according to ( [ eq : sparse - eig - cond ] ) .",
    "in this case , corollary  [ cor : omp - rip ] can be directly applied .",
    "it is interesting to observe that except for constants , the result of this paper for omp is as strong as those for more sophisticated greedy algorithms such as romp @xcite or cosamp @xcite .",
    "for example , corollary  [ cor : omp - rip ] can be applied when @xmath80 with @xmath74 , while a similar result for cosamp in @xcite applies when @xmath81 with @xmath82 .",
    "nevertheless , the difference in the constants may still suggest possible advantages for more complex algorithms such as cosamp under suitable conditions .    for quadratic objective function",
    ", a simple instantiation of @xmath49 using proposition  [ prop : delta ] leads to the following sparse recovery result that is relatively simple to interpret .    consider the quadratic objective function @xmath83 of ( [ eq : quad - opt ] ) , and the omp algorithm with @xmath18 .",
    "consider an arbitrary vector @xmath6 and let @xmath71 .",
    "if the rip condition @xmath70 holds , then when @xmath72 , we have @xmath84 where @xmath74 .",
    "[ cor : sparse - recovery - quad ]",
    "in this paper we proved a new result for a generalization of the omp algorithm .",
    "it is shown that if the rip is satisfied at sparsity level @xmath0 , then omp can recover a @xmath1-sparse signal in 2-norm . for compressed sensing applications , this result implies that in order to uniformly recover a @xmath1-sparse signal in @xmath2 , only @xmath85 random projections are needed @xcite .    our result for signal recovery is stronger than previous results for omp that relied on different conditions .",
    "for example , @xcite considered the problem of recovering the support set of a sparse signal under a stronger condition ( also see @xcite for recovery properties under stochastic noise ) .",
    "a similar analysis was employed in @xcite , where it was shown that for any fixed sparse signal @xmath11 with @xmath71 , omp can recover the signal with large probability using @xmath86 measurements . a more refined analysis in @xcite",
    "shows that a lower bound of @xmath87 measurements is enough for recovery .",
    "however , the above results are not uniform with respect to all @xmath1-sparse signals @xmath11 ( that is , for any set of random projections , there exist @xmath1-sparsity signals that fail the analysis ) . in comparison",
    ", the rip condition holds uniformly by definition , and hence our result applies uniformly to all @xmath1-sparse signals . although our result is stronger than previous results in terms of signal recovery in 2-norm , the result requires running the omp algorithm for more than @xmath1 iterations , and hence does nt recover the true support set of the ideal signal . in comparison , results such as @xcite also imply exact recovery of the correct support set ( but under stronger assumptions ) using only @xmath1 omp iterations .",
    "it is also known that it is impossible to uniformly recover the support set ( in @xmath1 iterations ) with the omp algorithm with @xmath86 measurements @xcite .",
    "this means that it is necessary to run omp for more than @xmath1 iterations in order to achieve the best 2-norm recovery performance with as few meausrements as possible .",
    "it is worth mentioning that some previous results apply uniformly to all @xmath1-sparse signals .",
    "for example , results in @xcite depend on the stronger mutual incoherence condition .",
    "unfortunately the mutual incoherence condition can only be satisfied with @xmath4 random projections .",
    "therefore in recent years there have been significant interests in studying omp under the rip .",
    "in addition to the current paper , a number of recent papers investigated this issue , reaching varying conclusions @xcite .",
    "for example , the rip - based analysis for sparse signals ( but without noise ) was considered in @xcite , with the conclusion that under a sufficiently strong assumption on the rip constant ( in fact , the resulting condition is similar to the mutual incoherence condition ) , exact recovery is possible in @xmath1 iterations .",
    "the condition required for the rip constant was weakened in @xcite , where the author showed that by running the omp algorithm more than @xmath1 iterations , it is possible to achieve exact recovery ( again assuming no noise ) .",
    "the condition in @xcite can be satisfied with only @xmath88 measurements , which is a significant improvement over the traditional @xmath4 measurements .",
    "the result obtained in the current paper is along the same line as @xcite , but reduced the required number of measurements to the optimal order of @xmath3 .",
    "it is also interesting to compare the new omp result in this paper to that of lasso , which is also known to work under the rip .",
    "however , a more refined comparison illustrates differences between the known theoretical results for these two methods . for omp",
    ", the result in theorem  [ thm : omp_rip ] can be applied as long as the condition @xmath89 is satisfied . with @xmath18 ,",
    "this roughly requires @xmath90 to grow sub - linearly as a function of @xmath56 in order to apply the theory . in comparison",
    ", the known condition for lasso ( e.g. , this has been made explicit in @xcite ) requires @xmath44 to grow sub - linearly as a function of @xmath56 . to compare the two conditions",
    ", we note that the condition for omp is weaker in terms of of the upper convexity constant as there is no explicit dependency on @xmath34 ; however , the dependency on @xmath33 is stronger in omp than lasso due to the logarithmic term .",
    "although it is unclear how tight these conditions are , the comparison nevertheless indicates that even though both algorithms work under the rip , there are still finer differences in their theoretical analysis : lasso is slightly more favorable in terms of its dependency on the lower strong convexity constant , while omp is more favorable in terms of its dependency on the upper strong convexity constant .",
    "we further conjecture that the extra logarithmic dependency @xmath91 in omp is necessary . in practice , some times lasso performs better while other times omp performs better ( for example , see experimental results in @xcite ) .",
    "therefore some discrepancy in their theoretical analysis is expected .",
    "more specifically , for sparse recovery , one often observes that lasso is superior when the nonzero coefficients have a similar magnitude ( which happens to be the case that the extra @xmath91 factor is required in our omp analysis ) while omp performs better when the nonzero coefficients exhibit rapid decay ( which happens to be the case that the extra @xmath91 factor can be removed from our analysis ) .",
    "the theory in this paper significantly narrows the previous theoretical gap between these two sparse recovery methods by positively answering the open question of whether omp can recover sparse signals under the rip .",
    "therefore our result allows practitioners to apply omp with more confidence than previously expected .",
    "the author would like to thank the anonymous referees for pointing out many relevant references and for suggestions to improve the presentation .",
    "we need a number of technical lemmas .",
    "lemma  [ lem : one - step ] and lemma  [ lem : progress ] , key to the proof , are based on earlier work of the author with collaborators @xcite .",
    "the first three lemmas use the following notations .",
    "let @xmath92 be two subsets of @xmath55 .",
    "let @xmath93 , and @xmath94      let @xmath97 , then by the definition of @xmath98 , we know that @xmath99 .",
    "therefore @xmath100 which implies the lemma .",
    "the first inequality is by the definitions of @xmath34 and @xmath49 .",
    "the last inequality follows from the fact that @xmath101 with @xmath102 and @xmath103 .",
    "from @xmath106 we obtain the desired inequality .",
    "the first inequality is by the definitions of @xmath33 and @xmath49 .",
    "the last inequality again follows from the fact that @xmath101 with @xmath102 and @xmath103 .",
    "the next lemma shows that each greedy search makes reasonable progress .",
    "this proof is essentially identical to a similar result in @xcite but with refined notations used in the current paper .",
    "we thus include the proof for completeness .",
    "it allows the readers to verify more easily that the proof in @xcite remains unchanged with our new definitions .      for all @xmath111 and @xmath112 , we define @xmath113 it follows from the definition of @xmath114 that @xmath115 since the choice of @xmath110 achieves the minimum of @xmath116 , the lemma is a direct consequence of the following stronger statement : @xmath117 with an appropriate choice of @xmath118 ; this is because @xmath119 therefore , we now turn to prove that holds .",
    "denoting @xmath120 , we obtain that @xmath121 since we assume that @xmath98 is optimal over @xmath122 , we get that @xmath123 for all @xmath124 .",
    "additionally , @xmath125 for @xmath126 and @xmath127 for @xmath128 .",
    "therefore , @xmath129 combining the above with the definition of @xmath33 , we obtain that @xmath130 combining the above with we get @xmath131   + u \\,\\rho_+(1 ) \\eta^2 .\\end{aligned}\\ ] ] setting @xmath132 /(2 u \\rho_+(1))\\ ] ] and rearranging the terms , we conclude our proof of ( [ eqn : plem:1 ] ) .",
    "the direct consequence of the previous lemma is the following result , which is critical in our analysis .",
    "the idea of using a nesting approximating sequence has appeared in @xcite , but the current version is improved .",
    "the change is necessary for the purpose of this paper . in the following @xmath133",
    "can be chosen as any positive number if @xmath134 .",
    "[ lem : progress ] consider the omp algorithm . consider a positive integer @xmath135 and subsets @xmath136 , where @xmath137 .",
    "assume that @xmath138 ( @xmath139 ) , @xmath140 , and let @xmath141 . if @xmath142 and @xmath143 then @xmath144    note that for any @xmath145 and @xmath93 , we have when @xmath108 : @xmath146 therefore lemma  [ lem : one - step ] implies that at any @xmath147 such that @xmath142 and @xmath148 , we have either @xmath149 or @xmath150 where we simply replace the target vector @xmath11 in lemma  [ lem : one - step ] by the optimal solution over @xmath151 , and replace @xmath98 by @xmath21 .",
    "the inequality , along with @xmath152 , implies that either @xmath149 or @xmath153    \\max\\left(0 , q({{\\mathbf x}}^{(k)})-q(\\bar{{{\\mathbf x}}})-q_\\ell\\right ) \\\\",
    "\\leq &   \\exp \\left[-\\frac{\\rho_-(s ) } { \\rho_+(1 ) |\\bar{f}_\\ell \\setminus f^{(k)}|}\\right ]    \\max\\left(0 , q({{\\mathbf x}}^{(k)})-q(\\bar{{{\\mathbf x}}})-q_\\ell\\right ) .\\end{aligned}\\ ] ] therefore for any @xmath154 and @xmath155 , we have either @xmath149 or @xmath156    \\max\\left(0 ,",
    "q({{\\mathbf x}}^{(k')})-q(\\bar{{{\\mathbf x}}})-q_\\ell\\right ) .\\nonumber\\end{aligned}\\ ] ]    we are now ready to prove the lemma by induction on @xmath135 .",
    "if @xmath134 , we can set @xmath157 in ( [ eq : proof - multi - steps ] ) and consider any @xmath158 .",
    "since @xmath159 , we have @xmath160 therefore when @xmath161 we have from ( [ eq : proof - multi - steps ] ) that if @xmath162 , then @xmath163   q_0\\\\ \\leq & ( 2\\mu)^{-1 } q_0 .\\end{aligned}\\ ] ] note that this inequality also holds when @xmath164 , and in such case ( [ eq : proof - multi - steps ] ) does not apply .",
    "this is because in this case @xmath165 .",
    "therefore the lemma always holds when @xmath134 .",
    "now assume that the lemma holds at @xmath166 for some @xmath167 .",
    "that is , with @xmath168 we have @xmath169 this implies that when @xmath170 : @xmath171 we thus obtain from ( [ eq : proof - multi - steps ] ) that if @xmath172 , then @xmath173   ( 2 q_{l-1})\\\\ \\leq & ( 2\\mu)^{-1 } ( 2 q_{l-1 } ) .\\end{aligned}\\ ] ] again this inequality also holds when @xmath174 , and in such case ( [ eq : proof - multi - steps ] ) does not apply .",
    "this is because in this case @xmath175 .",
    "this finishes the induction .",
    "assume that the claim holds with @xmath182 for some @xmath183 .",
    "now we consider the case of @xmath184 . without loss of generality ,",
    "we assume for notational convenience that @xmath185 , and @xmath186 in @xmath187 is arranged in descending order so that @xmath188 . let @xmath135 be the smallest positive integer such that for all @xmath189 , we have @xmath190 but @xmath191 where @xmath192 .",
    "we have @xmath193 because the second inequality is automatically satisfied when @xmath194 ( the right hand side is zero in this case ) .",
    "moreover , if the second inequality is always satisfied for all @xmath195 , then we can simply take @xmath134 ( and ignore the first inequality ) .",
    "lemma  [ lem : q - bound ] implies that for @xmath198 : @xmath199 moreover @xmath200 when @xmath201 .",
    "we can thus apply lemma  [ lem : progress ] to conclude that when @xmath202 we have @xmath203 where ( [ eq : proof - decay ] ) is used to derive the second inequality .",
    "now , if @xmath204 then ( [ eq : proof - barx - progress ] ) implies that ( [ eq : proof - strong ] ) holds automatically ( since @xmath205 ) , which finishes the induction .",
    "therefore in the following , we only consider the case ( [ eq : proof - barx - small ] ) does not hold , which implies that @xmath206 now lemma  [ lem : param - est ] implies that @xmath207 this implies that @xmath208 therefore @xmath209 .",
    "that is , @xmath210 .",
    "it follows from the induction hypothesis that after another @xmath211 omp iterations , ( [ eq : proof - strong ] ) holds .",
    "therefore by combining this estimate with ( [ eq : proof - ind - k ] ) , we know that the total number of omp iterations for ( [ eq : proof - strong ] ) to hold ( starting with @xmath212 ) is no more than @xmath213 this finishes the induction step for the case @xmath214 .",
    "tong zhang tong zhang received a b.a . in mathematics and computer science from cornell university in 1994 and a ph.d . in computer science from stanford university in 1998 .",
    "after graduation , he worked at ibm t.j .",
    "watson research center in yorktown heights , new york , and yahoo research in new york city .",
    "he is currently a professor of statistics at rutgers university .",
    "his research interests include machine learning , algorithms for statistical computation , their mathematical analysis and applications .                a.  fletcher and s.  rangan .",
    "orthogonal matching pursuit from noisy random measurements : a new analysis . in y.",
    "bengio , d.  schuurmans , j.  lafferty , c.  k.  i. williams , and a.  culotta , editors , _ advances in neural information processing systems 22 _ , pages 540548 .",
    "j.  huang , t.  zhang , and d.  metaxas .",
    "learning with structured sparsity .",
    "technical report , rutgers university , january 2009 .",
    "a short version appears in icml09 .",
    "available from http://arxiv.org/abs/0903.3002 ."
  ],
  "abstract_text": [
    "<S> this paper presents a new analysis for the orthogonal matching pursuit ( omp ) algorithm . </S>",
    "<S> it is shown that if the restricted isometry property ( rip ) is satisfied at sparsity level @xmath0 , then omp can stably recover a @xmath1-sparse signal in 2-norm under measurement noise . for compressed sensing applications </S>",
    "<S> , this result implies that in order to uniformly recover a @xmath1-sparse signal in @xmath2 , only @xmath3 random projections are needed . </S>",
    "<S> this analysis improves some earlier results on omp depending on stronger conditions that can only be satisfied with @xmath4 or @xmath5 random projections .    </S>",
    "<S> estimation theory , feature selection , greedy algorithms , statistical learning , sparse recovery </S>"
  ]
}