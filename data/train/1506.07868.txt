{
  "article_text": [
    "consider @xmath13 fixed and unknown , acquired through quadratic measurements of the form @xmath14 , @xmath15 . assuming @xmath4 has full column rank , this is equivalent to receiving noiseless samples @xmath16 of the positive semidefinite matrix @xmath17 .",
    "scenarios such as this arise in various applications : for a concrete example , suppose we receive a stream of high - dimensional centered gaussian vectors @xmath18 with unknown covariance matrix @xmath19 . if we believe @xmath19 to be ( approximately ) low rank , we can recover this structure via the sampled matrix @xmath20 however , due to the large dimensionality , storing all of the incoming vectors @xmath21 might be prohibitive . instead , we randomly draw a set of sensing vectors @xmath22 which are efficient to store ( e.g. , they are _ sparse _ ) and for each incoming data point compute @xmath23 .",
    "we are now only storing @xmath24 which is a sparse data set .",
    "note that if we define @xmath25 then @xmath26 has the form @xmath27 the question posed above is : can we compute the covariance structure of the @xmath28 given only this data ?",
    "the example above describes _ covariance sketching _ of high - dimensional data streams @xcite , but there are many other scenarios that fall under our problem setting , e.g. , phaseless measurements in physics and optics @xcite .",
    "because this data is invariant under the transformation @xmath29 for any orthogonal matrix @xmath30 , we can only hope to recover @xmath4 up to this action . in the complex rank one setting @xmath31 , this means we can only recover @xmath32 up to a global phase .",
    "_ phase retrieval _ problems of this type often arise in the physical sciences due to the nature of optical sensors , which can only record intensity information @xcite .",
    "it is now well - understood that if the measurement vectors @xmath12 are generic or e.g. independent gaussian random vectors , then @xmath33 measurements suffice for injectivity of the map @xmath34 up to phase @xcite .",
    "there is still a question of how to perform the inverse map in an efficient and stable manner , and in recent years several different algorithms have been proposed in this direction , see for example @xcite . in particular , @xcite noted that such measurements may be reformulated as @xmath35 so that one can consider this problem as that of recovering an unknown rank - one positive semi - definite matrix .",
    "inspired by the field of compressive sensing @xcite and low - rank matrix recovery @xcite , this led to many results demonstrating that well chosen convex and semidefinite programming ( sdp ) relaxations can provably recover the underlying signal up to phase with only @xmath36 gaussian measurements @xcite .",
    "however , as such algorithms optimize over the  lifted \" space of @xmath37 positive semidefinite matrices , the computational complexity becomes quite high . in the more general rank-@xmath0 setting , whereby the measurements are @xmath38 the recent works @xcite demonstrate that convex relaxation techniques based on nuclear norm minimization can solve such problems from an optimal number of measurements @xmath39 , but still require large computational cost .    in the rank-1 setting in particular ,",
    "several alternative reconstruction algorithms have been proposed with global phase recovery guarantees which operate directly on the lower - dimensional problem , and thus are more computationally efficient .",
    "notably , @xcite considers the nonconvex optimization problem @xmath40 and proves that after a judiciously chosen initialization , with high probability alternating minimization will converge to the underlying vector @xmath32 up to phase , assuming random gaussian measurements .",
    "subsequently @xcite used the same initialization to show convergence when followed by gradient descent without requiring resampling .",
    "both of these algorithms provably recover the underlying vector @xmath32 up to global phase , from a number of measurements @xmath2 which is optimal up to additional logarithmic factors in @xmath41 .",
    "very recently , the paper @xcite provides a modified gradient method which removes the additional logarithmic factors of @xmath41 in the number of measurements .    in a similar vein",
    ", many recent works have demonstrated global convergence guarantees for gradient descent on other nonconvex matrix factorization problems .",
    "specifically , in @xcite the authors consider gradient descent on the grassmannian and prove global convergence for a class of svd problems . in @xcite",
    "a stochastic gradient algorithm was shown to converge globally for a low - rank matrix least squares problem . in @xcite",
    "the authors consider the recovery of a full - rank matrix from sparse linear measurements via manifold optimization over the sphere . in all of these works including ours",
    ", the underlying idea is that the lack of convexity can be fixed by operating on an appropriate matrix manifold .    in this paper",
    ", we consider the more general version of problem in which the underlying matrix is of rank @xmath0 : @xmath42 and our measurements take the form @xmath43 as noted in @xcite , it seems unlikely that a deterministic rip condition holds in this setting . in any case ,",
    "our local convexity results are novel and might shed light on other nonconvex problems unrelated to matrix recovery .",
    "note that throughout the paper we assume that @xmath4 has full rank , and without loss of generality we can assume its columns are orthogonal .",
    "in contrast to previous algorithms operating directly on the rank one problem , we demonstrate that , under a gaussian assumption on the random measurement vectors , the function is strongly convex in certain directions , and we can recover @xmath4 ( up to an orthogonal matrix ) via spectral initialization followed by gradient descent . in particular , we prove gradient descent will converge linearly at a rate which depends on the condition number @xmath44 of the full matrix @xmath45 and the ambient dimension @xmath41 . moreover , the classical phase retrieval problem @xmath31 can be recast as a rank 2 recovery problem within our framework ( see  [ sec : complex ] ) .",
    "precisely , we show the following all hold with high probability :    * for general @xmath0 , we demonstrate that after @xmath46 gaussian samples , in a quantifiable region the function is _ strongly _ convex in directions perpendicular to the manifold of solutions * the size of this region is _ independent _ of both the ambient dimension and the rank * with an additional factor of @xmath47 samples , a simple spectral initialization will land within this region with high probability and thus standard gradient descent on will linearly converge to a global minimizer * if @xmath48 and @xmath49 is  not too peaky \" , then for more general sub - gaussian measurements , after @xmath50 subgaussian samples , the function appearing in is _ strongly _ convex in a ball centered at @xmath32 ( and @xmath51 ) , whose radius we explicitly compute .",
    "in the real - valued rank one setting , the strong convexity result we present actually holds in much more generality than the initialization result  for sub - gaussian measurements  and we believe this should be of independent interest ; in particular , our results hold for bernoulli measurements and sparse gaussian measurements .",
    "we note that in the rank-1 setting , recovery results from general sub - gaussian measurements were also provided in @xcite using convex optimization for reconstruction , and a similar incoherence condition on the underlying @xmath32 was also required there .",
    "while preparing this manuscript , we became aware of [ @xcite , p.250 ] which also certifies local convexity of the function , for the special case of gaussian measurements in the rank one setting .",
    "our results can be viewed as exact recovery guarantees for a special case of a _ manifold - constrained least squares problem _ where the manifold is the set of rank @xmath0 positive semidefinite matrices .",
    "this algorithm was studied empirically in @xcite .",
    "many nonconvex problems of interest can be reformulated as a manifold - constrained least squares problem , and we believe that the exact recovery guarantees presented here should be extendable to a broader class of problems .",
    "we aim to solve the nonconvex inverse problem of recovering the unknown matrix @xmath13 ( up to right multiplication by an orthogonal matrix ) from quadratic measurements of the form @xmath52 by solving the nonconvex optimization problem @xmath53 because the function appearing in is invariant under right multiplication by an orthogonal matrix , there is an entire manifold of solutions given by @xmath54 where @xmath55 is the set of @xmath8 orthogonal matrices .",
    "our strategy is to establish that a spectral initialization will land ( with high probability ) in a region of strong convexity around the manifold of global minimizers .",
    "an overview of our approach is given in algorithm [ algo : spectral_init ] .",
    "measurements @xmath56 where @xmath12 are gaussian +    @xmath57 , where the columns of @xmath58 contain the ( @xmath59-normalized ) eigenvectors corresponding to the @xmath0 largest eigenvalues @xmath60 of the matrix @xmath61 and the scaling is given by the diagonal matrix @xmath62    starting at @xmath63 , iteratively update @xmath10 via gradient descent on @xmath64 .    estimated global solution @xmath65 to the nonconvex problem .",
    "there are two main ingredients to proving performance guarantees for algorithm [ algo : spectral_init ] , namely , the strong convexity of the function @xmath66 in a region around the manifold of global minimizers at finite sample complexity , and a guarantee that spectral initialization will land within this region .",
    "the finite sample convexity result holds in more generality when @xmath48 , while for general @xmath0 we always assume gaussian measurements .",
    "we focus on the setting where @xmath67 is a fixed unknown matrix with orthogonal columns , and we receive noiseless samples of the form @xmath68 for i.i.d .",
    "standard gaussian vectors @xmath69 .",
    "now that we have an entire manifold of solutions given by @xmath54 we will need to consider the quantity @xmath70 which is well - defined by compactness of the orthogonal group .",
    "we note that the minimizer may not be unique , but this is not important for our purposes .",
    "we will also need to consider @xmath71 the non - zero eigenvalues of the positive semidefinite matrix @xmath45 . for a given matrix @xmath72 the notation @xmath73 stands for the hessian of the function , which is a random @xmath74 symmetric matrix .",
    "the main finite sample convexity result is as follows :    [ thm : main_convexity ] suppose we take @xmath75 samples of the form , where @xmath76 and @xmath77 are as in ; assume further that @xmath72 satisfies @xmath78 then with probability at least @xmath79 , it holds that @xmath80 where @xmath81 is a minimizer for .    for the proof of this theorem , we refer the reader to section  [ sec : rankr_convexity ] below . note that this theorem implies that for matrices @xmath72 close to the manifold of solutions , we can control the eigenvalues of the hessian ; in particular for such @xmath10 , _ the function @xmath64 is strongly and uniformly convex on the line connecting @xmath10 to its nearest point on the manifold of solutions _ , as measured by the function .",
    "we now show how this local strong convexity results in linear convergence to the true @xmath4 we seek to recover .",
    "we have the following theorem which concisely establishes the initialization and performance guarantees of algorithm [ algo : spectral_init ] .",
    "[ thm : main_theorem ] suppose we take @xmath82 samples of the form , where @xmath76 and @xmath77 are as in .",
    "define the matrix @xmath83 and the following associated quantities :    @xmath84    where @xmath85 are the eigenvalues of @xmath86 and @xmath87 are the corresponding normalized eigenvectors .",
    "if we iteratively update @xmath88 via gradient descent @xmath89 then with probability at least @xmath90 , @xmath91^kd(u_0)\\ ] ] where    @xmath92    and @xmath93 is given by .",
    "in particular , @xmath93 converges to zero geometrically as long as @xmath94 .    for a proof of theorem [ thm : main_theorem ] ,",
    "see section [ sec : init ] .",
    "a few remarks are in order .    1 .",
    "note that the quantity @xmath95 is scale invariant ; however , we have the bounds @xmath96 2 .",
    "one consequence of our result is that the sampling complexity is entirely independent of the desired solution tolerance .",
    "that is , the fixed set of @xmath97 samples suffices to produce a global solution up to arbitrary accuracy .",
    "our numerical results in  [ sec : examples ] suggest that in general the sampling complexity only linearly depends on the ambient dimension @xmath41 .",
    "consequently a more refined analysis and initialization procedure such as that found in the recent work of @xcite for the case of rank-1 recovery is most likely possible also in the general rank - r recovery setting .",
    "this method of analysis should find use in providing recovery guarantees by gradient descent for a broader class of nonconvex problems arising in machine learning applications such as matrix completion , nonnegative matrix factorization , clustering , etc .",
    "more generally , such an analysis could possibly be useful towards achieving provable guarantees for machine learning problems which have many unstable saddle points , such as neural networks @xcite .      in the rank one setting where @xmath49",
    ", we can provide local convexity guarantees holding more generally for sub - gaussian measurements , subject to appropriate incoherence conditions on @xmath32 .",
    "suppose we receive noiseless samples of the form @xmath98 for i.i.d .",
    "sub - gaussian vectors @xmath69 , which we assume satisfy : @xmath99&=0\\\\ \\mathbb{e}[a_ia_i^t]&=\\sigma\\\\ \\end{split}\\ ] ] where @xmath19 is the covariance matrix , which we assume is invertible . with this setup ,",
    "we then consider minimization of the random function @xmath100 note that here the hessian is given by @xmath101    we have the following convexity theorem :    [ thm : main_convexity1]let @xmath49 and @xmath69 be i.i.d .",
    "sub - gaussian satisfying @xmath102=0 $ ] and @xmath103=\\sigma$ ] . with @xmath104 as in ,",
    "define    @xmath105\\right).\\end{aligned}\\ ] ]    if @xmath106 then with probability greater than @xmath107 @xmath108 holds uniformly for all @xmath109 in the ellipse around @xmath32 defined by @xmath110 above , @xmath111 is a constant which depends only on the sub - gaussian norm of @xmath12 .    for a broad class of sub - gaussian measurements",
    ", we can provide an explicit lower bound on @xmath112\\right)$ ] thereby establishing a quantitative bound on the strong convexity parameter . in the case of gaussian measurements in particular ,",
    "such a bound holds independent of @xmath32 . for more general sub - gaussian measurements , additional incoherence constraints on @xmath32  that @xmath32",
    "not be  too peaky \"  are required for strong convexity .",
    "see lemma [ lem : quant_lb ] for details . while preparing this paper we became aware of related results in the thesis @xcite which demonstrate a similar lower bound on the hessian in the case of gaussian measurements .",
    "the finite sample convexity result holds for general sub - gaussian measurements satisfying , while our initialization results require more restrictive conditions , namely that the fourth moment of the measurements is close to that of gaussian measurements ; for simplicity we have only included the result for gaussians which follows from lemma [ thm : init ] in the next section .",
    "the rest of the paper is organized as follows : in  [ sec : rankr_convexity ] we prove the main _ finite sample convexity _ result theorem [ thm : main_convexity ] , which relies on classifying tangent and normal directions to the manifold of solutions @xmath54 and an explicit formula for the expected hessian . in  [ sec : rank_one ] we prove convexity results for the rank one case under more general randomness assumptions . in  [ sec : init ] we prove that with high probability the initialization step produces a matrix in a convex region around the manifold of solutions and establish the convergence of gradient descent . briefly in  [ sec : complex ] we describe how our results generalize to the complex setting . finally , in  [ sec : examples ] we conclude with some numerical experiments demonstrating the performance and robustness of the results presented here .",
    "here we present lemmas that are used in the proof of theorem [ thm : main_convexity ] , as well as a summary of the proof . for the full proof , we refer the reader to section [ sec : proof_convexity_r ] .",
    "the main lemma we rely on is the following simple characterization of the normal directions to the manifold of solutions :    [ lem : u_expansion ] assume @xmath4 has full column rank and let @xmath113 , which is not necessarily unique",
    ". then we can write @xmath114 where @xmath115 is a symmetric positive semidefinite matrix and @xmath116 is the projection onto the orthogonal complement of the column space of @xmath4 .",
    "this basically follows from the solution to the orthogonal procrustes problem @xcite .",
    "if we write @xmath117 for the singular value decomposition of @xmath118 , then we can expand the objective as follows :    @xmath119    we then find that @xmath120 is a symmetric positive semidefinite matrix . as @xmath121 is equivalent to @xmath122 ,",
    "we arrive at the stated claim .",
    "this lemma says that if we consider the direction @xmath123 between @xmath10 and its closest solution matrix @xmath124 we have that @xmath125 which is a _",
    "symmetric _ matrix .",
    "why symmetry is important will become apparent after the next lemma , which establishes formulas for the expectation of the hessian of :    [ lem : calculus ] the gradient of @xmath126 is given by @xmath127 where @xmath128 and the hessian of @xmath64 is given by @xmath129 moreover , if we suppose the @xmath12 s are i.i.d .",
    "centered gaussian random vectors satisfying @xmath130=id$ ] , then the expectation of is given by @xmath131 = a + d\\ ] ] where the @xmath74 block matrices @xmath132 and @xmath133 satisfy    @xmath134    for details , see  [ sec : exp_r_proofs ] in the appendix .",
    "we will also need a standard concentration result :    [ lem : hessian_concentration ] suppose we collect @xmath135 samples of the form @xmath136 , where @xmath137 and @xmath138 are given constants and @xmath139 _ _ rank__@xmath140 ; then we have that with probability greater than @xmath141 @xmath142\\right\\|_{op } < 2\\delta\\|x\\|_{op}^2.\\ ] ]    the sampling complexity can be improved , but we state lemma [ lem : hessian_concentration ] as a general proof - of - concept .",
    "for details see  [ sec : concentration_r_proofs ] .",
    "+    to complete the proof sketch , observe that @xmath143 can be written as a convex quadratic polynomial in @xmath144 , where the constant term is given by @xmath145 and consequently we can bound its smallest positive root using the remarks above ( see  [ sec : finite ] for the rank one setting , where this observation is more straightforward ) .",
    "we apply the concentration from above along with the following one - sided martingale bound from @xcite ( as stated in @xcite ) to establish the stated non - asymptotic bound . for details",
    "see  [ sec : proof_convexity_r ] .",
    "[ lem : one_side_bent ] suppose @xmath146 are i.i.d .",
    "real - valued random variables obeying @xmath147 for some nonrandom @xmath148 , @xmath149 = 0 $ ] , and @xmath150 = v^2 $ ] . setting @xmath151 , @xmath152\\leq \\min\\left\\{\\exp\\left(-\\frac{y^2}{\\sigma^2}\\right),c_0(1-\\phi(y/\\sigma))\\right\\}\\ ] ] where one can take @xmath153 and @xmath154 is the cdf for the standard normal .      in this section",
    "we restrict our attention to the setting where @xmath49 is a fixed unknown vector , and we receive noiseless samples of the form @xmath98 for i.i.d .",
    "sub - gaussian vectors @xmath69 , which we assume satisfy : @xmath99&=0\\\\ \\mathbb{e}[a_ia_i^t]&=\\sigma\\\\ \\end{split}\\ ] ] where @xmath19 is the covariance matrix , which we assume is invertible .",
    "consider the eigenvalue decomposition of the covariance matrix , @xmath155 .",
    "an important quantity in our analysis will be @xmath156 a coherence parameter for @xmath157 and @xmath158,\\ ] ] a 4th moment parameter .",
    "we consider convexity of the function @xmath159 defined in ( equivalently , positive semi - definiteness of the hessian matrix @xmath160 ) in the neighborhood of @xmath161 , first _ in expectation _ with respect to the draw of @xmath12 , or in the limit of infinitely many samples @xmath2 .",
    "these results are necessary for the proof of lemma [ lem : quant_lb ] .",
    "[ lem : exp_formula ] assume that @xmath69 are centered sub - gaussian random vectors with @xmath130=\\sigma$ ] .",
    "assume further that the transformed variables @xmath162 have independent coordinates and equal fourth moment parameter @xmath163 $ ] .",
    "then @xmath164 & = \\left(3\\|\\sigma^{1/2}u\\|_2 ^ 2-\\|\\sigma^{1/2}x\\|_2 ^ 2\\right)\\sigma\\\\ & \\ \\ \\ \\   + \\sigma \\left(6uu^t-2xx^t\\right)\\sigma + ( \\mu_4 - 3)\\sum_{k=1}^n\\left(3(v_k^t u)^2-(v^t_kx)^2\\right)v_kv_k^t . \\end{split}\\ ] ]    for details , see  [ sec : exp_rank1 ] in the appendix .",
    "we then have the following asymptotic convexity result :    [ lem : exp_convexity1 ] let @xmath49 and consider the function @xmath165 $ ] with @xmath159 defined in . under the same assumptions as in lemma [ lem : exp_formula ] above ,",
    "@xmath165 $ ] is convex in the ellipse @xmath166_-}{3+\\tau(x)[\\mu_4 - 3]_+}\\right)\\|\\sigma^{1/2}x\\|_2 \\right\\}\\ ] ] where @xmath167 is the coherence of @xmath168 as in , and above @xmath169_{- } = \\min\\{u,0\\}$ ] and @xmath169_{+ } = \\max\\{u,0\\}$ ] .",
    "the proof of lemma [ lem : exp_convexity1 ] relies on the fact that @xmath170 $ ] is a convex quadratic polynomial in @xmath171 . using this insight , we can actually bound the largest and smallest eigenvalues of the expected hessian whenever we are within a restricted region @xmath172_-}{3+\\tau(x)[\\mu_4 - 3]_+}\\right)\\|\\sigma^{1/2}x\\|_2\\ ] ] for some @xmath173 .",
    "in fact we find that a loose bound is given by    @xmath174\\right)&\\leq \\|\\sigma\\|_{op}\\|\\sigma^{1/2}x\\|_2 ^ 2\\left[\\frac{\\delta^2}{9 } + 6\\delta + 2(3+\\tau(x)[\\mu_4 - 3]_+)\\right]\\\\ \\lambda_{min}\\left(\\mathbb{e}[\\nabla^2f(u)]\\right)&\\geq \\|\\sigma^{-1}\\|_{op}^{-1}\\|\\sigma^{1/2}x\\|_2 ^ 2\\left[-2\\delta(3+\\tau(x)[\\mu_4 - 3]_-)+2(1+\\tau(x)[\\mu_4 - 3]_-)\\right].\\end{aligned}\\ ] ]    _ this result alone provides enough information to prove performance guarantees for stochastic gradient descent after an initialization procedure . via",
    "a union bound and covering argument , this result along with matrix concentration will also guarantee uniform convexity in this region at finite sample size @xmath175 .",
    "however , to ensure uniform convexity at finite sample size @xmath176 , we will need a more refined analysis based on the structure of the hessian matrix , as presented in the next section . _      here we present the sketch of the proof of theorem [ thm : main_convexity1 ] .",
    "for the full proof , we refer the reader to section [ sec : proof_convexity1 ] .    as before , we will use the standard concentration result :    [ lem : concentration1 ] let @xmath49 and @xmath69 be i.i.d .",
    "sub - gaussian , satisfying .",
    "then there exists a constant @xmath177 depending only on the sub - gaussian norm of @xmath12 such that if @xmath178 , then with probability greater than @xmath179 it holds that @xmath180\\right\\|_{op}<\\epsilon\\|\\sigma^{1/2}x\\|_2 ^ 2.\\ ] ]    this result can be proved by first truncating the norms of the measurements vectors and then applying matrix bernstein s inequality ( e.g. , @xcite ) .",
    "the sampling complexity can be improved , but we state lemma [ lem : concentration1 ] as a general proof - of - concept .",
    "for details see  [ sec : proof_concentration1 ] . for @xmath181",
    "sufficiently small , this result indicates that we can control the eigenvalues of @xmath182 for @xmath183 sufficiently close to @xmath32 . in particular ,",
    "if @xmath182 is positive definite in a region around @xmath32 , then @xmath159 is strongly convex and @xmath32 is the unique minimum in this region . it is not immediately clear how to extend such control to a _",
    "quantifiable _ region around @xmath32 .",
    "however , theorem [ thm : main_convexity1 ] requires only that we have a _ lower _ bound on the eigenvalues .    assuming that @xmath184 , the same technique from  [ sec : rankr_convexity ] can be applied : first write @xmath185 for a unit vector @xmath186 and observe that @xmath187 using , where we have defined    @xmath188    note that @xmath189 and consequently using lemma [ lem : concentration1 ] and lemma [ lem : exp_convexity1 ] we can control this term . as before , applying lemma [ lem : one_side_bent ] to the positive term @xmath190 yields the stated conclusion .    for a broad class of sub - gaussian measurements",
    ", we can provide an explicit lower bound on @xmath112\\right),$ ] thereby establishing a quantitative bound on the strong convexity parameter .",
    "[ lem : quant_lb ] suppose that @xmath69 are centered sub - gaussian random vectors with independent coordinates , standard covariance @xmath130=id,$ ] and equal fourth moment parameter @xmath191 $ ]",
    ". then @xmath192\\right)\\geq 2\\left(1+\\min\\{\\tau(x),1/2\\}\\min\\{\\mu_4 - 3 , 0 \\ } \\right)\\|x\\|_2 ^ 2\\ ] ] where @xmath193 is the coherence of @xmath32 .",
    "the proof of this uses the fact that the smallest eigenvalue is a concave function of @xmath194 ; the proof can be found in  [ sec : quant_lb_proof ] .",
    "we can now quantify the lower bound appearing in for a large class of sub - gaussian measurements :    1 .",
    "* bernoulli : * for standard bernoulli measurement vectors , where @xmath195 are i.i.d .",
    "@xmath196 with equal probability , @xmath197 and we have a quantifiable strong convexity guarantee so long as @xmath32 is incoherent , i.e. , @xmath198 . this is sharp in the sense that for @xmath199 the expected hessian has a 0 eigenvalue .",
    "* gaussian : * for vectors @xmath12 with i.i.d .",
    "standard gaussian entries , @xmath200 and lemma [ lem : quant_lb ] provides the uniform lower bound @xmath201 for all @xmath202 .",
    "* sparse gaussian : * note that holds anytime @xmath203 by lemma [ lem : quant_lb ] .",
    "this includes sparse gaussian vectors , whose coordinates are i.i.d .",
    "standard normal with probability @xmath204 and 0 with probability @xmath205 . in this case",
    "@xmath206      we have shown that the function is strongly convex in a quantifiable region around the global minimizers . to guarantee results for gradient descent",
    ", we will also need the following lemma which bounds the lipschitz constant of the gradient of our function .",
    "[ lem : lipschitz ] consider the function @xmath207 .",
    "suppose @xmath208 .",
    "for a universal constant @xmath111 , it holds with probability exceeding @xmath209 that for any @xmath10 within the region of convexity given by , @xmath210 with @xmath211 . here , @xmath212 are the eigenvalues of @xmath45 and @xmath213    by the sub - gaussian assumption , the following holds with probability exceeding @xmath214 : @xmath215 conditioning on this event , recalling that @xmath216 , and recalling the formula for the gradient @xmath217 in , observe the bound @xmath218 thus , @xmath219    it remains to certify a point in this region to initialize gradient descent .",
    "[ thm : init ] suppose we take @xmath220 samples of the form , where @xmath76 and @xmath77 are as in .",
    "define the matrix @xmath83 and the following quantities :    @xmath84    where @xmath85 are the eigenvalues of @xmath86 and @xmath87 are the corresponding normalized eigenvectors .",
    "then with probability at least @xmath221 we have that @xmath222 where @xmath223 is defined as @xmath224 .    for the proof , see  [ sec : init_proofs ] .",
    "initializing from a matrix satisfying guarantees we are close enough so that gradient descent will converge .",
    "we can now prove the main theorem , theorem [ thm : main_theorem ] :    given the number of samples @xmath225 the following events simultaneously occur with the stated probability :    * lemma [ thm : init ] holds , and thus @xmath226 .",
    "* theorem [ thm : main_convexity ] holds , and so considering the taylor expansion of @xmath66 around @xmath10 , the following holds for all @xmath10 satisfying @xmath227 @xmath228 * lemma [ lem : lipschitz ] holds with lipschitz constant @xmath229 .",
    "let @xmath230 and @xmath231",
    ". then @xmath232 \\| u_0 - x o^ { * } \\|_f^2 \\nonumber \\\\ & = [ 1 - 2 \\gamma \\ell + \\gamma^2 b^2 ] d(u_0)\\end{aligned}\\ ] ] and , by induction , @xmath233^k d(u_0)$ ] .",
    "suppose we are in the classical _ phase retrieval _ setting of attempting to recover an unknown vector @xmath234 via gaussian measurements of the form @xmath235 where @xmath236 .",
    "if we write @xmath237 as @xmath238 then a given measurement @xmath239 takes the form    @xmath240    note that we can cast @xmath237 as a matrix @xmath241 via @xmath242 and the measurement @xmath239 becomes @xmath243 where we note @xmath244 .",
    "moreover , the columns of @xmath58 are orthogonal , and the map @xmath245 gives us an isomorphism onto the orientation preserving component of the orthogonal group @xmath246 .",
    "thus this problem is equivalent to recovering an unknown real - valued rank 2 matrix , and the results in the previous sections reproduce known optimality guarantees for gradient descent in the phase retrieval model as found in e.g. , @xcite .",
    "specifically , we have shown the following :    given @xmath247 noiseless samples of the form @xmath248 where @xmath234 is an unknown vector , define @xmath249 and let @xmath250 be the output of algorithm [ algo : spectral_init ] applied to the data @xmath251 with constant step size @xmath252 then with probability at least @xmath253 we have that @xmath254^k\\ ] ] where @xmath255 is the number of iterations of gradient descent and @xmath58 is the matrix .",
    "first , we consider the performance of the algorithm in the rank-1 real - valued setting , where the measurements are @xmath256 .",
    "our numerical studies strongly suggest that the algorithm is stable to noise , that is , given measurements of the form @xmath257 the algorithm successfully returns an matrix @xmath258 up to the noise level @xmath259 .",
    "we consider three different measurement ensembles :    * _ bernoulli _ : @xmath260 are i.i.d .",
    "bernoulli random vectors * _ standard gaussian _ : @xmath12 are i.i.d . drawn from @xmath261 . *",
    "_ gaussian with covariance _ : @xmath12 are i.i.d drawn from @xmath262 with covariance matrix    @xmath263 in a first experiment , we fix an @xmath41-dimensional vector @xmath32 of unit norm with randomly - generated coefficients , and consider noiseless measurements @xmath264 .",
    "we implement the meta - algorithm [ algo : spectral_init ] , calling matlab s built - in function _ fminunc _ to find a stationary point starting from the initialization . in the local optimization procedure , we do not provide any information to fminunc other than the function itself ; by default matlab uses a quasi - newton method for local minimization .",
    "we run this experiment using the three different measurement ensembles above , at problem size @xmath265 and at a number of measurements @xmath266 .",
    "if the solution @xmath258 recovered by the algorithm is within the tolerance @xmath267 , we say the algorithm has succeeded in finding the global solution . in figure",
    "[ fig1 ] , the results of this experiment are displayed , averaged over 100 trials .",
    "next , we analyze numerically the stability of the algorithm to additive measurement noise .",
    "for these experiments , we consider noisy measurements of the form @xmath268 where @xmath269 are i.i.d .",
    "mean - zero uniformly distributed , and normalized such that @xmath270 for @xmath271 ( low signal to noise ratio ) and @xmath272 ( high signal to noise ratio ) .",
    "we observe that the meta - algorithm is robust to such additive noise , with relative reconstruction error @xmath273 averaging below the signal to noise threshold .",
    "we leave a theoretical analysis of this observed noise stability to future work .         at different signal - to - noise levels .",
    "right : @xmath274 and left : @xmath275.,title=\"fig:\",width=302,height=162 ]    at different signal - to - noise levels .",
    "right : @xmath274 and left : @xmath275.,title=\"fig:\",width=309,height=170 ]    finally , we test the performance of algorithm [ algo : spectral_init ] in the more general rank-@xmath0 case .",
    "we consider noiseless measurements @xmath276 where the @xmath12 are i.i.d .",
    "standard gaussian and @xmath277 is a rank-5 matrix with orthogonal columns , normalized so that @xmath278 .",
    "we implement algorithm [ algo : spectral_init ] , calling matlab s built - in function _ fminunc _ to find a stationary point starting from the initialization . in the local optimization procedure , we do not provide any information to fminunc other than the function itself ; by default matlab uses a quasi - newton method for local minimization .",
    "we run this experiment at problem size @xmath279 and @xmath280 .",
    "we declare the algorithm to have converged to global solution if the matrix @xmath281 recovered by the algorithm satisfies @xmath282 where @xmath283 is the singular value decomposition.n in figure [ fig1 ] , the results of the experiment are displayed , averaged over 100 trials .",
    "r. ward and c. white were funded in part by an nsf career grant and an afosr young investigator award .",
    "we would like to thank ju sun for pointing out a mistake in the original rank one proof , and thank mahdi soltanolkotabi and laurent jacques for additional helpful comments and corrections . c. white would like to thank aaron royer and ravi srinivasan for helpful conversations .",
    "abfm14    b.  alexeev , a.  s. bandeira , m.  fickus , and d.  g. mixon .",
    "phase retrieval with polarization .",
    ", 7(1):3566 , 2014 .",
    "r.  balan .",
    "reconstruction of signals from magnitudes of redundant representations .",
    ", 2012 .",
    "r.  balan , b.  g. bodmann , p.  g. casazza , and d.  edidin .",
    "painless reconstruction from magnitudes of frame coefficients .",
    ", 15(4):488501 , 2009 .",
    "r.  balan , p.  casazza , and d.  edidin . on signal reconstruction without phase . , 20(3):345356 , 2006 .",
    "v.  bentkus .",
    "an inequality for tail probabilities of martingales with differences bounded from one side .",
    "16(1):161173 , 2003 .    y.  chen and e.  candes . solving random quadratic systems of equations is nearly as easy as solving linear systems . , 2015 .",
    "y.  chen , y.  chi , and a.  goldsmith .",
    "exact and stable covariance estimation from quadratic sampling via convex programming . , 2013 .",
    "e.  j. candes , y.  c. eldar , t.  strohmer , and v.  voroninski .",
    "phase retrieval via matrix completion .",
    ", 57(2):225251 , 2015 .",
    "e.  j. candes and x.  li . solving quadratic equations via phaselift when there are about as many equations as unknowns .",
    ", 14(5):10171026 , 2014 .",
    "e.  candes , x.  li , and m.  soltanolkotabi .",
    "phase retrieval via wirtinger flow : theory and algorithms . ,",
    "a.  chai , m.  moscoso , and g.  papanicolaou .",
    "array imaging using intensity - only measurements .",
    ", 27(1):015005 , 2011 .",
    "e.  j. candes , t.  strohmer , and v.  voroninski .",
    "phaselift : exact and stable signal recovery from magnitude measurements via convex programming . , 66(8):12411274 , 2013 .",
    "e.  j. candes and t.  tao .",
    "near - optimal signal recovery from random projections : universal encoding strategies ?",
    ", 52(12):54065425 , 2006 .",
    "l.  demanet and p.  hand .",
    "stable optimizationless recovery from phaseless linear measurements .",
    ", 20(1):199221 , 2014 .",
    "d.  l. donoho .",
    "compressed sensing .",
    ", 52(4):12891306 , 2006 .",
    "y.  n. dauphin , r.  pascanu , c.  gulcehre , k.  cho , s.  ganguli , and y.  bengio . identifying and attacking the saddle point problem in high - dimensional non - convex optimization .",
    "in _ advances in neural information processing systems _ , pages 29332941 , 2014 .",
    "g.  dasarathy , p.  shah , b.  n. bhaskar , and r.  nowak .",
    "covariance sketching . in _ communication , control , and computing ( allerton ) , 2012 50th annual allerton conference on _ , pages 10261033 .",
    "ieee , 2012 .    c.  de  sa , k.  olukotun , and c.  r",
    ". global convergence of stochastic gradient descent for some nonconvex matrix problems . , 2014 .",
    "y.  eldar and s.  mendelson .",
    "phase retrieval : stability and recovery guarantees .",
    ", 36:473494 , 2014 .",
    "j.  r. fienup .",
    "phase retrieval algorithms : a comparison .",
    ", 21:27582769 , 1982 .",
    "m.  fickus and d.  mixon .",
    "projection retrieval : theory and algorithms .",
    ", 2015 .    r.  w. gerchberg .",
    "a practical algorithm for the determination of phase from image and diffraction plane pictures .",
    ", 35 , 1972 .",
    "f.  krahmer and y .- k .",
    "phase retrieval without small - ball probability assumptions : stability and uniqueness . ,",
    "r.  kueng , h.  rauhut , and u.  terstiege .",
    "low rank matrix recovery from rank one measurements . , 2014 .",
    "p.  netrapalli , p.  jain , and s.  sanghavi .",
    "phase retrieval using alternating minimization . in _ advances in neural information processing systems _ , pages 27962804 , 2013 .",
    "m.  raymer , m.  beck , and d.  mcalister .",
    "complex wave - field reconstruction using phase - space tomography .",
    ", 72(8):1137 , 1994 .",
    "b.  recht , m.  fazel , and p.  a. parrilo .",
    "guaranteed minimum - rank solutions of linear matrix equations via nuclear norm minimization . , 52(3):471501 , 2010 .",
    "p.  h. schnemann . a generalized solution of the orthogonal procrustes problem . ,",
    "31(1):110 , 1966 .",
    "m.  soltanolkotabi . .",
    "phd thesis , stanford university , 2014 .",
    "j.  sun , q.  qu , and j.  wright .",
    "complete dictionary recovery over the sphere . , 2015 .",
    "l.  tian , j.  lee , s.  b. oh , and g.  barbastathis . experimental compressive phase space tomography .",
    ", 20(8):82968308 , 2012 .",
    "j.  a. tropp .",
    "user - friendly tail bounds for sums of random matrices . , 12(4):389434 , 2012 .",
    "i.  waldspurger , a.  daspremont , and s.  mallat .",
    "phase recovery , maxcut and complex semidefinite programming .",
    ", 149(1 - 2):4781 , 2015 .",
    "y.  yu , t.  wang , and r.  j. samworth . a useful variant of the davis  kahan theorem for statisticians .",
    ", 102(2):315323 , 2015 .",
    "d.  zhang and l.  balzano",
    ". global convergence of a grassmannian gradient descent algorithm for subspace estimation . , 2015 .",
    "the proofs of and use standard vector calculus . for , we use the fact that for any vector @xmath49 @xmath284 = \\|x\\|_2 ^ 2id + 2xx^t\\ ] ] which can be seen by writing out the entries of the matrix individually .",
    "this implies @xmath285 & = \\sum_{i=1}^r\\mathbb{e}[(a^tx_i)^2aa^t]\\\\ & = \\sum_{i=1}^r \\left(\\|x_i\\|_2 ^ 2id + 2x_ix_i^t\\right ) \\end{split}\\ ] ] which , combined with @xmath286 = x_ix_j^t + x_jx_i^t+ x_i^tx_jid\\ ] ] yields the stated result .",
    "[ theorem : tensor_concentration ] let @xmath67 be a given matrix with orthogonal columns ; suppose @xmath287 where @xmath137 and @xmath138 are given constants and @xmath288__rank__@xmath140 . then we have that with probability greater than @xmath141 @xmath289\\right\\|_{\\big|_{x\\otimes\\mathbb{r}^n } } < \\delta\\ ] ] where @xmath290 is the operator norm of the matrix restricted to the subspace spanned by the columns of @xmath4 tensored with @xmath291 .",
    "let @xmath292 be an arbitrary unit vector .",
    "write @xmath293 , where @xmath294 and @xmath295",
    ". we must consider the quantity @xmath296 * first term .",
    "* note that we have a product of independent subexponential random variables , and so if we condition on the bounds @xmath297 both of which happen with probability at least @xmath298 , we find via bernstein that @xmath299\\leq 2\\exp\\big(-c\\min\\{\\frac{m\\delta^2}{108},\\frac{m\\delta}{54\\log m}\\}\\big)\\ ] ] which we can make smaller than @xmath300 so long as @xmath301 and we conclude via an @xmath181-net argument that @xmath302\\leq e^{-\\beta rn } + 2/m^2\\ ] ] where @xmath303 is the orthogonal projection onto the complement of @xmath4 in @xmath291 .   +   + * third term . *",
    "we recognize @xmath304 as a sub - gaussian random variable , and thus if we condition on @xmath305 we find via hoeffding that @xmath306\\leq\\exp\\big(1-\\frac{c\\delta^2m}{16}\\big).\\ ] ] we can make this bound smaller than @xmath300 so long as @xmath307 . as happens with probability greater than @xmath298",
    "we find @xmath308 \\leq e^{-\\beta rn } + 1/m^2.\\ ] ]   +   + * second term .",
    "* for this term we further decompose @xmath309 into its @xmath32-component and its @xmath310-component .",
    "we can apply the same analysis for the first and third terms to the @xmath310 terms , and have only to deal with @xmath311.\\ ] ] if we condition on @xmath312 and note that @xmath313\\big| \\leq 1/m^2\\ ] ] we find via hoeffding that @xmath314\\big|>\\delta/3]\\leq\\exp\\big(\\frac{-\\delta^2m^2}{cm + 81(\\log m)^2\\delta m}\\big)\\ ] ] which we can make smaller than @xmath315 so long as @xmath316 . moreover ,",
    "note that we can also make smaller than @xmath317 for @xmath318 .",
    "we conclude that @xmath319\\big\\|_{\\big|_{x\\otimes x}}>\\delta/3]\\leq 3\\min\\{e^{-\\beta r},1/m^2\\}+3/m^2.\\ ] ] combining , , and yields the stated result .",
    "[ cor : full_concentration ] suppose we collect @xmath320 samples of the form @xmath136 , where @xmath137 and @xmath138 are given constants and @xmath139 _ _ rank__@xmath140 ; then we have that with probability greater than @xmath141 @xmath321      [ cor : hessian_concentration ] suppose we collect @xmath320 samples of the form @xmath136 , where @xmath137 and @xmath138 are given constants and @xmath139 _ _ rank__@xmath140 ; then we have that with probability greater than @xmath141 @xmath142\\right\\|_{op } < 2\\delta\\|x\\|_{op}^2.\\ ] ]          [ lem : candes]suppose @xmath146 are i.i.d .",
    "real - valued random variables obeying @xmath147 for some nonrandom @xmath148 , @xmath149 = 0 $ ] , and @xmath150 = v^2 $ ] . setting @xmath151 , @xmath152\\leq \\min\\left\\{\\exp\\left(-\\frac{y^2}{\\sigma^2}\\right),c_0(1-\\phi(y/\\sigma))\\right\\}\\ ] ] where one can take @xmath153 and @xmath154 is the cdf for the standard normal .        consider the single - variable function @xmath332 which can be written @xmath333 it is straightforward to verify that @xmath334 which is a convex polynomial in @xmath171 ; observe that @xmath335 . if the linear term is positive , then clearly is positive for all @xmath171 and we have nothing to show ( the smallest eigenvalue is bounded below by @xmath336 in the direction @xmath337 ) .",
    "define the following quantities :      and note that can be written as @xmath339 consider the random variable @xmath340 observe that @xmath341 is a chi - squared random variable with 1 degree of freedom by the normalization @xmath342 .",
    "thus we have      by the definition of @xmath344 , we have @xmath345\\text{vec}(\\hat{w})\\ ] ] and so @xmath346 = \\frac{1}{2}\\text{vec}(\\hat{w})^t\\mathbb{e}\\left[\\nabla^2f(x)\\right]\\text{vec}(\\hat{w}).\\ ] ] moreover , @xmath347 & = \\mathbb{e}\\left[\\left(\\sum_{k=1}^r(a_i^tw_k)^2\\right)\\left(\\sum_{k=1}^r(a_i^tx_k)(a_i^tw_k)\\right)\\right]\\\\ & = \\mathbb{e}\\left[\\sum_{k , q=1}^r(a_i^tw_k)^2(a_i^tx_q)(a_i^tw_q)\\right]\\\\ & = \\sum_{k , q=1}^r 2(w_k^tx_q)(w_k^tw_q ) + \\|w_k\\|_2 ^ 2x_q^tw_q\\\\ & = 2\\sum_{k , q=1}^r x_q^tw_kw_k^tw_q+ \\sum_{q=1}^rx_q^tw_q\\\\ & = 2\\sum_{q=1}^rx_q^t\\hat{w}\\hat{w}^tw_q+ \\text{tr}(x^t\\hat{w})\\\\ & = 2\\text{tr}(x^t\\hat{w}\\hat{w}^t\\hat{w})+ \\text{tr}(x^t\\hat{w})\\\\ \\end{split}\\ ] ] we then have that the mean @xmath348 $ ] is given by @xmath349\\text{vec}(\\hat{w}).\\ ] ] next we consider the variance of @xmath350 : @xmath351 & \\leq \\mathbb{e}[z^2_i(t)]\\\\ & = \\mathbb{e}\\left[a_i^4\\right]t^4 + 4\\mathbb{e}\\left[a_i^3b_i\\right]t^3 + 6\\mathbb{e}\\left[a_i^2b^2_i\\right]t^2 + 4\\mathbb{e}\\left[a_ib^3_i\\right]t + \\mathbb{e}\\left[b_i^4\\right]\\\\ & = 105t^4 + 4\\mathbb{e}\\left[a_i^3\\langle x^ta_i,\\hat{w}^ta_i\\rangle\\right]t^3\\\\ & \\ \\ \\ \\   + 6\\mathbb{e}\\left[a_i^2\\langle x^ta_i,\\hat{w}^ta_i\\rangle^2\\right]t^2 + 4\\mathbb{e}\\left[a_i\\langle x^ta_i,\\hat{w}^ta_i\\rangle^3\\right]t + \\mathbb{e}\\left[\\langle x^ta_i,\\hat{w}^ta_i\\rangle^4\\right]\\\\ & \\leq105t^4 + 4\\sqrt{\\mathbb{e}[a_i^6]\\mathbb{e}\\left[\\langle x^ta_i,\\hat{w}^ta_i\\rangle^2\\right]}t^3\\\\ & \\ \\ \\ \\   + 6\\mathbb{e}[a_i^3\\|x^ta_i\\|_2 ^ 2]t^2 + 4\\sqrt{\\mathbb{e}[a_i^2]\\mathbb{e}[\\langle x^ta_i,\\hat{w}^ta_i\\rangle^6]}t",
    "+ \\mathbb{e}\\left[\\|x^ta_i\\|_2 ^ 4\\|\\hat{w}^ta_i\\|_2 ^ 4\\right ] \\end{split}\\ ] ] where we used either cauchy - schwarz or hlder s inequality on each term .",
    "proceeding with applications of hlder and cauchy - schwarz , recognizing that @xmath352 is also chi - squared with one degree of freedom under the assumption that @xmath329 , and noting from that @xmath353\\leq 3 $ ] we have @xmath354}t^3\\\\ & \\ \\ \\ \\   + 6\\mathbb{e}[a_i^3\\|x^ta_i\\|_2 ^ 2]t^2 + 4\\sqrt{3\\cdot\\mathbb{e}[\\langle x^ta_i,\\hat{w}^ta_i\\rangle^6]}t",
    "+ \\mathbb{e}\\left[\\|x^ta_i\\|_2 ^ 4\\|\\hat{w}^ta_i\\|_2 ^ 4\\right]\\\\ & \\leq 105t^4 + 707t^3 + 6\\sqrt{\\mathbb{e}[a_i^6\\|x^ta_i\\|_2 ^ 4]}t^2 + 4\\sqrt{3\\cdot\\mathbb{e}[\\|x^ta_i\\|_2 ^ 6\\|\\hat{w}^ta_i\\|_2 ^ 6]}t + \\sqrt{\\mathbb{e}\\left[\\|x^ta_i\\|_2 ^ 8]\\mathbb{e}[\\|\\hat{w}^ta_i\\|_2 ^ 8\\right]}\\\\ & \\leq 105t^4 + 707t^3 + 6\\left(\\mathbb{e}[a_i^{12}]\\mathbb{e}[\\|x^ta_i\\|_2 ^ 8]\\right)^{1/4}t^2 + 4\\sqrt{3}\\left(\\mathbb{e}[\\|x^ta_i\\|_2^{12}]\\mathbb{e}[\\|\\hat{w}^ta_i\\|_2^{12}]\\right)^{1/4}t + 105\\\\ & \\leq 105t^4 + 707t^3 + 7094t^2 + 707 t + 105\\\\ & = c(t)^2 .",
    "\\end{split}\\ ] ] observe that the mean can be bounded as @xmath355 now define      applying lemma [ lem : candes ] above yields @xmath357\\leq \\min\\left\\{\\exp\\left(-\\frac{\\lambda_r^2m}{144c(t)^2}\\right),25\\left(1-\\phi(\\frac{\\lambda_r\\sqrt{m}}{12c(t)})\\right)\\right\\}.\\ ] ] using the well - known bound @xmath358 we find that if @xmath359 then with probability at least @xmath360 we have @xmath361\\text{vec}(\\hat{w})-\\frac{1}{2}\\text{vec}(\\hat{w})^t\\nabla^2f(x)\\text{vec}(\\hat{w } ) \\end{split}\\ ] ] where we used to lower bound @xmath362 by @xmath363\\text{vec}(\\hat{w})\\ ] ] and the fact that @xmath364 moreover , an @xmath181-net argument over all directions @xmath337 shows that holds for an arbitrary @xmath337 with probability at least @xmath365 .",
    "further observe that our condition on @xmath2 guarantees @xmath142\\right\\|_{op } < \\frac{\\lambda_r}{4}\\ ] ] with probability at least @xmath141 by corollary [ cor : hessian_concentration ] .",
    "this implies that @xmath366\\text{vec}(\\hat{w})-\\frac{1}{2}\\text{vec}(\\hat{w})^t\\nabla^2f(x)\\text{vec}(\\hat{w})>\\frac{15}{8}\\lambda_r\\ ] ] so that @xmath367 with probability at least @xmath368 for any direction @xmath337 .",
    "thus by a tangent line bound we find that the smallest positive root of @xmath344 is bounded below by @xmath369 and we note that            now , @xmath376 is a chi - squared random variable with one degree of freedom ; consequently we find @xmath377 \\leq \\frac{m}{\\sqrt{cnr}}e^{-cnr } \\leq e^{-\\tilde{c}nr}\\ ] ] as long as @xmath378 .",
    "consequently an @xmath181-net argument shows us that for _ any _ @xmath379 we have @xmath380 with probability greater than @xmath365 .",
    "it suffices to prove the case @xmath391 . by corollary [ cor : full_concentration ] we have that with probability greater than @xmath141 @xmath392 so long as @xmath320 .",
    "this implies @xmath393 let @xmath394 and collect the unit normalized eigenvectors corresponding to the dominant @xmath0-dimensional subspace of @xmath132 in a matrix @xmath72 .",
    "let @xmath395 denote the eigenvalues of the observed matrix @xmath86 and define @xmath396 let @xmath397 , and @xmath398 where @xmath399 is the singular value decomposition and observe                      note that by we only need to compute @xmath405 $ ] for an arbitrary @xmath109 .",
    "we will consider the slightly more general expectation @xmath406 $ ] for arbitrary @xmath407 .",
    "begin by assuming @xmath408 where @xmath409 is the @xmath410 identity matrix .",
    "let @xmath411 $ ] be arbitrary coordinates .",
    "we have @xmath412 & = \\mathbb{e}[a_i^2\\sum_{k=1}^na_k^2u_kw_k + a_i^2\\sum_{k\\neq j}a_ka_ju_kw_j]\\\\ & = \\mu_4u_iw_i + \\sum_{k\\neq i}u_kw_k\\\\ & = ( \\mu_4 - 1)u_iw_i + u^tw \\end{split}\\ ] ] and for @xmath413 , @xmath414 & = \\mathbb{e}[a_ia_j\\sum_{k=1}^na_k^2u_kw_k + a_ia_j\\sum_{k\\neq l}a_ka_lu_kw_l]\\\\ & = u_iw_j + w_ju_i \\end{split}\\ ] ] so that @xmath415 = ( u^tw)id + uw^t + wu^t+ ( \\mu_4 - 3)\\sum_{k=1}^nu_kw_ke_ke_k^t.\\ ] ] if @xmath416 , then observe that if we define @xmath417 , @xmath418 then the inner term satisfies the assumptions needs for , and so we find @xmath419 & = \\sigma^{1/2}\\left(\\|\\sigma^{1/2}u\\|_2 ^ 2id + 2\\sigma^{1/2}uu^t\\sigma^{1/2 } + ( \\mu_4 - 3)\\sum_{k=1}^n(\\sigma^{1/2}u)_k^2e_ke_k^t\\right)\\sigma^{1/2}\\\\ & = \\|\\sigma^{1/2}u\\|_2 ^ 2\\sigma + 2\\sigma uu^t\\sigma + ( \\mu_4 - 3)\\sum_{k=1}^n(v_k^tu)^2v_kv_k^t \\end{split}\\ ] ] where @xmath420 .            1 .   if _ any _",
    "@xmath426 then @xmath427 is an eigenvalue .",
    "if all of the squared coordinates are distinct then each eigenvalue @xmath428 satisfies @xmath429 and because there will be a vertical asymptote at each @xmath430 we see the eigenvalues of @xmath58 interlace the squared coordinates , the smallest occurring somewhere between @xmath431 and the largest somewhere after @xmath432",
    ".                  begin by assuming @xmath184 and reparametrize an arbitrary @xmath109 as @xmath449 for @xmath450 .",
    "note that @xmath451 so using we find @xmath452 & = 3\\left[id + 2ww^t+(\\mu_4 - 3)\\sum_{k=1}^nw_k^2e_ke_k^t\\right]t^2\\\\ & \\ \\ \\ \\ -6\\left[(x^tw)id + xw^t + wx^t+ ( \\mu_4 - 3)\\sum_{k=1}^nx_kw_ke_ke_k^t\\right]t \\\\ & \\ \\ \\ \\ + 2\\left[\\|x\\|_2 ^ 2id + 2xx^t+(\\mu_4 - 3)\\sum_{k=1}^nx_k^2e_ke_k^t\\right ] . \\end{split}\\ ] ] observe that @xmath453_-\\right)\\|x\\|_2 ^ 2id \\end{split}\\ ] ] and that @xmath454_+\\right)\\|x\\|_2id.\\ ] ] for , we used lemma [ lem : quant_lb ] .",
    "lastly , note that @xmath455    consequently we can define the polynomials @xmath456y\\ ] ] and by convexity we can bound the smallest positive root by the intercept of the tangent line ; the bounds and thus yield the stated conclusion for @xmath184",
    ".        begin by assuming @xmath184 and @xmath438 .",
    "note that because of the sub - gaussian assumption we have that for @xmath460 @xmath461&\\leq \\exp\\left(1-\\hat{c}\\log m\\right)\\\\ & \\leq m^{-4}\\\\ \\mathbb{p}\\left[\\|a_i\\|_2 ^ 2\\geq cn\\log m\\right]&\\leq 2\\exp\\left(-\\hat{c}\\log m\\right)\\\\ & \\leq m^{-4 } \\end{split}\\ ] ] where the constants depend on the sub - gaussian norm of @xmath12 .",
    "consequently @xmath462\\leq 2m(m^{-4 } )",
    "\\leq 2 m^{-3}\\ ] ] and if we define the truncated random variables @xmath463 and the analogous truncated matrix @xmath464 bernstein s inequality ( theorem 4.1 in @xcite ) tells us that @xmath465\\right\\|_{op}\\geq \\delta\\right]\\leq n\\exp\\left(\\frac{-\\delta^2m^2/2}{\\sigma^2+mn(\\log m)^2\\delta/3}\\right)\\ ] ] where @xmath466 is given by @xmath467-\\left(\\mathbb{e}\\left[(\\tilde{a}_i^tx)^2\\tilde{a}_i\\tilde{a}_i^t\\right]\\right)^2\\right\\|_{op}\\\\ & = m\\left\\|\\mathbb{e}\\left[(\\tilde{a}^tx)^4\\|\\tilde{a}\\|_2 ^ 2\\tilde{a}\\tilde{a}^t\\right]-\\left(\\mathbb{e}\\left[(\\tilde{a}^tx)^2\\tilde{a}\\tilde{a}^t\\right]\\right)^2\\right\\|_{op}\\\\ & \\leq cmn \\end{split}\\ ] ] where @xmath177 is a constant which depends on the moments of @xmath12 .",
    "lastly observe that if @xmath468 then we can write @xmath469-\\mathbb{e}[m]\\right\\|_{op } & = \\left\\|\\mathbb{e}\\left[(\\hat{a}^tx)^2\\hat{a}\\hat{a}^t\\right]\\right\\|_{op}\\\\ & \\leq \\mathbb{e}\\left[(\\hat{a}^tx)^2\\|\\hat{a}\\|_2 ^ 2\\right]\\\\ & = \\int_0^{c^2n(\\log m)^2}\\mathbb{p}\\left[(a^tx)^2\\|a\\|_2 ^ 2\\geq c^2n(\\log m)^2\\right]\\ dt + \\int_{c^2n(\\log m)^2}^\\infty\\mathbb{p}\\left[(a^tx)^2\\|a\\|_2 ^ 2\\geq t\\right]\\ dt\\\\ & = c^2n(\\log m)^2\\left(\\mathbb{p}\\left[(a^tx)^2\\|a\\|_2 ^ 2\\geq c^2n(\\log m)^2\\right]+\\int_1^\\infty\\mathbb{p}\\left[(a^tx)^2\\|a\\|_2 ^ 2\\geq \\alpha c^2n(\\log m)^2\\right]\\ d\\alpha\\right)\\\\ & \\leq 3/m^2 \\end{split}\\ ] ] where we used jensen s inequality for the first line and have assumed @xmath36 .",
    "consequently we find that for @xmath36 @xmath470\\right\\|_{op}\\geq \\epsilon\\right]&\\leq \\mathbb{p}\\left[m\\neq \\tilde{m}\\right]+\\mathbb{p}\\left[\\left\\|\\tilde{m}-\\mathbb{e}[m]\\right\\|_{op}\\geq \\epsilon\\right]\\\\ & \\leq m(2m^{-4 } ) + n\\exp\\left(\\frac{-\\delta^2m^2/2}{\\sigma^2+mn(\\log m)^2\\delta/3}\\right ) \\end{split}\\ ] ] where @xmath471 from .",
    "now , all we have left is to show that the exponential can be made less than a power of @xmath2 .",
    "using we find that we need @xmath2 to satisfy @xmath472 for which it suffices to require @xmath473 for some constant @xmath177 which only depends on the moments of @xmath12 .",
    "for the more general statement note that our previous work shows @xmath474\\right\\|_{op}<\\epsilon\\|\\sigma\\|_{op}^{-1}\\|\\sigma^{1/2}x\\|_2 ^ 2\\ ] ] whenever @xmath475 .",
    "consequently , @xmath476\\right\\|_{op } & = \\left\\|\\sigma^{1/2}\\left(\\frac{1}{m}\\sum_{i=1}^m(b_i^t\\sigma^{1/2}x)^2b_ib_i^t-\\mathbb{e}\\left[(b^t\\sigma^{1/2}x)^2bb^t\\right]\\right)\\sigma^{1/2}\\right\\|_{op}\\\\ & \\leq \\|\\sigma\\|_{op}\\left\\|\\frac{1}{m}\\sum_{i=1}^m(b_i^t\\sigma^{1/2}x)^2b_ib_i^t-\\mathbb{e}\\left[(b^t\\sigma^{1/2}x)^2bb^t\\right]\\right\\|_{op}\\\\ & < \\epsilon\\|\\sigma^{1/2}x\\|_2 ^ 2 \\end{split}\\ ] ] which is the desired claim .",
    "assume without loss that @xmath438 and that @xmath477 is the normalized direction from @xmath183 to @xmath32 .",
    "moreover begin by assuming @xmath408 . closely following the proof of theorem [ thm : main_convexity ]",
    "we first note that @xmath478 and define @xmath479 where      note that by the computations done in lemma [ lem : exp_formula ] we have @xmath481\\\\ & = \\left(3+(\\mu_4 - 3)\\|\\hat{w}\\|_4 ^ 4\\right)t^2 + 2\\left(3x^t\\hat{w}+(\\mu_4 - 3)\\sum_k x_k\\hat{w}_k^3\\right)t + \\frac{1}{2}\\hat{w}^t\\mathbb{e}[\\nabla^2f(x)]\\hat{w } \\\\ & \\leq \\left(3+[\\mu_4 - 3]_+\\right)t^2 + \\left(6 + 2[\\mu_4 - 3]_+\\right)t + \\frac{3 + 2[\\mu_4 - 3]_+}{2 } \\end{split}\\ ] ] and that the variance @xmath482 \\leq c^2(t)\\ ] ] where @xmath483 depends only on the subgaussian norm of the @xmath12 .",
    "@xmath485_+\\right)t^2 + \\left(6 + 2[\\mu_4 - 3]_+\\right)t + \\frac{3 + 2[\\mu_4 - 3]_+}{2}\\\\ v^2(t ) & : = c(t)^2\\\\ \\sigma^2(t ) & : = mc(t)^2\\\\ y & : = m\\lambda_{min}(\\mathbb{e}[\\nabla^2f(x)])/12 = m\\lambda/12.\\end{aligned}\\ ] ]    applying lemma [ lem : candes ] above yields @xmath486\\leq \\min\\left\\{\\exp\\left(-\\frac{\\lambda^2m}{144c(t)^2}\\right),25\\left(1-\\phi(\\frac{\\lambda\\sqrt{m}}{12c(t)})\\right)\\right\\}.\\ ] ] using the well - known bound @xmath487 we find that if @xmath488 then with probability at least @xmath489 we have @xmath490_-)t^2 + ( 6[\\mu_4 - 3]_- -3)t + \\hat{w}^t\\mathbb{e}\\left[\\nabla^2f(x)\\right]\\hat{w } + \\frac{1}{2}\\hat{w}^t\\mathbb{e}\\left[\\nabla^2f(x)\\right]\\hat{w}-\\lambda/4 - \\frac{1}{2}\\hat{w}^t\\nabla^2f(x)\\hat{w}\\\\ & \\geq(9 + 3[\\mu_4 - 3]_-)t^2 + ( 6[\\mu_4 - 3]_- - 3)t + \\hat{w}^t\\mathbb{e}\\left[\\nabla^2f(x)\\right]\\hat{w } -\\lambda/4 - \\epsilon/2\\\\ & \\geq(9 + 3[\\mu_4 - 3]_-)t^2 + ( 6[\\mu_4 - 3]_- -3)t + \\lambda/2 \\end{split}\\ ] ] where we used the concentration guaranteed by lemma [ lem : concentration1 ] above with @xmath491 and the fact that @xmath492 consequently , using a tangent line bound for the smallest positive root we find that for all @xmath493_-}\\ ] ] we have @xmath494"
  ],
  "abstract_text": [
    "<S> this paper considers the recovery of a rank @xmath0 positive semidefinite matrix @xmath1 from @xmath2 scalar measurements of the form @xmath3 ( i.e. , quadratic measurements of @xmath4 ) . </S>",
    "<S> such problems arise in a variety of applications , including covariance sketching of high - dimensional data streams , quadratic regression , quantum state tomography , among others . </S>",
    "<S> a natural approach to this problem is to minimize the loss function @xmath5 which has an entire manifold of solutions given by @xmath6 where @xmath7 is the orthogonal group of @xmath8 orthogonal matrices ; this is _ non - convex _ in the @xmath9 matrix @xmath10 , but methods like gradient descent are simple and easy to implement ( as compared to semidefinite relaxation approaches ) .    in this paper </S>",
    "<S> we show that once we have @xmath11 samples from isotropic gaussian @xmath12 , with high probability _ ( a ) _ this function admits a dimension - independent region of _ local strong convexity _ on lines perpendicular to the solution manifold , and _ ( b ) _ with an additional polynomial factor of @xmath0 samples , a simple spectral initialization will land within the region of convexity with high probability . together </S>",
    "<S> , this implies that gradient descent with initialization ( but no re - sampling ) will converge linearly to the correct @xmath4 , up to an orthogonal transformation . </S>",
    "<S> we believe that this general technique ( local convexity reachable by spectral initialization ) should prove applicable to a broader class of nonconvex optimization problems . </S>"
  ]
}