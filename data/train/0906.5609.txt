{
  "article_text": [
    "in bayesian model selection , we have two or more competing hypotheses , @xmath0 and @xmath1 , with each possibly containing different parameters @xmath2 and @xmath3 .",
    "we wish to judge the plausibility of these two hypotheses in the light of some data @xmath4 , and some prior information @xmath5 , dropped hereafter for succinctness .",
    "bayes rule provides the means to update our plausibilities of these two models , to take into account the data @xmath4 : @xmath6 thus , the ratio of the posterior probabilities for the two models is the prior odds ratio times the evidence ratio .",
    "if the various probabilities on the right - hand side of equation  [ bayes ] are a good description of our prior beliefs , then the posterior probabilities will encode justified conclusions based on the data .",
    "however , practical use of equation  [ bayes ] is often regarded with scepticism @xcite .",
    "this is primarily because the probabilities on the right - hand side are difficult to specify without making ad hoc choices .    for reasons that are mostly historical , the prior distributions @xmath7 and @xmath8 for the parameters of each model are usually considered the most troubling .",
    "the prior model probabilities are often set to @xmath9 , citing symmetry , and the sampling distributions are usually considered uncontroversial . however , in many real scientific applications , assigning priors is trivial compared to the job of assigning sampling distributions ( hogg , priv comm ) ; i.e. modelling how the question of interest would affect our data .",
    "while many bayesians would assert that the dependence on subjective judgments exists because the result should actually depend on these judgments , it seems as though there ought to be ways to reduce the subjective influences in the prior probabilities and sampling distributions , even if they can never be entirely eliminated .",
    "in fact , this is the entire reason for using bayes rule in the first place @xcite . rather than simply looking at the data and then assigning a posterior distribution directly , we make use of one objective thing we actually know , bayes rule . in this paper",
    ", we discuss how the principle of maximum relative entropy ( me ) @xcite can be used to further reduce , though not eliminate , the subjectivity of bayesian inferences .",
    "the key requirement of this approach is that we must have a realistic probabilistic model of our prior beliefs about the data , i.e. our prior predictive distribution for the data must be modelled carefully .",
    "@xcite recommends that whenever some data is analysed using a model @xmath10 , the evidence @xmath11 be presented .",
    "this way , anyone proposing a different model @xmath12 can calculate their own evidence @xmath13 and carry out model comparison with equation  [ bayes ] without the need to recalculate @xmath14 , which was published by the first author .",
    "this is good advice that has been taken by many in the astronomical community @xcite , however , it is not the whole story .",
    "the plausibility of a model does not depend only on the evidence , it also depends on the prior probability ( equation  [ bayes ] ) .",
    "a large evidence ratio can easily be cancelled by a tiny prior probability ratio and vice versa .",
    "the sure thing problem , discussed in section  [ surething ] , is simple and well - known example of this fact .",
    "suppose a simple lottery is held , with tickets numbered from 1 to 1,000,000 .",
    "each ticket is sold to a different person . consider a hypothesis @xmath0 , which states that the lottery is fair , and thus the probability of any particular ticket winning is @xmath15 .",
    "the draw is carried out , producing the following data @xmath4 : the winner of the lottery was ticket # @xmath16 .",
    "alice publishes a paper that reports this data , and proposes the fair lottery model @xmath0 to explain it .",
    "she presents the evidence @xmath17 .",
    "bob , a professional rival of alice , reads her paper and proposes a different model , @xmath1 : the lottery was not fair .",
    "it was rigged in order to make ticket # @xmath16 the winner .",
    "bob writes a paper presenting the evidence @xmath18 .",
    "thus , he concludes , if @xmath0 and @xmath1 are initially equally plausible , the data makes @xmath1 a million times more plausible than @xmath0 . clearly , something is not quite right with this conclusion .",
    "@xcite resolves the sure thing paradox in the following way .",
    "when bob does a model selection between @xmath0 and @xmath1 with @xmath19 , he is implicitly stating that before getting the data , he would have predicted ticket # @xmath16 with a probability greater than 50 % .",
    "clearly , there is no way he could have known this before seeing the data .",
    "actually , before observing the data , there were 999,999 other `` sure thing '' hypotheses that were on an equal footing with @xmath1 .",
    "the correct analysis would involve a bigger hypothesis space containing 1,000,001 hypotheses : @xmath0 , and the 1,000,000 sure thing hypotheses @xmath20 , where @xmath21 .",
    "bob should have assigned 1/2 of the prior probability to @xmath0 and divided the other 1/2 evenly amongst the @xmath22 s .",
    "then , the prior probability of @xmath1 is @xmath23 and its posterior probability is @xmath9 .",
    "this is the correct result ; knowledge of the winning ticket number does not affect the plausibility of foul play .",
    "this argument resolves the sure thing problem by introducing a large number of alternatives into the hypothesis space , thus drastically reducing the prior probability of the particular sure thing hypothesis selected by the data .",
    "however , it is difficult to generalise this reasoning into more complicated scenarios where the principle of indifference can not be used .    before the lottery was drawn",
    ", bob would have assigned a uniform predictive distribution for the data .",
    "his reanalysis ought to reflect this , if not by introducing extra sure thing models , then by downweighting @xmath1 somehow to reduce the spike it produces in the predictive distribution .",
    "while this is not the explicit motivation for entropic priors , it is a pleasant side effect , as we will show in the next section .",
    "in this section we introduce the notion of an entropic prior @xcite .",
    "usually , bayesian inference is concerned with describing our knowledge in two stages : before taking into account the data , and then after taking into account the data .",
    "bayes rule is used to do this updating . before taking into account the data",
    ", there is a prior distribution @xmath24 and sampling distributions @xmath25 for all @xmath4 and @xmath26 .",
    "the reason for the subscript ` 1 ' will become clear later . by the product rule , this is equivalent to defining a _ joint prior _ on the product space of possible hypotheses and possible data : @xmath27 here , the usual prior @xmath24 ( actually a marginal distribution ) describes prior knowledge about @xmath26 , and the sampling distributions @xmath25 describe prior knowledge about how @xmath26 is related to the data @xmath4 that we plan to observe .",
    "the key point here is that before learning the data , we are uncertain both about the parameters _ and _ about the data : @xmath28 should model this state of uncertainty .    in this paper , we will be concerned with describing uncertain knowledge about @xmath29 , so we will be using probability distributions on the product space .",
    "we will start from a joint prior @xmath30 and update this distribution _",
    "twice _ to obtain the final joint posterior . we thus describe knowledge at three stages , defined below .",
    "* stage 0 : before we observe the data , or even know what sampling distributions are .",
    "however , the parameter space and the data space have been defined , as well as priors over these spaces . at stage 0 ,",
    "our knowledge is @xmath31 .",
    "* stage 1 : also before we observe the data .",
    "however , we have now specified the sampling distributions @xmath32 for all @xmath26 and @xmath4 . at stage 1 , our knowledge is @xmath28 .",
    "* stage 2 : we now have the data .",
    "our knowledge is @xmath33 .",
    "updating from stage 1 to stage 2 is what we typically think of as bayesian analysis .",
    "we prefer updating , rather than just writing down stage 2 probabilities , because we get to use an objective updating rule , bayes rule .",
    "the idea behind entropic priors is to split up the process of assigning stage 1 probabilities into two steps : assigning stage 0 probabilities , and then updating to stage 1 using another objective updating rule , me @xcite .",
    "there is a lot of confusion in the literature about the relationship between these two principles .",
    "however , there need not be any tension between them if it is understood that bayes rule is to be used when we learn about propositions built from those in the product space , such as ` @xmath34 ' or ` @xmath35 ' , whereas me applies to propositions about _ probability distributions on that space _",
    ", such as ` @xmath36 should be a gaussian ' .",
    "say we have a stage 0 joint prior , and we do nt know the sampling distributions yet .",
    "perhaps we havent calibrated the instruments to see what kinds of output they typically produce . at this point",
    "our knowledge of @xmath29 will often be independent , such that taking data before learning about the experiment does not tell us anything _ about the parameters _ ( it does tell us the data - and is therefore significant information in the product space .",
    "however , data is usually a nuisance parameter [ ! ] ) . however , for generality we will allow dependence in the stage 0 distribution : @xmath37 .",
    "we then learn information in the form of a _ constraint on allowable joint probability distributions _ :",
    "the sampling distributions @xmath32 for all @xmath26 and @xmath4 are given to us : @xmath38 , where @xmath39 is a given function .",
    "we must adjust our joint distribution so that this constraint is satisfied . by the rules of probability any distribution of the form @xmath40",
    "is allowed , and we have absolute freedom to vary @xmath24 while still satisfying the constraint on the sampling distributions .",
    "however , there is a best choice for @xmath24 @xcite : @xmath24 should be chosen such that @xmath28 is as close as possible to @xmath30 , i.e. we choose the @xmath24 that maximises the relative entropy @xmath41 differentiating with respect to each value of @xmath24 ( i.e. its value at each @xmath26 ) and setting to zero ( with lagrange multiplier term added ) : @xmath42 carrying out this calculation gives : @xmath43 where @xmath44 thus , the marginal for @xmath26 after learning the sampling distributions , @xmath25 , is proportional to the original marginal @xmath45 , but multiplied by the exponential of the entropy of the corresponding sampling distribution relative to @xmath46 ( usually just @xmath47 ) .",
    "this process of updating from stage 0 to stage 1 using me , and subsequently updating using data , is illustrated graphically in figure  [ updating ] .",
    "joint prior for two quantities @xmath48 and @xmath49 ( to be thought of as `` parameters '' and `` data '' respectively ) is updated once the sampling distributions @xmath50 are known to be @xmath51 .",
    "when the data are known ( in this case , @xmath52 ) , the joint distribution is updated again .",
    "this second updating is equivalent to the usual bayesian process.[updating ] ]    applying similar logic to model selection problems consists simply of applying the short - cut reasoning of the above paragraph : each hypothesis ( model and its parameter value ) gets its prior probability rescaled by a factor measuring the closeness of its predictions to our initial predictions .",
    "this makes it clear that invoking symmetry to assign @xmath53 is flawed : the symmetry may be broken as soon as we assign the sampling distributions .",
    "for the lottery problem , our knowledge about the data before getting it , and before the two models have been specified , is described by a uniform distribution over the integers from 1 to @xmath54 : @xmath55 for all @xmath4 . because the two models havent been specified yet",
    ", symmetry implies we must assign equal prior ( marginal stage 0 ) probabilities @xmath56 .",
    "the joint probabilities for the hypothesis and the data are therefore uniform and independent .",
    "the next step is to incorporate more information and update our probabilities to stage 1 .",
    "this information is not the data , but the specification of the sampling distributions : @xmath57 for all @xmath4 , and @xmath58 if @xmath59 and zero otherwise .",
    "as explained in section  [ entropic ] , the priors for the two hypotheses should be reweighted according to the exponential of the entropy of their sampling distributions with respect to the original predictive distribution .",
    "these entropies are    @xmath60    thus , the solution to the lottery problem is : @xmath61 the three factors here are the stage 0 odds ratio , the entropic correction factor , and the evidence ratio .",
    "the resulting conclusion is as it should be : knowing the winning lottery number provides no information about whether there is fraud or not .",
    "usually , models that make sharp , correct predictions are favoured by bayesian inference . in this example , this still occurs in the evidence ratio , but the entropic factor also _ penalizes _",
    "@xmath1 by the same amount for being unjustifiably confident compared to our honest prior predictive distribution @xmath47 .",
    "the nature of dark energy , thought to be responsible for causing the observed late - time accelerated expansion of the universe @xcite , is a key driver of many upcoming cosmological surveys and instruments @xcite . from a model selection point of view ,",
    "one of the key questions is whether the equation of state of dark energy , @xmath62 , is exactly equal to minus one for all time or whether it has any temporal variation .",
    "the former case is equivalent to einstein s cosmological constant , or non - zero vacuum energy , while any variation from this value , however small , indicates very different physics at play , such as the existence of a primordial scalar field , or other even more exotic possibilities @xcite . here",
    ", model selection is really the key goal ; the exact form of any evolution of @xmath62 is less interesting than simply being sure that it does evolve , or at least have a value different from the cosmological constant .",
    "there has been vigorous debate in the literature on how to best answer this question @xcite . here",
    "we outline the contribution entropic priors can make to this debate ; detailed analysis will be presented in a future contribution .    we will consider four different models for how the dark energy equation of state has varied throughout the universe s history .",
    "typically this is described as @xmath62 varying as a function of @xmath63 , the scale factor .",
    "[ cols= \" < , < \" , ]     observations of type ia supernovae , particularly how their apparent brightness decreases with redshift , is a strong probe of @xmath62 @xcite .",
    "the idea is to test the four models , given such data @xcite , and to forecast the informativeness of proposed future missions @xcite .",
    "however , not all of the models are physically well - motivated : e.g. @xmath64 arises naturally from general relativity , @xmath0 and @xmath1 are ad hoc `` simple models '' , and @xmath65 expresses gross ignorance .",
    "therefore , while it is fair to assign a large probability to @xmath64 at stage 1 , it does not _ automatically _ make sense to share the remaining probability evenly amongst @xmath0-@xmath65 .",
    "the reason for this is that @xmath0 , @xmath1 and @xmath65 may imply quite different predictive distributions for the data .",
    "if we build a @xmath47 that we trust , then the simple models @xmath0 and @xmath1 may be downgraded in prior probability solely because they make predictions that are too confident . of course",
    ", if , in the course of building @xmath47 , we explicitly think about the predictions of the @xmath66 s , then this will not occur - entropic priors do not magically generate information .",
    "what they do is implore us to think about @xmath67 when assigning priors , a key sanity check that is often overlooked .",
    "bayesian model selection is a difficult task , both computationally and philosophically .",
    "if we are not careful , we can obtain misleading results .",
    "the idea presented in this paper , of assigning a realistic predictive distribution for the data and then penalizing models whose predictions differ from it , should assist in making bayesian model selection analyses more reliable .",
    "caticha , a. , lectures on probability , entropy , and statistical physics",
    ". invited lectures at maxent 2008 , the 28th international workshop on bayesian inference and maximum entropy methods in science and engineering ( july 8 - 13 , 2008 , boraceia beach , sao paulo , brazil ) .",
    "available online at ` http://arxiv.org/abs/0808.0012 `                        rodriguez , c.  2002 .",
    "entropic priors for discrete probabilistic networks and for mixtures of gaussian models , in bayesian inference and maximum entropy methods in science and engineering , ed . by r. l. fry , aip conf.proc .",
    "617 , 410 ( 2002 ) .",
    "vegetti , s. , koopmans , l.  v.  e.  2009 .",
    "bayesian strong gravitational - lens modelling on adaptive grids : objective detection of mass substructure in galaxies .",
    "monthly notices of the royal astronomical society 392 , 945 - 963 .",
    "bjb would like to thank the following for valuable discussion : david w. hogg , john skilling , ariel caticha , adom giffin , iain murray , phil marshall and geraint lewis .",
    "i would also like to thank georgina wilcox for introducing me to ` mercurial ` , and tommaso treu for supporting my trip to oxford , mississippi ."
  ],
  "abstract_text": [
    "<S> we demonstrate that the principle of maximum relative entropy ( me ) , used judiciously , can ease the specification of priors in model selection problems . </S>",
    "<S> the resulting effect is that models that make sharp predictions are disfavoured , weakening the usual bayesian `` occam s razor '' . </S>",
    "<S> this is illustrated with a simple example involving what jaynes called a `` sure thing '' hypothesis . </S>",
    "<S> jaynes resolution of the situation involved introducing a large number of alternative `` sure thing '' hypotheses that were possible before we observed the data . </S>",
    "<S> however , in more complex situations , it may not be possible to explicitly enumerate large numbers of alternatives . </S>",
    "<S> the entropic priors formalism produces the desired result without modifying the hypothesis space or requiring explicit enumeration of alternatives ; all that is required is a good model for the prior predictive distribution for the data . </S>",
    "<S> this idea is illustrated with a simple rigged - lottery example , and we outline how this idea may help to resolve a recent debate amongst cosmologists : is dark energy a cosmological constant , or has it evolved with time in some way ? and how shall we decide , when the data are in ?     </S>",
    "<S> address = department of physics , university of california , santa barbara , ca , 93106 - 9530 , usa     address = sissa , via beirut , 2 - 4 34151 trieste ts , italy </S>"
  ]
}