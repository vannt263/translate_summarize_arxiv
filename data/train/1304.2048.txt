{
  "article_text": [
    "it has long been a bane of the bayesian approach that the solutions it proposed were intellectually attractive but inapplicable in practice .",
    "while some numerical analysis solutions were suggested ( see , e.g. * ? ? ?",
    "* ) , they were not in par with the challenges raised by handling non - standard probability densities , especially in high dimensional problems .",
    "this stumbling block in the development of the bayesian perspective became clear when new simulations methods appeared in the early 1990 s and the number of publications involving bayesian methods rised significantly ( no test available ! ) . while those methods were on principle open to any type of inference , they primarily benefited the bayesian paradigm as they were  ideally \" suited to the core object of bayesian inference , namely a mostly intractable posterior distribution .",
    "this chapter will not cover the historical developments of computational methods ( see , e.g. , @xcite ) nor the technical implementation details of simulation techniques ( see , e.g. , @xcite , @xcite , @xcite and @xcite ) , but instead focus on examples of application of those methods to bayesian computational challenges . given the limited length of the chapter ,",
    "it is to be understood as a sequence of illustrations of the main computational tools , rather than a comprehensive introduction , which is to be found in the books mentioned above and below .",
    "the starting point of a bayesian analysis being the posterior distribution , let us recall that it is defined by the product @xmath0 where @xmath1 denotes the parameter and @xmath2 the data .",
    "( the symbol @xmath3 means that the functions on both sides of the symbol are proportional as functions of @xmath1 , the missing constant being a function of @xmath2 , @xmath4 . ) the structures of both @xmath1 and @xmath2 can vary in complexity and dimension , although we will not discuss the non - parametric case when @xmath1 is infinite dimensional , referring the reader to @xcite for an introduction .",
    "the prior distribution is most often available in closed form , being chosen by the experimenter , while the likelihood function @xmath5 may be too involved to be computed even for a given pair @xmath6 . in special cases where @xmath5 allows for a demarginalisation representation @xmath7 where @xmath8 is a ( manageable ) probability density ,",
    "we will call @xmath9 the missing data .",
    "however , the existence of such a representation does not necessarily implies it is of any use in computations .",
    "( we will encounter both cases in sections [ sec : mcmc ] and [ sec : abc ] . )    since the posterior distribution is defined by @xmath10 a first difficulty occurs because of the normalising constant : the denominator is very rarely available in closed form .",
    "this is an issue only to the extent that the posterior density is defined up to a constant . in cases where the constant does not matter",
    ", inference can be easily conducted without the constant .",
    "cases when the constant matters include testing and model choice , since the marginal likelihood @xmath11 is central to the bayesian procedures addressing this inferential problem .",
    "indeed , when comparing two models against the same dataset @xmath2 , the prefered bayesian solution ( see , e.g. , @xcite , chapter 5 , or @xcite ) is to use _ the bayes factor _ , defined as the ratio of marginal likelihoods @xmath12 and compared to @xmath13 to decide which model is most supported by the data ( and how much ) .",
    "such a tool ",
    "quintessential for running a bayesian test ",
    "means that for almost any inference problem  barring the very special case of conjugate priors there is a computational issue , not the most promising feature for promoting an inferential method .",
    "this aspect has obviously been addressed by the community , see for instance @xcite that is entirely dedicated to the problem of approximating normalising constants or ratios of normalising constants , but i regret the issue is not spelled out much more clearly as one of the major computational challenges of bayesian statistics ( see also @xcite ) .    as a benchmark ,",
    "consider the case @xcite when a sample @xmath14 can be issued either from a normal @xmath15 distribution or from a double - exponential @xmath16 distribution with density @xmath17 ( this case was suggested to us by a referee of @xcite , however i should note that a similar setting opposing a normal model to ( simple ) exponential data used as a benchmark in @xcite for abc algorithms . ) then , as it happens , the bayes factor @xmath18 is available in closed form , since , under a normal @xmath19 prior , the marginal likelihood for the normal model is given by @xmath20 \\,{\\text{d}}\\mu/\\sqrt{2\\pi}\\sigma \\\\ & = ( 2\\pi)^{-n/2 } \\exp\\ { -\\sum_{i=1}^n ( x_i-\\bar x_n)^2/2\\}\\,\\\\ & \\quad \\times\\exp\\ { -n\\sigma^{-2 } ( \\bar x_n)^2/2 ( n+\\sigma^{-2 } ) \\ } /\\sigma\\sqrt{n+\\sigma^{-2}}\\end{aligned}\\ ] ] and , for the double - exponential model , by ( assuming the sample is sorted ) @xmath21\\end{aligned}\\ ] ] with obvious conventions when @xmath22 ( @xmath23 ) and @xmath24 ( @xmath25 ) . to illustrate the consistency of the bayes factor in this",
    "setting , figure [ fig : bf1a ] represents the distributions of the bayes factors associated with 100 normal and 100 double - exponential samples of sizes 50 and 200 , respectively .",
    "while the smaller samples see much overlay in the repartition of the bayes factors , for 200 observations , in both models , the log - bayes factor distribution concentrates on the proper side of zero , meaning that it discriminates correctly between the two distributions for a large enough sample size .",
    "another recurrent difficulty with using posterior distributions for inference is the derivation of credible sets  the bayesian version of confidence sets ( see , e.g. , @xcite)since they are usually defined as highest posterior density regions : @xmath26 where the bound @xmath27 is determined by the credibility of the set @xmath28 while the normalisation constant is irrelevant in this problem , determining the collection of parameter values such that @xmath29 and calibrating the lower bound @xmath30 on the product @xmath31 to achieve proper coverage are non - trivial problems that require advanced simulation methods .",
    "once again , the issue is somehow overlooked in the literature .    while one of the major appeals of bayesian inference is that it is not reduced to an estimation technique  but on the opposite offers a whole range of inferential tools to analyse the data against the proposed model , the computation of bayesian estimates is nonetheless certainly one of the better addressed computational issues .",
    "this is especially true for posterior moments like the posterior mean @xmath32 $ ] since they are directly represented as ratios of integrals @xmath33 = \\dfrac{\\int_\\uptheta \\theta \\pi(\\theta)f(x|\\theta)\\,\\text{d}\\theta}{\\int_\\uptheta \\pi(\\theta)f(x|\\theta)\\,\\text{d}\\theta}\\,.\\ ] ] the computational problem may however get involved for several reasons , including for instance    * the space @xmath34 is not euclidean and the problem imposes shape constraints ( as in some time series models ) ; * the dimension of @xmath34 is large ( as in non - parametrics ) ; * the estimator is the solution to a fixed point problem ( as in the credible set definition ) ; * simulating from @xmath35 is delicate or even impossible ;    the latter case being in general the most challenging and thus the most studied , as the following sections will show .",
    "monte carlo methods have been introduced by physicists in los alamos , namely ulam , von neumann , metropolis , and their collaborators in the 1940 s ( see @xcite ) . the idea behind",
    "monte carlo is a straightforward application of the _ law of large numbers _ , namely that , when @xmath36 are i.i.d .  from the distribution @xmath37 , the empirical average @xmath38 converges ( almost surely ) to @xmath39 $ ] when @xmath40 goes to @xmath41 .",
    "while this perspective sounds too simple to apply to complex problems  either because the simulation from @xmath37 itself is intractable or because the variance of the empirical average is too large to be manageable , there exist more advanced exploitations of this result that lead to efficient simulation solutions .",
    "consider computing the bayes factor @xmath42 by simulating a sample @xmath43 from the prior distribution , @xmath44 .",
    "the approximation to the bayes factor is then provided by @xmath45 given that in this special case the _ same _ prior and the _ same _ monte carlo samples can be used . figure [ fig : bf2 ] shows the convergence of @xmath46 over @xmath47 iterations , along with the true value .",
    "the method exhibits convergence .",
    "the above example can also be interpreted as an illustration of importance sampling , in the sense that the prior distribution is used as an importance function in both integrals .",
    "we recall that importance sampling is a monte carlo method where the quantity of interest @xmath48 $ ] is expressed in terms of an expectation under the importance density @xmath49 , @xmath50 = { \\mathbb{e}}_g[h(x)f(x)/g(x)]\\,,\\ ] ] which allows for the use of monte carlo samples distributed from @xmath49 .",
    "although importance sampling is at the source of the particle method @xcite , i will not develop this useful sequential method any further , but instead briefly introduce the notion of bridge sampling @xcite as it applies to the approximation of bayes factors @xmath51 ( and to other ratios of integrals ) .",
    "this method handles the approximation of ratios of integrals over identical spaces ( a severe constraint ) , by reweighting two samples from both posteriors , through a well - behaved type of harmonic average .    more specifically , when @xmath52 , possibly after a reparameterisation of both models to endow @xmath1 with the same meaning , we have @xmath53 where @xmath54 and @xmath55 are two independent samples coming from the posterior distributions @xmath56 and @xmath57 , respectively .",
    "( this identity holds for any function @xmath58 guaranteeing the integrability of the products .",
    ") however , there exists a quasi - optimal solution , as provided by @xcite : @xmath59 while this optimum can not be used  given that it relies on the normalising constants of both @xmath60 and @xmath61 , a practical implication of the result resorts to an iterative construction of @xmath62 .",
    "we gave in @xcite an alternative representation of the bridge factor that bypasses this difficulty ( if difficulty there is ! ) .",
    "if we want to apply the bridge sampling solution to the normal versus double - exponential example , we need to simulate from the posterior distributions in both models .",
    "the normal posterior distribution on @xmath63 is a normal @xmath64 distribution , while the double - exponential distribution can be derived as a mixture of @xmath65 truncated normal distributions , following the same track as with the computation of the marginal distribution above .",
    "the sum obtained in the above expression of @xmath66 suggests interpreting @xmath67 as ( once again assuming @xmath68 sorted ) @xmath69 where @xmath70 denotes a truncated normal distribution , that is , the normal @xmath71 distribution restricted to the interval @xmath72 , and where the weights @xmath73 are proportional to those summed in @xmath66 ( see example 1 ( bis ) ) .",
    "the outcome of one such simulation is shown in figure [ fig : lapost ] along with the target density : as seen there , since the true posterior can be plotted against the histogram , the fit is quite acceptable .",
    "if we start with an arbitrary estimation of @xmath74 like @xmath75 , successive iterations produce the following values for the estimation : @xmath76 , @xmath77 , @xmath77 , based on @xmath78 samples from each posterior distribution ( to compare with an exact ratio equal to @xmath79 and a monte carlo approximation of @xmath80 ) .",
    "while this bridge solution produces valuable approximations when both parameters @xmath81 and @xmath82 are within the same parameter space and have the same or similar absolute meanings ( e.g. , @xmath1 is equal to @xmath83 $ ] in both models ) , it does not readily apply to settings with variable dimension parameters . in such cases ,",
    "separate approximations of the evidences ,",
    "i.e.  of the numerator and denominator in @xmath74 are requested , with the exception of reversible jump monte carlo techniques @xcite presented in the following section .",
    "although using harmonic means for this purpose as in @xcite is fraught with danger , as discussed in @xcite and @xcite , we refer the reader to this later paper of ours for a model - based solution using an importance function restricted to an hpd region ( see also @xcite and @xcite ) .",
    "we however insist on ( and bemoan ) the lack of generic solution for the approximation of bayes factors , despite those being the workhorse of bayesian model selection and hypothesis testing .",
    "the above monte carlo techniques impose ( or seem to impose ) constraints on the posterior distributions that can be approximated by simulation .",
    "indeed , direct simulation from this target distribution is not always feasible in a ( time - wise ) manageable form , while importance sampling may result in very poor or even worthless approximations , as for instance when the empirical average @xmath84 suffers from an infinite variance .",
    "finding a reliable importance function thus requires some sufficient knowledge about the posterior density @xmath85 .",
    "markov chain monte carlo ( mcmc ) methods were introduced ( also in los alamos ) with the purpose of bypassing this requirement of an a priori knowledge on the target distribution . on principle , they apply to any setting where @xmath85 is known up to a normalising constant ( or worse , as a marginal of a distribution on an augmented space ) .    as described in another chapter of this volume ( craiu and rosenthal , 2013 ) , mcmc methods rely on ergodic theorems , i.e.  the facts that , for positive recurrent markov chains , ( a ) the limiting distribution of the chain is always the stationary distribution and ( b ) the law of large numbers applies . the fascinating feature of those algorithms is that it is straightforward to build a markov chain ( kernel ) with a stationary distribution equal to the posterior distribution , even when the latter is only know up to a normalising constant . obviously , there are caveats with this rosy tale : complex posteriors remain harder to approximate than essentially gaussian posteriors , convergence ( ergodicity ) may require in - human time ranges or simply not agree with the limited precision of computers .    for completeness sake , we recall here the format of a random walk metropolis  hastings ( rwmh ) algorithm @xcite    generate @xmath86 take @xmath87 with probability @xmath88 take @xmath89 otherwise .",
    "if we consider once again the posterior distribution on @xmath63 associated with a laplace sample , even though the exact simulation from this distribution was implemented in example 1 ( ter ) , an mcmc implementation is readily available . using a rwmh algorithm , with a normal distribution centred at @xmath90 and with scale @xmath91 ,",
    "the implementation of the method is straightforward .",
    "as shown on figure [ fig : rwmhl ] , the algorithm is less efficient than an iid sampler , with an acceptance rate of only @xmath92 .",
    "however , one must also realise that devising the code behind the algorithm only took five lines and a few minutes , compared with the most elaborate construction behind the iid simulation !      a special class of mcmc methods seems to have been especially designed for bayesian hierarchical modelling ( even though they do apply in a much wider generality ) .",
    "those go under the denomination of gibbs samplers , unfortunately named after gibbs for the mundane reason that one of their initial implementations was for the simulation of gibbs random fields ( in image analysis , @xcite ) .",
    "indeed , gibbs sampling addresses the case of ( often ) high - dimensional problems found in hierarchical models where each parameter ( or group of parameters ) is endowed with a manageable full conditional posterior distribution .",
    "( while the joint posterior is not manageable . )",
    "the principle of the gibbs sampler is then to proceed by local simulations from those full conditionals in a rather arbitrary order , producing a markov chain whose stationary distribution is the joint posterior distribution .",
    "let us recall that a bayesian hierarchical model is build around a hierarchy of probabilistic dependences , each level depending only on the neighbourhood levels ( except for global parameters that may impact all levels ) .",
    "for instance , @xmath93 induces a simple hierarchical model in that @xmath68 only depends on @xmath82 while @xmath94 only depends on @xmath82i.e .",
    ", @xmath68 is independent of @xmath94 given @xmath82 .",
    "examples of such structures abound :    a typical instance is made of random effect models as in the following instance ( inspired from @xcite ) of poisson observations @xmath95 @xmath96 where @xmath97 denotes a group or district label , @xmath98 the replication index , @xmath99 a vector of covariates , @xmath100 a population size . in this model ,",
    "given the data @xmath101 , a gibbs sampler generates from the joint distribution of @xmath102 , @xmath103 , @xmath104 , and @xmath105 by using the conditionals @xmath106 which are more or less manageable ( as they may require individual metropolis  hasting implementations where the poisson distribution is replaced with its normal approximation in the proposal ) .",
    "note , however , that this simple solution hides a potential difficulty with the choice of an improper prior on @xmath104 and @xmath105 .",
    "indeed , even though the above conditionals are well - defined for all samples , it may still be that the associated joint posterior distribution does not exist .",
    "this phenomenon of the _ improper posterior _ was exhibited in @xcite and analysed in @xcite .",
    "a growth measurement model was applied by @xcite to dental measurements of 11 girls and 16 boys , as a mixed - effect model .",
    "( the dataset is available in r as orthodont in package nlme . )",
    "compared with the random effect models , mixed - effect models include additional random - effect terms and are more appropriate for representing clustered , and therefore dependent , data arising in , e.g. , hierarchical , paired , or longitudinal data . ) for @xmath107 children and @xmath108 observations on each child , growth is expressed as    @xmath109 where @xmath110 is a sex factor with @xmath111 ( @xmath13 corresponds to female and @xmath112 to male ) and @xmath113 is the vector of ages .",
    "the random effects in this growth model are the @xmath114 s , which are independent @xmath115 variables .",
    "the priors on the corresponding parameters are chosen to be conjugate : @xmath116 where @xmath117 denotes the inverse gamma distribution .",
    "note that , while the posterior distribution is well - defined in this case , there is no garantee that the limit exists when @xmath118 goes to zero and thus that small values of @xmath118 should be avoided as they do not necessarily constitute proper default values .",
    "figure [ fig : dagoff ] summarises the bayesian model through a dag ( directed acyclic graph , see @xcite ) .",
    "thanks to this conjugacy , the full conditionals are available as standard distributions @xmath119 : @xmath120 where @xmath121 is the number of children with sex @xmath122 , and @xmath123 @xmath124 it is therefore straightforward to run the associated gibbs sampler",
    ". figures [ fig : pof1 ] and [ fig : pof2 ] show the raw output of some parameter series , based on @xmath125 iterations .",
    "for instance , those figures show that @xmath126 and @xmath127 are possibly equal , as their likely ranges overlap .",
    "this does not seem to hold for @xmath128 and @xmath129 .",
    "evolution of the gibbs markov chains for some parameters of the growth mixed - effect model of pothoff and roy ( 1964 ) _",
    "( right ) _ and density estimate of the corresponding posterior distribution _",
    "( right ) _ , based on @xmath125 iterations.,width=377 ]     same legend as figure [ fig : pof1].,width=377 ]    one of the obvious applications of the gibbs sampler is found in graphical models ",
    "an application that occurred in the early days of mcmc  since those models are defined by and understood via conditional distributions rather than through an unmanageable joint distribution . as detailed in @xcite ,",
    "undirected probabilistic graphs are markov with respect to the graph structure , which means that variables indexed by a given node @xmath130 of the graph only depend on variables indexed by nodes connected to @xmath130 .",
    "for instance , if the vector indexed by the graph is gaussian , @xmath131 , the non - zero terms of @xmath132 correspond to the edges of the graph . applications of this modelling abound , as for instance in experts systems @xcite .",
    "note that hierarchical bayes models can be naturally associated with dependence graphs leading to dags and thus fall within this category as well .",
    "although the principles of the mcmc methodology are rather straightforward to understand and to implement , resorting for instance to down - the - shelf techniques like rwmh algorithms , a more challenging setting occurs with the case of variable dimensional problems .",
    "these problems typically occur in a bayesian model choice situation , where several ( or an infinity of ) models are considered at once .",
    "the resulting parameter space is a _ millefeuille _",
    "collection of sets , with most likely different dimensions , and moving around this space or across those layers is almost inevitably a computational issue .",
    "indeed , the only case open to direct computation is the one when the posterior probabilities of the models under comparison can be evaluated , resulting in a two - stage implementation , the model being chosen first and the parameters of this model being simulated  as usual \" .",
    "however , as seen above , computing posterior probabilities of models is rarely a straightforward case .",
    "in other settings , moving around the collection of models and within the corresponding parameter spaces must occur simultaneously , especially when the number of models is large or infinite .    defining a markov chain kernel that explores the multi - layered space is challenging because of the difficulty of defining a reference measure on this complex space .",
    "however , @xcite came up with a solution that is rather simplex to express ( if not necessarily to implement ) .",
    "the idea behind green s ( 1995 ) reversible jump solution is to take advantage of the markovian nature of the algorithm : since all that matters in a markov chain is the most recent value of the chain , exploration of a multi - layered space , represented as a direct sum @xcite of those spaces , @xmath133 only involves a pair of sets @xmath134 at each step , @xmath135 and @xmath136 say .    therefore , the mathematical difficulty reduces to create a connection between both spaces , difficulty that is solved by green s ( 1995 ) via the introduction of auxiliary variables @xmath137 and @xmath138 in order for @xmath139 and @xmath140 to be in one - to - one correspondence , i.e.  @xmath141 .",
    "arbitrary distributions on @xmath137 and on @xmath138 then come to complement the target distributions @xmath142 and @xmath143 .",
    "the algorithm is call reversible because the symmetric move from @xmath139 to @xmath140 must follow @xmath144 . in other words , moves one way determine moves the other way .",
    "a schematic representation is as follows :    given current state @xmath145 , generate index @xmath146 from the prior probabilities @xmath147 .",
    "generate @xmath137 from the auxiliary distribution @xmath148 compute @xmath144 accept to switch to @xmath145 with probability @xmath149 else reproduce @xmath145    the important feature in the above acceptance probability is the jacobian term @xmath150 which corresponds to the change of density in the transformation .",
    "it is also a source of potential mistakes in the implementation of the algorithm .",
    "the simplest version of rjmcm is when @xmath151 , i.e.  when the move from one parameter space to the next involves adding or removing one parameter , as for instance in estimating a mixture with an unknown number of components @xcite or a @xmath152 time series with @xmath153 unknown .",
    "it can also be used with @xmath153 known , as illustrated below .",
    "an @xmath152 time series model  where ma stands for ` moving average'is defined by the equations @xmath154 where the @xmath155 s are iid @xmath44 . while this model can be processed without rjmcmc , we present here a resolution explained in @xcite that does not distinguish between the cases when @xmath153 is known and when @xmath153 is unknown .",
    "the associated  lag polynomial \" @xmath156 provides a formal representation of the series as @xmath157 , with @xmath158 , @xmath159 , ... as a polynomial it also factorises through its roots @xmath160 as @xmath161 while the number of roots is always @xmath153 , the number of ( non - conjugate ) complex roots varies between @xmath162 ( meaning no complex root ) and @xmath163 .",
    "this representation of the model thus induces a variable dimension structure in that the parameter space is then the product @xmath164 , where @xmath165 denotes the complex unit ball and @xmath166 is the number of real valued roots @xmath167 .",
    "the prior distributions on the real and ( non - conjugate ) complex roots are the uniform distributions on @xmath168 and @xmath165 , respectively . in other words ,",
    "@xmath169 moving around this space using rjmcmc is rather straightforward : either the number of real roots does not change in which case any regular mcmc step is acceptable or the number of real roots moves up or down by a factor of 2 , new roots being generated from the prior distribution , in which case the above rjmcmc acceptance ratio reduces to a likelihood ratio .",
    "an extra difficulty with the @xmath152 setup is that the likelihood is not available in closed form unless the past innovations @xmath170 are available .",
    "as explained in @xcite , they need to be simulated in a gibbs step , that is , conditional upon the other parameters with density proportional to @xmath171 where @xmath172,@xmath173 , @xmath174 and @xmath175 @xmath176 this recursive definition of the likelihood is rather costly since it involves computing the @xmath177 s for each new value of the past innovations , hence @xmath40 sums of @xmath153 terms .",
    "nonetheless , the complexity @xmath178 of this representation is much more manageable than the normal exact representation mentioned above .    as mentioned above",
    ", the difficulty with rjmcm is in moving from the general principle ( which indeed allows for a generic exploration of varying dimension spaces ) to the practical implementation : when faced with a wide range of models , one needs to determine which models to pair together  they must be similar enough  and how to pair them  so that the jumps are efficient enough .",
    "this requires the calibration of a large number of proposals , whose efficiency is usually much lower than in single - model implementations .",
    "whenever the number of models is limited , my personal experience is that it is more efficient to run separate ( and parallel ) mcmc algorithms on all models and to determine the corresponding posterior probabilities of those models by a separate evaluation , like chib s ( @xcite ) .",
    "( indeed , a byproduct of the rjmcmc algorithm is to provide an evaluation of the posterior probabilities of the models under comparison via the frequencies of accepted moves to such models . )",
    "see , e.g. , @xcite for an illustration in the setting of mixtures of distributions .",
    "we end up with a word of caution against the misuse of probabilistic structures over those collections of spaces , as illustrated by @xcite and @xcite @xcite .",
    "this section covers some aspects of a specific computational method called approximate bayesian computation ( abc in short ) , which stemmed from acute computational problems in statistical population genetics and rised in importance over the past decade .",
    "the section should be more methodological than the previous sections as the method is not covered in this volume , as far as i can assess .",
    "in addition , this is a special computational method in that it has been specifically developed for challenging bayesian computational problems ( and that it carries the bayesian label within its name ! ) .",
    "although the reader is referred to , e.g. , @xcite and @xcite for a deeper review on this method , i will cover here different accelerating techniques and the numerous calibration issues of selecting both the tolerance and the summary statistics .",
    "approximate bayesian computation ( abc ) techniques appeared at the end of the 20th century in population genetics @xcite , where scientists were faced with intractable likelihoods that mcmc methods were simply unable to handle with the slightest amount of success .",
    "some of those scientists developed simulation tools overcoming the jamming block of computing the likelihood function that turned into a much more general form of approximation technique , exhibiting fundamental links with econometric methods such as indirect inference @xcite .",
    "although some part of the statistical community was initially reluctant to welcome them , trusting instead massively parallelised mcmc approaches , abc techniques are now starting to be part of the statistical toolbox and to be accepted as an inference method _ per se _ , rather than being a poor man s alternative to more mainstream techniques .",
    "while details about the method are provided in recent surveys @xcite , we expose in algorithmic terms the basics of the abc algorithm :    generate @xmath179 from the prior @xmath180 .",
    "generate @xmath181 from the model @xmath182 .",
    "compute the distance @xmath183 .",
    "accept @xmath179 if @xmath184 .",
    "acceptance    the idea at the core of the abc method is to replace an acceptance based on the unavailable likelihood with one evaluating the pertinence of the parameter from the proximity between the data and a simulated pseudo - data .",
    "this proximity is using a distance or pseudo - distance @xmath185 between a ( summary ) statistic @xmath186 based on the data and its equivalent @xmath187 for the pseudo - data .",
    "we stress from this early stage that the summary statistic @xmath188 is very rarely sufficient and hence that abc looses some of the information contained in the data .    while the ma@xmath189 is manageable by other approaches",
    " since the missing data structure is of a moderate complexity , it provides an illustration of a model where the likelihood function is not available in closed form and where the data can be simulated in a few lines of code given the parameter . using the @xmath153 first autocorrelations as summary statistics @xmath190",
    ", we can then simulate parameters from the prior distribution and corresponding series @xmath191 and only keep the parameter values associated with the smallest @xmath192 s .",
    "as shown in figure [ fig : rawdist ] , reproduced from @xcite , there is a difference between the genuine posterior distribution and the abc approximation , whatever the value of @xmath193 is .",
    "this comparison also shows that the approximation stabilises quite rapidly as @xmath193 decreases to zero , in agreement with the general argument that the tolerance should not be too close to zero for a given sample size @xcite .",
    "abc suffers from an  information paradox \" , namely that it quickly stops to pay to increase the dimension of the summary statistic @xmath190 in the hope to bring the abc inference closer to a  perfect \" bayesian inference based on the whole data and thus fill the information gap .",
    "for one thing , increasing the dimension of the summary statistic invariably leads to increase the tolerance @xmath193 , as discussed below .",
    "for another thing , considering the most extreme case illuminates this paradox .",
    "as noted above , abc is almost always based on summary statistics , @xmath190 , rather than on the raw data .",
    "the reason why is obvious in example 4 ( bis ) , since using the raw time series instead of the vector of empirical autocorrelations would have been strongly detrimental as the distance between two simulated series grows with the time horizon and brings very little information about the value of the underlying parameter . in other words , it forces us to use a much larger tolerance @xmath193 in the algorithm . the paradox is easily explained by the following points :    * the ( initial ) intuition upon which abc is built considers the limiting case @xmath194 and the fact that @xmath195 is an approximation to @xmath196 , as opposed to the true setting being that @xmath197 is an approximation to @xmath198 and that it incorporates a monte carlo error as well ; * for a given computational effort , the tolerance @xmath193 is necessarily positive  if only to produce a positive acceptance rate  and deeper studies show that it behaves like a non - parametric bandwidth parameter , hence increasing with the dimension of @xmath188 while ( slowly ) decreasing with the sample size .    therefore , when the dimension of the raw data is large ( as for instance in the time series setting of example 4 bis ) , it is definitely not recommended to use a distance between the raw data @xmath199 and the raw pseudo - data @xmath200 : the _ curse of dimension _ operates in nonparametric statistics and clearly impacts the approximation of @xmath196 as to make it impossible even for moderate dimensions .    in connection with the above , it must be stressed that , in almost any implementation , the abc algorithm is not _",
    "correct _ for at least two reasons : the data @xmath199 is replaced with a roughened version @xmath201 and the use of a non - sufficient summary statistic @xmath202 .",
    "in addition , as in regular monte carlo approximations , a given computational effort produces a corresponding monte carlo error .",
    "the choice of the summary statistic @xmath190 is paramount in any implementation of the abc methodology if one does not want to end up with simulations from the prior distribution resulting from too large a tolerance ! on the opposite , an efficient construction of @xmath202 may result in a very efficient approximation for a given computational effort .",
    "the literature on abc abounds with more or less recommendable solutions to achieve a proper selection of the summary statistic .",
    "early studies were either experimental @xcite or borrowing from external perspectives .",
    "for instance , @xcite argue in favour of using neural nets in their non - parametric modelling for the very reason that neural nets eliminate irrelevant components of the summary statistic .",
    "however , the black box features of neural nets also mean that the selection of the summary statistic is implicit .",
    "another illustration of the use of external assessments is the experiment ran by @xcite in mixing local regression @xcite local regression tools with the bic criterion .    in my opinion ,",
    "the most accomplished ( if not ultimate ) development in the abc literature about the selection of the summary statistic is currently found in @xcite .",
    "those authors study the use of a summary statistic @xmath188 from a quasi - decision - theoretic perspective , evaluating the error by a quadratic loss @xmath203 where @xmath204 is a positive symmetric matrix , and obtaining in addition a determination of the optimal bandwidth ( or tolerance ) @xmath205 from non - parametric evaluations of the error .",
    "in particular , the authors argue that the optimal summary statistic is @xmath206 $ ] ( when estimating the parameter of interest @xmath1 ) . for this",
    ", they notice that the errors resulting from an abc modelling are of three types :    * one due to the approximation of @xmath207 by @xmath208 , * one due to the approximation of @xmath208 by @xmath209 \\pi(\\theta|{\\mathbf{s}})\\,\\text{d}{\\mathbf{s } } } { \\int \\pi({\\mathbf{s } } ) k[\\{{\\mathbf{s}}-{s({\\mathbf{x}}^0)}\\}/h]\\,\\text{d}{\\mathbf{s}}}\\ ] ] where @xmath210 is the kernel function used in the acceptance step  which is the indicator function @xmath211 in the above algorithm since @xmath212 is accepted with probability @xmath213 in this case , * one due to the approximation of @xmath214 by importance monte carlo techniques based on @xmath215 simulations , which amounts to @xmath216 , if @xmath217 is the expected number of acceptances .    for the specific case when @xmath218=\\hat\\theta$ ] , the expected loss satisfies @xmath219=\\mbox{trace}(a\\upsigma)+h^2\\int { \\mathbf{x}}^t a { \\mathbf{x}}k({\\mathbf{x}})\\mbox{d}{\\mathbf{x}}+o(h^2)\\,,\\ ] ] where @xmath220 , which means that the first type error vanishes with small @xmath205 s , given that it is equivalent to the bayes risk based on the whole data .",
    "from this decomposition of the risk , @xcite derive @xmath221 as an optimal bandwidth for the standard abc algorithm . from a practical perspective , using the posterior expectation @xmath206 $ ] as a summary statistic is obviously impossible , if only because even basic simulation from the posterior is impossible .",
    "@xcite suggest using instead a two - stage procedure :    1 .",
    "run a basic abc algorithm to construct a non - parametric estimate of @xmath206 $ ] following @xcite ; and 2 .",
    "use this non - parametric estimate as the summary statistic in a second abc run .    in cases",
    "when producing the reference sample is very costly , the same sample may be used in both runs , even though this may induce biases that will simply add up to the many approximative steps inherent to this procedure .    in conclusion , the literature on the topic has gathered several techniques proposed for other methodologies . while this perspective manages to eliminate the less relevant components of a pool of statistics , i feel the issue remains quite open as to which statistic",
    "should be included at the start of an abc algorithm .",
    "the problems linked with the curse of dimensionality (  not too many \" ) , identifiability (  not too few \" ) , and ultimately precision (  as many as components of @xmath1 \" ) of the approximations are far from solved and i thus foresee further major developments to occur in the years to come .      as stressed already above",
    ", model choice occupies a special place in the bayesian paradigm and this for several reasons .",
    "first , the comparison of several models compels the bayesian modeller to construct a meta - model that includes all these models under comparison as special cases .",
    "this encompassing model thus has a complexity that is higher than the complexities of the models under comparison .",
    "second , while bayesian inference on models is formally straightforward , in that it computes the posterior probabilities of the models under comparison  even though this raises misunderstanding and confusion in the non - bayesian applied communities , as illustrated by the series of controversies raised by templeton ( @xcite , the computation of such objects often faces major computational challenges .    from an abc perspective",
    ", the specificity of model selection holds as well . at first sight , and in sort of predictable replication of the theoretical setting , the formal simplicity of computing posterior probabilities can be mimicked by an abc - mc ( for model choice ) algorithm as the following one @xcite :    generate @xmath222 from the prior @xmath223 .",
    "generate @xmath224 from the prior @xmath225 .",
    "generate @xmath181 from the model @xmath226 .",
    "compute the distance @xmath183 .",
    "accept @xmath227 if @xmath184 .",
    "acceptance    where @xmath228 denotes the unknown model index , @xmath229 being one of the possible values , with @xmath230 the corresponding prior on the parameter @xmath231 .    as a consequence , the above algorithm process the pair @xmath232 as a regular parameter , using the same tolerance condition @xmath233 as the initial abc algorithm . from the output of abc - mc , the posterior probability @xmath234 can then be approximated by the frequency of acceptances of simulations from model @xmath229 @xmath235 improvements on this crude frequency estimate can be made using for instance a weighted polychotomous logistic regression estimate of @xmath234 , with non - parametric kernel weights , as in @xcite .",
    "if we resume our comparison of the normal and double - exponential models . running abc - mc in this case",
    "means    1 .",
    "picking normal @xmath236 or double - exponential @xmath237 with probability @xmath238 ; 2 .   simulating @xmath239",
    "; 3 .   simulating a normal @xmath240 sample @xmath200 if @xmath236 and a double - exponential @xmath241 sample @xmath200 if @xmath237 ; 4 .   compare @xmath242 and @xmath192    while the choice of @xmath190 is unlimited , some choices are relevant and others are to be avoided as discussed in @xcite .",
    "figures [ fig4 ] and [ fig5 ] show the difference in using for @xmath188 the median of the sample ( figure [ fig4 ] ) and the median absolute deviation ( mad , defined as the median of the absolute values of the differences between the sample and its median , @xmath243 ) statistics ( figure [ fig5 ] ) . in the former case",
    ", double exponential samples are not recognised as such and the posterior probabilities do not converge to zero . in the later case ,",
    "they do , which means the abc bayes factor is consistent in this setting .",
    "the conclusion of @xcite is that the outcome of an abc model choice based on a summary statistic that is insufficient may be untrustworthy and need to be checked by additional monte carlo experiments as those proposed in diyabc @xcite .",
    "more recently , @xcite exhibited conditions on the summary statistic for an abc model choice approach to provide a consistent solution .",
    "this chapter provides a snapshot via a few illustrations of the diversity of bayesian computational techniques .",
    "it also misses important directions , like the particle methods which are particularly suited for complex dynamical models @xcite . or variational bayes techniques which rely on optimised approximations to a complex target distribution @xcite . or partly analytical integration taking advantage of gaussian structures , as for the quickly expanding inla technology @xcite , which recent advances are covered by @xcite . or yet more remote approximations to the likelihood function based on higher order asymptotics @xcite .",
    "similarly , i did not mention recent simulations methodologies that coped with non - parametric bayesian problems @xcite and with stochastic processes @xcite .",
    "the field is expanding and the demands made by the  big data \" crisis are simultaneously threatening the fundamentals of the bayesian approach by calling for quick - and - dirty solutions and bringing new materials , by exhibiting a crucial need for hierarchical bayes modelling .",
    "thus , to conclude with dickens ( @xcite ) opening words , we may later consider that ",
    "it was the best of times , it was the worst of times , it was the age of wisdom , it was the age of foolishness \" .",
    "i am quite grateful to jean - michel marin for providing some of the material included in this chapter , around example 4 and the associated figures .",
    "it should have been part of the chapter on hierarchical models in our new book _ bayesian essentials with r _ , chapter that we eventually had to abandon to its semi - baked status .",
    "the section on abc was also salvaged from another attempt at a joint survey for a statistics and biology handbook , survey that did not evolve much further than my initial notes and obviously did not meet the deadline .",
    "therefore , jean - michel should have been a co - author of this chapter but he repeatedly declined my requests to join .",
    "he is thus named co - author _ in absentia_. thanks to jean - louis foulley , as well , who suggested using the pothoff and roy ( 1964 ) dataset in his ensai lecture notes ."
  ],
  "abstract_text": [
    "<S> this chapter surveys advances in the field of bayesian computation over the past twenty years , from a purely personnal viewpoint , hence containing some ommissions given the spectrum of the field . </S>",
    "<S> monte carlo , mcmc and abc themes are thus covered here , while the rapidly expanding area of particle methods is only briefly mentioned and different approximative techniques like variational bayes and linear bayes methods do not appear at all . </S>",
    "<S> this chapter also contains some novel computational entries on the double - exponential model that may be of interest _ per se_. </S>"
  ]
}