{
  "article_text": [
    "the first `` beowulf '' pc cluster [ 1 ] provided a proof of concept that sparked a revolution in the use of off - the - shelf computers for numerical simulations , data mining , and statistical analysis of large datasets .",
    "because these clusters use commodity components , this paradigm has provided an unbeatable ratio of computing power per dollar , and over the past decade beowulf - class systems have enabled a wide range of high - performance applications on department - scale budgets .",
    "beowulfs have unquestionably been highly beneficial in our own field of astrophysics , especially in smaller academic institutions that might not otherwise have access to substantial computing facilities .",
    "beowulfs have been successfully applied to numerical simulations that use a variety of algorithmic approaches to study systems spanning the range of scales from individual stars and supernovae ( see e.g. [ 2 ] ) to the horizon size of the universe ( see e.g.[3][4 ] ) .",
    "unfortunately , progress in the development of affordable parallel computers has not been matched by analogous progress in the development and dissemination of parallel software aimed at providing scientists with the tools needed to take advantage of the computing power of parallel machines .",
    "scientists with access to a beowulf cluster still need the expertise to develop parallel codes , and have to spend a tremendous amount of time in the development and testing of tools , even for the most common data analysis tasks .",
    "this is in stark contrast to the situation for serial data analysis and simulations , for which a large number of standard general - purpose ( e.g. matlab , maple , r / splus , mathematica , idl ) and specialized ( e.g. iraf for astronomical data analysis ) tools and libraries exist .",
    "the recent advent of multi - core pcs has broadened still further the base of commodity machines that enable computationally intensive parallel simulations and data analysis of large datasets . however",
    ", this impressive technological advance serves also to make the lack of general tools for parallel data analysis even more striking .",
    "the result is a significant barrier to entry for users or developers of parallel computing applications .    as the gap between increasing parallel computing power and the availability of software tools needed to exploit it has become more and more evident ,",
    "a number of projects have attempted to develop such tools .",
    "our team has developed a package of parallel computational tools  the beowulf analysis symbolic interface ( ) to deal with precisely these issues .",
    "is a suite of parallel computational tools for the management , analysis and visualization of large datasets .",
    "it is designed with the idea that not all scientists need to be specialists in numerics .",
    "rather , a user should be able to interact with his or her data in an intuitive way . in its current form",
    ", the package can be used either as a set of library functions in a monolithic program or interactively , from a shell , using the interface .",
    "is not the only package with this goal in mind .",
    "this magazine has recently presented descriptions of star - p [ 5 ] ( a commercial package aimed at providing environments such as matlab , maple and other with seamless parallel capabilities ) and pybsp [ 6 ] ( a python library for the development of parallel programs following the bulk synchronous parallel model ) . as an open - source project , growth is to be driven by the needs and contributions of users and developers both in the astrophysical community and , possibly , in other computationally intensive disciplines .",
    "the three basic components of are the data analysis engine ( hereafter dae ) , the user interface ( hereafter ui ) and the graphics display engine ( hereafter gde ) .",
    "1 illustrates the relationships among these system components .",
    "although the entire set of functions and tools can be accessed from within a program using as a library , the primary mode of usage of is one in which a user accesses its functionality from within an interactive shell , applying the analysis and visualization tools to a `` live '' dataset during an interactive data analysis session . a python gui interface which provides `` point - and - click '' access to all the functionality is also available .",
    "the communication between the client and the dae is managed by the tools included in ipython1 .",
    "a particularly important feature of the ipython1 package is the capability of allowing multiple clients to connect to the same remote engine .",
    "taking advantage of this feature , multiple distributed users can connect to the same dae , share the same data and collaborate on a data analysis session .",
    "the ui includes functionality to keep track of and save part or the entire history of a data analysis session . for a multi - user session ,",
    "the history also includes information on which of the participating users issued each command .",
    "the dae is the core of the package and is responsible for parallel data management and all data analysis operations .",
    "it is implemented in c++/mpi-2 and wrappers are created using the boost.python library .",
    "the basic dae objects visible to the user are of data , the objects associated with them and the individual comprising these objects .",
    "for example , for data coming from a simulation , a might be a set of nested grids of density fields , or arrays of particle properties .",
    "on the other hand , for a galaxy survey , a might represent a physical region of the sky , or data taken over a particular period of time .",
    "two distinct types of distributed data are supported : describe regularly sampled data , e.g. density values on a 3-d cartesian grid from a cfd calculation . represent irregularly sampled data , such as a set of arrays representing the properties of particles in a snapshot from an @xmath0-body simulation , or the right ascensions , declinations , and redshifts of galaxies in a survey .",
    "each individual property in a  or  is stored in an which can be accessed by the user , and on which the user can perform a variety of operations .",
    "some of the the main features of are :    1 .",
    "physically distributed data stored in an are accessed by the user as if they were a single data structure on a shared - memory machine .",
    "the initial data distribution is automatically managed by the class constructor .",
    "for example , + ....",
    "x = attribute_random(1000000,seed ) .... + will generate @xmath1 random numbers uniformly distributed ( by default ) between 0 and 1 , and automatically distribute them as evenly as possible across all nodes .",
    "as in a shared - memory machine , global indexes can be used to access individual data elements .",
    "all the complexities of data distribution and retrieval in a distributed - memory cluster are handled internally and hidden to the user , who can operate as though working on a shared - memory environment .",
    "2 .   can store @xmath2-dimensional arrays of both primitive datatypes and user - defined structures and objects .",
    "all the math operators and the math library functions are overloaded to operate on in a parallel vectorized form .",
    "for example , a new radius attribute may be computed from a list of cartesian components in a completely intuitive way : + ....      > > > r = sqrt(pow(x,2)+pow(y,2 ) ) .... 4 .",
    "all relational and logical operators are similarly overloaded to operate in a parallel vectorized form on .",
    "5 .   in order to take advantage of faster access to local data",
    ", the class allows each process to locate its own block of data and , if possible , to work on just that block , thus avoiding unnecessary inter - process data transfer .",
    "we have developed libraries for several specific astrophysical subfields .",
    "for example , the stellardynamics library includes many functions normally used in the analysis of @xmath0-body simulations of galaxies and star clusters , such as the determination of the center of the stellar distribution , measurement of characteristic scales ( such as the core , half - mass , tidal , and other lagrangian radii ) , robust estimates of local densities , and the computation of radial and other profiles of physically important quantities , such as density , velocity dispersion , ellipticity , anisotropy , mass function , and so on .",
    "similarly , the cosmology library allows a user to perform operations commonly encountered in cosmological contexts  for example , read in a list of galaxy redshifts , select a cosmological model , the compute a host of model - specific properties , such as lookback time and comoving distance . a user can transform between observational parameters ( @xmath3 , @xmath4 , @xmath5 ) , to cartesian coordinates , and quickly and easily view lengthy and/or complex datasets .",
    "also contains more general libraries for important mathematical operations , such as fast fourier transforms and statistical analysis .",
    "as often as possible , we have incorporated powerful existing packages into the framework .",
    "for example , we have imported the `` fastest fourier transform in the west '' ( fftw ) package as our central fft engine .",
    "the internals handle all of the issues involved in packing and unpacking data , and keeping track of whether the data are real or complex .",
    "the user simply invokes the transform via a one - line command .",
    "much of this functionality has been developed using the dae tools for data distribution and parallel operations , with minimal , or even no explicit use of mpi .",
    "users without advanced knowledge of mpi can therefore still add new functions and create new libraries for their own and general use .",
    "for example , the function for the parallel calculation the center of mass of a stellar system [ defined as @xmath6 where @xmath7 and @xmath8 are the masses and coordinates of the stars in the stellar system ] , might be implemented within as    .... vector < double >   center_of_mass(attribute & x , attribute & y , attribute & z , attribute & m ) {      vector < double > com(3 ) ;      double m_sum = sum(m ) ;      com[0 ] = sum(m*x)/m_sum ;      com[1 ] = sum(m*y)/m_sum ;      com[2 ] = sum(m*z)/m_sum ;      return com ; } ....    data distribution , parallel execution of required operations , and consolidation of the results of operations performed on individual nodes can all be performed using distributed data and parallel analysis commands , without explicit use of mpi functions .",
    "thus users can take advantage of the libraries already developed and , thanks to the low - level tools , can in many cases expand as needed without being experts in parallel programming .",
    "the current implementation of allows users to view data using standard python plotting libraries .",
    "a number of functions interface with the popular matplotlib package and with gnuplot . however , when standard python plotting tools are used , data are transfered to the local client machine .",
    "this is done automatically by without requiring the user to issue any specific command to transfer the data but while convenient for development purposes and small datasets , this is clearly not a desirable approach for the very large datasets is intended to handle .    for the visualization of large distributed datasets ,",
    "incorporates an interface to visit [ 7 ] , a parallel visualization package developed at lawrence livermore national laboratory . as with commands",
    ", all visit visualization functions can be invoked from a remote shell or from a gui ; and visit can therefore share the same client , from which it is possible to invoke both and visit commands which , in turn , are executed on the same parallel computational engine ( the dae ) .",
    "as an example of a analysis and visualization session , figure [ fg : sdss ] shows a snapshot of a typical session within the gui , which goes from a superset of the desired data to a selected , transformed , and binned dataset .",
    "the particular dataset in question is the sloan digital sky survey ( hereafter sdss ; [ 8 ] ) , which contains over 10 tbytes of `` high - quality '' reduced data , including a sample of approximately 700,000 spectroscopically measured galaxies in the sixth data release ( dr6 ) .    in this example",
    ", we can successfully ( and simply , from the user s perspective ) analyze the sdss spectroscopic galaxy sample , performing coordinate transformations , producing a volume - limited selection criterion , iteratively estimating the power spectrum of the resulting density field , and visualizing the results , all with 25 lines of scripting code .",
    "the analysis is done entirely in parallel , and serves as a template for how scientists , in an ideal world , would like to interact with data .",
    "the gui is also shown in figure [ fg : sdss ] . in the upper left ,",
    "a user can drop and drag to rearrange a hierarchical dataset , while the lower left shows a list of variables and data structures available to the user .",
    "the lower - right shows the equivalent python session , including commands which are automatically generated by dragging and dropping data into various dialog boxes .    from the gui ,",
    "many of the commands of a data analysis session are simple drag and drop operations , saving the user the effort even of finding the correct syntax in the api .",
    "the following examples illustrate how allows a user to easily develop scripts which transparently distribute data and perform parallel operations .",
    "the buffon - laplace needle algorithm is a simple monte carlo algorithm for the approximate calculation of @xmath9 .",
    "it has been used in previous articles in this magazine ( [ 5],[6 ] ) to illustrate the use of parallel software development tools .",
    "the algorithm is based on the estimation of the probability , @xmath10 , that a needle of length @xmath11 thrown on a two - dimensional grid with cells of length @xmath12 in one direction and @xmath13 in the other direction will intersect at a least one line @xmath14 the ratio of the number of times the needle crosses the grid to the total number of trials provides a numerical estimate of @xmath15 and allows to calculate @xmath9 .",
    "the implementation of this algorithm with a script is straightforward :    ....      > > > a = 1.0      > > > b = 1.0      > > > len = 0.6      > > > numtrials = 10000000      > >",
    "> phi = 2.*pi*attribute_random(numtrials )      > > >",
    "x = a*attribute_random(numtrials)+len*cos(phi )      > > >",
    "y = b*attribute_random(numtrials)+len*sin(phi )      > >",
    "> count = sum(x<=0 | x>=a | y<=0 | y>=b )      > > > pi",
    "= numtrials / float(count)*(2*len*(a+b)-len**2)/(a*b ) ....    the speed - up of the calculation for @xmath16 trials is shown in figure  [ fg : benchmark ] .     with @xmath16 trials ( filled dots ) and for the direct force calculation in a @xmath0-body system with @xmath17 particles ( crosses ) discussed in section 3 .",
    "the dashed line shows the ideal linear speed - up .",
    "the benchmark was performed on a beowulf cluster with nodes each containing 2 amd 1.5 ghz chips .",
    "nodes are connected via a channel - bonded 100 mbit / s ethernet switch.,height=264 ]      @xmath0-body simulations are key computational tools for studying the formation and evolution of stellar systems ranging from the relatively small open and globular star clusters to much larger objects , such as galaxies and galaxy clusters .",
    "depending on the type of stellar system and the time span of the simulation , many sophisticated algorithms have been devised to accelerate the calculation of the basic gravitational acceleration of a star due to all other stars in the system .",
    "however , the approximations introduced to speed up the calculation are not always adequate , and the computationally very expensive direct - force calculation is in many cases still the only viable option .    the parallel calculation of the gravitational acceleration on a star with position @xmath18 due to all the other stars with mass @xmath19 and position @xmath20 in a stellar system ( including softening ) @xmath21 is straightforward in /python , as illustrated by the following code fragment :    .... > > >",
    "xi = x[i ] > > > yi = y[i ] > > > zi = z[i ] > > > d = g*m / pow(eps2+(pow(x - xi,2)+pow(y - yi,2)+pow(z - zi,2)),1.5 ) > > > ax = sum((x - xi)*d ) > > >",
    "ay = sum((y - yi)*d ) > > > az = sum((z - zi)*d ) ....    each process calculates the contribution to the acceleration due only to stars stored locally .",
    "access to the coordinates of the star on which the acceleration is being calculated is straightforward , and the user does not need to perform any complex operation , or explicitly issue any mpi command , in order to locate the node on which the coordinates are stored or broadcast that information to other nodes .",
    "any element of an can be accessed through the bracket operator as on a shared memory machine .",
    "data distribution and remote access , parallel vectorized operations , reduction operations and interprocess communication , are all handled by the tools and are completely hidden to the user .",
    "is still under active development , and will be expanded in numerous ways in the near future .",
    "currently , the main areas of development are :    * benchmarking and scalability .",
    "we are currently studying the performance and the scalability of on the nsf teragrid superclusters with thousands of processors . * for graphics processing units ( gpus ) .",
    "general programming on gpus is becoming increasingly popular , in large part due to the nvidia cuda language , which provides programmers with a tool to more easily harness the enormous computational power of gpus .",
    "we are currently in the process of porting all atomic basin operations to the gpu architecture .",
    "the machine we envisage for the gpu - enabled is a beowulf cluster with each node hosting one or more gpus .",
    "data will still be distributed over all the nodes of a cluster , but the atomic operations performed internally by each node will be carried out on , or accelerated by , the gpu .",
    "the programming model therefore has two level of parallelism : the standard cluster - level parallelism , with data distributed by the mpi - based tools , and internal parallelism on each node , in which the capabilities of gpus are used to further accelerate assigned operations . * beyond mpi . all parallel functions and",
    "distributed data structures in are implemented using mpi .",
    "however , new languages and extensions to existing languages are currently being developed to facilitate the development of parallel codes .",
    "specifically , partitioned global address space languages ( see e.g. [ 9 ] ) such as co - array fortran , unified parallel c ( upc ) , and titanium seem to provide interesting alternatives to mpi - based code .",
    "we plan to build a upc - based implementation of all the low - level distributed data structures , to compare its performance and scalability with the mpi version . * beyond astrophysics .",
    "although the libraries currently available are focussed on the analysis of astrophysical data , all the tools for parallel operations and data distribution are completely general and can be used to develop packages relevant to other disciplines .",
    "we plan to expand and broaden its capabilities by collaborating with scientists in other fields , such computational biology and information sciences , involving computationally intensive data analysis and simulations .",
    "* integration with other science packages .",
    "we plan to integrate with the multiscale multiphysics simulation environment ( muse ) , a framework for the numerical study of the multi - scale physics of dense stellar systems .",
    "the result will be a single environment in which a user can run novel numerical simulations , including a wide variety of physical processes , and , within the same platform , perform parallel analysis and visualization of the results , in real time or post - production .",
    "we thank all the members of the basin team ( b. char , d. cox , a. dyszel , j. haaga , m. hall , l. kratz , s. levy , p. macneice , e. mamikonyan , m. soloff , a. tyler , m. vogeley and b. whitlock ) for many discussions and comments on the issues presented in this paper .",
    "[ 1 ] becker , d.j . ,",
    "sterling , t.j . , savarese , d.f . ,",
    "dorband , j.e . ,",
    "ranawak , u.a . , & packer , c.v . , 1995 , proceedings of the international conference on parallel processing + [ 2 ] blondin , j.m . ,",
    "`` discovering new dynamics of core - collapse supernova shock waves '' , 2005 , journal of physics : conference series 16 370 + [ 3 ] norman , m.l . , bryan , g.l . ,",
    "harkness , r. , bordner , j. , reynolds , d. , oshea , b. & wagner , r. , `` simulating cosmological evolution with enzo '' to appear in petascale computing : algorithms and applications , ed .",
    "d. bader , crc press llc ( 2007 ) , arxiv:0705.1556v1 + [ 4 ] springel v. et al .",
    "2005 , `` simulations of the formation , evolution and clustering of galaxies and quasars '' , nature 435 629 + [ 5 ] s. raghunathan , `` making a supercomputer do what you want : high - level tools for parallel programming '' , _ computing in science and engineering _ , vol .",
    "8 , no . 5 , 2006 , pp.70 - 80 + [ 6 ] k. hinsen , `` parallel scripting with python '' , _ computing in science and engineering _ , vol . 9 , no . 6 , 2007 , pp.82 - 89 + [ 7 ] childs h. , brugger e. , bonnell k. , meredith j. , miller m. , whitlock b. , max n. : a contract based system for large data visualization . in proc",
    "of visualization 2005 conference .",
    "191 + [ 8 ] d. york , et al .",
    ", `` the sloan digital sky survey : technical summary '' , _ astronomical journal _ , vol .",
    "120 , 2000 , p. 1579",
    "+ [ 9 ] carlson , b. , el - ghazawi , t. , numrich , r. , yelick k. , 2003 , `` programming in the partitioned global assdress space model '' , https://upc-wiki.lbl.gov/upc/images/b/b5/pgas _ tutorial",
    "_ sc2003.pdf +"
  ],
  "abstract_text": [
    "<S> the advent of affordable parallel computers such as beowulf pc clusters and , more recently , of multi - core pcs has been highly beneficial for a large number of scientists and smaller institutions that might not otherwise have access to substantial computing facilities . </S>",
    "<S> however , there has not been an analogous progress in the development and dissemination of parallel software : scientists need the expertise to develop parallel codes and have to invest a significant amount of time in the development of tools even for the most common data analysis tasks . </S>",
    "<S> we describe the beowulf analysis symbolic interface ( basin ) a multi - user parallel data analysis and visualization framework . </S>",
    "<S> basin is aimed at providing scientists with a suite of parallel libraries for astrophysical data analysis along with general tools for data distribution and parallel operations on distributed data to allow them to easily develop new parallel libraries for their specific tasks .    </S>",
    "<S> submitted for publication to _ computing in science and engineering _ + special issue on computational astrophysics </S>"
  ]
}