{
  "article_text": [
    "the importance of large - scale machine learning continues to grow in concert with the big data boom , the advances in learning techniques , and the deployment of systems that enable wider applications .",
    "as the amount of data scales up , the need to harness increasingly large clusters of machines significantly increases . in this paper",
    ", we address a question that is fundamental for applying today s loosely - coupled `` scale - out '' cluster computing techniques to important classes of machine learning applications :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ how to spread data and model parameters across a cluster of machines for efficient processing ? _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _        one big challenge for large - scale data processing problems is to distribute the data over processing nodes to fit the computation and storage capacity of each node . for instance , for very large scale graph factorization  @xcite",
    ", one needs to partition a natural graph in a way such that the memory , which is required for storing local state of the partition and caching the adjacent variables , is bounded within the capacity of each machine .",
    "similar constraints apply to graphlab  @xcite , where vertex - specific updates are carried out while keeping other variables synchronized between machines . likewise ,",
    "in distributed inference for graphical models with latent variables  @xcite , the distributed state variables must be synchronized efficiently between machines .",
    "furthermore , general purposed distributed machine learning framework such as the parameter server  @xcite face similar issues when it comes to data and parameter layout .",
    "shared parameters are synchronized via the communication network .",
    "the sheer number of parameters and the iterative nature of machine learning algorithms often produce huge amounts of network traffic .",
    "figure  [ fig : traffic_vs_datasize ] shows that , if we randomly assign data ( documents ) to machines in a text classification application , the total amount of network traffic is 100 times larger than the size of training data .",
    "specifically , almost 4 tb parameters are communicated for 300 gb training data .",
    "given that the network bandwidth is typically much smaller than the local memory bandwidth , this traffic volume can potentially become a performance bottleneck .",
    "there are three key challenges in achieving scalability for large - scale data processing problems :    limited computation ( cpu ) per machine : : :    therefore we need a well - balanced task distribution over machines . limited memory ( ram ) per machine : : :    the amount of storage per machine available for processing and caching    model variables is often constrained to a small fraction of the total    model .",
    "limited network bandwidth : : :    the network bandwidth is typically 100 times worse than the local    memory bandwidth . thus we need to reduce the amount of communication    between machines .",
    "one key observation is the sparsity pattern in large scale datasets : most documents contain only a small fraction of distinct words , and most people have only a few friends in a social graph .",
    "such nonuniformity and sparsity is both a boon and a challenge for the problem of dataset partitioning . due to its practical importance , even though the dataset partitioning problems are often np hard  @xcite , it is still worth seeking practical solutions that outperform random partitioning , which typically leads to poor performance .    * our contributions : * in this paper",
    ", we formulate the task of data and parameter placement as a graph partitioning problem .",
    "we propose _ _ , a parallel submodular approximation algorithm for solving this problem , and we analyze its theoretical guarantees",
    ". a straightforward implementation of the algorithm has running time in the order of @xmath0 is the number of edges in the graph . using an efficient vertex selection data structure , we provide an efficient implementation with time complexity @xmath1 .",
    "we also discuss the techniques including sampling , initialization and parallelization to improve the partitioning quality and efficiency .",
    "experiments on text datasets and social networks of various scales show that , on both partition quality and time efficiency , outperforms state - of - the - art methods , including metis @xcite , patoh @xcite and zoltan @xcite",
    ". can also significantly accelerate the ps .",
    ", a state - of - the - art general purpose distributed machine learning , on data of hundreds gbs size and with billions parameters .",
    "in this section , we first introduce the inference problem and the model of dependencies in distributed inference .",
    "then we provide the formulation of the data partitioning problem in distributed inference .",
    "we also present a brief overview of related work in the end .      in machine learning ,",
    "many inference problems have graph - structured dependencies .",
    "for instance , in risk minimization  @xcite , we strive to solve @xmath2 : = \\sum_{i=1}^m l(x_i , y_i , w ) + \\omega[w],\\end{aligned}\\ ] ] where @xmath3 is a loss function measuring the model fitting error in the data @xmath4 , and @xmath5 $ ] is a regularizer on the model parameter @xmath6 . the data and parameters are often correlated only via the nonzero terms in @xmath7 , which exhibit sparsity patterns in many applications . for example , in email spam filtering , elements of @xmath7 correspond to words and attributes in emails , while in computational advertising , they correspond to words in ads and user behavior patterns .    for undirected graphical models",
    "@xcite , the joint distribution of the random variables in logscale can be written as a summation of potential of all the cliques in the graph , and each clique potential @xmath8 only depends on the subset of variables @xmath9 in the clique @xmath10 .",
    "the learning and inference problems in undirected graphical models are often formulated as an optimization problem in the following form : @xmath11 : = \\sum_{c \\in \\ccal } \\psi_c(w_c),\\end{aligned}\\ ] ] where local variables interact through the model parameters @xmath9 of the cliques .",
    "similar problems occur in the context of inference on natural graphs @xcite , where we have sets of interacting parameters represented by vertices on the graph , and manipulating a vertex affects all of its neighbors computationally .    ]      the dependencies in the inference problems above can be modeled by a bipartite graph @xmath12 with vertex sets @xmath13 and @xmath14 and edge set @xmath15 .",
    "we denote the edge between two node @xmath16 and @xmath17 by @xmath18 . figure  [ fig : dep ] illustrates the case of risk minimization , where @xmath13 consists of the samples @xmath19 and @xmath14 consists of the parameters in @xmath6 .",
    "there is an edge @xmath20 if and only if the @xmath21-th element of @xmath7 is non - zero .",
    "therefore , @xmath22 is the working set of elements of @xmath6 for evaluating the loss function @xmath23 on the sample @xmath4",
    ".    we can construct such bipartite graph @xmath24 to encode the dependencies in undirected graphical models and natural graphs with node set @xmath14 and edge set @xmath15 .",
    "one construction is to define @xmath25 , and add an edge @xmath26 to the edge set @xmath27 if they are connected in the original graph .",
    "an alternative construction is to define the node set @xmath28 to be @xmath29 , the set of all cliques of the original graph , and add an edge @xmath30 to the edge set @xmath27 if node @xmath31 belongs to the clique @xmath10 in the original graph .    throughout the discussion",
    ", we refer to @xmath13 as the set of data ( examples ) nodes and @xmath14 as the set of parameters ( results ) nodes .",
    "the challenge for large scale inference is that the size of the optimization problem in is too large , and even the model @xmath6 may be too large to be stored on a single machine .",
    "one solution is to divide exploit the additive form of @xmath32 $ ] to decompose the optimization into smaller problems , and then employ multiple machines to solve these sub - problems while keeping the solutions ( parameters ) consistent .",
    "there exist several frameworks to simplify the developing of efficient distributed algorithms , such as hadoop  @xcite and its in - memory variant spark  @xcite to execute mapreduce programs , and graphlab for distributed graph computation  @xcite . in this paper ,",
    "we focus on the ps .",
    "framework  @xcite , a high - performance general - purpose distributed machine learning framework .    in the ps .",
    "framework , computational nodes are divided into server nodes and worker nodes , which are shown in figure  [ fig : ps_arch ] .",
    "the globally shared parameters @xmath6 are partitioned and stored in the server nodes . each worker node solves a sub - problem and communicates with the server nodes in two ways : to push local results such as gradients or parameter updates to the servers , and to pull recent parameter ( changes ) from the servers . both push and pull are executed asynchronously .       and @xmath14 , respectively .",
    "the inter - machine dependencies ( edges ) are highlighted and the communication costs for these three machines are 2 , 3 , and 3 , respectively . moving the 3rd vertex in @xmath14 to either machine 0 or 1",
    "reduces cost .",
    "[ fig : ps_partition ] ]    in distributed inference , we divide the problem in by partition the cost function @xmath32 $ ] as well as the associated dependency graph into @xmath33 blocks . without loss of generality",
    "we consider a ps . with @xmath33 server nodes and @xmath33 worker nodes , and",
    "each machine has exactly one server and one worker ( otherwise we can aggregate multiple nodes in the same machines without affecting the following analysis ) . for the bipartite dependency graph @xmath12",
    ", we partition the parameter set @xmath14 into @xmath33 parts and assign each part to a server node , and we partition the data set @xmath13 into @xmath33 parts and assign them to individual worker nodes . figure  [ fig : ps_partition ] illustrates an example for @xmath34 . more specifically , we want to divide both @xmath13 and @xmath14 into @xmath33 non - overlapping parts @xmath35 and assign the part @xmath36 and @xmath37 to the worker node and server node on machine @xmath38 respectively",
    ".    there are three goals when implementing the graph partitioning : * balancing the computational load .",
    "* we want to ensure that each machine has approximately the same computational load .",
    "assume that each example @xmath39 incurs roughly the same workload , then one of the objective to keep @xmath40 small : @xmath41 * satisfying the memory constraint .",
    "* inference algorithms frequently access the parameters ( at random ) .",
    "workers keep these parameters in memory to improve performance , yet ram is limited .",
    "denote by @xmath42 the neighbor set of @xmath39 @xmath43 then @xmath44 is the working set of the parameters worker @xmath38 needed . for simplicity",
    "we assume that each parameter @xmath45 has the same storage cost .",
    "our goal to limit the worker s memory footprint is given by @xmath46 * minimizing the communication cost .",
    "* the total communication cost per worker @xmath38 is @xmath47 , which is already minimized using our previous goal . to further reduce this cost",
    ", we can assign server @xmath38 to the same machine with worker @xmath38 , so that any communication uses memory rather than network .",
    "this reduces the inter - machine communication cost to @xmath48 .",
    "figure  [ fig : ps_partition ] shows an example",
    ". further note that if @xmath45 is not needed by worker @xmath38 , then server @xmath38 should never maintain @xmath45 .",
    "in other words , we have @xmath49 and the cost simplifies to @xmath50 .    on the other hand",
    ", @xmath51 is the communication cost of server @xmath38 because other workers must request parameters from server @xmath38 .",
    "therefore , the goal to minimize the maximal communication cost of a machine is @xmath52      graph partitioning has attracted much interest in scientific computing @xcite , scaling out large - scale computations @xcite , graph databases @xcite , search and social network analysis @xcite , and streaming processing @xcite .",
    "most previous work , such as metis @xcite , is concerned with edge cuts .",
    "only a few of them solve the vertex cut problem , which is closely related to this paper , to directly minimize the network traffic .",
    "patoh @xcite and zoltan @xcite used multilevel partitioning algorithms related to metis , while powergraph @xcite adopted a greedy algorithm .",
    "very recently @xcite studied the relation between edge cut and vertex cut .",
    "different to these works , we propose a new algorithm based on submodular approximation to solve the vertex - cut partitioning problem .",
    "we give theoretical analysis of the partition quality , and describe an efficient distributed implementation .",
    "we show that the proposed algorithm outperforms the state of the art on several large scale real datasets in both in terms of quality and speed .",
    "in this section , we present our algorithm parsa for solving the partitioning problem with multiple objectives in , and .",
    "note that is equal to a @xmath33-way graph partition problem on vertex set @xmath13 with vertex - cut as the merit .",
    "this problem is np - complete @xcite .",
    "furthermore , is more complex because of the involvement of @xmath14 . rather than solving all these objectives together",
    ", parsa decomposes this problem into two tasks : partition the data @xmath13 by solving and , and given the partition of @xmath13 partition the parameters @xmath14 by solving .",
    "intuitively , we first assigns data workers to balance the cpu load and minimize the memory footprint , and then distribute the parameters over servers to minimize inter - machine communication .",
    "graph @xmath53 , # partitions @xmath33 , maximal # iterations @xmath54 , residue @xmath55 , and improvement @xmath56 partitions of @xmath57 @xmath58 define @xmath59 @xmath60 * then break * [ algo : select_r ] find @xmath61 draw @xmath62 by choosing @xmath63 with probability @xmath64 @xmath65 * then next * [ algo : solv_t ] solve @xmath66 [ algo : updt_si ] * if * @xmath67 * then * @xmath68 and @xmath69 fail evenly assign the remainder @xmath13 to @xmath36    note that @xmath70 is a set function in the variable @xmath13 .",
    "it is a _ submodular _ function similar to convex and concave functions in real variables . although the problem in is np - complete , there exist several algorithms to solve it approximately by exploiting the submodularity @xcite . in our algorithm , we modified @xcite to solve and .",
    "the key difference is that we build up the sets @xmath36 incrementally , which is important for both partition quality and computational efficiency at a later stage .",
    "as shown in algorithm  [ algo : prototype ] , the algorithm proceeds as follows : in each round we pick the smallest partition @xmath36 and find the best set of elements to add to it . to do so , we first draw a small subset of candidates @xmath71 and select the best subset using a minimum - increment weight via @xmath72 . if the optimal solution @xmath73 satisfies @xmath74 , i.e. , the cost for increasing @xmath36 is not too large , we assign @xmath73 to partition @xmath36 .    before showing the implementation details in section  [ sec : impl ] , we first analyze the partitioning quality of algorithm  [ algo : prototype ] .    [ prop : bound ] assume that there exists some partitioning @xmath75 that satisfies @xmath76 .",
    "let @xmath77 , @xmath78 , @xmath79 and @xmath80 .",
    "then algorithm  [ algo : prototype ] will succeed with probability at least @xmath81 and it will generate a feasible solution with partitioning cost at most @xmath82 .    the proof is near - identical as @xcite .",
    "note that we overload the meaning of @xmath13 as it refers to the remaining variables in the algorithm .    for a given iteration , without loss of generality",
    "we assume that @xmath83 maximizes @xmath84 for all @xmath21 .",
    "denote this by @xmath85 . since @xmath73 is the optimal solution at the current iteration we have @xmath86 .",
    "further note that by monotonicity and submodularity @xmath87 .",
    "moreover , @xmath88 holds since @xmath13 contains only the leftovers .",
    "consequently @xmath89 .",
    "finally , the algorithm only increases the size of @xmath36 whenever the cost is balanced .",
    "hence @xmath90 . combining this yields @xmath91 using the results from the proof of ( * ? ? ?",
    "* theorem 5.4 ) we know that @xmath92 and therefore @xmath93 happens with probability at least @xmath94 . hence the probability of removing at least one vertex from @xmath13 within an iteration is greater than @xmath94 .",
    "chernoff bounds show that after @xmath95 iterations the algorithm will terminate with probability at least @xmath81 since the residual @xmath13 is small , i.e. ,  @xmath96 .",
    "the algorithm will never select a @xmath36 for augmentation unless @xmath97 ( there would always be a smaller set ) .",
    "moreover , the maximum increment at any given time is @xmath98 . hence @xmath99 and therefore @xmath100 .    finally , the contribution of the unassigned residual @xmath13 is at most @xmath101 since each @xmath36 is incremented by at most @xmath55 elements and since @xmath102 for all @xmath63 . in summary , this yields @xmath103 .",
    "the neighbor sets @xmath104 partitions @xmath105 @xmath106 @xmath107",
    "@xmath108 @xmath109 @xmath110    next , given the partition of @xmath13 , we find an assignment of parameters in @xmath14 to servers .",
    "we reformulate as a convex integer programming problem with totally unimodular constraints @xcite , which is then solved using a sequential optimization algorithm performing a sweep through the variables .",
    "we define index variables @xmath111 , @xmath112 to indicate which server node maintains a particular parameter @xmath113 .",
    "they need to satisfy @xmath114 .",
    "moreover , denote by @xmath115 variables that record whether @xmath116",
    ". then we can rewrite as a convex integer program :    [ eq : intprog ] @xmath117    here we exploited the fact that @xmath118 and that @xmath119 .",
    "these constraints are totally unimodular , since they satisfy the conditions of @xcite . as a consequence",
    "every vertex solution is integral and we may relax the condition @xmath111 to @xmath120 $ ] to obtain a convex optimization problem .    algorithm  [ algo : part_v ] performs a single sweep over to find a locally optimal assignment of one variable at a time .",
    "we found that it is sufficient for a near - optimal solution .",
    "repeated sweeps over the assignment space are straightforward and will improve the objective until convergence to optimality in a _",
    "finite _ number of steps : due to convexity all local optima are global .",
    "further note that we need not store the full neighbor sets in memory .",
    "instead , we can perform the assignment in a streaming fashion .",
    "the time complexity of algorithm  [ algo : part_v ] is @xmath121 , however , it could be @xmath122 for algorithm  [ algo : prototype ] , which is infeasible in practice .",
    "we now discuss how to implement algorithm  [ algo : prototype ] efficiently .",
    "we first present how to find the optimal @xmath73 and sample @xmath71 .",
    "then we address the parallel implementation with the ps .",
    ", and finally describe neighbor set initialization to improve the partition quality .      graph @xmath12 , # partitions @xmath33 , and initial neighbor sets @xmath124 partitioned @xmath57 and updated neighbor sets which are equal to @xmath125 @xmath58 @xmath63 * do * @xmath126 pick partition @xmath127 [ step : top ] pick the lowest - cost vertex @xmath128 assign @xmath129 to partition @xmath38 : @xmath130 remove @xmath129 from @xmath13 : @xmath131 [ step : remove ] * for * @xmath132 * do * remove @xmath129 from @xmath133 @xmath134 [ step : cost ] [ step : update ] * for * @xmath135 * do * @xmath136    -th entry is used for vertex @xmath39 , where assigned vertices are marked with gray color .",
    "header points and a doubly - linked list afford faster access .",
    "[ fig : array ] ]    the most expensive operation in the inner loop of algorithm  [ algo : prototype ] is step  [ algo : solv_t ] , determining which vertices , @xmath73 , to add to a partition .",
    "submodular minimization problems incur @xmath137 time @xcite .",
    "given the fact that this step is invoked frequently and the problem is large , this strategy is impractical .",
    "a key approximation parsa made is to add only a single vertex at a time instead of a set of vertices : given a vertex set @xmath71 and partition @xmath38 , it finds vertex @xmath129 that minimizes @xmath138 an additional advantage of this approximation is that we are now solving exactly the cpu load balancing problem .",
    "since we only assign one vertex at a time to the smallest partition , we obtain perfect balancing .    even though this approximation improves the performance ,",
    "a naive way to calculate is to compute all @xmath139 to find @xmath129 with the minimal value for each iteration .",
    "if the size of @xmath71 is a constant fraction of the entire graph , this leads to an undesirable time complexity of @xmath140 .",
    "this remains impractical for graphs with billions of vertices and edges .",
    "we accelerate computation as follows : we store all vertex costs to avoid re - computing them , and we create a data structure to locate the lowest - cost vertex efficiently .    * storing vertex costs . *",
    "if we subtract the constant @xmath141 from @xmath139 , we obtain the vertex cost @xmath142 this is the number of new vertices that would be added to the neighbor set of partition @xmath38 due top adding vertex @xmath143 to @xmath38 .    when adding @xmath143 to a partition @xmath38 , only the costs of a few vertices will be changed .",
    "denote by @xmath144 the set of new vertices will be added into the neighbor set of @xmath36 when assigning @xmath143 to partition @xmath38 . only vertices in @xmath13 connected to vertices in @xmath145 will have their costs affected , and these costs will only be reduced and never increased . due to the sparsity of the graph ,",
    "this is often a small subset of the total vertices .",
    "hence the overhead of updating the vertex costs is much smaller than re - computing them repeatedly .",
    "* fast vertex cost lookup .",
    "* we build an efficient data structure to store the vertex costs , which is illustrated at figure  [ fig : array ] . for each partition @xmath38",
    ", we use an array @xmath146 to store the @xmath21-th vertex cost , @xmath147 , in the @xmath21-th entry , denoted by @xmath148 .",
    "we then impose a doubly - linked list on top of this array in an increasing order to rapidly locate the lowest - cost vertex .",
    "when a vertex cost is modified ( always reduced ) , we update the doubly - linked list to preserve the order .",
    "note that most large - scale graphs have a power - law degree distribution .",
    "therefore a large portion of vertex costs will be small integers , which are always less equal that their degrees .",
    "we store a small array of `` head '' pointers to the locations in the list where the cost jumps to @xmath149 .",
    "the pointers accelerate locating elements in the list when updating . in practice",
    ", we found @xmath150 covers over 99% of vertex costs .",
    "the algorithm is illustrated in algorithm  [ algo : assign_a_blk ] .",
    "the inputs are a bipartite graph @xmath151 , the number of partitions @xmath33 , together with @xmath33 sets @xmath152 , which is the union neighbor set of vertices have been assigned to partition @xmath38 before .",
    "the outputs are the @xmath33 partitions @xmath153 and updated @xmath154 with the neighbor set of @xmath36 included . here",
    "we assume @xmath155 , the sampling strategy of @xmath71 will be addressed in next section .",
    "* runtime . *",
    "the initial @xmath156 can be computed in @xmath157 time and then be ordered in @xmath158 by counting sort , as they are integers , upper bounded by the maximal vertex degree .",
    "the most expensive part of algorithm  [ algo : assign_a_blk ] is updating @xmath146 in step  [ step : update ] .",
    "this is are evaluated at most @xmath159 times because , for each partition , a vertex @xmath17 together with its neighbors is accessed at most once .    for most cases , the time complexity of updating the doubly - linked lists",
    "is @xmath160 .",
    "the cost to access the @xmath21-th vertex is @xmath160 due to the sequential storing on an array .",
    "finding a vertex with the minimal value or removing a vertex from the list is also in @xmath160 time because of the doubly links . keeping the list ordered after decreasing",
    "a vertex cost by @xmath161 is @xmath160 in most cases ( @xmath158 for the worst case ) , as discussed above , by using the cached head pointers .",
    "the average time complexity of algorithm  [ algo : assign_a_blk ] is then @xmath162 , much faster than the naive implementation and orders of magnitude better than algorithm  [ algo : prototype ] .",
    "one goal of the sampling strategy used in algorithm  [ algo : prototype ] is to keep the partitions of @xmath13 balanced , because the vertices assigned to a partition at a time is being limited .",
    "the additional constraint @xmath163 introduced in the previous section ensures that only a single vertex is assigned each time , which addresses balancing .",
    "consequently we would like to sample as many vertices as possible to enlarge the search range of the optimal @xmath129 to partition quality .",
    "sampling remains appealing since it is a trade - off between computation efficiency and partition quality .",
    "first randomly divides @xmath13 into @xmath164 blocks .",
    "it next constructs the corresponding @xmath164 subgraphs by adding the neighbor vertices from @xmath14 and the corresponding edges , and then partitions these subgraphs sequentially by algorithm  [ algo : assign_a_blk ] .",
    "in other words , denote by @xmath165 the @xmath164 subgraphs , and @xmath124 the initialized neighbor sets , for instances @xmath166 for all @xmath38 ; at iteration @xmath167 , we sequentially feed @xmath168 and @xmath154 s into algorithm  [ algo : assign_a_blk ] to obtain the partitions @xmath169 of @xmath168 and updated @xmath154 s , which contain the previous partition information .",
    "then we union the results on each subgraph to the final partitions of @xmath13 by @xmath170 for @xmath171 .",
    "compared to the scheme described in section  [ sec : partition - u ] which samples a new subgraph ( @xmath71 ) for each single vertex assignment , fixes those subgraphs at the beginning .",
    "this sampling strategy has several advantages .",
    "first , it partitions a subgraph by directly using algorithm  [ algo : assign_a_blk ] , which takes advantage of the head pointers and linked list to improve the efficiency .",
    "next , it is convenient to place both the initialization of neighbor sets and parallelization which will be introduced soon on subgraph granularity .",
    "finally , this strategy is i / o efficient , because we must only keep the current subgraph in memory . as a result , it is possible to partition graphs of sizes much larger than physical memory .",
    "the number of subgraphs @xmath164 is a trade - off between partition quality and computational efficiency . in the extreme case of @xmath172 ,",
    "the vertex assigned to a partition is the optimal one from all unassigned vertices .",
    "it is , however , the most time consuming . in contrast , though the time complexity reduces to @xmath157 when letting @xmath173 , we only get random partition results .",
    "therefore , a well - chosen size @xmath164 not only removes the graph size constraint but also balances time and quality .",
    "although parsa can partition very large graphs with a single process by taking advantage of sampling , parallelization is desirable because of the reduction of both cpu and i / o times on each machine .",
    "parsa parallelizes the partitioning by processing different subgraphs in parallel ( on different nodes ) by using the shared neighbor sets .    to implement the algorithm using the ps .",
    ", we need the following three groups of nodes :    the scheduler : :    issues partitioning tasks to workers and monitors their progress .",
    "server nodes : :    maintain the global shared neighbor sets .",
    "they process push and pull    requests by workers .",
    "worker nodes : :    partition subgraphs in parallel .",
    "every time a worker first reads a    subgraph from the ( distributed ) file system .",
    "it then pulls the newest    neighbor sets associated with this subgraph from the servers .",
    "then , it    partitions this subgraph using algorithm  [ algo : assign_a_blk ] and    finally pushes the modified neighbor sets to the servers .",
    "the neighbor sets play a similar role as cluster centers on clustering methods , both of which affect the assignment of vertices .",
    "well - initialized neighbor sets potentially improve the partition results .",
    "initialization by empty sets , which prefers assigning vertices with small degrees first , however , often helps little , or even degrades , the resulting assignment .",
    "parsa uses several initialization strategies to improve the results :    individual initialization .",
    ": :    given a graph that has been divided into @xmath164 subgraphs , we    can runs @xmath174 iterations where the results for the first    @xmath175 iterations are used for initialization . in other    words , before processing the @xmath176-th subgraph ,    @xmath177 , we reset the neighbor set by @xmath178 , where @xmath179 are the    partitions of @xmath21-th subgraph .",
    "the old results are dropped    because otherwise a vertex @xmath143 will be assigned to its old    partition @xmath38 again as @xmath154 contains the    neighbors of @xmath143 and the cost    @xmath180 will then be 0 .",
    "global initialization .",
    ": :    in parallel partitioning , before starting all workers , we first sample    a small part from the graph and then let one worker partition this    small subgraph .",
    "then we can use the resulting neighbor sets as an    initialization to all workers .",
    "incremental partitioning .",
    ": :    in this setting , data arrives in an incremental way and we want to    partition the new data efficiently . since we already have the    partitioning results on the old data , we can use these results as    initialization of the neighbor sets .",
    "algorithm  [ algo : parallel ] shows parsa , which partitions @xmath13 into @xmath33 parts in parallel .",
    "then we can assign @xmath14 using algorithm  [ algo : part_v ] if necessary .",
    "the initial neighbor sets can be obtained from global initialization or incremental partitioning discussed in the previous section .",
    "there are several details worth noting : first , while communication in the ps .",
    "is asynchronous , parsa imposes a maximal allowed delay @xmath181 to control the data consistency .",
    "second , the worker might only push the changes of the neighbor sets to the servers to save the communication traffic . finally , a worker may start a separate data pre - fetching thread to run steps 3 , 4 and 5 to improve the efficiency .",
    "graph @xmath53 , initial neighbor sets @xmath182 , # partitions @xmath33 , max delay @xmath181 , initialization from @xmath175 , # subgraphs @xmath164 .",
    "partitions @xmath57    * scheduler : *    divide @xmath53 into @xmath164 subgraphs ask all workers to partition with @xmath183 ask all workers to partition with @xmath184    * server : *    start with a part of @xmath182 reply with the requested neighbor set @xmath182 @xmath185 for @xmath186 @xmath187 for @xmath186    * worker : *    receive hyper - parameters @xmath188 load a subgraph @xmath12 wait until all pushes before time @xmath189 finished the part of neighbor sets , @xmath182 , that contained in @xmath14 from the servers get partitions @xmath190 and updated neighbor sets @xmath191 using algorithm  [ algo : assign_a_blk ] @xmath192 for all @xmath193 @xmath194 to servers initializing * then * @xmath195 for all @xmath38",
    "we chose 7 datasets of varying type and scale , as summarized in table  [ tab : dataset ] .",
    "the first three are text datasets ; live - journal and orkut are social networks ; and the last two are click - through rate datasets from a large internet company .",
    "the numbers of vertices and edges range from @xmath196 to @xmath197 .",
    ".a collection of real datasets .",
    "[ cols=\"<,>,>,>,>\",options=\"header \" , ]",
    "this paper presented a new parallel vertex - cut graph partition algorithm , , to solve the data and parameter placement problem .",
    "our contributions are the following :    * we give theoretical analysis and approximation guarantees for both decomposition stages of what is generally an np hard problem .",
    "* we show that the algorithm can be implemented very efficiently by judicious use of a doubly - linked list in @xmath198 time . *",
    "we provide technologies such as sampling , initialization , and parallelizaiton , to improve the speed and partition quality .",
    "* experiments show that works well in practice , beating ( or matching ) all competing algorithms in _ both _ memory footprint and communication cost while also offering very fast runtime .",
    "* we used to accelerate a stat - of - the - art distributed solver for @xmath199-regularized logistic regression implemented in ps . .",
    "we observed a 1.6x speedup on 16 machines with a dataset containing 10 billion nonzero entries .",
    "v.  venkataramani , z.  amsden , n.  bronson , g.  cabrera  iii , p.  chakka , p.  dimov , h.  ding , j.  ferris , a.  giardullo , j.  hoon .",
    "tao : how facebook serves the social graph . in _",
    "acm sigmod _ , pages 791792 .",
    "acm , 2012 ."
  ],
  "abstract_text": [
    "<S> distributed computing excels at processing large scale data , but the communication cost for synchronizing the shared parameters may slow down the overall performance . fortunately , the interactions between parameter and data in many problems are sparse , which admits efficient partition in order to reduce the communication overhead . in this paper , we formulate data placement as a graph partitioning problem . we propose a distributed partitioning algorithm . </S>",
    "<S> we give both theoretical guarantees and a highly efficient implementation . </S>",
    "<S> we also provide a highly efficient implementation of the algorithm and demonstrate its promising results on both text datasets and social networks . </S>",
    "<S> we show that the proposed algorithm leads to 1.6x speedup of a state - of - the - start distributed machine learning system by eliminating 90% of the network communication . </S>"
  ]
}