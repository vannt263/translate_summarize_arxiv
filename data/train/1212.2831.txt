{
  "article_text": [
    "let @xmath0 be a finite state irreducible and aperiodic markov chain ( mc ) with transition probability matrix @xmath1 whose elements are the transition probabilities @xmath2    this mc admits a stationary distribution @xmath3 , which is the unique solution of @xmath4 the entropy rate @xmath5 is a measure of the average entropy growth of a sequence generated by the process @xmath0 and is defined as @xmath6 for the particular case of an irreducible and aperiodic mc , the limit above is equal to @xcite @xmath7 where @xmath8 denotes the @xmath9 row of @xmath1 and where @xmath10 is the _ local entropy _ of state @xmath11 .",
    "note that , throughout this paper , we use @xmath12 as a shorthand for the markov chain whose transition probability matrix is @xmath1 .",
    "we follow the setting of  @xcite closely .",
    "we define a _ random trajectory _ @xmath13 of a mc as a path with initial state @xmath14 , final state @xmath15 , and no intermediate state @xmath15 , i.e. , the trajectory is terminated as soon as it reaches state @xmath15 . using the markov property",
    ", we express the probability of a particular trajectory @xmath16 given that @xmath17 as @xmath18 let @xmath19 be the set of all trajectories that start at state @xmath14 and end as soon as they reach state @xmath15 . as",
    "the mc defined by the matrix @xmath1 is finite and irreducible , we have @xmath20 so the discrete random variable @xmath13 has as support the set @xmath19 , with the probability mass function @xmath21 .",
    "subsequently , we use @xmath21 as a shorthand for @xmath22 . we can now express the entropy of the random trajectory @xmath23 as @xmath24 we define the matrix of trajectory entropies @xmath25 where @xmath26 .",
    "ekroot and cover  @xcite provide a general closed - form expression for the matrix @xmath25 of an irreducible , aperiodic and finite state mc .",
    "the entropy @xmath27 of a trajectory from @xmath14 to @xmath15 given that it goes through @xmath28 is defined by @xmath29 where @xmath30 is the set of all trajectories in @xmath19 with an intermediate state @xmath28 @xmath31 the major challenge is to compute efficiently the entropy @xmath27 .",
    "even the costly approach of computing all the terms of the sum  ( @xmath32 ) is not always possible because the set @xmath33 has an infinite number of members in the case where , after removing state @xmath15 , the transition graph of the mc is not a dag .",
    "it is important to emphasize that the entropy @xmath27 is not the entropy of the random variable @xmath13 given another random variablea quantity which is easy to computebut the entropy of @xmath13 conditional on the realization of a dependent random variable .    in figure",
    "[ fig : mcexample ] , we show an example of a finite - state irreducible and aperiodic mc .",
    "note that the presence of cycles implies that the set of trajectories between some pair of states might have infinite cardinality ( @xmath34 for example ) .",
    "therefore , in addition to being complex , the naive approach of enumerating all trajectories is not always possible .    using the results of @xcite",
    ", we obtain the matrix of trajectory entropies @xmath35 the zero elements of the matrix @xmath25 correspond to deterministic trajectories such as @xmath36 , which is equal to the path @xmath37 with probability @xmath38 since no other path allows a walk to go from @xmath39 to @xmath40 .",
    "the entropy of the random trajectory @xmath41 is 1.56 bits .",
    "now imagine that we have an additional piece of information stating that the trajectory @xmath41 goes through state @xmath42 .",
    "intuitively , we would be tempted to argue that the entropy @xmath43 of the trajectory @xmath41 conditional on going through state @xmath42 is equal to @xmath44 , but this additivity property does not hold . indeed , the conditional entropy @xmath43 is zero because the trajectory @xmath41 , conditional on the intermediate state 4 , can only be equal to the path @xmath45 , whereas @xmath46 bits , hence @xmath47 bits .    in the next section",
    ", we study the entropy of markov trajectories conditional on _ multiple _ intermediate states and derive a general expression for this entropy .",
    "let @xmath48 denote the probability that the random trajectory @xmath13 goes through the state @xmath28 at least once : @xmath49 this is also equal to the probability that a walk reaches the state @xmath28 before the state @xmath15 , given that it started at @xmath14 . in order to compute @xmath48 , the technique from  @xcite",
    "is to make the states @xmath28 and @xmath15 absorbing ( a state @xmath11 is absorbing if and only if @xmath50 ) and compute the probability to be absorbed by state @xmath28 given that the trajectory has started at state @xmath14 .",
    "our first step towards computing @xmath27 is to express it as a function of quantities that are much simpler to compute .",
    "the idea is to relate the entropy of a trajectory conditional on a given state to its entropy conditional on _ not _ going through that state .",
    "therefore , we define the entropy @xmath51 of a trajectory from @xmath14 to @xmath15 given that it does _ not _ go through @xmath28 to be @xmath52 using the chain rule for entropy , we can derive the following equation which relates @xmath27 to @xmath53 and @xmath48 : @xmath54 for all @xmath28 , where @xmath55 is the entropy of a bernoulli random variable with success probability @xmath48 .",
    "first , we define the indicator variable @xmath56 by @xmath57 using the chain rule for entropy , we express the joint entropy @xmath58 in two different ways , @xmath59 because @xmath56 is a deterministic function of @xmath13 .",
    "so the entropy of the random trajectory @xmath13 can be expressed as @xmath60 since @xmath61 , we obtain @xmath62    as we know from  @xcite how to compute @xmath63 and @xmath48 , if we are able to compute @xmath51 , we can use  ( [ eq : chainruleentropy ] ) to find @xmath27 . however , generalizing  ( [ eq : chainruleentropy ] ) to trajectories conditional on passing through _ multiple _ intermediate states turns out to be difficult , hence we propose an approach that circumvents this problem .",
    "as we will see , the difficulty of our approach also boils down to computing the entropy of a trajectory conditional on _ not _ going through a given state .",
    "first , we define @xmath64 , the set of all trajectories in @xmath19 that exhibit the sequence of intermediate states @xmath65 , i.e. @xmath66    for an arbitrary sequence of states @xmath65 , satisfying @xmath67 , we prove the following lemma .",
    "[ thm : cond_entropy_as_sum ] @xmath68 where @xmath69 .    first , given @xmath70 , the random trajectory @xmath13 can be expressed as a sequence of random sub - trajectories @xmath71 .",
    "therefore , the conditional entropy @xmath72 , which we denote by @xmath73 , can be written as a joint sub - trajectory entropy @xmath74 applying the chain rule for entropy , we obtain successively @xmath75 the markovian nature of the process generating the trajectory @xmath13 implies that each of the sub - trajectories @xmath76 is independent of the preceding ones , given its starting point @xmath77 .",
    "since the sequence @xmath78 defines the starting point of each sub - trajectory , we can therefore write that @xmath79 using  , the expression for the conditional entropy becomes @xmath80 note that for each trajectory @xmath76 , the only restriction imposed by the event @xmath81 is that the final state @xmath15 can not be an intermediate state of any of the first @xmath82 trajectories @xmath83 . as a result , @xmath84 where @xmath85    now , if we are able to compute @xmath86 , we can use  ( [ eq : condientropy ] ) to derive @xmath72 .",
    "the following lemma shows how the conditional entropy @xmath86 can be obtained by a simple modification of the mc .",
    "+ we consider a mc whose transition probability matrix is @xmath1 , and @xmath14 , @xmath28 and @xmath15 three distinct states such that @xmath87 .",
    "let @xmath88 be the transition matrix of the same mc but where both states @xmath28 and @xmath15 are made absorbing , and whose entries are thus @xmath89 next , we define a second matrix @xmath90 , obtained by a transformation of the matrix @xmath88 @xmath91    [ thm : trajcondentropy ]    @xmath92  the matrix @xmath90 is stochastic and @xmath93  if @xmath94 is a random trajectory defined on the mc whose transition probability matrix is @xmath90 then @xmath95    @xmath92  the matrix @xmath88 is the transition probability matrix of a mc where the states @xmath28 and @xmath15 are absorbing .",
    "we can therefore introduce the vectors of absorption probability @xmath96 and @xmath97 where @xmath98 and @xmath99 are , respectively , the probability of being absorbed by @xmath28 and @xmath15 , given that the trajectory starts at @xmath11 .",
    "these vectors are eigenvectors of @xmath88 associated with the unit eigenvalue  @xcite @xmath100 moreover as @xmath101 has only two absorbing states @xmath28 and @xmath15 , for all @xmath11 , @xmath102 .",
    "recall that for all @xmath11 , @xmath103 hence @xmath104 can be written as @xmath105 note that all transitions leading to state @xmath28 in @xmath106 will have zero probability in @xmath107 .",
    "in fact , consider a state @xmath11 such that @xmath108 and @xmath109 . in the new matrix @xmath90 , the probability of transition from @xmath11 to @xmath28",
    "will be @xmath110 , which is zero because @xmath111 .",
    "proving that @xmath90 is stochastic is now straightforward : first , the entries of @xmath90 are positive ; second , they are properly normalized and sum up to one . indeed ,",
    "if we consider a state @xmath11 such that @xmath112 , we have that @xmath113 whereas if @xmath114 , we have that @xmath115 because of ( @xmath116 ) .",
    "let @xmath117 and @xmath118 be the probability measures defined , respectively , for @xmath119 and @xmath107 on the same sample space @xmath19 .",
    "any trajectory from the set @xmath19 has the form @xmath16 .    if @xmath120 , @xmath121 since we have constructed @xmath107 such that all transitions leading to state @xmath28 have zero probability .",
    "if @xmath122 , we have @xmath123 but @xmath124 as the probability to be absorbed by state @xmath15 , given that we have started at this same state , is @xmath38 . moreover , we know from   that @xmath125 , for all @xmath126",
    "as we have supposed that the trajectory @xmath127 does not admit either @xmath28 or @xmath15 as intermediate states , @xmath128 . rewriting ( [ eq : trajcondprob ] ) yields @xmath129 combining   and  , we have therefore proven , for all @xmath130 , that @xmath131 consequently , if the random variable @xmath94 describes the trajectory between @xmath14 and @xmath15 in @xmath107 , implies that @xmath132    for the particular case where @xmath133 , we still can use lemma  [ thm : trajcondentropy ] to express the conditional entropy @xmath134 : we modify the mc by removing the incoming transitions of @xmath14 and creating a new state @xmath135 that will inherit them .",
    "the conditional entropy @xmath134 in the original mc is equal to @xmath136 in the modified one and , since @xmath137 , we can use lemma  [ thm : trajcondentropy ] to express it .",
    "building on lemma  [ thm : cond_entropy_as_sum ] and lemma  [ thm : trajcondentropy ] , we can now state the main result of this paper : a general expression for the entropy of markov trajectories conditional on multiple intermediate states .",
    "[ thm : main_result ] let @xmath1 be the transition probability matrix of a finite markov chain and @xmath138 a sequence of states such that @xmath67 .",
    "then , we have the following equality @xmath139 where @xmath69 , and @xmath140 is a random trajectory defined on the markov chain whose transition probability matrix @xmath141 is defined as follows @xmath142    the matrix @xmath141 is obtained from @xmath1 using , which is equivalent to applying successively and where the starting , intermediate and ending states are , respectively , @xmath77 , @xmath15 and @xmath143 .",
    "therefore , using lemma  [ thm : trajcondentropy ] , we have @xmath144 for all @xmath145 . consequently ,",
    "we can write that @xmath146 where @xmath69 . using lemma  [ thm : cond_entropy_as_sum ] , we finally obtain @xmath147    now that we have derived a general expression for the entropy of markov trajectories conditional on multiple states , we introduce , in the next section , a method that allows us to compute this expression .",
    "the closed - form expression for the entropy of markov trajectories proposed by ekroot and cover  @xcite is valid only if the markov chain studied is irreducible .",
    "however , the markov chain @xmath107 obtained from @xmath119 after the transformations   and is not necessarily irreducible : all transitions leading to state @xmath28 have zero probability , which implies that possibly many states do not admit any path leading to @xmath15 .",
    "therefore , we need an expression for the entropy of markov trajectories that is valid under milder conditions . in order to identify these conditions , we study the properties of @xmath107 .",
    "let @xmath148 be the set of all states in @xmath107 and let @xmath149 and @xmath150 be two subsets that partition @xmath148 in the following manner @xmath151 the set @xmath149 is closed as no one - step transition is possible from any state in @xmath149 to any state in @xmath150 .",
    "in fact , if @xmath152 and @xmath153 , @xmath104 yields that @xmath154 clearly , all trajectories leading to state @xmath15 are composed of states belonging to @xmath149 .",
    "now , we propose a closed - form expression for the entropy of markov trajectories that is valid under the weaker condition that the destination state @xmath15 can be reached from any other state of the mc .",
    "moreover , we prove that the trajectory entropy can be expressed as a weighted sum of local entropies .",
    "we also provide an intuitive interpretation of the weights .",
    "[ thm : trajentropy2 ] let @xmath1 be the transition probability matrix of a finite state mc such that there exists a path with positive probability from any state to a given state @xmath15 .",
    "let @xmath155 be a sub - matrix of @xmath1 obtained by removing the @xmath156 row and column of @xmath1 .",
    "@xmath157 & & \\vdots \\\\ \\hline p_{d1 } & \\cdots & p_{dd } \\\\ \\end{array}. \\right)\\ ] ] for any state @xmath158 , the trajectory entropy @xmath63 can be expressed as @xmath159 where @xmath160 is the local entropy of state @xmath161 .",
    "first , observe that the matrix @xmath155 is a sub - matrix of @xmath1 corresponding to all states except state @xmath15 and that we use @xmath155 to derive the entropy of all trajectories ending at @xmath15 . applying the chain rule for entropy",
    ", we express the entropy of a trajectory as the entropy of the first step plus the conditional entropy of the rest of the trajectory given this first step @xmath162 we expand this equality further by recursively expanding the entropy @xmath163 as follows @xmath164 with @xmath165 .",
    "observe that the matrix @xmath155 describes the markov chain as long as it does not reach state @xmath15 .",
    "moreover , the matrix @xmath155 has a finite number of states and there is a path with positive probability from each state to state @xmath15 . as a consequence",
    ", the markov process will enter state @xmath15 with probability @xmath38 , i.e. , @xmath166 ( zero matrix ) .",
    "in addition , since @xmath167 we can easily verify that @xmath168 replacing ( [ eq : locentropy2 ] ) in ( [ eq : locentropy1 ] ) , we have @xmath169    we have shown that the entropy of a family of trajectories can be expressed as a weighted sum of the states local entropies .",
    "the weights are given by the matrix @xmath170 .",
    "in the markovian literature , the matrix @xmath170 is referred to as the fundamental matrix  @xcite . in fact , the @xmath171 element of the fundamental matrix ( defined with respect to the destination state @xmath15 ) can be seen as the expected number of visits to the state @xmath161 before hitting the state @xmath15 , given that we started at state @xmath14 . as a result , the entropy of the random trajectory @xmath23 is the sum over the chain states of the expected number of visits to each state multiplied by its local entropy .",
    "this is a remarkable observation since it links a global quantity , which is the trajectory entropy , to the local entropy at each state .",
    "recall that in the example shown in figure  [ fig : mcexample ] , we found that the entropy of the trajectory @xmath41 is equal to @xmath172 bits .",
    "we can retrieve this result by computing the fundamental matrix with respect to state @xmath40 .",
    "the @xmath173 element of this matrix is equal to the expected number of visits to state @xmath174 before hitting state @xmath40 , given that we started at state @xmath11 .",
    "multiplying the first row of the fundamental matrix @xmath175 by the column vector of local entropies @xmath176 yields @xmath177 bits .",
    "the following algorithm defines the set of steps to compute the entropy of markov trajectories conditional on a set of intermediate states :    [ alg : cond_entr ]    matrix of transition probability @xmath1 , source state @xmath14 , destination state @xmath15 , sequence of intermediate states  @xmath178 @xmath73 @xmath179 compute @xmath180 from @xmath1 using compute @xmath181 from @xmath180 using lemma  [ thm : trajentropy2 ] @xmath182 compute @xmath183 from @xmath1 using lemma  [ thm : trajentropy2 ] @xmath184 @xmath73    the worst - case running time for the algorithm is @xmath185 where @xmath186 is the number of states of @xmath119 , and @xmath82 the length of the sequence of intermediate states @xmath187 .",
    "this complexity is dominated by the cost of computing the inverse of the matrix @xmath188 , which is needed to compute the entropy @xmath63 in  .",
    "however , since we need only the @xmath189 row of the matrix @xmath188 to compute the trajectory entropy @xmath63 , we can solve a system ofpotentially sparselinear equations .",
    "moreover , many iterative methods  @xcite take advantage of the structure of the matrix representing the system of linear equations in order to solve them efficiently .    coming back to the example shown in figure  [ fig : mcexample ]",
    ", we use the algorithm above to compute the conditional entropy @xmath190 bit .",
    "we leave no ambiguity about the trajectory @xmath41 when we condition on both states @xmath191 and @xmath39 and find that @xmath192 bits .",
    "[ [ conditioning - on - a - set - of - states ] ] conditioning on a set of states + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in this paper , we focused on computing the entropy of markov trajectories conditional on a _ sequence _ of states . a natural extension is the computation of this entropy conditional on a _ non ordered _ set of states . finding a general expression for this conditional entropy appears very hard and there is no simple relation linking it to the entropy conditional on a sequence .",
    "we provide an example , shown in figure  [ fig : mcexample_2 ] , that illustrates an interesting and counter - intuitive result about conditioning on a set of states .",
    "intuitively , we would expect that the entropy of a random trajectory conditional on a sequence of states is always less than the entropy of the same trajectory conditional on the set formed by these states .",
    "however , this is not true . we take the mc shown in figure  [ fig : mcexample_2 ] as an example and we compute , using theorem  [ thm : main_result ] , the entropy of the random trajectory @xmath41 conditional on going through the sequence of intermediate states @xmath193 @xmath194 where @xmath195 is the entropy of a bernoulli random variable with success probability @xmath196 . to compute the entropy of the random trajectory @xmath41 conditional on going through the set of states @xmath197 ,",
    "we apply the chain rule for entropy and express the entropy of a trajectory as the entropy of the first two steps plus the conditional entropy of the rest of the trajectory given these first two steps @xmath198 since @xmath199 , we have that @xmath200 using and , we can write @xmath201 this difference can therefore be lower bounded by @xmath202 as a consequence , if @xmath203 , the entropy of the random trajectory @xmath41 conditional on going through the sequence @xmath193 is strictly greater than the entropy of the same trajectory conditional on going through the set of states @xmath197 .",
    "the reason is that conditioning on the sequence @xmath193 implies that the random trajectory @xmath41 is composed of a random sub - trajectory @xmath204 whose entropy can be made arbitrary large by increasing the parameter @xmath205 .",
    "more generally , this example illustrates the absence of a simple relation between the entropy of random trajectories conditional on a sequence of states and the entropy of the same trajectory conditional on the set formed by these same states .",
    "in this paper , we address the problem of computing the entropy of conditional markov trajectories .",
    "we propose a method based on a transformation of the original markov chain into a markov chain that yields the desired conditional entropy .",
    "we also derive an expression that allows us to compute the entropy of markov trajectories , under conditions weaker than those assumed in  @xcite .",
    "furthermore , this expression links the entropy of markov trajectoriesa global quantityto the local entropy of states .",
    "these results have applications in various fields including mobility privacy of the users of online services .",
    "in fact , using our framework , we are able to quantify the predictability of a user s mobility and its evolution with locations updates : we represent a location as a state of a markov chain .",
    "a sequence of visited locations is therefore a markovian trajectory , and location - updates amount to conditioning this trajectory on a set of intermediate states . in this setting",
    ", we can quantify the evolution of the user s mobility predictability as she / he discloses some of the locations she / he visited by computing the entropy of conditional markov trajectories .",
    "consequently , users are empowered with an objective technique to protect their privacy : they are able to anticipate the evolution of their mobility predictability as they reveal a subset of the locations they visited .",
    "the authors would like to thank olivier lvque and emre telatar for their feedback about this paper .      l.  yen , m.  saerens , a.  mantrach , and m.  shimbo , `` a family of dissimilarity measures between nodes generalizing both the shortest - path and the commute - time distances , '' in _ proceedings of the 14th sigkdd international conference on knowledge discovery and data mining _ , 2008 .                      mohamed kafsi(s12 ) is a ph.d .",
    "student in the school of computer and communication sciences at epfl .",
    "his research interests lie at the intersection of graph theory , information theory and data mining .",
    "he received the m.s . and",
    "degrees in communication systems from epfl . during his bachelor",
    "s studies , he spent one year at the electrical and computer engineering department of carnegie mellon university ( cmu ) .",
    "he is a member of the winning team of the 2013 nokia mobile data challenge .",
    "matthias grossglauser(m95 ) is an associate professor in the school of computer and communication sciences at epfl .",
    "he received his diplme dingnieur en systmes de communication degree from epfl in 1994 , the m.sc .",
    "degree from the georgia institute of technology in 1994 , and the ph.d . from the university pierre",
    "et marie curie ( paris 6 ) in 1998 .",
    "his research interests are in social and information networks , privacy , mobile and wireless networking , and network traffic measurement and modeling .",
    "he received the 1998 cor baayen award from the european research consortium for informatics and mathematics ( ercim ) , the ieee infocom 2001 best paper award , and the 2006 conext / sigcomm rising star award .",
    "he served on the editorial board of ieee / acm transactions on networking , and on numerous technical program committees . from 2007 - 2010 , he was with the nokia research center ( nrc ) in helsinki , finland , holding the positions of laboratory director , then of head of a tech - transfer program focused on data mining , analytics , and machine learning .",
    "in addition , he served on nokia s ceo technology council , a technology advisory group reporting to the ceo . from 2003 - 2007 , he was an assistant professor at epfl . from 1998 to 2002 , he was a senior , then principal member of research staff in the networking and distributed systems laboratory at at&t research in new jersey . from 1995 to 1998 , he was a ph.d .",
    "student at inria sophia antipolis , france .",
    "patrick thiran(s89  m96 ",
    "sm12 ) received the electrical engineering degree from the universit catholique de louvain , louvain - la - neuve , belgium , in 1989 , the m.s .",
    "degree in electrical engineering from the university of california at berkeley , usa , in 1990 , and the ph.d .",
    "degree from epfl , in 1996 .",
    "he is a full professor at epfl .",
    "he became an adjunct professor in 1998 , an assistant professor in 2002 , an associate professor in 2006 and a full professor in 2011 .",
    "from 2000 to 2001 , he was with sprint advanced technology labs , burlingame , ca .",
    "his research interests include communication networks , performance analysis , dynamical systems , and stochastic models .",
    "he is currently active in the analysis and design of wireless and plc networks , in network monitoring , and in data - driven network science .",
    "thiran served as an associate editor for the ieee transactions on circuits and systems in 1997 - 99 , and as an associate editor for the ieee / acm transactions on networking in 2006 - 10 .",
    "he was the recipient of the 1996 epfl ph.d .",
    "award and of the 2008 crdit suisse teaching award ."
  ],
  "abstract_text": [
    "<S> to quantify the randomness of markov trajectories with fixed initial and final states , ekroot and cover proposed a closed - form expression for the entropy of trajectories of an irreducible finite state markov chain . </S>",
    "<S> numerous applications , including the study of random walks on graphs , require the computation of the entropy of markov trajectories conditional on a set of intermediate states . </S>",
    "<S> however , the expression of ekroot and cover does not allow for computing this quantity . in this paper </S>",
    "<S> , we propose a method to compute the entropy of conditional markov trajectories through a transformation of the original markov chain into a markov chain that exhibits the desired conditional distribution of trajectories . </S>",
    "<S> moreover , we express the entropy of markov trajectoriesa global quantityas a linear combination of _ local entropies _ associated with the markov chain states .    the entropy of conditional markov trajectories    entropy , markov chains , markov trajectories .    </S>",
    "<S> the randomness of markov trajectories has applications in graph theory  @xcite and in statistical physics  @xcite , as well as in the study of random walks on graphs  @xcite . </S>",
    "<S> the need to quantify the randomness of markov trajectories first arose when lloyd and pagels  @xcite defined a measure of complexity for the macroscopic states of physical systems . </S>",
    "<S> they examine some intuitive properties that a measure of complexity should have and propose a universal measure called _ </S>",
    "<S> depth_. they suggest that the depth of a state should depend on the complexity of the process by which that state arose , and prove that it must be proportional to the shannon entropy of the set of trajectories leading to that state . </S>",
    "<S> subsequently , ekroot and cover  @xcite studied the computational aspect of the depth measure . in order to quantify the number of bits of randomness in a markov trajectory , they propose a closed - form expression for the entropy of trajectories of an irreducible finite state markov chain . </S>",
    "<S> their expression does not allow , however , for computing the entropy of markov trajectories conditional on the realisation of a set of intermediate states . </S>",
    "<S> computing the conditional entropy of markov trajectories turns out to be very challenging yet useful in numerous domains , including the study of mobility predictability and its dependence on location side information .    </S>",
    "<S> consider a scenario where we are interested in quantifying the predictability of route - choice behaviour . </S>",
    "<S> we can model the mobility of a traveller as a weighted random walk on a graph whose vertices represent locations and edges represent possible transitions  @xcite </S>",
    "<S> . we can therefore model a route as a sample path or trajectory in a markov chain . if we suppose that we know where the traveller starts and ends her / his route , the randomness of the route she / he would follow is represented by the distribution of trajectories between the source and destination vertices . consequently , the predictability of her / his route is captured by the entropy of markov trajectories between these two states . </S>",
    "<S> now , if we obtain side information stating that the traveller went ( or has to go ) through a set of intermediate vertices , quantifying the evolution of her / his route predictability requires the computation of the trajectory entropy conditional on the set of known intermediate states . </S>",
    "<S> the conditional entropy is also a way to quantify the informational value of the intermediate states revealed . </S>",
    "<S> for example , if the entropy conditional on the set of known intermediate states is zero , then this set reveals the whole trajectory of the traveller .    in our work , </S>",
    "<S> we propose a method to compute the entropy of markov trajectories conditional on a set of intermediate states . </S>",
    "<S> the method is based on a transformation of the original markov chain so that the transformed markov chain exhibits the desired conditional distribution of trajectories . </S>",
    "<S> we also derive an expression that enables us to compute the entropy of markov trajectories , under conditions weaker than those assumed in  @xcite . </S>",
    "<S> moreover , this expression links the entropy of markov trajectories to the local entropies at the markov chain states . </S>"
  ]
}