{
  "article_text": [
    "modelling nonparametric time series has received increasing interest among scholars for a few decades , see , for example , @xcite . in classical time series analysis , the stationarity of time series is a fundamental assumption .",
    "yet , it may be violated on some occasions in such the fields as finance , sound analysis and neuroscience , especially when the time span of observations tends to infinity .",
    "so , it is necessary to generalize the stationary process to the nonstationary process .",
    "priestley ( 1965 ) @xcite first introduced a stochastic process with evolutionary spectra , which locally displays an approximately stationary behavior .",
    "but in his framework , it is impossible to establish an asymptotic statistical inference .",
    "dahlhaus ( 1997 ) proposed a new generalization of stationarity , called locally stationary process , and investigated its statistical inference .",
    "more details can refer to @xcite .",
    "in essence , the locally stationary process is locally close to a stationary process over short periods of time , but its second order characteristic is gradually changing as time evolves . a formal description of locally stationary process can refer to assumption ( a1 ) in the appendix . in parametric context , the statistical inference of locally stationary process has been studied extensively by @xcite . in nonparametric context , vogt ( 2012 )",
    "@xcite considered the time - varying nonlinear autoregressive ( tvnar ) models including its general form and estimated the time - varying multivariate regression function using the kernel - type method .",
    "however , it still suffers the `` curse of dimensionality '' problem when the dimension of covariates is high .    in order to solve the aforementioned problem ,",
    "a familiar way is to adopt the additive nonparametric regression model suggested by @xcite .",
    "it not only remedies the `` curse of dimensionality '' , but also has an independent interest in practical applications due to its flexibility and interpretability .",
    "there exists abound research findings about the additive regression model in the literature . in the case of iid observations ,",
    "the additive nonparametric component functions can be estimated by kernel - based methods : the classic backfitting estimators of @xcite , the marginal integration estimators of @xcite , the smoothing backfitting estimators of @xcite , and two - stage estimators of @xcite . in the stationary time series context , there are kernel estimators via marginal integration of @xcite , spline estimators of @xcite , and the spline - backfitted kernel ( sbk ) estimators which borrow the strength of both kernel estimation and spline estimation , see @xcite .",
    "vogt @xcite considered the locally stationary additive model and proposed smooth backfitting method to estimate bivariate additive component functions .    on the other hand ,",
    "the varying - coefficient model is a natural extension of linear model which allows the coefficients to change over certain common covariates instead of being invariant .",
    "this model succeeds to relax the parameter limitation of linear model and may have practical as well as theoretical significance . for this model ,",
    "there are three types of estimation methods : local polynomial smoothing method @xcite , polynomial spline estimation method @xcite and smoothing spline method @xcite .",
    "zhang and wang @xcite proposed a so - called varying - coefficient additive model to catch the evolutionary nature of time - varying regression function in the analysis of functional data .",
    "their model assumes the evolutionary regression function has the form @xmath2 which is more flexible in the sense that it covers both varying - coefficient model and additive model as special cases . specifically speaking",
    ", it reduces to an additive model when @xmath3 are all constants , and a varying - coefficient model if @xmath4 are all linear functions . extracting the special meaning of time in functional data analysis",
    ", one can generalize time to some other common covariates .    in this paper , we model locally stationary time series . to concreteness ,",
    "let @xmath5 be a length-@xmath6 realization of @xmath7 dimension locally stationary time series , and assume that the data is generated by varying - coefficient additive model as follows @xmath8 where @xmath9 s are i.i.d , @xmath10 is the varying - coefficient component function , @xmath11 is the additive component function and @xmath12 is a bivariate nonparametric function , which allows the heteroscedasticity case . without loss of generality , we require @xmath13^{p}},$ ] where the superscript ` @xmath14 ' means transposition of vector or matrix . in order to identify these multiplied component functions , we require that @xmath15 where @xmath16 is the @xmath1 norm of any function @xmath17 defined on @xmath18 $ ] such that @xmath19    for functional data , zhang and wang @xcite proposed a two - step spline estimation procedure . in the first step , sorting the data within each subject in ascending order of time and averaging the response for each subject using trapezoidal rule to fit an additive model , then , in the second step , fitting a varying - coefficient model by substituting the estimated additive function into varying - coefficient additive model .",
    "his estimation methodology works since there are dense observation for every subject and covariates is independent of observation time within subject",
    ".    however , for some other practical problems , such as longitudinal data with finite observertion time , time series data , such an assumption fails . to circumvent this problem , under mild assumptions ,",
    "we derive an initial estimation of additive component by employing a segmentation technique",
    ". then we can fit a varying - coefficient model and an additive model , respectively , to get spline estimators of varying - coefficient function and additive function . as expected , we show that the proposed estimators of @xmath10 and @xmath11 are consistent and present the corresponding @xmath1 rate of convergence .    on the other hand",
    ", the product term in may simply reduce to a varying - coefficient term or an additive term in the case of @xmath11 being linear function or @xmath10 being constant .",
    "so , in the parsimony sense , identifying additive terms and varying - coefficient terms in are of interest . to this end",
    ", we propose a two - stage penalized least squares estimator based on scad penalty function , and , furthermore , show that our model identification strategy is consistent , i.e. , the additive term and the varying - coefficient term are correctly selected with probability approaching to 1 .",
    "meantime , @xmath1 rate of convergence of penalized spline estimator of each component function achieves the rate of the spline estimator of univariate nonparametric function .",
    "the rest of this paper is organized as follows .",
    "we propose a three - step spline estimation method in section 2 and a two - stage model identification procedure in section 3 .",
    "section 4 describes the smoothing parameter selection strategies .",
    "section 5 establishes the asymptotic properties of the proposed model estimation and identification methods .",
    "simulation studies are illustrated in section 6 .",
    "the main technical proofs are presented in the appendix .",
    "lemmas and other similar proofs are given in the supplementary .",
    "in this section , we propose a three - step spline estimation method for the proposed locally stationary varying - coefficient additive model .",
    "* step i : segment the rescaled time @xmath20 into several groups , and approximate each varying - coefficient function @xmath10 within the same group by a local constant . thus , model can be approximated by an additive model , and a scaled - version of the additive component functions @xmath11 can be obtained using spline - based method . * step ii : substitute the initial estimates of the scaled additive component functions into model to yield an approximated varying - coefficient model , and then obtain spline estimators of varying - coefficient component functions @xmath21 * step iii : plug - in spline estimators of varying - coefficient component functions @xmath10 into model to yield an approximated additive model , and then update the spline estimation of additive component functions @xmath11 .",
    "we first present with some notations before detailing our proposed estimation method .",
    "let @xmath22 be @xmath23 order b - spline basis with @xmath24 interior knots and @xmath25 is the number of b - spline functions estimating additive component function @xmath26 similarly , we denote @xmath27 as @xmath28 order b - spline basis with @xmath29 interior knots , and @xmath30 is the number of b - spline functions estimating varying - coefficient component function @xmath21 here , ` @xmath31 ' and ` @xmath32 ' in the subscript of b - spline functions and knots number mean that is for the additive component function and varying - coefficient function , respectively . denote @xmath33 and @xmath34 the nice properties of scaled b - spline basis are listed in the appendix . +",
    "* step i : initial estimators of scaled additive component functions *    we segment the sample @xmath35 in ascending order of time into @xmath36 groups with @xmath37 observations in each group , where @xmath38 hinges on the sample size @xmath6 and @xmath39 . then approximate @xmath40 in the @xmath41th group , i.e. @xmath42 by a constant @xmath43 , where @xmath43 is some constant dependent on @xmath44 and @xmath41 such that @xmath45    for the sake of convenient presentation , we suppress the triangular array index in locally stationary time series , and represent time index in the @xmath41th group as @xmath46 for given @xmath47 then one can approximate model as @xmath48 where @xmath49 if @xmath50 are all known , one can easily construct the spline estimator of @xmath26 suppose that @xmath51 minimizes @xmath52 ^ 2,\\ ] ] then @xmath53 where @xmath54    however , @xmath43 s are unknown",
    ". we instead rewrite as an additive model , @xmath55 where @xmath56 for each given @xmath57 let @xmath58 minimize @xmath59 ^ 2.\\ ] ] by and , it is easy to see that @xmath60 which implies @xmath61 in a word , although the additive component function in can not be estimated directly , the scaled additive component function @xmath62 with @xmath63 is estimable using the proposed segmentation techniques . + * step ii : spline estimators of varying - coefficient component functions *    define @xmath64 and @xmath65 , @xmath66 . by , substituting @xmath67 into yields @xmath68 where @xmath69.$ ]    model can be viewed as a varying - coefficient model , and the spline estimators of varying - coefficient functions @xmath70 are easily obtained .",
    "suppose that @xmath71 minimizes @xmath72 ^ 2.\\ ] ] then , @xmath73 by the definition of @xmath74 and identifiability conditions for @xmath75 we have the spline estimators of varying - coefficient functions @xmath76 s in model as @xmath77 * step iii : spline estimators of additive component functions *    substituting into model yields @xmath78 where @xmath79\\beta_{k}(x_{t , t}^{\\left(k\\right)}).$ ]    model can be viewed as a varying - coefficient model .",
    "suppose that @xmath80 minimizes @xmath81 ^ 2,\\ ] ] then , spline estimators of additive component functions in are given by @xmath82    * remark 1 : * the spline estimators @xmath83 and @xmath84 can be updated by iterating step ii and step iii .",
    "however , one step estimation is enough and there is no great improvement through iteration procedure .",
    "* remark 2 : * one may employ different b - spline basis functions in step i and step iii for estimating the additive component functions @xmath85 .",
    "yet , we do nt distinguish them in symbols for the sake of simplicity .",
    "the proposed varying - coefficient additive model is more general and flexible than either varying - coefficient model or additive model , and covers them as special cases .",
    "but , in practice , a parsimonious model is always one s preference when there exist several potential options .",
    "hence , it is of great interest to explore whether the varying - coefficient component function is truly varying and whether the additive component function degenerates to simply linear function . in this paper",
    ", we decompose varying - coefficient additive terms into additive terms @xmath11 and varying - coefficient terms @xmath10 , and , motivated by @xcite , propose a two - stage penalized least squares ( pls ) model identification procedure to identify the term that @xmath11 is constant ( @xmath86 ) or / and @xmath10 is linear ( @xmath87 ) .",
    "* stage i : plug - in the spline estimators of additive component functions @xmath11 obtained in the estimation stage into model , and penalize @xmath88 to identify linear additive terms . *",
    "stage ii : given the penalized spline estimators of additive component functions @xmath11 obtained in stage i of the model identification process , penalize @xmath89 to select constant varying - coefficient terms .",
    "we first introduce some notations .",
    "let @xmath90 and @xmath91 denote @xmath92 and @xmath93 * stage i : identifying linear additive terms *    by substituting the additive component functions @xmath94 by their spline estimates @xmath95 obtained in the estimation stage , model becomes @xmath96 where @xmath97.$ ] let @xmath98 with @xmath99 , and assume @xmath100 is determined by @xmath101 ^ 2\\\\ & + t\\sum_{k=1}^{p}p_{\\lambda_{t}}\\left(k_{c}^{-3/2}\\parallel\\alpha'_{k}\\parallel_{l_{2}}\\right ) , \\end{aligned}\\ ] ] where @xmath102 and @xmath103 is a penalty function with a tuning parameter @xmath104 then the penalized spline estimators of @xmath76 are given by @xmath105 here the superscript ` @xmath106 ' denotes the penalized spline estimation . +",
    "* stage ii : identifying constant varying - coefficient terms *    by replacing the varying - coefficient function @xmath76 with their penalized spline estimates @xmath107 obtained in stage i , model becomes @xmath108 where @xmath109\\beta_{k}\\left(x_{t , t}^{\\left(k\\right)}\\right).$ ]    let @xmath110 with @xmath111 , and assume @xmath112 is given by @xmath113 ^ 2\\\\ & + t\\sum_{k=1}^{p}p_{\\mu_{t}}\\left(k_{a}^{-3/2}\\parallel\\beta''_{k}\\parallel_{l_{2}}\\right ) , \\end{aligned}\\ ] ] where @xmath114 and @xmath115 is a penalty function with a tuning parameter @xmath116 therefore , the penalized spline estimators of @xmath117 are given by @xmath118,\\ k=1,\\cdots , p.\\ ] ]",
    "in this section , we discuss various implementation issues for the proposed model estimation and identification procedures .",
    "we predetermine the degree of polynomial spline .",
    "usual options are 0 , 1 or 2 , that is to choose linear , quadratic or cubic spline functions .",
    "it is known that , when sufficient number of knots is used , the spline approximation method is quite stable .",
    "therefore , we suggest to use the same number of interior knots @xmath119 for all component functions and @xmath120th order b - spline basis functions in the @xmath121th step estimation to facilitate the computation . by experience",
    ", it is reasonable to choose @xmath122 in addition , in order to solve the least squares problem in each group in step i estimation , we require @xmath123 under this constraint , we choose the optimal @xmath119 and @xmath37 by bic @xmath124 where @xmath125 ^ 2 $ ] and @xmath126 is the number of b - spline basis functions used in step iii estimation .      various penalty functions @xcite can be used in practice .",
    "we choose the scad penalty function proposed by @xcite , which is defined by its first derivative @xmath127 for some @xmath128 and @xmath129 where symbol @xmath130 it is well - known that the scad penalty function has nice properties such as unbiasedness , sparsity and continuous .",
    "meantime , it is singular at the origin , and have no continuous second order derivatives .",
    "yet it can be locally approximated by a quadratic function .",
    "specifically speaking , given an initial estimate @xmath131 or equivalently @xmath132 if @xmath133 then one can locally approximate @xmath134 by @xmath135 this implies that the objective function in , denoted by @xmath136 can be approximated , up to a constant , by @xmath137 where @xmath138 @xmath139 with @xmath140 and @xmath141 and @xmath142 therefore , we can find the solution of by iteratively computing the following ridge regression estimator @xmath143 until convergence .    in the same vein , we can iteratively solve the optimization problem .",
    "let @xmath144 @xmath145 with @xmath146 and @xmath147 and @xmath148 therefore , we can iteratively compute the following ridge regression estimator @xmath149 until convergence .",
    "based on the optimal segment length @xmath150 and the optimal number of interior knots @xmath151 , we then select tuning parameters @xmath152 and @xmath153 for the proposed two - stage model identification procedure .",
    "following @xcite , we take @xmath154 and find optimal tuning parameters @xmath152 and @xmath153 by bic in two steps .",
    "first , to select optimal @xmath152 , we define @xmath155 where @xmath156^{2}$ ] , @xmath157 is the number of b - spline basis functions adopted in the second step estimation and @xmath158 is the number of linear additive terms , i.e. , @xmath159 is sufficiently small , say , no larger than @xmath160    second , to select optimal @xmath153 , we define @xmath161 where @xmath162^{2}$ ] , and @xmath163 is the number of constant varying - coefficient terms , i.e. , @xmath164 is sufficiently small .",
    "thus , we select the optimal tuning parameters @xmath165",
    "+ in this section , we demonstrate that the proposed three - step spline estimation method is consistent under regularity conditions and show that the proposed two stage model identification procedure can correctly select additive terms and varying - coefficient terms with probability approaching one .",
    "furthermore , we conclude that @xmath1 rate of convergence of each component function achieves the optimal rate of the spline estimator of univariate nonparametric function stated in @xcite .",
    "the regularity conditions and assumptions are given in the appendix .",
    "+ let @xmath166 and @xmath167 .",
    "we introduce @xmath168}|\\beta_{k}\\left(x\\right)-\\mu_{k}\\left(x\\right)|,\\ \\ \\rho_{c}=\\sum_{k=1}^{p}\\inf_{\\nu_{k}\\in h_{k}}\\sup_{x\\in[0,1]}|\\alpha_{k}\\left(x\\right)-\\nu_{k}\\left(x\\right)|\\ ] ] to measure the degree of spline approximation of varying - coefficient component function and additive component function .",
    "proposition 1 establishes @xmath1 rate of convergence of initial estimators of scaled additive component functions @xmath169    [ prop1 ] under assumptions ( a1 ) , ( a2 ) , ( a4 ) , ( a5 ) , ( a7 ) , ( a8 ) and ( a9 ) , if @xmath170 as @xmath171 @xmath172 @xmath173^{2 } = o_p\\left(\\frac{\\rho_{a}^{2}}{n_{t}}+\\frac{k_{a}}{t}\\right),\\ ] ]    * remark 3 : * in comparison with the convergence of the spline estimation of univariate nonparametric function , we notice that the bias term in @xmath1 rate of convergence of initial estimators is smaller when the number of groups @xmath36 is larger than 1 .",
    "the larger number of segmentation groups , the smaller the bias , given the number of observations in each group is at least larger than the number of parameters in spline approximation of @xmath174    based on the result of proposition 1 , one can construct @xmath1 rate of convergence of the spline estimation of varying - coefficient component function @xmath10 as follows .",
    "[ alpha ] under assumptions ( a1 ) ( a10 ) , if @xmath175 as @xmath171 @xmath176 @xmath177^{2 } = o_p\\left(\\frac{\\rho_{a}^{2}}{n_{t}}+\\rho_{c}^{2}+\\frac{k_{a}\\vee k_{c}}{t}\\right),\\ ] ] where ` @xmath178 ' denotes the maximum of @xmath179 and @xmath180    * remark 4 : * theorem [ alpha ] shows that there exists an additional bias term @xmath181 in comparison with the convergence of the spline estimation of univariate nonparametric function .",
    "this term happens to be the rate of convergence obtained in proposition 1 and reflects the error of the initial estimator of scaled additive function @xmath174    next theorem presents @xmath1 rate of convergence of the spline estimation of additive component function @xmath26    [ beta ] under the assumptions of theorem 1 , @xmath182    * remark 5 : * similarly , in comparison with the rate of convergence of spline estimation for univariate nonparametric function , theorem [ beta ] also has an additional bias term @xmath183 .",
    "the reason this term exists is because the estimation of additive component function @xmath11 in step iii is based on the spline estimates of varying - coefficient function @xmath10 obtained in step ii .",
    "as expected , the convergence of the updated spline estimation of @xmath11 doe not depend on the number of segmentation groups in step i of the initial estimation of rescaled additive function @xmath174      we , here , respectively , demonstrate the consistency of selecting additive terms and varying - coefficient terms , and present the @xmath1 rate of convergence of penalized spline estimators of @xmath10 and @xmath26    [ cadditive ] suppose that @xmath184 given @xmath185 @xmath186 and @xmath187 with @xmath188 , then , under assumptions ( a1 ) ( a10 ) , as @xmath171    * with probability approaching to 1 , @xmath189 is constant a function a.s . for @xmath190",
    "* @xmath1 rate of convergence for penalized spline estimator of @xmath10 is given by @xmath191 for @xmath192 where @xmath193 and @xmath194    [ cvarying ] suppose that @xmath195 given @xmath196 @xmath197 and @xmath198 with @xmath199 then , under assumptions ( a1 ) ( a10 ) , as @xmath171    * with probability approaching to 1 , @xmath200 is a linear function a.s . for @xmath201",
    "* @xmath1 rate of convergence for penalized spline estimator of @xmath11 is given by @xmath202 for @xmath203 where @xmath204 and @xmath205    * remark 6 : * theorems [ cadditive ] and [ cvarying ] show that the penalized spline estimators of varying - coefficient component function @xmath10 and additive component function @xmath11 both have the same @xmath1 rate of convergence as that of the spline estimator of univariate nonparametric function .",
    "we consider two simulation examples to illustrate the finite sample performance of the proposed three - step spline estimation method and two - stage model selection procedure , respectively .",
    "the data are generated from the varying - coefficient additive model as follows @xmath206 where @xmath207 @xmath208 and @xmath209 are iid standard normal variables and @xmath210    to appraise the performance of the proposed three - step spline estimators , we use the mean integrated squared error ( mise ) based on @xmath211 monte carlo replications , that is @xmath212^{2}\\mathrm{d}u,\\ \\ k=0,1,2,\\ ] ] @xmath213^{2}\\mathrm{d}x_{k},\\ \\ k=1,2,\\ ] ] where @xmath214 and @xmath215 are estimators of @xmath76 and @xmath216 respectively , in the @xmath217-th monto carlo sample .",
    "the univariate nonparametric functions are approximated by b - spline of order of three or the quadratic splines .",
    "we consider the sample size @xmath218 the number of interior knots @xmath219 and the segmentation length @xmath220 in step i estimation .",
    "we run the simulation for 500 times , and find out the mise of three - step estimators decreases as the sample size increases , regardless the values of @xmath119 and @xmath221 .",
    "table [ tag:1 ] only gives the results of @xmath222 for different combinations of @xmath221 and @xmath6 .",
    "in addition , we list the mise of oracle estimators , which refer to the spline estimator of @xmath10 given all additive component functions are known in advance , or correspondingly , the spline estimator of @xmath11 given all varying - coefficient component functions are known in advance .",
    "as expected , mise of oracle estimators for varying - coefficient components and additive components are better than those of the proposed three - step spline estimators .",
    "the last two columns in table [ tag:1 ] depict the mise of spline estimators for nonparametric component functions in misspecified varying - coefficient model and misspecified additive model .",
    "we note that they are discernibly larger than three - step spline estimators in varying - coefficient additive model .",
    ".comparison of mise of different estimators in example 1 ( @xmath222 ) [ cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]",
    "let @xmath223 be the euclidean norm of a real valued vector @xmath224 denote the space of @xmath225-order smooth functions defined on @xmath18 $ ] as @xmath226=\\{m|m^{\\left(l\\right)}\\in c[0,1]\\}$ ] and the class of lipschitz continuous functions for some fixed constant @xmath227 as @xmath228,c\\right)=\\{m||m\\left(x\\right)-m\\left(x'\\right)|\\leq c|x - x'|,\\forall x , x'\\in[0,1]\\}.$ ] + the necessary conditions to prove asymptotic properties are listed as below .",
    "* the process @xmath229 is stationary locally in time , that is , for each rescaled time point @xmath230,$ ] there exists a strictly stationary process @xmath231 such that @xmath232 a.s . with @xmath233\\leq c$ ] for some @xmath234 and @xmath235 independent of @xmath236 and @xmath237 * at each rescaled time point @xmath230,$ ] the joint density function @xmath238 of the stationary approximation process @xmath239 is bounded below and above uniformly on @xmath230:$ ] @xmath240^{p}}f_{u}\\left(\\mathbf{x}\\right)\\leq\\sup_{\\mathbf{x}\\in[0,1]^{p}}f_{u}\\left(\\mathbf{x}\\right)\\leq c_{f } \\",
    "\\text{uniformly on}\\ u\\in[0,1].\\ ] ] meantime , @xmath241 has density function with respective to certain measure .",
    "* @xmath242 iid , and @xmath243 @xmath244 given @xmath245 @xmath246 * there exists positive constants @xmath247 and @xmath248 such that @xmath249 for all @xmath250 where @xmath251 is the @xmath252-mixing coefficients for process @xmath253 and defined as latexmath:[\\[\\alpha\\left(k\\right)=\\sup_{b\\in\\sigma\\{\\mathcal{z}_s , s\\leq t\\ } , c\\in \\sigma\\{\\mathcal{z}_s , s\\ge t+k\\ } }    * the conditional standard deviation function @xmath12 is bounded below and above uniformly on @xmath230,$ ] i.e. , @xmath255^{p}}\\sigma\\left(u,\\mathbf{x}\\right ) \\leq \\sup_{\\mathbf{x}\\in[0,1]^{p}}\\sigma\\left(u,\\mathbf{x}\\right)\\leq c_{\\sigma}\\ \\text{uniformly on}\\   u\\in[0,1]\\ ] ] for some positive constants @xmath256 and @xmath257 * @xmath258 $ ] and @xmath259,c_{1,k})$ ] where @xmath260 is an integer such that @xmath261 * @xmath262 $ ] and @xmath263,c_{2,k})$ ] where @xmath264 is an integer such that @xmath265 * the knots for @xmath266 @xmath267 and the knots for @xmath268 @xmath269 has bounded mesh ratio : @xmath270 @xmath271 * @xmath272 , @xmath273 @xmath274 and @xmath275 * @xmath276 and @xmath277    * remark 7 : * assumption ( a1 ) specifies a data generating process ( gdp ) . assumptions ( a2 ) - ( a5 ) are standard in time - series context , see @xcite .",
    "assumptions ( a6 ) - ( a8 ) are common in typical spline approximation literature , for instance , @xcite .",
    "firstly , we need some lemmas before proving main theorems .",
    "let @xmath278 and @xmath279 be any two - vector valued function .",
    "define empirical inner product @xmath280 and theoretical inner product @xmath281\\mathrm{d}u,\\ ] ] denote the induced norm by @xmath282 and @xmath283 as @xmath284 and @xmath285 respectively .",
    "in addition to , @xmath286 given sequences of positive numbers @xmath287 and @xmath288 @xmath289 means @xmath290 is bounded and @xmath291 means @xmath289 and @xmath292 hold .",
    "[ inducenorm ] let @xmath293 and @xmath294 for @xmath295 denote @xmath296 and @xmath297 then @xmath298 holds under assumption ( a2 ) .",
    "in combination with assumption ( a2 ) and the property ( iv ) of @xmath299 we have @xmath300\\mathrm{d}u = \\sum_{k=1}^{p}\\int_{0}^{1}e\\big[\\sum_{l=1}^{j_{k , a}}\\gamma_{kl}\\psi_{kl}(x_{t}^{\\left(k\\right)}\\left(u\\right))\\big]^{2}\\mathrm{d}u\\\\ \\asymp&\\sum_{k=1}^{p}\\int_{0}^{1}\\parallel g_{k}\\parallel^{2}_{l_{2}}\\mathrm{d}u=\\parallel\\mathbf{g}\\parallel_{l_{2}}^{2 } \\asymp\\sum_{k=1}^{p}|\\gamma_{k}|^{2}=|\\mathbf{\\gamma}|^{2 } \\end{aligned}\\ ] ]    [ inner ] let @xmath301 be the collection of vector valued functions @xmath278 with @xmath302 such that @xmath303 for @xmath304 where @xmath305 is defined in section 5.1 .",
    "then under assumption ( a1 ) , ( a2 ) , ( a4 ) and ( a9 ) , as @xmath171 @xmath306    for any @xmath307 there exists coefficients @xmath308 and @xmath309 @xmath310 such that @xmath311 for @xmath295 it is not difficult to see that @xmath312 for any given @xmath304 let @xmath313 if the intersection of the supports of @xmath314 and @xmath315 contains an open interval .",
    "that is , @xmath316 if @xmath317 moreover , it is known @xmath318 for some constant @xmath32 and all @xmath319 moveover , for any given @xmath320 and @xmath321 we have @xmath322\\mathrm{d}u|\\\\ \\leq & t^{-1}\\sum_{t=1}^{t}|\\psi_{kl}(x_{t , t}^{\\left(k\\right)})\\psi_{kl'}(x_{t , t}^{\\left(k\\right ) } ) -\\psi_{kl}(x_{t}^{\\left(k\\right)}(t / t))\\psi_{kl'}(x_{t}^{\\left(k\\right)}(t / t))|\\\\ + & t^{-1}\\sum_{t=1}^{t}|\\psi_{kl}(x_{t}^{\\left(k\\right)}(t / t))\\psi_{kl'}(x_{t}^{\\left(k\\right)}(t / t ) ) -e[\\psi_{kl}(x_{t}^{\\left(k\\right)}(t / t))\\psi_{kl'}(x_{t}^{\\left(k\\right)}(t / t))]|\\\\ + & |t^{-1}\\sum_{t=1}^{t}e[\\psi_{kl}(x_{t}^{\\left(k\\right)}\\left(t / t\\right))\\psi_{kl'}(x_{t}^{\\left(k\\right)}\\left(t / t\\right ) ) ] -\\int_{0}^{1}e[\\psi_{kl}(x_{t}^{\\left(k\\right)}\\left(u\\right))\\psi_{kl'}(x_{t}^{\\left(k\\right)}\\left(u\\right))]\\mathrm{d}u| .",
    "\\end{aligned}\\ ] ] by assumption ( a1 ) and the boundness of b - spline , the first term above is bounded by @xmath323 employing berstein s inequality , the second term is bounded by @xmath324 the last term is bounded by @xmath325 from the integral theory .",
    "therefore , using cauchy - schwartz inequality and assumption ( a9 ) , we obtain that @xmath326 where @xmath327 and @xmath328 denote the vectors with entries @xmath308 and @xmath309 respectively . by lemma [ inducenorm ]",
    ", we see @xmath329 which completes the proof .",
    "[ eigenvalue ] under assumption ( a1 ) , ( a2 ) ( a4 ) and ( a9 ) , as @xmath171    * for each @xmath330 @xmath331 has eigenvalues bounded away from 0 and @xmath332 with probability tending to one ; * @xmath333 has eigenvalues bounded away from 0 and @xmath334 with probability tending to one .",
    "we only show ( ii ) , the proof of ( i ) is similar . for",
    "any given vector @xmath296 with @xmath335 @xmath336 let @xmath293 and @xmath337 where @xmath338 denote @xmath339 then by lemma [ inducenorm ] and [ inner ] , @xmath340 which implies @xmath341 has eigenvalues bounded away from 0 and @xmath342 therefore , @xmath343 also has eigenvalues bounded away from 0 and @xmath342    * proof for proposition 1 .",
    "* + let @xmath344 then @xmath345 define @xmath346 @xmath347 and @xmath348 denote @xmath349 and @xmath350 @xmath351 note that @xmath352 by cauchy - schwartz inequality , we have @xmath353 it suffices to deal with the approximation error terms @xmath354 and stochastic error terms @xmath355    * approximate error terms : * we will show the rate of approximation error    @xmath356 we have @xmath357\\parallel^2_{l_{2}}\\\\ & \\preceq \\frac{1}{n_{t}^2}\\sum_{s=1}^{n_{t } } \\parallel\\tilde{\\beta}_{k}^{\\left(s\\right)}\\left(x_{k}\\right)-\\beta_{k}^{\\left(s\\right)}\\left(x_{k}\\right)\\parallel^2_{l_{2}}. \\end{aligned}\\ ] ] by the definition of @xmath358 there exists @xmath359 and @xmath360 such that @xmath361}\\big|\\breve{\\beta}_{k}^{\\left(s\\right)}\\left(x_{k}\\right)-\\beta_{k}^{\\left(s\\right)}\\left(x_{k}\\right)\\big|=o\\left(\\rho_{a}\\right).$ ] therefore , @xmath362 let @xmath363 then @xmath364 on the one hand , @xmath365 ^ 2 \\asymp|\\mathbf{\\tilde{\\alpha}}^{\\left(s\\right)}-\\mathbf{\\breve{\\alpha}}^{\\left(s\\right)}|^2 $ ] by lemma [ eigenvalue ] . on the other hand",
    ", @xmath366 ensures that @xmath367^{2 } \\leq \\frac{1}{i_{t}}\\sum_{j=1}^{i_{t}}(\\omega_{s , j}-v_{sj}^{\\tau}\\mathbf{\\breve{\\alpha}}^{\\left(s\\right)})^2\\\\ = & \\frac{1}{i_{t}}\\sum_{j=1}^{i_{t}}\\big\\{\\sum_{k=1}^{p}\\big[\\beta_{k}^{\\left(s\\right)}(x_{t_{sj},t}^{\\left(k\\right ) } ) -\\breve{\\beta}_{k}^{\\left(s\\right)}(x_{t_{sj},t}^{\\left(k\\right)})\\big]\\big\\}^{2}=o\\left(\\rho_{a}^{2}\\right ) .",
    "\\end{aligned}\\ ] ] thus , @xmath368 which means @xmath369 * stochastic error terms : * we next show the rate of stochastic errors : @xmath370 note that @xmath371 under assumption ( a5 ) , we obtain that @xmath372 \\varepsilon_{t_{sj}}^{2}\\big\\}\\\\ & + c_{\\sigma}e\\big\\{\\sum_{\\substack{j=1\\\\j\\neq j'}}^{i_{t}}\\big[1+\\sum_{k=1}^{p}\\sum_{l=1}^{j_{k , a}}\\psi_{kl}(x_{t_{sj},t}^{\\left(k\\right ) } ) \\psi_{kl}(x_{t_{sj'},t}^{\\left(k\\right)})\\big]\\varepsilon_{t_{sj}}\\varepsilon_{t_{sj'}}\\big\\}. \\end{aligned}\\ ] ] assumption ( a3 ) makes the second term be zero , and the first term is bounded by @xmath373^{2}.$ ] however , @xmath374^{2 } \\preceq & \\sum_{k=1}^{p}\\sum_{l=1}^{j_{k , a}}e\\big[\\psi_{kl}(x_{t_{sj},t}^{\\left(k\\right ) } ) -\\psi_{kl}(x_{t_{sj}}^{\\left(k\\right)}\\left(t_{sj}/t\\right))\\big]^{2}\\\\ & + \\sum_{k=1}^{p}\\sum_{l=1}^{j_{k , a}}e[\\psi_{kl}(x_{t_{sj}}^{\\left(k\\right)}\\left(t_{sj}/t\\right))]^{2}. \\end{aligned}\\ ] ] by assumption ( a2 ) and the properties of b - spline , @xmath375 ^ 2 \\asymp\\sum_{k=1}^{p}j_{k , a}=o\\left(k_{a}\\right ) . \\ ] ] on the other hand , @xmath376^{2 } = j_{k , a}e[b_{kl , a}(x_{t , t}^{\\left(k\\right)})-b_{kl , a}(x_{t}^{\\left(k\\right)}\\left(t / t\\right))]^{2}\\\\ \\leq & cj_{k , a}e[|\\mathbf{x}_{t , t}-\\mathbf{x}_{t}\\left(t / t\\right)|^2 ] \\leq cj_{k , a}\\frac{1}{t^{2}}e[u_{t , t}^{2}\\left(t / t\\right ) ] .",
    "\\end{aligned}\\ ] ] therefore , @xmath377^{2 } = o\\left(k_{a}i_{t}\\right)+\\frac{i_{t}}{t^{2}}\\sum_{k=1}^{p}j_{k , a}^{2}. \\end{aligned}\\ ] ] and in turn @xmath378 which completes the proof of and hence the first half of theorem 1 .",
    "the rest is direct from lemma [ eigenvalue ] . + * proof for theorem [ alpha ] .",
    "* let @xmath379 @xmath380 where @xmath381 denote @xmath382 @xmath383    denote @xmath384 where @xmath385 @xmath386 and @xmath387 then @xmath388    furthermore , assuming that @xmath389 with @xmath390 is given by @xmath391 and @xmath392 for @xmath393    by cauchy - schwartz inequality and identifiable condition @xmath394 we get @xmath395 * approximate error term : * we show the rate of approximate error term as follows @xmath396 by the definition of @xmath397 there exists @xmath398 such that @xmath399 satisfying @xmath400}|\\breve{\\alpha}_{k}\\left(u\\right)-\\alpha_{k}\\left(u\\right)|=o\\left(\\rho_{c}\\right)\\ ] ] for @xmath393 thus @xmath401    note that @xmath402 and the normal equation @xmath403 yields @xmath404 according to proposition 1 and boundness of @xmath405 @xmath406^{2 } = o_{p}\\left(\\frac{\\rho_{a}^{2}}{n_{t}}+\\frac{k_{a}}{t}\\right ) .",
    "\\end{aligned}\\ ] ] on the other hand , from assumption ( a7 ) @xmath407^{2 } + \\frac{1}{t}\\sum_{t=1}^{t}\\sum_{k=1}^{p}\\hat{\\gamma}_{k}^{2}(x_{t , t}^{\\left(k\\right ) } ) [ \\delta_{k}\\left(t / t\\right)-\\breve{\\delta}_{k}\\left(t / t\\right)]^{2}\\\\ \\preceq&\\rho_{c}^{2}+\\rho_{c}^{2}\\frac{1}{t}\\sum_{k=1}^{p}\\sum_{t=1}^{t}\\hat{\\gamma}_{k}^{2}(x_{t , t}^{\\left(k\\right)})\\\\ = & o\\left(\\rho_{c}^{2}\\right ) .",
    "\\end{aligned}\\ ] ] * stochastic error terms : * we next show the following rate of stochastic error term @xmath408 it is easy to see @xmath409 based on assumption ( a5 ) , it is sufficient to bound @xmath410    let @xmath411 and @xmath412 then @xmath413 obviously , under assumption ( a3 ) , @xmath414 .",
    "$ ] from assumption ( a1 ) and assumption ( a7 ) , @xmath415^{2 } \\preceq & e[\\gamma_{k}(x_{t , t}^{\\left(k\\right)})-\\gamma_{k}(x_{t}^{\\left(k\\right)}(t / t))]^{2 } + e[\\gamma_{k}(x_{t}^{\\left(k\\right)}(t / t))]^{2}\\\\ \\preceq & e|\\mathbf{x}_{t , t}-\\mathbf{x}_{t}\\left(t / t\\right)|^{2}+o\\left(1\\right)\\\\ = & o\\left(t^{-2}\\right)+o\\left(1\\right)=o\\left(1\\right ) , \\end{aligned}\\ ] ] where @xmath416 is the @xmath44-th component of stationary approximation process @xmath231 of locally stationary process @xmath229 at rescaled time @xmath417 in combination with @xmath418 we have @xmath419\\preceq & \\sum_{l=1}^{j_{0,c}}\\varphi_{0l}^{2}\\left(t / t\\right)+\\sum_{k=1}^{p}\\sum_{l=1}^{j_{k , c}}\\varphi_{kl}^{2}\\left(t / t\\right ) = o\\left(k_{c}\\right ) , \\end{aligned}\\ ] ] which means @xmath420 meantime , @xmath421^{2}. \\end{aligned}\\ ] ] by cauchy - schwartz inequality , @xmath422^{2 } \\leq 3 e[\\hat{\\gamma}_{k}(x_{t , t}^{\\left(k\\right)})-\\hat{\\gamma}_{k}(x_{t}^{\\left(k\\right)}\\left(t / t\\right))]^{2}\\\\ & + 3e[\\hat{\\gamma}_{k}(x_{t}^{\\left(k\\right)}\\left(t / t\\right))-\\gamma_{k}(x_{t}^{\\left(k\\right)}\\left(t / t\\right))]^{2 } + 3e[\\gamma_{k}(x_{t}^{\\left(k\\right)}\\left(t / t\\right))-\\gamma_{k}\\big(x_{t , t}^{\\left(k\\right)})]^{2}. \\end{aligned}\\ ] ] the third term is bounded by @xmath423 from assumption ( a1 ) and ( a7 ) .",
    "assumption ( a2 ) and proposition 1 ensure that the second term is bounded by @xmath424 for the first term , we note that @xmath425^{2}\\\\ = & e\\big[n_{t}^{-1}\\sum_{s=1}^{n_{t}}\\sum_{l=1}^{j_{k , a}}\\hat{h}^{\\left(s\\right)}_{kl}j_{k , a}^{1/2 } \\{b_{kl , a}(x_{t , t}^{\\left(k\\right)})-b_{kl , a}(x_{t}^{\\left(k\\right)}\\left(t / t\\right))\\}\\big]^{2}\\\\ \\preceq & j_{k , a}n_{t}^{-2}\\sum_{s=1}^{n_{t}}\\sum_{l=1}^{j_{k , a}}(\\hat{h}^{\\left(s\\right)}_{kl})^{2 }    = & o_p\\left(\\frac{k_{a}^{2}}{t^{2}n_{t}}\\right ) , \\end{aligned}\\ ] ] and thus @xmath426^{2 } o_{p}\\left(\\frac{\\rho_{a}^{2}}{n_{t}}+\\frac{k_{a}}{t}\\right ) = o_p\\left(\\frac{k_{c}}{t}\\big\\{\\frac{\\rho_{a}^{2}}{n_{t}}+\\frac{k_{a}}{t}\\big\\}\\right ) , \\end{aligned}\\ ] ] which shows . + * proof for theorem [ beta ] .",
    "* let @xmath427 @xmath428 and @xmath429 @xmath430 define @xmath431 where @xmath432 @xmath433 and @xmath434 for @xmath435 suppose @xmath436 is given by @xmath437 and @xmath438 analogously , represent @xmath439 with @xmath440 obviously , @xmath441 * approximation error term : * the rate of approximation error term is given by @xmath442 on the one hand , by the definition of @xmath358 there exists @xmath443 and @xmath444 such that @xmath445}|\\beta^*\\left(x_{k}\\right)-\\beta_{k}\\left(x_{k}\\right)|=o\\left(\\rho_{a}\\right),\\ ] ] which means @xmath446 on the other hand , @xmath447 furthermore , @xmath448 since @xmath449 according to theorem 1 , @xmath450^{2}+ \\frac{1}{t}\\sum_{k=1}^{p}[\\alpha_{k}\\left(t / t\\right)-\\hat{\\alpha}_{k}\\left(t / t\\right)]^{2}\\beta_{k}^{2}(x_{t , t}^{\\left(k\\right)})\\\\ = & o_{p}\\left(\\frac{\\rho_{a}^{2}}{n_{t}}+\\rho_{c}^{2}+\\frac{k_{a}\\vee k_{c } } { t}\\right ) .",
    "\\end{aligned}\\ ] ] finally , we note that @xmath451\\big|^{2}\\\\ \\preceq&\\frac{1}{t}\\sum_{t=1}^{t}\\sum_{k=1}^{p } \\hat{\\alpha}_{k}^{2}\\left(t / t\\right)[\\beta_{k}(x_{t , t}^{\\left(k\\right ) } ) -\\beta^*_{k}(x_{t , t}^{\\left(k\\right)})]^{2 } = o\\left(\\rho_{a}^{2}\\right ) \\end{aligned}\\ ] ] since for each @xmath304 @xmath452^{2 } + \\frac{1}{t}\\sum_{t=1}^{t}\\alpha_{k}^{2}\\left(t / t\\right)\\\\ = & \\int_{0}^{1}\\alpha_{k}^{2}\\left(u\\right)\\mathrm{d}u+o\\left(1\\right)=o\\left(1\\right ) .",
    "\\end{aligned}\\ ] ] therefore , holds .",
    "+ * stochastic error term : * we will show the rate of stochastic error term : @xmath453 firstly , @xmath454 however , @xmath455 which implies @xmath456 because of assumption ( a5 ) .",
    "similar the counterpart in the proof of theorem 1 , we have @xmath457\\\\ = & \\sum_{k=1}^p[\\hat{\\alpha}_{k}\\left(t",
    "/ t\\right)-\\alpha_{k}\\left(t / t\\right)]^{2}\\sum_{l=1}^{j_{k , a}}e[\\psi_{kl}(x_{t , t}^{\\left(k\\right)})]^2 . \\end{aligned}\\ ] ] furthermore , @xmath458",
    "^ 2\\preceq \\sum_{l=1}^{j_{k , a}}e[\\psi_{kl}(x_{t , t}^{\\left(k\\right ) } ) -\\psi_{kl}(x_{t}^{\\left(k\\right)}\\left(t / t\\right))]^{2 } + \\sum_{l=1}^{j_{k , a}}e[\\psi_{kl}(x_{t}^{\\left(k\\right)}\\left(t / t\\right))]^2 . \\end{aligned}\\ ] ] by assumption ( a1 ) , @xmath459^{2 } = & j_{k , a}e[b_{kl , a}(x_{t , t}^{\\left(k\\right)})-b_{kl , a}(x_{t}^{\\left(k\\right)}\\left(t / t\\right))]^{2}\\\\ \\preceq & j_{k , a}e|x_{t ,",
    "t}^{\\left(k\\right)}-x_{t}^{\\left(k\\right)}\\left(t / t\\right)|^{2}=o\\left(j_{k , a}/t^{2}\\right ) \\end{aligned}\\ ] ] assumption ( a2 ) leads to @xmath460 ^ 2 \\asymp \\sum_{l=1}^{j_{k , a}}\\int\\psi_{kl}^2\\left(z\\right)\\mathrm{d}z \\leq \\int\\big[\\sum_{l=1}^{j_{k , a}}\\psi_{kl}\\left(z\\right)\\big]^{2}\\mathrm{d}z = j_{k , a}. \\end{aligned}\\ ] ] therefore , @xmath461^{2 } = o\\left(j_{k , a}+j_{k , a}^{2}/t^{2}\\right)=o\\left(k_{a}+k_{a}^{2}/t^{2}\\right),$ ] which yields @xmath462 similarly , @xmath463\\\\ = & \\frac{1}{t^{2}}\\sum_{t=1}^{t}\\delta\\left(t / t\\right)^{\\tau } e[\\mathbf{\\psi}\\left(\\mathbf{x}_{t , t}\\right)\\mathbf{\\psi}\\left(\\mathbf{x}_{t , t}\\right)^{\\tau}]\\delta\\left(t / t\\right ) .",
    "\\end{aligned}\\ ] ] note that @xmath464 = & e\\big[\\sum_{l=1}^{j_{k , a}}\\psi_{kl}^{2}(x_{t , t}^{\\left(k\\right)})\\big]\\leq\\int \\big\\{\\sum_{l=1}^{j_{k , a}}\\psi_{kl}\\left(z\\right)\\big\\}^{2}f_{x_{t , t}}^{\\left(k\\right ) } \\left(z\\right)\\mathrm{d}z\\\\=&j_{k , a } , \\end{aligned}\\ ] ] where @xmath465 is the marginal density of @xmath44-th component of @xmath466",
    "so , @xmath467=diag\\left(j_{k , a}\\right)_{k=1}^{p}=k_{a}i_{p}\\ ] ] and @xmath468 which completes the proof of . + * proof for theorem [ cadditive ] : *    * without loss of generality , we assume the true model is @xmath469 let @xmath470 and @xmath471 as the collection of all functions having form @xmath472 it is sufficient to show @xmath473 for any @xmath474 such that @xmath475 and for any @xmath476 where @xmath477 such that @xmath478 + for the sake of convenient presentation , we also denote @xmath479 as @xmath480 if @xmath481 let @xmath482 and @xmath483 then @xmath484 furthermore , @xmath485 where @xmath486 is the inner product of vector @xmath179 and @xmath180 + let @xmath487 we have @xmath488 and @xmath489 where the last step holds since assumption ( a7 ) and theorem [ beta ] .",
    "+ therefore , @xmath490 in which @xmath491 lies between 0 and @xmath492 and @xmath493 the proof of part(i ) is completed since @xmath494 @xmath495 and @xmath496 * according to theorem 6 ( p149 ) of @xcite , under assumption ( a6 ) , there exists @xmath497 and @xmath498 such that @xmath499 let @xmath500 and @xmath501 with @xmath502 next , we will show that for any given @xmath503 there is a sufficiently large @xmath32 such that @xmath504 according to lemma 3 of @xcite , @xmath505 has eigenvalues bounded away from 0 and @xmath332 with probability tending to one as @xmath506 therefore , @xmath507 where we use the fact that @xmath508 and @xmath509 for @xmath510 notice that the first term @xmath511 we also may choose the sufficiently large @xmath32 such that the third term can be dominated by the first term uniformly on @xmath512 finally , we observe that the @xmath513-th element of @xmath514 is given by @xmath515 which is bounded by @xmath516 thus , the second is bounded by @xmath517 which is also dominated by the first term . in combination with the nonnegativity of the first term , we show , which implies with probability at least @xmath518 that there exists a local minimizer in the ball @xmath519 i.e. , @xmath520 again by the property of b - spline , we have that @xmath521 the proof is finished in combination with @xmath499    the proofs for theorem [ cvarying ] is very similar to theorem [ cadditive ] , and thus omitted here .",
    "9 cai , z. , fan , j. and yao , q. ( 2000 ) .",
    "functional - coefficient regression models for nonlinear time series .",
    "_ j. amer .",
    "assoc . _ * 95 * , 941956 .",
    "cai , z. and xu , x. ( 2008 ) .",
    "nonparametric quantile estimations for dynamic smooth coefficient models .",
    "_ j. amer .",
    "assoc . _ * 103 * , 15951607 .",
    "chiang , c .- t . , rice , j. a. andwu , c. o. ( 2001 ) .",
    "smoothing spline estimation for varying coefficient models with repeatedly measured dependent variables .",
    "_ j. amer .",
    "assoc . _ * 96 * , 605619 .",
    "mr1946428 dahlhaus , r. ( 1996a ) .",
    "asymptotic statistical inference for nonstationary processes with evolutionary spectra , _ in : athens conference on applied probability and time series analysis , springer .",
    "_ 145259 .",
    "dahlhaus , r. ( 1996b ) . on the kullback - leibler information divergence of locally stationary processes .",
    "_ stochastic process .",
    "appl . _ * 62 * , 139168 .",
    "dahlhaus , r. ( 1997 ) .",
    "fitting time series models to nonstationary processes .",
    "statist . _",
    "* 25 * , 137 .",
    "dahlhaus , r. , neumann , m. h. , and von sachs .",
    "nonlinear wavelet estimation of time - varying autoregressive processes . _ bernoulli . _ * 5 * , 873906 .",
    "dahlhaus , r. and rao , s. s. ( 2006 ) .",
    "statistical inference for time - varying arch processes .",
    "statist . _",
    "* 34 * , 10751114 .",
    "de boor , c. ( 1978 ) .",
    "_ a practical guide to splines_. springer - verlag , new york .",
    "fan , j. , hrdle , w. and mammen , e. ( 1998 ) .",
    "direct estimation of low dimensional components in additive models .",
    "statist . _",
    "* 26 * , 943971 .",
    "fan , j. and li , r. ( 2001 ) .",
    "variable selection via nonconcave penalized likelihood and its oracle properties .",
    "_ j. amer .",
    "assoc . _ * 96 * , 13481360 .",
    "fan , j. , yao , q. , and cai , z. ( 2002 ) .",
    "adaptive varying - coefficient linear models .",
    "_ j. r. stat .",
    "methodol . _ * 65 * , 5780",
    ". fan , j. and zhang , w. ( 1999 ) .",
    "statistical estimation in varying coefficient models . _ ann .",
    "statist . _",
    "* 27 * 14911518 .",
    "mr1742497 fryzlewicz , p. , sapatinas , t. and rao , s. s. ( 2008 ) .",
    "normalized least - squares estimation in time - varying arch models._ann .",
    "statist . _",
    "* 36 * , 742786 .",
    "fu , w. ( 1998 ) . penalized regression : the bridge versus the lasso .",
    "_ j. comput . graph .",
    "statist . _",
    "* 7 * , 397416 .",
    "hafner , c. m. and linton , o. ( 2010 ) .",
    "efficient estimation of a multivariate multiplicative volatility model",
    ". _ j. econometrics . _ * 159 * , 5573 .",
    "hastie , t. j. and tibshirani , r. j. ( 1990 ) .",
    "_ generalized additive models_. crc press .",
    "hastie , t. j. and tibshirani , r. j. ( 1993 ) varying - coefficient models .",
    "b. _ * 55 * , 757796 .",
    "mr1229881 hoover , d. r. , rice , j. a. , wu , c. o. and yang , l .- p .",
    "nonparametric smoothing estimates of time - varying coefficient models with longitudinal data .",
    "_ biometrika . _",
    "* 85 * , 809822 .",
    "mr1666699 horowitz , j. , klemel , j. and mammen , e. ( 2006 ) .",
    "optimal estimation in additive regression models .",
    "_ bernoulli .",
    "_ * 12 * , 271298 .",
    "huang , j. z. ( 1998 ) .",
    "projection estimation in multiple regression with application to functional anova models",
    "_ * 26 * , 242272 .",
    "huang , j. z. , wu , c. o. and zhou , l. ( 2002 ) .",
    "varying - coefficient models and basis function approximations for the analysis of repeated measurements .",
    "_ biometrika . _ * 89 * , 111128 .",
    "mr1888349 huang , j. z. and shen , h. ( 2004 ) .",
    "functional coefficient regression models for nonlinear time series : a polynomial spline approach .",
    "j. stat . _ * 31 * , 515534 .",
    "mr2101537 huang , j. z. and yang , l. ( 2004 ) .",
    "identification of non - linear additive autoregressive models .",
    "_ j. r. stat .",
    "methodol . _ * 66 * , 463477 .",
    "huang , j. z.,wu , c. o. and zhou , l. ( 2004 ) .",
    "polynomial spline estimation and inference for varying coefficient models with longitudinal data . _ statist .",
    "* 14 * 763788 .",
    "mr2087972 huang , j. , horowitz , j. l. and wei f. ( 2010 ) .",
    "variable selection in nonparametric additive models .",
    "statist . _",
    "* 38 * , 22822313 .",
    "kim , w. , linton , o. b. , and hengartner , n. w. ( 1999 ) .",
    "a computationally efficient oracle estimator for additive nonparametric regression with bootstrap confidence intervals .",
    "_ j. comput . graph .",
    "statist . _",
    "* 8 * , 278297 .",
    "linton , o. and nielsen , j. p. ( 1995 ) .",
    "a kernel method of estimating structured nonparametric regression based on marginal integration .",
    "_ biometrika .",
    "linton , o. ( 1997 ) .",
    "efficient estimation of additive nonparametric regression models",
    ". _ biometrika . _ * 84 * , 469473 .",
    "r. , yang .",
    "l. and hrdle , w. k. ( 2013 ) .",
    "oraclly efficient two - step estimation of generalized aditive model .",
    "_ j. amer .",
    "assoc . _ * 108 * , 619631 .",
    "koo , b. and linton , o. ( 2012 ) .",
    "semiparametric estimation of locally stationary diffusion models",
    ". _ j. econometrics . _ * 170 * , 210233 .",
    "mammen , e. , linton , o. and nielsen , j. ( 1999 ) . the existence and asymptotic properties of a backfitting projection algorithm under weak conditions . _",
    "statist . _",
    "* 27 * , 14431490 .",
    "priestley , m. b. ( 1965 ) .",
    "evolutionary spectra and non - stationary process .",
    "_ j. r. stat .",
    "methodol . _",
    "* 27 * , 204237 .",
    "stone , c. j. ( 1982 ) .",
    "optimal global rates of convergence for nonparametric regression .",
    "statist . _",
    "* 10 * , 10401053 .",
    "mr0673642 stone , c. j. ( 1985 ) .",
    "additive regression and other nonparametric models .",
    "statist . _",
    "* 13 * , 689705 .",
    "stone , c. j. ( 1994 ) .",
    "the use of polynomial splines and their tensor products in multivariate function estimation .",
    "statist . _",
    "* 22 * , 118171 .",
    "tibshirani , r. ( 1996 ) .",
    "regression shrinkage and selection vis the lasso . _ j. r. stat .",
    "soc . ser .",
    "methodol . _ * 58 * , 267288 .",
    "tjstheim , d. and auestad , b. h. ( 1994 ) .",
    "nonparametric identification of nonlinear time series : projections .",
    "vogt , m. ( 2012 ) .",
    "nonparametric regression for locally stationary time series .",
    "_ * 40 * , 26012633 .",
    "wang , l. and yang , l. ( 2007 ) .",
    "spline - backfitted kernel smoothing of nonlinear additive autoregression model .",
    "_ * 35 * , 24742503 .",
    "wang , j. and yang , l. ( 2009 ) .",
    "efficient and fast spline - backfitted kernel smoothing of additive regression model .",
    "_ * 61 * , 663690 .",
    "wu , c. o. , chiang , c. t. and hoover , d. r. ( 1998 ) .",
    "asymptotic confidence regions for kernel smoothing of a varying - coefficient model with longitudinal data .",
    "_ j. amer .",
    "_ * 93 * 13881402 .",
    "mr1666635 xue , l. and yang , l.(2006 ) .",
    "estimation of semi - parametric additive coefficient model",
    ". _ j. statist .",
    "plann . inference .",
    "_ * 136 * , 25062534 .",
    "xue , lan .",
    "consistent variable selection in additive models .",
    "_ _ statist .",
    "sinica.__**19 * * , 12811296 .",
    "yang , l. , hardle , w. and nielsen , j. ( 1999 ) .",
    "nonparametric autoregression with multiplicative volatility and additive mean .",
    "_ j. time series .",
    "_ * 20 * , 579604 .",
    "yuan , m. and lin , y. ( 2006 ) .",
    "model selection and estimation in regression with grouped variables .",
    "_ j. r. stat .",
    ". b. stat .",
    "_ * 68 * , 4967 .",
    "zhang , x. k. , park , b.u . and wang , j. l. time - varying additive models for longitudinal data .",
    "_ j. amer .",
    "* 108 * , 983998 .",
    "zhang , x. k , and wang , j. l. ( 2013 ) .",
    "varying - coefficient additive models for functional data .",
    "_ biometrika .",
    "_ * 102 * , 1532 .",
    "zou , h. ( 2006 ) .",
    "the adaptive lasso and its oracle properties .",
    "_ j. amer .",
    "* 101 * , 14181429 ."
  ],
  "abstract_text": [
    "<S> nonparametric regression models with locally stationary covariates have received increasing interest in recent years . as a nice relief of `` curse of dimensionality '' induced by large dimension of covariates , additive regression model is commonly used . however , in locally stationary context , to catch the dynamic nature of regression function , we adopt a flexible varying - coefficient additive model where the regression function has the form @xmath0 for this model , we propose a three - step spline estimation method for each univariate nonparametric function , and show its consistency and @xmath1 rate of convergence . furthermore , based upon the three - step estimators , we develop a two - stage penalty procedure to identify pure additive terms and varying - coefficient terms in varying - coefficient additive model . </S>",
    "<S> as expected , we demonstrate that the proposed identification procedure is consistent , and the penalized estimators achieve the same @xmath1 rate of convergence as the polynomial spline estimators . </S>",
    "<S> simulation studies are presented to illustrate the finite sample performance of the proposed three - step spline estimation method and two - stage model selection procedure </S>",
    "<S> .    locally stationary process , varying - coefficient additive regression model , b - spline , scad , penalized least squares </S>"
  ]
}