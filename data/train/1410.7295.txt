{
  "article_text": [
    "sparse signal reconstruction problems appear in many engineering fields . in most applications ,",
    "signals are often measured from an undersampled set of noisy linear transformations .",
    "typically , the problem of interest is the reconstruction of a _ sparse _ signal @xmath0 from a set of @xmath1 noisy measurements @xmath2 which is given by @xmath3 where @xmath4 is the measurement matrix , and @xmath5 is the noise vector with @xmath6 representing the noise magnitude .",
    "this problem has arisen in many areas , such as signal processing , communications theory , information science , and statistics , and is widely known as _ compressive sensing _ @xcite .    in the past few years",
    ", many recovery algorithms have been proposed , see @xcite for a recent exhaustive list of the algorithms .",
    "one popular suboptimal and low - complexity estimator is @xmath7-regularized least - squares ( ls ) , a.k.a .  least absolute shrinkage and selection operator ( lasso ) @xcite , which seeks @xmath8 by @xmath9 in ( [ eq : rls ] ) , @xmath10 is a design parameter , and the _ _ complex__-norm is defined as @xmath11 , which is different from the complex @xmath7-norm .",
    "a simple extension of lasso to the complex setting is to consider the complex signal and measurements as a @xmath12-dimensional real - valued signal and @xmath13-dimensional real - valued measurements , respectively .",
    "however , several papers ( e.g. , @xcite and the references therein ) have shown that lasso based on the complex @xmath7-norm is superior to the simple real - valued extension when the real and imaginary components of the signals tend to either zero or nonzero simultaneously .",
    "therefore , we consider lasso using the the complex @xmath7-norm definition of ( [ eq : rls ] ) rather than the simple real - valued extension of lasso . ]",
    "@xmath7-norm is defined as @xmath14 the optimization problem of ( [ eq : rls ] ) is convex , and there are various fast and efficient solvers proposed .",
    "for example , the proximal gradient method in ( * ? ?",
    "? * section 7.1 ) resolves ( [ eq : rls ] ) by iteratively performing @xmath15 where @xmath16 is the iteration counter , @xmath17 is the chosen step size , and @xmath18 is a soft - thresholding function in which @xmath19 if @xmath20 and is @xmath21 otherwise .",
    "evaluating @xmath22 requires one matrix - vector multiplication by @xmath23 and another by @xmath24 , plus a ( negligible ) vector addition .",
    "the complexity for evaluating the soft - thresholding function @xmath25 is negligible .",
    "this kind of iterative thresholding algorithm requires few computations per - iteration , and therefore enables the application of lasso in large - scale problems .",
    "much of the theoretical work on ( [ eq : rls ] ) has focused on studying how aggressively a sparse signal can be undersampled while still guaranteeing perfect signal recovery .",
    "the existing results include those based on the restricted isometry property ( rip ) @xcite , polyhedral geometry @xcite , message passing @xcite , and the replica method @xcite .",
    "although rip provides sufficient conditions for sparse signal reconstruction , the results provided by rip analysis are often conservative in practice .",
    "in contrast , using combinational geometry , message passing , or the replica method , it is possible to compute the exact necessary and sufficient condition for measuring the sparsity - undersampling tradeoff performance of ( [ eq : rls ] ) in the limit @xmath26 .",
    "however , the theoretical work largely focused on the case of having a measurement matrix @xmath23 with independent and identically distributed ( i.i.d . )",
    ". a natural question would be `` _ how does the choice of the measurement matrix affect the typical sparsity - undersampling tradeoff performance ? _ '' .",
    "there are strong reasons to consider different types of measurement matrix .",
    "although the proximal gradient method performs efficiently in systems of medium size , the implementation of ( [ eq : proxgrad ] ) will become prohibitively complex if the signal size is very large .",
    "this is not only because performing ( [ eq : proxgrad ] ) requires matrix - vector multiplications up to the order of @xmath27 but it also requires a lot of memory to store the measurement matrix .",
    "there is strong desire to consider special forms of measurement matrix permitting faster multiplication process and requiring less memory .",
    "one such example is the randomly generated discrete fourier transform ( dft ) matrices or discrete cosine transform ( dct ) matrices @xcite . using dft as the measurement matrix , fast fourier transform ( fft ) can be used to perform the matrix multiplications at complexity of @xmath28 and the measurement matrix is _ not _ required to be stored .",
    "the entries of a dft matrix are however not i.i.d ..    in the noiseless setup ( i.e. , @xmath29 ) , it has been revealed that the measurement matrix enjoys the so - called universality property ; that is , measurement matrices with i.i.d .",
    "ensembles and rotationally invariant ( or row - orthonormal ) ensembles exhibit the same recovery capability ( or the phase transition ) @xcite . the universality phenomenon is further extended to the measurement matrices which are constructed by concatenating several randomly square orthonormal matrices @xcite .    although the universality phenomenon of lasso is known for a broad class of measurement matrices in the noiseless setup , little progress has been made in the practical _ noisy _ setting . in the noisy",
    "setting , perfect recovery is rare so we are interested in the ( average ) mean squared error ( mse ) of reconstruction defined by @xmath30 , where @xmath31 denotes the average with respect to @xmath32 and @xmath8 . in @xcite , an analytical expression for mse in lasso reconstruction was obtained when the measurement matrix @xmath23 is a row - orthonormal matrix generated uniformly at random . nevertheless , the emphasis of @xcite was in support recovery rather than the mse of reconstruction .",
    "it was not until very recently that the superiority of row - orthonormal measurement matrices over their i.i.d .",
    "gaussian counterparts for the noisy sparse recovery problem was revealed in @xcite .",
    "this characteristics is in contrast to the noiseless setup mentioned above .",
    "meanwhile , the authors of @xcite supported the similar argument and further argued that one can still claim universality in the noisy setup if we restrict the measurement matrices to similar row - orthonormal type .",
    "these arguments showed that the choice of measurement matrices does have an impact in the mse of reconstruction when noise is present . despite these previous works ,",
    "the study of lasso in the case of orthonormal measurement matrices remains incomplete , for the following reasons .",
    "first , in many applications of interest , the measurement matrices are constructed by selecting a set of columns and rows from a standard orthonormal matrix as depicted in figure [ fig : m - orthogonal](b ) , which we call it `` type - b '' matrix .",
    "let a standard orthonormal matrix be an @xmath33 unitary matrix .",
    "then we have @xmath34 in a type - b matrix .",
    "note that it is also possible to obtain row - orthonormal matrices by selecting a set of rows from orthonormal matrices rather than selecting a set of rows _ and _ columns , and in this case , we refer to such row - orthonormal matrix as `` type - a '' matrix , see figure [ fig : m - orthogonal](a ) .",
    "one prominent application of type - b matrices is the sparse channel estimator in orthogonal frequency - division multiplexing ( ofdm ) systems @xcite . in that case",
    ", @xmath8 represents a time - domain channel vector and @xmath23 is a partial dft matrix .",
    "another popular application arises in compressive sensing imaging , where randomly sampling a dft matrix from its row is common practice @xcite . nonetheless , _",
    "additional _ selection in columns is often needed because the signal size would be smaller than the size of available fft operators and the signal will be modified by zero - padding to fit the available fft size . in that case , the measurement matrix corresponds to the matrix formed by selecting a set of columns from row - orthonormal matrices .",
    "while type - b measurement matrices are widely employed in a vast number of sparse recovery problems , surprisingly , little is known on the lasso performance based on such measurement matrices .",
    "measurement matrix that is constructed by concatenating several randomly chosen orthonormal bases is another common type , which is referred to as `` type - c '' matrix .",
    "such construction can have several variations as shown in figure [ fig : m - orthogonal](c ) due to certain implementation considerations @xcite .",
    "for example , we can exploit parallelism or distributed computation of ( [ eq : proxgrad ] ) by using a parallel matrix - vector multiplication . in that case",
    ", each sub - block of the measurement matrix would be taken from a partial block of scrambled dft matrix . in this context",
    ", the authors of @xcite demonstrated that type - a matrix and type - c.1 matrix ( constructed by concatenating several randomly _ square _ orthonormal matrices ) have the same performance .",
    "except for @xcite , however , little progress has been made on this type of measurement matrix .    in this paper",
    ", we aim to provide analytical characterization for the performance of lasso under such measurement matrices .",
    "in particular , we derive the mse of lasso in the general type - c setup by using the replica method from statistical physics as in @xcite .",
    "our mse result encompasses type - a and type - b matrices as special cases .",
    "then we compare their performances and behaviors with those for random i.i.d .",
    "gaussian matrices .",
    "we will show that all the structurally orthogonal matrices ( including types a ",
    "c ) perform at least as well as random i.i.d .",
    "gaussian matrices over arbitrary setups .",
    "specifically , we have made the following technical contributions :    * we show that type - a matrix has the best mse performance out of all other types of structurally orthogonal matrices and performs significantly better than the i.i.d .  gaussian matrices .",
    "* in contrast to type - a matrices , the row - orthogonality in type - b is no longer preserved if @xmath35 .",
    "the mse performance of type - b matrices degrades with decreasing the ratio of @xmath36 while they still perform at least as good as their random i.i.d .",
    "gaussian counterparts .",
    "* we show that type - a , type - c.1 , and type - c.2 matrices have the same mse performance .",
    "specifically , _ horizontally _ concatenating multiple row - orthonormal matrices have the same mse performance as its single row - orthonormal counterpart .",
    "this argument extends the result of @xcite to the case of concatenating multiple row - orthonormal matrices .",
    "further , we reveal that the measurement matrices formed by concatenating several randomly orthonormal bases in _ vertical _ direction result in significant degradation . for example , type - c.4 and type - c.5 matrices have the worst performance among type - c matrices although they are at least as good as their random i.i.d .  gaussian counterparts .",
    "the remainder of the paper is organized as follows . in section",
    "ii , we present the problem formulation including fundamental definitions of the structurally orthogonal matrices . in section iii , we provide the theoretical mse results of lasso based on the structurally orthogonal matrices .",
    "simulations and discussions are presented in section iv and the main results are summarized in section v.    _ notations_throughout this paper , for any matrix @xmath23 , @xmath37_{i , j}$ ] refers to the @xmath38th entry of @xmath23 , @xmath39 denotes the transpose of @xmath23 , @xmath24 denotes the conjugate transpose of @xmath23 , @xmath40 ( or @xmath41 ) denotes the principal square root of @xmath23 , @xmath42 denotes the trace of @xmath23 , @xmath43 is the column vector with entries being the ordered stack of columns of @xmath23 .",
    "additionally , @xmath44 denotes an @xmath45-dimensional identity matrix , @xmath46 denotes a zero matrix of appropriate size , @xmath47 denotes an @xmath45-dimensional all - one vector , @xmath48 denotes the euclidean norm , @xmath49 denotes the indicator of the statement , @xmath50 represents the expectation operator with respect to @xmath51 , @xmath52 is the natural logarithm , @xmath53 denotes dirac s delta , @xmath54 denotes kronecker s delta , @xmath55 represents the extremization of a function @xmath56 with respect to @xmath57 , @xmath58 is the standard @xmath59-function , and @xmath60 denotes the kronecker product .",
    "we say that the complex random variable @xmath61 is a standard gaussian , if its density function is given by @xmath62 .",
    "that is , the standard complex gaussian is the circularly - symmetric complex gaussian with zero mean and unit variance .",
    "finally , @xmath63 denotes the complex - valued gaussian integration measure ; i.e. , for an @xmath64 vector , @xmath65 , we have @xmath66 where @xmath67 and @xmath68 extract the real and imaginary components , respectively .",
    "we consider the sparse signal recovery setup in ( [ eq : sysmodel ] ) , where @xmath32 is assumed to be the standard complex gaussian noise vector .",
    "in addition , let us suppose that @xmath69 where @xmath70 for @xmath71 , and @xmath72 $ ] is the fraction of non - zero entries in @xmath8 .",
    "that is , the elements of @xmath8 are sparse and are i.i.d .  generated according to @xmath73 .    for generality , we consider the measurement matrix @xmath23 made of different blocks as outlined in fig .",
    "[ fig : type - c - matrix ] , which we refer to it as type - c matrix .",
    "the structurally random matrix was also considered by @xcite in the context of compressive sensing for different purposes . in the setup , @xmath4 is constructed by vertical and horizontal concatenation of @xmath74 blocks as @xmath75,\\ ] ] where each @xmath76 is drawn independently from the haar measure of @xmath33 random matrix ( referred to as the _ standard _ orthonormal matrix in this paper ) . to shape @xmath77 in an @xmath78-dimensional matrix",
    ", we randomly select @xmath79 rows and @xmath80 columns from the standard orthonormal matrix . we denote @xmath81 and @xmath82 the `` column selection rate '' and `` row selection rate '' , respectively . also , we define @xmath83 and @xmath84 . to make the setup more flexible",
    ", we assume that for the @xmath85th subblock , the standard orthonormal matrix has been multiplied by @xmath86 . by setting the values of @xmath87 appropriately ,",
    "each block can be made either only zeros or a partial orthonormal matrix .",
    "corresponding to the measurement matrix made of different blocks , the @xmath88 variables of @xmath8 are divided into @xmath89 blocks @xmath90 with @xmath80 variables in each block . meanwhile",
    ", the @xmath91 measurements @xmath92 are divided into @xmath93 blocks @xmath94 with @xmath79 measurements in each block . note that we have @xmath95 and @xmath96 .",
    "the measurement ratio of the system is given by @xmath97 .",
    "to facilitate our analysis based on the tools in statistical mechanics , we use the approach introduced in @xcite to reformulate the @xmath7-regularized ls problem ( [ eq : rls ] ) in a probabilistic framework .",
    "suppose that the posterior distribution of @xmath98 follows the distribution @xmath99 where @xmath100 is a constant and @xmath101 is the partition function ( or normalization factor ) of the above distribution function .",
    "given the posterior probability of ( [ eq : postdis_rls ] ) , the bayes way of estimating @xmath98 is given by @xcite @xmath102 as @xmath103 , the posterior mean estimator ( [ eq : estx ] ) condenses to the global minimum of ( [ eq : rls ] ) , i.e. , @xmath104 .    in ( [ eq : estx ] ) , @xmath105 ( or equivalently @xmath106 ) is estimated from @xmath92 given that @xmath23 is perfectly known .",
    "clearly , @xmath106 depends on @xmath92 and thus is _",
    "random_. we are thus interested in the ( average ) mse of @xmath106 given by @xmath107 where @xmath108 denotes an average over @xmath92 . specifically , we define @xmath109 where @xmath110 is defined by ( [ eq : px0 ] ) , and @xmath111 is the conditional distribution of @xmath92 given @xmath8 under ( [ eq : sysmodel ] ) .",
    "our aim of this paper is to derive an analytical result for @xmath112 .    in the analysis of @xmath112",
    ", we consider @xmath113 , while keeping @xmath81 and @xmath82 fixed and finite for @xmath114 and @xmath115 . for convenience",
    ", we refer to this large dimensional regime simply as @xmath113 .",
    "notice that the mse depends on the measurement matrix @xmath23 .",
    "however , in the large regime @xmath113 , we expect ( or assume ) that the average mse appears to be self - averaging . that is , the mse for any typical realization of @xmath23 coincides with its average over @xmath23 .",
    "from ( [ eq : defmse ] ) , the posterior distribution @xmath116 plays a role in the mse . in statistical mechanics",
    ", the key for finding the mse is through computing the partition function , which is the marginal of @xmath116 , or its logarithm , known as _",
    "free entropy_. following the argument of @xcite , it can be shown that @xmath112 is a saddle point of the free entropy .",
    "thanks to the self - averaging property in the large dimensional regime , we therefore compute @xmath112 by computing the _ average _ free entropy @xmath117 the similar manipulation has been used in many different settings , e.g. , @xcite .",
    "the analysis of ( [ eq : freeen ] ) is unfortunately still difficult .",
    "the major difficulty in ( [ eq : freeen ] ) lies in the expectations over @xmath92 and @xmath23 .",
    "we can , nevertheless , greatly facilitate the mathematical derivation by rewriting @xmath118 as @xcite @xmath119 in which we have moved the expectation operator inside the log - function .",
    "we first evaluate @xmath120 for an integer - valued @xmath121 , and then generalize it for any positive real number @xmath121 .",
    "this technique is known as the replica method , which emerged from the field of statistical physics @xcite and has recently been successfully applied to information / communications theory literature @xcite .",
    "details of the replica calculation are provided in appendix a. we here intend to give an intuition on the final analytical results ( i.e. , proposition [ pro1 ] to be shown later ) .",
    "note that the approach presented here is slightly different from that in appendix a. basically , the replica analysis allows us to understand the characteristics of the errors made by lasso by looking at the signal reconstruction via an equivalent scalar version of the linear system ( [ eq : sysmodel ] ) : @xmath122 where the subscript @xmath123 indicates that the equivalent linear system characterizes the signal in block @xmath123 , i.e. @xmath124 , and there are @xmath80 parallel equivalent linear systems of ( [ eq : eqscalsysmodel ] ) in block @xmath123 ; the parameters @xmath125 are arisen in the replica analysis to be given later in proposition [ pro1 ] ; @xmath126 is a random signal generated according to the distribution @xmath127 , @xmath128 is standard complex gaussian ; and @xmath129 is the effective measurement .",
    "in particular , our analysis shows that the characteristics of lasso output corresponding to the signal , @xmath124 , can be analyzed via the lasso output of the signal @xmath126 through the effective measurement @xmath129 , where @xmath130 and @xmath131 play the role of the _ effective _ measurement gain and _ effective _ noise level .",
    "therefore , following ( [ eq : rls ] ) , the recovery of @xmath126 from @xmath129 by lasso becomes @xmath132 using ( * ? ? ?",
    "* lemma v.1 ) , the optimal solution @xmath133 of ( [ eq : rls_eqscalsysmodel ] ) reads @xmath134 note that @xmath135 depends on @xmath129 and is therefore random .",
    "then the mse of @xmath135 is given by @xmath136 , where @xmath137 denotes an average over @xmath129 with @xmath138 as there are @xmath80 parallel equivalent systems in block @xmath123 , the mse of lasso reconstruction in group @xmath123 is given by @xmath139 where the second equality is due to @xmath81 and @xmath140 , and the last equality follows from the fact that @xmath141 and @xmath142 . using ( [ eq : optofxp ] ) and ( [ eq : postdis_eqscalsysmodel ] ) and following the steps of ( * ? ? ?",
    "* ( 349)(357 ) ) , one can get the analytical expressions of @xmath143 and @xmath144 , and then result in an analytical expression of @xmath145 .",
    "we summarize the results in the following proposition .",
    "[ pro1 ] consider a type - c matrix being the measurement matrix .",
    "let @xmath145 denote the mse of lasso reconstruction in block @xmath114 , and define    [ eq : defgc ] @xmath146    then as @xmath113 , the average mse over the entire vector becomes @xmath147 where @xmath148 with    [ eq : mq_pro1 ] @xmath149    in ( [ eq : mq_pro1 ] ) , we have defined @xmath150 where @xmath151 the parameters @xmath152 and @xmath153 are the solutions of the coupled equations    [ eq : mqx_pro1 ] @xmath154    where @xmath155    see appendix a.    note that except for @xmath156 , the remaining parameters in proposition [ pro1 ] are arisen from the replica analysis and can be regarded auxiliary .",
    "the parameters @xmath157 have to be solved in ( [ eq : mqx_pro1 ] ) for all @xmath158 .",
    "proposition [ pro1 ] provides not only a new finding but also a unified formula that embraces previous known results @xcite .",
    "for example , the mse of lasso under type - a measurement matrix in @xcite can be obtained if we set @xmath159 and @xmath160 in propositions [ pro1 ] . clearly , by setting @xmath161 ,",
    "we are also able to further study the mse of lasso under type - b measurement matrix . in the next section",
    ", we will discuss the mses of lasso under type - a and type - b measurement matrices and we compare their performances and behaviors with those for random i.i.d .",
    "gaussian matrices .",
    "another existing result is related to the type - c.1 measurement matrix in @xcite in which @xmath162 and @xmath163 for @xmath114 . in @xcite , a type - c.1 orthogonal matrix",
    "is referred to as the @xmath164-orthogonal matrix as the matrix is constructed by concatenating @xmath164 independent standard orthonormal matrices . also , @xcite only considered the real - valued setting , where the signal @xmath165 , the measurements @xmath92 , and the measurement matrix @xmath23 are all real - valued . in this case , the @xmath7-norm is defined as @xmath166 , which is different from the complex @xmath7-norm ( see footnote @xmath167 ) . in the real - valued setting ,",
    "the analytical mse expression of lasso in proposition [ pro1 ] also holds while @xmath168 and @xmath169 in ( [ eq : defgc ] ) should be replaced by    [ eq : defgr ] @xmath170    the difference between ( [ eq : defgc ] ) and ( [ eq : defgr ] ) is significant , and can be understood from ( [ eq : rls_eqscalsysmodel ] ) by considering its real - valued counterpart . in the real - valued setting of ( [ eq : rls_eqscalsysmodel ] ) , @xmath171 and @xmath129 are real - valued , and the optimal solution becomes @xmath172 , which is quite different from its complex - valued counterpart in ( [ eq : optofxp ] ) .",
    "this difference turns out to be reflected on @xmath143 and @xmath173 and thus on @xmath168 and @xmath169 of ( [ eq : mq_pro1 ] ) .",
    "in particular , the mse of lasso with the @xmath164-orthogonal matrix @xcite can be perfectly recovered if we set @xmath162 , and those @xmath163 and @xmath174 for @xmath114 in proposition [ pro1 ] and replace @xmath168 and @xmath169 by @xmath175 and @xmath176 , respectively .",
    "clearly , proposition [ pro1 ] provides a unified result that allows us to quantify the mse of lasso under a variety of measurement matrices .",
    "we will present detailed discussions in the next section .",
    "in this subsection , we aim to study the mse of lasso under type - a and type - b measurement matrices .",
    "in particular , we will compare their performances and behaviors with those for random i.i.d .",
    "gaussian matrices . to make comparison fair between different setups , all cases of the measurement matrices are normalized so that @xmath177 ( referred to as the power constraint of the measurement matrix ) . if the elements of @xmath23 are i.i.d .",
    "gaussian random variables with zero mean and variance @xmath178 , then the power constraint of the measurement matrix is satisfied .",
    "we call this matrix the i.i.d .  gaussian matrix . on the other hand ,",
    "if @xmath23 is a type - a matrix , the power constraint of the measurement matrix is naturally satisfied , and in fact , it satisfies the more stringent condition @xmath179 .",
    "meanwhile , in the type - b setup , we set the gain factor @xmath180 to satisfy this power constraint .",
    "since there is only one block , i.e. , @xmath181 , in the type - a and type - b setups , we omit the block index @xmath85 from all the concerned parameters hereafter , and proposition [ pro1 ] is anticipated to be greatly simplified .",
    "the mse of lasso under type - b orthogonal measurement matrix is given as follows .",
    "[ cor1 ] with the type - b orthogonal measurement matrix , the mse of lasso is given by @xmath182 , where @xmath183 are same as those in ( [ eq : mq_pro1 ] ) while the block index @xmath123 is omitted .",
    "the parameters @xmath183 are functions of @xmath184 which can be obtained by solving the following set of equations    [ eq : par1_typeb ] @xmath185    with the following definitions    [ eq : par2_typeb ] @xmath186    the above results can be obtained by substituting the corresponding parameters of type - b setup , i.e. , @xmath181 , into proposition [ pro1 ] .",
    "in addition , using ( [ eq : def_delta ] ) and ( [ eq : mqx_pro1a ] ) , we have eliminated @xmath187 .",
    "let us first consider the type - a setup , where we have @xmath188 and @xmath189 . if we set the gain factor @xmath190 , then we have @xmath191 . then ( [ eq : par2_typeb ] ) can be further simplified in the form    [ eq : par2_typea ] @xmath192    recall that in the real - valued setting , @xmath168 and @xmath169 in ( [ eq : defgc ] ) should be replaced by @xmath175 and @xmath176 in ( [ eq : defgr ] ) . in this case",
    ", the above result gives exactly the same mse result as reported in ( * ? ? ?",
    "* example 2 ) . it should be noticed that the setting of `` @xmath190 '' is used above to align the setting of @xcite . according to the power constraint of the measurement matrix in this paper",
    ", we should set @xmath193 rather than @xmath190 .    before proceeding , we present numerical experiments to verify our theoretical results . in the experiments ,",
    "type - a and type - b matrices were generated from a randomly scrambled @xmath194 dft matrix with @xmath195 .",
    "the proximal gradient method ( [ eq : proxgrad ] ) was used to solve lasso and obtain the reconstruction @xmath106 .",
    "the experimental average mse was obtained by averaging over @xmath196 independent realizations . to form a measurement matrix , the selected column and row sets at each realization",
    "were changed randomly .",
    "the experimental average mses of lasso under different selecting rates @xmath197 are listed in table [ tab : difselrate ] in which the theoretical mse estimates by corollary [ cor1 ] are also listed for comparison , with the parameters : @xmath198 , @xmath199 , @xmath200 , and @xmath201 . in table",
    "[ tab : difregpar ] , we fixed the selecting rate @xmath202 and repeated the previous experiment with different regularization parameters @xmath203 . finally , in table [ tab : difnoiselevel ] , we fixed the selecting rate @xmath204 and regularization parameter @xmath199 and repeated the experiment with different noise levels @xmath205 .",
    "we see that for all the cases , the differences between the two estimates are inappreciable .",
    "therefore , corollary [ cor1 ] provides an excellent estimate of the mse of lasso in large systems .",
    ".comparison between experimental and theoretical mses of lasso under different selecting rates @xmath197 for @xmath198 , @xmath199 , @xmath206 , and @xmath201 . [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]      +    [ tab : fourcases ]    next , we use the theoretical expression to examine the behaviors of mses under type - c measurement matrices . in figure",
    "[ fig : mse - fourcases ] , we compare the mses of lasso as a function of the regularization parameter @xmath203 for the four cases depicted in figure [ fig : fourcases ] .",
    "the mses for type - a and the i.i.d .",
    "gaussian counterparts are also plotted as references . as can be seen",
    ", type - a setup always gives the best mse result while the i.i.d .",
    "gaussian setup yields the worst mse result . however , type - a setup would not be always useful if the corresponding size of the fft operators is not available in some dsp chips . also , we see that case-1 and case-2 always have the same mse behaviors .",
    "this finding motivates us to get the following observation that can meet the same performance of type - b matrix via concatenating orthonormal bases .",
    "[ obs1 ] consider type - b measurement matrix with the column and row selection rates @xmath197 and @xmath207 .",
    "the mse of lasso under this measurement matrix is identical to that under the horizontal concatenation of @xmath89 matrices where each matrix is from a partial orthonormal matrix with the column and row selection rates @xmath197 and @xmath208 . for a meaningful construction , @xmath89",
    "should be subjected to @xmath209 .    to see an application of this observation ,",
    "let us take two examples .",
    "first , consider type - b measurement matrix with @xmath210 and @xmath211 . applying observation [ obs1 ] , we have that the mse of lasso under the row - orthonormal measurement matrix is identical to that under the measurement matrix of @xmath212 $ ] with each @xmath213 being a square orthogonal matrix .",
    "this argument was also revealed by @xcite .",
    "it is noted that the columns of each @xmath213 are orthogonal so that there is no interference within each square orthogonal matrix .",
    "the interference resulting from the other sub - block of the measurement matrix will degenerate the mse performance .",
    "therefore , we can infer that the matrix constructed by concatenating many square orthonormal matrices should lose its advantage over the i.i.d .  gaussian matrix .",
    "in other words , if the measurement ratio is small , i.e. , @xmath214 , the mses of lasso under the row - orthonormal measurement matrix and the i.i.d .",
    "gaussian matrix should be comparable .",
    "this inference also seems reasonable from the aspect of eigenvalue spectrum @xcite that : when @xmath214 , an i.i.d .",
    "gaussian matrix has approximately orthogonal rows and it behaves similar to a row - orthonormal matrix .",
    "next , let us consider another example that type - b measurement matrix is with @xmath215 and @xmath216 . with observation [ obs1 ]",
    ", we have that the mse of lasso under this measurement matrix is identical to that under the measurement matrix of @xmath217 $ ] with each @xmath213 being a partial orthogonal matrix with @xmath218 and @xmath219 . in this case , the columns of each @xmath213 are not orthogonal any more but _ nearly _ orthogonal .",
    "therefore , we can expect some performance degeneration under this measurement matrix .",
    "finally , it is clear that case-1 and case-2 in figure [ fig : mse - fourcases ] have the same mse behaviors but case-2 has a better structure in the parallel computation and less requirement in the size of the fft operator .    from figure [ fig : mse - fourcases ] , we also observe that the measurement matrix constructed by vertical _ and _ horizontal concatenation of several blocks , i.e. , case-4 , has the worst performance among the structurally orthogonal matrices . as a matter of fact ,",
    "if we continue to increase the number of concatenation blocks , then their mse performances will degrade accordingly .",
    "however , in any cases , they are at least as good as their random i.i.d .",
    "gaussian counterparts .",
    "this observation hence provides us another way to meet the random i.i.d .",
    "gaussian matrix via vertically and horizontally concatenating orthonormal bases .    finally , comparing the four cases in figure [ fig : mse - fourcases ]",
    ", we notice that if type - a matrix is not available , case-3 provides the best mse result .",
    "this observation together with the previous experiments indicate that to construct a measurement matrix aiming for a good mse performance in lasso formulation , one should follow the example of case-3 .",
    "that is to say , first try to use a row - orthogonal matrix that can best fit the dimension of the measurement matrix and then horizontally concatenate the remaining part .",
    "we have investigated the mse performance of estimating a sparse vector through an undersampled set of noisy linear transformations when the measurement matrix is constructed by concatenating several randomly chosen orthonormal bases and lasso formulation is adopted . using the replica method in conjunction with some novel matrix integration results , we derived the theoretical mse result .",
    "extensive numerical experiments have illustrated excellent agreement with the theoretical result .",
    "our numerical results also revealed the fact that the structurally orthogonal matrices are at least as well performed as the i.i.d .",
    "gaussian matrices .",
    "in particular , we have made the following observations :    * type - a matrices ( or row - orthogonal matrices ) have the best mse performance out of all the other types of structurally orthogonal matrices and is significantly better than the i.i.d .",
    "gaussian matrices . * the advantage of the row - orthogonal matrix over the i.i.d .",
    "gaussian matrix is still preserved even when a random set of columns is removed ( which leads to a type - b matrix ) . when increasing the number of the removed columns , the mse of lasso degenerates to the case of the i.i.d .",
    "gaussian matrices .",
    "in particular , we have shown that the asymptotic eigenvalue distribution of type - b matrix with small column selection rate converges to that of the i.i.d .  gaussian matrix . * in addition , a measurement matrix obtained by orthogonal matrix constructions has fast computation and facilitates parallel processing . for this purpose , we have provided a technique to meet the same performance of type - b matrix via horizontally concatenating orthogonal bases .",
    "our argument is more systematic than @xcite and leads to much wider applications .",
    "* on the other hand , we have shown that the measurement matrix constructed by vertical concatenation of blocks usually gets the worst performance compared to the horizontal concatenation .",
    "however , they are at least as good as their random i.i.d .",
    "gaussian counterparts .    as a consequence ,",
    "we conclude that in addition to the ease of implementation , the structurally orthogonal matrices are preferred for practical use in terms of their good estimation performance .",
    "it was reported that orthogonal measurement matrices also enhance the signal reconstruction threshold in the noisy setups when the optimal bayesian recovery is used @xcite .",
    "promising future studies include performance evaluation under the optimal bayesian recovery and development of recovery algorithms suitable for the structurally orthogonal matrices @xcite .",
    "first , recall that we have rewritten the average free entropy @xmath118 in ( [ eq : avgfreeen ] ) by using the replica identity . within the replica method",
    ", it is assumed that the limits of @xmath220 and @xmath221 can be exchanged .",
    "we therefore write @xmath222 we first evaluate @xmath120 for an integer - valued @xmath121 , and then generalize it for any positive real number @xmath121 . in particular , given the partition function of ( [ eq : partfun_b ] ) , we obtain @xmath223 with @xmath224 . using the @xmath121-th moment of the partition function and @xmath225 in ( [ eq : postdis_sys ] )",
    ", we have @xmath226 where @xmath227)$ ] with @xmath228 being the @xmath229-th replica signal vector of @xmath230 , and @xmath231 with @xmath232 $ ] .",
    "the equality of ( [ eq : replicpartfun_b ] ) follows from the fact that @xmath233 is a random vector taken from the input distribution @xmath234 in ( [ eq : px0 ] ) if @xmath235 and @xmath236 otherwise , and @xmath237 if @xmath235 and @xmath238 otherwise .    before proceeding ,",
    "we introduce the following preprocessing to deal with the cases in which @xmath77 is a randomly sampled orthogonal matrix ( or , deleting row / columns independently ) . in particular",
    ", we find that it is convenient to work with the enlarged orthogonal matrix @xmath239 with rows and columns setting to zero rather removed @xcite . for clarity , we use the following definition .",
    "@xcite a square matrix is called a diagonal projection matrix if its off - diagonal entries are all zeros and its diagonal entries are zeros or ones .",
    "let @xmath240 and @xmath241 be @xmath33 diagonal projection matrices , where the numbers of nonzero diagonal elements of @xmath240 and @xmath241 are @xmath79 and @xmath80 , respectively .",
    "therefore , we characterize each block by @xmath242 where @xmath243 is an @xmath33 standard orthonormal matrix . since @xmath244 are independent standard orthonormal matrices , the positions of nonzero elements of the diagonal projection matrices are irrelevant . for the sake of simplicity , we assume that all the diagonal entries @xmath167 of @xmath240 and @xmath241 appear first , i.e. , @xmath245~~\\mbox{and}~~   { { \\bf t}}_{q , p } = \\left [ \\begin{array}{cc } { { \\bf i}}_{n_p } & { { \\bf 0}}\\\\ { { \\bf 0 } } & { { \\bf 0}}\\end{array}\\right ] , ~\\forall p , q.\\ ] ] recall the type - c matrix in section ii that the standard orthonormal matrix has been multiplied by @xmath86 .",
    "the gain factor @xmath87 can be included in @xmath240 via a scaling factor . for notational convenience , here",
    ", we do not use the expression of @xmath246 but absorb @xmath87 into @xmath240 .",
    "also , we enlarge @xmath124 and @xmath247 to be @xmath248-dimensional vectors by zero padding . as a consequence",
    ", the input - output relationship of ( [ eq : sysmodel ] ) can be equivalently expressed as @xmath249}_{\\triangleq{{\\tilde{{{\\bf y } } } } } }   = \\underbrace{\\left[\\begin{array}{ccc } & | & \\\\ - & { { \\tilde{{{\\bf a}}}}}_{q , p } & - \\\\ & | & \\end{array}\\right]}_{\\triangleq{{\\tilde{{{\\bf a } } } } } }      \\underbrace{\\left[\\begin{array}{c } | \\\\ { { \\bf x}}_q \\\\ | \\end{array}\\right]}_{\\triangleq{{\\tilde{{{\\bf x } } } } } }   + \\sigma_0 \\underbrace{\\left[\\begin{array}{c } | \\\\{{\\bf w}}_q \\\\",
    "| \\end{array}\\right]}_{\\triangleq{{\\tilde{{{\\bf w}}}}}}.\\ ] ] notice that all the following derivations are based on the enlarged system ( [ eq : enlargedsysmodel ] ) .",
    "therefore , by abuse of notation , we continue to write @xmath124 , @xmath247 , @xmath77 , @xmath98 , @xmath92 , and @xmath23 for @xmath250 , @xmath251 , @xmath239 , @xmath252 , @xmath253 , and @xmath254 , respectively .",
    "next , we introduce a random vector per block @xmath255 the covariance of @xmath256 and @xmath257 is a @xmath258 hermitian @xmath259 with entries given by @xmath260_{a , b } .\\ ] ] for ease of exposition , we further write @xmath261 , @xmath262 , and @xmath263 .    now , we return to the calculation of ( [ eq : replicpartfun_b ] ) . in ( [ eq : replicpartfun_b ] )",
    ", the expectations introduce iterations between @xmath98 and @xmath23 .",
    "however , the resulting iterations depend only on the covariance as those shown in ( [ eq : defq ] ) .",
    "therefore , it is useful to separate the expectation over @xmath264 into an expectation over all possible covariance @xmath265 and all possible @xmath264 configurations with respect to a prescribed set of @xmath265 by introducing a @xmath266-function . as a result , ( [ eq : replicpartfun_b ] ) can be rewritten as @xmath267 where @xmath268 and @xmath269_{a , b}\\right ) } _ { { { \\bf x } } } { { \\rm d}}{{\\bf q}}.\\ ] ] next , we focus on the calculations of ( [ eq : g1 ] ) and ( [ eq : measure_mu ] ) , respectively .",
    "let us first consider ( [ eq : g1 ] ) . integrating over @xmath270 s in ( [ eq : g1 ] ) by applying lemma [ lemma_gaussianintegr ] yields @xmath271 where @xmath272.\\ ] ]    next , we consider ( [ eq : measure_mu ] ) . through the inverse laplace transform of @xmath266-function , we can show that @xmath273 where @xmath274 is the rate measure of @xmath275 and is given by @xcite @xmath276 with @xmath277 being a symmetric matrix and @xmath278 . inserting ( [ eq : measure_muinn ] ) into ( [ eq : sf_e2 ] ) yields @xmath279 .",
    "therefore , as @xmath280 , the integration over @xmath265 can be performed via the saddle point method , yielding @xmath281 .\\ ] ]    substituting ( [ eq : g3 ] ) and ( [ eq : rate_fun1 ] ) into ( [ eq : qf ] ) , we arrive the free entropy ( [ eq : limf_appa ] ) at the saddle - point asymptotic approximation @xmath282 where @xmath283 and    [ eq : tqq ] @xmath284    the saddle - points can be obtained by seeking the points of zero gradient of @xmath285 with respect to @xmath265 and @xmath286 .      rather than searching for the saddle - points over general forms of @xmath265 and @xmath286 , we invoke the following hypothesis : the dependence on the replica indices would not affect the physics of the system because replicas have been introduced artificially for the convenience of the expectation operators over @xmath92 and @xmath23 . it therefore seems natural to assume _ replica symmetry _",
    "( rs),_{0,0}=\\tilde{r}_{q , p}$ ] similar to that of @xmath287_{0,0}$ ] .",
    "it turns out that when @xmath288 , we get @xmath289 .",
    "therefore , to simplify notation , we set @xmath289 at the beginning .",
    "in addition , it is natural to let @xmath290 and @xmath291 be complex - valued variables .",
    "we will find that the whole exponents will depend only on the real part of @xmath290 , and @xmath291 turns out to be a real - valued variable .",
    "therefore , we let @xmath290 and @xmath291 be real - valued variables at the beginning . ] i.e. , @xmath292 , \\label{eq : rs_q } \\\\",
    "\\tilde{{\\bf q}}_{q , p}&= \\left [ \\begin{array}{cc } 0 & { { \\tilde{m}}}_{q , p } { { \\bf 1}}_{\\tau}^t \\\\ { { \\tilde{m}}}_{q , p } { { \\bf 1}}_{\\tau } & ( { { \\tilde{q}}}_{q , p}-{{\\tilde{q}}}_{q , p}){{\\bf i}}_{\\tau } + { { \\tilde{q}}}_{q , p}{{\\bf 1}}_{\\tau } { { \\bf 1}}_{\\tau}^t \\end{array } \\right ] .",
    "\\label{eq : rs_tq}\\end{aligned}\\ ] ] this rs has been widely accepted in statistical physics @xcite and used in the field of information / communications theory , e.g. , @xcite .    by lemma [ lemma_eigprojectionmatrix ]",
    ", we can show that for the rs of ( [ eq : rs_q ] ) , the eigenvalues of @xmath293 are given by for @xmath294 and @xmath259 , which is rather laborious but straightforward . for readers convenience ,",
    "we detail the calculation in appendix b. ]    [ eq : eigofsandq ] @xmath295    write @xmath296 , where @xmath297 $ ] is an @xmath298 orthogonal matrix .",
    "recall the covariance matrix of @xmath299 defined in ( [ eq : defq ] ) . from the linear algebra",
    ", one can easily obtain that @xmath300 is a vector with length @xmath301 for @xmath302 .",
    "applying lemma [ lemma_haarintegr ] to ( [ eq : tqq1 ] ) , we get @xmath303 where @xmath304 the solution to the extremization problem in ( [ eq : defgq ] ) , denoted by @xmath305 , enforces the condition @xmath306 where @xmath307    next , we focus the rs calculation of ( [ eq : tqq3 ] ) . substituting the rs form for @xmath308 in ( [ eq : rs_q ] ) and the definition of @xmath241 in ( [ eq : def_rt ] ) into ( [ eq : tqq3 ] ) , we obtain @xmath309 where @xmath310 then we decouple the first quadratic term in the exponent of ( [ eq : lt_rate_fun1 ] ) by using the hubbard - stratonovich transformation ( lemma [ lemma_gaussianintegr ] ) and introducing the auxiliary vector @xmath128 , to rewrite ( [ eq : lt_rate_fun1 ] ) as @xmath311 where the equality follows from the fact that @xmath233 is a random vector taken from the input distribution @xmath312 if @xmath313 .",
    "lastly , using ( [ eq : rs_q ] ) and ( [ eq : rs_tq ] ) into ( [ eq : tqq4 ] ) , we obtain @xmath314    recall that we have denoted @xmath315 . before proceeding ,",
    "we introduce the rescaled variables as @xmath316 and we define @xmath317 , @xmath318 , @xmath319 . using these variables into ( [ eq : rs_g ] ) , ( [ eq : lt_rate_fun2 ] ) , ( [ eq : rsqq ] ) , we obtain @xmath320 where we have defined @xmath321    substituting ( [ eq : rs_phi1_rescaled])([eq : rs_phi4_rescaled ] ) into ( [ eq : rs_fg ] ) , taking the derivative of @xmath285 with respect to @xmath121 , and letting @xmath322 , we find @xmath323 , where    [ eq : genfree_beta ] @xmath324    in ( [ eq : genfree_beta2 ] ) , we have used the fact that @xmath325 as @xmath103 .",
    "also , we have used the following result @xmath326 where the equality follows directly from taking the derivative of @xmath327 in ( [ eq : defgq ] ) . notice that when substituting @xmath328 into ( [ eq : extrgq ] ) , we get @xmath329 this identity will be used later to simplify some expressions . also , as @xmath103 , @xmath330 and @xmath331 .",
    "now , recall that we have to search @xmath265 and @xmath286 which achieve the extremal condition in ( [ eq : rs_fg ] ) . with the rs assumption",
    ", we only have to determine @xmath332 , which can be obtained by equating the partial derivatives of @xmath285 to zeros , i.e. , @xmath333 and then letting @xmath322 . evaluating these calculations , we obtain    [ eq : sdpoint_beta ] @xmath334    where @xmath335 and @xmath336    following @xcite , the expression of @xmath337 can be obtained via the inverse function theory @xmath338^{-1},\\ ] ] where @xmath339 is the jacobian matrix with its @xmath38th element being @xmath340 where the first equality follows from ( [ eq : difgq ] ) .",
    "in addition , @xmath341 can be explicitly obtained by applying the matrix inverse lemma .",
    "specifically , we have @xmath342_{p , r } = \\frac{1}{\\lambda } \\left(\\frac{1}{\\nu_{q } } \\frac{\\delta_{q , p}\\delta_{q , r}\\gamma_{q , p}\\gamma_{q , r}}{(1 - 2\\delta_{q , p})(1 - 2\\delta_{q , r } ) } \\left ( 1 + \\sum_{l=1}^{l_c } \\frac{1}{\\nu_{q } } \\frac{\\delta_{q , l}^2}{1 - 2\\delta_{q , l}}\\right)^{-1 } - \\frac{\\gamma_{q , r}^2}{1 - 2\\delta_{q , r } } \\delta_{p , r } \\right ) .",
    "\\label{eq : gammamatrixinvlemma}\\ ] ]    to get more explicit expressions for @xmath343 and @xmath344 , let us simplify @xmath345 in ( [ eq : genfree_beta4 ] ) . as @xmath103 , we obtain @xmath346 where we have used @xmath347 and have used the identity @xmath348 in ( [ eq : sdpoint_beta-3 ] ) to simplify the result .",
    "the optimal solution @xmath133 in ( [ eq : phibeta_minxp ] ) is given by ( see ( * ? ? ?",
    "* lemma v.1 ) for a derivation ) @xmath349 if we substitute the optimal solution ( [ eq : optofx ] ) into ( [ eq : phibeta_minxp ] ) , we get @xmath350 where we have defined @xmath351    notice that @xmath352 and @xmath353 are standard gaussian random vectors with i.i.d .  entries for all @xmath123 . therefore , let @xmath61 denote the standard gaussian random random variable .",
    "then ( [ eq : genfree_beta4 ] ) turns out to be @xmath354    to deal with the partial derivatives of @xmath345 , let us first define @xmath146 following the manipulations as those in ( * ? ? ? * ( 350)(353 ) ) , we have the following useful identities : @xmath355 and @xmath356 after assessing the partial derivatives of @xmath118 ( or more precisely @xmath357 ) with respect to the variables @xmath358 , we obtain    [ eq : sdpoint_beta2 ] @xmath359    in addition , directly from the definition of @xmath259 in ( [ eq : defq ] ) , we have @xmath360 . substituting @xmath361 and ( [ eq : sdpoint_beta ] ) into ( [ eq : defmmse_qp_beta ] )",
    ", we obtain @xmath343 . notice that @xmath290 , @xmath362 , @xmath344 and @xmath343 are irrelevant to index @xmath363 .",
    "we denote @xmath364 , @xmath365 , @xmath366 , and @xmath367 for clarity .    combining the definition in ( [ eq : defdelta ] ) , the result in ( [ eq : gammamatrixinvlemma ] ) , and all the coupled equations in ( [ eq : sdpoint_beta ] ) and ( [ eq : sdpoint_beta2 ] ) , we get the result in proposition [ pro1 ] . notice that in proposition [ pro1 ] , we have used the rescaled variable @xmath368 for the sake of notational simplicity .",
    "applying lemma [ lemma_eigprojectionmatrix ] to @xmath294 and @xmath259 , we get @xmath369 { { \\bf u}}^h,\\ ] ] where    @xmath370 , \\\\ { { \\bf b}}_{2,2 } & = \\frac{q_{q , p}-q_{q , p}}{\\sigma^2 } { { \\bf i}}_{\\tau-1}.\\end{aligned}\\ ] ]    it is easy to see that the rows of @xmath371 are linearly dependent , and the eigenvalues are @xmath372 and @xmath21 . therefore , the eigenvalues of @xmath293 are in the form of ( [ eq : eigofsandq ] ) .",
    "for readers convenience , we provide some mathematical tools needed in this appendix .",
    "( gaussian integral and hubbard - stratonovich transformation ) [ lemma_gaussianintegr ] let @xmath65 and @xmath373 be @xmath248-dimensional real vectors , and @xmath23 an @xmath374 positive definite matrix .",
    "then @xmath375 using this equation from right to left is usually called the _ hubbard - stratonovich _ transformation .",
    "[ lemma_eigprojectionmatrix ] for a matrix @xmath376 \\in { { \\mathbb c}}^{(\\tau+1 ) \\times ( \\tau+1)},\\ ] ] the eigen - decomposition of the matrix is given by @xcite @xmath377 { { \\bf u}}^h,\\ ] ] where @xmath378 $ ] denotes a @xmath379-dimensional orthonormal basis composed of @xmath380^t$ ] , @xmath381^t$ ] and @xmath382 orthonormal vectors @xmath383 , which are orthogonal to both @xmath384 and @xmath385 .",
    "[ lemma_haarintegr ] let @xmath386 be a set of vectors that satisfy @xmath387 for some non - negative real values @xmath388 , @xmath389 be a set of independent haar measure of random matrices , and @xmath390 be a set of positive - semidefinite matrices .",
    "define @xmath391 then for large @xmath248 , we have @xmath392 this lemma extends ( * ? ? ?",
    "* lemma 1 ) to deal with the formula of ( [ eq : integroforthrm ] ) when @xmath393 and @xmath394 are the haar measure of _ complex _ random matrices .      from the definition of @xmath395 and @xmath230 ,",
    "the vector @xmath396 can be considered to be uniformly distributed on a surface of a sphere with radius @xmath397 for each @xmath123 .",
    "then the joint probability density function ( pdf ) of @xmath398 is given by @xmath399 where @xmath400 is the normalization factor and @xmath401 is a set of complex numbers .",
    "the normalization factor is given by @xmath402 using the gaussian integration formula ( i.e. , lemma [ lemma_gaussianintegr ] ) with respect to @xmath403 , the normalization factor becomes @xmath404 since we are interested in the large @xmath248 analysis , the saddle - point method can further simplify the normalization factor to the form @xmath405 where the second equality is obtained by solving the extremization problem .    next , we deal with the calculation of @xmath406 by writting @xmath407 where the second equality follows from the definition of the joint pdf of @xmath398 . applying the hubbard - stratonovich transformation ( lemma [ lemma_gaussianintegr ] ) together with the expressions ( [ eq : pu_p ] ) and ( [ eq : zfinal ] ) to the above provides @xmath408\\\\ + \\log\\frac{\\sigma^2}{\\pi }   - \\frac{1}{n}\\log { { \\sf z}}.\\end{gathered}\\ ] ] using the gaussian integration repeatedly with respect to @xmath398 and @xmath65 yields @xmath409 where the third equality is obtained by applying the saddle - point method .                                        , `` observed universality of phase transitions in high - dimensional geometry , with implications for modern data an alysis and signal processing , '' _ philos . trans .",
    "london a , math .",
    "1906 , pp . 42734293 , nov .",
    "2011 .",
    "h.  nishimori , _ statistical physics of spin glasses and information processing : an introduction_.1em plus 0.5em minus 0.4emser . number 111 in int .",
    "series on monographs on physics .",
    "oxford u.k .",
    ": oxford univ . press , 2001 ."
  ],
  "abstract_text": [
    "<S> in this paper , we consider a compressed sensing problem of reconstructing a sparse signal from an undersampled set of noisy linear measurements . </S>",
    "<S> the regularized least squares or least absolute shrinkage and selection operator ( lasso ) formulation is used for signal estimation . </S>",
    "<S> the measurement matrix is assumed to be constructed by concatenating several randomly orthogonal bases , referred to as structurally orthogonal matrices . </S>",
    "<S> such measurement matrix is highly relevant to large - scale compressive sensing applications because it facilitates fast computation and also supports parallel processing . using the replica method from statistical physics </S>",
    "<S> , we derive the mean - squared - error ( mse ) formula of reconstruction over the structurally orthogonal matrix in the large - system regime . </S>",
    "<S> extensive numerical experiments are provided to verify the analytical result . </S>",
    "<S> we then use the analytical result to study the mse behaviors of lasso over the structurally orthogonal matrix , with a particular focus on performance comparisons to matrices with independent and identically distributed ( i.i.d . ) gaussian entries . </S>",
    "<S> we demonstrate that the structurally orthogonal matrices are at least as well performed as their i.i.d .  </S>",
    "<S> gaussian counterparts , and therefore the use of structurally orthogonal matrices is highly motivated in practical applications .    </S>",
    "<S> compressed sensing , lasso , orthogonal measurement matrix , the replica method . </S>"
  ]
}