{
  "article_text": [
    "graphs are becoming ubiquitous in modern applications spanning bioinformatics , social networks , search , computer vision , natural language processing , etc . computing meaningful similarity measure between graphs",
    "is a crucial prerequisite for a variety of learning algorithms operating on graph data .",
    "this notion of similarity typically varies with the application . in designing similarities ( e.g. , kernels ) between graphs , it is desirable to have a measure which incorporates the rich structural information and is not affected by spurious transformations like reordering of vertices .",
    "note that , in certain applications , graphs can come with additional label information such as node or edge labels  @xcite .",
    "these additional annotations are not always available in every domain ( e.g. social networks ) and are typically expensive to obtain . in this paper",
    ", we focus only on the basic graph structures , without assuming any additional information .",
    "a common approach for computing kernels is to extract an explicit feature map from the graph , and then compute the kernel values via certain standard operations between features ( e.g. , inner products ) .",
    "this line of techniques typically make use of * graph invariants *  @xcite such as eigenvalues of graph laplacian as features .",
    "for example ,  @xcite which uses harmonic analysis techniques to extract a set of graph invariants .",
    "it was shown that a simple linear kernel , i.e. , dot product between these graph invariant numbers , outperforms many other graph kernels .",
    "alternatively , one can design a kernel function @xmath0 given graphs @xmath1 and @xmath2 , directly using `` similarity '' between them  @xcite .",
    "for example , the random walk kernel  @xcite is based on counting common random walks between two given graphs .",
    "another example is the shortest - path kernel  @xcite which is based on counting pairs of vertices , between two graphs , having similar shortest distance between them .",
    "although random walk kernels and path based kernels are still among the widely adopted graph kernels , one common disadvantage with them is that walks and paths do not capture information of the substructures present in the graph  @xcite . to address this problem ,",
    "a flurry of interest arose on kernels based on counting common subgraph patterns . counting all possible common subgraphs",
    "was known to be @xmath3-complete  @xcite .",
    "this led to the development of graph kernels focusing only on counting small subgraphs ; for example ,  @xcite counts common subgraphs with only 1 , 2 , or 3 nodes also called as graphlets . this kind of technique is very popular in social network classification .",
    "recently ,  @xcite used histograms of size four subgraphs for classifying facebook social networks . however , simply counting common substructures like walks , paths , subgraphs , etc . , ignores some crucial relative information between the substructures .",
    "for instance , the information of how different triangles are relatively embedded in the graph structure can not be captured by simply counting the number of triangles .",
    "this relative information , as we show in this paper , is necessary for discriminating between different graph structures .",
    "this paper follows an altogether different approach .",
    "we represent a graph as an expressive functional object .",
    "we first use the dynamical properties of the graph adjacency matrix to construct an informative summary of the graph .",
    "we then impose a probability distribution over the summary , and we show that this distribution is a graph invariant .",
    "bhattacharyya kernel between the obtained distribution , which we call * power kernel * , significantly outperforms other well - known graph kernels on standard benchmark graph classification datasets .",
    "in addition , we show that , unlike other kernels , most of which require @xmath4 time to compute ( where @xmath5 is the number of nodes ) , our kernel can be computed in time linear in the number of edges ( which is at most @xmath6 ) .",
    "this makes the proposed methodology significantly more practical for larger graphs .",
    "given a graph @xmath7 with @xmath5 nodes , we denote its adjacency matrix by @xmath8 . in this paper ,",
    "entries of @xmath9 are binary ( 0/1 ) , i.e. , @xmath10 means there is an edge between node @xmath11 and node @xmath12 .",
    "we interchangeably use terms nodes and vertices , and terms graph @xmath7 and adjacency matrix @xmath9 .",
    "the graph will always be assumed to be unlabeled , undirected and unweighted with default @xmath5 number of nodes , unless otherwise specified .",
    "we use @xmath13 for a vector of all ones . by vector , we mean column vector ,",
    "i.e. , @xmath14 matrix .    to avoid overloading subscripts",
    ", we will follow _ matlab _ style notation while denoting rows and columns of a matrix . for a given matrix @xmath9",
    ", @xmath15 will denote the @xmath16 row of @xmath9 , while @xmath17 will refer to the @xmath16 column . for a vector @xmath18",
    ", @xmath19 will denote its @xmath16 component .",
    "every permutation @xmath20 is associated with a corresponding permutation matrix @xmath21 .",
    "one important property of a permutation matrix is that its transpose is equal to its inverse , @xmath22 .",
    "the effect of left multiplying a given matrix @xmath9 by @xmath21 shuffles its rows according to @xmath23 , i.e. , the @xmath24 row of @xmath25 is @xmath16 row of @xmath9 .",
    "the effect of right multiplying has the same effect on columns instead of rows . for any permutation matrix @xmath21 , graphs represented by adjacency matrices @xmath9 and @xmath26 are isomorphic , i.e.",
    ", they represent the same graph structure except that the nodes are reordered according to @xmath23 .",
    "one way of representing graphs is to think of adjacency matrix @xmath27 as a matrix operator operating in @xmath28 .",
    "a natural way of characterizing an operator is to see how it transforms a given vector @xmath29 .",
    "this idea was pioneered in case of graphs by works on diffusion kernels  @xcite followed by binet - cauchy kernels  @xcite . here , the adjacency matrix was treated as a dynamical system and similarity measure between these systems was used as a similarity between corresponding graphs .    in  @xcite ,",
    "graph with adjacency matrix @xmath9 was associated with the following noiseless arma model .",
    "@xmath30 it was shown that the random walk kernel between two graphs , with adjacency matrix @xmath9 and @xmath31 , is actually the binet - cauchy trace kernel over the corresponding arma models that takes the following form ( see eq .",
    "( 10 ) in  @xcite ) : @xmath32 where @xmath33 , @xmath34 and @xmath35 is a matrix of all ones .",
    "the discounting term @xmath36 is necessary for the finiteness of the summation .",
    "fortunately , the infinite summation in eq .",
    "( [ eq : armakernel ] ) has a closed form solution and can be computed in @xmath4  @xcite .",
    "it can be observed from eq .",
    "( [ eq : armakernel ] ) that random walk kernel is simply a discounted summation of similarity between @xmath37 and @xmath38 , where the summation is taken over @xmath39 .",
    "it does not take into account the covariance structure of the dynamical system .",
    "in particular , given the adjacency matrix a , if we think of @xmath40 as a series , one of the identifying characteristics of a series is how @xmath37 relates with @xmath41 for @xmath42 .",
    "such kind of auto - covariance structures are very crucial in time series modeling literature .",
    "unfortunately , this information is not taken into consideration while computing the similarity in eq .",
    "( [ eq : armakernel ] ) .",
    "there are more expressive kernels for arma models like the determinant kernel  @xcite",
    ". however , determinant kernel for arma models are not applicable for graphs because it is sensitive to reordering of rows  @xcite .",
    "it should be noted that given a permutation matrix @xmath21 and an adjacency matrix @xmath9 , @xmath9 and @xmath26 leads to different dynamical systems but the graphs represented by them are isomorphic .",
    "therefore , we need a very different approach for defining kernels between graphs which takes into account the covariance structure of the series @xmath40 .",
    "we proceed by computing an isomorphic invariant functional representation of a given graph , which captures the covariance information of the dynamical system .",
    "we describe this functional embedding in the next section .",
    "in eq . ( [ eq : arma ] ) , @xmath37 is simply a power iteration of matrix @xmath9 .",
    "a small history of power iteration often captures sufficient information about the underlying matrix  @xcite .",
    "our representation capitalizes on this fact .",
    "we first extract a summary of power iteration as shown in algorithm  [ alg : summary ] . in standard power iteration , we start with a given normalized vector @xmath43 and at each iteration @xmath44 , we generate vector @xmath45 recursively .",
    "the choice of normalization is not important .",
    "adjacency matrix @xmath46 ; initial vector @xmath47 ; number of power iterations @xmath48",
    "@xmath49    @xmath50 @xmath51",
    "we refer the @xmath52 matrix , whose @xmath53 column corresponds to the @xmath54 , as @xmath51 .",
    "@xmath51 is not permutation invariant because @xmath55 and @xmath56 , for general @xmath43 , are not equal .",
    "however , if the starting vector is @xmath57 ( where @xmath58 is a vector of all ones ) , then reordering the nodes by permutation matrix @xmath21 just shuffles the rows of @xmath59 in the same order . this fact can be stated as the following theorem .",
    "[ thoe:1 ] if p is any permutation matrix , then @xmath60 , and the all - one vector @xmath13 is the unique starting vector , up to scaling , having such property for all @xmath9 and @xmath21 .",
    "@xmath61 using the identity@xmath62 , it is not difficult to show that for any permutation matrix p , @xmath63 .",
    "this along with the fact @xmath64 , yields the required result . for uniqueness ,",
    "let @xmath43 have two different components at @xmath11 and @xmath12 , then @xmath65 in general .",
    "equality here forces a constraint on @xmath66 and @xmath43 .",
    "since we have limited degrees of freedom for @xmath43 compared to choices of @xmath9 and @xmath21 , this will end in contradiction .",
    "@xmath67 + one more intuitive way to see why theorem  [ thoe:1 ] is true is to disregard normalization and imagine that at time step @xmath68 , we associate every node in the graph with the starting number @xmath69 . during every iteration of algorithm  [ alg : summary ] , which is multiplication by @xmath9 , we update this number on every node with the sum of numbers on all its neighbors .",
    "a simple recursive argument tells us that the sequence of numbers generated on each node , under this process , is not going to change as long as the neighborhood structure is preserved .",
    "unit vector @xmath13 is the only starting choice that does not distinguish between nodes .",
    "in fact , each row vector of @xmath59 can be treated as a representation for the corresponding node in the graph .",
    "such kind of updates are very informative and is the motivation behind many celebrated link analysis algorithms including hyper - text induced topic search ( hits )  @xcite .    in light of theorem  [ thoe:1 ] , we can associate a set of @xmath5 vectors , corresponding to rows of @xmath70 , with graph @xmath7 as a permutation invariant representation . our proposal , therefore , is a mathematical quantity that describes this set of vectors as a representation for graph .",
    "we have two choices : 1 ) we can either think of a subspace represented by these @xmath5 vectors , or 2 ) we can think of these @xmath5 vectors as samples from some probability distribution  @xcite .",
    "this choice depends on size of @xmath5 and @xmath48 . in case where @xmath5 is large compared to @xmath48 , the subspace represented by @xmath5 vectors of dimension @xmath48",
    "will almost always be the whole @xmath48 dimensional euclidean vector space , and it will not be very informative",
    ". on the other hand , when @xmath48 is large compared to @xmath5 , the subspace representation may be more informative compared to fitting a distribution .",
    "for example , if we decide to fit a gaussian distribution over these vectors , when @xmath48 is more than @xmath5 , the covariance matrix is not very informative .",
    "power iteration converges very quickly due to its geometric rate of convergence .",
    "we therefore need much smaller values of @xmath48 compared to @xmath5 .",
    "hence , we associate a probability function with the rows of @xmath59 .",
    "we can get a variety of permutation independent functional embeddings by different choices of this distribution functions .",
    "we use the most natural distribution function , the gaussian , for two major reasons : the similarity computations are usually in closed form and it nicely captures the correlation structure of @xmath59 . since we will always use @xmath57 , for notational convenience we will drop the subscript @xmath13 .    given an undirected graph @xmath7 with adjacency matrix @xmath46 and @xmath71 computed from algorithm  [ alg : summary ] run for @xmath48 iterations . let @xmath72 be the mean of column vectors of @xmath71 and @xmath73 denote the covariance matrix of @xmath71 : @xmath74 we define @xmath75 of graph @xmath7 as a probability density function of multivariate gaussian with mean @xmath76 and covariance @xmath77 @xmath78    since , our representation is defined as a gaussian density over the bag of vectors , it can also be interpreted as a gaussian process , see  @xcite .",
    "this @xmath79 representation has the desired property that it is invariant under reordering of nodes .    for any permutation matrix p we have @xmath80 .",
    "[ theo:2 ]    @xmath61 using theorem  [ thoe:1 ] , it is not difficult to see that @xmath81 and @xmath82.@xmath67 +    although theorem  [ theo:2 ] captures graph isomorphism in one direction , this representation is not an _ if and only if _ relationship , and we can not hope for it as we would have then solved the _ graph isomorphism problem_. although the complexity of _ graph isomorphism problem _ is still a big open question , for most of the graphs in practice it is known to be easy and a small summary of power iteration is almost always enough to discriminate between non - isomorphic graphs . in fact , real wold graphs usually possess very distinct spectral behavior  @xcite .",
    "we can therefore expect the @xmath79 embedding to be an effective representation for graphs encountered in practice",
    ". it might seem little uncomfortable to call it a distribution because the row vectors of @xmath71 never change , and so there is nothing stochastic .",
    "it is better to think of this representation of graph as an object in a functional space @xmath83 .",
    "the distribution analogy gives the motivation of a mathematical object for a set of vectors , and a simple intuition as to why theorem  [ theo:2 ] is true given theorem  [ thoe:1 ] .",
    "we define the * power kernel * between two graphs with adjacency matrices @xmath9 and @xmath84 as a bhattacharyya kernel  @xcite between @xmath85 and @xmath86 @xmath87    since@xmath85 and @xmath86 are pdf of gaussians , eq .",
    "( [ eq : bhatta ] ) has closed form solution given by : @xmath88 while designing kernels for graph ensuring positive semi - definiteness is not trivial and many previously proposed kernels do not satisfy this property  @xcite .",
    "since our kernel is a kernel over well studied mathematical representation we get this property for free , which is an immediate consequence of the result that bhattacharyya kernels are positive semidefinite .",
    "power kernel is positive semidefinite.@xmath67    overall , we have a very simple procedure for computing kernel between two graphs with adjacency matrices @xmath89 and @xmath90",
    ". the procedure is summarized in algorithm  [ alg : powerkernel ] .",
    "a ( @xmath91 ) , b ( @xmath92 ) , @xmath48    \\1 ) compute @xmath93 and @xmath94 using algorithm  [ alg : summary ] for @xmath48 iterations .",
    "\\2 ) @xmath95    \\3 ) @xmath96    \\4 ) @xmath97    \\5 ) @xmath98    \\6 ) compute k(a , b ) using eq .",
    "( [ eq : kernel ] )    k(a , b )    the value of @xmath48 determines the number of power iterations in algorithm  [ alg : summary ] . for adjacency matrix @xmath9 ,",
    "let @xmath99 be the eigenvalues and @xmath100 be the corresponding eigenvectors .",
    "the @xmath101 iteration on vector @xmath43 will generate @xmath102 , where @xmath103 is the representation of @xmath43 in the basis of @xmath104 s , i.e. , @xmath105 .",
    "this gives @xmath106.\\ ] ]    we can see that power iteration looses information about the @xmath16 eigenvalue and eigenvector at an exponential rate of @xmath107 .",
    "a matrix is uniquely characterized by the set of its eigenvalues and eigenvectors , and we need all of them to fully capture the information in the matrix .",
    "it should be noted here , that unlike other machine learning applications where small eigenvalues corresponds to noise , in our case the information of the whole spectrum is needed .",
    "we therefore need small values of @xmath48 like 4 or 5 .",
    "larger values of @xmath48 will cause the information of the larger eigenvalues to dominate the representation , and this will make the kernel values biased towards the dominant spectrum of @xmath9 .",
    "we now analyze the running time of each step in algorithm  [ alg : powerkernel ] . for simplicity ,",
    "let @xmath108 .",
    "step ( 1 ) requires running algorithm  [ alg : summary ] on both the graphs , which consists of matrix vector multiplications for @xmath48 iterations .",
    "the complexity of step ( 1 ) is thus @xmath109 .",
    "steps ( 2 ) and ( 3 ) compute the mean of @xmath5 vectors , each with dimension @xmath48 , both of which cost @xmath110 . steps ( 4 ) and ( 5 ) compute the sample covariance matrix whose complexity is @xmath111 each . the final step requires evaluating eq .",
    "( [ eq : kernel ] ) on the computed mean and covariance matrices , which requires @xmath112 operations .",
    "overall , the total time complexity for computing the kernel from scratch is @xmath113 .",
    "the recommended value of @xmath48 is usually a small constant ( e.g. 4 or 5 ) even for large graphs . treating @xmath48 as a constant , the time complexity is @xmath6 in the worst case .",
    "in fact , due to the sparsity of the adjacency matrix @xmath9 , the actual time complexity is @xmath114 , where @xmath115 is the total number of edges ( which is at most @xmath6 ) . in other words ,",
    "our total running time is linear in the number of edges . the current state - of - the - art kernels including skew spectrum of graph  @xcite , random walk kernel  @xcite , require @xmath4 computations while shortest path kernel  @xcite is even costlier .",
    "the graphs that we encounter in most real - world applications are in general very sparse , i.e. , @xmath116 . moreover ,",
    "when the number of edges is on the order of the number of vertices ( which is not unusual ) , our algorithm is actually linear in @xmath5 .",
    "this makes our proposed power kernels scalable even for web applications .",
    "note that , we can pre - compute the first five steps in algorithms  [ alg : powerkernel ] , independently for each graph . after this preprocessing , kernel computation only requires @xmath112 per pair , which is a constant .",
    "the work of  @xcite was based on extracting permutation invariant features from graph using an algebraic approach .",
    "our representation leads to a new set of invariants . as a consequence of theorem  [ theo:2 ] , @xmath76 and @xmath77 are graph invariants .",
    "define @xmath117 as the number of disjoint paths of length @xmath39 , in the given graph @xmath7 , having node @xmath11 as one of its end points . in computing @xmath117 , we allow repetition of nodes .",
    "one simple observation is that the @xmath16 component of @xmath118 , i.e. @xmath119 , is equal to @xmath120 .",
    "this fact can be proven by a simple inductive argument , where the base case @xmath121 corresponds to the degree of the node @xmath11 .",
    "the @xmath101 component of @xmath76 is the mean of @xmath120 , i.e. @xmath122 which is a trivial graph invariant because it is the total number of paths of length @xmath39 , in the given graph , multiplied by a constant .",
    "the constant @xmath123 comes into the picture due to normalization .",
    "interesting set of invariants come from the matrix @xmath77 .",
    "the @xmath124 element of @xmath77 can be written as @xmath125 which the correlations among the number of paths of length @xmath126 and that of length @xmath127 having a common endpoint . when @xmath128 , it can be interpreted as the variance in the number of paths of length @xmath39 having a common endpoint . in the hindsight ,",
    "its not difficult to see that these aggregated statistics of paths of different lengths starting at a given node are graph invariants .",
    "we will see in the next section that this information is very useful in discriminating various graph structures .",
    "@xmath129 captures the information about the mean statistic of different kind of paths present in the graph . @xmath77",
    "captures the relative structure of nodes in each graph .",
    "the correlations between various kinds of paths relative to a node indicated its relative connectivity in the graph structure .",
    "this kind of relative correlation information were missing in random walk kernels and path based kernels , which only count common paths or walks of same length between two given graphs .",
    "even kernels trying to count common small subgraphs do not capture this relative structural information sufficiently .",
    "@xmath129 and @xmath77 capture aggregate behavior of paths relative to different nodes , and they can also be treated as an informative summary of the given graph .",
    "gaussian density function is just one way of exploiting this correlation structure .",
    "we can generate other functionals on the rows of @xmath93 .",
    "for example , we can generate an expressive functional by using kernel density estimation on these set of @xmath5 vectors , and theorem  [ thoe:1 ] guarantees that the obtained functional is a graph invariant .",
    "we believe that such invariants can provide deeper insight which could prove beneficial for many applications dealing with graphs .",
    "the behaviors of these graph invariants raise many interesting theoretical questions which could be of independent interest .",
    "for instance we can ask ,  what will be the behavior of these invariants if the graph has low expansion ? \"",
    "[ cols=\"<,^,^,^,^\",options=\"header \" , ]     we follow the evaluation procedure of  @xcite .",
    "we chose the same four benchmark graph classification datasets consisting of the graph structure of the chemical compounds : mutag , enzymes , nci1 and nci109 , used in  @xcite for their diversity in terms of size and as well as tasks . in each of these dataset , each data point is a graph structure associated with a classification label .",
    "mutag  @xcite is a dataset of 188 mutagenic aromatic and hetroaromatic nitro compounds , labeled according to whether or not they have mutagenic effect on gram - negative bacterium @xmath130 @xmath131 .",
    "the maximum number of nodes in this dataset is 28 with mean around 19 , while the maximum number of edges is 33 and the mean is around 20 .",
    "enzymes is a dataset of protein tertiary structure , which was used in  @xcite .",
    "it consists of 600 enzymes from the brenda enzymes database  @xcite .",
    "this is a multi - class classification task , where each enzyme has the label as to which of the 6 ec top level class it belongs to . the maximum number of nodes in this dataset is 126 with average around 32.6 , while the maximum number of edges is 149 and the mean is around 62 .",
    "the other two balanced datasets , nci1 and nci109 , classify compounds based on whether or not they are active in an anti - cancer screen  @xcite . for both nci1 and nci109",
    "the maximum number of nodes is 111 with mean around 30 , and the maximum number of edges is 119 with mean around 32 .",
    "our focus will remain on evaluating the basic structure captured by our functional representation @xmath132 .",
    "we therefore focus our comparisons with methodologies not relying on node and edge label information .",
    "we repeat evaluation procedure followed in  @xcite with power kernel .",
    "the evaluations consists of running kernel svm on the four datasets using different kernel .",
    "the standard evaluation procedure used is as follows .",
    "first split each dataset into 10 folds of identical size .",
    "combine 9 of these folds and again split it into 10 parts , then use the first 9 parts to train the c - svm  @xcite and use the 10th part as validation set to find the best performing value of c from @xmath133 . with this choice of c , train",
    "the c - svm on all the 9 folds ( form initial 10 folds ) and predict on the 10th fold acting as an independent evaluation set .",
    "the procedure is repeated 10 times with each fold acting as an independent test set once . for each dataset the whole procedure",
    "is then repeated 10 times randomizing over partitions . the mean classification accuracy and the standard errors",
    "are shown in table  [ tab : result ] .",
    "since the results are averaged over 10 runs with different partitions , the numbers are very stable .",
    "we borrowed the accuracy values of state - of - the - art unlabeled graph kernels : random walk kernel  @xcite , shortest path kernel  @xcite , graphlet count kernel  @xcite , and reduced skew spectrum of graph from  @xcite , where parameters , if any , for these kernels were optimized for best performance .",
    "as noted before , the value of @xmath48 should not be large .",
    "though we have the choice to tune this value for different datasets independently , to keep things simple and allow easy replication of results , we report the results for a fixed value of @xmath134 on all the four datasets .    from the results , e can see that other than the mutag dataset , power kernel outperforms other kernels on the remaining 3 datasets .",
    "on nci1 and nci109 , which are larger datasets with larger graphs compared to mutag , we beat the previous best performing kernel , which is based on skew spectrum of graph , by a huge margin . on these two datasets",
    ", power kernel gives a classification accuracy of around 70% while the best performing baseline can only achieve around 62% . in case of enzymes dataset , the shortest path kernel performs the best among other baseline kernels and achieves 27.53% accuracy , while we can achieve around 34.6% .",
    "this significant improvement clearly establishes the expressiveness of our representation in capturing structure of graphs .",
    "on mutag dataset the accuracy of 88.61% is achieved by reduced skew spectrum kernel while power kernel gives 83.22% .",
    "we believe that this is due to the fact that mutag consists of relatively much smaller graphs , and it seems that the few graph invariant features generated by reduced skew spectrum sufficiently capture the discriminative information in this dataset . on datasets with larger graphs , such features are less expressive than our functional representation , and hence power kernel leads to much better results . also mutag dataset contains only 188 data elements , and",
    "so the percentage difference is not significant as compared to larger dataset like nci1 and nci109 .",
    "we always outperform graphlet count kernel , random walk kernel and shortest path kernel .",
    "this shows that our basic representation is much more expressive and superior .",
    "it is not surprising because we are capturing higher order correlation information , while kernels based on counting common paths or subgraphs of small size miss this relative information .",
    "dissecting graphs into small subgraphs looses a lot of information .",
    "as shown in section  [ sec : comp ] our algorithm runs in @xmath114 and from the statistics of the dataset we can see that on an average the edges are of the order of vertices , and so the running time complexity of power kernel in this case is actually around @xmath135 , while all other competing methods except graphlet count kernel require at least @xmath4 .",
    "therefore , we have a huge gain in performance .",
    "the running time complexity of graphlet kernel is competitive with our method but accuracy wise our method is much superior .",
    "the whole procedure for power kernel is simple and since we havent tuned anything except @xmath136 for svm all these numbers are easily reproducible .",
    "the success of isomorphism capturing kernels is due to their ability of preserving near neighbors with high probability .",
    "our proposed power kernel posses the following two properties :    1 .",
    "if two graphs @xmath9 and @xmath84 are isomorphic then @xmath137 , and if they are not , then likely @xmath138 .",
    "2 .   if two graphs @xmath9 and @xmath84 are small perturbed versions of each other then @xmath139 should be close to 1 , in particular it should be higher compared to two random graphs .    determining which graphs are uniquely determined by their spectrum is in general a very hard problem , but all graphs encountered in practice are well behaved and uniquely determined by their dynamics .",
    "hence , our proposed embedding does not loose much information .    for power kernels ,",
    "it is clear from theorem  [ theo:2 ] that if two graphs are isomorphic then k(a , b ) = 1 .",
    "because of the permutation invariance property , we do not have to worry about which ordering of nodes to consider as long as there exist one which gives the required bijection .",
    "if two graphs are not isomorphic then their spectrum follow very different behaviors and hence kernel value between them should be much less than 1 . to illustrate why our representation satisfies property 2 , we use",
    "the fact that the spectrum of adjacency matrix is usually very stable under small perturbations , see  @xcite . here , the perturbations means operations like adding or deleting few nodes and edges .",
    "it is different from the usual small normed perturbations .",
    "moreover , our kernel relies on stable statistics such as covariance @xmath140 and mean @xmath141 of @xmath93 , which do not undergo any major jump by small changes in the @xmath93 , assuming the size of graph @xmath5 is large .",
    "our method thus ensures that small graph perturbations do not lead to any blow up causing relatively big changes in the kernel values .",
    "although , it might be difficult to quantify the sensitivity of power kernels with respect to small perturbations in the graph , we can empirically verify the above claim .",
    "we chose the same four datasets used in the experiments . from each dataset , we randomly sample 100 graphs for the evaluations .",
    "we perturb each graph structure by flipping a random edge , i.e. , we choose two nodes @xmath11 and @xmath12 randomly , if the edge @xmath142 was present in the graph then we delete the edge @xmath142 , otherwise we add the edge @xmath142 to the graph .",
    "we do this perturbation process 20 times one after the other , thereby obtaining a sequence of 20 graphs with increasing amount of perturbations . after each perturbation we compute the kernel value of the perturbed graph with the original graph .",
    "the value of @xmath48 was again set to be 5 .",
    "we plot the average kernel values over these 100 points on all the four datasets , in figure  [ fig : pert ] .",
    "we can clearly see that the kernel values smoothly decrease with increasing perturbations . for mutag dataset which consists of smaller graphs ,",
    "the effect of perturbations is more compared to other datasets with relatively bigger graphs , which is expected .",
    "the plots clearly demonstrate that small perturbations do not lead to discontinuous jumps in the kernel values .",
    "we approached the problem of graph kernels by finding an embedding in functional space .",
    "power kernel , which is based on kernel between these functionals , significantly outperforms the existing state - of - the - art kernels on benchmark graph classification datasets .",
    "our kernel only requires @xmath114 time to compute and thus this scheme is very practical .",
    "our focus was to demonstrate the power of an expressive functional representation in a simplest possible way .",
    "we believe that there is a huge scope of improvement in the proposed kernel owing to the possible flexibility in our approach .",
    "for example , the choice of gaussian functions was the natural one .",
    "there is a whole room for deriving more expressive functionals on row vectors of @xmath93 , like using kernel density estimators , etc .",
    "incorporating node and edge label information in power kernel is another area to explore .",
    "the idea of discounting subsequent columns of the power iteration could be a useful extension .",
    "we have demonstrated that our functional representation can provide an easy interface for dealing with graphs , a combinatorially hard object .",
    "although we have seen significant gains in the performance over the existing state - of - the - art kernels , in light of the possible future work , we believe a lot more is yet to come .",
    "the work was partially supported by nsf ( dms0808864 , ses1131848 , iii1249316 ) and afosr ( fa9550 - 13 - 1 - 0137 ) .",
    "k.  m. borgwardt , c.  s. ong , s.  schnauer , s.  v.  n. vishwanathan , a.  j. smola , and h.  kriegel . protein function prediction via graph kernels . in _ ismb ( supplement of bioinformatics ) _ , pages 4756 , 2005 .",
    "a.  k. debnath , r.  l.  lopez de  compadre , g.  debnath , a.  j. shusterman , and c.  hansch .",
    "structureactivity relationship of mutagenic aromatic and heteroaromatic nitro compounds .",
    "correlation with molecular orbital energies and hydrophobicity .",
    ", 34:786797 , 1991 .",
    "n.  shervashidze , s.  v.  n. vishwanathan , t.  petri , k.  mehlhorn , and k.  m. borgwardt .",
    "efficient graphlet kernels for large graph comparison . in _ proceedings of international conference on artificial intelligence and statistics .",
    "( aistats 2009 ) _ , pages 129143 , 2009 ."
  ],
  "abstract_text": [
    "<S> we propose a representation of graph as a functional object derived from the power iteration of the underlying adjacency matrix . </S>",
    "<S> the proposed functional representation is a graph invariant , i.e. , the functional remains unchanged under any reordering of the vertices . </S>",
    "<S> this property eliminates the difficulty of handling exponentially many isomorphic forms . </S>",
    "<S> bhattacharyya kernel constructed between these functionals significantly outperforms the state - of - the - art graph kernels on 3 out of the 4 standard benchmark graph classification datasets , demonstrating the superiority of our approach . </S>",
    "<S> the proposed methodology is simple and runs in time linear in the number of edges , which makes our kernel more efficient and scalable compared to many widely adopted graph kernels with running time cubic in the number of vertices . </S>"
  ]
}