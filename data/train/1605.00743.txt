{
  "article_text": [
    "visual attributes are middle - level concepts which humans use to describe objects , human faces , scenes , activities , and so on ( e.g. , four - legged , smiley , outdoor , and crowded ) . a major appeal of attributes is that they are not only human - nameable but also machine - detectable , making it possible to serve as the building blocks to describe instances  @xcite , teach machines to recognize previously unseen classes by zero - shot learning  @xcite , or offer a natural human - computer interactions channel for image / video search  @xcite .",
    "however , we contend that the long - standing pursuit after utilizing attributes for various computer vision problems * has left the most basic problem  how to accurately and robustly detect attributes from images or videos  far from being solved . * especially , the existing work rarely explicitly tackles the need that * attribute detectors should generalize well across different categories , including those previously unseen ones . * for instance ,",
    "the attribute detector  four - legged \" is expected to correctly tell a giant panda is four - legged even if it is trained from the images of horses , cows , zebras , and pigs ( i.e. , no pandas ) .    indeed ,",
    "most of the existing _ attribute _ detectors  @xcite are built using features engineered or learned for _ object _ recognition together with off - shelf machine learning classifiers  without tailoring them to reflect the idiosyncrasies of attributes .",
    "this is suboptimal ; the successful techniques on object recognition do not necessarily apply to attributes learning mainly for two reasons .",
    "first , attributes are in a different semantic space as opposed to objects ; they are in the _ middle _ of low - level visual cues and the high - level object labels .",
    "second , attribute detection can even be considered as an _ orthogonal _ task to object recognition , in that attributes are shared by different objects ( e.g. , zebras , lions , and mice are all `` furry '' ) and distinctive attributes are present in the same object ( e.g. , a car is boxy and has wheels ) . as shown in figure  [ fig : highlight ] , the boundaries between attributes and between object categories cross each other .",
    "therefore , we do not expect that the features originally learned for separating elephant , sheep , and giraffe could also be optimal for detecting the attribute `` bush '' , which is shared by them . in this paper",
    ", we propose to re - examine the fundamental attribute detection problem and aim to develop an attribute - oriented feature representation , such that one can conveniently apply off - shelf classifiers to obtain high - quality attribute detectors .",
    "we expect that the detectors learned from our new representation are capable of breaking the boundaries of object categories and generalizing well across both seen and unseen classes . to this end",
    ", we cast * attribute detection as a multi - source domain generalization problem *  @xcite by noting that the desired properties from attributes are analogous to the objective of the latter .",
    "particularly , a domain refers to an underlying data distribution .",
    "multi - source domain generalization aims to extract knowledge from several _ related _ source domains such that it is applicable to different domains , especially to those unseen at the training stage .",
    "this is in accordance with our objective for learning cross - category generalizable attributes detectors , if we consider each category as a distinctive domain .    motivated by this observation",
    ", we employ the unsupervised domain - invariant component analysis ( udica )  @xcite as the basic building block for our approach .",
    "the key principle of udica is that minimizing the distributional variance of different domains  categories in our context , can improve the cross - domain ( cross - category ) generalization capabilities of the classifiers .",
    "a supervised extension to udica was introduced in  @xcite depending on the inverse of a covariance operator as well as some mild assumptions . however , the inverse operation is both computationally expensive and unstable in practice .",
    "we instead propose to use the alternative of centered kernel alignment  @xcite to account for the attribute labeling information .",
    "we show that the centered kernel alignment can be seamlessly integrated with udica , enabling us to learn both category - invariant and attribute - discriminative feature representations .",
    "our approach takes as input the features of the training images , their class ( domain ) labels , as well as their attribute labels .",
    "it operates upon kernels derived from the input data and learns a kernel projection to `` distill '' category - invariant and attribute - discriminative signals embedded in the original features .",
    "the overall output is a new feature vector for each image , which can be readily used in traditional machine learning models like svms for training the cross - category generalizable attribute detectors .",
    "the contributions of the paper are summarized below .    to the best of our knowledge",
    ", this work is the first attempt to tackle attribute detection from the multi - source domain generalization point of view .",
    "this enables us to explicitly model the need that the attribute detectors should transcend different categories and generalize to previously unseen ones .",
    "we introduce the centered kernel alignment to udica and arrive at an integrated method to strengthen the discriminative power of the learned attributes on one hand , and eliminate the domain differences between categories on the other hand .",
    "we test our approach to four datasets : animal with attributes  @xcite , caltech - ucsd birds  @xcite , apascal - ayahoo  @xcite , and ucf101  @xcite , and test the learned representations on three tasks : attribute detection itself , zero - shot learning , and image retrieval .",
    "our results are significantly better than those of competitive baselines , verifying the effectiveness of the new perspective for solving attribute detection as domain generalization .",
    "the rest of this paper is organized as follows . in section  [ sec:2 ] ,",
    "we review related work in attribute detection , domain generalization , and domain adaptation .",
    "section  [ sec:3 ] and section  [ sec:4 ] present the attribute learning framework .",
    "the experimental settings and evaluation results are presented in section  [ sec:5 ] .",
    "section  [ sec:6 ] concludes the paper .",
    "our approach is related to two separate research areas , attribute detection and domain adaptation / generalization .",
    "we unify them in this work .    [ [ attributes - learning . ] ] attributes learning .",
    "+ + + + + + + + + + + + + + + + + + + +    earlier work on attribute detection mainly focused on modeling the correlations among attributes  @xcite , localizing some special part - related attributes ( e.g. , tails of mammals )  @xcite , and the relationship between attributes and categories  @xcite .",
    "some recent work has applied deep models to attribute detection  @xcite .",
    "none of these methods explicitly model the cross - category generalization of the attributes , except the one by farhadi et al .",
    "@xcite where the authors select features within each category to down - weight category - specific cues .",
    "likely due to the fact that the attribute and category cues are interplayed ,",
    "their feature selection procedure only gives limited gain .",
    "we propose to overcome this challenge by investigating all categories together and employing nonlinear mapping functions .",
    "attributes possess versatile properties and benefit a wide range of challenging computer vision tasks .",
    "they serve as the basic building blocks for one to compose categories ( e.g. , different objects )  @xcite and describe instances  @xcite , enabling knowledge transfer between them . attributes also reveal the rich structures underlying categories and are thus often employed to regulate machine learning models for visual recognition  @xcite .",
    "moreover , attributes offer a natural human - computer interaction channel for visual recognition with humans in the loop  @xcite , relevance feedback in image retrieval  @xcite , and active learning  @xcite . in this paper , we test the proposed approach on both attribute detection and its applications to zero - shot learning and image retrieval .",
    "[ [ domain - generalization - and - adaptation . ] ] domain generalization and adaptation .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    domain generalization is still at its early developing stage . a feature projection - based algorithm , domain - invariant component analysis ( dica ) ,",
    "was introduced in  @xcite to learn by minimizing the variance of the source domains .",
    "recently , domain generation has been introduce into computer vision community for object recognition  @xcite and video recognition  @xcite .",
    "we propose to gear multi - source domain generalization techniques for the purpose of learning cross - category generalizable attribute detectors .",
    "multi - source domain adaptation  @xcite is related to our approach if we consider a transductive setting ( i.e. , the learner has access to the test data ) .",
    "while it assumes a single target domain , in attribute detection the test data are often sampled from more than one unseen domain .",
    "denote by @xmath0 and @xmath1 respectively a reproducing kernel hilbert space and its associated kernel function .",
    "for an arbitrary distribution @xmath2 indexed by @xmath3 , the following mapping , @xmath4 = \\int k(\\bm{x},\\cdot ) \\text{d}p_y(\\bm{x } ) \\triangleq \\mu_y\\end{aligned}\\ ] ] is injective if @xmath5 is a characteristic kernel  @xcite .",
    "in other words , the kernel mean map @xmath6 in the rkhs @xmath0 preserves all the statistical information of @xmath2 .",
    "the distributional variance follows naturally , @xmath7 where @xmath8 is the map of the mean of all the distributions in @xmath9 . in practice",
    ", we do not have access to the distributions .",
    "instead , we observe the samples @xmath10 each drawn from a distribution @xmath2 and can thus empirically estimate the distributional variance by @xmath11 . here",
    "@xmath12 is the ( centered ) kernel matrix over all the samples , and @xmath13 collects the coefficients which depend on only the numbers of samples .",
    "we refer the readers to  @xcite for more details including the consistency between the distributional variance @xmath14 and its estimate @xmath15 .",
    "this section formalizes attribute detection and shows its in - depth connection to domain generalization .",
    "[ [ problem - statement . ] ] problem statement .",
    "+ + + + + + + + + + + + + + + + + +    suppose that we have access to an annotated dataset of @xmath16 images .",
    "they are in the form of @xmath17 where @xmath18 is the feature vector extracted from the @xmath19-th image @xmath20 , @xmath21\\triangleq\\{1,2,\\cdots,\\mathsf{m}\\}$ ]",
    ". two types of annotations are provided for each image , the category label @xmath22 $ ] and the attribute annotations @xmath23 .",
    "though we use binary attributes ( e.g. , the presence or absence of stripes ) to in this paper for clarity , it is straightforward to extend our approach to multi - way and continuous - valued attributes .",
    "note that a particular attribute @xmath24 could appear in many categories ( e.g. , zebras , cows , giant pandas , lions , and mice are all furry ) .",
    "moreover , there may be test images from previously unseen categories @xmath25 for example in zero - shot learning .",
    "our objective is to learn accurate and robust attribute detectors @xmath26 to well generalize across different categories , especially to be able to perform well on the unseen classes .",
    "$ ] over image representations @xmath27 and attribute labels @xmath28 , we extract knowledge useful for attribute detection and applicable to different domains / categories , especially to previously unseen ones @xmath29 . the domains are assumed related and sampled from a common distribution @xmath30.,scaledwidth=48.0% ]    [ [ attribute - detection - as - domain - generalization - a - new - perspective . ] ] attribute detection as domain generalization + a new perspective .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in this paper , we understand attribute detection as a domain generalization problem .",
    "a domain refers to an underlying data distribution . in our context",
    ", it refers to the distribution @xmath31 of a category @xmath32 $ ] over the input @xmath27 and attribute labels @xmath28 .",
    "as shown in figure  [ fdg ] , the domains / categories are assumed to be related and are sampled from a common distribution @xmath30 .",
    "this is reasonable considering that images and categories can often be organized in a hierarchy .",
    "thanks to the relationships between different categories , we expect to learn new image representations for attribute detection , such that the corresponding detectors will perform well on both seen and unseen classes .",
    "our key idea is to find a feature transformation of the input @xmath27 to eliminate the mismatches between different domains / categories in terms of their marginal distributions over the input , whereas ideally we should consider the joint distributions @xmath31 , @xmath32 $ ] . in particular , we use unsupervised domain invariant component analysis ( udica )  @xcite and centered kernel alignment  @xcite for this purpose .",
    "note that modeling the marginal distributions @xmath2 is a common practice in domain generalization  @xcite and domain adaptation  @xcite and performs well in many applications .",
    "we leave investigating the joint distributions @xmath31 for future work .    next , we present how to integrate udica and centered kernel alignment . jointly they give rise to new feature representations which account for both attribute discriminativeness and cross - category generalizability .      the projection from one rkhs to another results in the following transformation of the kernel matrices , @xmath33  @xcite . as a result , one can take @xmath34 as the empirical kernel map , i.e. , consider the @xmath19-th row of @xmath34 as the new feature representations of image @xmath20 and plug them into any linear classifiers .",
    "udica learns the transformation @xmath35 by imposing the following properties .",
    "[ [ minimizing - distributional - variance . ] ] minimizing distributional variance .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the empirical distributional variance ( cf .  section  [ sdv ] ) between different domains / categories becomes the following in our context , @xmath36 ) = \\text{tr}(\\widetilde{k}q ) = \\text{tr}(b^tkqkb).\\end{aligned}\\ ] ] intuitively , the domains would be perfectly matched when the variance is 0 . since there are many seen categories , each as a domain , we expect the learned projection to be generalizable and work well for the unseen classes as well .    [ [ maximizing - data - variance . ] ] maximizing data variance .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + +    starting from the empirical kernel map @xmath34 , it is not difficult to see that the data covariance is @xmath37 and the variance is @xmath38)= \\text{tr}(b^tk^2b ) / \\mathsf{m}.\\end{aligned}\\ ] ]    [ [ regularizing - the - transformation . ] ] regularizing the transformation .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    udica regularizes the transformation by minimizing @xmath39 alternatively , one can use the frobenius norm @xmath40 , as did in  @xcite , to constrain the complexity of @xmath35 .    combining the above criteria , we arrive at the following problem , @xmath41 where the nominator corresponds to the data variance and the denominator sums up the distributional variance and the regularization over @xmath35 .    by solving the above problem",
    ", we are essentially blurring the boundaries between different categories and match the classes with each other , due to the distributional variance term in the denominator .",
    "this thus eliminates the barrier for attribute detectors to generalize in various classes .",
    "our experiments verify the effectiveness the learned new representations @xmath34 .",
    "nonetheless , we can further improve the performance by modeling the attribute labels using centered kernel alignment .",
    "note that our training data are in the form of @xmath42 $ ] .",
    "for each image there are multiple attribute labels which may be highly correlated . besides",
    ", we would like to stick to kernel methods to be consistent with our choice of udica  indeed , the distributional variance is best implemented by kernel methods ( cf .",
    "section  [ sdv ] ) .",
    "these considerations lead to our decision on using kernel alignment  @xcite to model the multi - attribute supervised information .",
    "let @xmath43 be the kernel matrix over the attributes .",
    "since @xmath44 is computed directly from the attribute labels , it preserves the correlations among them and serves as the `` perfect '' target kernel for the transformed kernel @xmath45 to align to . the centered kernel alignment is then computed by",
    ", @xmath46 where we abuse the notation @xmath44 slightly to denote that it is centered  @xcite .",
    "we would like to integrate the kernel alignment with udica in a unified optimization problem . to this end , firstly it is safe to drop @xmath47 in eq .",
    "( [ ekernelalign ] ) since it has nothing to do with the projection @xmath35 we are learning",
    ". moreover , note that the role of @xmath48 duplicates with the regularization in eq .",
    "( [ edica ] ) to some extent , as it is mainly to avoid trivial solutions for the kernel alignment .",
    "we thus only add @xmath49 to the nominator of udica , @xmath50 where @xmath51 $ ] balances the data variance and the kernel alignment with the supervised attribute labeling information .",
    "we cross - validate @xmath52 in our experiments .",
    "we name this formulation * kdica * , which couples the centered kernel alignment and udica .",
    "the former closely tracks the attribute discriminative information and the latter facilitates the cross - category generalization of the attribute detectors to be trained upon kdica .    [",
    "[ optimization . ] ] optimization .",
    "+ + + + + + + + + + + + +    by writing out the lagrangian of the formalized problem ( eq .",
    "( [ eall ] ) ) and then setting the derivative with respect to @xmath35 to 0 , we arrive at a generalized eigen - decomposition problem , @xmath53 where @xmath54 is a diagonal matrix containing all the eigenvalues ( lagrangian multipliers ) . we find the solution @xmath35 as the leading eigen - vectors .",
    "the number of eigen - vectors is cross - validated in our experiments .",
    "again , we remind that @xmath34 serves as the new feature representations of the images for training attribute detectors .",
    "the details of our proposed framework has been shown in algorithm  [ algo : kdica ] .",
    "parameters @xmath52 and @xmath55 .",
    "training data @xmath56 projection @xmath57 calculate gram matrix @xmath58 = k(\\bm{x}_i , \\bm{x}_j)$ ] and @xmath59 = l(\\bm{a}_i , \\bm{a}_j)$ ] solve : @xmath60 .",
    "output @xmath35 and @xmath61 use @xmath34 as if they are the features to learn linear classifiers and @xmath62 for kernelized classifiers",
    "this section presents our experimental results on four benchmark datasets .",
    "we test our approach for both the immediate task of attribute detection and two other problems , zero - shot learning and image retrieval , which could benefit from high - quality attribute detectors .    [ cols=\"^,^,^,^,^\",options=\"header \" , ]",
    "in this paper , we propose to re - examine the fundamental attribute detection problem and develop a novel attribute - oriented feature representation by casting the problem as multi - source domain generalization , such that one can conveniently apply off - shelf classifiers to obtain high - quality attribute detectors .",
    "the attribute detectors learned from our new representation are capable of breaking the boundaries of object categories and generalizing well to unseen classes .",
    "extensive experiment on four datasets , and three tasks , validate that our attribute representation not only improves the quality of attributes , but also benefits succeeding applications , such as zero - shot recognition and image retrieval",
    ".    * acknowledgement . *",
    "this work was supported in part by nsf iis-1566511 .",
    "chuang gan was partially supported by the national basic research program of china grant 2011cba00300 , 2011cba00301 , the national natural science foundation of china grant 61033001 , 61361136003 .",
    "tianbao yang was partially supported by nsf iis-1463988 and nsf iis-1545995 .",
    "y.  jia , e.  shelhamer , j.  donahue , s.  karayev , j.  long , r.  b. girshick , s.  guadarrama , and t.  darrell .",
    "caffe : convolutional architecture for fast feature embedding . in _",
    "acm multimedia _ , volume  2 , page  4 , 2014 .",
    "a.  vedaldi , s.  mahendran , s.  tsogkas , s.  maji , r.  girshick , j.  kannala , e.  rahtu , i.  kokkinos , m.  b. blaschko , d.  weiss , b.  taskar , k.  simonyan , n.  saphra , and s.  mohamed .",
    "understanding objects in detail with fine - grained attributes . in _ cvpr _ , 2014 ."
  ],
  "abstract_text": [
    "<S> attributes possess appealing properties and benefit many computer vision problems , such as object recognition , learning with humans in the loop , and image retrieval . whereas the existing work mainly pursues utilizing attributes for various computer vision problems </S>",
    "<S> , we contend that the most basic problem  how to accurately and robustly detect attributes from images  has been left under explored . </S>",
    "<S> especially , the existing work rarely explicitly tackles the need that attribute detectors should generalize well across different categories , including those previously unseen . noting that this is analogous to the objective of multi - source domain generalization , if we treat each category as a domain , we provide a novel perspective to attribute detection and propose to gear the techniques in multi - source domain generalization for the purpose of learning cross - category generalizable attribute detectors . </S>",
    "<S> we validate our understanding and approach with extensive experiments on four challenging datasets and three different problems . </S>"
  ]
}