{
  "article_text": [
    "neural systems have the capacity , among others , to represent stimuli , objects and events in the outside world . here , we use the word _ representation _ to refer to an association between a certain pattern of neural activity and some external correlate .",
    "irrespectively of the identity or the properties of the items to be represented , information theory provides a framework where the capacity of a specific coding scheme can be quantified .",
    "how much information can be extracted from the activity of a population of neurons about the identity of the item that is being represented at any one moment ?",
    "such a problem , in fact , has already been studied experimentally @xcite .",
    "typically a discrete set of @xmath0 stimuli is presented to a subject , while the activity of a population of @xmath1 neurons is recorded . at its simplest",
    ", this activity can be described as an @xmath1 dimensional vector @xmath2 , whose components are the firing rates of individual neurons computed over a predefined time window .",
    "the measured response is expected to be selective , at least to some degree , to each one of the stimuli .",
    "this degree of selectivity can be quantified by the mutual information between the set of stimuli and the responses @xcite @xmath3 , \\label{inf}\\ ] ] where @xmath4 is the probability of showing stimulus @xmath5 , @xmath6 is the conditional probability of observing response @xmath2 when the stimulus @xmath5 is presented and @xmath7 the mutual information @xmath8 characterizes the mapping between the @xmath0 stimuli and the response space , and represents the amount of information conveyed by @xmath2 about which of the @xmath0 stimuli was shown .",
    "if each stimulus evokes a unique set of responses , i.e. no two different stimuli induce the same response , then eq . ( [ inf ] ) reduces to the entropy of the stimulus set , and is , therefore , @xmath9 .",
    "on the other hand , if a response @xmath2 may be evoked by more than one stimulus , the mutual information is less than the entropy of the stimuli . in the extreme case where the responses are independent of the stimulus shown , @xmath10 .    in fig .",
    "[ f1 ]    we show the mutual information extracted from neural responses from the inferior temporal cortex of a macaque when exposed to @xmath0 visual stimuli @xcite .",
    "diamonds correspond to @xmath11 , squares to @xmath12 and triangles to @xmath13 .",
    "the graph is plotted as a function of the number of neurons considered .",
    "initially , the information rises linearly .",
    "as @xmath1 grows , the increase of @xmath14 slows down , apparently saturating at some asymptotic value compatible with the theoretical maximum @xmath9 .    the behavior shown in fig .",
    "[ f1 ] is quite common observation also in other experiments of the same type @xcite . from the theoretical point of view",
    ", different conclusions have been drawn , over the years , from these curves .",
    "obviously , the saturation in itself implies that , after a while , adding more and more neurons provides no more than redundant information .",
    "gawne and richmond @xcite have considered a simple model which yields an analytical expression for @xmath14 under the assumption that each neuron provides a fixed amount of information , @xmath15 , and that a fixed fraction of such an amount , @xmath16 , is redundant with the information conveyed by any other neuron .",
    "the model yields @xmath17 .",
    "et . al _ @xcite have considered a more constrained model that in addition assumes that @xmath18 .",
    "later it was shown that this is , in fact , the mean pairwise redundancy if the information provided by different cells has a random overlap @xcite . in this kind of phenomenological description ,",
    "the information provided by a population of @xmath1 cells reads @xmath19 .",
    "\\label{grra}\\ ] ] the full line in fig .",
    "[ f1 ] shows a fit of eq .",
    "( [ grra ] ) to the data , in the case of @xmath11 .",
    "it has also been suggested @xcite that monitoring the linear rise for small @xmath1 may tell whether the representation of the stimuli is distributed or local . in a distributed scheme",
    "many neurons participate in coding for each stimulus . on the contrary , in a local representation ",
    "sometiemes called grandmother cell encoding  each stimulus is represented by the activation of just one or a very small number of equivalent neurons .    here",
    "we present a theoretical analysis of the dependence of @xmath8 on @xmath1 for independent units .",
    "in contrast to the previous phenomenological description , we model the response of each neuron to every stimulus . in sects .",
    "ii and iii we derive @xmath14 for several choices of the single unit response probability . in sect .",
    "iv we discuss the relation of the mutual information defined in eq .",
    "( [ inf ] ) to an informational measure of retrieval accuracy .",
    "we end in sect .",
    "v with some concluding remarks .",
    "in what follows , the issue of quantifying the mean amount of information provided by @xmath1 units is addressed .",
    "to do so , the response of each unit to every stimulus is specified . from such responses ,",
    "the mutual information is calculated using eq .",
    "( [ inf ] ) .",
    "two types of models are considered . in this section",
    "we deal with discrete noiseless units , while in sect .",
    "iii we turn to continuous noisy ones .",
    "we consider @xmath1 units responding to a set of stimuli . the response @xmath20 of unit @xmath21 is taken to vary in a discrete set of @xmath22 possible values .",
    "the states of the whole assembly of @xmath1 units are written as @xmath23 , where @xmath24 . throughout the paper , letters in bold stand for vectors in a @xmath1-dimensional space .",
    "the total number of states in @xmath25 is therefore @xmath26 .",
    "the stimuli @xmath27 to be discriminated constitute a discrete set @xmath28 of @xmath0 elements . for simplicity , we assume that they are all presented to the neural system with the same frequency , namely @xmath29 in order to calculate the mutual information between @xmath28 and @xmath25 we assume that each stimulus has a representation in @xmath25 . in other words , for each stimulus @xmath5 there is a fixed @xmath1-dimensional vector @xmath30 .",
    "superscipts label stimuli , while subscripts stand for units .",
    "the fact that the neurons are noiseless means that the mapping between stimuli and responses is deterministic .",
    "that is to say , for every stimulus there is a unique response @xmath30 .",
    "mathematically , @xmath31 therefore , for every @xmath32 there is one and only one @xmath33 .",
    "the reciprocal , however , is in general not true . if several stimuli happen to have the same representation  which may well be the case if too few units are considered  then a given @xmath2 may come as a response to more than one stimulus . in order to provide a detailed description of the way the stimuli are associated to the responses , we define @xmath34 as the number of stimuli whose representation is state @xmath2 .",
    "clearly , @xmath35 and @xmath36 when the conditional probability ( [ noiseless ] ) is inserted in ( [ inf ] ) , the sum on the responses can be carried out , since only a single vector @xmath37 gives a contribution .",
    "the mutual information reads @xmath38 thus , @xmath8 is entirely determined by the way the stimuli are clustered in the response space .",
    "for example :    * consider the case where all stimuli evoke the same response .",
    "this means that all the @xmath30 coincide .",
    "accordingly , @xmath39 while all the other @xmath40 vanish . there is no way the responses can give information about the identity of the representations , and @xmath10 . *",
    "if every stimulus evokes its distinctive response there are no two equal @xmath30 .",
    "this means that a number @xmath0 of the @xmath34 are equal to one , while the remaining vanish .",
    "the responses fully characterize the stimuli , and @xmath41 . *",
    "consider the case of even clustering , where the representations are evenly distributed among all the states of the system .",
    "this , or something close to it , may in fact happen when the number of representations is much larger than the number of states @xmath42 .",
    "thus , @xmath43 , for all @xmath2 , and @xmath44 .",
    "this is the maximum amount of information that can be extracted when the set of stimuli has been partitioned in @xmath26 subsets , and the responses are only capable of identifying the subsets , but not individual stimuli .",
    "we now consider another example , namely that of a local coding scheme , sometimes called a system of _ grandmother cells_. in 1972 barlow proposed a single neuron doctrine for perceptual psychology @xcite .",
    "if a system is organized in order to archieve as complete a representation as possible with the minimum number of active neurons , at progressively higher levels of sensory processing fewer and fewer cells should be active .",
    "however the firing of each one of these high level units should code for a very complex stimulus ( as for example , one s grandmother ) .",
    "the encoding of information of such a scheme is described as local .",
    "local coding schemes have been shown to have several drawbacks @xcite , as their extreme fragility to the damage of the participating units . nevertheless , there are some examples in the brain of rather local strategies such as , for example , retinal ganglion cells ( only activated by spots of light in a particular position of the visual field @xcite ) or the rodent s hippocampal place cells ( only responding when the animal is in a specific location in its enviroment @xcite ) .",
    "we now evaluate the mutual information in such grandmother - cell scheme , making use of eq .",
    "( [ idis ] ) . for simplicity , we take the units to be binary ( @xmath45 ) .",
    "we assume that each unit @xmath46 responds to a single stimulus @xmath47 .",
    "let us take that response to be 1 , and the response to any other stimulus to be 0 .",
    "all units are taken to respond to one single stimulus and , at first , we take at most one responsive unit per stimulus .",
    "thus , for the time being , @xmath48 .",
    "this particular choice for the representations means that out of the @xmath49 states of the response space , only a subset of @xmath50 vectors is ever used . actually , @xmath51 , while for all one - active - unit states @xmath52 , @xmath53 . for the remaining responses , @xmath54 .",
    "therefore , the mutual information reads @xmath55 in fig .",
    "[ f2 ]    we show the dependence of @xmath8 on the number of cells , for several values of @xmath0 .",
    "it can be readily seen that for @xmath56 @xmath57 in the limit of large @xmath0 eq .",
    "( [ grnchic ] ) coincides with the intuitive approximation @xmath58 . \\label{intuit}\\ ] ]    a linear rise in @xmath14 means that different neurons provide different information , or , in other words , that there is no redundancy in the responses of the different cells . as seen in fig .",
    "[ f2 ] ,    this is , in fact , the case when @xmath1 is small and @xmath0 is large enough .",
    "when a cell does not respond , it is still providing some information , namely , that it is not recognizing its specific stimulus .",
    "when two cells are considered , a part of this non - specific information overlaps with the information conveyed by the second cell , when responding . in other words , if two cells respond to different stimuli then , when one of them is in state 1 , the other is , for sure , in state 0 .",
    "therefore , strictly speaking , the information provided by different neurons in a grandmotherlike encoding is not independent .",
    "however , in the limit of @xmath59 the number of stimuli not evoking responses in any single cell is large enough as to make the information approximately additive .    as @xmath1 approaches @xmath0 , such an independence no longer holds , so the growth of @xmath14 decelerates , and the curve approaches @xmath9 . for @xmath60 ,",
    "the mutual information is exactly equal to @xmath61 , and remains constant when more units are added .",
    "in fact , @xmath62 noiseless units are enough to accurately identify @xmath0 stimuli .",
    "if all @xmath62 are silent , then the stimulus shown is the one represented by the missing unit .    in a slightly more sophisticated approach",
    ", each unit can have any number of responses @xmath22 .",
    "but as long as the conditional probability @xmath63 is the same for all those @xmath5 that are not @xmath47 , eq .",
    "( [ nonna ] ) still holds .    it should be kept in mind that up to now we have considered the optimal situation , in that different units always respond to different stimuli .",
    "if several cells respond to the same stimulus , a probabilistic approach is needed since otherwise , the growth of @xmath14 depends on the order in which the units are taken .",
    "averaging over all possible selections of @xmath1 cells from a pool of @xmath64 units ( the whole set is such that there are @xmath65 cells allocated to each stimulus ) the result shown in fig .",
    "[ f2a ] is obtained .",
    "we have taken @xmath66 , and different curves correspond to various values of @xmath64 .",
    "the probabilistic approach smoothes the sharp behavior observed in fig .",
    "actually , the asymptote @xmath9 can only be reached when there is certainty that there are @xmath62 units responding to different stimuli , that is , for @xmath67 .",
    "however , it is readily seen that with @xmath65 as large as 5 , the curves are already very near to the limit case of @xmath68 .      as an alternative to the local coding scheme described above ,",
    "we now treat the case of distributed encoding , ranging from sparsely to fully distributed .",
    "however , in doing so , we employ a different approach , namely , we average the information upon the details of the representation .    equation ( [ idis ] ) implies that the amount of information that can be extracted from the responses depends on the specific representations of the @xmath0 stimuli . since it is desirable to have a somewhat more general result , we define an averaged mutual information @xmath69 @xmath70 where the mean is taken over a probability distribution @xmath71 of having the representation in positions @xmath72 .",
    "this distribution , of course , is determined by the coding scheme used by the system . by averaging the information we depart from the experimental situation , where the recorded responses strongly depend on the very specific set of stimuli chosen .",
    "but , in return , the resulting information characterizes , more generaly , the way neurons encode a certain type of stimuli , rather than the exact stimuli that have actually been employed .",
    "we write @xmath73 as a product of single distributions for each representation , @xmath74 this implies that the representation of one item does not bias the probability distribution of the representation of any other . in this sense , we can say that eq .",
    "( [ indepmem ] ) assumes that representations are independent from one another .",
    "if in one particuar experiment the set of stimuli is large enough to effectively sample @xmath75 the averaged information will be close to the experimental result .",
    "we further assume that there is a probability distribution @xmath76 that determines the frequency at which unit @xmath46 goes into state @xmath77 ( or fires at rate @xmath77 ) . if @xmath78 is strongly peaked at a particular state  which can be always be taken as zero  the code is said to be sparse . on the contrary ,",
    "a flat @xmath78 gives rise to a fully distributed coding scheme .",
    "finally , we assume that different units are independent .",
    "in other words , we factorize the probability that a given stimulus is represented by the state @xmath2 as @xmath79    in order to average the information ( [ idis ] ) we need to derive the probability that stimuli are clustered into any possible set of @xmath80 .",
    "such a probability reads @xmath81^{s_{{\\bf r}}},\\ ] ] where @xmath82 therefore , the average mutual information may be written as @xmath83 the summation runs over all sets @xmath84 such that @xmath85 . replacing eq .",
    "( [ idis ] ) in ( [ esta ] ) , we obtain @xmath86^{s_{{\\bf r } } } \\ \\sum_{{\\bf r } ' } \\frac{s_{{\\bf r}'}}{p } \\ , \\log_2 \\left ( \\frac{p}{s_{{\\bf r } ' } } \\right).\\ ] ] rearranging the summation so as to explicitly separate out a single @xmath87 one may write @xmath88^{s_{{\\bf r } } } \\ \\frac{s_{{\\bf r}}}{p } \\log_2 \\left ( \\frac{p } { s_{{\\bf r } } } \\right ) \\frac{1}{\\left ( p - s_{{\\bf r } } \\right ) ! } \\ a,\\ ] ] where @xmath89 is the sum over all other @xmath90 , namely @xmath91^ { s_{{\\bf r } '' } }   = \\left [ 1 - p_1\\left({\\bf r } \\right ) \\right ] ^{p - s_{{\\bf r}}}.\\ ] ] thus , @xmath92 ! } \\log_2 \\left(\\frac{p } { s_{{\\bf r } } } \\right )",
    "\\ \\left[p_1\\left ( { \\bf r } \\right ) \\right]^{s_{{\\bf r } } } \\ \\left[1 - [ p_1\\left({\\bf r } \\right ) \\right]^ { p - s_{{\\bf r}}}. \\label{oh}\\ ] ]    we now discuss two particular cases of eq .",
    "( [ oh ] ) .",
    "first , we take the encoding to be fully distributed , namely @xmath93 . therefore , @xmath94 . if this is replaced in the previous expression , we obtain @xmath95 it may be seen that the dependence of the information on @xmath22 and @xmath1 always involves the combination @xmath26 .",
    "this means that neither the number of units , nor how many distinctive firing rates each unit has are relevant in themselves . only the total number of states matters .    in fig .",
    "[ f3 ]    we plot the relation between @xmath96 and @xmath1 for several values of @xmath0 .",
    "initially the information rises linearly with a slope only slightly dependent on @xmath0 . as @xmath1 increases",
    ", @xmath97 eventually saturates at @xmath9 .",
    "the limit cases are easily derived @xmath98 if the number of stimuli is large , eq .",
    "( [ chinpun ] ) becomes @xmath99 notice that in contrast to the local coding scheme eq .",
    "( [ nonna ] ) , the initial slope of @xmath14 hardly depends on @xmath0 ( actually , it increases slightly with @xmath0 ) .",
    "this makes the distributed encoding a highly efficient way to read out information about a large set of stimuli by the activity of just a few units .    as opposed to the fully distributed case , a sparse distributed encoding",
    "is now considered , with @xmath45 , @xmath100 , @xmath101 and @xmath102 .",
    "this choice is again a binary case , but with one response much more probable than the other . as a consequence ,",
    "the most likely representations in @xmath25 space are those with either zero or at most one active neuron .",
    "in fact , @xmath103 , whereas if the representation is a one - active - unit state @xmath52 , @xmath104 .",
    "the probability of all other representations is higher order in @xmath105 .",
    "accordingly , to first order in @xmath105 , we only consider the combinations of @xmath0 representations with at least @xmath62 of them in state @xmath106 .",
    "these are the only responses with a probability @xmath73 at most linear in @xmath105 .",
    "more precisely , the probability of representing all @xmath0 stimuli with the same state @xmath107 is @xmath108^p \\approx 1 - n p q$ ] . in the same way ,",
    "the probability of having @xmath109 stimuli in @xmath110 and a single one - active - unit state is @xmath105 .",
    "there are @xmath1 different possible one - active - unit states , and any one of the @xmath0 stimuli can be such a state . taking all this into account , we find that up to the first order in @xmath111 , @xmath112 .",
    "\\label{iuju}\\ ] ] expanding this expression for large @xmath0 , we obtain @xmath113 this means that from the experimental measurement of the slope of @xmath114 it is possible to extract the sparseness of an equivalent binary model , which can be compared with a direct measurement of the sparseness .",
    "if the number of stimuli can not be considered large , the whole of eq .",
    "( [ iuju ] ) can be used to derive a value for @xmath105 .",
    "it should be noticed that if @xmath115 eq .",
    "( [ iuju ] ) coincides with the expression ( [ intuit ] ) for a grandmother - like encoding .",
    "this makes sense , since @xmath115 implies that , on average , any one unit is activated by a single pattern . in short",
    ", it corresponds to a probabilistic description of the localized encoding .",
    "notice , though , that @xmath115 is outside the range of validity of our limit @xmath116 .",
    "in this section we turn to a more realistic description of the single neuron responses . specifically , we allow the states @xmath77 to take any real value . therefore , the response space @xmath25 is now @xmath117 .",
    "in addition , we depart from the deterministic relationship between stimuli and responses .",
    "this means that upon presentation of stimulus @xmath5 , there is no longer a unique response . instead , the response vector @xmath2 is most likely centered at a particular @xmath30 , and shows some dispersion to nearby vectors .",
    "the aim is to calculate the mutual information between the responses and the stimuli requiring as little as possible from the conditional probability @xmath118 .",
    "a single parameter @xmath119 is introduced as a measure of the noise in the representation .",
    "thus , @xmath120 where the index @xmath5 takes values from @xmath121 to @xmath0 .",
    "the conditional probability depends on the distance between the actual response @xmath2 and a fixed vector @xmath122 , which is the mean response of the system to stimulus @xmath5 .",
    "there is one such @xmath30 for every element in @xmath28 .",
    "the choice of gaussian functions is only to keep the description simple and analytically tractable . by factorizing @xmath123 in a product of one component",
    "probabilities an explicit assumption about the independence of the neurons is being made .",
    "figure [ f4 ] shows a numerical evaluation of the information ( [ inf ] ) , when the probability @xmath118 is as in ( [ ret ] ) .",
    "the information , just as in the previous section , has been averaged upon many selections of the representations @xmath30 .",
    "the curve is a function of the number of neurons considered @xmath1 .",
    "different lines correspond to different sizes of the set of stimuli , while in ( a ) @xmath124 , and in ( b ) @xmath125 , where @xmath126 is a parameter quantifying the mean discriminability among representations , to be defined precisely later . just as in the discrete distributed case , we observe an initial linear rise and a saturation at @xmath9 . moreover , and pretty much as in the experimental situation of fig .",
    "[ f1 ] , the initial slope does not seem to depend strongly on the number of stimuli , at least for large values of the noise @xmath119 . in what follows ,",
    "an analytical study of these numerical results is carried out . in particular , the relevant parameters determining the shape of @xmath14 are identified .",
    "we write the mutual information as @xmath127 where @xmath128 =   -   \\int d { \\bf r } \\",
    "p\\left({\\bf r } \\right ) \\ \\log",
    "_ 2 \\left[p\\left({\\bf r}\\right ) \\right ] , \\label{i2}\\ ] ] is the total entropy of the responses , and @xmath129 .",
    "\\label{i1}\\ ] ] is the conditional entropy of @xmath123 , averaged over @xmath5 .",
    "@xmath130 can be easily calculated .",
    "it reads @xmath131 .",
    "\\label{i1a}\\ ] ] it is therefore linear in @xmath1 .",
    "this stems from the independence of the units , since the entropy of the response space increases linearly with its dimension .",
    "it does not depend on the location of the representations @xmath30 , and it is a growing function of the noise @xmath119 .    in appendix",
    "a we solve the integral in @xmath2 of @xmath132 using the replica method .",
    "we obtain @xmath133 \\right\\ } - 1 \\right ) , \\nonumber\\end{aligned}\\ ] ] where @xmath134 now stands for the set @xmath135 specifying how many replicas are representing each pattern .",
    "the summation in @xmath134 runs over all sets of @xmath136 such that @xmath137 .",
    "the symbol in brackets is defined in ( [ multin ] ) .",
    "equation ( [ i2fin ] ) shows that the information depends explicitly on the ratio between all the possible differences @xmath138 and the noise @xmath119 . in other words , the capacity to determine which stimulus is being shown is given by a signal - to - noise ratio , characterizing the discriminability of the responses .    the mutual information @xmath8 characterizes the selectivity of the correspondence between stimuli and responses .",
    "if the distance between any two vectors @xmath138 is much greater than the noise @xmath119 , then the mapping is ( almost ) injective .",
    "thus , in this limit the mutual information approaches its maximal value , @xmath9 .    if , on the other hand , the noise level in @xmath139 is enough to allow for some vectors @xmath2 to be evoked with appreciable probability by more than one stimulus , the mutual information decreases . in this sense",
    ", @xmath8 can be interpreted as a comparison between the noise in @xmath123 and the distance between any two mean responses . for a specific choice of the representations ,",
    "the distance between any two of them is a non linear function of their components .",
    "therefore , in general , even though eq .",
    "( [ ret ] ) implies that different units are independent , it is not possible to write @xmath8 as a sum over units of single - units information .",
    "just as before , we now average the mutual information ( [ inf ] ) over a probability distribution @xmath140 of the representations @xmath141 , namely @xmath142 under the assumption that the responses to different stimuli are independent , @xmath73 reads @xmath143 adding the requirement of independent units , @xmath144 by replacing the average ( [ iprom ] ) in the separation ( [ separ ] ) we write @xmath145 since @xmath130 does not depend on the vectors @xmath30 .",
    "so we now turn to the calculation of @xmath146 , namely @xmath147 , \\nonumber\\end{aligned}\\ ] ] where @xmath148 .",
    "\\label{i2c}\\ ] ] the main step forward introduced by the average in ( [ iprom ] ) is that now , @xmath146 is symmetric under the exchange of any two responses , or any two neurons .",
    "in contrast , before the averaging process , the location of every single response by every single unit was relevant .",
    "the limit in eq .",
    "( [ i2d ] ) can be calculated in some particular cases . in the first place",
    ", we analyze the large @xmath1 limit . from eq .",
    "( [ i2c ] ) it is clear that @xmath149 .",
    "the equality holds , in fact , only when there is a single @xmath136 different from zero . in the calculation of @xmath146 , as stated in eq .",
    "( [ i2d ] ) , @xmath150 appears to the @xmath1-th power .",
    "therefore , when @xmath151 only the terms with @xmath152 give a non - vanishing contribution .",
    "there are @xmath0 of such terms .",
    "when the sum in ( [ i2d ] ) is replaced by @xmath0 , it may be shown that once more , @xmath153 .    in the following two subsections we compute @xmath114 for both large and small values of the noise @xmath119 .",
    "we now make the assumption that the noise @xmath119 is much larger than some average width of @xmath154 . in other words ,",
    "we suppose @xmath155 , for all @xmath156 and @xmath157 with non - vanishing probability . in this case , the exponential in ( [ i2c ] ) may be expanded in taylor series . up to the second order , @xmath158 & \\approx & 1 - \\label{smals } \\\\ & & \\frac{1}{4(n + 1)\\sigma^2 }",
    "\\sum_{m = 1}^p \\sum_{\\ell = 1}^p k_m k_\\ell ( r^m - r^\\ell)^2 + \\nonumber \\\\ & & \\frac{1}{2}\\left[\\frac{1}{4 ( n + 1)\\sigma^2 } \\sum_{m = 1}^p \\sum_{\\ell = 1}^p k_m k_\\ell ( r^m - r^\\ell)^2 \\right]^2 \\nonumber\\end{aligned}\\ ] ] if only the constant term is considered , the integral in eq . ( [ i2c ] ) becomes the normalization condition for @xmath73 .",
    "thus , the sums in ( [ i2d ] ) give @xmath159 , and it is readily seen that @xmath146 exactly cancels @xmath130 . as expected , in the limit @xmath160 the mutual information vanishes .    the next order of approximation is to consider the expansion ( [ smals ] ) up to the linear term .",
    "thus , the integral in ( [ i2c ] ) becomes @xmath161 where @xmath162 is the parameter quantifying the discriminability among representations , and appearing in fig .",
    "[ f4 ] .",
    "we have now gained a more precise insight of the large @xmath119 limit .",
    "it stands for taking @xmath163 .    since in eq .",
    "( [ i2d ] ) @xmath150 appears to the @xmath1-th power , in order to proceed further we have to estimate the size of @xmath164 .",
    "we first consider the small @xmath1 limit and assume , to start with , that @xmath165 .",
    "thus , we may expand @xmath166 in appendix b we calculate the sums in ( [ i2cd ] ) , thus obtaining @xmath146 .",
    "when the result is replaced in ( [ i2d ] ) we get @xmath167 for a large amount of noise , the information rises linearly with the number of neurons .",
    "this dependence should be compared with eq .",
    "( [ lindis ] ) , in the discrete distributed case .",
    "the two expressions coincide , if the number of discrete states @xmath22 is associated to @xmath168 . therefore , as regards to the mutual information , a dispersion @xmath119 in the representation is equivalent to having a number @xmath169 of distinguishable discrete responses . notice that both the noisy , continuous and the discrete , deterministic approch show the same dependence on the number of representations .",
    "regarding the dependence on @xmath119 , it is readily seen that as the noise decreases , the slope of @xmath8 increases .",
    "in other words , every single neuron provides a larger amount of information . since the mutual information saturates at @xmath9 for @xmath151",
    ", a small value of @xmath119 implies that the ceiling is quickly reached . as a consequence",
    ", the assumption @xmath170 can now be more precisely stated as @xmath171 . in this regime , linearity holds .",
    "as @xmath1 increases , saturation effects become evident , and the mutual information is no longer linear .",
    "the first hint of the presence of an asymptote at @xmath9 is given by the quadratic contribution to @xmath14 . in order to describe it , the whole of expansion ( [ smals ] ) must be replaced in eq .",
    "( [ i2c ] ) .",
    "carrying out the integral in @xmath172 , @xmath173 where @xmath174 ^ 2 \\ \\pi_{s = 1}^p \\ \\ \\rho\\left(r^s",
    "\\right ) \\ , dr^s \\label{eta}\\ ] ] extracting the sums from the integral , the limit in eq .",
    "( [ i2d ] ) can be solved , and , @xmath175   - \\frac{n^2}{\\ln 2 } \\left(\\frac{\\lambda^2}{4 \\sigma^2 } \\right)^2 \\frac{p - 1}{p^2}. \\label{des}\\ ] ] here , @xmath176 with @xmath177 our numerical simulations corroborate that if a quadratic function is fit to the initial rise of @xmath14 , the coefficients accompanying @xmath1 and @xmath178 depend on @xmath0 and @xmath119 just as predicted by eq .",
    "( [ des ] ) .      in the first place , we take @xmath179 .",
    "if the conditional probability ( [ ret ] ) is replaced by a @xmath180-function , it is readily seen that @xmath41 .    in appendix c",
    "we show that for small  but not vanishing  values of the noise @xmath119 , the mutual information is expected to grow as @xmath181 , \\label{pocoruid}\\ ] ] where @xmath182    in order to corroborate this result , we have fit a function of the form @xmath183 $ ] to the numerical evaluation of eq .",
    "( [ i2e ] ) . in fig .",
    "[ fnueva ]    we show the dependence of @xmath184 and @xmath185 with @xmath119 and @xmath0 .",
    "we observe that coefficient @xmath184 shows a dependence with the noise @xmath119 , in contrast to what is predicted by eq .",
    "( [ pocoruid ] ) .",
    "it is also in contrast to the prediction of the phenomenological model leading to eq .",
    "( [ grra ] ) , where @xmath186 .",
    "in addition , @xmath185 shows a variation with the number of stimuli @xmath0 .",
    "thus , although it is very easy to calculate the mutual information when @xmath119 is exactly equal to 0 , we have not been able to derive analytically the approach to the @xmath9 limit , as @xmath179 .",
    "up until now , we have considered the mutual information of eq.([inf ] ) , a quantifier of the capacity with which a given group of units can represent a fixed set of @xmath0 stimuli .",
    "this is a measure of direct relevance to neuronal recording experiments .",
    "a somewhat different information measure has been used in analysing mathematical network models , in particular models of memory storage and retrieval .",
    "we would like to clarify the relationship between the two measures .    consider the variability with which a typical stimulus is represented , which in a mathematical model might be described by a formula as simple as eq.([ret ] ) .",
    "there , @xmath2 is the response during a trial , while @xmath30 is the average response across trials with the same stimulus .",
    "the average variability may be quantified by the mutual information between @xmath2 and @xmath30 , @xmath187 .",
    "\\label{iha}\\ ] ] where @xmath30 is taken to span the space of average responses , described by the probability distribution @xmath188 . in a different model , @xmath30 might be the first response produced , and @xmath2 the second , or any successive response ; in yet other models @xcite , @xmath30 might be the stored representation of a memory item , and @xmath2 the representation emerging when the item is being retrieved . in all such cases , one need not refer to a discrete set of @xmath0 stimuli , but only to a probability distribution @xmath188 ( and , of course , to a conditional probability distribution @xmath189 ) .",
    "this measure of accuracy is simply related to the mutual information we have considered in this paper : it is given by its @xmath190 limit .",
    "in particular , the initial linear rise of @xmath191 with @xmath1 is the only regime relevant to the accuracy measure , which for independent units is always purely linear in @xmath1 .",
    "let us see this in formulas .",
    "just as before , we assume that @xmath188 factorizes as @xmath192 the equivalent of ( [ pglob ] ) is now @xmath193 in appendix d we show that @xmath194 in the derivation of eq . ( [ ih ] ) no assumption of small",
    "@xmath1 has been made . by comparison with eq .",
    "( [ imchico ] ) we see that , indeed , the information measure ( [ iha ] ) introduced in this section coincides with the initial rise of the information about which stimulus is being shown ( sect .",
    "iii ) , when the latter is calculated for a large number of stimuli .",
    "the capacity with which a system of @xmath1 independent units can code for a set of @xmath0 stimuli has been studied .",
    "more precisely , the growth of the mutual information @xmath8 between stimuli and responses has been calculated , for different models of the neural responses . in all these models ,",
    "the units were supposed to operate independently . that is to say , the conditional probability of response @xmath2 given stimulus @xmath5 is always a product of single - unit conditional probabilities .",
    "of course , the fact that neurons operate independently does not mean that they provide independent information . as stated in eq .",
    "( [ separ ] ) , the mutual information can always be separated into the difference between the entropy of the responses ( @xmath132 ) and the averaged stimulus specific entropy ( @xmath130 ) , sometimes called noise entropy . for independent units ,",
    "@xmath130 is always linear in @xmath1 .",
    "however , the factorization of the conditional probabilities does not imply the factorization of @xmath195 , meaning that @xmath132 need not be linear in the number of units . in other words , even independent units may produce correlated responses , and indeed strongly correlated , simply because every unit is driven by the same set of stimuli .",
    "imagine that each unit provides a very precise representation of the stimuli .",
    "if stimulus 1 is shown , the responses of the @xmath1 units will show almost no trial to trial variability . when the stimulus is changed , another set of @xmath1 responses is obtained .",
    "but the first responses always come together ( driven by stimulus 1 ) , and so do the second ones . even after averaging over all stimuli ,",
    "this coherent behavior implies strong correlations between the responses . in this example , @xmath196 and @xmath197 .    in other situations , when the number of stimuli is very large , or the representation of each one of them is noisier , the correlations in the responses are weaker .",
    "we have seen that , in these cases , @xmath132 tends to become linear in @xmath1 .    throughout the work ,",
    "the responses of the units was described by a vector @xmath2 .",
    "nothing was said , however , about what the components of the vector really are . in the experiment of fig .",
    "[ f1 ] , @xmath77 was the firing rate of neuron @xmath46 in a pre - defined time window .",
    "one might however consider a slightly more complex description in which a subset of @xmath64 components is associated to the response of unit @xmath46 .",
    "for example , the first @xmath64 principal components of its time course @xcite .",
    "our analysis would still apply , replacing @xmath1 units by @xmath198 components .    in the introduction",
    ", reference was made to the phenomenological models where the growth of @xmath14 as given by eq .",
    "( [ grra ] ) is entirely explained by ceiling effects .",
    "in such models , the information provided by different neurons is supposed to be independent , inasmuch this is compatible with the fact that the total amount of information must be @xmath9 .",
    "the models presented in this paper are not in principle opposed to the phenomenological ones ; rather they are at a more detailed level of description . instead of a direct assumption on how different units share the available information , we specify conditional probabilities for the responses . as a result",
    ", we find global trends that closely resemble those of eq .",
    "( [ grra ] ) , that is to say , an initial linear rise and an exponential saturation at @xmath9 .",
    "the detailed shape of @xmath14 is , however , different for each model .",
    "it should be kept in mind that whatever the detailed shape of the curve , the approach to @xmath9 is no more than a consequence of the fact that the number of stimuli is limited .",
    "the maximum information that can be extracted from the neural responses is @xmath9 .",
    "it is clear that if we have a set of neurons that already provides an information very near to this maximum , by adding one more neuron we will gain no more than redundant information . in other words , we have reached a regime where the neural responses correctly distinguish the identity of each stimulus .",
    "but we can not deduce from this that the representational capacity of the responses remains unchanged when the number of neurons increases .",
    "one should rather realize that the task itself is no longer appropriate to test the way additional neurons contribute in the encoding of stimuli .",
    "in contrast , the slope of the initial linear rise is an accurate quantification of the capacity of the system to represent items .",
    "we have found that distributed coding schemes result in an initial slope that is roughly independent of the number of stimuli .",
    "this means that the number of units needed to reach a given fraction of the maximum information scales as @xmath9at least , for large @xmath0 .",
    "in contrast , when a grandmother - cell encoding is used , the initial slope is proportional to @xmath199 , and hence , one should have @xmath200 .",
    "this makes distributed encoding much more efficient than localized schemes . in the example of the experiment of fig .",
    "[ f1 ] , the information measure supports the conclusion , already evident from the responses themselves , that the representation of faces in the inferior temporal cortex of the macaque is distributed .",
    "we thank damian zanette for a critical reading of the manuscript .",
    "this work has been supported with a grant of the human frontier science programm , number rg 01101998b .",
    "replacing the identity @xmath201 in ( [ i2 ] ) the integral in @xmath2 can be evaluated .",
    "this we show in the present appendix . @xmath202^n - 1 \\right\\ } \\nonumber \\\\   & = & \\frac{-1}{\\ln 2 } \\",
    "\\lim_{n \\to 0 } \\frac{1}{n } \\ \\left ( \\frac{h_{1a}}{p^{n + 1 } } - 1 \\right),\\end{aligned}\\ ] ] where @xmath203^n \\sum_{s_1 = 1}^p ... \\sum_{s_{n + 1 } = 1}^p \\pi_{k = 1}^{n } \\ \\",
    "h_{1b}(j ) , \\label{i2a}\\ ] ] and @xmath204 \\label{inty}\\ ] ] is a factor that depends on the @xmath46-th component of one particular way of distributing @xmath0 stimuli among the @xmath205 replicas . to calculate it we observe that @xmath206 ^ 2 + \\vec{\\xi}_j^ { \\ \\dag } \\ a \\",
    "\\vec{\\xi}_j , \\label{grando}\\ ] ] where @xmath207 is a vector of @xmath205 components such that @xmath208 .",
    "the vector notation is used for arrays of @xmath205 components .",
    "the matrix @xmath89 has dimensions @xmath209 , and reads @xmath210 where @xmath211 is an @xmath212 matrix , whith all its coefficients equal to unity .",
    "thus , the quadratic factor in ( [ grando ] ) can be extracted outside the integral in ( [ inty ] ) , and @xmath213 replacing this expression in ( [ i2a ] ) @xmath214^{-n } \\",
    "\\sum_{s_1 = 1}^p ... \\sum_{s_{n + 1 } =   1}^p \\exp\\left ( \\frac{-1}{2\\sigma^2 } \\sum_{j = 1}^n \\vec{\\xi}_j^ { \\ \\dag } \\ , a \\ , \\vec{\\xi}_j\\right ) \\label{i2aya}\\ ] ]    we now re - arrange the summation in ( [ i2aya ] ) , according to the number @xmath215 of _ different _ stimuli appearing in the @xmath205 replicas . for each realization of @xmath216",
    ", the replicas can be divided in @xmath215 classes , such that all the replicas belonging to the same class are associated to the same stimulus , and replicas of different classes correspond to different stimuli .",
    "the number of replicas adscribed to stimulus @xmath46 is @xmath217 . clearly , the sum of all the @xmath217 is @xmath205 , and only @xmath215 of the @xmath218 are different from zero .",
    "therefore , @xmath219 where the term in brackets is defined in ( [ multin ] ) , and the @xmath220-fold summation involves all possible sets of @xmath221 ranging from @xmath222 to @xmath205 , and whose total sum is @xmath205 .",
    "the advantage of this rearrangement is that the exponent in ( [ i2aya ] ) can be written as a function of only the differences between representations , namely @xmath223 therefore , replacing equations ( [ suma ] ) and ( [ expo ] ) in ( [ i2a ] ) we arrive at eq ( [ i2fin ] )",
    "replacing ( [ smals ] ) in ( [ i2d ] ) , we get @xmath224   - 1 \\right\\ } , \\label{lio}\\ ] ] with @xmath225 in order to compute @xmath90 we interchange the order of summation @xmath226 the terms with @xmath227 or @xmath228 equal to zero , do not contribute to @xmath90 .",
    "therefore , we can restrict the sum in ( [ s ] ) to @xmath229 .",
    "thus , the addition over all @xmath136 s ranging from 0 to @xmath205 whose total sum is @xmath205 can be replaced by another addition , where all @xmath136 s different from @xmath227 and @xmath228 range from 0 to @xmath230 , @xmath228 and @xmath227 go from 1 to @xmath231 , and the sum of all the @xmath136 s is @xmath205 . since there are @xmath232 choices for @xmath227 and @xmath228 , @xmath233 replacing equation ( [ ecochi ] ) in ( [ lio ] ) we get @xmath234 +   \\frac{n}{\\ln 2 } \\frac{p - 1}{p } \\left(\\frac{\\lambda}{2 \\sigma}\\right)^2.\\ ] ] when @xmath130 is summed to @xmath146 , equation ( [ imchico ] ) is obtained .",
    "we go back to eq .",
    "( [ i2d ] ) .",
    "we re - write ( [ i2c ] ) as @xmath235 , \\label{intexp}\\ ] ] where @xmath236 is a vector of @xmath0 components , such that @xmath237 , and @xmath238 the integrand in ( [ intexp ] ) is 1 in the origin , and also along the eignenvectors of @xmath64 corresponding to a zero eigenvalue .",
    "the number of such eigenvalues is equal or larger than the number of @xmath136 that are zero .",
    "we therefore re - arrange the numbering of the representations in such a way as to put all those with @xmath136 different from zero in the first @xmath215 places .",
    "thus , @xmath239 . with this ordering , matrix @xmath64 is filled with zeros in all those positions with a row or a column greater than @xmath215 . integrating in @xmath240 we get @xmath241 , \\label{intexp1}\\ ] ] where @xmath242 and @xmath243 are defined as @xmath236 and @xmath64 , but live in a space of @xmath215 dimensions ( and not @xmath0 ) .    in order to integrate eq .",
    "( [ intexp1 ] ) we observe that @xmath243 has a single eignevalue @xmath244 equal to zero , with eigenvector @xmath245 we call @xmath246 all the other eigenvectors corresponding to nonvanishing eigenvalues @xmath247 .",
    "we choose the eigenvectors normalized , and orthogonal to each other and to @xmath248 ( the symmety of @xmath64 allows us to do so ) . with this set of vectors",
    "we construct a new basis , and call @xmath249 the collection of coordinates in this new system .",
    "we define a matrix @xmath250 as the change of basis @xmath251 where @xmath252 and det(@xmath250 ) = 1 . in this new basis , @xmath253.\\ ] ] multiplying and dividing by the product of all @xmath254 , for @xmath255 $ ] , we get @xmath256 \\",
    "\\frac{\\exp\\left[-\\frac{1}{2 } \\sum_{k = 1}^d \\frac{\\lambda_k}{\\sigma^2(n + 1 ) } w_k^2\\right]}{\\pi_{j = 2}^d \\",
    "\\sqrt{\\frac{2 \\pi \\sigma^2 ( n + 1 ) } { \\lambda_j}}}. \\nonumber\\end{aligned}\\ ] ] in the limit @xmath179 , the integrand in ( [ bbl ] ) includes @xmath257 delta functions .",
    "once integrated , @xmath258^d.\\ ] ] it may be shown that @xmath259 thus , @xmath260 where @xmath261^d.\\ ] ]    we now turn to the calculation of @xmath262 where , as before , the summation runs over all sets of @xmath263 that add up to @xmath205 .",
    "equation ( [ mish ] ) states that @xmath264 depends on @xmath215 , that is , on the number of @xmath136 that are different from zero .",
    "therefore , we write the sum in ( [ sha ] ) as @xmath265^n \\ \\ s_1,\\ ] ] where @xmath266^{n/2}. \\label{ejuj}\\ ] ] the sum in @xmath267 involves only the @xmath215 values of @xmath136 that are different from zero .",
    "we now make the approximation @xmath268 but @xmath269 and , taking the limit @xmath270 moreover , if @xmath1 is large , as @xmath215 grows @xmath271 .",
    "therefore , keeping just @xmath272 and @xmath273 we may approximate @xmath274 replacing in eqs .",
    "( [ i2d ] ) and ( [ separ ] ) we arrive at ( [ pocoruid ] ) .",
    "the aim is to calculate ( [ iha ] ) under the assumption ( [ ret ] ) . replacing ( [ ret ] ) in ( [ pedey ] )",
    "the probability @xmath195 can be written as @xmath275 where @xmath276 just as before , we separate @xmath277 where @xmath278 =   \\frac{n}{2 \\ln 2 } \\left[1 + \\ln(2 \\pi \\sigma^2 ) \\right ] , \\\\ h_1 & = & - \\int d { \\bf r}^0 \\int d { \\bf r } \\ p({\\bf r } | { \\bf r}^0 ) p({\\bf r}^0 ) \\log_2\\left[p({\\bf r})\\right ] = - n \\int d t \\",
    "\\zeta(t ) \\ \\log_2\\left[\\zeta(t)\\right ]   \\nonumber\\end{aligned}\\ ] ] inserting the definition ( [ zeta ] ) of @xmath279 , and using the expression ( [ rep ] ) for the logarithm we get @xmath280 the last term in ( [ aca ] ) is nothing but the integral of @xmath281 over all @xmath282 , which can be shown to give 1 . to carry out the integral in @xmath283 in the first line of ( [ aca ] ) we observe that @xmath284 ^ 2 - \\frac{1}{2 \\sigma^2 } \\left [ \\sum_{j = 0}^{n + 1 } x_j^2 - \\frac{1}{n + 1}\\left ( \\sum_{k = 1}^{n + 1 } x_j \\right)^2 \\right ] \\right\\ } \\nonumber\\end{aligned}\\ ] ] when replacing this expression in ( [ aca ] )",
    ", the integration in @xmath283 can be done right away .",
    "the result is @xmath285^{1/2}$ ] .",
    "therefore , @xmath286 \\right\\ }   - 1\\right ] .",
    "\\label{aca2}\\end{aligned}\\ ] ] in the same way as in eq .",
    "( [ expo ] ) , we write @xmath287 thus , replacing eq . ( [ aca1 ] ) in ( [ aca2 ] ) , and making the expansion @xmath288 \\approx 1 - \\frac{1}{4 \\sigma^2(n + 1 ) } \\sum_{\\ell = 1}^{n + 1 } \\sum_{m = 1}^{n + 1 } ( x_\\ell - x_m)^2 , \\label{ximacion}\\ ] ] @xmath132 can be calculated .",
    "the result is @xmath289 + \\frac{\\lambda^2}{4 \\sigma^2 } \\right\\}.\\ ] ] when @xmath130 is substracted , eq .",
    "( [ ih ] ) is obtained .",
    "it should be noticed that eq .",
    "( [ ximacion ] ) is not an approximation .",
    "the @xmath46-th order in the taylor expansion of the exponential grows as @xmath290^j$ ] .",
    "therefore , only the linear term gives a contribution for @xmath291 .",
    "l. m. optican and b. j. richmond `` temporal encoding of two - dimensional patterns by single units in primate inferior temporal cortex : an information theoretical analysis '' _ j. neurophysiol . _ * 57 * 162 - 178 ( 1987 )    e. n. eskandar , b. j. richmond and l. m. optican `` role of inferior temporal neurons in visual memory : temporal encoding of information about visual images , recalled images and behavioural context '' _ j. neurophysiol . _ * 68 * 1277 - 1295 ( 1992 )              e. t. rolls , a. treves , and m. j. tovee `` the representational capacity of the distributed encoding of information provided by populations of neurons in primate temporal visual cortex '' _ exp . brain .",
    "res . _ * 114 * 149 - 162 ( 1997 ) ;              ins samengo `` independent neurons representing a finite set of stimuli : dependence of the mutual information on the number of units sampled '' , to be published in _ network : computation in neural systems _ , scheduled for february 2000 ."
  ],
  "abstract_text": [
    "<S> the capacity with which a system of independent neuron - like units represents a given set of stimuli is studied by calculating the mutual information between the stimuli and the neural responses . </S>",
    "<S> both discrete noiseless and continuous noisy neurons are analyzed . in both cases </S>",
    "<S> , the information grows monotonically with the number of neurons considered . under the assumption that neurons are independent </S>",
    "<S> , the mutual information rises linearly from zero , and approaches exponentially its maximum value . </S>",
    "<S> we find the dependence of the initial slope on the number of stimuli and on the sparseness of the representation . </S>"
  ]
}