{
  "article_text": [
    "the @xcite proportional hazards model is widely used in the regression analysis of censored survival data , notably in identifying risk factors in epidemiological studies and treatment effects in clinical trials when the outcome variable is time to event . in a traditional biomedical study , the number of covariates @xmath0 is usually relatively small as compared with the sample size @xmath1 .",
    "theoretical properties of the maximum partial likelihood estimator in the fixed @xmath0 and large @xmath1 setting have been well established .",
    "for example , @xcite proved the asymptotic normality of the maximum partial likelihood estimator .",
    "@xcite formulated the cox model in the context of the more general counting process framework and studied the asymptotic properties of the maximum partial likelihood estimator using martingale techniques .",
    "these results provide a solid foundation for the applications of the cox model in a diverse range of problems where time to event is the outcome of interest . in recent years",
    ", technological advancement has resulted in the proliferation of massive high - throughput and high - dimensional genomic data in studies that attempt to find genetic risk factors for disease and clinical outcomes , such as the age of disease onset or time to death .",
    "finding genetic risk factors for survival is fundamental to modern biostatistics , since survival is an important clinical endpoint .",
    "however , in such problems , the standard approach to the cox model is not applicable , since the number of potential genetic risk factors is typically much larger than the sample size .",
    "in addition , traditional variable selection methods such as subset selection are not computationally feasible when @xmath2 .",
    "the @xmath3-penalized least squares estimator , or the lasso , was introduced by @xcite . in the wavelet",
    "setting , the @xmath3-penalized method was introduced by @xcite as basis pursuit . since then , the lasso has emerged as a widely used approach to variable selection and estimation in sparse , high - dimensional statistical problems .",
    "it has also been extended to the cox model [ @xcite ] .",
    "@xcite implemented the lars algorithm [ @xcite ] to approximate the lasso in the cox regression model and applied their method to survival data with microarray gene expression covariates .",
    "their work demonstrated the effectiveness of the lasso for variable selection in the cox model in a @xmath2 setting .",
    "there exists a substantial literature on the lasso and other penalized methods for survival models with a fixed number of covariates @xmath0 .",
    "@xcite considered an adaptive lasso for the cox model and showed that , under certain regularity conditions and with a suitable choice of the penalty parameter , their method possesses the asymptotic oracle property when the maximum partial likelihood estimator is used as the initial estimator .",
    "@xcite proposed the use of the smoothly clipped absolute deviation ( scad ) penalty [ @xcite , @xcite ] for variable selection and estimation in the cox model which may include a frailty term . with a suitable choice of the penalty parameter , they showed that a local maximizer of the penalized log - partial likelihood has an asymptotic oracle property under certain regularity conditions on the hessian of the log - partial likelihood and the censoring mechanism .",
    "extensive efforts have been focused upon the analysis of regularization methods for variable selection in the @xmath2 setting . in particular ,",
    "considerable progress has been made in theoretical understanding of the lasso .",
    "however , most results are developed in the linear regression model .",
    "@xcite studied the prediction performance of the lasso in high - dimensional linear regression .",
    "@xcite showed that , for neighborhood selection in the gaussian graphical model , under a neighborhood stability condition and certain additional regularity conditions , the lasso is consistent even when @xmath4 .",
    "@xcite formalized the neighborhood stability condition in the context of linear regression as a strong irrepresentable condition on the design matrix .",
    "oracle inequalities for the prediction and estimation error of the lasso was developed in @xcite , @xcite , @xcite , @xcite , @xcite and @xcite , among many others .    a number of papers analyzed penalized methods beyond linear regression . @xcite established oracle properties for a local solution of concave penalized estimator in a general setting with @xmath5 .",
    "@xcite studied the lasso in high - dimensional generalized linear models ( glm ) and obtained prediction and @xmath3 estimation error bounds .",
    "@xcite studied penalized m - estimators with a general class of regularizers , including an @xmath6 error bound for the lasso in glm under a restricted convexity and other regularity conditions .",
    "@xcite made significant progress by extending the results of @xcite to a more general class of penalties in the cox regression model with large @xmath0 under different sets of regularity conditions .",
    "@xcite studied weighted absolute penalty and its adaptive , multistage application in glm .",
    "in view of the central role of the cox model in survival analysis , its widespread applications and the proliferation of @xmath7 data , it is of great interest to understand the properties of the related lasso approach .",
    "the main goal of the present paper is to establish theoretical properties for the lasso in the cox model when .",
    "specifically , we extend certain basic inequalities from linear regression to the case of the cox regression .",
    "we generalize the compatibility and cone invertibility factors from the linear regression model and establish oracle inequalities for the lasso in the cox regression model in terms of these factors at the true parameter value .",
    "moreover , we prove that the compatibility and cone invertibility factors can be treated as constants under mild regularity conditions .",
    "a main feature of our results is that they are derived under the more general counting process formulation of the cox model with potentially a larger number of time - dependent covariates than the sample size .",
    "this formulation is useful because it `` permits a regression analysis of the intensity of a recurrent event allowing for complicated censoring patterns and time - dependent covariates '' [ @xcite ] .",
    "a second main feature of our results is that the regularity conditions on the counting processes and time - dependent covariates are directly imposed on the compatibility and cone invertibility factors of the hessian of the negative log - partial likelihood evaluated at the true regression coefficients . under such regularity conditions ,",
    "the lasso estimator is proven to live in a neighborhood where the ratio between the estimated and true hazards is uniformly bounded away from zero and infinity .",
    "this allows unbounded and near zero ratios between the true and baseline hazards .",
    "our analysis can be also used to prove oracle inequalities based on the restricted eigenvalue . however , since the compatibility and cone invertibility factors are greater than the corresponding restricted eigenvalue [ @xcite , @xcite ] , the presented results are sharper .",
    "a third main feature of our results is that the compatibility and cone invertibility factors used , and the smaller corresponding restricted eigenvalue , are proven to be greater than a fixed positive constant under mild conditions on the counting processes and time - dependent covariates , including cases where @xmath7 . in the cox regression model ,",
    "the hessian matrix is based on weighted averages of the cross - products of time - dependent covariates in censored risk sets , so that the compatibility and cone invertibility factors and the restricted eigenvalue are random variables even when they are evaluated for the hessian at the true regression coefficients . under mild conditions ,",
    "we prove that these quantities are bounded from below by positive constants as certain truncated population versions of them .",
    "thus the compatibility and cone invertibility factors can be treated as constants in our oracle inequalities .",
    "the main results of this paper and the analytical methods used for deriving them are identical to those in its predecessor submitted in november 2011 , with section  [ sec4 ] as an exception .",
    "the difference in section  [ sec4 ] is that the lower bound for the compatibility and cone invertibility factors and the restricted eigenvalue is improved to allow time - dependent covariates .    during the revision process of our paper",
    ", we became aware of a number of papers on hazard regression with censored data .",
    "@xcite took an approach of @xcite to derive prediction and @xmath3 error bounds for the lasso in the cox proportional hazards regression under a quite different set of conditions from us .",
    "for example , they required an @xmath3 bound on the regression coefficients to guarantee a uniformly bounded ratio between hazard functions under consideration .",
    "@xcite considered the joint estimation of the baseline hazard function and regression coefficients in the cox model . as a result , lemler s ( @xcite ) error bounds for regression coefficients are of greater order than ours when the intrinsic dimension of the unknown baseline hazard function is of greater order than the number of nonzero regression coefficients .",
    "@xcite considered a quadratic loss function in place of a negative log - likelihood function in an additive hazards model .",
    "a nice feature of the additive hazards model is that the quadratic loss actually produces unbiased linear estimation equations so that the analysis of the lasso is similar to that of linear regression .",
    "the oracle inequalities in these three papers and ours can be all viewed as nonasymptotic .",
    "unlike our paper , none of these three papers consider time - dependent covariates or constant lower bounds of the restricted eigenvalue or related key factors for the analysis of the lasso .",
    "the rest of this paper is organized as follows . in section  [ sec2 ]",
    "we provide basic notation and model specifications . in section  [ sec3 ]",
    "we develop oracle inequalities for the lasso in the cox model . in section  [ sec4 ]",
    "we study the compatibility and cone invertibility factors and the corresponding restricted eigenvalue of the hessian of the log - partial likelihood in the cox model . in section  [ sec5 ]",
    "we make some additional remarks .",
    "all proofs are provided either right after the statement of the result or deferred to the .",
    "following @xcite , consider an @xmath1-dimensional counting process @xmath8 , @xmath9 , where @xmath10 counts the number of observed events for the @xmath11th individual in the time interval @xmath12 $ ] .",
    "the sample paths of @xmath13 are step functions , zero at @xmath14 , with jumps of size @xmath15 only .",
    "furthermore , no two components jump at the same time . for @xmath16 ,",
    "let @xmath17 be the @xmath18-filtration representing all the information available up to time @xmath19 .",
    "assume that for @xmath20 , @xmath21 has predictable compensator @xmath22 with @xmath23 where @xmath24 is a @xmath0-vector of true regression coefficients , @xmath25 is an unknown baseline cumulative hazard function and , for each @xmath11 , @xmath26 is a predictable at risk indicator process that can be constructed from data , and @xmath27 is a @xmath0-dimensional vector - valued predictable covariate process . in this",
    "setting the @xmath18-filtration can be the natural @xmath28 or a richer one .",
    "we are interested in the problem of variable selection in sparse , high - dimensional settings where  @xmath0 , the number of possible covariates , is large , but the number of important covariates is relatively small .",
    "define logarithm of the cox partial likelihood for survival experience at time @xmath19 , @xmath29 \\,d\\nbar(s),\\ ] ] where @xmath30 .",
    "the log - partial likelihood function is @xmath31 let @xmath32 .",
    "the maximum partial likelihood estimator is the value that minimizes @xmath33 .",
    "an approach to variable selection in sparse , high - dimensional settings for the cox model is to minimize an @xmath3-penalized negative log - partial likelihood criterion , @xmath34 [ @xcite ] , where @xmath35 is a penalty parameter .",
    "henceforth , we use notation @xmath36 for @xmath37 , @xmath38 and @xmath39 . for a given @xmath40 , the @xmath3-penalized maximum partial likelihood estimator , or the lasso estimator hereafter ,",
    "is defined as @xmath41      the lasso estimator can be characterized by the karush ",
    "tucker ( kkt ) conditions .",
    "since the log - partial likelihood belongs to an exponential family , @xmath33 must be convex in @xmath42 and so is @xmath43 .",
    "it follows that a vector @xmath44 is a solution to ( [ ph - lasso ] ) if and only if the following kkt conditions hold : @xmath45 where @xmath46 is the gradient of @xmath33 .",
    "the necessity and sufficiency of ( [ kkt ] ) can be proved by subdifferentiation of the convex penalized loss ( [ pen - lik ] ) .",
    "this does not require strict convexity .",
    "the kkt conditions indicate that the lasso in the cox regression model may be analyzed in a similar way to the lasso in linear regression .",
    "as can be seen in the subsequent developments , such analysis can be carried out by proving that @xmath47 is sufficiently small and the hessian of @xmath33 does not vanish for a sparse @xmath42 at the true @xmath48 .",
    "the ( local ) martingales for the counting process will play a crucial role to ensure that these requirements are satisfied .      since the @xmath49 are compensators , @xmath50 are ( local ) martingales with predictable variation / covariation processes @xmath51 for a vector @xmath52 , let @xmath53 , @xmath54 and @xmath55 .",
    "define @xmath56 where @xmath57 $ ] . by differentiation and rearrangement of terms ,",
    "it can be shown as in @xcite that the gradient of @xmath58 is @xmath59\\,dn_i(s ) , \\ ] ] and the hessian matrix of @xmath58 is @xmath60",
    "in this section , we derive oracle inequalities for the estimation error of lasso in the cox regression model .",
    "let @xmath61 be the vector of true regression coefficients , and define @xmath62 , @xmath63 and @xmath64 , where @xmath65 for a set @xmath66 denotes its cardinality .",
    "making use of the kkt conditions ( [ kkt ] ) , we first develop a basic inequality involving the symmetric bregman divergence and @xmath3 estimation error in the support @xmath67 of @xmath24 and its complement .",
    "the symmetric bregman divergence , defined as @xmath68 can be viewed as symmetric , partial kullback ",
    "leibler distance between the partial likelihood at @xmath69 and @xmath42 .",
    "thus , @xmath70 can be viewed as a measure of prediction performance .",
    "the basic inequality , given in lemma  [ lemh1 ] below , serves as a vehicle for establishing the desired oracle inequalities .",
    "[ lemh1 ] let @xmath69 be defined as in ( [ ph - lasso ] ) , @xmath71 and @xmath72 where @xmath73 and @xmath74 denote the subvectors of @xmath75 of components in @xmath67 and @xmath76 , respectively . in particular , for any @xmath77 , @xmath78 in the event @xmath79 .",
    "it follows from lemma  [ lemh1 ] that in the event @xmath80 , the estimation error @xmath81 belongs to the cone @xmath82 in linear regression , the invertibility of the gram matrix in the same cone , expressed in terms of restricted eigenvalues and related quantities , has been used to control the estimation error of the lasso . in what follows",
    ", we prove that a direct extension of the compatibility and cone invertibility factors can be used to control the estimation error of the lasso in the cox regression .    for the cone in ( [ cone ] ) and a given @xmath83 nonnegative - definite matrix @xmath84 , define @xmath85 .",
    "these quantities are closely related to the restricted eigenvalue [ @xcite , @xcite ] , @xmath86 is taken as  @xmath84 , and the oracle inequalities established in the papers cited in the above paragraph can be summarized as follows : in the event @xmath87 , @xmath88 and @xmath89\\\\[-8pt ] \\bigl|\\hbbeta-{\\bbeta}^o\\bigr|_q&\\le&\\frac{2(1 + 1/\\xi)^{-1}d_o^{1/q}\\lam } { f_q(\\xi,\\co;\\bx'\\bx / n)},\\qquad q\\ge1.\\nonumber\\end{aligned}\\ ] ]    in the cox regression model , we still take the hessian of the log - partial likelihood as @xmath84 , in fact the hessian at the true @xmath24 , so that ( [ re1 ] ) and ( [ fq ] ) become @xmath90 the reason for using these factors is that they yield somewhat sharper oracle inequalities than the restricted eigenvalue .",
    "it follows from @xmath91 that @xmath92 and @xmath93 .",
    "therefore , the first inequality of ( [ lm2 ] ) is subsumed by the second with @xmath94 . moreover , it is possible to have @xmath95 [ @xcite ] , and consequently , the @xmath6 error bound based on the cone invertibility factor may be of sharper order that the one based on the restricted eigenvalue .",
    "the following theorem extends ( [ lm1 ] ) and ( [ lm2 ] ) from the linear regression model to the proportional hazards regression model with @xmath96 let @xmath77 , @xmath97 , @xmath98 and @xmath99 be as in ( [ fq - cox ] ) .    [ th-1 ]",
    "let @xmath100 with a certain @xmath101",
    ". suppose condition ( [ cond - k ] ) holds and @xmath102 .",
    "then , in the event @xmath103 , @xmath104 and @xmath105 where @xmath106 is the smaller solution of @xmath107 .",
    "compared with ( [ lm1 ] ) and ( [ lm2 ] ) , the new inequalities ( [ th-1 - 1 ] ) and ( [ th-1 - 2 ] ) contain an extra factor @xmath108 .",
    "this is due to the nonlinearity in the cox regression score equation . aside from this factor ,",
    "the error bounds for the cox regression have the same form as those for linear regression , except for an improvement of a factor of @xmath109 for the @xmath3 oracle inequality .",
    "the theorem assumes condition ( [ cond - k ] ) , which asserts @xmath110 uniformly in @xmath111 .",
    "this condition is a consequence of the uniform boundedness of the individual covariates , and is reasonable in most practical situations ( e.g. , single - nucleotide polymorphism data ) . in the case where the covariates are normal variables with uniformly bounded variance",
    ", the condition holds with @xmath112 of @xmath113 order .    from an analytical perspective",
    ", an important feature of ( [ th-1 - 1 ] ) and ( [ th-1 - 2 ] ) is that the constant factors ( [ re1 ] ) and ( [ fq ] ) are both defined with the true @xmath114 in ( [ fq - cox ] ) .",
    "no condition is imposed on the gradient and hessian of the log - partial likelihood for @xmath115 . in other words",
    ", the key condition @xmath116 , expressed in terms of @xmath117 and the compatibility factor @xmath118 at the true @xmath24 , is sufficient to guarantee the error bounds in theorem  [ th-1 ] .",
    "thus , our results are much simpler to state and conditions easier to verify than existing ones requiring regularity conditions in a neighborhood of @xmath24 in the cox regression model .",
    "this feature of theorem  [ th-1 ] plays a crucial role in our derivation of lower bounds for @xmath118 and @xmath99 for time - dependent covariates in section  [ sec4 ] .",
    "we note that the local martingale structure is valid only at the true @xmath61 .",
    "to prove theorem  [ th-1 ] , we develop a sharpened version of an inequality of @xcite .",
    "this inequality , given in lemma  [ lemb ] below , explicitly controls the symmetric bregman - divergence and hessian of the log - partial likelihood in a neighborhood of @xmath42 .",
    "based on this relationship , theorem  [ th-1 ] is proved using the definition of the quantities in ( [ fq - cox ] ) and the membership of the error @xmath119 in the cone @xmath120 ( [ cone ] ) . for two symmetric matrices @xmath121 and @xmath122 , @xmath123 means @xmath124 is nonnegative - definite .",
    "[ lemb ] let @xmath58 and its hessian @xmath125 be as in ( [ pen - lik ] ) and ( [ hessian1 ] ) .",
    "then @xmath126 \\le e^{\\eta_{\\mathbf{b } } } \\mathbf{b}'\\ddell({\\bbeta})\\mathbf{b},\\ ] ] where @xmath127 .",
    "moreover , @xmath128    under the conditions of theorem  [ th-1 ] , the factors @xmath129 and @xmath130 in the inequalities in lemma  [ lemb ] are bounded by positive constants .",
    "these factors lead to the factor @xmath131 for @xmath106 ( and thus @xmath132 ) in the upper bounds in ( [ th-1 - 1 ] ) and ( [ th-1 - 2 ] ) .    since the oracle inequalities in theorem  [ th-1 ] are guaranteed to hold only within the event @xmath133 , a probabilistic upper bound is needed for @xmath134 .",
    "lemma  [ lemc ] below provides such a probability bound .",
    "similar inequalities can be found in @xcite .",
    "[ lemc ] let @xmath135 with @xmath136$]-valued predictable processes @xmath137 .",
    "then , for all , @xmath138    suppose that @xmath139 , where @xmath140 are the components of @xmath141 .",
    "let @xmath142 be the gradient in ( [ gradient1 ] ) .",
    "then , for all @xmath143 , @xmath144 in particular , if @xmath145 , then @xmath146 .    the following theorem states an upper bound of the estimation error , which follows directly from theorem  [ th-1 ] and lemma  [ lemc ] .    [ thma ]",
    "suppose ( [ cond - k ] ) holds and @xmath147 for all @xmath148 and @xmath9 .",
    "let @xmath77 and @xmath149 with a small @xmath150 ( e.g. , @xmath151 ) .",
    "let @xmath152 satisfying @xmath153 .",
    "let @xmath106 be the smaller solution of @xmath107 .",
    "then , for any @xmath154 , @xmath155 all hold with at least probability @xmath156 .",
    "it is noteworthy that this theorem gives an upper bound of the estimation error for all the @xmath157 norms with @xmath158 . from this theorem , for the @xmath157 error @xmath159 with @xmath158 to be small with high probability , we need to ensure that @xmath160 as @xmath161 .",
    "this requires @xmath162 .",
    "if @xmath163 is bounded , then @xmath0 can be as large as @xmath164 .",
    "@xcite considered estimation as well as variable selection and oracle properties for general concave penalties , including the lasso .",
    "their broader scope seems to have led to more elaborate statements and some key conditions that are more difficult to verify than those of theorems  [ th-1 ] and  [ thma ] , for example , their condition 2(i ) on a uniformly small spectrum bound between @xmath165 and its population version for a sparse @xmath166 in a neighborhood of @xmath61 .",
    "proof of theorem  [ thma ] let @xmath167 and @xmath168 in lemma  [ lemc ] .",
    "the probability of the event @xmath169 is at most @xmath170 .",
    "the desired result follows directly from theorem  [ th-1 ] .",
    "in section  [ sec3 ] , the oracle inequalities in theorems  [ th-1 ] and  [ thma ] are expressed in terms of the compatibility and weak cone invertibility factors . however , as mentioned in the , these quantities are still random variables .",
    "this section provides sufficient conditions under which they can be treated as constants .",
    "since these factors appear in the denominator of error bounds , it suffices to bound them from below .",
    "we also derive a lower bound for the restricted eigenvalue to facilitate further analysis of the cox model in high - dimension .",
    "we will prove that these quantities are bounded from below by the population version of their certain truncated versions .    compared with linear regression ,",
    "our problem poses two additional difficulties in the cox model : ( a ) time dependence of covariates , and ( b ) stochastic integration of the hessian over random risk sets .",
    "fortunately , the compatibility and weak cone invertibility factors in theorems  [ th-1 ] and  [ thma ] involve only the hessian of the log - partial likelihood at the true @xmath24 , so that a martingale argument can be used .",
    "to simplify the statement of our results , we use @xmath171 to denote any of the following quantities : @xmath172 where @xmath173 , @xmath174 , and @xmath175 are as in ( [ re1 ] ) , ( [ fq ] ) and ( [ re2 ] ) , respectively .",
    "if we make a claim about @xmath171 , we mean that the claim holds for any quantity it represents .",
    "let @xmath176 denote the smallest eigenvalue .",
    "the following lemma provides some key properties of @xmath171 used in the derivation of its lower bounds .",
    "[ lm - re ] let @xmath173 , @xmath174 , @xmath175 and @xmath171 be as in ( [ phi ] ) .",
    "let @xmath177 be the elements of @xmath84 and @xmath178 be another nonnegative - definite matrix with elements @xmath179 .    for @xmath180 , @xmath181    @xmath182 .",
    "if @xmath183 , then @xmath184 .    by the hlder inequality , @xmath185 in the cone and @xmath91 , we have @xmath186 this and @xmath91 yields part ( i ) by the definition of the quantities involved . part ( ii ) follows from @xmath187 and @xmath188 .",
    "part ( iii ) follows immediately from the definition of the quantities in ( [ phi ] ) .",
    "it follows from lemma  [ lm - re](ii ) and ( iii ) that quantities of type @xmath171 in ( [ phi ] ) can be bounded from below in two ways .",
    "the first is to bound the matrix @xmath84 from below and the second is to approximate @xmath84 under the supreme norm for its elements . in the @xmath7 setting ,",
    "our problem is essentially the rank deficiency of @xmath84 to begin with , so that its lower bound is still rank deficient .",
    "however , a lower bound of the random matrix @xmath189 , for example , a certain truncated version of it , may have a smaller variability to allow an approximation by its population version .",
    "this is our basic idea .",
    "in fact , our analysis takes advantage of this argument twice to remove different sources of randomness .    according to our plan described in the previous paragraph",
    ", we first choose a suitable truncation of @xmath190 as a lower bound of the matrix .",
    "this is done by truncating the maximum event time under consideration .",
    "it follows from ( [ hessian1 ] ) that for @xmath191 , @xmath192 this allows us to remove the randomness from the counting process by replacing the average counting measure @xmath193 by its compensator@xmath194 , where @xmath25 is the baseline cumulative hazard function .",
    "this approximation of @xmath195 can be written as @xmath196    to completely remove the randomness with @xmath197 , we apply the method again by truncating the weights @xmath198 with @xmath199 . for @xmath200 , define @xmath201 where @xmath202 with @xmath203 we will prove that the matrix ( [ hessian-3 ] ) is a lower bound of ( [ hessian-2 ] ) .",
    "suppose @xmath204 are i.i.d .",
    "stochastic processes from @xmath205 .",
    "the population version of ( [ hessian-3 ] ) is then @xmath206 where @xmath207 with @xmath208 } { e[y(t)\\min\\{m,\\exp(\\bz'{\\bbeta}^o)\\}]}. \\ ] ]    the analysis outlined above leads to the following main result of this section . for @xmath209 and @xmath210 with @xmath211 ,",
    "let @xmath171 represent all quantities of interest given in ( [ phi ] ) , @xmath98 and @xmath99 be the compatibility and weak cone invertibility factors in ( [ fq - cox ] ) with the hessian @xmath212 in ( [ hessian1 ] ) at the true @xmath42 , and @xmath175 be as in ( [ re2 ] ) .",
    "let @xmath213 .",
    "[ th - re ] @xmath214suppose @xmath215 are i.i.d .",
    "processes from @xmath216 with @xmath217 .",
    "let @xmath218 be positive constants and @xmath219 . then , @xmath220 with at least probability @xmath221 , where @xmath222 , @xmath223 and @xmath224 is the solution of @xmath225 .",
    "consequently , for @xmath180 , @xmath226 with at least probability @xmath221 , where @xmath227 with the matrix in  ( [ hessian-4 ] ) .",
    "theorem  [ th - re ] implies that the compatibility and cone invertibility factors and the restricted eigenvalue can be all treated as constants in high - dimensional cox model with time - dependent covariates .",
    "we note that @xmath228 is of smaller order than @xmath229 so that the lower bounds in ( [ th - re-1 ] ) and ( [ th - re-2 ] ) depend on the choice of @xmath230 and @xmath231 marginally through @xmath232 and @xmath233 .",
    "if @xmath234 is sufficiently small as assumed in theorem  [ thma ] , the right - hand side of ( [ th - re-2 ] ) can be treated as @xmath235 .",
    "it is reasonable to treat @xmath233 as a constant since it is the smallest eigenvalue of a population integrated covariance matrix in ( [ hessian-4 ] ) .    in the proof of theorem  [ th - re ] , the martingale exponential inequality in lemma  [ lemc ]",
    "is used to bound the difference between ( [ truncation ] ) and ( [ hessian-2 ] ) .",
    "the following bernstein inequality for @xmath236-statistics is used to bound the difference between ( [ hessian-3 ] ) and ( [ hessian-4 ] ) .",
    "this inequality can be viewed as an extension of the @xcite inequality for sums of bounded independent variables and nondegenerate @xmath237-statistics .",
    "[ lm - v - stat ] let @xmath238 be a sequence of independent stochastic processes and @xmath239 be functions of @xmath238 and @xmath240 with @xmath241 .",
    "suppose @xmath239 are degenerate in the sense of @xmath242 = e[f_{i , j}|x_j]=0 $ ] for all @xmath243 .",
    "let @xmath244 .",
    "then , @xmath245 where @xmath246 .",
    "our discussion focuses on the quantities in ( [ phi ] ) for the hessian matrix @xmath189 evaluated at the true vector of coefficients .",
    "still , through lemma  [ lemb ] , theorem  [ th - re ] also provides lower bounds for these quantities at any @xmath42 not far from the true @xmath24 in terms of the @xmath3 distance .",
    "we formally state this result in the following corollary .",
    "[ cor - re ] let @xmath171 represent any quantities in ( [ phi ] ) .",
    "then , @xmath247 where @xmath248 consequently , when @xmath249 , @xmath250\\end{aligned}\\ ] ] under the conditions of theorem  [ th - re ] .",
    "it is worthwhile to point out that unlike typical `` small ball '' analysis based on taylor expansion , corollary  [ cor - re ] provides nonasymptotic control of the quantities in an @xmath3 ball of constant size .",
    "since @xmath251 appears in the numerator of the quantities represented by @xmath171 , corollary  [ cor - re ] follows immediately from theorem  [ th - re ] and ( [ lemb2 ] ) .",
    "it implies that the hessian has sufficient invertibility properties in the analysis of the lasso when the estimator is not far from the true @xmath24 in @xmath3 distance . on the other hand ,",
    "if the hessian has sufficient invertibility properties in a ball of fixed size , nonasymptotic error bounds for the lasso estimator can be established .",
    "this `` chicken and egg '' problem is directly solved in the proof of theorem  [ th-1 ] .",
    "this paper deals with the cox proportional hazards regression model when the number of time - dependent covariates @xmath0 is potentially much larger than the sample size @xmath1 .",
    "the @xmath3 penalty is used to regularize the log - partial likelihood function .",
    "error bounds parallel to those of the lasso in linear regression are established . in establishing these bounds ,",
    "we extend the notion of the restricted eigenvalue and compatibility and cone invertibility factors to the cox model .",
    "we show that these quantities indeed provide useful error bounds .",
    "an important issue is the choice of the penalty level @xmath40 .",
    "theorem  [ thma ] requires a @xmath40 slightly larger than @xmath252 , where @xmath253 is a uniform upper bound for the range of individual real covariates .",
    "this indicates that the lasso is tuning insensitive since the theoretical choice does not depend on the unknowns . in practice ,",
    "cross validation can be used to fine tune the penalty level @xmath40 .",
    "theoretical investigation of the performance of the lasso with cross - validated @xmath40 , an interesting and challenging problem in and of itself even in the simpler linear regression model , is beyond the scope of this paper.=-1    general concave penalized estimators in the cox regression model have been considered in @xcite where oracle inequalities and properties of certain local solutions are considered .",
    "@xcite has provided a unified treatment of global and local solutions for concave penalized least squares estimators in linear regression .",
    "since this unified treatment relies on an oracle inequality for the global solution based on the cone invertibility factor , the results in this paper point to a possible extension of such a unified treatment of global and local solutions of general concave regularized methods in the cox regression model .",
    "here we prove lemmas  [ lemh1 ] ,  [ lemb ] ,  [ lemc ] and  [ lm - v - stat ] and theorems  [ th-1 ] and  [ th - re ] .",
    "proof of lemma  [ lemh1 ] since @xmath58 is a convex function , @xmath254 , so that the first inequality holds . since @xmath255 for @xmath256 , ( [ kkt ] ) gives @xmath257 & \\le&\\sum_{j\\in\\co^c}\\hbeta_j \\bigl(-\\lam \\sgn(\\hbeta_j )",
    "\\bigr ) + \\sum_{j\\in\\co}| { \\ttheta}_j|\\lam+ |\\tilde{\\btheta}|_1 z^*\\hspace*{5pt } \\\\[-2pt ] & = & \\sum_{j\\in\\co^c } - \\lam|\\tilde{\\theta}_j| + |\\tilde{\\btheta}_{\\co}|_1\\lambda+ z^ * |\\tilde { \\btheta}_{\\co}|_1 + z^ * |\\tilde{\\btheta}_{\\co^c}|_1\\hspace*{5pt } \\\\[-2pt ] & = & \\bigl(z^*-\\lambda\\bigr)|\\tilde{\\btheta}_{\\co^c}|_1 + \\bigl(\\lambda+z^*\\bigr)|\\tilde{\\btheta}_{\\co}|_1.\\hspace*{5pt}\\end{aligned}\\ ] ] thus the second inequality in ( [ lemh1e1 ] ) holds .",
    "note that the inequality in the third line above requires @xmath258 only in the set @xmath259 , since @xmath260 when @xmath261 and @xmath262 .",
    "proof of lemma  [ lemb ] we use similar notation as in @xcite .",
    "let @xmath263 , @xmath264 $ ] and @xmath265 . clearly , @xmath266 . by the definition of @xmath267 , @xmath268 & & \\qquad= \\sum_i \\mathbf{b } ' \\mathbf{z}_i(s )",
    "w_i e^{\\mathbf { b}'\\mathbf{z}_i(s ) } \\big/\\sum _ i w_i e^{\\mathbf{b}'\\mathbf{z}_i(s ) } - \\sum _",
    "i \\mathbf{b}'\\mathbf{z}_i(s ) w_i \\big/\\sum_i w_i \\\\[-2pt ] & & \\qquad= \\sum_i a_i w_i e^{a_i } \\big/\\sum_i w_i e^{a_i } - \\sum_i a_i w_i \\big/\\sum_i w_i \\\\[-2pt ] & & \\qquad= \\sum_{i , j } w_{i } w_{j } a_i\\bigl(e^ { a_i } - e^ { a_j}\\bigr ) \\big/\\sum _ { i , j } w_{i } w_{j } e^ { a_i } \\\\[-2pt ] & & \\qquad= \\sum_{i , j } w_{i } w_{j } ( a_i - a_j ) \\bigl(e^ { a_i - c } - e^ { a_j - c } \\bigr ) \\big/\\sum_{i , j } 2w_{i } w_{j } e^ { a_i - c } \\\\[-2pt ] & & \\qquad\\ge\\exp\\bigl({-2\\max_i}|a_i - c| \\bigr)\\sum _ { i , j } w_{i } w_{j } ( a_i - a_j)^2 \\big/\\sum _ { i , j } 2w_{i } w_{j } \\\\[-2pt ] & & \\qquad\\ge\\exp(-\\eta_{\\mathbf{b } } ) \\sum_{i } w_{i } a_i^2 \\big/\\sum _ { i } w_{i},\\end{aligned}\\ ] ] where the first inequality comes from @xmath269 . thus , since @xmath270 , ( [ hessian1 ] ) and ( [ gradient1 ] ) give @xmath271 this implies the lower bound in ( [ lemb1 ] ) .",
    "similarly , the lower bound in ( [ lemb2 ] ) follows from @xmath272 and @xmath273 the proof of the upper bounds in ( [ lemb1 ] ) and ( [ lemb2 ] ) , nearly identical to the proof of the lower bounds , is omitted .",
    "proof of lemma  [ lemc ] applying the union bound and changing the scale of the covariates if necessary , we assume without loss of generality that @xmath274 . in this case @xmath275 where @xmath276 , @xmath277 , are predictable and satisfy .",
    "thus , ( [ lm - c-1 ] ) follows from ( [ lm - c-0 ] ) .",
    "let @xmath278 be the time of the @xmath279th jump of the process @xmath280 , @xmath281 and @xmath282 .",
    "then , @xmath278 are stopping times . for @xmath283 , define @xmath284 since @xmath285 are martingales and @xmath137 are predictable , @xmath286 is a martingale with the difference @xmath287 .",
    "let @xmath288 be the greatest integer lower bound of @xmath289 .",
    "by the martingale version of the @xcite inequality [ @xcite ] , @xmath290 by ( [ martx ] ) , @xmath291 if and only if @xmath292 .",
    "thus , the left - hand side of ( [ lm - c-1 ] ) is no greater than @xmath293 .",
    "proof of lemma  [ lm - v - stat ] for integers @xmath294 , let @xmath295 be the number of appearances of @xmath279 in the sequence @xmath296 . since @xmath239 are degenerate , @xmath297 this is due to the fact that all terms with exactly one appearance of an index @xmath279 have zero expectation and all other terms are bounded by 1 . let @xmath298 be the expectation under which @xmath299 are i.i.d .",
    "uniform variables in @xmath300 and @xmath301 . since @xmath302 is multinomial@xmath303 ,",
    "the above inequality can be written as @xmath304 let @xmath305 and @xmath306 .",
    "it follows from the above moment inequality that @xmath307 since @xmath308 , we find@xmath309 .",
    "consequently , the monotonicity of @xmath310 for @xmath311 and the lower bound @xmath312 allow us to apply the markov inequality as follows : @xmath313 the conclusion follows from @xmath314 .",
    "proof of theorem  [ th-1 ] let @xmath315 and @xmath316 .",
    "it follows from the convexity of @xmath317 , as a function of @xmath318 , and lemma  [ lemh1 ] that , in the event @xmath319 , @xmath320 for @xmath321 $ ] and @xmath322 .",
    "consider all nonnegative @xmath318 satisfying ( [ pf - th-1 - 1 ] ) .",
    "we need to establish a lower bound for @xmath323 since @xmath324 , lemma  [ lemb ] yields @xmath325\\\\[-8pt ] & \\ge & x^2\\exp(-kx ) \\mathbf{b}'\\ddell\\bigl ( { \\bbeta}^o\\bigr)\\mathbf{b}.\\nonumber\\end{aligned}\\ ] ] this , combined with ( [ pf - th-1 - 1 ] ) and the definition of @xmath326 , gives @xmath327 in other words , any @xmath318 satisfying ( [ pf - th-1 - 1 ] ) must satisfy @xmath328 since @xmath329 is an increasing function of @xmath318 due to the convexity of  @xmath330 , the set of all nonnegative @xmath318 satisfying ( [ pf - th-1 - 1 ] ) is a closed interval @xmath331 $ ] for some @xmath332 .",
    "thus , ( [ pf - th-1 - 2 ] ) implies @xmath333 , the smaller solution of @xmath107 .",
    "this yields @xmath334 in ( [ th-1 - 1 ] ) . the first part of ( [ th-1 - 1 ] ) follows from ( [ re1 ] ) , ( [ fq - cox ] ) , ( [ lemb1 ] ) and ( [ lemh1e1 ] ) , due to @xmath335    finally , it follows from the definition of @xmath99 , ( [ pf - th-1 - 2a ] ) and ( [ pf - th-1 - 1 ] ) that , for @xmath336 , @xmath337 & \\le & \\frac{2\\xi\\lam d_o^{1/q}}{(\\xi+1)f_q(\\xi,\\co)|\\bb|_q}.\\end{aligned}\\ ] ] this gives the second inequality in ( [ th-1 - 2 ] ) due to @xmath338",
    ".    proof of theorem  [ th - re ] let @xmath339 & = & \\sum_{i=1}^nw_{ni } \\bigl(t,\\bbeta^o\\bigr)\\bigl\\{z_{i , j}(s)-{\\bar z}_{n , j}(s)\\bigr\\ } \\bigl\\ { z_{i , k}(s)-{\\bar z}_{n , k}(s)\\bigr\\}/k^2.\\end{aligned}\\ ] ] it follows from lemma  [ lemc](i ) with @xmath340 and @xmath167 that @xmath341 & & \\le2e^{-nx^2/2}.\\end{aligned}\\ ] ] thus , @xmath342 by the union bound and the respective definitions of @xmath343 and @xmath197 in ( [ truncation ] ) and ( [ hessian-2 ] ) .",
    "consequently , by ( [ truncation ] ) and lemma  [ lm - re](iii ) and ( ii ) @xmath344\\\\[-8pt ] & & \\qquad\\ge1-\\eps.\\nonumber\\end{aligned}\\ ] ] let us take the sample mean of @xmath11-indexed quantities with weights@xmath345 , so that @xmath346 is the sample mean of @xmath347 . since @xmath348 , @xmath349 ^ 2 y_i(t)\\min\\bigl\\{m,\\exp\\bigl ( \\bz'_i(t){\\bbeta}^o\\bigr)\\bigr\\ } \\\\[-2pt ] & \\ge & \\bu'\\hbg_n(t;m)\\bu.\\end{aligned}\\ ] ] thus , by the definition of @xmath350 in ( [ hessian-3 ] ) and lemma  [ lm - re](iii ) , @xmath351 in addition , the relationship between the sample second moment and variance gives @xmath352 by the definition of @xmath353 and @xmath354 , so that ( [ hessian-3 ] ) can be written as @xmath355\\\\[-8pt ] & & { } - \\int_0^{t^ * } \\bigl\\{\\bar{\\mathbf{z}}_n(t;m ) - \\bmu(t;m ) \\bigr\\}^{\\otimes 2}\\,d \\lambda_0(s).\\nonumber\\end{aligned}\\ ] ]    we first bound the second term on the right - hand side of ( [ pf - th - re-3 ] ) .",
    "define @xmath356 since @xmath357 is nonincreasing in @xmath19 , @xmath358 since @xmath359 is the average of i.i.d .",
    "variables uniformly bounded by @xmath231 and @xmath360 , the @xcite inequality gives @xmath361 since @xmath362 is an average of i.i.d .",
    "mean zero vectors , @xmath363 is a degenerate @xmath236-statistic for each @xmath364 .",
    "moreover , since the summands of these @xmath236-statistics are all bounded by @xmath365 , lemma  [ lm - v - stat ] yields @xmath366 thus , by ( [ pf - th - re-3 ] ) , ( [ pf - th - re-4 ] ) , the above two probability bounds and lemma  [ lm - re](ii ) , @xmath367 with at least probability @xmath368 .    finally , by ( [ hessian-4 ] ) , @xmath369 is an average of i.i.d .",
    "matrices with mean @xmath370 and the summands of @xmath371 are uniformly bounded by @xmath365 , so that the @xcite inequality gives @xmath372 by ( [ pf - th - re-1 ] ) , ( [ pf - th - re-2 ] ) , ( [ pf - th - re-5 ] ) , the above inequality with @xmath373 and lemma  [ lm - re](ii ) , @xmath374 with at least probability @xmath375 . since @xmath376 by lemma  [ lm - re](i ) and the definition in ( [ re2 ] ) , the conclusion follows .",
    "we are grateful to two anonymous reviewers , the associate editor and editor for their helpful comments which led to considerable improvements in the paper .",
    "we also wish to thank a reviewer for bringing to our attention the work of @xcite , @xcite and @xcite during the revision process of this paper ."
  ],
  "abstract_text": [
    "<S> we study the absolute penalized maximum partial likelihood estimator in sparse , high - dimensional cox proportional hazards regression models where the number of time - dependent covariates can be larger than the sample size . </S>",
    "<S> we establish oracle inequalities based on natural extensions of the compatibility and cone invertibility factors of the hessian matrix at the true regression coefficients . </S>",
    "<S> similar results based on an extension of the restricted eigenvalue can be also proved by our method . however , the presented oracle inequalities are sharper since the compatibility and cone invertibility factors are always greater than the corresponding restricted eigenvalue . in the cox regression model , </S>",
    "<S> the hessian matrix is based on time - dependent covariates in censored risk sets , so that the compatibility and cone invertibility factors , and the restricted eigenvalue as well , are random variables even when they are evaluated for the hessian at the true regression coefficients . under mild conditions , we prove that these quantities are bounded from below by positive constants for time - dependent covariates , including cases where the number of covariates is of greater order than the sample size . </S>",
    "<S> consequently , the compatibility and cone invertibility factors can be treated as positive constants in our oracle inequalities .    ,    ,    ,     + </S>"
  ]
}