{
  "article_text": [
    "counting the number of solutions for random constraint satisfaction problems is a very important and nontrivial problem which belongs to # p - complete class in computational complexity  @xcite and is much harder than determining whether a random formula has any solutions . in practice , we can only sample a very small part of a huge solution space which contains an exponential number of solutions .",
    "however , can we predict the number of solutions in the whole solution space only based on a finite number of sampled solutions ?",
    "this issue has generated broad interests across a variety of different disciplines such as computer science , probabilistic reasoning , statistical physics and computational biology  @xcite .",
    "an efficient sample - based counting strategy was proposed in ref .",
    "this strategy successively sets the most balanced variable until an exact counter is feasible on the reduced formula , and provides a lower bound on the true count .",
    "alternatively , we address the solution counting problem within the framework of inverse ising problem in examples of diluted modelsrandom @xmath2-sat problems and fully - connected modelbinary perceptron , and show that our method yields an estimate whose value could be very close to the true count when the constraint density is small .",
    "the constraint density is defined as the ratio of the number of constraints to that of variables in the system .",
    "the inverse ising problem  @xcite has recently attracted much attention not only in the development of fast mean field inverse algorithms  @xcite but also in modeling vast amounts of biological data  @xcite .",
    "the pairwise ising model is able to capture most of the correlation structure of the real neuronal network activity and is much more informative than the independent model where each neuron is assumed to fire independently  @xcite .",
    "the observed collective behavior of a large neuronal network results from interactions of many individual neurons .",
    "the joint activity patterns for a retina under naturalistic stimuli were reported to convey information about the visual stimuli  @xcite .",
    "estimating the information stored by the real neuronal network directly from data remains an open and important issue .",
    "we show in this work the information can be estimated reliably and sizes of metastable states for the neuronal network can also be predicted .",
    "the paper is organized as follows .",
    "the inverse ising problem is introduced in sec .",
    "[ invising ] , together with a brief description of the susceptibility propagation algorithm used to infer the disordered ising model . in sec .",
    "[ method ] , we present the belief propagation to estimate the entropy from the data and apply this method to predict the entropies of four different examples only from a limited number of samplings . finally , the conclusion suggests some implications of our study as well as potential applications of the presented methodology .",
    "for a system of @xmath3 variables , one can collect @xmath4 configurations or solutions @xmath5 either from real biological experiments ( e.g. , spike trains in multi - electrode array recordings  @xcite ) or from random walks in the solution space of a model .",
    "we assume @xmath6 takes ising - type value @xmath7 .",
    "the task of the inverse ising problem is to find couplings @xmath8 and fields @xmath9 to construct a minimal model @xmath10\\ ] ] such that its magnetizations and pairwise correlations are compatible with those measured , i.e. , @xmath11 .",
    "@xmath12 is the partition function and the inverse temperature @xmath13 as it can be absorbed in the strength of couplings and fields .",
    "hereafter , we define the measured magnetization and connected correlation as @xmath14 and @xmath15 respectively where @xmath16 denotes the average over the sampled configurations or solutions .",
    "we use susceptibility propagation ( susprop ) to infer the couplings and fields .",
    "susprop passes messages along the oriented edges of the network by iterative updating . to run susprop ,",
    "two kinds of messages are needed .",
    "one is the cavity magnetization of variable @xmath17 in the absence of variable @xmath18 denoted as @xmath19 ; the other is the cavity susceptibility @xmath20 that is the response of cavity field of variable @xmath17 without variable @xmath18 to a local perturbation of external field of variable @xmath21  @xcite .",
    "the update rule can be derived using belief propagation eq .",
    "( [ bpising ] ) and fluctuation - response relation  @xcite and reads as follows  @xcite :    [ susp ] @xmath22    where @xmath23 denotes neighbors of variable @xmath17 except @xmath18 , @xmath24 is the kronecker delta function and @xmath25 $ ] is introduced as a damping factor and should be appropriately chosen to prevent the absolute updated @xmath26 from being larger than @xmath27 . in practice , all couplings are initially set to be zero and for every directed edge of the network , the message @xmath19 is randomly initialized in the interval @xmath28 $ ] and @xmath29 if @xmath30 and @xmath31 otherwise .",
    "the susprop rule eq .",
    "( [ susp ] ) is then iterated until either the inferred couplings converge within a predefined precision @xmath32 or the preset maximal number of iterations @xmath33 is exceeded .",
    "after the set of couplings is obtained , the fields are inferred via @xmath34 $ ] .    to ensure a reliable estimate of the parameters",
    ", we define a convergence fraction @xmath35 as the ratio of the number of converged couplings to the total number of edges in the network . in the non - convergent case , we take the inferred parameters corresponding to @xmath36 where @xmath37 is the convergence fraction of @xmath38-th iteration . @xmath39 if the update rule converges . for an inverse problem ,",
    "@xmath40 serve as inputs to the update rule , and they are computed from @xmath4 sampled solutions or configurations .",
    "we use stochastic local search algorithms to sample the solution space of random @xmath2-sat ( @xmath41 here ) formulas and that of the binary perceptron .",
    "for the retinal network , the configurations were obtained from the spike trains in the multi - electrode recording experiments ( data courtesy of gasper tkacik , refs .",
    "we derive the entropy of the constructed ising model eq .",
    "( [ ising ] ) under bethe approximation ( also called cavity method  @xcite ) assuming sufficiently weak interactions among variables .",
    "we compute the entropy through site contributions @xmath42 and edge contributions @xmath43 as @xmath44 :    [ entropy ] @xmath45      \\cdot\\prod_{j\\in\\partial i\\backslash l}\\cosh j_{ij}(1+\\tanh j_{ij}m_{j\\rightarrow i})\\\\      & + e^{-h_{i}}\\sum_{l\\in\\partial i}\\bigl[j_{li}\\sinh j_{li}(1-\\tanh j_{li}m_{l\\rightarrow i})-j_{li}\\cosh j_{li}(1-\\tanh^{2}j_{li})m_{l\\rightarrow i}\\bigr ]      \\cdot\\prod_{j\\in\\partial i\\backslash l}\\cosh j_{ij}(1-\\tanh j_{ij}m_{j\\rightarrow i})\\biggr ]      \\end{split}\\\\      \\delta s_{\\left < ij\\right>}&=\\log z_{ij}-j_{ij}\\frac{\\tanh j_{ij}+m_{i\\rightarrow j}m_{j\\rightarrow i}}{1+\\tanh j_{ij}m_{i\\rightarrow j}m_{j\\rightarrow i}}\\end{aligned}\\ ] ]    where @xmath46 denotes neighbors of variable @xmath17 except @xmath47 . @xmath48 and @xmath49",
    ". the cavity magnetization @xmath19 obeys simple recursive equations :    [ bpising ] @xmath50    we first randomly initialize @xmath51 $ ] for every directed edge of the reconstructed network , then iterate eq .",
    "( [ bpising ] ) until all messages converge within the precision @xmath52 or the maximal number of iterations @xmath53 is reached . from the fixed point ,",
    "the entropy can be computed via eq .",
    "( [ entropy ] ) .",
    "the case where some variable , say @xmath17 is positively frozen , i.e. , corresponding measured magnetization @xmath54 , can also be handled . in this case ,",
    "@xmath55 , and @xmath42 is reduced to be @xmath56 where @xmath57 and @xmath58\\cdot\\prod_{j\\in\\partial i\\backslash l}\\cosh j_{ij}(1+\\tanh j_{ij}m_{j\\rightarrow i})$ ] .",
    "the edge contribution remains unchanged .",
    "the negatively frozen case is similarly treated . in numerical simulations ,",
    "we adopt @xmath59 , @xmath60 , @xmath61 .",
    "@xmath32 as well as @xmath62 depends on the following specific applications .",
    "we remark here that eq .",
    "( [ entropy ] ) is used specifically for the solution counting problem where we now have known the magnetizations and correlations and additionally some frozen cases ( some @xmath63 or @xmath64 ) should be treated . on the other hand ,",
    "the coupling or field distributions depend on the collected data and eq .",
    "( [ entropy ] ) is derived only under the weakly - coupled approximation but the fully connected topology is reserved .",
    "the first point is , the entropy we try to estimate is not only for two - body interaction system ( e.g. , random @xmath0-sat ) but also for three - body interaction system and densely - interacted system ( e.g. , the binary perceptron where each constraint involves all variables of the system ) .",
    "the second point is , the sampled solutions come from the zero energy ground state and the sampling process is always confined in a single cluster ( solutions in it are connected with each other by single variable flips )",
    ".    all underlying parameters of pairwise ising model are predicted directly from the observed data and the entropy of the original model is estimated based on the constructed ising model .",
    "we emphasize here that two layers of approximations are made .",
    "the first one is the disordered ising model eq .",
    "( [ ising ] ) is used to approximate the original model . when estimating the entropy from the data , we actually do not know the original model .",
    "the second layer is we use mean - field methods , specifically the message passing algorithms to infer the underlying parameters of the pairwise ising model . since the computational complexity of susprop is @xmath65 for the fully - connected network , we focus on small size networks with @xmath3 of order @xmath66 .",
    "when the constraint density is small , the efficiency of our methodology is supported by two concrete examples : random @xmath2-sat problem and the binary perceptron .",
    "for these two examples , we use @xmath67 to represent the entropy density computed by belief propagation with the knowledge of the original model ( for details , see ref .",
    "@xcite for random @xmath2-sat problem and ref .",
    "@xcite for binary perceptron ) . to show the efficiency of the pairwise ising model",
    ", we also compute the independent entropy @xmath68 $ ] assuming @xmath69 . for retinal network",
    ", we could neither know the true model underlying the network nor get the true value for the entropy ( when the network is large ) .",
    "therefore we just compare the result obtained by our current fast belief propagation with that obtained by time - consuming monte carlo method and show that the belief propagation not only reproduces the entropy value evaluated by monte carlo method but also yields rich information about the metastable states which are relevant for neuronal population coding  @xcite .",
    "in this case , we denote @xmath70 as the entropy density estimated by belief propagation and @xmath71 estimated by monte carlo method .",
    "note that both belief propagation and monte carlo method under the reconstructed ising model yield approximate value for the true entropy since we consider only up to second - order correlations in the observed data while the system may develop higher - order correlations in its solution space or energy landscape .",
    "the different natures of these examples imply wide applications of our methodology to evaluate the entropy of an unknown model with only a limited number of samplings .    .",
    "( a ) entropy density difference versus @xmath72 .",
    "the data points connected by dashed line are the differences between @xmath73 and @xmath67 , while those connected by solid line are the differences @xmath74 . the number of variables @xmath75 for random @xmath2-sat ( r@xmath0-sat or r@xmath1-sat ) problem and @xmath76 for binary perceptron ( bperc ) .",
    "@xmath67 is computed with the knowledge of the original model using belief propagation  @xcite .",
    "each point represents the average over eight random samples .",
    "( b ) scatter plot comparing @xmath77 with @xmath67 .",
    "the full line indicates equality .",
    ", title=\"fig : \" ] .2 cm .",
    "( a ) entropy density difference versus @xmath72 .",
    "the data points connected by dashed line are the differences between @xmath73 and @xmath67 , while those connected by solid line are the differences @xmath74 . the number of variables @xmath75 for random @xmath2-sat ( r@xmath0-sat or r@xmath1-sat ) problem and @xmath76 for binary perceptron ( bperc ) .",
    "@xmath67 is computed with the knowledge of the original model using belief propagation  @xcite .",
    "each point represents the average over eight random samples .",
    "( b ) scatter plot comparing @xmath77 with @xmath67 .",
    "the full line indicates equality .",
    ", title=\"fig:\"].2 cm      the random @xmath2-sat problem is finding a solution ( an assignment of @xmath3 boolean variables ) satisfying a random formula composed of logical and of @xmath78 constraints  @xcite .",
    "each constraint is a logical or function of @xmath2 randomly chosen distinct variables ( either directed or negated with equal probability ) .",
    "the constraint density @xmath79 . for @xmath80 , the threshold separating a sat phase from an unsat phase",
    "was confirmed to be @xmath81 below which the solution space is ergodic and a simple local search algorithm can easily identify a solution  @xcite . for @xmath82 , the estimated threshold @xmath83 below which the solution space exhibits richer structures  @xcite .",
    "the dynamical transition point locates at @xmath84 and separates the ergodic phase from non - ergodic phase .",
    "we use seqsat algorithm of ref .",
    "@xcite to first find a solution for a given @xmath72 , then @xmath85 single variable flips are performed in the current solution space , after that we perform random walks in the current solution space to sample one solution every @xmath86 steps .",
    "each step involves @xmath3 attempts to move from one solution to its adjacent one by single variable flip , i.e. , a randomly chosen variable is flipped and if the new configuration is a solution , the flip is accepted with probability @xmath87 ; otherwise the movement is rejected .",
    "we sample totally @xmath88 solutions to estimate the entropy .",
    "we choose @xmath89 for random @xmath0-sat and @xmath90 for random @xmath1-sat .",
    "results are reported in fig .",
    "[ compa ] .",
    "when @xmath72 is small , our method can predict the true entropy very well especially for @xmath80 which can be actually transformed into a pairwise ising model . as @xmath72 increases",
    ", the difference between @xmath91 and @xmath92  @xcite becomes large and this deviation is more obvious for @xmath82 , which manifests the presence of higher - order correlations in the solution space . at high",
    "@xmath72 ( e.g. , @xmath93 for @xmath82 ) , the belief propagation eq .  ( [ bpising ] ) would yield multiple fixed points .",
    "this signals ergodicity breaking phenomenon in the energy landscape of the constructed ising model or indicates that long range correlations develop in the original system  @xcite , although our samplings are still confined in a single cluster of the original model , as a result , the predicted entropy becomes rather inaccurate compared with the true one computed under the original model .",
    "the binary perceptron with @xmath3 binary weights connecting @xmath3 input nodes to a single output node performs a random classification of @xmath94 random binary patterns @xmath95 .",
    "the critical constraint density @xmath96 below which the solution space is non - empty  @xcite .",
    "given an input pattern @xmath97 , if the actual output @xmath98 is equal to the desired output @xmath99 assigned a value @xmath100 with equal probabilities , the configuration @xmath101 learns this pattern .",
    "the solution space of the binary perceptron consists of all configurations learning @xmath94 random patterns . before sampling , we first learn @xmath94 patterns using dwf algorithm of ref .",
    "the sampling procedure is the same as that used for random @xmath2-sat problems . in numerical simulations ,",
    "we choose @xmath102 .",
    "the deviation of estimated @xmath77 from @xmath67  @xcite is plotted against @xmath72 . for small @xmath72",
    ", our method can predict the true entropy well without the knowledge of the original model . the large deviation shown in fig .",
    "[ compa ] at high @xmath72 implies higher - order correlations start to dominate the solution space .",
    "( data courtesy of gasper tkacik , refs .",
    "@xcite ) . to infer the network , we choose @xmath89 .",
    "the network is inferred at @xmath103 .",
    "( b ) reconstructed @xmath104 using belief propagation eq .",
    "( [ corre ] ) versus the measured one .",
    "we only show the case @xmath105 since @xmath106 .",
    "the full line indicates equality .",
    ", title=\"fig : \" ] .1 cm   ( data courtesy of gasper tkacik , refs .",
    "@xcite ) . to infer the network , we choose @xmath89 .",
    "the network is inferred at @xmath103 .",
    "( b ) reconstructed @xmath104 using belief propagation eq .",
    "( [ corre ] ) versus the measured one .",
    "we only show the case @xmath105 since @xmath106 .",
    "the full line indicates equality .",
    ", title=\"fig:\"].2 cm    .estimated entropy density @xmath70 for the inferred retinal network through belief propagation eq .",
    "( [ bpising ] ) .",
    "@xmath107 where @xmath108 is computed from the fixed point of belief propagation eq .",
    "( [ bpising ] ) . the self - overlap @xmath109 .",
    "the last column gives the probability of appearance for each fixed point during @xmath110 runs of belief propagation with the same inferred parameters . [ cols=\"^,^,^,^\",options=\"header \" , ]      a recording of the activity of @xmath111 neurons in a salamander retina under natural movie stimuli could also be analyzed within the current setting .",
    "the total effective number of samplings @xmath112  @xcite .",
    "our estimated entropy density for the retinal network is @xmath113 consistent with that obtained by gasper tkacik et.al  @xcite using monte carlo method which produces an estimate @xmath114 but is rather time consuming for large @xmath3 .",
    "our result implies that the retina under the naturalistic movie stimuli stores @xmath115 effective configurations .",
    "if we approximate the true entropy using that calculated by belief propagation ( eq .  ( [ entropy ] ) and eq .  ( [ bpising ] ) ) or monte carlo method , then the multi - information measuring the total amount of correlations in the network  @xcite @xmath116 or @xmath117 where @xmath73 is the independent entropy density .",
    "the result is that the difference between these two multi - information values @xmath118 .",
    "the histogram of inferred parameters is shown in fig .",
    "[ histo ] ( a ) .",
    "note that most of predicted couplings concentrate around zero value with a long tail of distribution for large negative couplings whose weights are rather small .",
    "most of the predicted fields are negative since most of the neurons are silent across the movie presentations . using the same inferred parameters , we run belief propagation eq .",
    "( [ bpising ] ) @xmath110 times from different random initializations .",
    "several fixed points are found and one of them is consistent with the previous result  @xcite ( see table  [ table : retina ] ) .",
    "these fixed points represent different metastable states and the entropy measures the capacity of neurons to convey information about the visual stimulus which contains high - order correlation structure .",
    "the visual information could be encoded by identity of the basin of attraction  @xcite and these predicted metastable states may code for specific stimulus features .",
    "therefore , the information of the inputs to the retina can be stored in the couplings and fields which generate a free energy landscape with multiple metastable states for redundant error correction  @xcite .",
    "future research on neuronal population coding needs to elucidate this point .    in fig .",
    "[ histo ] ( b ) , we verify that the inferred pairwise ising model reproduces the measured connected correlations with very good agreement .",
    "the reconstructed correlations @xmath119 can be computed by the following message passing algorithm  @xcite :    [ bpc ] @xmath120    where two kinds of messages , @xmath19 and @xmath20 are updated . once both messages for each directed link in the network are converged , i.e. , iteration of eq .",
    "( [ bpc ] ) reaches fixed point , we compute the predicted connected correlations @xmath119 via    [ corre ] @xmath121    where the ising model is known and all messages needed to compute @xmath104 including @xmath122 and @xmath123 are read from the fixed point .",
    "this message passing strategy to evaluate the correlations is very fast and takes tens of iterations to converge .",
    "remarkably , the estimated magnetizations and correlations fit those measured very well . however , using monte carlo samplings to reconstruct the correlations , we failed to reproduce those measured .",
    "for example , after sufficient thermalization , the configuration is sampled every @xmath124 monte carlo sweeps , then the correlations are computed with @xmath124 sampled configurations .",
    "the obtained root mean square error is about @xmath125 .",
    "possible reason is , the reconstructed ising model by susprop already develops multiple states ( different fixed points of belief propagation eq .",
    "( [ bpising ] ) ) , therefore , at temperature @xmath126 and @xmath127 , as observed in our simulations , monte carlo samplings can have transitions between different states yielding the average energy density of sampled configurations @xmath128 with fluctuation of order @xmath129 while the state reproducing the measured correlations in fig .",
    "[ histo ] ( b ) has typical energy density @xmath130 but smallest entropy density @xmath131 of all observed states .",
    "actually , in this case , the correlation computed by monte carlo samplings corresponds to the average over different states while the measured data comes from one state of the reconstructed ising model .",
    "there exist three kinds of convergence patterns for different iterations of susprop rules .",
    "one is the convergence case shown by an example of random @xmath0-sat with @xmath132 ; the second type is the convergence fraction @xmath35 first increases then decreases ( see in fig .",
    "[ covp ] an example of binary perceptron with @xmath133 ) ; the last type is @xmath35 reaches a plateau with small fluctuations shown by an example of retina .        .",
    "the error bar shows the fluctuation across eight random samples .",
    "( b ) the distribution of inferred couplings for r@xmath1-sat with @xmath134 and @xmath75 .",
    "two results for @xmath135 and @xmath136 are shown .",
    ", title=\"fig : \" ] .2 cm .",
    "the error bar shows the fluctuation across eight random samples .",
    "( b ) the distribution of inferred couplings for r@xmath1-sat with @xmath134 and @xmath75 .",
    "two results for @xmath135 and @xmath136 are shown .",
    ", title=\"fig:\"].2 cm    in fig .",
    "[ edf](a ) , the influence of the number of samplings @xmath4 on the estimation of entropy from the finite samplings is shown .",
    "it seems that the entropy density difference decreases as @xmath4 becomes small .",
    "note that we construct the pairwise ising model based on the samplings from the ground state ( zero energy for these three constraint satisfaction problems ) and the underlying graphical model ( e.g. , r@xmath1-sat or bperc ) may not be a pairwise ising model .",
    "our samplings are always confined in a single solution cluster and the quality may depend on the fine structure of the solution space  @xcite .",
    "this case is different from studies on the reconstruction of sherrington - kirkpatrick model at high temperatures  @xcite . in our current setting ,",
    "when the sampling number @xmath4 decreases , the collected data seems to have more correlations ( this may be induced by the statistical errors ) and the susprop turns out to be not converged any more ( e.g. , in the case of r@xmath0-sat with @xmath137 ) , which can be justified from the fig .  [ edf](b ) that the inferred coupling distribution becomes broader with decreasing @xmath4 and thus the estimated entropy should take smaller values . as observed in fig .",
    "[ edf](b ) , once @xmath4 decreases down to some value , the estimated entropy value would underestimate the true one computed with the knowledge of the original model . on the other hand ,",
    "given small @xmath4 , the computed magnetizations and correlations would have large statistical errors and may not correctly reflect the correlations in the sampled solution cluster .",
    "safely , we select @xmath88 to reduce the statistical error for calculating the magnetizations and correlations .",
    "in this work , we address the important problem of counting the total number of solutions ( configurations ) based on a limited number of sampled solutions ( configurations ) .",
    "we formulate the solution ( configuration ) counting problem within the framework of inverse ising problem , and this idea is tested on both diluted models and fully - connected models , as well as on real neural data . in the first case , we do not know a priori the underlying graphical models and try to construct a disordered ising model from the collected data to evaluate the entropy of those unknown models .",
    "note that the sampled solutions come from the zero energy ground state ( single solution cluster ) and the number is limited to @xmath4 but the size of that sampled cluster is evaluated . to this end ,",
    "the pairwise model improves substantially the independent model ( see fig .",
    "[ compa](a ) ) .",
    "when the constraint density is small then the pairwise correlation dominates the solution space , the estimated entropy gets very close to the true one estimated with the knowledge of the original model .",
    "another interesting point to be demonstrated in our further work is , for small @xmath3 , one can compute the magnetizations and correlations through exact enumeration and further the real entropy value .",
    "the result of boltzmann learning algorithm  @xcite and monte carlo simulation  @xcite ( using a large enough amount of monte carlo samplings ) should provide an upper bound on the true entropy . instead ,",
    "using the approximate method we proposed , whether the obtained result provides a bound should be checked for small size system or proved in the limit of large @xmath3 , provided that the magnetizations and correlations are less noisy . in the second case ,",
    "the susceptibility propagation is applied to infer the retinal network and belief propagation is used to reproduce the entropy computed by monte carlo method .",
    "this message passing scheme is very fast and efficient especially for large network and the observed multiple fixed points predict other metastable states in the inferred retinal network .",
    "these metastable states may have intimate relation with the neuronal population coding  @xcite .",
    "extensions to the neuronal interaction network organized in a hierarchical and modular manner would be very interesting  @xcite .",
    "our presented framework constructs a statistical mechanics description of the system directly from either artificial data or real data , and has the potential to describe biological networks more generally and estimate the size of the solution space in various contexts especially when pairwise correlation dominates the system .",
    "we thank gasper tkacik for providing us multielectrode recordings of the salamander retina .",
    "the improvement of the manuscript benefited from comments and suggestions of anonymous referees .",
    "the present work was partially supported by the nsfc grant 10834014 and the 973-program grant 2007cb935903 and hkust 605010 ."
  ],
  "abstract_text": [
    "<S> we formulate the solution counting problem within the framework of inverse ising problem and use fast belief propagation equations to estimate the entropy whose value provides an estimate on the true one . </S>",
    "<S> we test this idea on both diluted models ( random @xmath0-sat and @xmath1-sat problems ) and fully - connected model ( binary perceptron ) , and show that when the constraint density is small , this estimate can be very close to the true value . the information stored by the salamander retina under the natural movie stimuli </S>",
    "<S> can also be estimated and our result is consistent with that obtained by monte carlo method . </S>",
    "<S> of particular significance is sizes of other metastable states for this real neuronal network are predicted . </S>"
  ]
}