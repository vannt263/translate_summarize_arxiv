{
  "article_text": [
    "inspired by the hierarchical structure of the visual cortex , recent studies on probabilistic models used deep hierarchical architectures to learn high order statistics of the data  @xcite .",
    "one widely used architecture is a deep believe network ( dbn ) , which is usually trained as stacked restricted boltzmann machines ( rbms )  @xcite .",
    "since the original formulation of rbms assumes binary input values , the model needs to be modified in order to handle continuous input values .",
    "one common way is to replace the binary input units by linear units with independent gaussian noise , which is known as gaussian - binary restricted boltzmann machines ( grbms ) or gaussian - bernoulli restricted boltzmann machines  @xcite first proposed by @xcite .",
    "the training of grbms is known to be difficult , so that several modifications have been proposed to improve the training .",
    "@xcite used a sparse penalty during training , which allowed them to learn meaningful features from natural image patches . @xcite",
    "trained grbms on natural images and concluded that the difficulties are mainly due to the existence of high - frequency noise in the images , which further prevents the model from learning the important structures .",
    "@xcite illustrated that in terms of likelihood estimation grbms are already outperformed by simple mixture models .",
    "other researchers focused on improving the model in the view of generative models  @xcite .",
    "@xcite suggested that the failure of grbms is due to the training algorithm and proposed some modifications to overcome the difficulties encountered in training grbms .",
    "the studies above have shown the failures of grbms empirically , but to our knowledge there is no analysis of grbms apart from our preliminary work  @xcite , which accounts the reasons behind these failures . in this paper , we extend our work in which we consider grbms from the perspective of density models , i.e. how well the model learns the distribution of the data .",
    "we show that a grbm can be regarded as a mixture of gaussians , which has already been mentioned briefly in previous studies  @xcite but has gone unheeded .",
    "this formulation makes clear that grbms are quite limited in the way they can represent data .",
    "however we argue that this fact does not necessarily prevent the model from learning the statistical structure in the data .",
    "we present successful training of grbms both on a two - dimensional blind source separation problem and natural image patches , and that the results are comparable to that of independent component analysis ( ica ) . based on our analysis",
    "we propose several training recipes , which allowed successful and fast training in our experiments .",
    "finally , we discuss the relationship between grbms and above mentioned modifications of the model .",
    "a boltzmann machine ( bm ) is a markov random field with stochastic _",
    "visible _ and _ hidden _ units @xcite , which are denoted as  @xmath0 and  @xmath1 , respectively .",
    "in general , we use bold letters denote vectors and matrices .",
    "the joint probability distribution is defined as @xmath2 where @xmath3 denotes an _ energy function _ as known from statistical physics , which defines the dependence between @xmath4 and @xmath5 .",
    "the temperature parameter @xmath6 is usually ignored by setting its value to one , but it can play an important role in inference of bms @xcite .",
    "the _ partition function _ @xmath7 normalizes the probability distribution by integrating over all possible values of @xmath4 and @xmath5 , which is intractable in most cases . so that in training bms using gradient descent the partition function is usually estimated using sampling methods .",
    "however , even sampling in bms remains difficult due to the dependencies between all variables .",
    "an rbm is a special case of a bm where the energy function contains no terms combining two different hidden or two different visible units .",
    "viewed as a graphical model , there are no lateral connections within the visible or hidden layer , which results in a bipartite graph .",
    "this implies that the hidden units are conditionally independent given the visibles and vice versa , which allows efficient sampling .",
    "the values of the visible and hidden units are usually assumed to be binary , i.e. @xmath8 . the most common way to extend an rbm to continuous data is a grbm , which assumes continuous values for the visible units and binary values for the hidden units . its energy function @xcite is defined as @xmath9 where @xmath10 denotes the euclidean norm of @xmath11 . in grbms",
    "the visible units given the hidden values are gaussian distributed with standard deviation @xmath12 .",
    "notice that some authors @xcite use an independent standard deviation for each visible unit , which comes into account if the data is not whitened @xcite .",
    "+ the conditional probability distribution of the visible given the hidden units is given by @xmath13    where @xmath14 and @xmath15 denote the @xmath16th row and the @xmath17th column of the weight matrix , respectively .",
    "@xmath18 denotes a gaussian distribution with mean @xmath19 and variance @xmath20 .",
    "and @xmath21 denotes an isotropic multivariate gaussian distribution centered at vector @xmath22 with variance @xmath20 in all directions . from ( [ eqn : probofxh1 ] ) to ( [ eqn : probofxh2 ] ) we used the relation @xmath23 the conditional probability distribution of the hidden units given the visibles can be derived as follows @xmath24 @xmath25 turns out to be a product of independent sigmoid functions , which is a frequently used non - linear activation function in artificial neural networks .",
    "maximum likelihood estimation ( mle ) is a frequently used technique for training probabilistic models like bms . in mle",
    "we have a data set @xmath26 where the observations @xmath27 are assumed to be independent and identically distributed ( i.i.d . ) .",
    "the goal is to find the optimal parameters @xmath28 that maximize the likelihood of the data , i.e. maximize the probability that the data is generated by the model  @xcite . for practical reasons one often considers the logarithm of the likelihood , which has the same maximum as the likelihood since it is a monotonic function .",
    "the log - likelihood is defined as @xmath29 we use the average log - likelihood per training case denoted by @xmath30 .",
    "for rbms it is defined as @xmath31 where @xmath32 .",
    "and @xmath33 denotes the expectation of the function @xmath34 with respect to variable @xmath35 .",
    "the gradient of the @xmath30 turns out to be the difference between the expectations of the energies gradient under the data and model distribution , which is given by @xmath36 in practice , a finite set of i.i.d .",
    "samples can be used to approximate the expectations in ( [ eqn : loglikelihooddiff ] ) . while we can use the training data to estimate the first term , we do not have any i.i.d",
    ". samples from the unknown model distribution to estimate the second term .",
    "since we are able to compute the conditional probabilities in rbms efficiently , gibbs sampling can be used to generate those samples .",
    "but gibbs - sampling only guarantees to generate samples from the model distribution if we run it infinite long . as this is impossible ,",
    "a finite number of @xmath37 sampling steps are used instead .",
    "this procedure is known as contrastive divergence  -  @xmath37 ( cd-@xmath37 ) algorithm , in which even @xmath38 shows good results  @xcite .",
    "the cd - gradient approximation is given by @xmath39 where @xmath40 denotes the samples after @xmath37 steps of gibbs sampling .",
    "the derivatives of the grbm s energy function with respect to the parameters are given by @xmath41 and the corresponding gradient approximations  ( [ eqn : loglikelihooddiffapprox1 ] ) become @xmath42 where @xmath43 , i.e. @xmath44 denotes a vector of probabilities .      from the perspective of density estimation ,",
    "the performance of the model can be assessed by examining how well the model estimates the data distribution .",
    "we therefore take a look at the model s marginal probability distribution of the visible units , which can be formalized as a product of experts ( poe ) or as a mixture of gaussians ( mog ) ) .",
    "we derive the marginal probability distribution of the visible units @xmath45 by factorizing the joint probability distribution over the hidden units .",
    "@xmath46      \\\\      & = : & \\frac{1}{z } \\prod_j^n{p_j\\left ( \\mathbf{x } \\right ) }      \\label{eqn : probofxgauss2}.        \\end{aligned}\\ ] ]    equation  ( [ eqn : probofxgauss2 ] ) illustrates that @xmath45 can be written as a product of @xmath47 factors , referred to as a product of experts  @xcite .",
    "each expert @xmath48 consists of two isotropic gaussians with the same variance @xmath49 .",
    "the first gaussian is placed at the visible bias @xmath50 .",
    "the second gaussian is shifted relative to the first one by @xmath47 times the weight vector @xmath15 and scaled by a factor that depends on @xmath15 and @xmath50 .",
    "every hidden unit leads to one expert , each mode of which corresponds to one state of the corresponding hidden unit .",
    "figure  [ fig : grbmpdf ] ( a ) and ( b ) illustrate @xmath45 of a grbm-2 - 2 viewed as a poe , where grbm-@xmath51-@xmath47 denotes a grbm with @xmath51 visible and @xmath47 hidden units .",
    "using bayestheorem , the marginal probaility of @xmath4 can also be formalized as : @xmath52 where @xmath53 denotes the set of all possible binary vectors with exactly @xmath37 ones and @xmath54 zeros respectively . as an example , @xmath55 sums over the probabilities of all binary vectors having exactly two entries set to one .",
    "@xmath56 in  ( [ eqn : probofxcomponent1 ] ) is derived as follows @xmath57    since the form in ( [ eqn : probofxexpandgauss ] ) is similar to a mixture of isotropic gaussians , we follow its naming convention . each gaussian distribution",
    "is called a _ component _ of the model distribution , which is exactly the conditional probability of the visible units given a particular state of the hidden units . as well as in mogs",
    ", each component has a _ mixing coefficient _ , which is the marginal probability of the corresponding state and",
    "can also be viewed as the prior probability of picking the corresponding component .",
    "the total number of components in a grbm is @xmath58 , which is exponential in the number of hidden units , see figure  [ fig : grbmpdf ]  ( c ) for an example .",
    "the locations of the components in a grbm are not independent of each other as it is the case in mogs .",
    "they are centered at @xmath59 , which is the vector sum of the visible bias and selected weight vectors .",
    "the selection is done by the corresponding entries in @xmath60 taking the value one .",
    "this implies that only the @xmath61 components that sum over exactly one or zero weights can be placed and scaled independently .",
    "we name them first order components and the anchor component respectively .",
    "all @xmath62 higher order components are then determined by the choice of the anchor and first order components .",
    "this indicates that grbms are constrained mogs with isotropic components .",
    "the general presumption in the analysis of natural images is that they can be considered as a mixture of independent super - gaussian sources @xcite , but see @xcite for an analysis of remaining dependencies . in order to be able to visualize how grbms model natural image statistics",
    ", we use a mixture of two independent laplacian distributions as a toy example .",
    "the independent sources @xmath63 are mixed by a random mixing matrix @xmath64 yielding @xmath65 where @xmath66 .",
    "it is common to whiten the data ( see section  [ sec : preprocessing ] ) , resulting in @xmath67 where @xmath68 is the whitening matrix calculated with principle component analysis ( pca ) . through all this paper",
    ", we used the whitened data .    in order to assess the performance of grbms in modeling the data distribution , we ran the experiments for @xmath69 times and calculated the @xmath30 for test data analytically . for comparision",
    ", we also calculated the @xmath30 over the test data for icafor the fast ica algorithm  @xcite we used for training , the @xmath30 for super gaussian sources can also be assessed analytically by @xmath70 . ] , an isotropic two - dimensional gaussian distribution and the true data distributionas we know the true data distribution , the exact @xmath30 can be calculated by @xmath71 , where @xmath72 . ] .",
    "the results are presented in table  [ tab : blindsrcsept ] , which confirm the conclusion of  @xcite that grbms are not as good as ica in terms of @xmath30 .",
    "ccc & +   + gaussian & @xmath73 + grbm & @xmath74 + ica & @xmath75 + data distribution & @xmath76 +    to illustrate how grbms model the statistical structure of the data , we looked at the probability distributions of the 200 trained grbms .",
    "about half of them ( 110 out of 200 ) recovered the independent components , see figure  [ fig : blindsrcsept_grbm ] ( a ) as an example .",
    "this can be further illustrated by plotting the amari errors and @xmath77 is defined as : @xmath78 . ] between the true unmixing matrix @xmath79 and estimated model matrices , i.e. the unmixing matrix of ica and the weight matrix of the grbm , as shown in figure  [ fig : amarierror ] .",
    "one can see that these 110 grbms estimated the unmixing matrix quite well , although grbms are not as good as ica .",
    "this is due to the fact that the weight vectors in grbms are not restricted to be orthogonal as in ica .    for the remaining 90 grbms",
    ", the two weight vectors pointed to the opposite direction as shown in figure  [ fig : blindsrcsept_grbm ] ( b ) .",
    "accordingly , these grbms failed to estimate the unmixing matrix , but in terms of density estimation these solutions have the same quality as the orthogonal ones .",
    "thus all the 200 grbms were able to learn the statistical structures in the data and model the data distribution pretty well .    for comparison , we plotted the probability distribution of a learned grbm with four hidden units , see figure  [ fig : blindsrcsept_grbm ] ( c ) , in which grbms can always find the two independent components correctly .",
    "+        to further show how the components contribute to the model distribution , we randomly chose one of the 110 grbms and calculated the mixing coefficients of the anchor and the first order components , as shown in table  [ tab : blindsrcsept_coefficients ] .",
    "the large mixing coefficient for the anchor component indicates that the model will most likely reach hidden states in which none of the hidden units are activated . in general , the more activated hidden units a state has , the less likely it will be reached , which leads naturally to a sparse representation of the data .",
    "cccccc & @xmath80 & @xmath81 & @xmath82 & @xmath83 & @xmath84 +   + grbm-2 - 2 & @xmath85 & @xmath86 & @xmath87-@xmath88 &  &  + grbm-2 - 4 & @xmath89 & @xmath90 & @xmath91-@xmath92 & @xmath93-@xmath94 & @xmath95-@xmath96 + mog-3 & @xmath97 & @xmath98 &  &  &  +    the dominance of @xmath99 and all @xmath100 can also be seen in figure  [ fig : blindsrcsept_grbm ] by comparing a grbm-2 - 2 ( a ) with an two dimensional mog having three isotropic components denoted by mog-2 - 3 ( d ) .",
    "although the mog-2 - 3 has one component fewer than the grbm-2 - 2 , their probability distributions are almost the same .",
    "in contrast to random images , natural images have a common underlying structure which could be used to code them more efficiently than with a pixel - wise representation .",
    "@xcite showed that sparse coding is such an efficient coding scheme and that it is in addition a biological plausible model for the simple cells in the primary visual cortex .",
    "@xcite showed that the independent components provide a comparable representation for natural images .",
    "we now want to test empirically whether grbms generate such biological plausible results like sparse coding and ica .",
    "we used the @xmath101 natural image database of @xcite and randomly sampled 70000 grey scale image patches with a size of @xmath102 pixels .",
    "the data was whitened using zero - phase component analysis ( zca ) , afterwards it was divided into 40000 training and 30000 testing image patches .",
    "we followed the training recipes mentioned in section  [ sec : trainingreceipt ] , since training a grbm on natural image patches is not a trivial task .    in figure",
    "[ fig : nifiltersgbrbm ] , we show the learned weight vectors namely features or filters , which can be regarded as receptive fields of the hidden units .",
    "they are fairly similar to the filters learned by ica  @xcite .",
    "similar to the 2d experiment , we calculated the anchor and first order mixing coefficients , as shown in table  [ tab : nicoefficientavgactiv ] .",
    "the coefficients are much smaller compared to the anchor and first order coefficients of the grbms in the two dimensional case .",
    "however , they are still significantly large , considering that the total number of components in this case is @xmath103 . similar to the two - dimensional experiments , the more activated hidden units a state has , the less likely it will be reached , which leads naturally to a sparse representation . to support this statement , we plotted the histogram of the number of activated hidden units per training sample , as shown in figure  [ fig : nihiddenactivityhist ] .",
    "lccc & @xmath80 & @xmath81 & @xmath104 +   + grbm-196 - 196 & @xmath105 & @xmath106 & @xmath107 +        we also examined the results of grbms in the over - complete case , i.e. grbm-196 - 588 .",
    "there is no prominent difference of the filters compared to the complete case shown in figure  [ fig : nifiltersgbrbm ] . to further compare the filters in the complete and over - complete case",
    ", we estimated the spatial frequency , location and orientation for all filters in the spatial and frequency domains , see figure  [ fig : nifiltersspatial ] and figure  [ fig : nifilterspolar ] respectively .",
    "this is achieved by fitting a gabor function of the form used by  @xcite .",
    "note that the additional filters in the over - complete case increase the variety of spatial frequency , location and orientation .     grid . the thickness and length of each bar",
    "are propotional to its spatial - frequency bandwidth.,title=\"fig : \" ]   grid .",
    "the thickness and length of each bar are propotional to its spatial - frequency bandwidth.,title=\"fig : \" ]    -bandwidth in spatial - frequency and orientation , @xcite .",
    ", title=\"fig : \" ] -bandwidth in spatial - frequency and orientation , @xcite .",
    ", title=\"fig : \" ]",
    "the training of grbms has been reported to be difficult  @xcite . based on our analysis",
    "we are able to propose some recipes which should improve the success and speed of training grbms on natural image patches .",
    "some of them do not depend on the data distribution and should therefore improve the training in general .",
    "the preprocessing of the data is important especially if the model is highly restricted like grbms .",
    "whitening is a common preprocessing step for natural images .",
    "it removes the first and second order statistics from the data , so that it has zero mean and unit variance in all directions .",
    "this allows training algorithms to focus on higher order statistics like kurtosis , which is assumed to play an important role in natural image representations  @xcite .",
    "the components of grbms are isotropic gaussians , so that the model would use several components for modeling covariances .",
    "but the whitened data has a spherical covariance matrix so that the distribution can be modelled already fairly well by a single component .",
    "the other components can then be used to model higher order statistics , so that we claim that whitening is also an important preprocessing step for grbms .",
    "the initial choice of model parameters is important for optimization process .",
    "using prior knowledge about the optimization problem can help to derive an initialization , which can improve the speed and success of the training .",
    "for grbms we know from the analysis above that the anchor component , which is placed at the visible bias , represents most of the whitened data .",
    "therefore it is reasonable in practice to set the visible bias to the data s mean .",
    "learning the right scaling is usually very slow since the weights and biases determine both the position and scaling of the components . in the final stage of training grbms on whitened natural images ,",
    "the first components are scaled down extremely compared to the anchor component .",
    "therefore , it will usually speed up the training process if we initialize the parameters so that the first order scaling factors are already very small . considering equation  ( [ eqn : probofxexpandgauss ] ) , we are able to set a specific first order scaling factor by initializing the hidden bias to @xmath108 so that the scaling is determined by @xmath109 , which should ideally be chosen close to the unknown final scaling factors . in practice ,",
    "the choice of @xmath110 showed good performance in most cases .",
    "the learning rate for the hidden bias can then be set much smaller than the learning rate for the weights .",
    "according to  @xcite , the weights should be initialized to @xmath111 , where @xmath112 is the uniform distribution in the interval [ a , b ] . in our experience , this works better than the commonly used initialization to small gaussian - distributed random values .",
    "the choice of the hyper - parameters has an significant impact on the speed and success of training grbms . for successful training in an acceptable number of updates , the learning rate needs to be sufficiently big .",
    "otherwise the learning process becomes too slow or the algorithm converges to a local optimum where all components are placed in the data s mean .",
    "but if the learning rate is chosen too big , the gradient can easily diverge resulting in a number overflow of the weights .",
    "this effect becomes even more crucial as the model dimensionality increases , so that a grbm with 196 visible and 1000 hidden units diverges already for a learning rate of 0.001 .",
    "we therefore propose restricting the weight gradient column norms @xmath113 to a meaningful size to prevent divergence . since we know that the components are placed in the region of data",
    ", there is no need for a weight norm to be bigger than twice the maximal data norm .",
    "consequently , this natural bound also holds for the gradient and can in practice be chosen even smaller .",
    "it allows to choose big learning rates even for very large models and therefore enables fast and stable training . in practice",
    ", one should restrict the norm of the update matrix rather than the gradient matrix to also restrict the effects of the momentum term and etc .",
    "since the components are placed on the data they are naturally restricted , which makes the use of a weight decay useless or even counter productive since we want the weights to grow up to a certain norm .",
    "thus we do recommend not to use a weight decay regularization .",
    "a momentum term adds a percentage of the old gradient to the current gradient which leads to a more robust behavior especially for small batch - sizes . in the early stage of training",
    "the gradient usually varies a lot , a large momentum can therefore be used to prevent the weights from converging to zero . in the late stage however , it can also prevent convergence so that in practice a momentum of 0.9 that will be reduced to zero in the final stage of training is recommended .      using the gradient approximation , rbms",
    "are usually trained as described in section  [ sec : trainingalgorithm ] .",
    "the quality of the approximation highly depends on the set of samples used for estimating the model expectation , which should ideally be i.i.d .",
    "but gibbs sampling usually has a low mixing rate , which means that the samples tend to stay close to the previously presented samples .",
    "therefore , a few steps of gibbs sampling commonly leads to a biased approximation of the gradient . in order to increase the mixing rate @xcite suggested to use a persistent markov chain for drawing samples from the model distribution , which is referred as persistent contrastive divergence ( pcd ) .",
    "@xcite proposed to use parallel tempering ( pt ) , which selects samples from a persistent markov chain with a different scaling of the energy function .",
    "in particular , @xcite analyzed pt algorithm for training grbms and proposed a modified version of pt .    in our experiments",
    "all methods above lead to meaningful features and comparable @xmath30 , but differ in convergence speed as shown in figure  [ fig : cdvspt ] . as for pt",
    ", we used original algorithm @xcite together with weight restrictions and temperatures from 0.1 to 1 with step - size 0.1 .",
    "although , pt has a better performance than cd , it has also a much higher computational cost as shown in table  [ tab : methodcpu ] .     of a grbm 196 - 16 on the whitened natural image dataset for cd , pcd using a @xmath37 of @xmath114 ,",
    "@xmath94 each and pt with 10 temperatures .",
    "the learning curves are the average over 40 trials .",
    "the learning rate was 0.1 , an initial momentum term of 0.9 was multiplied with 0.9 after each fifth epoch , the gradient was restricted to one hundredth of the maximal data norm ( 0.48 ) , no weight decay was used . ]",
    ".comparison of the cpu time for training a grbm with different methods . [ cols=\"^,^\",options=\"header \" , ]",
    "the difficulties of using grbms for modeling natural images have been reported by several authors  @xcite and various modifications have been proposed to address this problem .",
    "@xcite analyzed the problem from the view of generative models and argued that the failure of grbms is due to the model s focus on predicting the mean intensity of each pixel rather than the dependence between pixels . to model the covariance matrices at the same time , they proposed the mean - covariance rbm ( mcrbm ) .",
    "in addition to the conventional hidden units @xmath115 , there is a group of hidden units @xmath116 dedicated to model the covariance between the visible units .",
    "from the view of density models , mcrbms can be regarded as improved grbms such that the additional hidden units are used to depict the covariances .",
    "the conditional probabilities of mcrbm are given by @xmath117 where @xmath118 @xcite . by comparing ( [ eqn : probmcrbm ] ) and",
    "( [ eqn : probofxhgauss ] ) , it can be seen that the components of mcrbm can have a covariance matrix that is not restricted to be diagonal as it is the case for grbms .",
    "from the view of generative models another explanation for the failure of grbms is provided by  @xcite .",
    "although they agree with the poor ability of grbms in modeling covariances ,  @xcite argue that the deficiency is due to the binary nature of the hidden units . in order to overcome this limitation , they developed the spike - and - slab rbm ( ssrbm ) , which splits each binary hidden unit into a binary spike variable @xmath119 and a real valued slab variable @xmath120 . the conditional probability of visible units is given by @xmath121 where @xmath122 is a diagonal matrix and @xmath123 is determined by integrating the gaussian @xmath124 over the ball @xmath125",
    "in contrast to the conditional probability of grbms ( [ eqn : probofxhgauss ] ) , @xmath15 in ( [ eqn : ssrbm ] ) is scaled by the continuous variable @xmath120 , which implies that the components can be shifted along their weight vectors .",
    "we have shown that grbms are capable of modeling natural image patches and that the reported failures are due to the training procedure .",
    "@xcite showed also that grbms could learn meaningful filters by using a sparse penalty .",
    "but this penalty changes the objective function and introduced a new hyper - parameter .",
    "@xcite addressed these training difficulties , by proposing a modification of pt and an adaptive learning rate .",
    "however , we claim that the reported difficulties of training grbms with pt are due to the mentioned gradient divergence problem . with gradient restriction",
    "we were able to overcome the problem and train grbms with normal pt successfully .",
    "in this paper , we provide a theoretical analysis of grbm and showed that its product of experts formulation can be rewritten as a constrained mixture of gaussians .",
    "this representation gives a much better insight into the capabilities and limitations of the model .",
    "we use two - dimensional blind source separation task as a toy problem to demonstrate how grbms model the data distribution . in our experiments , grbms were capable of learning meaningful features both in the toy problem and in modeling natural images .    in both cases ,",
    "the results are comparable to that of ica .",
    "but in contrast to ica the features are not restricted to be orthogonal and can form an over - complete representation .",
    "however , the success of training grbms highly depends on the training setup , for which we proposed several recipes based on the theoretical analysis . some of them can be further generalized to other datasets or directly applied like the gradient restriction .    in our experience ,",
    "maximizing the @xmath30 does not imply good features and vice versa .",
    "prior knowledge about the data distribution will be beneficial in the modeling process .",
    "for instance , our recipes are based on the prior knowledge of the natural image statistics , which is center peaked and has heavy tails",
    ". it will be an interesting topic to integrate prior knowledge of the data distribution into the model rather than starting modeling from scratch .",
    "considering the simplicity and easiness of training with our proposed recipe , we believe that grbms provide a possible way for modeling natural images . since grbms are usually used as first layer in deep belief networks , the successful training of grbms should therefore improve the performance of the whole network .",
    "g. desjardins , a. courville , y. bengio , p. vincent , and o. delalleau .",
    "parallel tempering for training of restricted boltzmann machines . in _ proceedings of the international conference on artificial intelligence and statistics _ , 2010 .",
    "m. ranzato and g.  e. hinton .",
    "modeling pixel means and covariances using factorized third - order boltzmann machines .",
    "in _ proceedings of the ieee computer society conference on computer vision and pattern recognition _ , pages 25512558 , 2010 .",
    "p.  smolensky .",
    "information processing in dynamical systems : foundations of harmony theory . in _",
    "parallel distributed processing : explorations in the microstructure of cognition _ , pages 194281 . mit press , cambridge , ma , usa , 1986 .",
    "isbn 0 - 262 - 68053-x .",
    "t. tieleman .",
    "training restricted boltzmann machines using approximations to the likelihood gradient . in _ proceedings of the 25th annual international conference on machine learning _ , pages 10641071 , new york , ny , usa , 2008 .",
    "isbn 978 - 1 - 60558 - 205 - 4 .    j.  h. van hateren and a.  van  der schaaf .",
    "independent component filters of natural images compared with simple cells in primary visual cortex .",
    "_ proceedings of the royal society _ , 2650 ( 1394):0 359366 , 1998 .",
    "pmc1688904 .",
    "n. wang , j. melchior , and l. wiskott .",
    "an analysis of gaussian - binary restricted boltzmann machines for natural image . in michel verleysen , editor , _ proceedings of the 20th european symposium on artificial neural networks , computational intelligence and machine learning _ , pages 287292 , bruges , belgium , arpil 2012 .",
    "m. welling , m. rosen - zvi , and g.  e. hinton . exponential family harmoniums with an application to information retrieval . in _ proceedings of the 17th conference on neural information processing systems_. mit press , 12 2004 ."
  ],
  "abstract_text": [
    "<S> we present a theoretical analysis of gaussian - binary restricted boltzmann machines ( grbms ) from the perspective of density models . </S>",
    "<S> the key aspect of this analysis is to show that grbms can be formulated as a constrained mixture of gaussians , which gives a much better insight into the model s capabilities and limitations . </S>",
    "<S> we show that grbms are capable of learning meaningful features both in a two - dimensional blind source separation task and in modeling natural images . </S>",
    "<S> further , we show that reported difficulties in training grbms are due to the failure of the training algorithm rather than the model itself . based on our analysis </S>",
    "<S> we are able to propose several training recipes , which allowed successful and fast training in our experiments . </S>",
    "<S> finally , we discuss the relationship of grbms to several modifications that have been proposed to improve the model . </S>"
  ]
}