{
  "article_text": [
    "compressed sensing is a recently emerged technique for signal sampling and data acquisition which enables to recover sparse signals from undersampled linear measurements @xmath0 where @xmath1 is a sampling matrix with @xmath2 , @xmath3 denotes an @xmath4-dimensional sparse signal , and @xmath5 denotes the additive noise .",
    "the problem has been extensively studied and a variety of algorithms , e.g. the orthogonal matching pursuit ( omp ) algorithm @xcite , the basis pursuit ( bp ) method @xcite , the iterative reweighted @xmath6 and @xmath7 algorithms @xcite , and the sparse bayesian learning method @xcite were proposed . in many practical applications , in addition to the sparse structure , sparse signals may exhibit two - dimensional cluster patterns that can be utilized to enhance the recovery performance .",
    "for example , the target of interest in the synthetic aperture radar / inverse synthetic aperture radar ( sar / isar ) images often demonstrates continuity in both the range and cross - range domains @xcite . in video surveillance , the foreground image exhibits a cluster pattern since the foreground objects ( humans , cars , text etc . )",
    "generally occupy a small continuous region of the scene @xcite . besides these , block - sparsity is also present in temporal observations of a time - varying block - sparse signal whose support varies slowly over time @xcite .",
    "analyses @xcite show that exploiting the inherent block - sparse structure not only leads to relaxed conditions for exact reconstruction , but also helps improve the recovery performance considerably .",
    "a number of algorithms have been proposed for recovering block - sparse signals over the past few years , e.g. , block - omp @xcite , mixed @xmath8 norm - minimization @xcite , group lasso @xcite , model - based cosamp @xcite , and block - sparse bayesian learning @xcite .",
    "these algorithms , however , require _ a priori _ knowledge of the block partition ( e.g. the number of blocks and location of each block ) such that the coefficients in each block are grouped together and enforced to share a common sparsity pattern . in practice ,",
    "the prior information about the block partition of sparse signals is often unavailable , especially for two - dimensional signals since the block partition of a two - dimensional signal involves not only the location but also the shape of each block .",
    "for example , foreground images have irregular and unpredictable cluster patterns which are very difficult to be estimated _ a priori_. to address this difficulty , a few sophisticated bayesian methods which do not need the knowledge of the block partition were developed . in @xcite ,",
    "a `` spike - and - slab '' prior model was proposed , where by introducing dependencies among mixing weights , the prior model has the potential to encourage sparsity and promote a tree structure simultaneously .",
    "this `` spike - and - slab '' prior model was later extended to accommodate block - sparse signals @xcite .",
    "nevertheless , for the `` spike - and - slab '' prior introduced in @xcite , the posterior distribution can not be derived analytically , and a markov chain monte carlo ( mcmc ) sampling method has to be employed for bayesian inference . in @xcite ,",
    "a graphical prior , also referred to as the `` boltzmann machine '' , is employed as a prior on the sparsity support in order to induce statistical dependencies between atoms .",
    "with such a prior , the maximum a posterior ( map ) estimator requires an exhaustive search over all possible sparsity patterns . to overcome the intractability of the combinatorial search , a greedy method @xcite and a variational mean - field approximation method @xcite were developed to approximate the map . in @xcite , to cope with the unknown cluster pattern , an expanded model is employed by assuming that the original sparse signal is a superposition of a number of overlapping blocks , and the coefficients in each block share the same sparsity pattern .",
    "conventional block sparse bayesian learning algorithms such as those in @xcite can then be applied to the expanded model .    recently in @xcite , we proposed a pattern - coupled hierarchical gaussian prior model to exploit the unknown block - sparse structure . unlike the conventional hierarchical gaussian prior model @xcite where each coefficient is associated independently with a unique hyperparameter , the pattern - coupled prior for each coefficient not only",
    "involves its own hyperparameter , but also its immediate neighboring hyperparameters .",
    "this pattern - coupled hierarchical model is effective and flexible to capture any underlying block - sparse structures , without requiring the prior knowledge of the block partition .",
    "numerical results show that the pattern - coupled sparse bayesian learning ( pc - sbl ) method renders competitive performance for block - sparse signal recovery . nevertheless , a major drawback of the method is that it requires computing an @xmath9 matrix inverse at each iteration , and thus has a cubic complexity in terms of the signal dimension .",
    "this high computational cost prohibits its application to problems with even moderate dimensions .",
    "also , @xcite only considers recovery of one - dimensional block - sparse signals . in this paper , we generalize the pattern - coupled hierarchical model to the two - dimensional ( 2-d ) scenario in order to leverage block - sparse patterns arising from 2-d sparse signals . to address the computational issue , we resort to the generalized approximate message passing ( gamp ) technique @xcite and develop a computationally efficient method .",
    "specifically , the algorithm is developed within an expectation - maximization ( em ) framework , using the gamp to efficiently compute an approximation of the posterior distribution of hidden variables .",
    "the hyperparameters associated with the hierarchical gaussian prior are learned by iteratively maximizing the q - function which is calculated based on the posterior approximation obtained from the gamp .",
    "simulation results show that the proposed method presents superior recovery performance for block - sparse signals , meanwhile achieving a significant reduction in computational complexity .",
    "the rest of the paper is organized as follows . in section [ sec :",
    "model ] , we introduce a 2-d pattern coupled hierarchical gaussian framework to model the sparse prior and the pattern dependencies among the neighboring coefficients . in section [ sec : algorithm ] , a gamp - based em algorithm is developed to obtain the maximum a posterior ( map ) estimate of the hyperparameters , along with the posterior distribution of the sparse signal .",
    "simulation results are provided in section [ sec : simulation ] , followed by concluding remarks in section [ sec : conclusions ] .",
    "we consider the problem of recovering a two - dimensional block - sparse signal @xmath10 from compressed noisy measurements @xmath11 where @xmath12 denotes the compressed measurement vector , @xmath13 is a linear map : @xmath14 , with @xmath15 , and @xmath16 is an additive multivariate gaussian noise with zero mean and covariance matrix @xmath17 .",
    "let @xmath18 , the linear map @xmath19 can generally be expressed as @xmath20 where @xmath1 denotes the measurement matrix . in the special case",
    "where @xmath21 , then we have @xmath22 , in which @xmath23 stands for the kronecker product .",
    "the above model ( [ data - model ] ) arises in image applications where signals are multi - dimensional in nature , or in the scenario where multiple snapshots of a time - varying sparse signal are available . in these applications ,",
    "signals usually exhibit two - dimensional cluster patterns that can be utilized to improve the recovery accuracy .",
    "to leverage the underlying block - sparse structures , we introduce a 2-d pattern - coupled gaussian prior model which is a generalization of our previous work @xcite . before proceeding , we provide a brief review of the conventional hierarchical gaussian prior model @xcite , and some of its extensions .      for ease of exposition , we consider the prior model for the two - dimensional signal @xmath24 instead of its one - dimensional form @xmath3 .",
    "let @xmath25 denote the @xmath26th entry of @xmath24 . in the conventional sparse bayesian learning framework @xcite ,",
    "a two - layer hierarchical gaussian prior was employed to promote the sparsity of the solution . in the first layer ,",
    "coefficients @xmath27 of @xmath24 are assigned a gaussian prior distribution @xmath28 where @xmath29 is a non - negative hyperparameter controlling the sparsity of the coefficient @xmath25 .",
    "the second layer specifies gamma distributions as hyperpriors over the hyperparameters @xmath30 , i.e. @xmath31 as discussed in @xcite , for properly chosen @xmath32 and @xmath33 , this hyperprior allows the posterior mean of @xmath29 to become arbitrarily large . as a consequence",
    ", the associated coefficient @xmath25 will be driven to zero , thus yielding a sparse solution .",
    "this conventional hierarchical model , however , does not encourage structured - sparse solutions since the sparsity of each coefficient is determined by its own hyperparameter and the hyperparameters are independent of each other .",
    "in @xcite , the above hierarchical model was generalized to deal with block - sparse signals , in which a group of coefficients sharing the same sparsity pattern are assigned a multivariate gaussian prior parameterized by a common hyperparameter .",
    "nevertheless , this model requires the knowledge of the block partition to determine which coefficients should be grouped and assigned a common hyperparameter .      to exploit the 2-d block - sparse structure , we utilize the fact that the sparsity patterns of neighboring coefficients are statistically dependent . to capture the pattern dependencies among neighboring coefficients , the gaussian prior for each coefficient",
    "@xmath25 not only involves its own hyperparameter @xmath29 , but also its immediate neighbor hyperparameters .",
    "specifically , a prior over @xmath24 is given by @xmath34 where @xmath35 in which @xmath36 denotes the neighborhood of the grid point @xmath26 , i.e. @xmath37 changes accordingly .",
    "] , and @xmath38 $ ] is a parameter indicating the pattern relevance between the coefficient @xmath25 and its neighboring coefficients . clearly , this model is an extension of our previous prior model @xcite to the two - dimensional case .",
    "when @xmath39 , the prior model ( [ pc - model ] ) reduces to the conventional sparse bayesian learning model . when @xmath40 , we see that the sparsity of each coefficient @xmath25 is not only controlled by the hyperparameter @xmath29 , but also by the neighboring hyperparameters @xmath41 .",
    "the coefficient @xmath25 will be driven to zero if @xmath29 or any of its neighboring hyperparameters goes to infinity .",
    "in other words , suppose @xmath29 approaches infinity , then not only its corresponding coefficient @xmath25 will be driven to zero , the neighboring coefficients @xmath42 will decrease to zero as well .",
    "we see that the sparsity patterns of neighboring coefficients are related to each other through their shared hyperparameters . on the other hand , for any pair of neighboring coefficients ,",
    "each of them has its own hyperparameters that are not shared by the other coefficient .",
    "hence , no coefficients are pre - specified to share a common sparsity pattern , which enables the prior to provide flexibility to model any block - sparse structures .",
    "following @xcite , we use gamma distributions as hyperpriors over the hyperparameters @xmath43 , i.e. @xmath44 where we set @xmath45 , and @xmath46 .",
    "the choice of @xmath32 will be elaborated later in our paper .",
    "also , the noise variance @xmath47 is assumed unknown , and to estimate this parameter , we place a gamma hyperprior over @xmath48 , i.e. @xmath49 where we set @xmath50 and @xmath51 .",
    "we now proceed to perform bayesian inference for the proposed pattern - coupled hierarchical model .",
    "the following model is considered since the linear map @xmath19 can be expressed as @xmath52 @xmath53 we first translate the prior for the two - dimensional signal @xmath24 to a prior for its one - dimensional form @xmath3 . from ( [ pc - model ] ) , the prior over @xmath3 can be expressed as @xmath54 where @xmath55 in which @xmath56 denotes the neighbors of the point @xmath26 on the two - dimensional grid , i.e. @xmath57 changes accordingly . ] .",
    "the relation between @xmath58 and @xmath26 is given by @xmath59 , that is , @xmath60 , and @xmath61 , in which @xmath62 denotes the ceiling operator .",
    "note that for notational convenience , we , with a slight abuse of notation , use @xmath63 to denote the @xmath58th entry of @xmath3 and @xmath64 to denote the hyperparameter associated with the coefficient @xmath63 .",
    "also , let @xmath65 since its exact meaning remains unaltered . from ( [ alpha - prior ] ) , we have @xmath66    an expectation - maximization ( em ) algorithm can be developed for learning the sparse signal @xmath3 as well as the hyperparameters @xmath67 . in the em formulation ,",
    "the signal @xmath3 is treated as hidden variables , and we iteratively maximize a lower bound on the posterior probability @xmath68 ( this lower bound is also referred to as the q - function ) . briefly speaking , the algorithm alternates between an e - step and a m - step . in the e - step ,",
    "we need to compute the posterior distribution of @xmath3 conditioned on the observed data and the hyperparameters estimated from the @xmath69th iteration , i.e. @xmath70 it can be readily verified that the posterior @xmath71 follows a gaussian distribution with its mean and covariance matrix given respectively by @xmath72 where @xmath73 .",
    "the q - function , i.e. @xmath74 $ ] , can then be computed , where the operator @xmath75 $ ] denotes the expectation with respect to the posterior distribution @xmath71 . in the m - step , we maximize the q - function with respect to the hyperparameters @xmath67 .    it can be seen that the em algorithm , at each iteration , requires to update the posterior distribution @xmath71 , which involves computing an @xmath9 matrix inverse .",
    "thus the em - based algorithm has a computational complexity of @xmath76 flops , and therefore is not suitable for many real - world applications involving large dimensions . in the following",
    ", we will develop a computationally efficient algorithm by resorting to the generalized approximate message passing ( gamp ) technique @xcite .",
    "gamp is a very - low - complexity bayesian iterative technique recently developed @xcite for obtaining an approximation of the posterior distribution @xmath71 .",
    "it therefore can naturally be embedded within the em framework to replace the computation of the true posterior distribution . from gamp",
    "s point of view , the hyperparameters @xmath67 are considered as known . the hyperparameters can be updated in the m - step based on the approximate posterior distribution of @xmath3 .",
    "we now proceed to derive the gamp algorithm for the pattern - coupled gaussian hierarchical prior model .",
    "gamp was developed in a message passing - based framework . by using central - limit - theorem approximations , message passing between variable nodes and factor nodes",
    "can be greatly simplified , and the loopy belief propagation on the underlying factor graph can be efficiently performed . as noted in @xcite ,",
    "the central - limit - theorem approximations become exact in the large - system limit under i.i.d .",
    "zero - mean sub - gaussian @xmath77 .    for notational convenience ,",
    "let @xmath78 denote the hyperparameters .",
    "firstly , gamp assumes posterior independence among hidden variables @xmath79 and approximates the true posterior distribution @xmath80 by @xmath81 where @xmath82 and @xmath83 are quantities iteratively updated during the iterative process of the gamp algorithm . here",
    ", we have dropped their explicit dependence on the iteration number @xmath84 for simplicity . substituting ( [ x - prior ] ) into ( [ eqn-1 ] )",
    ", it can be easily verified that the approximate posterior @xmath85 follows a gaussian distribution with its mean and variance given respectively as @xmath86 another approximation is made to the noiseless output @xmath87 , where @xmath88 denotes the @xmath89th row of @xmath77 .",
    "gamp approximates the true marginal posterior @xmath90 by @xmath91 where @xmath92 and @xmath93 are quantities iteratively updated during the iterative process of the gamp algorithm .",
    "again , here we dropped their explicit dependence on the iteration number @xmath84 . under the additive white gaussian noise assumption",
    ", we have @xmath94 .",
    "thus @xmath95 also follows a gaussian distribution with its mean and variance given by @xmath96    with the above approximations , we can now define the following two scalar functions : @xmath97 and @xmath98 that are used in the gamp algorithm .",
    "the input scalar function @xmath97 is simply defined as the posterior mean @xmath99 @xcite , i.e. @xmath100 the scaled partial derivative of @xmath101 with respect to @xmath82 is the posterior variance @xmath102 , i.e. @xmath103 the output scalar function @xmath98 is related to the posterior mean @xmath104 as follows @xmath105 the partial derivative of @xmath106 is related to the posterior variance @xmath107 in the following way @xmath108 given the above definitions of @xmath97 and @xmath98 , the gamp algorithm tailored to the considered sparse signal estimation problem can now be summarized as follows ( details of the derivation of the gamp algorithm can be found in @xcite ) , in which @xmath109 denotes the @xmath110th entry of @xmath77 , @xmath111 and @xmath112 denote the posterior mean and variance of @xmath63 at iteration @xmath84 , respectively .    [ cols=\"<\",options=\"header \" , ]",
    "we developed a pattern - coupled sparse bayesian learning method for recovery of two - dimensional block - sparse signals whose cluster patterns are unknown _ a priori_. a two - dimensional pattern - coupled hierarchical gaussian prior model is introduced to characterize and exploit the pattern dependencies among neighboring coefficients .",
    "the proposed pattern - coupled hierarchical model is effective and flexible to capture any underlying block - sparse structures , without requiring the prior knowledge of the block partition .",
    "an expectation - maximization ( em ) strategy is employed to infer the maximum a posterior ( map ) estimate of the hyperparameters , along with the posterior distribution of the sparse signal .",
    "additionally , the generalized approximate message passing ( gamp ) algorithm is embedded in the em framework to efficiently compute an approximation of the posterior distribution of hidden variables , which results in a significant reduction in computational complexity .",
    "numerical results show that our proposed algorithm presents a substantial performance advantage over other existing state - of - the - art methods in image recovery .",
    "d.  wipf and s.  nagarajan , `` iterative reweighted @xmath6 and @xmath7 methods for finding sparse solutions , '' _ ieee journals of selected topics in signal processing _ ,",
    "vol .  4 , no .  2 , pp . 317329 , apr .",
    "2010 .",
    "v.  cevher , a.  sankaranarayanan , m.  f. duarte , d.  reddy , r.  g. baraniuk , and r.  chellappa , `` compressive sensing for background subtraction , '' in _",
    "european conf .",
    "comp . vision ( eccv ) _ , marseille , france , october 12 - 18 2008 .",
    "z.  zhang and b.  d. rao , `` sparse signal recovery with temporally correlated source vectors using sparse bayesian learning , '' _ ieee journal of selected topics in signal processing _",
    ", vol .  5 , no .  5 , pp . 912926 , sept .",
    "2011 .",
    "a.  drmeau , c.  herzet , and l.  daudet , `` boltzmann machine and mean - field approximation for structured sparse decompositions , '' _ ieee trans .",
    "signal processing _ , vol .",
    "60 , no .  7 ,",
    "34253438 , july 2012 .",
    "z.  zhang and b.  d. rao , `` extension of sbl algorithms for the recovery of block sparse signals with intra - block correlation , '' _ ieee trans . signal processing _ , vol .",
    "61 , no .  8 , pp . 20092015 , apr . 2013 .",
    "g.  warnell , d.  reddy , and r.  chellappa , `` adaptive rate compressive sensing for background subtraction , '' in _ ieee international conference on acoustics , speech and signal processing ( icassp ) _ , kyoto , japan , march 25 - 30 2012 .",
    "j.  p. vila and p.  schniter , `` expectation - maximization bernoulli - gaussian approximate message passing , '' in _",
    "45th asilomar conference on signals , symtems and computers _ , pacific grove , ca , usa , november 6 - 9 2011 ."
  ],
  "abstract_text": [
    "<S> we consider the problem of recovering two - dimensional ( 2-d ) block - sparse signals with _ unknown _ </S>",
    "<S> cluster patterns . </S>",
    "<S> two - dimensional block - sparse patterns arise naturally in many practical applications such as foreground detection and inverse synthetic aperture radar imaging . to exploit the block - sparse structure , </S>",
    "<S> we introduce a 2-d pattern - coupled hierarchical gaussian prior model to characterize the statistical pattern dependencies among neighboring coefficients . unlike the conventional hierarchical gaussian prior model where </S>",
    "<S> each coefficient is associated independently with a unique hyperparameter , the pattern - coupled prior for each coefficient not only involves its own hyperparameter , but also its immediate neighboring hyperparameters . </S>",
    "<S> thus the sparsity patterns of neighboring coefficients are related to each other and the hierarchical model has the potential to encourage 2-d structured - sparse solutions . an expectation - maximization ( em ) strategy is employed to obtain the maximum a posterior ( map ) estimate of the hyperparameters , along with the posterior distribution of the sparse signal . </S>",
    "<S> in addition , the generalized approximate message passing ( gamp ) algorithm is embedded into the em framework to efficiently compute an approximation of the posterior distribution of hidden variables , which results in a significant reduction in computational complexity . </S>",
    "<S> numerical results are provided to illustrate the effectiveness of the proposed algorithm .    </S>",
    "<S> pattern - coupled sparse bayesian learning , block - sparse structure , expectation - maximization ( em ) , generalized approximate message passing ( gamp ) . </S>"
  ]
}