{
  "article_text": [
    "in advertising , one is interested in segmenting people and targeting ads based on segments @xcite . with the rapid growth of the web as a publishing platform ,",
    "new advertising technologies have evolved , offering greater reach and new possibilities for targeted advertising .",
    "one such innovation is real - time bidding ( rtb ) , where upon a user s request for a specific url , an online real - time auction is started amongst numerous participants , competing to serve their advertisement .",
    "the participants are allotted a limited time on the order of 100ms to query their data sources and come up with a bid , and the winner gets to display their advertisement .",
    "thus if the computational complexity can be reduced , more complex decision processes can be invoked . in this work ,",
    "we evaluate how dimensionality reduction can be used to simplify predictors of click - through rate .",
    "we focus on three techniques for dimensionality reduction of the large bipartite graph of user - website interactions , namely singular value decomposition ( svd ) @xcite , non - negative matrix factorization ( nmf ) @xcite , and the infinite relational model ( irm ) @xcite .",
    "we are interested in how the different levels of sparsity of the output features imposed by each of the models affect the performance in a click - through rate prediction task . in the rtb setup , where low latency and high throughput are both of crucial importance , database queries need to require as little i / o as possible , and computing model predictions need to involve as few operations as possible .",
    "therefore a good idea is to `` compress '' very high - cardinality features using dimensionality reduction techniques and at the same time potentially benefit from recommender effects @xcite .",
    "this presents a trade - off between how much to compress in order to speed up i / o and calculations versus retaining , or exceeding , the performance of a high cardinality feature .    by investigating the svd , nmf , and the irm",
    ", we essentially vary the compression of a high - cardinality feature ( user - website engagements ) .",
    "the svd produces dense singular vectors , thus requiring the most i / o as well as computation .",
    "the nmf is known to produce sparse components @xcite , meaning that zeros need not be stored , retrieved nor used in computations , and thus requires less i / o and computation .",
    "the irm offers the most sparse representation , in that it produces hard cluster assignments , hence i / o and computation are reduced to a single weight per mode .",
    "we present results that use either of the dimensionality reduction techniques outputs as predictors for a click - through rate prediction task .",
    "our experiments show that a compact representation based on the nmf outperforms the other two options .",
    "if one however wants to use as little i / o and as simple computations as possible , the very compact representation from the irm model offers an interesting alternative . while incurring a limited loss of lift relative to the nmf , the irm based predictors yield the fastest training speed of the downstream logistic regression classifier and also results in the most economical usage of features and fastest possible computations at run - time .",
    "the irm further has the advantage that it alleviates the need for model order selection , which is required in nmf .",
    "while the dense features produced by svd also find usage in terms of predictive performance , the dense features inhibit the logistic regression training time , and if low database i / o as well as fast computation of predictions is a priority , the svd will not be of great use .",
    "a key enabling factor in running the irm with the data we present in this work , is a sampler written for the graphics processing unit ( gpu ) @xcite , without which learning of the irm model would not be feasible , at least not on a day - by - day schedule . to demonstrate the feasibility of the irm as a large - scale sparse dimensionality reduction",
    ", we run final tests on a full - scale click - through rate data set and compare the performances with not using any dimensionality reductions .      within the area of online advertising , computational targeting techniques",
    "are often faced with the challenge of very few observations per feature , particularly of the positive label ( i.e. , click , action , buy ) . a common approach to alleviate such label sparsity",
    "is to use collaborative filtering type algorithms , where one allow similar objects to `` borrow '' training data and thus constrain the related objects to have similar predicted behaviour .",
    "studies hereof are common for sponsored search advertising where the objects of interest are query - ad pairs @xcite , but the problem is similar to that of user - website pairs that we study . to our knowledge",
    "we are the first to report on the usage of the irm co - clustering of user - website pairs and the results should be applicable for query - add click - through rate prediction as well .    by representing users in a compressed or latent space based on the user - website graph",
    ", we are essentially building profiles of users based on their behaviour and using those profiles for targeted advertising .",
    "this approach is well studied with many other types of profiles based on various types of information : for using explicit features available for predicting click - through rates , @xcite is a good resource : latent factor models have been proposed to model click - through rates in online advertising , see e.g. @xcite : for examples of using dimensionality reduction techniques in the construction of click - through rate models , such as the nmf , see @xcite .",
    "we believe our contribution to have applications in many such setups , either as an additional predictor or for incorporation as _ a priori _ information ( priors , constraints , etc . ) which can help with identifiability of the models .",
    "we regard the problem of predicting click - through rates as a supervised learning task , i.e. , given historical observations with features ( or predictors ) available about the user , webpage , and ad , along with the labels of actions ( in our case click ( 1 ) or not - click ( 0 ) ) , the task is to learn a classifier for predicting unseen observations , given the features .",
    "this is the approach taken also by e.g. , @xcite . as in @xcite",
    ", we build a probabilistic model based on logistic regression for predicting click - through rates .",
    "what we add , is additional features based on dimensionality reduction , as well as a sparsity inducing constraint based on the @xmath0-norm .",
    "we are interested in estimation of features which can improve click - through rate predictions . in this work ,",
    "we focus on introducing features from different dimensionality reduction techniques based on a bipartite graph of users and websites ( urls ) , and using them in a simple probabilistic model for click - through rate prediction , namely logistic regression . in the following ,",
    "we introduce the dimensionality reduction techniques which we evaluate .",
    "the singular value decomposition ( svd ) of a rank @xmath1 matrix @xmath2 is given as the factorization @xmath3 , where @xmath4 and @xmath5 are unitary matrices @xmath6 and hold the left and right singular vectors of @xmath2 , respectively .",
    "the diagonal matrix @xmath7 contains the singular values of @xmath2 . by selecting only the @xmath8 largest singular values of @xmath7 ,",
    "i.e. , truncating all other singular values to zero , one obtains the approximation @xmath9 , which is the rank @xmath8 optimal solution to @xmath10 .",
    "this truncation corresponds to disregarding the @xmath11 dimensions with the least variances of the bases @xmath4 and @xmath12 as noise .",
    "non - negative matrix factorization ( nmf ) received its name as well as its popularity in @xcite .",
    "nmf is a matrix factorization comparable to svd , the crucial difference being that nmf decomposes into non - negative factors and impose no orthogonality constraints .",
    "given a non - negative input matrix @xmath2 with dimensions @xmath13 , nmf approximates the decomposition @xmath14 , where @xmath15 is an @xmath16 non - negative matrix , @xmath17 a @xmath18 non - negative matrix , and @xmath8 is the number of components . by selecting @xmath19",
    "one approximates the decomposition of @xmath20 , thereby disregarding some residual ( unconstrained ) matrix @xmath21 as noise .",
    "nmf has achieved good empirical results as an unsupervised learning technique within many applications , e.g. , for document clustering @xcite , visual coding @xcite , and bioinformatics @xcite . for nmf applications for computational advertising ,",
    "see also @xcite .",
    "the infinite relational model ( irm ) has been proposed as a bayesian generative model for graphs .",
    "generative models can provide accurate predictions and through inference of relevant latent variables they can inform the user about mesoscale structure .",
    "the irm model can be cast as co - clustering approach for bipartite networks where the nodes of each mode are grouped simultaneously .",
    "a benefit of the irm model over existing co - clustering approaches is that the model explicitly exploit the statistical properties of binary graphs and allows the number of components of each mode to be inferred from the data .    the generative process for the relational model @xcite is given by : + @xmath22 sample the row cluster probabilities , i.e. , @xmath23 .",
    "+ @xmath22 sample row cluster assignments , i.e. , @xmath24 @xmath25 .",
    "+ @xmath22 sample the column cluster probabilities , i.e. , @xmath26 .",
    "+ @xmath22 sample column cluster assignments , i.e. , @xmath27 @xmath28 .",
    "+ @xmath22 sample between cluster relations , i.e. , @xmath29 and @xmath30 @xmath31 .",
    "+ @xmath22 generate links , i.e. , @xmath24 and @xmath27 @xmath32 . + where @xmath33 and @xmath34 denote the number of row and column clusters respectively whereas @xmath35 and @xmath36 are vectors of ones with size @xmath33 and @xmath34 .",
    "the limits @xmath37 and @xmath38 lead to the infinite relational model ( irm ) which has an analytic solution given by the chinese restaurant process ( crp ) @xcite .    rather than collapsing the parameters of the model , we apply blocked sampling that allows for parallel gpu computation @xcite . moreover ,",
    "the crp is approximated by the truncated stick breaking construction ( tsb ) , and the truncation error becomes insignificant when the model is estimated for large values of @xmath33 and @xmath34 , see also @xcite .      for learning a model capable of predicting click - through rates trained on historical data ,",
    "we employ logistic regression with sparsity constraints ; for further details see for instance @xcite . given data consisting of @xmath39 observations with @xmath40-dimensional feature vectors @xmath41 and binary labels @xmath42",
    ", the probability of a positive event can be modeled with the _ logistic function _ and a single weight @xmath43 per feature .",
    "i.e. , @xmath44 , referred to as @xmath45 in the following .",
    "the optimization problem for learning the weights @xmath46 becomes @xmath47 where @xmath48 is added to control overfitting and produce sparse solutions .",
    "for skewed target distributions , an intercept term @xmath49 may be included in the model by appending an all - one feature to all observations .",
    "the corresponding regularization term @xmath50 then needs to be fixed to zero .    for training the logistic regression model",
    ", one can use gradient - descent type optimizers and quasi - newton based algorithms are a popular choice . with @xmath0-penalty , however , a little care must be taken since off - the - shelf newton - based solvers require the objective function to be differentiable , which is not due to the penalty function which is not differentiable in zero . in this work we base our logistic regression training on owl - qn @xcite for batch learning . for online learning using stochastic gradient descent with @xmath0-penalization , see @xcite .    performing predictions with",
    "a logistic regression model is as simple as computing the logistic function on the features of a test observation , @xmath51 . in terms of speed , however , it matters how the features of @xmath51 are represented .",
    "in particular for a binary feature vector @xmath52 @xmath53 i.e. , predicting for binary feature vectors scales in the number of non - zero elements of the feature vector , which makes computations considerably faster .",
    "additionally , using the right - hand side of , @xmath54 can be performed when storing the weights in memory or a database , hences saves further processing power .",
    "this has two consequences : 1 ) binary features are more desirable for making real - time predictions and 2 ) the sparser the features , the less computation time and i / o from databases is required .",
    "the data we use for our experiments originate from adform s ad transaction logs . in each transaction , e.g. , when an ad is served , the url where the ad is being displayed and a unique identifier of the users web browser is stored along with an identifier of the ad .",
    "likewise , a transaction is logged when a user clicks an ad . from these logs ,",
    "we prepare a data set over a period of time and use the final day for testing and use the rest for training .    as a pre - processing step ,",
    "all urls in the transaction log are stripped of any query - string that might be trailing the url , however the log data are otherwise unprocessed .      from the training set transactions , we produce a binary bipartite graph of users in the first mode and urls in the second mode .",
    "this is an unweighted , undirected graph where edges represent which urls a user has seen , i.e. , we do not use the number of times the user has engaged each url .",
    "the graph we obtain has @xmath55=9,304,402 unique users and @xmath56=7,056,152 unique urls .",
    "we denote this graph ` ul ` .",
    "as we will be repeating numerous supervised learning experiments , that each can be quite time consuming for the entire training set , we do our main analysis based on experiments from a subset of transactions . as an inclusion criteria",
    ", we select the top @xmath57=99,854 users based on the number of urls they have seen and urls with visits from at least 100 unique users , resulting in @xmath58=70,436 urls being included . based on those subsets of users and urls ,",
    "we produce a smaller transaction log , from which we also construct a bipartite graph denoted ` ul`@xmath59 .      for the sampled data for unsupervised learning , ` ul`@xmath59 , we use the different dimensionality reduction techniques presented in section [ sec : method ] to obtain new per - user and per - url features .    for obtaining the svd - based dense left and right singular vectors",
    ", we use ` svds ` included with matlab to compute the 500 largest eigenvalues with their corresponding eigenvectors . in the supervised learning , by joining our data by user and url with the left and right singular vectors , respectively",
    ", we can use anything from 1 to 500 of the largest eigenvectors for each modality as features .",
    "we use the nmf matlab toolbox from @xcite to decompose ` ul`@xmath59into non - negative factors .",
    "we use the original algorithm introduced in @xcite with the least - squares objective and multiplicative updates ( ` nmfrule ` option in the nmf toolbox ) .",
    "with nmf we need to decide the model order , i.e. , number of components to fit in each of the non - negative factors . hence , to investigate the influence of nmf model order , we train nmf using various model orders of 100 , 300 , and 500 number of components .",
    "we run the toolbox with the default configurations for convergence tolerance and maximum number of iterations .    as detailed in section [ sec : irm ] , we use the gpu sampling scheme from @xcite for massively speeding up the computation of the irm model .",
    "the irm estimation infers the number of components ( i.e. , clusters ) separately for each modality , however , it does require we input a maximum number of components for users and urls . for ` ul`@xmath59",
    ", we run with @xmath60=500 for both modalities and terminate the estimation after 500 iterations .",
    "the irm infers 216 user clusters and 175 url cluster for ` ul`@xmath59 , i.e. , well below the @xmath60 we specify .    for the full dataset ` ul ` , we have only completed the dimensionality reduction using irm , which is thanks to our access to the aforementioned gpu sampling code .",
    "again we run the irm for 500 iterations , and with 500 as @xmath60 for each modality .",
    "the irm infers 408 user clusters and 380 url clusters for ` ul ` ; again well below @xmath60 .",
    "running the svd and nmf for a data set the size of ` ul`within acceptable times ( i.e. , within a day or less ) , is in it self a challenge and requires specialized software , either utilizing gpus or distributed computation ( or both ) . as we have not had immediate access to any implementations capable hereof , the svd and nmf decompositions of ` ul`remain as future work .",
    "hence , for click - through rate prediction on the full data set , we demonstrate only the benefit of using the irm cluster features over not using any dimensionality reduction .      for testing the various dimensionality reductions , we construct several training and testing data sets from rtb logs with observations labeled as click ( 1 ) or non - click ( 0 ) .",
    "the features we use are summarized in table [ table : predictors ] .",
    ".names and descriptions of the predictors used to predict click - through rates . [ cols=\"<,<,<\",options=\"header \" , ]      cl@(c@,c@,c@)ccccc@l + & model & @xmath61 & @xmath62 & @xmath63 & time ( s ) & nnz@xmath64 & nnz@xmath65 & ll@xmath66 & % lift + & @xmath67 & 0.8 & - & - & 9 & 3612 & - & 93.83 & 0.00 +   + & @xmath67 , @xmath68 & 0.8 & 10.6 & - & 91 & 3943 & 760 & 88.15 & 6.05 +   + & @xmath67 , @xmath69 & 0.8 & - & 6.0e-4 & 13 & 3653 & - & 90.19 & 3.88 + & @xmath67 , @xmath70 & 0.8 & - & 7.0e-4 & 16 & 3674 & - & * 89.84 * & * 4.25 * & @xmath71 + & @xmath72 & 0.8 & 15.4 & 7.0e-4 & 76 & 3861 & 366 & 87.78 & 6.45 + & @xmath67 , @xmath73 & 0.8 & - & 0.1 & 19 & 3479 & - & 89.87 & 4.22 + & @xmath67 , @xmath74 & 0.8 & - & 0.3 & 29 & 3502 & - & 89.73 & 4.37 + & @xmath67 , @xmath75 & 0.8 & - & 0.3 & 56 & 3552 & - & 89.73 & 4.37 + & @xmath67 , @xmath73 , @xmath76 & 0.8 & - & 7.0e-4 & 649 & 3409 & - & 89.15 & 4.99 + & @xmath67 , @xmath74 , @xmath77 & 0.8 & - & 7.0e-4 & 2487 & 3702 & - & * 88.92 * & * 5.23 * & @xmath78 + & @xmath67 , @xmath75 , @xmath79 & 0.8 & - & 1.2e-3 & 4082 & 4027 & - & 89.55 & 4.56 + & @xmath80 & 0.8 & 10.8 & 7.0e-4 & 3291 & 4063 & 484 & 87.90 & 6.32 + & @xmath67 , @xmath81 & 0.8 & - & 6.0e-3 & 30 & 3453 & - & 89.38 & 4.74 + & @xmath67 , @xmath82 & 0.8 & - & 3.0e-3 & 40 & 3467 & - & 89.15 & 4.99 + & @xmath67 , @xmath83 & 0.8 & - & 2.0e-3 & 45 & 3521 & - & 88.68 & 5.49 + & @xmath67 , @xmath81 , @xmath84 & 0.8 & - & 5.0e-3 & 151 & 3389 & - & 89.05 & 5.09 + & @xmath67 , @xmath82 , @xmath85 & 0.8 & - & 6.0e-3 & 392 & 3468 & - & * 87.89 * & * 6.33 * & @xmath86 + & @xmath67 , @xmath83 , @xmath87 & 0.8 & - & 4.0e-3 & 740 & 3635 & - & 93.59 & 0.26 + & @xmath88 & 0.8 & 11.2 & 6.0e-3 & 641 & 3973 & 680 & 86.91 & 7.38 +    for the sampled data the number of observations are as follows : @xmath89=138,847 and @xmath90=4,273 . in order to give the reader an idea about the dimensionalities of the features as well as their sparsity , in table [ table : pred_usage ]",
    "we summarize some numbers on the predictors on the sampled data set . for features @xmath67,@xmath69 , and @xmath91 , the number of non - zeros ( nnz ) and sparsities are somewhat trivial , since these are categorical features represented as one - of - k binary vectors . for the svd features , @xmath92 and @xmath93",
    ", we see that the feature vectors become completely dense . for",
    "the nmf features , however , we can confirm the methods ability to produce sparse components , i.e. , only between 20 - 33% of the components turn up as non - zeros , yet they are far from the sparsities of the irm cluster features , @xmath69 and @xmath91 .    cl@(c@,c@,c@)ccccc@l + & model & @xmath61 & @xmath62 & @xmath63 & time ( s ) & nnz@xmath64 & nnz@xmath65 & ll@xmath66 & % lift + & @xmath67 & 0.7 & - & - & 34 & 14152 & - & 91.76 & 0.00 +   + & @xmath67 , @xmath68 & 0.7 & 10.2 & - & 195 & 15673 & 3010 & 88.71 & 3.32 +   + & @xmath67 , @xmath70 & 0.7 & - & 1.2e-3 & 51 & 13604 & - & 89.35 & 2.63 & @xmath71 + & @xmath72 & 0.7 & 10.2 & 1.2e-3 & 293 & 16018 & 2939 & 88.19 & 3.89 +    in table [ table : results1 ] , we report the normalized likelihoods , lifts and test - set optimal regularization strengths @xmath61 and @xmath94 , with varying features used for training .",
    "the lifts are all relative to model @xmath67 . the penalization strength @xmath95 is selected as the one maximizing the performance of the classifier using only @xmath67 , and is kept fixed for all the other classifiers .",
    "note , that generalization of the penalization terms is an issue we do not currently address .",
    "the time reported in the table are the seconds it takes to train the logistic regression classifier .",
    "nnz@xmath64 and nnz@xmath65 are the respective number of non - zero weights of the resulting classifier for all the features and the @xmath68 feature only    in order to be able to further elaborate on the pros and cons of using the various dimensionality reduction techniques as features in the logistic regression classifier , we carry out another set of experiments for the models highlighted ( bold and marked @xmath96 ) in table [ table : results1 ] .",
    "we fix the values of @xmath61 and @xmath94 to the values from @xmath97 , and @xmath98 , respectively , and append @xmath68 as an additional feature with each model and then tune the regularization strength @xmath62 .",
    "the results are shown in the rows of table [ table : results1 ] with the symbols @xmath99 , and @xmath100 under `` model '' .",
    "the final experiment we run is with the full data set where we only evaluate the irm based features and compare those to not using any dimensionality reduction .",
    "the number of observations for train and test are @xmath89=5,460,229 and @xmath90=188,867 .",
    "the selection of regularization terms we do as in the previous experiments .",
    "the results are reported in table [ table : results_full ] .",
    "from table [ table : results1 ] we first concentrate on the best models from each dimensionality reduction , i.e. , the results highlighted in bold . comparing the lifts , we see that the nmf-300 features perform roughly one % -point better than the svd-300 features , which then in turn perform roughly another % -point better than the irm cluster features . comparing to the classifier using just @xmath67 and @xmath68 , i.e. , no dimensionality reduction , we see that only the nmf - based classifier achieves slightly higher lift .",
    "hence , using svd or irm based features as _ a replacement _ for the @xmath68 feature would result in worse predictions . seeing the number of non - zero weights dropping from 3943 using @xmath68 to 3468 using both nmf-300 features",
    ", indicates that the nmf offers a more economical representation which can replace @xmath68 while not sacrificing performance .",
    "the performance gain of nmf-300 we expect is achieved by the implicit data grouping effects of nmf , i.e. , recommender effects .    in terms of training speed",
    ", we see that while the irm based features fare worst in terms of lift , the fact that each mode is a categorical value represented in a one - of - k binary vector makes the input matrix very sparse , which speeds up the training of our classifier significantly and the model trains at least an order of magnitude faster than the other dimensionality reduction techniques and even significantly faster than training the nodr model . hence , if fast training is a priority , either no dimensionality reduction should be used or the irm based features can be used , but at the cost of slightly lower lift .",
    "we now turn to the results for the models @xmath99 , and @xmath100 in table [ table : results1 ] . here",
    "we investigate how the learning of weights for the high - cardinality feature @xmath68 is affected when combined with each of the optimal settings from the reduced dimension experiments .",
    "again , observing the lifts , the nmf-300 based features combined with @xmath68 obtains the highest lift .",
    "however , the irm based features now outperform the svd ones and using either of the techniques in combination with @xmath68 , we are able to obtain higher lifts than using only @xmath68 .    for the training speed , we again see that the training using irm features is by far the fastest amongst svd and nmf and it is still faster than using @xmath68 only . what is more interesting , is the resulting number of non - zero weights , both in total and in the @xmath68 feature alone .",
    "of all the different dimensionality reductions as well as nodr , using the irm based representation requires the fewest non - zero weights at its optimal settings . additionally , recalling from section [ sec : lr ] , that predictions can be made computationally very efficient ,",
    "when the input features are binary indicator vectors , the irm becomes all the more tractable . by combining the irm based features with the explicit predictors @xmath67 and @xmath68 , our classifier is able to improve the lift over not using dimensionality reduction while reducing the need for fetching many weights for predictions and with only a small reduction in lift , compared to the more computationally expensive classifiers based on nmf and svd .",
    "finally , in table [ table : results_full ] we have run experiments using just the irm based predictors with the full data set .",
    "the results confirm our findings from table [ table : results1 ] and at the same time demonstrates both the feasibility of processing very large bipartite graphs using irm as well as the application of the user and url clusters as predictors of click - through rates .",
    "we have presented results that demonstrate the use of three bimodal dimensionality reduction techniques , svd , nmf , and irm , and their applications as predictors in a click - through rate data set .",
    "we show that the compact representation based on the nmf is , in terms of predictive performance , the best option . for applications where fast predictions are required , however",
    ", we show that the binary representation from the irm model is a viable alternative .",
    "the irm based predictors yield the fastest training speed in the supervised learning stage , produces the most sparse model and offers the fastest computations at run - time , while incurring only a limited loss of lift relative to the nmf . in applications such as real - time bidding , where fast database i / o and few computations are key to success , we recommend using irm based features as predictors ."
  ],
  "abstract_text": [
    "<S> in online advertising , display ads are increasingly being placed based on real - time auctions where the advertiser who wins gets to serve the ad . </S>",
    "<S> this is called real - time bidding ( rtb ) . in rtb , </S>",
    "<S> auctions have very tight time constraints on the order of 100ms . therefore mechanisms for bidding intelligently such as click - through rate prediction need to be sufficiently fast . in this work </S>",
    "<S> , we propose to use dimensionality reduction of the user - website interaction graph in order to produce simplified features of users and websites that can be used as predictors of click - through rate . </S>",
    "<S> we demonstrate that the infinite relational model ( irm ) as a dimensionality reduction offers comparable predictive performance to conventional dimensionality reduction schemes , while achieving the most economical usage of features and fastest computations at run - time . for applications such as real - time bidding , where fast database i / o and few computations are key to success </S>",
    "<S> , we thus recommend using irm based features as predictors to exploit the recommender effects from bipartite graphs . </S>"
  ]
}