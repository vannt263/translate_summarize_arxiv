{
  "article_text": [
    "magnetic resonance imaging ( mri ) is a widely used technique for visualizing the structure and functioning of the body .",
    "a limitation of mri is its slow scan speed during data acquisition .",
    "therefore , methods for accelerating the mri process have received much research attention .",
    "recent advances in signal reconstruction from measurements sampled below the nyquist rate , called compressed sensing ( cs ) @xcite@xcite , have had a major impact on mri @xcite .",
    "cs - mri allows for significant undersampling in the fourier measurement domain of mr images ( called @xmath0-space ) , while still outputting a high - quality image reconstruction . while image reconstruction using this undersampled data is a case of an ill - posed inverse problem , compressed sensing theory has shown that it is possible to reconstruct a signal from significantly fewer measurements than mandated by traditional nyquist sampling if the signal is sparse in a particular transform domain .    motivated by the need to find a sparse domain for signal representation , a large body of literature now exists on reconstructing mri from significantly undersampled @xmath0-space data .",
    "existing improvements in cs - mri mostly focus on ( @xmath1 ) seeking sparse domains for the image , such as contourlets @xcite@xcite ; ( @xmath2 ) using approximations of the @xmath3 norm for better reconstruction performance with fewer measurements , for example @xmath4 , focuss , @xmath5 quasi - norms with @xmath6 , or using smooth functions to approximate the @xmath3 norm @xcite@xcite ; and ( @xmath7 ) accelerating image reconstruction through more efficient optimization techniques @xcite . in this paper",
    "we present a modeling framework that is similarly motivated .",
    "cs - mri reconstruction algorithms tend to fall into two categories : those which enforce sparsity directly within some image transform domain @xcite@xcite , and those which enforce sparsity in some underlying latent representation of the image , such as a dictionary learning representation @xcite@xcite .",
    "most cs - mri reconstruction algorithms belong to the first category .",
    "for example sparse mri @xcite , the leading study in cs - mri , performs mr image reconstruction by enforcing sparsity in both the wavelet domain and the total variation ( tv ) of the reconstructed image .",
    "algorithms with image - level sparsity constraints such as sparse mri typically employ an off - the - shelf basis , which can usually capture only one feature of the image .",
    "for example , wavelets recover point - like features , while contourlets recover curve - like features .",
    "since mr images contain a variety of underlying features , such as edges and textures , using a basis not adapted to the image can be considered a drawback of the algorithms in this group .",
    "finding a sparse basis that is suited to the image at hand can benefit mr image reconstruction , since cs theory shows that the required number of measurements is linked to the sparsity of the signal in the selected transform domain . using a standard basis not adapted to the image under consideration",
    "will likely not provide a representation that can compete in sparsity with an adapted basis . to this end ,",
    "dictionary learning , which falls in the second group of algorithms , learns a sparse basis on image subregions called patches that is adapted to the image class of interest .",
    "recent studies in the image processing literature have shown that dictionary learning is an effective means for finding a sparse representation of an image on the patch - level @xcite@xcite , @xcite .",
    "these algorithms learn a patch - level basis ( i.e. , dictionary ) by exploiting structural similarities between patches extracted from images within a class of interest ( for example bm3d @xcite , mod @xcite and k - svd @xcite ) . among these approaches ,",
    "adaptive dictionary learning  where the dictionary is learned directly on the image being considered  based on patch - level sparsity constraints usually outperforms analytical dictionary approaches in denoising , super - resolution reconstruction , interpolation , inpainting , classification and other applications , since the adaptively learned dictionary suits the signal of interest @xcite@xcite .",
    "dictionary learning has been applied to cs - mri as a sparse basis for reconstruction ( e.g. , lost @xcite and dlmri @xcite ) . with these methods , parameters such as the dictionary size and patch sparsity",
    "are preset , and algorithms are considered that are non - bayesian . in this paper , we consider a new dictionary learning algorithm for cs - mri that is based on bayesian nonparametric statistics .",
    "specifically , we consider the beta process as a nonparametric prior for a dictionary learning model that provides the sparse representation necessary for cs - mri reconstruction .",
    "the beta process is a method for generating measures on infinite parameter spaces that can be employed in latent factor models @xcite@xcite ; in this case the latent factors are the dictionary elements and the measure is a value in @xmath8 $ ] that gives the corresponding activation probability . while the dictionary is theoretically infinite in size , through posterior inference the beta process learns a representation that is both sparse in dictionary size and in the dictionary usage for any given patch .",
    "the proposed bayesian nonparametric model gives an alternative approach to dictionary learning for cs - mri reconstruction to those previously considered .",
    "we derive a markov chain monte carlo ( mcmc ) sampling algorithm for stochastic optimization of the dictionary learning variables in the objective function .",
    "in addition , we consider including a sparse total variation ( tv ) penalty , for which we perform efficient optimization using the alternating direction method of multipliers ( admm ) .",
    "we organize the paper as follows . in section [ sec.background ]",
    "we review cs - mri inversion methods and the beta process for dictionary learning . in section [ sec : proposed ] , we describe the proposed regularization framework and optimization algorithm .",
    "we then show the advantages of the proposed bayesian nonparametric regularization framework on several cs - mri problems in section [ sec.experiments ] .",
    "we use the following notation : let @xmath9 be a @xmath10 mr image in vectorized form . let @xmath11 , @xmath12 , be the undersampled fourier encoding matrix and @xmath13 represent the sub - sampled set of @xmath0-space measurements .",
    "the goal is to estimate @xmath14 from the small fraction of @xmath0-space measurements @xmath15 . for dictionary learning ,",
    "let @xmath16 be the @xmath1th patch extraction matrix .",
    "that is , @xmath16 is a @xmath17 matrix of all zeros except for a one in each row that extracts a vectorized @xmath18 patch from the image , @xmath19 for @xmath20 .",
    "we work with overlapping image patches with a shift of one pixel and allow a patch to wrap around the image at the boundaries for mathematical convenience @xcite@xcite .",
    "we focus on cs - mri inversion via optimizing an unconstrained function of the form @xmath21 where @xmath22 is a data fidelity term , @xmath23 is a parameter and @xmath24 is a regularization function that controls properties of the image we want to reconstruct . as discussed in the introduction",
    ", the function @xmath25 can take several forms , but tends to fall into one of two categories according to whether image - level or patch - level information is considered .",
    "we next review these two approaches .",
    "cs - mri with an image - level , or global regularization function @xmath26 is one in which sparsity is enforced within a transform domain defined on the entire image . for example , in sparse mri @xcite the regularization function is @xmath27 where @xmath28 is the wavelet basis and @xmath29 is the total variation ( spatial finite differences ) of the image . regularizing with this function requires that the image be sparse in the wavelet domain , as measured by the @xmath4 norm of the wavelet coefficients @xmath30 , which acts as a surrogate for @xmath3 @xcite@xcite .",
    "the total variation term enforces homogeneity within the image by encouraging neighboring pixels to have similar values while allowing for sudden high frequency jumps at edges .",
    "the parameter @xmath31 controls the trade - off between the two terms .",
    "various other definitions of @xmath26 have also been proposed for mri reconstruction , which we briefly summarize .",
    "examples are over - complete contourlets @xcite , a combination of wavelets , contourlets and tv @xcite , and regularization of wavelet coefficient correlations based on gaussian scale mixtures @xcite .",
    "other methods replace the @xmath4 norm with approximations of the @xmath3 norm , for example focuss @xcite@xcite , @xmath5 norms @xcite , and homotopic @xmath3 minimization @xcite . numerical algorithms for optimizing ( [ eqn.objective ] ) with an image - level @xmath26",
    "include nonlinear conjugate gradient descent with backtracking line search @xcite , an operator - splitting algorithm ( tvcmri ) @xcite and a variable splitting method ( recpf ) @xcite .",
    "both tvcmri and recpf can replace iterative linear solvers with fourier domain computations , with substantial time savings .",
    "other methods in the literature include a combination of variable and operator splitting techniques @xcite , a fast composite splitting algorithm ( fcsa ) @xcite , a contourlet transform with iterative soft thresholding @xcite , a combination of gaussian scale mixture model with iterative hard thresholding @xcite , a variation on bregman operator splitting ( bos ) @xcite and alternating proximal minimization applied to the tv - based sense problem @xcite .",
    "the above algorithms generally employ variable and operator splitting techniques with the fft and alternating minimization to simplify the object function . in this work ,",
    "we follow a similar approach for total variation minimization .",
    "an alternative to the image - level sparsity constraint @xmath26 is a patch - level , or local regularization function @xmath32 , which enforces sparsity in a transform domain defined on patches ( square sub - regions of the image ) extracted from the full image .",
    "an example of such a regularization function is , @xmath33 where the dictionary matrix is @xmath34 and @xmath35 is a @xmath36-dimensional vector .",
    "an important difference between @xmath32 and @xmath26 is the additional function @xmath37 .",
    "while image - level sparsity constraints fall within a predefined transform domain , such as the wavelet basis , the sparse transform domain can be unknown for patch - level regularization and learned from data .",
    "the function @xmath38 enforces sparsity by learning a @xmath39 for which @xmath35 is sparse . and @xmath39 in @xmath32 .",
    "] for example , @xcite uses k - svd to learn @xmath39 off - line , and then approximately optimize the objective function @xmath40 using orthogonal matching pursuits ( omp ) @xcite .",
    "( note that this objective can be written using @xmath41 for some @xmath42 . ) in this case , the extra parameters @xmath35 are included in the objective function ( [ eqn.objective ] ) , and so the problem is no longer convex . using this definition of @xmath32 in ( [ eqn.objective ] )",
    ", a local optimal solution can be found by an alternating minimization procedure : first solve the least squares solution for @xmath14 using the current values of @xmath35 and @xmath39 , and then update @xmath35 and @xmath39 , or only @xmath35 if @xmath39 is learned off - line .",
    "the dictionary learning step can be thought of as a denoising procedure .",
    "that is , the combination of each @xmath43 in effect produces a denoised `` proposal reconstruction '' for @xmath14 , after which the reconstruction takes into account the squared error from this smooth proposal and from the sub - sampled @xmath0-space , with weight determined by the regularization parameters .    aside from sparse dictionary learning ,",
    "other patch - level algorithms have been reported .",
    "for example , regularization of patches in a spatial region with a robust distance metric @xcite , patch clustering followed by de - aliasing and artifact removal for reconstruction using 3dfft ( lost ) @xcite or directional wavelets @xcite .",
    "these methods each take into account similarities between image patches in determining the dictionary .",
    "next , we review our method for dictionary learning by using a bayesian nonparametric prior called the beta process .",
    "typical dictionary learning approaches require a predefined dictionary size and , for each patch , the setting of either a sparsity level @xmath44 , or an error threshold @xmath45 to determine how many dictionary elements are used . in both cases , if the settings do not agree with ground truth , the performance can significantly degrade .",
    "instead , we consider a bayesian nonparametric method called beta process factor analysis ( bpfa ) @xcite , which has been shown to successfully infer both of these values , as well as have competitive performance with algorithms in several application areas @xcite@xcite , and see @xcite@xcite for related algorithms .",
    "the beta process is driven by an underlying poisson process , and so it s properties as a stochastic process for bayesian modeling are well understood @xcite .",
    "originally used for survival analysis in the statistics literature , its use for latent factor modeling has been significantly increasing within the machine learning field @xcite@xcite,@xcite,@xcite@xcite .",
    "being a bayesian method , the prior definition of our proposed model gives a way ( in principle ) of generating images .",
    "writing the generative method for bpfa gives an informative picture of what the algorithm is doing and what assumptions are being made . to construct an image with the proposed model",
    ", we use the generative structure given in algorithm [ alg.bpfa ] .    1 .",
    "construct a dictionary @xmath46 $ ] : @xmath47 2 .",
    "draw a probability @xmath48 $ ] for each @xmath49 : @xmath50 3 .",
    "draw precision values for noise and each weight @xmath51 4 .   for the @xmath1th patch in @xmath14 : 1 .",
    "draw the vector @xmath52 2 .",
    "draw the binary vector @xmath53 with @xmath54 3 .",
    "define @xmath55 by an element - wise product .",
    "4 .   construct the patch @xmath56 with noise @xmath57 .",
    "5 .   construct the image @xmath14 as the average of all @xmath58 that overlap on a given pixel .    with this approach ,",
    "the model constructs a dictionary matrix @xmath59 of i.i.d .",
    "random variables , and assigns probability @xmath60 to vector @xmath49 .",
    "the parameters for these probabilities are set such that most of the @xmath60 are expected to be small , with a few large . in algorithm [ alg.bpfa ]",
    "we use an approximation to the beta process ; for a fixed @xmath61 and @xmath62 , convergence is guaranteed as @xmath63 @xcite@xcite . under this parameterization , each patch @xmath64 extracted from the image @xmath14 is modeled as a sparse weighted combination of the dictionary elements , as determined by the element - wise product of @xmath65 with the gaussian vector @xmath66 .",
    "what makes the model nonparametric is that for many values of @xmath0 , the values of @xmath67 will equal zero for all @xmath1 ; the model learns the number of these unused dictionary elements and their index values from the data .",
    "the independent bernoulli random variables ensure values of zero for the @xmath0th element of each @xmath53 when @xmath60 is very small , and thereby eliminates @xmath49 from the model .",
    "therefore , the value of @xmath36 should be set to a large number that is more than the expected size of the dictionary .",
    "it can be shown that under the assumptions of this prior , in the limit @xmath63 , the number of dictionary elements used by a patch is poisson@xmath68 distributed and the total number of dictionary elements used by the data grows like @xmath69 , where @xmath70 is the number of patches @xcite .",
    "another widely used dictionary learning method is k - svd @xcite .",
    "though they are models for the same problem , bpfa and k - svd have some significant differences that we briefly discuss .",
    "k - svd learns the sparsity pattern of the coding vectror @xmath35 using the omp algorithm @xcite for each @xmath1 .",
    "holding the sparsity fixed , it then updates each dictionary element and dimension of @xmath71 jointly by a rank one approximation to the residual .",
    "bpfa on the other hand updates the sparsity pattern by generating from a beta posterior distribution and generates weights and the dictionary from gaussian posterior distributions using bayes rule .",
    "because of this probabilistic structure , we derive a sampling algorithm for these variables that takes advantage of marginalization , and naturally learns the auxiliary variables @xmath72 and @xmath73 .",
    "we briefly illustrate bpfa on a denoising problem using @xmath74 patches extracted from a @xmath75 image and setting @xmath76 . in figures [ fig : subfig1:a ] and [ fig : subfig2:b ] we show the noisy and denoised images . in figures [ fig : subfig2:c ] and [ fig : subfig2:d ] we show some statistics from dictionary learning .",
    "for example , figure [ fig : subfig2:c ] shows the values of @xmath60 sorted , where we see that fewer than 100 elements are used by the data .",
    "figure [ fig : subfig2:d ] shows the empirical distribution of the number of elements per patch , where we see the ability of the model to adapt the sparsity to the patch . in table [ tab.denoising ] we show psnr results for three noise variance levels . for k - svd",
    ", we consider the case when the error parameter matches the ground truth , and when it mismatches it by a magnitude of five . as expected ,",
    "when k - svd does not have an appropriate setting of this value the performance suffers .",
    "bpfa on the other hand can adaptively infer the noise variance which leads to an improvement in denoising .",
    ".peak signal - to - noise ratio ( psnr ) for image denoised by bpfa and k - svd .",
    "performance is comparable when the noise parameter of k - svd is correct ( match ) .",
    "bpfa outperforms k - svd when this setting is wrong ( mismatch).[tab.denoising ] [ cols=\"^,^,^,^,^ \" , ]",
    "we next present our regularization scheme for reconstructing mr images from highly undersampled @xmath0-space data . in reference to the discussion in section [ sec.background ]",
    ", we consider a sparsity constraint of the form @xmath77 @xmath78 for the local regularization function @xmath32 we use bpfa as given in algorithm [ alg.bpfa ] in section [ sec.bpfa ] .",
    "the parameters to be optimized for this penalty are contained in the set @xmath79 , and are defined in algorithm [ alg.bpfa ] .",
    "the regularization term @xmath72 is a model variable that corresponds to an inverse variance parameter of the multivariate gaussian likelihood .",
    "this likelihood is equivalently viewed as the squared error penalty term in ( [ eqn.our_objective ] ) .",
    "this term acts as the sparse basis for the image and also aids in producing a denoised reconstruction , as discussed in section [ sec.bpfa ] .",
    "( we indicate how to construct the analytical form of @xmath38 in the appendix . ) for the global regularization function @xmath26 we use the total variation of the image .",
    "this term encourages homogeneity within contiguous regions of the image , while still allowing for sharp jumps in pixel value at edges due to the underlying @xmath4 penalty .",
    "the regularization parameters @xmath80 , @xmath72 and @xmath81 control the trade - off between the terms in this optimization , which is adaptively learned since @xmath72 changes with each iteration .    for the total variation penalty",
    "@xmath29 we use the isotropic tv model .",
    "let @xmath82 be the @xmath83 difference operator for pixel @xmath1 .",
    "each row of @xmath82 contains a @xmath84 centered on pixel @xmath1 , and @xmath85 on the pixel directly above pixel @xmath1 ( for the first row of @xmath82 ) or to the right ( for the second row of @xmath82 ) , and zeros elsewhere .",
    "let @xmath86^t$ ] be the resulting @xmath87 difference matrix for the entire image .",
    "the tv coefficients are @xmath88 , and the isotropic tv penalty is @xmath89 , where @xmath1 ranges over the pixels in the mr image . for optimization",
    "we use the alternating direction method of multipliers ( admm ) @xcite@xcite .",
    "admm works by performing dual ascent on the augmented lagrangian objective function introduced for the total variation coefficients . for completeness",
    ", we give a brief review of admm in the appendix .",
    "we present an algorithm for finding a local optimal solution to the non - convex objective function given in ( [ eqn.our_objective ] ) .",
    "we can write this objective as @xmath90 @xmath91 we seek to minimize this function with respect to @xmath14 and the dictionary learning variables @xmath79 .",
    "our first step is to put the objective into a more suitable form .",
    "we begin by defining the tv coefficients for the @xmath1th pixel as @xmath92^t = \\psi_i{\\mbox{\\textit{x}}}$ ] .",
    "we introduce the vector of lagrange multipliers @xmath93 , and then split @xmath94 from @xmath95 by relaxing the equality via an augmented lagrangian .",
    "this results in the objective function @xmath96 from the admm theory , this objective will have ( local ) optimal values @xmath97 and @xmath98 with @xmath99 , and so the equality constraints will be satisfied . and",
    "@xmath100 , the solution is also globally optimal . ] optimizing this function can be split into three separate sub - problems : one for tv , one for bpfa and one for updating the reconstruction @xmath14 .",
    "following the discussion of admm in the appendix , we define @xmath101 and complete the square in the first line of ( [ eqn.objective2 ] ) .",
    "we then cycle through the following three sub - problems , @xmath102    for each sub - problem , we use the most recent values of all other parameters . solutions for @xmath103 and @xmath104 are globally optimal and in closed form , while the update for @xmath105 follows from admm .",
    "since @xmath106 is non - convex , we can not perform the desired minimization , and so an approximation is required .",
    "furthermore , this problem requires iterating through the several dictionary learning variables of bpfa , and so a local optimal solution can not be given either .",
    "our approach is to use stochastic optimization for problem @xmath106 by gibbs sampling each variable in bpfa conditioned on current values of all other variables .",
    "we next present the updates for each sub - problem , and give an outline in algorithm [ alg.basic ] .",
    "input : @xmath15  undersampled @xmath0-space data +    output : @xmath14  reconstructed mr image +    initialize @xmath107 ( zero filling ) , and @xmath108 .",
    "initialize bpfa variables using @xmath14 .",
    "+    solve @xmath103 sub - problem by optimizing @xmath109 via shrinkage .",
    "+    update @xmath106 sub - problem by gibbs sampling bpfa variables .",
    "+    solve @xmath104 sub - problem in fourier domain , followed by inverse transform .",
    "+    update lagrange multiplier vector @xmath110 .",
    "+    * if * _ not converged _ *",
    "then * _ return to step 2 .",
    "_      we can solve for @xmath94 exactly for each pixel @xmath111 by using a generalized shrinkage operation @xcite , @xmath112 we recall that @xmath94 corresponds to the 2-dimensional tv coefficients for pixel @xmath1 , with differences in one direction vertically and horizontally .",
    "these coefficients have been been split from @xmath95 using admm , but gradually converge to one another and become equal in the limit .",
    "we recall that after updating @xmath14 , we update the lagrange multiplier @xmath113 .",
    "we update the parameters of bpfa using gibbs sampling .",
    "we are therefore stochastically optimizing ( [ eqn.objective2 ] ) , but only for this sub - problem .",
    "with reference to algorithm [ alg.bpfa ] , the p2 sub - problem entails sampling new values for the dictionary @xmath39 , the binary vectors @xmath114 and weights @xmath115 , with which we construct @xmath116 through the element - wise product , the precisions @xmath72 and @xmath117 , and the beta probabilities @xmath118 , which give the probability that @xmath119 . in principle , there is no limit to the number of samples that can be made , with the final sample giving the updates used in the other sub - problems .",
    "we found that a single sample is sufficient in practice and leads to a faster algorithm .",
    "the samples we make are given below .",
    "[ [ sample - dictionary - d ] ] sample dictionary @xmath39 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we define the @xmath17 matrix @xmath120 $ ] , which is the matrix of all vectorized patches extracted from the image @xmath14 .",
    "we also define the @xmath121 matrix @xmath122 $ ] containing the dictionary weight coefficients for the corresponding columns in @xmath123 such that @xmath124 is an approximation of @xmath123 prior to additive gaussian noise .",
    "the update for the dictionary @xmath39 is @xmath125 we note that the first term in equation ( [ eqn.dict_up ] ) is the @xmath126-regularized least squares solution for @xmath39 .",
    "correlated gaussian noise is then added to generate a sample from the conditional posterior of @xmath39 . since both",
    "the number of pixels and @xmath72 will tend to be very large , the variance of the noise is small and the mean term dominates the update for @xmath39 .",
    "[ [ sample - sparse - coding - alpha_i ] ] sample sparse coding @xmath35 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    sampling @xmath35 entails sampling @xmath127 and @xmath67 for each @xmath0 .",
    "we sample these values using block sampling .",
    "we recall that to block sample two variables from their joint conditional posterior distribution , @xmath128 , one can first sample @xmath129 from the marginal distribution , @xmath130 , and then sample @xmath131 from the conditional distribution .",
    "the other sampling direction is possible as well , but for our problem sampling @xmath132 is more efficient in finding a mode of the objective function .",
    "we define @xmath133 to be the residual error in approximating the @xmath1th patch with the current values from bpfa minus the @xmath0th dictionary element , @xmath134 .",
    "we then sample @xmath67 from its conditional posterior bernoulli distribution @xmath135 , where following a simplification , @xmath136 we observe that the probability that @xmath119 takes into account how well dictionary element @xmath49 correlates with the residual @xmath137 . after sampling @xmath67",
    "we sample the corresponding weight @xmath127 from its conditional posterior gaussian distribution , @xmath138 when @xmath139 , the mean of @xmath127 is the regularized least squares solution and the variance will be small if @xmath72 is large . when @xmath140 , @xmath127 is sampled from the prior .",
    "does not factor into the model in this case , since @xmath141 and @xmath127 is integrated out the next time @xmath67 is sampled . ]    [ [ sample - gamma_varepsilon - and - gamma_sk ] ] sample @xmath72 and @xmath117 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we next sample from the conditional gamma posterior distribution of the noise precision and weight precision , @xmath142 the expected value of each variable is the first term of the distribution divided by the second , which is close to the inverse of the average empirical error for @xmath72 .    [",
    "[ sample - pi_k ] ] sample @xmath60 + + + + + + + + + + + + + + + + + + + + + + + + + +    the conditional posterior of @xmath60 is a beta distribution sampled as follows , @xmath143 the parameters to the beta distribution include counts of how many times dictionary element @xmath49 was used by a patch .",
    "the final sub - problem is to reconstruct the image @xmath14 .",
    "our approach takes advantage of the fourier domain similar to other methods , e.g.  @xcite .",
    "the corresponding objective function is @xmath144 @xmath145 since this is a least squares problem , @xmath14 has a closed form solution that satisfies @xmath146 @xmath147    we recall that @xmath148 is the matrix of stacked @xmath82 .",
    "the vector @xmath109 is also obtained by stacking each @xmath94 , and similarly @xmath110 is the vector formed by stacking @xmath105 .",
    "the vector @xmath149 is the proposed reconstructed image from bpfa using the current @xmath39 and @xmath100 , which results from the equality @xmath150 .",
    "we observe that inverting the left @xmath151 matrix is computationally prohibitive , since @xmath70 is the number of pixels in the image .",
    "fortunately , given the form of the matrix in equation ( [ eqn.p3_1 ] ) we can simplify the problem by working in the fourier domain , which allows for element - wise updates in @xmath0-space , followed by an inverse fourier transform .",
    "we represent @xmath14 as @xmath152 , where @xmath153 is the fourier transform of @xmath14 and @xmath154 denotes the conjugate transpose .",
    "we then take the fourier transform of each side of equation ( [ eqn.p3_1 ] ) to give @xmath155 @xmath156 the left - hand matrix simplifies to a diagonal matrix , @xmath157 @xmath158 term - by - term this results as follows : the product of the finite difference operator matrix @xmath148 with itself yields a circulant matrix , which has the rows of the fourier matrix @xmath159 as its eigenvectors and eigenvalues @xmath160 .",
    "the matrix @xmath161 is a matrix of all zeros , except for ones on the diagonal entries that correspond to the indices of @xmath14 associated with the @xmath1th patch .",
    "since each pixel appears in @xmath162 patches , the sum over @xmath1 gives @xmath163 , and the fourier product cancels .",
    "the final diagonal matrix @xmath164 also contains all zeros , except for ones along the diagonal corresponding to the indices in @xmath0-space that are measured , which results from @xmath165 .    since the left matrix is diagonal we can perform element - wise updating of the fourier coefficients @xmath153 , @xmath166 we observe that the rightmost term in the numerator and denominator equals zero if @xmath1 is not a measured @xmath0-space location .",
    "we invert @xmath153 via the inverse fourier transform @xmath167 to obtain the reconstructed mr image @xmath168 .",
    "( left ) cartesian mask , ( right ) radial mask . ]",
    "we note that a feature of dictionary learning approaches is that @xmath81 can be allowed to go to infinity , and so parameter selection is nt necessary here .",
    "this is because a denoised reconstruction of the image is obtained through the dictionary learning reconstruction . in reference to equation ( [ eqn.k_space ] )",
    ", we observe that in this case we are fixing the measured @xmath0-space values and using the @xmath0-space projection of bpfa and tv to fill in the missing values .",
    "we present experimental results on synthetic data and the mri shown in figure [ fig.ground_truth ] .",
    "we consider a variety of sampling rates and masks , and compare with four other algorithms : sparsemri @xcite , pbdw @xcite , tv @xcite and dlmri @xcite .",
    "we use the publicly available code for these algorithms and tried several parameter settings , selecting the best ones for comparison .",
    "we also compare with bpfa without using total variation , which is a special case of our algorithm with @xmath169 .",
    "we consider two sampling trajectories in @xmath0-space corresponding to the two practical approaches to cs - mri : cartesian sampling with random phase encodes and radial sampling .",
    "we also considered random sampling and found comparable results , with reconstruction improved for each algorithm , as expected from cs theory . since this is not a practical sampling method we omit these results . in the first scheme ,",
    "measurement trajectories are sampled from a variable density cartesian grid and in the second we measure along radial lines uniformly spaced in angle .",
    "we show examples of these trajectories in figure [ fig.masks ] .",
    "we considered several subsampling rates for each trajectory , measuring 10% , 20% , 25% , 30% , and 35% of @xmath0-space . as a performance measure we use the peak signal - to - noise ratio ( psnr ) to the ground truth image , in addition to showing qualitative performance comparisons .",
    "for all images , we extract @xmath74 patches where each pixel defines the upper left corner of a patch and wrap around the image at the boundaries . for the synthetic data we learn",
    "complex - valued dictionaries , while for the mri data we restrict the model to real - valued dictionaries .",
    "we initialize @xmath14 by zero - filling in @xmath0-space .",
    "we use a dictionary with @xmath76 initial dictionary elements , recalling that the final number of dictionary elements will be smaller due to the sparse bpfa prior . if @xmath170 is found to be too small , @xmath36 can be increased with the result being a slower inference algorithm .",
    "( in principle @xmath36 can be infinitely large . )",
    "we ran @xmath171 iterations of the algorithm and show the results of the last iteration .    for regularization parameters of our model",
    ", we set the data fidelity regularization @xmath172 .",
    "we are therefore treating @xmath81 as effectively being infinity and allowing bpfa to fill in the missing @xmath0-space and denoise , as discussed in section [ sec.lambda ] . we also set @xmath173 and @xmath174 .",
    "for bpfa we set @xmath175 , @xmath176 , @xmath177 , @xmath178 , @xmath179 where @xmath180 is the empirical variance of the initialization .      in figure [ fig.gephantom ]",
    "we show results on the @xmath181 ge phantom with additive noise having standard deviation @xmath182 . in this experiment",
    "we use bpfa without tv to reconstruct the original image using @xmath183 cartesian sampling .",
    "we show the reconstruction using zero - filling in figure [ fig.gephantom](a ) . since @xmath172 , we see in figure [ fig.gephantom](b ) that bpfa essentially helps reconstruct the underlying noisy image for @xmath14 .",
    "however , using the denoising property of the bpfa model shown in figure [ fig : subfig2 ] , we obtain the denoised reconstruction of figure [ fig.gephantom](c ) by focusing on @xmath184 from equation ( [ eqn.p3_1 ] ) .",
    "this is in contrast with the best result we could obtain with tv in figure [ fig.gephantom](d ) , which places the sparse penalty directly on the reconstructed image . for tv the value of @xmath81",
    "relative to the regularization parameter becomes significant .",
    "we set @xmath185 and swept through values in @xmath186 for the tv regularization parameter .",
    "similar to figure [ fig : subfig2 ] we show some statistics from the bpfa model in figures [ fig.gephantom](e)-(g ) .",
    "roughly 80 dictionary elements were used , and an average of @xmath187 elements were used by a patch given that at least one was used ( which discounts the black regions ) .",
    "we next evaluate the performance of our algorithm using the mri shown in figure [ fig.ground_truth ] .",
    "as mentioned , we compare our algorithm with sparse mri @xcite , which is a combination of wavelets and total variation , tv @xcite using the isotropic model , dlmri @xcite , which is a dictionary learning model based on k - svd , and pbdw @xcite , which is patch - based method that uses directional wavelets , and therefore places greater restrictions on the dictionary . in all algorithms",
    ", we considered several parameter settings and picked the best results for comparison .",
    "in addition we consider our algorithm with and without the total variation penalty , denoted bpfa+tv and bpfa , respectively .",
    "we present quantitative and qualitative results for the reconstruction algorithms in figures [ fig.psnr_circle][fig.psnr_brain ] . in these figures ,",
    "we show the peak signal to noise ratio ( psnr ) for cartesian and radial sampling as a function of percentage sampled in @xmath0-space .",
    "we see that the proposed bayesian nonparametric dictionary learning method gives an improved reconstruction .",
    "we also see additional slight improvement when a tv penalty is added , though this is not always the case .",
    "given the denoising property of dictionary learning , this is perhaps not surprising .",
    "we also observe that radial sampling performed better than cartesian sampling in all experiments .",
    "we again note that we performed similar experiments using random sampling and observed similar relative results with an overall improvement compared with radial sampling , but we omit these results for space , and because random sampling is not practical for mri .    in each figure we also show example reconstructions for the algorithms considered , including zero - filling in @xmath0-space . in some mri , such as the circle of willis in figure [ fig.psnr_circle ] , the improvement is less in the structural information and more in image quality . in other mri ,",
    "the proposed method is able to capture structure in the image that is missed by the other algorithms . in figure [ fig.psnr_lumbar](a )",
    "we indicate one of these regions for the shoulder mri . in figure",
    "[ fig.residuals ] we show the residual errors ( in absolute value ) for several algorithms on the shoulder mri .",
    "( note that these images correspond to a different sampling pattern than in figure [ fig.psnr_shoulder ] . ) in this example we see that the errors for bpfa are more noise - like than for the other algorithms .",
    "the proposed method has several advantages , which we believe leads to the improvement in performance .",
    "a significant advantage is the adaptive learning of the dictionary size and per - patch sparsity level using a nonparametric stochastic process that is naturally suited for this problem .",
    "in addition to this , several other parameters such as the noise variance and the variances of the score weights are adjusted through a natural mcmc sampling approach .",
    "also , the regularization introduced by the prior helps prevent over - fitting , which is important since in the first several iterations bpfa is modeling an mri reconstruction that is significantly distorted .",
    "another advantage of our model is the markov chain monte carlo inference algorithm . in highly non - convex bayesian models ( or similar models with a bayesian interpretation ) , it is generally observed by the statistics community that mcmc sampling outperforms deterministic methods . given that bpfa is a bayesian model , such inference / optimization techniques are readily derived , as we showed in section [ sec.algorithm ] .",
    "a drawback of mcmc is that more iterations are required than deterministic methods ( we used 1000 iterations requiring approximately 1.5 hours , whereas the other algorithms required under 100 ) .",
    "however , we note that inference for the bpfa model is easily parallelizable , which can mitigate this problem .",
    "we next investigate the model learned by bpfa . in figure [ fig.example_bpfa ]",
    "we show dictionary learning results learned by bpfa+tv for radial sampling of the circle of willis . in the top portion",
    ", we show the dictionaries learned for 10% , 20% and 30% sampling .",
    "we see that they are consistent , but the number of elements increases as the sampling percentage increases , since more complex information is contained in the @xmath0-space measurements of the image .",
    "this is also shown in figure [ fig.example_bpfa](d ) . in this plot",
    "we show the cumulative sum of the ordered @xmath60 from bpfa .",
    "we can read off the average number of elements used per patch by looking at the right - most value .",
    "we see that more elements are used per patch as the fraction of observed @xmath0-space increases .",
    "we also see that for 10% , 20% and 30% sampling , roughly 60 , 80 and 95 , respectively , of the 108 total dictionary elements were significantly used , as indicated by the leveling off of these functions .",
    "this highlights the adaptive property of the nonparametric beta process prior . in figure",
    "[ fig.example_bpfa](e ) we show the empirical distribution on the number of dictionary elements used per patch for each sampling rate .",
    "we see that there are two modes , one for the empty background and one for the foreground , and the second mode tends to increase as the sampling rate increases .",
    "the adaptability of this value to each patch is another characteristic of the beta process model .",
    "we note that these results are typical of what we observed in the other experiments .",
    "we initialized the image using zero - filling and initialized the first 36 dictionary elements using the singular vectors of the patches from this image .",
    "we then randomly sampled the remaining dictionary elements from the prior .",
    "we initialized @xmath188 for all @xmath1 and @xmath0 , and @xmath189 .",
    "as mentioned , for the @xmath190 prior on the inverse noise variance of the patch , we set @xmath178 and @xmath179 , where @xmath180 is the empirical noise variance of the zero - filled image .",
    "this gives a prior expected noise of @xmath191 .",
    "here , @xmath192 indicates that the prior is @xmath193 the strength of the likelihood , and @xmath194 indicates that the prior expects a snr of 8 to 1 with respect to the zero - filled image .",
    "the purpose of this is that the mri we consider have very little noise and so using a non - informative prior ( where @xmath195 ) would cause dictionary learning to fit the early reconstructions tightly by correctly learning that there is very little noise .",
    "while we still observed good results , the convergence was very slow .",
    "strengthening the prior enforces a more smooth reconstruction in the early stages of inference .",
    "we note that for more significant levels of noise , such as our examples in sections [ sec.denoising ] and [ sec.simulated ] , this issue did not arise and non - informative priors could be used .",
    "we note that the added computation time for the tv penalty is very small compared with dictionary learning ; the total amount of time required for one iteration was between 5 and 6 seconds for the bpfa+tv model .",
    "this is significantly faster than dlmri , since our sampling approach is much less computationally intensive than the omp algorithm , which requires matrix inversions , but slower than the other algorithms we compare with .",
    "we have presented an algorithm for cs - mri reconstruction that uses bayesian nonparametric dictionary learning .",
    "our bayesian approach uses a model called beta process factor analysis ( bpfa ) for _ in situ _ dictionary learning . through this hierarchical generative structure",
    ", we can learn the dictionary size , sparsity pattern and additional regularization parameters .",
    "we also considered a total variation penalty term for additional constraints on image smoothness .",
    "we presented an optimization algorithm using the alternating direction method of multipliers ( admm ) and mcmc gibbs sampling for all bpfa variables .",
    "experimental results on several mr images showed that our proposed regularization framework compares favorably with other algorithms for various sampling trajectories and rates .",
    "we give some additional details of the bayesian structure of our dictionary learning approach .",
    "the unknown variables of the model are @xmath196 , @xmath197 , @xmath198 , @xmath199 , @xmath72 , @xmath200 .",
    "the `` data '' from the perspective of bpfa is the set of patches extracted from the current reconstruction , @xmath201 .",
    "the joint likelihood of these variables and data is @xmath202 @xmath203\\times\\ ] ] @xmath204p(\\gamma_\\varepsilon)\\textstyle\\prod_k p(\\gamma_{s , k}).\\ ] ]    the first bracketed group constitutes the patch - specific part of the likelihood .",
    "the second group contains the dictionary elements and their probabilities and the remaining distributions are for inverse variances .",
    "the specific distributions used are given in algorithm [ alg.bpfa ] . by writing out these distributions explicitly , the functional form of the joint likelihood",
    "can be obtained .",
    "the dictionary learning part of the objective function , which corresponds to sub - problem p2 , is @xmath205 @xmath206    optimizing this non - convex function is equivalent to finding a mode of the joint likelihood . rather than use a deterministic gradient - based method",
    ", we use the mcmc gibbs sampling to stochastically find a mode .",
    "the functional form is unnecessary for deriving the gibbs sampling algorithm .",
    "we note that many of the updates are essentially noisy versions of regularized least squares solutions .      to review the general form of admm @xcite we are interested in , we start with the convex optimization problem @xmath207 where @xmath25 is a non - smooth convex function , such as an @xmath4 penalty .",
    "admm decouples the smooth squared error term from this penalty by introducing a second vector @xmath208 such that @xmath209 this is followed by a relaxation of the equality @xmath210 via an augmented lagrangian term @xmath211 a minimax saddle point is found with the minimization taking place over both @xmath14 and @xmath208 and dual ascent for @xmath212 .",
    "another way to write the objective in ( [ eqn.aug_lag ] ) is to define @xmath213 and combine the last two terms .",
    "the result is an objective that can be optimized by cycling through the following updates for @xmath14 , @xmath208 and @xmath110 , @xmath214 this algorithm simplifies the optimization since the objective for @xmath14 is quadratic and thus has a simple analytic solution , while the update for @xmath208 is a proximity operator of @xmath25 with penalty @xmath215 , the difference being that @xmath208 is not pre - multiplied by a matrix as @xmath14 is in ( [ eqn.admm_generic ] ) .",
    "such optimization problems tend to be much easier to solve ; for example when @xmath25 is the tv penalty the solution for @xmath208 is analytical .",
    "e. cands , j. romberg , and t. tao , `` robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information , '' _ ieee trans .",
    "on information theory _ , vol .",
    "489 - 509 , 2000 . ,",
    "`` compressed sensing , '' _ ieee trans . on information theory _ ,",
    "1289 - 1306 , 2006",
    ". m. lustig , d. donoho , and j. m. pauly , `` sparse mri : the application of compressed sensing for rapid mr imaging , '' _ magnetic resonance in medicine _",
    "58 , no . 6 , pp . 1182 - 1195 , 2007 .",
    "y. kim , m. s. nadar , and a. bilgin , `` wavelet - based compressed sensing using gaussian scale mixtures , '' _ ieee trans . on image processing",
    "3108 - 3108 , 2012 .",
    ", `` iterative thresholding compressed sensing mri based on contourlet transform , '' _ inverse problems sci .",
    "_ , jun . 2009 . ,",
    "`` combined sparsifying transforms for compressed sensing mri , '' _ electronics letters _ , vol .",
    "121 - 123 , 2010 .",
    ", `` highly undersampled magnetic resonance image reconstruction via homotopic l0-minimization , '' _ ieee trans . on medical imaging _ ,",
    "106 - 121 , 2009 . , `` fast algorithms for nonconvex compressive sensing : mri reconstruction from very few data , '' in  _ proc .",
    "symp . on biomedical imaging _ ,",
    "262 - 265 , 2009 .",
    ", `` projection reconstruction mr imaging using focuss , '' _ magnetic resonance in medicine _ , vol .",
    "764 - 775 , 2007 .",
    ", `` k - t focuss : a general compressed sensing framework for high resolution dynamic mri , '' _ magnetic resonance in medicine _ , vol . 61 ,",
    "103 - 116 , 2009 . , `` a fast alternating direction method for tvl1-l2 signal reconstruction from partial fourier data , '' _ ieee j. sel .",
    "topics in signal processing _ , vol .",
    "288 - 297 , 2010 . , `` a novel method and fast algorithm for mr image reconstruction with significantly under- sampled data , '' _ inverse problems and imaging _ , vol .",
    "223 - 240 , 2010 .",
    ", `` efficient mr image reconstruction for compressed mr imaging , '' _ medical image analysis _ ,",
    "670 - 679 , 2011 .",
    "s. ji , y. xue and l. carin , `` bayesian compressive sensing , '' _ ieee trans . on signal processing _ ,",
    "56 , no . 6 , pp . 2346 - 2356 , 2008 . ,",
    "`` computational acceleration for mr image reconstruction in partially parallel imaging , '' _ ieee trans . on medical imaging _ ,",
    "5 , pp . 1055 - 1063 , 2011 .",
    ", `` fast mr image reconstruction for partially parallel imaging with arbitrary k - space trajectories , '' _ ieee trans . on medical imaging _ ,",
    "575 - 585 , 2011 .",
    ", `` low - dimensional - structure self - learning and thresholding : regularization beyond compressed sensing for mri reconstruction , '' _ magnetic resonance in medicine _ , vol .",
    "756 - 767 , 2011 .",
    ", `` mr image reconstruction from highly undersampled @xmath0-space data by dictionary learning , '' _ ieee trans . on medical imaging _ ,",
    "5 , pp . 1028 - 1041 , 2011 . , `` undersampled mri reconstruction with the patch - based directional wavelets , '' _ magnetic resonance in imaging _ , doi:10.1016/j.mri.2012.02.019 , feb . 2012 . ,",
    "`` robust non - local regularization framework for motion compensated dynamic imaging without explicit motion estimation , '' _ ieee int . symp . biomedical imaging _ , pp",
    ". 1056 - 1059 , 2012 . ,",
    "`` non - local sparse models for image restoration , '' in  _ int . conf . on computer vision _ ,",
    "2272 - 2279 , 2009 .",
    ", `` image denoising by sparse 3d transform - domain collaborative filtering , '' _ ieee trans . on image process _ ,",
    "16 , no . 8 , pp .",
    "2080 - 2095 , 2007 .",
    ", `` method of optimal directions for frame design , '' in  _ proc .",
    "acoustics , speech , signal processing _",
    "2443 - 2446 , 1999 . ,",
    "`` k - svd : an algorithm for designing of overcomplete dictionaries for sparse representation , '' _ ieee trans . on signal processing _ ,",
    "4311 - 4322 , 2006 . ,",
    "`` orthogonal matching pursuit : recursive function approximation with applications to wavelet decomposition , '' _",
    "27th conference on signals , systems and computers _",
    "40 - 44 , 1993 .",
    ", `` image sequence denoising via sparse and redundant representations , '' _ ieee trans . on image processing _",
    "27 - 36 , 2009 . ,",
    "`` nonparametric factor analysis with beta process priors , '' in  _ international conference on machine learning _ , 2009 .",
    ", `` stick - breaking beta processes and the poisson process , '' in  _ international conference on artificial intelligence and statistics _ , 2012 .",
    ", `` nonparametric bayesian dictionary learning for analysis of noisy and incomplete images , '' _ ieee trans . image process .",
    "130 - 144 , jun . 2012 . , `` nonparametric bayes estimators based on beta processes in models for life history data , '' _ annals of statistics _ , vol .",
    "1259 - 1294 , 1990 . ,",
    "`` hierarchical beta processes and the indian buffet process , '' in  _ international conference on artificial intelligence and statistics _ , san juan , puerto rico , 2007 .",
    ", `` a dual algorithm for the solution of nonlinear variational problems via finite - element approximations , '' _ computers and mathematics with applications _ , vol .",
    "17 - 40 , 1976 .",
    ", `` bregman iterative algorithms for l1 minimization with applications to compressed sensing , '' _ siam journal on imaging sciences _ , vol . 1 , no",
    ". 1 , pp . 143 - 168 , 2008 . , `` the split bregman method for l1 regularized problems , '' _ siam journal on imaging sciences _ , vol .",
    "323 - 343 , 2009 .",
    ", `` distributed optimization and statistical learning via the alternating direction method of multipliers , '' _ foundations and trends in machine learning _ , vol .",
    "1 , pp . 1 - 122 , 2010 . ,",
    "`` variational inference for stick - breaking beta process priors , '' in  _ international conference on machine learning _",
    ", bellevue , wa , 2011 .",
    ", `` the ibp compound dirichlet process and its application to focused topic modeling , '' in  _ international conference on machine learning _ , haifa , israel , 2010 .",
    ", `` sharing features among dynamical systems with beta processes , '' in  _ advances in neural information processing _ , vancouver , b.c . , 2011 . , `` variational inference for the indian buffet process , '' in  _ international conference on artificial intelligence and statistics _ , clearwater beach , fl , 2009 ."
  ],
  "abstract_text": [
    "<S> we develop a bayesian nonparametric model for reconstructing magnetic resonance images ( mri ) from highly undersampled @xmath0-space data . </S>",
    "<S> our model uses the beta process as a nonparametric prior for dictionary learning , in which an image patch is a sparse combination of dictionary elements . </S>",
    "<S> the size of the dictionary and the patch - specific sparsity pattern is inferred from the data , in addition to all dictionary learning variables . </S>",
    "<S> dictionary learning is performed as part of the image reconstruction process , and so is tailored to the mri being considered . </S>",
    "<S> in addition , we investigate a total variation penalty term in combination with the dictionary learning model . </S>",
    "<S> we derive a stochastic optimization algorithm based on markov chain monte carlo ( mcmc ) sampling for the bayesian model , and use the alternating direction method of multipliers ( admm ) for efficiently performing total variation minimization . </S>",
    "<S> we present empirical results on several mri , which show that the proposed regularization framework can improve reconstruction accuracy over other methods .    </S>",
    "<S> bayesian nonparametrics , dictionary learning , compressed sensing , magnetic resonance imaging </S>"
  ]
}