{
  "article_text": [
    "inferring a distribution function given a finite set of samples from this distribution function is a problem of considerable general interest .",
    "the literature contains many density estimation techniques , ranging from simply binning the data in a histogram and smoother versions of this collectively known as kernel density estimation , to more sophisticated techniques such as non - parametric ( penalized ) maximum likelihood fitting ( see , e.g. , * ? ? ? * for a review of all of the previous methods ) and full bayesian analyses ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "these techniques perform well in the high signal - to - noise regime , i.e. , when one has `` good data '' , however , in scientific applications the data generally come with large and heterogeneous uncertainties and one often only has access to lower dimensional projections of the full data , i.e. , there are often missing data . as a scientist",
    ", you are not interested in the observed distribution , what you really want to know is the underlying distribution , i.e. , the distribution you would have if the data had vanishingly small uncertainties and no missing data . in this paper we describe a general approach for inferring distribution functions when these complications are present .",
    "a frequently used density estimation technique is to model the distribution function as a sum of gaussian distributions by optimizing the likelihood of this model given the data ( e.g. * ? ? ? * ) .",
    "we show that this approach can be generalized in the presence of noisy , heterogeneous , and incomplete data .",
    "the likelihood of the model for each data point is given by the model convolved with the ( unique ) uncertainty distribution of that data point ; the objective function is obtained by simply multiplying these individual likelihoods together for the various data points .",
    "optimizing this objective function one obtains a maximum likelihood estimate of the distribution ( more specifically , of its parameters ) .",
    "while optimization of this objective function can , in principle , be performed by a generic optimizer , we develop an expectation - maximization ( em ) algorithm that optimizes the objective function .",
    "this algorithm works in much the same way as the normal em algorithm for mixture of gaussians density estimation , except that an additional degree of incompleteness is given by the actual values of the observables , since we only have access to noisy projections of the actual observables ; in the expectation step these actual values are estimated based on the noisy and projected measured values and the current estimate of the distribution function . in the limit",
    "in which the noise is absent but the data are lower dimensional projections of the quantities of interest , this algorithm reduces to the algorithm described in @xcite .",
    "we also show how bayesian priors on all of the parameters of the model can be naturally included in this algorithm as well as how a split - and - merge procedure that heuristically searches parameter space for better approximations to the global maximum can also be incorporated in this approach .",
    "these priors and the split - and - merge procedure can be important when applying the em algorithm developed here in situations with real data where the likelihood surface can have a very complicated structure .",
    "we also discuss briefly the practical issues having to do with model selection in the mixture model approach .",
    "applications to real data sets are discussed in detail in section [ sec : applications ] , both in general terms , i.e. , why the approach we put forward here is more appropriate when dealing with noisy , heterogeneous data than the more traditional density estimation techniques , as well as in some concrete examples .",
    "these concrete examples show that the technique developed in this paper performs extremely well even when the underlying distribution function has a complicated structure .",
    "the technique we describe below has many applications besides returning a maximum likelihood fit to the error - deconvolved distribution function of a data sample .",
    "for instance , when an estimate of the uncertainty in the estimated parameters or distribution function is desired or when a full bayesian analysis of the mixture model preferred , the outcome of the maximum likelihood technique developed here can be used as a seed for markov chain monte carlo ( mcmc ) methods for finite mixture modeling ( e.g. , * ? ? ? * ; * ? ? ? * ) .",
    "another possible application concerns fitting a linear relationship to a data set @xmath1 when the data has non - negligible errors both in @xmath2 as well as in @xmath3 , which can be correlated .",
    "this problem can be thought of as fitting the underlying , error - deconvolved distribution of the points @xmath4 with a gaussian ; the linear relationship then corresponds to the direction of the largest eigenvalue of the underlying distribution s covariance matrix .",
    "we describe this application in a specific case in section [ sec : straightline ] .",
    "our goal is to fit a model for the distribution of a @xmath0-dimensional quantity @xmath5 using a set of @xmath6 observational data points @xmath7 .",
    "therefore , we need to write down the probability of the data under the model for the distribution . the observations are assumed to be noisy projections of the true values @xmath8 @xmath9 where the noise is drawn from a gaussian with zero mean and known covariance tensor @xmath10 .",
    "the case in which there is missing data occurs when the projection matrix @xmath11 is rank - deficient .",
    "alternatively , we can handle the missing data case by describing the missing data as directions of the covariance matrix that have a formally infinite eigenvalue ; in practice we use very large eigenvalues in the noise - matrix .",
    "when the data has only a small degree of incompleteness , i.e. , when each data point has only a small number of unmeasured dimensions , this latter approach is often the best choice , since one often has some idea about the unmeasured values .",
    "for example , in the example given below of inferring the velocity distribution of stars near the sun we know that the stars are moving at velocities that do not exceed the speed of light , which is not very helpful , but also that none of the velocities exceed the local galactic escape speed , since we can safely assume that all the stars are bound to the galaxy . however , in situations in which each data point has observations of a dimensionality @xmath12 @xmath0 using the projections matrices will greatly reduce the computational cost , since , as will become clear below , the most computationally expensive operations all take place in the lower dimensional space of the observations",
    ".    we will model the distribution @xmath13 of the true values @xmath5 as a mixture of @xmath14 gaussians : @xmath15 where the amplitudes @xmath16 sum to unity and the function @xmath17 is the gaussian distribution with mean @xmath18 and variance tensor @xmath19 : @xmath20\\ , .\\ ] ]    for a given observation @xmath7 the likelihood of the model parameters @xmath21 given that observation and the noise covariance @xmath10 , which we will write as @xmath22 , can be written as : @xmath23 where @xmath24 this likelihood works out to be itself a mixture of gaussians  in order not to clutter the algebra we set all of the projection matrices equal to the unit matrix here @xmath25\\nonumber\\\\ & = & \\sum_j \\alpha_j \\int_{{\\mathbf{v}}}\\mathrm{d}{{\\mathbf{v}}}\\ , ( 2 \\pi)^{-d } \\det({{{\\mathbf{s}}}_i})^{-1/2 } \\det({{{\\mathbf{v}}}_{\\!j}})^{-1/2 } \\nonumber\\\\ & \\phantom{= } & \\qquad\\exp\\bigg[-\\frac{1}{2 } \\ { { { \\mathbf{v}}}{^{\\scriptscriptstyle \\top}}({{{\\mathbf{v}}}_{\\!j}}^{-1 } + { { { \\mathbf{s}}}_i}^{-1}){{\\mathbf{v}}}- { { \\mathbf{v}}}{^{\\scriptscriptstyle \\top}}({{{\\mathbf{v}}}_{\\!j}}^{-1}{{{\\mathbf{m}}}_j}+ { { { \\mathbf{s}}}_i}^{-1}{{{\\mathbf{w}}}_i } ) \\nonumber \\\\ & \\phantom{= } & \\qquad \\qquad-({{{\\mathbf{m}}}_j}{^{\\scriptscriptstyle \\top}}{{{\\mathbf{v}}}_{\\!j}}^{-1 } + { { { \\mathbf{w}}}_i}{^{\\scriptscriptstyle \\top}}{{{\\mathbf{s}}}_i}^{-1}){{\\mathbf{v}}}+ { { { \\mathbf{w}}}_i}{^{\\scriptscriptstyle \\top}}{{{\\mathbf{s}}}_i}^{-1}{{{\\mathbf{w}}}_i}+ { { { \\mathbf{m}}}_j}{^{\\scriptscriptstyle \\top}}{{{\\mathbf{v}}}_{\\!j}}^{-1}{{{\\mathbf{m}}}_j}\\ } \\bigg]\\nonumber\\ .\\end{aligned}\\ ] ] this integral works out to be @xmath26\\nonumber\\ , \\end{aligned}\\ ] ] which simplifies to @xmath27\\nonumber\\\\ & = & \\sum_j { \\alpha_j}{{\\cal n}}({{{\\mathbf{w}}}_i}|{{{\\mathbf{m}}}_j},{{{\\mathbf{t}}}_{ij}})\\ , , \\end{aligned}\\ ] ] where we have defined @xmath28 restoring the projection matrices @xmath11 , we have @xmath29 where @xmath30    the free parameters of this model can now be chosen such as to maximize an explicit , justified , scalar objective function @xmath31 , given here by the logarithm ( log ) likelihood of the model given the data , i.e. , @xmath32 this function can be optimized in several ways , one of which is to calculate the gradients and use a generic optimizer to increase the likelihood until it reaches a maximum .",
    "this approach is complicated by parameter constraints ( e.g. , the amplitudes @xmath16 must all be non - negative and add up to one , the variance tensors must be positive definite and symmetric ) . in what follows we will describe a different approach : an em algorithm that iteratively maximizes the likelihood , while naturally respecting the restrictions on the parameters .",
    "the problem of finding a maximum likelihood estimate of the parameters of the mixture of gaussians model by optimizing the total log likelihood given in equation  ( [ eq : totallike ] ) is not a problem with missing data .",
    "however , optimization of the total log likelihood is difficult and an analytical solution is not possible for @xmath33 . an analytical solution does exist when @xmath34 ( when the error covariances @xmath10 are equal ; see below ) .",
    "therefore , if we knew which gaussian a specific data point was sampled from , optimization would be simple . the formulation of the gaussian mixture density estimation as a hidden data problem takes advantage of this fact @xcite .",
    "when data is actually missing and/or when the data is noisy ( error covariances @xmath10 not equal to zero ) , an analytical solution does not exist anymore . as we will show below , in this case",
    "formulating the problem as a missing data problem can also lead to a simple , iterative algorithm that leads to a maximum likelihood estimate of the model parameters .",
    "first we will recapitulate how the em algorithm for gaussian mixtures works by applying it to the basic problem of fitting a set of data points with a mixture of gaussians , thus we will set all the error covariances equal to zero .",
    "then we will investigate how the problem can be solved by a similar em algorithm when we have incomplete and/or noisy data .",
    "a short summary of the general properties of the em methodology is given in appendix [ sec : emoverview ] .",
    "in the case of complete , precise observations ( i.e. , @xmath35 , @xmath36 , @xmath37 ) the log likelihood of the model given the data from equation  ( [ eq : totallike ] ) reduces to the following log likelihood : @xmath38 formulating this problem as a missing data problem introduces the indicator variables @xmath39 which indicate whether a data point @xmath40 was sampled from gaussian @xmath41 , i.e. , @xmath42 this variable can take on values between these extreme values , in which case @xmath39 corresponds to the probability that data point @xmath40 was generated by gaussian @xmath41 . in any case , for every data point we have that @xmath43 .    using this hidden indicator variable",
    "we can write the `` full - data '' log likelihood as @xmath44 using jensen s inequality ( e.g. , * ? ? ?",
    "* ) it is easy to see that optimizing this full - data log likelihood also optimizes the original log likelihood equation([eq : totallikecomplete ] ) .",
    "jensen s inequality for a concave function @xmath45 , numbers @xmath46 , and weights @xmath47 can be stated as @xmath48 the logarithm is a concave function and , defining @xmath49 , if we choose @xmath50 and weights @xmath39 , the indicator variables defined above , we find ( since @xmath51 ) @xmath52 with equality when we set @xmath53 since then @xmath54 this proves that the em algorithm applied to the full - data log likelihood in equation  ( [ eq : fulltotallikecomplete ] ) leads to a maximum likelihood estimate of the model parameters for the original likelihood , since the calculation of the e - step , i.e. , taking the expectation of the full - data log likelihood , essentially reduces to calculating the expectation of the indicator variables @xmath39 given the data and the current model estimate , which is exactly setting the @xmath39 equal to the posterior probability of the data point @xmath40 belonging to gaussian @xmath41 , as given in equation([eq : posteriorcomplete ] ) .",
    "optimization of the @xmath55-function from equation  ( [ eq : jensenapplied ] ) with respect to the model parameters is equivalent to optimization of the full - data log likelihood , since the second term in the definition of @xmath55 does not depend on the model parameters and the first term is equal to the full - data log likelihood .",
    "this view of the em algorithm for a mixture of gaussians @xcite can also be used to argue for different e- and/or m - steps in order to speed up convergence @xcite .",
    "for example , one can choose particular data points to target in the e - step or specific model parameters in the m - step .",
    "in the m  step we maximize @xmath56 with respect to the model parameters @xmath57 .",
    "the constraint on the amplitudes ( i.e. , that they add up to one ) can be implemented by adding a lagrange multiplier @xmath58 and an extra term @xmath59 to @xmath56 .",
    "taking the derivative of this with respect to @xmath60 then leads to @xmath61 the requirement that the @xmath60 add up to one leads to @xmath62 where we have used the fact that @xmath43 and @xmath6 is the number of data points .",
    "therefore the optimal value of @xmath60 is @xmath63    the rest of the optimization reduces to optimizing the reduced log likelihood @xmath64 \\ , \\ ] ] which we can simplify by using ( 1 ) that @xmath65 , ( 2 ) that for any number @xmath66 we can write that @xmath67 , and ( 3 ) the cyclical property of the trace : @xmath68 \\ , \\ ] ] where we have used the fact that @xmath69 ( which one can derive by differentiating @xmath70 ) .",
    "this is equal to zero when @xmath71    to summarize , the likelihood can be optimized by alternating the following e- and m - steps : @xmath72\\ , , \\end{aligned}\\ ] ] where @xmath73 .    a fatal flaw of the maximum likelihood technique described here , and",
    "we must emphasize that this is a flaw of the objective function and not of the em algorithm , is that the likelihood is unbounded . indeed , when one of the means @xmath74 is set equal to one of the data points , reducing the covariance matrix @xmath75 of that gaussian will lead to an unbounded increase in the probability of that data point , i.e. , the gaussian becomes a delta distribution centered on a particular data point and the model therefore has a infinite likelihood .",
    "this problem can be dealt with in a couple of different ways , some of which will be described below .",
    "we will describe a solution which assumes a prior for the model covariances , which leads to a regularization of the covariances at every step such that they can not become zero .",
    "however , we will use this technique to deal with the related problem of the model covariances reaching a maximum likelihood at the edge of their domain , i.e. , when the covariance becomes zero without making the likelihood infinite . in the next section",
    "we will describe a solution to the unbounded likelihood problem that is especially well motivated when dealing with experimental or observational data , i.e. , taking into account the measurement uncertainties .",
    "the problem we would like to solve has an additional component of incompleteness .",
    "the data we are using are noisy , as described by their covariance matrices @xmath10 .",
    "this noise can vary between small fluctuations to a complete lack of information for certain components of the underlying quantity @xmath5 ( as indicated by a formally infinite contribution to the covariance matrix ) .",
    "we can use the em algorithm to deal with this second kind of incomplete data as well .    in the case of full - rank projection matrices",
    ", we could try to proceed exactly as we did in the case of complete data with noise covariance matrices @xmath10 equal to the zero matrix .",
    "the e - step remains the same and the part of the m - step that updates the amplitudes will also be the same as before , however , in order to optimize the means and covariances , we would have to solve an equation similar to equation([eq : reducedphi ] ) , i.e. , @xmath76   = 0\\ , \\ ] ] in which we simply use @xmath77 instead of @xmath75 .",
    "this equation can not be solved analytically to give us update steps for the means and covariances of the gaussians when the noise covariances @xmath10 are different for each observation .",
    "a reasonable way to deal with this would be to use a generic optimizer to perform the m - step optimization , which would still give a better result than using a generic optimizer for the whole problem since the dimensionality of the problem is greatly reduced , however , we will describe a different procedure which deals with this problem in a similar way to how the em algorithm simplified the complete data case .",
    "essentially , what we will do is consider the situation of noisy measurements , which may or may not be projections of higher dimensional quantities , as a missing data problem in itself .",
    "that is , we will consider the true values @xmath8 as extra missing data ( in addition to the indicator variables @xmath39 ) .",
    "this allows us to write down the `` full data '' log likelihood as follows @xmath78 we will now show how we can use the em methodology to find straightforward update steps that maximize the full data likelihood of the model .",
    "then we will prove that these updates also maximize the likelihood of the model given the noisy observations .",
    "the e - step consists as usual of taking the expectation of the full data likelihood with respect to the current model parameters @xmath57 .",
    "writing out the full data log likelihood from equation([eq : incompletefulllike ] ) we find @xmath79\\ , \\ ] ] which shows that in addition to the expectation of the indicator variables @xmath39 for each component we also need the expectation of the @xmath80 terms and the expectation of the @xmath81 terms given the data , the current model estimate and the component @xmath41 .",
    "the expectation of the @xmath39 is again equal to the posterior probability that a data point @xmath7 was drawn from the component @xmath41 ( see equation  [ eq : posteriorcomplete ] ) .",
    "the expectation of the @xmath8 and the @xmath82 can be found as follows : consider the probability distribution of the vector @xmath83}{^{\\scriptscriptstyle \\top}}$ ] given the model estimate and the component @xmath41 . from the description of the problem",
    "we can see that this vector is distributed normally with mean @xmath84}\\ ] ] and covariance matrix @xmath85}\\ .\\ ] ] the conditional distribution of the @xmath8 given the data @xmath7 is normal with mean ( see appendix [ sec : marg ] ) @xmath86 and covariance matrix @xmath87 thus we see that the expectation of @xmath8 given the data @xmath7 , the model estimate , and the component @xmath41 is given by @xmath88 , whereas the expectation of the @xmath82 given the same is given by @xmath89 .",
    "given this the expectation of the full data log likelihood is given by @xmath90\\right]\\nonumber\\\\ & = & \\sum_{i , j } { q_{ij}}\\left [ \\ln \\alpha_j -\\frac{d}{2 }   \\ln ( 2 \\pi ) - \\frac{1}{2 } { \\mathrm{trace}}\\left[\\ln{{{\\mathbf{v}}}_{\\!j}}+ ( { { { \\mathbf{b}}}_{ij}}+ ( { { { \\mathbf{m}}}_j}- { { { { \\mathbf{b}}}_{ij}}})({{{\\mathbf{m}}}_j}- { { { { \\mathbf{b}}}_{ij}}}){^{\\scriptscriptstyle \\top } } ) { { { \\mathbf{v}}}_{\\!j}}^{-1 } \\right]\\right]\\ .\\end{aligned}\\ ] ] the update step for the amplitudes @xmath60 is given as before by equation  ( [ eq : alphaup ] ) . dropping the @xmath91 term the differential of the reduced expectation of the full data log likelihood @xmath92",
    "is given by @xmath93 - 2 { { { \\mathbf{v}}}_{\\!j}}^{-1 } ( { { { { \\mathbf{b}}}_{ij}}}- { { { \\mathbf{m}}}_j } ) d { { { \\mathbf{m}}}_j}{^{\\scriptscriptstyle \\top}}\\right ] \\ .\\ ] ]    the complete em algorithm given incomplete , noisy observations is then given by @xmath94 { { { { \\mathbf{b}}}_{ij}}}&\\leftarrow & { { { \\mathbf{m}}}_j}+ { { { \\mathbf{v}}}_{\\!j}}{{{\\mathbf{r}}}_i}{^{\\scriptscriptstyle \\top}}{{{\\mathbf{t}}}_{ij}}^{-1 } ( { { { \\mathbf{w}}}_i}- { { { \\mathbf{r}}}_i}\\,{{{\\mathbf{m}}}_j})\\nonumber\\\\[3pt ] { { { \\mathbf{b}}}_{ij}}&\\leftarrow & { { { \\mathbf{v}}}_{\\!j}}- { { { \\mathbf{v}}}_{\\!j}}{{{\\mathbf{r}}}_i}{^{\\scriptscriptstyle \\top}}{{{\\mathbf{t}}}_{ij}}^{-1 } { { { \\mathbf{r}}}_i}{{{\\mathbf{v}}}_{\\!j}}\\nonumber\\\\ \\mbox{\\textbf{m~step:}}\\;\\;\\ ; { \\alpha_j}&\\leftarrow & \\frac{1}{n}\\,\\sum_i { q_{ij}}\\nonumber \\\\     { { { \\mathbf{m}}}_j}&\\leftarrow & \\frac{1}{{q_j}}\\,\\sum_i { q_{ij}}\\,{{{{\\mathbf{b}}}_{ij}}}\\nonumber \\\\     { { { \\mathbf{v}}}_{\\!j}}&\\leftarrow & \\frac{1}{{q_j}}\\,\\sum_i { q_{ij}}\\left[({{{\\mathbf{m}}}_j}-{{{{\\mathbf{b}}}_{ij}}})\\,({{{\\mathbf{m}}}_j}-{{{{\\mathbf{b}}}_{ij}}}){^{\\scriptscriptstyle \\top}}+ { { { \\mathbf{b}}}_{ij}}\\right]\\ , , \\end{aligned}\\ ] ] where , as before , @xmath73 .",
    "we can prove that this procedure for maximizing the full data likelihood also maximizes the log likelihood of the data @xmath7 given the model .",
    "we use jensen s inequality in the continuous case ( e.g. , * ? ? ?",
    "i.e. , @xmath95 for a concave function @xmath45 and a non - negative integrable function @xmath96 , where we have assumed that @xmath96 is normalized , i.e. , @xmath96 is a probability distribution . for each observation",
    "@xmath97 we can then introduce a function @xmath98 such that @xmath99 where @xmath57 , as before , represents the set of model parameters , and @xmath100 is the entropy of the distribution @xmath98 .",
    "this inequality becomes an equality when we take @xmath101 since then @xmath102    the above holds for each data point , and we can write @xmath103 the last factor reduces to calculating the posterior probabilities @xmath104 and we can write the @xmath55 function as ( we drop the entropy term here , since it plays no role in the optimization , as it does not depend on the model parameters ) @xmath105\\\\ & = & \\sum_{i , j } { q_{ij}}\\left [ \\ln \\alpha_j + \\int_{{{\\mathbf{v}}}}\\mathrm{d}{{\\mathbf{v}}}\\ , p({{\\mathbf{v}}}|{{{\\mathbf{w}}}_i},\\theta , j ) \\left [ -\\frac{1}{2 }   \\ln \\det { { \\mathbf{v}}}_j -\\frac{1}{2 } ( { { \\mathbf{v}}}-{{{\\mathbf{m}}}_j}){^{\\scriptscriptstyle \\top}}{{{\\mathbf{v}}}_{\\!j}}^{-1}({{\\mathbf{v}}}-{{{\\mathbf{m}}}_j})\\right]\\right]\\ , , \\nonumber\\end{aligned}\\ ] ] where we dropped the @xmath106 terms since they do nt depend on the model parameters directly .",
    "this shows that this reduces exactly to the procedure described above , i.e. , to taking the expectation of the @xmath8 and @xmath82 terms with respect to the distribution of the @xmath8 given the data @xmath7 , the current parameter estimate , and the component @xmath41 .",
    "we conclude that the e - step as described above ensures that the expectation of the full data log likelihood becomes equal to the log likelihood of the model given the observed data .",
    "optimizing this log likelihood in the m - step then also increases the log likelihood of the model given the observations .",
    "therefore the em algorithm we described will increase the likelihood of the model in every iteration , and the algorithm will approach local maxima of the likelihood . convergence is identified , as usual , as extremely small incremental improvement in the log likelihood per iteration .",
    "some care must be taken to implement these equations in a numerically stable way .",
    "in particular , care should be taken to avoid underflow when computing the ratio of a small probability over the sum of other small probabilities ( see appendix [ sec : logsum ] ) .",
    "notice that we do nt have to explicitly enforce constraints on parameters , e.g. , keeping covariances symmetric and positive definite , since this is taken care of by the updates .",
    "for example , the update equation for @xmath75 is guaranteed by its form to produce a symmetric positive - semidefinite matrix .    in some cases we might want to keep some of the model parameters fixed during the em steps , and for the means and the covariances this",
    "can simply be achieved by not updating them during the m step . however , if we want to keep certain of the amplitudes fixed we have to be more careful , as we have to satisfy the constraint that the amplitudes add up to one at all times . therefore , if @xmath107 indexes the free amplitudes and @xmath108 the set of amplitudes we want to keep fixed , the lagrange multiplier term we have to add to the functional @xmath55 is @xmath109 , which leads to the following update equations for the amplitudes @xmath110 \\ .\\ ] ]",
    "singularities and local maxima are two problems , that can severely limit the generalization capabilities of the computed density estimates for inferring the densities of unknown data points .",
    "these are commonly encountered when using the em algorithm to iteratively compute the maximum likelihood estimates of gaussian mixtures .",
    "singularities arise when the covariance in equation([eq : updateemsimple ] ) or equation  ( [ eq : updateemincomplete ] ) becomes singular , i.e. , when a gaussian in the fit becomes very peaked or elongated . a naive way of dealing with this problem is by artificially enforcing a lower bound on the variances of the gaussians in the mixture , but this usually results in very low inference capabilities of the resulting mixture estimate .",
    "we will present a regularization scheme based on defining a bayesian prior distribution on the parameter space below .",
    "the problem of local maxima is a natural consequence of the em algorithm as presented above : as the em procedure ensures a monotonic increase in log likelihood of the model , the algorithm will exit when reaching a local maximum , since it can not reach the true maximum without passing through lower likelihood regions .",
    "a solution to this problem therefore has to discontinuously change the model parameters , thereby quite possibly ending up in a region of parameter space with a lower likelihood than the first estimate . as a result",
    "the log likelihood is no longer guaranteed to increase monotonically , however , decreases in log likelihood are rare and concentrated at these discontinuous jumps . a well - motivated algorithm based on moving gaussians from overpopulated regions to underpopulated regions of parameter - space",
    "is described below .",
    "however , instead of this deterministic approach , a stochastic em procedure could also be used to explore the likelihood surface @xcite .",
    "we also briefly discuss methods to set the remaining free parameters , the number of gaussians @xmath14 and the hyperparameters introduced in the bayesian regularization described below .",
    "a general bayesian regularization scheme consists of putting prior distributions on the gaussian mixtures parameters space @xmath111 @xcite .",
    "a conjugate prior distribution of a single multivariate normal distribution is the product of normal density @xmath112 and a wishart density @xmath113 @xcite , where @xmath114\\right]\\ , , \\ ] ] with @xmath115 a normalization constant .",
    "the proper prior distribution on the amplitudes @xmath16 is a dirichlet density @xmath116 , given by @xmath117 where @xmath118 is a normalizing factor , @xmath119 , and @xmath120 ( as is the case for the amplitudes in the density mixture ) .",
    "the log likelihood we want to maximize then becomes @xmath121\\ , .\\ ] ]    we can again use the em algorithm to find a local maximum to this function .",
    "the e - step is the same e - step as before ( eq .",
    "( [ eq : updateemincomplete ] ) ) . in",
    "the m step the functional we have to maximize is the same functional as in the unregularized case ( given in eq .",
    "( [ eq : incompletefulldataloglike ] ) ) with added terms : @xmath122\\ , .\\ ] ] of which we can take derivatives with respect to the model parameters again , which leads to the following update equations @xmath123 + \\eta({{{\\mathbf{m}}}_j}-\\hat{{{\\mathbf{m}}}})({{{\\mathbf{m}}}_j}-\\hat{{{\\mathbf{m}}}}){^{\\scriptscriptstyle \\top}}+ 2{{\\mathbf{w } } } } {       { q_j}+1 + 2(\\omega-(d+1)/2)}\\ , .\\end{aligned}\\ ] ]    these update equations have many free hyperparameters without well - motivated values .",
    "all of these could be used in principle to regularize the model parameters .",
    "for example , the @xmath124 could be used to keep the amplitudes @xmath60 from becoming very small .",
    "when one does not want to set these hyperparameters by hand , their optimal values can all be obtained by the model selection techniques described below . however , in what follows we will focus on the regularization of the covariances .",
    "it is easy to see from the update equation for the variances @xmath75 that the updated variances are bound from below , since the numerator is greater or equal to @xmath125 and the denominator has an upper limit .",
    "therefore we can focus on the matrix @xmath126 for the purpose of the regularization by setting the other parameters to the uninformative values : @xmath127 we can further reduce the complexity to one free parameter by setting @xmath128 , where @xmath129 is the @xmath0-dimensional unit matrix .",
    "the update equations in the m step then reduce to @xmath130 + w { { \\mathbf{i}}}\\right]\\ , .\\end{aligned}\\ ] ]    the best value to use for the parameter @xmath131 is not known a priori but can be determined , e.g. , by jackknife leave - one - out cross validation .      the split and merge algorithm starts from the basic em algorithm , with or without the bayesian regularization of the variances , and jumps into action after the em algorithm has reached a maximum , which more often than not will only be a local maximum .",
    "at this point three of the gaussians in the mixture are singled out , based on criteria detailed below , and two of these gaussians are merged while the third gaussian is split into two gaussians @xcite .",
    "let us denote the indices of the three selected gaussians as @xmath132 and @xmath133 , where the former two are to be merged while @xmath133 will be split .",
    "the gaussians corresponding to the indices @xmath134 and @xmath135 will be merged as follows : the model parameters of the merged gaussian @xmath136 are @xmath137 where @xmath138 stands for @xmath74 and @xmath75 .",
    "thus , the mean and the variance of the new gaussian is a weighted average of the means and variances of the two merging gaussians .",
    "the gaussian corresponding to @xmath133 is split as follows : @xmath139 thus , the gaussian @xmath133 is split into equally contributing gaussians with each new gaussian having a covariance matrix that has the same volume as @xmath140 . the means @xmath141 and @xmath142 can be initialized by adding a random perturbation vector @xmath143 to @xmath144 , e.g. , @xmath145 where @xmath146 and @xmath147 .    after this split and",
    "merge initialization the parameters of the three affected gaussians need to be re - optimized in a model in which the parameters of the unaffected gaussians are held fixed .",
    "this can be done by using the m step in equation  ( [ updatebayes ] ) for the parameters of the three affected gaussians , while keeping the parameters of the other gaussians fixed , including the amplitudes , i.e. , we need to use the update equation  ( [ eq : fixedampsupdate ] ) for the amplitudes .",
    "this ensures that the sum of the amplitudes of the three affected gaussians remains fixed .",
    "this procedure is called the _ partial em procedure_. after convergence this is then followed by the full em algorithm on the resulting model parameters . finally the resulting parameters are accepted if the total log likelihood of this model is greater than the log likelihood before the split and merge step .",
    "if the likelihood does nt increase the same split and merge procedure is performed on the next triplet of split and merge candidates .",
    "the question that remains to be answered is how to choose the 2 gaussians that should be merged and the gaussian that should be split .",
    "in general there are @xmath148 possible triplets like this which quickly reaches a large number when the number of gaussians @xmath14 gets larger . in order to rank these triplets",
    "one can define a _",
    "merge criterion _ and a _ split criterion_.    the merge criterion is constructed based on the observation that if many data points have equal posterior probabilities for two gaussians , these gaussians are good candidates to be merged .",
    "therefore one can define the merge criterion : @xmath149 where @xmath150 is the @xmath6-dimensional vector of posterior probabilities for the @xmath41th gaussian .",
    "pairs of gaussians with larger @xmath151 are good candidates for a merger .",
    "we can define a split criterion based on the kullback - leibler distance between the local data density around the @xmath152th gaussian , which can be written in the case of complete data as @xmath153 , and the @xmath152th gaussian density specified by the current model estimates @xmath154 and @xmath155 .",
    "the kullback - leibler divergence between two distributions @xmath156 and @xmath157 is given by @xcite : @xmath158 since the local data density is only non - zero at a finite number of values , we can write this as @xmath159\\ , .\\ ] ] the larger the distance between the local density and the gaussian representing it , the larger @xmath160 and the better candidate this gaussian is to be split .",
    "when dealing with incomplete data determining the local data density is more problematic .",
    "one possible way to estimate how well a particular gaussian describes the local data density is to calculate the kullback - leibler divergence between the model gaussian under consideration and each individual data point perpendicular to the unobserved directions for that data point .",
    "thus , we can write @xmath161\\ , .\\ ] ]    candidates for merging and splitting are then ranked as follows : first the merge criterion @xmath162 is calculated for all pairs @xmath163 and the pairs are ranked by decreasing @xmath162 .",
    "for each pair in this ranking the remaining gaussians are then ranked by decreasing @xmath164 .",
    "no real world application of gaussian mixture density estimation is complete without a well - specified methodology for setting the number of gaussian components @xmath14 and any hyperparameters introduced in the bayesian regularization described above , the covariance regularization parameter @xmath131 in our basic scheme .",
    "this covariance regularization parameter basically sets the square of the smallest scale of the distribution function on which we can reliable infer small scale features .",
    "therefore , this scale could be set by hand to the smallest scale we believe we have access to based on the properties of the data set .    in order to get the best results the parameters @xmath14 and @xmath131",
    "should be set by some objective procedure . as mentioned above ,",
    "leave - one - out cross validation @xcite could be used to set the regularization parameter @xmath131 , and the number of gaussians could be set by this procedure as well .",
    "the basic idea of leave - one - out cross validation is that one creates @xmath6 new data sets by sequentially taking one data point out of the original sample .",
    "for each of these @xmath6 new samples we optimize the scalar objective function and then record the logarithm of the likelihood of the data point that was left out under this new optimized parameter set .",
    "summing up all of these cross validation log likelihoods then gives a scalar which can be compared for different models and the best model is the one for which the total cross - validation log likelihood is the largest .",
    "this procedure , while simple , can be quite computationally intensive and is therefore often not feasible",
    ". the procedure can be simplified by ( 1 ) leaving out more than one data point at a time , i.e. , creating @xmath165 new samples by leaving out 1/@xmath166 of the full sample , optimizing the scalar objective using the new sample and recording the total cross validation likelihood of the data points left out at that step ; ( 2 ) restricting the optimization , e.g. , by starting from the optimized parameters for the full sample and only allowing certain parameters to vary in the individual optimization of the @xmath165 new samples .",
    "for example , one can choose to only allow the amplitudes of the optimized parameters found for the full sample to change during the cross validation optmizations .",
    "other techniques include methods based on bayesian model selection @xcite as well as approaches based on minimum coding inference @xcite , although these methods have difficulty dealing with significant overlap between components ( such as the overlap we see in the example in figure [ fig:4veldist ] ) , but there are methods to deal with these situations @xcite . alternatively ,",
    "when a separate , external data set is available , we can use this as a test data set to validate the obtained distribution function .",
    "all of these methods are explored in an accompanying paper on the velocity distribution of stars in the solar neighborhood from measurements from the _ hipparcos _  satellite ( see below ; * ? ? ?",
    "to summarize the full algorithm we briefly list all the steps involved :    1 .   run the em algorithm as specified in equations ( [ eq : updateemincomplete ] ) and ( [ updatebayes ] ) .",
    "store the resulting model parameters @xmath167 and the corresponding model log likelihood @xmath168 .",
    "2 .   compute the merge criterion @xmath169 for all pairs @xmath163 and the split criterion @xmath170 for all @xmath152 .",
    "sort the split and merge candidates based on these criteria as detailed above .",
    "3 .   for the first triplet @xmath171 in this sorted list",
    "set the initial parameters of the merged gaussian using equation  ( [ merged ] ) and the parameters of the two gaussian resulting from splitting the third gaussian using equations ( [ split])-([split2 ] ) .",
    "then run the partial em procedure on the parameters of the three affected gaussians , i.e. , run em while keeping the parameters of the unaffected gaussians fixed , and follow this up by running the full em procedure on all the gaussians .",
    "if after convergence the new log likelihood @xmath31 is greater than @xmath168 , accept the new parameter values @xmath172 and return to step two .",
    "if @xmath173 return to the beginning of this step and use the next triplet @xmath171 in the list .",
    "4 .   halt this procedure when none of the split and merge candidates improve the log likelihood or , if this list is too long , if none of the first @xmath174 lead to an improvement .",
    "deciding when to stop going down the split - and - merge hierarchy will be dictated in any individual application of this technique by computational constraints .",
    "this is an essential feature of any search - based approach to finding global maxima of ( likelihood ) functions .",
    "inferring the distribution function of an observable given only a finite , noisy set of measurements of that distribution function and the related problem of finding clusters and/or overdensities in the distribution is a problem of significant general interest in many areas of science and of astronomy in particular ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "the description you are interested in as a scientist is _ not _ the observed distribution , what you really want is the description of the distribution that you would have if you had good data , i.e. , data with vanishingly small uncertainties and with all of the dimensions measured . in the low signal - to - noise regime ,",
    "e.g. , large data sets at the bleeding edge in astronomy , the data never have these two properties such that the underlying , true distribution can not be found without taking the noise properties of the data into account . if you want know the underlying distribution , in order to compare your model with the data , you need to convolve the model with the data uncertainties , not deconvolve the data . when the given set of data has heterogeneous noise properties , that is , when the error convolution is different for each data point , each data point is a sample of a different distribution , i.e. , the distribution obtained from convolving the true , underlying distribution with the noise of that particular observation .",
    "incomplete data poses a similar problem when the part of the data that is missing is different for different data points .",
    "none of the current density estimation techniques confront all of these issues ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ) , although techniques that properly account for incomplete data have been developed @xcite .",
    "a first approximation to the problem of modeling the distribution of a set of real - valued data points that is often used is to fit a single ( multivariate ) normal distribution to the distribution of the data points , e.g. , when applying principal component analysis . in general this is hardly ever a good approximation to the full distribution , e.g. , when a distribution shows two distinct maxima or significant asymmetry this approximation is poorly suited to the problem at hand .",
    "however , fitting a distribution that is the sum of a large enough number of normal distributions provides a good approximation to any reasonably well - behaved underlying distribution . for example",
    ", a multi - modal distribution will be described by a sum of normal distributions with varying means , while a single - peak distribution with non - gaussian tails could be fit by a set of gaussians with similar means but varying amplitudes and covariance matrices .",
    "therefore , the technique described in the previous sections is perfectly suited for applications with real data sets .",
    "one of the simplest applications of the algorithm described above consists of fitting a linear relationship to a set of data points with , possibly correlated , uncertainties in both the independent and the dependent variable .",
    "many different methods have been devised to fit a straight line to a data set , most of them least - squares procedures ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "when measurement errors are present in both variables a `` double weighted '' regression can be performed which minimizes errors both in the dependent variables as well as in the independent variable @xcite .",
    "this procedure can be generalized to the case of correlated errors . in the case of uncorrelated errors ,",
    "one finds the parameters of the straight line @xmath175 which minimize the quantity @xmath176\\ , , \\ ] ] where the ( @xmath177 ) are the observed ( @xmath178 ) adjusted along a line of slope @xmath179 to lie on the straight line .",
    "this in fact assumes that the errors are highly correlated , even though by assumption they are not , and does not allow for the full exploration of each pairs covariance ellipse .",
    "in addition to this , it is difficult to implement numerically .    as mentioned in the introduction ,",
    "the density estimation technique developed in this paper can be applied to this problem as well .",
    "fitting a single gaussian distribution to the data will pick out the direction of the largest variance , which one can identify with the direction of the linear relation between two variables .",
    "since the technique described above takes care of the heterogeneous error properties of the data , this technique can be applied straightforwardly to this problem .",
    "this is the simplest application of the density estimation technique as it only concerns a single gaussian component in the mixture , and none of the extensions to the algorithm have to be applied to this problem .    in detail ,",
    "one fits a single gaussian distribution to the observed data , with no restrictions on the mean or variance of that gaussian .",
    "after convergence , one identifies the direction corresponding to the largest eigenvalue as the direction of the linear relation .",
    "the line in this direction going through the mean of the best - fit gaussian is then the desired linear fit to the data .",
    "this technique is similar to the procedures for fitting a line to data described at the beginning of the section in that it minimizes a quantity similar to the quantity @xmath180 in equation  ( [ eq : fitlinethem ] ) , but it uses the full uncertainty covariance matrices for each datapoint and has a simple numerical implementation given by the em procedure described in this paper . the objective function given in equation  ( [ eq : totallike ] ) is , as shown in section  [ sec : objective ] , justifiable under the assumption of gaussian distributed errors and a underlying gaussian distribution . the fact that the model space is more general than the the model of a straight line that we want to fit to the data  the straight line model is in fact part of the model space as an infinitely long , infinitely narrow gaussian distribution  can hardly be thought of as a disadvantage , as it can show that the assumption of a linear relation is suspect through the aspect ratio of the best fit gaussian .",
    "this procedure was used in @xcite to fit a line to a set of points with correlated errors in the abscissa and ordinate (  2 in that paper ) .",
    "here we fit the tully - fisher relation in several bands from hubble space telescope ( _ hst _ ) data as an example of this procedure .",
    "the tully - fisher relation is a power - law type relation between the luminosity and the velocity width of spiral galaxies .",
    "this relation can be used to measure distances in the universe because it relates a velocity , which can be measured relatively easily by doppler measurements , to the intrinsic luminosity , which is hard to observe , but depends on the easily observed apparent brightness of the galaxy by the distance squared .",
    "the relation between luminosity @xmath181 and velocity width @xmath182 is of the form @xmath183 where the exponent @xmath184 depends on the wavelength at which the luminosity is measured but generally lies somewhere between three and four .",
    "calibrating the tully - fisher relation by measuring accurate cepheid distances to nearby galaxies was one of the key projects of the hubble space telescope and as an illustration we fit the tully - fisher relation in five different bandpasses here from the _ hst _  data ( table 2 of * ? ? ? * ) .",
    "uncertainties in both the velocity widths ( measured at 20percent of the peak hi flux ) of all the galaxies as well as in the absolute magnitudes are large such that our procedure naturally applies ( the fits in @xcite are bivariate fits minimizing errors in both variables ) . the resulting linear relation is shown in  [ fig : tf ] for the different bandpasses . using a leave - one - out jackknife procedure to establish the uncertainties on the slopes and intercepts , we find the following relations @xmath185 these relations agree within the uncertainties with the relations found in @xcite .",
    "we have applied the technique developed in this paper to the problem of inferring the velocity distribution of stars in the solar neighborhood from transverse angular data from the _ hipparcos_satellite and we present in this section some results of this study to demonstrate the performance of the algorithm on a real data set .",
    "a more detailed and complete account of this study is presented elsewhere @xcite .    the astrometric esa space mission _ hipparcos _ , which collected data over a 3.2 year period around 1990 , provided for the first time an all - sky catalogue of absolute parallaxes and proper motions , with typical uncertainties in these quantities on the order of mas @xcite . from this catalogue of @xmath186 stars",
    "kinematically unbiased samples of stars with accurate positions and velocities can be extracted @xcite . since astrometric measurements of velocities",
    "basically just compare the position of a star at different times to calculate velocities , the only components of a star s velocity that can be measured astrometrically are the tangential components .",
    "the line - of - sight velocities of the stars in the _ hipparcos _  sample were therefore not obtained during the _ hipparcos _  mission . since the proper motions that are measured by differencing sky positions are angular velocities ,",
    "the distance to a star is necessary in order to convert the proper motions to space velocities .",
    "distances in astronomy are notoriously hard to measure precisely , and at the accuracy level of the _ hipparcos _  mission distances can only be reliably obtained for stars near the sun ( out to @xmath187 pc ) .",
    "in addition to this , since distances are measured as inverse distances ( parallaxes ) only distances that are measured relatively precise will have approximately gaussian uncertainties associated with them .",
    "balancing the size of the sample with the accuracy of the distance measurement leaves us with distance uncertainties that are typically @xmath188-percent , such that the tangential velocities that are obtained from the proper motions and the distances have low signal - to - noise .    of course , if we want to describe the distribution of the velocities of the stars in this sample , we need to express the velocities in a common reference frame , which for kinematical studies of stars around the sun is generally chosen to be the galactic coordinate system , in which the @xmath2-axis points towards the galactic center , the @xmath3-axis points in the direction of galactic rotation , and the @xmath189-axis points towards the north galactic pole @xcite .",
    "the measured tangential velocities are then projections of the three - dimensional velocity of a star with respect to the sun in the two - dimensional plane perpendicular to the line - of - sight to the star .",
    "therefore , this projection is different for each individual star .",
    "the discussion in the previous paragraphs shows that this sample of stars exhibits all of the properties ( incomplete data and noisy measurements , different from observation to observation ) for which a procedure such as the one developed in this paper is necessary .",
    "we have studied the velocity distribution of a sample of main sequence stars selected to have accurate distance measurements ( parallax errors @xmath190 ) and to be kinematically unbiased ( in that the sample of stars faithfully represents the kinematics of similar stars ) . in detail , we use the sample of @xmath191 stars from @xcite , but we use the new reduction of the _ hipparcos _  raw data , which has improved the accuracy of the astrometric quantities . a particular reconstruction of the underlying velocity distribution of the stars is shown in figure [ fig:4veldist ] , in which 10 gaussians are used and the regularization parameter @xmath131 is set to 4 km@xmath192 s@xmath193 .",
    "we choose this value for @xmath131 since we believe that the smallest scale of the features we can see is a few km s@xmath194 , because of the scale of the differences in rotational velocities around the galactic center of the stars in the @xmath187 pc around the sun ( which is rotating around the galactic center at @xmath195 km s@xmath194 at @xmath196 kpc ) .",
    "what is shown are two - dimensional projections of the three - dimensional distribution .",
    "the recovered distribution compares favorably with other reconstructions of the velocity distribution of stars in the solar neighborhood , based on the same sample of stars ( using a maximum penalized likelihood density estimation technique , * ? ? ?",
    "* ) , as well as with those based on other samples of stars for which three - dimensional velocities are available .",
    "all of the features in the recovered distribution function are real and correspond to known features ; this includes the feature at @xmath197 km s@xmath194 , which is known as the arcturus moving group .",
    "therefore , we conclude that the method developed in this paper performs very well on this complicated data set .    the convergence of the algorithm is shown in figure [ fig : convergence ] .",
    "only split - and - merge steps that improved the likelihood are shown in this plot , therefore , the actual number of iterations is much higher than the number given on the @xmath2-axis .",
    "it is clear that all of the split - and - merge steps only slightly improve the initial estimate from the first em procedure , but since what is shown is the likelihood per data point , the improvement of the total likelihood is more significant .",
    "the algorithm presented in this paper was implemented in the c programming language , depending only on the standard c library and the gnu scientific library .",
    "the code is available at http://code.google.com/p/extreme-deconvolution/ ; instructions for its installation and use are given there .",
    "the code can be compiled into a shared object library , which can then be incorporated into other projects or accessed through an idl wrapper function supplied with the c code .",
    "the code can do everything described above .",
    "the convergence of the algorithm is slow , which is mostly due to the large number of split - and - merge steps that can be taken by the algorithm ( the split - and - merge aspect of the algorithm , however , can easily be turned off or restricted by setting the parameter specifying the number of steps to go down the split - and - merge hierarchy ) .",
    "it is a pleasure to thank frdric arenou and phil marshall for comments and assistance .",
    "jb and dwh were partially supported by nasa ( grant nnx08aj48 g ) . during part of the period in which this research was performed ,",
    "dwh was a research fellow of the alexander von humboldt foundation of germany .",
    "the calculation of the posterior likelihoods @xmath39 in the e step ( eq",
    ".  [ [ eq : updateemincomplete ] ] ) can be tricky since it can be the ratio of a small probability over the sum of small probabilities , which can lead to underflow and a significantly different result for the @xmath39s .",
    "for instance , imagine a situation in which only one of the terms is slightly larger than the smallest positive number allowed by the compiler and all the other terms are slightly smaller than this number .",
    "the result of this would be that the @xmath39 corresponding to the gaussian with the largest probability will be set to 1 and all the other @xmath39s will be set to zero , while in reality the distribution is much more uniform than this .",
    "we will deal with this problem in two steps : ( a ) we will calculate not the numerator of @xmath39 but its logarithm , which will allow us to keep very small probabilities ( since in general the smallest negative number is much smaller than the logarithm of the smallest positive number set by the compiler ) ; ( b ) we will design a function that given the logarithms of all the @xmath39s returns the logarithm of their sum . then we can normalize the numerators by subtracting the logarithm of their sum from their logarithms .",
    "this function performs a kind of ` logsum ' .",
    "cleverly designing this function will allow us to obtain the sum of a set of numbers , each of which is smaller than the smallest positive number .",
    "we will denote the numerators of the @xmath39 by @xmath198 in what follows .",
    "the simplest way of summing a set of numbers given as logarithms would be to exponentiate each number and add it to the sum .",
    "however , @xmath199 might be below the smallest positive floating point threshold set by the compiler for each term . to avoid this underflow problem , we want to add a constant @xmath200 to each of the logarithms such that when exponentiated they are all above the threshold , after which we can sum them and take the logarithm of the sum .",
    "finally we can subtract this constant @xmath200 again from the final answer .",
    "in short , we use the identity @xmath201 - c \\,.\\ ] ]    to find what this constant @xmath200 should be we will define dbl_min to be the smallest positive floating point number available , and dbl_max to be the largest floating point number . avoiding underflow",
    "is then achieved when each term is larger than dbl_min , or , when @xmath202 > \\mbox{dbl\\_min}\\,,\\ ] ] which means @xmath203    however , this can be quite large and will therefore sometimes lead to _ overflow _ for the larger terms in the sum , which is arguably a greater problem than the original underflow .",
    "overflow will be avoided when the sum is smaller than dbl_max , which we can ensure by demanding @xmath204 < \\mbox{dbl\\_max}\\,,\\ ] ] where @xmath14 is the number of terms in the sum .",
    "this means that we should have @xmath205    the second bound obtained here is the most stringent because overflow is worse than underflow .",
    "this is simply because the underflow will only ignore a very small term , whereas overflow will lead to an infinite answer , which will affect any following calculation .",
    "moreover , as we are adding probabilities , i.e. , positive numbers , overflow of the kind described in the previous paragraph implies that the underflow is probably irrelevant ( and if it is not , the situation is completely hopeless ) .",
    "therefore , we should choose @xmath200 as @xmath206",
    "suppose we are given a gaussian distribution for a vector @xmath5 with mean @xmath18 and covariance matrix @xmath19 which can be partitioned into a @xmath207-dimensional and a @xmath208-dimensional component ( @xmath209 ) @xmath210 , @xmath211 , and @xmath212}\\ ] ] respectively , where @xmath213 .",
    "we can now ask what the distribution of @xmath214 marginalizing over @xmath215 is , and what the distribution of @xmath215 given @xmath214 is ? to answer this question we will rewrite the joint distribution function in such a way as to make clear the following identity @xmath216 where @xmath217 .",
    "first we note that we can block - diagonalize the covariance matrix as follows : @xmath218}{\\left[}\\begin{array}{cc } { { \\mathbf{v}}}_{11 } & { { \\mathbf{v}}}_{12 } \\\\ { { \\mathbf{v}}}_{21 } & { { \\mathbf{v}}}_{22 } \\end{array } { \\right]}{\\left[}\\begin{array}{cc } { { \\mathbf{i}}}_{d_1 } & 0 \\\\ -{{\\mathbf{v}}}_{22}^{-1 } { { \\mathbf{v}}}_{21 } & { { \\mathbf{i}}}_{d_2 } \\end{array } { \\right]}= { \\left[}\\begin{array}{cc } { { \\mathbf{v}}}_{11 } - { { \\mathbf{v}}}_{12 } { { \\mathbf{v}}}_{22}^{-1 } { { \\mathbf{v}}}_{21 } & 0 \\\\ 0 & { { \\mathbf{v}}}_{22 } \\end{array }   { \\right]}\\ .\\ ] ] using this we can write the argument of the exponential in the gaussian distribution as follows @xmath219}{^{\\scriptscriptstyle \\top}}{\\left[}\\begin{array}{cc } { { \\mathbf{v}}}_{11 } & { { \\mathbf{v}}}_{12 } \\\\ { { \\mathbf{v}}}_{21 } & { { \\mathbf{v}}}_{22 } \\end{array } { \\right]}^{-1 } { \\left[}\\begin{array}{c } { { \\mathbf{v}}}_1 - { { \\mathbf{m}}}_1 \\\\ { { \\mathbf{v}}}_2 - { { \\mathbf{m}}}_2 \\end{array } { \\right]}\\nonumber \\ , \\end{aligned}\\ ] ] which becomes using the identity in equation  ( [ eq : blockpartition ] ) @xmath220}{^{\\scriptscriptstyle \\top}}{\\left[}\\begin{array}{cc } { { \\mathbf{i}}}_{d_1 } & 0 \\\\ -{{\\mathbf{v}}}_{22}^{-1 } { { \\mathbf{v}}}_{21 } & { { \\mathbf{i}}}_{d_2 } \\end{array } { \\right]}{\\left[}\\begin{array}{cc } { { \\mathbf{v}}}_{11 } - { { \\mathbf{v}}}_{12 } { { \\mathbf{v}}}_{22}^{-1 } { { \\mathbf{v}}}_{21 } & 0 \\\\ 0 & { { \\mathbf{v}}}_{22 } \\end{array }   { \\right]}^{-1 } { \\left[}\\begin{array}{cc } { { \\mathbf{i}}}_{d_1 } & -{{\\mathbf{v}}}_{12 } { { \\mathbf{v}}}_{22}\\\\ 0 & { { \\mathbf{i}}}_{d_2 } \\end{array } { \\right]}{\\left[}\\begin{array}{c } { { \\mathbf{v}}}_1 - { { \\mathbf{m}}}_1 \\\\ { { \\mathbf{v}}}_2 - { { \\mathbf{m}}}_2 \\end{array } { \\right]}\\nonumber \\ , \\end{aligned}\\ ] ] which can be simplified to give the following @xmath221}{^{\\scriptscriptstyle \\top}}{\\left[}\\begin{array}{cc } { { \\mathbf{v}}}_{11 } - { { \\mathbf{v}}}_{12 } { { \\mathbf{v}}}_{22}^{-1 } { { \\mathbf{v}}}_{21 } & 0 \\\\ 0 & { { \\mathbf{v}}}_{22 } \\end{array }   { \\right]}^{-1 } { \\left[}\\begin{array}{c } { { \\mathbf{v}}}_1 - { { \\mathbf{m}}}_1   - { { \\mathbf{v}}}_{12}{{\\mathbf{v}}}_{22}^{-1 } ( { { \\mathbf{v}}}_2 - { { \\mathbf{m}}}_2 ) \\\\ { { \\mathbf{v}}}_2 - { { \\mathbf{m}}}_2 \\end{array } { \\right]}\\ .\\ ] ] introducing @xmath222 and @xmath223 this becomes @xmath224}{^{\\scriptscriptstyle \\top}}{\\left[}\\begin{array}{cc } { { \\mathbf{v}}}_{1|2 } & 0 \\\\ 0 & { { \\mathbf{v}}}_{22 } \\end{array }   { \\right]}^{-1 } { \\left[}\\begin{array}{c } { { \\mathbf{v}}}_1 - { { \\mathbf{m}}}_{1|2 } \\\\ { { \\mathbf{v}}}_2 - { { \\mathbf{m}}}_2 \\end{array } { \\right]}=}\\nonumber\\\\ & & -\\frac{1}{2 } ( { { \\mathbf{v}}}_1 - { { \\mathbf{m}}}_{1|2}){^{\\scriptscriptstyle \\top}}{{\\mathbf{v}}}_{1|2}^{-1 } ( { { \\mathbf{v}}}_1 - { { \\mathbf{m}}}_{1|2})-\\frac{1}{2 } ( { { \\mathbf{v}}}_2 - { { \\mathbf{m}}}_{2}){^{\\scriptscriptstyle \\top}}{{\\mathbf{v}}}_{22}^{-1 } ( { { \\mathbf{v}}}_2 - { { \\mathbf{m}}}_{2 } ) \\ .\\end{aligned}\\ ] ] using the identity in equation  ( [ eq : blockpartition ] ) it can also be shown that @xmath225 since the determinants of the left and right multiplying matrices on the left hand side of equation  ( [ eq : blockpartition ] ) are equal to one ( for block triangular matrices the determinant is equal to the product of the determinants of the diagonal blocks , these are both unit matrices ) . therefore we can write @xmath226 we can now show that this factorization corresponds to the factorization given in equation  ( [ eq : marginalizecondition ] ) by marginalizing over @xmath215 , i.e. , @xmath227 therefore , we can identify @xmath228 with @xmath229 .    to summarize",
    "we can write @xmath230",
    "the em algorithm is a very general algorithm designed to deal with missing data in maximum likelihood estimates and was first written down in full generality by @xcite , although versions of it had already been around longer ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "the name expectation - maximization captures its main ingredients : the em algorithm seeks to iteratively maximize the full - data likelihood ( i.e. , given data + missing data ) but since some of the data is missing , it takes the expectation of this full - data likelihood given the data and the previous model estimate ; the maximization step then maximizes this expectation of the likelihood with respect to the model parameters .",
    "the em algorithm properly should not be called an `` algorithm '' since it does not specify the actual sequence of steps actually required to carry out a single e- or m - step , however , this generality has attributed to much of its success and it is now applied to a large variety of problems and numerous extensions and different interpretations of the original algorithm have been proposed @xcite .",
    "the original em paper established its general properties and applied the algorithm to some specific examples , among which the case of finite mixtures . among the em algorithm s properties",
    "are that it converges monotonically to a local maximum of the likelihood function @xcite , a property that is shared by the so - called generalized em algorithms ( gem ) in which the m - step merely increases the likelihood instead of maximizing it .",
    "the convergence properties of the em algorithm are generally very good , i.e. , given an initial estimate far away from a local maximum it will converge rather quickly to a region of parameter space close to a local maximum and eventually reach the local maximum , as opposed to simple , generic optimizers such as the newton - raphson method which need a good initial estimate to find a local maximum and which does not converge monotonically .",
    "however , the convergence of the em algorithm can be very slow , i.e. , it is approximately linear , often many orders of magnitude slower than quadratic methods .",
    "numerous methods have been proposed to accelerate the convergence , e.g. , using the jacobian to find improved estimates of the model parameters @xcite ; employing a generalized conjugate gradient approach @xcite ; considering a hybrid approach that switches to a quadratically converging optimization method close to the local maximum ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) ( these hybrid approaches make use of the fact that most of the overall change in likelihood is achieved during the first few iterations @xcite ) ; or by extending the em algorithm through the use of conditional maximization in the m - step @xcite",
    ". these methods are generally less stable and much more complicated than the standard em algorithm @xcite .",
    "em is often easy to implement , especially when the e- and m- step are given by closed expressions , but a significant drawback is that it does not give an estimate of the covariance matrix of the mles .",
    "however , the em algorithm can be extended to give such estimates , either by considering the information matrix @xcite or by a bootstrap approach @xcite .    in a nutshell",
    "we can describe the em algorithm as follows : suppose we have a likelihood function @xmath231 for a set of parameters @xmath57 , dependent on the data @xmath232 and the missing data @xmath233 .",
    "the likelihood @xmath231 can then be maximized iteratively in two steps @xcite : in the @xmath234th e - step we calculate the expectation of the likelihood given the data and the current parameter estimate , that is , the function @xmath235 this function is then maximized in the m - step , leading to the new parameter estimate @xmath236 for many problems that on the surface are not missing data problems it can nevertheless be useful to phrase them in such a way that they can be handled by the em - algorithm , since often the maximization in the m - step is much simpler than the original maximization . for example , below we will show that by formulating the gaussian mixture density estimation problem as a missing data problem , a completely analytical solution for the e- and m - steps can be found .",
    "even when an analytical solution does not exist for the m - step , that maximization can ( 1 ) be lower dimensional than the original maximization ; ( 2 ) be handled by a gem algorithm @xcite in which a single step of a regular optimizer can be sufficient for overall convergence if it increases the likelihood ; or ( 3 ) be reduced to several conditional m - steps for which analytical solutions exist or that are of a lower dimensionality still @xcite",
    ".                                                                                                                                   is well fit by a power - law for spiral galaxies .",
    "the relation is found by fitting a single gaussian distribution to the distribution of points in the absolute magnitude@xmath238 plane for each bandpass . ]",
    "km@xmath192 s@xmath193 .",
    "the top right plot shows 1-sigma covariance ellipses around each individual gaussian in the @xmath239@xmath240 plane ; the thickness of each covariance ellipse is proportional to the natural logarithm of its amplitude @xmath60 . in the other three panels the density grayscale is linear and contours",
    "contain , from the inside outward , 2 , 6 , 12 , 21 , 33 , 50 , 68 , 80 , 90 , 95 , 99 , and 99.9 percent of the distribution .",
    "50 percent of the distribution is contained within the innermost dark contour .",
    "the feature at @xmath241 km s@xmath194 is real and corresponds to a known feature in the velocity distribution : the arcturus moving group ; indeed , all the features that appear in these projections are real and correspond to known features.,title=\"fig : \" ] +   km@xmath192 s@xmath193 .",
    "the top right plot shows 1-sigma covariance ellipses around each individual gaussian in the @xmath239@xmath240 plane ; the thickness of each covariance ellipse is proportional to the natural logarithm of its amplitude @xmath60 . in the other three panels the density grayscale is linear and contours",
    "contain , from the inside outward , 2 , 6 , 12 , 21 , 33 , 50 , 68 , 80 , 90 , 95 , 99 , and 99.9 percent of the distribution .",
    "50 percent of the distribution is contained within the innermost dark contour .",
    "the feature at @xmath241 km s@xmath194 is real and corresponds to a known feature in the velocity distribution : the arcturus moving group ; indeed , all the features that appear in these projections are real and correspond to known features.,title=\"fig : \" ]"
  ],
  "abstract_text": [
    "<S> we generalize the well - known mixtures of gaussians approach to density estimation and the accompanying expectation - maximization technique for finding the maximum likelihood parameters of the mixture to the case where each data point carries an individual @xmath0-dimensional uncertainty covariance and has unique missing data properties . </S>",
    "<S> this algorithm reconstructs the error - deconvolved or `` underlying '' distribution function common to all samples , even when the individual data points are samples from different distributions , obtained by convolving the underlying distribution with the unique uncertainty distribution of the data point and projecting out the missing data directions . </S>",
    "<S> we show how this basic algorithm can be extended with bayesian priors on all of the model parameters and a `` split - and - merge '' procedure designed to avoid local maxima of the likelihood . </S>",
    "<S> we apply this technique to a few typical astrophysical applications . </S>"
  ]
}