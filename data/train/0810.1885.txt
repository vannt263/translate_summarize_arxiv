{
  "article_text": [
    "the large - scale structure ( lss ) of the universe , as traced by galaxies and clusters , encodes important information about the basic cosmological parameters and also the physical process which underpin the formation of cosmic structures .",
    "lss measurements are now routinely used in conjunction with measurements of the cosmic microwave background ( cmb ) to constrain cosmological parameters ( e.g. cole  2005 ; sanchez  2006 ; tegmark  2006 ; percival  2007 ) . however , the estimation of errors in cmb measurements is , by comparison with lss work , quite sophisticated and rigorous .    to constrain the cosmological world model from the galaxy distribution ,",
    "large volumes and galaxy numbers are needed .",
    "the current generation of local galaxy surveys , such as the 2dfgrs ( colless  2001 ) and sdss ( york  2000 ) satisfy both of these requirements and can be divided into large sub - samples to study clustering trends with galaxy properties ( e.g. norberg  2001 , 2002 ; zehavi   2002 , 2004 , 2005 ; madgwick 2003 ; croton  2004 , 2007 ; gaztaaga  2005 ; li 2006 , 2007 ) . in anticipation of even bigger surveys at intermediate and high redshifts ( e.g. gama , vvds - cfhtls , euclid ) , it is timely to revisit the techniques used to estimate errors on clustering statistics from galaxy redshift surveys . in particular , we explore ways to robustly quantify reliable errors estimates on two - point clustering statistics in 3-dimensional space .",
    "this has important consequences for the determination of the cosmological parameters , both in terms of setting values and in the number of free parameters needed to describe the data , and in uncovering trends which can be used to constrain galaxy formation models .    in a perfect world , the optimal way to estimate the error on a clustering measurement would be to generate a large sample of independent mock galaxy catalogues that look the same as the data ( at least in so far as the observed data may be affected by sampling fluctuations , e.g. efstathiou & moody 2001 ) .",
    "the challenge here is to ensure that the mocks are a faithful reproduction of the observations .",
    "this is a more demanding requirement than one might at first think . in order to estimate accurate errors on the two - point statistics ,",
    "it is necessary that the mocks also reproduce the higher order clustering displayed by the data , since the errors on 2-point statistics implicitly depend on these higher order moments .",
    "this approach will inevitably involve n - body simulations , in order to accurately model the underlying dark matter , and so becomes computationally expensive .",
    "various techniques have been adopted in the literature to populate such simulations with galaxies ( for a survey see baugh 2008 ) .",
    "the number of simulations required is large , running into several tens to get reasonable estimates of the variance and hundreds or even thousands to get accurate covariance matrices .",
    "an alternative empirical method is to use the observed data itself to make an estimate of the error on the measurement .",
    "such `` internal '' error estimates use a prescription to perturb the data set in some way in order to make copies .",
    "these copies allow the statistical distribution which underlies the data to be probed without having to assume anything about its form . in this paper",
    "we investigate the performance of two common approaches , the jackknife and bootstrap internal error estimates ; both will be compared to the errors calculated from a large suite of simulated mock data sets .",
    "the jackknife method was developed as a quite generic statistical tool in the 1950s ( quenouille 1956 ; tukey 1958 ) .",
    "the bootstrap method ( e.g. efron 1979 ) is a modification or extension of the jackknife made possible by the availability of fast computers in the 1970s .",
    "these internal estimators first divide the data set into subsamples , which consist either of individual objects or groups of objects , which are then resampled in a particular way ( see section 2 for a full description of the procedure ) .",
    "the first applications of either technique to astronomical observations date from the early 1980s with the analysis of the velocities of galaxies in clusters ( lucey 1979 ; bothun  1983 ) .",
    "the bootstrap method was first applied to galaxy clustering by barrow , bhavsar & sonoda ( 1984 ; see also ling , frenk & barrow 1986 ) .    in these early works sub - samples",
    "were simply taken using the individual objects themselves .",
    "however , as pointed out by fisher  ( 1994 ) , this can lead to unreliable errors . from a monte - carlo analysis using a series of n - body simulations , fisher  showed that when sampling individual galaxies the bootstrap method underestimates the error on the density estimate in voids and , likewise , overestimates it in clusters .",
    "mo , jing & brner ( 1992 ) present an analytic estimate for the errors on 2-point statistics and show that the bootstrap errors obtained by resampling using individual galaxies give incorrect errors compared with the ensemble average under certain conditions .",
    "these valid objections can be avoided by resampling the data set divided into sub - volumes instead of individual galaxies .",
    "hence , in this paper , we generate copies of data sets by selecting sub - volumes of the data set .",
    "resampling of patches or regions of surveys has already been applied to error estimation in lss analyses .",
    "the jackknife technique has been extensively used in projection , including the angular clustering of galaxies ( scranton 2002 ) , in the analysis of cmb maps ( e.g. gaztaaga  2003 ) and , perhaps most heavily , in efforts to detect the integrated sachs - wolfe effect by cross - correlating sky maps of galaxy or cluster positions with temperature fluctuations in the cosmic microwave background ( fosalba , gaztaaga & castander 2003 ; fosalba & gaztaaga 2004 ; cabr  2007 ) . in projection ,",
    "the jackknife variance agrees well with monte - carlo estimates from mock catalogues ( e.g. cabr  2007 ) .",
    "this is perhaps an easier case than the three dimensional surveys dealt with in this paper , since the distributions of galaxy fluctuations will tend to look more gaussian in projection ( although distinct non - gaussian behaviour is clearly evident in the projected galaxy fluctuations , e.g. gaztaaga 1994 ) .",
    "the jackknife method has also been applied by many authors to the estimation of errors on galaxy clustering in three dimensions using volume resampling .",
    "zehavi  ( 2002 ) carried out a simple test of the accuracy of the jackknife estimate of the variance and found that the jackknife produced an essentially unbiased estimate , but with a scatter that could approach 50% . here",
    "we carry out a more exhaustive comparison of internal error estimators with the `` external '' monte - carlo simulation methods .",
    "the paper is arranged in the following way .",
    "section  [ sec : err_est ] defines the three error estimators we use throughout , while section   [ sec : err_met ] introduces the simulations which will be used to generate fake data sets for our numerical experiments , along with the clustering statistics and the principal component decomposition , needed for the error estimator comparison .",
    "section  [ sec : err_anal ] presents the raw results from three different error techniques and remains rather technical .",
    "we consider in section  [ sec : test_case ] a simple test case scenario , where the implications of using different error estimators for a `` straightforward '' two parameter fit to the projected correlation function are inferred .",
    "we summarize our findings in section  [ sec : conclusion ] .",
    "there are numerous ways to estimate the error on a clustering measurement . in this section ,",
    "we give an outline of four of the most popular non - parametric methods in use in the literature , three of which we compare in this paper .",
    "we do not consider in this paper analytic error estimates ( like poisson ) , nor parametric error estimates , like those derived from gaussian or log - normal density fields .",
    "the latter methods are commonly used for estimating errors in the linear clustering regime ( see e.g. percival  2001 and cole  2005 for applications to power spectrum estimates ) .",
    "below , we describe three `` internal '' methods which use the data itself to derive an estimate of the error on a measurement , the sub - sample , jackknife and bootstrap techniques ( sections  [ sec : data_errsubsample ] , [ sec : data_errjk ] and  [ sec : data_errbs ] )",
    ". then we describe in section  [ sec : mock_err ] the commonly used `` external '' method of creating monte - carlo realizations or reproductions of the data .",
    "we close by giving an overview of the advantages and disadvantages of each method in section  [ sec : overview ] .",
    "each of the techniques involves performing measurements on copies of the data in order to sample the underlying probability distribution of the quantity we are trying to measure .",
    "the internal estimates make copies or resamplings from the observed data whereas the external method generates fake data sets , without manipulating the observed data .",
    "three assumptions are implicit in the internal or `` data inferred '' approaches : 1 ) the data gives an accurate representation of the underlying probability distribution of measurements ; 2 ) the sub - samples into which the data is split are sufficient in number to allow accurate enough estimates of the errors and 3 ) the sub - sample volumes are large enough to be representative .",
    "condition 1 would be violated if a second independent measurement from an equivalent data set gave a significantly different estimate of the measurement .",
    "in such a case , this would mean that the original data set is subject to sampling fluctuations ( sometimes called cosmic variance ) which affect the clustering measurements .",
    "conditions 2 and 3 are related .",
    "the number of sub - samples that should be used in the internal estimators depends on the questions to be answered and the ( unknown ) form of the underlying probability distribution : for example , to obtain an estimate of the variance accurate to 10% would require 50 resamplings of the data set for a gaussian distribution .",
    "at the same time , we need to be careful not to make the sub - samples so small so that they become strongly correlated .",
    "norberg & porciani ( in prep . )",
    "investigate this problem using the two - pair correlation function .",
    "later we address the question of whether or not one can put qualitative constraints , and maybe even quantitative constraints ( using a large suite of simulations ) , on the representativeness of a given size of sub - sample .",
    "the first applications of non - parametric internal techniques to the estimation of the error on the two - point correlation function considered the removal of individual objects from the data set ( barrow , bhavsar & sonoda 1984 ; ling , frenk & barrow 1986 ) .",
    "this is computationally infeasible for modern galaxy catalogues , and it has been shown at galaxy number densities currently considered that this type of error estimate strongly underestimates the true clustering uncertainty ( e.g. mo , jing & brner 1992 ; fisher  1994 ) . furthermore , we want to reduce the correlation between the sub - samples into which the data set is split .",
    "for this reason , we divide the samples by volume and split our data sets into  cubes of equal volumes .",
    "the internal methods then reconstruct copies of the original data sets , choosing or weighting the  sub - volumes in different ways .",
    "for each copy or resampling of the data , we make an estimate of the correlation function , which we denote by @xmath4 in this section . here",
    "the subscript @xmath5 refers to the bin of spatial separation and the superscript @xmath6 refers to the resampling of the data for which the correlation function is measured .",
    "note that we use the terms `` a resampling '' and `` copy '' of the data set interchangeably ; here , a resampling refers to a `` full '' copy of a data set , rather than to the act of selecting an individual sub - volume .",
    "the covariance matrix of @xmath7 independent realizations is , by definition , given by @xmath8 where it is assumed that the mean expectation value , @xmath9 , is not estimated from the @xmath10 samples , but from an independent realization of the data .",
    "hence , the simplest error method , commonly referred to as the sub - sample method , consists of splitting the data set into   independent samples and estimating the covariance matrix using eq .",
    "[ eq : cov_mock ] , where the clustering statistic is estimated for each one of the sub - samples separately . for  independent sub - samples ,",
    "this returns the correct covariance for a sample of volume 1/  of the original volume , implying that the covariance of the full dataset is actually  times smaller ( as the variance scales directly with the volume considered ) .",
    "this method has been used in several studies , in particular where the survey volumes considered are large ( e.g. maddox  1990 , hamilton 1993a and fisher  1994 for the apm , the iras 2jy and the iras 1.2jy galaxy surveys respectively ) .",
    "however , one basic assumption made in this approach is never really satisfied for galaxy clustering studies in the universe : the sub - samples are never fully independent of each other , irrespective of the clustering scales considered .",
    "this is due to the presence of long - range modes in the density fluctuations , making all sub - samples to some extent correlated with each other .",
    "this can be related to the fact that the correlation function has a small but non - zero value on large scales .",
    "therefore there is a need to consider alternative internal estimators , accounting hopefully for these limitations .",
    "hereafter we will not consider the sub - sample method , even though it has been extensively used in the past .",
    "we consider the `` delete one jackknife '' method ( shao 1986 ) .",
    "a copy of the data is defined by systematically omitting , in turn , each of the  sub - volumes into which the data set has been split .",
    "the resampling of the data set consists of the @xmath11 remaining sub - volumes , with volume @xmath12 times the volume of the original data set .",
    "the clustering measurement is repeated on the copy or resampling of the original data set . by construction , there are only @xmath13  different copies of the data set that are created in this way . the covariance matrix for @xmath7 jackknife resamplings is then estimated using @xmath14 where @xmath15 is the @xmath16 measure of the statistic of interest ( out of @xmath7 total measures ) , and it is assumed that the mean expectation value is given by @xmath17 note the factor of @xmath18 which appears in eq .",
    "[ eq : cov_jk ] ( tukey 1958 ; miller 1974 ) .",
    "qualitatively , this factor takes into account the lack of independence between the @xmath7 copies or resamplings of the data ; recall that from one copy to the next , only two sub - volumes are different ( or equivalently @xmath19 sub - volumes are the same ) .",
    "hereafter , we will refer to a jackknife estimate from  sub - samples as jack- .",
    "a standard bootstrap resampling of the data set is made by selecting  sub - volumes at random , with replacement , from the original set ( efron 1979 ) .",
    "effectively , a new weight is generated for each sub - volume . in the original data set , all sub - volumes have equal weight . in a resampled data",
    "set , the weight is simply the number of times the sub - volume has been selected e.g. @xmath20 .",
    "the clustering measurement is repeated for each resampled data set . for a given",
    ", the mean fractional effective volume of the resampled data sets tends to a fixed fraction of the original sample volume . for = ,",
    "the mean effective volume is less than the volume of each of the jackknife resamples .",
    "we further develop this discussion in ",
    "[ sec : variance_rel ] and  [ sec : boot_resampling ] .    unlike for jackknife",
    "error estimates , in principle there is no limit on the number @xmath7 of resamplings or copies of the data for bootstrap error estimates . in practice",
    ", the variance on a measurement converges relatively slowly with increasing numbers of trials ( efron & tibshirani 1993 ) .",
    "furthermore , for our application , resamplings are cheap to generate but expensive to analyse . for a data set divided into  sub - volumes and from which one draws sub - volumes at random with replacement , there are @xmath21 different possible bootstrap resamplings , which even in the modest example of = = 10 corresponds to 92  378 different bootstrap resamplings . here",
    "we restrict ourselves to of the order of one hundred resamplings ( i.e. @xmath22 ) . until now ,",
    "most , if not all , bootstrap clustering estimates have used resamplings consisting of = sub - volumes . in this paper",
    "we test this assumption by considering up to 4  sub - volumes to construct each resampling of the data set ( section  [ sec : boot_resampling ] ) .",
    "the covariance matrix for @xmath7 bootstrap resamplings of the data is given by @xmath23 where it is assumed that the mean expectation value is given by eq .",
    "[ eq : mean_xi ] .",
    "note that there is no @xmath18 factor in the numerator of this expression , as was the case for the jackknife .",
    "qualitatively the data set copies are thought of as being `` more '' independent in the case of bootstrap resampling than for jackknife resampling , something we address in detail in sections  [ sec : err_anal ] and  [ sec : test_case ] .",
    "in what follows we will refer to the mean bootstrap estimate from  sub - samples as boot- .",
    "the monte carlo method consists of creating @xmath7 statistically equivalent versions of the data set being analysed , on each of which the full analysis is repeated .",
    "technically , bootstrap resampling is also a monte - carlo method .",
    "however , the distinction here is that what we have termed the monte carlo approach makes no explicit reference to the observed data set to generate the synthetic data sets .",
    "we are not resampling the data in any way . instead",
    ", we assume that we know the underlying statistical or physical processes which shaped the observed data set and feed these into a computer simulation . here",
    "we have run n - body simulations which model the clustering evolution of the dark matter in the universe ( see section  [ sec : icc1340 ] ) .",
    "the monte carlo element in this case refers to the initial conditions , which are drawn from a distribution of density fluctuations consistent with cosmological constraints ( see e.g. sanchez  2006 ) .",
    "the level of realism of the computer simulation determines the cost of this method . for a clustering analysis of an observed galaxy catalogue ,",
    "the demands on the monte carlo approach are even greater as the n - body simulations need to be populated with galaxies , according to some prescription , with the goal of statistically reproducing the galaxy sample as faithfully as possible ( see e.g. baugh 2008 for a review of the techniques used to build synthetic galaxy catalogues ) .",
    "we hereafter refer to the monte carlo catalogues generated in the external error estimation as `` mocks '' , and their associated errors as mock errors . in the current work using n - body simulations , we use eq .",
    "[ eq : cov_boot ] as the definition of the monte carlo covariance matrix , since we define our `` reference sample '' as the mean measurement of the correlation function extracted from the ensemble of simulations .",
    "each of the error estimation techniques described above has its advantages and disadvantages . by definition ,",
    "errors calculated directly from the data take into account any hidden or unforeseen systematics and biases that might otherwise be missed .",
    "this is particularly important for clustering statistics , where the errors on the 2-point correlation function also depend on the higher order clustering of the data .",
    "these properties of the galaxy distribution are usually not otherwise appropriately accounted for in an error analysis .",
    "other properties of the data , such as the galaxy mix and survey selection , are also naturally satisfied in an internal approach . in the case of external error estimation using mocks , only the statistics that have been deliberately included in the monte carlo realization are guaranteed to be taken into account in the error analysis . if a biasing scheme has been constrained to reproduce the two - point function of galaxy clustering , there is no guarantee that the higher order moments of the distribution will also match those of the observed data set .",
    "on the other hand , internal error estimates are often severely limited by the size of the data set itself .",
    "this can be particularly problematic for clustering statistics , as studied here .",
    "obviously , sampling or cosmic variance on scales larger than sample volume can not be addressed by internal estimates , but can be included in external estimates made using mocks .",
    "our aim in this paper is to repeat the early comparisons of internal and external error estimates in the context of current and forthcoming galaxy redshift surveys .",
    "we draw data sets from numerical simulations which are described in section  [ sec : icc1340 ] .",
    "the clustering analysis of these data sets is described in section  [ sec : xi_2pt ] .",
    "finally , in section  [ sec : pca ] , we give an outline of the principal component decomposition of the covariance matrix of clustering measurements , which is an invaluable diagnostic in error estimation .",
    "our analysis uses the @xmath24 output from the l - basicc ensemble of n - body simulations carried out by angulo  ( 2008 ) .",
    "the set comprises 50 moderate resolution runs , each representing the dark matter using @xmath25 particles of mass @xmath26 in a box of side @xmath27mpc . each l - basicc run was evolved from a different realization of a gaussian density field set up at @xmath28 .",
    "the adopted cosmological parameters are broadly consistent with recent data from the cosmic microwave background and the power spectrum of galaxy clustering ( e.g. sanchez et  al .",
    "2006 ) : @xmath29 , @xmath30 , @xmath31 , @xmath32 , and @xmath33 .",
    "throughout we assume a value for the hubble parameter of @xmath34 .",
    ".a summary of the numerical experiments conducted in this paper .",
    "1 lists the error estimation technique applied ; col .",
    "2 gives the number of sub - samples into which each data set is split up ; col .",
    "3 indicates whether the analysis was performed in real ( r ) or redshift ( z ) space ; col .",
    "4 gives the number of different data sets used ; col .",
    "5 shows the number of resamplings and clustering measurements performed for each data set , @xmath7 ; col .",
    "6 lists , for the case of bootstrap errors only , the relative number of sub - volumes selected at random with replacement from the original list w.r.t . ; col .  7 shows the sampling fraction w.r.t . our nominal mean density which is set to match that of a  galaxy sample .",
    "the first group of experiments yielded our main results and used in the majority of the plots ; the other experiments are variants to test different components of the analysis and referred to in the text . [ cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]     the combination of a large number of independent realizations and the huge simulation volume make the l - basicc ensemble ideal for our purposes .",
    "angulo et  al .",
    "( 2008 ) showed that nonlinear and dynamical effects are still important for the evolution of matter fluctuations even on scales in excess of @xmath35mpc and that these scales contribute to the growth of smaller scale perturbations .",
    "a smaller simulation volume would not be able to model the growth of fluctuations accurately because these long wavelength modes would be missed .",
    "the large volume also means that it is still possible to extract more than one region from each simulation which can be considered as being effectively independent from the others due to their large spatial separation .",
    "equally important it is also possible to mimic with such a large simulation volume the real situation of a survey like the 2dfgrs , for which there are two distinct survey regions in two nearly opposite directions on the sky .",
    "from each of the 50 simulation cubes we extract two cubical sub - volumes of @xmath36 on a side which are separated by at least @xmath37 from one other .",
    "we could have extracted more than 40 volumes of this size from each simulation cube without choosing the same volume twice .",
    "however , we chose to follow this more conservative approach as we want to be able to treat the two sub - volumes as being effectively independent of one another .",
    "although they come from the same simulation volume , the fluctuations on scales greater than @xmath38 are relatively weak .",
    "hence , the total number of available data sets constructed in this way comes to 100 , each of which is fully independent of 98 of the others and essentially independent of the remaining 99@xmath39 data set .",
    "the size of each data set is chosen to match the volume of a typical  volume - limited sample extracted from the main sdss galaxy redshift survey ( e.g. zehavi  2004 , 2005 ) .",
    "note that this volume is about 10 times larger than the equivalent volume - limited samples of  galaxies used in the final 2dfgrs clustering analyses ( norberg  in prep . ) .",
    "finally , we have randomly diluted the number of dark matter particles in each data set in order to match the number density of a  galaxy sample of @xmath40 , which mimics the discreteness or shot noise level in typical observational data sets .",
    "note that on this occasion , we do not attempt to model a particular galaxy sample in detail , as our aim here is to provide a generic analysis that is as widely applicable and reproducible as possible , within a framework which is likely to remain the same in the foreseeable future .",
    "moreover , if one attempted to model a galaxy sample rather than the dark matter , this would open up the issue of how well can the model reproduce the higher order clustering of the galaxies .",
    "this is a problem to be addressed by those building mock galaxy catalogues for particular surveys and is beyond the scope of the current paper . by focusing on dark matter clustering only , our analysis remains well defined and fully accurate within our chosen cosmological model .",
    "a summary of the samples and error calculations carried out is given in table  [ tab : sum_runs ] .",
    "each data set drawn from a simulation can be divided into sub - volumes in order that it be resampled according to the jackknife or bootstrap algorithm .",
    "thus , we have up to 100 independent experiments for which we can compare internal estimates of the error on the correlation function , and consider for the first time in detail the whole error distribution of internal error estimates .",
    "the external `` mock '' estimate of the error is obtained using all 100 data sets . in all , we have performed about 250 thousand correlation function estimates , representing about one million cpu hours of calculations .      throughout this paper",
    "we consider only standard two - point clustering statistics and we delay to a future paper the study of more recent two - point clustering statistics , like the @xmath41-statistic of padmanabhan , white & eisenstein ( 2007 ) .",
    "we measure the two point correlation function , @xmath42 , as a function of pair separation perpendicular to ( @xmath43 ) and parallel to ( @xmath44 ) the line of sight of a fiducial observer , in real ( x=@xmath45 ) and in redshift space ( x=@xmath46 ) . in order to remain as comparable as possible with the analysis of observational data ,",
    "we do _ not _ use the common distant observer approximation , but instead perform a realistic angular treatment with a location chosen for the observer within the simulation box , i.e. : @xmath47 where @xmath48 and @xmath49 are the position vectors of the two objects in the pair and a hat indicates that the vector is appropriately normalized ; @xmath50 is the mean distance to the pair of objects along the line of sight from the observer .",
    "redshift space distortions are modelled in the same way , i.e. : @xmath51 where @xmath52 is the galaxy s redshift without peculiar motions , @xmath53 is its peculiar velocity in addition to the hubble flow and @xmath54 is the speed of light .",
    "we calculate the two dimensional correlation function @xmath42 for each data set using standard estimators , e.g. hamilton ( 1993 ) and landy & szalay ( 1993 ) . in order to obtain a robust estimate of the mean density",
    ", these estimators require that a large number of points be randomly distributed within the boundaries of the data set , according to the angular and radial selection functions which define the data .",
    "the number of randomly distributed points is typically larger than the number of data points by a factor of @xmath55 , and we use the technique outlined in landy & szalay ( 1993 ) to speed up the calculation of the random - random pair counts . in practice , to avoid fluctuations in the mean density at small pair separations , we typically use 4 times as many random points as data points and repeat the estimate of the pair counts about 25 times .",
    "we use a unique set of randoms for each calculation , so that our statistics are not influenced by any features in the randoms , which when an experiment is repeated numerous times could actually show up in a systematic fashion . finally to compute the @xmath56 estimate ,",
    "we limit the counts to pairs separated by less than 10 degrees for two reasons : 1 ) results effectively estimated in redshift space can be properly interpreted using the redshift space distortion model of kaiser ( 1987 ) , developed in the distant observer approximation ; 2 ) the projected correlation function , defined below , can be considered as a real space quantity .",
    "these issues are discussed in greater detail in e.g. matsubara ( 2000 ) and szapudi ( 2004 ) , but also applied in some clustering studies , like hawkins  ( 2003 ) and cabr & gaztaaga ( 2008 ) .",
    "using logarithmic binning in both the @xmath43 and @xmath44-directions , we estimate the projected correlation function , ( sometimes also written as @xmath57 and with @xmath58 ) , by integrating @xmath42 in the @xmath44 direction ( see eq .",
    "[ eq : wp_rp ] below ) .",
    "the only real motivation for this exercise is that the projected correlation function is , in theory , free from distortions arising from gravitationally induced peculiar motions .",
    "this point is discussed further in section  [ sec : sum_runs ] .",
    "we also estimate the spherically averaged correlation functions , @xmath59 and @xmath60 in `` @xmath45 '' real and `` @xmath46 '' redshift space respectively , by averaging over shells in pair separation given by @xmath61 . as for the projected correlation function we use logarithmic binning and accumulate pair counts directly , i.e. not by integrating over the 2-d correlation function estimate .    given @xmath42 ,",
    "the different clustering statistics are related by @xmath62 with x=@xmath45 or @xmath46 , real and redshift space respectively . here ,",
    "the integral for @xmath63 is carried out to a maximum value of the separation along the line of sight of @xmath64 .",
    "theoretically  is only a true real space quantity when @xmath65 . in section  [ sec : sum_runs ]",
    "we discuss the systematic implications of a finite choice for @xmath64 .",
    "the covariance matrix of correlation function measurements , is not , in itself , of much interest .",
    "however , the inverse of the covariance matrix plays a pivotal role , appearing , for example , in all simple @xmath66 estimates . as matrix inversions are highly non - linear by nature , the impact of noise in the estimate of the covariance matrix is hard to judge , unless extensive tests are carried out .",
    "a number of procedures exist to help with the matrix inversion and analysis .",
    "first , we prewhiten the covariance matrix , which means that all diagonal terms are rescaled to unity and all non - diagonal terms are rescaled to fall between -1 and 1 , @xmath67 where @xmath68 is the normalized covariance matrix , and @xmath69 , hereafter also referred to as @xmath70 .",
    "second , we decompose the @xmath71 normalized covariance matrix into its principal components , solving the eigen - equations : @xmath72 where @xmath73 and @xmath74 are the normalized eigenvalues and eigenvectors of the normalized covariance matrix .",
    "we have deliberately defined @xmath75 as the dimension of the covariance matrix actually used in the analysis , and not assumed it to be necessarily equal to the total number of data points for the given statistic ( e.g. the number of bins of pair separation in the correlation function ) . even though the parts of the covariance matrix that are used do not depend on those that are not used , principal component decomposition and matrix inversion strongly rely on the full matrix set up . for that reason ,",
    "it is essential to pre - define the parts of the covariance matrix to be considered , a fact that is most often neglected .",
    "there are many useful properties of principal component decomposition worth listing here ( see for example kendall 1975 ; press et  al . 1992 ) :    * the @xmath76 form a complete orthogonal basis , from which the whole covariance matrix can be reconstructed .",
    "* a principal component decomposition minimises the error for the number of components used . * a ranking of the eigenvectors in order of decreasing eigenvalue highlights the principal trends of the covariance matrix . * for a given covariance matrix there is a unique set of principal components .",
    "comparing covariance matrices is equivalent to the comparison of principal components .    with respect to this last point , it is essential to understand noise in the statistic and its non - trivial effect on estimating the principal components of the covariance matrix",
    ". clearly , in the absence of noise , two covariance matrices are identical if , and only if , their principal components are identical . in real systems that contain noise this is no longer true .    for this paper",
    "the most important use of the principal component decomposition technique is to redefine the @xmath66 of a model given the data : @xmath77 with @xmath78 defined by eq .",
    "[ eq : cov_norm ] , and where @xmath79 is the rotation matrix composed of the unique eigenvectors @xmath74 previously defined in eq .",
    "[ eq : eigen ] . the beauty of eq .",
    "[ eq : chi2_pca ] is that , when summed over all @xmath75 terms , it is exactly equivalent to the standard @xmath66 given in eq .",
    "[ eq : chi2 ] . additionally , the properties of the principal component decomposition ensure it yields the most efficient representation of the covariance matrix if we truncate it to include fewer than @xmath75 modes .",
    "hence , hereafter , the number of principal components considered is given by n@xmath80 , and only when n@xmath81 are we considering a full covariance matrix analysis .",
    "in this section we present our main results , carrying out a systematic comparison of applying different error estimation techniques to data sets which are representative of current and forthcoming galaxy surveys .",
    "the numerical experiments we have carried out are summarised in table  [ tab : sum_runs ] . to recap , we have created 100 test data sets by extracting independent volumes from an ensemble of n - body simulations (  [ sec : icc1340 ] ) . in our fiducial case , the density of dark matter particles in the test data sets has been diluted to match the abundance of  galaxies .",
    "each data set can be divided into equal sized cubical sub - volumes and resampled according to the jackknife and bootstrap algorithms to make internal estimates of the error on the two - point correlation function .",
    "we examine the scatter in the internal error estimates by comparing results obtained from different data sets . in our analysis",
    ", we vary the number of sub - samples each data set is divided up into , the sampling rate of the particles , the number of sub - volumes selected to construct a copy of the data set in the case of the bootstrap and also show results for clustering measured in real and redshift - space .",
    "our benchmark in this study is the external estimate of the error obtained by treating our data sets as 100 independent experiments .",
    "this is regarded as the  truth \" in our paper .",
    "this section is quite long and rather detailed .",
    "for that reason , on a first reading of the paper the reader may wish to go directly to the summary in section  [ sec : conclusion ] or to the case study in section  [ sec : test_case ] . in section  [ sec : sum_runs ]",
    "we present the correlation function measurements made from the ensemble of data sets .",
    "we then go through different aspects of the error analysis : in section  [ sec : variance_rel ] we compare the relative errors obtained from the different techniques ; in section  [ sec : error_dist ] we look at the uncertainty in the error ; in section  [ sec : eigen_val ] and section  [ sec : eigen_vec ] we examine the distribution of eigenvalues and eigenvectors respectively of the covariance matrices produced by each approach , before ending in section  [ sec : stab_covmat ] with a discussion of the stability of the inversion of the respective covariance matrices .",
    "we present results for the projected correlation function and the spherically averaged correlation functions in both real and redshift space .",
    "selected results later on in the section depend on the number of bins used to represent the correlation function . in most plots we focus on the scales",
    "@xmath82 for the following reasons .",
    "first , this range is accurately modelled in the l - basicc simulations , and can be measured reliably within all of the sub - volumes the data sets are split into .",
    "second , the model fitting presented in section  [ sec : test_case ] is only valid down to @xmath83 , and it therefore makes sense to present other results over the same range of scales for comparison .",
    "third , this range is the most appropriate to draw direct analogies with more general analyses of the error in galaxy clustering measurements . on sub - mpc",
    "scales the precise way that galaxies are distributed within their host halo strongly influences the correlation function and its error . in real - space , scales larger than @xmath84 are even more problematic as estimates can be affected by systematics at the @xmath85  % level ( see fig .  [",
    "fig : clustering ] ) , if special care is not taken when accounting for redshift space distortions .",
    "fourth , this choice of scale produces clustering estimates that can be split into sufficiently small but still well sampled bins .",
    "we first present measurements of the basic correlation functions we will be working with throughout the remainder of the paper . fig .",
    "[ fig : clustering ] shows the spherically averaged correlation functions , @xmath59 and @xmath60 , and the projected correlation function , , in the left and right panels respectively , as estimated from the 100 independent data sets of @xmath36 aside . in the left panel of fig .",
    "[ fig : clustering ] , we see the clear impact of redshift space distortions on the measured @xmath60 . on small scales ,",
    "the clustering signal is severely damped by the peculiar motions of the dark matter particles ( responsible for the fingers - of - god due to cluster - mass haloes in a @xmath86 representation ) .",
    "on intermediate scales , large scale infall enhances the clustering amplitude compared with the real space measurement ( see peacock & dodds 1994 for a model of the small and large scale redshift space distortion ) .",
    "@xmath59 shows the well known shoulder between 1 and @xmath87 ( zehavi  2004 ) , a sign of the transition between the one and two halo terms used in halo occupation distribution models ( e.g. berlind  2003 ) .    in the right hand panel of fig .",
    "[ fig : clustering ] we show the projected correlation functions , , in real and redshift space . by real - space , here",
    "we mean that we integrate over an estimate of @xmath88 made without including peculiar motions .",
    "the comparison between these results illustrates the sensitivity of the projected correlation function to the choice of @xmath64 in eq .",
    "[ eq : xi_proj ] . theoretically , if one integrates out to @xmath89 , eq .",
    "[ eq : xi_proj ] would return a purely real space quantity .",
    "for numerical as well as computational reasons , since the number of pairs increases in proportion to the additional volume included as @xmath44 increases .",
    "] we restrict the integration to @xmath90 .",
    "even at @xmath91 there is a systematic difference of 10  % in this case between  estimated for data in real and redshift space .",
    "this difference increases with scale , and by @xmath92 it is greater than 50  % .",
    "due to the large sub - volumes used this systematic effect is statistically non - negligible . taking the individual errorbars at face value ( which we show below to be ill - advised ) , at @xmath84",
    "this difference is about 1-@xmath3 . for comparison purposes ,",
    "we plot  estimated in real space data using @xmath93 ( cyan ) . even in real space",
    ", the chosen value of @xmath64 has non - negligible consequences .",
    "the fact that the real and redshift space measurements of the projected correlation function do not perfectly agree in the right hand panel of fig .",
    "[ fig : clustering ] has important implications for the interpretation of .",
    "an estimate of the projected correlation function made from data in redshift space will still be influenced by the underlying redshift space distortions if the integral is not performed out to large enough scales . in our analysis ,",
    "the magnitude of the systematic shifts is calculated for simulated cold dark matter particles . in the case of real galaxies , some",
    "of which are strongly biased with respect to the underlying dark matter , the discrepancy could be even larger .",
    "one should therefore be cautious about considering  as a `` real space '' measurement , and take even more care in understanding the effect of the limiting @xmath64 value .      in fig .",
    "[ fig : variance ] we present the mean relative variance of @xmath59 ( left ) , @xmath60 ( centre ) and  ( right ) , all as function of scale for different error estimators .",
    "the mean relative variance for each statistic is defined in the top left corner of each panel .",
    "the relative variance on the clustering measurements estimated from the scatter over the 100 data sets is robust ( solid black ) .",
    "if we consider the first 50 and last 50 data sets as separate ensembles ( solid red ) , we find very similar results for the relative variance .",
    "however , for all three statistics considered ( @xmath59 , @xmath60 and ) , neither the bootstrap nor jackknife is able to fully recover the relative variance of the mocks as function of scale .",
    "bootstrap estimates for which the number of sub - volumes selected , , is equal to  tend to systematically overestimate the relative variance by 40 - 50  % on all scales .",
    "jackknife estimates show a scale and statistic dependent bias , varying from no bias on scales larger than @xmath94 , to a 25  % overestimate for , and as much as 200 to 400  % for @xmath59 and @xmath60 on sub - mpc scales .",
    "we have checked that these differences are not due to a systematic uncertainty in the recovered mean of each statistic : even when rescaling the variance by a simple power - law , the conclusions remain unchanged",
    ".    a closer look at the data set copies generated here with the bootstrap method reveals that the mean effective volume of the copies is only @xmath95 of the volume of the original data set ( i.e. in the case where the number of sub - samples selected at random with replacement equals the number of sub - samples the data set is divided into , =) .",
    "perhaps somewhat surprisingly , this value for the mean effective volume depends only weakly on the number of sub - volumes into which the original data set is divided : in fact , for a data set split into a large enough number of sub - volumes the mean is insensitive to the exact  value .",
    "this reduction in effective volume could , in principle , explain why the mean relative variance is systematically larger for the bootstrap than that expected from the mocks .",
    "to increase the effective volume of the bootstrap copy , we need to sample more sub - volumes , i.e. oversample the number of sub - volumes w.r.t .",
    "the standard case = .",
    "we have experimented with oversampling the number of sub - volumes 2 , 3 and 4 times ( lower half of table  [ tab : sum_runs ] ) . as expected",
    ", the mean relative effective volume does tend then towards unity : for  = 2  , 3  and 4  , the mean relative effective volumes are @xmath96 , @xmath97 and @xmath98 respectively .",
    "the effect of this on the mean relative variance is discussed further in  [ sec : boot_resampling ] .",
    "the shapes of the relative variance computed using the different methods are different as a function of scale . while @xmath99 for @xmath100 and @xmath60 show a clear minimum between 3 - 6 and 2 - 3@xmath101 respectively , @xmath99 for  shows no evidence for an increase in the relative variance on moving to the smallest scales considered .",
    "this dependence could be related to the number density of dark matter particles used in the analysis , with @xmath59 and @xmath60 being maybe more sensitive to poisson noise on small scales than the integral statistic .",
    "the flattening in the relative variance seen in the mocks for @xmath100 corresponds to the scale where the dominant contribution to the correlation function changes from the one - halo term ( small scales ) to the two - halo term .",
    "it is important to remember that it is not possible from fig .",
    "[ fig : variance ] alone to quantify the inevitable correlations between different scales in the clustering statistic .",
    "it would be naive to assume that these are unimportant , but let us here for simplicity consider that naive situation of uncorrelated errors . in the case of jackknife errors ,",
    "the different scale dependence of the relative variance ( compared with the `` truth '' ) makes their interpretation more complicated and therefore more likely to lead to misleading conclusions , especially if the small scale clustering is included . on the positive side , the jackknife errors ,",
    "as presented here , tend to overestimate the true errors , which is certainly better than underestimating them . in the case of the bootstrap errors , on the other hand , the relative variance",
    "as a function of scale has a similar shape to that of the variance from the mocks , and so the variance could be rescale by an appropriate factor to take into account the overall overestimate of the relative variance . as with jackknife errors , the bootstrap errors ,",
    "as presented here , do not underestimate the errors on any of the scales considered , which is a good thing if one is to consider an uncorrelated error analysis .",
    "having compared the estimates of the relative variance of the correlation function in the previous section , let us now look at the distributions of the measurements of the correlation functions themselves .",
    "the motivation for doing this is clear . recall that in order to be able to interpret @xmath66 fitting in the standard way , assigning the confidence intervals associated with a gaussian distribution to set differences in @xmath66 , we need to find a quantity which is gaussian distributed to use in the fitting process .",
    "the distribution of the measured projected correlation functions are compared in fig .",
    "[ fig : error_dist ] .",
    "each panel corresponds to a different error estimation method ( left : mocks ; centre : jackknife ; right : bootstrap ) .",
    "we plot the @xmath102 of the measured projected correlation function divided by the mean value , which shows deviations on either side of the mean on an equal footing .",
    "the different histograms show the distributions of @xmath102 /@xmath103@xmath104 measured on different scales covering the range @xmath105 to @xmath106 .",
    "the dotted curves show the corresponding gaussian distributions , i.e. with same mean and variance .",
    "a quick comparison of the shapes of the histograms in the different panels shows that the distribution of @xmath102  /@xmath103@xmath104 appear gaussian for all estimates on small scales , while on larger scales , this is no longer true for jackknife and bootstrap estimates ( @xmath107 and @xmath108 respectively ) . on the largest scales considered here",
    ", even the distribution from the mocks seems to be boarder than its corresponding gaussian equivalent .",
    "the distributions yielded from the jackknife estimator tend to show asymmetry on most scales plotted .",
    "thus , despite these deviations from a gaussian distribution , we conclude that @xmath102   is closer to being gaussian than  is , and , as such , is a more appropriate choice of variable to use in a @xmath66 fitting .",
    "the middle panel of fig .",
    "[ fig : error_dist ] shows the distribution of the jackknife  estimates .",
    "note that , as a result of the method of construction , jackknife errors are much more highly correlated than all the others , which is reflected by the additional factor of @xmath18 in the numerator of the expression for the covariance matrix ( eq .  [ eq : cov_jk ] ) . hence , to compare the jackknife distribution with the others , we first need to rescale the ratio  by @xmath109 .",
    "similarly , it is essential to account for this factor when plotting jackknife errorbars on data points , as otherwise they do not correspond to the variance in the jackknife estimates .",
    "a quick comparison of the middle and left panels in fig .",
    "[ fig : error_dist ] shows that the distribution of jackknife estimates over this medium range of scales is similar to the corresponding mock distribution : the jackknife distribution is slightly wider than the mock on small scales , which agrees with the comparisons of the variances presented in  [ sec : variance_rel ] .",
    "however , this figure also shows a concerning trend in that the jackknife distribution tends to become more asymmetric on larger scales ( @xmath110 ) , implying that jackknife errors for @xmath102  are not gaussianly distributed on such scales .    finally , in the right panel of fig .",
    "[ fig : error_dist ] we show the distribution of bootstrap  measurements . on all scales",
    "considered the distribution is clearly wider than that obtained from the mocks .",
    "this highlights the fact that , when only considering the diagonal terms from the covariance matrix ( as is commonly done ) , bootstrap errors are generally larger than both mock and jackknife errors , as was clear from  [ sec : variance_rel ] already .",
    "however , this does not necessarily mean that bootstrap errors overestimate the uncertainty , as we need to take into account the correlation between errors as well .",
    "clearly , in an analysis which ignores the correlation of the errors , the errorbars will be overestimated using the bootstrap method .",
    "it should be stressed that the values of @xmath102  obtained from the bootstrap , unlike those from the jackknife , are certainly more gaussianly distributed , with perhaps some hint of a non - gaussian error distribution appearing only for @xmath111 .",
    "so far we have ignored any correlations between bins when presenting the errors . to correct for this omission",
    ", we now consider the first step in a principal component decomposition of the covariance matrix , the distribution of normalized eigenvalues .",
    "the normalized eigenvalues quantify the contribution to the variance of the corresponding eigenvector .    in the two panels of fig .",
    "[ fig : eigenval ] we present the normalized eigenvalues for the covariance matrix of redshift space correlation function measurements , @xmath60 ( left ) , and for the projected correlation function ,  ( right ) , in both cases as function of eigenvalue number .",
    "the eigenvalues are ranked in order of decreasing variance , with the first eigenvalue accounting for the largest variance .",
    "the results in this plot are dependent on the number of data points used . in this case , the correlation functions are estimated in logarithmic bins of pair separation with width of 0.2 dex .    in each panel , the black line corresponds to the mock eigenvalues ( which we call the `` truth '' ) , and the red lines indicate the eigenvalues obtained from the covariance matrix constructed using just one half of all the mock realizations .",
    "this provides an indication of the accuracy with which the mock eigenvalues are measured .",
    "interestingly , the accuracy of the eigenvalues in this test is higher for  ( right ) than it is for @xmath60 ( left ) .",
    "this is most likely related to the fact that , within the range of scales considered , @xmath60 is more sensitive to poisson noise than , as was the case for the results in section  [ sec : variance_rel ] .",
    "[ fig : eigenval ] shows that in all cases , the eigenvalues decrease strongly in amplitude with increasing eigennumber .",
    "the first two eigenvalues alone typically account for 80 or even 90  % of the total variance .",
    "this indicates that a very strong correlation exists between the bins in the correlation function measurements .",
    "the shape of each eigenvalue curve is dependent on the correlation function itself .",
    "somewhat surprisingly , @xmath60 appears to be a less correlated statistic than , as more eigenvalues are needed in the former case to represent the covariance matrix with the same level of fidelity i.e. to the same total variance . or , in other words , the eigenvalue curve is shallower for @xmath60 than it is for . changing the range of scales or the number of data points used to represent the correlation function has only a marginal impact on the form of the eigenvalue curves .",
    "eigenvalues derived from the bootstrap covariance matrix are shown by the green line in each panel of fig .",
    "[ fig : eigenval ] , with error bars indicating the scatter over the 100 mock data sets . here",
    "we show only the bootstrap eigenvalues obtained using 27 sub - volumes . on average , the bootstrap method recovers the expected eigenvalue curve rather accurately .",
    "although it is not a perfect match , the `` truth '' ( i.e. mock ) is always within the scatter presented for both clustering statistics .",
    "note that changing the scale or the number of data points considered does not appear to influence these conclusions .",
    "the eigenvalues obtained from the jackknife covariance matrix are shown in fig .",
    "[ fig : eigenval ] for two choices for the number of sub - volumes , 27 and 64 , shown by the blue and cyan lines respectively .",
    "once again , error bars indicate the scatter in the eigenvalues obtained by applying this technique to each of the 100 data sets .",
    "neither of the two jackknife measurements fully recovers the true eigenvalues .",
    "interestingly , the more sub - samples that are used , the further away the eigenvalue curves move from the `` truth '' . for @xmath60 , this appears to be mostly due to the much smaller first eigenvalue , which is @xmath112 instead of @xmath113 .",
    "furthermore , the slope of the eigenvalue curve is different than that obtained from the mock data sets and is very sensitive to the number of sub - volumes the data is divided into .",
    "we have checked that further increasing the number of sub - samples the data is split into simply exacerbates this problem .",
    "this is a first yet clear indication that the covariance matrix estimated from the jackknife technique is not equivalent to the `` correct '' mock estimate .",
    "however , these issues do not necessarily imply that the jackknife error estimate is incorrect , since the eigenvalues are just one part of the principal component decomposition . on the other hand , this highlights the limitation in how accurately the jackknife methodology can recreate the true underlying errors , as given here by the mocks .",
    "most of the above remarks remain true for  as well . as was the case with the bootstrap analysis , changing the scale or the number of data points does not significantly influence any of the above conclusions .      after considering the eigenvalues of the principal component decomposition we now examine the associated eigenvectors .",
    "together , the eigenvalues and eigenvectors completely describe the full normalized covariance matrix .    in fig .",
    "[ fig : eigenvec ] , we show the first five eigenvectors derived from the covariance matrices constructed for the spherically averaged correlation function ( @xmath60 , left ) , and for the projected correlation function ( , right ) .",
    "note these eigenvectors are in the same order as the eigenvalues presented in fig .",
    "[ fig : eigenval ] . only the eigenvector corresponding to the smallest eigenvalue is not shown , which contributes less than @xmath114  % to the total variance .",
    "the colour coding remains the same as that used in fig .",
    "[ fig : eigenval ] , with the contribution to the relative variance indicated in the bottom right corner of each panel . at first sight ,",
    "the results are rather encouraging , as the mean eigenvectors for all error calculation methods overlap reasonably well with that measured from the mocks ( the `` truth '' , shown by the black line ) .",
    "the scatter on the mock result is indicated by the spread between the red lines , which , as before , shows the results obtained from splitting the mock data sets into two groups of 50 .",
    "let us consider each eigenvector for both @xmath60 and  more carefully .",
    "the first and most important eigenvector ( top left corner of both panels in fig .  [",
    "fig : eigenvec ] ) is very flat and is related to the uncertainty in the mean density of objects in the data sets .",
    "this uncertainty causes all the clustering measurements to move up and down more or less coherently . to within the quoted scatter on each clustering measurement ,",
    "all of the first - ranked eigenvectors are identical .",
    "however , some interesting trends can already be seen in this decomposition .",
    "for example , increasing the number of sub - samples used from 8 to 27 increases the difference between the mock and jackknife estimates , with the shape of the data inferred eigenvector tending to show less ( more ) correlation on small ( larger ) scales .",
    "this is a real effect that is further enhanced as one considers even smaller scales than shown here .",
    "the second eigenvector in fig .",
    "[ fig : eigenvec ] , shown in the top middle row of both panels , displays a strong scale dependence unlike the first eigenvector .",
    "the second eigenvector gives a much smaller contribution to the total variance , i.e. around the @xmath115  % level , as opposed to @xmath116  % for the first eigenvector .",
    "the form of the second eigenvector reveals that small scales are anti - correlated with larger ones .",
    "it is worth noting that all three error methods yield eigenvectors which look very similar for both @xmath60 and .",
    "increasing the number of sub - volumes decreases slightly the scatter in the recovered second eigenvector , and for a fixed number of sub - samples the scatter is marginally smaller for the bootstrap error estimates .",
    "finally , it is worth noting that the correlations as function of scale , despite having different slopes for @xmath60 and , are in fact very similar : the orientation of the eigenvectors is only determined up to a sign , so all the curves in fig .",
    "[ fig : eigenvec ] can be arbitrarily multiplied by -1 .",
    "the remaining three eigenvectors plotted in fig .",
    "[ fig : eigenvec ] combined contribute less than a few percent of the total variance , and the smaller their contribution to the variance , the larger is the scatter from the different resamplings .",
    "the fifth eigenvector ( bottom right panel ) certainly tends to be dominated by point to point variations in the data itself .",
    "this behaviour is particularly obvious when most of the eigenvector signal appears to come from adjacent points with opposite correlations , as seen in the lower panels for both statistics .",
    "note that it is precisely this last point , whether or not the eigenvector provides a useful or important description of the data , that highlights the difficulty behind using a principal component analysis . when fitting a model to the data , we need to select how many principal components to use in the fit ( see the example in section  [ sec : test_case ] ) .",
    "if features in the data are real then any theoretical model should attempt to reproduce them , and the eigenvectors which encode these features should be retained . however , if any facet of the data is dominated by noise then we should not attempt to reproduce it , and we should omit the corresponding eigenvector .",
    "one can compare covariance matrices without worrying about whether or not to retain different eigenvector components ; however , this issue has a major bearing on the success of model fitting .",
    "so far we have compared the principal component decomposition of covariance matrices constructed using different techniques to what we know is the `` right '' answer - the covariance matrix obtained from a set of mock data sets .",
    "unfortunately , in practice , it is rarely the case that one knows the correct answer beforehand .",
    "it would be useful , therefore , to devise a statistic which will allow us to quantify the degree to which a given technique can recover a covariance matrix . clearly no statistic can ever know the precision to which a measurement is actually correct , since that would require a priori knowledge of the `` truth '' .",
    "but the statistic should at least be sensitive to the level of noise which remains in the covariance matrix estimate . in the absence of such a statistic at present",
    ", we consider the usefulness of a few well - known results on the stability of errors in our quest to determine the stability of the inversion of the covariance matrix .",
    "one of the easiest tests to check the stability of a covariance matrix inversion is to repeat the whole analysis using only the odd or even data points ( i.e. the 1st , 3rd , 5th ... data points ) , and to check that the results remain within the quoted errors , and , furthermore , that no systematic shift has been introduced in either case .",
    "similarly , the result should remain stable to a simple rebinning of the data .",
    "note that the rebinning should be done at the level of the data itself and not on the processed results .",
    "this is to ensure that non - linearities in the binning procedure do not introduce unwanted biases . in the case of 2-point clustering statistics",
    ", this means that pair counts should be rebinned .",
    "porciani & norberg ( 2006 ) considered a quantitative test for the bootstrap method , which consists of using @xmath66 values in order to assess the stability of the recovered covariance matrix .",
    "their method relies on quantifying how many components of the principal component decomposition can be reconstructed for a fixed number of bootstrap realizations .",
    "the figure of merit is obtained by comparing the @xmath66 distributions for two different sets and numbers of bootstrap samples , as a function of the number of principal components used . with this method ,",
    "it is possible to at least make sure that the number of bootstrap realizations is large enough , but under no circumstance can we tell whether the method chosen has converged to the `` true '' covariance matrix .",
    "one nice feature of the method is that it can actually show that certain components of the decomposition will under no circumstance be recovered accurately enough , in which case this gives an indication of how many components should actually be considered .",
    "last but not least , the results should be stable w.r.t . the number of sub - samples considered ( within moderation of course ) .",
    "this is something which has to be considered in both bootstrap and jackknife analyses , and remains probably one of the better ways to at least attempt to show that the results obtained with internal errors estimates are as accurate and realistic as possible .    in section  [ sec : test_case ]",
    ", we put the above remarks into practice and show that each of them are able to fix or highlight different and often problematic issues in the error analysis using internal estimates .",
    "we note here that it is due to the numerous mock realizations that we are actually able to discover , on a more rapid timescale , which of the different methods is the more promising .",
    "certainly the task would be much more difficult in their absence .",
    "in this section we illustrate the different error estimation methods introduced and discussed in sections  [ sec : err_est ] and  [ sec : err_anal ] with a case study involving the projected correlation function .",
    "the aim is to infer two basic cosmological parameters , the matter density , @xmath117 , and the amplitude of density fluctuations , @xmath118 , using measurements of the projected correlation function made from the mock data sets between 1 and @xmath119 . in this example , because we are using data sets extracted from n - body simulations , we know the true values of the parameters beforehand , and can therefore test the performance of the error estimators .",
    "we first describe how to compute a grid of models for the projected correlation function using linear perturbation theory .",
    "first , we construct a grid of linear perturbation theory power spectra , using the parameterization of the cold dark matter transfer function introduced by eisenstein & hu ( 1998 ) .",
    "this parameterization is an approximation to the more accurate results generated from boltzmann codes such as camb ( lewis & challinor 2002 ; see sanchez  2008 for a comparison ) .",
    "the initial conditions for the l - basicc simulations were computed using camb , so we would expect a small systematic error in the values of the recovered cosmological errors . however , for this application , the measurement errors are much larger than these systematic errors , and we use the eisenstein & hu ( 1998 ) equations for speed and simplicity .",
    "the parameters varied to construct the grid are the normalization of the power spectrum , @xmath120 ( over a range @xmath121 to @xmath122 in steps of 0.02 ) and the present day matter density parameter @xmath123 ( covering the range @xmath124 to @xmath125 in steps of 0.02 ) .",
    "the other cosmological parameters are held fixed at the values used in the simulation , as described in section  [ sec : icc1340 ] .",
    "the power spectra are output at @xmath24 to match the redshift of the simulation output .",
    "the next step is to produce an approximate estimate of the non - linear matter power spectrum , using the halofit code of smith  ( 2003 ) , which has been calibrated against the results of n - body simulations .",
    "finally , we transform the power spectrum into the projected correlation function .",
    "the relation between the real - space correlation function , @xmath126 , and the projected correlation function is well known : @xmath127 using the fact that the power spectrum is the fourier transform of the correlation function , we can replace @xmath126 to obtain , in the limit that @xmath128 : @xmath129 note that if a finite value of @xmath64 is used to obtain the measured projected correlation function , then strictly speaking , @xmath130 in eq .",
    "[ eq : wrp ] should be replaced by a finite integral over @xmath131 .",
    "instead we correct for the small systematic error introduced by retaining @xmath128 by forcing the theoretical prediction for @xmath132 and @xmath133 to agree with the measurement from the full simulation volume ; this correction is applied to all the projected correlation functions on the grid and works well , which is clear since the returned best fitted values are still centred on the expected values .",
    "the dependence of the projected two - point correlation function on @xmath123 and @xmath118 is illustrated in fig .",
    "[ fig : xi_rp_model ] .",
    "the left hand panel shows the dependence of  on @xmath118 for @xmath117 fixed to the value used in the l - basicc simulation , while the right hand panel shows the dependence on @xmath117 with @xmath118 fixed to the simulation value .",
    "armed with our grid of theoretical models we are now ready to test the internal error estimators against the external estimate , the mocks , which we regard as the `` truth '' .",
    "we note that this is an idealized case , which is perfect for assessing the performance of the internal estimators : we know the true values of the cosmological parameters and we know that our model should provide a very good description of the measurements , at least over the range of scales used in the fit . in a more realistic situation ,",
    "the complications of galaxy bias , sample selection and how well we could model these effects would have an impact on performance ( see for example angulo et  al .",
    "2008 ) .    as this is such a straightforward fitting problem , we choose in fig .",
    "[ fig : om_sig_plane ] to plot the raw 95  % confidence interval contours or @xmath134 for 2-parameter fits , and sometimes to @xmath135 or @xmath136 for 1-parameter fits .",
    "] in the @xmath117-@xmath118 plane for three hundred and one error estimates . remember that in a normal situation with 1 data set , we would only be able to obtain three estimates of the error , whereas in our case we have 100 data sets and can therefore study the distribution of internal error estimates .",
    "the black solid contour corresponds to the 95  % confidence interval inferred from the mocks , which we consider as the `` truth '' or the benchmark error that the other methods are trying to match .",
    "the red - dotted lines correspond to boot-27 ( with =) , blue - short dashed to jack-27 and cyan - long dashed to jack-64 .",
    "the rather messy nature of this plot tells us that there is little agreement between each of the error methods and even between different realizations using the same estimator .",
    "two generic trends can be seen in fig .",
    "[ fig : om_sig_plane ] .",
    "first , not all contours are centred on the true values used in the simulations of @xmath137 and @xmath138 , but appear to be systematically shifted around .",
    "this is the effect of sample variance , also more commonly ( but less correctly ) referred to as cosmic variance .",
    "indeed , each data set , despite being very large ( a cube of side @xmath36 ) , is sensitive to the large scale density fluctuations present in the much larger simulation volumes .",
    "irrespective of the error method used , the distributions of best fit @xmath117 values are all symmetric , but not exactly gaussian ( the central part being slightly more peaked than a gaussian with similar dispersion ) and with mean values less than a @xmath139-binsize away from the simulation value .",
    "the distributions of best fit @xmath118 values are all very well described by gaussian distributions , with similar dispersion and with mean values within a @xmath140-binsize from the icc1340 simulation value .",
    "second , some of the error contours from the internal estimators are much larger than the mock error contour , while others are clearly much smaller .",
    "this , on the other hand , is of concern for internal error estimates and merits therefore closer investigation .    to further quantify the conclusions drawn from fig .",
    "[ fig : om_sig_plane ] , we construct a relative area statistic , defined as the area encompassing the 95  % confidence interval in an internal error estimator divided by the area of the external , `` true '' mock error estimate , a / a@xmath141 , for the same number of principal components .",
    "the motivation behind this statistic , hereafter also referred to as the normalized figure of merit , is twofold : 1 ) for a 2 parameter model it is natural to make comparisons between different error estimators using the full parameter plane ; and 2 ) due to its dimensionless nature , this statistic is easily interpreted in terms of confidence levels ( see below ) .",
    "a drawback of this figure of merit is that it does not account for uncertainties in the determination of the mock area / contours , which , given the number of data sets available , can not be neglected .",
    "a zeroth order estimate of that effect is given by a comparison between the following three mock results : those obtained using all 100 mocks , the first 50 and the last 50 respectively .",
    "additionally , it is not possible to estimate this statistic for real data , but only for slight variants of it .    in fig .",
    "[ fig : area_dist ] we plot the distribution of the relative area statistic .",
    "the case of boot-27 ( with =) is shown in red , results for jack-27 and jack-64 are shown in blue and cyan respectively , while the black vertical line indicates the optimal value of the mock .",
    "the arrows show the median of each distribution .",
    "the difference between the two panels is the binning used for , and hence , indirectly , in the number of principal components considered .",
    "focusing on the left panel of fig .",
    "[ fig : area_dist ] first , we see that for most data sets , the internal error estimates ( of which three hundred are presented in this figure ) tend to overestimate the 95  % ci area , on average by factors of @xmath142 , @xmath143 and @xmath144 for boot-27 , jack-64 and jack-27 respectively .",
    "furthermore , the variation in the area of the error contours is substantial : the central 68  % of values is typically spread over 0.15 in log@xmath145  a  /  a@xmath141 , i.e. a factor of @xmath146 in the area of the error contour .",
    "hence , even for this particularly simple case , the uncertainty on the internal estimate of the ci is large and certainly not negligible . as seen in earlier cases ,",
    "the difference between jack-27 and jack-64 is also quite large , with jack-64 yielding a marginally more centrally concentrated distribution than jack-27 , whereas jack-64 , on average , overestimates the true area by an additional 30  % .",
    "the situation is clearly worst for bootstrap errors , which display by far the largest systematic offset , but with a spread that looks amazingly similar to jackknife results .",
    "interpreting these offsets in the framework of gaussian errors for this figure of merit , we conclude that on average a 95  % boot-27 ci corresponds effectively to a 99.6  % ci , a 95  % jack-64 to a 98.0  % ci , and a 95  % jack-27 ci to a @xmath14790  % ci .",
    "this is without taking into account the spread in relative areas from different realizations .",
    "unfortunately , this is not the end of the confidence interval story .",
    "we now consider the right hand panel of fig .",
    "[ fig : area_dist ] , which is equivalent to the left except that 13 bins instead of 6 have been used for  ( over the same range of scales ) and hence all 13 principal components are used in the fitting , instead of just 6 as in the left panel . the difference in the figure of merit between right and left panels is clear , with the offsets of two of the three error methods changing in a systematic way .",
    "the right panel area ratios are biased by factors @xmath148 , @xmath143 and @xmath149 for boot-27 , jack-64 and jack-27 respectively .",
    "the only positive note is that all distributions plotted are well described by gaussians , shown by the dotted lines , with small dispersions .",
    "indirectly , we conclude that on average an estimated 95  % ci corresponds , with 95  % confidence , to a true ci in the range 79.2  %  -  99.8  % for the case considered here .",
    "we note that only jack-64 seems to remain stable w.r.t .",
    "the change in binning .",
    "this is most likely related to the results found by hartlap  ( 2007 ) .",
    "they showed that internal error estimators tend to be systematically biased low on average when the ratio of the number of bins considered to the number of samples becomes too large ( typically 20 % or more ) .",
    "the differences between the distributions of error contour areas shown in the left and right panels of fig .",
    "[ fig : area_dist ] is disconcerting , since only the number of data points used has changed ( i.e. the binning of  over a fixed range of scales ) . for mock errors",
    ", we measure an increase of 15  % in the area covered by the 95  % confidence intervals when changing from 6 to 13 data points .",
    "this is much smaller than the systematic shifts displayed by the internal error estimates under the same circumstances , which present a 30  % to 80  % change .",
    "it is unlikely that these strong shifts are due to the nature of the data alone , but much more likely to be due to errors propagating through the full covariance matrix analysis . in both panels ,",
    "the fits are performed using the full covariance matrices , and therefore we investigate in the next section if noise in the covariance matrix is the root of the problem .",
    "perhaps the large variation in the error contours returned by the internal estimators is due to noise in the covariance matrix , resulting from too many principal components being retained in the analysis .    to test this idea ,",
    "we show in fig .",
    "[ fig : area_dist_npca ] the impact on the relative area statistic of varying the number of principal components for a fixed number of data points . in the left panel of fig .",
    "[ fig : area_dist_npca ] , 6 data points are used to represent .",
    "the variation of the 95  % confidence interval area in the external mock estimate w.r.t .",
    "the area with n@xmath150 is plotted as a black line .",
    "this is a measure of how stable the mock results are as function of the number of principal components .",
    "there is a modest reduction in the size of the error contour , @xmath151dex , on using 6 principal components instead of 3 , as shown by the shallow slope of the line .",
    "the situation is slightly more unstable in the right panel ( where 13 data points are used instead of 6 as in the left panel ) , with up to a 40  % change over the full range of numbers of principal components considered .",
    "this variation is significantly reduced if one starts with at least 5 principal components , where the change in the figure of merit over the full range is then less than @xmath152  % .    in fig .",
    "[ fig : area_dist_npca ] we also show the median , and 16@xmath39 and 84@xmath39 percentiles of the relative area distributions for boot-27 ( with =) , jack-27 and jack-64 estimates in red , blue and cyan respectively .",
    "the impression gained from the two panels is rather different , especially for boot-27 and jack-27 .",
    "while the main properties of the relative area distributions for those two estimators remain roughly the same as function of n@xmath80 in the left panel ( very weak dependence on n@xmath80 ) , they change systematically and significantly with n@xmath80 in the right panel .",
    "these changes result in unstable error estimates , most likely due to the propagation of a bias through the covariance matrix estimate ( see hartlap  2007 ) .",
    "only jack-64 seems to return a somewhat stable results as function of n@xmath80 .    finally , in fig .",
    "[ fig : dist_fit_para ] we examine the fraction of `` outlier '' experiments as function of the number of principal components used in the fit .",
    "an outlier is defined as a data set for which the error contour from an internal estimate ( defined by @xmath153 ) does not include the true underlying cosmology ( i.e. @xmath137 and @xmath138 ) . for a gaussian distribution , we would expect no more than 5 `` outliers '' defined in this way out of 100 data sets .",
    "the left panel looks reasonable for most n@xmath80 , especially considering the number of realizations available : remember we have just 100 data sets at our disposal , so the poisson noise on the expectation value of this statistic is non - negligible .",
    "there is still a tendency from this panel to say that boot-27 ( with =) overestimates the errors and jack-27 underestimates them .",
    "this is in relatively good agreement with our conclusions from fig .",
    "[ fig : area_dist_npca ] . in the right hand panel",
    "the situation is significantly different .",
    "first all three estimators present radical changes in the number of `` outliers '' as function of n@xmath80 .",
    "clearly we have to exclude fits which use too many principal components : a cut can therefore be made at about 10 principal components for boot-27 and jack-64 , while the cut has to be made at about 6 or 7 components for jack-27 already .",
    "neither fig .",
    "[ fig : area_dist_npca ] nor fig .",
    "[ fig : dist_fit_para ] can be made without the full battery of data sets we have , nor without the knowledge of what the `` truth '' is .",
    "however , we can nevertheless learn some simple tricks which will be useful for the analysis of real data .",
    "for example , for each internal error estimator we can plot the relative area statistic , by using a fiducial reference value for the area .",
    "if this quantity varies in a significant manner as function of n@xmath80 or as a function of the internal error estimator used , we are made aware of a change of regime , without necessarily knowing what to do .",
    "the most likely and robust solution will be to introduce less components in the fit .",
    "we summarize here a few results from our simple case study . to recap",
    "the exercise consisted of fitting the projected correlation function ( on scales between 1 and @xmath119 ) , using what we knew beforehand was the _ correct theoretical model _ and which was only dependent on the parameters @xmath117 and @xmath118 , to one hundred projected correlation function estimates , taken from one hundred data sets of @xmath36 on a side , extracted from 50 totally independent n - body simulations of @xmath154 on a side .",
    "the main conclusions for internal error estimates are , with confidence interval abbreviated as ci :    * the average systematic offset encountered on an estimated 2-@xmath3 ci implies it can be mistaken for a 1.6-@xmath3 to 2.9-@xmath3 ci in reality . *",
    "the spread in errors for all internal error estimates is large : for an unbiased estimator , a 2-@xmath3 ci corresponds to a real ci in the range 1.3-@xmath3 to 3.1-@xmath3 , at the 95  % confidence level . *",
    "the ci from bootstrap errors , estimated with = , tend to be systematically more biased than jackknife errors . * the true 95  % ci , as measured from one hundred mocks ,",
    "is known to @xmath152  % accuracy .",
    "the above remarks depend on the mean number density of the sample , here fixed to mimic a  galaxy sample ( see  [ sec : icc1340 ] ) . in the next section ,",
    "we indirectly show how the amplitude of the errors scales w.r.t . the mean number density considered .      throughout our comparisons",
    "we have seen that the bootstrap gives errors that are systematically larger than the `` truth '' ( see for example figs .",
    "[ fig : variance ] and  [ fig : area_dist ] ) .",
    "previously , we have set = , i.e. the number of sub - volumes chosen with replacement was equal to the number of sub - volumes each data set is divided up into .",
    "this is an arbitrary choice and there is no reason why we can not increase the number of sub - volumes used to define each realization of the data set . here",
    "we consider varying the number of sub - volumes chosen .",
    "we also consider increasing the number of sub - volumes the data set is split into , which affects both the bootstrap and jackknife methods ( recall that in the jackknife , the choice of  sets the number of realizations we can generate ) .    in the left panel of fig .  [",
    "fig : boot_resampling ] we show the impact of changing the number of sub - volumes chosen on the relative variance obtained from the bootstrap with @xmath155 .",
    "two families of 5 different coloured curves are shown : the top ones correspond to a sampling fraction of 10  % of the standard mean number density , while the bottom ones corresponds to a 25  % sampling fraction .",
    "each family is composed of three bootstrap samples ( = 1 , 2 , 3  , in green , blue and cyan respectively ) and three mock curves : all mocks in black , and two red curves for the 50 first and 50 last respectively .",
    "the relative variance decreases as the number of sub - volumes selected increases , with the biggest change coming when the number of sub - volumes is oversampled by a factor of two ( blue line ) . with an oversampling of a factor of 3 ( cyan line ) ,",
    "the bootstrap errors are in very good agreement with those derived from the external estimate .",
    "if we increase the oversampling rate further , the relative variance returned by the bootstrap becomes too small ( not shown for clarity ) . from this figure ,",
    "an oversampling factor of about three seems to be optimal , i.e. selecting at random , with replacement , =3  sub - volumes from the original list of length .",
    "we note that the left panels of figs .",
    "[ fig : variance ] and  [ fig : boot_resampling ] , in which the density of objects in the data set is varied , give extremely valuable information on how the relative errors scale for samples with different mean densities , as together they span an order of magnitude variation in the mean density .",
    "increasing the mean density by a factor 2.5 leads to a reduction in the relative variance error by @xmath156  % ( top versus bottom black lines in fig .",
    "[ fig : boot_resampling ] ) , while a mean density 4 times larger decreases the relative error by a further 50  % , at which stage the change becomes more scale dependent .",
    "indeed , the shape of the mean relative variance in the left panel of fig .",
    "[ fig : variance ] is slightly different to those in fig .",
    "[ fig : boot_resampling ] .",
    "we attribute this to the fact that there is a limiting number density beyond which the errors are not any longer dominated by sample shot - noise .    the right panel of fig .  [",
    "fig : boot_resampling ] shows the same information as the left panel , but for a different number of sub - samples : 125 instead of 27 .",
    "the remarkable point to realize here is that both panels are virtually identical , leading to the conclusion that the precise number of sub - samples the data is split into is not a primary factor for determining the size of bootstrap errors .",
    "this is in strong contrast to jackknife errors for which we observed , in fig .",
    "[ fig : variance ] , a systematic change in the relative variance as function of scale with respect to the mocks , with increasingly discrepant results on smaller scales for the jackknife analysis with increasing number of sub - samples .",
    "so on adopting this essentially new prescription for the bootstrap error analysis , i.e. allowing for bootstrap resamplings of typically three times the number of sub - samples considered ( =3  ) , what is the impact on the other results presented in sections  [ sec : err_anal ] to  [ sec : test_case ] ?",
    "this question is best answered in figs .",
    "[ fig : om_sig_plane_nm ] and  [ fig : area_dist_nm ] , which show the same information as figs .",
    "[ fig : om_sig_plane ] and  [ fig : area_dist ] , but for bootstrap estimates with =3  .    fig .",
    "[ fig : om_sig_plane_nm ] is a great contrast to the messy fig .",
    "[ fig : om_sig_plane ] : this time internal error estimates seem to return ci which are in much better agreement with the `` truth '' .",
    "we show , together with the mock ci in black , two sets of bootstrap estimates , each applied to one hundred different data sets : boot-27 ( in red ) and boot-125 ( in green ) with =3  .",
    "we also note that the mock contour is on average @xmath157  % larger than in fig .",
    "[ fig : om_sig_plane ] .",
    "this is due to the 25  % lower number density considered in this case , which is in good agreement with the findings of fig .",
    "[ fig : boot_resampling ] , for which we also found a typical @xmath157  % change in error w.r.t . the reference mean number density .    fig .",
    "[ fig : area_dist_nm ] present the distributions of the relative area statistic for a suite of internal error estimators , all estimated over the range 1 to @xmath119 , with n@xmath158 . in the left panel ,",
    "we show the distributions of the standard bootstrap estimates with =  and those of the corresponding jackknife estimates : boot-27 in red , boot-125 in green , jack-27 in blue , jack-64 in cyan and jack-125 in magenta . in the right panel",
    ", we show the distributions for bootstrap with =3  : boot-27 in red , boot-64 in blue and boot-125 in green .",
    "[ fig : area_dist_nm ] makes several points .",
    "first it becomes clear from the right panel that the number of sub - samples drawn , with replacement , from a set of  sub - samples has to be typically three times larger than  for bootstrap errors to best reproduce the `` true '' mock inferred errors with minimal bias .",
    "secondly , once that regime is reached , bootstrap errors are only very marginally dependent on the number of sub - samples the data set is split into , i.e. once the criteria on the number of data points discussed in hartlap  ( 2007 ) is satisfied . for our clustering statistics ,",
    "this seems to be true once  is at least 5 times larger than n@xmath80 .",
    "thirdly , the log - normal variance is limited to @xmath159 .",
    "hence in the absence of bias , the area of the ci are known to within a factor of 2 ( with a 95  % confidence level ) , explicitly meaning that an estimated 2-@xmath3 ci can be mistaken to be pessimistically a true @xmath143-@xmath3 or optimistically a true 3-@xmath3 .",
    "finally the observed systematic bias seen in fig .",
    "[ fig : area_dist_nm ] is actually less important than it looks , as the uncertainty on the mock error is larger for the number density considered here . for @xmath160 ,",
    "we observe a @xmath152  % scatter on the mock error , which is more than twice as large compared to our findings with @xmath161 ( see  [ sec : model_fit ] ) .    additionally , the left hand panel of fig .",
    "[ fig : area_dist_nm ] , once compared to fig .",
    "[ fig : area_dist ] , shows that the dispersion on the internal error estimates seem to be rather insensitive to the actual number density of the samples analysed . decreasing the sampling rate down to 25  % of the original ( as in fig .",
    "[ fig : area_dist_nm ] ) makes barely any difference to the quoted dispersions , as long as the distributions of relative areas are still well described by gaussians . on the other hand",
    ", the bias seems to be a much stronger function of the assumed mean number density , with lower number densities tending to systematically overestimate the errors by quite a significant amount .",
    "this is explicitly shown by the large mean values of each relative area distribution shown in the left panel , ranging from @xmath143 to as large as @xmath162 in the case of jack-125 .",
    "the bias increases by 1.5 to 2.5 times , depending on the estimator , when the mean density becomes 4 times smaller .",
    "when the bias becomes so large and the changes so unpredictable , it is no longer clear whether there is any positive side to be seen with these traditional internal estimators .",
    "in this paper we have carried out an extensive comparison of the relative performance of internal and external error estimators for two - point clustering statistics .",
    "we devise a set of numerical experiments , extracting independent data sets from n - body simulations .",
    "the data sets are chosen to have a volume comparable to that of the typical volume limited samples constructed from the sdss .",
    "the benchmark for this exercise is the error estimate obtained from the scatter over our independent data sets , which we refer to as `` mock '' errors or `` truth '' .",
    "this is then compared to internal estimates made using the jackknife and bootstrap techniques ( e.g. tukey 1958 ; efron 1979 ) .",
    "we revisit the assumptions and the free parameters behind these techniques to see if we can lay down a recipe that would reproduce the more expensive ( and not even always possible ) external errors .",
    "we summarize below our findings , starting with uncorrelated statistics , followed by covariance matrix based results and ending with the conclusions drawn from a case study , aimed at fitting two cosmological parameters to our clustering results .    perhaps surprisingly",
    "( as they are so widely used in the literature ) , we find that both internal estimators of the errors have worrying failings .",
    "neither was able to faithfully reproduce the relative variance of the external estimate , over a limited range of scales ( 0.5 to 25@xmath163 ) . at face value ,",
    "the standard bootstrap variance is nearly 50  % larger on all scales , where standard refers to the case for which the number of sub - samples selected at random , with replacement , equals the number of sub - samples the data set is divided into ( i.e. =) . in ",
    "[ sec : boot_resampling ] , we solve this problem of the overestimated variance simply by increasing  by a factor of three , i.e. by setting =3  . the variance measured with the jackknife method is fairly accurate on large scales ( greater than @xmath164 ) , but the variance on small scales ( less than 2 - 3  @xmath101 ) is clearly overestimated by a significant amount and in a method dependent way : the bias depends strongly on the number of sub - volumes the data set is split into , as explicitly shown in fig .  [",
    "fig : variance ] .",
    "another major result from section  [ sec : error_dist ] is that all error estimators considered present gaussianly distributed errors on the smallest projected separations .",
    "however , slowly but surely , the error distributions become non - gaussian already on 6  ( 15 )  @xmath101 and larger for the jackknife ( bootstrap ) method , while they all remain gaussian for the mocks over the full range of scales considered .",
    "the details of the recovered covariance matrices , or more precisely the recovered principal component decomposition of the covariance matrices for our 100 data sets , show equally worrying features in sections  [ sec : eigen_val ] and  [ sec : eigen_vec ] . generally speaking bootstrap inferred ( normalized )",
    "eigenvalues and eigenvectors are in good agreement with mock inferred ones , while jackknife inferred ( normalized ) eigenvalues and eigenvectors present distinctive features which are not present in the mocks and , furthermore , are also dependent on the number of sub - volumes the data set is split into .",
    "these features are particularly sensitive to the smallest scales used : as for the variance estimates , the jackknife method becomes increasingly discrepant w.r.t .",
    "the mocks when scales smaller than 2 - 3  @xmath101 are used . however , the direct influence of those subtle differences in a such technical analysis is hard to grasp and hence a better understanding in reached on an example scenario .",
    "in section  [ sec : test_case ] we present a case study in which the different error estimation methods are used to extract constraints on cosmological parameters from measurements of the projected correlation function , using 100 independent data sets . the discrepancy in the relative variance returned by the different methods , as described above , propagates through this analysis and gives different constraints on the fitted parameters .",
    "we quantify these differences in terms of the area of the confidence interval which would correspond to 2-@xmath3 for the case of a two parameter fit for gaussian distributed measurements . with 100 independent data sets , we find that the internal estimators return , on average , error ellipses with larger area than that found in the mocks , resulting indirectly into a redefinition of the confidence limit of the measurement .",
    "this is particularly true for standard bootstrap estimates , for which the number of sub - samples selected at random with replacement equals the number of sub - samples the data set is divided into ( i.e. =) .",
    "however , we show in  [ sec : boot_resampling ] that increasing the number of sub - samples drawn at random by a factor of three ( i.e. =3  ) solves most problems : in that case , the confidence intervals are only marginally different to the mock ones . in the case of jackknife errors ,",
    "the area of the error ellipse is to some extent sensitive to the number of sub - volumes the data set is split into . for all error estimators ,",
    "we find , as expected , that the error ellipse area is sensitive to the number of principal components used in the analysis .",
    "the diagnosis for the internal estimators is therefore mixed .",
    "the jackknife method has problems recovering the scale dependence of errors and the results are sensitive to the number of sub - samples the data set is split into .",
    "there is little scope to fix these problems given the definition of the method ; the only thing we can vary is the number of sub - samples into which the data set is split .",
    "we did not find one choice for the number of sub - samples which could cure all of the ailments of this method .",
    "the prognosis for the bootstrap is on the other hand more encouraging .",
    "the problem of overestimating the variance can be traced to the effective volume of the data set used when the number of sub - volumes chosen at random with replacement is equal to the number of sub - volumes the data set is divided into . by oversampling the sub - volumes , this problem can be fixed , with the effective volume used tending to the original data set volume .",
    "better still , for our application at least , there appears to be an optimal factor , three times , to oversample , with higher rates producing too little variance .",
    "unfortunately there seems to be no hard and fast rules for the best way to set about a principal component analysis of the covariance matrix of clustering measurements .",
    "the value of the principal component analysis is that it helps break down the information contained in a clustering measurement .",
    "the measurement will be expressed in a restricted number of bins .",
    "the choice of the number of bins is arbitrary . using more bins",
    "does not necessarily imply that there will be more information in the correlation function .",
    "the pca breaks the covariance matrix down into eigenvectors .",
    "these are ranked in terms of how much information or variance they contain .",
    "the variance drops quickly with the order of the eigenvector for the examples we considered , indicating that most of the information in the clustering measurements can be broken down into a few terms .",
    "the best advice we can give here would be to compare the results obtained using different numbers of eigenvectors and choose a value where the results and conclusions do not change significantly .",
    "the analysis presented in this paper is applicable to any galaxy or cluster survey with three dimensional information .",
    "some of the issues discussed relating to redshift space distortions are particular to local surveys in which the distant observer approximation does not hold .",
    "a new set of experiments would be required to extend our results to the calculation of errors for photometric surveys , which look at angular clustering , or to multi - band photometric surveys , which will use photometric redshifts to look at clustering in redshift slices .",
    "the projection involved in these catalogues changes the underlying statistics , making them look more gaussian perhaps .",
    "this is the most likely explanation why cabr  ( 2007 ) find such a good agreement between jackknife and mock errors for angular clustering statistics .",
    "pn wishes to acknowledge numerous stimulating discussions with cristiano porciani , martin white , idit zehavi as well as many other participants at the 2006 and 2007 aspen summer workshops , and the kind use of many computers at the ifa and the icc .",
    "pn is supported by a pparc / stfc pdra fellowship fellowship .",
    "eg acknowledge support from spanish ministerio de ciencia y tecnologia ( mec ) , project aya2006 - 06341 and research project 2005sgr00728 from generalitat de catalunya .",
    "cmb is supported by a royal society university research fellowship .",
    "dc acknowledges the financial support from nsf grant ast00 - 71048 .",
    "this work was supported by the ec s alfa - ii programme via its funding of the latin american european network for astrophysics and cosmology .",
    "the l - basicc simulations in this paper were carried out by raul angulo using the virgo supercomputing consortium computers based at the institute for computational cosmology at durham university .",
    "barrow j.d . ,",
    "bhavsar s.p . ,",
    "sonoda d.h . , 1984 , mnras , 210 19 baugh c.m . , 2008 , phil . trans .",
    "a. , in press berlind a.a . , , 2003 , apj , 593 , 1 bothun g.d . , geller m.j . , beers t.c .",
    ", huchra j.p . , 1983 , apj 268 , 47      efron b. , 1979 , annals of statistics , 7 , 1 efron b. , tibshirani r.j . , 1993 , an introduction to the bootstrap , chapman & hall efstathiou g. & moody s.j . , 2001 , mnras 325 , 1603 eisenstein d. & hu w. , 1998 , apj 496 , 605            landy s.d . &",
    "szalay a.s . , 1993 , apj , 412 , 64 lewis a. & challinor a. , 2002 , phrvd 66b , 3531 li c. , , 2006 , mnras 368 , 21 li c. , , 2007 , mnras 376 , 984 ling e.n . , frenk c.s . ,",
    "barrow j.d . , 1986 , mnras , 223 , 21 lucey j.r . , 1979 , phd thesis    maddox s.j . , esftathiou g. , sutherland w.j . , loveday j. , 1990 , mnras , 242 , 43 madgwick d.s . , , 2003 , mnras , 344 , 847 matsubara t. , 2000 , apj 535 , 1 miller r.g . , 1974 ,",
    "biometrika , 61 , 1 mo h.j . , jing y.p . , brner g. , 1992 , apj 392 , 452      padmanabhan n. , white m. , eisenstein d. , 2007 , mnras 376 , 1702 peacock j.a . & dodds s.j . , 1994 , mnras , 267 , 1020 percival w. , , 2001 , mnras 327 , 1297 percival w. , , 2007 , mnras 381 , 1053 press w.h .",
    ", teukolsky s.a . , vetterling w.t . ,",
    "flannery b.p . , 1992 ,",
    "numerical recipes in c , cambridge university press .",
    "porciani c. , norberg p. , 2006",
    ", mnras 371 , 1824 quenouille g. , 1956 , biometrika 43 , 353 sanchez a.g .",
    ", , 2006 , mnras 366 , 189 sanchez a.g . ,",
    "baugh c.m . , angulo r. , 2008 , mnras submitted ( arxiv0804.0233 ) scranton r. , , 2002 , apj 579 , 48 shao j. , 1986 , annals of statistics , 14 , 1322 .",
    "szapudi i. , 2004 , apj 614 , 51"
  ],
  "abstract_text": [
    "<S> we present a test of different error estimators for 2-point clustering statistics , appropriate for present and future large galaxy redshift surveys . using an ensemble of very large dark matter  n - body simulations , </S>",
    "<S> we compare internal error estimators ( jackknife and bootstrap ) to external ones ( monte - carlo realizations ) . for 3-dimensional clustering statistics , </S>",
    "<S> we find that none of the internal error methods investigated are able to reproduce neither accurately nor robustly the errors of external estimators on 1 to @xmath0 scales . </S>",
    "<S> the standard bootstrap overestimates the variance of @xmath1 by @xmath2% on all scales probed , but recovers , in a robust fashion , the principal eigenvectors of the underlying covariance matrix . </S>",
    "<S> the jackknife returns the correct variance on large scales , but significantly overestimates it on smaller scales . </S>",
    "<S> this scale dependence in the jackknife affects the recovered eigenvectors , which tend to disagree on small scales with the external estimates . </S>",
    "<S> our results have important implications for the use of galaxy clustering in placing constraints on cosmological parameters . </S>",
    "<S> for example , in a 2-parameter fit to the projected correlation function , we find that the standard bootstrap systematically overestimates the 95  % confidence interval , while the jackknife method remains biased , but to a lesser extent . </S>",
    "<S> the scatter we find between realizations , for gaussian statistics , implies that a 2-@xmath3 confidence interval , as inferred from an internal estimator , could correspond in practice to anything from 1-@xmath3 to 3-@xmath3 . finally , by an oversampling of sub - volumes , it is possible to obtain bootstrap variances and confidence intervals that agree with external error estimates , but it is not clear if this prescription will work for a general case .    </S>",
    "<S> galaxies : statistics , cosmology : theory , large - scale structure . </S>"
  ]
}