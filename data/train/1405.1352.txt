{
  "article_text": [
    "let @xmath2 be a real random variable , such that @xmath3 almost surely .",
    "we want to approximate the following probability : @xmath4 where @xmath5 is a given threshold , such that @xmath6 .",
    "when @xmath7 goes to infinity , the above probability goes to  @xmath8 , meaning that @xmath9 becomes a rare event .",
    "such problems appear in many contexts , such as molecular dynamics simulations  @xcite or reliability problems with many industrial applications , for example .    estimating a rare event using a direct monte carlo estimation is inefficient , as can be seen by the analysis of the relative error .",
    "indeed , let @xmath10 be a sequence of independent and identically distributed random variables with the same law as @xmath2 .",
    "then for any positive integer @xmath11 , @xmath12 is an unbiased estimator of @xmath13 : @xmath14=p$ ] .",
    "it is also well - known that its variance is given by @xmath15 and therefore the relative error writes : @xmath16 assume that the simulation of one random variable @xmath17 requires a computational cost @xmath18 . for a relative error of size @xmath19 ,",
    "the cost of a direct monte carlo method is thus of the order @xmath20 which is prohibitive for small probabilities ( say @xmath21 ) .",
    "many algorithms devoted to the estimation of the probability of rare events have been proposed . here",
    "we focus on the so - called adaptive multilevel splitting ( ams ) algorithm  @xcite .",
    "let us explain one way to understand this algorithm ( see  @xcite for a more general presentation ) .",
    "the splitting strategy relies on the following remark .",
    "let us introduce @xmath22 intermediate levels : @xmath23 .",
    "the small probability @xmath13 satisfies : @xmath24 where @xmath25 , with @xmath26 and @xmath27 . in order to use this identity to build an estimator of @xmath13",
    ", one needs to ( i ) define appropriately the intermediate levels and ( ii ) find a way to sample according to the conditional distributions @xmath28 to approximate each @xmath29 using independent monte carlo procedures .    in this article",
    ", we will be interesting in the idealized case where we assume we have a way to draw independent samples according to the conditional distributions @xmath30",
    ", @xmath31 . we will discuss at length this assumption below .",
    "it is then easy to check that for a given @xmath22 , the variance is minimized when @xmath32 , and that the associated variance is a decreasing function of @xmath22 .",
    "it is thus natural to try to devise a method to find the levels @xmath33 such that @xmath34 . in ams algorithms ,",
    "the levels are defined in an adaptive and random way in order to satisfy ( up to statistical fluctuations ) the equality of the factors @xmath29 .",
    "this is based on an interacting particle system approximating the quantiles @xmath35 using an empirical distribution .",
    "the version of the algorithm we study depends on two parameters : @xmath1 and @xmath0 .",
    "the first one denotes the total number of particles .",
    "the second one denotes the number of resampled particles at each iteration : they are those with the @xmath0 lowest current levels ( which mean that a sorting procedure is required ) .",
    "thus , the levels are defined in such a way that @xmath36 and the estimator of the probability @xmath13 writes : @xmath37 where @xmath38 is the number of iterations required to reach the target level @xmath7 , and @xmath39 $ ] is a correction factor precisely defined below ( see  ) .",
    "notice that @xmath40 if @xmath41 .",
    "in all the following , we will make the following assumption :    [ hyp : static ] @xmath2 is a real - valued positive random variable which admits a continuous cumulative distribution function @xmath42 .",
    "this ensures for example that ( almost surely ) , there is no more than one particle at the same level , and thus that the resampling step in the algorithm is well defined .",
    "we will show in section  [ sec : hyp ] below that this assumption can be relaxed : the continuity of the cumulative distribution function is actually only required on @xmath43 , in which case the ams algorithm still yields an estimate of @xmath44 ( with a large inequality ) . from section  [ sec : expo_wp ]",
    ", we will always work under assumption  [ hyp : static ] , and thus we will always use for simplicity strict rather than large inequalities on @xmath2 ( notice that under assumption  [ hyp : static ] , @xmath45 ) .",
    "our aim in this article is twofold .",
    "first , we show that for any values of @xmath1 and @xmath0 , the estimator @xmath46 is an unbiased estimator of @xmath13 ( see theorem [ th : unbiased ] ) : @xmath47 second , we are able to obtain an explicit asymptotic expression for the variance of the estimator @xmath46 in the limit of large @xmath1 , for fixed @xmath0 and @xmath13 , and thus for the relative error ( see proposition  [ propo : var ] , equation  ): @xmath48 thus , if we consider the cost associated to the monte carlo estimator based on @xmath11 independent realizations of the algorithm , and if @xmath11 is chosen in such a way that the relative error is of order @xmath19 , one ends up with the following asymptotic cost for the ams algorithm ( see theorem  [ theo : cost ] , equations   and  ): for fixed @xmath0 and @xmath13 , in the limit of large @xmath1 @xmath49 } \\ ] ] where @xmath18 denotes the cost for drawing one sample according to the conditional distributions @xmath50 ( assumed to be independent of @xmath51 , to simplify ) , and @xmath52 is the cost associated with the sorting procedures involved in the algorithm . here again the landau symbol @xmath53 in the above depend non - uniformly on @xmath0 and @xmath13 .",
    "the two results   and   should be compared to the corresponding formulae for direct monte carlo simulation   and   above . from these results , we conclude that , in this asymptotic regime :    a.   the choice @xmath41 is the optimal one in terms of variance and cost b.   ams yields better result than direct monte carlo if @xmath54 which will be the case for sufficiently small probability @xmath13 .",
    "the assumption that we are able to sample according to the conditional distributions @xmath50 ( idealized setting ) is a severe limitation , from a practical viewpoint .",
    "let us make three comments about this idealized setting .",
    "first , to the best of our knowledge , all the theoretical results which have been obtained so far in the literature  @xcite rely on such an assumption .",
    "it is believed that the qualitative conclusions obtained under this assumption are still meaningful for the actual algorithm used in practice , where the conditional distributions @xmath28 are only approximately sampled ( using for example metropolis hastings procedures ) . in some sense , in the idealized setting ,",
    "one studies the optimal performance one could reach with this algorithm .",
    "second , we will describe in section  [ sect : samp ] situations where this assumption makes sense in practice : this is in particular the case in the 1d dynamic setting described in section  [ sec:1ddyn ] .",
    "third , in a paper in preparation  @xcite , we will actually show that it is possible to obtain an unbiased estimator of @xmath13 using ams in a very general setting .",
    "in other words , the idealized setting is crucial to obtain estimates on the variances and the costs of the algorithm , but not to prove the unbiasedness property for the estimator of the rare event probability .",
    "let us now review the results known from the literature on ams . as mentioned above ,",
    "the ams algorithm has been introduced in  @xcite , where it is proven that their estimator @xmath46 indeed converges almost surely to @xmath13 ( in the limit @xmath55 ) .",
    "a central limit theorem is also provided , in the asymptotic regime where @xmath56 is fixed . in  @xcite",
    ", the authors consider the case @xmath41 , prove the unbiasedness of the estimator and analyze the variance . in  @xcite , the authors analyze the large @xmath1 limit , with fixed @xmath56 . in summary ,",
    "our results differ from what has been proven before in two ways : we prove that @xmath46 is an unbiased estimator for any @xmath57 ( and not only @xmath41 ) and we analyze the variance and the cost at a fixed @xmath0 ( in the limit @xmath55 ) and show that in this regime , @xmath41 is optimal .",
    "in addition to the new results presented in this paper , we would like to stress that the techniques of proof we use seem to be original .",
    "the main idea is to consider ( under assumption [ hyp : static ] ) the family @xmath58}$ ] of conditional probabilities : @xmath59 and to define associated estimators @xmath60 thanks to the ams algorithm .",
    "we are then able to derive an _",
    "explicit functional equation on @xmath61 $ ] as a function of @xmath51 _ ; which follows from the existence of an explicit expression for the distribution of order statistics of independent random variables .",
    "we can then check the unbiased property ( see theorem [ th : unbiased ] ) @xmath62=p(x).\\ ] ] finally noting that @xmath63 , we get  . to analyze the computational cost ( theorem  [ theo : cost ] ) , we follow a similar strategy , with several more technical steps .",
    "first , we derive functional equations for the variance and the mean number of iterations in the algorithm . in general",
    "we are not able to give explicit expressions to the solutions of these equations , and we require the following auxiliary arguments :    a.   we show how one can relate the general case to the so - called exponential case , when @xmath2 has an exponential distribution with mean @xmath64 ; b.   we then prove that the solutions of the functional equations are solutions of linear ordinary differential equations of order @xmath0 ; c.   we finally get asymptotic results on the solutions to these odes in the limit @xmath65 , with fixed values of @xmath13 and @xmath0 .",
    "the paper is organized as follows . in section  [ sec : ams ] , we introduce the ams algorithm and discuss the sampling of the conditional distributions @xmath66 . in section  [ sec : expo_wp ] , we show how to relate the case of a general distribution for @xmath2 to the case when @xmath2 is exponentially distributed .",
    "we then prove that the algorithm is well defined , in the sense that it terminates in a finite number of iterations ( almost surely ) . in section [",
    "sect : unbiased ] , we show one of our main result , namely theorem  [ th : unbiased ] which states that the estimators @xmath60 are unbiased .",
    "finally , in section [ sect : cost ] , we study the cost of the algorithm , with asymptotic expansions in the regime where @xmath13 and @xmath0 are fixed and @xmath1 goes to infinity .",
    "the proofs of the results of section [ sect : cost ] are postponed to section [ sect : details ] .",
    "after presenting the ams algorithm in section  [ sect : algo ] , we discuss the fundamental assumption that we know how to sample according to the conditional distributions @xmath67 in section [ sect : samp ] .",
    "we will in particular show that this assumption is actually practical at least in one setting : the one - dimensional dynamic setting presented in section  [ sec:1ddyn ] .",
    "finally , we discuss assumption  [ hyp : static ] in section [ sec : hyp ] and show that it can be replaced by a less stringent hypothesis .",
    "this is particularly useful in the framework of the high - dimensional dynamic setting described in section  [ sec : nddyn ] .",
    "we would like to stress that sections @xmath68 and @xmath69 discuss practical aspects of the implementation of ams and can be skipped if the reader is only interested in the two main results about the unbiasedness and cost of ams , presented in sections [ sect : unbiased ] and [ sect : cost ] .",
    "we fix a total number @xmath1 of particles , as well as @xmath70 the number of resampled particles at each iteration of the algorithm . in the sequel ,",
    "when we consider a random variable @xmath71 , the subscript @xmath72 denotes the index in @xmath73 of a particle , and the superscript @xmath74 denotes the iteration of the algorithm .    in the algorithm below and in the following , we use classical notations for @xmath0-th order statistics . for @xmath75 an ensemble of independent and identically distributed ( i.i.d . )",
    "real valued random variables with continuous cumulative distributions function , there exists almost surely a unique ( random ) permutation @xmath76 of @xmath73 such that @xmath77 . for any @xmath78",
    ", we then use the classical notation @xmath79 to denote the @xmath0-th order statistics of the sample @xmath80 .    for any @xmath81",
    "$ ] , we define the adaptive multilevel splitting algorithm as follows ( in order to approximate @xmath82 one should take @xmath83 , but we consider the general case @xmath81 $ ] for theoretical purposes ) .",
    "[ algo : ams ]     * initialization : * define @xmath84 .",
    "sample @xmath1 i.i.d .",
    "realizations @xmath85 , with the law @xmath50 .",
    "define @xmath86 , the @xmath0-th order statistics of the sample @xmath87 , and @xmath88 the ( a.s . )",
    "unique associated permutation : @xmath89 .",
    "set @xmath90 .",
    "* iterations ( on @xmath91 ) : * while @xmath92 :    conditionally on @xmath93 , sample @xmath0 new independent be i.i.d .",
    "random variables , uniformly distributed on @xmath94 and independent from all the other random variables . then set @xmath95 , where @xmath96 is the inverse distribution function associated with the ( conditional ) probability distribution @xmath97 , see [ eq : fxy_cdf ]",
    ". we slightly abuse notation by using @xmath98 rather than @xmath99 . ] random variables @xmath100 , according to the law @xmath101 .",
    "set @xmath102    in other words , the particle with index @xmath72 is killed and resampled according to the law @xmath101 if @xmath103 , and remains unchanged if @xmath104 .",
    "notice that the condition @xmath105 is equivalent to @xmath106 .",
    "define @xmath107 , the @xmath0-th order statistics of the sample @xmath108 , and @xmath109 the ( a.s . )",
    "unique is justified by proposition [ propo : why_expo ] . ]",
    "associated permutation : @xmath110 .",
    "finally increment @xmath111 .    *",
    "end of the algorithm : * define @xmath112 as the ( random ) number of iterations . notice that @xmath113 is such that @xmath114 and @xmath115 .",
    "the estimator of the probability @xmath116 is defined by @xmath117 with @xmath118    notice that @xmath119 .",
    "more generally , @xmath120 .",
    "since we are interested in the algorithm starting at @xmath83 , we introduce the notation @xmath121    we finally stress that the computations of the sampled random variables @xmath122 for the initialization and of the @xmath123 for each iteration @xmath74 can be made in parallel .      at each iteration of the algorithm",
    ", we need to sample @xmath0 random variables according to conditional distributions @xmath67 , with @xmath124 taking values in the sequence @xmath125 .",
    "as explained above , we develop our theoretical analysis of the properties of the algorithm ( bias , variance and computational cost ) in the idealized situation where it is possible to sample according to these conditional distributions @xmath67 for any @xmath126 $ ] . from a practical point of view , this assumption is generally unrealistic .",
    "one possible situation where it is realistic is the dynamic setting presented in the present section ( in contrast with the static setting ) .      in a general framework",
    ", there is no simple way to sample the distributions @xmath67 . in practice , this can be done thanks to a metropolis - hastings procedure , see for example  @xcite .",
    "of course , this introduces a bias and correlations between the particles at each iteration ( compared with the idealized algorithm studied in this paper ) .",
    "this bias and these correlations asymptotically vanish when the number of iterations in the metropolis - hastings procedure goes to infinity .",
    "the error analysis associated to this procedure is out of the scope of this paper .",
    "there is a simple example where it is actually possible to sample the distributions @xmath67 , namely if @xmath2 is exponentially distributed . indeed ,",
    "if @xmath2 has exponential law @xmath127 with mean @xmath64 , then the conditional distribution @xmath128 is a shifted exponential variable , for any @xmath129 . in the following , we will refer to this situation as _ the exponential case_. of course , this has no practical interest since in this case , @xmath130 is analytically known .",
    "however , this particular case plays a crucial role in the analysis hereafter , since as will be precisely explained in section  [ sect : wellposed ] , the study of the general case can be reduced to the study of the exponential case after some change of variable .",
    "this trick was already used in the original papers  @xcite .      in the one - dimensional dynamic setting , @xmath2 is defined as @xmath131 where @xmath132 is a strongly markovian time - homogeneous random process with values in @xmath133 , and @xmath134 is a stopping time . in this setting , the conditional distribution @xmath135 is easily sampled : it is the law of @xmath136 , where @xmath137 denotes the stochastic process @xmath138 which is such that @xmath139 .    having in mind applications in molecular dynamics  @xcite ,",
    "a typical example is when @xmath140 satisfies a stochastic differential equation @xmath141 with smooth drift coefficient @xmath142 and inverse temperature @xmath143 .",
    "the stopping time is for example @xmath144 for @xmath145 $ ] , and for some @xmath146 and one can then consider @xmath147 .",
    "let us consider the target level @xmath148 .",
    "the ams algorithm then yields an estimate of @xmath149 the probability that the stochastic process starting from @xmath8 reaches the level @xmath64 before the level @xmath150 .",
    "such computations are crucial to compute transition rates and study the so - called reactive paths in the context of molecular dynamics , see  @xcite .",
    "notice that in practice , a discretization scheme must be employed , which makes the sampling of the conditional probabilities more complicated .",
    "another point of view on the ams algorithm is then required in order to prove the unbiasedness of the estimator of the probability @xmath13 , see  @xcite .",
    "the exponential case can be obtained from a dynamic setting .",
    "indeed , consider the following stochastic process : a particle starts at a given position @xmath51 , moves on the real line with speed @xmath151 on the random interval @xmath152 $ ] where @xmath153 is exponentially distributed , and with speed @xmath154 on the interval @xmath155 .",
    "more precisely , @xmath156 notice that @xmath157 is a markov process such that @xmath158 is continuous .",
    "then for any initial condition  @xmath51 and any given threshold @xmath159 we have @xmath160 .",
    "in particular , @xmath161 is an exponential random variable with parameter @xmath64 .",
    "let us consider again a strongly markovian time - homogeneous stochastic process @xmath140 , but with values in @xmath162 for @xmath163 . in this case , the levels need to be defined using a ( continuous ) function @xmath164 , sometimes called a reaction coordinate in the context of molecular dynamics .",
    "let us focus for simplicity on the case when @xmath140 is solution of the stochastic differential equation  ( sde ) : @xmath165 with smooth potential @xmath166 , inverse temperature @xmath143 and @xmath167 a @xmath168-dimensional wiener process .",
    "let us consider two disjoint closed subsets @xmath169 and @xmath170 of @xmath171 .",
    "let us define the stopping time @xmath172 where @xmath173 let us assume that the function @xmath174 is such that @xmath175 we then set , for a fixed initial condition @xmath176 , @xmath177 let us set @xmath148 as the target level . in this case , the probability @xmath178 is the probability that the path starting from @xmath179 reaches  @xmath170 before  @xmath169 . as explained above , this is a problem of interest in molecular dynamics for example , to study reactive paths and compute transition rates in high dimension , typically when  @xmath169 and  @xmath170 are metastable regions for @xmath180 .",
    "the problem to apply the ams algorithm is again to sample according to the conditional distributions @xmath67 .",
    "a natural idea is to use the following branching procedure in the resampling step at the @xmath74-th iteration : to build one of the new @xmath0 trajectories , one of the @xmath181 remaining trajectories is chosen at random , copied up to the first time it reaches the level @xmath182 and then completed independently from the past up to the stopping time @xmath134 .",
    "the problem is that this yields in general a new trajectory which is correlated to the copied one through the initial condition on the level set @xmath182 .",
    "indeed , in general , given @xmath183 such that @xmath184 , the laws of @xmath185 and @xmath186 are not the same .",
    "as a consequence , it is unclear how to sample @xmath67 , except if we would be able to build a function  @xmath174 such that the law of @xmath187 only depends on @xmath188 .",
    "this is actually the case if @xmath174 is the so - called committor function associated to the dynamics   and the two sets @xmath169 and @xmath170 .",
    "let @xmath169 and @xmath170 be two disjoint closed subsets in @xmath171 . the committor function @xmath174 associated with the sde and the sets @xmath169 and @xmath170 is the unique solution of the following partial differential equation ( pde ) : @xmath189    [ propo : comm ] assume that @xmath174 is the committor function , solution of . for any @xmath190 , we set @xmath191 . notice that @xmath192 is a random variable with values in @xmath193 $ ] .",
    "we have @xmath194 for any @xmath195 and @xmath196 . in particular ,",
    "if @xmath184 , then we have the equality @xmath197 .",
    "moreover , for any @xmath198 , @xmath199 with @xmath200 , @xmath201 .",
    "this previous proposition ( which is proven below ) fully justifies the branching procedure described above to sample @xmath202 at the @xmath74-th iteration of the algorithm : pick at random one of the @xmath181 remaining trajectories ( say @xmath180 ) , copy it up to the first time it reaches the level @xmath182 ( let us denote @xmath203 the first hitting point of this level ) and then complete the trajectory , independently from the past . by the strong markov property ,",
    "this yields a new @xmath2 sampled according to @xmath204 which is indeed @xmath205 , since @xmath206 .",
    "as already mentioned in the previous section , in practice , the sde   is discretized in time , say with a timestep @xmath207 . then",
    ", the branching procedure consists in copying the selected trajectory up to the first time index @xmath1 such that @xmath208 , and then to complete it independently from the past .",
    "this introduces a difference compared to the continuous in time situation considered above , since doing so we do not sample according to the conditional distribution @xmath209 . to treat this situation , one needs to resort to other techniques to analyze the algorithm , see  @xcite .",
    "in particular , one can show that the algorithm still yields an unbiased estimator of @xmath13 , using very different techniques of proof than those presented in this paper .",
    "this approach is also useful to treat non - homogeneous in time markov processes .    at this stage",
    ", we can thus conclude that in the high - dimensional dynamic setting , if the committor function is known , the ams algorithm can be practically implemented , and that it enters the framework of this paper .",
    "there are however two difficulties , that we will now discuss .",
    "first , the random variable @xmath210 , where @xmath174 is the committor function , does not satisfy assumption  [ hyp : static ] : we have @xmath211 ( @xmath2 takes values in",
    "@xmath212 $ ] ) and therefore , the cumulative distribution @xmath213 is not continuous at @xmath214 .",
    "more precisely , from proposition  [ propo : comm ] , we have : @xmath215 , @xmath216 and @xmath217 , @xmath218 .",
    "this is actually not a problem , as explained in the next section in a general setting : the continuity of the cumulative distribution function is only required over @xmath219 ( or more generally over @xmath43 in the general case when the target level is @xmath7 ) .",
    "the second difficulty is that knowing the committor function is actually a very strong assumption .",
    "computing @xmath174 solution to   is actually impossible in practice since this is a high - dimensional pde .",
    "moreover , if @xmath174 was known , then we would actually know the small probability we want to estimate since @xmath220 .",
    "this is a consequence of the well - known probabilistic representation to solutions to  :    recall the definitions   of the stopping times @xmath221 and @xmath222 .",
    "then , if @xmath174 is the committor function associated to the sde and the sets @xmath169 and @xmath170 , then , for any @xmath223 @xmath224    thus , this high - dimensional dynamic case with known committor function should also be considered as an idealized setting , which is only useful for theoretical purposes , in order to study the best performance we could expect for the ams algorithm .",
    "we end up this section with a proof of proposition [ propo : comm ] .",
    "* proof of proposition [ propo : comm ] : * let us consider @xmath174 satisfying  , @xmath225 solution to   and @xmath191 . for",
    "any given @xmath226 , and any @xmath227 , let us introduce @xmath228 one easily checks the identity @xmath229 by continuity of @xmath174 and of the trajectories of the stochastic process @xmath140 , and by the strong markov property at the stopping time @xmath230 , we get for any @xmath51 and any @xmath231 @xmath232\\\\ & = { \\mathbb{e}}\\bigl[\\mathds{1}_{\\tau_{z}^{x}<\\tau_{a}^{x}}\\mathds{1}_{\\tau_{b}^{y_{\\tau_z^x}^x}<\\tau_{a}^{y_{\\tau_z^x}^{x}}}\\bigr]\\\\ & = { \\mathbb{e}}\\bigl[\\mathds{1}_{\\tau_{z}^{x}<\\tau_{a}^{x}}\\xi(y_{\\tau_z^x}^{x})\\bigr]\\\\ & = { \\mathbb{p}}(\\tau_{z}^{x}<\\tau_{a}^{x})z.\\end{aligned}\\ ] ]    this identity proves the first claim of the proposition .",
    "moreover , since the law of @xmath233 depends on @xmath51 only through @xmath188 , we also more generally get @xmath201 for any @xmath198 , @xmath199 with @xmath200 , .      in this section , we show that assumption  [ hyp : static ] is actually too stringent .",
    "if one assumes the following    [ hyp : static ] @xmath2 is a real - valued positive random variable such that @xmath234 is continuous ,    then the algorithm  [ algo : ams ] is well defined , and all the results presented below hold . in particular , the estimator @xmath46 is an unbiased estimator of @xmath235    we notice that assumption  [ hyp : static ] is indeed more natural than assumption  [ hyp : static ] since the ams algorithm only applies a resampling procedure with conditional distributions @xmath98 to realizations such that @xmath236 : this is why the continuity of the cumulative distribution function @xmath42 is actually only required over @xmath43 .",
    "the argument to show that one can recover the setting of assumption  [ hyp : static ] assuming only assumption  [ hyp : static ] is the following coupling argument .",
    "in lemma  [ lemma : pareto ] below , it is proven that there exists a random variable @xmath237 such that :    * @xmath237 satisfies assumption [ hyp : static ] ; * for any @xmath238 , @xmath239 is equivalent to @xmath240 and @xmath241 ; * @xmath242 is equivalent to @xmath243 and thus , in particular , @xmath244 , the probability to be estimated .",
    "the last two properties show that running the ams algorithm on @xmath237 is equivalent to running the ams algorithm on @xmath2 : the iterations , the stopping criterion and the estimator are the same .",
    "the theory developed in this paper ( unbiased estimator , analysis of the cost and of the computational cost ) is then applied to the algorithm applied to the auxiliary random variable @xmath237 instead of @xmath2 , which is completely equivalent to the algorithm applied to @xmath2 .    in all the following , for simplicity",
    ", we will always assume that assumption  [ hyp : static ] holds , keeping in mind that it can be relaxed to assumption  [ hyp : static ] .",
    "thus , inequalities which involve the random variable @xmath2 can be changed from large to strict without modifying the associated events ( almost surely ) .",
    "we end this section with a lemma which defines the random variable @xmath237 as a function of @xmath2 .",
    "[ lemma : pareto ] let @xmath2 be a random variable satisfying assumption  [ hyp : static ] , and let us define @xmath245 where @xmath246 is a random variable independent of @xmath2 and uniformly distributed on @xmath94 .",
    "then , ( i ) @xmath237 satisfies assumption [ hyp : static ] , ( ii ) for any @xmath238 , @xmath239 is equivalent to @xmath240 , and the two laws @xmath247 and @xmath248 coincide on @xmath249 and ( iii ) @xmath242 is equivalent to @xmath243 .",
    "* proof of lemma [ lemma : pareto ] : * since @xmath250 , it is easy to check the items ( ii ) and ( iii ) .",
    "let us now consider the cumulative distribution of @xmath237 .    for @xmath251 , @xmath252 and",
    "thus @xmath253 is continuous for @xmath254 by assumption  [ hyp : static ] .    for @xmath255 , @xmath256 and",
    "thus @xmath253 is continuous for @xmath257 .    finally , with these expressions one easily checks left and right continuity of @xmath258 at @xmath7 .",
    "this concludes the proof of the fact that @xmath237 satisfies assumption [ hyp : static ] , and thus the proof of lemma  [ lemma : pareto ] .",
    "the aim of this section ( see section  [ sect : wellposed ] ) is to prove the well - posedness of the algorithm , namely the fact that @xmath113 is almost surely finite , when the probability @xmath259 to estimate is positive .",
    "the argument is based on the fact that the general case is related to the exponential case through a change of variable , see section  [ sec : expo ] ( this will be instrumental in the rest of the paper ) .",
    "section  [ sect : notations ] first gives a few notation that will be useful below .",
    "we will use the following set of notations , associated to the random variable @xmath2 satisfying assumption  [ hyp : static ] .",
    "we denote by @xmath260 the cumulative distribution function of the random variable @xmath2 : @xmath261 for any @xmath262 , and @xmath263 . from assumption  [ hyp : static ] , the function @xmath260 is continuous .",
    "notice that it ensures that if @xmath80 is an independent copy of @xmath2 , then @xmath264 , and thus , in the algorithm , there is only at most one sample at a given level .",
    "we recall that our aim is to estimate the probability @xmath265 given a threshold @xmath5 .",
    "more generally , we define for any @xmath81 $ ] @xmath59 so that we have @xmath259 .",
    "notice that @xmath266 .    for any @xmath81 $ ]",
    ", @xmath50 admits a cumulative distribution function @xmath267 , which satisfies : for any @xmath268 , @xmath269 while the variable @xmath51 always denotes the parameter in the conditional distributions @xmath50 , we use the variable @xmath270 as a dummy variable in the associated densities and cumulative distribution functions .    by assumption",
    "[ hyp : static ] , @xmath271 in the definition above can be replaced with @xmath272 .",
    "notice that @xmath273 .",
    "moreover , with these notations , we have @xmath274    an important tool in the following is the family of functions : for any @xmath81 $ ] and any @xmath275 @xmath276\\\\ \\lambda(y)=\\lambda(y;0)=-\\log(1-f(y ) ) .",
    "\\end{gathered}\\ ] ]    we remark the following identity : for @xmath277 , @xmath278      in some places , we will assume that @xmath2 admits a density @xmath142 with respect to the lebesgue measure ( which indeed implies assumption  [ hyp : static ] ) .",
    "this assumption is in particular satisfied in the exponential case ( namely when @xmath2 is exponentially distributed ) , which we will consider in several arguments below to study the bias and the computational cost .",
    "if @xmath2 admits a density @xmath142 , the law @xmath50 of @xmath2 conditionally on @xmath279 also admits a density @xmath280 , which satisfies : for any @xmath281 , @xmath282 notice that @xmath283 .",
    "we finally introduce some notations about order statistics of samples of i.i.d . real random variables . if @xmath2 admits a density @xmath142 , then the @xmath0-th order statistics of an i.i.d .",
    "sample @xmath284 ( distributed according to the law of @xmath2 ) admits a density @xmath285 which satisfies : for any @xmath275 , @xmath286 the associated cumulative distribution function is @xmath287 .",
    "likewise , we introduce notations for the density and the cumulative distribution function of order statistics for the law @xmath50 : when @xmath277 we set @xmath288      one of the key tool in the following is the reduction of the general case to the exponential case , thanks to the use of the function @xmath289 , defined by .",
    "we recall that the exponential case refers to the case when @xmath2 is distributed according to the exponential law with parameter 1 , see section  [ sec : static_expo ] .",
    "the basic remark is the following .",
    "since by assumption [ hyp : static ] the cumulative distribution function @xmath267 is continuous , we have the following classical result , see for instance proposition @xmath68 in @xcite .",
    "[ lemma : lambda_expo ] if @xmath290 , then @xmath291 is uniformly distributed on @xmath94 , and thus @xmath292 has an exponential law with parameter @xmath64 .",
    "let us first state a result on the algorithm without any stopping criterion .",
    "[ propo : why_expo ] let us consider the sequence of random variables @xmath293 generated by the ams algorithm  [ algo : ams ] _ without any stopping criterium_.set @xmath294 and @xmath295 .",
    "then we have the following properties .",
    "* for any @xmath296 , @xmath297 is a family of i.i.d .",
    "exponentially distributed random variables , with parameter @xmath64 . * for any @xmath91 , @xmath297 is independent of @xmath298 . *",
    "the sequence @xmath299 is i.i.d ..    as a consequence , in law , the sequence @xmath300 is equal to the sequence of random variables @xmath293 obtained by the realization of the ams algorithm _ without any stopping criterion _ in the exponential case , with initial condition @xmath301 .",
    "* proof of proposition [ propo : why_expo ] : * the last assertion is a direct consequence of the three former items and of the fact that in the exponential case the function @xmath289 is the identity mapping .    item ( iii ) is a consequence of items ( i ) and ( ii ) .",
    "it remains to prove jointly those two items , which is done by induction on @xmath74 .",
    "when @xmath302 , the result follows from the way the algorithm is initialized : for each @xmath303 , @xmath304 is exponentially distributed ( see lemma [ lemma : lambda_expo ] ) , and the independence property is clear . assuming that the properties ( i ) and ( ii ) are satisfied for all @xmath305 , it is sufficient to prove that :    a.   @xmath306 are i.i.d . and",
    "exponentially distributed with mean @xmath64 ; b.   @xmath306 is independent of @xmath307 .    for any positive real numbers @xmath308 , and any @xmath309 , this is equivalent to proving that @xmath310    we want to decompose the probability with respect to the value of @xmath311 .",
    "we recall that almost surely we have @xmath312 in fact , in order to preserve symmetry inside the groups of resampled and not - resampled particles , we decompose over the possible values for the random set @xmath313 .",
    "we thus compute a sum over all partitions @xmath314 such that @xmath315 .",
    "@xmath316    in the last line , we used the fact that by construction of the algorithm , on the event we consider , namely @xmath317 , we have the equality of random variables @xmath318 . the latter are i.i.d . and independent of all the other random variables used at this stage of the algorithm .",
    "moreover : @xmath319    let us now introduce a notation : for @xmath320 we set @xmath321 remark that on the event @xmath317 , almost surely , we have @xmath322 and @xmath323 . using the independence properties of the @xmath324 ( and thus of @xmath325 ) from the induction hypothesis @xmath326",
    ", we obtain @xmath327 we can then integrate the @xmath328 using the induction hypothesis @xmath329 : @xmath330\\\\ & = \\prod_{i\\in i_{+}}\\exp\\bigl(-y_i\\bigr){\\mathbb{e}}\\bigl[\\exp\\bigl(- ( n - k)m^j_{i_- } \\bigr)\\mathds{1}_{m^j_{i_-}>s^{j+1}}\\bigr]\\end{aligned}\\ ] ]    the proof is now complete since : @xmath331\\\\ & \\hspace{3cm}{\\mathbb{p}}\\bigl(s^{1}-s^{0}>s^1,\\ldots , s^{j}-s^{j-1}>s^{j}\\bigr)\\\\ & = \\exp\\bigl(\\sum_{i=1}^{n}y_i\\bigr){\\mathbb{p}}\\bigl(s^{1}-s^{0}>s^1,\\ldots , s^{j}-s^{j-1}>s^{j}\\bigr)\\sum_{\\overset{i_{-}\\subset\\left\\{1,\\ldots , n\\right\\}}{\\text{card}(i_{-})=k}}{\\mathbb{e}}\\bigl[\\exp\\bigl(-(n - k ) m^j_{i_- } \\bigr)\\mathds{1}_{m^j_{i_-}>s^{j+1}}\\bigr],\\end{aligned}\\ ] ] which proves .    in particular ,",
    "taking @xmath332 , we see that @xmath333={\\mathbb{p}}(s^{j+1}-s^{j}>s^{j+1}).\\ ] ]    this concludes the proof of proposition [ propo : why_expo ] .",
    "the next lemma shows that the ams algorithm applied to @xmath2 with target level @xmath7 and the ams algorithm applied to @xmath334 with target level @xmath335 stop at the same iteration .",
    "[ lemme : as_equal ] let us consider the sequence of random variables @xmath293 generated by the ams algorithm  [ algo : ams ] _ without any stopping criterium _ , and set @xmath294 and @xmath295 .",
    "for any @xmath336 , almost surely , @xmath337 .",
    "* proof of lemma [ lemme : as_equal ] : * : first , @xmath289 is non - decreasing so that @xmath338 .",
    "moreover , one easily checks that @xmath339 . from proposition",
    "[ propo : why_expo ] we know that @xmath340 admits a density with respect to the lebesgue measure , since the @xmath341 have the density of the @xmath0-th order statistics of independent exponentially distributed random variables with parameter  @xmath64",
    ". therefore @xmath342 .",
    "a direct corollary of proposition  [ propo : why_expo ] and lemma  [ lemme : as_equal ] is that the original problem reduces to the exponential case .",
    "[ cor : expo ] consider the sequences of random variables @xmath343 and  @xmath344 generated by the ams algorithm  [ algo : ams ] . set @xmath294 and @xmath295 .",
    "then , the sequences @xmath345 and @xmath346 are equal in law to the sequences of random variables @xmath347 and @xmath348 obtained by the realization of the ams algorithm in the exponential case , with initial condition @xmath301 and target level @xmath335 .",
    "finally , in the next sections , we need the following result , which is a consequence of proposition [ propo : why_expo ] .",
    "[ cor : condit ] for any @xmath296 , conditionally on @xmath93 , the random variables @xmath349 are i.i.d .",
    "with law @xmath101 .",
    "* proof : * thanks to proposition [ propo : why_expo ] , we see that @xmath350 are i.i.d . and exponentially distributed with mean @xmath64 . since @xmath351 , we observe that for any @xmath352 , @xmath353 this concludes the proof .      to ensure that the algorithm giving an estimator of the probability is well - defined , namely that it gives a result after a finite number of steps , we prove in this section that @xmath113 is almost surely finite , when the probability @xmath259 is positive .",
    "the proof relies on the reduction to the exponential case explained in the previous section .",
    "[ prop : finitej ] suppose @xmath354 .",
    "then for any @xmath81 $ ] , we have @xmath355 , and for any integers @xmath1 and @xmath0 with @xmath356 , the number of iterations in the ams algorithm is almost surely finite : @xmath357 a.s .    *",
    "proof of proposition [ prop : finitej ] : * to prove this result , we consider the ams algorithm  [ algo : ams ] without any stopping criterion ( namely the condition @xmath358 ) . as a consequence ,",
    "we define sequences of random variables with the iteration index @xmath359 : we get @xmath360 , for any @xmath361 and @xmath362 .",
    "proposition [ prop : finitej ] is then equivalent to the following statement : almost surely , @xmath363 .",
    "thanks to proposition [ propo : why_expo ] , we write for any @xmath296 @xmath364 where @xmath365 are independent and identically distributed positive random variables , satisfying @xmath366 . indeed , @xmath367 where we recall that @xmath304 are independent and exponentially distributed with parameter @xmath64 . to prove that @xmath368",
    ", we write @xmath369 since it is easily checked that @xmath370 has an exponential distribution , with mean @xmath371 .    by the strong law of large numbers ,",
    "when @xmath372 , we have the almost sure convergence @xmath373 which yields @xmath374 , almost surely , when @xmath372 . as a consequence , almost surely , there exists some @xmath359 such that @xmath375 . using lemma  [ lemme : as_equal ] , this then implies that @xmath376 , and that @xmath357 .    in the case",
    "@xmath41 , following the ideas in the proof of proposition [ prop : finitej ] , one can easily identify the law of the number of iterations ( see  @xcite for a similar result ) .",
    "[ propo : poisson1 ] the random variable @xmath377 has a poisson distribution with mean @xmath378 .    * proof of proposition [ propo : poisson1 ] : * in the case @xmath41 , @xmath379 has an exponential distribution with mean @xmath371 .",
    "we recall that @xmath380 is a sequence of independent and identically distributed random variables .",
    "let us introduce the poisson process , with intensity @xmath1 , associated with the sequence of independent and exponentially distributed increments @xmath381 : @xmath382    since @xmath383 , we identify that @xmath384 the result follows since for any @xmath385 @xmath386 has a poisson distribution with mean @xmath387 .",
    "recall that @xmath2 satisfies assumption [ hyp : static ] .",
    "let us fix a total number of replicas @xmath1 , as well as @xmath389 the number of killed and resampled replicas at each iteration . given a level @xmath5 , we recall that the estimator of the conditional probability @xmath390 for each value of @xmath81 $ ] is @xmath388 , defined by .",
    "we introduce the following notation : @xmath391.\\ ] ]    recall that we are specifically interested in estimating the probability @xmath259 , and that the introduction of @xmath392 for @xmath51 in the interval @xmath393 $ ] is a tool to prove that the estimator is unbiased .",
    "we write the result in its full generality , and then specify it to the estimation of @xmath13 .",
    "[ th : unbiased ] for any @xmath394 , for any @xmath5 , such that @xmath395 , and any @xmath81 $ ] , @xmath388 is an unbiased estimator of the conditional probability @xmath392 : @xmath396=p(x).\\ ] ]    in particular , when @xmath83 , we have @xmath397=p$ ] .    from section  [ sec : expo ]",
    "( see corollary  [ cor : expo ] ) , it is sufficient to prove the result in the exponential case .",
    "indeed , let us assume that   holds in the exponential case , and let us consider the general case of a random variable @xmath2 satisfying assumption  [ hyp : static ] , then we have @xmath398 & = { \\mathbb{p}}(\\lambda(x ) > \\lambda(a ) | \\lambda(x ) > \\lambda(x ) ) \\\\ & = \\exp(-\\lambda(a ) + \\lambda(x))\\\\ & = { \\mathbb{p}}(x > a | x > x)=p(x).\\end{aligned}\\ ] ] the first equality is a consequence of corollary  [ cor : expo ] and theorem  [ th : unbiased ] in the exponential case . the third equality is a direct consequence of the definition   of @xmath289 .",
    "the aim of this section is thus to prove theorem  [ th : unbiased ] in the exponential case . in all the following ,",
    "we denote by @xmath399 the density of @xmath2 , and we will use the notation introduced in section  [ sec : density ] above for the density of the @xmath0-th statistics . actually , the proof given below is valid as soon as @xmath2 has a density @xmath142 : we do not use the specific form of the density , and this specific form would not make the argument easier .",
    "the proof of this result is divided into two steps .",
    "first , we show that the function @xmath400 is solution of a functional equation .",
    "second , we show that the function @xmath401 is its unique solution .",
    "[ propo : funct_p ] let us assume that @xmath2 admits a density @xmath142 .",
    "assume @xmath354 . the function @xmath81\\mapsto p^{n , k}(x)$ ]",
    "is solution of the following functional equation ( with unknown @xmath403 ) : for any @xmath404 @xmath405 with@xmath406 where @xmath407 are independent and identically distributed with density @xmath280 ( see  ) , while for @xmath408 , @xmath409 denotes the @xmath410-th order statistics of this @xmath1-sample . by convention",
    ", we set @xmath411 .",
    "* proof : * the key idea is to decompose the expectation @xmath397 $ ] according to the ( random but almost surely finite ) value of the number @xmath113 of iterations .",
    "the function @xmath412 appears as the result of the algorithm when @xmath413 , while the integral formulation corresponds to the case @xmath414 . in the latter case",
    ", we then condition on the value of the first level @xmath86 and use corollary [ cor : condit ] .",
    "more precisely , we have @xmath415={\\mathbb{e}}\\left[\\hat{p}^{n , k}(x)\\mathds{1}_{j^{n , k}(x)=0}\\right]+{\\mathbb{e}}\\left[\\hat{p}^{n , k}(x)\\mathds{1}_{j^{n , k}(x)>0}\\right].\\ ] ]    first , we have from and @xmath416&={\\mathbb{e}}\\left[c^{n , k}(x)\\mathds{1}_{j^{n , k}(x)=0}\\right]={\\mathbb{e}}\\left[c^{n , k}(x)\\mathds{1}_{x_{(k)}^{0}>a}\\right]\\\\ & = \\sum_{l=0}^{k-1}\\frac{n - l}{n}{\\mathbb{e}}\\left[\\mathds{1}_{x_{(l)}^{0}\\leq a < x_{(l+1)}^{0}}\\right]=\\theta_{p}^{n , k}(x).\\end{aligned}\\ ] ]    second , using conditional expectation with respect to @xmath417 , @xmath418&={\\mathbb{e}}\\left[{\\mathbb{e}}\\left[\\left(1-\\frac{k}{n}\\right)\\left(1-\\frac{k}{n}\\right)^{j^{n , k}(x)-1}c^{n , k}(x ) | z^1\\right]\\mathds{1}_{z^1\\leq a}\\right]\\\\ & = { \\mathbb{e}}\\left[\\left(1-\\frac{k}{n}\\right){\\mathbb{e}}\\left[\\left(1-\\frac{k}{n}\\right)^{j^{n , k}(z^1)}c^{n , k}(z^1 ) | z^1\\right]\\mathds{1}_{z^1\\leq a}\\right]\\\\ & = \\left(1-\\frac{k}{n}\\right){\\mathbb{e}}\\left[{\\mathbb{e}}\\left[\\hat{p}^{n , k}(z^1 ) |z^1\\right]\\mathds{1}_{z^1\\leq a}\\right]\\\\ & = \\left(1-\\frac{k}{n}\\right){\\mathbb{e}}\\left[p^{n , k}(z^1)\\mathds{1}_{z^1\\leq a}\\right]\\\\ & = \\left(1-\\frac{k}{n}\\right)\\int_{x}^{a}p^{n , k}(y)f_{n , k}(y;x)dy,\\end{aligned}\\ ] ] where on the event @xmath419 the equality @xmath420={\\mathbb{e}}[(1-\\frac{k}{n})^{j^{n , k}(z^1)}c^{n , k}(z^1 ) | z^1]$ ] is a consequence of corollary [ cor : condit ] , and of the fact that both @xmath113 and @xmath421 are almost surely finite .",
    "the intuition for this computation is that after the first step , if the algorithm does not stop , we just have to restart the algorithm from the level @xmath417 , and consider its associated estimator of the probability .",
    "the multiplication by @xmath422 corresponds to the first iteration , which allows to go from level @xmath423 to level  @xmath417 .",
    "we will need in the following a more explicit formula for @xmath424 .",
    "[ lemme : unbiased ] we have for any @xmath81 $ ] @xmath425    * proof : * we recall that @xmath407 denotes a @xmath1-sample of i.i.d .",
    "random variables with law @xmath135 , and that for any @xmath426 the random variable @xmath427 denotes the @xmath410-th order statistics of this @xmath1-sample : almost surely we have @xmath428 by convention we moreover have @xmath411 .    the proof is based on the partition of the @xmath1-sample into the @xmath429-sample @xmath430 and the random variable @xmath431 .",
    "we express the probabilities appearing in the definition of @xmath424 , using the cumulative distribution function of @xmath431 and of the @xmath410-th order statistics @xmath432 of the @xmath429-sample .",
    "first , starting from , we write@xmath433 since the random variables @xmath434 are independent , for @xmath435 .    now , for a fixed @xmath436 , using the fact that @xmath437 are i.i.d . and",
    "changing the position of @xmath438 in the ordered sample @xmath439 , we have for any @xmath440 @xmath441 with the convention that for @xmath442 the right - hand side above is @xmath443 while for @xmath444 it is @xmath445    we obtain ( since all the terms in the sum below are all the same ) @xmath446 the last equality comes from independence , and the sum expresses the fact that there are @xmath447 positions to insert @xmath438 in the increasing sequence @xmath448 .    thus @xmath449 this concludes the proof of lemma [ lemme : unbiased ] .",
    "notice that we have proved a stronger statement : for @xmath450 @xmath451      let us first state a uniqueness result .",
    "[ lem : uniquenes ] the functional equation   admits at most one solution @xmath452 \\to { \\mathbb{r}}^+$ ] in @xmath453)$ ] .",
    "* proof : * let @xmath454\\rightarrow { \\mathbb{r}}^+$ ] be two bounded solutions . then , we have for any @xmath81 $ ] @xmath455 which shows that @xmath456 , since @xmath457 .    notice that both functions @xmath402 and @xmath458 take values in @xmath212 $ ] , and are therefore bounded .",
    "thanks to proposition  [ propo : funct_p ] , @xmath402 satisfies , and theorem  [ th : unbiased ] is thus a direct consequence of lemma  [ lem : uniquenes ] if we prove that @xmath458 is also solution of this functional equation .",
    "* proof of theorem [ th : unbiased ] : *    the proof consists in proving that @xmath459 is solution of . for this",
    "we have to compute for @xmath460 $ ] , @xmath461 thanks to the definitions of @xmath285 and @xmath462 , and the relation @xmath463 for any @xmath464 , obtained from .",
    "we then conclude by checking the following identity , which is a consequence of lemma  [ lemme : unbiased ] .",
    "@xmath465 which concludes the proof .",
    "in this section , we introduce a notion of cost for the algorithm ( related to the variance of the estimator and to the expected number of iterations ) , which allows to study the influence of the parameters @xmath1 and @xmath0 .",
    "we then give asymptotic expansions of the variance , the expected number of iterations , and the cost , when @xmath1 tends to @xmath466 , for fixed values of @xmath0 and of the probability @xmath13 , and give interpretations of the results , compared to a direct monte carlo estimate as presented in the introduction .",
    "all these results are given under assumption  [ hyp : static ] .",
    "the proofs are then given in section  [ sect : details ] .",
    "in the following , we denote by @xmath18 the cost corresponding to the simulation of one random variable sampled according to the law @xmath135 , for any @xmath81 $ ] .",
    "we assume that this cost does not depend on @xmath51 .",
    "we consider the monte carlo approximation of @xmath13 using @xmath11 independent realizations of the ams algorithm  [ algo : ams ] .",
    "the associated estimator is @xmath467 where @xmath468 is the ams estimator for the @xmath469-th independent realization of the algorithm .",
    "following the reasoning used in the introduction on the direct monte carlo estimator , for a given tolerance error @xmath19 , we want the relative error @xmath470 to be less than @xmath19 , i.e. @xmath471    we thus have to choose @xmath472 .    for each realization @xmath469 of the algorithm ,",
    "@xmath473 iterations are necessary , so that @xmath474 random variables are sampled : @xmath1 at the initial step , and then @xmath0 new ones at each iteration .",
    "this gives a cost @xmath475 , where @xmath18 is the computational cost of the sampling of one random variable distributed according to @xmath97 .",
    "moreover , at the first iteration , one needs to sort the random variables @xmath476 ( with an associated cost @xmath477 ) and at each iteration , one has to insert the @xmath0 new sampled particles into the already sorted @xmath181 remaining particles ( with an associated cost @xmath478 ) .",
    "the sorting procedures are thus associated with a cost @xmath479 .",
    "the total cost is thus @xmath480 and by an application of the law of large numbers , when @xmath11 is large , it is legitimate to consider that the cost to obtain a relative error of size @xmath19 is thus @xmath481 where @xmath482+n\\right)}{p(x)^2}.\\ ] ]    this is consistent with the standard definition of the efficiency of a monte carlo procedure as `` inversely proportional to the product of the sampling variance and the amount of labour expended in obtaining this estimate '' , see  ( * ? ? ?",
    "* section 2.5 ) .",
    "this should be compared with the cost of a direct monte carlo computation , which we recall ( see  ):",
    "@xmath483    if we add the possibility of using @xmath484 processors to sample in parallel the required random variables at each iteration ( assuming for simplicity that @xmath485 is an integer ) then the cost is divided by @xmath486 , and is thus @xmath487 .",
    "notice that this resulting cost is the same as if we run in parallel @xmath486 independent realizations of @xmath468 to compute the estimator @xmath488 ( assuming for simplicity that @xmath489 is an integer ) .",
    "both these parallelization strategies have the same effect on the cost in the setting of this article .",
    "let us set a few notations : for @xmath460 $ ] , @xmath490\\text { and } t^{n , k}(x)={\\mathbb{e}}\\left[j^{n , k}(x)\\right]+1.\\ ] ] notice that @xmath491 is the expected number of steps in the algorithm ( the initialization plus @xmath113 iterations ) . using this notation , we have @xmath492      we divide the results into three parts .",
    "we first study the variance , and then the average number of iterations .",
    "finally , we combine the results to get the cost",
    ". we do not have explicit expressions for each value of  @xmath0 and  @xmath1 , but we get informative asymptotic results when  @xmath65 .",
    "we assume that @xmath354 , and consider @xmath493 , such that @xmath494 .",
    "note that if @xmath495 , then almost surely @xmath496 , @xmath497 and @xmath498 , so that no asymptotic expansion is necessary .",
    "[ propo : var ] for any fixed @xmath0 and any @xmath499 with @xmath494 , when @xmath65 we have @xmath500(k-1)}{2n}+{\\rm o}\\left(\\frac{1}{n}\\right)\\right ) . \\ ] ]    [ propo : time ] for any fixed @xmath0 and @xmath499 with @xmath494 , when @xmath65 we have @xmath501+\\frac{3k-1}{2kn}+{\\rm o}\\left(\\frac{1}{n}\\right)\\right ) . \\ ] ]    finally , we have the following result on the cost :    [ theo : cost ] for any fixed @xmath0 and @xmath499 with @xmath494 , when @xmath65 we have @xmath502\\\\ & + \\frac{1}{n}\\left(-\\log ( p(x))\\left[k-1 \\right]+\\frac{1}{2}\\left(\\log ( p(x))\\right)^2-\\frac{1}{2}\\left(\\log ( p(x))\\right)^3\\right)+{\\rm o}\\left(\\frac{1}{n}\\right ) . \\end{aligned}\\ ] ]    the proof of theorem  [ theo : cost ] from the propositions [ propo : var ] and [ propo : time ] is straightforward using  .",
    "the proof of the two propositions is long and technical , and is postponed to section  [ sect : details ] .",
    "let us also state an immediate corollary of theorem [ th : unbiased ] and of proposition [ propo : var ] .    for any fixed @xmath0 and @xmath499 with @xmath494 ,",
    "when @xmath65 , we have the following convergence in probability : @xmath503    [ rem : cas_expo ] the case @xmath41 is much simpler than the general case @xmath504 , and allows for direct computations .",
    "as seen in section [ sect : wellposed ] , for @xmath505 and @xmath404 , @xmath377 has a poisson distribution with parameter @xmath378 .",
    "we can then easily check the unbiased property @xmath506 , and compute @xmath507 and @xmath508 .",
    "in particular , no asymptotic expansions are required in order to understand the behavior of the computational cost .",
    "notice that in the case @xmath41 , @xmath509=-n\\log(p(x))$ ] , and thus @xmath510 is an unbiased estimator of @xmath511 , with variance @xmath512 .",
    "there is no such statement when @xmath513 : from proposition [ propo : time ] , we have the limit @xmath514\\rightarrow \\frac{-\\log(p(x))}{k}$ ] , but the first - order term in @xmath371 is equal to @xmath515 and is therefore not zero , except if @xmath41 or @xmath516 - in which case one should compute an higher - order expansion to prove that there is a bias .",
    "let us now recall the main two practical consequences of theorem  [ theo : cost ] , that we already stressed in the introduction .",
    "first , in view of   and  - , the ams algorithm is more efficient than a direct monte carlo procedure to estimate @xmath259 if @xmath517 which is always true for sufficiently small @xmath13 .",
    "second , from theorem [ theo : cost ] , we observe that all choices of @xmath0 give the same leading order term for the cost . but looking at the term of order @xmath518 , we see that the optimal choice is @xmath41 .",
    "this conclusion can also be deduced from the asymptotic expansion on the variance given in proposition  [ propo : var ] .",
    "this section is devoted to the proof of propositions [ propo : var ] and [ propo : time ] , which together yield the cost estimate of theorem [ theo : cost ] .",
    "the main steps are the following :    * in section  [ sec : reduc_expo ] , we first show that we can reduce the analysis to the exponential case , by the change of variable using the function @xmath289 , as explained in section  [ sec : expo ] . * in section  [",
    "sect : funct_vt ] , we then derive functional equations on the two functions @xmath519 and @xmath520 ( defined by  ) .",
    "these functional equations can actually be obtained not only in the exponential case , but for any @xmath2 which admits a density with respect to the lebesgue measure . * in section  [ sec : ode ] , we prove that , in the exponential case , the functional equations on @xmath519 and @xmath520 are equivalent to linear ordinary differential equations of order @xmath0 . *",
    "finally , we compute asymptotic expansions of @xmath519 ( in section  [ sec : v ] ) and @xmath520 ( in section  [ sec : t ] ) in the large @xmath1 limit , for fixed @xmath0 and @xmath13 .        we have seen in section  [ sec : expo ] ( see corollary  [ cor : expo ] ) that the estimator @xmath388 obtained with the ams algorithm applied to a general random variable @xmath2 ( satisfying assumption  [ hyp : static ] ) with initial condition @xmath51 and target level @xmath7 is exactly the same in law as the estimator @xmath521 that is obtained with the ams algorithm applied to an exponentially distributed random variable @xmath2 , with initial condition @xmath522 and target level @xmath335 .",
    "it is therefore sufficient to prove the propositions [ propo : var ] and [ propo : time ] in the exponential case . indeed ,",
    "if we obtain in the exponential case , for an initial condition @xmath51 and a target level @xmath523 : @xmath524(k-1)}{2n}+{\\rm o}\\left(\\frac{1}{n}\\right)\\right ) , \\label{eq : expo_expan_v}\\\\ t^{n , k}(x)&=n\\left((a - x)\\left[\\frac{1}{k}-\\frac{k-1}{2kn}\\right]+\\frac{3k-1}{2kn}+{\\rm o}\\left(\\frac{1}{n}\\right)\\right),\\label{eq : expo_expan_t}\\end{aligned}\\ ] ] then the general case is easily obtained by replacing @xmath51 by @xmath522 and @xmath7 by @xmath335 , since @xmath525 .",
    "we now write functional equations satisfied by @xmath526 and @xmath491 defined by  .",
    "even though we will only need these functional equations in the exponential case as explained above , we derive these functional equations in a more general setting , namely when @xmath2 admits a density @xmath142 with respect to the lebesgue measure .",
    "we refer to section  [ sec : density ] for relevant notation associated to this setting .",
    "notice that the derivations of these functional equations are very similar to the derivation of the functional equation on @xmath402 in the proof of proposition  [ propo : funct_p ] .",
    "[ propo : funct_v ] assume @xmath527",
    ". the function @xmath528 is solution of the following functional equation ( with unknown @xmath529 ) : for any @xmath404 @xmath530 with @xmath531 where @xmath407 are independent and identically distributed with density @xmath532 , and @xmath409 denotes the @xmath410-th order statistics of this @xmath1-sample . by convention , @xmath411",
    ".        we do not give the details of the proofs of these two results .",
    "the proof of proposition [ propo : funct_v ] follows exactly the same lines as the proof of proposition [ propo : funct_p ] . for proposition",
    "[ propo : funct_t ] , using the same arguments , one obtains : @xmath536 which is indeed  .",
    "* proof of lemma [ lemme : theta_v ] : * the notations are the following : @xmath540 are @xmath1 independent random variables distributed according to @xmath97 , @xmath432 is the @xmath410-th order statistic of the random variables @xmath541 , and @xmath542 is the @xmath410-th order statistic of the random variables @xmath543 . here again",
    "@xmath544 .",
    "as mentioned above , the functional equations - and the equation   on @xmath547 are valid for any @xmath2 with a density @xmath142 .",
    "however , we are only able to exploit them in the exponential case . from now on , we thus only consider the exponential case : @xmath548 , @xmath549 and @xmath550 .",
    "[ [ sec : ode ] ] ordinary differential equations on @xmath402 , @xmath519 and @xmath520 in the exponential case ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~      [ propo : ode ] let @xmath1 and @xmath551 be fixed and let us assume that @xmath552 .",
    "there exist real numbers @xmath553 and @xmath554 , depending only on @xmath1 and @xmath0 , such that @xmath402 , @xmath519 and @xmath520 satisfy the following linear ordinary differential equations ( odes ) of order @xmath0 : for @xmath555 $ ] : @xmath556 notice that in the summation starts at @xmath557 , while in and it starts at @xmath558 .",
    "the coefficients @xmath553 and @xmath554 are defined by a simple induction formula , see  .",
    "the main tool for the proof of proposition [ propo : ode ] is the following formula on the derivative of the density @xmath562 with respect to @xmath51 : for all @xmath563 , @xmath564 recall that @xmath565 for @xmath566 .",
    "the proof of the first formula in is straightforward , since @xmath567 . for @xmath568 ,",
    "we write ( using  ) @xmath569    [ rem : expo_gen ] a generalization of holds in a more general case than the exponential setting .",
    "indeed , if @xmath2 has a density @xmath142 , then , for @xmath563 @xmath570 in the exponential case , the simplification @xmath571 helps getting simpler formulae , which lead to the linear odes of proposition [ propo : ode ] .",
    "it is also worth noting the following formula @xmath572 , which explains the role played by the change of variable using the function @xmath289 to reduce the general case to the exponential case .",
    "* @xmath581 ( this is the functional equation ) . * for any @xmath582 , we prove by induction that the following formula holds : @xmath583 the coefficients are defined recursively as follows : @xmath584 * the ode is obtained at @xmath585 , using the definition of @xmath586 and setting @xmath587 and @xmath588 : @xmath589        the argument is as follows .",
    "using lemma [ lemme : unbiased ] , in the special case of exponential random variables , elementary computations show that for @xmath404 @xmath594 this shows that @xmath412 as well as its derivatives , are linear combinations of the linearly independent functions @xmath595 , @xmath596 , @xmath597 . for our purpose , the exact expression of the coefficients",
    "does not matter .",
    "it is actually possible to prove directly the identity   from the definition of the coefficients @xmath602 , without resorting to the result of theorem [ th : unbiased ] .",
    "indeed , let us introduce @xmath603 .",
    "thanks to the recursion formula  , introducing an appropriate telescoping sum , one obtains that the coefficients @xmath604 satisfy @xmath605 and @xmath606 which implies that @xmath607 for @xmath600 .",
    "one thus obtains that @xmath402 satisfies the ode   without using theorem [ th : unbiased ] .",
    "this approach actually yields an alternative proof of theorem [ th : unbiased ] in the exponential case , that can then be extended to the general case using the reduction to the exponential case explained in section  [ sec : expo ] . for this",
    ", we check that @xmath608 is solution of the differential equation on @xmath402 .",
    "first , it satisfies the appropriate condition at @xmath559 given in proposition [ propo : ode ] .",
    "second , we observe that @xmath214 is a root of the characteristic polynomial equation associated with the linear ode which writes ( the unknown variable being @xmath609 ) : @xmath610 see   below for the derivation of the caracteristic equation .    to conclude the proof of proposition [ propo : ode ] , it remains to show the boundary conditions at @xmath559 . from the recursion equation   leading to the differential equation on @xmath402 , and lemma [ lemme : unbiased ] , we have for @xmath611 @xmath612 indeed , the second identity of yields @xmath613 for any @xmath614 .          in the next sections ,",
    "we analyze the differential equations on @xmath519 and @xmath520 .",
    "we are not able to derive explicit expressions for the solutions , except when @xmath41 ( see remark [ rem : cas_expo ] ) . however , we are able to analyze quantitatively the behavior when @xmath65 .      in this section , we prove proposition [ propo : var ] in the exponential case , namely  .",
    "since @xmath617 and we know from theorem [ th : unbiased ] that @xmath598 , we focus on @xmath519 .",
    "this function is solution of the linear ode of order @xmath0 given in proposition [ propo : ode ] , which we rewrite here : @xmath618          [ propo_beta ] let @xmath0 be fixed .",
    "there is a unique root of in the real interval @xmath624 $ ] , denoted by @xmath625 .",
    "the other roots of in @xmath626 are denoted by @xmath627.moreover , we have the following asymptotic expansions when @xmath65 : @xmath628    * proof of proposition [ propo_beta ] : * we observe that the complex numbers @xmath629 are solutions of the following polynomial equation of degree @xmath0 ( with unknown @xmath630 ) : @xmath631 the claim for @xmath632 then follows by continuity of the roots of a polynomial function of fixed degree with respect to the coefficients .",
    "indeed , in the limit @xmath65 we obtain the equation @xmath633 , whose roots are @xmath634 . if @xmath577 , we thus obtain @xmath635 .",
    "this argument for @xmath579 yields that @xmath636 goes to @xmath8 , and this is not sufficient for our purposes .",
    "to study the behavior of @xmath625 , let us introduce the polynomial function @xmath637 this function is strictly non - decreasing on the interval @xmath638 $ ] ( which contains no root of @xmath639 and of its derivative ) .",
    "now straightforward computations show that @xmath640 , and thus @xmath639 admits a root in the interval @xmath624 $ ] . for @xmath1 sufficiently large , since @xmath641 $ ] , for @xmath642 , we get @xmath643 $ ] .",
    "moreover , elementary computations show that @xmath644 and therefore since @xmath645 is non - decreasing , @xmath646 : the sequence @xmath647 is non - decreasing . since @xmath648 for any @xmath649 , the sequence converges .      for @xmath1 large enough , all the roots are therefore simple , and we can express the function @xmath519 in the following way : for @xmath404 @xmath655 for some complex numbers @xmath656 , satisfying appropriate conditions to satisfy the boundary conditions  .",
    "in particular , these complex numbers are such that @xmath519 is real - valued which implies necessarily @xmath657 .",
    "actually , these complex numbers are solution to a system of linear equations ( which corresponds to  ) . using cramer s rule",
    ", we give explicit formulae and then get asymptotic expansions for each @xmath658 when @xmath65 .",
    "thanks to the previous result and to the expression of @xmath519 as a combination of exponential functions  , subtracting @xmath660 , we obtain the desired asymptotic expansion   for the variance when @xmath65 . notice that since @xmath51 is assumed to be strictly smaller than @xmath7 , the only contribution which remains is @xmath661 .",
    "the other terms in the sum   vanish exponentially fast .",
    "this concludes the proof of proposition  [ propo : var ] .",
    "* proof of proposition  [ propo : asymptv ] : * the family @xmath656 is solution of the following system of linear equations ( using  ): @xmath662 using cramer s rule ( which gives the solution of an invertible linear system thanks to ratios of determinants ) , we see that @xmath663 where @xmath664 denotes the vandermonde determinant of the complex numbers @xmath665 .",
    "we recall that @xmath666 .          in this section , we prove proposition [ propo : time ] in the exponential case , namely  , following the same approach as in the previous section .",
    "recall from proposition [ propo : ode ] that @xmath520 is solution of the following linear differential equation of order @xmath0 : @xmath673          we omit the proof of proposition  [ propo_alpha ] , since it is very similar to the proof of proposition [ propo_beta ] .",
    "we have already identified that @xmath679 and the asymptotic formulae for @xmath680 when @xmath681 are obtained with exactly the same arguments as for @xmath668 in proposition  [ propo_beta ] . again , for @xmath1 large enough , the roots @xmath682 are pairwise distinct .",
    "the two differences with the analysis performed in the previous section are the following . first , the differential equation   on @xmath520 contains a non - zero right - hand side ( namely a constant ) .",
    "moreover , constant functions are solutions of the differential equation without this right - hand side , since @xmath8 is a root of",
    ".    therefore @xmath520 can be expressed as a sum of an affine function and of exponential functions , for @xmath1 large enough ( so that roots are pairwise distinct ) : for any @xmath404 @xmath683 for some complex coefficients @xmath684 and @xmath685 , for @xmath573",
    ".        * proof : * first , the differential equation   on @xmath520 in proposition [ propo : ode ] is in fact valid on @xmath687 $ ] , not only on @xmath393 $ ] .",
    "it corresponds to the estimation by the ams algorithm of @xmath688 for @xmath689 and @xmath690 exponentially distributed .",
    "the values of @xmath685 for @xmath573 depend only on the conditions at @xmath559 for @xmath520 and its derivatives up to order @xmath703 , namely  .",
    "they are solutions of a system of linear equations and they can be expressed thanks to cramer s rule .",
    "the family @xmath704 is solution of the following system of linear equations : @xmath705              a similar cramer s rule holds for each @xmath685 when @xmath719 .",
    "it is then easy to check that @xmath720 .",
    "in fact , an analytical formula for the limit of @xmath685 in the limit @xmath65 can be written , but we do not need such a sharp result for our purposes .",
    "f.  crou and a.  guyader .",
    "adaptive particle techniques and rare event estimation . in _",
    "conference oxford sur les mthodes de monte carlo squentielles _ , volume  19 of _ esaim proc .",
    "_ , pages 6572 .",
    ", les ulis , 2007 ."
  ],
  "abstract_text": [
    "<S> the adaptive multilevel splitting algorithm @xcite is a very powerful and versatile method to estimate rare events probabilities . </S>",
    "<S> it is an iterative procedure on an interacting particle system , where at each step , the @xmath0 less well - adapted particles among @xmath1 are killed while @xmath0 new better adapted particles are resampled according to a conditional law . </S>",
    "<S> we analyze the algorithm in the idealized setting of an exact resampling and prove that the estimator of the rare event probability is unbiased whatever @xmath0 . </S>",
    "<S> we also obtain a precise asymptotic expansion for the variance of the estimator and the cost of the algorithm in the large @xmath1 limit , for a fixed @xmath0 . </S>"
  ]
}