{
  "article_text": [
    "analysis and forecasting missing data is a well - known area of statistics going back to earlier works of bartlett @xcite , tocher @xcite , wilks @xcite , yates @xcite and many others ( see review paper @xcite ) .",
    "there is a large number of review papers and books related to this subject , @xcite , @xcite , @xcite , @xcite and @xcite , to mention a few .",
    "there are various approaches to missing data , including bayes methods @xcite , maximum likelihood , multiple imputations methods , methods of non - parametric regression and others , e.g. @xcite , @xcite , @xcite .    in the present paper",
    "we suggest a new method of predicting a special class of missing observations in different time series including regression and auto - regression models .",
    "we suggest a simple recurrence procedure , and to the authors knowledge , it is new and simpler than the computational procedures that were known before .",
    "we study autoregressive time series with missing observations , which we propose to predict using a control method .",
    "this method is developed for different types of autoregressive models including ar(@xmath0 ) models in the case of scalar variables and ar(1 ) in the case of vector - valued observations .",
    "forecasting missing data in autoregressive time series has received a special attention in the literature : @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite and @xcite . the typical approach for forecasting missing data in autoregressive models considered in most of papers",
    "is based on maximization of likelihood ratio , which can be computationally intensive .",
    "the approach of the present paper , referred to as a _",
    "control method _ allows to obtain easily computable formulae for missing data .",
    "it is known that a one - dimensional ar(@xmath0 ) model can be transformed to the special case of a @xmath0-dimensional ar(1 ) model ( e.g. anderson @xcite ) . in the present paper",
    "we consider both one - dimensional ar(@xmath0 ) models and multidimensional ar(1 ) models nevertheless .",
    "the representations obtained in the case of one - dimensional ar(@xmath0 ) models are simpler for computations than that for a multidimensional ar(1 ) model . whereas representations for a one - dimensional ar(@xmath0 ) model is recurrence formulae and can be calculated directly , the computations for a multidimensional ar(1 ) model requires two steps . in the first step we calculate the vector norm , and then the vector corresponding to a missing value .",
    "we assume that in the time series : @xmath1 the first @xmath2 observations are known/ observed as well as the last value @xmath3 is assumed to be given too .",
    "the values @xmath4 , @xmath5 ,  , @xmath6 are missing .",
    "this set up may have various applications , for example in economics and finance , where historical data indices are given , while the last value can be obtained from financial derivatives , or might be set externally . in finance , for example",
    ", on basis of historical volatilities and a future value obtained from options one predicts the dynamics of the volatility .    although the paper concerns with data structure , the results can be extended to different more complicated structures of missing values .",
    "indeed , consider for instance the following data @xmath7 here in the data indexed from 1 to @xmath2 and from @xmath8 to @xmath9 are known , the last point @xmath3 is assigned , and the rest of data are missing .",
    "then we have two groups of missing data , and standard decomposition arguments can be used to reduce analysis to that of a single group with missing data .",
    "the paper is structured as follows . in section",
    "[ approaches ] we discuss the known methods of forecasting missing observation as well as the method of the present paper with comparisons . in the following section we discuss forecasting missing data by a control method in order of increasing complexity .",
    "specifically , in section [ autoregressive ] we study the problem for the simplest ar(1 ) model of time series , and in section [ ar models ] we extend the results for ar(@xmath0 ) models , @xmath10 .",
    "the multi - dimensional observations of ar(1 ) model are studied in section [ multi - autoregressive ] .",
    "then , in section [ multi - regression ] the problem is solved for models of regression .",
    "the results of this section are easily understandable and simple . in section [ numerical work ]",
    "two numerical examples are considered in finance and archaeology .",
    "there is a large number of papers on estimation and forecasting of missing observation in autoregressive models .",
    "jones @xcite provides the method for calculation of exact likelihood function of stationary arma time series .",
    "the method is based on akake s markovian representation and application of kalman s recursions @xcite .",
    "an advantage of kalman s recursions is that the matrices and vectors being used in calculations have dimensions @xmath11 , where @xmath0 is the order of the auto - regression , and @xmath12 is the order of moving average , rather than dimensions corresponding to the number of observations . a non - linear optimization program is then used to find the maximum likelihood estimates .",
    "kohn and ansley @xcite study interpolation missing data for non - stationary arima models .",
    "the likelihood ratio for these models does not exist in the usual sense , and the authors define marginal likelihood ratio .",
    "they show , that marginal likelihood approach reduces in some cases to the usual likelihood approachforecasting missing observations in is based on a modified kalman filter , which has been introduced in the earlier paper of these authors @xcite .",
    "shin and pantula @xcite discuss the testing problem for a unit root in an autoregressive model where data are available for each @xmath13-th period .",
    "the idea is to use characteristic polynomials and properties of their coefficients . under special assumption",
    "@xcite estimate parameters of arma(@xmath0 , @xmath0 - 1 ) by fitting an arma(1 , @xmath0 - 1 ) model . by using a monte carlo simulation ,",
    "the results were compared by those obtained in earlier papers of pantula and hall @xcite , said and dickey @xcite and shin and fuller @xcite , who also studied the same testing problem .",
    "forecasting in autoregressive models has also been studied by kharin and huryn @xcite and @xcite .",
    "@xcite investigate the case of unknown parameters of an autoregressive model based on the so - called  plug - in \" approach .",
    "plug - in \" approach consists of two steps : ( i ) estimation of the model parameters by some known approach and ( ii ) forecasting , based on estimation of the parameters in the first step .",
    "this method has lower computational complexity than other methods , such as straightforward joint maximum likelihood estimation of the parameters and future values of time series , or expectation - maximization algorithm ( e.g. little and rubin @xcite , jordan and jacobs @xcite ) . in @xcite the mean - squared error of maximum likelihood forecasting in the case of missing values",
    "is obtained for many autoregressive time series .",
    "the above - mentioned papers @xcite and @xcite all study a general scheme of missing data .",
    "together with vector - valued time series they introduce a binary vector characterizing a  missing pattern \" but the solution for this general formulation is hard to implement in practice .",
    "the aim of the present paper is prediction ( interpolation ) of missing observations whereas the aim of two above - mentioned papers is forecasting _ in the presence _ of missing observations , i.e. the forecasting procedure takes into account missing observations .",
    "furthermore , the approach of the present paper deals with specific data structures ( section [ introduction ] ) , and can be extended to more complicated structures of missing data . in the initial step",
    "we use least squares predictors for the preliminary extrapolation of missing values .",
    "then , taking into account the last known observation we make corrections by formulating and solving a control problem .",
    "the control problem is formulated in terms of minimization of sums of squares of errors , which in itself is a classical approach . however , our method of is based on a novel application of the cauchy - schwartz inequality in a simple case , and then extended to other more complicated cases .",
    "the use of the cauchy - schwartz inequality is a known technique in optimization , e.g. @xcite and @xcite , however , in the context of prediction of missing data this method seems to be new .",
    "in addition , this method yields easily computable recurrence formulae for missing values .",
    "in this section we consider autoregressive time series of the following type : @xmath14 the values @xmath15 , @xmath16 , ",
    ", @xmath17 are assumed to be observed , while by @xmath4 , @xmath5 ,  , @xmath6 we denote estimates of missing observations .",
    "the value @xmath18 is also known .",
    "it is convenient to denote this value by tilde , i.e. @xmath19 .    [ thm1 ] best predictors for the missing values are given by @xmath20 where the coefficients @xmath21 and @xmath22 are the least squares solutions of the autoregressive equations @xmath23 for the first @xmath2 observations , @xmath24 .",
    "taking into consideration the first @xmath2 observed values one can build the linear least square predictor as @xmath25 for @xmath26 , where parameters @xmath21 and @xmath22 are the regression coefficients .",
    "these @xmath21 and @xmath22 are then used for control problem , which is to find the unknown points , minimizing sum of squares of controls leading to the known final value .",
    "namely , for @xmath27 ( @xmath28=@xmath29 ) @xmath30 it can be seen as a _ correction _ of the initial linear equation for @xmath31 with a control sequence @xmath32 .",
    "the control problem is to minimize the sum of squares of controls under the condition that the auto - regression ends up at the specified point @xmath33=@xmath34 @xmath35 this minimization problem is solved as follows . by and @xmath36 and taking into account that @xmath37 and @xmath38 from we obtain @xmath39 by the cauchy - schwartz inequality , @xmath40 the equality in is achieved if and only if @xmath41 for some constant @xmath42 , and since the equality in is associated with the minimum of the left - hand side of , the problem reduces to find an appropriate value @xmath43 such that @xmath44 therefore , @xmath45 and then finally for @xmath46 we have : @xmath47 thus , the sequence @xmath32 satisfying is @xmath48 and its substitution for yields the desired result .",
    "under the assumption that is given , we first find the best linear predictor for ar(2 ) model as @xmath49 ( @xmath50 , @xmath51 +  , @xmath52 ) , and then extend the result for the general case of ar(@xmath0 ) model .",
    "[ thm2 ] for ar(2 ) model , the best predictor is given by @xmath53 where @xmath54 , and the coefficients @xmath55 are as follows : @xmath56 , @xmath57    the coefficients @xmath58 , @xmath59 and @xmath22 for equation are the minimum in the least - square sense of the autoregressive equation @xmath60 which are obtained by the first @xmath2 observations .    in the case of ar(2 ) model we have @xmath61 and similarly to , @xmath62 ( @xmath63 ) .",
    "let us now consider the difference @xmath64 .",
    "for this difference we have the following expansion @xmath65 with some coefficients @xmath66 .",
    "now , the main task is to determine these coefficients .",
    "write @xmath56 . then , using induction we obtain @xmath67 specifically , for the first steps we have the following . setting @xmath68 leads to the obvious identity @xmath69 . in the case",
    "@xmath70 we have @xmath71u_{n-1}\\\\ & = \\gamma_0u_n+\\gamma_1u_{n-1}. \\end{aligned}\\ ] ] in the case @xmath72 we have @xmath73u_{n-2}\\\\ & = \\gamma_0u_n+\\gamma_1u_{n-1}+[a_1(\\alpha_1+\\beta_1)+a_2\\gamma_0 + 1]u_{n-2}\\\\ & = \\gamma_0u_n+\\gamma_1u_{n-1}+\\gamma_2u_{n-2}. \\end{aligned}\\ ] ] the next steps follow by induction , and we have recurrence relation - above",
    ".    therefore , @xmath74 and similarly to by cauchy - schwartz inequality @xmath75 the equality in is achieved if and only if @xmath76 for some constant @xmath42 , and since the equality in is associated with the minimum of the left - hand side of , the problem reduces to find an appropriate value @xmath43 such that @xmath77 this finishes the proof .",
    "the results above are easily extended to general ar(@xmath0 ) models .",
    "specifically , we have @xmath78 and @xmath79 ( @xmath80 , @xmath81 +  , @xmath52 ) , and @xmath82 where @xmath83 and @xmath84 thus , similarly to we have the following formula @xmath85 ( @xmath86 ) , where @xmath55 are now defined according to .",
    "in this section we study a multidimensional version of the problem for ar(1 ) .",
    "let @xmath87 for this last value we shall also write @xmath88 ( with tilde ) .",
    "as above , the values @xmath89 , @xmath90 ,  , @xmath91 are assumed to be observed values , while @xmath92 , @xmath93 ,  , @xmath94 are missing observations .    taking into consideration only the first @xmath2 observed values one can build the linear least square predictor as @xmath95 for @xmath26 . here",
    "@xmath96 is a square matrix , and @xmath97 is a vector .    for @xmath27 ( @xmath98=@xmath99 )",
    "we find the unknown points by @xmath100    the problem is to find the vectors @xmath101 , @xmath27 such that they minimize the sum of squares of their lengths subject to the constraint that the auto - regression attains the specified point @xmath102 @xmath103 where @xmath104 and @xmath105 denotes the @xmath106th component of the ( @xmath107-dimensional ) vector @xmath101 .    according to and @xmath108 and for endpoint @xmath109 we have @xmath110",
    "let @xmath111 denotes element ( @xmath112 , @xmath106 ) of matrix @xmath113 .",
    "we have the following .",
    "the @xmath112th element of multiplication of @xmath114 to vector @xmath115 can be written as @xmath116 where @xmath105 is the @xmath106th element of the vector @xmath115 .",
    "therefore can be written as @xmath117\\right)^2\\\\ & = \\left(\\sum_{n = n_0 + 1}^{n}\\sum_{j=1}^{k}\\underbrace{\\left\\{\\sum_{i=1}^ka_{i , j}^{(n - n)}\\right\\}}_{\\text{first term } } \\underbrace{u_{n , j}}_{\\text{second term } } \\right)^2\\\\ \\end{aligned}\\ ] ] therefore , by cauchy - schwartz inequality , we have @xmath118 ^ 2\\right)\\times \\left(\\sum_{n = n_0 + 1}^{n}\\sum_{j=1}^{k}u_{n , j}^2\\right)\\\\ & = \\left(\\sum_{n = n_0 + 1}^{n}\\sum_{j=1}^{k}\\left[\\sum_{i=1}^ka_{i , j}^{(n - n)}\\right]^2\\right)\\times \\left(\\sum_{n = n_0 + 1}^{n}\\|{\\bf u}_n\\|^2\\right ) .",
    "\\end{aligned}\\ ] ] the equality in is achieved if and only if for some constant @xmath42 , @xmath119 and similarly to that of section [ autoregressive ] the optimal value of this constant @xmath46 is @xmath120 ^ 2}.\\ ] ] for the sequence @xmath121 we have : @xmath122 ^ 2}\\cdot \\sum_{j=1}^{k}\\sum_{i=1}^k a_{i , j}^{(n - n)},\\ ] ]    let us now find the vectors @xmath101 , @xmath123 . from and",
    "we have the following : @xmath124 therefore for components of the vector @xmath125 we have equations @xmath126 where @xmath127 denotes the @xmath112th row of the matrix @xmath114 .",
    "therefore , by cauchy - schwartz inequality @xmath128 ^ 2\\right ) \\left(\\sum_{n = n_0 + 1}^{n}\\sum_{j=1}^{k}u_{n , j}^2\\right ) , \\end{aligned}\\ ] ] where the equality achieves in the case if for some @xmath129 @xmath130 ^ 2}=c_i\\|{\\bf u}_n\\|.\\ ] ] therefore , substituting for we obtain : @xmath131^\\top}.\\ ] ]",
    "regression models with incomplete data has been studied intensively in the literature , and there are many approaches the solution of this problem .",
    "the theoretical aspect of the present approach seems to be new nevertheless .",
    "consider first the following data : @xmath132 as above we use the notation @xmath133 .",
    "we first find the vector @xmath134 and parameter @xmath22 by linear least square predictor , so for @xmath135 , @xmath136,  ,@xmath52",
    "we have @xmath137 we have @xmath138 where @xmath139 .    therefore considering @xmath140 where @xmath141 , and @xmath142 and the same problem to @xmath143 we arrive at @xmath144    \\2 .",
    "let us consider a more extended problem @xmath145 where the vectors @xmath146 of the first row all of dimension @xmath13 .    by the linear least square predictor we have @xmath147 here the vectors @xmath148 are of dimension @xmath13 ,",
    "the matrix @xmath96 is of @xmath149 and the vector @xmath97 is of @xmath13 .",
    "we have : @xmath150 where @xmath151=@xmath152 .",
    "let us now consider the equation @xmath153 in this specific case we have @xmath154 by the same calculations as earlier ( see ) we have : @xmath155 and all the constants @xmath129 defined in section [ multi - autoregressive ] are the same .",
    "therefore @xmath156 .",
    "we finally have @xmath157",
    "numerical work of this paper consists of two different parts .",
    "the first part is related to the case of interpolating missing data in autoregressive models .",
    "the two numerical results of this part are reflected in figure 1a and figure 1b .",
    "the second part of numerical work is related to two - dimensional autoregressive model .",
    "the data for this model are related to archaeological field and taken from the paper of cavanagh , buck and litton @xcite .",
    "the real data of volatility dynamic of ibm company calculated on the base of the stock information by the method of @xcite have been used for figure 1a .",
    "we removed some data from the middle and the end of this dynamic and then forecasted missing data by ar(1 ) model for the construction of missing data described by .",
    "the value @xmath2 is equal to 418 , and the corresponding number of missing data is 85 .",
    "then the value @xmath158 is equal to 1058 and the corresponding number of missing data is 106 .    in the second example ( figure 1b )",
    "we use the volatility dynamic of exchange rates of usd and new israel shekel .",
    "the historical period , @xmath2 , is 1319 , and the total length , @xmath52 , is 1466 .",
    "assuming that volatility dynamic is ar(1 ) model , @xmath159 , then by calculation of parameters by linear least square predictor we have @xmath1600.999576 and @xmath161 . assuming that volatility dynamics satisfies ar(2 ) model , @xmath162 , we correspondingly obtain @xmath163 , @xmath164 and @xmath165 . as we can see , although the difference between these predicted models is small , both curves are visible in the graph nevertheless .",
    "in this part we use data from @xcite .",
    "this is data on phosphate concentration reflected in figure 1 ( p.94 ) .",
    "there are missing data in the fifth and sixth row of these data , and these two rows are the rows of table 1 corresponding to two - dimensional vector @xmath166 with missing data , where the missing data there are indicated by ` + ' ."
  ],
  "abstract_text": [
    "<S> consider a time series with missing observations but a known final point . </S>",
    "<S> using control theory ideas we estimate / predict these missing observations . </S>",
    "<S> we obtain recurrence equations which minimize sum of squares of a control sequence . </S>",
    "<S> an advantage of this method is in easily computable formulae and flexibility of its application to different structures of missing data . </S>"
  ]
}