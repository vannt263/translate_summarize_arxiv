{
  "article_text": [
    "k - means @xcite is one of the most widely known clustering algorithms .",
    "the basic problem it solves is as follows : for a fixed natural number @xmath0 and dataset @xmath1 return a set of centers @xmath2 such that it is the solution to the _ k_-means problem : @xmath3 then , using this set of centers , return one of @xmath0 labels for each datum : @xmath4    lloyd s algorithm @xcite is a particularly long - lived strategy for locally solving the k - means problem ( cf .",
    "algorithm [ alg : lloyd ] ) ; it suggests randomly selecting a subset of size @xmath0 from @xmath5 as initial centers , then alternating updates to cluster assignments and new centers .",
    "lloyd s algorithm does not have an approximation guarantee .",
    "hence , for certain datasets , it can return centers that result in a large value of the objective function in equation ( [ eqn : exact ] ) with high probability . thus , even running independent copies of the algorithm and choosing the best result could yield poor results .",
    "macqueen @xcite conjectures that exactly solving equation ( [ eqn : exact ] ) is difficult .",
    "he is correct ; mahajan et al .",
    "@xcite show that even for two dimensional data , k - means is np - hard . because of this , practitioners instead seek to approximately solve the k - means problem .",
    "arthur and vassilvitskii @xcite present k - means++ , a randomized @xmath6 approximation algorithm running in @xmath7 time ( for their initialization step , which is all that is required for the approximation bound ) that works by modifying lloyd s algorithm to choose initial centers with unequal weighting ( cf .",
    "algorithm 2 ) .",
    "their results are remarkable because the algorithm runs in a practical amount of time .",
    "this work inspired others to propose alternative randomized initializations for lloyd s algorithm for streaming data , @xcite parallel implementations , @xcite and bi - approximations with extra ( @xmath8 ) centers that can then be re - clustered to yield @xmath0 centers .",
    "@xcite    in semi - supervised learning , there is additional information available about the true labels of some of the data .",
    "these typically take the form of label information ( e.g. @xmath9 ) or pair - wise constrains ( e.g. @xmath10 or @xmath11 in recent years , there has been a fair amount of interest in solving problems with these additional constraints .",
    "wagstaff et al .",
    "@xcite propose the cop - kmeans algorithm , which uses a modified assignment step in lloyd s algorithm to avoid making cluster assignments that would be in violation of the constraints .",
    "basu et al .",
    "@xcite focused on using label information in their seeded - kmeans and constrained - kmeans algorithms .",
    "both algorithms use the centroids of the labeled points as initial starting centers .",
    "basu et al .",
    "@xcite use the expectation - maximization ( em ) algorithm @xcite as a modified llyod s algorithm to modify the pairwise supervision algorithms to include a step wherein the distance measure is modified ( so that they do not necessarily use euclidean distance ) .",
    "finley and joachims @xcite learn pairwise similarities to account for semi - supervision .",
    "the structure of the remainder of this paper is as follows .",
    "first , we will introduce the definitions and notation to be used afterwards in the remainder of the paper .",
    "next , we present the main algorithm , where we modify the k - means++ algorithm for the semi - supervised with labels case .",
    "we then prove an approximation bound that improves with the amount of supervision .",
    "finally , we include numerical experiments showing the efficacy of the algorithm on simulated and real data under a few performance metrics .",
    "choose an @xmath12 uniformly at random .",
    "+ let @xmath13 +",
    "we will present a few definitions to clarify the notation used in the theoretical results . recall that @xmath14 is our data .",
    "we will additionally partition @xmath15 into the unsupervised and supervised data , respectively .",
    "a clustering , say @xmath16 is a set of centers that are used for cluster assignments of unlabeled points .",
    "fix a clustering @xmath17 define the potential function as @xmath18 be defined such that for @xmath19 , @xmath20    let @xmath21 be a clustering solving the k - means problem from equation ( [ eqn : exact ] ) .",
    "let @xmath22 an optimal cluster @xmath23 is defined such that @xmath24    call the current clustering @xmath16 .",
    "define @xmath25 such that @xmath26",
    "we now propose an extension to the k - means++ algorithm for the semi - supervised case . we will called this the ss - k - means++ algorithm .",
    "suppose that we want to partition our data @xmath14 into @xmath27 groups .",
    "let us agree that the semi - supervision occurs in the following way :    1 .",
    "choose a class @xmath28 uniformly at random ; 2 .   choose @xmath29 observations uniformly at random from @xmath30 3 .   and label these @xmath29 observations as being from class @xmath31 .",
    "we optionally allow repetition of steps @xmath32 to give more partially supervised classes . the modified k - means algorithm , which is algorithm [ algo : sskppinit ] followed by algorithm [ alg : lloyd ] , replaces the initial step of choosing a point at random by choosing @xmath29 points as above , then setting the first center to the centroid of those points .",
    "also , during the @xmath33 probabilistic selection process , we do not allow centers to be chosen from the supervised points .",
    "this makes sense because that cluster is already covered .",
    "note that this essentially the k - means++ version of the constrained - kmeans algorithm @xcite .",
    "let @xmath34 be the number of supervised datapoints with label @xmath35 + let @xmath36 +",
    "consider the objective function @xmath37 the potential function associated with a clustering @xmath38 .",
    "arthur and vassilvitskii @xcite prove that the expectation of the potential function after the seeding phase of the k - means++ algorithm satisfies @xmath39 \\leq 8 ( \\log(k ) + 2 ) \\phi_{opt},\\ ] ] where @xmath40 corresponds to the potential using the optimal @xmath27 centers .",
    "we will improve this bound for our algorithm by mostly following their analysis , _",
    "mutatis mutandis_.    the sketch of the proof is as follows :    1 .   bound the expectation of the potential for the first cluster ( chosen by semi - supervision ) 2 .   bound the expectation of the potential for clusters with centers chosen via @xmath33 weighting conditioned on the center being from an uncovered cluster .",
    "3 .   bound the expectation of the potential when choosing a number of new centers at once in a technical result 4 .   specialize the technical result to our algorithm and get the overall bound    consider a collection of data @xmath41 of size @xmath42 .",
    "suppose we have @xmath43 uniformly chosen at random members of @xmath41 in a set @xmath44 that we consider pre - labeled .",
    "consider the mean of these datum , say @xmath45 , to be the proposed center of @xmath41 , then the expectation of the potential function is @xmath46 = \\mathbb{e}\\left [ \\sum_{a \\in a } \\|a - \\bar{a}_s\\|^2 \\right],\\ ] ] where the expectation is over the choice of the elements of @xmath47 .",
    "we can compute this expectation explicitly .",
    "[ boundonpotentialusingrandommean ] if @xmath44 is a subset of @xmath48 of size @xmath43 chosen uniformly at random from all subsets of @xmath41 of size @xmath43 , then @xmath46 = \\left(1+\\frac{n - g}{g(n-1)}\\right)\\phi_{opt}(a),\\ ] ] where @xmath49 @xmath50 is the centroid of @xmath41 ( i.e. @xmath51 ) , and @xmath45 is the centroid of @xmath47 .",
    "let @xmath52 .",
    "observe @xmath53 & = & \\mathbb{e } \\left [ \\sum_{a \\in a } \\|a - \\bar{a}_s\\|^2\\right ] \\\\",
    "& = & \\mathbb{e } \\left [ \\sum_{a \\in a } \\|a -c(a)\\|^2 + n \\|\\bar{a}_s - c(a ) \\|^2 \\right ] \\mbox { by lemma } \\ref{distancetoarbpoint}\\\\ & = & \\sum_{a \\in a } \\|a -c(a)\\|^2 + n \\mathbb{e } \\left [ \\|\\bar{a}_s - c(a ) \\|^2 \\right ] \\\\ & = & \\phi_{opt}(a ) + n \\mathbb{e } \\left [ \\|\\bar{a}_s - c(a ) \\|^2 \\right].\\end{aligned}\\ ] ]    let us determine @xmath54.$ ] observe @xmath55&= & \\mathbb{e } \\left [ \\bar{a}_s^t \\bar{a}_s \\right ] - 2 c(a)^t \\mathbb{e}\\left [ \\bar{a}_s \\right ] + c(a)^tc(a ) \\\\",
    "& = & \\mathbb{e } \\left [ \\bar{a}_s^t \\bar{a}_s \\right ]   - 2 g^{-1 } c(a)^t \\mathbb{e}\\left [ \\left(\\sum_{a \\in s}^n a\\right ) \\right ] + c(a)^tc(a ) \\\\ & = & \\mathbb{e } \\left [ \\bar{a}_s^t \\bar{a}_s \\right ]   - 2 g^{-1 } c(a)^t \\frac{g}{n } \\sum_{i=1}^n a_i + c(a)^tc(a ) \\mbox { by lemma } \\ref{expectationofsubset}\\\\ & = & \\mathbb{e } \\left [ \\bar{a}_s^t \\bar{a}_s \\right ]   - 2 c(a)^t c(a)+ c(a)^tc(a ) \\\\ & = & g^{-2}\\mathbb{e } \\left [ \\left(\\sum_{a \\in s}^n a\\right)^t \\left(\\sum_{a",
    "\\in s}^n a\\right ) \\right ] -   c(a)^t c(a).\\end{aligned}\\ ] ] applying lemma [ secondmomentofsubset ] , we have @xmath55 & = & g^{-2}\\left ( \\frac{g ( g-1 ) } { n ( n-1 ) } \\sum_{i \\not= j } a_i^t a_j + \\frac{g}{n } \\sum_{i=1}^n a_i^t a_i\\right ) -   c(a)^t c(a)\\\\ % & = & \\frac{1}{gn } \\left(\\frac{g-1}{n-1 } \\sum_{i , j } a_i^t a_j + \\left(1 - \\frac{g-1}{n-1}\\right ) \\sum_{i=1}^n",
    "a_i^t a_i \\right ) -   c(a)^t c(a ) \\\\ % & = & \\frac{1}{gn } \\left(\\frac{g-1}{n-1 } n^2 c(a)^t c(a)+ \\left(1 - \\frac{g-1}{n-1}\\right ) \\sum_{i=1}^n",
    "a_i^t a_i \\right ) -   c(a)^t c(a ) \\\\ % & = & \\left ( \\frac{g-1}{g}\\frac{n}{n-1 } -1\\right ) c(a)^tc(a)+ \\frac{1}{gn}\\left(1 - \\frac{g-1}{n-1}\\right ) \\sum_{i=1}^n a_i^t",
    "a_i   \\\\ & = & - \\frac{n - g}{g(n-1 ) } c(a)^tc(a)+ \\frac{n - g}{gn(n-1 ) } \\sum_{i=1}^n a_i^t a_i   \\\\ & = &   \\frac{n - g}{g(n-1 ) } \\left(\\frac{1}{n}\\sum_{i=1}^n a_i^t a_i   - c(a)^tc(a)\\right )   \\\\ % & = &   \\frac{n - g}{gn(n-1 ) } \\left(\\sum_{i=1}^n a_i^t a_i   - nc(a)^tc(a)\\right )   \\\\ & = &   \\frac{n - g}{gn(n-1 ) } \\phi_{opt}(a ) .",
    "\\\\\\end{aligned}\\ ] ]    hence , @xmath56 = \\left(1+\\frac{n - g}{g(n-1)}\\right ) \\phi_{opt}(a).$ ]    we present several technical lemmas used above .",
    "[ distancetoarbpoint ] let @xmath57 and @xmath58 be the centroid of a. let @xmath59 for any @xmath60 , @xmath61    observe @xmath62 hence , @xmath63 which was what was wanted .",
    "[ expectationofsubset ] if @xmath44 is a subset of @xmath48 of size @xmath43 chosen uniformly at random from all subsets of @xmath41 of size @xmath43 , then @xmath64 = \\frac{g}{n } \\sum_{x_i \\in a } x_i.\\ ] ]    let @xmath65 be the indicator random variable that is @xmath66 if @xmath67 and @xmath68",
    "otherwise ( i.e. @xmath69 observe @xmath70 & = & \\mathbb{e } [ \\sum_{i=1}^{n } x_i x_i | \\sum_{i=1}^n x_i = g ] \\\\ & = &   \\sum_{i=1}^{n } x_i \\mathbb{e } [ x_i | \\sum_{i=1}^n x_i = g ] \\\\\\end{aligned}\\ ] ]    observe @xmath71 = \\frac{\\binom{n - 1}{g - 1}}{\\binom{n}{g}}$ ] , the probability that @xmath72 is chosen from for a group of size @xmath43 from @xmath42 objects in @xmath41 .",
    "the conclusion follows .",
    "[ secondmomentofsubset ] if @xmath44 is a subset of @xmath48 of size @xmath43 chosen uniformly at random from all subsets of @xmath41 of size @xmath43 , then @xmath73 = \\frac{g ( g-1 ) } { n ( n-1 ) } \\sum_{i \\not= j } x_i^t x_j + \\frac{g}{n } \\sum_{i=1}^n x_i^t x_i.\\ ] ]    let @xmath65 be the indicator random variable that is @xmath66 if @xmath67 and @xmath68 otherwise ( i.e. @xmath69 observe @xmath74 & = & \\mathbb{e}\\left [ \\left(\\sum_{i=1}^{n } x_i x_i \\right)^t \\left(\\sum_{j=1}^{n }",
    "x_j x_j \\right ) | \\sum_{\\ell=1}^n x_\\ell = g\\right ] \\\\",
    "& = &   \\mathbb{e}\\left [ \\sum_{i , j}^{n } x_i^t x_j x_i x_j \\",
    "| \\sum_{\\ell=1}^n x_\\ell = g\\right ] \\\\ & = &   \\sum_{i , j}^{n } x_i^t x_j \\mathbb{e}\\left [   x_i x_j \\",
    "| \\sum_{\\ell=1}^n x_\\ell = g \\right ] .",
    "\\\\\\end{aligned}\\ ] ]    observe @xmath75   =   \\begin{cases } \\frac{\\binom{n-2}{g-2}}{\\binom{n}{g } } & \\mbox { if } i \\not = j \\\\ \\frac{\\binom{n - 1}{g - 1}}{\\binom{n}{g } } & \\mbox { if } i = j \\end{cases},\\ ] ] since the first case represents the probability that @xmath72 and @xmath76 are chosen together and the second case represents the probability that @xmath72 is chosen , as @xmath77 .",
    "the conclusion follows .",
    "the first result will handle the semi - supervised cluster .",
    "suppose that @xmath78 is the optimal set of cluster centers .",
    "now , we consider the contribution to the potential function of a cluster @xmath79 from @xmath78 when a center is chosen from @xmath80 with @xmath33 weighting .",
    "if we can prove a good approximation bound , then we can say that conditioned on choosing centers from uncovered clusters , we will have a good result on average .",
    "[ res : dsqcontrib ] let @xmath38 be the current ( arbitrary ) set of cluster centers . note that @xmath38 is probably not a subset of @xmath78",
    "let @xmath81 be any cluster in @xmath78 .",
    "let @xmath82 be a point from @xmath81 chosen at random with @xmath33 weighting .",
    "then , @xmath83 \\leq 8 \\phi(\\mathcal{x}^ * ; c_{opt}),\\ ] ] where the expectation is over the choice of new center @xmath82 .",
    "unchanged from lemma 3.2 in @xcite .",
    "[ lem : inductivebound ] fix a clustering @xmath84 suppose there are @xmath85 uncovered clusters from the optimal clustering @xmath86 denote the points in these uncovered clusters as @xmath87 ( not to be confused with @xmath88 ) .",
    "let @xmath89 be the set of points in covered clusters .",
    "use @xmath33 weighting ( excluding supervised data ) to add @xmath90 new centers to @xmath38 to form @xmath91 . in a slight abuse of notation ,",
    "let @xmath92 , @xmath93 and @xmath94 .",
    "then , @xmath95 \\leq \\left(\\phi(\\mathcal{x}_c ) + 8 \\phi_{opt}(\\mathcal{x}_u)\\right)(1+h_t ) + \\frac{u - t}{u}\\phi(\\mathcal{x}_u),\\ ] ] where @xmath96",
    "we have that the probability of choosing a point from a fixed set @xmath41 with @xmath33 weighting ignoring supervised points is @xmath97 further , note that @xmath98 , since all supervised clusters are covered .",
    "following the argument in @xcite using the above probabilities , we have our result .",
    "[ thm : apxbnd ] suppose our story about how the supervision occurs holds .",
    "let @xmath99 for each label @xmath100 that we have supervised exemplars of , add the centroid of the supervised data labeled @xmath101 say @xmath102 to c. suppose that @xmath103 let @xmath104 be the number of supervised exemplars with label @xmath105 for @xmath106 then , we have @xmath107 uncovered clusters . add @xmath108 new centers using @xmath33 weighting ignoring the supervised points .",
    "the expectation of the resulting potential , @xmath109 , is then @xmath95 \\leq 8 \\phi_{opt}(\\mathcal{x})(2+\\log(k - g)).\\ ] ]    applying lemma [ lem : inductivebound ] with @xmath110 we have @xmath95 \\leq \\left(\\sum_{j=1}^g\\left(\\phi(\\mathcal{x}_{\\ell_j } ) -8 \\phi_{opt}(\\mathcal{x}_{\\ell_j})\\right)+ 8 \\phi_{opt}(\\mathcal{x})\\right)(1+h_{k - g}).\\ ] ] applying lemma [ boundonpotentialusingrandommean ] to each @xmath111 we have @xmath95 \\leq \\left(\\sum_{j=1}^g \\left(7 - \\frac{n - n_{\\ell_j}}{n_{\\ell_j}(n-1)}\\right ) \\phi_{opt}(\\mathcal{x}_{\\ell_j } ) + 8 \\phi_{opt}(\\mathcal{x})\\right)(1+h_{k - g}).\\ ] ] finally , using the fact that @xmath112 , we have our result .",
    "the end result is a modest improvement over that of @xcite that scales with the level of supervision .",
    "the final inequality in the proof is tighter than the result stated in the theorem , since the factor of @xmath113 could be lower depending on the contributions of the supervised clusters in the optimal clustering .",
    "we use several measures for each experiment .",
    "first , we use the cost , as estimated by the potential function . for comparing to the theoretical bound",
    ", we also use the fraction of optimal cost , where `` optimal '' is derived by taking the centroids for each class as determined by the ground truth labels .",
    "next , we use the number of lloyds iterations until convergence .",
    "finally , we will use the adjusted rand index ( ari ) @xcite , which is an index that compares how closely two partitions agree .",
    "the ari is the rand index , the ratio of number of agreements between two partitions , after adjusting for chance .",
    "it is essentially chance at 0 , meaningless @xmath114 , and perfect at its maximal value , unity .",
    "since ground truth labels are available for our datasets , we can compare them to the partitions yielded from the output of the algorithms in section [ sec : alg ] .",
    "thus , a large ari value indicates good clustering performance as determined by fidelity to the ground truth partition .",
    "we showcase our algorithm on three datasets ( cf .",
    "figure [ fig : data ] for depiction ) .",
    "the first , gaussian mixture was inspired by both @xcite .",
    "we drew @xmath115 centers from the 15-dimensional hypercube with side length of @xmath116 .",
    "for each center @xmath102 , we drew @xmath117 points from a multivariate guassian with mean @xmath102 and identity covariance .",
    "this dataset is remarkable because it is easy to cluster by inspection ( at least with larger side - length , as in the original papers ) yet is difficult for lloyd s algorithm when initialized with bad centers . for our chosen side length ,",
    "it is not easy to cluster by eye .",
    "note that the supervision story ( where centroids of the class labels correspond to best centers ) is likely to hold for most realizations of the data .",
    "the next two datasets are real data for which the assumption that the labels match up with minimum cost clusters is not met .",
    "the second dataset is the venerable iris dataset @xcite , which uses @xmath118 variables to describe @xmath119 different classes of flowers . while this dataset is old , it is nonetheless difficult for k - means to handle from a clustering standpoint . this fact is widely known",
    "; indeed , even the wikipedia page for the iris dataset has a rendering of k - means failing on it.@xcite we compared the ari for this dataset and the gaussian mixture dataset while varying the ratio of side length of the hypercube to standard deviation ( @xmath120 with @xmath121 fixed ) , and we found that the datasets were roughly equivalent for side length around 3.25 .",
    "this is under one percent of the side length and @xmath122 times the volume of the norm25 dataset @xcite that our gaussian mixture dataset is based on .",
    "thus , we observe that the iris dataset is harder to cluster than the synthetic dataset .",
    "the third dataset , hyperspectral , is a naval space command hymap hyperspectral dataset representing an aerial photo a runway and background terrain of the airport at dalgren , virginia as originally seen in @xcite ( cf .",
    "figure [ fig : hyppic ] for a depiction of the location ) .",
    "each pixel in the photo is associated with @xmath123 features representing different spectral bands ( e.g. visible and infrared ) .",
    "we took the first six principal components to form a dataset with @xmath124 data in @xmath125 , as chosen by the minimum number of dimensions to capture @xmath126 of the total variance .",
    "the first two principal components are depicted in fig [ fig : data ] .",
    "the @xmath127 classes are the identities of each pixel ( i.e. runway , pine , oak , grass , water , scrub , and swamp ) . based on the ari scores presented in the forthcoming results section ,",
    "this dataset is only a little easier to cluster than iris .          for both datasets",
    ", we apply several algorithms : ss - k - means++ , constrained - kmeans , ss - k - means++ ( without lloyds ) , constrained - kmeans ( without lloyds ) , and constrained - kmeans algorithm initialized at the true class centroids .",
    "we consider the latter algorithm as an approximation to the optimal solution .",
    "constrained - kmeans and constrained - kmeans ( without lloyds ) use a random sample of the unsupervised data weighted uniformly for the remaining initial centers ( after using centroids of the labeled points ) .",
    "the algorithms without lloyds use their respective initialization strategy to choose initial centers then move straight to class assignment without updating the initial centers .",
    "we consider these algorithms as `` initialization only '' methods for this reason .",
    "we vary the supervision level from 0% to 100% , where we add supervised classes and sample 5/5/50 datapoints per class to label for guassian mixture , iris , and hyperspectral , respectively .",
    "note that this is percent of clusters which have exemplars and not percent of all points which are labels .",
    "also , at 100% supervision , ss - k - means++ and constrained - kmeans are the same , since there are no additional centers to choose",
    ". we did not allow the supervised data to change cluster assignment , so the approximation to the optimal can change with the level of supervision and with different supervised data chosen .",
    "we set @xmath0 equal to the true number of groups ( 24 for gaussian mixture , 3 for iris , and 7 for hyperspectral ) .",
    "we used 100 monte carlo replicates at each level of supervision .",
    "figure [ fig : cost ] shows the cost as the level of supervision changes .",
    "we observe the cost decreases with more supervision . also , we see the same relative performances of the algorithms , with the + + version outperforming the benchmark .",
    "observe that the approximation to the optimal solution is the best .",
    "figure [ fig : fcost ] depicts the theoretical bound .",
    "all algorithms are below the bound ( in expectation ) .",
    "figure [ fig : iter ] shows the number of iterations before lloyd s converges .",
    "we can see that improved selection of by @xmath33 weighted randomization leads to fewer iterations before convergence .",
    "we expected this ; arthur and vassilvitskii @xcite observed a similar phenomenon with no supervision .",
    "more supervision did not seem to affect the number of iterations until very high levels ( near 100% ) .",
    "for the real world datasets , we can see that the approximation to the optimal algorithm required more than one iteration to converge , indicating that the centroids of the true class labels do not match with the locally minimal cost solutions .",
    "this means that the conditions for the supervision in our proofs do not hold for this dataset .",
    "nevertheless , both cost and ari improve with additional supervision .",
    "figure [ fig : ari ] shows the ari for all algorithms .",
    "note that supervision improves the ari , as expected .",
    "also , ss - k - means++ generally outperforms constrained - kmeans .",
    "the same observation holds for the initialization only versions as well .",
    "remarkably , the true centroids and lloyd s algorithm is outperformed by the initialization only methods on the iris and hyperspectral datasets at 100% supervision for the ari metric .",
    "this is due to the fact that the true classes do not correspond to the minimum cost solution , which is what lloyd s iterations would improve ( apparently at the cost of ari ) .",
    "in this paper , we present a natural extension of k - means++ and constrained - kmeans .",
    "then , we prove the corresponding bound on the expectation of the cost under some conditions on the supervision .",
    "no assumptions are made about the distribution of the data .",
    "finally , we demonstrated that on three datasets judicious supervision and good starting center selection heuristics improve clustering performance , cost , and iteration count .",
    "possible future theoretical work includes incorporating the advances set forth in the extensions to the original k - means++ paper .",
    "for example , we could produce semi - supervised versions of k - means # @xcite and k - means|| @xcite with commensurately improved bounds .",
    "relaxing the constraints to the pairwise cannot - link and must - link constraints as in @xcite is also desirable , because the assumption of exogenously provided hard labels is often untenable .",
    "other assumptions that would be nice to relax would be the equal cluster shapes and cluster volume implicit in k - means clustering .",
    "the authors would like to thank theodore d. drivas for helping to test the codes used in the experiments and for consultation on aesthetics .",
    "this work is partially funded by the national security science and engineering faculty fellowship ( nsseff ) , the johns hopkins university human language technology center of excellence ( jhu hlt coe ) , and the xdata program of the defense advanced research projects agency ( darpa ) administered through air force research laboratory contract fa8750 - 12 - 2 - 0303 .",
    "macqueen  j , et  al . some methods for classification and analysis of multivariate observations . in : proceedings of the fifth berkeley symposium on mathematical statistics and probability ; vol .  1 .",
    "oakland , ca , usa . ; 1967 .",
    "p. 281297 .",
    "arthur  d , vassilvitskii  s. k - means++ : the advantages of careful seeding . in : proceedings of the eighteenth annual acm - siam symposium on discrete algorithms .",
    "society for industrial and applied mathematics ; 2007 .",
    "p. 10271035 .",
    "wagstaff  k , cardie  c , rogers  s , schrdl  s , et  al . constrained",
    "k - means clustering with background knowledge . in : proceedings of the eighteenth international conference on machine learning ; vol .  1 ; 2001 .",
    "p. 577584 .",
    "basu  s , bilenko  m , mooney  rj .",
    "a probabilistic framework for semi - supervised clustering . in : proceedings of the tenth acm sigkdd international conference on knowledge discovery and data mining .",
    "acm ; 2004 ."
  ],
  "abstract_text": [
    "<S> traditionally , practitioners initialize the k - means algorithm with centers chosen uniformly at random . randomized initialization with uneven weights ( k - means++ ) has recently been used to improve the performance over this strategy in cost and run - time . </S>",
    "<S> we consider the k - means problem with semi - supervised information , where some of the data are pre - labeled , and we seek to label the rest according to the minimum cost solution . by extending the k - means++ algorithm and analysis to account for the labels , we derive an improved theoretical bound on expected cost and observe improved performance in simulated and real data examples . </S>",
    "<S> this analysis provides theoretical justification for a roughly linear semi - supervised clustering algorithm .    * kmeans , clustering , semi - supervised , partially labeled , approximation algorithm *    68w20 , 68w25 , 68u05 , 62h30 </S>"
  ]
}