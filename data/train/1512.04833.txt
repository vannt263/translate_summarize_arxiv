{
  "article_text": [
    "compressed sensing ( cs ) is a signal processing technique that aims to reconstruct a sparse signal with a higher - dimension @xmath0 space from an underdetermined lower - dimension @xmath1 measurement space . in the literature ,",
    "@xmath2-norm minimization is the most widely used scheme in signal reconstruction because it is convex and hence can be solved efficiently @xcite . despite its efficient solution , however , @xmath2-reconstruction is far from being optimal @xcite .",
    "if the probabilistic properties of the signal are known , then the probabilistic bayesian inference offers the optimal reconstruction in the minimum mean - square - error ( mse ) sense ; however , the optimal bayes estimation is not computationally tractable . by using belief propagation ,",
    "an efficient and less complex alternative , referred to as approximate message passing ( amp ) @xcite , has recently emerged .",
    "the implementation of amp still requires many matrix multiplications up to an order of @xmath3 . considering a special sensing matrix that allows",
    "a fast multiplication procedure is therefore of great interest .",
    "the partial discrete fourier transform ( dft ) sensing matrix , i.e. , a randomly selected dft matrix , is one such example @xcite . using dft as the sensing matrix , fast fourier transform can be used to perform matrix multiplications down to the order of @xmath4 , and most importantly , the sensing matrix is not required to be stored .",
    "the entries of a dft matrix are , however , not i.i.d .. in contrast to the case with matrices with independent entries , amp does not perform well for sensing matrices with a row - orthogonal ensemble . recently , ma _",
    "_ in @xcite developed a signal recovery algorithm for the partial dft sensing matrix by exploiting a turbo principle in iterative decoding .",
    "in contrast to other developments in this area , e.g. , @xcite , the turbo signal recovery ( turbo - sr ) algorithm @xcite has an excellent convergence property , and most importantly , the state evolution of this algorithm perfectly agrees with that predicted by the replica method .",
    "the latter characteristic indicates the optimality of the turbo - sr algorithm over the partial dft sensing matrix .",
    "the turbo - sr algorithm is developed under _ linear _ measurements .",
    "however , in cs problems , quantization is often a necessary step in the acquisition of measurements",
    ". especially , signal recovery problems from low - resolution ( or even 1-bit ) measurements are of particular interest in recent years @xcite .",
    "this study presents a novel algorithm , the _",
    "generalized _ turbo signal recovery ( gturbo - sr ) algorithm , to recover signal from a row - orthogonal sensing matrix followed by a quantized measurement channel .",
    "this algorithm extends the earlier turbo - sr algorithm to deal with arbitrary distribution on the output of the measurements .",
    "the state evolution ( se ) of the gturbo - sr is derived and is shown to be consistent with that obtained with the replica method .    _ notations_for a complex - valued variable @xmath5",
    ", we use @xmath6 and @xmath7 to denote the real and imaginary parts of @xmath5 , respectively . a random variable @xmath5 drawn from the proper complex gaussian distribution of mean @xmath8 and variance @xmath9",
    "is described by @xmath10 .",
    "we use @xmath11 to denote the real gaussian integration measure @xmath12 , and we use @xmath13 to denote the complex gaussian integration measure . finally , @xmath14 denotes the cumulative gaussian distribution function , and we have @xmath15 .",
    "we consider the noisy cs problem @xmath16 where @xmath17 is a measurement vector , @xmath18 denotes a known sensing matrix , @xmath19 is a signal vector , and @xmath20 is the additive white gaussian noise vector with zero mean and element - wise variance @xmath21 .",
    "we denote by @xmath22 the measurement ratio ( i.e. , number of measurements per variable ) . the sensing matrix @xmath23",
    "is obtained by random selection of a set of rows from an unitary matrix or the standard dft matrix @xmath24 \\in { { \\mathbb c}}^{n \\times n}$ ] .",
    "we refer to such @xmath23 as the row - orthogonal matrix .",
    "for ease of `` expression '' and convenience , we work with the enlarged orthogonal matrix with rows and columns by setting them to zero rather than removing them . therefore , we enlarge @xmath25 and @xmath26 to be @xmath27-dimensional vectors by zero padding and denote them by @xmath28 and @xmath29 .",
    "we let matrix @xmath30 be a diagonal projection matrix , in which its off - diagonal entries are all zeros , and its diagonal entries are zeros or ones .",
    "let @xmath31 be an @xmath27-dimensional vector .",
    "then , the input output relationship of ( [ eq : sysmodel ] ) can be equivalently expressed as @xmath32 note that all the following descriptions are based on the enlarged system ( [ eq : sysmodel_enlarge ] ) . by abuse of notation",
    ", we continue to write @xmath25 and @xmath33 for @xmath34 and @xmath35 , respectively .    in this study , we are interested in the measurements acquired through quantizers .",
    "let @xmath36 be a complex - valued quantizer @xmath36 , which is defined as @xmath37 , i.e. , the real and imaginary parts are quantized separately .",
    "the quantization is applied element - wise on each measurement , and the resulting quantized signal @xmath38 is given by @xmath39 the output is assigned the value @xmath40 when the quantizer input falls in the interval @xmath41 $ ] . should be specified as @xmath42 or @xmath43 .",
    "we abuse @xmath40 to denote each real channel . ] for example , for a typical uniform @xmath44-bit quantizer with quantization step size @xmath45 , the quantized output is given by @xmath46 and the associated lower and upper thresholds are given by    [ quanbound ] @xmath47    our aim is to estimate @xmath48 and @xmath49 from @xmath38 given @xmath23 ( or , more precisely , @xmath50 ) .",
    "we consider the bayesian optimal inference because this methodology can achieve the best estimates in terms of mse . toward this end",
    ", we introduce the distributions of signals @xmath48 and measurements @xmath38 .",
    "we suppose that each entry of @xmath48 is generated from a distribution @xmath51 independently , @xmath52 on the other hand , the distribution of the quantized measurements under ( [ eq : qsys ] ) , conditional on @xmath49 , is given by @xmath53 where @xmath54 with @xmath55",
    "the gturbo - sr is presented in algorithm [ ago : gturbo - sr ] , which is a generalization of the turbo - sr algorithm in @xcite .",
    "figure [ fig : gturbo - sr ] illustrates the block diagram of the algorithm .",
    "the idea of the algorithm use the turbo principle in iterative decoding to compute extrinsic messages of @xmath48 and @xmath49 in modules a and b , on the basis of _ linear _ transform @xmath56 . specifically , in lines 3 and 4 , @xmath57 and @xmath58 are estimates of @xmath59 and its corresponding variance , respectively .",
    "subsequently , by excluding the prior knowledge @xmath60 from the posterior moments @xmath61 , lines 57 compute the extrinsic mean and variance of @xmath48 . similar to those in lines 3 and 4 , @xmath62 and @xmath63 in lines 10 and 11 are estimates of @xmath56 and its corresponding variance , respectively .",
    "the extrinsic mean and variance of @xmath49 are then evaluated in lines 1214 .    in the gturbo - sr algorithm ,",
    "the estimate of the posterior mean and the variance of @xmath49 in lines 1 and 2 are nonlinear , and they consider the prior mean and variance of @xmath64 from @xmath65 in line 12 and @xmath66 in line 13 . assuming the prior @xmath67 and combining @xmath68 in ( [ eq : lnkelihood_each ] ) , we take the expectation and variance in lines 1 and 2 with respect to ( w.r.t . ) the posterior probability @xmath69 similarly , the posterior mean and variance of @xmath48 in lines 8 and 9 are taken w.r.t .",
    "the posterior probability @xmath70 the algorithm produces a sequence of estimates @xmath71 and @xmath72 for the unknown vectors @xmath48 and @xmath49 , respectively .",
    "[ ago : gturbo - sr ]    explicit expressions of the posterior mean and variance of @xmath73 are provided in @xcite and are shown in ( [ eq : hatz_realgaussian ] ) and ( [ eq : msez_realgaussian ] ) at the top of the next page .",
    "@xmath74 where @xmath75    we have abused @xmath40 and @xmath76 in ( [ eq : hatz_realgaussian ] ) and ( [ eq : msez_realgaussian ] ) to denote @xmath42 and @xmath77 , respectively .",
    "the estimator for the imaginary part @xmath78 can be obtained analogously as ( [ eq : hatz_realgaussian ] ) and ( [ eq : msez_realgaussian ] ) , whereas @xmath40 should be replaced by @xmath43 .",
    "on the other hand , suppose that the elements of @xmath48 are drawn i.i.d .",
    "from the bernoulli - gaussian ( bg ) distribution , i.e. , @xmath79 where @xmath80 denotes the dirac delta , @xmath81 the sparsity rate , and @xmath82 the active - coefficient variance .",
    "then , the explicit expressions of the posterior mean and variance of @xmath48 in lines 8 and 9 are given by @xcite .",
    "please refer also to @xcite for other distributions of @xmath83 such as the qam constellations .",
    "the behavior of the gturbo - sr algorithm can be described by a set of se equations .",
    "we then derive the se equations in the large - system regime where @xmath84 and @xmath27 reach infinity , whereas the ratio @xmath85 remains fixed .",
    "similar to the notation used in @xcite , we define @xmath86 let us consider a scalar awgn channel @xmath87 where @xmath88 .",
    "the posterior mean estimator of @xmath89 from ( [ eq : scalarsysmodel ] ) is given by @xmath90 where @xmath91 and @xmath92 .",
    "then , we define @xmath93 of this estimator as @xmath94 where the expectation is taken over the joint distribution @xmath95 .    combining the above definitions into lines 9 , 11 , and 13 of algorithm [ ago : gturbo - sr ]",
    ", we can easily characterize the se of @xmath96 as @xmath97 where the superscript @xmath98 represents the iteration indices .",
    "next , with @xmath99 , we aim at evaluating @xmath58 in line 4 of algorithm [ ago : gturbo - sr ] . toward this end",
    ", we have to obtain the large - system behavior of @xmath58 . in the large - system limit @xmath58 for a given @xmath38",
    "has small deviations from the expectation of @xmath100 w.r.t .",
    "@xmath38 , which is called the self - averaging property in the large - system limit . to compute this expectation",
    ", we need the joint distribution @xmath101 .",
    "our strategy to obtain @xmath101 is via the marginal distribution @xmath102 .",
    "first we consider the joint distribution @xmath103 .",
    "we notice that both @xmath104 and @xmath49 are sums over many independent terms .",
    "therefore , according to the clt , they can be approximated as gaussian random variables .",
    "their means are zero because @xmath105 are zero means .",
    "the covariance matrix between @xmath73 and @xmath106 can be shown to be @xmath107,\\ ] ] where @xmath108 is the variance of @xmath89 .",
    "we find that the covariance matrix becomes asymptotically independent of index @xmath109 .",
    "therefore , we omit index @xmath109 from the covariance matrix between @xmath73 and @xmath106 . altogether , these provide the following bivariate gaussian distribution : @xmath110    next , we have @xmath111 , and , thus , we obtain @xmath112 note that for ease of explanation , we consider the real parts of @xmath40 and @xmath113 in ( [ eq : ycz ] ) . therefore , the power of the corresponding parameters in the subsequent expressions should be half per real and imaginary part . combining ( [ eq : bigau ] ) and ( [ eq : ycz ] ) and using the definition in ( [ eq : def_psi_b ] )",
    ", we obtain @xmath114    using ( [ eq : p_yz ] ) , we can evaluate the expectation of @xmath115 in ( [ eq : msez_realgaussian ] ) w.r.t .",
    "@xmath116 . to perform this expectation ,",
    "we write @xmath115 in ( [ eq : msez_realgaussian ] ) for @xmath117 as @xmath118 where the definition of @xmath119 is given by ( [ eq : def_psi_b ] ) , and @xmath120 as a result , the expectation of @xmath115 reads @xmath121 ^ 2 }    { \\psi{\\big({\\tilde{y}};\\sqrt{\\frac{v_{x}}{2 } - \\frac{v_{{{\\sf a}}}^{\\rm pri}}{2}}z,\\frac{\\sigma^2}{2}+\\frac{v_{{{\\sf a}}}^{\\rm pri}}{2 } \\big ) } } \\right).\\end{gathered}\\ ] ] note that we have included the contributions of the real and imaginary parts of @xmath115 into ( [ eq : va_post ] ) . substituting ( [ eq : va_post ] ) into line 6 of algorithm [",
    "ago : gturbo - sr ] , we have closed the entire loop and present the following proposition .",
    "the se of the gturbo - cs algorithm is characterized by @xmath122 ^ 2 }       { \\psi{\\big({\\tilde{y}};\\sqrt{\\frac{v_{x}}{2 } - \\frac{v^{t}}{2 } } z,\\frac{\\sigma^2}{2}+\\frac{v^{t}}{2 } \\big ) } } ,   \\label{eq : sea } \\\\",
    "\\eta^{t+1 } & = \\frac{1 } { ( \\alpha \\vartheta^{t})^{-1 } - v^{t } } , \\label{eq : seb }   \\\\",
    "v^{t+1 } & = \\left ( \\frac{1}{{{\\sf mmse}}(\\eta^{t+1 } ) } - \\eta^{t+1 } \\right)^{-1 } , \\label{eq : sec}\\end{aligned}\\ ] ] with initialization @xmath123 .",
    "interestingly , as @xmath124 , the se converges to the se equations derived from the replica method @xcite .",
    "computer simulations are conducted to verify the accuracy of our analytical results . in particular , we compare the se of @xmath125 in ( [ eq : sec ] ) with those obtained by the simulations . in each simulation , the sensing matrix is generated from a randomly scrambled @xmath126 dft matrix , and we perform algorithm [ ago : gturbo - sr ] and compute the mse @xmath127 the simulation results are obtained by averaging of over 2,000 realizations . the snr is defined as @xmath128 .    for comparison , the simulation scenarios completely follow those presented in @xcite .",
    "the system parameters are set as follows : @xmath129 , @xmath130 , @xmath131 , and @xmath132db . the signal distribution @xmath51 follows bg distribution with @xmath133 and @xmath134 . in this case , @xmath135 in ( [ eq : defmmse ] ) can be obtained explicitly @xcite @xmath136 we use the typical uniform quantizer with quantization step size @xmath137 .",
    "figure [ fig : mse_gau_n8192 ] shows the corresponding mse results under different quantization levels .",
    "the markers denote the simulation results , whereas the curves characterize the analytical behavior .",
    "the figure clearly demonstrates that the se analysis precisely predicts the per iteration performance .",
    "other simulations , which are not reported here , show that the gturbo - sr algorithm as well as the se analysis , can also apply to other priors and other nonlinear measurements . for arbitrary distributions on the measurements @xmath138 , @xmath119 in our study",
    "should be replaced with @xmath139",
    "we presented a novel algorithm called gturbo - sr algorithm to estimate a signal vector observed through a row - orthogonal sensing matrix followed by quantized measurements .",
    "the se analysis was provided to precisely describe the asymptotic behavior of the gturbo - sr algorithm .",
    "the gturbo - sr algorithm can be applied to a large class of nonlinear measurements with a precisely predicable asymptotic se .",
    "other related works with random orthogonal ensembles can be found in @xcite ."
  ],
  "abstract_text": [
    "<S> in this study , we propose a generalized turbo signal recovery algorithm to estimate a signal from quantized measurements , in which the sensing matrix is a row - orthogonal matrix , such as the partial discrete fourier transform matrix . </S>",
    "<S> the state evolution of the proposed algorithm is derived and is shown to be consistent with that obtained with the replica method . </S>",
    "<S> numerical experiments illustrate the excellent agreement of the proposed algorithm with theoretical state evolution .    </S>",
    "<S> compressed sensing , quantization , partial dft matrix , state evolution , replica method . </S>"
  ]
}