{
  "article_text": [
    "# 1 # 1#1 # 1 # 1 # 1#2 an important problem , for both humans and machines , is to extract relevant information from complex data .",
    "to do so , one must be able to define which aspects of data are relevant and which should be discarded .",
    "the ` information bottleneck ' ( ib ) approach , developed by tishby and colleagues [ 1 ] , provides a principled way to approach this problem .",
    "the idea behind the ib approach is to use additional ` variables of interest ' to determine which aspects of a signal are relevant .",
    "for example , for speech signals , variables of interest could be the words being pronounced , or alternatively , the speaker identity .",
    "one then seeks a coding scheme that retains maximal information about these variables of interest , constrained on the information encoded about the input .",
    "the ib approach has been used to tackle a wide variety of problems , including filtering , prediction and learning [ 2 - 5 ] .",
    "however , it quickly becomes intractable with high - dimensional and/or non - gaussian data .",
    "consequently , previous research has primarily focussed on tractable cases , where the data comprises a countably small number of discrete states [ 1 - 5 ] , or is gaussian [ 6 ] .    here , we extend the ib algorithm of tishby et al .",
    "[ 1 ] using a variational approximation . the algorithm maximizes a lower bound on the ib objective function , and is closely related to variational em . using this approach , we derive an ib algorithm that can be effectively applied to ` sparse ' data in which input and relevance variables are generated by sparsely occurring latent features .",
    "the resulting solutions share many properties with previous sparse coding models , used to model early sensory processing [ 7 ] .",
    "however , unlike these sparse coding models , the learned representation depends on : ( i ) the relation between the input and variable of interest ; ( ii ) the trade - off between encoding quality and compression .",
    "finally , we present a kernelized version of the algorithm , that can be applied to a large range of problems with non - linear relation between the input data and variables of interest .",
    "let us define an input variable @xmath1 , as well as a ` relevance variable ' , @xmath0 , with joint distribution @xmath2 .",
    "the goal of the ib approach is to compress the variable @xmath1 through another variable @xmath3 , while conserving information about @xmath0 .",
    "mathematically , we seek an encoding model , @xmath4 , that maximizes : @xmath5 where @xmath6 is a lagrange multiplier that determines the strength of the bottleneck .",
    "tishby and colleagues showed that the ib loss function can be optimized by applying iterative updates : @xmath7 $ ] , @xmath8 and @xmath9 [ 1 ] . unfortunately however , when @xmath10 is high - dimensional and/or non - gaussian these updates become intractable , and approximations are required .    due to the positivity of the kl divergence ,",
    "we can write , @xmath11 for any approximative distribution @xmath12 . this allows us to formulate a variational lower bound for the ib objective function : @xmath13 where @xmath14 and @xmath15 are variational distributions , and we have replaced the expectation over @xmath10 with the empirical expectation over training data .",
    "( note that , for notational simplicity we have also omitted the constant term , @xmath16 . )    setting @xmath17 and @xmath18 fully tightens the bound ( so that @xmath19 ) , and leads to the iterative algorithm of tishby et al .",
    "however , when these exact updates are not possible , one can instead choose a restricted class of distributions @xmath20 and @xmath21 for which inference is tractable .",
    "thus , to maximize @xmath22 with respect to parameters @xmath23 of the encoding distribution @xmath24 , we repeat the following steps until convergence :    * for fixed @xmath23 , find @xmath25 * for fixed @xmath26 and @xmath15 , find @xmath27    we note that using a simple approximation for the decoding distribution , @xmath28 , can carry additional benefits , besides rendering the ib algorithm tractable . specifically , while an advantage of mutual information is its generality , in certain cases this can also be a drawback .",
    "that is , because shannon information does not make any assumptions about the code , it is not always apparent how information should be best extracted from the responses : just because information is ` there ' does not mean we know how to get at it .",
    "in contrast , using a simple approximation for the decoding distribution , @xmath28 ( e.g.  linear gaussian ) , constrains the ib algorithm to find solutions where information about @xmath0 can be easily extracted from the responses ( e.g.  via linear regression ) .",
    "in previous work on gaussian ib [ 6 ] , responses were equal to a linear projection of the input , plus noise : @xmath29 , where @xmath30 is an @xmath31 matrix of encoding weights , and @xmath32 , where @xmath33 is an @xmath34 covariance matrix .",
    "when the joint distribution , @xmath10 , is gaussian , it follows that the marginal and decoding distributions , @xmath35 and @xmath36 , are also gaussian , and the parameters of the encoding distribution , @xmath30 and @xmath33 , can be found analytically .    to illustrate the capabilities of the variational algorithm , while permitting comparison to gaussian ib , we begin by adding a single degree of complexity . in common with gaussian ib ,",
    "we consider a linear gaussian encoder , @xmath37 , and decoder , @xmath38 .",
    "however , unlike gaussian ib , we use a student - t distribution to approximate the response marginal : @xmath39 , with scale and shape parameters , @xmath40 and @xmath41 , respectively .",
    "when the shape parameter , @xmath41 , is small then the student - t distribution is heavy - tailed , or ` sparse ' , compared to a gaussian distribution .",
    "thus , we call the resulting algorithm ` sparse ib ' . unlike gaussian ib , the introduction of a student - t marginal means the ib algorithm can not be solved analytically , and one requires approximations .",
    "recall that the ib objective function consists of two terms : @xmath42 , and @xmath43 .",
    "we begin by describing how to optimize the lower and upper bound of each of these two terms with respect to the variational distributions @xmath28 and @xmath44 , respectively .",
    "the first term of the ib objective function is bounded from below by : @xmath45 maximizing the lower bound on @xmath42 with respect to the decoding parameters , @xmath46 and @xmath47 , gives : @xmath48 where @xmath49 , @xmath50 , and @xmath51 .",
    "unfortunately , it is not straightforward to express the bound on @xmath43 in closed form",
    ". instead , we use an additional variational approximation , utilising the fact that the student - t distribution can be expressed as an infinite mixture of gaussians : @xmath52 [ 8 ] . following a standard em procedure [ 9 ]",
    ", one can thus write a tractable lower bound on the log - likelihood , @xmath53 $ ] , which corresponds to an upper - bound on the bottleneck term : @xmath54 -\\frac{1}{2}\\log|\\sigma|+\\mathrm{const}.\\nonumber \\end{aligned}\\ ] ] where @xmath55 , and @xmath56 denote variational parameters for the @xmath57 unit and @xmath58 data instance .",
    "we used the shorthand notation , @xmath59 , where @xmath60 is the @xmath57 diagonal element of @xmath33 and @xmath61 is the @xmath57 row of @xmath30 . for notational simplicity , terms that do not depend on the encoding parameters",
    "were pushed into the function , @xmath62 $ ] , where @xmath63 is the entropy of a gamma distribution with shape and rate parameters : @xmath64 , and @xmath65 , respectively [ 9 ] . ] .    minimizing the upper bound on @xmath43 with respect to @xmath40 , @xmath55 and @xmath56 ( for fixed @xmath66 )",
    "gives : @xmath67 the shape parameter , @xmath41 , is then found numerically on each iteration ( for fixed @xmath55 and @xmath64 ) , by solving : @xmath68,\\ ] ] where @xmath69 is the digamma function [ 9 ] .",
    "next we maximize the full variational objective function @xmath22 with respect to the encoding distribution , @xmath4 ( for fixed @xmath28 and @xmath44 ) .",
    "maximizing @xmath22 with respect to the encoding noise covariance , @xmath33 , gives : @xmath70 where @xmath71 and @xmath72 are @xmath73 diagonal covariance matrices with diagonal elements @xmath74 , and @xmath75 , respectively .",
    "finally , taking the derivative of @xmath22 with respect to the encoding weights , @xmath30 , gives : @xmath76 setting the derivative to zero , we can solve for @xmath30 directly .",
    "one may verify that , when variational parameters , @xmath55 , are unity , the above iterative updates are identical to the iterative gaussian ib algorithm described in [ 6 ] .      .",
    "the goal of the ib algorithm is to learn a linear code that maximized information about the original patches , @xmath0 , constrained on information encoded about the input , @xmath1 .",
    "( b ) a selection of linear encoding ( left ) , and decoding ( right ) filters obtained with the gaussian ib algorithm .",
    "( c ) same as b , but for the sparse ib algorithm .",
    "( d ) response histograms for the 10 units with highest variance , for the gaussian ( red ) and sparse ( blue ) ib algorithms . ( e )",
    "information curves for the gaussian ( red ) and sparse ( blue ) algorithms , alongside a ` null ' model , where responses were equal to the original input , plus white noise .",
    "( f ) fraction of response variance attributed to signal fluctuations , for each unit .",
    "solid and dashed curves correspond to strong and weak bottlenecks , respectively ( corresponding to the vertical dashed lines in panel e ) . ]    in our framework , the approximation of the response marginal , @xmath15 , plays an analogous role to the prior distribution in a probabilistic generative model .",
    "thus , we hypothesized that a sparse approximation for the response marginal , @xmath44 , would permit the ib algorithm to recover sparsely occurring input features , analogous to the effect of using a sparse prior .    to show this , we constructed artificial @xmath77 image patches from combinations of orientated bar features .",
    "each bar had a gaussian cross - section , with maximum amplitude drawn from a standard normal distribution of width 1.2 pixels .",
    "patches were constructed by linearly combining 3 bars , with uniformly random orientation and position .",
    "initially , we considered a simple de - noising task , where the input , @xmath1 , was a noisy version of the original image patches ( gaussian noise , with variance @xmath78 ; figure 1a ) .",
    "training data consisted of 10,000 patches .",
    "figure 1b and 1c show a selection of encoding ( @xmath30 ) and decoding ( @xmath46 ) filters obtained with the gaussian and sparse ib models , respectively .",
    "as predicted , only the sparse ib model was able to recover the original bar features .",
    "in addition , response histograms were considerably more heavy - tailed for the sparse ib model ( fig .",
    "1d ) .    the relevant information , @xmath79 , encoded by the sparse model was greater than for the gaussian model , over a range of bottleneck strengths ( fig .",
    "1e ) . while the difference may appear small , it is consistent with work showing that sparse coding models achieve only a small improvement in log - likelihood for natural image patches [ 10 ]",
    "we also plotted the information curve for a ` null model ' , with responses sampled from @xmath80 .",
    "interestingly , the performance of this null model was almost identical to the gaussian ib model .",
    "figure 1f plots the fraction of response variance due to the signal , for each unit ( @xmath81 ) .",
    "solid and dashed curves denote strong and weak bottlenecks , respectively . in both cases ,",
    "the gaussian model gave a smooth spectrum of response magnitudes , while the sparse model was more ` all - or - nothing ' .",
    "one way the sparse ib algorithm differs qualitatively from traditional sparse coding algorithms , is that the learned representation depends on the relation between @xmath1 and @xmath0 , rather than just the input statistics . to illustrate this , we conducted simulations with patches corrupted by spatially correlated noise , aligned along the vertical direction ( fig .",
    "the spatial covariance of the noise was described by a gaussian envelope , with standard deviation @xmath82 pixels in the vertical direction and @xmath83 pixel in horizontal direction .     and patch , @xmath0 .",
    "spatial noise correlations were aligned along the vertical direction .",
    "( b ) subset of decoding filters obtained with the sparse ib algorithm .",
    "( c ) distribution of encoded orientations .",
    "( d ) example stimulus ( left ) and reconstruction ( right ) of bars presented at variable orientations ( presented with zero input noise , so that @xmath84 for this example ) . ]",
    "figure 2b shows a selection of decoding filters obtained from the sparse ib model , with correlated input noise .",
    "the shape of individual filters was qualitatively similar to those obtained with uncorrelated noise ( fig .",
    "however , with this stimulus , the ib model avoided ` wasting ' bits by representing features co - orientated with the noise ( fig .  2c ) .",
    "consequently , it was not possible to reconstruct vertical bars from the responses , when bars were presented alone , even with zero noise ( fig .",
    ", was restricted to 2 columns to either side of the patch .",
    "the target variable , @xmath0 , was the central region .",
    "( b ) subset of decoding filters , @xmath46 , for the sparse kernel ib ( ` sparse kib ' ) algorithm .",
    "( c ) as for b , for other versions of the ib algorithm .",
    "( d ) information curves for the gaussian kib ( blue ) sparse kib ( green ) and sparse ib algorithms ( red ) .",
    "the bottleneck strength for the other panels in this figure is indicated by a vertical dashed line .",
    "( e ) response histogram for the 10 units with highest variance , for the gaussian and sparse kib models .",
    "( f ) ( above ) three test stimuli , used to demonstrate the non - linear properties of the sparse kib code .",
    "( below ) reconstruction obtained from responses to test stimulus .",
    "( g ) responses of two units which showed strong responses to stimulus 3 .",
    "the decoding filters for these units are shown above the bar plots . ]",
    "one way to improve the ib algorithm is to consider non - linear encoders .",
    "a general choice is : @xmath85 , where @xmath86 is an embedding to a high - dimensional non - linear feature space .",
    "the variational objective functions for both gaussian and sparse ib algorithms are quadratic in the responses , and thus can be expressed in terms of dot products of the row vector , @xmath86 .",
    "consequently , every solution for @xmath61 can be expressed as an expansion of mapped training data , @xmath87 [ 11 ] .",
    "it follows that the variational ib algorithm can be expressed in`dual space ' , with responses to the @xmath58 input drawn from @xmath88 , where @xmath89 is an @xmath90 matrix of expansion coefficients , and @xmath91 is the @xmath58 column of the @xmath92 kernel - gram matrix , @xmath93 , with elements @xmath94 . in this formulation",
    ", the problem of finding the linear encoding weights , @xmath30 , is replaced by finding the expansion coefficients , @xmath89 .",
    "the advantage of expressing the algorithm in the dual space is that we never have to deal with @xmath86 directly , so are free to consider high- ( or even infinite ) dimensional feature spaces .",
    "however , without additional constraints on the expansion coefficients , @xmath89 , the ib algorithm becomes degenerate ( i.e.  the solutions are independent of the input , @xmath1 ) . a standard way to deal with this is to add an l2 regularization term that favours solutions with small expansion coefficients . here ,",
    "this is achieved here by replacing @xmath95 with @xmath96 , where @xmath97 is a fixed regularization parameter .",
    "doing so , the derivative of @xmath22 with respect to @xmath89 becomes : @xmath98 setting the derivative to zero and solving for @xmath89 directly requires inverting an @xmath99 matrix , which is expensive .",
    "instead , one can use an iterative solver ( we used the conjugate gradients squared method ) .",
    "in addition , the computational complexity can be reduced by restricting the solution to lie on a subspace of training instances , such that , @xmath100 , where @xmath101",
    ". the derivation does not change , only now @xmath93 has dimensions @xmath102 [ 11 ] .",
    "when @xmath44 is gaussian ( equivalent to setting @xmath103 ) , solving for @xmath89 gives : @xmath104 where @xmath105 are the coefficients obtained from kernel ridge - regression ( krr ) .",
    "this suggests the following two stage algorithm : first , we learn the regularisation constant , @xmath97 , and parameters of the kernel matrix , @xmath93 , to maximize krr performance on hold - out data ; next , we perform variational ib , with fixed @xmath93 and @xmath97 .          to illustrate the capabilities of the kernel ib algorithm , we considered an ` occlusion ' task , with the outer columns of each patch presented as input , @xmath1 ( 2 columns to the far left and right ) , and the inner columns as the relevance variable @xmath0 , to be reconstructed .",
    "image patches were as before .",
    "note that performing the occlusion task optimally requires detecting combinations of features presented to either side of the occluded region , and is thus inherently nonlinear .",
    "we used gaussian kernels , with scale parameter , @xmath106 , and regularisation constant , @xmath97 , chosen to maximize krr performance on test data . both test and training data consisted of 10,000 images .",
    "however , @xmath89 was restricted to lie on a subset of 1000 randomly chosen training patches ( see earlier ) .",
    "figure 3b shows a selection of decoding filters ( @xmath46 ) learned by the sparse kernel ib algorithm ( ` sparse kib ' ) . a large fraction of filters resembled near - horizontal bars , traversing the occluded region .",
    "this was not the case for the sparse linear ib algorithm , which recovered localized blobs either side of the occluded region , nor the gaussian linear or kernelized models , which recovered non - local features ( fig .",
    "figure 3d shows a small but significant improvement in performance for the sparse kib versus the gaussian kib model",
    ". most noticeable , however , is the distribution of responses , which are much more heavy tailed for the sparse kib algorithm ( fig .",
    "3e ) .    to demonstrate the non - linear behaviour of the sparse kib model , we presented bar segments : first to either side of the occluded patch , then to both sides simultaneously . when bar segments were presented to both sides simultaneously , the sparse kib model ` filled in ' the missing bar segment , in contrast to the reconstruction obtained with single bar segments ( fig .",
    "this behaviour was reflected in the non - linear responses of certain encoding units , which were large when two segments were presented together , but near zero when one segment was presented alone ( fig .",
    "3 g ) .    finally , we repeated the occlusion task with handwritten digits , taken from the usps dataset ( www.gaussianprocess.org/gpml/data ) .",
    "we used 4649 training and 4649 test patches , of 16@xmath10716 pixels .",
    "however , expansion coeffecients were restricted to a lie on subset of 500 randomly patches .",
    "we set @xmath1 and @xmath0 , to be the left and right side of each patch , respectively ( fig .",
    "4a ) .    in common with the artificial data ,",
    "the response distributions achieved with the sparse kib algorithm were more heavy - tailed than for the gaussian kib algorithm ( fig .",
    "likewise , recovered decoding filters closely resembled handwritten digits , and extended far into the occluded region ( fig .",
    "this was not the case for the alternative ib algorithms ( fig .",
    "previous work has shown close parallels between the ib framework and maximum - likelihood estimation in a latent variable model [ 12 , 13 ] . for the sparse ib algorithm presented here ,",
    "maximizing the ib objective function is closely related to maximizing the likelihood of a ` sparse coding ' latent variable model , with student - t prior and linear gaussian likelihood function .",
    "however , unlike traditional sparse coding models , the encoding ( or ` recognition ' ) model @xmath108 is conditioned on a seperate set of inputs , @xmath1 , distinct from the image patches themselves .",
    "thus , the solutions depend on the relation between @xmath1 and @xmath0 , not just the image statistics ( e.g.  see fig .  2 ) .",
    "second , an additional parameter , @xmath109 , not present in sparse coding models , controls the trade - off between encoding and compression .",
    "finally , in contrast to traditional sparse coding algorithms , ib gives an unambiguous ordering of features , which can be arranged according to the response variance of each unit ( fig .",
    "our work is also closely related to the i m algorithm , proposed by barber et al .  to solve the information maximization ( ` infomax ' ) problem [ 14 ] .",
    "however , a general issue with infomax problems is that they are usually ill - posed , necessitating additional _ ad hoc _ constraints on the encoding weights or responses [ 15 ] .",
    "in contrast , in the ib approach , such constraints emerge automatically from the bottleneck term .",
    "a related method to find low - dimensional projections of @xmath1/@xmath0 pairs is canonical correlation analysis ( ` cca ' ) , and its kernel analogue [ 16 ] .",
    "in fact , the features obtained with gaussian ib are identical to those obtained with cca [ 6 ] .",
    "however , unlike cca , the number and ` scale ' of the features are not specified in advance , but determined by the bottleneck parameter , @xmath109 .",
    "secondly , kernel cca is symmetric in @xmath1 and @xmath0 , and thus performs nonlinear embedding of both @xmath1 _ and _ @xmath0 .",
    "in contrast , the ib problem is assymetric : we are interested in recovering @xmath0 from an input @xmath1 .",
    "thus , only @xmath1 is kernelized , while the decoder remains linear . finally , the features obtained from gaussian ib ( and thus , cca ) differ qualitatively from the sparse ib algorithm , which recovers sparse features that account jointly for @xmath1 and @xmath0 .",
    "sparse ib can be extended to the nonlinear regime using a kernel expansion . for the gaussian model ,",
    "the expansion coefficients , @xmath89 , are a linear projection of the coefficients used for kernel - ridge - regression ( ` krr ' ) .",
    "a general disadvantage of krr , is that it can be difficult to know which aspects of @xmath1 are relied on to perform the regression .",
    "in contrast , the kernel ib framework provides an intermediate representation , allowing one to visualize the features that jointly account for both @xmath1 and @xmath0 ( figs .",
    "3b & 4c ) .",
    "furthermore , this learned representation permits generalisation across different tasks that rely on the same set of latent features ; something not possible with krr .    finally , the ib approach has important implications for models of early sensory processing [ 17 , 18 ] .",
    "notably , ` efficient coding ' models typically consider the low - noise limit , where the goal is to reduce the neural response redundancy [ 7 ] . in contrast , the ib approach provides a natural way to explore the family of solutions that emerge as one varies internal coding constraints ( by varying @xmath109 ) and external constraints ( by varying the input , @xmath1 ) [ 19 , 20 ] .",
    "further , our simulations suggest how the framework can be used to go beyond early sensory processing : for example to explain higher - level cognitive phenomena such as perceptual filling in ( fig .",
    "3 g ) . in future",
    ", it would be interesting to explore how the ib framework can be used to extend the efficient coding theory , by accounting for modulations in sensory processing that occur due to changing task demands ( i.e.  via changes to the relevance variable , @xmath0 ) , rather than just the input statistics ( @xmath1 ) .",
    "[ 1 ] tishby , n. pereira , f c.  & bialek , w.  ( 1999 ) the information bottleneck method . _ the 37th annual allerton conference on communication , control and computing .",
    "_  pp .  368377                  [ 9 ] scheffler , c.  ( 2008 ) .",
    "a derivation of the em updates for finding the maximum likelihood parameter estimates of the student - t distribution . technical note .",
    "url www.inference.phy.cam.ac.uk/cs482/publications/scheffler2008derivation.pdf",
    "[ 11 ] mika , s. ratsch , g. weston , j. scholkopf , b. smola , a.  j. & muller , k.  r.  ( 1999 ) .",
    "invariant feature extraction and classification in kernel spaces . in _ advances in neural information processing systems 12",
    "_ pp . 526532 .",
    "[ 15 ] doi , e. , gauthier , j.  l. field , g.  d. shlens , j. sher , a. greschner , m. ( 2012 ) .",
    "efficient coding of spatial information in the primate retina .",
    "_ the journal of neuroscience _",
    "32(46 ) , pp .",
    "1625616264      [ 17 ] bialek , w. , de ruyter van steveninck , r. r. , & tishby , n. ( 2008 ) .",
    "efficient representation as a design principle for neural coding and computation . in _ information theory , 2006 ieee international symposium _ pp",
    ".  659663        [ 20 ] tkacik , g. prentice , j.  s. balasubramanian , v. & schneidman , e.  ( 2010 ) . optimal population coding by noisy spiking neurons .",
    "_ proceedings of the national academy of sciences _",
    "107(32 ) , pp .",
    "14419 - 14424 ."
  ],
  "abstract_text": [
    "<S> in many applications , it is desirable to extract only the relevant aspects of data . a principled way to do </S>",
    "<S> this is the information bottleneck ( ib ) method , where one seeks a code that maximizes information about a ` relevance ' variable , @xmath0 , while constraining the information encoded about the original data , @xmath1 . </S>",
    "<S> unfortunately however , the ib method is computationally demanding when data are high - dimensional and/or non - gaussian . here </S>",
    "<S> we propose an approximate variational scheme for maximizing a lower bound on the ib objective , analogous to variational em . using this method </S>",
    "<S> , we derive an ib algorithm to recover features that are both relevant and sparse . </S>",
    "<S> finally , we demonstrate how kernelized versions of the algorithm can be used to address a broad range of problems with non - linear relation between @xmath1 and @xmath0 . </S>"
  ]
}