{
  "article_text": [
    "claude shannon created the foundations of information theory , a mathematical theory of communication , in his landmark 1948 paper @xcite .",
    "however , until fairly recently few attempts were made to study the transmission and processing of quantum states .",
    "the excellent survey paper @xcite provides considerable motivation for the study of quantum information theory .",
    "important application areas include quantum cryptographic protocols that are more secure than and quantum computers that are dramatically faster than their classical counterparts .",
    "the first problem that shannon addressed in @xcite was the ultimate data compression achievable on the output of a discrete information source .",
    "shannon initially considered the set of encoding rules for which the source sequence can be perfectly retrieved from the encoded sequence , at least with high probability . for any discrete , stationary , and ergodic source , shannon defined the _",
    "entropy _ of the source as a function of the probabilities of the source and demonstrated that the minimum achievable average number of code symbols per source symbol is the entropy of the source .",
    "later in another paper @xcite , shannon also treated the problem of encoding a source given a _ fidelity criterion _ or a _ measure of the distortion _ for a representation of the source output .",
    "the goal in this case is to minimize the expected distortion attainable at a particular rate . for a wide class of distortion measures and source models , shannon provided a generalization of the source entropy , known as the _ rate distortion function _ , which establishes the exact trade off between the distortion level and the compression rate .",
    "an important problem in the field of quantum information theory is the generalization of classical results on data compression to the quantum domain . to our knowledge ,",
    "the literature thus far treats quantum analogs of discrete , memoryless sources and assumes that the reconstruction must have arbitrarily high fidelity in the limit as the source string length approaches infinity .    in order to describe a discrete , memoryless quantum source",
    ", we must first define _ pure _ and _ mixed _ quantum states .",
    "the state space of a quantum system is a complete description of the properties of the particles in the system .",
    "it includes information about positions , momentums , polarizations , spins , and so on .",
    "the state space is commonly modelled by a hilbert space of wave functions .",
    "the mathematical tools used for the study of quantum information systems are finite dimensional complex vector spaces with an inner product that are spanned by abstract wave functions .",
    "a thorough discussion of mathematical conventions and terminology which are standard in quantum mechanics can be found in @xcite . in particular , a state is a _ ray _ in a hilbert space , where a ray is defined as an equivalence class of unit norm vectors that differ by multiplication by a nonzero complex scalar .",
    "if we are looking at a subsystem of a larger quantum system , then the state of the subsystem is not necessarily a ray .",
    "if the state of the subsystem is a ray , then the state is called _ pure _ and otherwise it is called _",
    "mixed_. when we are considering these subsystems , the state of the system is represented by a _ density operator _",
    ", i.e. , a positive semi - definite matrix with unit trace . in the special case of a pure state ,",
    "the density operator is the rank one outer product of the corresponding ray with its conjugate transpose . for a mixed state ,",
    "the density operator is a convex combination of the density operators of two or more pure states .",
    "a discrete , memoryless quantum information source is an ensemble of density operators @xmath0 emitted with probabilities @xmath1 .",
    "each density operator corresponds to a pure or a mixed state .",
    "the goal of the quantum data compression problem formulated in @xcite is to compress a sequence of pure quantum states into the smallest possible hilbert space with arbitrarily good reconstruction fidelity in the limit as the sequence length approaches infinity . in the special case where the ensemble consists of only pure states ,",
    "the problem has been solved in @xcite , @xcite , @xcite .",
    "the more general problem where the ensemble contains at least one mixed state was first mentioned in @xcite . in this case",
    ", the optimal compression rate is unknown @xcite , @xcite , @xcite , but these papers provide upper and lower bounds on the best achievable compression rates .",
    "when the matrices corresponding to the density operators for an ensemble of mixed and/or pure states commute , the quantum compression problem has been reformulated in @xcite as an equivalent classical information theory problem in which probability distributions are compressed and communicated .",
    "our analysis will be in terms of this formulation .",
    "the problem of optimal mixed state coding has been considered in two different scenarios . in the first case ,",
    "called the _ visible source case _ , the encoder knows the precise sequence of states or probability distributions that it is transmitting . in the second case , called the _ hidden source case _ , the encoder only has access to a measurement or `` side information '' sequence .",
    "each entry of this second sequence is found by taking a measurement of the corresponding state ; i.e. , taking one experimental outcome of the probability distribution of the analogous entry in the original sequence . elsewhere in the quantum information literature",
    "this is called the _ blind case _ , but the terminology `` hidden '' is more standard in the communications literature .",
    "references @xcite , @xcite , and @xcite provide lower and upper bounds for the optimal rate of asymptotically faithful compression which apply to both variants of the problem .",
    "we provide a rate distortion interpretation of the problem which unifies the analysis of both variants and leads to the exact optimal rates for both the visible and blind versions .",
    "furthermore , the rate distortion framework leads to a natural generalization of the quantum compression problem in which the expected fidelity of reconstruction is asymptotically bounded from below but is not necessarily perfect . to our knowledge",
    ", this problem has not been addressed earlier in the literature .",
    "our techniques provide the optimal compression rate for the both the visible and blind commuting cases in this setting .",
    "it has come to our attention that @xcite presents an alternate proof of the achievability of the lower bound in the visible , commuting case where reconstruction is asymptotically perfect .",
    "suppose that we have an ensemble of @xmath2 states with the corresponding discrete probability mass functions @xmath3 that assume outcome values from the alphabet @xmath4 .",
    "we represent the alphabet @xmath5 by @xmath6 .",
    "let @xmath7 denote the probability that a measurement of the @xmath8 state leads to outcome value @xmath9 .",
    "hence , @xmath10 and @xmath11    assume there is a memoryless source emitting a sequence of the mass functions .",
    "in other words , there is a probability distribution on @xmath6 and with probability @xmath12 the source emits state @xmath13 .",
    "the source simultaneously produces a second sequence on @xmath14 which can be viewed as side information .",
    "when the source emits state @xmath13 , it also emits a side - information output symbol @xmath15 with probability @xmath16 .",
    "let @xmath17 and @xmath18 be the output of the source corresponding to the sequence of states and the sequence of side information , respectively . for the original problem posed in @xcite , we wish to consider codes in which a receiver that knows the source model generates a sequence @xmath19 of output values that fall in the `` strongly typical set '' ( see , e.g. , @xcite ) for the state sequence @xmath17 .",
    "more specifically , for each state @xmath13 the relative frequencies of the @xmath20 output symbols corresponding to the positions where @xmath13 is the state emitted from the source should asymptotically converge to the probability mass function @xmath21 with probability @xmath22 .",
    "in other words , we measure the fidelity of the output sequence @xmath19 through the empirical distribution of sequences of pairs @xmath23 . in practice",
    ", coding is performed from finite strings @xmath24 to output strings @xmath25 .",
    "pick a block length @xmath26 and let @xmath27 denote the sample frequency of state and output pairs @xmath28 over the range @xmath29 . then for the compression problem with asymptotically perfect reconstruction",
    "we require the bhattacharyya - wootters overlap @xcite of the true probabilities @xmath30 and the empirical frequencies of the state and output pairs to be arbitrarily close to 1 in the limit as @xmath26 approaches infinity .",
    "more precisely , we choose our code to satisfy the constraint @xmath31 for arbitrarily small positive constants @xmath32 and @xmath33 whenever @xmath26 is sufficiently large .",
    "the code may use probabilistic processes for the encoding and/or decoding .",
    "the objective of the encoder is to compress the state sequence as much as possible .    the source model for this problem",
    "superficially resembles the composite source models discussed in @xcite .",
    "the key difference is the reversal of what is viewed as the side information sequence and what is viewed as the primary source sequence .",
    "for this reason , the analysis techniques developed for that source coding problem do not appear to apply to this setting .",
    "there are two obvious upper bounds to the minimum average number of bits per symbol required in the encoding .",
    "one of these bounds applies to both the visible and the blind versions of the compression problem and the other applies only to the visible case . for the visible problem",
    ", the encoder may simply transmit the sequence @xmath17 and the decoder may use the appropriate probability mass function every time it receives a state to generate the output sequence . with this algorithm ,",
    "the expected number of bits per symbol used by the encoder can come arbitrarily close to the entropy @xcite of the state alphabet : @xmath34 another possibility for either the blind or the visible case is for the encoder to transmit the sequence @xmath18 and the decoder to use the sequence without modifying it .",
    "the entropy of this sequence is @xmath35 it is easy to find situations where both of these procedures are suboptimal .",
    "consider the case where the @xmath2 probability mass functions are identical , @xmath36 for all states @xmath13 , and @xmath37 for all pairs of states @xmath13 and output symbols @xmath9 . in this case , transmitting the sequence @xmath17 will require @xmath38 bits per symbol on average and transmitting the sequence @xmath18 will require @xmath39 bits per symbol on average . here the optimal coding procedure for both the visible and blind versions of the problem would be to have the encoder transmit nothing and the decoder generate independent and equiprobable output symbols .",
    "this coding procedure uses the ideal of zero bits per symbol .",
    "it is possible to modify the entropy upper bound for some sources to avoid the simple counterexample above .",
    "suppose that there are two or more output symbols @xmath9 which have a `` common randomness ; '' i.e. , for which the @xmath16 are equal for all @xmath40",
    ". then an encoding strategy would be to introduce an erasure symbol , to replace all occurrences of output symbols with common randomness in @xmath18 with the erasure symbol , and to encode the resulting sequence to its entropy .",
    "the decoder will not modify the ordinary symbols , and when it sees an erasure symbol it will generate a symbol of `` common randomness '' with the appropriate conditional probability . in the case",
    "where @xmath41 for all pairs @xmath42 , we will show that for the blind version of the problem with asymptotically perfect fidelity it is impossible to do better than this modified entropy bound .",
    "some additional care needs to be provided in specifying the solution for the blind version of the problem when there are pairs @xmath42 with @xmath43 , but the solution is in the form of a mutual information .",
    "@xcite and @xcite prove that a lower bound to the optimal compression ratio for both versions of the problem with asymptotically perfect fidelity is the mutual information between the state alphabet and the output alphabet @xmath44 but leaves open the question whether this lower bound is attainable in either the visible or the blind variants .",
    "we will establish that it is achievable for the visible version of the problem .",
    "our analysis takes advantage of the tools of rate distortion theory . the quantum information literation thus far has focused upon the bhattacharyya - wootters overlap ( see ( [ eq : bw ] ) ) as a way to measure the closeness of two probability distributions .",
    "this overlap is non - negative and is equal to one exactly when the two probability distributions are identical .",
    "an equivalent and opposite way to measure the closeness of two probability distributions is to discuss their `` distance '' or the distortion generated by approximating one by the other . in this",
    "setting , perfect fidelity corresponds to zero distortion .",
    "the bhattacharyya - wootters overlap can be converted into such a distortion function by being subtracted from one .",
    "there are many other examples of interesting distortion functions that appear in the probability and classical information literature .",
    "the advantage of this interpretation is that rate distortion theory has been studied extensively since @xcite .",
    "we will show that there is a very simple way to formulate and solve the problem of compressing probability distributions in the rate distortion setting .",
    "it is also straightforward to generalize these results to the case where the reconstruction fidelity is imperfect .",
    "we begin with several basic information - theoretic definitions .",
    "suppose we have two discrete and finite random variables @xmath45 and @xmath46 whose joint probability distribution is @xmath47 .",
    "the _ entropy _ of @xmath45 and _ conditional entropy _ of @xmath45 given @xmath46 are defined as ( see ( * ? ? ?",
    "2 ) ) @xmath48 where @xmath49 is the support of @xmath50 , i.e. , the set of @xmath51 such that @xmath52 . as done here",
    ", we will continue to write random variables with upper - case letters and values they take on with lower - case letters .",
    "the _ informational divergence _ between @xmath50 and @xmath53 is defined as @xmath54 and we write @xmath55 when there is an @xmath51 in @xmath49 such that @xmath56 . the informational divergence is also called the `` information for discrimination , '' the `` relative entropy '' and the `` kullback - leibler distance '' @xcite , @xcite .",
    "the _ mutual information _ between @xmath45 and @xmath46 is defined as @xmath57 a well - known property of these quantities is that they are all non - negative ( * ? ? ?",
    "2 ) . furthermore , @xmath58 if and only if @xmath59 for all @xmath51 in @xmath49 .",
    "this implies that @xmath60 if and only if @xmath45 and @xmath46 are statistically independent .",
    "two other important properties involving convexity are given as lemmas .",
    "@xmath61 is convex in the pair @xmath62 , i.e. , if @xmath63 , @xmath64 , are pairs of distributions , then for any nonnegative @xmath65 which sum to one we have @xmath66 equivalently , we can view @xmath67 as a function of @xmath47 and say that @xmath67 is convex in @xmath47 .",
    "[ lemma : dconvexity ]    let @xmath68 be a random variable taking on the value @xmath69 with probability @xmath65 , @xmath70",
    ". we can write ( [ eq : dconvexity1 ] ) as @xmath71     \\ge d\\left ( { { \\rm e}}_j \\left [ p_{x_j } \\right ] \\,\\|\\ ,                 { { \\rm e}}_j \\left [ p_{y_j } \\right ] \\right )     \\label{eq : dconvexity}\\end{aligned}\\ ] ] where @xmath72 $ ] denotes expectation with respect to the random variable @xmath68 . we will sometimes drop the subscript @xmath68 and write @xmath73 $ ] if it is clear with respect to which random variable we are taking expectations .",
    "the mutual information @xmath74 is concave in @xmath50 when @xmath75 is fixed , and convex in @xmath75 when @xmath50 is fixed . in other words , we have @xmath76        \\le i\\left ( { { \\rm e}}_j\\left[x_j\\right ] \\,;\\ , { { \\rm e}}_j\\left[y_j\\right ] \\right )        \\nonumber     \\end{aligned}\\ ] ] when @xmath77 is the same for all @xmath68 , and @xmath76        \\ge i\\left ( { { \\rm e}}_j\\left[x_j\\right ] \\,;\\ , { { \\rm e}}_j\\left[y_j\\right ] \\right )        \\label{eq : iconvexity }     \\end{aligned}\\ ] ] when @xmath78 is is the same for all @xmath68 .",
    "[ lemma : iconvexity ]    our distortion measures will be defined in terms of the _ empirical probability distribution _ of finite - length sequences or strings .",
    "the empirical probability distribution of the length-@xmath26 string @xmath79 with @xmath80 is defined as @xmath81 where @xmath82 is the number of occurrences of the letter @xmath83 in the string @xmath84 @xcite,@xcite . a simple yet important property of @xmath85",
    "is given by the following lemma .",
    "@xmath86 =        \\frac{1}{l } \\sum_{\\ell=1}^l p_{x_\\ell}.     \\end{aligned}\\ ] ]    [ lemma : empirical ]    we have , for all @xmath87 , @xmath88 & =           { { \\rm e}}_{x^l } \\left [ \\frac{n_{x^l}(a)}{l } \\right ]        \\nonumber \\\\        & = \\frac{1}{l } { { \\rm e}}_{x^l } \\left [ \\sum_{\\ell=1}^l 1(x_\\ell = a ) \\right ]        \\nonumber     \\end{aligned}\\ ] ] where @xmath89 is the indicator function that is 1 if its argument is true and is 0 otherwise .",
    "since the expectation of a sum is the sum of the expectations  @xcite , we have @xmath90        & = \\frac{1}{l } \\sum_{\\ell=1}^l              { { \\rm e}}_{x^l } \\left [ 1(x_\\ell = a ) \\right ]        \\nonumber \\\\        & = \\frac{1}{l } \\sum_{\\ell=1}^l p_{x_\\ell}(a ) .        \\nonumber     \\end{aligned}\\ ] ]      we describe the rate distortion problem as considered by shannon @xcite ( see fig .",
    "[ fig : rdmodel ] ) . a discrete memoryless source ( dms ) produces a message string @xmath91 of @xmath26 independent and identically distributed letters from a finite alphabet @xmath92 .",
    "@xmath91 is encoded into one of @xmath93 received strings @xmath94 of @xmath26 letters from a finite alphabet @xmath95 .",
    "the rate of the encoder is thus @xmath96 bits per letter , because one can represent any @xmath97 by a string of @xmath98 bits .",
    "there is a distortion measure @xmath99 that associates a non - negative number @xmath100 with each pair @xmath101 of message letter @xmath51 and receive letter @xmath102 .",
    "the distortion between the strings @xmath84 and @xmath97 is defined as the average of the letter - to - letter distortions : @xmath103 where we have abused notation by using the same symbol @xmath104 for the letter - to - letter and string distortions .",
    "shannon generalized the letter - to - letter distortion measure in @xcite , but we will not be using that generalization here .        the fundamental problem of rate distortion theory is to determine the minimum code rate @xmath96 such that the average distortion between @xmath91 and @xmath94 is upper bounded by some number @xmath105 . the _ rate distortion function _",
    "@xmath106 is thus defined as the greatest lower bound on @xmath96 such that @xmath107 \\le \\delta$ ] .",
    "shannon showed that @xmath106 has the simple form given by the following lemma .",
    "the rate distortion function of a dms with distribution @xmath50 and letter - to - letter distortion measure @xmath99 is @xmath108 \\le \\delta \\end{sb }                 i(x;y ) .",
    "\\nonumber     \\end{aligned}\\ ] ]    the achievability of the rate distortion function is usually demonstrated by choosing a _ random code _ as follows : the @xmath26 letters of each of the @xmath109 code words are chosen independently using @xmath53 .",
    "one then associates the `` typical '' strings @xmath84 , i.e. , those @xmath84 for which @xmath85 is close to @xmath50 , with one of the code words @xmath97 for which @xmath110 is close to @xmath47 , where @xmath110 is the empirical distribution of the @xmath26 pairs @xmath111 .",
    "one can show that if @xmath112 and @xmath26 is large , such a code word @xmath97 exists and @xmath113 with high probability .",
    "a generalization of the rate distortion problem was given by dobrushin and tsybakov in @xcite ( see fig .  [",
    "fig : rdmodel_hidden ] ) .",
    "the new twist is that the encoder sees only a noisy version @xmath114 of the message @xmath91 , where @xmath115 is generated by @xmath116 via the memoryless channel @xmath117 for all @xmath69 .",
    "the dms is sometimes called a `` remote source '' @xcite,@xcite .",
    "we will call the dms a _",
    "hidden source _",
    ", @xmath91 the _ hidden source string _",
    ", @xmath114 the _ visible source string _ and @xmath118 the _ visible distribution_. note that if @xmath119 we have the original rate distortion problem .",
    "the goal is again to determine the minimum code rate @xmath96 such that the average distortion between @xmath91 and @xmath94 is upper bounded by some number @xmath105 .",
    "the rate distortion function @xmath106 is thus defined as before , and dobrushin and tsybakov proved the following lemma .",
    "the rate distortion function of a hidden dms with distribution @xmath50 , visible distribution @xmath118 , and single - letter distortion measure @xmath99 is @xmath120 \\le \\delta \\end{sb }                 i(v;y ) .",
    "\\nonumber     \\end{aligned}\\ ] ]    note that @xmath121 where the second equality follows because @xmath46 is independent of @xmath45 given @xmath122 , and the inequality follows because conditioning can not increase entropy @xcite .",
    "thus , not surprisingly , the best rate when @xmath91 is hidden is at least as large as when @xmath91 is visible .",
    "the random variables of the rate distortion problem with a hidden source satisfy @xmath123 where @xmath124 .",
    "[ lemma : ibounds ]    the first inequality follows by the non - negativity of @xmath125 .",
    "in fact , @xmath94 is usually a function of @xmath114 so that @xmath126 and @xmath127 .",
    "the second inequality follows by @xmath128 the third inequality follows by viewing the sum over the @xmath129 as @xmath26 times @xmath130 $ ] , where @xmath68 takes on the value @xmath69 with probability @xmath131 for @xmath70 .",
    "the bound ( [ eq : iconvexity ] ) then gives the desired result .",
    "we deal with the visible and hidden ( or blind ) source problems simultaneously by introducing an auxiliary string @xmath132 to the model of fig . [",
    "fig : rdmodel_hidden ] ( see fig . [",
    "fig : qmodel ] ) .",
    "@xmath132 represents the outcomes of measurements and is called side information in section [ subsec : transmitpd ] .",
    "the terms of @xmath132 take on values in the alphabet @xmath133 and are generated together with @xmath114 as outputs of a memoryless channel @xmath134 .",
    "we are interested in string distortion measures @xmath99 that depend on @xmath135 only through the empirical distribution @xmath110 .",
    "thus , with some abuse of notation we can write @xmath136 . for example , using ( [ eq : bw ] ) the bhattacharyya - wootters distortion measure could be defined as @xmath137 ^ 2 ,     \\label{eq : bwd}\\end{aligned}\\ ] ] where @xmath138 plays the role of the measurement outcomes in section [ subsec : transmitpd ] .",
    "the visible case has @xmath119 while the hidden case can have @xmath139 and has @xmath140 . as a second example ,",
    "a natural information - theoretic distortion measure is the informational divergence @xmath141 where @xmath142 is defined as @xmath143 p_{z|x}(b|a)$ ] for all @xmath144 and @xmath145 , i.e. , @xmath146 . observe that low distortion is achieved only if the empirical distribution of @xmath135 is close to the desired distribution @xmath147 .",
    "we next impose an additional restriction on @xmath148 , namely that @xmath149 be convex in @xmath47 , i.e. , @xmath150     \\ge d\\left ( { { \\rm e}}_j \\left [ p_{x_j y_j } \\right ] \\right ) ,     \\label{eq : dconvexity}\\end{aligned}\\ ] ] where @xmath68 is a finite random variable . the distortion measure ( [ eq : idd ] ) meets this requirement by lemma [ lemma : dconvexity ] .",
    "the distortion measure ( [ eq : bwd ] ) also meets this requirement since , for @xmath151 and @xmath152 , we have @xmath153 ^ 2     & \\le     1- \\sum_{\\ell } \\left [ \\sum_{x , z }",
    "\\sqrt {        \\lambda_\\ell a_\\ell(x , z ) } \\right]^2 \\nonumber \\\\     & = \\sum_{\\ell } \\lambda_\\ell \\left\\ { 1 -         \\left [ \\sum_{x , z } \\sqrt{a_\\ell(x , z ) } \\right]^2 \\right\\ } ,     \\nonumber\\end{aligned}\\ ] ] where @xmath154 and where the first step follows by minkowski s inequality @xcite .",
    "we call the problem of finding the rate distortion function for this set - up the _ quantum commuting density operator _ ( quantum cdo ) rate distortion problem .",
    "the following lemma gives a lower bound on the rate distortion function .",
    "the rate @xmath96 of the quantum cdo rate distortion problem with expected distortion @xmath155 = \\delta$ ] satisfies @xmath156 [ lemma : lowerbound ]    a simple upper bound on @xmath157 is the logarithm of the number of possible values @xmath94 takes on with nonzero probability ( * ? ? ?",
    "6 ) , i.e. , the logarithm of the number of code words .",
    "we thus have @xmath158 \\le \\delta \\end{sb } i({\\overline v};{\\overline y } ) ,        \\nonumber     \\end{aligned}\\ ] ] where the second inequality follows by ( [ eq : ibounds ] ) , and the third inequality because of the minimization .",
    "next , we have @xmath159 &        \\ge d\\left ( { { \\rm e}}\\left [ p^e_{x^ly^l } \\right ] \\right )        = d\\left ( p_{{\\overline x}\\,{\\overline y } } \\right ) ,        \\nonumber     \\end{aligned}\\ ] ] where @xmath160 .",
    "the inequality follows by the convexity of @xmath148 and the equality by lemma  [ lemma : empirical ] .",
    "thus , the condition @xmath107",
    "\\delta$ ] implies that @xmath161 , and we have @xmath162 this is the same as ( [ eq : lowerbound ] ) because the minimization over @xmath163 is the same as the minimization over @xmath164 .",
    "we next show that the lower bound of lemma  [ lemma : lowerbound ] can be approached arbitrarily closely , and is thus the desired rate distortion function .    for any @xmath165 and distortion @xmath105 there",
    "exists a block code of sufficiently large block length for which @xmath166 [ lemma : achievablerates ]    we give only a very brief sketch of the proof for this preliminary version of the paper .",
    "+ the code is generated by choosing some @xmath167 and then randomly choosing each symbol of the @xmath109 code words independently using the resulting @xmath53 .",
    "let the @xmath168th code word be @xmath169 and choose some @xmath170 .",
    "for each @xmath171 satisfying @xmath172 for all @xmath83 , one looks for a code word @xmath169 such that @xmath173 for all @xmath83 and @xmath174 .",
    "let @xmath175 be the event that the @xmath168th code word @xmath176 , now regarded as a random variable , is such a code word .",
    "lemma 13.6.2 in @xcite assures us that @xmath177 } \\le \\pr\\left[{{\\cal e}}_k(v^l)\\right ] \\le        2^{-l\\ , [ i(v;y)-\\epsilon_1 ] } , \\nonumber     \\end{aligned}\\ ] ] where @xmath178 as @xmath179 and @xmath180 .",
    "continuing as in ( * ? ? ?",
    "13.6 ) , one will need @xmath181 code words to ensure that @xmath175 occurs for at least one @xmath168 for all the `` typical '' @xmath171 .",
    "one can also use the approach in ( * ? ? ?",
    "13.6 ) to show that the distortion criterion is met for each such @xmath182 pair with high probability .",
    "the code construction we have just described can be done for any @xmath167 , so we choose that @xmath167 which minimizes the rate @xmath183 .",
    "the rate distortion function of the quantum cdo rate distortion problem is @xmath184 [ theorem : quantumrd ]      we give examples to demonstrate how one can apply the above results .",
    "consider example 8 of @xcite in which the states @xmath185 and @xmath186 have prior probabilities @xmath187 and @xmath188 , respectively , where @xmath189 is a diagonal matrix with entries @xmath83 and @xmath174 . in @xcite it is shown that one may regard the two states as biased coins @xmath190 and @xmath191 that take on the values h ( for heads ) with respective probabilities @xmath192 and @xmath193 , and the value t ( for tails ) with respective probabilities @xmath194 and @xmath195 . adapting this to fig .",
    "[ fig : qmodel ] , we let @xmath91 be the sequence of coins and @xmath132 a sequence of outcomes of coin tosses , i.e. , @xmath196 , @xmath197 , @xmath198 , @xmath199 , and so on .    consider the visible case where @xmath119 , so that the rate distortion function is @xmath200 if @xmath201 then @xmath202 for both the bhattacharyya - wootters and the informational divergence distortion measures .",
    "thus , we have @xmath203 ,     \\nonumber\\end{aligned}\\ ] ] where @xmath204 is the _ binary entropy function _",
    "7 ) . for a concrete example , set @xmath205 , @xmath206 and @xmath207 .",
    "then @xmath208 is the ultimate limit on data compression with no distortion ; fig .",
    "[ fig : visibleexample ] shows @xmath106 as a function of @xmath105 for both the bhattacharyya - wootters ( bw ) and informational divergence ( i d ) distortion measures .",
    "observe that @xmath106 is convex @xcite .",
    "consider next the hidden source case ( or blind case ) where @xmath140 .",
    "we thus have @xmath209 again , if @xmath201 then @xmath202 for both the bhattacharyya - wootters and the informational divergence distortion measures .",
    "performing the optimization , we find that @xmath210 can be a _",
    "discontinuous _ function of @xmath193 ; for @xmath211 we have @xmath212 and for @xmath213 we have @xmath214 .",
    "for example , suppose that @xmath205 and @xmath215 .",
    "we plot @xmath106 as a function of @xmath193 for various @xmath105 and the bhattacharyya - wootters distortion measure in fig .",
    "[ fig : hiddenexample ] . observe that as @xmath216 we will have a discontinuity at @xmath217 . in practice , this discontinuity does not occur because @xmath201 is impossible for finite block lengths .",
    "furthermore , if @xmath105 is not too small , say @xmath218 , then for many @xmath193 one can achieve substantially better compression rates than @xmath210 .",
    "the problem of determining optimal compression limits for quantum information has recently generated considerable interest . in the special case of an ensemble of mixed states with commuting density operators , we use rate distortion theory to find the optimal rates in both the visible and blind versions of the problem .",
    "we also generalize this special case of the quantum compression problem to the setting where the reconstruction is not faithful .",
    "we would like to thank c. fuchs for bringing this problem to our attention and v. goyal for feedback on the manuscript .",
    "we would also like to thank i. cirac for providing us with a draft of @xcite ."
  ],
  "abstract_text": [
    "<S> we provide a rate distortion interpretation of the problem of quantum data compression of ensembles of mixed states with commuting density operators . </S>",
    "<S> there are two versions of this problem . in the _ visible _ case the sequence of states is available to the encoder and in the _ blind _ or _ </S>",
    "<S> hidden _ case the encoder may access only a sequence of measurements . </S>",
    "<S> we find the exact optimal compression rates for both the visible and hidden cases . </S>",
    "<S> our analysis includes the scenario in which asymptotic reconstruction is imperfect .    </S>",
    "<S> [ section ] [ theorem]proposition [ theorem]lemma [ theorem]corollary [ theorem]conjecture [ theorem]conjecture    * quantum data compression of ensembles of mixed states with commuting density operators * +   + serap a. savari , bell labs 2c-451 , murray hill nj 07974 + </S>"
  ]
}