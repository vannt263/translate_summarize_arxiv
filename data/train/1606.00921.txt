{
  "article_text": [
    "we are motivated by recent advances in neuroimaging of structural interconnections among anatomical regions in the human brain .",
    "our focus is on learning how brain structural connectivity networks  also known as connectomes  vary across individuals , and the extent to which such variability is associated with differences in human cognitive traits .    in our application ,",
    "brain networks are estimated exploiting structural magnetic resonance imaging and diffusion tensor imaging to obtain a @xmath0 symmetric adjacency matrix @xmath1 for each subject @xmath2 .",
    "each cell @xmath3 $ ] in the matrix corresponds to a pair of brain regions , with @xmath4}=a_{i[uv]}=1 $ ] if there are fibers connecting brain regions @xmath5 and @xmath6 in subject @xmath7 , and @xmath4}=a_{i[uv]}=0 $ ] otherwise .",
    "there are @xmath8 regions in our study @xcite equally divided in the left and right hemisphere .",
    "refer to figure  [ fig:1 ] for an example of the available data .",
    "there has been an increasing focus on using brain imaging technologies to better understand the neural pathways underlying human traits , ranging from personality to cognitive abilities and mental disorders @xcite .",
    "our aim is to develop flexible procedures to improve understanding of how the brain structural connectivity architecture varies in relation to a trait of interest @xmath9 measured for each subject @xmath10 . in our application",
    "this trait represents a measure of intelligence available via the fsiq ( full scale intelligence quotient ) score @xcite .",
    "network data are challenging to analyze because they require not only dimensionality reduction procedures to effectively deal with the large number of pairwise relationships , but also flexible formulations to account for the topological structures of the network .",
    "current literature addresses these goals only for a single network observation .",
    "notable examples include exponential random graph models ( e.g. * ? ? ?",
    "* ) , and factorizations covering stochastic block models @xcite , mixed membership stochastic block models @xcite and latent space models @xcite .",
    "these procedures reduce dimensionality , incorporate network properties and have been generalized to accommodate regression settings in which there are response variables and network - specific predictors associated with every node @xmath11 in a single network ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "this type of network regression is fundamentally different from our interest in relating a network @xmath12 specific to individual @xmath7 to a corresponding predictor @xmath13 , for @xmath14 .    in relating the network @xmath12 to a specific trait @xmath13 ,",
    "a common strategy in neuroscience is to estimate a separate logistic regression for each pair of brain regions to learn changes in their connectivity with the predictor .",
    "however , as discussed in @xcite , such massive univariate edge - based studies do not incorporate dependence in connectivity patterns , and therefore ignore relevant wiring mechanisms in the brain architecture .",
    "this has motivated an increasing interest in how topological characteristics of a complex network change as a function of a trait ( e.g. * ? ? ?",
    "typical procedures address this aim by computing a set of summary measures for each network  e.g. network density , transitivity , average path length , assortativity  and enter these statistics as responses in a regression model ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "however , reducing the rich network data to a subset of summary measures can discard important information about the brain connectivity and underestimate its changes across the trait .        with these issues in mind",
    ", we develop a network  response regression model , which considers the brain network @xmath12 as an object - type response variable having conditional expectation changing flexibly with @xmath13 . to our knowledge",
    ", there is no literature addressing this problem , though there are a rich variety of methods for characterizing dynamic changes in a time - specific network @xmath15 , with @xcite , @xcite , @xcite considering discrete equally - spaced times and @xcite providing a continuous - time formulation .",
    "although the later article provides a useful building block , the time - series setting is fundamentally different from the regression case we consider , motivating careful modifications to incorporate subject - specific variability and other relevant structure .",
    "there is an increasing availability of data motivating network ",
    "response regression models .",
    "we propose a bayesian semiparametric formulation that reduces dimensionality and efficiently exploits network information via a flexible latent space representation , with the latent coordinates of the brain regions varying both systematically  according to a trait of the individual , such as fsiq  and randomly  due to unobserved traits or measurement errors  across individuals .",
    "this formulation allows coherent inference at different scales , including global changes in network topological structures and local variations in edge probabilities .",
    "the paper is organized as follows . in section [ method ]",
    "we focus on model formulation , prior specification and posterior computation .",
    "a simulation study is examined in section [ simu ] . in section [ app ]",
    "our model is applied to learn changes in the brain network with the fsiq score , showing improved performance in inference , edge prediction and uncertainty quantification .",
    "let @xmath16 denote the binary adjacency matrix characterizing the undirected brain network with no self - relationships for the subject @xmath7 , and @xmath13 the corresponding trait value , for every @xmath10 .",
    "as self - relationships are not of interest and @xmath16 is symmetric , we model @xmath17 by defining a probabilistic generative mechanism for the vectors @xmath18 , with @xmath19 } , a_{i[31 ] } , \\ldots , a_{i[v1]},$ ] @xmath20 } , \\ldots a_{i[v(v-1)]})^\\intercal$ ] the vector encoding the lower triangular elements of @xmath1 , which uniquely characterize @xmath12 . hence , @xmath21 is a vector of binary elements @xmath22 , @xmath23 , encoding the absence or presence of an edge among the @xmath24th pair of brain regions in subject @xmath7 .",
    "based on our notation , developing a regression model for a network - valued response translates into statistical modeling of how a vector of binary data changes across the values of a trait of interest . however , it is important to explicitly incorporate the network structure of our data .",
    "in fact , networks are potentially characterized by specific underlying topological patterns which induce dependence among the edges within each brain . as a result , by carefully accommodating the network structure in modeling of @xmath18",
    ", one might efficiently borrow information within each @xmath21 and across the trait @xmath13 , while reducing dimensionality and inferring specific network properties along with their changes across @xmath13 .",
    "in modeling of @xmath18 we look for a representation which can flexibly characterize variability across individuals in brain connectivity , while accommodating network structure within each brain and learning changes with the trait @xmath13 . individual variability ( e.g * ? ? ?",
    "* ) and specific network structures ( e.g * ? ? ?",
    "* ) have been shown to substantially affect the functioning of networked brain systems , with these systems often varying with human traits ( e.g * ? ? ?",
    "consistent with the above goals and letting @xmath25 denote the random variable associated with the brain network of subject @xmath7 , we characterize individual variability by assuming the edges among pairs of brain regions are conditionally independent bernoulli variables , given a subject - specific edge probability vector @xmath26 , @xmath27 independently for each pair @xmath23 and @xmath10 .",
    "equation   incorporates individual variability , but fails to account for two key sources of information in our data .",
    "in fact , we expect dependence between the edge probabilities in each @xmath28 due to the network topology .",
    "moreover , it is reasonable to expect that subjects with similar traits will have comparable brain networks . to incorporate these structures",
    ", we define the edge probabilities as a function of subject - specific node positions in a latent space , with these positions centered on a higher - level mean which changes with @xmath13 .",
    "specifically , letting @xmath24 denote the pair of brain regions @xmath5 and @xmath6 , @xmath29 , we first borrow information within each @xmath28 by defining @xmath30 for each @xmath23 and @xmath10 . in , @xmath31 is a similarity measure for the @xmath24th pair of regions shared among all individuals , whereas @xmath32 and @xmath33 denote the @xmath34th coordinate of the brain regions @xmath5 and @xmath6 for subject @xmath7 , respectively .",
    "this construction has also an intuitive interpretation .",
    "in fact , @xmath35 may measure the propensity of brain region @xmath5 towards the @xmath34th cognitive function in subject @xmath7 . according to ,",
    "if regions @xmath5 and @xmath6 have propensities in the same direction , they will have a high chance @xmath36 to be connected .",
    "moreover , embedding the brain regions in a lower - dimensional space via allows dimensionality reduction , and can accommodate several topological properties @xcite .      to conclude our bayesian specification , we choose priors for the shared parameters @xmath37 , @xmath23 , and the subject - specific latent coordinates @xmath38 for each @xmath11 , @xmath39 and @xmath10 .",
    "these priors are defined to facilitate simple posterior computation , favor borrowing of information between different individuals and allow the latent coordinates to smoothly change across the values of the predictor .",
    "subjects with similar traits are expected to have comparable brain networks .",
    "we incorporate this structure by centering the prior for the subject - specific latent coordinates on a higher - level mean smoothly changing with the trait of interest .",
    "then , by updating this prior with the likelihood provided by the data we expect the posterior to flexibly account for possible deviations from prior assumptions , including allowance for uninformative traits .",
    "consistent with the above considerations , we let @xmath40 independently for @xmath23 , and @xmath41 independently for every @xmath11 , @xmath39 and @xmath10 .",
    "we set the prior variance of @xmath38 at 1 because we observed indistinguishable results when replacing with a student-@xmath42 distribution .",
    "hence we maintain the gaussian prior to keep the model parsimonious and computationally tractable . to accommodate systematic deviations , we incorporate mean functions @xmath43 characterizing changes in the @xmath34th latent coordinate of the @xmath5th brain region with the trait of interest .    in modeling @xmath43",
    ", we could consider a gaussian process ( gp ) prior for each @xmath11 and @xmath39 .",
    "however , as the number of nodes increases , we face scalability issues .",
    "to reduce dimensionality , we define each @xmath43 as a linear combination of a smaller number of dictionary functions @xmath44 , @xmath45 and @xmath39 , @xmath46 for each @xmath11 and @xmath39 , where @xmath47 , for @xmath11 and @xmath48 , are coefficients common to all the subjects .",
    "factorization further reduces the number of unknown functions from @xmath49 to @xmath50 , where @xmath51 is typically smaller than @xmath52 .",
    "factorizations and are not unique ; however , we avoid identifiability constraints as the focus of our inference is not on latent coordinates but on how the overall network structure varies systematically with traits and randomly across individuals .",
    "such inferences can be accomplished via appropriate functionals of the edge probabilities in @xmath28 , as we will illustrate .    in choosing priors for the components in factorization , we first let @xmath53 independently for each @xmath48 and @xmath54 , with @xmath55 the squared exponential correlation function @xmath56 , @xmath57 . to allow adaptive deletion of unnecessary dictionary functions , we incorporate a shrinkage effect in the prior for the coefficients @xmath47 , @xmath11 and @xmath48 , letting @xmath58 in",
    ", @xmath59 provides a global column - wise shrinkage effect on @xmath47 .",
    "high values of @xmath59 force the prior for @xmath47 to be concentrated around zero _ a priori _ , deleting the effect of the corresponding dictionary function @xmath44 in factorization .",
    "equation is carefully defined to allow this shrinkage effect to be increasingly strong as @xmath60 grows .",
    "a graphical representation of our hierarchical model formulation is provided in figure  [ fig : graphic ] .",
    "= [ circle , minimum size = 11.5 mm , thick , draw = black!80 , node distance = 6 mm ] = [ -latex , thick ] = [ rectangle , draw = black!100 ] ( a ) @xmath61 ; ( pi ) [ left = of a ] @xmath28 ; ( z ) [ above = of pi ] @xmath37 ; ( y ) [ left = of pi ] @xmath35 ; ( mu ) [ left = of y ] @xmath43 ; ( g ) [ above = of mu ] @xmath47 ; ( tau ) [ left = of g ] @xmath62 ; ( w ) [ left = of mu ] @xmath44 ; ( x ) [ left = of w ] @xmath13 ; ( pi ) edge [ connect ] ( a ) ; ( z ) edge [ connect ] ( pi ) ; ( y ) edge [ connect ] ( pi ) ; ( mu ) edge [ connect ] ( y ) ; ( g ) edge [ connect ] ( mu ) ; ( tau ) edge [ connect ] ( g ) ; ( w ) edge [ connect ] ( mu ) ; ( x ) edge [ connect ] ( w ) ;      given priors defined in equations  , posterior computation for model with subject - specific edge probabilities factorized as in , proceeds via a simple gibbs sampler leveraging plya - gamma data augmentation @xcite , which allows conjugate inference in bayesian logistic regression .",
    "we summarize below the main steps of the mcmc routine .",
    "step - by - step derivations are provided in the supplementary material .",
    "* update each augmented data @xmath63 , @xmath23 , @xmath10 , from its full conditional plya - gamma distribution . *",
    "given the data @xmath64 , @xmath65 , the latent coordinates matrix @xmath66 , @xmath65 , and the plya - gamma augmented data @xmath63 , @xmath23 , @xmath10 , the full conditionals for @xmath37 , @xmath23 are gaussian distributions .",
    "* in updating the subject - specific coordinates matrix @xmath66 , for each @xmath10 , we block sample the rows of @xmath66 in turn conditionally on the rest of the matrix and the parameters @xmath47 , @xmath11 , @xmath48 , @xmath67 , @xmath48 , @xmath68 .",
    "this approach allows rewriting the model  as a bayesian logistic regression for which the plya - gamma data augmentation scheme guarantees conjugate gaussian full conditionals .",
    "* given the coordinates matrix @xmath66 , @xmath10 and the traits @xmath13 , @xmath10 , updating for the parameters @xmath47 , @xmath11 , @xmath48 and the trajectories @xmath44 ,",
    "@xmath48 , @xmath68 at the observed trait values , proceed by exploiting the properties of gp priors and standard steps in bayesian linear regression . since the data are observed for a finite number of subjects , this step uses the multivariate gaussian representation of the gp .",
    "however it is worth noticing that our model is inherently semiparametric as the gp in induces a prior on the infinite - dimensional space of smooth functions .",
    "* conditioned on @xmath47 , @xmath11 , @xmath48 , the shrinkage parameters @xmath62 , @xmath48 are updated from their full conditional gamma distributions . * to obtain each @xmath69 , @xmath23 , @xmath10 simply apply equation to the posterior samples of @xmath66 , for each @xmath10 and @xmath37 , @xmath70 . *",
    "impute missing edges @xmath71 from @xmath72    obtaining posterior samples for the subject - specific edge probabilities associated to missing edges is a key for prediction . under our bayesian procedure and recalling equation , prediction of unobserved edges",
    "can be obtained by exploiting the mean of the posterior predictive distribution @xmath73 \\label{eq10 } \\\\ & = \\mbox{e } \\ { \\pi_l^{(i ) } \\mid \\mathcal{l}({a}_1 ) , \\ldots , \\mathcal{l}({a}_n ) \\nonumber \\ } , \\quad l=1 , \\ldots , v(v-1)/2 , \\end{aligned}\\ ] ] for each possible missing edge in subject @xmath10 , where the last expectation coincides with the posterior mean of @xmath36 .",
    "note that we use standard font @xmath74 to define the observed vectors of edges and italics notation @xmath75 to denote the associated random variable .",
    "we evaluate the performance of our methods on synthetic data simulated from a generating process different than our statistical model .",
    "our goal is to assess whether the proposed methods are sufficiently flexible to learn global and local changes in brain connectivity structures , even when such variations arise from different generative mechanisms .    to accomplish the above goal",
    ", we simulate multiple brain networks @xmath12 with @xmath76 nodes and having predictors @xmath13 observed on a discrete grid @xmath77 . in particular ,",
    "for each unique predictor value , four networks @xmath12 are generated , for a total of @xmath78 subjects . to imitate the hemispheres and lobes in the brain ,",
    "we define four node blocks @xmath79 , @xmath80 , @xmath81 and @xmath82 . nodes in @xmath83 and @xmath84 belong to the first lobe in the left and right hemisphere , respectively , whereas nodes in @xmath85 and @xmath86 belong to the second lobe in the left and right hemisphere , respectively .    in simulating the data @xmath12",
    ", we aim to incorporate different topological properties typically observed in human brain networks  covering block - structures by hemispheres and lobes , along with small - world architectures ( e.g * ? ? ?",
    "in particular , for each unique predictor value in @xmath87 , half of the subjects have high assortativity by hemisphere , whereas the others have brains with high lobe assortativity .",
    "subjects with an intermediate predictor value @xmath88 are characterized by brain networks having small - world behavior according to the @xcite model .",
    "finally  consistent with initial descriptive analyses of our data  we increase the interhemispheric density and reduce the intrahemispheric connectivity in the brain networks of the subjects with high predictor value @xmath89 . as shown in figure [ fig:2 ] this construction represents a challenging scenario characterized by different network architectures not generated from our model , and changing across the predictors values with varying patterns of smoothness and subject - specific variability .        to highlight the possible benefits provided by our statistical model , we compare performance with a massive univariate approach estimating a flexible logistic regression for each pair of nodes as follows @xmath90 for @xmath23 , where @xmath55 is the correlation function discussed in section [ method ] , @xmath91 is the mean function and @xmath92 is a scaling parameter controlling variability . to borrow information across edges ,",
    "we set @xmath91 equal to the log - odds of the empirical edge probabilities computed for each predictor value in @xmath93 and let @xmath94 to allow flexible deviations in each edge trajectory .",
    "posterior inference under the statistical model in equation can be easily performed leveraging the r package ` bayeslogit ` .",
    "in performing posterior computation under our model we let @xmath95 , @xmath96 , @xmath97 , @xmath98 and set the upper bounds for the latent dimensions at @xmath99 .",
    "we consider @xmath100 gibbs iterations with a burn - in of @xmath101 and thin the chains every @xmath102 samples  after burn - in .",
    "these choices provide good settings for convergence and mixing based on the inspection of the trace - plots for the subject - specific edge probabilities .",
    "posterior computation for our model takes @xmath10316 minutes under a naive r ( version 3.2.1 ) implementation in a machine with 8 intel core i7 3.4 ghz processor and 16 gb of ram .",
    "hence , there are substantial margins to reduce computational time .",
    "we consider the same mcmc settings when performing posterior inference for the model in , obtaining comparable results for convergence and mixing .",
    "the simulated data set provides a challenging scenario to assess robustness of our methods to model misspecification .",
    "we answer this question via posterior predictive checks @xcite assessing the flexibility of our formulation in characterizing the network summary measures in figure [ fig:2 ]  of particular interest in neuroscience @xcite .",
    "calculation of the posterior predictive distributions for these measures is straightforward using the posterior samples of @xmath69 , @xmath23 , @xmath10 , and equation .",
    "figure [ fig:3 ] compares the network summary measures computed from the simulated data with their posterior predictive distribution arising from our network ",
    "response regression model and the nonparametric massive univariate logistic regressions in , respectively .",
    "the closer the points in figure [ fig:3 ] are to the dashed diagonal line , the better the model characterizes the data . according to results in figure [ fig:3 ] , our formulation achieves general good performance in characterizing the observed networks ,",
    "even though these data are not generated from a model similar to that described in section [ method ] . differently from our flexible network ",
    "response regression , the massive univariate logistic regressions fail to carefully incorporate network structure and subject - specific variability , obtaining worse performance .",
    "although the substantial dimensionality reduction induced by our low - dimensional factorizations in and provide reassurance against over - fitting issues , we empirically assess this property via out - of - sample edge prediction . in accomplishing this goal",
    ", we perform posterior computation under both models holding out , for two networks  out of four  in each unique predictor value , those hard - to - predict edges characterized by more evident variability across subjects .",
    "for these held - out data  comprising the @xmath104 of the total number of edges in @xmath12  we measure out - of - sample predictive performance via the area under the roc curve ( auc ) , computed by predicting the edges with the posterior mean of their corresponding edge probabilities estimated from the training data according to equation .",
    "explicitly incorporating network structure in modeling of each @xmath12 , while accounting for subject - specific variability , allows us to obtain also accurate out - of - sample inference with an auc equal to @xmath105 . when performing prediction under the massive univariate logistic regressions in we obtain a lower auc equal to @xmath106 .",
    "these results confirm the usefulness of our model as a general and flexible procedure to provide accurate inference on global and local changes in brain networks across traits of interest .",
    "we apply our model to the dataset ` mrn-114 ` , which consists of brain structural connectivity data for @xmath107 subjects along with their cognitive ability measured via fsiq score @xcite .",
    "this score ranges from 86 to 144 with 48 unique values observed in our data . in studying the association between intelligence and brain architecture , previous works either focus on detecting the activated brain regions in cognitive control ( e.g. * ? ? ? * ) or study relationships between intelligence and topological network measures ( e.g. * ? ? ?",
    "we hope to obtain new insights into the neural pathways underlying human intelligence .",
    "in performing posterior computation we consider a total of @xmath100 gibbs samples , setting @xmath108 , @xmath109 , @xmath110 , and @xmath111 . in this case",
    "the algorithm required @xmath1033.5 hours . as in the simulation study , trace - plots for the edge probabilities suggest that convergence is reached after 1,000 burn - in iterations .",
    "we additionally thin the chain by collecting every 4 samples . since the latent dimensions @xmath112 and @xmath51 are unknown , we perform posterior computation for increasing @xmath113 and stop when there is no substantial improvement in out - of - sample edge prediction based on the auc . in computing the auc , we randomly select 20% of the edges and hold them out for a randomly chosen 50% of the subjects . for these",
    "held - out test edges , the auc is computed as discussed in the simulation .",
    "we repeat the procedure 5 times for each setting and report the average auc in table [ tab : auc - different - r ] .",
    "the network - response model having @xmath99 provides a good choice for inference and prediction .",
    "it is additionally worth noticing how all the aucs are very high .",
    "the reason for this result is that a wide set of brain connections are easier to predict in being either almost always present or absent in the subjects under study .",
    ".average auc computed for the test data at varying @xmath112 and @xmath51.[tab : auc - different - r ] [ cols=\"<,<,<,<,<,<\",options=\"header \" , ]     in the following subsections we discuss the improved performance of our procedure  considering @xmath99  in relation to our competitor in , including in inference , prediction and uncertainty quantification .",
    "these relevant improvements are fundamental in refining inference on how network structures change across a continuous trait of interest .      as discussed in section [ intro ] and [ simu ] , restrictive statistical models for how a brain network architecture changes with a trait can lead to substantially biased inference and conclusions .",
    "hence , prior to presenting our findings , we first assess the performance of our statistical model in characterizing the observed brain network data .",
    "consistent with the analyses in section [ simu ] , this is accomplished via posterior predictive checks for relevant topological properties . as shown in figure [ fig:4 ] ,",
    "our model achieves good performance in characterizing the observed network summary measures , substantially improving over our competitor .",
    "this motivates further analyses on how the brain network changes , on average , across fsiq scores .",
    "the cerebrum of the brain is divided into five main anatomical lobes  named frontal , limbic , occipital , parietal and temporal lobes @xcite . in order to provide interpretable inference on how the network structure changes with fsiq",
    ", we focus on the posterior distribution for the trajectories of aggregated connectivity patterns considering possible combinations of hemispheric and lobe membership .",
    "for example , the left plot in figure [ fig:5 ] displays the posterior distribution of the averaged edge probabilities connecting brain regions in different hemispheres , but belonging both to the frontal lobe .",
    "figure [ fig:5 ] shows that the aggregated pathway linking regions in the left and right frontal lobe , as well as the one connecting regions in the frontal lobe with those in the limbic cortex , increase with fsiq .",
    "this result is in line with findings on the role of the frontal lobe in intelligence @xcite . to provide insights on local changes in the brain architecture with fsiq ,",
    "figure [ fig:6 ] highlights the connections whose posterior distributions show evident trends with fsiq .",
    "in particular all the trajectories for the edges highlighted in figure [ fig:6 ] significantly increase with fsiq .",
    "consistent with the results in the left plot of figure [ fig:5 ] , almost all these edges connect regions in opposite hemispheres but belonging both to the frontal lobe .",
    "results in figure [ fig:4 ] are appealing in demonstrating that the substantial dimensionality reduction induced by our model via and , carefully preserves flexibility in characterizing complex brain network structures and their changes with fsiq . to further investigate the benefits induced by our parsimonious representation we assess performance in edge prediction for a challenging scenario holding out  for @xmath114 of the subjects  only the hard - to - predict edges having empirical probability is defined as @xmath115 for each @xmath116 . ]",
    "consistent with the results in the simulation study we obtain an auc equal to @xmath106 , substantially out - performing the auc of @xmath118 for our competitor .",
    "another advantage of our flexible bayesian approach over methods relying on optimization or restrictive hierarchical models is the ability to accurately characterize uncertainty in learning how the brain structure varies with subjects , systematically in relation to a trait and randomly due to unobserved conditions or measurement errors .",
    "we assess performance in uncertainty quantification by evaluating probability calibration in the above prediction task .",
    "in particular , we bin the estimated probabilities for the held - out edges in intervals @xmath119,(0.1,0.2 ] , \\ldots , ( 0.9,1]$ ] and  within each bin  we calculate the proportion of actual edges among those predicted to have an edge probability within that bin .",
    "if the values of these empirical proportions are actually within the bins they refer to , the procedure is well calibrated and properly quantifies uncertainty . according to table [ tab : probability calibration ]",
    "our model has good performance in uncertainty quantification .",
    "> m2.5cmcccccccccc & [ 0 , 0.1 ] & ( 0.1 , 0.2 ] & ( 0.2 , 0.3 ] & ( 0.3 , 0.4 ] & ( 0.4 , 0.5 ] & ( 0.5 , 0.6 ] & ( 0.6 , 0.7]&(0.7 , 0.8]&(0.8 , 0.9 ] & ( 0.9 , 1 ] + bayesian network ",
    "response regression & 0.07 & 0.16 & 0.23 & 0.34 & 0.42 & 0.53 & 0.62 & 0.70 & 0.80&0.91 + massive univariate gp logistic regression&0.23 & 0.27 & 0.33 & 0.39 & 0.45 & 0.50 & 0.59 & 0.62&0.63 & 0.70 +",
    "motivated by a neuroscience study , we developed a novel model to flexibly infer changes in a network - valued random variable with a continuous trait .",
    "the simulation study and the application to learn variations in the brain connectivity architecture across fsiq show substantial improvements in inference , edge prediction and uncertainty quantification .",
    "although we focus on a single trait , the method is trivially generalized to accommodate multiple traits of an individual .",
    "moreover our formulation can be easily adapted to incorporate directed networks via two subsets of latent coordinates  for each brain region  modeling outgoing and incoming edges , respectively .",
    "our procedure also has a broad range of possible applications in social science and econometrics .",
    "although our initial results suggest that binary brain networks already contain valuable information about the individual s brain structure , future research generalizing the proposed model to account for weighted edges , may benefit from the additional information contained in the fiber counts .",
    "we thank joshua t. vogelstein for getting us interested in statistical and computational methods for analysis of brain networks .",
    "we are also grateful to the associate editor and the referees for their valuable comments on a first version of this manuscript .",
    "this work was partially supported by the grant n00014 - 14 - 1 - 0245 of the united states office of naval research ( onr ) , and grant cpda154381/15 of the university of padova , italy .    * supplementary material *",
    "given the priors defined in equations ( 3)(8 ) and based on the plya - gamma data augmentation for bayesian logistic regression , the gibbs sampler for our network - response regression model in ( 1)(2 ) alternates between the following steps .",
    "* update the plya - gamma augmented data for each pair of brain regions @xmath24 in every subject @xmath7 , from @xmath120 for every @xmath116 and @xmath2 . *",
    "sample  for each subject @xmath10  the latent coordinates for her nodes comprising the @xmath49 matrix @xmath66 .",
    "we accomplish this by block updating the elements in each row @xmath121 , @xmath11 of @xmath66  representing the @xmath112 coordinates of node @xmath5 in subject @xmath7  given all the others @xmath122 . recalling equations ( 1)(2 )",
    "we can obtain the full conditional posterior distribution for @xmath123 , by recasting the problem as a bayesian logistic regression with @xmath123 acting as a coefficient vector .",
    "in particular , let @xmath124 where @xmath125 denotes the @xmath126 matrix obtained by removing the @xmath5th row of @xmath66 , while @xmath127 and @xmath128 are @xmath129 vectors obtained by stacking elements @xmath71 and @xmath37 for all the @xmath24 corresponding to pairs @xmath130 such that @xmath131 or @xmath132 , with @xmath133 and ordered consistently with equation ( 11 ) .",
    "recalling equations ( 4)(5 ) , the prior for @xmath123 is @xmath134 , with @xmath135 the @xmath136 matrix of coefficients , @xmath137 the @xmath138 matrix containing the values of the basis functions at @xmath13 and @xmath139 the @xmath140 identity matrix .",
    "hence , the plya - gamma data augmentation for the model ( 11 ) ensures that the full conditional for each row of @xmath66 is @xmath141 with @xmath142 with @xmath143 the @xmath144 diagonal matrix with entries obtained by stacking the plya - gamma augmented data consistently with ( 11 ) .",
    "* sample each shared similarity score @xmath145 , @xmath23 from its gaussian full conditional @xmath146 where @xmath147 and @xmath148 $ ] , with @xmath5 and @xmath6 the nodes corresponding to pair @xmath24 .",
    "* update each basis function @xmath44 , @xmath48 and @xmath39 from its full conditional posterior .",
    "in particular , our gaussian process prior ( 6 ) for the basis functions implies that @xmath149 where @xmath150 are the unique values of @xmath151 and @xmath152 is the gaussian process covariance matrix with @xmath153 .",
    "hence , in updating @xmath154 , let @xmath155 , and @xmath156 standard conjugate posterior analysis provides the following full conditional @xmath157 for each @xmath158 , with @xmath159 * conditioned on the hyperparameters @xmath59 , the gaussian prior on the elements of @xmath135 in equation ( 7 ) yields the following full conditional for each row of @xmath135 : @xmath160 for each @xmath161 , with @xmath162 where @xmath163 . *",
    "the global shrinkage hyperparameters are updated as @xmath164 for each @xmath48 .",
    "* update the subject - specific edge probabilities by applying equation @xmath165 to the posterior samples of @xmath37 and @xmath66 for each @xmath23 and @xmath10 ."
  ],
  "abstract_text": [
    "<S> there is increasing interest in learning how human brain networks vary as a function of a continuous trait , but flexible and efficient procedures to accomplish this goal are limited . </S>",
    "<S> we develop a bayesian semiparametric model , which combines low - rank factorizations and flexible gaussian process priors to learn changes in the conditional expectation of a network - valued random variable across the values of a continuous predictor , while including subject - specific random effects . </S>",
    "<S> the formulation leads to a general framework for inference on changes in brain network structures across human traits , facilitating borrowing of information and coherently characterizing uncertainty . </S>",
    "<S> we provide an efficient gibbs sampler for posterior computation along with simple procedures for inference , prediction and goodness - of - fit assessments . </S>",
    "<S> the model is applied to learn how human brain networks vary across individuals with different intelligence scores . </S>",
    "<S> results provide interesting insights on the association between intelligence and brain connectivity , while demonstrating good predictive performance .    </S>",
    "<S> # 1    _ keywords : _ latent - space model ; low - rank factorization ; gaussian process ; brain networks . </S>"
  ]
}