{
  "article_text": [
    "methods of inference are not new to econometrics . in fact",
    ", one could say that the subject is founded on inference methods .",
    "econophysics , on the other hand , is a much newer idea .",
    "it is based on the premise that some ideas and methods from physics can be applied to economic situations . in this paper",
    "we aim to show how a physics concept such as entropy can be applied to an economic problem .    in 1957",
    ", jaynes @xcite showed that maximizing statistical mechanic entropy for the purpose of revealing how gas molecules were distributed was simply the maximizing of shannon s information entropy @xcite with statistical mechanical information .",
    "the method was true for assigning probabilities regardless of the information specifics .",
    "this idea lead to maxent or his use of the method of maximum entropy for assigning probabilities .",
    "this method has evolved to a more general method , the method of maximum ( relative ) entropy ( mre ) shorejohnson80,skilling88,catichagiffin06 which has the advantage of not only assigning probabilities but _ updating _ them when new information is given in the form of constraints on the family of allowed posteriors .",
    "one of the draw backs of the maxent method was the inability to include data .",
    "when data was present , one used bayesian methods .",
    "the methods were combined in such a way that maxent was used for assigning a prior for bayesian methods , as bayesian methods could not deal with information in the form of constraints , such as expected values .",
    "the main purpose of this paper is to show both general and specific examples of how the mre method can be applied using data and moments .",
    "the numerical example in this paper addresses a recent paper by grendar and judge ( gj ) @xcite where they consider the problem of criterion choice in the context of large deviations ( ld ) . specifically , they attempt to justify the method by owen @xcite in a ld context with a new method of their own .",
    "they support this idea by citing a paper in the econometric literature by kitamura and stutzer @xcite who also use ld  to justify a particular empirical estimator .",
    "we attempt to simplify their ( gj ) initial problem by providing an example that is a bit more practical .",
    "however , our example has the same issue ; what does one do when one has information in the form of an `` average '' of a large data set _ and _ a small sample of that data set",
    "? we will show by example that the ld approach is a special case of our method , the method of maximum ( relative ) entropy    in section 2 we show a general example of updating simultaneously with two different forms of information : moments and data .",
    "the solution resembles bayes rule .",
    "in fact , if there are no moment constraints then the method produces bayes rule _ exactly _ @xcite .",
    "if there is no data , then the maxent solution is produced .",
    "the realization that mre includes not just maxent but also bayes rule as special cases is highly significant .",
    "it implies that mre is _ capable of producing every aspect of orthodox bayesian inference _ and proves the complete compatibility of bayesian and entropy methods .",
    "further , it opens the door to tackling problems that could not be addressed by either the maxent or orthodox bayesian methods individually ; problems in which one has data and moment constraints .    in section 3 we comment on the problem of non - commuting constraints .",
    "we discuss the question of whether they should be processed simultaneously , or sequentially , and in what order .",
    "our general conclusion is that these different alternatives correspond to different states of information and accordingly we expect that they will lead to different inferences .    in section 4 ,",
    "we provide two toy examples that illustrate potential economic problems similar to the ones discussed in gj .",
    "the two examples ( ill - behaved as mentioned in gj ) are solved in detail .",
    "the first example will demonstrate how data and moments can be processed sequentially .",
    "this example is typically how bayesian statistics traditionally uses maxent principles where maxent is used to create a prior for the bayesian formulation .",
    "the second example illustrates a problem that bayes and maxent alone can not handle : simultaneous processing of data and moments .",
    "these two examples will seem trivially different but this is deceiving .",
    "they actually ask and answer two completely different questions .",
    "it is this triviality that is often a source of confusion in bayesian literature and therefore we wish to expose it .    in section 6",
    "we compare a numerical example that is solved by mre and one that is solved by gj s method .",
    "since gj s solution comes out of ld , they rely on asymptotic arguments ; one assumes an infinite sample set which is not necessarily realistic .",
    "the mre method does not need such assumptions to work and therefore can process finite amounts of data well .",
    "however , when mre is taken to asymptotic limits one recovers the same solutions that the large deviation methods produce .",
    "our first concern when using the mre method to update from a prior to a posterior distribution is to define the space in which the search for the posterior will be conducted .",
    "we wish to infer something about the values of one or several quantities , @xmath0 , on the basis of three pieces of information : prior information about @xmath1 ( the prior ) , the known relationship between @xmath2 _ and _ @xmath1 ( the model ) , and the observed values of the data @xmath3 . since we are concerned with both @xmath2 _ and _ @xmath1 , the relevant space is neither @xmath4 nor @xmath5 but the product @xmath6 and our attention must be focused on the joint distribution @xmath7 .",
    "the selected joint posterior @xmath8 is that which maximizes the entropy so that @xmath9 this is the same as minimizing the relative entropy.],@xmath10=-\\int dxd\\theta ~p\\left ( x,\\theta \\right ) \\log \\frac{% p\\left ( x,\\theta \\right ) } { p_{\\text{old}}\\left ( x,\\theta \\right ) } ~ , \\label{entropy}\\]]subject to the appropriate constraints . @xmath11",
    "contains our prior information which we call the _ joint prior_. to be explicit,@xmath12where @xmath13 is the traditional bayesian prior and @xmath14 is the likelihood .",
    "it is important to note that they _ both _ contain prior information .",
    "the bayesian prior is defined as containing prior information . however ,",
    "the likelihood is not traditionally thought of in terms of prior information .",
    "of course it is reasonable to see it as such because the likelihood represents the model ( the relationship between @xmath1 and @xmath15 that has already been established .",
    "thus we consider both pieces , the bayesian prior and the likelihood to be _ prior _ information .",
    "the new information is the _ observed data _ , @xmath16 , which in the mre framework must be expressed in the form of a constraint on the allowed posteriors .",
    "the family of posteriors that reflects the fact that @xmath2 is now known to be @xmath16 is such that@xmath17where @xmath18 is the dirac delta function",
    ". this amounts to an _",
    "infinite _ number of constraints : there is one constraint on @xmath7 for each value of the variable @xmath2 and each constraint will require its own lagrange multiplier @xmath19 . furthermore , we impose the usual normalization constraint , @xmath20and include additional information about @xmath1 in the form of a constraint on the expected value of some function @xmath21 , @xmath22note : an additional constraint in the form of @xmath23 could only be used when it does not contradict the data constraint ( [ data ] ) .",
    "therefore , it is redundant and the constraint would simply get absorbed when solving for @xmath19 .",
    "we also emphasize that constraints imposed at the level of the prior need not be satisfied by the posterior .",
    "what we do here differs from the standard bayesian practice in that we _ require _ the constraint to be satisfied by the posterior distribution .",
    "we proceed by maximizing ( [ entropy ] ) subject to the above constraints .",
    "the purpose of maximizing the entropy is to determine the value for @xmath24 when @xmath25 meaning , we want the value of @xmath24 that is closest to @xmath26 given the constraints .",
    "the calculus of variations is used to do this by varying @xmath27 , i.e. setting the derivative with respect to @xmath24 equal to zero .",
    "the lagrange multipliers @xmath28 and @xmath29 are used so that the @xmath24 that is chosen satisfies the constraint equations .",
    "the actual values are determined by the value of the constraints themselves .",
    "we now provide the detailed steps in this maximization process .",
    "first we setup the variational form with the lagrange multipiers , @xmath30+\\alpha \\left [ \\int dxd\\theta p\\left ( x,\\theta \\right ) -1% \\right ] \\\\",
    "+ \\beta \\left [ \\int dxd\\theta p\\left ( x,\\theta \\right ) f\\left ( \\theta \\right ) -f\\right ] \\\\",
    "+ \\int dx\\lambda ( x)\\left [ \\int d\\theta p\\left ( x,\\theta \\right ) -\\delta \\left ( x - x% % tcimacro{\\u{b4}}% % beginexpansion { \\acute{}}% % endexpansion \\right ) \\right]% \\end{array}% \\right\\ } = 0~.\\]]we expand the entropy function ( [ entropy ] ) , @xmath31 \\\\   + \\beta \\left [ \\int dxd\\theta p\\left ( x,\\theta \\right ) f\\left ( \\theta \\right ) -f\\right ] \\\\",
    "+ \\int dx\\lambda ( x)\\left [ \\int d\\theta p\\left ( x,\\theta \\right ) -\\delta \\left ( x - x% % tcimacro{\\u{b4}}% % beginexpansion { \\acute{}}% % endexpansion \\right ) \\right]% \\end{array}% \\right\\ } = 0~.\\]]next , vary the functions with respect to @xmath32 @xmath33 \\\\",
    "+ \\beta \\left [ \\int dxd\\theta ~\\delta p\\left ( x,\\theta \\right ) f\\left ( \\theta \\right ) \\right ] \\\\",
    "+ \\int dx\\lambda ( x)\\left [ \\int d\\theta ~\\delta p\\left ( x,\\theta \\right ) % \\right]% \\end{array}% \\right\\ } = 0~,\\]]which can be rewritten as @xmath34the terms inside the brackets must sum to zero , therefore we can write , @xmath35or @xmath36    in order to determine the lagrange multipliers , we substitute our solution ( [ posterior 1 ] ) into the various constraint equations .",
    "the constant @xmath37 is eleiminated by substituting ( [ posterior 1 ] ) into ( normalization ) , @xmath38dividing both sides by the constant @xmath39 , @xmath40then substituting back into ( [ posterior 1 ] ) yields @xmath41where @xmath42 in the same fashion , the lagrange multipliers @xmath19 are determined by substituting ( [ posterior 2 ] ) into ( [ data])@xmath43or@xmath44the posterior now becomes@xmath45where @xmath46    the lagrange multiplier @xmath47 is determined by first substituting ( posterior - both ) into ( [ moment]),@xmath48 f\\left ( \\theta \\right ) = f~.\\]]integrating over @xmath2 yields,@xmath49where @xmath50 . now @xmath51 can be determined rewriting ( [ f1 ] ) as @xmath52    the final step is to marginalize the posterior , @xmath8 over @xmath2 to get our updated probability,@xmath53additionally , this result can be rewritten using the product rule ( @xmath54 ) as@xmath55where @xmath56 the right side resembles bayes theorem , where the term @xmath57 is the standard bayesian likelihood and @xmath58 is the prior .",
    "the exponential term is a _ modification _ to these two terms . in an effort to put some names to these pieces",
    "we will call the standard bayesian likelihood the _ likelihood _ and the exponential part the _ likelihood modifier _ so that the product of the two gives the _",
    "modified likelihood_. the denominator is the normalization or _",
    "marginal modified likelihood_. notice when @xmath59 ( no moment constraint ) we recover bayes rule . for @xmath60 bayes rule",
    "is modified by a canonicalexponential factor .",
    "when we are confronted with several constraints , such as in the previous section , we must be particularly cautious . in what order",
    "should they be processed ? or should they be processed at the same time ?",
    "the answer depends on the nature of the constraints and the question being asked giffincaticha07 .",
    "we refer to constraints as _ commuting _ when it makes no difference whether they are processed simultaneously or sequentially .",
    "the most common example of commuting constraints is bayesian updating on the basis of data collected in multiple experiments . for the purpose of inferring @xmath1",
    "it is well known that the order in which the observed data @xmath61 is processed does not matter .",
    "the proof that mre is completely compatible with bayes rule implies that data constraints implemented through @xmath62 functions , as in ( [ data ] ) , commute just as they do in bayes .",
    "it is important to note that when an experiment is repeated it is common to refer to the value of @xmath2 in the first experiment and the value of @xmath2 in the second experiment .",
    "this is a dangerous practice because it obscures the fact that we are actually talking about _ two _ separate variables",
    ". we do not deal with a single @xmath2 but with a composite @xmath63 and the relevant space is @xmath64 .",
    "after the first experiment yields the value @xmath65 , represented by the constraint @xmath66 , we can perform a second experiment that yields @xmath67 and is represented by a second constraint @xmath68 . these constraints @xmath69 and @xmath70 commute because they refer to _ different _ variables @xmath71 and @xmath72    as a side note , use of a @xmath62 function has been criticized in that by implementing it , the probability is completely constrained , thus it can not be updated by future information .",
    "this is certainly true ! an experiment ,",
    "once performed and its outcome observed , can not be _ un - performed _ and its result can not be _ un - observed _ by subsequent experiments .",
    "thus , imposing one constraint does not imply a revision of the other .    in general constraints need not commute and when this is the case the order in which they are processed is critical .",
    "for example , suppose the prior is @xmath73 and we receive information in the form of a constraint , @xmath74 .",
    "to update we maximize the entropy @xmath75 $ ] subject to @xmath74 leading to the posterior @xmath76 as shown in fig 1 .",
    "next we receive a second piece of information described by the constraint @xmath77 . at this point",
    "we can proceed in essentially two different ways :    \\a ) sequential updating -    having processed @xmath78 , we use @xmath76 as the current prior and maximize @xmath79 $ ] subject to the new constraint @xmath77 .",
    "this leads us to the posterior @xmath80 .",
    "\\b ) simultaneous updating -    use the original prior @xmath26 and maximize @xmath75 $ ] subject to both constraints @xmath78 and @xmath77 simultaneously .",
    "this leads to the posterior @xmath81 . at first sight",
    "it might appear that there exists a third possibility of  simultaneous updating : ( c ) use @xmath76 as the current prior and maximize @xmath82 $ ] subject to both constraints @xmath74 and @xmath77 simultaneously .",
    "fortunately , and this is a valuable check for the consistency of the me method , it is easy to show that case ( c ) is equivalent to case ( b ) .",
    "whether we update from @xmath26 or from @xmath83 the selected posterior is @xmath81 .    to decide which path ( a ) or ( b ) is appropriate , we must be clear about how the mre method treats constraints .",
    "the mre machinery interprets a constraint such as @xmath78 in a very mechanical way : all distributions satisfying @xmath84 are in principle allowed and all distributions violating @xmath78 are ruled out .",
    "updating to a posterior @xmath76 consists precisely in revising those aspects of the prior @xmath26 that disagree with the new constraint @xmath78 . however , there is nothing final about the distribution @xmath76 .",
    "it is just the best we can do in our current state of knowledge and we fully expect that future information may require us to revise it further . indeed , when new information @xmath77 is received we must reconsider whether the original @xmath74 remains valid or not .",
    "are _ all _ distributions satisfying the new @xmath77 really allowed , even those that violate @xmath78 ? if this is the case then the new @xmath77 takes over and we update from @xmath76 to @xmath85 .",
    "the constraint @xmath78 may still retain some lingering effect on the posterior @xmath80 through @xmath86 but in general @xmath78 has now become obsolete .    alternatively , we may decide that the old constraint @xmath78 retains its validity .",
    "the new @xmath77 is not meant to revise @xmath78 but to provide an additional refinement of the family of allowed posteriors . in this case",
    "the constraint that correctly reflects the new information is not @xmath77 but the more restrictive space where @xmath78 and @xmath77 overlap .",
    "the two constraints should be processed simultaneously to arrive at the correct posterior @xmath81 .    to summarize : sequential updating is appropriate when old constraints become obsolete and are superseded by new information ; simultaneous updating is appropriate when old constraints remain valid .",
    "the two cases refer to different states of information and therefore _ we expect _ that they will result in different inferences .",
    "these comments are meant to underscore the importance of understanding what information is being processed ; failure to do so will lead to errors that do not reflect a shortcoming of the mre method but rather a misapplication of it .",
    "this is an example of a problem using the mre method : .",
    "the general background information is that a factory makes @xmath87 different kinds of bouncy balls . for reference",
    ", they assign each different kind with a number , @xmath88 .",
    "they ship large boxes of them out to stores .",
    "unfortunately , there is no mechanism that regulates how many of each ball goes into the boxes , therefore we do not know the amount of each kind of ball in any of the boxes .    for this problem",
    "we are informed that the company does know the average of all the kinds of balls , @xmath89 that is produced by the factory over the time that they have been in existence .",
    "this is information about the _ factory_. by using this information with mre we get what one would get with the old maxent method , a distribution of balls for the whole factory .",
    "however , we would like to know the probability of getting a certain kind of ball in a particular box .",
    "therefore , we are allowed to randomly select a few balls , @xmath90 from the particular box in question and count how many of each kind we get , @xmath91 ( or perhaps we simply open the box and look at the balls on the surface ) .",
    "this is information about the _",
    "particular box_. now let us put the above example in a more mathematical format .",
    "let the set of possible outcomes be represented by , @xmath92 from a sample where the total number of balls , @xmath93 for the me method to work .",
    "we simply wish to use the description of the problem that is common in information - theoretic examples .",
    "it must be strongly noted however that in general a sample average is not an expectation value . ] and whose sample average is @xmath94 further , let us draw a _ data _ sample of size @xmath95 from a particular subset of the original sample , @xmath96 where @xmath97 and whose outcomes are counted and represented as @xmath98 where @xmath99 .",
    "we would like to determine the probability of getting _ any _ particular type in one draw ( @xmath100 ) out of the subset given the information . to do this we start with the appropriate joint entropy,@xmath10~\\text{=}-\\sum\\limits_{m}\\int d\\theta p(m,\\theta |n)\\log   \\frac{p\\left ( m,\\theta |n\\right ) } { p_{\\text{old}}(m,\\theta |n)}. \\label{entropy p1}\\]]we then maximize this entropy with respect to @xmath101 to process the first piece of information that we have which is the moment constraint , @xmath74 that is related to the factory,@xmath102subject to normalization , where @xmath103 @xmath104 and where function in both ( 18 )  and ( 19 ) are used to clarify the summation notaion used in ( 16 ) .",
    "they are not _ information _",
    "constraints.on @xmath24 as in ( 18 ) and later ( 25).]@xmath105and@xmath106this yields,@xmath107where the normalization constant @xmath108 and the lagrange multiplier @xmath109 are determined from@xmath110we need to determine what to use for our joint prior,@xmath111 in our problem .",
    "the mathematical representation of the  situation where we wish to know the probability of selecting @xmath112 balls of the @xmath113 type from a sample of @xmath90 balls of @xmath87-types is simply the multinomial distribution .",
    "therefore , the equation that we will use for our model , the _ likelihood , _",
    "@xmath114 is,@xmath115since at this point we are completely ignorant of @xmath116 we use a _ prior _ that is flat , thus @xmath117 _",
    "constant_. being a constant , the prior can come out of the integral and cancels with the same constant in the numerator .",
    "( also , the particular form of @xmath118 is not important for our current purpose so for the sake of definiteness we can choose it flat for our example .",
    "there are most likely better choices for priors , such as a jeffrey s prior . )",
    "thus , after marginalizing over @xmath119 the joint distribution ( [ joint p1 ] ) can be rewritten as@xmath120    now we wish to process the next piece of information which is the data constraint,@xmath121here we use a kronecker delta function since @xmath122 is discrete in this example .",
    "our goal is to infer the @xmath1 that apply to our particular box .",
    "the original constraint @xmath78 applies to the whole factory while the new constraint @xmath77 refers to the actual box of interest and thus takes precedence over @xmath123 as @xmath124 we expect @xmath78 to become less and less relevant .",
    "therefore the two constraints should be processed sequentially .",
    "we maximize again with our new information which yields,@xmath125marginalizing over @xmath122 and using ( [ p1 ] ) the final posterior for @xmath1 is@xmath126where@xmath127    those familiar with using maxent and bayes will undoubtedly recognize that ( [ posterior a ] ) is precisely the result obtained by using maxent to obtain a prior , in this case @xmath128 given in ( [ p1 ] ) , and then using bayes rule to take the data into account .",
    "this familiar result has been derived in detail for two reasons : first , to reassure the readers that mre does reproduce the standard solutions to standard problems and second , to establish a contrast with the example discussed next .",
    "note : since the constraints @xmath78 and @xmath77 do not commute one will get a different result if they are processed in a different order .",
    "this is another example of a problem using the mre method : .",
    "the general background information is the same as the previous example . for this problem",
    "we are informed that the company knows the average of all the kinds of balls , @xmath89 in _ each _ _ box_. by using this information with mre we get what one would get with the old maxent method , a distribution of balls for each box .    however , we still would like to know the probability of getting a certain kind of ball in a particular box and we are allowed to randomly select a few balls , @xmath90 from the particular box in question once again . since both of these pieces of information apply to the _ same _ box , they must be processed simultaneously . in other words ,",
    "both constraints must hold , always .",
    "we proceed as in the first example by maximizing ( [ entropy p1 ] ) subject to normalization and the following constraints simultaneously,@xmath129(notice @xmath130 because they are two difference pieces of information ) and@xmath131this yields,@xmath132where@xmath133and@xmath134    this looks like the sequential case ( [ posterior a ] ) , but there is a crucial difference : @xmath135 and @xmath136 . in the sequential updating case , the multiplier @xmath137 is chosen so that the intermediate @xmath76 satisfies @xmath78 while the posterior @xmath85 only satisfies @xmath77 . in the simultaneous updating case",
    "the multiplier @xmath47 is chosen so that the posterior @xmath81 satisfies both @xmath78 and @xmath77 or @xmath138 .",
    "ultimately , the two distributions @xmath139 are different because they refer to different problems . for more examples using this method see giffincaticha07 .",
    "the purpose of this section is two fold : first , we would like to provide a numerical example of a mre solution .",
    "second , we wish to examine a current , relevant econometric solution proposed by gj in @xcite using the method of types , specifically large deviation theory , for an `` ill - posed '' problem that is similar to the one discussed in section 5 . this solution will be compared with a solution using mre .    to summarize the problem once again :",
    "the factory makes @xmath87 different kinds of bouncy balls and for reference , they assign each different type with a number , @xmath140 .",
    "we are informed that the company knows the expected type of ball , @xmath89 in each box over the time that they have been in existence .",
    "we would like a better idea of how many balls are in each box so we randomly select a few balls , @xmath90 from a particular box and count how many of each type we get , @xmath91 .    or stated in a more mathematical format : let the set of possible outcomes of a be represented by , @xmath141 from a sample where the total number of balls , @xmath142 . and where the average of the types of balls is @xmath94 further , let us draw a _ data _ sample of size @xmath95 from the original sample ,",
    "whose outcomes are counted and represented as @xmath143 where @xmath99 . the problem becomes ill - posed when the sample average of the counts@xmath144significantly deviates from the expected average of the types , @xmath94    we would like to determine the probability of getting _ any _ particular outcome in one draw ( @xmath100 ) given the information .      in @xcite a form of sanov s theorem is used . here",
    "we give a brief description of sanov s theorem .",
    "it is not intended to be a proof or exhaustive .",
    "it is simply shown to give a general indication of the basis for the solution in @xcite .",
    "the key equation is ( [ econ sanov solution ] ) . for a more detailed proof and explanation",
    "see @xcite .",
    "sanov s theorem -    let @xmath145 be independent and identically distributed ( i.i.d . ) with values in an arbitrary set @xmath146 with common distribution @xmath147 .",
    "let @xmath148 be a set of probability distributions .",
    "then,@xmath149where @xmath150is the distribution in @xmath151 that is closest to @xmath152 in the relative entropy or information divergence , @xmath153 and @xmath90 is the number of types . if in addition , the set @xmath151 is the closure of its interior , @xmath154the two equations become equal in the asymptotic limit .",
    "essentially what this theorem says is that in the asymptotic limit , the frequency of the sample @xmath152 , can be used to produce an estimate , @xmath155 of the `` true '' probability , @xmath156 by way of minimizing the relative entropy ( info divergence ) .    for our problem ,",
    "the solution for the probability using sanov is of the form , @xmath157where @xmath152 for our problem is the frequency of the counts , @xmath158 and @xmath159 is a lagrange multiplier that is determined by the sample average ( sample avg ) , not an expected value as in our method .",
    "this solution seems very similar to our general solution using the mre method ( [ posterior b ] ) in which we also minimize an entropy ( maximize our negative relative entropy ) .",
    "we could even think of @xmath152 as a kind of joint prior and likelihood . however , there are many differences in the two methods , but the most glaring is that the gj solution is only valid in the asymptotic case .",
    "we are not handicapped by this when mre is used .",
    "we illustrate the differences between the methods be examining a specific version of the above problem : let the there be three kinds of balls labeled 1 , 2 and 3 .",
    "so for this problem we have @xmath160 @xmath161 and @xmath162 further , we are given information regarding the expected value of each box , @xmath163 for our example this value will be , @xmath164 notice that this implies that on the average there are more @xmath165 s in each box .",
    "next we take a sample of one of the boxes where @xmath166 @xmath167 and @xmath168    using the mre method in the same way that we have in each of the previous sections , we arrive at a posterior solution after maximizing the proper entropy subject to the constraints,@xmath169where the lagrange multiplier @xmath47 was determined using newton s method on the equation ( [ f b ] ) and found to be @xmath170 we show the relationship between @xmath47 and @xmath89 in fig 2 .",
    "this result is then put into our calculation of @xmath171 so that @xmath172 two plots are provided that show the marginal distributions of @xmath173 and @xmath174 ( see fig 3 ) .",
    "choose _ to have a single number represent @xmath175 and @xmath176 a popular choice is the mean , which is calculated for each marginal ( see appendix for details),@xmath177    we now use the gj solution ( [ econ sanov solution ] ) to compute the `` probabilities '' .",
    "we use the frequencies , @xmath158 for @xmath152 or @xmath178 @xmath179 and @xmath180 and _ assume _ that @xmath89 represents the sample average for the entire population of balls .",
    "this produces the following results:@xmath181clearly the results are very close , however , there are several drawbacks to using the sanov approach .",
    "the first is that @xmath155 is estimated on the basis of a frequency , @xmath152 that is being used to represent an estimate of the entire population .",
    "as is well known this can only be the case when @xmath182 mre needs not make such assumptions . similarly mre can incorporate actual expectation values , not sample averages disguised as them .",
    "second , the correct distribution to be used is the multinomial when one is counting , not the frequencies of the observables .",
    "third , and practically most important , because the mre solution produces a probability distribution , one can take into account fluctuations .",
    "a single number would not give any indication as to the uncertainty of the estimate . with our method",
    ", one has the choice of which estimator one would like to use .",
    "perhaps the distribution is almost flat .",
    "then our method would indicate that almost any choice is equally likely .",
    "there is an underlying theme here : probabilities are not equivalent to frequencies _ except _ in the asymptotic case .",
    "therefore , if one wishes to know the probable outcome of a problem in all cases , use mre .",
    "the realization that the mre method incorporates maxent and bayes rule as special cases has allowed us to go beyond bayes rule and maxent methods to process both data and expected value constraints simultaneously .",
    "therefore , we would like to emphasize that anything one can do with bayesian or maxent methods , one can now do with mre . additionally , in mre one now",
    "has the ability to apply additional information that bayesian or maxent methods could not .",
    "further , any work done with bayesian techniques can be implemented into the mre method directly through the joint prior .",
    "it is not uncommon to claim that the non - commutability of constraints represents a _ problem _ for the mre method .",
    "processing constraints in different orders might lead to different inferences .",
    "we have argued that on the contrary , the information conveyed by a particular sequence of constraints is not the same information conveyed by the same constraints in different order .",
    "since different informational states should in general lead to different inferences , the way mre processes non - commuting constraints should not be regarded as a _ shortcoming _ but rather as a _ feature _ of the method .",
    "two specific econometric examples were solved in detail to illustrate the application of the method .",
    "these cases can be used as templates for real world problems .",
    "numerical results were obtained to illustrate explicitly how the method compares to other methods that are currently employed .",
    "the mremethod was shown to be superior in that it did not need to make asymptotic assumptions to function and allows for fluctuations .",
    "it must be emphasized that in the asymptotic limit , the mre form is analogous to sanov s theorem .",
    "however , this is only one special case .",
    "the mre method is more robust in that it can also be used to solve traditional bayesian problems .",
    "in fact it was shown that if there is no moment constraint one recovers bayes rule .",
    "* acknowledgements :",
    "* i would like to acknowledge valuable discussions with a. caticha , m. grendar , c. rodrguez and e. scalas .",
    "here we show how the means @xmath183 and @xmath184 were calculated explicitly in the numerical solutions section .",
    "the program maple was used to calculate all results after the integral from was created .        where @xmath186 differs from @xmath187 in ( [ zeta b ] ) only by a combinatorial coefficient ,",
    "@xmath188a brute force calculation gives @xmath186 as a nested hypergeometric series , @xmath189where each @xmath190 is written as a sum of @xmath191 functions ,      where @xmath193 the index @xmath194 takes all values from @xmath195 to @xmath196 and the other symbols are defined as follows : @xmath197 , @xmath198 and@xmath199with @xmath200 .",
    "the terms that have indices @xmath201 are equal to zero ( i.e. @xmath202 etc . ) .",
    "a few technical details are worth mentioning : first , one can have singular points when @xmath203 . in these cases",
    "the sum must be evaluated in the limit as @xmath204 second , since @xmath205 and @xmath206 are positive integers the gamma functions involve no singularities .",
    "lastly , the sums converge because @xmath207 . the normalization for the first example ( [ z2 ] )",
    "can be calculated in a similar way .    specifically for ( [ mre posterior ] ) , the lagrange multiplier @xmath47 was determined using newton s method on the equation ( [ f b ] ) and found to be @xmath170 this result is then put into ( [ zc ] ) in order to attain @xmath208 next , the means were calculated by increasing @xmath209 and @xmath210 then recalculating so that @xmath211    currently , for small values of @xmath87 ( less than 10 , depending on memory ) it is feasible to evaluate the nested sums numerically ; for larger values of @xmath87 it is best to evaluate the integral for @xmath186 using sampling methods .    99 e. t. jaynes , phys . rev . *",
    "106 * , 620 and * 108 * , 171 ( 1957 ) ; r. d. rosenkrantz ( ed . ) , _",
    "e. t. jaynes : papers on probability , statistics and statistical physics _",
    "( reidel , dordrecht , 1983 ) ; e. t. jaynes , _ probability theory : the logic of science _ ( cambridge university press , cambridge , 2003 ) .          a. caticha and a. giffin ,",
    "updating probabilities , _ bayesian inference and maximum entropy methods in science and engineering _ , ali mohammad - djafari ( ed . ) aip conf .",
    "proc . * 872 * , 31 ( 2006 ) ( http://arxiv.org/abs/physics/0608185 ) .",
    "a. giffin and a. caticha , updating probabilities with data and moments , _ bayesian inference and maximum entropy methods in science and engineering _ , kevin knuth et al(eds . ) , aip conf .",
    "* 954 * , 74 ( 2007 ) ( http://arxiv.org/abs/0708.1593 ."
  ],
  "abstract_text": [
    "<S> econophysics , is based on the premise that some ideas and methods from physics can be applied to economic situations . </S>",
    "<S> we intend to show in this paper how a physics concept such as entropy can be applied to an economic problem . in so doing , we demonstrate how information in the form of observable data and moment constraints are introduced into the method of maximum relative entropy ( mre ) . </S>",
    "<S> a general example of updating with data and moments is shown . </S>",
    "<S> two specific econometric examples are solved in detail which can then be used as templates for real world problems . </S>",
    "<S> a numerical example is compared to a large deviation solution which illustrates some of the advantages of the mre method .    </S>",
    "<S> econophyisics , econometrics , entropy , maxent , bayes </S>"
  ]
}