{
  "article_text": [
    "calibration is ubiquitous in all fields of science and engineering . it is an essential step to guarantee that the devices measure accurately what scientists and engineers want . if sensor devices are not properly calibrated , their measurements are likely of little use to the application .",
    "while calibration is mostly done by specialists , it often can be expensive , time - consuming and sometimes even impossible to do in practice .",
    "hence , one may wonder whether it is possible to enable machines to calibrate themselves automatically with a smart algorithm and give the desired measurements .",
    "this leads to the challenging field of _ self - calibration _ ( or _ blind calibration _ ) .",
    "it has a long history in imaging sciences , such as camera self - calibration  @xcite , blind image deconvolution  @xcite , self - calibration in medical imaging  @xcite , and the well - known phase retrieval problem ( phase calibration )  @xcite .",
    "it also plays an important role in signal processing  @xcite and wireless communications  @xcite .",
    "self - calibration is not only a challenging problem for engineers , but also for mathematicians .",
    "it means that one needs to estimate the calibration parameter of the devices to adjust the measurements as well as recover the signal of interests .",
    "more precisely , many self - calibration problems are expressed in the following mathematical form , @xmath4 where @xmath5 is the observation , @xmath6 is a partially unknown sensing matrix , which depends on an unknown parameter @xmath7 and @xmath8 is the desired signal .",
    "an uncalibrated sensor / device directly corresponds to  _ imperfect sensing _ \" , i.e. , uncertainty exists within the sensing procedure and we do not know everything about @xmath6 due to the lack of calibration .",
    "the purpose of self - calibration is to resolve the uncertainty i.e. , to estimate @xmath7 in @xmath6 and to recover the signal @xmath8 at the same time .",
    "the general model   is too hard to get meaningful solutions without any further assumption since there are many variants of the general model under different settings . in",
    ", @xmath6 may depend on @xmath7 in a nonlinear way , e.g. , @xmath7 can be the unknown orientation of a protein molecule and @xmath8 is the desired object  @xcite ; in phase retrieval , @xmath7 is the unknown phase information of the fourier transform of the object  @xcite ; in direction - of - arrival estimation @xmath7 represents unknown offset , gain , and phase of the sensors  @xcite .",
    "hence , it is impossible to resolve every issue in this field , but we want to understand several scenarios of self - calibration which have great potential in real world applications . among all the cases of interest , we assume that @xmath6 _ linearly _ depends on the unknown @xmath7 and will explore three different types of self - calibration models that are of considerable practical relevance .",
    "however , even for linear dependence , the problem is already quite challenging , since in fact we are dealing with _ bilinear ( nonlinear ) inverse problems_. all those three models have wide applications in imaging sciences , signal processing , wireless communications , etc . , which will be addressed later .",
    "common to these applications is the need for _ fast _ self - calibration algorithms , which ideally should be accompanied by theoretical performance guarantees .",
    "we will show under certain cases , these self - calibration problems can be solved by _ linear least squares _ exactly and efficiently if no noise exists , which is guaranteed by rigorous mathematical proofs .",
    "moreover , we prove that the solution is also robust to noise with tools from random matrix theory .      by assuming that @xmath6 linearly depends on @xmath7 ,   becomes a bilinear inverse problem , i.e. , we want to estimate @xmath7 and @xmath8 from @xmath5 , where @xmath5 is the output of a bilinear map from @xmath9 bilinear inverse problems , due to its importance , are getting more and more attentions over the last few years . on the other hand , they are also notoriously difficult to solve in general .",
    "bilinear inverse problems are closely related to low - rank matrix recovery , see  @xcite for a comprehensive review",
    ". there exists extensive literature on this topic and it we could not possible do justice to all these contributions . instead we will only highlight some of the works which have inspired us .    blind deconvolution might be the most important examples of bilinear inverse problems  @xcite , i.e. , recovering @xmath10 and @xmath11 from @xmath12 , where  @xmath13 \" stands for convolution .",
    "if both @xmath10 and @xmath11 are inside known low - dimensional subspaces , the blind deconvolution can be rewritten as @xmath14 , where @xmath15 , @xmath16 and  @xmath17 \" denotes the fourier transform . in the inspiring work  @xcite , ahmed , romberg and recht apply the  lifting \" techniques  @xcite and convert the problem into estimation of rank-1 matrix @xmath18 .",
    "it is shown that solving a convex relaxation enables recovery of @xmath18 under certain choices of @xmath19 and @xmath20 .",
    "following a similar spirit ,  @xcite uses  lifting \" combined with a convex approach to solve the scenarios with sparse @xmath8 and  @xcite studies the so called  blind deconvolution and blind demixing \" problem .",
    "the other line of blind deconvolution follows a nonconvex optimization approach  @xcite . in  @xcite ,",
    "ahmed , romberg and krahmer , using tools from generic chaining , obtain local convergence of a sparse power factorization algorithm to solve this blind deconvolution problem when @xmath21 and @xmath8 are sparse and @xmath19 and @xmath20 are gaussian random matrices , . under the same setting as  @xcite , lee et al .",
    "@xcite propose a projected gradient descent algorithm based on matrix factorizations and provide a convergence analysis to recover sparse signals from subsampled convolution .",
    "however , this projection step can be hard to implement . as an alternative",
    ", the expensive projection step is replaced by a heuristic approximate projection , but then the global convergence is not fully guaranteed .",
    "both  @xcite achieve nearly optimal sampling complexity .",
    "@xcite proves global convergence of a gradient descent type algorithm when @xmath19 is a deterministic fourier type matrix and @xmath20 is gaussian .",
    "results about identifiability issue of bilinear inverse problems can be found in  @xcite .",
    "another example of self - calibration focuses on the setup @xmath22 , where @xmath23 .",
    "the difference from the previous model consists in replacing the subspace assumption by multiple measurements .",
    "there are two main applications of this model .",
    "one application deals with blind deconvolution in an imaging system which uses randomly coded masks  @xcite .",
    "the measurements are obtained by ( subsampled ) convolution of an unknown blurring function @xmath24 with several random binary modulations of one image . both  @xcite and  @xcite developed convex relaxing approaches ( nuclear norm minimization ) to achieve exact recovery of the signals and the blurring function .",
    "the other application is concerned with calibration of the unknown gains and phases @xmath24 and recovery of the signal @xmath8 , see e.g.  @xcite .",
    "cambareri and jacques propose a gradient descent type algorithm in  @xcite and show convergence of the iterates by first constructing a proper initial guess .",
    "an empirical study is given in  @xcite when @xmath8 is sparse by applying an alternating hard thresholding algorithm .",
    "recently ,  @xcite study the blind deconvolution when inputs are changing .",
    "more precisely , the authors consider @xmath25 where each @xmath26 belongs to a different known subspace , i.e. , @xmath27 .",
    "they employ a similar convex approach as in  @xcite to achieve exact recovery with number of measurements close to the information theoretic limit .",
    "an even more difficult , and from a practical viewpoint highly relevant , scenario focuses on _ self - calibration from multiple snapshots _  @xcite . here",
    ", one wishes to recover the unknown gains / phases @xmath23 and a signal matrix @xmath28 $ ] from @xmath29 . for this model , the sensing matrix @xmath20",
    "is fixed throughout the sensing process and one measures output under different snapshots @xmath30 .",
    "one wants to understand under what conditions we can identify @xmath24 and @xmath30 jointly . if @xmath20 is a fourier type matrix , this model has applications in both image restoration from multiple filters  @xcite and also network calibration  @xcite .",
    "we especially benefitted from work by gribonval and coauthors  @xcite , as well as by balzano and nowak  @xcite .",
    "the papers  @xcite study the noiseless version of the problem by solving a linear system and  @xcite takes a total least squares approach in order to obtain empirically robust recovery in the presence of noise .",
    "if each @xmath31 is sparse , this model becomes more difficult and gribonval et al .",
    "@xcite give a thorough numerical study .",
    "very recently ,  @xcite gave a theoretic result under certain conditions .",
    "this calibration problem is viewed as a special case of the dictionary learning problem where the underlying dictionary @xmath32 possesses some additional structure .",
    "the idea of transforming a blind deconvolution problem into a linear problem can also be found in  @xcite , where the authors analyze a certain non - coherent wireless communication scenario .      in our work",
    ", we consider three different models of self - calibration , namely , @xmath22 , @xmath33 and @xmath34 .",
    "detailed descriptions of these models are given in the next section .",
    "we do not impose any sparsity constraints on @xmath8 or @xmath31 .",
    "we want to find out @xmath31 ( or @xmath8 ) and @xmath24 when @xmath35 ( or @xmath5 ) and @xmath36 ( or @xmath20 ) are given .",
    "roughly , they correspond to the models in  @xcite respectively . though all of the three models belong to the class of bilinear inverse problems , we will prove that simply solving _ linear least squares _ will give solutions to all those models exactly and robustly for invertible @xmath24 and for several useful choices of @xmath20 and @xmath37 moreover , the sampling complexity is nearly optimal ( up to poly - log factors ) with respect to the information theoretic limit ( degree of freedom of unknowns ) .",
    "as mentioned before , our approach is largely inspired by  @xcite and  @xcite ; there the authors convert a bilinear inverse problem into a linear problem via a proper transformation .",
    "we follow a similar approach in our paper .",
    "the paper  @xcite provides an extensive empirical study , but no theoretical analysis .",
    "nowak and balzano , in  @xcite provide numerical simulations as well as theoretical conditions on the number of measurements required to solve the noiseless case .",
    "our paper goes an important step further : on the one hand we consider more general self - calibration settings . and",
    "on the other hand we provide a rigorous theoretical analysis for recoverability and , perhaps most importantly , stability theory in the presence of measurement errors . owing to the simplicity of our approach and the structural properties of the underlying matrices , our framework yields self - calibration algorithms that are numerically extremely efficient , thus potentially allowing for deployment in applications where real - time self - calibration is needed .",
    "we introduce notation which will be used throughout the paper .",
    "matrices are denoted in boldface or a calligraphic font such as @xmath38 and @xmath39 ; vectors are denoted by boldface lower case letters , e.g.  @xmath40 the individual entries of a matrix or a vector are denoted in normal font such as @xmath41 or @xmath42 for any matrix @xmath38 , @xmath43 denotes its operator norm , i.e. , the largest singular value , and @xmath44 denotes its the frobenius norm , i.e. , @xmath45 . for any vector @xmath46 , @xmath47 denotes its euclidean norm . for both matrices and vectors , @xmath48 and @xmath49 stand for the transpose of @xmath38 and @xmath46 respectively while @xmath50 and @xmath51 denote their complex conjugate transpose .",
    "we equip the matrix space @xmath52 with the inner product defined by @xmath53 a special case is the inner product of two vectors ,",
    "i.e. , @xmath54 for a given vector @xmath55 , @xmath56 represents the diagonal matrix whose diagonal entries are given by the vector @xmath55 .",
    "@xmath57 is an absolute constant and @xmath58 is a constant which depends linearly on @xmath59 , but on no other parameters . @xmath60 and @xmath61 always denote the @xmath62 identity matrix and a column vector of  @xmath63 \" in @xmath64 respectively . and",
    "@xmath65 and @xmath66 stand for the standard orthonormal basis in @xmath67 and @xmath68 respectively . ",
    "@xmath13 \" is the circular convolution and  @xmath69 \" is the kronecker product .",
    "the paper is organized as follows .",
    "the more detailed discussion of the models under consideration and the proposed method will be given in section  [ s : model ] .",
    "section  [ s : main ] presents the main results of our paper and we will give numerical simulations in section  [ s : numerics ] .",
    "section  [ s : proof ] contains the proof for each scenario .",
    "we collect some useful auxiliary results in the appendix .",
    "this section is devoted to describing three different models for self - calibration in detail .",
    "we will also explain how those bilinear inverse problems are reformulated and solved via linear least squares .",
    "[ [ self - calibration - from - repeated - measurements ] ] self - calibration from repeated measurements + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    suppose we are seeking for information with respect to an unknown signal @xmath70 with several randomized linear sensing designs . throughout this procedure ,",
    "the calibration parameter @xmath24 remains the same for each sensing procedure .",
    "how can we recover the signal @xmath70 and @xmath24 simultaneously ?",
    "let us make it more concrete by introducing the following model , @xmath71 where @xmath72 is a diagonal matrix and each @xmath73 is a measurement matrix . here",
    "@xmath35 and @xmath36 are given while @xmath24 and @xmath70 are unknown .",
    "for simplicity , we refer to the setup   as  _ self - calibration from repeated measurements _ \" .",
    "this model has various applications in self - calibration for imaging systems  @xcite , networks  @xcite , as well as in blind deconvolution from random masks  @xcite .",
    "[ [ blind - deconvolution - via - diverse - inputs ] ] blind deconvolution via diverse inputs + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    suppose that one sends several different signals through the same unknown channel , and each signal is encoded differently .",
    "namely , we are considering @xmath74 how can one estimate the channel and each signal jointly ?",
    "in the frequency domain , this ",
    "_ blind deconvolution via diverse inputs _ \"  @xcite problem can be written as ( with a bit abuse of notation ) , @xmath75 where @xmath72 and @xmath73 are the fourier transform of @xmath10 and @xmath76 respectively .",
    "we aim to recover @xmath30 and @xmath24 from @xmath77    [ [ self - calibration - from - multiple - snapshots ] ] self - calibration from multiple snapshots + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    suppose we take measurements of several signals @xmath30 with the same set of design matrix @xmath32 ( i.e. , each sensor corresponds one row of @xmath20 and has an unknown complex - valued calibration term @xmath78 ) .",
    "when and how can we recover @xmath24 and @xmath30 simultaneously ?",
    "more precisely , we consider the following model of _ self - calibration model from multiple snapshots _ : @xmath79 here @xmath23 is an unknown diagonal matrix , @xmath80 is a sensing matrix , @xmath30 are @xmath81 unknown signals and @xmath82 are their corresponding observations .",
    "this multiple snapshots model has been used in image restoration from multiple filters  @xcite and self - calibration model for sensors  @xcite .      throughout our discussion , we assume that @xmath24 is _ invertible _ , and we let @xmath83 . here , @xmath23 stands for the calibration factors of the sensor(s )  @xcite and hence it is reasonable to assume invertibility of @xmath24 . for",
    ", if a sensor s gain were equal to zero , then it would not contribute any measurements to the observable @xmath5 , in which case the associated entry of @xmath5 would be zero .",
    "but then we could simply discard that entry and consider the correspondingly reduced system of equations , for which the associated @xmath24 is now invertible .",
    "one simple solution is to minimize a _ nonlinear _",
    "linear squares objective function .",
    "let us take   as an example ( the others   and   have quite similar formulations ) , @xmath84 the obvious difficulty lies in the _ biconvexity _ of  , i.e. , if either @xmath24 or @xmath8 is fixed , minimizing over the other variable is a convex program . in general",
    ", there is no way to guarantee that any gradient descent algorithm / alternating minimization will give the global minimum .",
    "however , for the three models described above , there is one shortcut towards the exact and robust recovery of the solution via _ linear _ least squares if @xmath24 is invertible .",
    "we continue with   when @xmath85 , i.e. , @xmath86 where @xmath87 with @xmath88 the original measurement equation turns out to be a _ linear _ system with unknown @xmath89 and @xmath8 .",
    "the same idea of _ linearization _ can be also found in  @xcite . in this way ,",
    "the ground truth @xmath90 lies actually inside the _ null space _ of this linear system .",
    "two issues arise immediately : one the one hand , we need to make sure that @xmath91 spans the whole null space of this linear system .",
    "this is equivalent to the identifiability  @xcite because if @xmath92 or @xmath93 for @xmath94 is the unique solution ( up to a scalar ) to  , then @xmath95 spans the null space of  . on the other hand",
    ", we also need to avoid the trivial scenario @xmath96 , which does not bear any physical meanings . to resolve the latter issue , we add an extra linear constraint to exclude the trivial scenario , see also  @xcite .",
    "therefore , we hope that without noise , it suffices to solve the following linear system to recover @xmath97 up to a scalar , i.e. , @xmath98 where the scalar @xmath99 can be any nonzero number . as mentioned previously , the last constraint is to avoid the trivial solution . in the presence of additive noise ,",
    "we replace the linear system above by a linear least squares problem @xmath100 with respect to @xmath89 and @xmath8 , or equivalently , @xmath101 where @xmath102 , @xmath103 , and @xmath104 is the matrix on the left hand side of  .",
    "following from the same idea ,   and   can also be reformulated into linear systems and solved via linear least squares .",
    "the matrix @xmath104 and the vector @xmath46 take a slightly different form for those cases , see   and  , respectively .",
    "note that solving may not be the optimal choice to recover the unknowns from the perspective of statistics since the noisy perturbation actually enters into @xmath104 instead of @xmath105 .",
    "more precisely , the noisy perturbation @xmath106 to the left hand side of the corresponding linear system for  ,   and  , is always in the form of @xmath107 the size of @xmath106 depends on the models .",
    "hence total least squares  @xcite could be a better alternative while it is more difficult to analyze and significantly more costly to compute . since computational efficiency is essential for many practical applications ,",
    "a straightforward implementation of total least squares is of limited use .",
    "instead one should keep in mind that the actual perturbation enters only into @xmath108 , while the other matrix blocks remain unperturbed . constructing a total",
    "least squares solution that obeys these constraints , doing so in a numerically efficient manner and providing theoretical error bounds for it , is a rather challenging task , which we plan to address in our future work .",
    "numerical simulations imply that the performance under noisy measurements depends on the choice of @xmath109 , especially how much @xmath109 and @xmath110 are correlated .",
    "one extreme case is that @xmath111 , which is not able to avoid @xmath112",
    ". it might be better to add a constraint like @xmath113 .",
    "however , this will lead to a nonlinear problem which may not be solved efficiently .",
    "we present our theoretical findings for the three models  ,   and   respectively for different choices of @xmath36 or @xmath114 in one of our choices the @xmath36 are gaussian random matrices .",
    "the rationale for this choice is that , while a gaussian random matrix is not useful or feasible in most applications , it often provides a benchmark for theoretical guarantees and numerical performance .",
    "our other choices for the sensing matrices are structured random matrices , such as e.g.  the product of a deterministic partial ( or a randomly subsampled ) fourier matrix or a hadamard matrix with a diagonal binary random matrix .",
    "these matrices bring us closer to what we encounter in real world applications .",
    "indeed , structured random matrices of this type have been deployed for instance in imaging and wireless communications , see e.g.  @xcite .    by solving simple variations ( for different models ) of  , we can guarantee that the ground truth is recovered exactly up to a scalar if no noise exists and robustly if noise is present .",
    "the number of measurements required for exact and robust recovery is nearly optimal , i.e. , close to the information - theoretic limit up to a poly - log factor .",
    "however , the error bound for robust recovery is not optimal .",
    "it is worth mentioning that once the signals and calibration parameter @xmath24 are identifiable , we are able to recover both of them exactly in absence of noise by simply solving a linear system",
    ". however , identifiability alone can not guarantee robustness .      for model   we will focus on three cases :",
    "a.   @xmath36 is an @xmath115 complex gaussian random matrix , i.e. , each entry in @xmath36 is given by @xmath116 .",
    "b.   @xmath36 is an @xmath115  tall \" random dft / hadamard matrix with @xmath117 , i.e. , @xmath118 where @xmath119 consists of the first @xmath120 columns of an @xmath121 dft / hadamard matrix and each @xmath122 is a diagonal matrix with entries taking on the value @xmath123 with equal probability . in particular",
    ", there holds , @xmath124 c.   @xmath36 is an @xmath115  fat \" random partial dft matrix with @xmath125 , i.e. , @xmath118 where @xmath119 consists of @xmath126 columns of an @xmath62 dft / hadamard matrix and each @xmath122 is a diagonal matrix , which is defined the same as case ( b ) , @xmath127    our main findings are summarized as follows :    [ thm : main1 ] consider the self - calibration model given in  , where @xmath104 is as in  .",
    "then , for the solution @xmath46 of   and @xmath128 , there holds @xmath129    a.   with probability @xmath130 if @xmath36 is gaussian and @xmath131 ; b.   with probability @xmath132 if each @xmath36 is a  tall \" @xmath133 random hadamard / dft matrix and @xmath134 ; c.   with probability @xmath132 if each @xmath36 is a  fat \" @xmath135 random hadamard / dft matrix and @xmath136 .",
    "here @xmath137 and @xmath138 .",
    "our result is nearly optimal in terms of required number of measurements , because the number of constraints @xmath139 is required to be slightly greater than @xmath140 , the number of unknowns .",
    "theorem  [ thm : main1 ] can be regarded as a generalized result of  @xcite , in which @xmath24 is assumed to be positive and @xmath36 is gaussian . in our result , we only need @xmath24 to be an invertible complex diagonal matrix and @xmath36 can be a gaussian or random fourier type matrix .",
    "the approaches are quite different , i.e. ,  @xcite essentially uses nonconvex optimization by first constructing a good initial guess and then applying gradient descent to recover @xmath24 and @xmath8",
    ". our result also provides a provable fast alternative algorithm to  the blind deconvolution via random masks \" in  @xcite where a sdp - based approach is proposed .",
    "we now analyze model  . following similar steps that led us from   to  , it is easy to see that the linear system associated with   is given by @xmath141 [ s : model2-setup ] we consider two scenarios :    a.   @xmath36 is an @xmath115 complex gaussian random matrix , i.e. , each entry in @xmath36 yields @xmath116",
    ". b.   @xmath36 is of the form @xmath142 where @xmath143 is a random partial hadamard / fourier matrix , i.e. , the columns of @xmath144 are uniformly sampled without replacement from an @xmath121 dft / hadamard matrix ; @xmath145 is a diagonal matrix with @xmath146 being i.i.d .",
    "bernoulli random variables .",
    "[ thm : main2 ] consider the self - calibration model given in  , where @xmath104 is as in  .",
    "let @xmath147 and @xmath148 .",
    "then , for the solution @xmath46 of   and @xmath128 , there holds @xmath149    a.   with probability at least @xmath150 if @xmath36 is an @xmath115 @xmath151 complex gaussian random matrix and @xmath152 b.   with probability at least @xmath153 if @xmath36 yields   and @xmath154    note that if @xmath155 i.e. , in the noiseless case , we have @xmath156 if @xmath157 here",
    "@xmath139 is the number of constraints and @xmath158 is the degree of freedom .",
    "therefore , our result is nearly optimal in terms of information theoretic limit . compared with a similar setup in  @xcite",
    ", we have a more efficient algorithm since  @xcite uses nuclear norm minimization to achieve exact recovery . however , the assumptions are slightly different , i.e. , we assume that @xmath24 is invertible and hence the result depends on @xmath24 while  @xcite imposes  incoherence \" on @xmath7 by requiring @xmath159 relatively small , where @xmath17 denotes fourier transform .      we again follow a by now familiar procedure to derive the linear system associated with  , which turns out to be @xmath160 for this scenario we only consider the case when @xmath20 is a complex gaussian random matrix .    [ thm : main3 ] consider the self - calibration model given in  , where @xmath104 is as in  .",
    "let @xmath147 and @xmath148 .",
    "then , for the solution @xmath46 of   and @xmath161 there holds @xmath162 with probability at least @xmath163 if @xmath20 is a complex gaussian random matrix and @xmath164 where @xmath165 is the gram matrix of @xmath166 in particular , if @xmath167 and @xmath168 ,   becomes@xmath169    when @xmath170 , theorem  [ thm : main3 ] says that the solution to   is uniquely determined up to a scalar if @xmath171 , which involves the norm of gram matrix @xmath165 .",
    "this makes sense if we consider two extreme cases : if @xmath172 are exactly the same , then we have @xmath173 and @xmath174 and if @xmath172 are orthogonal to one another , @xmath175 .",
    "balzano and nowak  @xcite show exact recovery of this model when @xmath20 is a deterministic dft matrix ( discrete fourier matrix ) and @xmath30 are generic signals drawn from a probability distribution , but their results do not include stability theory in the presence of noise .    for theorem  [ thm : main2 ] and theorem  [ thm : main3 ] it does not come as a surprise that the error bound depends on the norm of @xmath30 and @xmath24 as well as on how much @xmath110 and @xmath109 are correlated .",
    "we can not expect a relatively good condition number for @xmath104 if @xmath176 varies greatly over @xmath177 . concerning the correlation between @xmath110 and @xmath109 , one extreme case is @xmath178 , which does not rule out the possibility of @xmath112 .",
    "hence , the quantity @xmath179 affects the condition number .",
    "this section is devoted to numerical simulations .",
    "four experiments for both synthetic and real data will be presented to address the effectiveness , efficiency and robustness of the proposed approach .",
    "for all three models presented  ,   and  , the corresponding linear systems have simple block structures which allow fast implementation via the conjugate gradient method for non - hermitian matrices  @xcite . in our simulations",
    ", we do not set up @xmath104 explicitly .",
    "the iteration stops if either the number of iterations reaches at most @xmath180 or the residual of the corresponding normal equation is smaller than @xmath181 .",
    "therefore , we are able to deal with problems from imaging processing without much computational issue .    throughout our discussion , the @xmath182 ( signal - to - noise ratio ) in the scale of db is defined as @xmath183 we measure the performance with @xmath184 where @xmath185 which is actually equal to @xmath186 here @xmath187 and @xmath70 are the ground truth . in particular , for image examples , we only measure the relative error with respect to the recovered image @xmath188 , i.e. , @xmath189      suppose we have a target image @xmath8 and try to estimate @xmath8 through multiple measurements . however , the sensing process is not perfect because of the missing calibration of the sensors . in order to estimate both the unknown gains and phases as well as the target signal , a randomized",
    "sensing procedure is used by employing several random binary masks .",
    "we assume that @xmath190 where @xmath119 is a  tall \" low - frequency dft matrix , @xmath191 is a diagonal @xmath123-random matrix and @xmath70 is an image of size @xmath192 we set @xmath193 and @xmath194 with @xmath195 ; the oversampling ratio is @xmath196 .",
    "we compare two cases : ( i ) @xmath7 is a sequence distributed uniformly over @xmath197 $ ] with @xmath198 , and ( ii ) @xmath7 is a steinhaus sequence ( uniformly distributed over the complex unit circle ) with @xmath199 .",
    "we pick those choices of @xmath109 because we know that the image we try to reconstruct has only non - negative values . thus , by choosing @xmath109 to be non - negative , there are fewer cancellation in the expression @xmath200 , which in turn leads to a smaller condition number and better robustness .",
    "the corresponding results of our simulations are shown in figure  [ fig : pos - repeat-1 ] and figure  [ fig : pos - repeat-2 ] , respectively . in both cases , we only measure the relative error of the recovered image .",
    "0.5 cm    in figure  [ fig : pos - repeat-1 ] , we can see that both uncalibrated / calibrated image are quite good . here",
    "the uncalibrated image is obtained by first applying the inverse fourier transform and the inverse of the mask to each @xmath201 and then taking the average of @xmath202 samples .",
    "we explain briefly why the uncalibrated image still looks good .",
    "note that @xmath203 where @xmath204 is the pseudo inverse of @xmath205 here @xmath206 is actually a diagonal matrix with random entries @xmath207 . as a result ,",
    "each @xmath208 is the sum of @xmath126 rank-1 matrices with random @xmath209 coefficients and is relatively small due to many cancellations inside it .",
    "moreover ,  @xcite showed that most 2-d signals can be reconstructed within a scale factor from only knowing the phase of its fourier transform , which applies to the case when @xmath7 is positive .",
    "however , when the unknown calibration parameters are complex variables ( i.e. , we do not know much about the phase information ) , figure  [ fig : pos - repeat-2 ] shows that the uncalibrated recovered image is totally meaningless .",
    "our approach still gives a quite satisfactory results even at a relatively low snr of 5db .",
    "the second experiment is about blind deconvolution in random mask imaging  @xcite .",
    "suppose we observe the convolution of two components , @xmath210 where both , the filter @xmath21 and the signal of interests @xmath70 are unknown .",
    "each @xmath191 is a random @xmath123-mask .",
    "the blind deconvolution problem is to recover @xmath211 .",
    "moreover , here we assume that the filter is actually a low - pass filter , i.e. , @xmath212 is compactly supported in an interval around the origin , where @xmath17 is the fourier transform .",
    "after taking the fourier transform on both sides , the model actually ends up being of the form   with @xmath213 where @xmath119 is a  fat \" partial dft matrix and @xmath7 is the nonzero part of @xmath212 . in our experiment",
    ", we let @xmath70 be a @xmath214 image and @xmath215 be a 2-d gaussian filter of size @xmath216 as shown in figure  [ fig : random - mask-1 ] .",
    "0.5 cm    0.5 cm    in those experiments , we choose @xmath217 since both @xmath7 and @xmath70 are nonnegative .",
    "figure  [ fig : random - mask-2 ] shows the recovered image from @xmath218 sets of noiseless measurements and the performance is quite satisfactory . here",
    "the oversampling ratio is @xmath219 we can see from figure  [ fig : random - mask-3 ] that the blurring effect has been removed while the noise still exists .",
    "that is partially because we did not impose any denoising procedure after the deconvolution .",
    "we choose @xmath36 to be random hadamard matrices with @xmath220 and @xmath221 and @xmath222 with @xmath187 being a positive / steinhaus sequence , as we did previously .",
    "each @xmath31 is sampled from standard gaussian distribution .",
    "we choose @xmath223 if @xmath187 is uniformly distributed over @xmath224 $ ] and @xmath225 for steinhaus @xmath226 100 simulations are performed for each level of @xmath182 .",
    "the test is also given under different choices of @xmath202 .",
    "the oversampling ratio @xmath227 is @xmath228 , @xmath229 and @xmath230 for @xmath231 respectively . from figure",
    "[ fig : diverse ] , we can see that the error scales linearly with snr in db .",
    "the performance of steinhaus @xmath187 is not as good as that of positive @xmath187 for snr @xmath232 .",
    "that is because @xmath200 is much larger when @xmath187 and @xmath109 are both nonnegative , which gives better condition number .     where @xmath233 , @xmath222 and each @xmath36 is a random hadamard matrix",
    "@xmath187 is a steinhaus sequence.,width=257 ]     where @xmath233 , @xmath222 and each @xmath36 is a random hadamard matrix .",
    "@xmath187 is a steinhaus sequence.,width=257 ]      we make a comparison of performances between @xmath187 as a positive or steinhaus sequence with a gaussian random matrix @xmath20 .",
    "each @xmath31 is sampled from the standard gaussian distribution and hence the underlying gram matrix @xmath165 is quite close to @xmath234 ( this closeness could be easily made more precise , but we refrain doing so here ) .",
    "the choice of @xmath109 and oversampling ratio are the same as those in section  [ s : numerics - diverse ] . from figure",
    "[ fig : multiple ] , we see that the performance for positive @xmath187 is better than that for the steinhaus case , especially in the lower snr regime ( snr @xmath232 ) .",
    "the reason is the low correlation between @xmath109 and @xmath110 when @xmath187 is steinhaus and @xmath235     where @xmath233 , @xmath222 and @xmath20 is a gaussian random matrix .",
    "left : @xmath187 is a random vector with each entry uniformly distributed over @xmath224 $ ] ; right : @xmath187 is a random vector with each entry uniformly distributed over unit circle.,width=283 ]     where @xmath233 , @xmath222 and @xmath20 is a gaussian random matrix . left",
    ": @xmath187 is a random vector with each entry uniformly distributed over @xmath224 $ ] ; right : @xmath187 is a random vector with each entry uniformly distributed over unit circle.,width=283 ]",
    "for each subsection , we will first give the result of noiseless measurements .",
    "we then prove the stability theory by using the result below .",
    "@xcite[prop : perturb ] suppose that @xmath236 is a consistent and overdetermined system .",
    "denote @xmath237 as the least squares solution to @xmath238 with @xmath239 . if @xmath240 , there holds , @xmath241    to apply the proposition above , it suffices to bound @xmath242 and @xmath243 .",
    "let us start with   when @xmath85 and denote @xmath244 and @xmath245 , @xmath246 then we rewrite @xmath247 as @xmath248 where @xmath249 by definition , @xmath250 our goal is to find out the smallest and the largest eigenvalue of @xmath104 .",
    "actually it suffices to understand the spectrum of @xmath251 .",
    "obviously , its smallest eigenvalue is zero and the corresponding eigenvector is @xmath252 let @xmath253 be the @xmath254-th column of @xmath255 and we have @xmath256 under all the three settings in section  [ s : model1-setup ] .",
    "hence , @xmath257    it is easy to see that @xmath258 and the null space of @xmath259 is spanned by @xmath260 .",
    "@xmath259 has an eigenvalue with value @xmath63 of multiplicity @xmath261 and an eigenvalue with value @xmath228 of multiplicity @xmath63 .",
    "more importantly , the following proposition holds and combined with proposition  [ prop : perturb ] , we are able to prove theorem  [ thm : main1 ] .    [ prop : main1 ] there holds @xmath262    a.   with probability @xmath130 if @xmath36 is gaussian and @xmath131 ; b.   with probability @xmath132 if each @xmath36 is a  tall \" @xmath133 random hadamard / dft matrix and @xmath134 ; c.   with probability @xmath132 if each @xmath36 is a  fat \" @xmath135 random hadamard / dft matrix and @xmath136 .",
    "[ rmk : prop1 ] proposition  [ prop : main1 ] actually addresses the identifiability issue of the model   in absence of noise .",
    "more precisely , the invertibility of @xmath263 is guaranteed by that of @xmath24 . by weyl",
    "s theorem for singular value perturbation in  @xcite , @xmath264 eigenvalues of @xmath251 are greater than @xmath265 hence , the rank of @xmath266 is equal to @xmath267 if @xmath202 is close to the information theoretic limit under the conditions given above , i.e. , @xmath268 . in other words , the null space of @xmath266",
    "is completely spanned by @xmath269 .",
    "note that proposition  [ prop : main1 ] gives the result if @xmath85 .",
    "the noisy counterpart is obtained by applying perturbation theory for linear least squares .",
    "let @xmath270 where @xmath271 is the noiseless part and @xmath106 is defined in  . without loss of generality , we assume that @xmath272 according to proposition  [ prop : perturb ] , it suffices to estimate the condition number @xmath273 of @xmath271 and @xmath274 . note that @xmath275 where @xmath276 from proposition  [ prop : main1 ] and theorem 1 in  @xcite , we know that @xmath277 where @xmath278 and @xmath279 following from  , we have @xmath280 on the other hand , @xmath281 follows from proposition  [ prop : main1 ] . in other words",
    ", we have found the lower and upper bounds for @xmath282 or equivalently , @xmath283 now we proceed to the estimation of @xmath284 let @xmath285 be a unit vector , where @xmath286 with @xmath287 and @xmath288 are the eigenvectors of @xmath251 .",
    "then the smallest eigenvalue of @xmath289 defined in   has a lower bound as follows : @xmath290 which implies @xmath291 .",
    "combined with @xmath292 , @xmath293 therefore , with  , the condition number of @xmath294 is bounded by @xmath295 from  , @xmath243 is bounded by @xmath296 applying proposition  [ prop : perturb ] gives the following upper bound of the estimation error @xmath297 which gives theorem  [ thm : main1 ] .      *",
    "[ proof of proposition  [ prop : main1](a ) ] * from now on , we assume @xmath298 , i.e. , the @xmath254-th column of @xmath255 , obeys a complex gaussian distribution , @xmath299 .",
    "let @xmath300 be the @xmath254-th column of @xmath301 ; it can be written in explicit form as @xmath302 denoting @xmath303 , we obtain @xmath304 obviously each @xmath305 is independent . in order to apply theorem  [ thm : bern1 ] to estimate @xmath306 , we need to bound @xmath307 and @xmath308 . due to the semi - definite positivity of @xmath309",
    ", we have @xmath310 and hence @xmath311 this implies @xmath312 now we consider @xmath313 by computing @xmath314 and @xmath315 , i.e. , the @xmath316-th and @xmath317-th block of @xmath318 , @xmath319 { \\boldsymbol{e}}_i{\\boldsymbol{e}}_i^ * , \\\\ ( { \\mathcal{z}}_{l , i}{\\mathcal{z}}_{l , i}^*)_{2,2 } & = & \\frac{1}{m } ( { \\boldsymbol{a}}_{l , i}{\\boldsymbol{a}}_{l , i}^ * - { \\boldsymbol{i}}_n){\\boldsymbol{v}}{\\boldsymbol{v}}^*({\\boldsymbol{a}}_{l , i}{\\boldsymbol{a}}_{l , i}^ * - { \\boldsymbol{i}}_n ) + \\frac{1}{m^2}({\\boldsymbol{a}}_{l , i}{\\boldsymbol{a}}_{l , i}^ * - { \\boldsymbol{i}}_n)^2.\\end{aligned}\\ ] ] following from  ,  ,   and lemma  [ lem : pos ] , there holds @xmath320 by applying the matrix bernstein inequality ( see theorem  [ berngaussian ] ) we obtain @xmath321 with probability @xmath322 .",
    "in particular , by choosing @xmath323 , i.e , @xmath324 the inequality above holds with probability @xmath325      * [ proof of proposition  [ prop : main1](b ) ] * each @xmath301 is independent by its definition in   if @xmath118 where @xmath119 is an @xmath115 partial dft / hadamard matrix with @xmath117 and @xmath326 and @xmath327 is a diagonal random binary @xmath123 matrix .",
    "let @xmath328 ; in explicit form @xmath329 where @xmath330 follows from the assumption .",
    "first we take a look at the upper bound of @xmath331 it suffices to bound @xmath332 since @xmath333 and @xmath259 is positive semi - definite . on the other hand , due to lemma  [ lem : pos ]",
    ", we have @xmath334 and hence we only need to bound @xmath335 . for @xmath335 , there holds @xmath336 also for any pair of @xmath337 , @xmath338 can be rewritten as @xmath339 where @xmath340 is the @xmath254-th column of @xmath341 and @xmath342 . then there holds @xmath343 where the third inequality follows from lemma  [ lem : rade ] . applying lemma  [ lem : pos ] to @xmath344 , @xmath345 with probability at least @xmath346 .",
    "denote the event @xmath347 by @xmath348 .",
    "now we try to understand @xmath349 the @xmath316-th and @xmath317-th block of @xmath350 are given by @xmath351 by using  ,   and @xmath352 , we have @xmath353 combining  ,  ,   and lemma  [ lem : pos ] , @xmath354 by applying   with @xmath323 and @xmath355 over event @xmath348 , we have @xmath356 with probability @xmath132 if @xmath134 .      *",
    "[ proof of proposition  [ prop : main1](c ) ] * each @xmath301 is independent due to  .",
    "let @xmath328 ; in explicit form @xmath357 here @xmath213 where @xmath119 is a  fat \" @xmath358 partial dft / hadamard matrix satisfying @xmath359 and @xmath191 is a diagonal @xmath360-random matrix",
    ". there holds @xmath361 where @xmath362 and @xmath363 for each @xmath364 , @xmath365 hence , there holds , @xmath366 with probability at least @xmath367 , which follows exactly from   and lemma  [ lem : pos ] .",
    "now we give an upper bound for @xmath368 .",
    "the @xmath316-th and @xmath317-th block of @xmath350 are given by @xmath369 by using  ,   and @xmath370 , we have @xmath371 for @xmath372 , we have @xmath373 where @xmath374 note that @xmath375 , and there holds , @xmath376 combining  ,  ,  ,   and lemma  [ lem : pos ] , @xmath377 by applying   with @xmath323 , we have @xmath378 with probability @xmath132 if @xmath136 .",
    "we start with   by setting @xmath85 . in this way",
    ", we can factorize the matrix @xmath104 ( excluding the last row ) into @xmath379 where @xmath380 is the normalized @xmath31 , @xmath177 .",
    "we will show that the matrix @xmath38 is of rank @xmath381 to guarantee that the solution is unique ( up to a scalar ) .",
    "denote @xmath382 and @xmath383 with @xmath384 where @xmath66 is a standard orthonormal basis in @xmath68 .",
    "the proof of theorem  [ thm : main2 ] relies on the following proposition .",
    "we defer the proof of proposition  [ prop : main2 ] to the sections  [ s : model2-a ] and  [ s : model2-b ] .",
    "[ prop : main2 ] there holds ,",
    "@xmath385    a.   with probability at least @xmath150 with @xmath386 if @xmath36 is an @xmath115 @xmath151 complex gaussian random matrix and @xmath387 b.   with probability at least @xmath388 with @xmath386 if @xmath36 yields   and @xmath389    [ rmk : prop2 ] note that @xmath259 has one eigenvalue equal to 0 and all the other eigenvalues are at least 1 .",
    "hence the rank of @xmath259 is @xmath390 .",
    "similar to remark  [ rmk : prop1 ] , proposition  [ prop : main2 ] shows that the solution @xmath391 to   is uniquely identifiable with high probability when @xmath392 and @xmath393    [ * proof of theorem  [ thm : main2 ] ] * from  , we let @xmath394 where @xmath271 is the noiseless part of @xmath104 .",
    "now ,   gives @xmath395 define @xmath396 and @xmath397 . from proposition  [ prop : main2 ] and theorem 1 in  @xcite , we know that the eigenvalues @xmath398 of @xmath399 fulfill @xmath400 and @xmath401 for @xmath402 since @xmath403 ; and the eigenvalues of @xmath259 are 0 , 1 and 2 with multiplicities 1 , @xmath404 , @xmath63 respectively .",
    "the key is to obtain a bound for @xmath405 from  , @xmath406 where @xmath407 on the other hand ,   gives @xmath408 since @xmath409 . for @xmath410 , @xmath411",
    "denote @xmath412 such that @xmath413 and @xmath414 where @xmath415 . by using the same procedure as  , @xmath416 where @xmath417 with @xmath418 .",
    "since @xmath419 the smallest eigenvalue of @xmath420 yields , @xmath421 combining   and   leads to @xmath422 applying proposition  [ prop : perturb ] and @xmath423 , we have @xmath424 which finishes the proof of theorem  [ thm : main2 ] .      in this section",
    ", we will prove that proposition  [ prop : main2](a ) if @xmath425 where @xmath298 is the @xmath254-th column of @xmath255 . before moving to the proof",
    ", we compute a few quantities which will be used later .",
    "define @xmath300 as the @xmath426-th column of @xmath427 @xmath428 where @xmath429 and @xmath430 are standard orthonormal basis in @xmath67 and @xmath68 respectively ; ",
    "@xmath69 \" denotes kronecker product . by definition , we have @xmath431 and all @xmath300 are independent from one another .",
    "@xmath432    and its expectation is equal to @xmath433 it is easy to verify that @xmath434 .",
    "[ * proof of proposition  [ prop : main2](a ) ] * the tool is to use apply matrix bernstein inequality in theorem  [ berngaussian ] .",
    "note that @xmath435 let @xmath436 and we have @xmath437 since @xmath438 follows from lemma  [ lem : pos ] .",
    "therefore , the exponential norm of @xmath439 is bounded by @xmath440 and as a result @xmath441    now we proceed by estimating the variance @xmath442 .",
    "we express @xmath305 as follows : @xmath443 the @xmath316-th and the @xmath317-th block of @xmath318 are @xmath444 and @xmath445.\\ ] ] following from  ,   and  , we have @xmath446 due to lemma  [ lem : pos ] , there holds , @xmath447 by applying  , @xmath448 with @xmath449 , there holds @xmath450 with probability at least @xmath150 if @xmath451      we prove proposition  [ prop : main2 ] based on assumption  .",
    "denote @xmath253 and @xmath452 as the @xmath254-th column of @xmath255 and @xmath453 and obviously we have @xmath454 denote @xmath455 and let @xmath301 be the @xmath364-th block of @xmath50 in  , i.e. , @xmath456 with @xmath330 , we have @xmath457 where the expectation of @xmath254-th row of @xmath458 yields @xmath459 . hence @xmath460 its expectation equals @xmath461     [ * proof of proposition  [ prop : main2](b ) ] * note that each block @xmath301 is independent and we want to apply bernstein inequality to achieve the desired result .",
    "let @xmath462 and by definition , we have @xmath463 note that @xmath464 since   implies that @xmath465 , we have @xmath466 with probability at least @xmath346 .",
    "we proceed with estimation of @xmath467 by looking at the @xmath316-th and @xmath317-th block of @xmath350 , i.e. , @xmath468 note that @xmath469 .",
    "the @xmath254-th diagonal entry of @xmath470 is @xmath471 where @xmath472 and   implies @xmath473 since @xmath474 is still a unit vector ( note that @xmath475 is unitary since @xmath452 is a column of @xmath453 ) .",
    "therefore , @xmath476 by using lemma  [ lem : laal ] , we have @xmath477 with @xmath478 and independence between @xmath479 and @xmath191 , we have @xmath480 where @xmath481 follows from   and @xmath482 by using  ,  ,   and lemma  [ lem : pos ] , @xmath483 is bounded above by @xmath484 conditioned on the event @xmath485 , applying   with @xmath486 gives @xmath487 with probability at least @xmath388 and it suffices to require @xmath488      recall that @xmath104 in   and the only difference from   is that here all @xmath36 are equal to @xmath20 .",
    "if @xmath85 , @xmath104 ( excluding the last row ) can be factorized into @xmath489 where @xmath490 is the normalized @xmath491 .    before we proceed to the main result in this section we need to introduce some notation .",
    "let @xmath492 be the @xmath254-th column of @xmath493 , which is a complex gaussian random matrix ; define @xmath494 to be a matrix whose columns consist of the @xmath254-th column of each block of @xmath50 , i.e. , @xmath495 where  @xmath69 \" denotes kronecker product and both @xmath429 and @xmath430 are the standard orthonormal basis in @xmath67 and @xmath68 , respectively . in this way , the @xmath496 are independently from one another . by definition ,",
    "@xmath497 where @xmath498 and @xmath499 with @xmath500 the expectation of @xmath501 is given by @xmath502    0.25 cm our analysis depends on the  _ mutual coherence _ of @xmath172 .",
    "one can not expect to recover all @xmath172 and @xmath24 if all @xmath172 are parallel to each other .",
    "let @xmath165 be the gram matrix of @xmath172 with @xmath503 , i.e. , @xmath504 and @xmath505 and in particular , @xmath506 .",
    "its eigenvalues are denoted by @xmath507 with @xmath508 .",
    "basic linear algebra tells that @xmath509 where @xmath510 is unitary and @xmath511 let @xmath512 , then there holds @xmath513 since @xmath514 here @xmath515 and @xmath516 .",
    "in particular , if @xmath517 , then @xmath167 ; if @xmath518 for all @xmath519 , then @xmath520 and @xmath521    we are now ready to state and prove the main result in this subsection .    [ prop : main3 ] there holds @xmath522 with probability at least @xmath163 if @xmath523 and each @xmath524 is i.i.d .",
    "complex gaussian , i.e. , @xmath525 .",
    "in particular , if @xmath167 and @xmath168 ,   becomes @xmath169    the proof of theorem  [ thm : main3 ] follows exactly from that of theorem  [ thm : main2 ] when proposition  [ prop : main3 ] holds .",
    "hence we just give a proof of proposition  [ prop : main3 ] .",
    "[ * proof of proposition  [ prop : main3 ] ] * let @xmath526 , where @xmath527 and @xmath528 are defined as @xmath529    [ [ estimation - of - sum_i1mmathcalz_i1 ] ] estimation of @xmath530 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    following from  , we have @xmath531 where @xmath532 is a @xmath533 matrix with orthonormal rows and hence each @xmath534 is rayleigh distributed .",
    "( we say @xmath535 is rayleigh distributed if @xmath536 where both @xmath537 and @xmath538 are standard real gaussian variables . )    due to the simple form of @xmath527 , it is easy to see from bernstein s inequality that @xmath539 with probability @xmath540 here @xmath541 where @xmath542 . therefore , @xmath543 now we only need to find out @xmath544 and @xmath545 in order to bound @xmath546 .",
    "[ [ estimation - of - mathcalz_i2-_psi_1 ] ] estimation of @xmath547 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    denote @xmath548 and there holds , @xmath549 for @xmath550 , its @xmath551-norm is bounded by @xmath552 @xmath553 let @xmath554 and @xmath555 .",
    "the @xmath551-norm of @xmath556 yields @xmath557 where the second inequality follows from the cauchy - schwarz inequality , @xmath558 , and @xmath559 . therefore , @xmath560 and there holds @xmath561    [ [ estimation - of - sigma_02 ] ] estimation of @xmath483 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    note that @xmath562 .",
    "let @xmath563 and @xmath564 be the @xmath316-th and @xmath317-th block of @xmath565 respectively , i.e. , @xmath566_{1\\leq k\\leq l\\leq p } +   \\frac{1}{m^2 } { \\boldsymbol{i}}_p\\otimes ( { \\boldsymbol{a}}_i{\\boldsymbol{a}}_i^ * - { \\boldsymbol{i}}_n)^2 .",
    "\\label{eq : ai22 } \\ ] ] since @xmath565 is a positive semi - definite matrix , lemma  [ lem : pos ] implies @xmath567 so we only need to compute @xmath568 and @xmath569 .",
    "@xmath570 where @xmath571 now we have @xmath572 for @xmath573 note that @xmath574 which follows from  . by  ,   and",
    "lemma  [ lem : pos ] , there holds , @xmath575 applying   to @xmath576 with   and  , we have @xmath577 with probability @xmath322 . by combining   with   and letting @xmath578 ,",
    "we have @xmath579 with probability @xmath163 if @xmath580 or equivalently , @xmath581",
    "[ lem : pos ] for any hermitian positive semi - definite matrix @xmath582 with @xmath583 , there holds , @xmath584 in other words , @xmath585 .",
    "[ thm : bern1 ] consider a finite sequence of @xmath344 of independent centered random matrices with dimension @xmath590 .",
    "we assume that @xmath591 and introduce the random matrix @xmath592 compute the variance parameter @xmath593 then for all @xmath594 @xmath595 with probability at least @xmath322 where @xmath596 is an absolute constant .",
    "[ berngaussian ] for a finite sequence of independent @xmath590 random matrices @xmath344 with @xmath601 and @xmath483 as defined in  , we have the tail bound on the operator norm of @xmath600 , @xmath602 with probability at least @xmath322 where @xmath596 is an absolute constant .",
    "suppose that @xmath608 is a rademacher sequence and for any fixed @xmath605 and @xmath606 , there holds @xmath609 where @xmath610 is an @xmath62 matrix with only one nonzero entry equal to 1 and at position @xmath611 in particular , setting @xmath612 gives @xmath613    since @xmath614 is a rademacher sequence , i.e , each @xmath615 takes @xmath123 independently with equal probability , this implies @xmath616 and @xmath617 therefore , @xmath618 the @xmath619-th entry of @xmath620 is @xmath621 .",
    "[ lem : laal ] there holds @xmath629 where @xmath630 , @xmath631 and @xmath632 is a deterministic unit vector .",
    "@xmath362 is a random partial fourier / hadamard matrix with @xmath326 and @xmath117 , i.e. , the columns of @xmath119 are uniformly sampled without replacement from an @xmath121 dft / hadamard matrix ; @xmath633 is a diagonal matrix with entries random sampled from @xmath123 with equal probability ; moreover , we assume @xmath633 and @xmath119 are independent from each other . in particular ,",
    "if @xmath634 , @xmath635      let @xmath637 be the @xmath254-th column of @xmath341 and the @xmath638-th entry of @xmath639 is @xmath640 where @xmath641 .",
    "the randomness of @xmath642 comes from both @xmath119 and @xmath633 and we first take the expectation with respect to @xmath633 .",
    "@xmath643 where @xmath644 follows from each entry in @xmath633 being a bernoulli random variable .",
    "hence , @xmath645    let @xmath646 be the @xmath647-th column of @xmath119 and @xmath648 and  @xmath649 \" denotes the hadamard ( pointwise ) product .",
    "so we have @xmath650 there holds , @xmath651 where the third equation follows from linearity of the hadamard product and from @xmath652 the last one uses the fact that @xmath653 if @xmath646 is a vector from the dft matrix or hadamard matrix . by the property of conditional expectation , we know that @xmath654 and due to the linearity of expectation , it suffices to find out for @xmath626 , @xmath655 where @xmath646 and @xmath656 , by definition , are the @xmath647-th and @xmath364-th columns of @xmath119 which are sampled uniformly without replacement from an @xmath121 dft matrix @xmath657 .",
    "note that @xmath658 is actually an _ ordered _ pair of random vectors sampled without replacement from columns of @xmath657 .",
    "hence there are in total @xmath659 different choices of @xmath660 and @xmath661 where @xmath662 is defined as the @xmath254-th column of an @xmath121 dft matrix @xmath657 .",
    "now we have , for any @xmath626 , @xmath663 where @xmath664 and @xmath665 now we return to @xmath666 . by substituting   into",
    ", we end up with @xmath667 where @xmath668 follows from @xmath669      a.  ahmed , a.  cosse , and l.  demanet . a convex approach to blind deconvolution with diverse inputs . in",
    "_ computational advances in multi - sensor adaptive processing ( camsap ) , 2015 ieee 6th international workshop on _ , pages 58 .",
    "ieee , 2015 .",
    "s.  curtis , j.  lim , and a.  oppenheim .",
    "signal reconstruction from one bit of fourier transform phase . in _ acoustics , speech , and signal processing , ieee international conference on icassp84 .",
    "_ , volume  9 , pages 487490 .",
    "ieee , 1984 .",
    "r.  gribonval , g.  chardon , and l.  daudet .",
    "blind calibration for compressed sensing by convex optimization . in _ acoustics ,",
    "speech and signal processing ( icassp ) , 2012 ieee international conference on _ , pages 27132716 .",
    "ieee , 2012 .",
    "j. shin , p.  e. larson , m.  a. ohliger , m.  elad , j.  m. pauly , d.  b. vigneron , and m.  lustig .",
    "calibrationless parallel imaging reconstruction based on structured low - rank matrix completion .",
    ", 72(4):959970 , 2014 .",
    "r.  vershynin .",
    "introduction to the non - asymptotic analysis of random matrices . in y.  c. eldar and g.  kutyniok , editors , _ compressed sensing : theory and applications _ , chapter  5 .",
    "cambridge university press , 2012 ."
  ],
  "abstract_text": [
    "<S> whenever we use devices to take measurements , calibration is indispensable . while the purpose of calibration is to reduce bias and uncertainty in the measurements </S>",
    "<S> , it can be quite difficult , expensive and sometimes even impossible to implement . </S>",
    "<S> we study a challenging problem called _ self - calibration _ , i.e. , the task of designing an algorithm for devices so that the algorithm is able to perform calibration automatically . </S>",
    "<S> more precisely , we consider the setup @xmath0 where only partial information about the sensing matrix @xmath1 is known and where @xmath1 linearly depends on @xmath2 . the goal is to estimate the calibration parameter @xmath2 ( resolve the uncertainty in the sensing process ) and the signal / object of interests @xmath3 simultaneously . for three different models of practical relevance </S>",
    "<S> we show how such a _ bilinear _ inverse problem , including blind deconvolution as an important example , can be solved via a simple _ linear least squares _ approach . as a consequence , </S>",
    "<S> the proposed algorithms are numerically extremely efficient , thus allowing for real - time deployment . </S>",
    "<S> explicit theoretical guarantees and stability theory are derived and the number of sampling complexity is nearly optimal ( up to a poly - log factor ) . </S>",
    "<S> applications in imaging sciences and signal processing are discussed and numerical simulations are presented to demonstrate the effectiveness and efficiency of our approach . </S>"
  ]
}