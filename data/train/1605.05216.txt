{
  "article_text": [
    "one of the greatest challenges facing artificial neural network research is appropriate model selection .",
    "currently , research in this field heavily relies on manually tuning hyperparameters of painstakingly contrived handcrafted architectures . with this paradigm ,",
    "minor changes to an architecture means additional days or weeks to retrain that network .",
    "furthermore , default architectures are not necessarily always the best choices or the only choices for a task .",
    "problems could be solved more efficiently and effectively given an appropriate modality for automatic model selection .",
    "previously , efforts have primarily studied _ topology and weight evolving artificial neural networks _ ( tweanns ) , yet automatic activation function selection has received little attention . of the studies done in this area , many examined heterogeneous neural networks ( networks using more than one kind of activation function ) by selecting from a fixed pool of hand - selected functions [ 10 , 11 , 12 , 13 , 16 , 17 , 18 , 19 , 20 ] .",
    "most approaches stick to using conventional functions like sigmoid and gaussian .",
    "few , however , have explored alternative paradigms .",
    "yao thoroughly reviewed a variety of studies that primarily evolved network topology and activation functions [ 9 ] .",
    "duch and jankowski surveyed a multitude of activation functions and organized them into a taxonomy [ 15 , 21 ] .",
    "here , i present combinatorially generated piecewise activation functions , which is a novel approach for automatically selecting activation functions .",
    "each generated piecewise activation function expresses , what i refer to as , a resting state , the left side , and an active state , the right side .",
    "i hypothesize that combinatorially generating activation functions enhances the expressive power of neuroevolutionary algorithms .",
    "i also demonstrate that this technique improves average accuracy and can easily be adapted into existing algorithms while only slightly increasing the average number of required evaluations and average network size , and slightly decreasing average generalization .",
    "neuroevolution is a genetic programming paradigm for automatic model selection of artificial neural networks and is capable of discovering globally optimal representations for any aspect of a neural network .",
    "neuroevolutionary algorithms represent a network as a genome , which is a collection of genes .",
    "each gene typically represents some subset of the network e.g. a node or connection , and accompanying attributes .",
    "training begins by generating an initial population of networks .",
    "each generation , all genomes are evaluated on the task and assigned a fitness score . at the end of each generation , networks mate and perform crossover of genes to generate the next generation . each component in a gene",
    "is then probabilistically mutated .",
    "after a number of generations , referred to as the drop - off age , the lowest performing portion of the population is eliminated or restricted from the rest of the population .",
    "the algorithm continues until one of the following occurs : a maximum number of generations is reached , the population collectively meets a desired fitness value , or the entire population degenerates and fails to make progress .",
    "if the population survives , a network can be selected for testing .",
    "together , these mechanisms create artificial evolutionary pressure , search bias for an optimal genome model [ 14 ] , to systematically train different architectures until they produce a globally optimal final population of networks .",
    "additionally , genetic algorithms do not use gradient information to optimize networks .",
    "therefore , they are not susceptible to becoming trapped in local optima .",
    "because of this , neuroevolution is capable of optimizing nondifferentiable functions and dynamic networks that change in size as they evolve , unlike backpropagation .",
    "thus , neuroevolution a suitable modality for evaluating combinatorially generated piecewise activation functions .",
    "neuroevolution techniques can also be combined with gradient optimization techniques as described by white and ligomenides [ 11 ] .",
    "although neuroevolution can be more attractive than gradient - based techniques , they may take more time than gradient - based methods , and may use many neuroevolutionary hyperparameters that require manual tuning before each experiment .",
    "_ neuroevolution of augmenting topologies _",
    "( neat ) is a neuroevolutionary algorithm for simultaneously evolving weights and adaptable , complex topologies for stochastic reinforcement learning control tasks .",
    "neat performs particularly well in continuous and high - dimensional state spaces , and outperforms many other evolutionary approaches [ 1 ] .",
    "neat grows networks from minimal structure .",
    "networks start with a single input node , no hidden nodes , and a single output node , to reduce the search space , and only broadens its search as needed . additionally , the algorithm employs a principled method of crossover to address competing conventions , when many genomes express the same phenotype ( behavior ) .",
    "lastly , neat protects innovation that may require several generations to optimize by separating genomes into species .",
    "this allows new genomes to only compete with similar genomes instead of the entire population .",
    "these three properties make neat ideal for evolving networks to use combinatorially generated piecewise activation functions , which is why i chose it for my experiments .",
    "other variations of neat also exist and were considered [ 2 , 3 , 4 ] , but provided unnecessary additional behaviors .",
    "the original neat was tested on several pole balancing control benchmarks .",
    "here i only focus on its hardest benchmark , double pole balancing with no velocity information . in this task ,",
    "neat controls a virtual cart on a finite length track with two poles on top attached with hinges .",
    "it is also given a fitness score , which is calculated by the total number of times both poles remained balanced , and the angular information of the poles .",
    "poles are considered balance as long as they do not fall more than 36 degrees from the starting position .",
    "to succeed , networks needed to balance the poles for 1,000 time steps each generation for 100 generations , which is about 30 minutes .",
    "the best performing networks from each species each generation were then tested for generalization performance . for these tests",
    ", they needed to pass at least 200 out of 625 different starting states . for each starting state",
    ", its networks needed to balance both poles for 1,000 time steps .",
    "for this study , i developed a modified version of neat , which i cleverly dubbed neatwise",
    ". neatwise evolves networks as the original , but when it mutates nodes into the network , it selects functions to represent the node s resting state and active state from a pool of activation functions .",
    "i was initially inspired by several previous studies [ 5 , 6 , 7 , 8 ] that showed that relu , which is a piecewise activation function , performed better than others in complex architectures .",
    "canonical activation functions , such as the logistic function , are typically chosen for their nice mathematical properties - continuous differentiability , monotonicity , and fast computation - so networks can be optimized with backpropagation .",
    "continuous differentiability allows for weights to be updated .",
    "monotonicity ensures a convex error surface .",
    "sigmoidal functions are usually bounded above and below , and are easy to compute .",
    "although these are desired properties for networks using backpropagation , neuroevolution does not impose such constraints and is therefore capable of exploring a wider range of activation functions .",
    "for this study , i created a pool of seven non - parametric canonical activation functions ( sine , sigmoid , arctan , tanh , bent identity , relu , and elu ) that could be paired together to generate piecewise smooth functions , so i could observe different mathematical properties .",
    "each time the algorithm mutated a new node into the network , the node selected two functions uniformly at random from the pool for the resting state and the active state .",
    "this meant the algorithm could generate 49 different piecewise activation functions . for a network of _ n _ nodes ,",
    "there are @xmath0 possible configurations of activation functions for a single topology .",
    "this exponential search space further justifies my choice for neat , which was designed to handle high - dimensional search .",
    "after initial experiments , i discovered that neatwise using all seven functions always failed .",
    "i observed that the population fitness continuously dropped below the fitness of the previous generation when the algorithm halted .",
    "i hypothesized that this was likely due to the exponential search space . to confirm this",
    ", i evaluated the accuracy of each canonical function on the original neat .",
    "the best performing networks were the out of the box neat which used a sigmoid with a slope of 4.924273 and neat which used an unaltered arctan with a slope of 1 as shown in table 1 .",
    "i then restricted neatwise s pool to these two functions to reduce the search space .",
    "however , neatwise still failed all experiments .",
    "i then observed that by increasing the drop - off age and biasing activation function selection towards arctan , networks survived and completed the task with a higher accuracy than the original neat that only used a scaled sigmoid .",
    "the success of the networks was due to preserving the search bias , which was being destroyed by uniform random noise .",
    ".out of the box and homogeneous neats [ cols=\"^,^\",options=\"header \" , ]",
    "as this is a novel approach for evolving activation functions , there are plenty of opportunities for future work .",
    "one of the most challenging aspects that could be heavily improved on is developing a mechanism for more principled automatic activation function selection that adapts to the problem domain .",
    "my experiments required information about each of the canonical activation function s performance _ a priori_. future research could also examine the effects of different activation functions , such as parametric functions , or distance functions as presented by duch and jankowski [ 15 ] . analyzing and optimizing biases could also be further examined by defining a distribution over the pool of functions .",
    "multi - piece piecewise activation functions could potentially allow for even greater degrees of freedom .",
    "combinatorially generating piecewise activation functions in different architectures or with different neuroevolutionary algorithms would be very important to further explore the strengths and weaknesses of this technique .",
    "networks generated with this technique could be examined to further delve into what suboptimal structures or mechanisms cause populations of networks to degenerate .",
    "additionally , research could be done to determine the optimal non - topological hyperparameters such as drop - off age , mutation rate , or initial population size as the problem domain changes .",
    "combinatorially generating piecewise activation functions with neat outperforms the out of the box solution on the challenging stochastic reinforcement control task , dpnv . although neatwise failed experiments more often than neat , neatwise generated networks with a higher average success rate and achieved solutions with a comparable network sizes , generalization scores , but with slightly greater number of evaluations .",
    "this technique enhances the expressive power of _ neuroevolution of augmenting topologies _ , and automatically selects appropriate models for dpnv .",
    "giving neural networks more precise tools enables them to more accurately solve and adapt to tasks , which is important for a general purpose algorithm such as this .",
    "[ 1 ] stanley , kenneth o. , and risto miikkulainen . evolving neural networks through augmenting topologies .",
    "evolutionary computation 10.2 ( 2002 ) : 99 - 127 .",
    "[ 3 ] stanley , kenneth o. , bobby d. bryant , and risto miikkulainen . `` evolving adaptive neural networks with and without adaptive synapses . ''",
    "evolutionary computation , 2003 .",
    "the 2003 congress on .",
    "4 . ieee , 2003 .",
    "[ 11 ] d. white and p. ligomenides , `` gannet : a genetic algorithm for optimizing topology and weights in neural network design , '' in proc .",
    "workshop artificial neural networks ( iwann93 ) , lecture notes in computer science , vol .",
    "berlin , germany : springer - verlag , 1993 , pp .",
    "322 - 327 .",
    "[ 18 ] turner , andrew james , and julian francis miller .",
    "`` cartesian genetic programming encoded artificial neural networks : a comparison using three benchmarks . '' proceedings of the 15th annual conference on genetic and evolutionary computation .",
    "acm , 2013 .",
    "[ 19 ] sebald , anthony v. , and kumar chellapilla .",
    "`` on making problems evolutionarily friendly part 1 : evolving the most convenient representations.''evolutionary programming vii .",
    "springer berlin heidelberg , 1998 ."
  ],
  "abstract_text": [
    "<S> in the neuroevolution literature , research has primarily focused on evolving the number of nodes , connections , and weights in artificial neural networks . </S>",
    "<S> few attempts have been made to evolve activation functions . </S>",
    "<S> research in evolving activation functions has mainly focused on evolving function parameters , and developing heterogeneous networks by selecting from a fixed pool of activation functions . </S>",
    "<S> this paper introduces a novel technique for evolving heterogeneous artificial neural networks through combinatorially generating piecewise activation functions to enhance expressive power . </S>",
    "<S> i demonstrate this technique on neuroevolution of augmenting topologies using arctan and sigmoid , and show that it outperforms the original algorithm on non - markovian double pole balancing . </S>",
    "<S> this technique expands the landscape of unconventional activation functions by demonstrating that they are competitive with canonical choices , and introduces a purview for further exploration of automatic model selection for artificial neural networks . </S>"
  ]
}