{
  "article_text": [
    "hidden markov models have proven their usefulness across a wide variety of applications . in many of these applications , the user or",
    "the experimenter will have some way of influencing the transitions of the underlying markov chain , as in markov decision processes , and such a process is called a partially observed markov decision process ( pomdp ) , see monahan @xcite .",
    "if we assume that the transition probability matrix is governed by some unknown parameters , an important problem is to understand how the process can be influenced to get data that is most informative about the parameters .",
    "we can think of this as experimental design for partially observed markov decision processes .",
    "we consider a pomdp @xmath0 . in this setting @xmath1",
    "is an unobserved markov chain , where the transition probabilities depend in a parametric way on what control @xmath2 is chosen at time @xmath3 and an unknown parameter @xmath4 .",
    "the process @xmath5 is observed and depends on which state @xmath1 is in .",
    "our goal is to find ways to use the controls @xmath2 to improve parameter estimates of @xmath4 .",
    "since the maximum likelihood estimates for @xmath4 will be asymptotically efficient , our general strategy will be to use the controls to try to minimize the sample variance of the maximum likelihood estimates of @xmath6 .",
    "this will be achieved by maximizing a fisher information for @xmath4 .",
    "the controls are calculated using dynamic programming , a popular maximization algorithm from markov decision processes which outputs an adaptive control policy , i.e. the control chosen at time @xmath3 is based on observations up to time @xmath3 .    the first attempt at using dynamic controls to maximize a fisher information was by lin et al .",
    "@xcite who proposed maximizing the fisher information that corresponds to direct observations of the underlying process @xmath1 , labeled the full information fisher information ( fofi ) , and using a filter to compute @xmath1 if it is not observed directly .",
    "this paper extends their work by making use of the pomdp structure and proposes maximizing a fisher information that is based on the observations @xmath5 , labeled the partial observation fisher information ( pofi ) .",
    "the control policies based on these two fisher informations , fofi and pofi , are compared , and we illustrate a setting in which the two control policies are quite different .",
    "the methods we use to calculate controls for maximizing fisher information will depend on the unknown parameter @xmath4 .",
    "we illustrate how this problem can partially be overcome by assuming a prior for @xmath4 to calculate a control policy before running the experiment .",
    "additionally we describe how , using data acquired as the experiment progresses , a posterior for @xmath4 can be used to calculate a more precise control policy .",
    "that is , parameter information from observations acquired at a time @xmath3 can be used to improve the policy used in what is left of the experiment .",
    "these methods will be based on the value iteration algorithm ( via ) , which is closely related to dynamic programming .",
    "the methods developed have application value beyond partially observed markov decision processes .",
    "lin et al .",
    "@xcite considered stochastic systems of the form @xmath7 where @xmath4 is the parameter of interest , to be estimated , @xmath8 is a control that can be chosen by the user , @xmath9 is the vector of state variables , @xmath10 is a vector valued function , @xmath11 a wiener process , and additionally @xmath12 is only observed partially or noisily . by discretizing time ,",
    "state and observation spaces the process can be approximated by a pomdp , allowing us to use the methods developed in this paper to devise a control policy that maximizes information about the parameter @xmath4 .    in order to illustrate our methods we present four examples , ordered by level of complexity , with the first two being pomdp s and the latter two continuous stochastic systems .",
    "we use the unknown @xmath4 to calculate controls in the first three examples to highlight the differences between pofi and fofi , but in the last example we examine means to deal with the dependence of the pofi and fofi policies on the parameter of interest .    in the first example we hypothesize about the kind of systems in which we will observe large improvement in parameter estimation by using the pofi control policy over the fofi policy . following a discussion",
    "we construct a mock partially observed markov decision process , in which this improvement is shown using a simulation study .    to illustrate the real - world applicability of design in discrete pomdp s we consider a realistic pomdp from experimental economics .",
    "the model will consist of a simple adversarial game similar to the  rock - paper - scissor \" game where one player tries to play in such a way that maximizes information about the other players strategy .",
    "the third example is a stochastic version of the morris - lecar neuron model , a dynamical system which models voltage in a single neural cell .",
    "this model is two dimensional , but only one dimension is observed .",
    "the model has multiple parameters and we investigate how the pofi and fofi control policies perform in estimating them .",
    "fourthly we consider an example from biology , a polymerase chain reaction ( pcr ) experiment where dna template is grown in liquid substrate .",
    "the population dynamics are modeled in a dynamical system with stochastic errors , and the aim is to estimate the half - saturation constant , a parameter which controls the saturation of the template .",
    "here we compare using a prior for @xmath4 and using via to calculate a control policy .    in section @xmath13",
    "we describe the pomdp setup , and then introduce dynamic programming , an algorithm from markov decision processes .",
    "this algorithm is then applied to evaluate control policies that maximize both the partial observation fisher information and the full observation fisher information .    in section @xmath14",
    "we show how the control policies can be applied to pomdp examples and then , in section @xmath15 , describe how stochastic dynamical systems can be transformed to pomdps by using discretization methods , and give an example of such a system . in section @xmath16 we discuss the problem of parameter dependence when running the dynamic programs , and how these issues can be dealt with .",
    "this is illustrated in the last example , and a discussion follows .",
    "we consider a markov decision process @xmath17 . in this setting @xmath18",
    "is a markov chain , but the transition probabilities at time @xmath3 depend on a control @xmath2 chosen at that time .",
    "we assume a finite state space @xmath19 for the state process @xmath20 and that the controls available belong to some finite set @xmath21 .",
    "we let @xmath22 denote the size of @xmath19 and @xmath23 the size of @xmath21 .",
    "the transition probabilities are assumed to be parametric and we write @xmath24 short for @xmath25 where @xmath26 and @xmath27 .",
    "in addition to this we assume that the process @xmath18 is latent and we only observe the related observations @xmath28 whose relation to the @xmath18 can also depend on @xmath4 .",
    "we write @xmath29 short for @xmath30 , where @xmath31 and @xmath32 , and let @xmath33 denote the size of @xmath34 .",
    "this makes the system a partially observed markov decision process .",
    "it has a finite horizon @xmath35 in which we observe @xmath36 .",
    "we will use the short hand notation @xmath37 to denote @xmath38 , i.e. the observations between time @xmath39 and @xmath3 , and analogous notation for @xmath2 and @xmath1 .",
    "the objective is to use the controls to maximize the information we get about the parameter @xmath4 through the observed process @xmath40 .",
    "the parameter estimation is done using maximum likelihood and it is therefore natural to try to maximize the fisher information of our observed process which we can express as @xmath41\\ ] ] which we label as the partial observation fisher information ( pofi ) , see appendix [ sec : express fi ] for details on its construction . when we consider continuous time dynamical systems the observation spaces will be continuous , but we will use this discretized fisher information as an approximation to the actual fisher information of the observations .      in order to maximize the fisher information we employ the techniques of stochastic dynamic programming .",
    "assuming a reward function @xmath42 , our objective is to maximize the total expected reward @xmath43 $ ] by use of the controls .",
    "the essence of dynamic programming is that by starting at time @xmath44 and working backwards , we can compute an optimal policy that maps a state @xmath1 to a control @xmath2 that accounts for the choices of @xmath2 that we will make in the future .    in a generic dynamic program we set @xmath45 and and then going backwards from @xmath46",
    "solve @xmath47 \\}\\ ] ] where @xmath48 is called the value function , and we get the associated control @xmath49 \\}\\ ] ] for every state @xmath1 .",
    "this will give us a policy of what control to use at a certain state @xmath1 at a certain time @xmath3 .",
    "the use of these controls will maximize the total expected reward @xmath50 $ ] .",
    "we refer to puterman @xcite for a detailed description of dynamic programming .    with this in mind , we set @xmath51 and we try to maximize the fisher information @xmath52 $ ] .",
    "note that in this instance the reward function depends on the entire history of observations and controls up to time @xmath3 .",
    "the value function in the corresponding dynamic program is @xmath53    \\right \\}\\ ] ] and we denote it the fisher information to go .",
    "a problem here is that just in the first step of the dynamic program ( @xmath54 ) we would have to calculate the fisher information to go for @xmath55 many combinations of @xmath56 and @xmath57 .",
    "this is formidable for even modest dimensions .",
    "we therefore approximate the process by conditioning only on the last @xmath58 observations in the fisher information ; @xmath59 where @xmath60 is some prior that we assume for @xmath61 , although we generally suppress it in notation since we assume it is fixed . if @xmath62 we set @xmath63 to mean @xmath64 to ease notation .",
    "the reward becomes @xmath65 and @xmath66    \\right \\}$ ] the fisher information to go .",
    "the pseudocode for the corresponding dynamic program is given in appendix [ sec : dyn pofi ] .",
    "for this approximate dynamic program to be sensible we want the approximated fisher information to approach the true fisher information as @xmath39 increases .",
    "this holds given that the pomdp process satisfies certain technical mixing conditions , which we have stated in detail in assumption [ mixing ] in appendix [ sec : mixing ] .",
    "assume the conditions in assumption [ mixing ] hold .",
    "then , for @xmath67 and any control policy , we have @xmath68 [ fi approx ] where the constant @xmath69 and @xmath70 do not depend on @xmath39 or @xmath35 , see appendix [ sec : bounds ] .",
    "the proof requires extensions of work in cappe et al .",
    "@xcite and is given in appendix [ sec : bounds ] with further technical results reserved to appendix 2 .",
    "theorem [ fi approx ] states that @xmath71 approaches the true fisher information exponentially as @xmath39 increases , and is thus a viable approximation for the fisher information in the dynamic program .",
    "the runtime of the dynamic program however also grows exponentially in @xmath39 and we found that while setting @xmath72 , i.e. conditioning on one observation , gave poor results in some of our simulations , conditioning on two observations , i.e. @xmath73 , generally gave good results when compared to other control policies . setting",
    "@xmath74 increased runtime greatly and was in most of our applications infeasible without making more approximations to how the dynamic program is run .",
    "the exact effect of increasing @xmath39 is problem specific , and a thorough analysis of what @xmath39 to choose is beyond the scope of this paper .",
    "an alternative method to choose controls was proposed by lin et al .",
    "they considered constructing an optimal control policy for the fisher information that would apply if @xmath75 were observed directly ; @xmath76 this is labeled the full observation fisher information ( fofi ) . as noted before , when considering continuous time stochastic systems , the state space is continuous , but we use this fisher information as an approximation to the continuous state fisher information .",
    "an advantage of using fofi over pofi is that when running the dynamic program the markov property of the markov decision process @xmath77 allows us to only consider a maximization over the state space @xmath78 but not past values @xmath79 .",
    "the dynamic program for fofi is given in appendix [ sec : dyn fofi ] .",
    "however , maximizing fofi can lead to suboptimal controls since it is not the correct fisher information for the data .",
    "additionally , when the actual experiment is run we do not observe @xmath18 .",
    "instead we have to use the observed values to get a probability distribution ( a filter ) on the state @xmath1 , @xmath80 and use the control associated with the state that has the highest probability .",
    "the computation cost of running a dynamic program with fofi is @xmath81 , generally lower than that of pofi ; @xmath82 .",
    "the cost of estimating the state @xmath18 at runtime is @xmath83 at each time point @xmath3 .",
    "for details see b.1 in appendix 2 .      after running an experiment , using one of the control policies , the parameter @xmath4 is estimated either via an em algorithm or by directly maximizing the loglikelihood .",
    "these two estimation methods had similar accuracy in our simulations , although the em algorithm was generally slower .",
    "convergence of the em algorithm is discussed in cappe et al .",
    "@xcite for hidden markov models and extends naturally to pomdp s .    for the asymptotic properties of the mle we refer to cappe as well @xcite , where conditions for consistency and asymptotic normality in hidden markov models are given",
    "the central elements of their proof are the stationarity of the process @xmath84 along with certain forgetting properties of the filter , meaning that ignoring all but the past @xmath39 observations ( as we do ) yields an error in the filter distribution that decreases exponentially in @xmath39 .",
    "we note that if we employ a time - independent control policy ( as we do in section 5 ) , we obtain to a hidden markov model and can rely on @xcite if we assume stationarity . in appendix 2",
    "we establish extensions of forgetting properties for pomdp models more generally , which points to a more general asymptotic theory for the mle in this case , but do not pursue this here .",
    "theorem [ fi approx ] shows that using @xmath71 is a good approximation to the partial observation fisher information for running a dynamic program .",
    "this provides a control policy that is an approximation to the optimal control policy .",
    "now consider using this approximate policy to run an experiment and then estimating @xmath4 by evaluating the mle .",
    "the asymptotic variance of this mle will be the the inverse of the partial observation fisher information , with controls from the approximate policy .",
    "it is therefore of interest to compare pofi with an optimal policy and pofi with the approximate policy . in appendix",
    "[ sec : best fi ] we show    given that the mixing conditions in assumption [ mixing ] ( [ sec : mixing ] ) hold we have @xmath85 [ best fi ]    where @xmath86 are the optimal controls , @xmath87 the approximate optimal controls and fi is the partial observation fisher information . the constant @xmath88 and @xmath70 do not depend on @xmath39 or @xmath35 , see appendix [ sec : best fi ] for details .    that the asymptotic variance of the mle converges to the best possible asymptotic variance exponentially quickly in @xmath39 further supports our approximations .",
    "while the fofi strategy has been shown to be effective in lin et al . @xcite it is possible to define systems in which the strategy is not optimal and may in fact be worse than just using fixed or random controls .",
    "usually certain parts of state space will give more information about a parameter than others , given that the state space is perfectly observed .",
    "in these cases optimal controls would try to move the process to these states .",
    "however , if the state space is only partially observed , most information might be obtained in different parts of state space and the fofi controls become suboptimal . in cases like",
    "this pofi often does better than fofi . in this example",
    ", we demonstrate a system where fofi and pofi choose very different controls , and using a simulation study , we show that the controls chosen by pofi produce less variable parameter estimates .",
    "consider a discrete time markov chain @xmath1 with state space @xmath89 and a transition probability matrix @xmath90\\ ] ] where the parameter of interest is @xmath91 $ ] and the control is @xmath92 . for",
    "@xmath93 or @xmath94 , choosing the control @xmath95 will increase the probability of the markov chain staying in its current state while choosing @xmath96 will increase the probability of it leaving its state .",
    "+ now assume this process is nt observed directly but through a related process @xmath5 with state space @xmath97 whose transition probabilities depend on which state @xmath1 is in .",
    "we denote the transition probability matrices with @xmath98 given by @xmath99 , p_2= \\left [ \\begin{array}{c c } .5 & .5 \\\\ .5 & .5\\\\",
    "\\end{array } \\right ] , p_3= \\left [ \\begin{array}{c c } 1-\\frac{p}{2 } & \\frac{p}{2 } \\\\",
    "\\frac{p}{2 } & 1-\\frac{p}{2 } \\\\",
    "\\end{array } \\right]\\ ] ] if @xmath1 were observed we would get information about the parameter @xmath100 when @xmath1 leaves state @xmath101 and from @xmath5 when @xmath94 .",
    "the idea here is that since the fofi controls assume the whole state space is observed they might encourage @xmath1 to be in state @xmath101 , while the pofi controls that take into account what is actually observed might choose the controls more intelligently . indeed when calculating the controls according to fofi the long run control is to  leave one s state \" if @xmath94 and  stay in one s state \" if @xmath93 .",
    "it is harder to predict what kind of controls result from using pofi , but the control policy is given in table  [ 6 state pofi ] .",
    "@xmath5 & 1 & 2 & 1 & 2 & 1 & 2 & 1 & 2 + @xmath102 & 1 & 1 & 2 & 2 & 1 & 1 & 2 & 2 + @xmath103 & 1 & 1 & 1 & 1 & -1 & -1 & -1 & -1 +    to illustrate this difference , a simulation study was carried out to test which method performed better : the process @xmath1 was run for @xmath104 steps with @xmath105 , using controls chosen by pofi and again using those chosen by fofi .",
    "additionally we ran a simulation of the same length , but where the control was chosen randomly , with @xmath96 and @xmath95 having equal probability .",
    "then the parameter @xmath100 was estimated using an em algorithm .",
    "this was done 500 times to get an empirical distribution for the estimates of @xmath100 .",
    "the results are given in table  [ 6 state table ] .",
    "estimates of @xmath100 using a pofi policy had the lowest mse and variability , but estimates based on the random policy had lower bias .",
    "estimates using a fofi policy were comparatively worse .",
    "fofi & .0109 & .1098 & .0121 + pofi & .0094 & .0867 & .0076 + random & .0059 & .0941 & .0089 +",
    "the following example describes an application of the above methods in the context of experimental economics .",
    "the problem is derived from sachat et .",
    "@xcite , in which we wish to model how humans change their game - playing strategies over time .",
    ".rewards in the gamble safe game .",
    "the first number is the reward for the row player and the second number the reward for the column player , given a certain outcome . [",
    "cols=\"^,^,^\",options=\"header \" , ]",
    "in order to apply the methods described above to dynamical systems , we need to approximate them by a suitable partially observed markov decision process . we achieve this by discretizing time , state and observation spaces . in this paper ,",
    "the continuous stochastic dynamical systems considered are of the form @xmath106 where @xmath4 is the parameter of interest , to be estimated , @xmath8 is a control that can be chosen by user , @xmath9 is the vector of state variables , @xmath10 is a vector valued function and @xmath11 a wiener process .",
    "the dynamical system is approximated on a fine grid of times @xmath107 and we obtain a discrete - time model @xmath108 where @xmath109 are independent normal random variables . we assume the underlying state variables @xmath1 are only observed partially or noisily .",
    "@xmath110 where @xmath111 .",
    "in order to approximate this as a markov chain , the state space is discretized in each dimension and the model is then thought of as moving between the different boxes .",
    "the probability of moving from box to box is approximated using the normal p.d.f . at the midpoints of the boxes . in the examples in this paper ,",
    "only equidistant discretization is considered , but this restriction can be readily removed .",
    "if we label the two midpoints as @xmath112 and @xmath113 and the area of the second box as @xmath114 this probability is given as @xmath115 @xmath116 where @xmath117 is the dimension of @xmath9 .",
    "the probabilities are then normalized to make sure they sum to @xmath101 .",
    "if the controls @xmath2 can be chosen on a continuous scale then this scale has to be discretized as well .",
    "@xmath118 is then a markov decision process , and one can run the fofi dynamic program .    for the pofi dynamic program the observation space needs to be discretized as well .",
    "the probability of what observation box is observed depends on in which box the underlying markov chain is in .",
    "if we label the midpoint of the underlying markov chain midpoint as @xmath119 and the midpoint of the observed process box midpoint as @xmath120 , and the area of the latter box as @xmath121 this probability is given as @xmath122 these probabilities are also normalized to sum to @xmath101 .",
    "the process @xmath123 is now a partially observed markov decision process and one can run the pofi dynamic program .",
    "the morris lecar model @xcite describes oscillatory electric behavior in a single neural cell , as regulated by flow of potassium and calcium ions across the cell membrane .",
    "these models are defined in terms of state variables @xmath124 and @xmath125 representing the voltage across the membrane and the flux of the potassium channel respectively .",
    "@xmath126    \\dot{n}_t & = -\\phi\\cdot ( n_t - n_\\infty(v_t ) ) / \\tau_n(v_t ) \\label{eq : ml - n}\\end{aligned}\\ ] ]    where @xmath127 , @xmath128 and @xmath129 .",
    "we will write @xmath130 and @xmath131 as shortcuts equations ( [ eq : ml - v ] ) and ( [ eq : ml - n ] ) . the voltage between cells depends on potassium and calcium concentrations , and on the amount of leakage .",
    "the further these factors are away from their equilibriums @xmath132 the greater the rate of change in voltage .",
    "the multiplicative value @xmath125 changes the conductance of the potassium channel and is modeled through the second differential equation in which @xmath125 is driven towards a voltage - dependent equilibrium level defined by @xmath133 but converges to this at a much slower rate then the dynamics of @xmath124 .",
    "the neuron is stimulated by an external applied current , @xmath134 ( our control ) , and @xmath124 is measured . our goal is to maximize information about the parameters @xmath135 and @xmath136 , considered separately .",
    "we consider a stochastic version of this neural firing model , derived from @xcite , by adding @xmath137 and @xmath138 to equations ( [ eq : ml - v ] ) and ( [ eq : ml - n ] ) respectively , where @xmath139 and @xmath140 are independent wiener processes .",
    "stochastic models are important in this context in order to accommodate observable variation in the inter - spike interval where a deterministic model will require a fixed period ; see @xcite , for example .",
    "the first step is to discretize these equations with respect to time .",
    "we get that @xmath141 and @xmath142 where @xmath143 .",
    "we discretized @xmath124 onto the range @xmath144 $ ] and @xmath125 onto @xmath145 $ ] , after running a few trial versions of the model .",
    "both ranges where discretized into @xmath146 intervals .",
    "only @xmath124 is measured and it is measured noisily , @xmath147 where @xmath148 .",
    "the observation space was discretized to the same range as @xmath124 but into @xmath149 intervals .",
    "these approximations give rise to a partially observed markov decision process to which our methods can be applied .",
    "both fofi and pofi controls were calculated for this experiment , for the parameters @xmath150 , @xmath151 and @xmath136 , considered separately .",
    "the values for the parameters were set to be @xmath152 , @xmath153 , @xmath154 , @xmath155 , @xmath156 , @xmath157 , @xmath158 , @xmath159 , @xmath160 , @xmath161 , @xmath162 , @xmath163 and @xmath164 . the controls range was set to be @xmath165 $ ] and discretized to the set @xmath166 .",
    "an example of what the control policy looks like is given in figure  [ fig : morris - lecar ] .",
    "a simulation study was run for each of the three parameters ; the system was simulated within the discretized markov chain framework with @xmath167 time steps and all schemes had 100 simulations .",
    "the parameter in question was estimated for each simulation using an em algorithm . as a baseline comparison we also ran a simulation study using a fixed control ( @xmath168 ) .",
    "the results are given in table  [ morris - lecar ] .",
    "the difference between pofi and fofi turns out to be not very dramatic , likely due to the observations providing a great deal of information about the underlying state variables , which is when fofi performs well .",
    "& fofi & .4234 & 2.4722 & 6.2913 + & pofi & .4129 & 2.4068 & 5.9632 + & fixed & .9098 & 3.4240 & 12.551 + & fofi & .0613 & .3671 & .1385 + & pofi & .0158 & .3706 & .1376 + & fixed & .0249 & .6193 & .3841 + & fofi & .00485 & .01085 & .00014 + & pofi & .00257 & .01037 & .00011 + & fixed & .01357 & .02643 & .00088 +",
    "in the discussion above we calculated the dynamic program assuming knowledge of the parameter @xmath4 , the very thing we wish to estimate with maximal precision . since the dynamic programs we have considered are run before the experiment",
    "is started we generally wo nt have data to estimate @xmath4 .",
    "additionally , for the fofi simulations we have used @xmath4 directly to estimate @xmath1 within the filter to get the appropriate control , but this will not be possible in practice .",
    "there are a few ways of dealing with this .    assuming some prior information one can use a prior for @xmath4 to run the dynamic program . to do this , we add one more expectation for @xmath4 at every time step @xmath3 , and then maximize the expected fisher information to get the best control .",
    "this strategy was employed in lin et al .",
    "@xcite .",
    "the rather obvious deficiency here , for both pofi and fofi , is that as the experiment runs , we get observations that can be used to improve our prior for @xmath4 , and could be used to get better controls , if we could brake the experiment and rerun the dynamic program .      in some systems",
    "the time spent in each state is very short , too short to perform many calculations , making it valuable to have a ",
    "look - up table \" of controls . here",
    "the pofi controls have an advantage over the fofi controls , in the sense that they are of the  look - up \" kind , as fofi requires estimation of the underlying @xmath1 process , before the control can be looked up .",
    "in other systems , there is time to do some calculations between transitions .",
    "note , for example , that at time @xmath3 we have observed @xmath169 and this will allow us to calculate a posterior distribution @xmath170 for our parameter of interest .",
    "this posterior could then be used to run the dynamic program again , as described above , from time @xmath44 to time @xmath3 .",
    "this can be quite time consuming if done at each time step @xmath3 , so we propose a method that relies on the value iteration algorithm .",
    "a popular algorithm from the theory of markov decision processes is the value iteration algorithm ( via ) , see puterman @xcite .",
    "the theoretical motivation of via is similar to dynamic programming , but here the objective is to maximize an expected total reward @xmath139 that has a discounting factor @xmath171 , where @xmath172 , and the time horizon is assumed to be infinite ; @xmath173\\ ] ] and @xmath139 is labeled as the expected total discounted reward .",
    "@xmath139 exists if @xmath174 is bounded , which is the case in the problems we consider . in puterman",
    "@xcite it is shown that an optimal control policy exists and it can be chosen to be time independent , i.e. to depend only on the state @xmath1 , and not the time @xmath3 . moreover",
    "this optimal control can be approximated using the value iteration algorithm described below .",
    "our experimental setting is neither discounted nor has it an infinite time horizon , but blackwell optimality guarantees that controls that maximize @xmath139 also maximize the expected average reward @xmath140 ( or its @xmath175 if the limit does nt exist ) ; @xmath176\\ ] ] given that @xmath171 is chosen close enough to one .",
    "a blackwell optimal control policy exists if the state and action spaces are finite , which is the case in our setting .",
    "maximizing @xmath140 effectively amounts to maximizing the average input of each observation in our fisher information ; a reasonable strategy . how small @xmath177 needs to be is generally hard to determine , and",
    "choosing @xmath171 too high will cause the algorithm to converge slowly . see puterman @xcite chapter @xmath178 for more on blackwell optimality . in",
    "via we we calculate @xmath179    \\right \\}\\ ] ] in a while - loop until @xmath180 converges to some fixed point , within some tolerance .",
    "convergence is guaranteed since each iteration of @xmath180 is a contraction mapping .",
    "our aim with via is to maximize what we in the pofi case label , the average partial observation fisher information @xmath181 ( or in the fofi case , the average full observation fisher information ) which is a reasonable quantity to maximize in order to obtain a time - invariant policy .",
    "we propose running via at every time step @xmath3 , but to use the posterior for @xmath4 , @xmath170 , which is conditioned on all the data observed so far , instead of using the prior for @xmath4 . this will give a control that maximizes the average fisher information , using all the parameter information that is available at time @xmath3 .",
    "instead of starting via at each time @xmath3 with @xmath182 , considerable time can be saved by using the last value vector @xmath180 from the previous run of via at time @xmath183 .",
    "this is because the posterior for @xmath4 often does nt change much between time steps , and the last @xmath180 from time @xmath183 thus being relatively close to the fixed point at time @xmath3 .",
    "let @xmath184 denote the value vector at time @xmath3 at the @xmath185th iteration of the @xmath3th via and let @xmath170 denote the posterior for @xmath4 given observations up till time @xmath3 .",
    "also , to ease notation , let @xmath186 .",
    "the pseudocode for this modified via using pofi is :    set @xmath187 and @xmath188 @xmath189 @xmath190 and calculate and store @xmath191 @xmath192\\ ] ] n = n+1 set @xmath193 now let @xmath194 @xmath195\\ ] ] use control @xmath2 , and observe @xmath196 and then update the posterior for @xmath4 , @xmath197    updating fofi policies online using via can be done in a similar way . in the next example",
    "we compare using updated policies .",
    "polymerase chain reaction is a well established method to copy and multiply dna .",
    "we are interested in modeling the growth dynamics of dna template ( @xmath1 ) , for a fixed amount of substrate .",
    "the model we use is @xmath198 where @xmath199 . here",
    "@xmath1 is the amount of dna template , @xmath200 and @xmath201 the parameters of the model and @xmath2 the control , the percentage of template removed at each time point .",
    "we are interested in estimating the parameter @xmath201 , labeled the half - saturation constant .",
    "a good reference for pcr models is @xcite .",
    "we measure the amount of dna template at each time point , but with an error .",
    "our observations are @xmath202    and thus we have a dynamical system which when discretized becomes a partially observed markov decision process .",
    "the range for @xmath1 was set to be @xmath203 $ ] and then discretized into @xmath204 intervals , and @xmath5 was discretized to the same range , but only into @xmath205 intervals .",
    "the parameter values were set to be @xmath206 , @xmath207 , @xmath208 , @xmath164 and the possible values of the control @xmath209 .",
    "still with the objective of maximizing fisher information , we more realistically assumed priors for the parameters of the system , as discussed above .",
    "we conducted a simulation study using controls based on these priors for both pofi and fofi , and then compared their performance to controls that are updated online using via , also both for pofi and fofi .",
    "as a baseline comparison we also ran simulations using fixed controls and simulations where the true parameter is used ( unrealistically ) to calculate the control policy via dynamic programming as in the previous examples . for fixed controls we report the simulation with the lowest mse , which was when @xmath210 .",
    "the range for @xmath201 was set to be @xmath211 $ ] and then we discretized that interval into 10 points @xmath212 .",
    "we then considered a uniform prior on these points and a prior that is somewhat inaccurate , and puts the weight @xmath213 on the point @xmath214 and gives the others equal weight .",
    "the discounting factor for via was set to be @xmath215 .",
    "our simulation study had the time length @xmath216 and there were @xmath217 simulations for each case .",
    "the parameter @xmath201 was estimated using an em algorithm .",
    "the simulation results are given in table  [ chemostat ] .",
    "c c     & bias & st .",
    "dev . & mse + fofi & 0.1059 & 0.6598 & 0.4465 + pofi & 0.0053 & 0.6189 & 0.3831 +   + & bias & st .",
    "dev . & mse + fofi & 0.0755 & 0.6374 & 0.4120 + pofi & 0.0516 & 0.7051 & 0.4998 +   + fixed & .1264 & .7466 & 0.5734 +    &     & bias & st .",
    "dev . & mse + fofi & 0.0388 & 0.6180 & 0.3834 + pofi & 0.0766 & 0.5999 & 0.3658 +   + & bias & st . dev . & mse + fofi & 0.0713 & 0.6787 & 0.4657 + pofi & 0.0954 & 0.6750 & 0.4648 +   + & bias & st . dev . & mse + fofi & 0.0659 & 0.6235 & 0.3932 + pofi & 0.0323 & 0.6249 & 0.3916 +     +    we note that when we calculate the controls prior to the experiment ( no online updating ) , both the pofi and fofi controls are significantly better than using a fixed control , and pofi seems to do better than fofi when we use an uniform prior .",
    "interestingly in the fofi case , calculating the controls using the inaccurate prior does better then using the uniform prior , likely due to a reduction in prior variance , in spite of additional bias .",
    "accuracy increases in most cases when we allow for online updating using the via algorithm .",
    "starting the via with an uniform prior does better than starting with the inaccurate one , which is probably due to the via having to spend more time  repairing \" the prior . also , we note that via controls with uniform prior have a similar performance to a control policy using the true ( unknown ) parameter .",
    "additionally , in figure  [ via_time ] , we see that using the previous final value vector as the starting value vector of via when going from time point @xmath3 to @xmath218 , does save considerable time , and more so as @xmath3 grows and the posterior for the parameter starts to change less .    , for pofi using a uniform prior for the pcr model .",
    "in this paper we compared two possible ways to conduct experimental design in parametric pomdp s , based on using dynamic programming to maximize either the partial observation fisher information or the full observation fisher information .",
    "settings can arise where controls chosen by fofi are not optimal , due to focusing on the underlying process rather than the observed process , and in these cases controls chosen with pofi often perform better , as in the six state example and the adversarial game . in some of the examples analyzed they performed similarly .    in recent years",
    ", there has been growing interest in statistical procedures within dynamical systems , such as parameter estimation and hypothesis testing , and many of these procedures could be performed more efficiently given good experimental design . in this paper",
    "we fully discretized the state and observational spaces to transform dynamical systems with stochastic errors into partially observed markov decision processes , allowing us to use the methods developed for pomdp s to our advantage .",
    "we also noted how the problem of parameter dependence can be overcome by averaging over a prior .",
    "additionally given that there is enough time between consecutive time steps , we showed how the controls can be efficiently updated online using observations gathered so far , by using a variant of the value iteration algorithm .",
    "this was demonstrated in the pcr example .",
    "finding controls that maximize information about parameters is a computationally challenging task .",
    "we have successfully demonstrated techniques for up to two dimensional systems , for a one dimensional parameter .",
    "adding dimensions in state , parameter or observation space quickly make the methods considered computationally intractable . considering a longer lag of past observations for pofi might also increase accuracy , but again at the cost of computation time . in order to extend these methods to higher dimensions one could possibly use techniques of approximate dynamic programming , see powell @xcite .",
    "we maximize @xmath59 with reward function @xmath65 and fisher information to go @xmath219",
    "\\right \\}\\ ] ] the pseudocode for this dynamic program is : +    @xmath220 @xmath189 @xmath221 and calculate and store @xmath222    \\right \\}$ ] @xmath223    \\right \\}$ ]      we maximize @xmath76 by setting the reward function as @xmath224 .",
    "the pseudocode for this dynamic program is : +    @xmath225 @xmath189 @xmath1 and calculate and store @xmath226    \\right \\}$ ] @xmath227    \\right \\}$ ]      in this section we find useful expressions for the fisher information of an experiment , that are needed to derive convergence arguments and set up dynamical programs .",
    "we use the short hand notation @xmath228 where the dependence on @xmath229 is suppressed .    for data @xmath230",
    "the fisher information for @xmath4 can be expressed in one or two derivatives @xmath231=   e\\left[\\left(\\sum_{t=0}^{t-1 } \\dot{h}_{t+1}\\right)^2 \\right]\\ ] ] and similarly the fisher information to go @xmath232    = e\\left[\\left(\\sum_{t = k}^{t-1 } \\dot{h}_{t+1}\\right)^2   \\middle|y_{0:k},u_{0:k-1}\\right]\\ ] ] where the equality is justified by both quantities being the fisher information for the same observations .",
    "we set @xmath233",
    ".    the fisher information to go can be calculated recursively ( in both one or two derivatives ) :    @xmath234 = e\\left[\\left ( \\dot{h}_{k+1}\\right)^2+fi_{k+1}\\middle|y_{0:k},u_{0:k-1}\\right]\\ ] ] [ fi recursive ]    in the case of using two derivatives this follows from iterated expectation . in one derivative",
    "we have @xmath235\\ ] ] @xmath236\\ ] ] the cross term is @xmath237\\ ] ] @xmath238\\middle|y_{0:k},u_{0:k-1}\\right]\\ ] ] @xmath239\\middle|y_{0:k},u_{0:k-1}\\right]\\ ] ] @xmath240 = 0\\ ] ] thus    @xmath241\\ ] ] @xmath242 \\middle|y_{0:k},u_{0:k-1}\\right]\\ ] ] @xmath243\\ ] ]    @xmath244\\ ] ] and similarly @xmath245\\ ] ] [ fi express ]    this follows from using induction and lemma [ fi recursive ] .",
    "running an exact dynamic program , with @xmath246 as the value function , is not feasible because of the curse of dimensionality , leading us to certain approximations .",
    "we set    @xmath247 where @xmath248 is the assumed distribution of @xmath249 and we will consider it to be fixed and known",
    ". allowing @xmath39 to be negative will ease notation when @xmath250 .",
    "we now set @xmath251\\ ] ] @xmath252\\ ] ] and denote it the approximated fisher information to go . that the formulation in one derivative is equal to the one in two derivatives follows from the individual parts of each sum having a fisher information interpretation .",
    "cappe et al .",
    "@xcite establish forgetting properties of the filter by assuming mixing conditions for hidden markov models .",
    "we use the same conditions , slightly modified to allow for controls , for pomdps ;    modified strong mixing conditions . for each control",
    "@xmath253 there exist a transition kernel @xmath254 and measurable functions @xmath255 and @xmath256 from @xmath257 to @xmath258 such that for any @xmath259 , @xmath260 and @xmath261 , @xmath262 [ mixing ]    cappe et al.s @xcite discussion on what models satisfy these conditions applies analogously to pomdp s .",
    "given these conditions we can prove the following , which is a modification of lemma @xmath263 in @xcite ;    assume the strong mixing conditions in assumption [ mixing ] .",
    "then @xmath264 where @xmath265 and @xmath266 [ lemma 12.5.3 ]    a proof is provided in appendix 2 .      in this section",
    "we show that the approximated fisher information approaches the true fisher information exponentially as one conditions on more and more observations , while using the same controls .    by corollary [ fi express ]",
    "the true fisher information is @xmath267 but since that is hard to optimize we consider @xmath268 see definitions for @xmath269 above .",
    "here we use fisher information in one derivative , but as noted above it is equivalent to using the formulation in two derivatives . also note that where @xmath270 we just set it to @xmath271 and use the initial distribution of @xmath272 .",
    "assume the mixing conditions in assumption [ mixing ] hold",
    ". then    @xmath273 @xmath274 [ bound lemma ]    set @xmath275 which sets an upper bound on the length of @xmath276 .",
    "note that @xmath277 also bounds @xmath278 since @xmath279 . now @xmath273 @xmath280 @xmath281 @xmath282 using theorem [ lemma 12.5.3 ] . setting @xmath283 gives the result , although that might not be the best bound .",
    "we now restate theorem [ fi approx ] from section @xmath284 .",
    "assume the conditions in assumption [ mixing ] hold .",
    "then , for @xmath67 and any control policy , we have @xmath285 where @xmath286 and @xmath287 is the bound from lemma [ bound lemma ] ; @xmath288 . [ fi approx append ]    @xmath289 @xmath290 @xmath291 and by cauchy schwarz @xmath292    for the first parenthesis we use theorem [ lemma 12.5.3 ] to get @xmath293 and the second one is bounded by the lemma [ bound lemma ] .",
    "we get    @xmath294 @xmath295    exactly the same arguments can be used to show that the approximated fisher information to go @xmath296 approaches the true fisher information to go as @xmath39 increases .",
    "a related problem we are interested in is how well controls that consider the last say @xmath39 observations do in comparison with controls that consider all past observations .",
    "that is , we want a bound on the best possible fisher information given controls that consider all past observations , compared with the best possible fisher information given controls that only consider the last @xmath39 observations .",
    "we first establish a baseline difference between two parts of the fisher information .",
    "assume the conditions in assumption [ mixing ] hold .",
    "then , for any control policy and any @xmath117 , we have @xmath297 where @xmath288 is the bound from lemma [ bound lemma ] .",
    "[ ind diff ]    follows from lemma [ bound lemma ] and theorem [ lemma 12.5.3 ] as in the proof of theorem [ fi approx ] .",
    "as above , we assume that the fisher information to go ; @xmath298\\ ] ] is approximated by @xmath299\\ ] ]    given that our controls are obtained by dynamic programming , we have that the optimal control at time @xmath3 is dependent on the optimal control obtained at time @xmath218 .",
    "let @xmath300 denote the set of optimal controls obtained in this manner , i.e. @xmath301 maximize @xmath246 .",
    "as argued these controls are hard to calculate and thus we resort to approximate optimal controls , here denoted @xmath302 , where @xmath303 maximize @xmath296 .",
    "we now restate theorem [ best fi ] from section @xmath304 on the loss of using approximate controls instead of exact ones in fisher information , in an experiment of length @xmath35 .    given that the mixing conditions in assumption [ mixing ] hold we have",
    "@xmath305 where @xmath306 , and @xmath287 is the bound from lemma [ bound lemma ] .",
    "[ best fi append ]    we analyze the difference by bounding errors in each step of the dynamic program inductively , starting at time @xmath54 and going backwards .",
    "set @xmath307 .",
    "we find that @xmath308 @xmath309 so far only using that @xmath310 maximizes @xmath311 and @xmath312 maximizes @xmath313 .",
    "@xmath314 @xmath315 by lemma [ ind diff ] .",
    "we now inductively assume @xmath316 where @xmath317 , and then get @xmath318 @xmath319 @xmath320 now moving from @xmath321 to @xmath322 we have @xmath323 since @xmath324 are the controls that maximize @xmath325 . by adding and subtracting the same quantity",
    "we get the following equivalent inequality @xmath326",
    "@xmath327 @xmath328    line @xmath329 is bounded by @xmath330 by lemma [ ind diff ] and line @xmath331 by @xmath332 using @xmath333 and lemma [ ind diff ] .",
    "therefore    @xmath334 @xmath335 and for the whole experiment we find @xmath336",
    "we analyze the computational complexities of fofi and pofi .",
    "the computations required can be split into computations done prior to the experiment and computations that are required while running the experiment .",
    "a direct comparison is not completely fair since fofi requires computations at runtime while pofi does not as discussed below .",
    "we assume that the transition probability matrix @xmath338 is given .",
    "calculating @xmath339 is negligible compared to the calculations required for the dynamic program ; if we set @xmath340 then for a given time @xmath3 in the dynamic program we need to maximize @xmath341\\ ] ] over @xmath342 for each @xmath78 , where @xmath343 is the value function from the previous step @xmath218 .",
    "this calculation requires adding @xmath344 and @xmath343 which are two @xmath345 tensors with cost @xmath346 .",
    "next we need a dot product between @xmath347 and @xmath348 over the @xmath349 dimension which has cost @xmath350 . finally maximizing over @xmath2 for each @xmath1",
    "has cost @xmath351 .",
    "thus each step @xmath3 has cost @xmath352 and the dynamic program in total has cost @xmath81    in runtime a filter is required to estimate the state @xmath1 .",
    "the filter for time @xmath218 can be calculated via the following recursive formula @xmath353 and then normalizing .",
    "this requires @xmath354 dot products of vectors of length @xmath22 , with cost @xmath83 and the normalization has cost @xmath355 .",
    "thus we have @xmath83 computations at each time step @xmath3 during runtime .      here the dynamic program maximizes the approximated partial observation fisher information , @xmath356\\ ] ] first we note that @xmath357 is a @xmath358 tensor , and it can be calculated using bayes rule at the cost @xmath359 . calculating @xmath360 can also be done at the cost @xmath359 , but can also be effectively approximated using the finite difference approximation to the derivative .",
    "the cost analysis of the pofi dynamic program is just like the analysis of fofi . at a given time @xmath3 adding @xmath344 and @xmath343 has cost @xmath361 , the dot product between @xmath362 and @xmath363 has cost @xmath361 and the maximization has cost @xmath364 .",
    "this section is devoted to expanding hidden markov model theory to partially observed markov decision processes .",
    "we base it completely on cappe et al .",
    "@xcite and use their notation , only changing what is necessary . the purpose is to prove theorem 3 in appendix @xmath366 , which is a modified version of lemma @xmath263 in @xcite . in most cases the changes will amount to adding controls and seeing that the theory follows through , although the proof of theorem 3 has more substantial changes .",
    "let @xmath367 and @xmath368 be the state space and the observations space respectively",
    ". let @xmath369 be a transition kernel for our state space , where @xmath253 is a control , and @xmath21 is finite .",
    "also let @xmath370 be the transition kernel for moving from the state space to the observation space .",
    "we generally assume that the markov chain is initialized with distribution @xmath371 , and then runs for @xmath185 steps @xmath372 and that @xmath373 decisions are made on what controls @xmath253 to use .",
    "this results in @xmath185 observations @xmath374 and @xmath373 control @xmath375 .",
    "a standard result in hmm theory is that conditional on the observations @xmath385 the process @xmath386 still is a markov chain , although non - homogeneous , with a transition kernel called the forward smoothing kernel .",
    "we state the transition kernel here for our case , also conditional on the controls .",
    "the forward smoothing kernel allows us a convenient way of calculating the smoothing distributions .",
    "we first compute all the backward variables @xmath394 using the backward recursion given .",
    "we then note that @xmath395 can be calculated as @xmath396 and then we have the following recursion @xmath397 where @xmath398 are the forward kernels , and the last equation is a short hand way of writing the integral .        to continue towards forgetting properties we need to introduce total variation ( see definition 4.3.1 in @xcite ) .",
    "let @xmath400 be a signed measure ( it can be negative ) and let @xmath401 where @xmath402 are ( positive ) measures .",
    "so if @xmath403 is the state space @xmath404 then we need to define the dobrushin coefficient ( see definition 4.3.7 in @xcite ) .",
    "let @xmath22 be a transition kernel from @xmath403 to @xmath257 .",
    "@xmath405 the dobrushin coefficient is sub - multiplicative ( see prop . 4.3.10 in @xcite ) . if @xmath406 are @xmath13 transition kernels we have @xmath407            when considering forgetting properties it seems logical to show that the filter @xmath414 depends less and less on the initial distribution of @xmath415 , as @xmath117 increases .",
    "specifically we have that when comparing initial distributions @xmath371 and @xmath416 : @xmath417 @xmath418        and since the dobrushin coefficient is sub - multiplicative @xmath422 and since the dobrushin coefficient @xmath423 satisfies @xmath424 we at least have that the difference between the @xmath13 filters is non - expanding .",
    "establishing forgetting properties thus amounts to showing @xmath425 for the forward smoothing kernels @xmath426 .",
    "note that so far no assumptions have been made on how quickly the hidden markov model mixes .",
    "those assumptions are made to get @xmath427 .        in our case",
    "we have different transition kernels for each control .",
    "the weakest assumptions we can get away with is , if each transition kernel @xmath430 has a corresponding transition kernel @xmath431 and measurable functions @xmath432 and @xmath433 satisfying the strong mixing condition . by letting @xmath434 and @xmath435",
    "we see that we can consider the same @xmath436 functions for each transition kernel @xmath430 .",
    "we restate the strong mixing conditions :    modified strong mixing conditions . for each control",
    "@xmath253 there exist a transition kernel @xmath254 and measurable functions @xmath255 and @xmath256 from @xmath257 to @xmath258 such that for any @xmath259 and @xmath260 , @xmath437    lemma @xmath438 in cappe et al . @xcite uses the mixing conditions stated above to establish contracting bounds on the dobrushin coefficient .",
    "we restate the lemma for the pomdp case , where we also condition on the controls , and use the modified mixing conditions .",
    "a.   for any non - negative integers @xmath117 and @xmath185 such that @xmath439 and @xmath261 , @xmath440 ( x ) \\leq \\prod_{j = k+1}^n \\varsigma^+(y_j)\\ ] ] b.   for any non - negative integers @xmath117 and @xmath185 such that @xmath439 and any probability measures @xmath371 and @xmath416 on @xmath367 , @xmath441(x)}{\\int \\nu'(dx ) \\beta_{k|n}[y_{k+1:n},u_{k : n}](x ) } \\leq   \\frac{\\varsigma^+(y_{k+1})}{\\varsigma^-(y_{k+1})}\\ ] ] c.   for any non - negative integers @xmath117 and @xmath185 such that @xmath439 , there exists a transition kernel @xmath442 from @xmath443 to @xmath367 such that for any @xmath444 , @xmath445 , and @xmath446 , @xmath447(x , a)\\ ] ] @xmath448 d.   for any non - negative integers @xmath117 and @xmath185 , the dobrushin coefficient of the forward smoothing kernel @xmath449 $ ] satisfies @xmath450 ) \\leq \\rho_0(y_{k+1 } ) : = 1 -\\frac{\\varsigma^-(y_{k+1})}{\\varsigma^+(y_{k+1})}\\ ] ] if @xmath439 , and @xmath450 ) \\leq 1 -\\int \\varsigma^-(y ) dy\\ ] ] if @xmath451 .",
    "a.   letting @xmath452 in the strong mixing conditions we find that for all @xmath253 @xmath453 we also have @xmath454 @xmath455 @xmath456 @xmath457 @xmath458 the other inequality is similar .",
    "b.   using the recursion for the backward variables we find @xmath459 @xmath460 @xmath461\\beta_{k+1|n}(y_{k+2:n},u_{k+1:n},dx_{k+1})\\ ] ] @xmath462\\beta_{k+1|n}(y_{k+2:n},u_{k+1:n},dx_{k+1})\\ ] ] @xmath463 we get a similar inequality for @xmath255 .",
    "also note that the last integral does nt depend on @xmath371 , so it cancels when we take the ratio .",
    "the result follows .",
    "c.   we have that @xmath464(x , a ) = \\frac{\\int_a q^{u_k}(x , dx_{k+1})g(x_{k+1},y_{k+1 } ) \\beta_{k+1|n}(x_{k+1})}{\\int q^{u_k}(x , dx_{k+1})g(x_{k+1},y_{k+1 } ) \\beta_{k+1|n}(x_{k+1})}\\ ] ] @xmath465 and we can set @xmath466 d.   using @xmath467 we find that @xmath464(x , a ) \\geq \\frac{\\varsigma^-(y_{k+1})}{\\varsigma^+(y_{k+1 } ) } \\lambda_{k|n}(y_{k+1:n},u_{k : n},a)\\ ] ] and thus assumption @xmath468 holds and lemma @xmath469 gives @xmath470      a.   we let @xmath371 and @xmath416 be two different initial distributions for @xmath471 .",
    "now for @xmath472 @xmath473 - \\phi_{\\nu',k|n}[y_{0:n},u_{0:n-1}]||_{tv}\\ ] ] @xmath474||\\phi_{\\nu,0|n}[y_{0:n},u_{0:n-1 } ] - \\phi_{\\nu',0|n}[y_{0:n},u_{0:n-1}]||_{tv}\\ ] ] @xmath475\\ ] ] b.   for any non - negative integers",
    "@xmath476 such that @xmath477 @xmath478 @xmath479 where @xmath371 is the initial distribution of @xmath471 .",
    "a.   earlier we had @xmath480 and the first inequality now follows from the lemma @xmath438 part @xmath481 .",
    "the factor `` 2 '' follows from using the triangle inequality on the difference of two probability measures .",
    "b.   this is just like part @xmath482 except we consider different initial distributions for @xmath483 .",
    "it is easily seen that @xmath493 is minimized as a function of @xmath4 at @xmath494 and thus @xmath495 assuming we can exchange derivatives with integration .",
    "the last equation is called fisher s identity .          and @xmath499\\ ] ] @xmath500 + e_{\\theta'}[\\log g(x_0,y_0;\\theta)|y_{0:n},u_{0:n-1}]\\ ] ] @xmath501\\ ] ] we set @xmath502 and get @xmath503 @xmath504    + e_{\\theta}[\\frac{\\partial}{\\partial \\theta}\\log g(x_0,y_0;\\theta)|y_{0:n},u_{0:n-1}]\\ ] ] @xmath505\\ ] ]          we now wish to use the expression for @xmath508 derived in the last section .",
    "we have that @xmath509 but also @xmath510 this gives an alternative expression of @xmath511 .",
    "we get @xmath512 and for @xmath513 @xmath514 @xmath515\\ ] ] @xmath516\\ ] ] this expression can be generalized to starting the process at other values than zero ; @xmath517\\ ] ] @xmath518\\ ] ] @xmath519\\ ] ] this is done in cappe et al .",
    "@xcite to extend the process to minus infinity ( @xmath520 ) .",
    "we do nt extend the process to infinity , but rather think of @xmath39 as indicating lack of information , that is assuming that the process starts at @xmath521 .      assuming strong mixing conditions . then for @xmath513 cappe et al .",
    "@xcite prove the following inquality in the hmm case : @xmath522\\right)^{1/2 }     \\frac{\\rho^{(k+m)/2 - 1}}{1-\\rho}\\ ] ] we do nt extend the process to @xmath523 , but rather starting at @xmath471 and we prove the following inequality , also for @xmath513 @xmath524 where @xmath525 ( see theorem [ lemma 4.3.22 ] ) . [ lemma 12.5.3 ]      just like in the proof of lemma @xmath263 in @xcite we match together different pairs of terms within the sums , depending on their index @xmath119 .",
    "more specifically for @xmath530 we match together the terms where @xmath530 in @xmath333 and @xmath331 . for @xmath531",
    "we match the terms in @xmath333 with @xmath331 and the terms in @xmath329 with those in @xmath532 . for @xmath533",
    "we match terms in @xmath333 with terms in @xmath329 and terms in @xmath331 with those in @xmath532 . that leaves @xmath534 in @xmath535 where we match @xmath333 and @xmath329 .",
    "if we look at the case where @xmath333 is matched with @xmath331 we have @xmath536     -e[\\phi_{\\theta}(x_{i-1},x_i , y_i , u_{i-1})|y_{1:k},u_{0:k-1}]||\\ ] ] @xmath537 @xmath538|\\ ] ] @xmath539 where @xmath540 $ ] is the forward smoothing kernel , and the inequality stems from proposition @xmath541 @xmath482 where the second line can be thought of as two different initial distributions for @xmath521 , and the kernel @xmath542 is bounded by @xmath101 .",
    "matching @xmath329 with @xmath532 is similar . for matching @xmath333 with @xmath329 and @xmath331 with @xmath532",
    "we need a `` backwards bound '' ; @xmath543 @xmath544 that is established below , see theorem [ prop 12.5.4 ] . for matching @xmath331 with @xmath532",
    "we get @xmath545\\ ] ] @xmath546||\\ ] ] @xmath547 @xmath548|\\ ] ] @xmath549 where @xmath550 is the backwards smoothing kernel described below . matching @xmath333 with @xmath329",
    "is a special case of the above .",
    "going back to our orginal objective , we have @xmath551 where @xmath552 is a sum over the pairs we considered above .",
    "now by minkowski s inequality we have @xmath553 now we have that @xmath554 where @xmath555 is the power of @xmath70 associated with @xmath556 .",
    "@xmath557 at this point cappe et al .",
    "@xcite argue that since in their case the process was started at inifinity and the process is homogeneous the expected value over @xmath558 is always the same by stationarity , and @xmath558 can be exchanged by @xmath559 . since argueing for stationarity is more of stretch for us , we also take the supremum over @xmath257 and remember that that set is also finite .",
    "@xmath560 @xmath561 we now deal with the sum of @xmath70 to different powers .    from @xmath530 we have @xmath562 where we matched @xmath333 with @xmath331 . for @xmath563",
    "we have @xmath564 where we matched @xmath333 with @xmath331 and @xmath329 with @xmath532 .",
    "for @xmath565 we have @xmath566 from matching @xmath333 with @xmath329 and @xmath331 with @xmath532 .",
    "finally for @xmath567 we have @xmath568 from matching @xmath333 with @xmath329 .",
    "this gives @xmath569 @xmath570 @xmath571 thus , finally we have @xmath572      the idea behind this proof is to replicate all the results derived so far for the backward smoothing kernel .",
    "that is , conditional on @xmath575 , @xmath576 and @xmath577 the time - reversed process @xmath403 is a non - homogeneous markov chain , where the conditional probability of moving from @xmath578 to @xmath483 given all the observations @xmath579 , controls @xmath580 and initial condition ends up only depending on @xmath581 , @xmath582 and the initial condition , and is governed by the backwards smoothing kernel given by @xmath583(x , f)\\ ] ] @xmath584 just as we did in lemma @xmath438 we can show @xmath585\\ ] ] @xmath586(x_j,\\cdot \\,)\\ ] ] @xmath587\\ ] ] where @xmath588(f)\\ ] ] @xmath589 as we showed there this gives @xmath590 we now get that the 2 smoothers we are interested in can be thought of as smoothers of the reversed markov chain from @xmath591 to @xmath39 with 2 different initial distributions for @xmath592 , the starting position .",
    "we get @xmath593 @xmath594 @xmath595 @xmath596 ( where @xmath597 )"
  ],
  "abstract_text": [
    "<S> this paper deals with the question of how to most effectively conduct experiments in partially observed markov decision processes so as to provide data that is most informative about a parameter of interest . </S>",
    "<S> methods from markov decision processes , especially dynamic programming , are introduced and then used in an algorithm to maximize a relevant fisher information . the algorithm </S>",
    "<S> is then applied to two pomdp examples . </S>",
    "<S> the methods developed can also be applied to stochastic dynamical systems , by suitable discretization , and we consequently show what control policies look like in the morris - lecar neuron model , and simulation results are presented . </S>",
    "<S> we discuss how parameter dependence within these methods can be dealt with by the use of priors , and develop tools to update control policies online . </S>",
    "<S> this is demonstrated in another stochastic dynamical system describing growth dynamics of dna template in a pcr model . </S>"
  ]
}