{
  "article_text": [
    "the amount of texts readily available in the world is growing at an astonishing rate ; classifying these texts through machine learning techniques , promptly and without much human intervention , has thus become an important problem in data mining .",
    "much research , in the field of supervised learning , has been done to find accurate algorithms to classify documents in a dataset to their appropriate categories , e.g. see @xcite or @xcite for a detailed survey of text categorization .",
    "the most widely used model for text categorization is the vector space model ( vsm ) @xcite . under this model",
    ", a data dictionary @xmath0 consisting of unique words across the documents in the dataset is constructed .",
    "the documents are represented by real - valued vectors in the space @xmath1 with dimension equaling to the size of the dictionary . given @xmath2 , the @xmath3-th coordinate of a vector is the relative frequency of the word @xmath3 in a given document .",
    "when some of the documents actual class labels are known and used for training , many well - known classifiers in supervised machine learning , such as svm @xcite , @xmath4-nn @xcite , and random forest @xcite , can then be applied to categorize documents .",
    "text categorization decidedly comes across as a problem of detecting similarities between a given text and a collection of texts of a particular type . although distance - based learning rules for text categorization , such as the @xmath4-nearest neighbour classifier , e.g. @xcite , are not new , they are currently based on the entire feature space , while any dimension reduction steps are done independently beforehand @xcite .",
    "we aim to fill this gap by suggesting a novel supervised learning algorithm for text categorization , called the domain - specific classifier .",
    "it discovers specific words for each category , or domain , of documents in training and classifies based on similarity searches in the space of word frequency distributions supported on the respective domain - specific words .    for each class label , @xmath5",
    ", our algorithm extracts class , or domain , specific words from labeled training documents , that is , words that appear in the class @xmath6 more frequently than in all the other document classes combined ( modulo a given threshold ) .",
    "now a given unlabeled document is assigned a label @xmath6 if the normalized frequency of domain - specific words for @xmath6 in the document is higher than for any other label .    to see that this classifier is indeed similarity search based , let @xmath7 be a binary vector whose @xmath3-th coordinate is @xmath8 if and only if @xmath3 is domain - specific to @xmath6 , and 0 otherwise . normalize @xmath9 according to the @xmath10 distance , and let a document be represented by a vector @xmath11 .",
    "then the label assigned to @xmath12 is that of the closest neighbour to @xmath12 among @xmath13 with regard to the simplest similarity measure , the inner product on @xmath14 . in other words ,",
    "we seek to maximize the value of @xmath15 over @xmath5 .",
    "notice that the well - known cosine similarity measure , cf .",
    "e.g. @xcite , corresponds to the special case @xmath16 .",
    "this algorithm was first used in the 3rd cybersecurity data mining competition ( cdmc 2012 ) to notable success , as the team of authors placed second in the text categorization challenge , and first in classification accuracy @xcite .",
    "in addition , the classification performance of the algorithm was validated on a sub - collection of the popular reuters 21578 dataset @xcite , consisting of single - category documents from the top eight most frequent categories , with the standard `` modapt '' training / testing split . in terms of accuracy ,",
    "our classifier performs slightly better than svm with a linear kernel , and is significantly faster .",
    "this paper is organized as follows .",
    "section [ backg ] surveys common feature selection and extraction methods and classifiers considered in the text categorization literature .",
    "section [ domainclass ] explains the new domain - specific classifier in detail and casts it as a similarity search problem .",
    "section [ discuss ] discusses results from the cdmc 2012 data mining competition and experiments from the competition and on the reuters 21578 dataset .",
    "finally , section [ conclude ] concludes the paper with some discussion and directions for future work .",
    "in this section , we describe the vsm model and provide a brief survey on widely known methods for text categorization .    from this section",
    "onwards , following notation similar to @xcite , we let @xmath17 denote the dataset of documents , with size @xmath18 , and @xmath19 the data dictionary of all unique words from documents in @xmath20 , with size @xmath21 . given a document @xmath22 and a word @xmath23",
    ", @xmath24 denotes the number of words in @xmath22 and @xmath25 indicates that the word @xmath3 is found in @xmath22 .",
    "the vector space model ( vsm ) @xcite is the most common model for document representation in text categorization . according to @xcite",
    ", there is usually a standard preprocessing step for the documents in @xmath20 , where all alphabets are converted to lowercase and all stop words , such as articles and prepositions , are removed .",
    "sometimes , a stemming algorithm , such as the widely used porter stemmer @xcite , is applied to remove suffices of words ( e.g. the word `` connection '' @xmath26 `` connect '' ) .    in vsm , the data dictionary , consisting of all unique words that appear in at least one document in @xmath20 ,",
    "is first constructed .",
    "sometimes , @xmath27-grams , which are phrases of words , are also included in the dictionary ; however , the benefit of these additional phrases is still up for debate @xcite .",
    "given the data dictionary , each document can be represented as a vector in the real - valued vector space with dimension equaling the size of the dictionary .",
    "two common methods for associating a document to a vector are explained below .",
    "the simplest method assigns to a document @xmath22 the vector consisting of the relative term frequencies for @xmath22 , see e.g. @xcite .",
    "the second , known as the @xmath28-@xmath29 method , assigns @xmath22 to the vector consisting of the products of term and inverse document frequencies @xcite . mathematically speaking ,",
    "a document @xmath22 is mapped to a real - valued vector of length @xmath30 : @xmath31 , for @xmath32 or @xmath33 where @xmath34 denotes the number of times the word @xmath35 appears in @xmath22 .",
    "other representations include binary and entropy weightings and the normalized @xmath28-@xmath29 method @xcite .",
    "once the documents are represented as vectors , the dataset can be interpreted as a data matrix @xmath36 of size @xmath37 .",
    "however , a main challenge for text categorization is that the size of the data dictionary is usually immense so the data matrix is extremely high dimensional .",
    "dimension reduction techniques must often be applied before classification to reduce complexity @xcite .      due to the potentially large size of the data dictionary , feature selection and extraction methods",
    "are often applied to reduce the dimension of the data matrix .",
    "feature selection methods assign to each feature , a word in the data dictionary , a statistical score based on some measure of importance .",
    "only the highest scored features , past some defined threshold , are kept and a lower dimensional data matrix is created from only these features . some known feature selection methods in text categorization include calculating the document frequency , e.g. @xcite , mutual information @xcite , and @xmath38 statistics , e.g. @xcite .",
    "see e.g. @xcite or @xcite for a thorough study of text feature selection methods .",
    "feature extraction methods transform the original list of features to a smaller list of new features , based on some form of feature dependency .",
    "common well - known feature extraction techniques , as surveyed in @xcite , are latent semantic indexing ( lsi ) @xcite , linear discriminant analysis ( lda ) @xcite , partial least squares ( pls ) @xcite , and random projections @xcite .",
    "well - known classifiers that have been applied to text categorization include the @xmath4-nearest neighbour classifier @xcite , support vector machines @xcite , the naive bayes classifier @xcite , and decision trees @xcite .",
    "we ask the reader to refer to indicated references , or to survey articles , such as @xcite , @xcite , and @xcite .",
    "the paper @xcite provides comparable empirical results on some of these classifiers .",
    "the standard approach in literature for text categorization is that one , or more , feature selection or extraction technique is first applied to a data matrix , since the original data matrix is often extremely high - dimensional . a learning algorithm , independent of the dimension reduction process ,",
    "is then used for classification @xcite .",
    "the novel approach in this paper is that we consider a new classifier based only on extracted class specific words , which naturally reduces time complexity and the dimension of the dataset . in other words ,",
    "the domain - specific classifier both performs dimension reduction and classifies , in consecutive and dependent steps .",
    "our algorithm consists of two distinct stages : extraction of domain - specific words from training samples and classification of documents based on the closest labeled point determined by these domain - specific words .",
    "fix an alphabet @xmath39 and denote @xmath40 as the set of all possible  words \" formed from @xmath39 .",
    "a document @xmath22 is then simply an ordered sequence of  words \" , @xmath41 , and the data dictionary @xmath42 is a subset of @xmath40 .",
    "given a set of labeled documents , we can denote it as @xmath43 , where @xmath44 is a document and @xmath45 is its label , out of a possible @xmath4 different labels .",
    "in addition , we can partition @xmath46 into subsets of documents according to their labels : @xmath47 where @xmath48 is the set of documents of label @xmath6 . then , for a particular label @xmath6 and a word @xmath23 in the data dictionary , we denote @xmath49 as the average proportion of times the word @xmath3 appears in documents with label @xmath6 : @xmath50    domain - specific words are those words which appear , on average , proportionally more often in one label type of documents in @xmath46 than other types .",
    "let @xmath51 .",
    "a word @xmath52 in the data dictionary is _ domain _ ( or _ class _ ) _ @xmath6 specific _ if @xmath53    this definition of domain - specific words depends on the parameter @xmath54 and hence , so does the domain - specific classifier .",
    "as @xmath54 increases from @xmath55 , the number of domain - specific words for each class label decreases ; as a result , @xmath54 can be thought of as a threshold parameter , and an optimal choice for @xmath54 is determined through cross - validation using training data .",
    "let now @xmath22 be an unclassified document .",
    "we associate to it a vector @xmath56 ( a relative frequency distribution of words ) as in eq .",
    "( [ eq : frequency ] ) , that is , for every @xmath57 , @xmath58    let @xmath6 be a label . denote @xmath59 the set of domain - specific words to @xmath6 . define the total relative frequency of domain @xmath6 specific words found in @xmath22 : @xmath60 = \\sum_{t\\in cs_j } w(t)=\\frac{1}{\\vert d\\vert}\\sum_{t\\in cs_j } c(t , d).\\ ] ] the classifier assigns to @xmath22 the label @xmath6 for which the following ratio is the highest : @xmath61}{\\vert cs_i\\vert^{1/p}}.\\ ] ] here , @xmath62 $ ] is a parameter , which normalizes a certain measure with regard to the @xmath10 distance , cf .",
    "below in section [ spacemeasure ] .",
    "a ( positive ) measure on a finite set @xmath0 is simply an assignment @xmath63 to every @xmath57 of a non - negative number @xmath64 ; a probability measure also satisfies @xmath65 .",
    "denote @xmath66 the set of all positive measures on @xmath42 .    fix a parameter",
    "@xmath62 $ ] .",
    "the following is a positive measure on @xmath42 : @xmath67 if @xmath68 , we obtain a probability measure uniformly supported on the set of domain @xmath6 specific words . in general , values of @xmath69 $ ]",
    "correspond to different normalizations of the uniform measure supported on these words , according to the @xmath10 distance .",
    "( the case when @xmath70 , that is , the @xmath71 distance , corresponds to non - normalized uniform measure . )    among the similarity measures on @xmath72 , we single out the standard inner product @xmath73 notice that for every @xmath74 and each @xmath6 , @xmath75}{\\vert cs_j\\vert^{1/p}},\\ ] ] and for this reason , the classification algorithm ( [ eq : classifier ] ) can be rewritten as follows : @xmath76 our classifier is based on finding the closest point @xmath77 to the input point @xmath12 in the sense of the simplest similarity measure , the inner product .",
    "the similarity workload is a triple @xmath78 , consisting of the domain @xmath79 , the similarity measure @xmath80 equal to the standard inner product ( a rather common choice in the problems of statistical learning , cf .",
    "@xcite ) , and the dataset @xmath81 of normalized uniform measures corresponding to the text categories and domain - specific words extracted at the preprocessing stage",
    ".    note that the well - known cosine similarity measure arises in the special case when the normalizing parameter is @xmath16 ; hence , it is not necessary to consider this measure separately .",
    "our experiments have shown that different datasets require different normalizing parameters for @xmath9 , and that the optimal normalization depends on the sizes of the document categories ; section [ conclude ] includes a discussion on this topic .",
    "this section details the experiments and results obtained for the domain - specific classifier , in the 2012 cybersecurity data mining competition and on the reuters 21578 dataset .",
    "all of the programming for this section were done with standard packages in r @xcite and with the specialized packages @xmath82 @xcite and @xmath83 @xcite , on a desktop running windows 7 enterprise , with a intel i5 3.10 ghz processor and 4 gb of ram .      the 3rd cybersecurity data mining competition ( cdmc 2012 ) @xcite , associated with the 19th international conference on neural information processing ( iconip 2012 ) in doha , qatar from november 12 - 15 , 2012 @xcite , included three supervised classification tasks : electronic news ( e - news ) text categorization , intrusion detection , and handwriting recognition .",
    "the domain - specific classifier was first developed by the team of authors for the e - news text categorization challenge , which required classifying news documents to five topics : business , entertainment , sports , technology , and travel .",
    "the documents were collected from eight online news sources .",
    "the words in these documents were obfuscated , and all punctuations and stop words removed . here is a sample scrambled text document paragraph from the competition :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ hujr xj gjxzmuxe fajjaek uo jwxea ursek uyjmx xji k seew eowrjjeer zarwzdek uak wdjkmzxzme kxr ua erreaxzur bmerxzja rzaze zjowuazer xjkuj omrx uzzjowrzrx ojde izxx weied wejwre uxe ojrx rmzzerrwmr rxudxmwr omrx _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    in total , 1063 e - news documents for training , each labeled as one of the @xmath84 topics , were given for the goal of classifying 456 documents .",
    ".information on the dataset size for e - news classification task .",
    "[ cols=\"^,<,^\",options=\"header \" , ]     the domain - specific classifier performed slightly better than svm with the linear kernel , and better than random forest in terms of accuracy . with respect to the f - measure ,",
    "our classifier performed better than svm for categories with large sizes , and better than random forest in 6 of the 8 categories , while svm had a higher f - measure on two of the smaller categories , undoubtably due to svm s class weight adjustment .",
    "computationally , our classifier ran considerably faster than svm and random forest .",
    "in this paper , we have introduced a novel text categorization algorithm , the domain - specific classifier , based on similarity searches in the space of measures on the data dictionary .",
    "the classifier finds domain - specific words for each document category , which appear in this category relatively more often than in the rest of the categories combined , and associates to it a normalized uniform measure supported on the domain - specific words . for an unlabeled document ,",
    "the classifier assigns to it the category whose associated measure is most similar to the document s vector of relative word frequencies , with respect to the inner product .",
    "the cosine similarity measure arises as a special case corresponding to the @xmath85 normalization .",
    "our classifier involves a similarity search problem in a suitably interpreted domain .",
    "we believe that this is the right viewpoint with the aim of further improvements .",
    "it is worthwhile noting that our algorithm is unrelated to previously used distance - based algorithms ( e.g. the @xmath4-nn classifier @xcite ) .",
    "the dataset in the similarity workload is completely different , and as a result , unlike most algorithms in text categorization , this classifier does not require any separate dimension reduction step beforehand .",
    "the process of selecting domain - specific words in our algorithm is actually an implicit feature selection method which is class - dependent , something we have not seen before from a classifier in text categorization . for each class , instead of a centroid",
    ", we are choosing an outlier , a uniform measure supported on the domain - specific words , which is representative of this class and not of any other class . not only does each uniform measure lead to a reduction in the dimension of the feature space ( as most words are not domain - specific ) for similarity calculations , it does so dependent of the class labels , since domain - specific words are chosen relative to all classes .",
    "this algorithm was first developed for the 2012 cybersecurity data mining competition and brought the team of authors 2nd place in the text categorization challenge , and 1st place in accuracy .",
    "this is evidence that our algorithm outperformed many existing text categorization algorithms , as surveyed in section [ backg ] .",
    "in addition , our algorithm was evaluated on a sub - collection of the reuters 21578 dataset against two state - of - the - art classifiers , and shown to have a slightly higher classification accuracy than svm , with a higher f - measure for the larger categories , and overall performed better than random forest .",
    "computationally , our classifier ran significantly faster than either , especially in the training stage .",
    "the normalizing parameter @xmath86 plays a significant role : it is to account for class imbalance .",
    "when there are categories with very few documents , @xmath70 should be used to avoid over - emphasizing the smaller categories ; and small values of @xmath86 should be used when the categories have roughly the same number of documents .    for future work , we hope to test the domain - specific classifier on biological sequence databases .",
    "other definitions of domain - specific words can be investigated , for instance the one proposed in @xcite .",
    "we would like to experiment with assigning non - uniform measures on the domain - specific words , for instance , by putting weights based on their relative occurrences or on @xmath54 .",
    "finally , we would like to extend the process of selecting domain - specific words to a general classification context , by defining class - specific features relative to the classes and performing classification on only these class - dependent features .",
    "bingham , e. , mannila , h. : random projection in dimensionality reduction : applications to image and text data . in : kdd",
    "01 , san francisco , usa 2001 .",
    "proceedings of 7th acm sigkdd int .",
    "conf . knowledge discovery and data mining , pg .",
    "245 - 250 .",
    "dimitriadou , e. , hornik , k. , leisch , f. , meyer , d. , weingessel , a. : e1071 : misc functions of the department of statistics ( e1071 ) , tu wien .",
    "r package version 1.6 , available at http://cran.r-project.org/package=e1071 ( 2011 ) .",
    "joachims , t. : text categorization with support vector machines : learning with many relevant features . in : ecml-98 , chemnitz , germany 1998 .",
    "proceedings of 10th european conference on machine learning , pg .",
    "137 - 142 .",
    "keim , d.a . ,",
    "oelke , d. , rohrdantz , c. : analyzing document collections via context - aware term extraction . in : nldb 09 saarbrcken , germany .",
    "proceedings of 14th inter . conf .",
    "on appl . of natural language to information systems , pg .",
    "154 - 168 .",
    "r development core team : r : a language and environment for statistical computer .",
    "r foundation for statistical computing , vienna , austria .",
    "isbn 3 - 900051 - 07 - 0 , available at http://www.r-project.org ( 2008 ) .",
    "schtze , h. , hull , d.a . , pedersen , j.o . : a comparison of classifiers and document representations for the routing problem . in : sigir 95 seattle , usa .",
    "proceedings of sigir 1995 , 18th acm international conference on research and development in information retrieval , pg . 229 - 237 .",
    "yang , y. , liu , x. : a re - examination of text categorization methods . in : sigir 99 , berkeley , usa 1999 .",
    "proceedings of the 22nd annual international acm sigir conference on research and development in information retrieval , pg .",
    "42 - 49 .",
    "yang , y. , pedersen , j.o . : a comparative study on feature selection in text categorization . in : icml 97 ,",
    "nashville , usa 1997 .",
    "proceedings of icml-97 , 14th international conference on machine learning , pg .",
    "412 - 420 ."
  ],
  "abstract_text": [
    "<S> we present a supervised learning algorithm for text categorization which has brought the team of authors the 2nd place in the text categorization division of the 2012 cybersecurity data mining competition ( cdmc2012 ) and a 3rd prize overall . </S>",
    "<S> the algorithm is quite different from existing approaches in that it is based on similarity search in the metric space of measure distributions on the dictionary . at the preprocessing stage , </S>",
    "<S> given a labeled learning sample of texts , we associate to every class label ( document category ) a point in the space of question . unlike it is usual in clustering , this point is not a centroid of the category but rather an outlier , a uniform measure distribution on a selection of domain - specific words . at the execution stage , </S>",
    "<S> an unlabeled text is assigned a text category as defined by the closest labeled neighbour to the point representing the frequency distribution of the words in the text . </S>",
    "<S> the algorithm is both effective and efficient , as further confirmed by experiments on the reuters 21578 dataset . </S>"
  ]
}