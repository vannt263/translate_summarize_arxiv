{
  "article_text": [
    "in the context of cellular networks , it is widely believed that aggressive densification , overlaying the standard macro - cell network with a large number of small cells ( e.g. , pico- or femto - cells ) , is a promising way of dealing with the ongoing data crunch  @xcite . as this densification puts a tremendous pressure on the backhaul network ,",
    "researchers have suggested storing popular content at the `` edge '' , e.g. , at small cells  @xcite , user devices  @xcite , or vehicles acting as mobile relays  @xcite in order to avoid congesting the capacity - limited backhaul links , and reduce the access latency to such content .",
    "local content caching has been identified as one of the five most disruptive enablers for 5 g networks  @xcite , sparking a tremendous interest of academia and industry alike . while caching had been widely studied in peer - to - peer systems and content distribution networks ( cdns )  @xcite , the number of storage points required in future dense hetnets are many orders of magnitude more than in traditional cdns ( e.g. , 1000s small cells per area covered by one cdn server ) .",
    "therefore , the storage space per local cache must be significantly smaller to keep costs reasonable .",
    "hence , even though studies assuming a large ( cdn - type ) cache deep inside the core network  @xcite give promising hit ratios , only a tiny fraction of the constantly and exponentially increasing content catalog could realistically be stored at each edge , leading to low `` local '' cache hit ratios  @xcite .",
    "additional `` global '' caching gains could be sought by increasing the `` effective '' cache size visible to each user through : ( a ) small cell overlaps , where each user is in range of multiple cells and caches ( e.g. , in the femto - caching case  @xcite ) , ( b ) collocated users overhearing the same broadcast channel and benefiting from cached content in other users caches ( as in coded caching  @xcite ) , and ( c ) delayed content access , where a user might wait up to a ttl for its request , during which time more than one ( fixed  @xcite or mobile  @xcite ) caches can be seen .",
    "these ideas could theoretically increase the cache hit ratio significantly , when the `` global '' cache size becomes large enough ( e.g. , when , in the latter example , the aggregate size of all caches a user sees within a ttl becomes comparable to the content catalog ) .",
    "nevertheless , in most practical cases a local edge cache would realistically fit at most @xmath0 of the catalog ( e.g. , just the entire netflix catalogue is about 3pbs ) .",
    "even if the above methods offered a @xmath1 effective cache increase , they would not suffice to achieve significant cache hit ratios ( e.g. , in the notation of  @xcite , the key factor @xmath2 would be equal to @xmath3 , leading to a global caching gain of @xmath4 , a mere @xmath5 of extra gain ) .",
    "operators , are thus left with a very costly dilemma : bear a huge cost for the backhaul infrastructure ( e.g. , fiber everywhere ) or bear a huge cost for cdn - like storage at each and every small cell .",
    "we believe this dilemma stems from the common underlying assumption of almost every caching scheme to try to satisfy _ every _ possible user request , either from the local cache or , in the worst case , the content server .",
    "this leads to an immense catalogue of potential content .",
    "our main assertion in this paper is that , in an internet which is becoming increasingly content - centric and entertainment - oriented , a radically different approach could be beneficial , namely _ moving away from satisfying a given user request towards satisfying the user .",
    "_ e.g. , a user requesting a content x , not available locally ( e.g. , a fan wanting to follow last weekend s premier league s games ) , might be equally satisfied ( in the best case ) or not fully dissatisfied ( in many cases ) , if she receives another content y related to x ( e.g. , another premier league game from that weekend ) .",
    "another example is users streaming content _ in sequence _",
    "( e.g. , browsing youtube videos back - to - back or listening to personalized radio ) . in that case , the selected content at each step is often _ recommended related to the previous one _ , and the user might be almost equally happy with many alternatives .",
    "we will use the term _ soft cache hit _ to describe such scenarios .",
    "finally , we believe such a system is timely given the recent interest of content providers with sophisticated recommendation engines , such as netflix and youtube ( i.e. , google ) , to act as mobile virtual network operators ( mvno ) in the context of ran sharing  @xcite .",
    "to this end , we perform here a preliminary analysis and performance evaluation of such a system , in order to obtain initial insights .",
    "we first formulate the problem of edge caching with _ soft cache hits _ , and analyze the expected gains .",
    "we then show using both synthetic data and a real dataset of related video contents that interesting caching gains could be achieved in practice .",
    "our problem formulation and analysis takes place in the context of _ delayed content access _ via static or mobile small cells  @xcite , for two reasons : ( a ) we believe such delayed access is interesting for low - cost users ( e.g. , 2 euro plans for operators like free  @xcite ) or developing regions , and ( b ) could be easily combined with soft cache hits to achieve multiplicative gains . nevertheless , the basic tenets of our approach are equally applicable to femto - caching ( i.e. , the framework of  @xcite ) or even other phy - aware caching systems  @xcite .    to the best of our knowledge ,",
    "the closest related work to the idea of soft cache hits is roadcast  @xcite , proposing a query - response based p2p vanet system , where users query requirements can be relaxed in order to get a matching response sooner .",
    "nevertheless , this work focuses mostly on content similarity metrics and considers heuristics to achieve a square root based allocation policy , known to be optimal in p2p systems .",
    "square root policies are suboptimal in our problem setup , as proven later , with or without soft cache hits  @xcite .",
    "_ content model _ : we consider a wireless network with randomly distributed users , requesting contents from a catalogue @xmath6 with @xmath7 contents",
    ". a user requests content @xmath8 with probability @xmath9 . without loss of generality",
    "( `` w.l.o.g . '' ) we assume all contents have the same size .    _ network model _ :",
    "our network consists of @xmath10 small cells ( sc ) .",
    "these scs can be either static ( as in the femto - caching model  @xcite ) or mobile ( e.g. a vehicular cloud as in  @xcite ) .",
    "we denote the set of all scs as @xmath11 .",
    "we also assume that each sc is equipped with storage capacity of @xmath12 contents . accessing content directly from the local cache , i.e. a _ cache hit _ ,",
    "is considered `` cheap '' while a _ cache miss _ leads to an `` expensive '' access ( e.g. of the backhaul link in  @xcite or the macro - cell in  @xcite ) .",
    "_ delayed access protocol _ :",
    "if the requested content is not available in a nearby small cell , the user waits until it encounters other small cells ( as a result of user or cell mobility ) , until a time - to - live @xmath13 .",
    "if the content is not found in any sc within @xmath13 , a cache miss occurs and the content is fetched over the expensive link .    _ meeting model _ :",
    "meetings between each user and each sc are iid , with the _ residual _ time until such a meeting occurs being a random variable with cdf @xmath14 .",
    "[ lemma : pmiss ] if there are @xmath15 total scs storing the requested content , the probability of not encountering any of them within @xmath13 is @xmath16    the above result follows directly from the definition of @xmath14 and the assumption of iid meetings .    for simplicity , in this paper we will focus on @xmath17 , so that @xmath18 .",
    "the identical meeting rates assumption can be further relaxed , as explained in section  [ sec : discussion ] .",
    "up to this point , the problem setup is the same as in  @xcite .",
    "the main departure from that model is captured in the following .",
    "_ content relation graph _ : each content @xmath8 has a set of _ related contents_. let @xmath19 denote the utility a given user gets if she originally asks for content @xmath20 but instead receives content @xmath21 , where @xmath22 and @xmath23 . the set of related contents @xmath24 can be formally defined as : @xmath25 .",
    "these relations define a content relation matrix ( or graph ) @xmath26 .",
    "_ delayed access with soft cache hits ( sch ) _ :",
    "a user again performs delayed access .",
    "however , if the requested content @xmath20 is not found within @xmath13 , but a content in @xmath27 is found in one of the encountered caches , a soft cache hit occurs ( and thus no expensive access is needed ) . a cache miss occurs if neither the requested nor any related content is found within @xmath13 , in which case the original content is retrieved over the expensive link .",
    "the soft cache hit utility is equal to @xmath19 .",
    "we will consider two main cases for @xmath28 .",
    "* _ soft cache hits ( case 1 ) : _ @xmath29 .",
    "any related content gives a cache hit .",
    "as soon as one is found , the user stops looking . *",
    "_ soft cache hits ( case 2 ) : _ @xmath30 .",
    "if a related content @xmath27 is found before @xmath13 , the user now continues looking for @xmath20 until @xmath13 .",
    "if it fails , a _ soft cache hit _ occurs and the access to the expensive link is still avoided .",
    "however , the utility attained is less than 1 ( equal to @xmath31 ) , which creates an interesting tradeoff . if neither @xmath20 nor any related @xmath21 is found by @xmath13 , then a cache miss occurs , as usual .",
    "the goal in the above defined problem is to minimize the number of bytes accessed over the expensive `` link '' ( which is , as explained , a radio access link to a macro - cell and/or the backhaul network ) .",
    "when all contents have the same size , this is simplified to minimizing the number of ( expensive ) accesses , or equivalently , _ maximizing the cache hit ratio_.    [ def : feasible ] let @xmath32 denote the number of sc caches storing content @xmath20 .",
    "a placement vector @xmath33 is `` feasible '' , if it satisfies the following constraints : @xmath34    @xmath32 are the main optimization variables for our problem .",
    "constraint ( [ eq : const - cells ] ) says that the number of scs storing content @xmath20 is non - negative and at most equal to the total number of scs , and constraint ( [ eq : const - total ] ) that the total number of content replicas stored at all the edge caches can not exceed their total capacity .    in the traditional case of delayed access",
    "no soft cache hits are allowed .",
    "this will serve as our _ baseline _ scenario .",
    "the problem objective ( i.e. , the expected hit ratio ) in this case is given in the following lemma .",
    "[ lemma : base - objective ] assume a feasible placement vector @xmath35 . the cache hit rate , i.e.",
    ", the expected number of user requests served locally when no soft cache hits are allowed is equal to @xmath36    the objective ( eq.([eq : obj - base ] ) ) in the above lemma is straightforward in light of lemma  [ lemma : pmiss ] and the model of section  [ sec : model ] .",
    "as explained earlier , when we do allow soft cache hits , if content @xmath20 is requested , a cache hit can occur also if other contents @xmath21 ( related to @xmath20 ) can be accessed on time .",
    "the modified objective for cases 1 and 2 of the content relation graph @xmath28 is given in the following two lemmas ( the proofs are based on basic probabilistic arguments , and are omitted for brevity ) .",
    "[ lemma : obj - case1 ] assume a feasible placement vector @xmath35 , and a content relation graph @xmath28 , where @xmath37 .",
    "the cache hit rate for @xmath35 is equal to @xmath38    [ lemma : obj - case2 ] assume a feasible placement vector @xmath35 , and a content relation graph @xmath28 , where @xmath23 , and @xmath39 . the",
    "cache hit rate for @xmath35 is equal to @xmath40 \\label{eq : obj - case2}\\end{aligned}\\ ] ]    the main difference between these two cases is that , in the first case , finding a related content gives utility @xmath41 and is equivalent to a normal cache hit .",
    "however , in the second case , a related content allows the operator to avoid accessing the expensive link , but is penalized because the utility for the user is lower , leading to a utility of @xmath42 ( we remind the reader that @xmath43 in the second term of eq.([eq : obj - case2 ] ) includes all related contents @xmath21 , such that @xmath44 , but does not include content @xmath20 ) .      maximizing the objective of lemma  [ lemma : base - objective ] within the feasibility region of definition  [ def : feasible ] ,",
    "defines the optimal cache allocation problem for the baseline scenario ( no soft cache hits ) .",
    "this is in general an inlp ( integer non - linear program ) that relates to a `` multiple knapsack '' problem ( with equal capacities and logarithmic rather than linear utilities ) and is np - hard to solve .",
    "various polynomial approximation algorithms exist with good performance when the size of the caches are large enough to fit many contents .",
    "one such approximation can be achieved by solving a continuous relaxation of the problem ( related to the fractional knapsack problem ) , where the optimization variables @xmath45 $ ] are continuous . in that case , it is easy to show that the baseline problem is convex , whose optimal solution can be found analytically using lagrangian multipliers and solving the kkt conditions ( we refer the interested reader to  @xcite for more details ) .",
    "specifically , the optimal solution is given by @xmath46 where @xmath47 , @xmath48 , and @xmath49 is an appropriate lagrange multiplier corresponding to the capacity constraint of eq.([eq : const - total ] ) .",
    "value as follows : if @xmath50 , @xmath51 of content @xmath20 is allocated to @xmath52 caches , and one more cache stores only @xmath53 of the content .",
    "if a user encounters the latter , she retrieves the remaining @xmath54 from the infrastructure . ]    replacing @xmath55 in the objective of the baseline problem ( eq.([eq : obj - base ] ) ) gives us the optimal cache hit ratio , if we ignored related content . at the same time , replacing @xmath55 in the objective of eq.([eq : obj - case1 ] ) gives us the cache hit ratio when we can satisfy a request with related content , _ but the caching decisions were already taken and are the original ones_. ( we will show later that we could do even better by considering the related content graph @xmath28 when solving the cache placement problem . ) the following theorem provides the expected improvement in terms of load on the expensive link , for a simple scenario where @xmath56 .",
    "assume that @xmath57 .",
    "the expected improvement in the cache hit ratio by recommending alternative contents , when the optimal cache placement algorithm is oblivious to these recommendations , is equal to @xmath58    the cache miss ratio ( or `` load '' on the main infrastructure ) in the baseline problem is @xmath59 . replacing eq.([eq : optn - base ] ) into eq.([eq : obj - base ] ) gives @xmath60 similarly , let s assume that an original request could be satisfied with a related content as in lemma  [ lemma : obj - case1 ] .",
    "the cache miss ratio , denoted as @xmath61 , can be calculated as : @xmath62 hence , the gain from soft cache hits ( case 1 ) is equal to @xmath63 , which gives the desired eq.([eq : obj - base ] ) .",
    "the case where some contents receive no or maximum ( @xmath10 ) copies , as in eq.([eq : optn - base ] ) , can be easily derived by modifying the summation in the above proofs . as a very simple example , consider the case of uniform content popularity , i.e. @xmath64 . after some simple calculations",
    ", we get that the performance benefits by related content are equal to @xmath65 .",
    "however , we know that @xmath66 , since it is the cache miss rate of the base policy ( see eq.([eq : base - miss - rate ] ) ) .",
    "therefore , the above gain @xmath67 , and is increasing in @xmath68 , the number of related contents per content @xmath20 , as one would expect .",
    "a similar result can be easily derived for case 2 , as well as when the number of non - zero elements on each row @xmath20 of @xmath28 is different ( i.e. not all equal to @xmath69 ) .",
    "we have so far assumed that the caching policy is unaffected by the ability to recommend alternative contents . while this already leads to performance gains , as shown earlier , it is still suboptimal .",
    "for example , assume a user requesting content @xmath70 would be ok to receive instead content @xmath71",
    "( i.e. @xmath72 ) and a user requesting content @xmath71 would be ok to receive content @xmath70 instead ( i.e. @xmath73 ) . if both contents @xmath70 and @xmath71 are popular , _ a standard caching policy would give a high number of replicas to both _",
    ", according to eq.([eq : optn - base ] ) . however , this is clearly suboptimal here , since the caching algorithm could just store only one of the two at each cache , saving valuable capacity that could be used to store other contents . the following two theorems",
    "formalize this for the two content relation graph cases , discussed in section  [ sec : model ] .",
    "due to space limitations , we only show the proof for the more generic case 2 .",
    "[ lemma : opt - case1 ] assume a content relation graph @xmath28 , where @xmath37 . the optimal content placement that directly exploits related contents is given by vector @xmath74 which is the solution to the following optimization problem @xmath75 furthermore , the above problem is a convex optimization problem .",
    "[ lemma : opt - case2 ] the optimal content placement defined by maximizing the objective of lemma  [ lemma : obj - case2 ] , subject to the feasibility constraints of definition  [ def : feasible ] , gives the optimal content allocation vector @xmath76 .",
    "furthermore , the problem is also convex .",
    "it is easy to see that the feasibility region ( definition  [ def : feasible ] ) is convex .",
    "the objective function needs to be concave ( since this is formulated as a maximization problem ) .",
    "a sufficient condition is if its hessian matrix @xmath77 is negative semi - definite , i.e. , @xmath78 .    taking the derivatives of the objective function @xmath79",
    ", we calculate the terms of the hessian matrix @xmath80\\end{aligned}\\ ] ] and for @xmath81 @xmath82 where @xmath83 is @xmath41 if @xmath84 ; otherwise is @xmath85 .    then , the product @xmath86 is given by the expression @xmath87\\end{aligned}\\ ] ] which is always @xmath88 .",
    "* mobility trace . *",
    "we use the tvcm mobility model to generate a trace , where nodes move in a square area @xmath89 comprising three sub - areas of interest ( communities ) .",
    "each node moves inside its community for 60% of the time , and leaves it for a few short periods .",
    "the area is entirely covered by macro - cell bss , and also includes 25 non overlapping small - cell base stations ( scs ) , with a communication range of 100 m .",
    "* content popularity .",
    "* we create @xmath90 contents and assign to each of them a popularity value @xmath91 drawn from a zipf distribution , @xmath92 $ ] with shape parameter @xmath93 .",
    "power - law distributions have been shown to capture well real popularity patterns  @xcite .",
    "* utility matrix .",
    "* to investigate the effect of the matrix u , we generate different matrices belonging to two generic classes :    _ ( a ) random u _ : for each content pair @xmath94 , the utility is @xmath95 with probability @xmath96 ( otherwise it is 0 ) , such that each content has on average @xmath69 related contents , i.e. , @xmath97 $ ] .",
    "_ ( b ) popularity proportional u _ : for each content pair @xmath94 , the utility is @xmath95 with probability @xmath98 ( otherwise it is 0 ) , where @xmath99 is the popularity of content @xmath21 , and @xmath100 is a normalization parameter that determines @xmath101 $ ] .",
    "* youtube datasets . *",
    "in addition to the synthetic popularity / utility patterns , we use real datasets from youtube that contain information about _ video popularity _ and _ related video lists _  @xcite .",
    "table  [ table : youtube - instances ] contains information about the datasets we use , and some main statistics .",
    "we pre - process the data to remove entries with @xmath85 or no popularity value .",
    "for each video @xmath21 appearing in the related videos list of a video @xmath20 , we set @xmath102 and @xmath103 . due to the sparseness of the datasets , we consider only the videos belonging to the largest connected component of the graph with vertices @xmath104 and edges @xmath105 .",
    ".youtube dataset instances information ( after processing ) .",
    "[ cols=\"<,^,^,^\",options=\"header \" , ]",
    "our initial results suggest that soft cache hits could be a promising way to make edge caching scale , opening up new interesting operator - user performance tradeoffs .",
    "some limitations and potential extensions of the proposed model are discussed here .",
    "_ user - dependent recommendations : _ throughout this work , we have been assuming that the related contents for a requested content item @xmath20 , and their related utilities depend only on item @xmath20 , and not on the user that requested it . in a sense , this relates to _ item - item _ collaborative filtering , where a new / alternative item is recommended based on its similarity with the requested one .",
    "item - item recommendations have been claimed to offer some advantages compared to _ user - user _ collaborative filtering  @xcite .",
    "nevertheless , one user might be less happy than another , with the same alternative content . on the modeling side , one could take this into account by making @xmath19 a random variable and using its expected value @xmath106 $ ] in the objective functions of section  [ sec : theory ] .",
    "finally , on the recommendation side , a recommendation system could actually combine both types of collaborative filtering to make better recommendation .",
    "this would lead to different @xmath28 graphs per user ( or user clusters ) , whose integration and impact on our framework is part of future work .",
    "_ generalization of @xmath28 graph : _ for simplicity , in our analysis we assumed that related contents bring the same amount of utility ( @xmath41 in case 1 , and @xmath42 in case 2 ) .",
    "in general , different related contents might bring different amounts of utility .",
    "we could generalize our model by assuming a _ case 3 _ where @xmath107 . as in case 2 , if a user requesting content @xmath20 , accesses ( before @xmath20 ) any content @xmath108 , she will be satisfied @xmath109 ( less than @xmath41 ) .",
    "she will keep on requesting @xmath20 till time @xmath13 , but will _ not _ accept any other related content .",
    "contrary to case 2 , however , the value of the utility @xmath19 ( to be contributed at the objective function ) is not known a priori , since we can not know a priori which content @xmath110 will be accessed .",
    "one can still derive a closed form objective function with appropriate conditioning on all possible @xmath21 , but we defer elaborating on this scenario for future work . _",
    "generic mobility : _ although it would be quite hard to relax the independent mobility assumption ( using traces in simulations , where most such assumptions break , tends to be the de facto way of testing this ) the identical contact rate assumption could be relaxed .",
    "e.g. , in the context of exponential meetings , it has been shown that heterogeneous rates could be approximated with their mean , either asymptotically or as a bound  @xcite . _ soft cache hits for femto - caching : _ the proposed approach of soft cache hits and alternative content recommendations could apply equally well to more traditional caching frameworks that do not allow any delay , as is the popular femto - caching framework  @xcite .",
    "the relation between users and small cells that each user can access is captured by a bipartite graph , and the control variables @xmath111 define whether a content @xmath112 is stored in a cache @xmath21 . in the case of @xmath28 as in case 1 ,",
    "if some user requests content @xmath20 , and the small cells in her range are @xmath113 , the hit probability is given by @xmath114 instead of @xmath115 , in the original femtocaching case ( see  @xcite for more details ) .",
    "in this paper , we have proposed the idea of _ soft cache hits _ for mobile edge caching systems with delay tolerance , where a user request can sometimes be ( partially ) satisfied , even if the original content is not available locally , by recommending some related contents .",
    "we have formulated and analyzed the performance of such a joint system , and derived the optimal related content aware cache placement .",
    "our theoretical analysis and initial evaluation suggest that significant performance gains can be achieved , even with simple modifications to the baseline system .",
    "furthermore , our results suggest that the structure of the content relation graph plays an important role on the actual achievable performance .",
    "b.  han , p.  hui , v.  s.  a. kumar , m.  v. marathe , j.  shao , and a.  srinivasan , `` mobile data offloading through opportunistic communications and social participation , '' _ ieee trans . on mobile computing _ ,",
    "11 , no .  5 , 2012 .",
    "p.  sermpezis , , and t.  spyropoulos , ",
    "\" effects of content popularity in the performance of content - centric opportunistic networking : an analytical approach and applications ,  _ acm / ieee trans . on networking _ , 2016 .",
    "p.  sermpezis and t.  spyropoulos , `` delay analysis of epidemic schemes in sparse and dense heterogeneous contact environments , '' tech .",
    "http://www.eurecom.fr/~sermpezi/techrep_hetepid.pdf , eurecom , 2012 ."
  ],
  "abstract_text": [
    "<S> caching popular content at the edge of future mobile networks has been widely considered in order to alleviate the impact of the data tsunami on both the access and backhaul networks . </S>",
    "<S> a number of interesting techniques have been proposed , including femto - caching and `` delayed '' or opportunistic cache access . </S>",
    "<S> nevertheless , the majority of these approaches suffer from the rather limited storage capacity of the edge caches , compared to the tremendous and rapidly increasing size of the internet content catalog . </S>",
    "<S> we propose to depart from the assumption of hard cache misses , common in most existing works , and consider `` soft '' cache misses , where if the original content is not available , an alternative content that is locally cached can be recommended . given that internet content consumption is increasingly entertainment - oriented , we believe that a related content could often lead to complete or at least partial user satisfaction , without the need to retrieve the original content over expensive links . in this paper , we formulate the problem of optimal edge caching with soft cache hits , in the context of delayed access , and analyze the expected gains . </S>",
    "<S> we then show using synthetic and real datasets of related video contents that promising caching gains could be achieved in practice . </S>"
  ]
}