{
  "article_text": [
    "observation driven time - series models , introduced by @xcite , has a wide variety of real applications , including econometrics ( garch models ) and applied mathematics ( inferring initial conditions and parameters of ordinary differential equations ) .",
    "the model can be described as follows .",
    "we observe @xmath9 , @xmath10 which are associated to a dynamic system @xmath11 , @xmath12 which is potentially unknown .",
    "define the process @xmath13 ( with @xmath14 some arbitrary point on @xmath15 ) on a probability space @xmath16 , where , for every @xmath17 , @xmath18 is a probability measure .",
    "denote by @xmath19 .",
    "the model is defined as , for @xmath20 @xmath21 where @xmath22 $ ] , @xmath23 and for every @xmath24 , @xmath25 ( the probabilities on @xmath26 ) . throughout",
    ", we assume that for any @xmath27 @xmath28 admits a density w.r.t .",
    "some @xmath29finite measure @xmath30 , which we denote as @xmath31 .",
    "next , we define a prior probability distribution @xmath32 on @xmath33 , with lebesgue density @xmath34 . thus , given @xmath1 observations @xmath35 the object of inference is the posterior distribution on @xmath36 : @xmath37 where we have used the notation @xmath38 and @xmath39 is lebesgue measure . in most applications of practical interest",
    ", one can not compute the posterior point - wise and has to resort to numerical methods , such as mcmc , to draw inference on @xmath40 and/or @xmath41 .    in this article , we are not only interested in inferring the posterior distribution , but the scenario for which @xmath31 can not be evaluated point - wise , nor do we have access to an unbiased estimate of it ( it is assumed we can simulate from the associated distribution ) .",
    "in such a case , it is not possible to draw inference from the true posterior , even using numerical techniques .",
    "the common response in bayesian statistics , is now to adopt an approximation of the posterior using the notion of approximate bayesian computation ( abc ) ; see @xcite for a recent overview .",
    "abc approximations of posteriors are based upon defining a probability distribution on an extended state - space , with the additional random variables lying on the data - space and usually distributed according the true likelihood .",
    "the closeness of the abc posterior distribution is controlled by a tolerance parameter @xmath42 and often the approximation is exact as @xmath43 .    in this paper , we introduce a new abc approximation of observation driven time - series models , which is closely associated to that developed in @xcite for hidden markov models ( hmms ) and later for static parameter inference from hmms @xcite .",
    "this latter abc approximation is particularly well behaved and a noisy variant ( which pertrubs the data ; see e.g.  @xcite ) is shown under some assumptions to provide maximum - likelihood estimators ( mle ) which asympotically in @xmath1 are the true parameters .",
    "the new abc approximation that we develop is studied from a theoretical perspective .",
    "relying on the recent work of @xcite we show that , under some conditions , as @xmath0 , with @xmath1 the length of the time series , the abc posterior has , almost surely , a map estimator of @xmath40 which is different from the true parameter @xmath44 say .",
    "however , a noisy abc map of @xmath40 asymptotically converges to the true parameter , almost surely .",
    "these results establish that the particular approximation adopted is reasonably sensible .",
    "the other main contribution of this article is a development of a new mcmc algorithm designed to sample from the abc approximation of the posterior . due to the nature of the abc approximation",
    "it is easily seen that standard mcmc algorithms ( e.g.  @xcite ) will have an acceptance probability that will fall at an exponential rate in @xmath1 .",
    "in addition , more advanced ideas such as those based upon the ` pseudo marginal ' @xcite , have recently been shown to perform rather poorly in theory ; see @xcite .",
    "these latter algorithms are based upon exact approximations of marginal algorithms @xcite , which in our context is just sampling @xmath45 .",
    "we develop an mcmc kernel , related to recent work in @xcite , which is designed to have a random running time per - iteration , with the idea of improving the exploration ability of the markov chain .",
    "we show that the expected cost per iteration of the algorithm , under some assumptions and for reasonable performance , is @xmath2 , which compares favourably with competing algorithms .",
    "we also show , empirically , that this new mcmc method out - performs standard pseudo marginal algorithms .",
    "this paper is structured as follows . in section [ sec : model ]",
    "we introduce our abc approximation and give our theoretical results on the map estimator . in section [ sec : comp ] , we give our new mcmc algorithm , along with some theoretical discussion about its computational cost and stability . in section [ sec : examples ] our approximation and mcmc algorithm is illustrated on toy and real examples . in section [ sec : summ ] we conclude the article with some discussion of future work .",
    "the proofs of our theoretical results are given in the appendix .",
    "as it was emphasised in section [ sec : intro ] , we are interested in performing inference when @xmath31 can not be evaluated point - wise , nor do we have access to an unbiased estimate of it .",
    "we will instead assume it is possible to sample from @xmath46 .",
    "in such scenaria , one can not use standard simulation based methods .",
    "for example , in a standard mcmc approach the metropolis - hastings acceptance ratio can not be evaluated , even though it may be well - defined .",
    "following the work in @xcite for hidden markov models , we introduce an abc approximation for the density of the posterior in as follows : @xmath47 with @xmath42 and @xmath48 where we denote @xmath49 as the open ball centred at @xmath50 with radius @xmath51 and write @xmath52 .",
    "when @xmath30 is the lebesgue measure , @xmath53 corresponds to the volume of the ball @xmath49 .    in general we will refer to abc as the procedure of performing inference for the posterior in .",
    "in addition , we will call _ noisy abc _ the inference procedure that uses instead of the original observation sequence a perturbed one , namely @xmath54 , where each @xmath55 is given by @xmath56 with each @xmath57 is identically independently distributed ( i.i.d . ) uniformly on @xmath58 ( shorthand @xmath59 ) .      in this section",
    "we will investigate some interesting properties of the abc posterior in .",
    "in particular , we will look at the asymptotic behaviour with @xmath1 of the resulting map estimators for @xmath40 .",
    "the properties of the map estimator reveal information about the mode of the posterior distribution as we obtain increasingly more data . to simplify the analysis in this section",
    "we will assume that :    [ as:1 - 1 ]    * @xmath60 is fixed and known , i.e. @xmath61 , where @xmath62 denotes the dirac delta measure on @xmath26 and @xmath63 is known .",
    "* @xmath64 is bounded and positive everywhere in @xmath65 . *",
    "the observations actually originated from the true model model for some @xmath66 , i.e. we look at a well - specified problem .",
    "* @xmath67 and @xmath68 do not depend upon @xmath40 .",
    "thus we have the following model recursions for the true model : @xmath69 where we will denote associated expectations to @xmath70 as @xmath71 .",
    "in addition , for this section we will introduce some extra notations : @xmath72 is a compact , complete and separable metric space and @xmath73 is a compact metric space , with @xmath74 . for two measures @xmath75 and @xmath76 of bounded variation denote the convolution @xmath77 .",
    "let also @xmath78 be the probability law associated to the random sequence @xmath79 , where each @xmath57 is an i.i.d .",
    "sample from the uniform distribution defined on @xmath58 .",
    "we proceed with some additional technical assumptions :    [ as:1 ] @xmath80 is a stationary stochastic process , with @xmath81 strict sense stationary and ergodic , following .",
    "[ as:2 ] for every @xmath82 , @xmath83 is continuous .",
    "in addition , there exist @xmath84 such that for any @xmath85 , @xmath86",
    ". finally @xmath87 , for every @xmath82 .",
    "[ as:3 ] there exist a measurable @xmath88 , such that for every @xmath89 @xmath90    [ as:4 ] the following statements hold :    1 .",
    "@xmath91 if and only if @xmath92 2 .   if @xmath93 holds @xmath94a.s .",
    ", then @xmath95 .",
    "assumptions ( a[as:1]-[as:4 ] ) and the compactness of @xmath65 are standard assumptions for maximum likelihood estimation ( ml ) and they can be used to show the uniqueness of the maximum likelihood estimator ( mle ) ; see @xcite for more details .",
    "therefore , if the prior @xmath96 is bounded and positive everywhere on @xmath65 it is a simple corollary that the map estimator will correspond to the mle .",
    "in the remaining part of this section we will adapt the analysis in @xcite for mle to the abc setup .",
    "in particular , we are to estimate @xmath40 using the log - likelihood function : @xmath97 we define the abc - mle for an @xmath1-long sequence as @xmath98 we proceed with the following proposition :    [ prop1 ] assume ( a[as:1 - 1]-[as:3 ] ) .",
    "then for every @xmath63 and fixed @xmath42 @xmath99 where @xmath100 $ ] .",
    "the result establishes that the estimate will converge to a point , which is typically different to the true parameter .",
    "hence there is an intrinsic asymptotic bias for the plain abc procedure . to correct this bias , consider the noisy abc procedure , of replacing the observations by @xmath101 where @xmath102 .",
    "the noisy abc mle estimator is then : @xmath103 we have the following result :    [ prop2 ] assume ( a[as:1 - 1]-[as:4 ] ) .",
    "then for every @xmath63 and fixed @xmath42 @xmath104    the result shows that the noisy abc mle estimator is asymptotically unbiased .",
    "therefore , given that in our setup the abc map estimator corresponds to the abc mle we can conclude that the mode of the posterior distribution as we obtain increasingly more data is converging towards the true parameter .",
    "finally we note that our assumptions indeed pose some restrictions , but these are shown to be realistic for a few interesting models in @xcite .",
    "in addition , the main purpose of this result is to motivate the use of the approximate posterior in when the observation sequence is long or its marginal likelihood is quite informative .",
    "recall that we formulated in the abc posterior written in .",
    "one can rewrite the approximate posterior in : @xmath105 with @xmath106 note we have just used fubini s theorem to rewritte the likehood @xmath107 as an integral of a product instead of a product of integrals @xmath108 shown in - . in this paper",
    "we will focus only on mcmc algorithms and in particular on the metropolis - hastings ( m - h ) approach . in order to sample from the posterior",
    "@xmath109 one runs an ergodic markov chain with the invariant density being @xmath109 .",
    "then after a few iterations when the chain has reached stationarity , one can treat the samples from the chain as approximate samples from @xmath109 .",
    "this is shown in algorithm [ alg : mcmc ] , where for convenience we denote @xmath110 .",
    "the one - step transition kernel of the mcmc chain is usually described as the _ m - h kernel _ and follows from step 2 in algorithm [ alg : mcmc ] .    1 .   * ( initialisation ) * at @xmath111 sample @xmath112 .",
    "2 .   * ( m - h kernel ) * for @xmath113 : * sample @xmath114 from a proposal @xmath115 with density @xmath116 . *",
    "accept the proposed state and set @xmath117 with probability @xmath118 otherwise set @xmath119 .",
    "set @xmath120 and return to the start of 2 .",
    "unfortunately @xmath107 is not available analytically and can not be evaluated , so this rules out the possibility of using of traditional mcmc approaches like algorithm [ alg : mcmc ] .",
    "however , one can resort to the so called _ pseudo - marginal _ approach whereby unbiased estimates of @xmath107 are used instead within an mcmc algorithm .",
    "we will refer to this algorithm as abc - mcmc .",
    "the resulting algorithm can be posed as one targeting a posterior defined on an extended state space , so that its marginal coincides with @xmath121 .",
    "we will use these ideas to present abc - mcmc as a m - h algorithm which is an exact approximation to an appropriate marginal algorithm .",
    "to illustrate an example of these ideas , we proceed by writing a posterior on an extended state - space @xmath122 as follows : @xmath123 it is clear that is the marginal of and hence the similarity in the notation .",
    "as we will show later in this section , extending the target space in the posterior as in is not the only and certainly not the best choice .",
    "we emphasise that the only essential requirement for each choice is that the marginal of the extended target is @xmath121 , but one should be cautious because the particular choice will affect the mixing properties and the efficiency of the mcmc scheme that will be used to sample from @xmath124 in or another variant .",
    "we will now look at two basic different choices for extending the abc posterior while keeping the marginal fixed to @xmath121 . in the remainder of the paper",
    "we will denote @xmath110 as we did in algorithm [ alg : mcmc ] .    initially consider the abc approximation when be extended to the space @xmath122 : @xmath125 recall one can not evaluate @xmath126 and is only able to simulate from it . in algorithm",
    "we present a natural m - h proposal that could be used to sample from @xmath127 instead of the one shown step 2 at algorithm [ alg : mcmc ] .",
    "note that this time the state of the mcmc chain is composed of @xmath128 . here",
    "each @xmath129 assumes the role of an auxiliary variable to be eventually integrated out at the end of the mcmc procedure .",
    "* sample @xmath130 from a proposal @xmath131 with density @xmath132 . *",
    "sample @xmath133 from a distribution with joint density @xmath134 * accept the proposed state @xmath135 with probability : @xmath136    however , as @xmath1 increases , the m - h kernel in algorithm [ alg : simple ] will have an acceptance probability that falls quickly with @xmath1 .",
    "in particular , for any fixed @xmath137 , the probability of obtaining such a sample will fall at an exponential rate in @xmath1 .",
    "this means that this basic abc mcmc approach will be inefficient for a moderate value of @xmath1 .",
    "this issue can be dealt with by using @xmath138 multiple trials , so that at each @xmath139 , some auxiliary variables ( or pseudo - observations ) are in the ball @xmath140 .",
    "this idea originates from @xcite and in fact augments the posterior to a larger state - space , @xmath141 , in order to target the following density : @xmath142 again , it is easy to show that the marginal of interest @xmath143 is preserved , i.e. @xmath144 in algorithm [ alg : ntry ] we present an m - h kernel with invariant density @xmath145 .",
    "the state of the mcmc chain now is @xmath146 .",
    "we remark that as @xmath138 grows , one expects to recover the properties of the ideal m - h algorithm in algorithm [ alg : mcmc ] .",
    "nevertheless , it has been shown in @xcite that even the m - h kernel in algorithm [ alg : ntry ] does not always perform well .",
    "it can happen that the chain gets often stuck in regions of the state - space @xmath147 where @xmath148 is small .    * sample @xmath130 from a proposal @xmath131 with density @xmath132 .",
    "* sample @xmath149 from a distribution with joint density @xmath150 . *",
    "accept the proposed state @xmath151 with probability : @xmath152      we will address this shortfall detailed above , by proposing an alternative augmented target and corresponding m - h kernel .",
    "the basic idea is that a random number of trials is used based on the value of @xmath153",
    ". then it will be possible to use more computational effort when the chain is at regions where @xmath153 is low .",
    "consider an alternative extended target , for @xmath154 , @xmath155 , @xmath156 : @xmath157 standard results for negative binomial distrubutions ( see @xcite for more details ) imply that @xmath158 holds and this can be used to deduce that @xmath159 is an unbiased estimator for @xmath160 .",
    "in addition , from it follows that the marginal w.r.t .",
    "@xmath137 is the one of interest : @xmath161 in algorithm [ alg : new ] we present a m - h kernel with invariant density @xmath162 .",
    "the state of the mcmc chain this time is @xmath163 .",
    "* sample @xmath130 from a proposal @xmath131 with density @xmath132 . *",
    "for @xmath164 repeat the following : sample @xmath165 with probability density @xmath166 until there are @xmath138 samples lying in @xmath140 ; the number of samples to achieve this ( including the successful trial ) is @xmath167 . *",
    "accept @xmath168 with probability : @xmath169    the potential benefit of this kernel is that one expects the probability of accepting a proposal is higher than the previous m - h kernel ( for a given @xmath138 ) .",
    "this comes at a computational cost which is both increased and random .",
    "the proposed kernel is based on the _ @xmath170hit _ kernel of @xcite , which has been adapted here to account for the data being a sequence of observations resulting from a time series .",
    "finally , it is important to mention that in algorithms [ alg : ntry ] and [ alg : new ] generating multiple trials can be implemented very efficiently in parallel using appropriate computing hardware , such as multi - core processors or computing clusters .      to implement the proposed kernel , one needs to select @xmath138 . in practice to it is difficult to know a good value this _ a priori _ , so we present a theoretical result that can add some intuition on choosing @xmath138 . let @xmath171 $ ] denote expectation w.r.t .",
    "@xmath172 given @xmath173 .",
    "we will also pose the assumption :    [ ass : a_k_positive ] for any fixed @xmath42 , @xmath174 , we have @xmath175 .    the the following result holds , whose proof can be found in the appendix . :    [ prop : rel_var ] assume ( a[ass : a_k_positive ] ) and let @xmath176 , @xmath177 and @xmath178 . then for fixed @xmath179 we have @xmath180\\leq\\frac{cn}{n}\\ ] ] where @xmath181 .",
    "the result shows that one should set @xmath182 for the relative variance not to grow with @xmath1 , which is unsuprising , given the conditional independence structure of the @xmath183 . to get a better handle on the variance , suppose @xmath184 ,",
    "then for @xmath137 fixed @xmath185 = \\frac{\\alpha_1(y_{1},\\epsilon,\\gamma)(1-\\alpha_1(y_{1},\\epsilon,\\gamma))}{n}.\\ ] ] for the new approach one can show @xmath186 \\leq \\frac{\\alpha_1(y_{1},\\epsilon,\\gamma)^2}{(n-2)}.\\ ] ] not taking into account the computational cost , one prefers this new estimate with regards to variance if @xmath187 which is likely to occur if @xmath188 is not too large ( recall we want @xmath51 to be small , so that we have a good approximation of the true posterior ) and @xmath138 is moderate - this is precisely the scenario in practice .    [ rem : gamma_indep ] it is easily shown that the relative variance associated to the estimate @xmath189 $ ] is @xmath190 - 1.\\ ] ] note this quantity is not uniformly upper - bounded in @xmath137 unless @xmath191 , which may not occur .",
    "conversely , proposition [ prop : rel_var ] shows that the relative variance of the new estimator is uniformly upper - bounded in @xmath137 under minimal conditions .",
    "we suspect that this means in practice that the kernel with random number of trials may mix faster .",
    "as the cost per - iteration is random , we will investigate this further .",
    "we denote the proposal of @xmath192 as @xmath193 .",
    "let @xmath194 be the initial distribution of the mcmc chain and @xmath195 the distribution of the state at time @xmath196 .",
    "in addition , denote by @xmath197 the proposed state for @xmath198 at iteration @xmath196 .",
    "finally , we will write as @xmath199 the expectation of a random variable proposed by @xmath193 given the simulated state at time @xmath196 .",
    "we will assume that the observations are fixed and known .",
    "then we have the following result :    [ prop : time ] let @xmath42 , and suppose that there exists a constant @xmath200 such that for any @xmath177 we have @xmath201 , @xmath202a.e .. then it holds for any @xmath154 , @xmath113 , that : @xmath203\\leq\\frac{nn}{c}.\\ ] ]    the expected computational cost grows linearly with @xmath1 .",
    "thus , coupled with the result in proposition [ prop : rel_var ] , one has a cost of @xmath2 per - iteration , which is comparable to many exact approximations of mcmc algorithms ( e.g.  @xcite ) , albeit in a much simpler situation .",
    "note also that the kernel in algorithm [ alg : ntry ] is expected to require a cost of @xmath204 per iteration for reasonable performance , although this cost here is deterministic . as mentioned above",
    ", one expects the approach with random number of trials to work better with regards to the mixing time , especially when the values of @xmath153 are not large .",
    "we attribute this to algorithm [ alg : new ] providing a more ` targetted ' way to use the simulated auxiliary variables .",
    "this will be illustrated numerically in section [ sec : examples ] .",
    "a comparison of our results with the interesting work in @xcite seems relevant .",
    "there the authors deal with a more general context and show that we should choose @xmath138 as a particular asymptotic ( in @xmath138 ) variance ; the main point is that the ( asymptotic ) variance of the estimate of @xmath160 should be the same for each @xmath137 .",
    "we conjecture that in our set - up one should choose @xmath138 such that the actual variance of the estimate of @xmath160 is constant with respect to @xmath137 . in this scenario , on inspection of the proof of proposition [ prop : rel_var ] , for a given @xmath137 , one should set @xmath138 to be the solution of @xmath205 for some desired ( upper - bound on the ) variance @xmath206 ( whose optimal value would need to be obtained ) .",
    "this makes @xmath138 a random variable , in addition , but does not change the simulation mechanism .",
    "unfortunately , one can not do this in practice , as the @xmath207 are unknown . taking into account remark [ rem : gamma_indep ] , this latter approach may not be so much of a concern in practice .",
    "we conclude this discussion by adding a related comment regarding the ergodicity of the proposed mcmc kernel .",
    "if there exists a constant @xmath208 such that @xmath209 and the marginal mcmc kernel in algorithm [ alg : mcmc ] is geometrically ergodic , then by ( * ? ?",
    "* propositions 7 , 9 ) the mcmc kernel of algorithm [ alg : new ] is also geometrically ergodic .",
    "for this example let each @xmath210 be a scalar real random variable and consider the model : @xmath211 with @xmath212 and @xmath213 , where we denote @xmath214 the zero mean normal distribution with variance @xmath215 .",
    "the prior on @xmath40 is @xmath216 .",
    "this model is usually referred to as the standard normal means model in one dimension and the posterior is given by : @xmath217 where @xmath218 note that if @xmath219 , then the posterior on @xmath40 is consistent and concentrates around @xmath220 as @xmath0 .",
    "the abc approximation after marginalizing out the auxiliary variables has a likelihood given by : @xmath221\\ ] ] where @xmath222 is the standard normal cumulative density function .",
    "thus , this is a scenario where we can perform the marginal mcmc .",
    "three data sets are generated from the model with @xmath223 and @xmath224 .",
    "in addition , for @xmath225 we perturb the data - sets in order to use them for noisy abc . for the sake of comparison",
    ", we also generate a noisy abc data - set for @xmath226 .",
    "we will also use a prior with @xmath227 .",
    "we run the new mcmc kernel ( the proposal in algorithm [ alg : new ] - we will frequently use the expression ` algorithm ' to mean an mcmc kernel with the given proposal mechansim of the algorithm ) , old mcmc kernel ( algorithm [ alg : ntry ] ) and a marginal mcmc algorithm which just samples on the parameter space @xmath228 ( i.e.  the posterior density is proportional to @xmath229 ) .",
    "each algorithm is run with a normal random walk proposal on the parameter space , with the same scaling .",
    "the scaling chosen yields an acceptance rate of around 0.25 for each run of the marginal mcmc algorithm .",
    "the new mcmc kernel is run with @xmath230 and the old with a slightly higher value of @xmath138 so that the computational times are about the same ( so for example , the running time of the new kernel is not a problem in this example ) .",
    "the algorithms are run for 10000 iterations and the results can be found in figures [ fig : marginal_den_normal]-[fig : marginal_acf_normal ] .    in figure",
    "[ fig : marginal_den_normal ] the density plots for the posterior samples on @xmath40 , from the marginal mcmc can be seen for @xmath231 and each value of @xmath1 .",
    "when @xmath225 , we can observe that both abc and noisy abc both get closer to the true posterior as @xmath1 grows .",
    "for noisy abc , this is the behavior that is predicted in section [ sec : consis ] . for the abc approximation ,",
    "following the proof of theorem 1 in @xcite , one can see that the bias falls with @xmath51 ; hence , in this scenario there is not a substantial bias for the standard abc approximation .",
    "when we make @xmath51 much larger a more pronounced difference between abc and noisy abc can be seen and it appears as @xmath1 grows that the noisy abc approximation is slightly more accurate ( relative to abc ) .",
    "we now consider the similarity of the new and old mcmc kernels to the marginal algorithm ( i.e.  the kernel both procedures attempt to approximate ) , the results are in figures [ fig : marginal_den_oldnew_normal]-[fig : marginal_acf_normal ] .",
    "with regards to both the density plots ( figure [ fig : marginal_den_oldnew_normal ] ) and auto - correlations ( figure [ fig : marginal_acf_normal ] ) we can see that both mcmc kernels appear to be quite similar to the marginal mcmc . it is also noted that the acceptance rates of these latter kernels are also not far from that of the marginal algorithm ( results not shown ) .",
    "these results are unsuprising , given the simplicity of the density that we target , but still reassuring ; a more comprehensive comparison is given in the next example .",
    "encouragingly , the new and old mcmc kernels do not seem to noticably worsen as @xmath1 grows ; this shows that , at least for this example , the recommendation of @xmath182 is quite useful .",
    "we remark that whilst these results are for a single batch of data , the results are consistent with other data sets .",
    "set , for @xmath232 @xmath233 where @xmath234 ( i.e.  a stable distribution , with location 0 , scale @xmath235 and asymmetry and skewness parameters @xmath236 ) .",
    "we set @xmath237 where @xmath238 is a gamma distribution with mean @xmath239 and @xmath240 .",
    "this is a garch(1,1 ) model with an intractable likelihood .",
    "we consider daily log - returns data from the s&p 500 index from 03/1/11 to 14/02/13 , which constitutes 533 data - points . in the priors",
    ", we set @xmath241 and @xmath242 , which are not overly informative .",
    "in addition , @xmath243 and @xmath244 .",
    "we consider @xmath245 and only a noisy abc approximation of the model .",
    "algorithms [ alg : ntry ] and [ alg : new ] are to be compared .",
    "the mcmc proposals on the parameters are random - walks on the log - scale and for both algorithms we set @xmath246 .",
    "it should be noted that our results are fairly robust to changes in @xmath247 $ ] , which are the values we tested the algorithm with .    in figure",
    "[ fig : tracee05 ] we present the trace - plot of 50000 iterations of both mcmc kernels when @xmath248 .",
    "algorithm [ alg : ntry ] took about 0.30 seconds per iteration and algorithm [ alg : new ] took about 1.12 seconds per iteration we modified the proposal variances to yield an acceptance rate around 0.3 .",
    "the plot shows that both algorithms appear to move across the state - space in a very reasonable way .",
    "the new algorithm takes much longer and in this situation does not appear to be required .",
    "this run is one of many we performed and we observed this behaviour in many of our runs .    in figure [ fig : tracee001 ] we can observe the trace plots from a particular ( typical ) run when @xmath249 . in this case , both algorithms are run for 200000 iterations .",
    "algorithm [ alg : ntry ] took about 0.28 seconds per iteration and algorithm [ alg : new ] took about 2.06 seconds per iteration ; this issue is discussed below . in this scenario , considerable effort was expended for algorithm [ alg : ntry ] to yield an acceptance rate around 0.3 , but despite this , we were unable to make the algorithm traverse the state - space .",
    "in contrast , with less effort , algorithm [ alg : new ] appears to perform quite well and move around the parameter space ( the acceptance rate was around 0.15 versus 0.01 for algorithm [ alg : ntry ] ) .",
    "whilst the computational time for algorithm [ alg : new ] is considerably more than algorithm [ alg : ntry ] , in the same amount of computation time , it still moves more around the state space ; algorithm runs of the same length are provided for presentational purposes .",
    "we remark that , whilst we do not claim that it is ` impossible ' to make algorithm [ alg : ntry ] mix well in this example , we were unable to do so and , alternatively , for algorithm [ alg : new ] we expended considerably less effort for very reasonable performance .",
    "this example is typical of many runs of the algorithm and examples we have investigated and is consistent with the discussion in section [ sec : comp_consider ] , where we stated that algorithm [ alg : new ] is likely to out - perform algorithm [ alg : ntry ] when the @xmath153 are not large , which is exactly the scenario in this example .    turning to the cost of simulating algorithm [ alg : new ] ; for the case @xmath248 we simulated the data an average of 148000 times ( per - iteration ) and for @xmath249 this figure was 330000 .",
    "in this example signifcant effort is expended in simulating the @xmath183 .",
    "this shows , at least in this example , that one can run the algorithm without it failing to sample the @xmath183 .",
    "the results here suggest that one should prefer algorithm [ alg : new ] only in challenging scenarios , as it can be very expensive in practice .    finally , we remark that the mle for a gaussian garch model , is @xmath250 .",
    "this differs to the posterior means , which may indicate that a stable distribution could be useful for modelling the observations for this class of models .",
    "in this article we have considered approximate bayesian inference from observation driven time series models .",
    "we looked at some consistency properties of the corresponding map estimators and also proposed an efficient abc - mcmc algorithm to sample from these approximate posteriors .",
    "the performance of the latter was illustrated using numerical examples .",
    "there are several interesting extensions to this work :    * the asymptotic analysis of the abc posterior in section [ sec : consis ] can be further extended .",
    "for example , one may consider bayesian consistency or bernstein von - mises theorems , which could provide further justification to the approximation that was introduced here .",
    "alternatively , one could look at the the asymptotic bias of the abc posterior w.r.t .",
    "@xmath51 or the asymptotic loss in efficiency of the noisy abc posterior w.r.t .",
    "@xmath51 similar to the work in @xcite for hidden markov models . * the geometric ergodicity of the presented mcmc sampler can be further investigated in the spirit of @xcite . *",
    "an investigation to extend the ideas here for sequential monte carlo methods should be beneficial .",
    "this has been initiated in @xcite in the context of particle filtering for a different class of models .",
    "a. jasra acknowledges support from the moe singapore and funding from imperial college london .",
    "n. kantas was kindly funded by epsrc under grant ep / j01365x/1 .",
    "[ proof of proposition [ prop1 ] ] the proof of @xmath251 follows from ( * ? ? ? * theorem 21 ) if we can establish conditions ( b1 - 3 ) for our perturbed abc model .",
    "clearly ( b1 ) and part of ( b2 ) holds .",
    "( b3-i ) hold via ( * ? ? ?",
    "* lemma 21 ) via ( a[as:3 ] ) .",
    "we first need to show that for any @xmath252 that @xmath253 is continuous .",
    "consider @xmath254 let @xmath255 , then , by ( a[as:2 ] ) there exists a @xmath256 such that for @xmath257 @xmath258 and hence for @xmath259 as above @xmath260 which establishes ( b2 ) of @xcite .",
    "now , for ( b3-ii ) of @xcite , we note that as @xmath261 ( see ( a[as:2 ] ) ) the @xmath262 function is lipshitz and @xmath263 for some @xmath208 that does not depend upon @xmath264 . now @xmath265\\mu(dy)|\\ ] ] and @xmath266|.\\ ] ]",
    "thus , by ( a[as:3 ] ) and the fact that ( b3-i ) of @xcite holds : @xmath267 note , finally that ( b3-iii ) trivially follows by @xmath268 .",
    "hence we have proved that @xmath269    [ proof of proposition [ prop2 ] ] this result follows from ( * ? ? ?",
    "* proposition 23 )",
    ". one can establish assumptions ( b1 - 3 ) of @xcite using the proof of proposition [ prop1 ] .",
    "thus we need only prove that @xmath270 now , for any @xmath271 @xmath272\\mu(dy).\\ ] ] by ( a[as:4 ] ) @xmath273 @xmath274 , so @xmath275\\mu(dy)\\leftrightarrow x = x'\\ ] ] which completes the proof .",
    "[ proof of proposition [ prop : rel_var ] ] we have @xmath180=\\frac{1}{(\\prod_{k=1}^{n}\\frac{\\alpha_{k}(y_{1:k},\\epsilon,\\gamma)}{n-1})^{2}}\\bigg(\\prod_{k=1}^{n}\\mathbb{e}_{\\gamma , n}\\big[\\frac{1}{(m_{k}-1)^{2}}\\big]-\\big(\\prod_{k=1}^{n}\\frac{\\alpha_{k}(y_{1:k},\\epsilon,\\gamma)}{n-1}\\big)^{2}\\bigg).\\ ] ] now , by @xcite ( @xmath276 ) for any @xmath277 @xmath278=\\frac{\\alpha_{k}(y_{1:k},\\epsilon,\\gamma)^{2}}{(n-1)(n-2)}\\ ] ] and thus clearly @xmath279\\leq\\frac{\\alpha_{k}(y_{1:k},\\epsilon,\\gamma)^{2}}{(n-1)(n-2)}.\\ ] ] hence @xmath180\\leq(n-1)^{2n}\\big(\\frac{1}{(n-1)^{n}(n-2)^{n}}-\\frac{1}{(n-1)^{2n}}\\big).\\label{eq : maineq1}\\ ] ] now the r.h.s.of is equal to @xmath280}{n^{n}-2nn^{n-1}+\\sum_{i=2}^{n}\\binom{n}{i}n^{n - i}(-2)^{i}}.\\label{eq : maineq2}\\ ] ] now , we will show @xmath281\\leq0.\\label{eq : ineq1}\\ ] ] the proof is given when @xmath1 is odd .",
    "the case @xmath1 even follows by the proof as @xmath282 is odd and the additional term is negative .",
    "now we have for @xmath283 that the sum of consecutive even and odd terms is equal to @xmath284\\ ] ] which is negative as @xmath285 thus we have established .",
    "we will now show that @xmath286 following the same approach as above ( i.e.  @xmath1 is odd ) the sum of consecutive even and odd terms is equal to @xmath287.\\ ] ] this is positive if @xmath288 as @xmath289 and @xmath290 it follows that @xmath291 ; thus one can establish .",
    "[ proof of proposition [ prop : time ] ] we have @xmath294 & = & \\int_{(\\theta\\times\\mathsf{x})^{2}}\\sum_{\\mathsf{m}_{n}^{n}}\\big(\\sum_{k=1}^{n}m_{k}\\big)\\big\\{\\prod_{k=1}^{n}\\binom{m_{k}-1}{n-1}\\alpha_{k}(y_{1:k},\\gamma',\\epsilon)^{n}(1-\\alpha_{k}(y_{1:k},\\gamma',\\epsilon))^{m_{k}-n}\\big\\ } q(\\gamma,\\gamma')\\zeta k^{i}(d\\gamma)d\\gamma'\\\\   & = & \\int_{(\\theta\\times\\mathsf{x})^{2}}\\big(\\sum_{k=1}^{n}\\frac{n}{\\alpha_{k}(y_{1:k},\\gamma,\\epsilon)}\\big)q(\\gamma,\\gamma')\\zeta k^{i}(d\\gamma)d\\gamma'\\leq\\frac{nn}{c}.\\end{aligned}\\ ] ] where we have used the expectation of a negative - binomial random variable and applied @xmath201 , @xmath202a.e . in the inequality",
    "10 andrieu , c. , doucet , a. & holenstein , r.  ( 2010 ) .",
    "particle markov chain monte carlo methods ( with discussion ) .",
    "_ j. r. statist .",
    "b _ , * 72 * , 269342 .",
    "andrieu , c. & vihola , m.  ( 2012 ) .",
    "convergence properties of pseudo - marginal markov chain monte carlo algorithms .",
    "arxiv:1210.1484 [ math.pr ]"
  ],
  "abstract_text": [
    "<S> in the following article we consider approximate bayesian parameter inference for observation driven time series models . </S>",
    "<S> such statistical models appear in a wide variety of applications , including econometrics and applied mathematics . </S>",
    "<S> this article considers the scenario where the likelihood function can not be evaluated point - wise ; in such cases , one can not perform exact statistical inference , including parameter estimation , which often requires advanced computational algorithms , such as markov chain monte carlo ( mcmc ) . </S>",
    "<S> we introduce a new approximation based upon approximate bayesian computation ( abc ) . under some conditions , we show that as @xmath0 , with @xmath1 the length of the time series , the abc posterior has , almost surely , a maximum _ a posteriori _ ( map ) estimator of the parameters which is different from the true parameter . </S>",
    "<S> however , a noisy abc map , which perturbs the original data , asymptotically converges to the true parameter , almost surely . in order to draw statistical inference , for the abc approximation adopted </S>",
    "<S> , standard mcmc algorithms can have acceptance probabilities that fall at an exponential rate in @xmath1 and slightly more advanced algorithms can mix poorly . </S>",
    "<S> we develop a new and improved mcmc kernel , which is based upon an exact approximation of a marginal algorithm , whose cost per - iteration is random but the expected cost , for good performance , is shown to be @xmath2 per - iteration . </S>",
    "<S> we implement our new mcmc kernel for parameter inference from models in econometrics . </S>",
    "<S> + * key words : * observation driven time series models , approximate bayesian computation , asymptotic consistency , markov chain monte carlo .    * approximate inference for observation driven time series models with intractable likelihoods *    by ajay jasr@xmath3 , nikolas kanta@xmath4 , & elena ehrlic@xmath5    @xmath6department of statistics & applied probability , national university of singapore , singapore , 117546 , sg . </S>",
    "<S> + e-mail:`staja@nus.edu.sg `    @xmath7department of statistical science , university college london , london , wc1e 6bt , uk . </S>",
    "<S> + e-mail:`n.kantas@ucl.ac.uk `    @xmath8department of mathematics , imperial college london , london , sw7 2az , uk . </S>",
    "<S> + e-mail:`e.ehrlich05@imperial.ac.uk ` </S>"
  ]
}