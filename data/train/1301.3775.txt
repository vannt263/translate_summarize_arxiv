{
  "article_text": [
    "deep networks complement the hierarchical structure in natural data  @xcite . by breaking complex calculations into many steps , deep networks can gradually build up complicated decision boundaries or input transformations , facilitate the reuse of common substructure , and explicitly compare alternative interpretations of ambiguous input  @xcite .",
    "leveraging these strengths , deep networks have facilitated significant advances in solving sensory problems like visual classification and speech recognition  @xcite .",
    "although deep networks have traditionally used independent parameters for each layer , they are equivalent to recurrent networks in which a disjoint set of units is active on each time step .",
    "the corresponding representations are sparse , and thus invite the incorporation of powerful techniques from sparse coding  @xcite .",
    "recurrence opens the possibility of sharing parameters between successive layers of a deep network .",
    "this paper introduces the _ discriminative recurrent sparse auto - encoder _ model ( drsae ) , comprising a recurrent encoder of rectified linear units  ( relu ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , connected to two linear decoders that reconstruct the input and predict its supervised classification .",
    "the recurrent encoder is unrolled in time for a fixed number of iterations , with the input projecting to each resulting layer , and trained using backpropagation - through - time  @xcite .",
    "training initially minimizes an unsupervised sparse reconstruction error ; the loss function is then augmented with a discriminative term on the supervised classification . in its temporally - unrolled form , the network can be seen as a deep network , with parameters shared between the hidden layers .",
    "the temporal depth allows the system to exhibit far more representational power , while keeping the number of trainable parameters fixed .",
    "interestingly , experiments show that drsae does not just discover more discriminative `` parts '' of the form conventionally produced by sparse coding .",
    "rather , the hidden units spontaneously differentiate into two types : a small number of categorical - units and a larger number of part - units .",
    "the categorical - units have decoder bases that look like prototypes of the input classes .",
    "they are weakly influenced by the input and activate late in the dynamics as the result of interaction with the part - units .",
    "in contrast , the part - units are strongly influenced by the input , and encode small transformations through which the prototypes of categorical - units can be reshaped into the current input .",
    "categorical - units compete with each other through mutual inhibition and cooperate with relevant part - units .",
    "this can be interpreted as a representation of the data manifold in which the categorical - units are points on the manifold , and the part - units are akin to tangent vectors along the manifold .",
    "the encoder architecture of drsae is modeled after the iterative shrinkage and threshold algorithm ( ista ) , a proximal method for sparse coding  @xcite .",
    "@xcite showed that the sparse representations computed by ista can be efficiently approximated by a structurally similar encoder with a less restrictive , learned parameterization .",
    "rather than learn to approximate a precomputed optimal sparse code , the lista autoencoders of @xcite are trained to directly minimize the sparse reconstruction loss function .",
    "drsae extends lista autoencoders with a non - negativity constraint , which converts the shrink nonlinearity of lista into a rectified linear operator ; and introduces a unified classification loss , as previously used in conjunction with traditional sparse coders  @xcite and other autoencoders  @xcite .",
    "drsaes resemble the structure of deep sparse rectifier neural networks @xcite , but differ in that the parameter matrices at each layer are tied @xcite , the input projects to all layers , and the outputs are normalized .",
    "drsaes are also reminiscent of the recurrent neural networks investigated by @xcite , but use a different nonlinearity and a heavily regularized loss function .",
    "finally , they are similar to the recurrent networks described by @xcite , but have recurrent connections amongst the hidden units , rather than between the hidden units and the input units , and introduce classification and sparsification losses .",
    "is the hidden representation after iteration @xmath0 of @xmath1 , and is initialized to @xmath2 ; @xmath3 is the input ; and @xmath4 is the supervised classification .",
    "overbars denote approximations produced by the network , rather than the true input .",
    "@xmath5 , @xmath6 , @xmath7 , and @xmath8 are learned parameters .",
    "[ architecture_figure],width=528 ]    in the following , we use lower - case bold letters to denote vectors , upper - case bold letters to denote matrices , superscripts to indicate iterative copies of a vector , and subscripts to index the columns ( or rows , if explicitly specified by the context ) of a matrix or ( without boldface ) the elements of a vector .",
    "we consider discriminative recurrent sparse auto - encoders ( drsaes ) of rectified linear units with the architecture shown in figure  [ architecture_figure ] : @xmath9 for @xmath10 , where @xmath11-dimensional vector @xmath12 is the activity of the hidden units at iteration @xmath0 , @xmath13-dimensional vector @xmath3 is the input , and @xmath14 . unlike traditional recurrent autoencoders @xcite , the input projects to every iteration .",
    "we call the @xmath15 parameter matrix @xmath5 the encoding matrix , and the @xmath16 parameter matrix @xmath6 the explaining - away matrix",
    ". the @xmath11-element parameter vector @xmath8 contains a bias term .",
    "the parameters also include the @xmath17 decoding matrix @xmath7 and the @xmath18 classification matrix @xmath19 .",
    "we pretrain drsaes using stochastic gradient descent on the unsupervised loss function @xmath20 with the magnitude of the columns of @xmath7 bounded by @xmath21 , ; otherwise , the magnitude of @xmath22 will shrink to zero and the magnitude of the columns of @xmath7 will explode .",
    "this and all other such constraints are enforced by a projection after each sgd step . ] and the magnitude of the rows of @xmath5 bounded by @xmath23 .",
    "weight regularization @xcite",
    ". the particular value of the bound is heuristic , and was determined by an informal search of parameter space .",
    "] we then add in the supervised classification loss function @xmath24 where the multinomial logistic loss function is defined by @xmath25 and @xmath26 is the index of the desired class .",
    "and @xmath27 only depend directly on the final iteration of the hidden units @xmath28 . ]",
    "starting with the parameters learned by the unsupervised pretraining , we perform discriminative fine - tune by stochastic gradient descent on @xmath29 , with the magnitude of the rows of @xmath19 bounded by @xmath30 . weight regularization @xcite .",
    "the particular value of the bound is heuristic , and was determined by an informal search of parameter space . ]",
    "the learning rate of each matrix is scaled down by the number of times it is repeated in the network , and the learning rate of the classification matrix is scaled down by a factor of @xmath30 , to keep the effective learning rate consistent amongst the parameter matrices .",
    "we train drsaes with @xmath31 recurrent iterations ( ten nontrivial passes through the explaining - away matrix  @xmath6 ) . ] and @xmath32  hidden units on the mnist dataset of @xmath33 grayscale handwritten digits @xcite , with each input normalized to have @xmath34 magnitude equal to @xmath21 .",
    "we use a training set of 50,000 elements , and a validation set of 10,000 elements to perform early - stopping .",
    "encoding , decoding , and classification matrices learned via this procedure are depicted in figure  [ dictionary_figure ] .",
    "bars above the decoders indicate the angle between the encoder and the decoder for the displayed unit .",
    "the most prototypical unit always makes the strongest contribution to the classification , and has a large ( but not necessarily the largest ) angle between its encoder and decoder . some units that make large contributions to the classification represent global transformations , such as rotations , of a prototype  @xcite .",
    "[ most_categorical_filters_figure],title=\"fig:\",width=384 ]    it is widely believed that natural stimuli , such as images and sounds , fall near a low - dimensional manifold within a higher - dimensional space ( the _ manifold hypothesis _ ) @xcite .",
    "the low - dimensional data manifold provides an intuitively compelling and empirically effective basis for classification @xcite .",
    "the continuous deformations that define the data manifold usually preserve identity , whereas even relatively small invalid transformations may change the class of a stimulus .",
    "for instance , the various handwritten renditions of the digit @xmath35 in in the last column of figure  [ gradual_reconstruction_figure](c ) barely overlap , and so the euclidean distance between them in pixel space is greater than that to the nearest @xmath36 formed by closing both loops .",
    "nevertheless , smooth deformations of one @xmath35 into another correspond to relatively short trajectories along the data manifold,(c ) shows how each input can be produced by identity - preserving deformations from a common prototype , using the tangent space decomposition produced by our network . ] whereas the transformation of a @xmath35 into an @xmath36 requires a much longer path within the data manifold . a prohibitive amount of data is required to fully characterize the data manifold @xcite , so it is often approximated by the set of linear submanifolds tangent to the data manifold at the observed datapoints , known as the _ tangent spaces _ @xcite .",
    "drsaes naturally and efficiently form a tangent space - like representation , consisting of a point on the data manifold indicated by the categorical - units , and a shift within the tangent space specified by the part - units .    before discriminative fine - tuning",
    ", drsaes perform a traditional part - based decomposition , familiar from sparse coding , as shown in figure  [ gradual_reconstruction_figure](a ) .",
    "the decoding matrix columns are class - independent , local pen strokes , and many units make a comparable , small contribution to the reconstruction .",
    "after discriminative fine - tuning , the hidden units differentiate into sparse coding local part - units , and global prototype categorical - units that integrate over them .",
    "as shown in figure  [ gradual_reconstruction_figure](b , c ) , the input is decomposed into a prototype , corresponding to a point on the data manifold ; and a set of deformations from this prototype along the data manifold , corresponding to shifts within the tangent space",
    ". the same prototype can be used for very different inputs , as demonstrated in figure  [ gradual_reconstruction_figure](c ) , since the space of deformations is rich enough to encompass diverse transformations without moving off the data manifold .",
    "even when the prototype is very different from the input , all steps along the reconstruction trajectories in figure  [ gradual_reconstruction_figure](b , c ) are recognizable as members of the same class .",
    "the prototypes learned by the categorical - units for each class are not simply the average over the elements of the class , as depicted in figure  [ most_categorical_filters_figure ] .",
    "each class includes many possible input variations , so its average is blurry .",
    "the prototypes , in contrast , are sharp , and look like representative elements of the appropriate class .",
    "many categorical - units are available for each class , as shown in figure  [ categorical_connection_figure ] .",
    "not all categorical - units correspond to full prototypes ; some capture global transformations of a prototype , such as rotations  @xcite .",
    "consistent with prototypes for the non - negative mnist inputs , the decoding matrix columns of the categorical - units are generally positive , as shown in figure  [ categorical_connection_statistics_figure](e ) . in contrast , the decoders of the part - units are approximately mean - zero and so can not serve as prototypes themselves .",
    "rather , they shift and transform prototypes , moving activation from one region in the image to another , as demonstrated in figure  [ gradual_reconstruction_figure](b , c ) .",
    "discrepancies between the prototype and the input due to transformations along the data manifold are explained by class - consistent part - units , and only serve to further activate the categorical - units of that class , as in figure  [ categorical_connection_figure](a , c ) .",
    "discrepancies between the prototype and the input due to deformations orthogonal to the data manifold are explained by class - incompatible part - units , and serve to suppress the categorical - units of that class , both directly and via activation of incompatible categorical - units .",
    "if the wrong prototype is turned on , the residual input will generally contain substantial unexplained components .",
    "part - units obey ista - like dynamics and thus function as a sparse coder on the residual input , so part - units that match the unexplained components of the input will be activated .",
    "these part - units will have positive connections to categorical - units with compatible prototypes , and so will tend to activate categorical - units associated with the true class ( so long as the unexplained components of the input are diagnostic ) .",
    "the spuriously activated categorical - unit will not be able to sustain its activity , since few compatible part - units will be required to capture the residual input .",
    "the classification approach used by drsaes is different from one based upon a traditional sparse coding decomposition : it projects into the space of deviations from a prototype , which is not the same as the space of prototype - free parts , as is clear from figure  [ gradual_reconstruction_figure](a , b ) .",
    "for instance , a @xmath30 can easily be constructed using the parts of a @xmath37 , making it difficult to distinguish the two .",
    "indeed , the first seven progressive reconstruction steps of the @xmath37 in figure  [ gradual_reconstruction_figure](a ) could just as easily be used to produce a @xmath30 . however , starting from a @xmath37 prototype , the parts required to break the bottom loop are outside the data manifold of the @xmath37 class , and so will tend to change the active prototype .",
    "bengio , y. , & gingras , f. ( 1996 ) .",
    "recurrent neural networks for missing or asynchronous data . in d.  touretzky , m.  mozer , & m.  hasselmo ( eds . ) _ advances in neural information processing systems ( nips 8) _ ( pp .",
    "395401 ) .",
    "bradley , d. m. , & bagnell , j. a. ( 2008 ) .",
    "differentiable sparse coding . in d. koller , d. schuurmans , y. bengio , & l. bottou ( eds . ) _ advances in neural information processing systems ( nips 21 ) _",
    "113120 ) .",
    "chambolle , a. , de vore , r. a. , lee , n. y. , & lucier , b. j. ( 1998 ) .",
    "nonlinear wavelet image processing : variational problems , compression , and noise removal through wavelet shrinkage .",
    "_ ieee transactions on image processing _ , _",
    "7_(3 ) , 319335 .",
    "ciresan , d. , meier , u. , & schmidhuber , j. ( 2012 ) .",
    "multi - column deep neural networks for image classification . in _ proceedings of the 25th ieee conference on computer vision and pattern recognition ( cvpr 2012 ) _",
    "36423649 ) .",
    "coates , a. , & ng , a. y. ( 2011 ) . the importance of encoding versus training with sparse coding and vector quantization l. getoor & t. scheffer ( eds . ) _ proceedings of the 28th international conference on machine learning ( icml 2011 ) _",
    "921928 ) .",
    "dahl , g. e. , yu , d. , deng , l. , & acero , a. ( 2012 ) .",
    "context - dependent pre - trained deep neural networks for large - vocabulary speech recognition .",
    "_ ieee transactions on audio , speech , and language processing _ , _",
    "20_(1 ) , 3042 .",
    "daubechies , i. , defrise , m. , & de mol , c. ( 2004 ) . an iterative thresholding algorithm for linear inverse problems with a sparsity constraint . _",
    "communications on pure and applied mathematics _ , _",
    "57_(11 ) , 14131457 .",
    "glorot , x. , bordes , a. , & bengio , y. ( 2011 ) .",
    "deep sparse rectifier neural networks . in g. gordon , d. dunson , & m. dudik ( eds . ) _ jmlr w&cp : proceedings of the fourteenth international conference on artificial intelligence and statistics ( aistats 2011 ) _",
    "315323 ) .",
    "gregor , k. , & lecun , y. ( 2010 ) .",
    "learning fast approximations of sparse coding . in j. frnkranz & t. joachims ( eds . ) _ proceedings of the 27th international conference on machine learning ( icml 2010 ) _ ( pp .",
    "399406 ) .",
    "jarrett , k. , kavukcuoglu , k. , ranzato , m. a. , & lecun , y. ( 2009 ) .",
    "_ what is the best multi - stage architecture for object recognition ? _ in _ proceedings of the 12th international conference on computer vision ( iccv 2009 ) _",
    "21462153 ) .",
    "lecun , y. , bottou , l. , bengio , y. , & haffner , p. ( 1998 ) .",
    "gradient - based learning applied to document recognition .",
    "_ proceedings of the ieee _ , _",
    "86_(11 ) , 22782324 .",
    "lee , h. , ekanadham , c. , & ng , a. ( 2008 ) .",
    "sparse deep belief net model for visual area v2 . in j. c. platt , d. koller , y. singer & s. roweis ( eds . ) _ advances in neural information processing systems ( nips 20 ) _ , ( pp . 873880 ) .",
    "mairal , j. , bach , f. , ponce , j. , sapiro , g. , & zisserman , a. ( 2009 ) . supervised dictionary learning . in d. koller , d. schuurmans , y. bengio , & l. bottou ( eds . )",
    "_ advances in neural information processing systems ( nips 21 ) _",
    "10331040 ) .",
    "nair , v. , & hinton , g. e. ( 2010 ) . rectified linear units improve restricted boltzmann machines . in j. frnkranz & t. joachims ( eds . ) _ proceedings of the 27th international conference on machine learning ( icml 2010 ) _ ( pp .",
    "807 - 814 ) .",
    "narayanan , h. & mitter , s. ( 2010 ) .",
    "sample complexity of testing the manifold hypothesis . in j. lafferty , c. k. i. williams , j. shawe - taylor , r.s .",
    "zemel , & a. culotta ( eds . ) _ advances in neural information processing systems ( nips 23 ) _",
    "17861794 ) .",
    "ranzato m. , poultney , c. , chopra , s. , & lecun , y. ( 2006 ) .",
    "efficient learning of sparse representations with an energy - based model . in b. schlkopf , j. platt , & t. hoffman ( eds . ) _ advances in neural information processing systems ( nips 19 ) _ , ( pp . 11371144 ) .",
    "ranzato , m. , & szummer , m. ( 2008 ) .",
    "semi - supervised learning of compact document representations with deep networks in a. mccallum & s. roweis ( eds . ) , _ proceedings of the 25th annual international conference on machine learning ( icml 2008 ) _ ( pp . 792799 ) .",
    "rifai , s. , dauphin , y. , vincent , p. , bengio , y. , & muller , x. ( 2011 ) . the manifold tangent classifier . in j. shawe - taylor , r. s. zemel ,",
    "p. bartlett , f. c. n. pereira , & k. q. weinberger ( eds . ) _ advances in neural information processing systems ( nips 24 ) _",
    "( pp . 22942302 ) .",
    "rumelhart , d. e. , hinton , g. e. , & williams , r. j. ( 1986 ) .",
    "learning internal representations by error propagation . in d. e. rumelhart ,",
    "j. l. mcclelland , and the pdp research group ( eds . ) , _ parallel distributed processing : explorations in the microstructure of cognition : vol .",
    "1 . foundations _",
    "318362 ) .",
    "cambridge , ma : mit press .",
    "salinas , e. , & abbott , l. f. ( 1996 ) .",
    "a model of multiplicative neural responses in parietal cortex .",
    "_ proceedings of the national academy of sciences of the united states of america _ , _",
    "93_(21 ) , 1195611961 .",
    "seung , h. s. ( 1998 ) .",
    "learning continuous attractors in recurrent networks . in m.",
    "i. jordan , m. j. kearns , & s. a. solla ( eds . ) _ advances in neural information processing systems ( nips 10 ) _",
    "654660 ) .",
    "simard , p. , lecun , y. , & denker , j. s. ( 1993 ) .",
    "efficient pattern recognition using a new transformation distance . in s. j. hanson , j. d. cowan ,",
    "& c. l. giles ( eds . ) _ advances in neural information processing systems ( nips 5 ) _ ( pp . 5058 ) .",
    "simard , p. , lecun , y. , denker , j. , & victorri , b. ( 1998 ) .",
    "transformation invariance in pattern recognition : tangent distance and tangent propagation . in g. orr , & k. muller ( eds . ) ,",
    "_ neural networks : tricks of the trade_. berlin : springer .",
    "sprechmann , p. , bronstein , a. , & sapiro , g. ( 2012 ) .",
    "learning efficient structured sparse models . in j. langford & j. pineau ( eds . ) _ proceedings of the 29th international conference on machine learning ( icml 12 ) _ ( pp .",
    "615622 ) .",
    "deng , l. & yu , d. ( 2011 ) .",
    "deep convex net : a scalable architecture for speech pattern classification . in _ proceedings of the 12th annual conference of the international speech communication association ( interspeech 2011 ) _ ( pp .",
    "2285 - 2288 ) .",
    "zeiler , m. d. , taylor , g. w. , & fergus , r. ( 2011 ) .",
    "adaptive deconvolutional networks for mid and high level feature learning . in _ proceedings of the 13th international conference on computer vision ( iccv 2011 ) _",
    "20182025 ) ."
  ],
  "abstract_text": [
    "<S> we present the discriminative recurrent sparse auto - encoder model , comprising a recurrent encoder of rectified linear units , unrolled for a fixed number of iterations , and connected to two linear decoders that reconstruct the input and predict its supervised classification . </S>",
    "<S> training via backpropagation - through - time initially minimizes an unsupervised sparse reconstruction error ; the loss function is then augmented with a discriminative term on the supervised classification . </S>",
    "<S> the depth implicit in the temporally - unrolled form allows the system to exhibit far more representational power , while keeping the number of trainable parameters fixed . from an initially unstructured network </S>",
    "<S> the hidden units differentiate into categorical - units , each of which represents an input prototype with a well - defined class ; and part - units representing deformations of these prototypes . </S>",
    "<S> the learned organization of the recurrent encoder is hierarchical : part - units are driven directly by the input , whereas the activity of categorical - units builds up over time through interactions with the part - units . even using a small number of hidden units per layer , </S>",
    "<S> discriminative recurrent sparse auto - encoders achieve excellent performance on mnist . </S>"
  ]
}