{
  "article_text": [
    "animals are challenged in various ways to learn , produce , reproduce and predict temporal patterns .",
    "a prominent example are the numerous motor programs necessary to interact efficiently with the environment .",
    "one specific manifestation is the vocal motor system of song birds .",
    "it has been shown that the temporal sequence of syllables in a bird s song corresponds to temporal sequences of bursts in the neurons of the forebrain control system  @xcite .",
    "these are learned and stored by the adolescent bird .",
    "temporal codes seem to be used for a variety of other tasks as well .",
    "temporal coding in the retina  @xcite is an example , as is information transport in the olfactory system of the locust . in the latter",
    "it has been shown that the purely identity coded information of the receptor neurons is transformed into an identity - temporal code inside the antennal lobe  @xcite .    whereas there is a long history of research on sequence learning and recognition in the framework of abstract neural networks ( cf the relevant chapters in @xcite and @xcite and references therein ) it is an open question",
    "how the learning and memory of time sequences is accomplished in real biological neural systems .",
    "three main principles for representing time in neural systems are frequently discussed :    * the first makes use of delays and filters .",
    "there are various ways of processing of temporal information in the dendritic tree  @xcite or through axonal delays  @xcite .",
    "other examples are multilayer neural networks in which the delay of the synaptic connections between layers allows to represent or decode temporal information and propagating waves as known from the thalamo - cortical system  @xcite . *",
    "the second principle rests on feedback . through delayed feedback",
    "temporal information can be processed on the level of individual neurons as well as on the level of larger structures .",
    "a prominent example for this are recurrent multi layer neural networks which play a role in sequence memory in the hippocampus  @xcite . *",
    "the third principle is to transform the temporal information into spatial information .",
    "this can occur through the dynamics of a network with asymmetric lateral inhibition  @xcite .",
    "in this paper we demonstrate an alternative mechanism which maps the temporal information to the strength of synapses in a network through spike timing dependent plasticity ( stdp ) .",
    "similar mechanisms have been suggested for predictive activity and direction selectivity in the visual system  @xcite and learning in the hippocampus  @xcite as well as prediction in hippocampal place fields and route learning in rats  @xcite .",
    "in contrast to these earlier works we focus on questions of learning of several distinct input sequences in one system and a sparse coding scheme . this learning capability is necessary in order to process the identity - temporal code believed to be generated by winnerless competition in sensory systems  @xcite .",
    "synaptic plasticity in the connections among neurons allows networks to alter the details of their interaction and develop memories of previous input signals .",
    "the details of the methods by which biological neurons express plasticity at synapses is not fully understood at the biophysical level , but many aspects of the phenomena which occur when presynaptic and postsynaptic neurons are jointly activated are now becoming clear .",
    "first of all , it seems well established that activity at both the presynaptic and the postsynaptic parts of a neural junction is required for the synaptic strength to change .",
    "arrival of a presynaptic action potential will induce , through normal neurotransmitter release and reception by postsynaptic receptors , a postsynaptic electrical action which generally leads to no change in the coupling strength at that synapse .",
    "depolarization of the postsynaptic cell by various means _ coupled with _ arrival of a presynaptic action potential can lead to changes in synaptic strength in a variety of experimental protocols .",
    "it is quite important that changes in the synaptic strength , which we denote in terms of a conductivity change @xmath0 can be either positive , called potentiation , or negative , called depression .",
    "when the expression of @xmath0 is long lasting , several hours or even much longer after induction , increases in @xmath1 are called long term potentiation or ltp , and decreases in @xmath1 are called long term depression or ltd . good reviews of the current situation are found in  @xcite .",
    "ltp and ltd can be induced by ( 1 ) depolarizing the postsynaptic cell to a fixed membrane voltage and presenting presynaptic spiking activity at various frequencies , by ( 2 ) inducing slow ( ltd ) or rapid ( ltp ) release of @xmath2  @xcite , or by ( 3 ) activating the presynaptic terminal a few tens of milliseconds before activating the postsynaptic cell , leading to ltp , or presenting the activation in the other order , leading to ltd  @xcite .    in this paper",
    "we study numerically a network composed of integrate - and - fire neurons which are densely coupled with synaptic interactions whose maximal conductances are permitted to change in accordance with the observations on closely spaced spike arrival times to the presynaptic and postsynaptic junctions of the synapse .",
    "stdp learning rule .",
    "@xmath3 for @xmath4 and @xmath5 for @xmath6 , @xmath7 , @xmath8 .",
    "this form of the learning rule was directly inferred from experimental data @xcite ]    the response of a learning synapse to the arrival of a presynaptic spike at @xmath9 and a postsynaptic spike at @xmath10 is a function only of @xmath11 and for @xmath4 @xmath12 is positive , ltp , and for @xmath6 @xmath12 is negative , ltd .",
    "@xmath13 where @xmath7 , @xmath14 , @xmath15 and @xmath16 are positive constants ( see fig.[figure1 ] ) .",
    "synaptic plasticity of this type is often referred to as spike timing dependent plasticity ( stdp ) . for many mammalian _ in vitro _ or cultured preparations the characteristic ltd time @xmath16 is about two or three times longer than the characteristic ltp time @xmath15 .    here we inquire how a network composed of familiar integrate - and - fire neurons can develop preferred spatial patterns of connectivity when interacting through synapses which update their strength according to the stdp learning rule just given .",
    "this rule is a simplification , which applies for our setting of spiking neurons , of more general models  @xcite which indicate how @xmath12 behaves under stimulus of arbitrary presynaptic and postsynaptic waveforms .    the transformation of temporal information into synapse strength through stdp maps a temporal sequence of excitations of neurons to a chain of stronger or weaker synapses among these neurons .",
    "if the synapses are excitatory , a strengthened chain of synapses facilitates subsequent excitations of the same temporal pattern up to a point that activation of a few neurons from the temporal sequence allows the system to complete the remaining sequence .",
    "the temporal sequence thus has been learned by the system .",
    "we demonstrate this type of sequence learning in a computer simulation of a system with integrate - and - fire neurons and rall type synapses and investigate the reliability of learning , the storage capacity in terms of the number of stored sequences , the scaling of both with system size and sequence length and the robustness against different types of noise .",
    "morphology of the model system .",
    "the ovals are artificial input neurons producing rectangular spikes of @xmath17 duration at specified times .",
    "each is connected by a non - plastic excitatory synapse to one of the neurons in the main `` cortex '' ( dotted lines ) .",
    "the full circles depict the integrate - and - fire neurons of the main `` cortex '' .",
    "they are connected all - to - all by stdp synapses shown as solid gray lines .",
    "the big full circle on the right depicts a neuron with slow calcium dynamics which inhibits all neurons in the `` cortex '' through the non - plastic synapses shown as dashed lines . ]     typical piece of a training session .",
    "the rectangular spikes in the upper panel are the input signal spaced by @xmath18 in this example .",
    "the traces in the middle panel are the integrate - and - fire memory neurons .",
    "the slow spike train in the bottom panel belongs to the globally inhibitory neuron . note the instantaneous onset of the spikes in the integrate - and - fire neurons and how the inhibitory neuron segments the input into pieces of 6 spikes each . ]    to explore the learning principle we simulated a network with the topology shown in fig .",
    "[ figure2 ] . in this network",
    "@xmath19 integrate - and - fire neurons are connected all - to - all while each neuron also receives input from one `` input neuron '' ( filled ovals in fig.[figure2 ] ) .",
    "the `` input neurons '' generate rectangular spikes of @xmath17 duration at times determined by externally chosen input sequences .",
    "each of these spikes is sufficient to trigger exactly one spike in the receiving neuron ( see fig.[figure3 ] ) .",
    "the input sequences are chosen such that only one `` input neuron '' spikes at any given time and the time between input spikes was fixed in the normal test setup . in section [ noisesec ] these input neurons",
    "are replaced by poisson neurons with random spike times .",
    "the membrane voltage of the integrate - and - fire neurons used in this study for sub - threshold activity is given by @xmath20 where @xmath21 , @xmath22 and @xmath23 .",
    "whenever the membrane potential @xmath24 reaches @xmath25 , it is set to the firing voltage @xmath26 , kept at that voltage for @xmath27 and then released into the normal integration state .",
    "the neuron is subsequently refractory for @xmath28 before another firing event is allowed . during the refractory period",
    "the neurons integrates normally but the transition of the firing threshold has no effect . in the implementation of integrate - and - fire neurons used in this work",
    "no crossing of the firing threshold from below is necessary to elicit a spike in a super - threshold neuron after the refractory period .",
    "see fig .",
    "[ figure3 ] , middle panel , for typical spike forms .",
    "a neuron connected to all neurons in the network ( large filled circle in fig .",
    "[ figure2 ] ) provides global inhibition whenever the activity in the network exceeds a certain threshold .",
    "the inhibitory neuron is an integrate - and - fire neuron governed by ( [ ifeqn ] ) with @xmath29 , @xmath30 , @xmath31 , @xmath32 , @xmath33 and @xmath34 .",
    "in contrast to the memory neurons this neuron is reset to its resting potential @xmath35 after each firing . then the membrane potential is fixed to @xmath35 for @xmath36 until normal integration resumes .",
    "the inhibitory neuron was implemented as a resetting integrate - and -fire neuron because it has a very weak leak current allowing integration over long time windows .",
    "this weak leak current would cause very unnatural broad spikes in a non - resetting neuron .",
    "a typical voltage trace is shown in the lowest panel of fig .",
    "[ figure3 ] .",
    "our model of the synapses comes from rall  @xcite and now is a standard model for simplified synaptic dynamics  @xcite . in particular , we use @xmath37 where @xmath38 satisfies @xmath39 and @xmath40 , @xmath41 , @xmath42 , @xmath43 and @xmath44 are the pre- and postsynaptic membrane potentials and @xmath45 is the strength of the synapse . @xmath46 and @xmath47 is the usual heaviside function .",
    "typical epsps generated by these synapses can be seen in the middle panel of fig .",
    "[ figure3 ] .",
    "the synaptic strength of the internal synapses is adjusted according to the synaptic plasticity rule shown in in fig .",
    "[ figure1 ] whenever a spike in their presynaptic and postsynaptic neuron occurs . in itself , this rule may lead to `` run - away '' behavior of the synaptic strengths . while this may be avoided in the dynamical model of synaptic plasticity  @xcite",
    ", we need to address this within the simpler model used here .",
    "we do so by two approaches : ( 1 ) we add a long term , slow decay to the synaptic plasticity which would , all other factors being absent , bring it back to a nominal allowed level a long time after alteration by our rule .",
    "this we implement with @xmath48 where @xmath49 is the initial value of the unmodified synapse strength .",
    "so after potentiation or depression according to the synaptic plasticity rule , the synaptic strength is allowed to slowly decay back to its original value .",
    "the time scale of this exponential decay is set by @xmath50 .",
    "( 2 ) @xmath51 is an intermediate variable which is then translated into the synaptic strength @xmath45 via a sigmoid saturation rule @xmath52 where @xmath53 is the largest allowed value for the synaptic conductivity , and @xmath54 sets the threshold where saturation to this value is implemented .",
    "all data shown in this work was obtained with @xmath55 , @xmath56 and @xmath57 .",
    "in addition the globally inhibitory neuron tends to curb the tendency of the network to saturate its synaptic strengths .",
    "these features of our model reflect our lack of knowledge of the biophysical factors setting the synaptic strength in the first place and our equivalent lack of knowledge how these factors bound the eventual rise or fall of synaptic strength .",
    "our assumption in using these rules is that the actual mechanisms , while surely more complicated in detail , will provide the same effective bounding feature .    the complete system is realized in c++ using an order @xmath58/@xmath59 variable time step runge - kutta algorithm  @xcite .",
    "the error goal per time step was @xmath60 in all simulations .",
    "a run of @xmath61 simulated seconds of a system with 50 neurons takes about @xmath62 hours on an athlon @xmath63 ghz processor .",
    "this model system mimics the situation of a highly connected piece of cortex receiving input from the neural periphery .",
    "our input can be interpreted in two ways",
    ". it might be a single strong excitatory postsynaptic potential ( epsp ) received from an upstream neuron which is strong enough to trigger a spike .",
    "it could also be interpreted as the coincidence of several weaker epsps received from various presynaptic neurons being sufficient to cause a spike .      to test the ability of this network to store ( learn ) and retrieve ( remember ) temporal - identity patterns",
    "it was trained with sets of randomly chosen sequences of inputs .",
    "these sequences were chosen without repetition of neurons within the sequence .",
    "note that this implies a minimal time of the order of the length of the sequence between spikes in each neuron .",
    "for this reason the choice of resetting or non - resetting neurons is not important as the integration times of the neurons are small compared to the total length of the sequences and the time scale of the global inhibition .",
    "our choice of non - resetting integrate - and - fire neurons was mainly guided by the more natural spike form of the non - resetting variety .",
    "the sequences were presented continuously with the first neuron of the sequence following the last with the same time delay as the neurons within the sequence .",
    "the global inhibition of the system partitions this continuous input of spikes into pieces of about @xmath64 spikes at a time . between these input windows",
    "the whole system is inhibited and thus reset .",
    "this mechanism can be seen in the example training session shown in fig .",
    "[ figure3 ] .",
    "note that the details of the global inhibition mechanism do not matter as long as the system is efficiently reset after an appropriate amount of activity .",
    "the learning rate @xmath7 and the time scale of forgetting @xmath65 in the synaptic plasticity learning rule were chosen such that learning reaches a steady state after a learning time of about @xmath66 , where @xmath67 is the fixed inter spike interval between input activations . for an example of the learning protocol see fig.[figure3 ] .",
    "in all studies described below @xmath67 was chosen as @xmath68 , @xmath69 or @xmath70 .",
    "the learning rule has to accommodate all these input speeds and possibly values in between .",
    "in particular we here chose @xmath71 , @xmath72 , @xmath73 , @xmath74 and @xmath75 .",
    "after the training phase the network was presented with pieces of the training patterns .",
    "we presented all possible ordered pieces of one to four input spikes and recorded the number and identity of spiking neurons in the network in response to this input .",
    "perfect learning of the patterns would correspond to obtaining a spike from each of the network neurons in a given pattern when presenting a piece of two or three inputs from that pattern to the `` input neurons '' .",
    "furthermore , all other network neurons should remain inactive if the pattern is reproduced exactly .    as a result of incomplete or ineffective learning two types of errors can occur .",
    "( 1 ) neurons which should be excited within the given pattern do not spike or ( 2 ) neurons which are not supposed to spike do so .",
    "due to overlap of input patterns , the learning efficiency is a function of the number of learned patterns as well as the size of the network .",
    "therefore , estimating the expected amount of overlaps in the randomly chosen input sequences provides information about the optimally achievable system performance .    the probability distribution for the number @xmath76 of _ ordered _ @xmath77-tuples occurring in at least @xmath78 out of @xmath79 patterns with @xmath80 neurons each for a system with a total number of @xmath19 neurons can be calculated in the following way : first consider a given ordered @xmath77-tuple and a given pattern with @xmath80 neurons .",
    "the sequence is presented continuously and therefore needs to be interpreted as cyclically closed .",
    "thus there are @xmath80 possibilities to position the @xmath77 tuple in the sequence ( starting at neuron @xmath81 to starting at neuron @xmath80 ) and @xmath82 possibilities to choose the remaining neurons in the sequence .",
    "the total number of sequences of length @xmath80 is @xmath83 .",
    "therefore , the probability @xmath84 to have a given _ ordered _",
    "@xmath77-tuple in a given pattern with @xmath80 active neurons is given by @xmath85 if @xmath79 sequences of length @xmath80 are chosen independently , the probability to have any given _ ordered _",
    "@xmath77-tuple of neurons in @xmath78 or more of the @xmath79 sequences is given by the binomial distribution with parameters @xmath79 and @xmath84 , @xmath86 in good approximation one can treat the events of one given @xmath77-tuple being in @xmath78 or more sequences and another @xmath77-tuple being so as independent . in this approximation",
    "the probability distribution for @xmath76 is again a binomial distribution with parameters @xmath87 and @xmath88 , @xmath89     comparison of the expectation values for @xmath90 ( lower line ) and @xmath91 ( upper line ) obtained from ( [ ydistri ] ) and ( [ xdistri ] ) to the normalized number of occurrences of unordered @xmath62-tuples ( gray dots ) and ordered @xmath92-tuples ( black dots ) in more than @xmath92 sequences in @xmath93 randomly generated sets of @xmath94 sequences of length @xmath95 .",
    "the inlay shows a closeup of the data on ordered tuples in the region with system size around @xmath96 neurons which is the size used in most numerical simulations . ]",
    "[ figure4 ] shows a comparison of the expectation value for @xmath97 obtained from this approximate distribution compared to the relative number of occurrences in @xmath93 randomly generated sets of @xmath94 sequences of length @xmath95 .",
    "there is no significant discrepancy which demonstrates the precision of the estimate .    the probability distribution for the number @xmath98 of _ unordered _ @xmath77-tuples occurring in at least @xmath78 out of @xmath79 patterns with @xmath80 neurons each for a system with a total number of @xmath19 neurons can be calculated pretty much in the same way .",
    "now the the probability @xmath99 to have a given _ unordered _",
    "@xmath77-tuple in a given pattern with @xmath80 active neurons is @xmath100 then , the probability @xmath101 to have any given _",
    "@xmath77-tuple of neurons in @xmath78 or more of @xmath79 independently chosen patterns is the binomial distribution with parameters @xmath79 and @xmath99 , @xmath102 again taking the approximation of assuming independence for the occurrence of distinct tuples this leads once more to a binomial distribution , now with parameters @xmath103 and @xmath101 , @xmath104 the comparison of the expectation values @xmath105 with numerically observed relative numbers of occurrence in fig .",
    "[ figure4 ] shows again a perfect match .",
    "the model parameters were chosen such that two to three spiking predecessors of a given neuron in a trained sequence are sufficient to excite that neuron .",
    "the learning performance is therefore poor as long as there is a significant amount of ordered @xmath92-tuple overlaps in the patterns .",
    "the rule of thumb @xmath106 for the expectation value of @xmath76 , provides an estimate for number @xmath79 of pattern of length @xmath80 that can be successfully stored in a system of @xmath19 neurons .",
    "another estimate for the number of learnable sequences is provided by the rule of thumb @xmath107 , i.e.  the overlaps in input sequences should have negligible impact on the learning if there is no significant amount of _ unordered",
    "_ @xmath62-tuples occurring in more than one pattern .",
    "typically capacity estimates are given in the limit of the system size @xmath19 tending to infinity .",
    "as shown in appendix [ appe ] the leading term of the taylor expansion of @xmath88 with respect to @xmath84 around @xmath108 is @xmath109 such that the asymptotic equation @xmath110 leads to @xmath111 such that the capacity @xmath112 is asymptotically @xmath113 in the same way @xmath114 leads to @xmath115     estimate for the maximum storage capacity of the system .",
    "the dashed lines divide the plane into two regions with @xmath116 above and @xmath117 below for @xmath118 ( topmost line ) , @xmath94 ( middle line ) , and @xmath119 ( lowest line ) respectively .",
    "the thin solid lines are the corresponding estimates for the asymptotically correct values @xmath120 .",
    "the dash - dotted lines analogously mark the boundaries between regions with @xmath121 above and @xmath122 below .",
    "again the thin lines are the asymptotically correct estimates @xmath123    the dashed lines in fig .",
    "[ figure5 ] are some examples for the first rule of thumb @xmath124 and the thin solid lines are the corresponding values of @xmath125 .",
    "the estimates based on the rule @xmath126 are shown as dash - dotted lines in fig.reffigure5 and the corresponding values of the asymptotically correct @xmath127 are again shown as thin solid lines .",
    "the correspondence between the exact evaluation of the capacity estimators and the asymptotically correct capacity functions @xmath112 and @xmath128 is noteworthy .",
    "the relative capacities @xmath129 and @xmath130 behave quite differently . whereas the former is constant with respect to @xmath80 the latter is falling in @xmath80 .",
    "so , depending whether a system is more sensitive to ordered tuple overlaps or to unordered tuple overlaps , the relative capacity is constant or falling in @xmath80 . in particular for systems sensitive to unordered tuple overlaps it will be beneficial to store many short sequences instead of a few long ones .",
    "simple example of a learned identity - temporal pattern .",
    "the neurons at the corners of the octagon have been repeatedly excited in clockwise order .",
    "the width and grayscale of the connections encodes the strength of the corresponding synapse and the small circle at the end shows its direction . as one clearly can see the temporal pattern is transformed into an ordered spatial pattern by synaptic plasticity . ]",
    "the synaptic plasticity of synapses transforms time sequences of excitation of neurons into directed spatial patterns as intended .",
    "a simple example is shown in fig .",
    "[ figure6 ] for one input pattern . for randomly chosen input sequences",
    "the patterns are structured in the same way but are not so easy to detect with the human eye .",
    "development of synaptic strength during training .",
    "the network of @xmath96 neurons was trained with @xmath58 sequences of length @xmath95 in sequential order .",
    "the topmost panel shows the data for sequences entrained with inter spike interval @xmath131 , the middle with @xmath132 , and the lowest with @xmath133 .",
    "each sequence was presented for @xmath134 at a time .",
    "the data shown are average synaptic strengths of synapses between the neurons of one of the trained sequences .",
    "the topmost points are the average strengths of all synapses between the neurons and their direct successors in the sequence , the middle are the corresponding strengths of synapses between neurons who are next nearest neighbors in the sequence under consideration , and the lower points correspond to strengths of synapses between neurons with distance @xmath62 in the sequence .",
    "the lowest data points are the strengths between the neurons of the sequence as described above but _ against _ the order of activation in the trained sequence .",
    "the sharp rises in synaptic strength correspond to training of the particular sequence shown here and the falling flanks correspond to the decay while other patterns are trained . ]    during training the synapses between consecutively active neurons are strengthened if pointing in the direction of the activation order or weakened if connecting the neurons in the wrong direction .",
    "an example of the development of the average synaptic strength of synapses between neurons of one out of @xmath58 trained sequences is shown in fig .",
    "[ figure7 ] .",
    "note that the time course and final strength of the synapses depends on the speed with which the sequences are entrained due to the non - constant learning curve ( [ learneqn ] ) .",
    "the ability to store more than one pattern was tested in various setups .",
    "we mainly varied choice , number and length of input sequences and the speed of entrainment .",
    "typical recall episodes .",
    "the system of @xmath96 neurons was trained with @xmath92 ( left panel ) or @xmath58 ( right panel ) sequences for @xmath135 per sequence , where @xmath68 .",
    "it then receives a cue of two spikes from one of the trained sequences and autonomously completes the sequence until stopped by the globally inhibitory neuron .",
    "note that although the recall of the identity and order of the neurons is perfect in both cases , the exact timing is lost .",
    "in general one sees a tendency of speed - up to the end of the recalled sequence .",
    "this can have the effect of destroying the correct order of recall in the later sequence if the global inhibition is not present . ]",
    "examples of learning in a @xmath96 neuron network after @xmath135 sequential training with @xmath58 input sequences of length @xmath95 .",
    "the left and the right panel show results for two independently chosen sets of @xmath58 input sequences labeled with numbers 0 to 4 in each set .",
    "the filled symbols show the average number of spiking neurons within a tested sequence and the open symbols erroneously spiking neurons .",
    "the test cue were fractions of length @xmath92 from the trained sequences .",
    "the circles were obtained with a training speed of @xmath68 , the squares with @xmath132 , and the triangles with @xmath136 .",
    "note that the results depend on the structure of the input set .",
    "whereas in the left case all sequences have some overlap , in the right case sequence @xmath137 and sequence @xmath62 are pretty much disjoint from the others . ]    a typical example for a network of @xmath96 neurons trained with @xmath58 sequences of length @xmath95 is shown in fig .  [ figure8 ] and fig .  [ figure9 ] .",
    "there are several important features to point out .",
    "first of all the recall never comprises all @xmath95 neurons of the trained sequence but only up to @xmath138 active neurons .",
    "this is however not a universal feature but rather a characteristic of the global inhibition circuit shutting down the system after ca .",
    "@xmath138 spike occurrences , see fig .",
    "[ figure8 ] .",
    "furthermore , note that the recall of the sequences speeds up toward the end of the sequence .",
    "this is partly due to the fact that the integrate - and - fire neurons used here do not have a finite rise time for their spikes which allows them to instantaneously affect their postsynaptic neurons .    in a network with more realistic neurons",
    "one would expect that there is a lower limit on the speed with which sequences can be recalled in the system .",
    "preliminary studies with realistic hodgkin - huxley type neurons show this effect @xcite .",
    "it has clear advantages for maintaining the correct order of recall in the system .",
    "the microscopic internal dynamics of the neurons thus seems to be non - negligible for the macroscopic performance of the system .",
    "this will be discussed in more detail in forthcoming work .",
    "the quality of recall of sequences depends very much on the sequence and the piece presented as a cue .",
    "this is however also no surprise because sequence overlaps occur at certain neurons in the sequence and if these are used as a cue the performance is less good as when other neurons are used . in fig.[figure9 ] one can see how some sequences are reproduced very well and error free while others lead to activation of quite a few incorrect neurons .    to test for the capacity of the system",
    "systematically we trained a network of @xmath96 neurons with @xmath92 up to @xmath94 sequences of length @xmath95 . for each number of sequences @xmath58 independent sets of randomly chosen sequences",
    "were tested .",
    "[ figure10 ] shows the average response of the trained systems to pieces of @xmath92 inputs taken from the learned sequences .",
    "the averages are over all possible subsequences and all @xmath58 input sequence sets for each data point .",
    "this experiment was done with @xmath62 different input speeds , i.e.  the input was presented with fixed inter spike intervals of length @xmath68 , @xmath69 and @xmath70 . as one can see in fig .",
    "[ figure10 ] the performance dramatically decreases for the slowest entrainment speed .",
    "this is due to the fact that the fixed width of the learning window in ( [ learneqn ] ) leads to weaker synapses for all the synapses in this case as spikes are separated further in time , see last row of fig .",
    "[ figure7 ] .",
    "the minimum and maximum possible speed of the entrainment are thus directly determined by the learning window .",
    "if one chooses a larger learning window the slower sequences could be entrained as well .",
    "however , this would also lead to decreased performance for faster sequences .",
    "scaling of storage quality with the number of input sequences .",
    "a system with @xmath96 neurons was trained with a varying number of input sequences of length @xmath95 .",
    "the figure shows the response after a total of @xmath135 training for each input sequence .",
    "the filled symbols show the average number of responding neurons within a tested sequence and the open symbols the number of incorrectly responding neurons .",
    "the test cues were pieces of @xmath92 inputs from the trained sequences .",
    "the circles were obtained with sequences trained with inter spike intervals @xmath139 , the squares with @xmath132 , and the triangles with @xmath133 .",
    "all data points are averages of trials with @xmath58 independently chosen sets of input sequences . ]",
    "scaling of storage quality with the length of input sequences .",
    "a system with @xmath96 neurons was trained with sets of @xmath58 input sequences of different lengths .",
    "the figure shows the response after a total of @xmath140 training for each input sequence .",
    "the filled symbols show the average number of responding neurons within a tested sequence and the open symbols the number of incorrectly responding neurons .",
    "the test cues were pieces of @xmath92 inputs from the trained sequences .",
    "the circles were obtained with sequences of length @xmath59 , the squares with length @xmath138 , the triangles with length @xmath95 , and the diamonds with length @xmath141 .",
    "all data points are averages of trials with @xmath58 independently chosen sets of input sequences . ]    to test for the dependence of learning success on the length of presented sequences we entrained a @xmath96 neuron system with sets of @xmath58 sequences of length @xmath59 to @xmath141 . fig .",
    "[ figure11 ] shows the performance of the system . on first sight",
    "it is surprising that the system performs less good for shorter sequences .",
    "naively one would expect a better performance because overlaps are less likely .",
    "indeed one really can see that the number of erroneous spikes is smaller . on the other hand",
    "the number of correct spikes is also considerably smaller .",
    "this is due to the fact that the spikes preceding a given spike in a sequence are also succeeding it because of the periodic presentation of the sequences ( see e.g.  fig.[figure3 ] ) .",
    "synapses between the corresponding neurons are therefore enhanced as well as depressed . for shorter sequences",
    "the last presentation of the sequence is closer and the depression effect therefore stronger leading to lesser overall synapse strength , cf fig .",
    "[ figure12 ] .",
    "this creates the fewer retrieved spikes for shorter sequences in fig .",
    "[ figure11 ] . to some extent",
    "this can be seen as an artifact because longer learning time or slightly larger learning increments @xmath7 could diminish this effect . on the other hand",
    "this might have negative effects on the performance of the system in other parameter regions .",
    "development of synaptic strength during training of a sequence of length @xmath59 with speed @xmath68 .",
    "the network of @xmath96 neurons was trained with @xmath58 sequences of length @xmath59 in sequential order .",
    "each sequence was presented for @xmath134 at a time .",
    "the data shown are average synaptic strengths of synapses between the neurons of one of the trained sequences .",
    "the topmost points are the average strengths of all synapses between the neurons and their direct successors in the sequence , the middle are the corresponding strengths of synapses between neurons who are next nearest neighbors in the sequence under consideration , and the lower points correspond to strengths of synapses between neurons with distance @xmath62 in the sequence .",
    "note how the synaptic strength for these synapses is suppressed because a spike being the third predecessor of a given spike is also the third successor of this spike due to cyclic training .",
    "the lowest data points are the strengths between the neurons of the sequence as described above but _ against _ the order of activation in the trained sequence . ]",
    "biological neural systems are subject to various external and internal noise sources . starting from internal thermal noise within the system",
    "this ranges over noisy or unreliable input and influences from other parts of the organism up to external electromagnetic fields . to test the effect of noise on the learning success of our model systems we focused on two types of noise .",
    "we implemented a gaussian white noise in the membrane potential of the integrate - and - fire neurons and we implemented unreliable input .",
    "the internal white noise was added to the membrane potential of each neuron independently .",
    "it is fully characterized by its mean , @xmath142 and its variance for which several values between @xmath143 and @xmath144 were tested .    to simulate unreliable input we implemented poisson input neurons .",
    "these neurons produce rectangular spikes of width @xmath145 as before but the time of spiking is stochastic .",
    "the spike times are determined by the poisson distribution @xmath146 where @xmath147 is the number of spikes occurring in an interval of length @xmath148 and the parameter @xmath149 is the mean firing rate . for small @xmath148",
    "this can be approximated by @xmath150 , @xmath151 and @xmath152 for @xmath153 .",
    "this is the probability distribution we use to decide whether a neuron fires within a time step of the runge kutta algorithm used . after firing",
    "the neurons are refractory for @xmath36 .",
    "the training protocol is that the mean firing rate of the first neurons is switched from @xmath137 to some activity level @xmath154 for @xmath155 , the next neuron is switched on after @xmath67 for also @xmath156 and so on .",
    "different reliability of the input can be adjusted by the parameter @xmath154 .",
    "impact of gaussian white noise in the membrane potential .",
    "the data points are the number of spiking neurons within tested sequences after @xmath157 training at @xmath68 ( full symbols ) and the number of erroneously spiking neurons ( open symbols ) .",
    "the small symbols were obtained when the noise was only present during learning and the large ones when noise was always present .",
    "the circles correspond to a cue of two inputs in testing and the squares to a cue of three inputs . ]",
    "impact of noisy input on the learning performance .",
    "the input sequences were provided by stochastic poisson neurons as described in the text .",
    "the data points are the number of spiking neurons within tested sequences after @xmath157 training at @xmath68 ( full symbols ) and the number of erroneously spiking neurons ( open symbols ) .",
    "the small symbols were obtained when the stochasticity of the input was only present during learning and the large ones when input was always stochastic .",
    "the circles correspond to a cue of two inputs in testing and the squares to a cue of three inputs . ]",
    "[ figure13 ] and [ figure14 ] show the impact of the two types of noise on the learning performance .",
    "[ figure13 ] shows the effect of additive white noise at the membrane potential in the learning stage and in both learning and recalling .",
    "as mentioned , the standard deviation of the noise was chosen between @xmath158 and @xmath159 .",
    "the system seems to be more or less unaffected by noise of this magnitude . as to be expected the learning is even less sensitive to noise than the recalling due to the fact that the effect of the temporally uncorrelated noise on the synaptic strength is averaged out over time .    fig .",
    "[ figure14 ] shows the learning success if the input neurons fire stochastically during learning only and during learning and recall as described above .",
    "the parameter @xmath154 was varied from @xmath160 to @xmath161 .",
    "the stochastic firing of the input neurons seem to only affect the overall number of spikes , i.e.  correct spikes as well as incorrect ones but not their ratio .",
    "this indicates that mainly missing input spikes during the training and especially during testing are responsible for the decreased spikes in the response .",
    "it is to be expected that longer training can diminish these effects even more . like in the case of noise in the membrane potential",
    "the learning stage is not affected as much by the noisy input as the recall .",
    "again the same argument applies .",
    "the effects of the stochasticity of the input spikes is averaged out over time during the multiple repetitions in the training phase .",
    "it has been demonstrated that stdp allows the transformation of temporal information into spatial information providing an efficient mechanism for storing temporal sequences which does not require a sophisticated network topology .",
    "it is however not obvious how to _ quantify _ the storage capacity of the system from the observed recall performance for different numbers of stored sequences . taking the heuristic rule to allow for successful storage on average one incorrect spike in recall",
    ", the capacity of a system of @xmath96 neurons is about @xmath162 sequences ( see fig .",
    "[ figure10 ] .",
    "the capacity estimates for @xmath163 and @xmath164 are @xmath165 and @xmath166 .",
    "the storage capacity of the system therefore seems to be mainly limited by the statistical properties of the input , i.e.  the overlap probabilities for randomly chosen input sequences .",
    "the biologically found stdp learning rule obviously does not imply severe restrictions on the ability to learn sequences but on the contrary seems to be very well suited for this task .",
    "there are indications that the learning mechanism is even more reliable with biologically more realistic conductance based model neurons which have non - trivial intrinsic dynamics which to some extent prevents the speedup in recall already discussed above .    the successful storage of arbitrary input sequences , however , crucially depends on the existence of the corresponding synapses making the all - to - all connections in the investigated system a necessary requirement . in realistic systems",
    "such global all - to - all connections can not be found , but this might be compensated through divergence and redundancy of the input . if the density of connections and the number of neurons each input excites is high enough , pairs of connected neurons being excited by successive inputs will appear on a statistical basis .",
    "this mechanism will be discussed more thoroughly in forthcoming work .",
    "the realistic implementation of saturation of synaptic strength for additive learning rules is another important topic . for the system investigated here we implemented a combination of two mechanisms . on the one hand",
    "the synaptic strength was directly bounded by use of the sigmoid filtering function applied to the bare synaptic strength subject to the additive learning rule , a technique commonly used by biologists . on the other hand the steady decay of synaptic strength and the continuous stimulation of the network by the inputs",
    "lead to a dynamical steady state thereby bounding the synaptic strength dynamically .",
    "whereas the direct bound through a sigmoid filtering function might capture some aspects of the behavior of real synapses , the decay of synaptic strength necessary to achieve a realistic dynamical steady state is clearly too fast to be realistic .",
    "the system forgets much too fast if it is not continuously stimulated with appropriate input .",
    "alternative solutions to the saturation problem include competition based mechanisms suggested by recent findings of interactions of various kinds between neighboring synapses on a dendritic tree  @xcite and learning rules which depend on the synaptic strength itself like e.g.multiplicative learning rules .",
    "the system is reasonably robust against noise .",
    "it is noteworthy that it is not very sensitive to internal high - frequency noise . in the range of noise applied in our trials the recall barely depended on the level of noise ( see fig .",
    "[ figure13 ] . whether this is an effect of the integrate - and - fire neuron model used here is beyond the scope of this work .",
    "the tolerance to biologically more relevant noise in the spike timing of the input is also rather impressive taking into account that @xmath167 corresponds to a total firing probability of only @xmath168 for each of the input neurons within their activity window of @xmath169 .",
    "nevertheless the system still was able to store at least parts of the presented sequences at this high noise level .",
    "we thank walter senn for numerous helpful remarks and suggestions . this work was partially supported by the u.s .",
    "department of energy , office of basic energy sciences , division of engineering and geosciences , under grants no .",
    "de - fg03 - 90er14138 and no .",
    "de - fg03 - 96er14592 , by grants from the national science foundation , nsf phy0097134 and nsf eia0130708 , by a grant from the army research office , daad19 - 01 - 1 - 0026 , by a grant from the office of naval research , n00014 - 00 - 1 - 0181 , and by a grant from the national institutes of health , nih r01 ns40110 - 01a2 .",
    "we first need to proof the identity                for all @xmath179 the @xmath19-th derivative contains a non - zero power of @xmath84 and is thus @xmath180 at @xmath108 .",
    "furthermore , if @xmath181 then all @xmath80 are less then @xmath182 and therefore the whole sum over @xmath80 is empty .",
    "we end up with @xmath183 for any @xmath184 the inner sum is @xmath185 therefore , the leading term of the taylor expansion of @xmath88 is @xmath186"
  ],
  "abstract_text": [
    "<S> we suggest a mechanism based on spike time dependent plasticity ( stdp ) of synapses to store , retrieve and predict temporal sequences . </S>",
    "<S> the mechanism is demonstrated in a model system of simplified integrate - and - fire type neurons densely connected by stdp synapses . </S>",
    "<S> all synapses are modified according to the so - called normal stdp rule observed in various real biological synapses . after conditioning through repeated input of a limited number of of temporal sequences </S>",
    "<S> the system is able to complete the temporal sequence upon receiving the input of a fraction of them . </S>",
    "<S> this is an example of effective unsupervised learning in an biologically realistic system . </S>",
    "<S> we investigate the dependence of learning success on entrainment time , system size and presence of noise . </S>",
    "<S> possible applications include learning of motor sequences , recognition and prediction of temporal sensory information in the visual as well as the auditory system and late processing in the olfactory system of insects . </S>"
  ]
}