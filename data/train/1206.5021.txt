{
  "article_text": [
    "increasingly large astronomical data warehouses are being built to support the needs of scientific collaborations . the international virtual observatory alliance ( ivoa )",
    "laid down the standards of transport data models and communication protocols to help the federation of geographically distributed data sets .",
    "the astronomer community is now working hard on the implementation of systems that will bring the power of petabyte - scale data warehouses and the information of hundreds of large archives to the desktops of scientists .",
    "the key point in the federation of astronomical data sets is the cross - identification ( cross - matching ) of detections belonging to the same physical object .",
    "the detections are usually made by using different imaging filters or entirely different instruments . due to the exponential growth in the data volume",
    ", our solution has to be scalable .",
    "also , since the largest data sets will be geographically distributed and data co - location might not be an option in the future , any solution will have to be optimized for the networking .    in this paper",
    ", we present a scalable solution for on - demand cross - matching of large catalogs hosted on a cluster of database servers . in the sec .",
    "[ sec : obs ] , we explain the characteristic properties of astronomy catalogs . the cross - identification problem is introduced in sec .",
    "[ sec : xmatchintro ] .",
    "[ sec : sqlext ] describes our sql extensions to explicitly formulate the problem of coordinate based matching in a query request .",
    "the details of our hardware and software setup are highlighted in sec .",
    "[ sec : hwsw ] . in sec .",
    "[ sec : implementation ] we focus on the most important aspects of the implementation . sec .",
    "[ sec : summary ] concludes the paper , and outlines specific future work .",
    "today s high - performance astronomical _ imaging instruments _ ( ground - based or space - borne telescopes with their custom - built cameras ) are mostly operated in _ survey mode _ ,",
    "i.e. significantly large regions of the sky are mapped systematically .",
    "every telescope is designed to work in a certain range of the electromagnetic spectrum , thus the division of fields of astronomy according to these regions is evident : radio , infra red ( ir ) , optical ( near - ir , visible , near - uv ) , ultra violet ( uv ) , x - ray and gamma . from ir to uv , different sets of _ imaging filters _ are used to further subdivide a given range of the electromagnetic spectrum allowing for `` color photography '' of the sky .",
    "the goal of imaging sky surveys is to take snapshots of the sky to be able to identify all celestial objects in each imaging filter to a given faintness limit .    with the advance of detector technology and growth of the mirror area of telescopes ,",
    "an exponentially growing area of the sky can be surveyed in a given amount of time .",
    "this not only helps map a larger portion of the universe , but also to take measurements in the time domain .",
    "multi - wavelength astronomy combines information from different instruments to investigate the physical properties and to constrain theoretical models of celestial objects .",
    "time domain astronomy is the emerging field of the 21@xmath0 century . while objects of variable brightness have been systematically observed before , the new surveys , like panstarrs and lsst , will provide and unprecedented opportunity to find the faint , fast - moving celestial bodies of the solar system , the serendipitous variable stars of the milky way and nearby galaxies .",
    "they will help constrain models of quasars and to detect distant supernovae to test our understanding of the dynamics of the entire universe .",
    "images taken during astronomical surveys are _ reduced _ by the so called _ photometric _ or _ imaging _ pipe - line software . during the reduction process",
    ", individual objects are identified in the images and their readily measurable properties are determined , such as integrated brightness , brightness profiles , morphological parameters etc .",
    "the number of measured properties is typically in the hundreds .    because of the distortions introduced by the optical systems of the telescopes and ( in case of ground - based telescopes ) the turbulent motions in the atmosphere ,",
    "sophisticated _ astrometric _ algorithms using non - linear models are needed to precisely determine the coordinates of the detected sources .",
    "the quality of astrometry is limited by the goodness of the model used to calibrate positions to well knows standards , and the atmospheric conditions ( the so called _ seeing _ ) . _",
    "astrometric errors _ are usually given in arc seconds ( as ) .",
    "the typical error of optical surveys is in the @xmath1-@xmath2  as range . on the other hand ,",
    "as the direction of gamma photons is much harder to determine , the astrometric error of high - energy transient events , like gamma - ray bursts can be as high as tens of degrees .",
    "photometric and morphological properties and the astrometric parameters of the identified objects are organized into _",
    "catalogs_. catalogs are traditionally created as plain files listing all objects found in an imaging frame . since the millennium",
    ", relational database management systems have been widely used to organize , process and mine the information gathered by sky surveys .",
    "the numbers of objects detected by the surveys are all on scales , currently topping in the hundred million range .",
    "ongoing and future surveys will provide information about billions of objects , about a hundred detections of each at different times .",
    "the typical data volume of current reduced catalogs tops in the @xmath3  tb range , quickly moving toward hundreds of terabytes , reaching the petabyte range by the end of the decade .",
    "the amount of raw imaging data collected and processed during the surveys can be about ten to a hundred times more .    when loaded into relational databases , in the simplest case ,",
    "object detection catalogs occupy one large table , while additional , much smaller tables are used to store metadata .",
    "a typical catalog table has an integer primary key , usually composed of different identifiers of the observation .",
    "time and location on the celestial sphere are usually converted to a standardized coordinate system , so conversion between systems is not necessary at runtime . the directions of the detections are often stored as the cartesian coordinates of the unit vectors .",
    "the rest of the table columns consist of various identifiers , classification parameters ( integers ) and measured observational parameters ( floating point ) .",
    "the tables often have several hundred columns .",
    "the frequency of usage of the columns in user queries , however , varies significantly .      since the spread of relational databases in astronomy",
    ", the sql language has become an every day tool of researchers .",
    "its industry - wide support and its ability to describe complex problems with simple syntax in a declarative manner makes it a good choice of programmatic interface to astronomical data warehouses .",
    "although several of the typical data filtering tasks could be done with web forms or other types of custom user interfaces , sql gives the ability to _ script _ the operations .",
    "this is absolutely necessary for astronomers dealing with data processing issues , as astronomical data usually have to be reprocessed many times during a research project .    to support analysis of astronomical data ,",
    "extensive libraries of scientific functions have been developed , which can be accessed directly from sql via user - defined functions .",
    "functions include cosmological distance calculations @xcite and various spherical indexing schemes for fast coordinate and region - based searches @xcite .",
    "together with user - defined functions , data filtering capabilities and aggregate functions , the sql language turned out to be a very powerful tool for statistical analysis of astronomical data .",
    "being able to solve all problems using sql makes it possible to process data without pulling it out from the database server .",
    "the region of the sky covered by a survey is called _",
    "footprint_. catalog footprints can be exceptionally complex due to observation strategies and the strange geometries of the instruments .",
    "_ masks _ , also geometric shapes , similar to footprints in complexity , are usually generated from the images to exclude regions of observations of low quality .",
    "using bitmaps to represent footprints and masks is not favorable because of the limited resolution , the polar singularities and the complicated tessellation schemes of the sphere . on the other hand , bitmaps make it extremely easy to determine the boolean combinations of footprints and masks , once the same pixelization is used .",
    "budavri et al . developed a software library to describe regions on the surface of the sphere analytically @xcite .",
    "arbitrarily complex regions are constructed as unions of spherical convexes .",
    "convexes themselves are defined as intersections of circles drawn on the surface of the sphere .",
    "the resolution that can be reached using double precision arithmetics is about .",
    "the library supports exact boolean operations between regions .",
    "clearly , using an analytic approach can be computationally more expensive than using bitmaps , but the much better resolution and the higher flexibility favor the analytical description .",
    "we will use the analytic library in our cross - match solution presented in this paper .",
    "when we have multiple detections of the same celestial source , the measured coordinates will slightly differ . in order to cross -",
    "match the detections of two catalogs , we have to measure the distance between all detection pairs , and only accept those pairs as matches that are closer than a given threshold . in practice ,",
    "cross - matching is done by excluding obvious false matches first .",
    "different indexing schemes of the sphere have been invented for relational databases to find matching candidates efficiently @xcite .",
    "we will briefly explain the so called _ zone algorithm _ in sec .",
    "[ sec : zone ] , which is the fastest available algorithm for microsoft sql server so far @xcite .",
    "there are three different ways a catalog can be cross - matched with other catalogs .",
    "one can require that certain catalogs _ must _ contain good candidate detections of an object in order to accept a match .",
    "additionally , some other catalogs _ may _ contain detections and should be taken into account , if possible .",
    "these are typically catalogs with brighter detection limits . in the third case ,",
    "one requires that the catalog _ must not _ contain any candidate detections that would match with detections of other catalogs .",
    "this third case is called _ drop - out detection_. while the first case is similar to inner joins , and the second case is to outer joins , there is no simple equivalent of the third case in standard sql .",
    "drop - out detection is particularly important in multi - wavelength astronomy because even if a certain object is not detected by an instrument due to its too high detection limits , an upper limit to the brightness of the object can still be given based on the known the sensitivity of the instrument .",
    "looking for missing objects can be used to find , for example , a certain kind of galaxy that is bright in the infra red but too faint in ultra violet to show up in the uv images . to safely detect drop - outs ,",
    "it is fundamentally important to know whether an object is not in the catalog because its celestial location was not observed by the survey at all , it was missed by the instrument because it was too faint or it was intentionally masked out and excluded from the catalog due to other reasons . to account for this problem ,",
    "an exact description of the observed areas , the so called _ footprints _ , and _ masks _ is necessary .",
    "the first automatic cross - identification on - line service was implemented by budavri et al . as a set of xml soap web services @xcite . as it was a prototype built to demonstrate the then new web service technology , not much attention to the performance and scaling properties was paid . based on the idea , an open , soap - web - service - based standard ,",
    "open skyquery was developed by the national virtual observatory to federate geographically distributed data sets @xcite .",
    "both of these versions used the sql language with some custom functions as the main programming interface . because open skyquery could not benefit from co - located data sets , and due to performance issues the system was limited to process only 5000 matches in a run .",
    "the virtual observatory alliance standardized the astronomical data query language ( adql ) intended to be used as the _ lingua franca _ of astronomical catalogs @xcite .",
    "though the adql language defines many new , astronomy - induced constructs compared to sql , including spherical region expressions that can be used to circumscribe cross - matching problems , adql was not designed with query optimizability in mind .",
    "the new cds xmatch service implements a high performance cross - match engine that partially uses database technology , and can perform two - way joins only @xcite .",
    "the current version of the service features a form - based user interface only and no scripting support .",
    "the proper statistical formulation of the problem is based on bayesian probability theory .",
    "every detection has a measured direction unit vector @xmath4 accompanied by its uncertainty , which is determined in the calibration process",
    ". often the catalogs would assume gaussian errors and quote a single @xmath5 value , others estimate the precision per detection , @xmath6 .",
    "these two together form a likelihood function of the unknown model position @xmath7 on the sky . in the general spherical case , where @xmath8 is the fisher distribution @xmath9 this is the simplest analog of the gaussian on the surface of the unit sphere @xcite , where @xmath10 is its precision parameter , which is @xmath11 in the limit of high accuracies .",
    "given a set of detections with their positions on the celestial sphere and the corresponding uncertainties , we compare two competing complementary hypotheses : are the detections from the same source or not ? in one case the model assumes a single object at an unknown position @xmath7 and the likelihood of the hypothesis is a result of an integral over all possible @xmath7 : @xmath12 the function @xmath13 is the prior , which can be assumed to be isotropic in most cases .",
    "the complementary hypothesis allows any of the detections to come from a separate source , hence its parametrization consists of a set of directions @xmath14 , one for each observation .",
    "the integral over the entire parameter space falls apart into the product : @xmath15 the bayes factor is the ratio of these two likelihoods , @xmath16=@xmath17 .",
    "when this ratio is around unity the data is indecisive but when it is much larger than 1 , the data favors the match .",
    "alternatively , when @xmath16 is close to 0 , the evidence points toward separate sources .",
    "for the fisher distribution , the calculation can be done analytically and yields @xmath18 where @xmath19 . in the limit of high accuracies",
    ", the result takes a familiar gaussian form .",
    "we note that based on the bayes factor and the density of detections in the given catalogs , it is also possible to define a posterior probability for each match . for more details and the full mathematical discussion ,",
    "we refer the interested reader to @xcite . in our cross - match engine",
    ", we use the above bayes factor as a discriminator between positive and negative matches .",
    "because of the reasons detailed in sec .  [ sec : sqlreasons ] , we decided to base our skyquery language on sql and extend the language to support easy expression of cross - identification problems and spatial filtering .",
    "building on the basis of sql not only makes it easy to learn the extended syntax , but also allows for backward compatibility with traditional sql queries .    when extending the original sql syntax , we wanted to avoid any interference with the existing behavior of sql clauses .",
    "this is why we introduced the new ` xmatch ` clause ( see sec .  [",
    "sec : xmatchsql ] ) instead of incorporating its functionality into the standard ` from ` clause and ` join ` operators .",
    "there are some ideas that are worth considering when creating extensions to a declarative query language .",
    "our language extensions were designed such a way that all queries that can be described using the language will be executable and can be optimized efficiently .",
    "this is in strong contrast , for example , with the way most gis systems implement spatial constraints ( complex boolean expression in the ` where ` clause ) where efficient optimization is an issue ; or in contrast with the rather flexible adql language where even query executability is a problem . while flexible syntax might broaden the range of applications a language can be used for , clever syntax restrictions to certain expressions",
    "can ensure that all queries can be optimized easily and executed efficiently , without the cost of losing flexibility .",
    "one such syntax restriction is to move certain filtering criteria ( for example spatial constraints ) from the ` where ` clause to a new clause that does not allow for complex logical expressions .",
    "simple implementation is always a main objective , especially in case of scientific projects with limited budgets .",
    "cross - identification queries are written to join two are more tables containing detections of celestial objects .",
    "the join among these tables is made based on the coordinates of the detections in a probabilistic manner .",
    "tables taking part in the cross - match join must contain columns for the measured coordinates of the detections as the two spherical coordinates or as the three components of cartesian unit vectors ; conversion between the two representations is done automatically .",
    "these tables must contain a primary key ( composites keys are supported ) and may contain the astrometric error and any number of other columns .",
    "also , when a catalog table contains multiple detections for each object , a two way _ self - join _ can be used to find detections of the same object .",
    "our solution also supports joining in additional tables in the same cross - match queries using traditional joins based on foreign keys .",
    "there are no restrictions on additional tables joined in using traditional joins , even sub - queries and table valued functions are supported .    in sec .",
    "[ sec : xmatchintro ] , we explained the three ways a catalog table can be matched to other catalogs .",
    "these three methods are attributes to the tables themselves and not to the operators joining them .",
    "this has to be taken into account when designing the sql language extensions .",
    "the result of a cross - match query , just like any other sql query , is a table .",
    "cross - match queries may return any combination of the columns of the joined tables , including columns resulting from expressions .",
    "the future versions of the cross - match engine might support aggregate functions as well .",
    "aggregations is particularly interesting as cross - match queries are partitioned and executed on many machines in parallel .    as a result of the cross - identification , the best estimate coordinates and bayes factors characterizing the goodness of the matches are calculated . because these values have to be referenced in the select list of the queries somehow , we will introduce a _ virtual table _ which contains the aforementioned parameters for every n - way match . from the aspect of traditional sql",
    ", one can consider this table as a result set of a table valued function .    as",
    "catalog tables might contain a large number of columns that are out of the interest of most users , for space saving reasons , it would be more convenient to vertically split the tables and mirror only the frequently used columns to all cluster nodes .",
    "the less frequently accessed parts of the tables could be kept on larger , possibly slower , network accessible storage .",
    "we plan to address this problem in a future version of the cross - match engine using a `` lazy join '' algorithm .",
    "the cross - match algorithm we use was developed by gray et al . and is implemented entirely in sql to benefit from the query optimizer @xcite .",
    "the algorithm partitions the sphere along equally spaced latitude circles into _ zones_. the actual matching consist of multiple steps , each producing a new table which is materialized in a staging database . first , a _ zone table _ is generated for each catalog table which contains a special hash of the coordinates ( calculated based on the zones ) .",
    "clever hashing makes finding candidate matches easier , i.e. finding detection pairs that are close enough to each other . from two zone tables ,",
    "we generate a _ pair table _ containing the matching detection candidates .",
    "this is the step when we impose the strict angular separation cuts .",
    "then , joining the pair table with the two catalog tables , a _ match table _ is created .",
    "match tables contain the updated coordinate estimates , parameters to calculate the bayes factor and all the columns necessary to evaluate the final query .",
    "if more than two tables have to be cross - matched , matching is done iteratively , two catalogs at a time .",
    "catalogs are either processed sequentially , matching the next catalog with the match table of the previous iteration , or done in a cascading way , matching two times two catalogs first ( in parallel ) , then matching the resulting two match tables with each other , and so on .",
    "while our current implementation matches catalogs sequentially , future versions might favor the cascading approach to further parallelize processing .",
    "in the future , the cascading approach also can be useful in cases when geographically distributed large data sets have to be matched .",
    "one can easily imagine a scenario where certain catalogs are only available at one site while other catalogs are only at another data processing facility .",
    "matching the locally available catalogs first at each site and transferring only the matched results from one site to another would help reducing network traffic significantly .",
    "we call this approach _ co - location - aware optimization_.      in sec .",
    "[ sec : sqlext ] , we explained why we decided to keep much of the traditional sql syntax intact and introduce only new clauses . to explain the behavior of the new clauses , we consider query  [ que : sample ] .",
    "the first half of the query ( above the customized ` xmatch ` clause ) is in traditional sql .",
    "data sets listed in the ` from ` clause are called sdss , twomass and galex after three frequently used astronomical catalogs .",
    "table names are separated from data set identifiers by colons .",
    "each of the listed tables contains observations of galaxies .",
    "each table has an integer field ` objid ` which is the primary key .",
    "spherical coordinates are stored in the ` ra ` and ` dec ` columns in traditional spherical coordinates .",
    "dec stands for declination , the angle measured from the equator toward the poles ; the equivalent of @xmath20 . ] .",
    "cartesian coordinates are named ` cx ` , ` cy ` and ` cz ` .",
    "columns denoted with ` mag_`__x _ _ are brightness measurements of the objects in different imaging filters .",
    ".... select x.ra , x.dec ,         s.objid , s.ra , s.dec , s.mag_g , s.mag_r , s.mag_i ,         g.objid , g.ra , g.dec , g.mag_nuv , g.mag_fuv ,         t.objid , t.ra , t.dec , t.mag_j , t.mag_h , t.mag_k into    mydb : newresults from    sdss : photoobjall as s         cross join galex : photoobjall as g         cross join twomass : photoxsc as t where   s.galaxy = 1 xmatch bayesian as x         must s on point(s.cx , s.cy , s.cz ) ,",
    "0.1         must g on point(g.ra , g.dec ) , 0.2         may   t on point(t.ra , t.dec ) , 0.5         having limit 1e6 region circle j2000 180 0 60 ....    the ` from ` clause simply produces the cartesian product of the three catalog tables . in traditional sql , one would write a ` where ` clause which filters the cartesian product leaving only matching detections .",
    "obviously , tables of high cardinality can not be matched that way .",
    "one solution would be to analyze the ` where ` clause describing the cross - match criteria and optimize query execution accordingly .",
    "such expression analysis and optimization algorithm is way too complicated .",
    "instead , we introduce the new ` xmatch ` clause that eliminates the need for complex expression analysis and simplifies optimization a lot .    at this point",
    ", we would like to point out that a typical cross - identification query not only returns the ids of the matching detections , but also various columns ( or expressions of them ) of the original catalog tables .",
    "if only the list of matching ids was returned , at the end users would need to execute an n - way inner join to retrieve these additional columns .",
    "such an n - way join could cost as much ( or more ) in i / o terms as the entire cross - identification . doing this n - way",
    "inner join at the same time with cross - identification , overall i / o costs can be significantly reduced .    in query",
    "[ que : sample ] , the ` bayesian ` keyword belongs to the ` xmatch ` clause and defines the method of cross - identification .",
    "currently only bayesian is supported .",
    "the ` as x ` alias is used to make the columns calculated by the cross - matching algorithm being able to be referenced by the rest of the query .",
    "note the first line of query  [ que : sample ] and the ` x.ra ` and ` x.dec ` column references .",
    "these columns will contain the best coordinate estimates computed by the bayesian algorithm .",
    "the same virtual table also returns the bayes factor as ` x.bf ` which can be used to calculate posterior probabilities . the `",
    "having limit ` clause is required and specifies the minimum value of the bayes factor for a positive match .",
    "again , we decided to use a custom keyword , instead of incorporating this criterion into the ` where ` clause to make implementation simpler .      as it was mentioned in sec .",
    "[ sec : xmatchintro ] , precise information about the footprint of the catalogs is required to run certain cross - identification queries .",
    "similarly , queries might be restricted by the users to a certain area of the sky .",
    "this is demonstrated in the last line of query  [ que : sample ] , where the region of interest is restricted to a circle on the sky centered on the coordinate @xmath21 with a radius of @xmath22  am ( arc minute ) .",
    "the ` j2000 ` prefix defines the equinox of the coordinate system .",
    "an ivoa standard exists @xcite to describe spherical regions as string , similar to the sample in query  [ que : sample ] .",
    "these descriptions , however , become increasingly verbose as the regions get more complex . consequently , a future syntax extension will have to support both inline region descriptions ( as in the sample ) and a way to reference regions already stored somewhere in some standardized format .",
    "one repository of region descriptions could be based on footprint services developed earlier by our group @xcite .",
    "according to the considerations we made in sec .",
    "[ sec : sqlext ] , language extensions to restrict cross - identification queries to a given area of the sky should use a special construct , most favorably a new clause to describe the spatial filtering criteria . mixing it with the ` where ` clause conditions , where boolean algebraic combinations of various types of filtering criteria are allowed , would make query optimization significantly more complicated .",
    "for example , a spatial constraint could be combined with another constraint using the ` or ` operator which would make it very hard to determine at which point of the query execution the spatial constraint has to be applied . on the other hand ,",
    "if the spatial constraint is not combined with any other criterion , it can be imposed in the very first step of the zone - based algorithm , when the zone table of the first table is being built for the very first catalog joined in in the query , see sec .",
    "[ sec : zone ] .",
    "the previously introduced syntax allows for limiting matches based on the bayes factor .",
    "the latter is calculated solely on the basis of measured coordinates and astrometric errors .",
    "it might be necessary , however , to further filter matches based on parameters of the detections other than the coordinates .",
    "for instance , stars and quasars may look very similar in images but their colors are very different . if a star and a quasar appear very close to each other , the measured coordinates of their detections alone might not be enough to associate the detections with the real objects , so we would end up with false positive matches mixing detections of the star with those of the quasar .",
    "incorporating colors into the model , however , allows for easy separation . in the bayesian framework introduced in sec .",
    "[ sec : bayesian ] probabilistic models can be very easily extended . in the current version",
    ", we only support cuts on the bayes factor , but additional cuts on posterior probabilities can be easily imposed in the ` where ` clause of the queries .",
    "our current system consists of five dell power edge 2950 servers with two xeon e5430 processors and @xmath23  gb of ram each .",
    "the i / o subsystem consists of two dell perc 5/e raid controllers connected to two dell powervauld md1000 storage units containing 15 disk drives each .",
    "disk drives are configured in four 7-disk raid  5 volumes .",
    "raid  5 is not an evident choice for a high performance system . according to our tests ,",
    "however , the sequential read bandwidth of the raid  5 volumes saturates at the same value as the speed of simple striped volumes .",
    "write speed is about half of that , but since our workload is highly biased toward sequential reads , this did not happen to have a huge impact on the overall performance .",
    "the benefit from using raid  5 volumes is the increased directly attached storage capacity of the servers .",
    "the total capacity of the i / o system is @xmath24  tb per server with a sustained sequential read throughput of @xmath25  gb / s .",
    "the system is supposed to be very easily scaled up to higher performance servers according to the scaling abilities of sql server .",
    "the cluster nodes are connected with @xmath3  gb / s network links using ip protocol .",
    "the database servers are supplemented by a head node dedicated to coordinating job execution and running the web server for the user interfaces .",
    "the head node also contains a central database storing the state of the cluster .",
    "we based the implementation on microsoft sql server 2008 r2 , windows server 2008 and the .net  platform .",
    "our decade long experience with the sql server line , and many existing libraries written in c@xmath26  for the .net  framework made these the obvious choice .",
    "porting the system to other platforms would require rewriting a major fraction of the code including legacy libraries .",
    "while most of the cross - identification algorithm is implemented in pure sql , c@xmath26  code is used to generate the queries on the fly .",
    "we extensively use user - defined functions running inside the sql clr to perform region based calculations , see sec .",
    "[ sec : region ] . to implement the parallel job execution and queueing system ,",
    "see sec .",
    "[ sec : jobs ] , we rely on version  4 of .net  workflow foundation .",
    "its massive support for parallel activities , automatic workflow persistence and integration with the .net  framework made it the best candidate to implement our query pipelines .",
    "database layouts are optimized for the underlying hardware .",
    "database tables of the astronomical catalogs are organized into file groups ( separate file group for each large table ) to optimize for long table scans that happen when cross - matching entire catalogs .",
    "file groups contain multiple files split across the raid volumes .",
    "the database server makes sure that data is evenly distributed among the database files .",
    "databases storing catalog data are mirrored to every cluster node to allow for parallelization and load - balancing .",
    "these databases are all set to read - only .",
    "we create a so called _ mini _ version of every catalog to support gathering query statistics on the fly .",
    "mini databases are uniformly sampled from the original databases at a @xmath27 sampling rate .",
    "sampling is done such a way that foreign key references remain intact . in the case of astronomical catalogs",
    "it usually can be achieved by sampling the object table first and simply enforcing foreign key constraints on the rest of the tables .",
    "skyquery uses staging databases to store intermediate output produced by the cross - identification algorithm , see sec .",
    "[ sec : zone ] .",
    "these databases are heavily used for both reading and writing . since in sql server transaction logging",
    "can not be turned off completely , we optimize all queries writing their results into the staging databases to do minimal logging .",
    "currently we have one staging database per cluster node , but the api supports automatic selection of high speed background storage volumes , like ssd - based arrays for staging , if necessary .",
    "as users interact with the system via sql , the most convenient way to store query results is to allocate a moderately sized database for each user , and save query results there @xcite .",
    "users can also upload their own data tables and store them in their so called _",
    "mydb_. the final result sets can be easily downloaded as files . in the current configuration ,",
    "mydbs are distributed among the cluster nodes ; only one copy of a database per user .",
    "in addition to table storage , in the final system , users will be able to create their own views and write their own user - defined functions as well .",
    "a future version will give the users restricted access to the big staging databases on the cluster nodes .",
    "this is important in cases when the results of a computation require only limited storage but internal steps might produce larger outputs .",
    "unfortunately , microsoft sql server does not support clustering of database servers other than fail - over clustering .",
    "although there exists support in sql server for linking servers over a network connection and execute cross - server queries , the performance is poor",
    ". consequently , we have to rely on single server databases and build our solutions around this limitation .",
    "we target two types of problems : a ) partition databases too big to fit on a single server and distribute them over a set of servers , and b ) mirror exact copies of databases to a set of servers to distribute the load of small queries and to parallelize big queries .    to be able to easily develop advanced federated database solutions , first we built a _",
    "cluster management _ application and api that can register and manage all the necessary information about the hardware and software configuration , database layouts , and users of the cluster .",
    "the registry stores detailed information about the individual server machines .",
    "machines are organized by roles , so different hardware configurations can be assigned to different tasks .",
    "information about the i / o system is also stored , including the size , bandwidth , fail - over level etc .",
    "of the logical disk volumes .",
    "disk volumes can be easily assigned to various tasks like storage , staging , temporary , log etc .",
    "this becomes increasingly important with applications having both cpu - bound and i / o - bound components that should be targeted to the most appropriate hardware .",
    "we organize databases into _",
    "federations_. this is a loosely bound set of databases belonging to a certain application .",
    "federations can contain _ database definitions_. a database definition is not an actual database but a prototype with all the schema but no data .",
    "database definitions also contain information about how the data should be distributed over the cluster machines .",
    "actual physical databases are created based on these prototype schemas automatically by the api .",
    "the cluster management api also contains functionality to manage users associated with the federations and a job queueing system we will describe in sec .",
    "[ sec : jobs ] in detail .      to parse the special extensions introduced to the sql language in sec .",
    "[ sec : sqlext ] , we wrote a parser generator from scratch that we use to generate a parser from the extended grammar described in bnf .",
    "the main reason behind writing our own solution instead of using common parser generators was that we wanted to use certain features of the c@xmath26language that was not supported by other generators .",
    "once a query expression is parsed and the parsing tree is built , identifiers referencing tables and columns are resolved based on the underlying database schema . at this point",
    "we can collect all the information needed to execute a query .",
    "first , tables and databases referenced by the query are identified .",
    "although all large databases are mirrored to the cluster nodes , cross - identification queries may reference tables from the mydb of the user which is only available on one of the nodes .",
    "all tables in mydb are assumed to be small enough so they can be simply cached in the staging databases of the worker nodes prior to query execution .",
    "future versions of the system will have the ability to fetch data from remote , virtual observatory compatible data sources a similar way , via the internet .    before proceeding to the query optimization step ,",
    "sanity checks are performed to make sure all tables referenced in the query conform with the requirements of the cross - match algorithm , for example all tables have appropriate primary keys and the data types of the columns storing spherical coordinates are compatible .",
    "the zone algorithm cross - matches catalogs pairwise .",
    "once two catalogs have been cross - identified , the new best estimate coordinates are calculated and passed on to the next iteration . from the perspective of optimization , starting with catalogs of the least cardinality is the best choice , except when looking for drop - outs , see sec .",
    "[ sec : xmatchintro ]  and  [ sec : zone ] .",
    "since the extended sql syntax supports filtering the data , and spatial constraints can also be applied to the queries , there is not much use to store static cardinality information about the catalogs . instead , our system is designed to gather statistics about each query prior to optimization . in sec .",
    "[ sec : dblayout ] , we mentioned that random subsets of the source catalogs , the so called _ mini _ databases are created and stored on the cluster nodes .",
    "we use these mini catalogs to get quick statistics about the source tables referenced by the queries .",
    "once the tables are identified in the parsing tree , all criteria restricting the rows of that table are also collected from the ` on ` conditions of the ` join ` expressions and from the ` where ` clause of the query . from this information we are not only able to estimate the cardinality of the source tables but also to determine the spatial distribution of the object detections of the astronomical catalogs after all the selection criteria have been applied .",
    "information about the spatial distribution of the data points is essential in order to be able to efficiently partition the query .",
    "as the zone algorithm indexes the surface of the sphere based on the declination ( latitude ) angle , partitioning on the right ascension ( longitude ) angle is a convenient choice .",
    "based on the histogram of right ascensions , the surface of the sphere is split into disjoint partitions defined by great circles intersecting at the poles .",
    "partition boundaries can be chosen anywhere as all the data is mirrored to every cluster node .",
    "this also eliminates the need of buffer zones along partition boundaries .",
    "boundaries are chosen such a way that an approximately equal number of detections fall into each partition .",
    "the number of partitions is chosen to be a multiple of the available cluster nodes .",
    "we use more partitions than the number of available physical machines to execute the cross - match task , because higher granularity makes error recovery much easier , only smaller parts of the job had to be redone when unexpected events happen .",
    "every partition of a cross - match query translates to a sequence of ordinary sql queries , and these query sequences run in parallel on many machines .",
    "because of the complexities of multi - threaded application development , we decided to implement the cross - match jobs as _ workflows _ written for .net   workflow foundation ( wf ) version 4 .",
    "wf has extensive support for parallel execution of _ activities _ ( atomic components of workflows ) , and also for exception handling and workflow cancellation logic .",
    "workflows make it sure that ordinary sql queries performing the cross - matching will run in the necessary order and that partitions will be processed in parallel .",
    "all ordinary sql queries are written such a way that they do not return any data but write all results into the staging databases of the particular cluster nodes .",
    "also , all heavy computations are coded into these sql queries .",
    "these design constraints make it possible to build workflows that do only basic computations and issue regular sql queries to remote servers to do the rest . as a result",
    ", all workflows can be run on a single head node of the cluster instead of scheduling non - sql user code on the cluster nodes .    for each cross - match query",
    ", we create a new job ( in the form of a wf workflow ) and schedule its execution with a custom - written queueing system .",
    "the queueing system supports execution of jobs with different time - out intervals .",
    "this is particularly important in open access database systems , like skyquery , where queries written by the users can have any complexity .",
    "users developing queries would submit them first to the _ quick _ queue to see if the queries work correctly on smaller chunks of the data .",
    "once they are satisfied with the results , they can send the queries to the long queue with much longer time - out interval for guaranteed completion .    to avoid moving large amounts of data between servers , we schedule the execution of an entire partition of a cross - identification job on the same cluster node .",
    "cluster nodes are assigned to the partitions in a round robin fashion .",
    "we chose round robin scheduling over complex load balancing because of some problems arising from the behavior of database servers .",
    "for instance , it is hard to correctly measure the load on a particular database server as either cpu load or i / o load can vary heavily during query processing .",
    "assigning servers to tasks in round robin seems a much simpler and reasonable way .",
    "we extended the wf base library with a custom activity to support re - execution of a certain branch of a workflow upon an exception .",
    "the typical scenario we wanted to cover is when a single cluster node goes down and a regular sql query fails . in this case",
    ", another cluster node can be assigned to the failing branch of the workflow and the branch can be re - executed without affecting other branches of the entire workflow .",
    "the number of re - tries can be limited , so permanent errors in the system wo nt cause the workflows to go into an infinite loop but to fail permanently .",
    "because we expect some really long running jobs , we had to deal with the issue of suspending and resuming jobs for system maintenance reasons .",
    "although wf supports persisting the state of workflows , a workflow can only be suspended at transition points between activities . when a suspend request arises , most workflows can be suspended in a short time interval .",
    "some activities executing longer - running ordinary sql queries that do not complete within the time - out period are simply cancelled and restarted whenever the system comes on - line again .",
    "this is another reason to use partitions of higher granularity . since ordinary sql queries",
    "can not be suspended , only cancelled , partitioning is a good way to limit the size of operations done in a single step , thus to shorten single query execution times .    to maintain the integrity of the system and to save temporary and staging storage space ,",
    "all job workflows are designed to fail gracefully and to be able to be cancelled gracefully , i.e. they remove all temporary data generated and restore the original state of the system .",
    "job workflows are implemented and installed as .net  binary assemblies , so any change to the workflows requires recompilation of the assemblies with higher version numbers .",
    "we designed the job queueing system to be able to handle multiple version of workflow assemblies , so changes to the workflows or activities will not require a system restart .      since we have to deal with large amounts of data we had to optimize all data moving operations , especially those that happen between machines .",
    "we implemented a service that runs on all servers of the cluster and handles bulk data copy requests from the head node .",
    "entire data tables or subsets of tables are copied across servers using sql server bulk - copy , while entire databases are copied using robust file copy .",
    "we implemented large copy jobs as workflows .",
    "for instance , a workflow was written to mirror databases to all cluster nodes in a cascading manner : first a single copy of the original database is made , then the two copies are used to make two more mirrors in parallel , and so on .",
    "the relational data model itself only uses table names and column names to identify quantities stored in the databases .",
    "scientific applications , on the other hand , require detailed description of the physical quantities .",
    "the internatinal virtual observatory alliance defines the ontology and metadata models to describe astronomical data . in sql server , extended properties",
    "can be added to every database schema object .",
    "these properties can be easily queried via special views . for skyquery , we use these extended properties to store meta - information .",
    "metadata includes description of the quantities using identifiers based on the ontology but also human - readable text to display on web pages , etc .",
    "since all data in our system are manipulated with sql scripts , results of the computations are manifested as output tables stored in the users mydbs . because we parse every sql query executed we have complete control over the schema and metadata of the output tables as well .",
    "it will be very convenient in the future to derive metadata and provenance information about query outputs directly from the sql scripts .",
    "in this paper we have introduced a new , scalable implementation of software for cross - identification of co - located astronomical catalogs . compared to the earlier reincarnations , for the third version of skyquery , the following improvements have been made .",
    "a ) instead of single - server operation , queries are partitioned and executed on a cluster of identical database servers having identical versions of all data sets .",
    "b ) an easy to optimize syntax extension to the sql language was invented to support simple formulation of cross - match problems .",
    "c ) queries are translated into complex workflows of traditional sql queries .",
    "workflows are implemented in windows workflow foundation to support parallel execution .    for the next versions of the system",
    ", we are working on the following additions .",
    "a ) right now , all queries are run from scratch , i.e. helper tabled used to speed up cross - matching are newly created every time when needed .",
    "certain helper tables , e.g. zone tables , could be cached to further speed up execution .",
    "b ) we are designing a generic framework for handling metadata which will allow extracting provenance information directly from the queries written by users .",
    "c ) a lazy - join algorithm is being designed to allow vertically partition tables .",
    "this will make it possible to move less frequently used columns to cheaper storage .",
    "d ) we will add support to reference tables from remote data sets accessible to virtual observatory standard protocols . in the future",
    ", we plan to update the system to be able to cooperate with remote data centers and support co - location - aware query optimization .",
    "this work was supported by the following hungarian grants : nkth : polnyi and kckha005 ."
  ],
  "abstract_text": [
    "<S> multi - wavelength astronomical studies require cross - identification of detections of the same celestial objects in multiple catalogs based on spherical coordinates and other properties . because of </S>",
    "<S> the large data volumes and spherical geometry , the symmetric n - way association of astronomical detections is a computationally intensive problem , even when sophisticated indexing schemes are used to exclude obviously false candidates . </S>",
    "<S> legacy astronomical catalogs already contain detections of more than a hundred million objects while the ongoing and future surveys will produce catalogs of billions of objects with multiple detections of each at different times . </S>",
    "<S> the varying statistical error of position measurements , moving and extended objects , and other physical properties make it necessary to perform the cross - identification using a mathematically correct , proper bayesian probabilistic algorithm , capable of including various priors . </S>",
    "<S> one time , pair - wise cross - identification of these large catalogs is not sufficient for many astronomical scenarios . </S>",
    "<S> consequently , a novel system is necessary that can cross - identify multiple catalogs on - demand , efficiently and reliably . in this paper , we present our solution based on a cluster of commodity servers and ordinary relational databases . </S>",
    "<S> the cross - identification problems are formulated in a language based on sql , but extended with special clauses . </S>",
    "<S> these special queries are partitioned spatially by coordinate ranges and compiled into a complex workflow of ordinary sql queries . </S>",
    "<S> workflows are then executed in a parallel framework using a cluster of servers hosting identical mirrors of the same data sets . </S>",
    "<S> the whole system consists of custom - written software modules for the probabilistic algorithms , cluster management , job queuing , high - performance bulk data copying , query parsing and optimization , and metadata management . </S>",
    "<S> the system is designed to be accessible via various types of web interfaces for human and program clients . </S>"
  ]
}