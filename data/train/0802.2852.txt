{
  "article_text": [
    "for a positive integer @xmath11 , assume a probability distribution @xmath1 on @xmath12 is given .",
    "consider the following random process . a token moves in @xmath0 , as follows :    * initially , place the token in some position in @xmath3 .",
    "* in round @xmath4 : the token is at position @xmath13 .",
    "choose an element @xmath5 from @xmath14 at random , according to @xmath1 .",
    "if @xmath15 , move the token to position @xmath7 ( the step is `` accepted '' ) , otherwise leave it where it is ( the step is `` rejected '' ) .    when the token has reached position 0 , no further moves are possible , and we regard the process as finished .    at the beginning",
    "the token is placed at a position chosen uniformly at random from @xmath16 .",
    "( for simplicity of notation , we prefer this initial distribution over the possibly more natural uniform distribution on @xmath17 .",
    "of course , there is no real difference between the two starting conditions . )",
    "let @xmath8 be the number of rounds needed until position @xmath18 is reached .",
    "a basic performance parameter for the process is @xmath19 .",
    "as @xmath1 varies , the value @xmath19 will vary .",
    "the probability distribution @xmath1 may be regarded as a strategy .",
    "we ask : how should @xmath1 be chosen so that @xmath19 is as small as possible ?",
    "it is easy to exhibit distributions @xmath1 such that @xmath20 .",
    "( all asymptotic notation in this paper refers to @xmath21 . ) in particular , we will see that the `` harmonic distribution '' given by @xmath22 where @xmath23 is the @xmath11th harmonic number , satisfies @xmath24 . as the main result of the paper",
    ", we will show that this upper bound is optimal up to constant factors : @xmath25 , for every distribution @xmath1 .",
    "for the proof of this lower bound , we introduce a novel potential function technique , which may be useful in other contexts .",
    "-0.3 cm      consider the problem of minimizing a function @xmath26 \\rightarrow { \\mathbbm{r}}$ ] , in which the definition of @xmath27 is unknown : the only information we can gain about @xmath27 is through trying sample points .",
    "this is an instance of a _ black box optimization problem _  @xcite .",
    "one algorithmic approach to such problems is to start with an initial random point , and iteratively attempt to improve it by making random perturbations .",
    "that is , if the current point is @xmath28 $ ] , then we choose some distance @xmath29 $ ] according to some probability distribution @xmath1 on @xmath10 $ ] , and move to @xmath30 or @xmath31 if this is an improvement .",
    "the distribution @xmath1 may be regarded as a `` search strategy '' .",
    "such a search is `` blind '' in the sense that it does not try to estimate how close to the minimum it is and to adapt the distribution @xmath1 accordingly .",
    "the problem is how to specify @xmath1 . of course , an optimal distribution @xmath1 depends on details of the function @xmath27 .",
    "the difficulty the search algorithm faces is that for general functions @xmath27 there is no information about the scale of perturbations which are necessary to get close to the minimum .",
    "this leads us to the idea that the distribution might be chosen so that it is _ scale invariant _ , meaning that steps of all `` orders of magnitude '' occur with about the same probability .",
    "such a distribution is described in  @xcite .",
    "one starts by specifying a minimum perturbation size @xmath32 . then one chooses the probability density function @xmath33 for @xmath34 , and @xmath35 otherwise , where @xmath36 is the _ precision _ of the algorithm .",
    "( a random number distributed according to this density function may be generated by taking @xmath37 , where @xmath38 is uniformly random in @xmath39 $ ] . )    for general functions @xmath27 , no analysis of this search strategy is known , but in experiments on standard benchmark functions it ( or higher dimensional variants ) exhibits a good performance .",
    "( for details see  @xcite . ) from here on , we focus on the simple case where @xmath27 is _ unimodal _ , meaning that it is strictly decreasing in @xmath40 $ ] and strictly increasing in @xmath41 $ ] , where @xmath42 is the unknown minimum point .    if one is given the information that @xmath27 is unimodal , one will use other , deterministic search strategies , which approximate the optimum up to @xmath32 within @xmath43 steps . as early as 1953 , in  @xcite , `` fibonacci search '' was proposed and analyzed , which for a given tolerance @xmath32 uses the optimal number of steps in a very strong sense .",
    "the `` blind search '' strategy from  @xcite can be applied to more general functions @xmath27 , but the following analysis is valid only for unimodal functions . if the distance of the current point @xmath44 from the optimum @xmath42 is @xmath45 then every distance @xmath5 with @xmath46 will lead to a new point with distance at most @xmath47 .",
    "thus , the probability of at least halving the distance to @xmath42 in one step is at least @xmath48 which is independent of the current state @xmath44 . obviously , then",
    ", the expected number of steps before the distance to @xmath42 has been halved is @xmath49 .",
    "we regard the algorithm to be successful if the current point has distance smaller than @xmath50 from @xmath42 . to reach this goal , the initial distance has to be halved at most @xmath51 times , leading to a bound of @xmath52 for the expected number of steps .",
    "the question then arises whether this is the best that can be achieved .",
    "is there perhaps a choice for @xmath1 that works even better on unimodal functions ? to investigate this question , we consider a discrete version of the situation .",
    "the domain of @xmath27 is @xmath0 , and @xmath27 is strictly increasing , so that @xmath27 takes its minimum at @xmath53 . in this case",
    ", the search process is very simple : the actual values of @xmath27 are irrelevant ; going from @xmath54 to @xmath55 is never an improvement .",
    "actually , the search process is fully described by the simple random process from section  [ sec : problem ] .",
    "how long does it take to reach the optimal point 0 , for a @xmath1 chosen as cleverly as possible ?",
    "for @xmath56 , we will show an upper bound of @xmath57 , with an argument very similar to that one leading to the bound @xmath52 in the continuous case .",
    "the main result of this paper is that the bound for the discrete case is optimal .",
    "-0.3 cm      for the sake of simplicity , we let from now on @xmath58 $ ] denote the discrete interval @xmath59 if @xmath54 and @xmath60 are integers . given a probability distribution @xmath1 on @xmath61 $ ] , the markov chain @xmath62 is defined over the state space @xmath63 $ ] by the transition probabilities @xmath64 clearly , @xmath18 is an absorbing state .",
    "we define the random variable @xmath65 let us write @xmath19 for the expectation of @xmath8 if @xmath66 is uniformly distributed in @xmath67 $ ] .",
    "we study @xmath19 in dependence on @xmath1 . in particular , we wish to identify distributions @xmath1 that make @xmath19 as small as possible ( up to constant factors , where @xmath11 is growing ) .",
    "[ sec : closedexpression ]    [ obs : mu : one : positive ] if @xmath68 then @xmath69    this is because with probability @xmath70 position 1 is chosen as the starting point , and from state 1 , the process will never reach 0 if @xmath68 . as a consequence , for the whole paper we assume that all distributions @xmath1 that are considered satisfy @xmath71    next we note that it is not hard to derive a `` closed expression '' for @xmath19 .",
    "fix @xmath1 . for @xmath13 , let @xmath72)=\\sum_{1\\le d\\le a}\\mu(d).$ ] we note recursion formulas for the expected travel time @xmath73 when starting from position @xmath13 .",
    "it is not hard to obtain ( details are omitted due to space constraints ) @xmath74 where the sum ranges over all @xmath75 nonempty subintervals @xmath76 $ ] of @xmath61 $ ] . by definition of @xmath77",
    ", we see that @xmath19 is a rational function of @xmath78 . by compactness",
    ", there is some @xmath1 that minimizes @xmath19 .",
    "unfortunately , there does not seem to be an obvious way to use ( [ eq:30 ] ) to gain information about the way @xmath19 depends on @xmath1 or what a distribution @xmath1 that minimizes @xmath19 looks like .",
    "in this section , we establish upper bounds on @xmath19 . we split the state space @xmath3 and the set @xmath14 of possible distances into `` orders of magnitude '' , arbitrarily choosing @xmath79 as the base .",
    "means `` logarithm to the base @xmath79 '' throughout .",
    "] let @xmath80 , and define @xmath81 , for @xmath82 , and @xmath83 $ ] . define @xmath84 clearly , then , @xmath85 . to simplify notation ,",
    "we do not exclude terms that mean @xmath86 for @xmath87 or @xmath88 .",
    "such terms are always meant to have value @xmath18 .",
    "consider the process @xmath62 .",
    "assume @xmath89 and @xmath90 .",
    "if @xmath91 then all numbers @xmath92 will be accepted as steps and lead to a progress of at least @xmath93 .",
    "hence @xmath94 further , if @xmath95 , we need to choose step sizes from @xmath96 at most twice to get below @xmath97 .",
    "since the expected waiting time for the random distances to hit @xmath96 twice is @xmath98 , the expected time process @xmath99 remains in @xmath100 is not larger than @xmath98 .",
    "adding up over @xmath101 , the expected time process @xmath99 spends in the interval @xmath102 $ ] , where @xmath103 is the starting position , is not larger than @xmath104 after the process has left @xmath105 $ ] , it has reached position @xmath18 or position @xmath106 , and the expected time before we hit 0 is not larger than @xmath107 .",
    "thus , the expected number @xmath108 of steps to get from @xmath103 to 0 satisfies @xmath109 this implies the bound @xmath110 for arbitrary @xmath1 .",
    "if we arrange that @xmath111 we will have @xmath112 . clearly , then , @xmath20 as well . the simplest distribution @xmath1 with @xmath113 is the one that distributes the weight evenly on the powers of 2 below @xmath114 : @xmath115 thus , @xmath116 the `` harmonic distribution '' defined by ( [ eq : harmonic : distribution ] ) satisfies @xmath117 , and we also get @xmath118 and @xmath24 .",
    "more generally , all distributions @xmath1 with @xmath119 , where @xmath120 is constant , satisfy @xmath20 .    -0.3",
    "we show , as the main result of this paper , that the upper bound of section  [ sec : upperbound ] is optimal up to a constant factor .",
    "[ thm : lower : bound ] @xmath25 for all distributions @xmath1 .",
    "this theorem is proved in the remainder of this section .",
    "the distribution @xmath1 is fixed from here on ; we suppress @xmath1 in the notation . recall that we may assume that @xmath121 .",
    "we continue to use the intervals @xmath122 that partition @xmath61 $ ] , as well as the probabilities @xmath86 , @xmath123 .",
    "-0.3 cm      the basic idea for the lower bound is the following .",
    "for the majority of the starting positions , the process has to traverse all intervals @xmath124 .",
    "consider an interval @xmath100 .",
    "if the process reaches interval @xmath125 , then afterwards steps of size @xmath126 and larger are rejected , and so do not help at all for crossing @xmath100 .",
    "steps of size from @xmath125 , @xmath100 , @xmath96 , @xmath127 may be of significant help .",
    "smaller step sizes will not help much .",
    "so , very roughly , the expected time to traverse interval @xmath100 completely when starting in @xmath125 will be bounded from below by @xmath128 since @xmath129 is the waiting time for the first step with a `` significant '' size to appear .",
    "if it were the case that there is a constant @xmath130 with the property that for each @xmath131 the probability that interval @xmath125 is visited is at least @xmath132 then it would not be hard to show that the expected travel time is bounded below by @xmath133 ( we picked out only the even @xmath134 to avoid double counting . ) now the sum of the denominators in the sum in ( [ eq:60 ] ) is at most @xmath79 , and the sum is minimal when all denominators are equal , so the sum is bounded below by @xmath135 , hence the expected travel time would be @xmath136 .",
    "it turns out that it is not straightforward to turn this informal argument into a rigorous proof .",
    "first , there are ( somewhat strange ) distributions @xmath1 for which it is not the case that each interval is visited with constant probability .",
    "( for example , let @xmath137 , for a large base @xmath138 like @xmath139 .",
    "then the `` correct '' jump directly to 0 has an overwhelming probability to be chosen first . ) even for reasonable distributions @xmath1 , it may happen that some intervals or even blocks of intervals are jumped over with high probability .",
    "this means that the analysis of the cost of traversing @xmath100 has to take into account that this traversal might happen in one big jump starting from an interval @xmath140 with @xmath141 much larger than @xmath142 .",
    "second , in a formal argument , the contribution of the steps of size smaller than @xmath143 must be taken into account .    in the remainder of this section",
    ", we give a rigorous proof of the lower bound .",
    "for this , some machinery has to be developed .",
    "the crucial components are a reformulation of process @xmath99 as another process , which as long as possible defers decisions about what the ( randomly chosen ) starting position is , and a potential function to measure how much progress the process has made in direction to its goal , namely reaching position 0 .",
    "cm      we change our point of view on the process @xmath99 ( with initial distribution uniform in @xmath61 $ ] ) .",
    "the idea is that we do not have to fix the starting position right at the beginning , but rather make partial decisions on what the starting position is as the process advances .",
    "the information we hold on for step @xmath4 is a random variable @xmath144 , with the following interpretation : if @xmath145 then @xmath146 is uniformly distributed in @xmath147 $ ] ; if @xmath148 then @xmath149 .",
    "what properties should the random process @xmath150 on @xmath151 $ ] have to be a proper model of the markov chain @xmath99 from section  [ sec : formalization ] ?",
    "we first give an intuitive description of process @xmath152 , and later formally define the corresponding markov chain .",
    "clearly , @xmath153 : the starting position is uniformly distributed in @xmath61 $ ] . given @xmath154",
    "$ ] , we choose a step length @xmath5 from @xmath14 , according to distribution @xmath1 .",
    "then there are two cases .",
    "_ case _ 1 : @xmath155 .",
    " if @xmath156 , this step can not be used for any position in @xmath157 $ ] , thus we reject it and let @xmath158 .",
    "if @xmath159 , no further move is possible at all , and we also reject .",
    "_ case _ 2 : @xmath160 .",
    " then @xmath156 , and the token is at some position in @xmath157 $ ] .",
    "what happens now depends on the position of the token relative to @xmath5 , for which we only have a probability distribution .",
    "we distinguish three subcases :    * the position of the token is larger than @xmath5 .",
    " this happens with probability @xmath161 . in this case",
    "we `` accept ''",
    "the step , and now know that the token is in @xmath162 $ ] , uniformly distributed ; thus , we let @xmath163 .",
    "* the position of the token equals @xmath5 .",
    " this happens with probability @xmath164 . in this case",
    "we `` finish '' the process , and let @xmath148 .",
    "* the position of the token is smaller than @xmath5 .",
    " this happens with probability @xmath165 in this case we `` reject '' the step , and now know that the token is in @xmath166 $ ] , uniformly distributed ; thus , we let @xmath167 .    clearly , once state 0 is reached , all further steps are rejected via case 1 .",
    "we formalize this idea by defining a new markov chain @xmath168 , as follows .",
    "the state space is @xmath63 $ ] . for a state @xmath169",
    ", we collect the total probability that we get from @xmath170 to @xmath169 . if @xmath171 , this probability is 0 ; if @xmath172 , this probability is @xmath173 ; if @xmath174 , this probability is @xmath175 ; if @xmath176 , this probability is @xmath177 , since @xmath5 could be @xmath178 or @xmath179 .",
    "thus , we have the following transition probabilities : @xmath180 again , several initial distributions are possible for process @xmath152 . the version with initial distribution with @xmath181 is meant to describe process @xmath99 .",
    "define the stopping time @xmath182 we note that it is sufficient to analyze process @xmath152 ( with the standard initial distribution ) .",
    "[ lemma : equivalence]@xmath183    for @xmath184 , consider the version @xmath185 of process @xmath99 induced by choosing the uniform distribution on @xmath157 $ ] ( for @xmath156 ) resp .",
    "@xmath186 ( for @xmath159 ) as the initial distribution .",
    "we let @xmath187 clearly , @xmath188 and @xmath189 .",
    "we derive a recurrence for @xmath190 .",
    "let @xmath156 , and assume the starting point @xmath66 is chosen uniformly at random from @xmath157 $ ] .",
    "we carry out the first step of @xmath185 , which starts with choosing @xmath5 .",
    "the following situations may arise .",
    "* @xmath155 .",
    " this happens with probability @xmath191 .",
    "this distance will be rejected for all starting points in @xmath157 $ ] , so the expected remaining travel time is @xmath192 again . *",
    "@xmath193 . for each @xmath5 , the probability for this to happen is @xmath194 . for the starting point @xmath66",
    "there are three possibilities : * * @xmath195 $ ] ( only possible if @xmath196 ) .",
    " this happens with probability @xmath197 .",
    "the remaining expected travel time is @xmath198 . *",
    "* @xmath199 .",
    " this happens with probability @xmath200 .",
    "the remaining travel time is @xmath18 . * * @xmath201 $ ] ( only possible if @xmath202 ) .",
    " this happens with probability @xmath203 .",
    "the remaining expected travel time in this case is @xmath204 .",
    "we obtain : @xmath205 we rename @xmath206 into @xmath169 in the first sum and @xmath207 into @xmath169 in the second sum and rearrange to obtain @xmath208 next , we consider process @xmath152 . for @xmath184 ,",
    "let @xmath209 be the process obtained from @xmath152 by choosing @xmath170 as the starting point .",
    "clearly , @xmath210 always sits in @xmath18 , and @xmath211 is just @xmath152 .",
    "let @xmath212 the expected number of steps process @xmath152 needs to reach 0 when starting in @xmath170 . then @xmath213 and @xmath214 .",
    "we derive a recurrence for @xmath215 .",
    "let @xmath156 .",
    "carry out the first step of @xmath209 , which leads to state @xmath169 .",
    "the following situations may arise .",
    "* @xmath216 .",
    " this occurs with probability @xmath217 , and the expected remaining travel time is @xmath218 again .",
    "* @xmath174 .",
    " in this case the expected remaining travel time is @xmath213 . * @xmath219 .",
    " this occurs with probability @xmath220 .",
    "the expected remaining travel time is @xmath221 .    summing up ,",
    "we obtain @xmath222 solving for @xmath218 yields : @xmath223 since @xmath224 and the recurrences ( [ eq : new:42 ] ) and ( [ eq : new:50 ] ) are identical , we have @xmath225 , as claimed .",
    "we introduce a potential function @xmath226 on the state space @xmath63 $ ] to bound the progress of process @xmath152 .",
    "our main lemma states that for any @xmath227 , for a random transition from @xmath228 to @xmath229 the expected loss in potential is at most constant ( i.e. , @xmath230 ) .",
    "this implies that @xmath231 .",
    "since the potential function will satisify @xmath232 , the lower bound follows .",
    "we start by trying to give intuition for the definition .",
    "a rough approximation to the potential function we use would be the following : for interval @xmath100 there is a term @xmath233 for some constant @xmath234 with @xmath235 , e.g. , @xmath236 .",
    "for later use we note that @xmath237 since @xmath238 and @xmath239 .",
    "the term @xmath240 tries to give a rough lower bound for the expected number of steps needed to cross @xmath100 in the following sense : the summands @xmath241 reflect the fact that step sizes that are close to @xmath100 will be very helpful for crossing @xmath100 , and step sizes far away from @xmath100 might help a little in crossing @xmath100 , but they do so only to a small extent ( @xmath242 ) or with small probability ( @xmath243 ) .",
    "the idea is then to arrange that a state @xmath244 has potential about @xmath245 it turns out that analyzing process @xmath152 on the basis of a potential function that refers to the intervals @xmath100 is possible but leads to messy calculations and numerous cases .",
    "the calculations become cleaner if one avoids the use of the intervals in the definition and in applying the potential function .",
    "the following definition derives from ( [ eq:90 ] ) and ( [ eq:100 ] ) by splitting up the summands @xmath240 into contributions from all positions @xmath246 and smoothing out the factors @xmath247 , for @xmath246 and @xmath248 , into @xmath249 , which is @xmath250 for @xmath251 and @xmath252 for @xmath253 .",
    "this leads to the following .",
    ", the range @xmath61 $ ] is implicitly understood . ]",
    "assumption ( [ eq : obs : mu : one : positive ] ) guarantees that in the formulas to follow all denominators are nonzero .",
    "[ definition : potential]for @xmath254 let @xmath255 and @xmath256 . for @xmath184 define @xmath257 the random variable @xmath258 , @xmath259 , is defined as @xmath260 .",
    "we note some easy observations and one fundamental fact about @xmath258 , @xmath261 .",
    "[ lemma : potential : lower : bound ]    * @xmath258 , @xmath261 , is nonincreasing for @xmath4 increasing .",
    "* @xmath262 @xmath263 @xmath148 . *",
    "@xmath264 _ _ ( _ _ @xmath265 is a number that depends on @xmath11 and @xmath1__)__.    \\(a ) is clear since @xmath144 , @xmath261 , is nonincreasing and the terms @xmath266 are positive .  ( b ) is obvious since @xmath262 if and only if @xmath267 is the empty sum , which is the case if and only if @xmath148 .",
    " we prove ( c ) .",
    "in this proof we use the intervals @xmath100 and the probabilities @xmath86 , @xmath123 , from section  [ sec : upperbound ] .",
    "we use the notation @xmath268 .",
    "we start with finding an upper bound for @xmath269 by grouping the summands in @xmath269 according to the intervals .",
    "let @xmath236 .",
    "@xmath270    hence @xmath271 with @xmath240 from ( [ eq:90 ] ) .",
    "thus , @xmath272 let @xmath273 be the reciprocal of the summand for @xmath142 in ( [ eq:130 ] ) , @xmath82 . from ( [ eq:92 ] )",
    "we read off that @xmath274 , for some constant @xmath275 . now",
    "@xmath276 with @xmath277 is minimal if all @xmath278 are equal to @xmath279 .",
    "together with ( [ eq:130 ] ) this entails @xmath280 , which proves part ( c ) of lemma  [ lemma : potential : lower : bound ] .",
    "the crucial step in the lower bound proof is to show that the progress made by process @xmath152 in one step , measured in terms of the potential , is bounded :    [ lemma : potential : main ] there is a constant @xmath281 such that for @xmath282 , we have @xmath283    the proof of lemma  [ lemma : potential : main ] is the core of the analysis .",
    "it will be given in section  [ sec : proofmainlemma ] . to prove theorem  [ thm : lower : bound ] , we need the following lemma , which is stated and proved ( as lemma 12 ) in  @xcite .",
    "( it is a one - sided variant of wald s identity . )",
    "[ lem : jaegerskuepper ] let @xmath284 denote random variables with bounded range , let @xmath285 and let @xmath286 . if @xmath287 and @xmath288 for all @xmath289 , then @xmath290 .",
    "_ proof _ of [ thm : lower : bound ] : since @xmath148 if and only if @xmath262 ( lemma  [ lemma : potential : lower : bound](b ) ) , the stopping time @xmath291 of the potential reaching 0 satisfies @xmath292 .",
    "thus , to prove theorem  [ thm : lower : bound ] , it is sufficient to show that @xmath293 . for this",
    ", we let @xmath294 , the progress made in step @xmath4 in terms of the potential . by lemma  [ lemma :",
    "potential : main ] , @xmath295 , for all @xmath296 , and hence @xmath297 observe that @xmath298 and hence @xmath299 . applying lemma  [ lem : jaegerskuepper ] , and combining with lemma  [ lemma : potential : lower : bound ] , we get that @xmath300 , which proves theorem  [ thm : lower : bound ] .",
    "the only missing part to fill in is the proof of lemma  [ lemma : potential : main ] .",
    "-0.3 cm      fix @xmath301 $ ] , and assume @xmath302 .",
    "our aim is to show that the `` expected potential loss '' is constant , i.e. , that @xmath303 clearly , @xmath304 , where @xmath305 we show that @xmath306 is bounded by a constant , by considering @xmath307 , @xmath308 , and @xmath309 separately .    for @xmath310 ,",
    "the potential difference @xmath311 is 0 , and thus @xmath312    [ [ bounding - deltas0 ] ] * bounding @xmath308 : * + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    according to the definition of the process @xmath152 , a step from @xmath302 to @xmath148 has probability @xmath313 . since @xmath314 ,",
    "the potential difference is @xmath315 .",
    "thus , we obtain    @xmath316 @xmath317    we bound @xmath318 . for @xmath319 and @xmath320 ,",
    "the quotient of the summands in the numerator and denominator of @xmath318 that correspond to @xmath60 is @xmath321 . for @xmath322 and @xmath320 ,",
    "the quotient is @xmath323 .",
    "thus , @xmath324 .",
    "this implies ( recall that @xmath325 ) : @xmath326    [ [ bounding - sum_1leq - x - sdeltasx ] ] * bounding @xmath327 : * + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    assume @xmath328 . according to the definition of the process @xmath152 , @xmath329 the potential difference is @xmath330 .",
    "thus we have @xmath331 where @xmath332 and @xmath333 .",
    "we bound @xmath334 and @xmath335 separately .",
    "observe first that @xmath336 ( we used the definition of @xmath266 , and omitted some summands in the denominator . ) recall that @xmath121 , so the denominator is not zero .",
    "for each @xmath319 we clearly have @xmath337 , thus the sum in the numerator in ( [ eq:220 ] ) is smaller than the sum in the denominator , and we get @xmath338 .    next , we bound @xmath335 for @xmath339 : @xmath340 the denominator is not zero because @xmath121 .",
    "hence , if @xmath341 for all @xmath342 , then @xmath343 . otherwise , by omitting some of the summands in the denominator we obtain @xmath344    ( if @xmath345 , the first sum in both numerator and denominator is empty .",
    ") now consider the quotient of the summands for each @xmath60 with @xmath346 . for @xmath347 ,",
    "this quotient is @xmath348 for @xmath349 , the quotient of the corresponding summands is @xmath350 hence , @xmath351 plugging this bound on @xmath335 and the bound @xmath338 into ( [ eq:210 ] ) , and using that @xmath352_1^s=1 + 2\\sqrt{s}-2 < 2\\sqrt{s},\\ ] ] we obtain @xmath353 summing up the bounds from ( [ eq:160 ] ) , ( [ eq:190 ] ) , and ( [ eq:270 ] ) , we obtain @xmath354 thus lemma  [ lemma : potential : main ] is proved .    -0.3",
    "\\1 . we conjecture that the method can be adapted to the continuous case to prove a lower bound of @xmath355 for approximating the minimum of some unimodal function @xmath27 by a scale - invariant search strategy ( see section  [ sec : blind : optimization ] ) .",
    "it is an open problem whether our method can be used to prove a lower bound of @xmath356 for finding the minimum of an arbitrary unimodal function @xmath357 by a scale invariant search strategy .    -0.3",
    "the authors thank two anonymous referees for their careful reading of the manuscript and for providing several helpful comments .",
    "rowe , j.e . , and hidovi , d. , an evolution strategy using a continuous version of the gray - code neighbourhood distribution , in : k. deb _",
    "_ , _ eds .",
    "gecco 2004 , part 1 , lncs vol .",
    "3102 , springer - verlag , pp ."
  ],
  "abstract_text": [
    "<S> we analyze a simple random process in which a token is moved in the interval @xmath0 : fix a probability distribution @xmath1 over @xmath2 . </S>",
    "<S> initially , the token is placed in a random position in @xmath3 . in round </S>",
    "<S> @xmath4 , a random value @xmath5 is chosen according to @xmath1 . </S>",
    "<S> if the token is in position @xmath6 , then it is moved to position @xmath7 . </S>",
    "<S> otherwise it stays put . </S>",
    "<S> let @xmath8 be the number of rounds until the token reaches position 0 . </S>",
    "<S> we show tight bounds for the expectation of @xmath8 for the optimal distribution @xmath1 . </S>",
    "<S> more precisely , we show that @xmath9 . for the proof , </S>",
    "<S> a novel potential function argument is introduced . </S>",
    "<S> the research is motivated by the problem of approximating the minimum of a continuous function over @xmath10 $ ] with a `` blind '' optimization strategy .    </S>",
    "<S> martin dietzfelbinger    jonathan e.  rowe    ingo wegener    philipp woelfel        -0.5 </S>"
  ]
}