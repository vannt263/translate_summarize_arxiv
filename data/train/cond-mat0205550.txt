{
  "article_text": [
    "in principle , it is our purpose to study learning in a neural net as it occurs in nature .",
    "the theory of recurrent neural nets @xcite provides us with a model of content - addressable memory as it might be realized , to some extent , in the brain .",
    "learning , in such a model , corresponds to adjusting the synaptic matrix @xmath1 in such a way that @xmath2 memorized patterns @xmath3 , @xmath4 , become fixed points of the neuron state dynamics .",
    "this can be achieved in a recurrent neural net by sequentially clamping its neurons to a well - defined and unique set of patterns , and adjusting the weights of the connections according to some hebbian learning rule . however , in reality , a neural net can not be clamped to a fixed set of ideal patterns .",
    "a more realistic assumption would be that the clamping of the net to a pattern always is more or less distorted .",
    "consider , for instance , the visual system as a system in which the clamping is imposed by input from the retina .",
    "since neurons are noisy objects , which once in a while fire spontaneously , an internal representation of a stimulus in the brain will hardly ever be identical to the representation corresponding to a previous stimulus .",
    "we therefore introduce noise to the set of patterns , thus making the set less well - defined and less unique .",
    "a network state array of a net of @xmath5 neurons is denoted by @xmath6 where @xmath7 if neuron @xmath8 is active and @xmath9 if it is non - active . at every learning step @xmath10",
    ", @xmath11 will be similar to one of the @xmath2 given patterns @xmath12 , but it has nonzero probability , for each neuron @xmath8 , @xmath13 , of deviating from it . at each learning step @xmath10 ,",
    "synaptic connections @xmath1 will adapt themselves , according to a hebbian learning rule which is a function of the weights @xmath14 and of the ( binary ) neuron states @xmath15 , @xmath16 . for the class of learning",
    "rules that we will use in our model , the case of noiseless learning has been studied in detail ( @xcite , @xcite , @xcite ) . at every learning step",
    "@xmath10 a pattern @xmath3 , @xmath17 is chosen .",
    "if , for each @xmath10 , we put @xmath18 , the resulting weights @xmath1 for @xmath19 are known to coincide with the pseudo - inverse solution .",
    "the pseudo - inverse solution is a particular solution of the under - determined set of @xmath20 equations @xmath21 for the @xmath22 unknowns @xmath1 @xmath23 , @xmath24 . here",
    ", @xmath25 is a positive number , and @xmath26 a constant .",
    "it is easily verified that these equations guarantee that the so - called stability coefficients @xmath27 are positive for all patterns @xmath3 . in our description , however , the network state during learning is determined by a probability distribution @xmath28 , centered around the patterns @xmath3 . by means of the master equation derived in section [ mastersec ]",
    ", we will arrive at the following equation for the expectation value of the weights in the limit of @xmath19 : @xmath29x_j = 0 \\label{ourequation}\\ ] ] where @xmath30 is the collection of all possible @xmath31 arrays @xmath11 .",
    "in contrast to the equations ( [ piequations ] ) , this is a completely determined set of equations , of which the solution is essentially different from the pseudo - inverse solution of ( [ piequations ] ) .",
    "it will turn out to exist only if the variance of the probability distribution @xmath28 is non - zero , _",
    "i.e. _ , in the presence of noise .",
    "this is the subject of section [ solutionsec ] .    in section [ intermediatesec ] we will study numerically the weights @xmath32 as a function of @xmath10 .    in section [ stabilitysection ]",
    "we will show that , on the average , this solution does yield stability coefficients close to @xmath25 if the ` noise parameter ' , @xmath33 , which will be introduced in the probability distribution @xmath28 , is small enough , which shows that the solution found is stable indeed .",
    "finally , in section [ basinssection ] , we study the size of the basins of attraction around our new solution . it is found that the sizes are larger than those around the pseudo - inverse solution , a result that is in perfect agreement with earlier observations @xcite that learning with noise enlarges the basins of attraction . in these earlier studies , however , no analytical expression for the average values of the weights has been given .",
    "we consider a recurrent net of @xmath5 binary neurons .",
    "the strengths of the synaptic connection between the post - synaptic neuron @xmath34 and the pre - synaptic neuron @xmath8 will be denoted by @xmath1 .",
    "the neurons @xmath8 @xmath35 can take the values @xmath9 or @xmath7 , corresponding to the non - active and active state , respectively . it is useful to associate with each neuron @xmath8 a set @xmath36 , defined as the collection of neuron indices @xmath34 with which neuron @xmath8 has an adaptable , _",
    "i.e. _ , a non - zero , non - constant afferent synaptic connection .",
    "in other words , for all @xmath37 , there is an axon going from neuron @xmath34 to a dendrite of neuron @xmath8 , and the corresponding weight @xmath1 is adaptable in a learning process .",
    "the collection of neurons @xmath34 with which @xmath8 has no connection , or a non - changing synaptic connection , will be denoted as the complementary set , @xmath38 .",
    "we suppose that the synaptic strengths @xmath1 between the neurons are changed in steps according to a rule of the general form @xmath39 where @xmath40 is a function of the states @xmath15 of all @xmath5 neurons of the net and all afferent synaptic weights @xmath14 ( @xmath8 fixed , @xmath41 . in general , the functions @xmath40 will be linear in all @xmath15 , since @xmath42 ( recall that @xmath15 equals @xmath43 or @xmath44 ) , but non - linear in the weights @xmath14 . in this article",
    "we suppose , however , that the @xmath40 do depend linearly on the weights @xmath14 . hence , in this article , @xmath45 is a linear function in all @xmath15 and all @xmath14 ( @xmath46 ) .",
    "we abbreviated @xmath47 it is unrealistic to describe a biological neural net as a deterministic system , since there are many unknown parameters that influence its development in time .",
    "we therefore choose a probabilistic description .",
    "we suppose that the neuron states @xmath48 are mutually independent stochastic variables , _",
    "i.e. _ , the probability that neuron @xmath8 has value @xmath48 is given by a probability distribution @xmath49 which is independent of @xmath34 @xmath50 .",
    "since the changes @xmath40 of the weights @xmath1 are functions of the stochastic variables @xmath48 @xmath51 , and a function of a stochastic variable is a stochastic variable , the changes @xmath40 , and , hence , the @xmath1 themselves , are stochastic variables .",
    "now let @xmath52 be the probability that , due to a learning step , a transition takes place from the value @xmath1 to the value @xmath53 , for a given set @xmath54 .",
    "then we have @xmath55 where @xmath30 is the collection of all @xmath31 possible states of the neural net @xmath56 and @xmath57 is the probability of occurrence of the network state @xmath11 , which we suppose to be independent of the variables @xmath1 . [ the relation between @xmath57 and @xmath49 is left unspecified at this stage of the reasoning ; compare , however , ( [ pxvoorp ] ) and ( [ pmufactor ] ) below . ]",
    "we have @xmath58 the delta - function in ( [ transition ] ) guarantees that only transitions take place which obey the learning rule ( [ leerregel ] ) . using ( [ somp(x ) ] ) we find from ( [ transition ] ) , that @xmath59 let @xmath60 be the probability of occurrence of the variable @xmath1 at a time step @xmath10 @xmath61 .",
    "then @xmath62 and @xmath63 are related according to @xmath64 \\label{pij}\\ ] ] demanding that the probability @xmath62 is normalized initially , @xmath65 we find from ( [ pij ] ) and ( [ normtij ] ) , by induction , that @xmath66 for all @xmath10 . from ( [ normtij ] ) and ( [ pij ] )",
    "it follows that @xmath67\\displaystyle{\\prod_{k\\neq j } } [ p_{ik}(w_{ik}',n)dw_{ik}']dw_{ij } '   \\nonumber \\\\",
    "\\qquad \\qquad ( i=1,\\ldots , n ; j \\in",
    "v_i ) \\label{master}\\end{aligned}\\ ] ] which is the so - called discrete master equation for the weights @xmath1 .",
    "it masters the evolution of the weights @xmath1 as a function of @xmath10 , and determines the values of the weights in the long run .    in order to obtain an expression for the expectation value of the weights after infinitely many learning steps , we first consider the expectation value at time step @xmath10 : @xmath68 the latter expression yields , using the master equation ( [ master ] ) , @xmath69\\displaystyle{\\prod_{k\\neq j } } [ p_{ik}(w_{ik}',n)dw_{ik}']dw_{ij } ' dw_{ij } \\label{masterw1}\\end{aligned}\\ ] ] or , interchanging the primed and unprimed variables @xmath1 and @xmath70 in the first term on the right hand side , @xmath71dw_{ij } ' dw_{ij } \\label{masterw2}\\end{aligned}\\ ] ] or , with ( [ transition ] ) and integrating over @xmath70 , @xmath72 or , with ( [ normpijn ] ) and ( [ expvalue1 ] ) , @xmath73 where we used that @xmath40 is linear in the @xmath74 to replace @xmath75 by the expectation value @xmath76 in the expression for @xmath40 .",
    "if we assume that the expectation values of the synaptic connections @xmath32 converge to finite values , @xmath77 , for @xmath10 tending to infinity , we can solve this equation for @xmath19 .",
    "this is the subject of the next section .",
    "if we suppose that the left - hand side of ( [ masterw4 ] ) vanishes in the limit of @xmath10 tending to infinity , we have @xmath78 at this point , we need an expression for the increment @xmath79 , in the @xmath10-th learning step .",
    "we take the biologically motivated learning rule @xcite @xmath80(2x_i-1)x_j \\qquad ( i=1 , \\ldots , n ; j \\in v_i ) \\label{leerregelhee}\\ ] ] where @xmath81 is the learning rate , @xmath25 the margin parameter and @xmath82 the stability coefficient given by @xmath83 \\label{stabiliteit}\\ ] ] [ cf .",
    "( [ stabcoef ] ) ] . here , @xmath84 is the membrane potential of neuron @xmath8 at step @xmath10 of the learning process @xmath85 and @xmath26 the threshold potential of neuron @xmath8 .",
    "it should be noted that in ( [ potential ] ) @xmath15 is the state of neuron @xmath86 at step @xmath10 of the learning procedure .",
    "substituting ( [ leerregelhee ] ) with ( [ stabiliteit ] ) and ( [ potential ] ) into ( [ masterw5 ] ) we find , using the fact that @xmath87 , @xmath88x_j = 0 \\label{nlarge1}\\ ] ] where we divided by the learning rate @xmath81 .    up to now , the precise form of the probability distribution @xmath57 has been left unspecified . at this point ,",
    "let us specify our probability distribution @xmath57 to be such that the chosen patterns @xmath11 are centered around representative patterns @xmath3 .",
    "to that end , we choose our probability distribution @xmath57 such that it is a sum of @xmath2 equally probable , individually independent probability distributions , _",
    "i.e. _ , @xmath89 where @xmath28 is factorizable , @xmath90 _ i.e. _ , the neurons behave independently from one another .",
    "the quantity @xmath91 is the probability that , once the pattern index @xmath92 is chosen , neuron @xmath8 is in the state @xmath48 .",
    "one therefore has @xmath93 in a learning process , at every step @xmath10 , the index @xmath92 is drawn from a collection of @xmath2 equally probable pattern indices , thus fixing the probability distribution @xmath28 according to which the pattern @xmath11 is chosen for that learning step @xmath10 .",
    "let us denote averages with respect to the probability @xmath28 by @xmath94 @xmath95 implying , in view of ( [ pmufactor ] ) and ( [ imunorm ] ) , @xmath96 thus a bar with an index @xmath92 indicates an average with respect to the probability distribution @xmath28 . with the choice ( [ pxvoorp ] ) , the result ( [ nlarge1 ] ) can be rewritten in terms of these averages , where we must take be aware that a term @xmath97 appears in the sum over @xmath86 : @xmath98 =      \\displaystyle{\\sum_{\\mu = 1}^p }   \\left[\\kappa(2\\overline{x_i}^{\\mu } -1 ) - ( \\displaystyle{\\sum_{k=1}^n }   \\langle w_{ik } \\rangle_{\\infty } \\overline{x_k}^{\\mu } - \\theta_i)\\right ] \\overline{x_j}^{\\mu } \\ \\ \\ j",
    "\\in v_i \\label{nlarge2}\\end{aligned}\\ ] ] the latter result can be rewritten as @xmath99 where we abbreviated @xmath100 \\label{variance}\\end{aligned}\\ ] ] and where we split up the sum over all @xmath86 in a sum over @xmath36 and a sum over its complement @xmath38 : @xmath101\\ , \\overline{x_j}^{\\mu } ,   \\qquad & i=1,\\ldots , n ; \\ \\ j \\in v_i \\label{bij } \\end{aligned}\\ ] ] note that the matrix @xmath102 is a symmetric matrix , the dimension of which equals the number of indices in @xmath36 , _",
    "i.e. _ , the number of adaptable afferent synaptic connections of neuron @xmath8 . in the matrix @xmath103",
    ", we could write @xmath104 rather than @xmath105 , since @xmath106 for @xmath107 .",
    "it is easy to solve the equation ( [ nlarge3 ] ) .",
    "first , rewrite it as @xmath108 \\langle w_{ik }   \\rangle_{\\infty } = b_{ij } \\label{nlarge4}\\ ] ] where @xmath109 is the diagonal matrix @xmath110 the matrix @xmath111 is non - singular , and can be inverted . inserting the explicit form of @xmath112 ( [ bij ] ) ,",
    "we then find @xmath113\\ , \\overline{x_k}^{\\mu } \\ \\ \\ j",
    "\\in v_i \\label{solutionwij}\\ ] ] where we used that @xmath109 and @xmath102 are symmetric matrices . in the usual treatments of noiseless recurrent neural networks ( @xmath114 for all @xmath34 ) ,",
    "one finds for the @xmath115 the so - called pseudo - inverse solution @xcite , @xcite , which reads , in our notation , @xmath116\\xi_j^{\\nu } \\ \\ \\ j \\in v_i \\label{pi}\\ ] ] where @xmath117 is the inverse of the correlation matrix @xmath118 . apparently , our result ( [ solutionwij ] ) is not a simple generalization of the standard result for noiseless recurrent neural networks .",
    "note that the usual pseudo - inverse solution ( [ pi ] ) depends on the initial values @xmath119 of all the weights , whereas our solution ( [ solutionwij ] ) depends only on @xmath119 for @xmath120 and not on the initial value @xmath119 of the changing weights ( @xmath37 ) .",
    "apparently , a little bit of noise completely wipes out the effect of the initial state of changing connections , since the result ( [ solutionwij ] ) is true for any value of the noise unequal zero .    in the limit that all @xmath121 ( [ variance ] ) vanish , the set of equations ( [ nlarge4 ] ) becomes under - determined for @xmath24 , since the matrix @xmath102 is then singular .",
    "hence , the solution ( [ solutionwij ] ) does not exist for a noiseless net .",
    "explicitly , this can be seen as follows .",
    "let us suppose that the @xmath2 average patterns @xmath122 , @xmath4 , span a @xmath2-dimensional vector space . then , for @xmath123 there are coefficients @xmath124 such that @xmath125 for all @xmath126 .",
    "it follows that every column @xmath127 , ( @xmath8 fixed , @xmath34 a running index of @xmath36 and @xmath128 a fixed number larger than @xmath2 ) is a linear combination of the first @xmath2 columns of @xmath129 ( @xmath8 fixed , @xmath34 a running index of @xmath36 and @xmath130 smaller than or equal to @xmath2 ) .",
    "consequently , the matrix @xmath102 has a vanishing determinant , and is not invertible .",
    "therefore , in case the average squared deviation ( [ variance ] ) would vanish , the unique solution ( [ solutionwij ] ) would not exist .",
    "the fact that for vanishing variances @xmath121 our set of equations for the final weights is under - determined has been mentioned already in the introduction , in the text under equation ( [ ourequation ] ) .    in @xcite the occurrence of the average squared deviation ( [ variance ] ) has been overlooked .",
    "this enabled the authors to solve the master equation ( [ masterw4 ] ) in the usual way . by means of the so - called gauss - seidel procedure they obtained a modified version of the usual pseudo - inverse solution for the connections , rather than the expression ( [ solutionwij ] ) .",
    "since our approach was simply based on the assumption of convergence of the @xmath32 for @xmath19 , we had no knowledge of the intermediate values of the weights @xmath32 for finite @xmath10 .",
    "however , we can predict the evolution of the weights through an iterative procedure . if we repeat the derivation in section [ solutionsec ] , starting from ( [ masterw4 ] ) in stead of ( [ masterw5 ] ) , we find @xmath131 in the limit @xmath132 , equation ( [ recurs ] ) implies ( [ nlarge2 ] ) , provided that the weights @xmath32 converge .    using the relation ( [ recurs ] ) , one can find , by numerical iteration , the quantities @xmath32 for any @xmath10 , given the starting values @xmath133 .",
    "hence , we can verify numerically that the @xmath32 are independent of these starting values .",
    "moreover , one can study the convergence of the learning procedure . in order to do so",
    ", one must make a particular choice for the probability distribution @xmath28 , which , up to now , was left unspecified . for our choice [ see ( [ pjmub ] ) ] ,",
    "this distribution will depend on a so - called noise parameter @xmath33 @xmath134 , such that @xmath135 and @xmath136 if the noise parameter @xmath33 vanishes @xmath137 . through the parameter @xmath33",
    "we can tune the amount of noise during the learning process .",
    "numerical calculations show that the @xmath32 do indeed converge in the limit @xmath19 , for arbitrary @xmath33 , including @xmath138 , if @xmath81 is small enough .",
    "interestingly , convergence times to the final values ( [ solutionwij ] ) diverge for a decreasing noise parameter @xmath33 ( _ i.e. _ , @xmath139 ) , but the time of convergence drops to a small value if @xmath138 ( see figure [ convergfigure ] ) , indicating that something peculiar happens in this limit .",
    "in other words , if one demands existence of the solution ( [ solutionwij ] ) , one may choose @xmath33 arbitrarily small , but not zero , and convergence to the solution is faster for larger values of @xmath33 .",
    "if one puts @xmath33 to zero in the iterative application of ( [ recurs ] ) , one observes rapid convergence of the weights , to the pseudo - inverse values ( [ pi ] ) .",
    "maybe surprisingly , these values have no continuous relation with the values for finite @xmath33 , despite the fact that the expressions ( [ recurs ] ) , the difference equations that determine the weights @xmath1 , do depend continuously on the @xmath140 , and , hence [ see equation ( [ sigmab ] ) below ] , on @xmath33 . in view of the difference in the solutions @xmath77 for the cases @xmath138 [ eq . ( [ solutionwij ] ) ] versus @xmath141 [ eq .",
    "( [ pi ] ) ] , this is obvious : there can not be a continuous relationship between them , since the pseudo - inverse solution ( [ pi ] ) depends on all the initial values @xmath119 , whereas our solution ( [ solutionwij ] ) is independent of the initial values of the weights @xmath119 for @xmath37 .    in the next section we investigate whether our final solution for the weights corresponds to the storage of patterns in a stable way .",
    "it is well - known that a neural net with fixed weights @xmath1 ( in our case this will be after the learning phase ) and deterministic neuron dynamics evolves , in the course of time , to limit cycles of finite length @xmath10 @xmath142 .",
    "cycles with @xmath143 , or fixed points , are of particular interest in neural network theory .",
    "if a pattern @xmath3 is a fixed point of the dynamics of a neural net for a given set of weights @xmath1 , the stability coefficients ( [ stabcoef ] ) or ( [ stabiliteit ] ) are positive for all @xmath8 , in which case the system remains in the pattern @xmath3 , _",
    "i.e. _ , the system is stable @xcite . besides the fact that @xmath144 is a measure for the size of the basin of attraction of fixed point @xmath3 @xcite , it is plausible that it is also a measure that determines to what extent the network state @xmath11 remains in a neighborhood of @xmath3 when the deterministic evolution of this neuron state @xmath11 is replaced by a stochastic version of this evolution . in order to get an idea of the effectiveness of the learning process discussed in the preceding section",
    ", we will therefore consider the expectation value of the stability coefficients ( [ stabcoef ] ) in the limit @xmath19 : @xmath145 once a set of patterns @xmath146 is known , these quantities can be explicitly calculated with the help of the expression ( [ solutionwij ] ) . in this section we will attempt to derive , analytically , an approximation of ( [ gammaksi2 ] ) by averaging over sets of patterns @xmath146 with given mean activity @xmath147 . if , however , we would calculate the average of @xmath77 over these patterns directly , we would lose all dependence on neuron indices @xmath34 and pattern indices @xmath92 , such that the correlations with the @xmath148 would disappear .",
    "we therefore have to take a different route .",
    "an approximation for the expectation value ( [ gammaksi2 ] ) is @xmath149 where @xmath150 is the membrane potential of neuron @xmath8 averaged with respect to @xmath28 : @xmath151 in fact , the approximation ( [ gammaksi3 ] ) would be exact if @xmath152 would be equal to @xmath153 for all @xmath34 , _",
    "i.e. _ , in the limit that the probability function is such that @xmath152 equals @xmath148 , for all @xmath34 .",
    "the average potential occurring in ( [ gammaksi3 ] ) can be found from ( [ nlarge2 ] ) . indeed , multiplying by @xmath152 and summing with respect to @xmath37",
    ", we find from this equation : @xmath154 \\displaystyle{\\sum_{j \\in v_i}}\\overline{x_j}^{\\nu } \\overline{x_j}^{\\mu }   \\label{nlarge5}\\ ] ] where we also used ( [ variance ] ) .",
    "the average square deviation @xmath155 occurring in this equation depends on the neuron @xmath34 . in this article",
    "we will consider the case in which all neurons have the same standard deviation @xmath121 @xmath156 _ i.e. _ , the probability @xmath157 is supposed to be such that the uncertainty to find neuron @xmath34 in a state @xmath148 is the same for all neurons of the neural net .",
    "using the identity @xmath158 we find from ( [ nlarge5 ] ) @xmath159 \\displaystyle{\\sum_{j \\in v_i } } \\overline{x_j}^{\\nu } \\overline{x_j}^{\\mu } \\label{nlarge6}\\ ] ] where we used @xmath160 for @xmath120 .",
    "an alternative form for ( [ nlarge6 ] ) reads @xmath161^{\\mu \\nu } \\langle \\overline{h_i}^{\\nu } \\rangle_{\\infty } = \\displaystyle{\\sum_{\\nu=1}^p } f_i^{\\nu}c_i^{\\nu \\mu } + g_i^{\\mu } \\label{nlarge7}\\ ] ] where @xmath162 is the @xmath163 unit matrix and where @xmath164 , the correlation matrix for averaged neuron states , is defined by @xmath165 furthermore , we abbreviated @xmath166 multiplying both sides of the matrix equation ( [ nlarge7 ] ) by the inverse of the ( symmetric ) matrix occurring on its left - hand side we obtain the solution @xmath167^{\\lambda \\mu } + \\displaystyle{\\sum_{\\nu=1}^p }   g_i^{\\nu}[(p\\sigma^2 { \\mathbbm{1}}+ c_i)^{-1}]^{\\nu \\mu } \\label{himu}\\ ] ]    once a particular probability distribution @xmath28 of patterns centered around @xmath3 is given , we can evaluate @xmath168 and @xmath169 , and , hence , via ( [ himu ] ) , the average stability coefficient ( [ gammaksi3 ] ) .",
    "in contrast to our expression ( [ solutionwij ] ) for the expectation value of the final value of the weights @xmath77 , the result for the expected average potential @xmath170 does exist for vanishing @xmath171 . this is clear , already , from ( [ himu ] ) , in which the existence of the inverse @xmath172 does not depend on the presence of the extra term @xmath173 as long as the average patterns @xmath174 are linearly independent , since then @xmath164 is invertible . using ( [ solutionwij ] ) and ( [ deff ] ) , and assuming @xmath175 for all @xmath120 we may write the average potential @xmath176 as @xmath177 comparing this to ( [ himu ] ) with ( [ gimu ] )",
    ", we obtain the identity @xmath178^ { \\lambda \\nu}\\ ] ] hence , though the matrix @xmath179 occurring in ( [ hisubst ] ) itself does not exist for @xmath138 , the above combination clearly does : it reduces to @xmath180 , as we see from the right hand side for @xmath181 , implying that in the limit of vanishing noise @xmath182 which is already clear from ( [ himu ] ) and is equivalent to the average of eq .",
    "( [ piequations ] ) .",
    "thus , although the values of the weights themselves do not have a continuous relation with the values corresponding to the pseudo - inverse solution , the average values for the membrane potentials , and , therefore , of the stability coefficients , do .    in the following",
    "we suppose that @xmath183 for all @xmath120 .",
    "this corresponds to a neural net in which all existing connections are of adaptable strength and the only connections with constant strength are the non - existing connections . for @xmath120 , we then have @xmath184 , and , hence , @xmath185 , implying that @xmath186 let us choose the probability distribution @xmath187 which fulfills ( [ imunorm ] ) , and from which @xmath28 follows by the prescription ( [ pmufactor ] ) . the noise parameter @xmath33 is a probability ( @xmath188 ) .",
    "more specifically , for given @xmath92 @xmath189 , @xmath190 is the probability that the activity @xmath191 of neuron @xmath34 equals that of the pattern @xmath148 .",
    "we suppose that @xmath33 is small compared to unity . as follows from ( [ pjmub ] ) , the noise parameter @xmath33 is related to the width of the distribution of input patterns around each pattern @xmath192 .",
    "we can immediately calculate the average neuron state ( [ average2 ] ) associated with the distribution ( [ pjmub ] ) @xmath193 the coefficient ( [ deff ] ) @xmath194 as well as the average squared deviation ( [ variance ] ) @xmath195 the fact that @xmath196 is @xmath34-independent is a consequence of the particular choice ( [ pjmub ] ) for @xmath197 , _ i.e. _ , of the fact that all neurons @xmath34 are supposed to have the same uncertainty to be in state @xmath148 .",
    "hence , the supposition ( [ jindep1 ] ) is satisfied .",
    "let us suppose that the probability that @xmath198 is @xmath147 , for each @xmath34 independent of any other neuron index @xmath86 , and , hence , that the probability that @xmath199 is @xmath200 , for all of the patterns @xmath201 .",
    "we can now use this to arrive at an estimate value for the average potential of neuron @xmath8 , eq .",
    "( [ himu ] ) , which is exact in the limit of @xmath202 , for @xmath203 fixed , and smaller than @xmath44 . from ( [ cmunu ] ) we find , for @xmath204 , @xmath205 while for @xmath206 we get @xmath207 defining the dilution @xmath208 as the average fraction of neurons from which an arbitrary neuron does not have an incoming connection , each neuron has on the average @xmath209 incoming connections .",
    "hence , using ( [ xjavmu ] ) , we find from ( [ cmunuu ] ) and ( [ cmunue ] ) @xmath210\\ } \\label{cmunu2}\\end{aligned}\\ ] ] we thus have achieved that the correlation matrix @xmath164 for an @xmath5 neuron net has been expressed in parameters typical for the network , namely the dilution @xmath208 , the mean activity @xmath147 and the noise @xmath33 .",
    "an alternative way to write ( [ cmunu2 ] ) is @xmath211 where @xmath212 and @xmath213 are shorthand for combinations of the typical network parameters @xmath147 , @xmath33 and @xmath208 @xmath214 \\label{deflm}\\end{aligned}\\ ] ] with ( [ cmunu3 ] ) , the matrix occurring in ( [ himu ] ) can be cast into the form @xmath215 the inverse of a @xmath2-dimensional matrix @xmath216 with elements @xmath217 is given by the matrix @xmath218 with elements @xmath219/x(x+py ) \\label{ainvers}\\ ] ] from ( [ cmunu3 ] ) , and ( [ ainvers ] ) applied to ( [ iplusc ] ) , we find @xmath220 ^{\\lambda \\mu } \\approx \\frac { l[l+p(\\sigma^2 + m)]\\delta^{\\mu \\nu } + mp\\sigma^2 } { ( l+p\\sigma^2)[l+ p(\\sigma^2+m ) ] } \\label{cmunucinvers}\\end{aligned}\\ ] ] substituting this result , together with ( [ gimu ] ) , into the expression ( [ himu ] ) , yields for the average potential of neuron @xmath8 in pattern @xmath92 the expression @xmath221+mp\\sigma^2 \\}f_i^{\\mu } + mp\\sigma^2   \\sigma_{\\nu \\neq \\mu}f_i^{\\nu } } { ( l+p\\sigma^2)[l+p(\\sigma^2 + m ) ] } \\label{himufinal}\\end{aligned}\\ ] ] the sum over the @xmath168 occurring in this expression can be calculated with ( [ fimub ] ) , @xmath222 \\label{sumfi}\\ ] ] where we used that the average value of the @xmath223 neuron activities @xmath224 can be approximated by @xmath147 , the average activity of the net .",
    "we can now write down the final result for the stability parameters ( [ gammaksi3 ] ) , which is a function of the network parameters @xmath208 , @xmath147 and @xmath33 , the number of patterns @xmath2 , the number of neurons of the net @xmath5 , and the neuron properties @xmath25 and @xmath26 : @xmath225+mp\\sigma^2 \\}[(1 - 2b)\\kappa + \\theta_i(2\\xi_i^{\\mu}-1 ) ] } { ( l+p\\sigma^2)[l+p(\\sigma^2 + m ) ] } \\nonumber \\\\",
    "+ \\frac { mp\\sigma^2(p-1)[(2a-1)(1 - 2b)\\kappa + \\theta_i](2\\xi_i^{\\mu}-1)}{(l+p\\sigma^2)[l+p(\\sigma^2 + m ) ] }   -\\theta_i(2\\xi_i^{\\mu}-1 ) \\label{finalabdpn}\\end{aligned}\\ ] ] note that for @xmath226 we immediately recover @xmath227 and @xmath228 , as we should , from the equations ( [ himufinal ] ) and ( [ finalabdpn ] ) respectively .",
    "the final average stability coefficient of neuron @xmath8 takes two different values respectively , depending on whether @xmath229 or @xmath230 .",
    "in figure ( [ figuur2 ] ) we plotted this quantity for a chosen average activity @xmath231 , as a function of @xmath33 .",
    "it is clear that the stability coefficient can be expected to remain positive . in the same figure we plotted the actual values of @xmath232 , as obtained by choosing randomly a set of patterns",
    "@xmath146 with given mean activity @xmath231 , and using ( [ gammaksi2 ] ) with ( [ solutionwij ] ) for the calculation .",
    "the difference between the curves is evident , and indicates that we must be careful not to overestimate the accuracy of our result as an indication for @xmath232 .",
    "in fact , in a large region , the storage of undisturbed patterns is better than our estimate suggests by a factor @xmath233 to @xmath234 , as can be concluded from the figure . with this in mind",
    ", we may assume that after the noisy learning process , the patterns @xmath3 are indeed fixed points under the deterministic network dynamics for a small noise parameter @xmath33 .",
    "in this section we address the question what happens to the average size of the basins of attraction if noiseless learning ( training parameter @xmath138 ) is compared to noisy learning ( @xmath235 ) .",
    "after the network has been trained with patterns @xmath11 with noise @xmath236 , we numerically check the retrieval capacity of the net by presenting patterns with noise @xmath237 , _ i.e. _ , patterns chosen according to a probability distribution of the form ( [ pjmub ] ) , in which @xmath33 has been replaced by @xmath237 .",
    "the presented patterns evolve under deterministic parallel dynamics @xmath238 .",
    "the attempt to retrieve a pattern is successful if the network state @xmath11 runs into a fixed point equal to the clean , undistorted pattern @xmath3 of which a noisy version was the initial state .",
    "the result can be read off from figure [ basins ] . since the curves obtained via noisy learning lie above the curve with noiseless learning , the basins of attraction are , apparently , enlarged in the presence of noise during the learning stage .",
    "the result is in agreement with earlier studies by gardner et al @xcite , and wong & sherrington @xcite , @xcite , @xcite .    in @xcite , like in our case , noise is added to patterns during a training stage . however , the algorithm is of a different kind , because it includes an error - mask , _",
    "i.e. _ , the weights are updated if and only if , upon presentation of a noisy pattern during the learning stage , the membrane potential @xmath239 has the wrong sign . in this way , if the learning algorithm converges , retrieval of patterns for which the amount of noise is equal to that of the training patterns , is guaranteed .    in @xcite , @xcite and @xcite ,",
    "various retrieval properties of a neural network are discussed .",
    "it is argued that optimizing ( by finding the optimal weights ) the overlap of a noisy pattern with its corresponding training representative after one retrieval step is , in fact , a way of noisy training @xcite . the optimal network is sought for via a replica - calculation that minimizes a cost - function , thus optimizing the first step retrieval .",
    "no explicit learning rule is used in these articles . an explicit expression for the final values of the weights",
    "is not given .",
    "our approach is different from those discussed above , in the sense that we start from an explicit learning rule , which is biologically acceptable : it is derived from the principle that energy cost for synaptic adaptation is minimal @xcite ; it is a function of local variables ; it does not contain error masks ; neurons are assumed to be noisy .",
    "though in our network the basins of attraction are not optimal ( it was not our goal to optimize the basins of attraction ) , we do have an explicit learning algorithm as well as a final expression for the expectation value of weights .",
    "we have shown that learning with noise leads to final values for the weights @xmath1 which are different from those found in the corresponding situation without noise .",
    "surprisingly , the solution for the values of the weights @xmath1 of a noisy system in the limit of vanishing noise , does _ not _ converge to the values of the solution of the system without noise .",
    "moreover , in a system without noise the values of the final weights depend on the initial values of all weights , whereas in a noisy system the initial values of the changing weights , the @xmath1 for @xmath37 , are wiped out in the course of time .",
    "our noisy trained networks have larger basins of attraction than noiselessly trained networks .",
    "this is in agreement with earlier findings in the literature .",
    "the exact dependence of the retrieval properties on various parameters , such as the mean activity @xmath147 and the memory load @xmath240 is still to be elucidated .",
    "the authors are indebted to wouter kager for carefully reading this manuscript and suggesting some improvements .",
    "99 mller b , reinhardt j and strickland m t 1995 _ neural networks : an introduction _",
    "( berlin : springer ) personnaz l , guyon i and dreyfus g 1985 _ j. physique lett . _ * 46 * l359 .",
    "diederich s and opper m 1987 _ phys .",
    "lett . _ * 58 * 949 .",
    "heerema m and van leeuwen w a 1999 _ j. phys . a : math .",
    "gen . _ * 32 * 26386 .",
    "gardner e j , stroud n and wallace d j 1989 _ j. phys . a : math . gen .",
    "_ * 22 * 201930 .",
    "wong k y m and sherrington d 1990 _ j. phys .",
    "* 23 * l17582 .",
    "wong k y m and sherrington d 1990 _ j. phys . a : math .",
    "* 23 * 465972 .",
    "wong k y m and sherrington d 1993 _ phys .",
    "e. _ * 47 * 446582 heerema m and van leeuwen w a 2000 _ j. phys .",
    "a : math . gen . _ * 33 * 178195 .",
    "kinzel w and opper m 1991 _ models of neural networks _",
    "ed domany e ( berlin : springer ) p  152 gardner e 1988_j .",
    "phys . a : math . gen _",
    "* 21 * 25770 ."
  ],
  "abstract_text": [
    "<S> a recurrent neural net is described that learns a set of patterns @xmath0 in the presence of noise . </S>",
    "<S> the learning rule is of a hebbian type , and , if noise would be absent during the learning process , the resulting final values of the weights @xmath1 would correspond to the pseudo - inverse solution of the fixed point equation in question . for a non - vanishing noise parameter , an explicit expression for the expectation value of the weights is obtained . </S>",
    "<S> this result turns out to be unequal to the pseudo - inverse solution . </S>",
    "<S> furthermore , the stability properties of the system are discussed . </S>"
  ]
}