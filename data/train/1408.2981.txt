{
  "article_text": [
    "highly efficient solvers for elliptic partial differential equations ( pdes ) are required in many areas of fluid modelling , such as numerical weather- and climate- prediction ( nwp ) , subsurface flow simulations @xcite and global ocean models @xcite .",
    "often these equations need to be solved in `` flat '' domains with high aspect ratio , representing a subsurface aquifer or the earth s atmosphere . in both cases",
    "the horizontal extent of the area of interest is much larger than the vertical size .",
    "for example , the euler equations , which describe the large scale atmospheric flow , need to be integrated efficiently in the dynamical core of nwp codes like the met office unified model @xcite .",
    "many forecast centres such as the met office and european centre for medium - range weather forecasts ( ecmwf ) use semi - implicit semi - lagrangian ( sisl ) time stepping @xcite to advance the atmospheric fields forward in time because it allows for larger model time steps and thus better computational efficiency .",
    "however , this method requires the solution of a anisotropic elliptic pde for the pressure correction in a thin spherical shell at every time step . as the elliptic solve can account for a significant fraction of the total model runtime , it is important to use algorithmically efficient and parallel scalable algorithms .",
    "suitably preconditioned krylov - subspace and multigrid methods ( see e.g. @xcite ) have been shown to be highly efficient for the solution of elliptic pdes encountered in numerical weather- and climate prediction ( see @xcite and the comprehensive review in @xcite ) .",
    "multigrid methods are algorithmically optimal , i.e. the number of iterations required to solve a pde to the accuracy of the discretisation error is independent of the grid resolution . however - as far as we are aware - multigrid algorithms are currently not widely implemented operationally in atmospheric models and one of the aims of this paper is to demonstrate that they can be used very successfully in fluid simulations at high aspect ratio . whereas `` black - box '' algebraic multigrid ( amg ) @xcite solvers such as the ones implemented in the dune - istl @xcite and hypre @xcite libraries can be applied under very general circumstances on unstructured grids and automatically adapt to potential anisotropies , they suffer from additional setup costs and lead to larger matrix stencils on the coarse levels . on",
    "( semi- ) structured grids which are typical in many atmospheric and oceanographic applications , geometric multigrid algorithms usually give much better performance , as they can be adapted to the structure of the problem by the developer .",
    "in contrast to amg algorithms which explicitly store the matrix on each level , it is possible to use a matrix - free approach : instead of reading the matrix from memory , it is reconstructed on - the - fly from a small number of `` profiles '' .",
    "this leads to a more regular memory access pattern and significantly reduces the storage costs , in particular if these profiles can be factorised into a horizontal and vertical component .",
    "as the code is memory bandwidth limited this also has a direct impact on the performance of the solver .",
    "robust geometric multigrid methods adapt the smoother or coarse grid transfer operators to deal with very general anisotropies in the problem ( see e.g. @xcite )",
    ". however , this robustness comes at a price and these methods are often computationally expensive and difficult to parallelise .    in the problems we consider ,",
    "the tensor - product structure of the underlying mesh and the grid - aligned anisotropy make it possible to use the much simpler but highly efficient tensor - product multigrid approach described for example in @xcite : line - relaxation in the strongly coupled direction is combined with semi - coarsening in the other directions only .",
    "the implementation is straightforward : in addition to an obvious modification of the intergrid operators , every smoother application requires the solution of a tridiagonal system of size @xmath0 in each vertical column with @xmath0 grid cells .",
    "the tridiagonal solve requires @xmath1 operations and hence the total cost per iteration is still proportional to the total number of unknowns .",
    "the method is also inherently parallel as in atmospheric applications domain decomposition is typically in the horizontal direction only .    in @xcite",
    "this method was analysed theoretically for equations with a strong vertical anisotropy on a two dimensional tensor - product grid .",
    "the authors show that optimal convergence of the tensor - product multigrid algorithm in two dimensions follows from the optimal convergence of the standard multigrid algorithm for a set of one - dimensional elliptic problems in the horizontal direction .",
    "while the original work in @xcite applies in two dimensions , it has been extended to three dimensions in @xcite and the algorithm has recently been applied successfully to three dimensional problems in atmospheric modelling in @xcite .",
    "although the proof in @xcite relies on the coefficients in the pde to factorise exactly into horizontal - only and vertical - only contributions , we stress that this property is not required anywhere in the implementation of the multigrid algorithm . in practice",
    "we expect the algorithm to work well also for approximately factorising coefficients and under suitable assumptions we are able to also prove this rigorously .",
    "to demonstrate this numerically , we carry out experiments for the elliptic pde arising from semi - implicit semi - lagrangian time stepping in the dynamical core of atmospheric models such as the met office unified model @xcite , where the coefficients only factorise approximately but the multigrid convergence is largely unaffected .",
    "alternatively , we also investigate approximate factorisations of the atmospheric profiles and then apply the tensor product multigrid algorithm to the resulting , perturbed pressure equation to precondition iterative methods for the original system , such as a simple richardon iteration or bicgstab @xcite . as the operator is usually `` well behaved '' in this direction ( i.e. it is smooth and does not have large variations on small length scales ) , the multigrid algorithm will converge in a very small number of iterations .",
    "an additional advantage of applying the multigrid method only to the perturbed problem with factorised profiles is the significant reduction in storage requirements for the matrix .",
    "as the algorithm is memory bound and the cost of a matrix application or a tridiagonal solve depends on the efficiency with which the matrix can be read from memory this leads to performance gains in the preconditioner : we find that the time per iteration can be reduced by around @xmath2 , but this has to be balanced with a possibly worse convergence rate .",
    "nevertheless , our numerical experiments show , that in some cases the factorised preconditioner can be faster overall . on novel",
    "manycore computer architectures , such as gpus , where around 30 - 40 floating point operations can be carried out per global memory access , we expect the performance gains from this matrix - free tensor - product implementation to be even more dramatic . if the matrix is stored in tensor product format and the local stencil is calculated on - the - fly , the costs for the matrix construction can essentially be neglected compared to the cost of reading fields from memory .",
    "for example , carrying out a sparse matrix - vector product requires 1 global memory read and @xmath3 global writes per grid cell compared to @xmath4 reads and 1 write if the @xmath3-point matrix stencil is stored explicitly - a speedup of almost a factor two .",
    "the benefits of this matrix - free implementation on gpus has recently been shown in a similar context in @xcite .    in state - of - the - art global weather prediction models",
    "the horizontal resolution is of the order of tens of kilometres with the aim of reducing this to around one kilometre in the next decade ( the number of vertical grid cells is typically around @xmath5 ) .",
    "the resulting problems with @xmath6 degrees of freedom can only be solved on operational timescales if their scalability can be guaranteed on massively parallel computers .",
    "in addition to the sequential algorithmic performance we demonstrate the parallel scalability of our solvers on hector , the uk s national supercomputer which is hosted and managed by the edinburgh parallel computing centre ( epcc ) . we find that our solvers show very good weak scaling on up to 20,480 amd opteron cores and can solve a linear system with 11 billion unknowns in less than 5 seconds ( reducing the residual by five orders of magnitude ) .",
    "all our code is implemented in the distributed and unified numerics environment ( dune ) @xcite , which is an object oriented c++ library and provides easy to use interfaces to common parallel grid implementations such as alugrid @xcite and ug @xcite . due to the modular structure of the library and because we can rely on the underlying parallel grid implementations , the implementation of our solvers on tensor - product grids is straightforward . throughout the code performance",
    "is guaranteed by using generic metaprogramming based on c++ templates .",
    "[ [ structure ] ] structure + + + + + + + + +    this paper is organised as follows . in section [ sec : ellipticpde ] we describe the pressure correction equation arising in semi - implicit semi - lagrangian time stepping in atmospheric models and discuss the discretisation of the resulting linear pde with particular emphasis on the tensor - product structure of the grid .",
    "the theory of the tensor - product multigrid algorithm is reviewed in section [ sec : tensorproductmultigrid ] where we extend the analysis in @xcite to three dimensions following @xcite . in this section",
    "we also prove the convergence of the preconditioned richardson iteration for non - factorising profiles .",
    "the grid structure and the discretisation of the equation as well as the implementation of our algorithms in the dune framework are described in section [ sec : implementation ] .",
    "numerical results for different test cases are presented together with parallel scaling tests in section [ sec : numericalresults ] .",
    "we conclude and present ideas for future work in section [ sec : conclusions ] .",
    "some more technical aspects can be found in the appendices , in particular the finite - volume discretisation is described in detail in appendix [ sec : discretisation ] .",
    "the elliptic pde which arises in semi - lagrangian semi - implicit time stepping in atmospheric forecast models is derived for example in @xcite for the endgame dynamical core of the unified model . for simplicity ( and in contrast to @xcite )",
    "the work in this paper is based on a finite volume discretisation of a continuous version of this pde and in the following we outline the main steps in the construction of the corresponding linear algebraic problem .",
    "the euler equations describe large scale atmospheric flow as a set of coupled non - linear differential equations for the velocity @xmath7 , ( exner- ) pressure @xmath8 , potential temperature @xmath9 and density @xmath10 .",
    "@xmath11    { \\frac{d\\theta}{dt } } & = r_\\theta \\qquad \\text{(thermodynamic equation)}\\\\[1ex ]    { \\frac{d\\rho}{dt } } & = - \\rho \\nabla\\cdot v \\qquad\\text{(mass conservation)}\\\\[1ex ]    \\rho\\theta & = \\gamma\\pi^{\\gamma } \\qquad\\text{(equation of state ) }    \\end{aligned }    \\label{eqn : eulerequations}\\ ] ] the @xmath12-terms describe external- and sub - gridscale- forcings such as gravity and unresolved convection .",
    "the constants @xmath13 and @xmath14 are defined as    3 & p_0/r_d , & & , & & r_d / c_p ,    where @xmath15 is a reference pressure ; @xmath16 and @xmath17 are the specific heat capacity and specific gas constant of dry air .",
    "system can be written schematically for the state vector @xmath18 as @xmath19 .",
    "\\label{eqn : eulerschematic}\\ ] ] advection is described in the semi - lagrangian @xcite framework , i.e. material time derivatives @xmath20 are replaced by @xmath21 where @xmath22 is the departure point of a parcel of air at time @xmath23 which is advected to position @xmath24 at time @xmath25 .",
    "the right - hand - side of ( [ eqn : eulerschematic ] ) is treated semi - implicitly @xcite .",
    "because of the small vertical grid spacing and the resulting large courant number of vertical sound waves , vertical advection needs to be treated fully implicitly , but some of the other terms are evaluated at the previous time step and thus treated explicitly ; we write @xmath26 .",
    "we use the @xmath9-method with off - centering parameter @xmath27 for implicit time stepping and replace @xmath28 & =    \\mathcal{n}^{(\\text{impl.})}[\\phi({\\ensuremath{\\boldsymbol{x}}},t ) ] +    \\mathcal{n}^{(\\text{expl.})}[\\phi({\\ensuremath{\\boldsymbol{x}}},t ) ] \\\\    & \\mapsto    \\mu \\mathcal{n}^{(\\text{impl.})}[\\phi^{(t+\\delta t)}({\\ensuremath{\\boldsymbol{x } } } ) ] +    ( 1-\\mu ) \\mathcal{n}^{(\\text{impl.})}[\\phi^{(t)}({\\ensuremath{\\boldsymbol{x } } } ) ] +    \\mathcal{n}^{(\\text{expl.})}[\\phi^{(t)}({\\ensuremath{\\boldsymbol{x } } } ) ]    \\end{aligned }    \\label{eqn : semiimplicit}\\ ] ] and in the following we always assume that @xmath29 which corresponds to the scheme described in @xcite . by eliminating the potential temperature , density and all velocities from the resulting equation ,",
    "one ( non - linear ) equation for the pressure @xmath30 at the next time step can be obtained . to solve this equation via ( inexact ) newton iteration",
    ", all fields are linearised around a suitable reference state ( which can for example be the atmospheric fields at the previous time step ) denoted by subscript `` ref '' . to this",
    "end the pressure at the next time step is written as @xmath31 with analogous expressions for @xmath32 and @xmath33 ; the reference velocities @xmath34 are assumed to be zero .",
    "it should , however , be kept in mind that the linearisation does not need to be `` exact '' as the non - linear equation can be solved with an inexact newton iteration . in particular ,",
    "some terms can be moved to the right hand side which is equivalent to treating them explicitly or lagging them in the non - linear iteration .",
    "naturally , there will be a tradeoff between faster convergence of the newton iteration and the cost of the inversion of the linear operator ; for example , in @xcite all couplings to non - direct neighbours , which can be large in the case of steep orography , are moved to the rhs to reduce the size of the stencil of the linear operator . while these considerations are relevant for the optimisation of the non - linear solve in a particular model , in this article we focus on the solution of the linear equation , which is the computationally most expensive component of the newton iteration .",
    "once the exner pressure @xmath30 has been calculated , the evaluation of the remaining atmospheric fields at the next time step is straightforward and does not require any additional ( non-)linear solves .",
    "in contrast to explicit time stepping methods the courant number can be chosen significantly larger than 1 , which makes semi - implicit semi - lagrangian time stepping very popular in operational models . however , because of the short advective time scale and to ensure that large scale flow is described correctly , the courant number is usually limited to around 10 , i.e. the implicit time step size is no more than one order of magnitude larger than what would be allowed in an explicit method . to evaluate the overall performance of the method , the benefits of a larger time step would have to be balanced against the additional cost for the elliptic solve .      for ease of notation",
    "we simply write @xmath35 in the following and drop the time indices .",
    "then the non linear equation for @xmath8 is of the form @xmath36 to solve this equation iteratively we expand all fields around a reference state ( which can , for example , be given by the fields at the previous time step ) to obtain a linear operator @xmath37 . as discussed above , in practise some terms",
    "might be lagged in the non - linear iteration , i.e. moved to the right hand side of the linear equation . at each step @xmath38 of the nonlinear iteration",
    "we write @xmath39 for the approximate solution to ( [ eqn : non_linear ] ) and update the pressure as follows : @xmath40 every iteration requires the solution of a linear equation @xmath41 for the pressure correction @xmath42 , which we denote as @xmath43 in the following . to construct the linear operator @xmath37 we proceed as follows : starting from ( [ eqn : eulerequations ] ) the semi - lagrangian framework in ( [ eqn : semilagrangian ] ) is used for horizontal advection and vertical advection is treated implicitly ( to ensure that mass is exactly conserved , advection is treated implicitly in all three spatial dimensions in the mass equation ) .",
    "the right hand sides are expanded according to ( [ eqn : semiimplicit ] ) .",
    "we linearise around reference profiles @xmath44 , @xmath45 and @xmath46 which fulfil the equation of state @xmath47 , i.e. write @xmath48 etc . and",
    "assume that the velocity expansion is around zero @xmath49 .",
    "if we split up the velocity into a tangential- and vertical- component @xmath50 the time - discretised euler equations in ( [ eqn : eulerequations ] ) finally become in a spherical geometry @xmath51    w & = & r_w ' - { \\ensuremath{\\mu \\delta t}}c_p \\left({\\ensuremath{\\theta_{\\operatorname{ref}}}}{\\partial_r}\\pi ' + ( { \\partial_r}{\\ensuremath{\\pi_{\\operatorname{ref } } } } ) \\theta'\\right ) , \\label{eqn : sislmom_vert}\\\\[1ex ]    \\theta ' & = & r'_\\theta - { \\ensuremath{\\mu \\delta t}}({\\partial_r}{\\ensuremath{\\theta_{\\operatorname{ref}}}})w , \\label{eqn : sislthermodyn}\\\\[1ex ]    \\rho ' & = & r_\\rho ' - { \\ensuremath{\\mu \\delta t}}\\left (      \\frac{1}{r^2}{\\partial_r}\\left(r^2 { \\ensuremath{\\rho_{\\operatorname{ref } } } } w\\right ) + \\frac{1}{r}\\left({\\ensuremath{{\\boldsymbol{\\nabla}}_{\\!\\!{\\mathcal{s}}}}}\\cdot \\left({\\ensuremath{\\rho_{\\operatorname{ref}}}}{\\ensuremath{\\boldsymbol{v}}}_{{{\\mathcal{s}}}}\\right)\\right )    \\right),\\label{eqn : sislmass}\\\\[1ex ]    \\pi ' & = & \\frac{{\\ensuremath{\\pi_{\\operatorname{ref}}}}}{\\gamma}\\left(\\frac{\\rho'}{{\\ensuremath{\\rho_{\\operatorname{ref } } } } } + \\frac{\\theta'}{{\\ensuremath{\\theta_{\\operatorname{ref}}}}}\\right ) , \\label{eqn : sislstate}\\end{aligned}\\ ] ] where @xmath52 is the normal component of the derivative and @xmath53 is the component tangential to a unit sphere @xmath54 with outer normal @xmath55 .",
    "any terms that depend on the current time step are absorbed in the @xmath56-terms .",
    "we then rewrite ( [ eqn : sislstate ] ) as a function of @xmath57 and insert it together with ( [ eqn : sislmom_horiz ] ) into ( [ eqn : sislmass ] ) to obtain an equation with @xmath58 , @xmath57 and @xmath43 only @xmath59 by solving ( [ eqn : sislmom_vert ] ) and ( [ eqn : sislthermodyn ] ) for @xmath58 and @xmath60 we obtain    2 w & = @xmath61 ( f_2-@xmath62c_p @xmath63(_r ) ) , &  & = @xmath61 ( f_3+(@xmath62)^2 c_p @xmath63(_r@xmath63)(_r ) ) [ eqn : wtheta ]    where @xmath64 arises from the implicit treatment of vertical advection and the ( squared ) vertical buoyancy ( or brunt - visl- ) frequency is given by @xmath65 the functions @xmath66 and @xmath67 only depend on the fields at the current time step .",
    "we rescale the vertical coordinate @xmath68 by the radius of the earth @xmath69 and the potential temperature by a reference temperature @xmath70 at ground level to make it dimensionless .",
    "finally , we multiply equation by @xmath46 and denote the typical horizontal velocity by @xmath71 is the speed of sound in a parcel of air with temperature @xmath70 . furthermore we introduce the dimensionless quantity @xmath72 after eliminating @xmath58 and @xmath9 from ( [ eqn : substitution1 ] ) with the help of ( [ eqn : wtheta ] ) we obtain a second order equation for the pressure correction @xmath43 : @xmath73    \\right\\ } \\\\    &",
    "-\\omega^4 \\frac{1}{r^2 } { \\ensuremath{{\\boldsymbol{\\nabla}}_{\\!\\!{\\mathcal{s}}}}}\\cdot\\left({\\ensuremath{\\lambda_{\\operatorname{ref}}}}{\\ensuremath{\\rho_{\\operatorname{ref}}}}({\\ensuremath{{\\boldsymbol{\\nabla}}_{\\!\\!{\\mathcal{s}}}}}{\\ensuremath{\\pi_{\\operatorname{ref}}}})({\\partial_r}{\\ensuremath{\\theta_{\\operatorname{ref}}}})({\\partial_r}\\pi')\\right )    + \\gamma \\frac{{\\ensuremath{\\rho_{\\operatorname{ref}}}}}{{\\ensuremath{\\pi_{\\operatorname{ref}}}}}\\pi ' = rhs    \\end{aligned}\\label{eqn : helmholtzapp}\\ ] ] the @xmath74 term arises due to the last term in ( [ eqn : sislmom_horiz ] ) . in @xcite",
    "this term is not included in the linear operator since all terms which stem from reference profiles that do not depend exclusively on the vertical coordinate are neglected . to be consistent with this approach",
    ", the @xmath74 term is assumed to be moved to the right hand side of the linear equation in the following .",
    "the first two terms in the curly brackets are the sum of a vertical advection and a vertical diffusion term .",
    "in contrast , in @xcite , the linear pressure correction equation is derived from the discretised euler equations .",
    "however , it can be shown that ( [ eqn : helmholtzapp ] ) is identical to the continuum limit of equation ( 67 ) in @xcite if the latter is written down explicitly in spherical coordinates . denoting the unknown pressure correction @xmath43 by @xmath75 , as is common in the mathematical literature , the elliptic operator can be written as @xmath76    & =    -\\omega^2    \\begin{pmatrix }      { \\partial_r } , &      \\frac{1}{r}{\\ensuremath{{\\boldsymbol{\\nabla}}_{\\!\\!{\\mathcal{s}}}}}\\end{pmatrix}^t    \\begin{pmatrix }      \\alpha_r & 0 \\\\[1ex ]      0 & \\alpha_{{{\\mathcal{s}}}}{\\ensuremath{\\operatorname{id}_{2\\times 2}}}\\end{pmatrix }    \\begin{pmatrix }      { \\partial_r}\\\\[1ex ]      \\frac{1}{r}{\\ensuremath{{\\boldsymbol{\\nabla}}_{\\!\\!{\\mathcal{s}}}}}\\end{pmatrix }    u    - \\omega^2    \\begin{pmatrix }      \\xi_r , & 0    \\end{pmatrix}^t    \\begin{pmatrix }      { \\partial_r}\\\\[1ex ]      \\frac{1}{r}{\\ensuremath{{\\boldsymbol{\\nabla}}_{\\!\\!{\\mathcal{s}}}}}\\end{pmatrix }    u    + \\beta u    \\end{aligned } \\label{eqn : helmholtzvector}\\ ] ] where @xmath77 is the @xmath78 identity matrix .",
    "the equation is solved in a thin spherical shell , @xmath79 $ ] and @xmath80 is the ratio of the thickness of the atmosphere and the radius of the earth .",
    "the solution @xmath81 depends on the coordinates @xmath82 $ ] and @xmath83 .",
    "in contrast to global latitude - longitude grids , on quasi - uniform grids the ratio between the smallest and largest grid spacing is bounded . to ensure that the horizontal acoustic courant number @xmath84 ( where @xmath85 is the smallest grid spacing ) remains unchanged as the horizontal resolution",
    "is increased , the time step size @xmath86 has to decrease linearly with @xmath85 .",
    "a simple scaling argument shows that the vertical advection term is much smaller than the diffusion term at high resolution .",
    "the functions @xmath87 , @xmath88 , @xmath89 and @xmath90 are referred to as `` profiles '' in the following and can be obtained from the background fields @xmath45 , @xmath44 and @xmath46 by comparing the elliptic operators in ( [ eqn : helmholtzapp ] ) and ( [ eqn : helmholtzvector ] ) :    4 _ r & = r^2 @xmath61 @xmath91@xmath63 ( = r^2 @xmath61 _ ) , & _ & = @xmath91@xmath63 , & _ r & = @xmath61@xmath91(_r@xmath63 ) , & & = .",
    "[ eqn : profiles ]      after discretisation , the helmholtz equation in ( [ eqn : helmholtzvector ] ) can be written as a large algebraic system of the form @xmath92 where the finite - dimensional field vector @xmath93 represents the pressure correction in the entire atmosphere .",
    "if we assume that the horizontal resolution is around 1 kilometre and @xmath94 vertical grid levels are used , each atmospheric variable has @xmath95 degrees of freedom . problems of this size can only be solved with highly efficient iterative solvers and on massively parallel computers .",
    "current forecast models , such as the met office unified model , use suitable preconditioned krylov subspace methods ( see e.g. @xcite for an overview ) such as bicgstab @xcite .",
    "due to the flatness of the domain the equation is highly anisotropic : typical grid spacings in the horizontal direction are at the order of tens of kilometres , whereas the distance between vertical levels can be as small as a few metres close to the ground .",
    "while this anisotropy is partially compensated by the ratio @xmath96 in ( [ eqn : profiles ] ) , it remains large in particular for small time steps @xmath86 for which @xmath97 ( recall that we chose units such that @xmath98 ) .",
    "as discussed in the literature @xcite , a highly efficient preconditioner for krylov methods in this case is vertical line relaxation .",
    "this amounts to a block jacobi or block sor iteration where the degrees of freedom in one vertical column are relaxed simultaneously by solving a tridiagonal equation .",
    "however , ( geometric ) multigrid algorithms have also been considered by the atmospheric modelling community @xcite and recently some of the authors have demonstrated their superior behaviour for a simplified model equation @xcite .",
    "efficient algorithms for the solution of anisotropic equations have been studied extensively in the multigrid literature . for general anisotropies in convection dominated problems ,",
    "robust schemes have been designed by adapting the smoother ( see e.g. @xcite ) or the coarsening strategy and the restriction / prolongation operators ( see e.g. @xcite ) .",
    "for example , in @xcite alternating approximate plane- and line- smoothers are discussed . alternatively ,",
    "if algebraic multigrid ( amg ) @xcite is used , the coarse grids and smoothers will automatically adapt to any anisotropies and the method can even be applied on unstructured grids . however , amg has additional setup costs for the coarse grids and explicitly stores the coarse grid matrices .",
    "this has a significant impact on the performance in bandwidth - dominated applications .",
    "while these `` black - box '' approaches work well for very general problems and do not require anisotropies to be grid - aligned , they can be computationally expensive and difficult to parallelise .",
    "the problem is simplified significantly in the case of grid - aligned anisotropies , which are typical in atmospheric- and ocean - modelling applications .",
    "it has long been known that if the problem is anisotropic in one direction only , this can be dealt with effectively by either adapting the smoother or coarsening strategy ( see e.g. @xcite and also the discussion for simple anisotropic model problems in @xcite ) .",
    "both methods can be combined as for example discussed in @xcite where the solution of two dimensional anisotropic problems with grid - aligned anisotropies is studied . by using line - relaxation in the @xmath68-direction together with semi - coarsening in the @xmath99-direction only , the multigrid solver is robust with respect to anisotropies in both the @xmath99- and @xmath68-direction as long as they are grid - aligned . in the following",
    "we will refer to multigrid algorithms which combine horizontal semi - coarsening with vertical line relaxation in the strongly coupled direction as _ tensor product multigrid _ ( tpmg ) methods ( both in 2d and in 3d ) .    in @xcite",
    "the convergence of such a tensor - product multigrid solver for elliptic equations of the form @xmath100 in a two dimensional domain @xmath101\\times[0,1]$ ] is analysed under the assumption that the coefficients in the diagonal @xmath78 matrix can be factorised , i.e.    2 _ r(r , x)&=_r^r(r)_r^x(x ) , & _",
    "x(r , x)&=_x^r(r)_x^x(x).[eqn : alphaxrfac ]    the authors show that the tensor product multigrid algorithm applied to this problem converges uniformly provided the standard multigrid algorithm with point relaxation and uniform coarsening converges uniformly for one dimensional ( horizontal ) operators of the form @xmath102 where the positive values @xmath103 are the eigenvalues of the vertical galerkin matrices .",
    "in particular , they analyse the strongly anisotropic case of @xmath104 , which arises for example in the case of a polar grid on a disk with a ( small ) hole at the origin .      based on these observations , we propose two approaches for solving the pressure correction equation in ( [ eqn : helmholtzvector ] ) . in both cases",
    "we use an iterative method such as a richardson iteration or bicgstab and precondition it with the tensor - product multigrid algorithm .",
    "[ [ tensor - product - multigrid - with - full - non - factorising - profiles - operatornametpmgoperatornamefull ] ] tensor - product multigrid with full , non - factorising profiles ( @xmath105 ) + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    often the profiles encountered in atmospheric flow simulations only factorise approximately .",
    "although the theory in @xcite applies only if the coefficient functions @xmath106 and @xmath107 can be written as the product of a vertical and a horizontal function as in ( [ eqn : alphaxrfac ] ) , this assumption is not used anywhere in the implementation .",
    "our numerical experiments demonstrate that good convergence can be achieved even in the non - factorising case where we use the full operator in the multigrid preconditioner .",
    "[ [ tensor - product - multigrid - with - approximately - factorised - profiles - operatornametpmgoperatornameotimes ] ] tensor - product multigrid with approximately factorised profiles ( @xmath108 ) + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    given a set of profiles , we explicitly construct an approximate factorisation and use the resulting operator in the multigrid preconditioner ; we apply a matrix - free approach , where the local stencil is reconstructed on - the - fly from the profiles . depending on the quality of the factorisation , this may lead to a slight increase in the number of iterations of the underlying iterative solver .",
    "however , in terms of computational cost this increase is usually offset by a reduction in the amount of data that needs to be transferred from main memory .",
    "as the algorithm is memory bound , this will translate directly into performance gains .",
    "if the profiles factorise , for each horizontal cell and edge , @xmath109 entries which describe the horizontal coupling need to be stored . in the vertical direction",
    "four vectors of length @xmath0 are required _ for the entire grid_. hence , in this case the matrix can be reconstructed from @xmath110 values where @xmath111 is the number of horizontal grid cells .",
    "this should be compared to @xmath112 data transfers for constructing the matrix in the @xmath105  approach .",
    "we also prove formally in the following section that the richardson iteration with @xmath108  preconditioner converges if the non - factorising part of the operator is small .",
    "the convergence theory for factorising profiles in a spherical shell is a straightforward generalisation of the two dimensional case in @xcite and is written down in detail in @xcite based on the multigrid convergence theory in @xcite . in the following we outline the proof for the pressure correction in ( [ eqn : helmholtzvector ] ) .",
    "this is done in two steps : we first argue that if the profiles factorise and the advection term is dropped , the resulting symmetric positive definite equation can be solved efficiently with a tensor - product multigrid iteration .",
    "we then show that if this factorising operator is used as a preconditioner for a richardson iteration , the method converges also for the non - factorising equation provided the non - factorising contribution is sufficiently small .",
    "consider the following pde in the spherical shell @xmath79 $ ] : @xmath113+\\beta(r,{\\ensuremath{\\hat{{\\ensuremath{\\boldsymbol{r } } } } } } ) u(r,{\\ensuremath{\\hat{{\\ensuremath{\\boldsymbol{r } } } } } } ) = f(r,{\\ensuremath{\\hat{{\\ensuremath{\\boldsymbol{r}}}}}})\\ ] ] with @xmath114 $ ] , @xmath115 .",
    "this should be compared to ( [ eqn : helmholtzvector ] ) ; for simplicity we do not consider the vertical advection term in this section , as it may in general destroy the positive definiteness of the problem .",
    "however , for high horizontal resolution this term is small and can be treated as a perturbation .",
    "we further assume that the @xmath116 matrix @xmath117 and the function @xmath118 have the following form that factorises into the product of a horizontal and a vertical function :    2 & =    _ r(r,@xmath119 ) & 0 + 0 & _ ( r,@xmath119 )    =    _ r^r(r ) _",
    "r^(@xmath119 ) & 0 + 0 & _ ^r ( r)_^(@xmath119 )    , & ( r,@xmath119 ) & = ^r(r)^(@xmath119 ) .",
    "we also require that @xmath120 , which is satisfied for all factorisations that we use in our numerical experiments .",
    "the @xmath78 matrix @xmath121 is required to be symmetric positive definite and we assume @xmath122 .    to discretise the problem , we choose finite element spaces @xmath123 over @xmath124 and @xmath125 over @xmath126 $ ] and tensorise them to obtain the product space @xmath127 over @xmath128 .",
    "we write @xmath129 and @xmath130 . for any two functions @xmath131 , @xmath132 in @xmath133",
    "the bilinear form @xmath134 associated with the operator @xmath135 in can be expressed in terms of the bilinear forms @xmath136 as @xmath137 using the kronecker product , the galerkin - matrix representation @xmath138 of the bilinear form @xmath139 can then be expressed in terms of the galerkin matrices of the bilinear forms in ( [ eqn : bilinearformstp ] ) , i.e. @xmath140 here @xmath141 correspond to @xmath142 , @xmath143 and @xmath144 respectively and describe the vertical derivative- and mass- matrices .",
    "analogously the derivative and mass matrix in the horizontal direction are described by @xmath145 , which correspond to @xmath146 and @xmath147 .    to use the tensor - product multigrid approach , we further assume that there is a nested sequence @xmath148 of finite element spaces over @xmath124 , where the subscript @xmath149 denotes the multigrid level ; for the icosahedral and cubed sphere grid this hierarchy naturally exists .",
    "we then use @xmath150 to discretise the full three dimensional problem on the multigrid level @xmath149 , i.e. we do not coarsen in the vertical direction .",
    "the line smoother then corresponds to collectively relaxing all degrees of freedom in each of the @xmath0-dimensional subspaces @xmath151 where @xmath152 are the nodal basis functions on level @xmath149 .",
    "the two - dimensional prolongation @xmath153 and restriction @xmath154 naturally induce intergrid transfer operators between the three dimensional spaces @xmath155 and @xmath156 by @xmath157 , @xmath158 . on each multigrid level",
    "the matrix @xmath159 can be constructed recursively using the galerkin product @xmath160 and it is easy to see that @xmath159 and the ( block-)smoother @xmath161 can be written as @xmath162 in the case of weighted block - jacobi relaxation , for example , the matrices @xmath163 and @xmath164 are the weighted diagonals of @xmath165 and @xmath166 .",
    "one v - cycle of the tensor product multigrid algorithm can now be written down compactly as follows .",
    "on the finest level @xmath167 , this v - cycle is applied to the right hand side @xmath168 of the original problem until the residual error is reduced below a certain tolerance .",
    "we typically choose the numbers of smoothing steps to be @xmath169 and @xmath170 , for @xmath171 , and @xmath172 on the coarsest grid . to simply apply a few steps of the smoother on the coarsest grid",
    "is sufficient because the cfl condition ensures that the system matrix @xmath173 on the coarsest grid is dominated by the mass matrix term @xmath174 and thus well - conditioned .",
    "[ [ reduction - of - the - theory - to - two - dimensions ] ] reduction of the theory to two dimensions + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the crucial idea in @xcite is now that it is possible to construct a set of @xmath0 invariant @xmath175-dimensional subspaces such that the convergence of the tensor product multigrid method for the problem in @xmath176 can be analysed by independently studying the convergence of a standard multigrid algorithm in each of these subspaces over @xmath177 .",
    "this can be seen as follows : because both @xmath178 and @xmath179 are positive definite , there exists an eigenbasis @xmath180 , @xmath181 , of @xmath125 and a corresponding set of strictly positive eigenvalues @xmath103 such that    2 ( ^2 a^r+b^r)_j^r & = _ j m^r_j^r , & m^r_j^r,_k^r= _ j , k j , k\\{1,  ,n}. [ eqn : eigenvalueequation ]    it follows from simple identities for the inner product on tensor product spaces that @xmath182 and so the subspaces spanned by the different @xmath183 are @xmath159-orthogonal , with a similar property for the smoother matrix @xmath184 . as we do not coarsen in the vertical direction , the intergrid operators @xmath185 and @xmath186 do not mix different subspaces . for each @xmath187",
    "the space @xmath188 is trivially isomorphic to @xmath189 and each of the @xmath0 independent subspaces corresponds to a two dimensional problem on @xmath124 with the following matrix representation of the linear operator and smoother :    2 a^_,j & ^2 a_^+ _ j m_^ , & w^_,j & ^2 w^a,_+ _ j w^m,_.    in particular , @xmath190 is the galerkin matrix which is obtained from discretising the bilinear form @xmath191 on @xmath189 .",
    "this bilinear form is the weak formulation of the following two dimensional operator : @xmath192    [ [ convergence - of - two - dimensional - multigrid ] ] convergence of two dimensional multigrid + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    according to theorem 10.7.15 in @xcite , the multigrid v - cycle converges for each of the two dimensional operators @xmath193 , @xmath194 if there exists a @xmath195 such that the smoothing property @xmath196 and the approximation property @xmath197 are satisfied on all levels @xmath198 .",
    "the smoothing property ( [ eqn : smoothingproperty2d ] ) is automatically satisfied for ( sufficiently damped ) point jacobi and sor smoothers ( remark 4.6.5 in @xcite ) . to see this ,",
    "denote the matrix consisting only of the diagonal entries of @xmath199 by @xmath200 and use @xmath201 , i.e. weighted point jacobi relaxation .",
    "the relaxation parameter is chosen such that @xmath202 where @xmath203 is the spectral norm .",
    "then ( [ eqn : smoothingproperty2d ] ) follows by definition from the equivalence @xmath204 @xmath205 applied to @xmath206 .",
    "a proof of the approximation property is significantly harder and we will not give it here ( see lemma 10.7.8 and remark 10.7.13 in @xcite ) .",
    "it depends on some minimal regularity assumptions on the profiles @xmath207 and @xmath208 .",
    "the constant @xmath195 may depend on the contrast , i.e. the maximum variation of the profiles .",
    "we stress again that we use quasi - uniform grids for the horizontal discretisation ( see the review in @xcite for a discussion of grids considered in meteorological application ) .",
    "in contrast to latitude - longitude grids , where the convergent grid lines near the pole introduce an additional horizontal anisotropy , the ratio between the smallest and largest grid spacing is bounded from below in the grids we consider .",
    "hence the simple block - jacobi and block - sor smoothers which relax all degrees of freedom in one vertical column simultaneously will be efficient and no additional horizontal plane smoothing or selective semi - coarsening as described in @xcite is required .",
    "as the two dimensional equations are solved on the unit sphere , the operator @xmath193 could become near - singular if @xmath209 .",
    "however , it is easy to see that this is not the case . as noted in section [ sec : linearequation ] we require the scaling @xmath210 to keep the courant number fixed as the horizontal resolution increases .",
    "therefore the second order term in ( [ eqn : cont2doperator ] ) is of order @xmath211 and hence the relative importance of the two terms in ( [ eqn : cont2doperator ] ) is independent of grid resolution .",
    "it follows that all the eigenvalues @xmath103 of ( [ eqn : eigenvalueequation ] ) are of order 1 .",
    "it is a reasonable assumption that the profiles @xmath207 , @xmath208 are `` well - behaved '' in the sense that they are dominated by large scale variations due to global weather systems , small scale phenomena such as strong local variations carry substantially less energy . in this case",
    "we expect the spectrum of @xmath193 to be bounded from above and below by two constants which are independent of @xmath212 .",
    "[ [ convergence - of - three - dimensional - multigrid ] ] convergence of three dimensional multigrid + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    as argued above , the three dimensional problem can be decoupled into a set of @xmath0 two - dimensional problems . due to the particular form of the smoother and of the prolongation / restriction matrices ,",
    "it is in fact easy to verify that the smoothing property and the approximation property    2 a^ _ & w^ _ & 0 ( a^_+1)^-1 - p_^h ( a^_)^-1 r_^h & c_a ( w^_+1)^-1 ,    for the tensor product multigrid algorithm for the original 3d problem on @xmath128 follow directly from the respective properties ( [ eqn : smoothingproperty2d ] ) and ( [ eqn : approximationproperty2d ] ) for the 2d problems on @xmath54 , for all @xmath194 .",
    "let us assume that ( [ eqn : smoothingproperty2d ] ) and ( [ eqn : approximationproperty2d ] ) are satisfied , for all @xmath194 , and let @xmath213 denote the iteration matrix for one step of the tensor product multigrid v - cycle defined above , i.e. @xmath214 where @xmath215 is the exact solution of the equation @xmath216 .",
    "then the convergence rate @xmath217 independent of @xmath212 , where @xmath218 is the energy norm induced by @xmath138 .",
    "this is the main result given and proved for the two dimensional case in ( * ? ? ?",
    "* theorem 2 ) . as we have seen above",
    ", the proof extends directly also to three dimensions and to our pressure correction problem here . in that case",
    "the assumptions of the theorem are satisfied as discussed above .",
    "we now assume that the matrix @xmath219 can be written as the sum of a perfectly factorising symmetric positive definite matrix @xmath138 and a small correction @xmath220 , namely @xmath221 .",
    "we quantify the deviation from perfect factorisation by @xmath222 and assume that @xmath223 .",
    "we also assume that the theory in section [ sec : prooffactorising ] applies and the multigrid iteration for the factorising operator @xmath138 converges , i.e. the error is reduced by a factor @xmath224 in every multigrid v - cycle .",
    "the richardson iteration for the full operator @xmath219 preconditioned with @xmath27 multigrid v - cycle cycles for @xmath138 can then formally be written as @xmath225\\left({{a^{\\otimes}}}\\right)^{-1}\\left(\\mathbf{f}-a\\mathbf{u}^{(k)}\\right).\\ ] ] then at every step the error @xmath226 to the exact solution @xmath227 is reduced by a factor @xmath228\\left({{a^{\\otimes}}}\\right)^{-1}a||_{{{a^{\\otimes } } } } } \\nonumber\\\\ & \\ \\le \\ { ||\\left({{a^{\\otimes}}}\\right)^{-1}a-{\\operatorname{id}}||_{{{a^{\\otimes}}}}}+{||{{m^{\\otimes}}}||_{{{a^{\\otimes}}}}}^\\mu{||\\left({{a^{\\otimes}}}\\right)^{-1}a||_{{{a^{\\otimes } } } } } \\",
    "\\delta + ( 1 + \\delta ) ( \\rho^\\otimes_a)^\\mu .",
    "\\label{eqn : richardsonconvergencerate}\\end{aligned}\\ ] ] thus , for an arbitrary @xmath223 the convergence rate @xmath229 is less than 1 , provided the number of v - cycles @xmath230 . on the other hand , if we only apply one v - cycle ( @xmath231 ) , then a convergence rate @xmath232 can still be guaranteed provided @xmath233 .",
    "similar results can also be proved for the convergence of krylov solvers , such as bicgstab , preconditioned with @xmath27 multigrid v - cycle cycles for @xmath138 .",
    "in practise , and as we demonstrate in the following , the tensor product preconditioners will be efficient for a wider range of problems not covered by the formal theory .",
    "we now describe the discretisation and dune implementation of the solvers we used in our numerical experiments .",
    "for simplicity we use a simple finite volume discretisation for all numerical experiments in this work .",
    "more complex schemes such as mimetic mixed finite elements are also currently under consideration for the development of dynamical cores @xcite and might require the solution of the equation in a different pressure space , such as higher order dg space . however , the basic ideas described in this work can still be applied .",
    "grids used in meteorological applications ( and also in many ocean models @xcite ) usually have a tensor - product structure .",
    "they consist of a semi - structured two dimensional horizontal grid on the surface of the sphere and a one - dimensional vertical grid which is often graded to achieve higher resolution near the surface .",
    "in particular each three dimensional grid cell @xmath234 can be uniquely identified by the corresponding horizontal cell @xmath235 and a vertical index @xmath236 .",
    "this tensor - product structure in itself has important implications for the performance of any implementation : while it might be necessary to use indirect indexing for the horizontal grid , the vertical grid can always be addressed directly . as typically the number of vertical levels is large with @xmath237",
    ", the cost of indirect addressing in the horizontal direction can be `` hidden '' @xcite , a phenomenon which we have confirmed numerically for our solvers in section [ sec : indirectaddressing ] .",
    "furthermore fields can be stored such that the levels in each column are stored consecutively in memory , which leads to efficient cache utilisation ( however , as discussed in @xcite a different memory layout has to be used on gpu architectures where the vertically - consecutive storage would prevent coalesced memory access in the tridiagonal solve ) . to be able to use the geometric multigrid solvers described in this work",
    ", we also assume that the horizontal grid has a natural hierarchy ; this is true for the icosahedral grids which are used in our numerical tests where each triangular coarse grid cell consist of four smaller triangles on the next - finer multigrid level .",
    "in contrast to a simple longitude - latitude grid , these semi - structured grids have no pole - problem , i.e. the ratio between the size of the largest and smallest grid spacing is bounded .",
    "this implies that there is no additional horizontal anisotropy which would further complicate the construction of a solver ( however , as has been shown in @xcite , the tensor - product multigrid approach can still be applied for longitude - latitude grids if the horizontal coarsening strategy is adapted appropriately ) .    in the finite volume discretisation",
    "any continuous field @xmath238 is approximated by its average value in a grid cell .",
    "in particular , for each _ horizontal _ grid cell @xmath235 we store one vector @xmath239 of length @xmath0 representing the field in the vertical column .",
    "in this cell the discrete equation ( [ eqn : algebraicequation ] ) for the @xmath0-vector @xmath239 can be written as @xmath240 where the sum runs over all horizontal neighbours @xmath241 of @xmath235 . in this expression @xmath242 and @xmath243",
    "are @xmath244 tridiagonal- and diagonal matrices of the form    2 a_t & = ( _ t,_t,_t ) , & a_tt & = ( _ tt ) .",
    "[ eqn : tridiagonalsystem ]    both matrices can be reconstructed on - the - fly from a number of scalar quantities , which are obtained from a discrete approximation of the profiles in ( [ eqn : profiles ] ) and geometric factors .",
    "this reduces the amount of main memory access , in particular if the factorising profiles in the @xmath108  preconditioner are used . for each horizontal cell",
    "@xmath235 the explicit expressions of the diagonals @xmath245 , @xmath246 and upper- and lower- subdiagonals @xmath247 , @xmath248 depend on whether the profiles can be factorised or not and are given explicitly in the next section . a block - sor iteration with overrelaxation factor @xmath249",
    "can then be written as @xmath250 and requires a tridiagonal solve in each vertical column to apply the inverse of the matrix @xmath242 to the residual .",
    "this can be implemented using the thomas algorithm @xcite .",
    "all code was implemented using the dune library @xcite , which provides a set of c++ classes for solving pdes using grid based methods . in particular",
    "it provides interfaces to ( parallel ) grid implementations such as alugrid @xcite and uggrid @xcite .",
    "the implementation of the grids is separated from data which is attached to the grid by the user via mapper functions between different grid entities ( cells , edges , vertices ) and the local data arrays . in our case",
    "we used the dune - grid module to implement a two dimensional host grid and then attached a whole column of length @xmath0 to each horizontal grid cell @xmath235 .",
    "we represent the matrix as follows : in the non - factorising case ( @xmath105 ) , we store a vector @xmath251 of length @xmath0 at each horizontal cell to represent the zero order term , two vectors @xmath252 and @xmath253 of length @xmath254 to represent the vertical diffusion and advection terms , and one vector @xmath255 of length @xmath0 at each horizontal edge @xmath256 .",
    "the explicit form of these vectors is obtained by a standard finite volume discretisation of the problem and is written down in equation ( [ eqn : hatfunctionsnonfac ] ) in the appendix .",
    "the vectors @xmath245 , @xmath247 , @xmath248 and @xmath246 in ( [ eqn : tridiagonalsystem ] ) are @xmath257 in the factorising case ( @xmath108 ) it is only necessary to store _ scalars _",
    "@xmath258 , @xmath259 , @xmath260 and @xmath261 on the horizontal cells and edges .",
    "in addition to this , four vectors of length @xmath0 and @xmath254 ( @xmath262 , @xmath263 , @xmath264 and @xmath265 ) which arise from the vertical discretisation need to be stored once for the entire grid .",
    "the explicit form of these quantities is given in ( [ eqn : hatfunctionsfac ] ) in the appendix .",
    "similarly to ( [ eqn : tridiagonalnonfactorising ] ) the matrix entries in ( [ eqn : tridiagonalsystem ] ) can be calculated on the fly as @xmath266 the scalars @xmath258 , @xmath259 , @xmath260 and @xmath261 only need to be read once per vertical column and the associated cost can be hidden together with the cost of indirect addressing on the horizontal grid for large enough @xmath0 . moreover , the vectors @xmath262 , @xmath263 , @xmath264 and @xmath265 require only a small amount of memory and can be cached . in summary , the cost of memory access for the matrix is likely to be significantly smaller than the cost of accessing field vectors such as @xmath239 and @xmath247 when solving the tridiagonal system in ( [ eqn : blockjacobi ] ) or in the matrix vector product .",
    "the dune - grid interface provides iterators over the horizontal grid cells and over the neighbours of each cell . to implement for example the sparse matrix vector product ( spmv ) in ( [ eqn : columnequation ] )",
    "we iterate over all horizontal grid cells @xmath235 , and then in each cell we loop over the edges @xmath256 for all neighbours @xmath267 to read the profiles stored on the cells and edges from memory and construct the matrices @xmath242 and @xmath243 .",
    "these are then applied to the local vectors @xmath239 and @xmath268 to evaluate @xmath269 , which requires inner loops over the vertical levels .",
    "of all grids that are currently available through the dune interface we found that only alugrid can be used to represent a two - dimensional sphere embedded in three dimensional space .",
    "unfortunately the scalability of alugrid is very limited because in a parallel implementation the entire grid is stored on each processor .",
    "alternatively we used a three dimensional uggrid implementation for a thin spherical shell consisting of one vertical layer to represent the unit sphere .",
    "based on the coarsest grid , finer multigrid levels can be constructed by refinement in the horizontal direction only . any geometric quantities in this thin three dimensional grid can then be related to the corresponding values on the two dimensional grid by simple scaling factors .",
    "we implemented both a gnomonic cubed sphere grid @xcite and an icosahedral grid , for which the grid points are projected onto the sphere , and all numerical results reported in this work were obtained with the icosahedral grid .    as is typical in atmospheric applications ,",
    "parallel domain decomposition is in the horizontal direction only .",
    "as the dune host grids that we used are already inherently parallel , parallelisation of the code was straightforward by calling the relevant halo exchange routines when necessary .",
    "load balancing was achieved by choosing the problem size such that the number of cells on the coarses level is identical to the number of processors and each processor `` owns '' one coarse grid cell and the corresponding child cells .",
    "while at first sight this might cause a problem for large core counts because the coarsest level still has a relatively large number of degrees of freedom and the multigrid hierarchy is very shallow , it turns out that the zero order term in the helmholtz equation ( [ eqn : helmholtzvector ] ) averts potential problems .",
    "this is because relative to the zero order term the importance of the horizontal diffusion term decreases with a factor of four on each coarse level , and so after a small number of coarsening steps the problem is well conditioned and can be solved by a very small number of smoothing iterations .",
    "an alternative and more physical explanation is that any interactions in the continuous pde in ( [ eqn : helmholtzvector ] ) are exponentially damped with an intrinsic length scale @xmath270 and hence it is not necessary to coarsen the grid beyond this scale . this has been confirmed numerically for a simplified test problem in @xcite , where it has been shown that as little as four multigrid levels still give very good convergence for typical grid spacings and time step sizes . in the parallel scaling tests in this work",
    "we typically used 6 or 7 multigrid levels and one iteration of the smoother to solve the coarse grid problem .",
    "in the following we study the performance of the two tensor - product preconditioners @xmath105  and @xmath108  described in section [ sec : tpmgs ] applied to two test cases in atmospheric flow simulation .",
    "we confirm the optimality and robustness of @xmath105  even for non - factorising profiles , compare the performance of the two variants and study their parallel scalability .",
    "all runs ( including the sequential tests ) were carried out on the phase 3 configuration of the hector supercomputer , which consists 2816 compute nodes with two 16-core amd opteron 2.3ghz interlagos processors each .",
    "the entire cluster contains 90,112 cores in total .",
    "the code was compiled with version 4.6.3 of the gnu c compiler .    unless stated otherwise we always used 6 multigrid levels with two vertical line - sor pre- and post- smoothing steps on each level ( @xmath271 ) ; the overrelaxation factor in the smoother was set to @xmath272 .",
    "one smoother iteration is used to solve the coarse grid problem .",
    "we use linear interpolation to prolongate the solution to the next - finer grid ( @xmath273 ) . the right hand side , which in each cell represent a cell integral of a field , is restricted to the next - coarser level ( @xmath274 ) by summing the fine grid values of all four fine grid cells comprising the coarse grid cell .",
    "recall that these integrid - operations only require interpolation and summation in the horizontal direction .",
    "the tolerance in the iterative solver was set to @xmath275 , i.e. we iterate until the residual has been reduced by at least five orders of magnitude .",
    "the number of vertical levels was set to @xmath276 , which is typical for current meteorological applications .",
    "we note , however , that all runtimes should be directly proportional to @xmath0 ( and this is confirmed in the following section ) .      while data in one vertical column is stored consecutively in memory and can be addressed directly , in general indirect addressing has to be used in the horizontal directions .",
    "however , as the horizontal lookup is only required once per column , the relative penalty for this will be very small provided @xmath0 is large enough . as discussed in @xcite , in this case the overhead from indirect addressing",
    "can be `` hidden '' behind work in the vertical direction . to verify this we ran our solver with two different dune grid implementations and measured the time per iteration for different numbers of vertical levels .",
    "we expect this time to depend on @xmath0 as follows @xmath277 where @xmath278 is the overhead of indirect addressing and depends on the grid implementation . the constant @xmath279 encapsulates any other work which is only done once per column and both @xmath279 and the slope @xmath280 are independent of the horizontal grid .",
    "figure [ fig : nzdependency ] shows the results for the alugrid and uggrid implementation and confirms the linear dependency in ( [ eqn : nzdependency ] ) .     of vertical levels for two grid implementations ( uggrid in red , open squares and alugrid in blue , filled circles ) on an icosahedral grid ; results for shown both for the @xmath105  ( dashed lines ) and @xmath108  ( solid lines ) preconditioner . ]",
    "as can be seen from this plot , for both preconditioners @xmath105  and @xmath108  the overhead from indirect addressing @xmath278 and the additional overhead @xmath279 together are at the order of less than @xmath2 as soon as @xmath281",
    ". incidentally both dune grid implementations that we tested are equally efficient .",
    "we stress that in both grids data in adjacent vertical columns is not necessarily stored consecutively in memory .",
    "not surprisingly , the slope @xmath280 is larger for the more expensive @xmath282 preconditioner . the results in this section also confirm that performance tests carried out on a directly addressed horizontal grid , such as the results in @xcite , can be generalised to indirectly addressed grids .",
    "we first test our solver with the profiles from a simplified meteorological test problem which corresponds to a balanced atmosphere with constant buoyancy frequency and zonal flow with one jet in each hemisphere .",
    "the advantage of this test case is that the deviation of the atmospheric profiles from a perfect factorisation can be controlled by varying a single parameter . in @xcite",
    "it is shown that under the assumption that the velocity field points in the longitudinal direction and the buoyancy frequency @xmath283 defined in ( [ eqn : buoyancyfrequency ] ) is constant , a solution of the euler equations is given by @xmath284^{\\gamma}{{e^{{{{\\mathcal{s}}}}}(\\phi)}}{{e^r(r ) } } , &    u({\\ensuremath{\\hat{{\\ensuremath{\\boldsymbol{r}}}}}},r ) & = u_{{{\\mathcal{s}}}}(\\phi )    \\end{aligned }    \\label{eqn : balancedflow}\\ ] ] where the functions @xmath285 and @xmath286 are defined as    2 e^()&= , & e^r(r)&= .    in the horizontal direction the profiles only vary in the latitudinal direction @xmath287 $ ] .",
    "the parameter @xmath288 is related to the buoyancy frequency by @xmath289 with @xmath290 .",
    "the function @xmath291 is related to the velocity field @xmath292 as @xmath293 with angular velocity @xmath294 . for our numerical experiments",
    "we choose the velocity such that it corresponds to two jets with peak velocity @xmath295 in the mid latitudes ( @xmath296 , @xmath297 ) : @xmath298    \\label{eqn : velocityfield}\\ ] ] as plotted together with the corresponding @xmath291 in figure [ fig : exnerpressure ] .     and jet function @xmath299 defined in eqns .",
    "( [ eqn : jetfunction ] ) and ( [ eqn : velocityfield ] ) for @xmath300 ( @xmath301 ) .",
    "right : exner pressure @xmath8 and relative difference @xmath302 in the @xmath303-plane for the same value of @xmath283 .",
    "the height above ground is measured in units of the depth of the atmosphere . ]     and jet function @xmath299 defined in eqns .",
    "( [ eqn : jetfunction ] ) and ( [ eqn : velocityfield ] ) for @xmath300 ( @xmath301 ) .",
    "right : exner pressure @xmath8 and relative difference @xmath302 in the @xmath303-plane for the same value of @xmath283 .",
    "the height above ground is measured in units of the depth of the atmosphere . ]",
    "if we fix the reference pressure and temperature to physically realistic values @xmath304 and @xmath305 , the only free parameter in ( [ eqn : balancedflow ] ) is the buoyancy frequency .",
    "in particular if @xmath283 is identical to @xmath306 , i.e. @xmath307 , the first term in the expression for the exner pressure in ( [ eqn : balancedflow ] ) vanishes and all profiles factorise exactly .    in the following we present numerical results for a range of buoyancy frequencies between @xmath308 and @xmath300 .",
    "as a preconditioner we use both a multigrid algorithm with the full model operator and the tensor - product multigrid algorithm with an approximate factorisation of the exner pressure @xmath309 which reduces to the expression in ( [ eqn : balancedflow ] ) for @xmath307 . both the exner pressure and the relative difference @xmath302 , which is an indicator of the quality of the factorisation , are plotted for @xmath300 in the @xmath310 plane in figure [ fig : exnerpressure ] . as can be seen from this figure",
    ", the relative difference between the profiles can be larger than 15% .    , blue columns and dashed curves ) and the approximate factorisation ( @xmath108 , hatched green columns and solid curves ) in ( [ eqn : facapproxbalancedflow ] ) . in all cases",
    "a problem with @xmath276 and @xmath311 total degrees of freedom was solved sequentially on hector . ]    , blue columns and dashed curves ) and the approximate factorisation ( @xmath108 , hatched green columns and solid curves ) in ( [ eqn : facapproxbalancedflow ] ) . in all cases",
    "a problem with @xmath276 and @xmath311 total degrees of freedom was solved sequentially on hector . ]    the time per iteration is shown in figure [ fig : titerniterbreakdown ] ( left ) for two grid implementations .",
    "both a preconditioned richardson iteration and bicgstab are used with one multigrid v - cycle as a preconditioner .",
    "it is important to note that bicgstab requires two applications of the preconditioner and two sparse matrix - vector products per iteration , while the richardson iteration only requires one of each , and not surprisingly the figure demonstrates that most of the time is taken up by the multigrid preconditioner in all cases .",
    "the number of iterations for each of the combinations is plotted in figure [ fig : titerniterbreakdown ] ( right ) for a range of @xmath288 .",
    "first of all we note the almost perfect robustness of the full preconditioner @xmath105  for this test problem where the profiles strongly deviate from the factorising case , but the convergence of preconditioned richardson iteration and preconditioned bicgstab are essentially not affected .",
    "the practically observed convergence rate for the v - cycle ( in the richardson iteration ) is around @xmath312 .",
    "this confirms the theoretical results in sections [ sec : prooffactorising ] and [ sec : proofnonfactorising ] .",
    "bicgstab converges in approximately half the number of iterations than richardson , as expected . in terms of time per iteration , the multigrid preconditioner with factorised profiles ( @xmath108 ) can be up to @xmath313 faster than the algorithm with non - factorising profiles ( @xmath105 ) .",
    "however , this comes at the expense of an increase in the number of iterations for larger values of @xmath288 that can be seen in figure [ fig : titerniterbreakdown ] ( right ) .",
    "while for the richardson iteration the increase is almost threefold if @xmath108  is used , this is much less dramatic for bicgstab where @xmath108  only requires twice as many iterations as @xmath105  for the largest @xmath288 .    finally , the total solution time is shown in figure [ fig : tsolvebalancedflow ] .    , dashed curves ) and the approximate factorisation ( @xmath108 , solid curves ) in ( [ eqn : facapproxbalancedflow ] ) .",
    "in all cases a problem with @xmath276 and @xmath311 total degrees of freedom was solved sequentially on one node of the hector supercomputer . ]",
    "as expected , the total solution time for solvers with @xmath108  preconditioner grows as @xmath288 increases .",
    "however , as the time per iteration is about 25% smaller for this preconditioner , for small @xmath288 the total solution time is also reduced by a similar factor .",
    "the most robust solver appears to be bicgstab , which gives the best overall performance for large @xmath288 , even with the factorising preconditioner @xmath108 .",
    "while the runs in the previous section were carried out under idealised and not necessarily realistic conditions , we also tested our solver for profiles obtained from common meteorological test cases .",
    "we first obtained the profiles @xmath8 , @xmath9 and @xmath10 from an aquaplanet run of the met office unified model .",
    "while these fields contain significantly more variation than the idealised profiles described in section [ sec : balancedflow ] and also describe phenomena such as convection near the ground and baroclinic instabilities , they are largely `` well behaved '' in the sense that most of them can be factorised approximately into a horizontal and a vertical variation . to quantify this further , we plot for each of the profiles the average , minimum and maximum over the horizontal grid on each vertical level in figure [ fig : verticalprofiles ] ( right ) .",
    ", @xmath314 , @xmath315 and @xmath316 ) .",
    "the horizontal variation is also represented by gray bands between the minimum and maximum value on each grid level ( dashed curves ) .",
    "right : zero - order term @xmath317 on the lowest grid level .",
    "the horizontal variation in the field is at the order of @xmath318 . ]    , @xmath314 , @xmath315 and @xmath316 ) .",
    "the horizontal variation is also represented by gray bands between the minimum and maximum value on each grid level ( dashed curves ) .",
    "right : zero - order term @xmath317 on the lowest grid level .",
    "the horizontal variation in the field is at the order of @xmath318 . ]    for most profiles the horizontal variation is small and the average value decays exponentially with height ; see for example figure [ fig : verticalprofiles ] ( left ) , which shows the profile @xmath118 on the lowest grid level .",
    "the only exception is @xmath319 which shows significant horizontal variation in the lower atmosphere .",
    "this is mainly due to the fact that , as can be seen from the explicit expressions in ( [ eqn : profiles ] ) , this profile contains the buoyancy frequency and hence vertical derivatives of the potential temperature , which can vary significantly from column to column due to convection in the lower atmosphere .",
    "we found that for these more typical profiles the factorising preconditioner @xmath108  causes both solvers to diverge .",
    "an easy fix for this is to factorise all profiles except @xmath316 .",
    "we denote the resulting preconditioner with partial factorisation , where we keep the full non - factorising profile for @xmath316 , as @xmath320 .",
    "as table [ tab : titeraquaplanet ] demonstrates , this increases the time per iteration by just over @xmath321 relative to the fully factorising case ( @xmath108 ) , but it is still significantly smaller than in the non - factorised case ( @xmath105 ) .",
    "[ tab : titeraquaplanet ]    .time per iteration and speedups relative to @xmath105  for different solvers and preconditioners . in all cases a problem with @xmath276 and @xmath311 total degrees of freedom was solved sequentially on one node of hector using the alugrid implementation .",
    "[ cols= \" < , > , > , > , > , > , > \" , ]      in addition to studying the sequential performance of the solvers , and in particular ensuring that they are algorithmically efficient , it is crucial to guarantee their parallel scalability on large computer clusters . for this",
    "we carried out scaling tests of our solvers for the balanced flow testcase described in section [ sec : balancedflow ] with @xmath322 ; in contrast to the previous runs we always used 7 multigrid levels so that on the coarsest level each processor stores one vertical column of data . in figure",
    "[ fig : weakscaling ] ( left ) the weak scaling of the time per iteration on the hector supercomputer is shown for up to 20,480 cores , the largest problem that was solved has just over @xmath323 degrees of freedom .",
    "we find that the number of iterations does not increase with the core count , and even drops in some cases .",
    "the richardson solver requires seven iterations to reduce the residual by five orders of magnitude for both preconditioners , whereas bicgstab requires 4 ( @xmath108 ) and 3 ( @xmath105 ) iterations for the same residual reduction .",
    "consequently the total solution time in figure [ fig : weakscaling ] ( right ) shows the same excellent weak scaling .",
    "in this work we discussed several multigrid preconditioners for anisotropic problems in flow simulations in `` flat '' domains with high aspect ratio . the algorithms are based on the tensor - product multigrid approach proposed and analysed for two - dimensional problems with separable coefficients in @xcite .",
    "we extended the method and its analysis to three dimensional problems and via a perturbation argument also to non - separable coefficients .",
    "we demonstrated the excellent performance of tensor - product multigrid for two model pdes arising in semi - implicit semi - lagrangian time stepping in atmospheric modelling .",
    "the numerical tests confirm the theoretically predicted optimality and effectivity of the method .",
    "the practically observed convergence rates are around @xmath312 .",
    "the tests also show that under certain conditions a preconditioner based on an approximate factorisation of the atmospheric profiles can reduce the total solution time .",
    "we found this to be the case both for an idealised flow scenario and for a more realistic aquaplanet test case .",
    "we also demonstrated the excellent weak parallel scaling on up to 20,480 cores of the hector supercomputer .",
    "overall our work demonstrates that bespoke multigrid preconditioners are highly efficient for solving the pressure correction equation encountered in nwp models .",
    "there are several ways to further improve this work : so far all tests have been carried out without any orography .",
    "it is known that steep gradients can lead to deteriorating performance of the non - linear iteration and we plan to study this by looking at the full non - linear solve for more realistic model problems . for simplicity",
    "we used a finite volume discretisation , but more advanced approaches such as higher - order mixed finite elements can also be used in this framework .",
    "this will require the solution of a suitable pressure correction equation in higher order fem spaces .",
    "the parallel performance can also be further improved by , for example , overlapping calculations and communications and strong scaling tests should also be carried out .",
    "finally , the performance gains from approximate factorisations of the matrix are expected to be significantly higher on gpu systems and hence on such architectures its use may be more justified and more efficient for a wider class of profiles .",
    "+ this work was funded as part of the nerc project on next generation weather and climate prediction ( ngwcp ) , grant numbers ne / j005576/1 and ne / k006754/1 .",
    "we gratefully acknowledge input from discussions with our collaborators in the met office dynamics research group and the gungho ! project , in particular tom allen , terry davies , markus gross and nigel wood .",
    "ian boutle kindly provided the aquaplanet um output used in section [ sec : resultsaquaplanet ] .",
    "we would like to thank all dune developers and in particular oliver sander for his help with extending the parallel scalability of the ug grid implementation .",
    "this work made use of the facilities of hector , the uk s national high - performance computing service , which is provided by uoe hpcx ltd at the university of edinburgh , cray inc and nag ltd , and funded by the office of science and technology through epsrc s high end computing programme .",
    "to use a finite volume discretisation we assume that the horizontal grid is divided into elements @xmath235 of area @xmath324 where the centre of each cell is denoted by @xmath325 .",
    "the vertical grid is defined by grid levels @xmath326 each three dimensional grid cell @xmath327 is defined by a cell @xmath235 of the horizontal grid and a vertical index @xmath38 , such that cell @xmath328 with @xmath329 is bound by @xmath330 and @xmath331 and we write @xmath332 .",
    "any continuous field @xmath238 can be approximated by its average value @xmath333 in a grid cell as @xmath334 where we have used @xmath335 as all degrees of freedom in one vertical column are stored consecutively in memory , we also introduce the vector @xmath239 with @xmath336 . then it is straightforward to write down the finite volume discretisation of the individual terms in ( [ eqn : helmholtzvector ] ) .",
    "after integration by part in the horizontal direction the first term becomes @xmath339 where the sum runs over the neighbours @xmath267 of the horizontal grid cell .",
    "@xmath340 is the length of the edge between the cells @xmath235 and @xmath267 and @xmath341 is an outward normal vector on this edge and tangential to the sphere .",
    "note that @xmath342 is a field that `` lives '' on the vertical faces of a three dimensional grid cell .",
    "the second term in ( [ eqn : discrdiffusion ] ) is treated similarly by a vertical integration by parts to obtain @xmath343 often the vertical boundary conditions are fixed by requiring that the vertical velocity is zero at the top and bottom of the atmosphere .",
    "this implies mixed boundary conditions of the form @xmath344 for the pressure . to simplify the discussion",
    ", we use homogeneous neumann boundary conditions , i.e. @xmath345 . because all degrees of freedom in a vertical column are relaxed simultaneously in our solver this should not have any impact on the performance .",
    "furthermore , due to the presence of the zero - order term in ( [ eqn : helmholtzvector ] ) both the individual tridiagonal systems in one column and the elliptic operator are non - singular .",
    "in ( [ eqn : verticaldiffusion ] ) neumann boundary conditions are enforced by setting @xmath346 and @xmath347 otherwise .",
    "the field @xmath348 is defined on the horizontal faces of each three dimensional grid cell .",
    "based on the results in the previous sections we we can now give explicit expressions for the quantities which are needed in ( [ eqn : tridiagonalnonfactorising ] ) and ( [ eqn : tridiagonalfactorising ] ) to construct the entries of the ( tri- ) diagonal matrices in ( [ eqn : tridiagonalsystem ] ) . in the most general case",
    "all profile functions depend on the radial coordinate @xmath68 and the horizontal coordinate @xmath350 . in this case",
    "define @xmath351      ( \\hat{\\alpha}_{{{{\\mathcal{s}}}}})_{tt',k } & \\equiv \\omega^2(r_{k+1}-r_{k } ) \\frac{|s_{tt'}|{\\ensuremath{\\boldsymbol{n}}}_{tt'}\\cdot({\\ensuremath{\\hat{{\\ensuremath{\\boldsymbol{r}}}}}}_{t'}-{\\ensuremath{\\hat{{\\ensuremath{\\boldsymbol{r}}}}}}_t)}{|{\\ensuremath{\\hat{{\\ensuremath{\\boldsymbol{r}}}}}}_{t'}-{\\ensuremath{\\hat{{\\ensuremath{\\boldsymbol{r}}}}}}_t|^2}(\\alpha_{{{{\\mathcal{s}}}}})_{tt',k } , &      ( \\hat{\\alpha}_{{{\\mathcal{s}}}})_{t , k } & \\equiv      \\omega^2\\sum_{t'\\in{\\mathcal{n}(t)}}(\\hat{\\alpha}_{{{\\mathcal{s}}}})_{tt',k}\\\\[1ex ]      ( \\hat{\\alpha}_r)_{t , k } & \\equiv \\omega^2 \\sigma_k|t| \\frac{2r_k^2}{r_{k+1}-r_{k-1 } } ( \\alpha_r)_{t , k } , &      ( \\hat{\\xi}_{r})_{t , k } & \\equiv \\omega^2\\sigma_k |t|\\frac{r_k^2(r_{k+1}-r_{k})}{r_{k+1}-r_{k-1}}(\\xi_{r})_{t , k }    \\end{aligned }    \\label{eqn : hatfunctionsnonfac}\\ ] ] which can all be precomputed .",
    "if the profile functions can be written as          in this case define @xmath352      ( \\hat{\\alpha}_{{{\\mathcal{s}}}}^r)_k & \\equiv ( r_{k+1}-r_{k } ) ( \\alpha_{{{\\mathcal{s}}}}^r)_k , &      ( \\hat{\\alpha}_{{{\\mathcal{s}}}}^{{{\\mathcal{s}}}})_{tt ' } & \\equiv \\omega^2 \\frac{|s_{tt'}|{\\ensuremath{\\boldsymbol{n}}}_{tt'}\\cdot({\\ensuremath{\\hat{{\\ensuremath{\\boldsymbol{r}}}}}}_{t'}-{\\ensuremath{\\hat{{\\ensuremath{\\boldsymbol{r}}}}}}_t)}{|{\\ensuremath{\\hat{{\\ensuremath{\\boldsymbol{r}}}}}}_{t'}-{\\ensuremath{\\hat{{\\ensuremath{\\boldsymbol{r}}}}}}_t|^2}(\\alpha_{{{{\\mathcal{s}}}}}^{{{{\\mathcal{s}}}}})_{tt'}\\\\[1ex ]      & & ( \\hat{\\alpha}_{{{\\mathcal{s}}}}^{{{\\mathcal{s}}}})_{t } & \\equiv      \\sum_{t'\\in{\\mathcal{n}(t)}}(\\hat{\\alpha}_{{{\\mathcal{s}}}}^{{{\\mathcal{s}}}})_{tt'}\\\\[1ex ]      ( \\hat{\\alpha}_r^r)_k & \\equiv \\sigma_k \\frac{2r_k^2}{r_{k+1}-r_{k-1 } } ( \\alpha_r^r)_k , &      ( \\hat{\\alpha}_r^{{{\\mathcal{s}}}})_t & \\equiv \\omega^2|t| ( \\alpha_r^{{{\\mathcal{s}}}})_t \\\\[1ex ]      ( \\hat{\\xi}_r^r)_k & \\equiv \\sigma_k \\frac{r_k^2(r_{k+1}-r_{k})}{r_{k+1}-r_{k-1}}(\\xi_r^r)_k , &      ( \\hat{\\xi}_r^{{{\\mathcal{s}}}})_t & \\equiv \\omega^2|t|(\\xi_r^{{{\\mathcal{s}}}})_t    \\end{aligned }    \\label{eqn : hatfunctionsfac}\\ ] ]            t.  davies , m.  j.  p. cullen , a.  j. malcolm , m.  h. mawson , a.  staniforth , a.  a. white , and n.  wood . a new dynamical core for the met office s global and regional modelling of the atmosphere .",
    ", 131(608):17591782 , 2005 .",
    "n.  wood , a.  staniforth , a.  white , t.  allen , m.  diamantakis , m.  gross , t.  melvin , c.  smith , s.  vosper , m.  zerroukat , and j.  thuburn . an inherently mass - conserving semi - implicit semi - lagrangian discretisation of the deep - atmosphere global nonhydrostatic equations . ,",
    "2013 . published online december 4th 2013 .",
    "j.  r. bates , f.  h.  m. semazzi , r.  w. higgins , and s.  r.  m. barros .",
    "integration of the shallow - water equations on the sphere using a vector semi - lagrangian scheme with a multigrid solver .",
    ", 118(8):16151627 , 1990 .",
    "a.  qaddouri and j.  ct .",
    "preconditioning for an iterative elliptic solver on a vector processor . in j.  palma , a.  sousa , j.  dongarra , and v.  hernndez , editors ,",
    "_ high performance computing for computational science ( vecpar 2002 ) _ , volume 2565 of _ lecture notes in computer science _ , pages 451455 .",
    "springer , berlin , 2003 .",
    "s.  d. buckeridge , m.  j.  p. cullen , r.  scheichl , and m.  wlasak . a robust numerical method for the potential vorticity based control variable transform in variational data assimilation . , 137(657):10831094 , 2011 .",
    "r.  d. falgout and u.  meier - yang .",
    "hypre : a library of high performance preconditioners . in p.",
    "m.  a. sloot , c.  j.  k. tan , j.  j. dongarra , and a.  g. hoekstra , editors , _ lecture notes in computer science _ ,",
    "volume 2331 , pages 632641 .",
    "springer , 2002 .",
    "p.  bastian , m.  blatt , a.  dedner , c.  engwer , r.  klfkorn , r.  kornhuber , m.  ohlberger , and o.  sander . a generic grid interface for parallel and adaptive scientific computing .",
    "part ii : implementation and tests in dune .",
    ", 82(2 - 3):121138 , 2008 .",
    "p.  bastian , m.  blatt , a.  dedner , c.  engwer , r.  klfkorn , m.  ohlberger , and o.  sander .",
    "a generic grid interface for parallel and adaptive scientific computing .",
    "part i : abstract framework . , 82(2 - 3):103119 , 2008 .",
    "a.  burri , a.  dedner , r.  klfkorn , and m.  ohlberger . an efficient implementation of an adaptive and parallel grid in dune . in _ proceedings of 2nd russian - german advanced research workshop on computational science and high performance computing , stuttgart"
  ],
  "abstract_text": [
    "<S> many problems in fluid modelling require the efficient solution of highly anisotropic elliptic partial differential equations ( pdes ) in `` flat '' domains . </S>",
    "<S> for example , in numerical weather- and climate - prediction an elliptic pde for the pressure correction has to be solved at every time step in a thin spherical shell representing the global atmosphere . </S>",
    "<S> this elliptic solve can be one of the computationally most demanding components in semi - implicit semi - lagrangian time stepping methods which are very popular as they allow for larger model time steps and better overall performance . with increasing model resolution , </S>",
    "<S> algorithmically efficient and scalable algorithms are essential to run the code under tight operational time constraints . </S>",
    "<S> we discuss the theory and practical application of bespoke geometric multigrid preconditioners for equations of this type . </S>",
    "<S> the algorithms deal with the strong anisotropy in the vertical direction by using the tensor - product approach originally analysed by brm and hiptmair [ numer . </S>",
    "<S> algorithms , 26/3 ( 2001 ) , pp . </S>",
    "<S> 219 - 234 ] . </S>",
    "<S> we extend the analysis to three dimensions under slightly weakened assumptions , and numerically demonstrate its efficiency for the solution of the elliptic pde for the global pressure correction in atmospheric forecast models . for this </S>",
    "<S> we compare the performance of different multigrid preconditioners on a tensor - product grid with a semi - structured and quasi - uniform horizontal mesh and a one dimensional vertical grid . </S>",
    "<S> the code is implemented in the distributed and unified numerics environment ( dune ) , which provides an easy - to - use and scalable environment for algorithms operating on tensor - product grids . </S>",
    "<S> parallel scalability of our solvers on up to 20,480 cores is demonstrated on the hector supercomputer .    * keywords * :    * ams classifiers * : 65n55 , 65y20 , 65f08 , 65y05 , 35j57 , 86a10 </S>"
  ]
}