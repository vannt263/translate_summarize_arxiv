{
  "article_text": [
    "the problem of systems with long - range spatial and/or temporal correlations ( lrcs ) is one of the topics of intensive research in modern physics , as well as in the theory of dynamic systems and the theory of probability .",
    "the lrc systems are usually characterized by a complex structure and contain a number of hierarchic objects as their subsystems .",
    "the lrcs are the subject of study in physics , biology , economics , linguistics , sociology , geography , psychology , etc .",
    "@xcite .",
    "one of the efficient methods to investigate the correlated systems is based on a decomposition of the space of states into a finite number of parts labeled by definite symbols .",
    "this procedure referred to as coarse graining can be accompanied by the loss of short - range memory between states of system but does not affect and does not damage its robust invariant statistical properties on the large scales .",
    "the most frequently used method of the decomposition is based on the introduction of two parts of the phase space . in other words",
    ", it consists in mapping the two parts of states onto two symbols , say 0 and 1 .",
    "thus , the problem is reduced to investigating the statistical properties of the symbolic binary sequences .",
    "this method is applicable for the examination of both discrete and continuous systems .",
    "one of the ways to get a correct insight into the nature of correlations consists in an ability of constructing a mathematical object ( for example , a correlated sequence of symbols ) possessing the same statistical properties as the initial system .",
    "there are many algorithms to generate long - range correlated sequences : the inverse fourier transform  @xcite , the expansion - modification li method  @xcite , the voss procedure of consequent random addition  @xcite , the correlated levy walks  @xcite , etc .",
    "we believe that , among the above - mentioned methods , using the markov chains is one of the most important .",
    "this was demonstrated in ref .",
    "@xcite , where the markov chains with the _ step - like memory function _ ( mf ) were studied .",
    "it was shown that there exist some dynamical systems ( coarse - grained sequences of the eukarya s dna and dictionaries ) with correlation properties that can be properly described by this model .",
    "the many - step markov chain is the sequence of symbols of some alphabet constructed using a conditional probability function , which determines the probability of occurring some definite symbol of sequence depending on @xmath0 previous ones .",
    "the property of additivity of markov chain means the _ independent _ influence of different previous symbols on generated one .",
    "the concept of additivity , primarily introduced in paper  @xcite , was later generalized for the case of binary _ non - stationary _ markov chains  @xcite .",
    "another generalization was based on consideration of markov sequences with a many - valued alphabet  @xcite . here",
    "we generalize the results of paper  @xcite to the biased case where the numbers of zeros and unities are not supposed to be equal .    in the present work",
    ", we also continue investigating into additive markov chains with more complex memory functions .",
    "an equation connecting mutually - complementary characteristics of a random sequence , i.e. the memory and correlation functions , is obtained . upon finding the memory function of the original random sequence on the basis of the analysis of its statistical properties ,",
    "namely , its correlation function , we can build the corresponding markov chain , which possesses the same statistical properties as the initial sequence .",
    "let us consider a stationary binary sequence of symbols @xmath3 , @xmath4 , @xmath5 . to determine the @xmath0-_step markov chain _ we have to introduce the _ conditional probability _",
    "@xmath6 of occurring the definite symbol @xmath7 ( for example , @xmath8 ) after @xmath0-word @xmath9 , where @xmath9 stands for the sequence of symbols @xmath10 .",
    "thus , it is necessary to define @xmath11 values of the @xmath12-function corresponding to each possible configuration of the symbols @xmath7 in the @xmath0-word @xmath9 .",
    "we suppose that the conditional probability @xmath13 differs from zero and unity for any word @xmath9 that provides the metrical transitivity of the markov chain ( see appendix ) . in turn , according the markov theorem , this property leads to the ergodicity of the symbolic system under consideration .    since we suppose to apply our theory to the sequences with long memory lengths of the order of @xmath14 , some special restrictions to the class of @xmath12-functions should be imposed .",
    "we consider the memory function of the _ additive _ form , @xmath15 here the function @xmath16 describes the additive contribution of the symbol @xmath17 to the conditional probability of occurring the symbol unity , @xmath18 , at the @xmath1th site .",
    "the homogeneity of the markov chain is provided by independence of the conditional probability eq .",
    "( [ 1 ] ) of the index @xmath1 .",
    "it is possible to consider eq .",
    "( [ 1 ] ) as the first term in expansion of conditional probability in the formal series , where each term corresponds to the additive ( unary ) , binary , ternary , and so on functions up to the @xmath0-ary one .",
    "it is reasonable to assume the function @xmath19 to be decreasing with an increase of the distance @xmath20 between the symbols @xmath17 and @xmath3 in the markov chain .",
    "however , for the sake of simplicity we consider a step - like memory function @xmath21 independent of the second argument @xmath20 . as a result , the model is characterized by three parameters only , specifically by @xmath22 , @xmath23 , and @xmath0 : @xmath24 note that the probability @xmath12 in eq .",
    "( [ 2 ] ) depends on the numbers of symbols 0 and 1 in the @xmath0-word but is independent of the arrangement of the elements @xmath25 . instead of two parameters @xmath22 and @xmath23 it is convenient to introduce new independent parameters @xmath26 and @xmath27 ( see below eq .",
    "( [ 3 ] ) ) , @xmath28 parameter @xmath26 provides the statistical inequality of the numbers of symbols zero and unity in the markov chain under consideration . in other words , the chain is biased . indeed , taking into account eqs .",
    "( [ 2 ] ) and ( [ 2a ] ) and the sequence of equations , @xmath29 one can see the lack of symmetry with respect to interchange @xmath30 in the markov chain if @xmath31 . here",
    "@xmath32 is the symbol ",
    "opposite  to @xmath3 , @xmath33 , and @xmath34 is the word  opposite  to @xmath35 .",
    "therefore , the probabilities of occurring the words @xmath36 and @xmath37 are not equal to each other for any word of the length @xmath2 . at @xmath38",
    "this yields nonequal average probabilities that symbols @xmath39 and @xmath40 occur in the chain .",
    "particularly , probability of occurring symbol @xmath39 is grater by @xmath41 than that of symbol @xmath40 . if @xmath42 one has non - biased case .    taking into account the symmetry of the conditional probability @xmath12 with respect to a permutation of symbols @xmath3 ( see eq .",
    "( [ 2 ] ) ) , we can simplify the notations and introduce the conditional probability @xmath43 of occurring the symbol zero after the @xmath0-word containing @xmath44 unities , e.g. , after the word @xmath45 , @xmath46 @xmath47 with the correlation parameter @xmath48 being defined by the relation @xmath49    we focus mainly our attention on the region of @xmath48 determined by the persistence inequality @xmath50 . in this case , each of the symbols unity in the preceding @xmath0-word promotes the birth of new symbol unity .",
    "nevertheless , the major part of our results is valid for the anti - persistent region @xmath51 as well .",
    "note that inequalities @xmath52 and @xmath53 follow from eq .",
    "( [ 14 ] ) ) . without loss of generality",
    ", we consider a case @xmath54 only .      in order to investigate the statistical properties of the markov chain , we consider the distribution @xmath55 of the words of definite length @xmath2 by the number @xmath44 of unities in them , @xmath56 and the variance of @xmath44 , @xmath57 where @xmath58 if @xmath59 one arrives at the known result for the non - correlated brownian diffusion , @xmath60 we will show that the distribution function @xmath55 for the sequence determined by eq .",
    "( [ 14 ] ) ( with nonzero but not extremely close to @xmath61 parameter @xmath48 ) is the gaussian with the variance @xmath62 nonlinearly dependent on @xmath2 .",
    "however , at @xmath63 the distribution function can differ strongly from the gaussian .      for the stationary markov chain , the probability @xmath64 of occurring a certain word",
    "@xmath65 satisfies the condition of compatibility for the chapman - kolmogorov equation ( see , for example , ref .",
    "@xcite ) : @xmath66 @xmath67 thus , we have @xmath11 homogeneous algebraic equations for the @xmath11 probabilities @xmath68 of occurring the @xmath0-words and the normalization equation @xmath69 .",
    "this set of equations is equivalent to that of eq .",
    "( [ eq8 ] ) . in the case under consideration , the set of eqs .",
    "( [ 10 ] ) can be substantially simplified owing to the following statement :    * proposition 1 * : _ the probability @xmath64 depends on the number @xmath44 of unities in the @xmath0-word only _ , i.  e. , it is independent of the arrangement of symbols in the word @xmath70 .     of occurring a word @xmath71 vs its number",
    "@xmath72 expressed in the binary code , @xmath73 , for @xmath74 , @xmath75 , @xmath76.,title=\"fig:\",scaledwidth=48.0%,scaledwidth=32.0% ]    this statement illustrated by fig .",
    "1 is valid owing to the chosen simple model ( [ 2 ] ) , ( [ 14 ] ) of the markov chain .",
    "it can be easily verified directly by substituting the obtained below solution ( [ b ] ) into the set of eqs .",
    "( [ 10 ] ) .",
    "note that according to the markov theorem , eqs .",
    "( [ 10 ] ) do not have other solutions  @xcite .",
    "proposition 1 evidently leads to the very important property of isotropy : any word @xmath77 appears with the same probability as the inverted one , @xmath78 .",
    "let us apply the set of eqs .",
    "( [ 10 ] ) to the word @xmath79 : @xmath80 @xmath81 this yields the recursion relation for @xmath82 , @xmath83 @xmath84 the probabilities @xmath85 for @xmath86 satisfy the sequence of inequalities ,    @xmath87 @xmath88 which is the reflection of persistent properties for the chain .    the solution of eq .",
    "( [ 10 ] ) is @xmath89 with the parameters @xmath90 and @xmath91 defined by @xmath92 the constant @xmath93 will be found below by normalizing the distribution function .",
    "its value is , @xmath94",
    "in this section we investigate into the statistical properties of the markov chain , specifically , the distribution of the words of definite length @xmath2 by the number @xmath44 of unities .",
    "the length @xmath2 can also be interpreted as the number of jumps of some particle over an integer - valued 1d lattice or as the time of the diffusion imposed by the markov chain under consideration .",
    "the form of the distribution function @xmath55 depends , to a large extent , on the relation between the word length @xmath2 and the memory length @xmath0 .",
    "therefore , the first thing we will do is to examine the simplest case @xmath95 .",
    "the value @xmath85 is the probability that an @xmath0-word contains @xmath44 unities with a _",
    "definite _ order of symbols @xmath7 .",
    "therefore , the probability @xmath96 that an @xmath0-word contains @xmath44 unities with _",
    "arbitrary _ order of symbols @xmath7 is @xmath85 multiplied by the number @xmath97 of different permutations of @xmath44 unities in the @xmath0-word , @xmath98 combining eqs .",
    "( [ b ] ) and ( [ 19 ] ) , we find the distribution function , @xmath99 the normalization constant @xmath100 can be obtained from the equality @xmath101 , @xmath102 comparing eqs .",
    "( [ b ] ) , ( [ 19])-([17 ] ) , one can get eq .",
    "( [ 17a ] ) for the constant @xmath93 in eq .",
    "( [ b ] ) .      in terms of the correlation parameter @xmath27",
    ", this limiting case corresponds to the values of @xmath27 not very close to 1/2 , @xmath104 this inequality can be rewritten via the @xmath19-function ( see eqs .",
    "( [ 2])([3 ] ) ) , @xmath105    in the absence of correlations , @xmath106 , eq .",
    "( [ 18 ] ) and the stirling formula yield the gaussian distribution at @xmath107 , @xmath108 . given the persistence is not too strong , @xmath109 one can also obtain the gaussian form for the distribution function , @xmath110 with the @xmath27-dependent variance , @xmath111 @xmath112 , \\label{28}\\ ] ] @xmath113.\\ ] ] it is followed from eq .",
    "( [ 27 ] ) that @xmath0-words containing @xmath114 unties are the most probable .",
    "it is interesting to note , that the persistence leads to a _ decrease _ of the variance @xmath115 with respect to @xmath116 if @xmath117 in other case , for instance , at @xmath118 , the persistence results in an increase of the variance @xmath119 . to put it differently",
    ", the persistence is conductive to the intensification of the diffusion under conditions opposite to inequality ( [ 288a ] ) .",
    "inequality @xmath120 gives @xmath121 .",
    "therefore , despite the increase of @xmath122 , the fluctuations of @xmath123 of the order of @xmath0 are exponentially small .",
    "if the parameters @xmath90 and @xmath91 are integers of the order of unity , the distribution function @xmath96 is a polynomial of degree @xmath125 . in particular , at @xmath126 , the function @xmath96 is constant , @xmath127 at @xmath128 , @xmath96 has a maximum within the interval @xmath129 $ ] . at @xmath130 and @xmath131 ,",
    "@xmath96 decreases monotonously with an increase of @xmath44 .",
    "if the parameter @xmath91 satisfies the inequality , @xmath132 or @xmath133 then one can neglect the parameters @xmath90 and @xmath91 in the arguments of the functions @xmath134 , @xmath135 , and @xmath136 in eq .",
    "( [ 18 ] ) . in this case",
    ", the distribution function @xmath96 assumes its maximal values at @xmath137 and @xmath138 , @xmath139 formula ( [ 20 ] ) describes the sharply decreasing @xmath96 as @xmath44 varies from @xmath39 to @xmath40 ( and from @xmath0 to @xmath140 ) .",
    "then , at @xmath141 , the function @xmath96 decreases more slowly with an increase in @xmath44 , @xmath142 at @xmath143 the probability @xmath96 achieves its minimal value , @xmath144    it follows from normalization ( [ 17 ] ) that the values @xmath100 and @xmath145 are approximatively equal to @xmath146 and @xmath147 respectively . neglecting the terms of the order of @xmath148 ,",
    "one gets @xmath149 @xmath150 in the straightforward calculation using eqs .",
    "( [ 7 ] ) and ( [ 21 ] ) the variance @xmath151 is @xmath152    thus , the variance @xmath122 is equal to @xmath153 in the leading approximation in the parameter @xmath154 .",
    "this fact has a simple explanation .",
    "the probability of occurrence the @xmath0-word containing @xmath0 unities is approximatively equal to @xmath147 .",
    "so , the relations @xmath155 and @xmath156 give ( [ 22b ] ) .",
    "the case of strong persistence corresponds to the so - called ballistic regime of diffusion : if we chose randomly some symbol @xmath7 in the sequence , it will be surrounded by the same symbols with the probability close to unity .    the evolution of the distribution function @xmath157 from the gaussian form to the inverse one with a decrease of the parameters @xmath90 and @xmath91 is shown in fig .  2 .",
    "in the interval @xmath158 the curve @xmath96 is concave and the maximum of function @xmath96 inverts into minimum . at @xmath159 and @xmath160 ,",
    "the curve remains a smooth function of its argument @xmath44 as shown by curve with @xmath161 in fig .",
    "below , we will not consider this relatively narrow region of the change in the parameter @xmath91 .",
    "formulas ( [ 27 ] ) , ( [ 28 ] ) , ( [ 21 ] ) and ( [ 22a1 ] )  ( [ 22b ] ) describe the statistical properties of @xmath2-words for the fixed  diffusion time",
    " @xmath162 .",
    "below , we examine the distribution function @xmath55 for more general situation , @xmath163 .     for @xmath0=20 and different values of the parameters",
    "@xmath90 and @xmath91 shown near the curves.,title=\"fig:\",scaledwidth=45.0%,scaledwidth=35.0% ]        the distribution function @xmath55 at @xmath164 can be given as @xmath165 this equation follows from the consideration of @xmath0-words consisting of two parts , @xmath166 the total number of unities in this word is @xmath1 .",
    "the right - hand part of the word ( @xmath2-sub - word ) contains @xmath44 unities .",
    "the remaining ( @xmath167 ) unities are situated within the left - hand part of the word ( within @xmath168-sub - word ) .",
    "the multiplier @xmath169 in eq .",
    "( [ 29 ] ) takes into account all possible permutations of the symbols  1  within the @xmath0-word on condition that the @xmath2-sub - word always contains @xmath44 unities .",
    "then we perform the summation over all possible values of the number @xmath1 . note that eq .",
    "( [ 29 ] ) is a direct consequence of the proposition 1 formulated in subsec .",
    "c of the previous section .    the straightforward summation in eq .",
    "( [ 29 ] ) yields the following formula that is valid at any value of the parameters @xmath90 and @xmath91 : @xmath170 where @xmath171    it is of interest to note that the parameters @xmath27 , @xmath26 and the memory length @xmath0 are presented in eqs .",
    "( [ w(l ) ] ) , ( [ w(0 ) ] ) via the parameters @xmath90 and @xmath91 only .",
    "this means that the statistical properties of the @xmath2-words with @xmath164 are defined by these `` combined '' parameters .    in the limiting case of weak persistence , @xmath172 , at @xmath173 , eq .",
    "( [ w(l ) ] ) along with the stirling formula give the gaussian distribution function , @xmath174 with the variance @xmath62    @xmath175\\left[1-\\frac{4\\nu^2}{(1 - 2\\mu ) ^2}\\right ] \\label{32}\\ ] ]    and @xmath176.\\ ] ]    in the case of strong persistence ( [ 24a ] ) , the asymptotic expression for the distribution function eq .  ( [ w(l ) ] )",
    "can be written as @xmath177 @xmath178    both the distribution @xmath55 ( [ 45f ] ) and the function @xmath96 ( [ 21 ] ) have concave forms .",
    "the former assumes the maximal values ( [ 45b2 ] ) at the edges of the interval @xmath179 $ ] and has a minimum at @xmath180 .      using the definition eq .",
    "( [ 7 ] ) and the distribution function eq .",
    "( [ w(l ) ] ) one can obtain a very simple formula for the variance @xmath62 ,    @xmath181=\\frac{l}{4}\\left[1+\\frac{2\\mu(l-1)}{n-2\\mu(n-1)}\\right]\\left[1-\\frac{4\\nu^2 } { ( 1 - 2\\mu)^2}\\right].\\ ] ]    eq .",
    "( [ d(l ) ] ) shows that the variance @xmath62 obeys the parabolic law independently of the correlation strength in the markov chain .    in the case of weak persistence , at @xmath172",
    ", we obtain the asymptotics of eq .",
    "( [ 32 ] ) .",
    "it allows one to analyze the behavior of the variance @xmath62 with an increase in the `` diffusion time '' @xmath2 . at small @xmath27",
    ", the dependence @xmath62 follows the classical law of the brownian diffusion , @xmath182 .    for the case of strong persistence , @xmath183 , eq .",
    "( [ d(l ) ] ) gives the asymptotics , @xmath184 the ballistic regime of diffusion leads to the quadratic law of the @xmath62 dependence in the zero approximation in the parameter @xmath183 .",
    "the unusual behavior of the variance @xmath62 raises an issue as to what particular type of the diffusion equation corresponds to the nonlinear dependence @xmath62 in eq .",
    "( [ 32 ] ) . in the following subsection , when solving this problem",
    ", we will obtain the conditional probability @xmath185 of occurring the symbol zero after a given @xmath2-word with @xmath164 .",
    "the ability to find @xmath185 , with some reduced information about the preceding symbols being available , is very important for the study of the self - similarity of the markov chain ( see subsubsec .  4 of this subsection ) .",
    "it is quite obvious that the distribution @xmath55 satisfies the equation @xmath186 here @xmath187 is the probability of occurring  0  after an average - statistical @xmath2-word containing @xmath44 unities and @xmath188 is the probability of occurring  1  after an @xmath2-word containing @xmath189 unities . at @xmath164 ,",
    "the probability @xmath187 can be written as @xmath190 the product @xmath191 in this formula represents the conditional probability of occurring the @xmath0-word containing @xmath1 unities , the right - hand part of which , the @xmath2-sub - word , contains @xmath44 unities ( compare with eqs .",
    "( [ 29 ] ) , ( [ 29b ] ) ) .",
    "the product @xmath192 in eq .",
    "( [ 34 ] ) is a sharp function of @xmath1 with a maximum at some point @xmath193 whereas @xmath194 obeys the linear law ( [ 14 ] ) .",
    "this implies that @xmath194 can be factored out of the summation sign being taken at point @xmath193 .",
    "the asymptotical calculation shows that point @xmath195 is described by the equation ,    @xmath196    expression ( [ 14 ] ) taken at point @xmath195 gives the desired formula for @xmath185 because @xmath197 is obviously equal to @xmath198 .",
    "thus , we have    @xmath199    let us consider a very important point relating to eq .",
    "( [ 35 ] ) .",
    "if the concentration of unities in the right - hand part of the word ( [ 29b ] ) is higher than @xmath200 , @xmath201 , then the most probable concentration @xmath202 of unities in the left - hand part of this word is likewise increased , @xmath203 . at the same time , the concentration @xmath202 is less than @xmath204 , @xmath205 this implies that the increased concentration of unities in the @xmath2-words is necessarily accompanied by the existence of a certain tail with an increased concentration of unities as well .",
    "such a phenomenon is referred by us as the _ macro - persistence_. an analysis performed in the following section will indicate that the correlation length @xmath206 of this tail is @xmath207 with @xmath208 dependent on the parameters @xmath27 and @xmath26 only .",
    "it is evident from the above - mentioned property of the isotropy of the markov chain that there are two correlation tails from both sides of the @xmath2-word .",
    "note that the distribution @xmath198 is a smooth function of arguments @xmath44 and @xmath2 near its maximum in the case of weak persistence and @xmath209 . by going over to the continuous limit in eq .",
    "( [ 33 ] ) and using eq .",
    "( [ 36 ] ) with the relation @xmath210 , we obtain the diffusion fokker - planck equation for the correlated markov process ,    @xmath211    where @xmath212 .",
    "equation ( [ 39 ] ) has a solution of the gaussian form eq .",
    "( [ 31 ] ) with the variance @xmath62 satisfying the ordinary differential equation , @xmath213 its solution , given the boundary condition @xmath214 , coincides with ( [ 32 ] ) .      in this subsection",
    ", we point to one of the most interesting properties of the markov chain being considered , namely , its self - similarity .",
    "let us reduce the @xmath0-step markov sequence by regularly ( or randomly ) removing some symbols and introduce the decimation parameter @xmath215 , @xmath216 here @xmath217 is a renormalized memory length for the reduced @xmath217-step markov chain . according to eq .",
    "( [ 36 ] ) , the conditional probability @xmath218 of occurring the symbol zero after @xmath44 unities among the preceding @xmath217 symbols is described by the formula , @xmath219 with    @xmath220    the comparison between eqs .",
    "( [ 14 ] ) and ( [ 42 ] ) shows that the reduced chain possesses the same statistical properties as the initial one but it is characterized by the renormalized parameters ( @xmath217 , @xmath221 , @xmath222 ) instead of ( @xmath0 , @xmath26 , @xmath48 ) .",
    "thus , eqs .",
    "( [ 41 ] ) and ( [ 43 ] ) determine the one - parametrical renormalization of the parameters of the stochastic process defined by eq .",
    "( [ 14 ] ) .",
    "the astonishing property of the reduced sequence consists in that _ the variance @xmath223 is invariant with respect to the one - parametric decimation transformation _ ( [ 41 ] ) , ( [ 43 ] ) .",
    "in other words , it coincides with the function @xmath62 for the initial markov chain : @xmath224 = d(l ) , \\\\",
    "l < n^{\\ast } .\\ ] ] indeed , according to eqs .",
    "( [ 41 ] ) , ( [ 43 ] ) , the renormalized parameters @xmath225 and @xmath226 of the reduced sequence coincides exactly with the parameters @xmath90 and @xmath91 of the initial markov chain . since the shape of the function @xmath198 eq .",
    "( [ w(l ) ] ) is defined by the invariant parameters @xmath227 and @xmath228 , the distribution @xmath198 is also invariant with respect to the decimation transformation .    the transformation ( @xmath0 , @xmath26 , @xmath48 ) @xmath229 ( @xmath230 , @xmath221 , @xmath222 ) ( [ 41 ] ) , ( [ 43 ] ) possesses the properties of semi - group , i.  e. , the composition of transformations ( @xmath0 , @xmath26 , @xmath48 ) @xmath229 ( @xmath217 , @xmath221 , @xmath222 ) and ( @xmath217 , @xmath221 , @xmath222 ) @xmath229 ( @xmath231 , @xmath232 , @xmath233 ) with transformation parameters @xmath234 and @xmath235 is likewise the transformation from the same semi - group , ( @xmath0 , @xmath26 , @xmath27 ) @xmath229 ( @xmath236 , @xmath237 , @xmath233 ) , with parameter @xmath238 .",
    "the invariance of the function @xmath62 at @xmath164 was referred to by us as the phenomenon of _ self - similarity_. it is demonstrated in fig .",
    "[ fig3 ] .",
    "it is interesting to note that the property of self - similarity is valid for any strength of the persistency .",
    "indeed , the result eq .",
    "( [ 36 ] ) can be obtained directly from eqs .",
    "( [ b])-([17a ] ) , and ( [ 34 ] ) not only for @xmath172 but also for the arbitrary value of @xmath91 .     on the tuple length @xmath2 for the generated sequence with @xmath239 , @xmath240 and @xmath241 ( dotted line ) and for the decimated sequences ( the parameter of decimation @xmath242 ) .",
    "squares and circles correspond to the stochastic and deterministic reduction , respectively .",
    "the solid line describes the non - correlated brownian diffusion , @xmath243.,title=\"fig:\",scaledwidth=45.0%,scaledwidth=35.0% ]",
    "typically , the correlation function and other moments are employed as the input characteristics for the description of the correlated random sequences .",
    "however , the correlation function describes not only the direct interconnection of the elements @xmath7 and @xmath244 , but also takes into account their indirect interaction via all other intermediate elements .",
    "our approach operates with the `` origin '' characteristics of the system , specifically , with the memory function .",
    "the correlation and memory functions are mutual - complementary characteristics of a random sequence in the following sense .",
    "the numerical analysis of a given random sequence enables one to directly determine the correlation function rather than the memory function . on the other hand ,",
    "it is possible to construct a random sequence using the memory function , but not the correlation one .",
    "therefore , we believe that the investigation of memory function of the correlated systems will permit one to disclose their intrinsic properties which provide the correlations between the elements .    the memory function used in refs .",
    "@xcite was characterized by the step - like behavior and defined by two parameters only : the memory depth @xmath0 and the strength of symbol s correlations .",
    "such a memory function describes only one type of correlations in a given system , the persistent or anti - persistent one , which results in the super- or sub - linear dependence @xmath62  @xcite .",
    "obviously , both types of correlations can be observed at different scales in the same system .",
    "thus , one needs to use more complex memory functions for detailed description of the systems with both type of correlations .",
    "besides , we have to find out a relation connecting the mutually - complementary characteristics of random sequence , the memory and correlation functions .",
    "let us rewrite eq .",
    "( [ 1 ] ) in an equivalent form , @xmath245 with @xmath246.\\ ] ] the constant @xmath68 is the value of @xmath3 averaged over the whole sequence ,",
    "@xmath247 : @xmath248 indeed , according to the ergodicity of the markov chain , @xmath249 coincides with the value of @xmath7 averaged over the ensemble of realizations of the markov chain .",
    "so , we can write @xmath250 here @xmath251 is the probability of occurring the symbol @xmath3 equal to unity and @xmath252 is the probability of occurring the definite word @xmath9 in the considering ensemble of sequences . substituting @xmath253 from eq .",
    "( [ m2a ] ) into eq .",
    "( [ m2b ] ) and taking into account the obvious relation @xmath254 , one gets , @xmath255 the sum @xmath256 does not depend on the subscript @xmath20 and obviously coincides with @xmath249 .",
    "so , we have @xmath257 . from this equation",
    "we conclude that @xmath247 .",
    "thus , we can rewrite eq .",
    "( [ m2a ] ) as @xmath258    we refer to @xmath259 as the _ memory function _ ( mf ) .",
    "it describes the strength of influence of previous symbol @xmath17 upon a generated one , @xmath3 . to the best of our knowledge ,",
    "the concept of memory function for many - step markov chains was introduced in ref .",
    "the function @xmath260 contains the complete information about correlation properties of the markov chain .",
    "we suggest below two methods for finding the memory function @xmath259 of a random binary sequence with a known correlation function . the first one is based on the minimization of a `` distance '' @xmath261 between the markov chain generated by means of a sought - for mf and the initial sequence of symbols .",
    "this distance is determined by the formula , @xmath262 with the conditional probability @xmath12 defined by eq .",
    "( [ m2 ] ) .",
    "let us express distance  ( [ optim1 ] ) in terms of the correlation function , @xmath263 from eqs .",
    "( [ m2 ] ) , ( [ optim1 ] ) , one obtains @xmath264 @xmath265 the minimization equation , @xmath266 yields the relationship between the correlation and memory functions , @xmath267 equation  ( [ mmain ] ) can also be derived by straightforward calculation of the average @xmath268 in eq .",
    "( [ cor ] ) using definition  ( [ m2 ] ) of the memory function .    [ 0.8 ] ) ( solid line ) and the reconstructed one ( dots ) vs the distance @xmath20 . in inset ,",
    "the correlation function @xmath269 obtained by a numerical analysis of the sequence constructed by means of the memory function eq .",
    "( [ eqmf]).,title=\"fig : \" ]    the second method resulting from the first one , establishes a relationship between the memory function @xmath259 and the variance @xmath62 , @xmath270 @xmath271).\\ ] ] it is a set of linear equations for @xmath259 with coefficients @xmath272 determined by @xmath273 .",
    "the relations , @xmath274/2 $ ] obtained in ref .",
    "@xcite and @xmath275 are used here .",
    "[ 0.8 ] described by eq .",
    "( [ k(r ) ] ) ( solid line ) .",
    "the dots correspond to the reconstructed correlation function . in inset ,",
    "the memory function @xmath259 obtained by numerical solution of eq .",
    "( [ mmain ] ) with correlation function eq .",
    "( [ k(r)]).,title=\"fig : \" ]    let us verify the robustness of our method by numerical simulations .",
    "we consider a model `` triangle '' _ memory function _ , @xmath276 presented in fig .",
    "[ fig11 ] by solid line .",
    "using eq .",
    "( [ m2 ] ) , we construct a random non - biased , @xmath277 , sequence of symbols @xmath278 .",
    "then , with the aid of the constructed binary sequence of the length @xmath14 , we calculate numerically the correlation function @xmath269 .",
    "the result of these calculations is presented in inset fig .",
    "[ fig11 ] .",
    "one can see that the correlation function @xmath269 mimics roughly the memory function @xmath259 over the region @xmath279 . in the region @xmath280 ,",
    "the memory function is equal to zero but the correlation function does not vanish  @xcite .",
    "then , using the obtained correlation function @xmath269 , we solve numerically eq .  ( [ mmain ] ) .",
    "the result is shown in fig .",
    "[ fig11 ] by dots .",
    "one can see a good agrement of initial , eq .",
    "( [ eqmf ] ) , and reconstructed memory functions @xmath259 .",
    "the main and very nontrivial result of our paper consists in the ability to construct a binary sequence with an arbitrary _ prescribed correlation function _ by means of eq .",
    "( [ mmain ] ) . as an example , let us consider the model correlation function , @xmath281 presented by the solid line in fig .",
    "[ fig12 ] .",
    "we solve eq .",
    "( [ mmain ] ) numerically to find the memory function @xmath259 using this correlation function .",
    "the result is presented in inset fig .",
    ". then we construct the binary markov chain using the obtained memory function @xmath259 . to check up a robustness of the method",
    ", we calculate the correlation function @xmath269 of the constructed chain ( the dots in fig .",
    "[ fig12 ] ) and compare it with eq .",
    "( [ k(r ) ] ) .",
    "one can see an excellent agreement between the initial and reconstructed correlation functions .",
    "[ 0.8 ] for the coarse - grained text of bible ( solid line ) and for the sequence generated by means of the reconstructed memory function @xmath259 ( dots ) .",
    "the dotted straight line describes the non - biased non - correlated brownian diffusion , @xmath282 .",
    "the inset demonstrates the anti - persistent dependence of ratio @xmath283 on @xmath2 at short distances.,title=\"fig : \" ]    let us demonstrate the effectiveness of our concept of the additive markov chains when investigating the correlation properties of coarse grained literary texts .",
    "first , we use the coarse - graining procedure and map the letters of the text of bible  @xcite onto the symbols zero and unity ( here , @xmath284 ) .",
    "then we examine the correlation properties of the constructed sequence and calculate numerically the variance @xmath62 .",
    "the result of simulation of the normalized variance @xmath285 is presented by the solid line in fig .",
    "[ fig13 ] .",
    "the dominator @xmath286 in the equation for the normalized variance @xmath287 is inserted in order to take into account the inequality of the numbers of zeros and unities in the coarse - grained literary texts .",
    "the straight dotted line in this figure describes the variance @xmath282 , which corresponds to the _ non - biased non - correlated _ brownian diffusion .",
    "the deviation of the solid line from the dotted one demonstrates the existence of correlations in the text .",
    "it is clearly seen that the diffusion is anti - persistent at small distances , @xmath288 , ( see inset fig .",
    "[ fig13 ] ) whereas it is persistent at long distances .",
    "the memory function @xmath259 for the coarse - grained text of bible at @xmath289 obtained by numerical solution of eq .",
    "( [ mf ] ) is shown in fig .",
    "[ fig14 ] . at long distances , @xmath290 ,",
    "the memory function can be nicely approximated by the power function @xmath291 , which is presented by the dash - dotted line in inset fig .",
    "[ fig14 ] .",
    "[ 0.8 ] for the coarse - grained text of bible at short distances . in inset , the power - law decreasing portions of the @xmath259 plots for several texts .",
    "the dots correspond to `` pygmalion ''",
    "by b. shaw .",
    "the solid line corresponds to power - law fitting of this function .",
    "the dash dotted and dashed lines correspond to bible in english and russian , respectively.,title=\"fig : \" ]    note that the region @xmath292 of negative anti - persistent memory function provides much longer distances @xmath293 of anti - persistent behavior of the variance @xmath62 .",
    "our study reveals the existence of two characteristic regions with different behavior of the memory function and , correspondingly , of persistent and anti - persistent portions in the @xmath62 dependence .",
    "this appears to be a prominent feature of all texts written in any language .",
    "the positive persistent portions of the memory functions are given in inset fig .",
    "[ fig14 ] for the coarse - grained english- and russian - worded texts of bible ( dash - dotted and dashed lines , refs .",
    "@xcite and  @xcite , correspondingly ) .",
    "besides , for comparison , the memory function of the coarse - grained text of `` pygmalion '' by b. shaw  @xcite is presented in the same inset ( dots ) , the power - law fitting is shown by solid line .",
    "it is interesting to note that the memory function of any text mimics the correlation function , as it was found for the model example eq .",
    "( [ k(r ) ] ) . this fact is confirmed by fig .  [ fig15 ] where the correlation function of the coarse - grained text of bible is shown .",
    "one can see that its behavior at both short and long scales is similar to the memory function presented in fig .",
    "[ fig14 ] .",
    "however , the exponents in the power - law approximations of @xmath269 and @xmath259 functions differ essentially .",
    "[ 0.8 ] for the coarse - grained text of bible at short distances . in inset ,",
    "the power - law decreasing portions of the @xmath269 plot for the same text .",
    "the solid line corresponds to power - law fitting of this function.,title=\"fig : \" ]",
    "thus , the simple , exactly solvable model of the uniform binary @xmath0-step markov chain is presented .",
    "the memory length @xmath0 , the parameter @xmath27 of the persistent correlations and the biased parameter @xmath26 are three parameters in our theory .",
    "the correlation function @xmath294 is usually employed as the input characteristics for the description of the correlated random systems . yet",
    ", the function @xmath294 describes not only the direct interconnection of the elements @xmath7 and @xmath244 , but also takes into account their indirect interaction via other elements . since our approach operates with the  original ",
    "parameters @xmath0 , @xmath27 and @xmath26 , we believe that it allows us to reveal the intrinsic properties of the system which provide the correlations between the elements .    we have demonstrated the efficiency of description of the symbolic sequences with long - range correlations in terms of the memory function . an equation connecting the memory and correlation functions of the system under study is obtained .",
    "this equation allows reconstructing a memory function using a correlation function of the system .",
    "actually , the memory function appears to be a suitable informative `` visiting card '' of any symbolic stochastic process .",
    "the effectiveness and robustness of the proposed method is demonstrated by simple model examples .",
    "memory functions for some concrete examples of the coarse - grained literary texts are constructed and their power - law behavior at long distances is revealed .",
    "thus , we have shown the complexity of organization of the literary texts in contrast to a previously discussed simple power - law decrease of correlations  @xcite .",
    "if the memory length @xmath0 of the system under consideration is of order of the very system length then the markov chain , modeling the system , could be non - stationary . in this case",
    "the proposed method does not allow to describe the system precisely , as distinct from the method proposed in  @xcite .",
    "in this appendix , we prove the property of metrical transitivity of the @xmath0-step markov chains .",
    "it is possible to look at the markov chain from the other point of view and consider it as a @xmath40-step _ vector _ markov chain . to this end",
    ", we introduce the @xmath0-component vector - function @xmath295 , @xmath296 the number of different sets of symbols @xmath297 is equal to @xmath298 .",
    "we number the different states of the vector @xmath295 by their binary representation , @xmath299 the matrix elements @xmath300 of the probability matrix @xmath301 , i.e. the probabilities of transition of the vector @xmath302 into the vector @xmath303 can be expressed via the function of conditional probability @xmath13 .",
    "the subscripts @xmath1 and @xmath44 of the matrix @xmath300 are determined by the binary representations of the sequences @xmath304 and @xmath305 , correspondingly : @xmath306 and @xmath307 .",
    "every matrix row contains only two non - zero elements since the vector @xmath308 can take up two values only , namely , @xmath309 and @xmath310 . for @xmath311 ,",
    "let us denote the probability of occurring of @xmath312 as @xmath313 , where the index @xmath44 is equal to @xmath314 in the binary representation .    for the index @xmath44 being in the range from @xmath315 to @xmath316",
    ", we denote the probability of occurring of symbol @xmath312 after the word @xmath317 as @xmath318 .",
    "then , @xmath313 is the probability of occurring of the symbol unity .",
    "taking into account that @xmath319 for @xmath320 and obvious relations , @xmath321 @xmath322 we get the transition probabilities matrix @xmath301 : @xmath323 thus , to determine the vectors @xmath68 of probability distribution of @xmath0-words in the stationary markov chain we need to solve the system of equations , @xmath324 in other words , one needs to obtain the normalized eigenvector corresponding to the eigenvalue @xmath325 of the matrix @xmath326 of the order @xmath298 .",
    "it is clear that if the vector @xmath68 satisfies to the condition @xmath327 then for every integer @xmath44 the condition @xmath328 is also true , here @xmath329 is the power @xmath44 of the matrix @xmath301 .",
    "let us consider the matrix @xmath330 and show that all matrix elements are positive . in this case",
    ", following the markov theorem we can conclude that the matrix @xmath301 determines uniquely the probability of the words distribution .",
    "let us suggest that for any @xmath331 the matrix @xmath329 satisfies to the next conditions : in the first row the elements @xmath332 for @xmath333 are positive , in the second row the positive elements are @xmath334 with @xmath335 , ... in the @xmath336-th row  @xmath337 . in the next rows this order is repeated .",
    "let us demonstrate that if the matrix @xmath329 obeys to this rules , then it is true for the matrix @xmath338 also .",
    "after multiplication of matrixes @xmath329 and @xmath301 the elements of obtained matrix are defined by the expression : @xmath339 let us consider the first row of the matrix @xmath338  @xmath340 .",
    "in each column of the matrix @xmath301 only two elements are non - zero .",
    "after multiplication of the first row of the matrix @xmath329 to some column of the matrix @xmath301 the result is non - zero ( positive ) for @xmath341 only , because positive elements of the matrix @xmath301 corresponds to the positive zone ( @xmath342 ) of the first row of matrix @xmath329 only for this @xmath343 .",
    "so the described rule remains for the first row of the matrix @xmath344 .",
    "similarly this fact can be proved for other rows .",
    "the matrix @xmath345 obeys to this rule , consequently , by induction , it is true for all @xmath329 . in according to this rule ,",
    "if power @xmath138 , then all elements of the matrix @xmath330 are positive .",
    "therefore , from the markov theorem , there is the unique solution of the system @xmath346 ( or @xmath327 ) .",
    "this solution can be obtained by the method of successive approximations , @xmath347 if we start from the arbitrary initial distribution @xmath348 . in the limit @xmath349 we get to the stationary distribution of the probability vector @xmath68 .",
    "taking into account the explicit form of the matrix @xmath301 , the equation  ( [ eq8 ] ) comes to the next equations : @xmath350 @xmath351 for @xmath352 we get the well known result  @xcite : @xmath353 @xmath354 and in the case @xmath355 we obtain the next result : @xmath356 @xmath357                                              note that we here discuss the dependence of the variance @xmath151 upon the length @xmath2 that describes the persistent ( or antipersistent ) correlations in the words of different lengths @xmath2 .",
    "this length does not coincides with the memory length @xmath0 , @xmath358 .",
    "the dependence @xmath62 is completely different from the dependence @xmath151 on the memory length @xmath0 discussed in ref .",
    "@xcite .",
    "the existence of the `` additional tail '' in the correlation function is in agreement with ref .",
    "@xcite and corresponds to the well known fact that the correlation length is always larger then the region of memory function action ."
  ],
  "abstract_text": [
    "<S> a theory of symbolic dynamic systems with long - range correlations based on the consideration of the binary @xmath0-step markov chains developed earlier in phys . </S>",
    "<S> rev . lett . * 90 * , 110601 ( 2003 ) is generalized to the biased case ( non equal numbers of zeros and unities in the chain ) . in the model , </S>",
    "<S> the conditional probability that the @xmath1-th symbol in the chain equals zero ( or unity ) is a linear function of the number of unities ( zeros ) among the preceding @xmath0 symbols . </S>",
    "<S> the correlation and distribution functions as well as the variance of number of symbols in the words of arbitrary length @xmath2 are obtained analytically and verified by numerical simulations . </S>",
    "<S> a self - similarity of the studied stochastic process is revealed and the similarity group transformation of the chain parameters is presented . </S>",
    "<S> the diffusion fokker - planck equation governing the distribution function of the @xmath2-words is explored . </S>",
    "<S> if the persistent correlations are not extremely strong , the distribution function is shown to be the gaussian with the variance being nonlinearly dependent on @xmath2 . </S>",
    "<S> an equation connecting the memory and correlation function of the additive markov chain is presented . </S>",
    "<S> this equation allows reconstructing a memory function using a correlation function of the system . </S>",
    "<S> effectiveness and robustness of the proposed method is demonstrated by simple model examples . </S>",
    "<S> memory functions of concrete coarse - grained literary texts are found and their universal power - law behavior at long distances is revealed . </S>"
  ]
}