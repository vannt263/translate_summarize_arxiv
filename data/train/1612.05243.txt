{
  "article_text": [
    "this is the sixth in a series of seven papers that describe the pan - starrs1 surveys , the data processing algorithms , calibration , and the resulting data products .",
    "* paper i ) describes the  surveys , an overview of the  system , the resulting image and catalog data products , a discussion of the overall data quality and basic characteristics , and a summary of of important results .",
    "* paper ii ) describes how the various data processing stages are organised and implemented , ( * ? ? ?",
    "* paper iii ) describes the details of the pixel processing algorithms , including detrending , warping , and adding ( to create stacked images ) and subtracting ( to create difference images ) and resulting image products and their properties .",
    "* paper iv ) describes the details of the source detection and photometry , including point - spread - function and extended source fitting models , and the techniques for forced photometry measurements .",
    "* paper v ) describes the final calibration process , and the resulting photometric and astrometric quality .",
    "this paper ( paper vi ) describes the  database , the data products , and details of their organization in the  database .",
    "* paper vii ) will describe the medium deep survey in detail , including the unique issues and data products specific to that survey .",
    "the  project teamed with alex szalay s database development group at the johns hopkins university to undertake the task of providing a hierarchical database for  @xcite .",
    "the jhu team was the major developer of the sdss database @xcite , and it would prove useful to reuse as much of the software developed for the sdss as possible .",
    "however due to its intrinsic size and more complicated dataset ( time domain ) major changes were required .",
    "the system developed is called the _ published science products system _ , or psps @xcite .",
    "the key to moving from the sdss database to a system capable of dealing with  data is the design of the data storage layers .",
    "it was immediately clear that a single monolithic database design ( like sdss ) would not work for the ps1 problem .",
    "our approach has been to use several features available within the microsoft sql server product line to implement a system that would meet our requirements . while sql server does not have ( at present ) a cluster implementation ,",
    "this can be implemented by hand using a combination of distributed partition views and slices @xcite .",
    "this allowed us to partition data into smaller databases spread over multiple server machines and still treat the information as a unified table ( from the users perspective ) .",
    "further , by staying with sql server we are able to retain a wealth of software tools developed for sdss , including the use of hierarchical triangular mesh @xcite .",
    "this paper covers multiple data releases .",
    "the first several  data releases will release 3@xmath1 survey data in stages , with plans to release the medium deep survey data thereafter .",
    "the first  data release ( dr1 ) covers the 3@xmath1 _ stack _ images and the static sky catalog .",
    "the available pixel data products for dr1 include the ps1 _ stack _ image products from the 3@xmath1 survey .",
    "these are deep combined _ stack _ images along with ancillary data including signal , masks , variance , and number maps .",
    "the available catalog data products include the ps1 static sky 3@xmath1 catalog .",
    "this includes _",
    "objectthin _ , _ meanobject _ , _ stackobjectthin _ , _ stackobjectattributes _ , _ stackapflx * _ , _ stackmodel * _ , and _",
    "tables , as well as associated metadata tables for these tables . data release 2 ( dr2 ) , scheduled for 2017 , will add the rest of the ps1 image products from the 3@xmath1 survey , this includes the single epoch _ warp _ images , their ancillary data including signal , masks , and variance maps .",
    "dr2 will add the _ detection _ tables and _ forced * _ tables , these are the single epoch detections and forced photometry .",
    "future data releases will provide the 3@xmath1 _ diff _ image products and catalogs , and analogous data products for the medium deep ( md ) surveys .",
    "the two options for data access are via the _ barbara a. mikulski archive for space telescopes _ ( mast ) at stsci , and psps , each of which provides access to different types of data products .",
    "mast provides the access point for downloading different pixel data products and their associated metadata and source catalogs .",
    "this includes fits downloads , fits and jpeg image cutouts , scriptable image access , color jpeg images , and an interactive image browser with catalog overlays .",
    "the psps is a queryable database .",
    "it is based on _ casjobs _ , searches are done using sql queries , and users are able to do complicated and large queries that are saved to their private mydb spaces .",
    "the psps contains calibrated catalogs of photometric and astrometric parameters for single epoch exposures , stacks , difference images , and forced photometry as well . the database schema for the psps",
    "is briefly described in section  [ sec : schemaintro ] , and is fully expanded in appendix  [ sec : schema ] .",
    "example queries are described in appendix  [ sec : query ] .",
    "the pan - starrs1 database schema is organized into four sections :    1 .",
    "fundamental data products .",
    "these are attributes that are calculated from either detrended but untransformed pixels or warped pixels .",
    "the instrumental fluxes or magnitudes have been re - calibrated , as have their positions . because of these calibrations , the catalog values are to be preferred to making a new measurement from the images .",
    "see table [ table : fundamentalipp ] .",
    "2 .   derived data products .",
    "these are higher order science products that have been calculated from the fundamental data products , such as proper motions and photometric redshifts .",
    "these tables will come in later data releases .",
    "observational metadata .",
    "this is metadata that provides detailed information about the individual exposures ( e.g. psf model fit ) or which exposures went into an image combination ( stacks and diffs ) of exposures , as well as information such as detection efficiencies .",
    "see table [ table : observationalmetadata ] .",
    "4 .   system metadata .",
    "these tables have fixed information about the system and the database itself , including descriptions of various flags .",
    "see table [ table : systemmetadata ] .",
    "various database `` views '' are also constructed as an aide for standard types of queries .",
    "views act like tables , and primarily consist of joins of different commonly used tables , in order to simplify queries .",
    "see table [ table : views ] .",
    "this paper covers the data products and schema for the 3@xmath1 data releases .",
    "the majority of the information is the same for the md fields .",
    "we present a condensed version of the flow of data starting with raw image processing and ending with the steps used to generate the catalog database , so that the end user will have a better understanding of the terms that we use . during the various processing steps",
    "some data is lost or flagged out , causing apparent inconsistencies .",
    "we describe these here to provide additional information about the process that will help the user determine the best data products to use .",
    "a flow chart of the whole process can be seen in figure  [ fig : revisedipptopsps ] .",
    "first , images are taken at the summit , then they undergo ipp processing which produces catalogs files from the _ camera _ stage , _ stack _ stage , _ diff _ stage and _ forced _ stage .",
    "these catalog files ( in the form of binary fits files called _ cmf / smf _ files ) are then ingested into a `` desktop virtual observatory '' ( dvo , described in section  [ sec : dvodatabase ] ) database using ` addstar ` .",
    "the dvo database is then calibrated .",
    "next , _ ipptopsps _ creates batches from the dvo and _ smf / cmf _ files to be loaded into psps , and later the database is made available to the user .",
    "the first stage of data products involves processing raw images into detrended and warped images , followed by the generation of _ stacks _ , _ diff images _ and catalogs .",
    "finally forced photometry on the _ warps _ is performed using positions of sources detected in the stacks .",
    "this processing is done by the _ image processing pipeline _ ( ipp ) .",
    "this first iteration of processing is called nightly science processing .",
    "it was designed to be fast to allow the  _ moving object processing system _ ( * ? ? ?",
    "* mops ) and the ps1 science consortium access to the data a few hours after observation in order to detect moving objects , supernovae , and other time sensitive transients .",
    "the 3@xmath1 data spans 4 years of observations , a time during which the ipp was actively being developed and improved . to make a consistent set of data , both for the internal catalog database as well as for the public release , all of the raw images were reprocessed from scratch , using part of the ipp called _ large area processing _",
    "( lap ) , all using a specific revision number of the ipp code , for consistency .",
    "the data released to the public in dr1 is internally called processing version 3 ( pv3 ) .",
    "what follows below is a short description of the different stages of this pipeline , chunked into sections based on final available image data products and products that are used to create the large relational database .",
    "the first stage takes the raw images , generally 60 fits files , one fits file per ota , and detrends them .",
    "this stage is called the _ chip _ stage , and analysis is done per chip .",
    "this involves applying the dark , flat , bias , background correction , etc .",
    ", as described in the detrend paper @xcite , as well as to find the sources and do photometry on them using _ psphot _ @xcite",
    ". the next step , called camera stage , is to mosaic the _ chip _ processing together , to do basic astrometry on the image , to generate a binary fits table called an _",
    "file which holds the catalog information for the image .",
    "the end product of _ camera _ produces the catalog _ smf _ files which are later ingested into a relational database ( dvo ) .",
    "the next two steps are the _ fake _ and the _ warp _ stages .",
    "the _ fake _ stage ingests a number of fake sources into an image , and then performs photometry on the image to recover the input sources .",
    "this stage is skipped for the 3@xmath1 reprocessing ( lap / pv3 ) as we instead do forced photometry at a later stage .",
    "the _ warp _ stage rotates and bins the _ chip _ outputs to be on a tangential ra / dec plane , with 0.25 arcsecond pixels , with the images chopped into smaller 6242x6254 pixel skycells .    for 3@xmath1",
    ", the tessellation for the majority of the skycells is ` rings.v3 ` , based on a budavari - magnier rings tessellation @xcite .",
    "this tessellation subdivides the sky into projections cell rings , each projection cell is 4.0 x 4.0 degrees , subdivided into 10 x 10 skycells , each with 30 arcseconds or 120 pixels of overlap on a side .",
    "the pole region of 3@xmath1 uses the tessellation ` cnp.pole ` , defined as a projection cell of 4.4 x 4.4 degrees , subdivided into 11x11 skycells , centered around ra = 0.0 and dec = 90.0 , same 120 pixels of overlap on each side .",
    "all image data products beyond _ warp _ ( _ stacks_/_forced warps_/_diffs_/ etc . ) are laid out in skycells as well .",
    "the warp image products will be available to users via mast for dr2 .",
    "there are 3 stack related stages : stack , staticsky , and skycal .",
    "the stack stage generates the images , and staticsky and skycal generates the catalogs files .",
    "stacks _ are generated from adding together _ warp _ exposures with the same skycell i d and filter , the process is described in more detail in @xcite .",
    "there are several _ stack _ types , they are listed in the _ stacktype _ in the psps database . for the 3@xmath1 database ,",
    "the _ stack _ type is set to ` deep_stack ` , i.e. , all available and good quality _ warps _ for a given skycell and filter within 3@xmath1 are used to generate the best and deepest _ stack _ possible .",
    "the 3@xmath1 deep _ stacks _ consist of one deep _ stack _ per skycell per filter .",
    "this step produces unconvolved _ stacks _ , _ stacks _ with input _ warps _ convolved to 6 pixels , and _ stacks _ with input _ warps _ convolved to 8 pixels .",
    "masks , weights , variance , and number maps are also generated for each deep stack .",
    "once the _ stack _ images are created , further processing is done on stacks , via _ staticsky _ and _ skycal _ stages , to do photometry and astrometry on the stacks .",
    "for the _ staticsky _ stage , photometric analysis is run on all 5 filters at once , both for unconvolved stacks and _ stacks _ convolved to 6 and 8 pixels .",
    "catalog files , one per filter , of matched sources within a 5 pixel radius are generated .",
    "the _ skycal _ stage calibrates the _ staticsky _ catalogs relative to the reference catalog .",
    "the _ skycal _ catalog files are later ingested into the dvo database and then into the psps database .",
    "due to the overlap between skycells , sources that land in the overlaps can be reported 2 , 3 , or 4 times in the dvo and psps database .",
    "use the ` bestdetection ` flag to select the best one of these .",
    "stack image products are available to users via mast .      in general , for the ipp , there are several different types of _",
    "these are ` warp_warp ` , ` warp_stack ` , ` stack_warp ` and ` stack_stack ` _",
    "diffs_. for 3@xmath1 , difference images are generated between _",
    "warps _ and deep _ stacks _ ,",
    "i.e. , ` warp_stack ` diffs .",
    "that is , each good quality _ warp _ within the 3@xmath1 dataset is subtracted from the appropriate 3@xmath1 deep _ stack _ ( same filter and skycell_id ) .",
    "the results from this stage of processing include warp images and related files , and warp catalog files .",
    "the warp image files are used in later stages of processing ( stack and diff ) .",
    "users can access individual warp images via mast ( dr2 ) .",
    "photometry done in the earlier stages is unforced .",
    "that is , the sources are located on the images , and then photometry is performed .",
    "forced photometry is done with prior knowledge of source positions .",
    "there are 2 steps of forced photometry for the ipp : one performs forced photometry on the individual _ warp _ exposures , and the other calculates forced galaxy model fits on the stacks .",
    "the forced warp photometry stage takes the positions of sources located in the deep _ stacks _ and then calculates photometry at those positions on the individual warps .",
    "the catalogs generated by this process are first ingested into the dvo database , then translated using _ ipptopsps _ into the _ forcedwarp * _ tables for the psps .    forced galaxy photometry is done on the _ stack _ images .",
    "this step calculates galaxy model fits on the known sources on the stacks.catalog files are produced for this stage of processing , and these are later ingested into the psps database , as the _ forcedgalaxy * _ tables .",
    "desktop virtual observatory ( dvo ) is a database developed by eugene magnier that takes a subset of photometric , astrometric , flags , metadata , etc . from the catalog fits files _ smf / cmf _ from various ipp stages and puts them into a relational database built from fits files .",
    "catalog files are ingested into the dvo database via the ` addstar ` program ( part of the ipp code ) .",
    "there are several stages of ipp processing that are ingested into the dvo database .",
    "these are the catalogs produced by the _",
    "skycal _ , _ camera _ , forced photometry on warps , and forced galaxy model fit stages .",
    "difference image catalogs are ingested into the diff dvo database .",
    "once the dvo database is built , it is then calibrated .",
    "steps include relative photometry and astrometry .",
    "average properties are also calculated .",
    "full details are in @xcite .",
    "there are several categories of dvo files that are used by _",
    "ipptopsps _ to populate the psps database .",
    "short summaries of these file types are included here , full details are available in @xcite .",
    "this list is only a subset , and only the files that are most relevant for _",
    ".cpt : :    object information - each .cpt table has one entry for each object in    that region of the sky .",
    "it summarizes the average properties of that    object as long as those properties can be derived independently of the    filter used .",
    "information such as mean ra and dec are listed in these    files . .cpm",
    ": :    measurements - each .cpm table contains all of the measurement    information for each object in the .cpt file . contains measurement    information for detections from the stack / skycal cmf , camera smfs , and    forced warp smfs . .cps",
    ": :    mean properties - the .cps table has filter dependent average property    information for each object listed in the .cpt file .",
    "information such    as mean magnitudes are located in these files . .cpx",
    ": :    lensing measurements - the .cpx files contains lensing parameters    measured from all the forced warp cmfs . .cpy",
    ": :    lensing objects - the .cpy table has one entry for each object in that    region of the sky , same object ids as for objects in the .cpt file .",
    "it    summarizes the average properties of the lensing measurements . .cpq",
    ": :    forced galaxy - the .cpq table has one entry for each object in that    region of the sky , same object ids as for objects in the .cpt files .",
    "it summarizes the extended source galaxy shape measurements .",
    "the dvo database contains a subset of information , primarily average properties of sources , and precisely calibrated astrometric and photometric information .",
    "it is necessary , when building the psps database , to combine the dvo database as well as extra columns from the individual _ smf _ and _ cmf _ files .",
    "ipptopsps _ stage does this for the mean properties ( split into mean properties for single exposures , calibrations against the gaia catalogue @xcite , difference exposures , and _ forcedwarp _ exposures ) , subdivided by dvo fits files with ra / dec boundaries , and again for the individual stages , which represent either single exposures or skycells from other stages ( _ camera _ , _ stacks _ , _ forced warps _ , _ difference images _ ) .",
    "the _ ipptopsps _ translates the dvo and various catalog files associated with processing ( _ camera_/_stacks_/_forced warp_/_diff _ ) into smaller batches which are loaded onto data stores to be ingested by psps .",
    "this is the stage that transforms the data into the same schema as what the end user sees .",
    "the _ ipptopsps _ is written in python / jython , stilts @xcite , uses _ mysql _ for the database ( _ ipptopsps _ ) used to track processing as well as for scratch databases , and uses some of the core tools within ipp to access the gpc1 database , to retrieve files , and to read the dvo database .",
    "the _ ipptopsps _ uses the gpc1 database to generate a list of data products from the different stages that were successfully ingested into the dvo database .",
    "the gpc1 database is used by ipp to keep track of the images and their progress through different stages of processing including if they are in the dvo database . for _ ipptopsps",
    "_ , only data products ingested successfully into the dvo database are queued up to be processed .",
    "therefore it is extremely important to verify and finalize the dvo database prior to running _",
    "ipptopsps_.    there are multiple types of batches that are generated by _ ipptopsps _",
    ", these include _ init _ ( in ) , _ object _ ( ob ) , _ stack _ ( st ) , _ detection _ ( p2 ) , _ forced mean objects _ ( fo ) , _ forced warp _ ( fw ) , _ forced galaxy _ ( fg ) , _ gaia objects _ ( go ) , _ diff detection objects _ ( do ) , and _ diff detections _ ( df ) . for dr1 , in , ob , st , fo and go batches are processed through _ ipptopsps _ and ingested into psps .",
    "dr2 has p2 , fw , fg , do and df batches .",
    "an overview of the different batch types and associated dvo files and smf / cmf files is shown in the flowchart on figure  [ fig : ipptopsps ] .",
    "the init batch ( in ) is the first batch created by _",
    "ipptopsps _ , and it is first to be ingested into psps .",
    "this includes the system metadata tables described in section  [ sec : schemameta ] , with flag bits listed in appendix  [ sec : flagtables ] .",
    "this metadata is hardwired and fixed .",
    "the object batches ( ob ) create the _",
    "objectthin _ and _ meanobject _ tables , described in more detail in section  [ sec : schemaobject ] .",
    "each batch represents individual dvo files which are subdivided into small rectangular patches of sky .",
    "columns are filled from the dvo database ( cpt and cps files ) .",
    "the detection batches ( p2 ) , create the _ detection _ tables , described in more detail in section  [ sec : schemap2 ] .",
    "each batch corresponds to a single exposure from .",
    "columns within are filled from the dvo database ( cpt and cpm files ) as well as the _ camera _ stage catalog file ( smf ) .",
    "the _ stack _ batches ( st ) create the _ stack _ tables , described in more detail in section  [ sec : schemast ] .",
    "each batch corresponds to a skycell from the skycal stage .",
    "columns are filled from the dvo database ( cpt and cpm files ) as well as from the corresponding skycal catalog files ( cmf ) for all 5 filters ( or what is available ) .",
    "the _ forced mean objects _ ( fo ) create the _ forcedmeanobject _ and _ forcedmeanlensing _ tables , described in more detail in section  [ sec : schemaobject ] . forced warp processing is ingested into a dvo , forced objects are determined and their mean properties are calculated and contained withing the dvo .",
    "each batch contains data from individual dvo files ( cpt , cps , cpy ) .",
    "the _ forced galaxy _ batches ( fg ) create the _ forcedgalaxyshape _ table , described in more detail in section  [ sec : schemaobject ] . forced galaxy model fits calculated from the stacks are ingested into the dvo , the forced galaxy objects are determined within the dvo .",
    "each batch contains data from individual dvo files ( cpt , cps , cpq ) .    the _ forced warp _ batches ( fw ) create the _ forcedwarp * _ tables , described in more detail in section  [ sec : schemafw ] .",
    "each batch corresponds to a different skycell , and contains all the _ forced warp _",
    "catalogs for that skycell .",
    "each batch contains data from individual dvo files ( cpm , cpt , cpx ) as well as from the corresponding _ forced warp _ catalog files ( _ cmf _ ) .",
    "the _ diff _ object batches ( do ) create the _ diffdetobject _ table , described in more detail in section  [ sec : schemaobject ] .",
    "diff detection catalog images are ingested into a dvo , _ diff _ objects are defined and mean properties are calculated for the _ diff _ objects .",
    "columns are filled from the dvo _ diff _ database ( cpt and cps files ) .",
    "the _ diff detection _ batches ( df ) create the _ diffdetection _ table , described in more detail in section  [ sec : schemadiff ] .",
    "each batch corresponds to a difference image catalog file created in the _ diff _ stage , and will contain all the skycells for a given exposure .",
    "columns are filled from the dvo database ( cpt and cpm files ) , and from the corresponding _ diff _ catalog file ( _ cmf _ ) .",
    "gaia dr1 @xcite was released after all of the object tables were ingested into the psps database .",
    "we used the gaia data to recalibrate the dvo object positions , this improved the astrometry significantly . rather than dump the database and start over ( with corrected ra and dec positions ) , we added a new type of batch , ( go ) , that contains minimal metadata information , the ` objid ` for the objects , the _ objectinfoflags _ , and the corrected ra and dec as well as the errors",
    ". it is based on exactly the same dvo files as ob batches , has updated ra and dec calibrated to gaia , and ignores the rest of the dvo columns .    within _ ipptopsps",
    "_ it is possible to verify that the expected batches were generated , and to re - queue and regenerate batches that failed .",
    "batches fail for a variety of reasons , none of the failures are terminal .",
    "batches can fail if any of the associated _ mysql _ databases time out or are unavailable , if there are disk i / o glitches or other disk problems .",
    "the dvo database sets the expected number of batches to generate , and failures are investigated and retried until they are resolved . within _ ipptopsps _",
    "it is also possible to poll the psps to verify if batches have been ingested .",
    "this allows for easy removal of batches that have been loaded to psps in order to regain disk space .",
    "[ sec : psps ]    we present an overview of the psps , first so that the user understands better how the psps database is constructed in order to optimize the queries , and secondly to describe the currently known issues within the psps database .",
    "_ introduction _ : psps consists of 5 parts : the data transformation layer ( dxlayer ) , the object database manager ( odm ) , the workflow manager database ( wmd ) , the data retrieval layer ( drl ) , and the  science interface ( psi ) .",
    "the dxlayer polls the _ ipptopsps _ datastores for new batches and prepares them for loading .",
    "the odm is the software that all loading nodes run to load , merge , copy and publish the psps databases .",
    "the wmd is the database containing all the logs about the psps databases .",
    "the drl is the intermediate layer between the client and the psps database .",
    "the psi is the web based interface for ps1 consortium members , for interacting with the drl .",
    "each of these components is described in more detail below , and a diagram of the process is shown in fig  [ fig : odm_data_flow ]    _ partitioning the psps _ : the psps uses distributed partitioned views ( dvp ) , these are tables that are partitioned to reside on different files on different linked servers .",
    "the tables are partitioned into slices , with cuts based on declination , and with each slice containing a similar amount of data .",
    "partition slices are customized for each database ( @xmath0 vs md ) fig  [ fig : odm_data_layout ] shows how the data is partitioned across a subset of the machines .    _",
    "the dxlayer _ : the dxlayer is the first stage in the psps , this is the stage that polls for new batches to load and preps them for the next step ( odm ) .",
    "fig  [ fig : dxlayerprocess ] shows the flowchart of the dxlayer process , and fig  [ fig : psps_loadprocess ] shows a more detailed flowchart of how batches are loaded and verified within the dxlayer and odm .",
    "psps loads batches created by the _ ipptopsps_. a batch corresponds to either products from different processing stages ( _ camera _ , _ stack / skycal _ , _ diff _ , _ forced warp _ ) or from different dvo files ( arranged by area of sky ) .",
    "batches contain a manifest file that describes the batch information such as type of batch , min / max ` objid ` , md5 checksum , and the tables to load .",
    "batches data is stored in fits files , which are transformed to csv files in the dxlayer .",
    "the batch area can not exceed two psps slices , else it will not load correctly .",
    "the psps slices are chosen so that this does not happen .",
    "-0.6 cm        -1.0 cm    _ the odm _ : the nodes within the odm have naming conventions for their roles : load / merge ( lm ) , slice ( s ) , head ( h ) and admin ( a ) .",
    "all odm processes are named workflows ( load , merge , copy , flip ) . everything ( logs , process , requests ) are inserted into an administration database called the workflow manager database ( wmd ) .",
    "databases are named by roles load , cold , warm , hot .",
    "these databases are ms - sql server and are broken into four volumes with 96 file partitions each .",
    "the disk layout is raid1 + 0 with 64 bit alignment .",
    "all of the metadata tables and _ objectthin _ , _ gaiaframecoordinate _ , _ stackobjectthin _ are merged into a head database to provide faster queries .",
    "moar words    _ the wmd _ : moar words    _ the drl _ : the drl is the layer between the user and the psps database .",
    "the drl is responsible for management of queries that the user submits via the drl api , is based from casjobs , and has many similar features .",
    "it primarily keeps track of all user queries and provides progress of those queries in a secure way .",
    "it also kills queries that use too many resources or take too long .",
    "the drl api is accessed via simple access object protocal ( soap ) , allowing for multiple ways for the user to access the database .",
    "consortium members used the psi ( a web user interface ) , but it is also possible for the users to query the database via soap calls from command line scripts .",
    "a flowchart of the drl can be seen in figure  [ fig : psps_drl ] .",
    "_ psi _ : the psi is the web user interface , but only for consortium members . as it was part of the design of psps it is included here for reference .",
    "this interface provides many useful features including a query request page , information on query progress , mydb management tools , graphing tools , access to the pixel data products , and interactive help .",
    "the query request page allows for the user to easily submit queries to a variety of databases ( 3pi / md / mydb ) , to upload query files or to check the syntax , to name mydb results tables and to select the queue to submit to .",
    "the mydb management tools allow the user to easily select which mydb tables to purge as well as well as methods to extract to csv , fits or xml files to download .",
    "some of the interactive features include an interactive schema browser , a query builder to easily create a query with multiple joins and conditions , and a flag generator to create bitmasks for the different types of flag tables .",
    "the plotting tool allowed the user to quickly plot data from the psps databases or their own mydb within the browser .    -0.6",
    "cm        -1.0 cm        -1.0 cm        -1.0 cm        -1.0 cm        -0.5 cm",
    "there are over 50 different tables that make up the psps schema . here",
    "we give a brief overview of the tables and indexes , to help aid the user in selecting the most appropriate table for queries .",
    "the core concept is that the database has a unique ` objid ` for each object detected within  data .",
    "an object is defined to be something which has measurable flux at a given ra and dec .",
    "multiple detections of an object will all share the same ` objid ` , as well as multiple detections within one arcsecond of that object ( which might not be associated with that object , for example , blended sources ) .",
    "this ` objid ` is the core index used to join the object and detection tables .",
    "for example , _ objectthin _ has the astrometric information for the objects , one would join against the _ detection _ table , using ` objid ` , in order to get the individual photometric attributes for all the detections of that object within the single exposures ( at a given ra and dec ) .",
    "there are 4 main types of tables within the psps database , these are fundamental data product tables , observational metadata tables , derived data product tables , and system metadata tables . fundamental data product tables can be categorized into either object tables ( summarized in section  [ sec : schemaobject ] or detection tables ( summarized in section  [ sec : schemadetections ] .",
    "object tables contain basic information on each source in the sky , including the mean photometric and astrometric information .",
    "detection tables contain photometric and astrometric information on the individual exposures .",
    "detection tables contain detections from individual exposures ( _ detection _ ) , stacked images ( _ stack * _ tables ) , forced _ warp",
    "_ images ( _ forcedwarp * _ tables ) that contain forced photometry of individual exposures using positional knowledge of sources detected on _ stacked images _ , _ difference images _ ( _ stack _ - individual exposures ) .",
    "observational metadata contains information about the different data products and how they are mapped to the individual exposures .",
    "observational metadata tables are summarized in sections  [ sec : schemadetections ] .",
    "derived data products will come later and are not described in this paper .",
    "system metadata tables are summarized in section  [ sec : schemameta ] , they contain hardwired information about the psps , this includes tables about flags , filters , tessellations and other useful metadata that is fixed .",
    "this section will start by briefly describing the system metadata tables ( section  [ sec : schemameta ] ) , followed by object tables ( section  [ sec : schemaobject ] .",
    "the detection tables ( section  [ sec : schemadetections ] ) are organized by stage of ipp processing and contain a mix of fundamental data product tables and observational metadata tables as this seemed the more natural way to organize this part of the schema ."
  ],
  "abstract_text": [
    "<S> this paper describes the organization of the database and the catalog data products from the  @xmath0 steradian survey . </S>",
    "<S> the catalog data products are available in the form of an sql - based relational database from mast , the mikulski archive for space telescopes at stsci . </S>",
    "<S> the database is described in detail , including the construction of the database , the provenance of the data , the schema , and how the database tables are related . </S>",
    "<S> example queries for a range of science goals are included . </S>",
    "<S> the catalog data products are available in the form of an sql - based relational database from mast , the mikulski archive for space telescopes at stsci . </S>"
  ]
}