{
  "article_text": [
    "a large variety of methods has been developed for classification of randomly drawn data .",
    "most of these fall into one of two basic categories : _ set estimators _ or _ plug - in estimators_. both of these families are based on some underlying form of approximation . in the case of set estimators ,",
    "one directly approximates the _ bayes set _ , using elements from a given family of sets .",
    "for plug - in estimators , one approximates the underlying _ regression function _ and builds the classifier as a level set of this approximation .",
    "the purpose of this paper is to introduce a family of classification algorithms using tree - based adaptive partitioning and to analyze their risk performance as well as their friendliness to numerical implementation .",
    "these algorithms fall into the category of set estimators .",
    "tree - based classification algorithms have been well studied since their introduction in @xcite , and their convergence properties have been discussed both in terms of oracle inequalities and minimax convergence estimates ; see , for example , @xcite and @xcite . among the specific features of the approach followed in our paper are ( i ) the use of decorated trees which allow us to derive faster rates for certain classes of distributions than obtained when using standard trees and ( ii ) a convergence analysis based on nonlinear approximation theory which allows us to significantly weaken the usual assumptions that are made to establish a given convergence rate .",
    "more detailed comparisons with existing methods and results are decribed later .",
    "we place ourselves in the following setting of binary classification .",
    "let @xmath3 , @xmath4 and @xmath5 .",
    "we assume that @xmath6 is a probability measure defined on @xmath7 .",
    "we denote by @xmath8 the probability that @xmath9 given @xmath10 and by @xmath11 the regression function @xmath12 where @xmath13 denotes expectation . for any set @xmath14",
    ", we use the notation @xmath15 a classifier returns the value @xmath9 if @xmath10 is in some set @xmath16 and @xmath17 otherwise .",
    "therefore , the classifier is given by a function @xmath18 where @xmath19 is some @xmath20 measurable set , and @xmath21 is its complement . with a slight abuse of notation",
    ", we sometimes refer to the set @xmath19 itself as the classifier .",
    "we denote by @xmath22 the risk ( probability of misclassification ) of this classifier , and by @xmath23 the bayes classifier which minimizes this risk @xmath24 , or equivalently , maximizes the quantity @xmath25 among all possible sets @xmath19 .",
    "we measure the performance of a classifier @xmath19 by the _ excess risk _ @xmath26 with @xmath27 the symmetric difference between @xmath28 and @xmath29 .",
    "given the data @xmath30 , @xmath31 , @xmath32 , drawn independently according to @xmath33 , a classification algorithm uses the draw to find a set @xmath34 to be used as a classifier . obtaining a concrete estimate of the decay of the excess risk for a given classification algorithm as @xmath35 grows requires assumptions on the underlying measure @xmath33 .",
    "these conditions are usually spelled out by assuming that @xmath33 is in a _ model class _ @xmath36 .",
    "model classes are traditionally formed by two ingredients : ( i )  assumptions on the behavior of @xmath33 near the boundary of the bayes set @xmath37 and  ( ii )  assumptions on the smoothness of the regression function @xmath38 .",
    "conditions that clarify ( i ) are called margin conditions and are an item of many recent papers ; see , for example , @xcite .",
    "one formulation ( sometimes referred to as the tsybakov condition ) requires that @xmath39 for some constant @xmath40 and @xmath41 .",
    "this condition becomes more stringent as @xmath0 tends to @xmath42 .",
    "the limiting case @xmath43 , known as massart condition , means that for some @xmath44 , we have @xmath45 almost everywhere",
    ". a common choice for ( ii ) in the classification literature ( see , e.g. , @xcite ) is that @xmath38 belongs to the hlder space @xmath46 .",
    "this space can be defined for any @xmath47 as the set of functions @xmath48 such that @xmath49 where @xmath50 is the @xmath51th power of the finite difference operator defined by @xmath52 , with @xmath51 being the smallest integer such that @xmath53 .",
    "the hlder class may be viewed intuitively as the set of functions whose derivatives of fractional order @xmath2 belong to @xmath54 .",
    "an important observation is that there is a conflict between margin and smoothness assumptions , in the sense that raising the smoothness @xmath2 limits the range of @xmath0 in the margin condition .",
    "for example , when @xmath20 is the lebesgue measure on @xmath55 , it is easily checked that the constraint @xmath56 must hold as soon as the bayes boundary @xmath57 has nonzero @xmath58-dimensional hausdorff measure .",
    "an instance of a convergence result exploiting ( i ) and ( ii ) , theorem  4.3 in @xcite , is that under the assumption that the density of @xmath20 with respect to the lebesgue measure is uniformly bounded , certain classifiers based on plug - in rules achieve in expectation the rate @xmath59 if the margin assumption holds with parameter @xmath0 , and if @xmath38 belongs to the hlder class @xmath46 .    the classification algorithms that we study in this paper have natural links with the process of approximating the regression function using piecewise constant or piecewise polynomials on adaptive partitions , which is an instance of _ nonlinear approximation_. it is well known in approximation theory that , when using nonlinear methods , the smoothness condition needed to attain a specified rate can be dramatically weakened .",
    "this state of affairs is reflected in the convergence results for our algorithms that are given in theorems [ settheorem ] and [ settheorem2 ] .",
    "these results say that with high probability ( larger than @xmath60 where @xmath61 can be chosen arbitrarily large ) , our classifiers achieve the rate @xmath62 if the margin assumption holds with parameter @xmath0 and if @xmath38 belongs to the besov space @xmath63 with @xmath64 such that @xmath65 .",
    "this besov space is defined by the condition @xmath66 with @xmath51 being an integer such that @xmath67 and may be viewed as the set of function whose derivatives of fractional order @xmath2 belong to @xmath68 .",
    "notice that the constraint @xmath65 ensures that @xmath69 is compactly embedded in @xmath54 .",
    "therefore our algorithm achieves the same rate as ( [ piholderrate ] ) , save for the logarithm , however , with a significant weakening on the smoothness condition imposed on the regression function because of the use of adaptive partitioning .",
    "in particular , an individual regression function may have significantly higher smoothness @xmath2 in this scale of besov spaces than in the scale of hlder spaces , resulting in a better rate when using our classifier .",
    "in addition , the weaker smoothness requirement for a given convergence rate allows us to alleviate the conflict between smoothness and margin conditions in the sense that the constraint @xmath56 can be relaxed when using the space @xmath63 ; see  ( [ weaker ] ) .",
    "let us also observe that our risk bound in ( [ treerate ] ) holds in the stronger sense of high probability , rather than expectation , and that no particular assumption ( such as equivalence with lebesgue measure ) is made on the density of @xmath20 .",
    "finally , let us stress that our algorithms are numerically implementable and do not require the a priori knowledge of the parameters @xmath0 and @xmath2 .",
    "the distinction between theorems [ settheorem ] and [ settheorem2 ] is the range of @xmath2 for which they apply .",
    "theorem [ settheorem ] only applies to the range @xmath70 and can be seen as the analog of using piecewise constant approximation on adaptive partition for plug - in estimators .",
    "on the other hand , theorem [ settheorem2 ] applies for any @xmath71 .",
    "the gain in the range of @xmath2 results from the fact that the algorithm uses decorated trees .",
    "this corresponds to piecewise affine approximation for plug - in methods . in principle",
    ", one can extend the values of @xmath2 arbitrarily by using higher polynomial order decorated trees .",
    "however , the numerical implementation of such techniques becomes more problematic and is therefore not considered in this paper . in the regression context , piecewise polynomial estimators on adaptive partitions have been considered .",
    "set estimators aim at approximating the bayes set @xmath37 by elements @xmath14 from a family of sets @xmath72 in the sense of the distance defined by the excess risk .",
    "our approach to deriving the risk bounds in theorems [ settheorem ] and [ settheorem2 ] is by splitting this risk into @xmath73 where @xmath74 .",
    "the two terms are positive , and are , respectively , referred to as the estimation error and approximation error .",
    "we bound in section  [ sec2 ] the estimation error by introducing a certain modulus , which is defined utilizing any available uniform estimate between the quantity @xmath75 and its empirical counterpart computed from the draw . for set estimators based on empirical risk minimization",
    ", we show in section  [ sec3 ] how margin conditions can be used to bound this modulus , and therefore the estimation error term .    in section  [ sec4 ] ,",
    "we turn to estimates for the approximation term .",
    "this analysis is based on the smoothness of @xmath38 and the margin condition . a typical setting when",
    "building set classifiers is a nested sequence @xmath76 of families of sets , that is , @xmath77 , where @xmath51 describes the complexity of @xmath78 in the sense of vc dimension .",
    "the value of @xmath51 achieving between the optimal balance between the estimation and approximation terms depends on the parameters @xmath0 and @xmath2 , which are unknown .",
    "a standard model selection procedure is discussed in section  [ sec5 ] that reaches this balance for a variety of model classes @xmath79 over a range of @xmath0 and @xmath2 .",
    "many ingredients of our analysis of general classification methods appear in earlier works ; see , for example , @xcite .",
    "however , in our view , the organization of the material in these sections helps clarify various issues concerning the roles of approximation and estimation error bounds .    in section  [ sec6 ]",
    ", we turn to our proposed classification methods based on adaptive partitioning .",
    "we analyze their performance using the results from the previous sections and obtain the aforementioned theorems [ settheorem ] and [ settheorem2 ] . the implementation and complexity of these algorithms",
    "are discussed in section  [ sec7 ] .",
    "[ sgeneral ] in view of @xmath80 , if @xmath81 is any empirical estimator for @xmath82 , a natural way to select a classifier within @xmath72 is by @xmath83 one of the most common strategies for building @xmath81 is by introducing the empirical counterparts to ( [ defs ] ) , @xmath84 the choice @xmath85 is equivalent to minimizing the empirical risk over the family  @xmath72 , namely choosing @xmath86 however , other choices of @xmath81 are conceivable , leading to other classifiers .",
    "we give in this section a general method for bounding the estimation error , whenever we have an empirical estimator @xmath81 for @xmath82 , with a bound of the form @xmath87 for each set @xmath88 . in the case where we use for @xmath81 the set estimators @xmath89 defined in ( [ empiricaleta ] )",
    ", we have the following bound .    [ vctheorem ] for any sufficiently large constant @xmath44 ,",
    "the following holds .",
    "if @xmath90 is a collection of @xmath20 measurable sets @xmath91 with finite vc dimension @xmath92 , and if @xmath93 where @xmath61 is arbitrary , then there is an absolute constant @xmath94 such that for any @xmath95 , with probability at least @xmath96 on the draw @xmath97 , we have @xmath98    the techniques for proving this result are well known in classification , but we can not find any reference that give the bounds in this theorem in the above form , and therefore we give its proof in the supplementary material  @xcite .",
    "[ finites ] the above theorem covers , in particular , the case where @xmath72 is a _",
    "finite _ collection of sets , since then trivially @xmath99 .",
    "alternatively , in this case , a  straightforward argument using bernstein s inequality yields the same result with the explicit expression @xmath100 and probability at least @xmath101 .    to analyze the estimation error in classifiers ,",
    "we define the following modulus : @xmath102 notice that the second argument @xmath103 is not a number but rather a set function . in the next section",
    ", we discuss this modulus in some detail and bring out its relation to other ideas used in classification , such as margin conditions . for now , we use it to prove the following theorem .",
    "[ th31 ] suppose that for each @xmath104 , we have that ( [ fundamental ] ) holds with probability @xmath105 .",
    "then with this same probability , we have @xmath106 with @xmath107 being the approximation error from ( [ errsplit ] ) .",
    "we consider any data @xmath108 such that ( [ fundamental ] ) holds and prove that ( [ want ] ) holds for such @xmath108 .",
    "let @xmath109 and @xmath110 so that @xmath111 .",
    "notice that , in contrast to @xmath112 and @xmath113 , the sets @xmath114 are generally not in @xmath72 .",
    "we start from the equality @xmath115 we can assume that @xmath116 , since otherwise we have nothing to prove . from the definition of @xmath113 , we know that @xmath117 . using this in conjunction with  ( [ fundamental ] ) , we obtain @xmath118 in going further , we introduce the following notation . given a set @xmath91 , we denote by @xmath119 and @xmath120 . thus @xmath121 on @xmath122 and @xmath123 on @xmath124 . also @xmath125 and @xmath126 .",
    "hence we can write @xmath127 , where @xmath128 and @xmath129 .",
    "note that @xmath130 .",
    "we consider two cases .    if @xmath131 , then @xmath132 where we have used the fact that @xmath133 and @xmath134 .",
    "if @xmath135 , then , by ( [ implies ] ) , @xmath136 this means that @xmath113 is one of the sets appearing in the definition of @xmath137 , and  ( [ want ] ) follows in this case from the fact that @xmath138    from theorem [ th31 ] , we immediately obtain the following corollary .    [ cor1 ] suppose that for each @xmath104 , ( [ fundamental ] ) _ holds _ with probability @xmath105 . then with this same probability we have @xmath139    [ remlebesgue ] the corollary does not impose any particular assumption on @xmath33 and @xmath72 , apart from finite vc dimension . for later comparisons with existing results ,",
    "we briefly mention how @xmath140 can be sharpened under additional assumptions on  @xmath20 .",
    "assume that @xmath20 is ( or is equivalent to ) the lebesgue measure , and for any arbitrary integer @xmath141 , consider a uniform partition @xmath142 of @xmath143^d$ ] into @xmath144 cubes of side length @xmath145 , providing the collection @xmath72 of all sets @xmath14 that are unions of cubes from @xmath142 .",
    "then , defining @xmath146 application of a union bound on bernstein s inequality to the random variable @xmath147 gives @xmath148 where @xmath149 is an absolute constant depending on @xmath150 .    [ rempi ] theorem [ th31 ] can be applied to any classification method that is based on an estimation @xmath81 of @xmath82 , once the bounds for @xmath151 in terms of @xmath140 have been established for all @xmath88 .",
    "this determines @xmath137 and thereby gives a bound for the estimation error .",
    "[ rem1 ] the usual approach to obtaining bounds on the performance of classifiers is to assume at the outset that the underlying measure @xmath33 satisfies a margin condition .",
    "our approach is motivated by the desire to obtain bounds with no assumptions on @xmath33 .",
    "this is accomplished by introducing the modulus @xmath152 .",
    "as we discuss in the following section , a margin assumption allows one to obtain an improved bound on @xmath152 and thereby recover existing results in the literature .",
    "another point about our result is that we do not assume that the bayes classifier @xmath37 lies in @xmath72 . in some approaches , as discussed in the survey @xcite , one first bounds the estimation error under this assumption , and then later removes this assumption with additional arguments that employ margin conditions .",
    "[ smargin ] the modulus @xmath152 introduced in the previous section is not transparent and , of course , depends on the set function @xmath140 . however , as we now show , for the types of @xmath103 that naturally occur , the modulus is intimately connected with margin conditions .",
    "margin assumptions are one of the primary ingredients in obtaining estimates on the performance of empirical classifiers .",
    "the margin condition ( [ tsy1 ] ) recalled in the has the following equivalent formulation : for any measurable set @xmath14 , we have @xmath153 \\label{tsy}\\ ] ] for some constant @xmath154 and @xmath155 $ ] .",
    "this condition is void when @xmath156 and becomes more stringent as @xmath157 tends to @xmath158 .",
    "the case @xmath159 gives massart s condition .",
    "in going further , we define @xmath160 as the set of all measures @xmath33 such that @xmath20 satisfies  ( [ tsy1 ] ) or equivalently ( [ tsy ] ) , and we define @xmath161 we want to bring out the connection between the modulus @xmath152 and the condition  ( [ tsy ] ) . in the definition of @xmath152 and its application to bounds on the estimation error ,",
    "we assume that we have an empirical estimator for which ( [ fundamental ] ) holds with probability @xmath105 .",
    "notice that this is only assumed to hold for sets @xmath88 which is a distinction with ( [ tsy ] ) .",
    "we shall make our comparison first when @xmath162 is of the form @xmath163 as appears for set estimators in theorem [ vctheorem ] .",
    "we introduce the function @xmath164 where now in this definition , we allow arbitrary measurable sets @xmath14 ( not necessarily from @xmath72 ) . under our assumption on the form of @xmath103 , we have @xmath165 , and so the decay of @xmath166 gives us a bound on the decay of @xmath152 .",
    "we say that @xmath33 satsifies the @xmath166-condition of order @xmath167 if @xmath168 for some constants @xmath169 and @xmath167 .",
    "[ nclemma ] suppose that @xmath33 is a measure that satisfies ( [ tsy1 ] ) for a given value of @xmath170 .",
    "then @xmath33 satisfies the @xmath171-condition ( [ new ] ) for @xmath172 with @xmath94 depending only on @xmath173 and @xmath0 .",
    "conversely , if @xmath174 satisfies the @xmath166-condition with @xmath172 and a constant @xmath175 , then it satisfies ( [ tsy1 ] ) for the value @xmath0 with the constant @xmath173 depending only on @xmath1 and @xmath94 .",
    "suppose that @xmath33 satisfies ( [ tsy1 ] ) for @xmath0 and constant @xmath173 , which equivalently means that it satisfies ( [ tsy ] ) for @xmath176 and a constant @xmath177 . to check that the @xmath166-condition is satisfied for @xmath178 , we let @xmath179 $ ] be fixed and let @xmath14 be such that @xmath180 . from ( [ tsy ] ) , @xmath181 from this ,",
    "one easily derives @xmath182 with a constant @xmath183 depending only on @xmath177 and @xmath157 . to see this ,",
    "suppose to the contrary that for some ( arbitrarily large ) constant @xmath183 @xmath184 rewriting ( [ know1 ] ) as @xmath185 and using ( [ contrary ] ) to estimate @xmath186 on both sides from below , we obtain @xmath187 since @xmath188 , we have @xmath189 , which yields @xmath190 when @xmath183 is chosen large enough , we have @xmath191 which is a contradiction thereby proving ( [ derives ] ) .",
    "it follows from ( [ know1 ] ) and ( [ derives ] ) that @xmath192 where @xmath94 depends on @xmath177 and @xmath157 .",
    "taking now a supremum over all such sets @xmath14 gives @xmath193 which is the desired inequality .",
    "we now prove the converse .",
    "suppose that @xmath33 satisfies the @xmath171-condition of order @xmath172 with constant @xmath94 .",
    "we want to show that @xmath194 with @xmath195 depending only on @xmath1 and @xmath94 . as we noted before , this is equivalent to condition ( [ tsy ] ) of order @xmath196 . to prove ( [ show ] ) , it is enough to prove @xmath197 since then ( [ show ] ) follows easily by a summation argument .",
    "we fix @xmath198 and define @xmath199 and @xmath200 $ ]",
    ". then we have @xmath201 this means that @xmath14 is an admissible set in the definition of @xmath202 in ( [ phidef ] ) . hence from the @xmath166-condition ( [ new ] ) , we know @xmath203 in other words , we have @xmath204 which completes the proof .",
    "the purpose of this section is to show the connection of the modulus @xmath205 with the existing and well - studied margin conditions .",
    "however , the estimates for performance given in ( [ want1 ] ) can be applied without any specific assumption such as a margin condition , which corresponds to the case @xmath156 .",
    "one could also examine other types of bounds for @xmath206 than the power bound ( [ new ] ) and obtain similar results .",
    "[ sbias ] the approximation error @xmath208 depends on @xmath33 and the richness of the collection @xmath72 .",
    "a typical setting starts with a nested sequence @xmath209 of families of sets , that is such that @xmath77 for all @xmath210 .",
    "the particular value of @xmath51 and the collection @xmath211 that is used for a given draw of the data depends on @xmath35 and properties of @xmath33 ( such as the smoothness of @xmath38 and margin conditions ) and is usually chosen through some form of model selection as discussed further . in order to analyze the performance of such classification algorithms , we would like to know conditions on @xmath33 that govern the behavior of the approximation error as @xmath212 .",
    "we study results of this type in this section .    the error @xmath213 is monotonically decreasing .",
    "we define the approximation class @xmath214 as the set of all @xmath33 for which @xmath215 is finite .",
    "our goal is to understand what properties of @xmath33 guarantee membership in  @xmath216 . in this section , we give sufficient conditions for @xmath33 to be in an approximation classes @xmath217 for both set estimators and plug - in estimators .",
    "these conditions involve the smoothness ( or approximability ) of @xmath38 and margin conditions .",
    "given a measure @xmath33 , it determines the regression function @xmath38 and the bayes set @xmath218 .",
    "we fix such a @xmath33 , and for each @xmath219 , we define the level set @xmath220 .",
    "notice that @xmath221 if @xmath222 . also , @xmath223 for each @xmath224 we define @xmath225 for convenience , we assume that there is always an @xmath226 such that @xmath227 .",
    "[ if no such set exists then one replaces @xmath228 by @xmath229 with @xmath230 arbitrarily small and arrives at the same conclusion ( [ upperest ] ) given below . ]",
    "it follows that @xmath231 if @xmath33 satisfies the margin condition ( [ tsy1 ] ) , then @xmath232 thus a sufficient condition for @xmath33 to be in @xmath216 is that @xmath233 .",
    "the following example illustrates how the margin condition ( [ tsy1 ] ) combined with hlder smoothness of the regression function implies that @xmath33 belongs to the approximation class @xmath217 where @xmath1 depends on the margin and smoothness parameters . to be specific ,",
    "let @xmath143^d$ ] .",
    "let @xmath234 be the collection of dyadic cubes @xmath235 contained in @xmath55 , that is , cubes @xmath236 of the form @xmath237^d)$ ] with @xmath238 and @xmath239 .",
    "let @xmath240 , @xmath241 be the collection of dyadic cubes of sidelength @xmath242 .",
    "let @xmath243 be the collection of all sets of the form @xmath244 , where @xmath245 .",
    "this corresponds to the family @xmath72 considered in remark [ remlebesgue ] for @xmath246 .",
    "in fact , @xmath247 and @xmath248 .",
    "we complete the family @xmath209 by setting @xmath249 when @xmath250 .",
    "[ proplebesgue ] we assume that @xmath33 has the two following properties :    the regression function @xmath38 is in the lipschitz ( or hlder ) space @xmath46 for some @xmath251 , that is , @xmath252    @xmath33 satisfies the margin condition ( [ tsy1 ] )",
    ".    then one has @xmath253    we claim that @xmath254 with @xmath255 .",
    "to this end , we first note that when @xmath256 , and @xmath257 is the center of @xmath235 , then @xmath258 we define @xmath259 as the union of all @xmath260 for which @xmath261 .",
    "if @xmath262 , then we claim that @xmath263 for example , if @xmath264 , then @xmath265",
    ". so , if @xmath266 , then @xmath261 and hence @xmath267 . similarly , if @xmath268 , then @xmath261 and hence @xmath269 for all @xmath270 , and this implies the right containment in ( [ contain1 ] ) .    it is well known that margin and smoothness conditions are coupled , in the sense that higher values of @xmath0 force the regression function to have a sharper transition near the bayes boundary , therefore putting restrictions on its smoothness . as an example , assume that @xmath20 is bounded from below by the lebesgue measure , that is , there exists a constant @xmath271 such that for any @xmath88 , @xmath272 in the most typical setting , the bayes boundary @xmath57 is a ( @xmath273)-dimensional surface of nonzero @xmath274 hausdorff measure . if @xmath275 with @xmath276 , then @xmath277 is smaller than @xmath278 at any point @xmath10 which is at distance less than @xmath279 from this boundary .",
    "it follows that @xmath280 where @xmath281 depends on @xmath282 and @xmath283 , showing that @xmath56 . in such a case the approximation rate is therefore limited by @xmath284 .",
    "as observed in @xcite one can break this constraint either by considering pathological examples , such as regression functions that satisfy @xmath285 , or by considering marginal measures @xmath20 that vanish in the vicinity of the bayes boundary .",
    "we show in section [ sadaptiveclass ] that this constraint can also be broken when the hlder spaces @xmath46 are replaced by the besov spaces @xmath63 , defined by ( [ besov - def ] ) , that govern the approximation rate when @xmath243 is replaced by a collection of adaptive partitions .",
    "[ sriskperf ] in this section , we combine our previous bounds for approximation and estimation errors in order to obtain an estimate for risk performance of classification schemes .",
    "let us assume that we have a sequence @xmath209 of families @xmath78 of sets that are used to develop a binary classification algorithm .",
    "we suppose that for some constant @xmath94 , @xmath286 and we denote by @xmath287 the empirical risk minimization classifier picked in @xmath78 according to ( [ empclassifier ] ) with @xmath85 .",
    "theorem [ vctheorem ] gives that such an estimator provides a bound ( [ vcl22 ] ) with @xmath288 and @xmath149 depending only on @xmath150 and @xmath94 .",
    "if @xmath289 , for some @xmath167 , then according to corollary [ cor1 ] , for any @xmath290 , we have with probability @xmath291 , @xmath292 if in addition @xmath33 satisfies the margin condition ( [ tsy1 ] ) of order @xmath293 , then using lemma [ nclemma ] and the fact that @xmath294 , we obtain @xmath295 where @xmath149 depends on @xmath296 . if we balance the two terms appearing on the right in  ( [ risk2 ] ) by taking @xmath297 , we obtain that , with probability at least @xmath291 , @xmath298 where @xmath149 depends on @xmath296 and @xmath299 .",
    "the best rates that one can obtain from the above estimate correspond to @xmath43 ( massart s condition ) and @xmath300 ( the regression function @xmath38 has arbitrarily high smoothness ) , and are limited by the so - called fast rate @xmath301 .    to obtain the bound ( [ risk3 ] ) , we need to know both @xmath1 and @xmath302 in order to make the optimal choice of @xmath51 and @xmath78 .",
    "of course , these values are not known to us , and to circumvent this we employ a standard model selection technique based on an independant validation sample .",
    "let @xmath76 be any collection of set estimators . for notational convenience ,",
    "we assume that @xmath35 is even , that is , @xmath303 .",
    "given the draw @xmath108 , we divide @xmath108 into two independent sets @xmath304 and @xmath305 of equal size @xmath306 .    for each @xmath307 , we let @xmath308 be defined by ( [ empclassifier ] ) with @xmath309 and @xmath108 replaced by @xmath304 .    we now let @xmath310 and let @xmath311 be the set chosen from  @xmath312 by ( [ empclassifier ] ) when using @xmath305 .",
    "the set @xmath313 is our empirical approximation of @xmath37 obtained by this model selection procedure . to see how well it performs ,",
    "let us now assume that @xmath314 and that @xmath33 also satisfies the margin condition ( [ tsy1 ] ) for @xmath0 . in step  1 , we know that for each @xmath51 , @xmath315 satisfies ( [ risk2 ] ) with @xmath35 replaced by @xmath306 with probability at least @xmath316",
    ". thus , with probability @xmath317 , we have @xmath318\\\\[-8pt ] \\eqntext{m=1,\\ldots,\\bar n.}\\end{aligned}\\ ] ] it follows that for @xmath312 of step  2 , we have @xmath319 since @xmath320 , we can take @xmath321 in remark  [ finites ] and a suitable constant @xmath149 when bounding performance on @xmath312 .",
    "hence , from corollary [ cor1 ] , we have for the set @xmath313 given by step  2 , @xmath322 in estimating the minimum , we choose @xmath51 that balances the two terms and obtain @xmath323 thus , the set @xmath313 , while not knowing @xmath0 and @xmath1 gives the same estimate we obtained earlier when assuming we knew @xmath0 and @xmath1 .",
    "note that we have done our model selection without using a penalty term .",
    "the use of a penalty term would have forced us to know the value of @xmath0 in ( [ tsy ] ) .",
    "a discussion of why penalty approaches may still be of interest in practice can be found in  @xcite .    a simple application of ( [ risk31 ] ) and proposition [ proplebesgue ] gives an estimate in the general case . in the case of remark [ remlebesgue ]",
    "one has @xmath324 and can balance the terms in the estimate corresponding to ( [ risk31 ] ) by taking @xmath325 .",
    "these give the following result .",
    "[ corprelim ] let @xmath209 be the sequence of family of sets built from uniform partitions as are used in proposition [ proplebesgue ] .",
    "assume that @xmath33 satisfies the margin condition ( [ tsy1 ] ) of order @xmath0 and that @xmath38 is hlder continuous of order @xmath2 .",
    "then the classifier resulting from the above model selection satisfies @xmath326 where @xmath149 depends on @xmath327 .",
    "if one assumes in addition that @xmath20 is equivalent to the lebesgue measure , one obtains @xmath328\\\\[-8pt]\\nonumber & & \\qquad \\ge1-cn^{-r}.\\end{aligned}\\ ] ]    case   illustrates the improvement of the rates that results from constraining the marginal @xmath20 . in view of our earlier comments on the conflict between margin and hlder smoothness conditions of high order ,",
    "the main deficiency of both results is the underlying strong assumption of hlder smoothness .",
    "the sequence @xmath209 is based on uniform partitions and does not allow us to exploit weaker besov - smoothness conditions .",
    "in what follows , we remedy this defect by turning to classification algorithms based on _ adaptive _ partitioning . in doing so ,",
    "we avoid any a priori constraints on @xmath20 and hence use the set function @xmath103 given by ( [ vcl11 ] ) .",
    "[ sadaptiveclass ] one of the most natural ways to try to capture @xmath37 is through adaptive partitioning .",
    "indeed , such partitioning methods have the flexibility to give fine scale approximation near the boundary of @xmath37 but remain coarse away from the boundary .",
    "we now give two examples .",
    "the first is based on simple dyadic tree partitioning , while the second adds wedge ornation on the leaves of the tree to enhance risk performance . for simplicity of presentation , we only consider dyadic partitioning on the specific domain @xmath143^d$ ] , even though our analysis covers far greater generality .",
    "we recall the dyadic cubes @xmath234 introduced in section [ sbias ] .",
    "these cubes organize themselves into a tree with root @xmath55 .",
    "each @xmath256 has @xmath329 children which are its dyadic subcubes from @xmath330 .",
    "a finite subtree @xmath331 of @xmath234 is a finite collection of cubes with the property that the root @xmath55 is in @xmath331 , and whenever @xmath332 its parent is also in @xmath331 .",
    "we say a tree is _ complete _ if , whenever @xmath235 is in @xmath331 , then all of its siblings are also in @xmath331 .",
    "the set @xmath333 of leaves of such a tree @xmath331 consists of all the cubes @xmath332 such that no child of @xmath235 is in @xmath331 .",
    "the set of all such leaves of a complete tree forms a partition of @xmath55 .",
    "any finite complete tree is the result of a finite number of successive cube refinements .",
    "we denote by @xmath334 the collection of all complete trees @xmath331 that can be obtained using @xmath51 refinements .",
    "any such tree @xmath335 has @xmath336 leaves .",
    "we can bound the number of trees in @xmath335 by assigning a bitstream that encodes , that is , precisely determines , @xmath331 as follows .",
    "let @xmath335 .",
    "we order the children of  @xmath55 lexicographically and assign a one to every child which is refined in @xmath331 and a zero otherwise .",
    "we now consider the next generation of cubes ( i.e. , the grandchildren of  @xmath55 ) in @xmath331 . we know",
    "these grandchildren from the bits already assigned .",
    "we arrange the grandchildren lexicographically and again assign them a one if they are refined in @xmath331 and a zero otherwise .",
    "we continue in this way and receive a bitstream which exactly determines @xmath331 .",
    "since @xmath331 has exactly @xmath337 cubes , every such bitstream has length @xmath338 and has a one in exactly @xmath339 positions .",
    "hence we have @xmath340 for each @xmath335 and any @xmath341 , we define @xmath342 .",
    "we denote by @xmath78 the collection of all such sets @xmath14 that can be obtained from a @xmath335 and some choice of @xmath343 .",
    "once @xmath331 is chosen , there are @xmath344 choices for @xmath345 . hence @xmath346 with @xmath347 .",
    "given our draw @xmath108 , we use the set estimator and model selection over @xmath76 as described in the previous section .",
    "we discuss the numerical implementation of this algorithm in section [ ssnumerical ] .",
    "this results in a set @xmath348 , and we have the following theorem for its performance .    [ settheorem ] for any @xmath61 , there is a constant @xmath271 such that the following holds . if @xmath314 , @xmath167 and @xmath33 satisfy the margin condition ( [ tsy1 ] ) , then with probability greater than @xmath349 , we have @xmath350 with @xmath149 depending only on @xmath351 , @xmath352 and the constant in ( [ tsy1 ] ) .",
    "if @xmath353 with @xmath354 and @xmath355 and if @xmath33 satisfies the margin condition ( [ tsy1 ] ) , then with probability greater than @xmath349 , we have @xmath356 with @xmath149 depending only on @xmath351 , @xmath357 and the constant in ( [ tsy1 ] ) .    since @xmath358 where @xmath94 depends only on @xmath359 , we have that @xmath360 is bounded by the right - hand side of ( [ risk31 ] ) which proves ( i ) . we can derive ( ii ) from ( i ) if we prove that the assumptions on @xmath33 in ( ii ) imply that , @xmath361 . to see that this is the case",
    ", we consider the approximation of @xmath38 by piecewise constants subordinate to partitions @xmath362 , @xmath335 .",
    "it is known ( see  @xcite ) that the besov space assumption on @xmath38 implies that there is a tree @xmath363 and piecewise constant @xmath364 on @xmath365 that satisfies @xmath366 with @xmath367 depending on @xmath368 and @xmath359 .",
    "let @xmath369 and @xmath370",
    ". then @xmath371 and @xmath372 , and so @xmath373 as desired .",
    "we want to remove the restriction @xmath70 that appears in theorem [ settheorem ] by enhancing the family of sets @xmath78 of the previous section .",
    "this enhancement can be accomplished by choosing , for each @xmath374 , a subcell of @xmath235 obtained by a hyperplane cut ( henceforth called an _",
    "h - cell _ ) and then taking a union of such subcells . to describe this",
    ", we note that , given a dyadic cube @xmath235 , any ( @xmath273)-dimensional hyperplane @xmath375 partitions @xmath235 into at most two disjoint sets @xmath376 and @xmath377 which are the intersections of @xmath235 with the two open half spaces generated by the hyperplane cut . by convention",
    "we include @xmath378 in @xmath376 .",
    "given a tree @xmath335 , we denote by @xmath379 any mapping that assigns to each @xmath380 an h - cell @xmath381 . given such a collection @xmath382 , we define @xmath383 for any given tree @xmath331 , we let @xmath384 be the collection of all such sets that result from arbitrary choices of @xmath385 . for any @xmath290 , we define @xmath386 thus any such @xmath387 is the union of h - cells of the @xmath388 , with one h - cell chosen for each @xmath380 .",
    "clearly @xmath78 is infinite , however , the following lemma shows that @xmath78 has finite vc dimension .",
    "[ vclemma1 ] if @xmath389 are each collections of sets from @xmath55 with vc dimension @xmath390 , then the collection @xmath391 has vc dimension not greater than @xmath392 .",
    "we follow the notation of section  9.4 in @xcite .",
    "let us consider any set of points @xmath393 from @xmath55 .",
    "then from theorem  9.2 in @xcite , the shattering number of @xmath394 for this set of point satisfies @xmath395 and therefore @xmath396 by hoeffding s inequality , if @xmath397 , we have @xmath398 with @xmath399 .",
    "it follows that if @xmath400 , we have @xmath401 which shows that @xmath402 .",
    "we apply lemma [ vclemma1 ] with the role of the @xmath403 being played by the collection @xmath384 , @xmath335 .",
    "as shown in ( [ count1 ] ) , we have @xmath404 .",
    "we note next that the vc dimension of each @xmath384 is given by @xmath405 in fact , given @xmath331 placing @xmath406 points in every @xmath407 shows that @xmath408 points can be shattered since @xmath406 points can be shattered by hyperplanes in @xmath409 .",
    "no matter how one distributes more than @xmath410 points in @xmath55 , at least one @xmath407 contains more than @xmath406 points .",
    "these points can no longer be shattered by a hyperplane which confirms ( [ vcst1 ] ) .",
    "lemma [ vclemma1 ] now says that @xmath411 where @xmath412 .",
    "given our draw @xmath108 , we use the set estimator and model selection as described in section  [ sriskperf ] with @xmath78 now given by ( [ sm ] ) .",
    "this results in a set @xmath348 , and we have the following theorem for the performance of this estimator .",
    "[ settheorem2 ] for any @xmath61 , there is a constant @xmath271 such that the following holds .",
    "if @xmath314 , @xmath167 and @xmath33 satisfy margin condition ( [ tsy1 ] ) , then with probability greater than @xmath349 , we have @xmath413 with @xmath149 depending only on @xmath351 , @xmath352 and the constant in ( [ tsy1 ] ) .",
    "if @xmath353 with @xmath414 and @xmath415 and if @xmath33 satisfies the margin condition ( [ tsy ] ) , then with probability greater than @xmath349 , we have @xmath416 with @xmath149 depending only on @xmath351 , @xmath357 and the constant in ( [ tsy1 ] ) .    in view of ( [ vcsm ] ) we can invoke theorem  [ vctheorem ] with @xmath417 , where @xmath149 depends on @xmath359 and @xmath150 , to conclude that @xmath418 satisfies  ( [ vcl22 ] ) and hence is an admissible set function for the modulus ( [ defmod ] ) . now",
    "( i ) follows from ( [ risk31 ] ) .    to derive ( ii ) from ( i ) , we prove that the assumptions on @xmath33 in ( ii ) imply that @xmath314 , @xmath419 , for @xmath420 $ ] . to see that this is the case , we consider the approximation of @xmath38 by piecewise _ linear _ functions subordinate to partitions @xmath333 ,",
    "it is known ( see @xcite ) that the besov space assumption on @xmath38 implies that there is a tree @xmath363 and a piecewise linear function @xmath364 on @xmath421 that satisfies @xmath422 .",
    "now for any cube @xmath235 consider the h - cell mapping @xmath423 .",
    "then @xmath424 is in @xmath78 and @xmath372 so that @xmath425 as desired .",
    "it is , in theory , possible to further extend the range of @xmath2 by considering more general decorated trees , where for each considered cube @xmath235 , we use an algebraic surface @xmath28 of degree @xmath426 instead of a hyperplane @xmath375 that corresponds to the case @xmath427 .",
    "the resulting families @xmath78 consist of level sets of piecewise polynomials of degree @xmath428 on adaptive partitions obtained by @xmath51 splits . from this",
    "one easily shows that the corresponding vc dimension is again controlled by @xmath51 ( with multiplicative constants now depending both on @xmath359 and @xmath428 ) and that ( [ settheorem13 ] ) now holds for all @xmath429 .",
    "however , the practical implementation of such higher order classifiers appears to be difficult .",
    "we have seen in section  [ sec5 ] that the approximation rate for nonadaptive partitioning is also given by @xmath430 , but with @xmath2 denoting the smoothness of @xmath38 in the sense of the hlder space @xmath46 .",
    "the results established in this section show that the same approximation rate is obtained under the weaker constraint that @xmath431 with @xmath415 if we use adaptive partitioning .",
    "we also observed in section  [ sec5 ] that the hlder smoothness @xmath2 and the parameter @xmath0 in the margin condition are coupled , for example , by the restriction @xmath56 when @xmath20 is bounded from below by the lebesgue measure .",
    "replacing the hlder space @xmath46 by a besov space @xmath432 with @xmath415 allows us to relax the above constraint . as a simple example consider the case where @xmath20 is the lebesgue measure and @xmath433 for some @xmath434 , so that @xmath435 , and margin condition ( [ tsy1 ] ) holds with @xmath0 up to @xmath436 .",
    "then one checks that @xmath431 for @xmath2 and @xmath437 such that @xmath438 .",
    "the constraint @xmath439 may then be rewritten as @xmath440 or equivalently @xmath441 which is an improvement over @xmath56 .",
    "[ ssnumerical ] the results we have presented thus far on adaptive partitioning do not constitute a numerical algorithm since we have not discussed how one would find the sets @xmath442 required by ( [ empclassifier ] ) and used in the model selection .",
    "we discuss this issue next .    given the draw @xmath108 , we consider the collection of all dyadic cubes in @xmath443 with @xmath444 which contain an @xmath445 , @xmath446 .",
    "these cubes form a tree @xmath447 which we call the _ occupancy tree_. adding to all such cubes their siblings , we obtain a complete tree @xmath448 whose leaves form a partition of @xmath55 .",
    "let us first discuss the implementation of algorithm i. for each complete subtree @xmath449 we define @xmath450 which we call the _ energy _ in @xmath331 .",
    "the set estimator @xmath287 corresponds to a complete tree @xmath451 which maximizes the above energy .",
    "note that several different trees may attain the maximum .",
    "since only the values @xmath452 are considered in the model selection procedure , and since there is no gain in subdividing a nonoccupied cube , a maximizing tree is always a subtree of @xmath448 .",
    "further , for each cube @xmath453 , we denote by @xmath454 the collection of all complete trees @xmath331 with root @xmath235 obtained using at most @xmath51 subdivisions and being contained in @xmath448 .",
    "we then define @xmath455 again , this maximum may be attained by several trees in @xmath456 .",
    "in fact , if , for instance , for a maximizer @xmath457 , @xmath458 holds for all @xmath459 , the children of some parent node @xmath460 , then the subtree @xmath461 of @xmath331 obtained by removing @xmath462 from @xmath331 , has the same energy .",
    "we denote by @xmath463 any tree in @xmath456 that attains the maximum @xmath464 . by convention ,",
    "we set @xmath465 when @xmath235 is not occupied . with this notation",
    ", we define @xmath466 to be used in the model selection discussed earlier .",
    "we now describe how to implement the maximization that gives @xmath467 and therefore @xmath287 .",
    "notice that @xmath468 and @xmath463 is empty when @xmath235 is not occupied , and therefore these values are available to us for free .",
    "thus the computational work in this implementation is solely determined by the occupied cubes that form @xmath447 . for @xmath469 , we define @xmath470 the set of occupied cubes of resolution level @xmath471 . notice that @xmath472 .",
    "we work from the leaves of @xmath447 toward the root , in a manner similar to cart optimal pruning ( see @xcite ) , according to the following steps :    * @xmath473 : we compute for each @xmath474 the quantities @xmath475 and define @xmath476 , @xmath477 .",
    "this requires at most @xmath306 arithmetic operations .",
    "* for @xmath478 : suppose we have already determined the quantities @xmath479 and @xmath475 , as well as the trees @xmath480 , for all @xmath481 and @xmath482 .",
    "recall that @xmath480 is a complete subtree .",
    "now for all @xmath483 and all cubes @xmath484 , we compute @xmath485 where @xmath486 denotes the set of occupied children of @xmath235 .",
    "notice that the above argmax may not be unique , in which case we can pick any maximizer .",
    "we obviously have for each @xmath487 and any @xmath488 , @xmath489 with @xmath490 for @xmath491 , we compute the @xmath475 for all @xmath492 by summing the @xmath493 for @xmath494 and define @xmath495 and @xmath496 .",
    "* at the final step @xmath497 , the set @xmath498 consists only of the root @xmath55 , and we have computed @xmath499 for @xmath500 .",
    "this provides the estimators @xmath287 for @xmath500 .    to estimate the complexity of the algorithm",
    ", we need to bound for each @xmath501 the number of computations required by ( [ kr ] ) and ( [ energy ] ) . with proper organization , the argmax in ( [ kr ] )",
    "can be found using at most@xmath502 operations",
    ". we can execute ( [ energy ] ) with the same order of computation .",
    "the total complexity over all levels is therefore at most @xmath503 [ a  finer analysis can reduce it to  @xmath504 .",
    "also each optimal tree @xmath463 can be recorded with at most @xmath505 bits .",
    "it should be noted that the complexity with respect to the data size @xmath35 is independent of the spatial dimension @xmath359 which only enters when encoding the optimal trees @xmath499 .",
    "we turn now to the implementation of algorithm ii .",
    "we denote by @xmath506 the set of all ( @xmath273)-dimensional hyperplanes . using the notation therein , for any subtree @xmath331 of @xmath448 and any @xmath380 ,",
    "the energy is now defined as @xmath507 the set estimator @xmath287 corresponds to a tree @xmath508 which maximizes the above energy .",
    "similarly to the previous discussion , we define @xmath509 and define as before @xmath464 and @xmath463 by ( [ energy1 ] ) and ( [ energy2 ] ) .",
    "the procedure of determining the trees @xmath499 for @xmath510 is then , in principle , the same as above , however with a significant distinction due to the search for a `` best '' hyperplane @xmath511 that attains the maximum in ( [ ornate1 ] ) .",
    "since a cube @xmath235 contains a finite number @xmath512 of data , the search can be reduced to @xmath513 hyperplanes and the cost of computing @xmath514 is therefore bounded by @xmath515 . in addition , the search of @xmath516 needs to be performed on _ every _ cube @xmath453 , so that a crude global bound for this cost is given by @xmath517 .",
    "this additional cost is affordable for small @xmath359 but becomes prohibitive in high dimension .",
    "an alternate strategy is to rely on more affordable classifiers to produce an affine ( or even higher order algebraic ) decision boundary on each @xmath235 .",
    "examples are plug - in classifiers that are based on least - square estimation of @xmath38 on @xmath235 by a polynomial .",
    "the authors wish to thank stephane gaiffas and lszl gyrfi for various valuable suggestions and references , as well as the anonymous referees for their constructive comments ."
  ],
  "abstract_text": [
    "<S> algorithms for binary classification based on adaptive tree partitioning are formulated and analyzed for both their risk performance and their friendliness to numerical implementation . </S>",
    "<S> the algorithms can be viewed as generating a set approximation to the bayes set and thus fall into the general category of _ set estimators_. in contrast with the most studied tree - based algorithms , which utilize piecewise constant approximation on the generated partition [ _ ieee trans . </S>",
    "<S> inform . theory _ </S>",
    "<S> * 52 * ( 2006 ) 13351353 ; _ mach . </S>",
    "<S> learn . </S>",
    "<S> _ * 66 * ( 2007 ) 209242 ] , we consider decorated trees , which allow us to derive higher order methods . </S>",
    "<S> convergence rates for these methods are derived in terms the parameter @xmath0 of margin conditions and a rate @xmath1 of best approximation of the bayes set by decorated adaptive partitions . </S>",
    "<S> they can also be expressed in terms of the besov smoothness @xmath2 of the regression function that governs its approximability by piecewise polynomials on adaptive partition . </S>",
    "<S> the execution of the algorithms does not require knowledge of the smoothness or margin conditions . </S>",
    "<S> besov smoothness conditions are weaker than the commonly used hlder conditions , which govern approximation by nonadaptive partitions , and therefore for a given regression function can result in a higher rate of convergence . </S>",
    "<S> this in turn mitigates the compatibility conflict between smoothness and margin parameters .    ,    , </S>"
  ]
}