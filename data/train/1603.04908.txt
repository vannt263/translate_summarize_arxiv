{
  "article_text": [
    "our visual sensation is developed along with the neuromotor system while interacting with surrounding objects  @xcite .",
    "this tight interplay between visual sensation and motor signal makes spatial scene and object understanding possible through our interactions with objects .",
    "for instance , consider a woman entering a canned food corner at a grocery store as shown in figure  [ main_fig ] .",
    "when she schemes through hundreds of canned foods to find the tuna can that she looks for , she remains 3 - 5 m from the food stand for efficient search .",
    "once she finds the tuna , she approaches it ( 1 - 3 m ) , and then reaches her left hand to pick the tuna can ( @xmath01 m ) . while she gazes at the expiration date in the label of the can , the distance gets smaller ( @xmath00.5 m ) .",
    "not only does the tuna can stimulate her visual attention but it also affects her physical actions , such as head or hand movements .",
    "we define such object as an action - object ",
    "an object that triggers conscious visual and motor signals .",
    "the key properties of an action - object are : ( 1 ) it facilitates a person s tactile ( touching a cup ) or ( 2 ) visual ( watching a tv ) interactions and ( 3 ) it exhibits a characteristic distance and orientation with respect to the person .",
    "these properties provide a strong cue to predict person s behavior and also to better understand his visual sensorimotor coordination  @xcite .",
    "a fundamental research question is whether we can build a model that can detect action - objects as we observe a person interacting with his environment without a gaze tracking devices or tactile sensors .",
    "this question is challenging despite of recent success of computer vision systems because ( a ) a person s gaze direction does not necessarily correspond to action - objects .",
    "in other words , not all objects within the person s field of view are consciously attended .",
    "( b ) additionally , action - objects are often task - dependent , which makes it difficult to detect them without knowing the task beforehand .",
    "( c ) finally , action - objects are not specific to object category because many object categories can correspond to the same action , e.g. , tv and a mirror both afford a seeing action , and therefore , an object specific model may not be able to represent the action - objects .    in this paper , we address these challenges by leveraging a first - person stereo camera and our proposed egonet model that integrates visual appearance , 3d spatial cues , and 2d first - person spatial information .",
    "using first - person cameras is beneficial because a ) a first - person view contains inherent task - intention via a person s head and body orientation relative to the objects , b ) this task information does not need to be modeled explicitly using action recognition as is commonly done in prior work .",
    "additionally , a first - person camera captures not only what the person sees but also if he is positioned and oriented himself for a specific action .",
    "his relative head orientation indicates what he may be looking at , whereas his body position relative to the objects constrains physical feasibility of an action .",
    "we leverage these first - person properties along with the 3d spatial cues to build a model that predicts action - objects from the first - person rgbd data .",
    "building such model is different from a classic object detection task because action - object is associated with the actions without explicit object categories .",
    "it also differs from a visual saliency detection because the visual saliency does not necessarily correspond to a specific action .",
    "finally , our action - object task differs from activity recognition tasks because we detect action - objects by exploiting common person - object spatial configurations instead of modeling activity - specific interactions , which makes our model applicable to different activities .",
    "we employ wearable stereo cameras to collect the data of 2d visual and 3d spatial information around the person , which includes object interactions during activities such as cooking , shopping , dish - washing , working , etc .",
    "the camera wearer who is aware of the task and who can disambiguate conscious visual attention and subconscious gaze activities provides per - pixel binary labels of the first - person images , which we then use to build our action - object detection model in a form of egonet .",
    "egonet is a joint two - stream rgb and dhg network that takes a first - person rgbd image as an input and predicts a per - pixel action - object likelihood map .",
    "our network is composed of rgb and dhg pathways : one of which learns visual appearance cues , while the other exploits 3d spatial information indicative of action - objects .",
    "the information from both pathways is combined via a joint pathway , which also performs a first - person coordinate embedding to learn an action - object spatial distribution in the first - person image .",
    "the entire network is jointly optimized to the action - object ground truth that is provided by the camera wearer .",
    "we quantitatively justify the architecture choices of our egonet model and show that it outperforms all prior approaches for the action - object detection task across multiple first - person datasets .",
    "we also show that egonet generalizes well on a variety of novel datasets , even when ( 1 ) trained on a small action - object dataset , and ( 2 ) even without being adapted to a specific task as is commonly done  @xcite .",
    "* first - person object detection . * there exist multiple prior methods that explore object detection from first - person images as a main task  @xcite , or as an auxiliary task for activity recognition  @xcite or video summarization  @xcite .",
    "below we summarize how our action - object detection task is different from this prior work .    the work in  @xcite attempts to predict gaze from the first - person images and use it for activity recognition .",
    "however , we know that a person s gaze direction does not always correspond to action - objects but instead capture noisy eye movement patterns , which may not be useful for activity recognition . in the context of our problem , the camera wearer who was performing the task and who can disambiguate conscious visual attention and subconscious gaze activities provides per - pixel binary labels of the first - person images , which we then use to build our action - object model .",
    "the methods in  @xcite perform object detection and activity recognition disjointly : first an object detector is applied to find all objects in the scene , and then those detections are used for activity recognition without necessarily knowing , which objects the person may be interacting with .",
    "furthermore , these methods employ a set of predefined object classes .",
    "however , many object categories can correspond to the same action , e.g. , tv and a mirror both afford a seeing action , and thus , an object class specific model may not be able to represent the action - objects accurately .",
    "some prior work focused specifically on handled object detection  @xcite .",
    "however , action - object detection task also requires detecting conscious visual interactions that do not necessarily involve hand manipulation ( e.g. watching a tv ) .",
    "furthermore , from a development point of view , conscious visual attention is one way for a person to interact with the world .",
    "for instance , for the babies who lack motor skills , their conscious visual attention is the only thing that indicates their action - objects , and thus detecting handled objects only may not be enough .",
    "the most relevant to our action - object detection task is the work in  @xcite that detects objects of interest for activity recognition  @xcite .",
    "however , this method is designed specifically for recognizing various cooking activities , which requires detecting mostly handled - objects as in  @xcite .",
    "thus , the authors  @xcite manually bias their method to recognize objects near hands",
    ". such an approach may not work for other types of activities that do not involve hand manipulation such as watching a tv , interacting with a person , etc .",
    "finally , none of the above methods , fully exploit common person - object spatial configuration .",
    "we hypothesize that a first - person view contains inherent task - intention via a person s head and body orientation relative to the objects . in other words , during an interaction with an object , people position themselves at a certain distance and orientation relative to that object .",
    "thus , 3d spatial information provides essential cues that could be used to recognize action - objects .",
    "we leverage such 3d cues , by using first - person stereo cameras , and building an egonet model that uses 3d depth and height cues to reason about action - objects .    to recap , despite an overlap between the prior work and our action - object detection task",
    ", none of this prior work fully captures the concept of action - object detection task , which requires detecting conscious tactile and visual interactions , and exploiting common person - object spatial configuration without knowing a - priori about the specific task that the person is performing .",
    "thus , an action - object detection task unifies all these prior ideas into a single task .",
    "* structured prediction in first - person data . * a task such as action - object detection or visual saliency prediction requires producing a dense probability output for every pixel . to achieve this goal most prior first - person methods employed a set of hand - crafted features combined with a probabilistic or discriminative classifier .",
    "for instance , the work in  @xcite uses manually engineered set of egocentric features with a linear regression classifier to assign probabilities to each region in a segmented image .",
    "the method in  @xcite exploits the combination of geometric and egocentric cues and trains random forest classifier to predict saliency in first - person images .",
    "the work in  @xcite uses optical flow cues and graph cuts  @xcite to compute handled - object segmentations , whereas  @xcite employs transductive svm to compute foreground segmentation in an unsupervised manner .",
    "finally , some prior work  @xcite integrates a set of hand - crafted features in the graphical model to predict per pixel probabilities of camera wearer s gaze .",
    "we note that the recent introduction of the fully convolutional networks ( fcns )  @xcite has led to remarkable results in a variety of structured prediction tasks such as edge detection  @xcite and semantic image segmentation  @xcite . following this line of work , a recent method  @xcite , used fcns for joint object segmentation and activity recognition in first person images using a two stream appearance and optical flow network with a multi - loss objective function . since the core of our proposed egonet model is also based on fcns , this latter method  @xcite is the most similar to our work .",
    "we stress that there are several essential differences between our egonet model , traditional fcns and the model in  @xcite . in comparison to the traditional fcns and the network in  @xcite ,",
    "our model employs a two - stream architecture that exploits not only visual appearance cues as is common for traditional fcns but also 3d spatial information in a form of dhg ( depth , height , grayscale ) input . in the experimental section ,",
    "we show that such 3d spatial cues are essential for accurate action - object detection .",
    "furthermore , in comparison to the networks in  @xcite , our egonet is specifically adapted to the first - person data via our proposed first - person coordinate embedding , which enables our model to learn an action - object specific spatial distribution .",
    "for instance , a typical fcn will detect an object such as a laptop keyboard regardless of where it appears in the image .",
    "however , in the context of action - object detection task , a laptop keyboard is often seen at the bottom of a first person image because we often look down at it while typing . thus , the keyboard appearing bottom of a first - person image is more likely to be an action - object than the keyboard appearing at the top of a first - person image .",
    "our proposed first - person coordinate embedding allows us to model this intuition .",
    "we show empirically that using such a coordinate embedding allows egonet to achieve superior performance over the models that do not use it .",
    "we use two stereo gopro hero 3 cameras with 100 mm baseline to capture first - person rgbd videos as shown in figure  [ arch ] .",
    "the stereo cameras are synchronized manually and each camera is set to 1280@xmath1960 with 100 fps .",
    "the fisheye lens distortion is pre - calibrated and depth image is computed by estimating disparities between cameras via dense image matching with dynamic programming .",
    "two subjects participated in capturing their daily interactions with objects in activities such as cooking , shopping , working at their office , dining , buying groceries , dish - washing , and staying in a hotel room .",
    "7 scenes were recorded and @xmath2 frames with per - pixel action - objects were annotated by the subjects with grabcut  @xcite .",
    "please find the supplementary material for more details .",
    "we acknowledge that our dataset has a limited number of sequences , and subjects .",
    "however , due to a variety of different activities in our dataset , our egonet model learns to leverage common person - object spatial configurations during people s interactions with objects , which allows it to accurately predict action - objects on novel datasets , even if they contain previously unseen scenes , objects or activities .",
    "thus , in the experimental section , we show that despite the limitations of our dataset , we can use it to build a model that generalizes well on a variety of first - person datasets .",
    "in this section , we describe egonet , a predictive network model that detects action - objects from a first - person rgbd image .",
    "egonet is a two - stream fcn that holistically integrates a set of visual appearance , head direction , and 3d spatial cues , and that is specifically adapted for first - person data via a first - person coordinate embedding .",
    "egonet consists of three distinct pathways : 1 ) a rgb pathway that learns object visual appearance cues ; 2 ) a dhg pathway that learns to detect action - objects based on 3d depth and height measurements around the person ; and a 3 ) a joint pathway that combines the information from both pathways , and which also incorporates our proposed first - person coordinate embedding to model a spatial distribution of action - objects in the first - person view .",
    "the detailed illustration of egonet s architecture is presented in figure  [ arch ] .",
    "we now explain each of egonet s components in more detail .",
    "an action - object that stimulates person s visual attention typically exhibits a particular visual appearance .",
    "for instance , we are more likely to look at the objects that are colored brightly and stand out from the background visually . thus , our model should detect visual appearance cues that are indicative of action - objects . to achieve this goal we use deeplab  @xcite , which is a fully convolutional adaptation of a vgg network  @xcite .",
    "deeplab has been shown to yield excellent results on problems such as semantic segmentation  @xcite .",
    "just like the segmentation task , action - object detection also requires producing a per - pixel probability map .",
    "thus , inspired by the success of a deeplab system on semantic segmentation task , we adopt a pretrained deeplab s network architecture as our rgb pathway .",
    "an action - object also possesses the following 3d spatial properties .",
    "it exhibits characteristic distance to the person due to anthropometric constraints , e.g. , arm length .",
    "for example , when a man picks up a tuna can , his distance from the can is approximately 0.5 m .",
    "the action - object also has a specific orientation relative to a person because of its design .",
    "for instance , when the person carries a cup , he holds it via the handle , which determines the pose of the cup with respect to that person .",
    "these 3d spatial properties are essential for predicting the action - objects .",
    "considering this intuition , we use our collected first - person stereo data to encode spatial 3d cues in a dhg ( depth , height , and grayscale ) image input .",
    "we use depth and height  @xcite to represent the 3d environment around the person , and to handle the pitch movements of the head , i.e. , the height information tells us about the orientation of the person s head with respect to 3d environment .",
    "in addition , the gray scale image is used to capture basic visual appearance cues .",
    "note that we do not use full rgb channels so that the dhg pathway would focus more on depth and height cues than the visual appearance cues .",
    "this diversifies the information learned by the two pathways , which allows egonet to learn complementary action - object cues . in figure  [ filters ]",
    ", we visualize the activation values from the _ fc7 _ layer of rgb and dhg pathway averaged across all channels .",
    "note that the two pathways learn to detect complementary action - object cues . whereas rgb pathway detects objects that are visually prominent , dhg pathway has high activation values around the objects with a certain distance and orientation relative to the person .",
    "c | c c | c c | c c | c c | c c | c c | c c x c c | & & & & & & & & + & & & & & & & & & & & & & & & & + & 0.091 & 0.033 & 0.181 & 0.084 & 0.052 & 0.020 & 0.158 & 0.054 & 0.225 & 0.105 & 0.110 & 0.044 & 0.077 & 0.021 & 0.128 & 0.051 + & 0.188 & 0.091 & 0.160 & 0.092 & 0.048 & 0.021 & 0.286 & 0.189 & 0.523 & 0.428 & 0.102 & 0.051 & 0.063 & 0.030 & 0.182 & 0.107 + & 0.213 & 0.113 & 0.188 & 0.097 & 0.043 & 0.014 & 0.216 & 0.128 & 0.499 & 0.487 & 0.098 & 0.047 & 0.124 & 0.063 & 0.197 & 0.136 + & 0.243 & 0.119 & 0.436 & 0.357 & 0.115 & 0.034 & 0.122 & 0.018 & 0.197 & 0.062 & 0.269 & 0.175 & 0.140 & 0.069 & 0.217 & 0.119 + & 0.342 & 0.220 & 0.220 & 0.143 & 0.134 & 0.063 & 0.146 & 0.065 & 0.292 & 0.213 & 0.158 & 0.073 & 0.262 & 0.128 & 0.222 & 0.129 + & 0.366 & 0.264 & 0.195 & 0.084 & 0.180 & 0.086 & * 0.394 & 0.222 & 0.421 & 0.327 & 0.137 & 0.074 & 0.267 & 0.178 & 0.280 & 0.176 + & 0.224 & 0.113 & 0.400 & 0.283 & * 0.243 & * 0.126 & 0.274 & 0.136 & 0.597 & 0.389 & 0.200 & 0.093 & 0.281 & 0.170 & 0.317 & 0.187 + & 0.330 & 0.230 & 0.525 & 0.246 & 0.208 & 0.117 & 0.267 & 0.159 & 0.340 & 0.264 & 0.301 & 0.102 & 0.290 & 0.154 & 0.323 & 0.181 + & 0.306 & 0.182 & * 0.551 & 0.451 & 0.188 & 0.105 & 0.361 & * 0.238 & 0.501 & 0.378 & * 0.404 & * 0.284 & 0.257 & 0.184 & 0.367 & 0.260 + & * 0.482 & * 0.415 & 0.509 & * 0.473 & 0.193 & 0.121 & 0.298 & 0.183 & * 0.643 & * 0.597 & 0.242 & 0.134 & * 0.406 & * 0.272 & * 0.396 & * 0.313 + & 0.419 & - & 0.576 & - & 0.408 & - & 0.477 & - & 0.556 & - & 0.357 & - & 0.415 & - & 0.458 & - + & 0.453 & - & 0.551 & - & 0.327 & - & 0.444 & - & 0.516 & - & 0.518 & - & 0.436 & - & 0.464 & - + & 0.453 & - & 0.604 & - & 0.197 & - & 0.453 & - & 0.581 & - & 0.340 & - & 0.439 & - & 0.438 & - + & 0.506 & - & 0.631 & - & 0.358 & - & 0.489 & - & 0.579 & - & 0.371 & - & 0.407 & - & 0.477 & - + & 0.488 & - & 0.660 & - & 0.227 & - & 0.444 & - & 0.598 & - & 0.373 & - & 0.435 & - & 0.461 & - + * * * * * * * * * * * * * * * *      to combine the information from rgb and dhg pathways for action - object prediction we introduce a joint pathway .",
    "the joint pathway first concatenates @xmath3 dimensional _ fc7 _ features from both rgb and dhg pathways . additionally , we also concatenate the resized two - channel grid of x and y coordinates corresponding to each pixel in the resized first - person image . afterwards",
    ", we perform a first - person coordinate embedding , which consists of ( 1 ) using the first - person spatial coordinates in a standard convolution operation , and then ( 2 ) attaching another convolutional layer to blend the visual and spatial information in the new layer .",
    "the first - person coordinate embedding allows egonet to use visual and spatial features in conjunction , which we show is beneficial for an accurate action - object detection in first - person images .",
    "intuitively the importance of first - person coordinate embedding can be explained as follows .",
    "the way a person positions himself during an interaction relative to an action - object , affects a location where such an action - object will be mapped in a first - person image .",
    "for instance , a laptop keyboard is often seen at the bottom of a first person image because we often look down at it while typing with our hands .",
    "our proposed first - person coordinate embedding allows egonet to learn such an action - object spatial distribution , which is different than most prior work that assumes a universal object spatial prior ( e.g. a center prior )  @xcite .",
    "one may think that our proposed first - person coordinate embedding should have a minimal effect to the network s performance because traditional fcns also incorporate certain amount of spatial information in its prediction mechanism .",
    "however , unlike traditional fcns , egonet uses first - person coordinates as features directly in the 2d convolution operation , which forces egonet to produce a different convolutional output than traditional fcns would .",
    "despite the simplicity of our first - person coordinate embedding scheme , in our experiments , we show that it significantly boosts the action - object detection accuracy .      in this section ,",
    "we quantitatively characterize the design factors of our egonet architecture . due to a relatively small dataset size",
    ", we use our first - person action - object rgbd dataset for all of the experiments , which allows more efficient training and testing .",
    "we evaluate the action - object detection accuracy using maximum f - score ( mf ) , and average precision ( ap ) evaluation metrics , which are obtained by thresholding the action - object probability maps at small intervals and computing a precision and recall curve .",
    "* are separate rgb and dhg pathways necessary ? * an intuitive alternative to our egonet architecture is a single - stream network that concatenates rgb , dhg , and first - person coordinate inputs and feeds them through the network .",
    "we compare such a baseline to our egonet model and report that our approach yields @xmath4 and @xmath5 higher mf and ap scores than such a baseline .",
    "* what is the contribution of each pathway ? *",
    "our egonet model predicts action - objects based on the visual appearance and 3d spatial cues , which are learned in the separate rgb and dhg pathways . to examine how important each pathway is ,",
    "we train two independent rgb and dhg single - stream networks ( both with the first - person coordinate embedding ) .",
    "whereas the rgb network obtains @xmath6 and @xmath7 mf and ap score , the dhg network achieves @xmath8 and @xmath9 mf and ap results .",
    "thus , based on the results we observe that both independent rgb and dhg networks produce similar performance according to mf score , which suggests that both visual appearance and 3d spatial cues are almost equally important .",
    "additionally , we note that our proposed joint two - stream egonet model achieves @xmath10 and @xmath11 according to mf and ap metrics , which is at least @xmath12 and @xmath13 better than either of the independent rgb and dhg networks . this suggests , that rgb and dhg pathways learn complementary action - object information , and combining these two sources of information in the joint pathway further improves action - object detection accuracy .    *",
    "do first - person coordinates help ?",
    "* earlier we claimed that using first - person coordinates in the joint pathway is essential for a good action - object detection performance .",
    "to test this claim , we train a network with an identical architecture as egonet except that it * does not * use first - person coordinates .",
    "such a network yields @xmath14 and @xmath15 mf and ap scores , which is considerably lower than @xmath10 and @xmath11 mf and ap results produced by our proposed egonet model , that uses first - person coordinates .",
    "such a big accuracy difference between the two models suggests that first - person coordinates play a crucial role in an action - object detection task .",
    "* is the coordinate embedding useful ? * in the previous section , we also claimed that the first - person coordinate embedding ( see fig .",
    "[ arch ] ) is crucial for a good action - object detection accuracy . to test this claim",
    "we train a network with an identical architecture as egonet except that we remove the last layer before softmax loss ( i.e. where the coordinate embedding is performed ) .",
    "we observe that the network that * does not * use the coordinate embedding produces a @xmath11 and @xmath16 mf and ap scores , which is @xmath17 and @xmath18 lower than our full egonet model .",
    "we acknowledge that a larger number of parameters in one of these baselines may contribute to better results .",
    "however , our prior experiments indicate that simply adding an additional layer to the network typically yields significantly smaller improvement ( @xmath19 ) or no improvement at all .",
    "thus , we believe that the first - person coordinate embedding is the primary reason for an improved action - object detection accuracy .",
    "we implement egonet using caffe  @xcite .",
    "the rgb and dhg pathways are built upon deeplab architecture  @xcite the entire egonet network is trained jointly for @xmath20 iterations , at the learning rate of @xmath21 , momentum of @xmath22 , weight decay of @xmath23 , batch size of @xmath24 , and the dropout rate of @xmath25 . to optimize the network",
    ", we used a per - pixel softmax loss with respect to the action - object ground truth .",
    "in this section , we present quantitative and qualitative results of our egonet method on ( 1 ) our collected first person action - object rgbd , ( 2 ) gtea gaze+  @xcite , and ( 3 ) social children interaction  @xcite datasets . additionally , to gain a deeper insight into an action - object detection problem we also include an action - object human study where @xmath26 human subjects perform this task on our dataset .    to evaluate an action - object detection accuracy we use maximum f - score ( mf ) , and average precision ( ap ) evaluation metrics , which are obtained by thresholding probabilistic action - object maps at small intervals and computing a precision and recall curve .",
    "our evaluations provide evidence for three main conclusions :    * in subsection  [ human_study_sec ] , we show that humans achieve better action - object detection accuracy than the machines . * in subsection  [ rgbd_ao_exp ]",
    ", we show that our egonet model outperforms all other approaches by a considerable margin on our first - person action - object rgbd dataset . * in subsections  [ gtea_exp ] ,  [ child_exp ] we demonstrate strong egonet s generalization power by applying it on the other first - person datasets that contain novel scenes , activities and objects .     human subjects . in many cases ,",
    "third - person human subjects detect action - objects correctly and consistently . however , some activities such as shopping makes this task difficult even for a human observer since he does not know what the camera wearer was thinking . ]     human subjects .",
    "in many cases , third - person human subjects detect action - objects correctly and consistently",
    ". however , some activities such as shopping makes this task difficult even for a human observer since he does not know what the camera wearer was thinking . ]     human subjects . in many cases ,",
    "third - person human subjects detect action - objects correctly and consistently . however , some activities such as shopping makes this task difficult even for a human observer since he does not know what the camera wearer was thinking . ]     human subjects . in many cases ,",
    "third - person human subjects detect action - objects correctly and consistently . however , some activities such as shopping makes this task difficult even for a human observer since he does not know what the camera wearer was thinking . ]     human subjects . in many cases ,",
    "third - person human subjects detect action - objects correctly and consistently . however , some activities such as shopping makes this task difficult even for a human observer since he does not know what the camera wearer was thinking . ]     human subjects . in many cases ,",
    "third - person human subjects detect action - objects correctly and consistently . however , some activities such as shopping makes this task difficult even for a human observer since he does not know what the camera wearer was thinking . ]     human subjects . in many cases , third - person human subjects detect action - objects correctly and consistently . however , some activities such as shopping makes this task difficult even for a human observer since he does not know what the camera wearer was thinking . ]     human subjects .",
    "in many cases , third - person human subjects detect action - objects correctly and consistently .",
    "however , some activities such as shopping makes this task difficult even for a human observer since he does not know what the camera wearer was thinking . ]      to gain a deeper insight into an action - object detection task , we conduct a human study experiment to see how well humans can detect action - objects from first - person images .",
    "we randomly select @xmath27 different first - person images from each of @xmath28 activities from our first - person action - object rgbd dataset , and ask human subjects to identify a location of each action - object in such a first - person image .",
    "we use @xmath27 different images from each activity to keep the experiment s duration under an hour .",
    "also , instead of collecting per - pixel or bounding box labels , we ask the subjects to identify action - objects by clicking at the center of an action - object , which is very efficient .",
    "we collect experimental action - object detection data from @xmath26 subjects .    to obtain full action - object segmentations from the points selected by the subjects ,",
    "we place a gaussian with a width of @xmath29 around the location of human selected point and project it on mcg  @xcite regions as is done in  @xcite .",
    "we acknowledge that due to the use of gaussian and the errors in the mcg algorithm , our scheme of obtaining per - pixel segmentations out of a single point may slightly degrade human subject results .",
    "however , even under current conditions a single experiment took about an hour or even more to complete .",
    "thus , we believe that our chosen experiment conditions provided the best trade - off between the experiment duration and the quality of the action - object detection results provided by the subjects .    in the bottom of table  [ egod_table ] , we provide human subject results according to the mf evaluation metric .",
    "unlike in our other experiments , the action - object masks obtained by the human subjects were skewed towards the extreme probability values of @xmath30 and @xmath31 , which made the ap metric less informative .",
    "thus , we only used mf score in this case .",
    "based on these results , we observe that in most cases , each of the @xmath26 subjects perform better than the machines .",
    "we also observe that the action - object detection results achieved by the human subjects are quite consistent across most of different activities from our dataset .",
    "this indicates that humans can perform action - object detection from first - person images pretty effectively , despite not knowing exactly what the camera wearer was thinking ( but possibly predicting it ) .    in figure  [ human_study_fig ]",
    ", we also present some qualitative human study results , where we average the predictions across all @xmath26 human subjects .",
    "these results indicate that in some instances ( second row ) , detecting action - objects is pretty difficult even for the human subjects since they do not know what the camera wearer was thinking .",
    "we include more qualitative results in the supplementary material .       c | c | c | c | c | c | c | c | c | c | c | c | c | c | c x c | c | & & & & & & & & + & & & & & & & & & & & & & & & & + & 0.192 & 0.113 & 0.233 & 0.147 & 0.303 & 0.225 & 0.191 & 0.112 & 0.202 & 0.117 & 0.346 & 0.248 & 0.211 & 0.108 & 0.240 & 0.153 + & 0.358 & 0.291 & 0.342 & 0.261 & 0.507 & 0.504 & 0.344 & 0.274 & 0.339 & 0.291 & 0.475 & 0.435 & 0.348 & 0.294 & 0.388 & 0.336 + & 0.310 & 0.223 & 0.318 & 0.214 & 0.522 & 0.482 & 0.404 & 0.329 & 0.393 & 0.315 & 0.452 & 0.374 & 0.403 & 0.304 & 0.400 & 0.320 + & 0.364 & 0.281 & 0.456 & 0.380 & 0.486 & 0.399 & 0.401 & 0.310 & 0.411 & 0.346 & 0.511 & 0.454 & 0.321 & 0.207 & 0.422 & 0.340 + & 0.403 & 0.307 & * 0.499 & * 0.443 & 0.488 & 0.380 & 0.337 & 0.211 & 0.408 & 0.273 & 0.472 & 0.375 & 0.369 & 0.268 & 0.425 & 0.322 + & 0.400 & 0.344 & 0.440 & 0.429 & 0.577 & 0.564 & 0.429 & 0.374 & 0.467 & 0.429 & 0.456 & 0.419 & 0.368 & 0.304 & 0.448 & 0.409 + & * 0.433 & * 0.357 & 0.475 & 0.383 & * 0.607 & * 0.568 & * 0.512 & * 0.450 & * 0.532 &",
    "* 0.505 & * 0.576 & * 0.486 & * 0.454 & * 0.358 & * 0.513 & * 0.443 + * * * * * * * * * * * * * * * *    in table  [ egod_table ] we present the results on our first - person action - object dataset , which contains @xmath32 annotated images .",
    "all the results are evaluated according to the mf and ap metrics .",
    "we include the following baseline methods in our comparisons : ( 1 - 2 ) gbvs  @xcite and judd  @xcite : two bottom - up visual saliency methods ; ( 3 ) fp - mcg  @xcite : a multiscale object segmentation proposal method that was trained on our first - person dataset ; ( 4 ) handled+viewed object : our trained method that detects objects around hands , if no hands are detected it predicts objects near the center of an image , which is where a person may typically look at ; ( 5 ) action - object prior ( aop ) : the average action - object location mask obtained from our dataset ; ( 6 ) deeplab - obj  @xcite : a deeplab network trained for traditional object - segmentation on our dataset with 41 object classes ; ( 7 - 8 ) deeplab - rgb  @xcite and deeplab - dhg  @xcite : a deeplab network trained for action - object detection using rgb and dhg images as its inputs respectively ; ( 9 ) salobj+depth  @xcite : a salient object detection system , which we adapt to also handle a depth input .",
    "note that all the methods except for gbvs are trained on our dataset .",
    "the training is conducted using the leave - one - out cross validation scheme as is standard for first - person videos .",
    "we acknowledge that due to a small number of subjects in our dataset , the data from the same subject may appear in both training and testing splits .",
    "however , since the training and testing splits contain activities that are quite different and occur at different environments , such an overlap does not cause any overfitting to the data by our model . in subsections  [ gtea_exp ] ,  [ child_exp ]",
    ", we verify that our model does not overfit by showing that egonet performs well on the other first - person datasets without being trained on them .",
    "based on the results in table  [ egod_table ] , we observe that egonet outperforms all baseline methods by at least @xmath33 ( mf ) and @xmath34 ( ap ) . in figure",
    "[ egod_preds ] , we also show our qualitative action - object results . unlike other methods , egonet correctly detects and localizes action - objects in all cases .",
    "we include more qualitative results in the supplementary material .      to show strong egonet s generalization ability , in table  [ gt_table ] , we present our action - object detection results on the gtea gaze+ dataset  @xcite , which consists of first - person videos that capture people cooking @xmath28 different meals .",
    "the dataset provides the annotations of objects that people are interacting with during a cooking activity . in comparison",
    "to our first - person action - object rgbd dataset , gtea gaze+ contains many novel scenes , and many new object classes , which makes it a good dataset for testing egonet s generalization ability .",
    "additionally , we note that , gtea gaze+ does not have depth information in the scene .",
    "thus , we augment the dataset with depth predictions using  @xcite . note that to test each model s generalization ability ,",
    "all the methods are trained * only * on our first - person action - object rgbd dataset .",
    "based on the results in table  [ gt_table ] , we can conclude that egonet shows the strongest generalization power since it outperforms other baselines by at least @xmath35 and @xmath36 according to mf and ap evaluation metrics .",
    "these results indicate that egonet is able to exploit common person - object spatial configurations for an effective action - object detection ( 1 ) using a small number of training examples , and ( 2 ) also without the need to be adapted to specific tasks as some prior methods  @xcite .",
    "we include the qualitative detection results from this dataset in the supplementary material .      finally , we present our results on social children interaction dataset  @xcite , which includes @xmath37 first person videos of three children playing a card game , building block towers , and playing hide - and - seek .",
    "the dataset consists of @xmath38 frames that are annotated with the location of children s attention  @xcite .",
    "similar to gtea gaze+ dataset , this dataset only contains rgb images so we complement it with the depth predictions using the method in  @xcite .    to test the generalization power , we train each method * only * on our first - person action - object rgbd dataset , and then test it on social children interaction dataset .",
    "we report that egonet achieves @xmath39 and @xmath40 mf and ap scores , which is at least @xmath41 and @xmath42 better than the other baselines .",
    "additionally , we note that 3d spatial cues are beneficial as the joint egonet model achieves an improved accuracy of @xmath43 mf and @xmath44 ap compared to a single - stream rgb model .",
    "we also illustrate some qualitative predictions in figure  [ children_preds ] .",
    "please see the supplementary material for more quantitative and qualitative results .",
    "in this work , we unify the ideas from prior work on first - person object detection into a concept of an action - object , that allows us to study a person s visual attention and his motor actions from a first - person visual signal . to do this , we introduce egonet , a two - stream network that holistically integrates visual appearance , head direction , 3d spatial cues , and that also employs first - person coordinate embedding for an accurate action - object detection from the first - person rgbd data . due to an inherent task - intention information encoded in a first person image ,",
    "our egonet model is able to exploit common person - object spatial configurations and predict action - objects ( 1 ) from a limited number of training examples and ( 2 ) without an explicit adaptation to a specific task as is done in prior work  @xcite .",
    "we believe that egonet s predictive power and its strong generalization ability makes it readily applicable to many applications such as personalized video dialog creation , video summarization , and empirical understanding of visual sensorimotor systems of humans ."
  ],
  "abstract_text": [
    "<S> a first - person camera , placed at the person s head , captures our visual sensorimotor object interactions . </S>",
    "<S> can a single first - person image tell us about our momentary visual attention and motor action with objects , without a gaze tracking device or tactile sensors ? </S>",
    "<S> to study the holistic correlation of visual attention with motor action , we use the concept of action - objects  objects that capture person s conscious visual ( watching a tv ) or tactile ( taking a cup ) interactions . </S>",
    "<S> action - objects may be task - dependent but since many tasks share common person - object spatial configurations , action - objects exhibit a characteristic 3d spatial distance and orientation with respect to the person .    </S>",
    "<S> inspired by these observations , we propose to detect action - objects with egonet , a joint two - stream rgb and dhg network that holistically integrates visual appearance , head direction , 3d spatial cues , and that uses a first - person coordinate embedding , which is designed to learn spatial distribution of action - objects in the first - person data . in our experiments , we show that egonet consistently outperforms other approaches and that it also generalizes well to the previously unseen first - person datasets . </S>"
  ]
}