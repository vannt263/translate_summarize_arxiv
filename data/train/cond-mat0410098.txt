{
  "article_text": [
    "one of the greatest advantages of quantum monte carlo ( qmc ) simulations is the possibility to deal with complex and large size systems .",
    "the tremendous increase in computing capabilities and the development of new qmc based algorithms in recent years gives rise to new opportunities for qmc simulations .",
    "however , the possibility of producing new data implies a series of new problems for processing and analyzing the data .    despite the qmc successes , these simulations have some general limitations .",
    "one such difficulty is the sign problem , which affects a large class of quantum models and appears when the sampling weight of some configurations is not positive definite .",
    "another drawback of qmc simulations is that , while static measurements are easily obtained , calculating dynamical quantities is extremely difficult . when the sign problem is present , this difficulty is much more serious . in the past",
    ", the limited computing capabilities available did nt allow for simulations with a small average sign . with the advent of new parallel vector machines such as the cray x1 at ornl however ,",
    "the speed of these calculations is significantly improved , making simulations with a small average sign feasible .",
    "this necessitates major improvements in the methods used to analyze the new data .    the standard technique of extracting dynamical spectra from qmc simulations based on the matsubara - time path integral formalism is the maximum entropy method ( mem )  @xcite .",
    "the dynamical properties contain important information about excited states and describe the system s response to different external perturbations , making the direct connection between model and experiment .",
    "therefore an algorithm able to produce dynamical quantities is of crucial importance .",
    "the goal of this paper is to describe an improved mem technique of calculating dynamical properties of a system from qmc data with a sign problem .",
    "mem recasts the problem of spectra calculation from a deterministic problem to one of probability optimization . in principle , by knowing the imaginary time response functions , the dynamical spectra can be obtained by solving an integral equation .",
    "however , in practice , the calculation of spectra is an ill - defined problem . due to the fact that qmc provides information on a finite number of time points with a certain error bar , an infinite number of solutions consistent with the data exists .",
    "mem is an algorithm which , based on bayesian inference  @xcite , provides the most probable spectrum compatible with the available data  @xcite .",
    "the spectrum probability is calculated assuming _ gaussianly distributed _ and _ uncorrelated _ data .",
    "as the central limit theorem requires , the statistics of any average is gaussian as long as the average is taken over a large number of uncorrelated points .",
    "methods have been developed to reduce the correlations in the data , both those between adjacent measurements and those at different matsubara time points in the same measurement  @xcite .",
    "however , when the sign problem is present , the qmc data becomes very poorly conditioned , which greatly complicates the mem problem .",
    "non - gaussian distributions and strong correlations of the data turn out to be very severe problems .",
    "they can not be removed by the standard techniques , mainly due to the strong correlation between the data and the averaged sign of the configurations which produce these data .",
    "this makes it essentially impossible to calculate spectra long before the minus sign problem makes the calculation of static properties impractical . in this paper",
    "we address this problem and describe a solution which greatly increases the resolution of mem when calculating spectra from such poorly conditioned data .",
    "this paper is organized as follows . in sec .",
    "[ sec : mem ] we introduce the general mem formalism . in sec .",
    "[ sec : datas ] we discuss and exemplify the problems which appear when the qmc simulations suffer by the sign problem .",
    "a solution to the problem is given in sec .",
    "[ sec : newmem ] . a comparison of spectra obtained with the standard and",
    "the improved method is presented in sec .",
    "[ sec : newdata ] .",
    "the conclusions are given in sec .",
    "[ sec : conc ] .",
    "we start with a brief introduction of mem@xcite .",
    "mem is an algorithm which aims to determine the spectral decomposition of one- or two - particle green s functions .",
    "most qmc methods only produce estimates of the imaginary time green s functions .",
    "the relation between the spectral density , @xmath0 , and the imaginary time green s function , @xmath1 , is given by an integral equation  @xcite @xmath2 where the kernel , @xmath3 , is given by @xmath4 for the one - particle green s function , and respectively @xmath5 for the two - particle susceptibility   ( @xmath6 ) .",
    "in this paper we use @xmath1 ( @xmath0 ) . ] .",
    "the determination of the spectrum is an ill - posed problem , since an infinite number of solutions exists which are consistent with the qmc data and associated error bars .",
    "mem selects from these solutions the most probable one . according to bayesian logic  @xcite ,",
    "given the data @xmath7 , the conditional probability of the spectrum @xmath8 , @xmath9 $ ] , is given by @xmath10=p[g|a]~p[a]/p[g]\\,.\\ ] ] here @xmath11 $ ] is the _ likelihood function _ which represents the conditional probability of the data @xmath7 given @xmath8 , @xmath12 $ ] is the _ prior probability _ which contains prior information about @xmath8 and @xmath13 $ ] is called the _ evidence _ and can be considered a normalization constant .    the _ prior probability _ is given by @xmath14=e^{\\alpha s},\\ ] ] with a real positive constant @xmath15 and the entropy function @xmath16 defined by @xmath17).\\ ] ] @xmath18 is a function called `` default model '' .",
    "the specific form of the entropy function is a result of some general and reasonable assumption imposed on the spectrum , like subset independence , coordinate invariance , system independence and scaling . by defining the entropy relative to a default model ,",
    "the prior probability is also used to incorporate prior knowledge about the spectrum , such as the high - frequency behavior and certain sum - rules . in the absence of data the resultant spectrum will be identical to the model .",
    "the entropic probability and its consequences are discussed at large in a series of papers  @xcite , and does not constitute the subject of this study .    the main focus of this investigation is the calculation of the _ likelihood function _ , @xmath11 $ ] .",
    "the central limit theorem shows that the distribution of the data obtained in a qmc process is always gaussian if every data point is taken as an average of a large enough number of measurements so that different data are independent .",
    "this implies @xmath19=e^{-\\chi^2/2}\\ ] ] where @xmath20_{ij}(\\bar{g}_j - g_j(a)),\\ ] ]    @xmath21    with the covariance    @xmath22    here we considered that qmc provides @xmath1 on @xmath23 time points @xmath24 , and denote @xmath25 . for every time point , @xmath24 , we have @xmath26 measured @xmath27 , centered at @xmath28 ( eq .  [ eq : gave ] ) .",
    "@xmath29 in eq .",
    "( [ eq : cov ] ) is the covariance matrix which characterizes the second moment of the data .",
    "@xmath30 in eq .",
    "( [ eq : chisq ] ) is the value of @xmath31 which corresponds to the spectrum @xmath0 according to eq .",
    "[ eq : inteq ] or its discretized form .",
    "mem requires gaussianly distributed data . otherwise , the likelihood probability defined in eq .",
    "( [ eq : likely ] ) has no meaning . in theory , the requirement for a gaussian distribution is achieved by averaging many measurements to obtain one data point .",
    "however , in practice when computational resources are limited , this condition is often difficult to satisfy . in mem literature",
    "the data points obtained by averaging many measurements are called _",
    "bins_. the usual way to remove the correlation between bins is to re - average ( coarse - grain ) more successive bins which results in increasing the number of measurements per bin .",
    "however , for a fixed amount of data , this process of increasing the bin size will reduce the number of data points ( bins ) .",
    "if the number of bins is too small , the data can not properly describe a statistic process , and the covariance matrix becomes pathological .",
    "therefore a successful mem for correlated data requires a large number of measurements , implying both large bins and a large number of bins .",
    "the correlation of data between different time points , is the other relevant problem which causes mem to fail .",
    "these correlations can be removed by a rotation @xmath32 which diagonalizes the covariance matrix .",
    "@xmath33    the data and the kernel should also be rotated accordingly @xmath34 the rotated @xmath35 are the statistically independent directions , and in this basis , @xmath36 reduces to    @xmath37    in eq .",
    "[ eq : chisqr ] the discretized form of eq .",
    "[ eq : inteq ] was used , the summation over @xmath38 meaning summation over frequency .",
    ", b ) sign , @xmath39 and c ) @xmath40 when the bin size is increased five times which corresponds to @xmath41 measurements per bin .",
    "the dashed lines represent the best gaussian fit to the data.,width=307 ]    when the sign problem is present in qmc simulations , the condition of gaussianly distributed @xmath42 becomes more difficult to satisfy .",
    "very often , a huge number of measurements , beyond the available computing possibilities , would be required to accomplish this task .",
    "the difficulty in obtaining good data points for @xmath42 can be easily understood from the measurement process . in a qmc process where the sign of the sampling weight is negative",
    ", it can no longer define a probability .",
    "therefore , the sign of the sampling weight must be associated with the measurement . for the green s function ,",
    "we can no longer measure @xmath42 but rather the product of it and the sign @xmath39 of the configuration , @xmath43 , and the sign @xmath39 . at the end of the simulation ,",
    "i.e.  after a large number of measurements , we then obtain @xmath44 , where the overbar denotes averaging over the number of measurements .",
    "two problems related with the sign affect the quality of the @xmath42 data .",
    "first , in order to obtain good data points @xmath45 , for every data point we need to average a very large number of @xmath46 and @xmath39 and afterwards calculate @xmath47 . here",
    "@xmath48 and @xmath49 both denote averages over the measurements that form the bin @xmath50 .",
    "smaller average signs @xmath51 worsen the problem , since any small variation of @xmath39 has a large effect on @xmath42 ( @xmath52 ) .",
    "second , as within the same bin @xmath50 there is a strong correlation between different data points @xmath48 , there is also a strong correlation between data points @xmath48 and @xmath49 .",
    "the points @xmath47 are obtained by a nonlinear operation of these correlated quantities , and there is no reason to expect them to be normal distributed .    in order to exemplify the problems discussed above , we employed a qmc based algorithm  @xcite to produce a very large amount of data for the single - particle green s function and the two - particle spin susceptibility in the two - dimensional hubbard model on a square lattice .",
    "the hubbard model is characterized by the single - particle hopping @xmath53 between nearest neighbors and the on - site coulomb repulsion @xmath32 .",
    "we choose @xmath54 so that the bandwidth @xmath55 and set @xmath56 . to make the sign - problem worse , we add a next - nearest neighbor hopping @xmath57 to frustrate the lattice .",
    "we perform calculations on a 16-site @xmath58 cluster at @xmath59 doping , down to temperatures @xmath60 where we experience a severe sign - problem , @xmath61 .",
    "we simulate the model using the dynamical cluster approximation ( dca ) with the hirsch - fye algorithm as a cluster solver.@xcite the dca is a coarse - graining approximation , in which the one particle green s function is coarse - grained in the first brillouin zone of the reciprocal space of the lattice .",
    "it is defined over cluster points @xmath62 and imaginary time @xmath63 , and accurately describes short - ranged correlations .",
    "we performed the simulations on the cray @xmath64 supercomputer at oak ridge national laboratory to cope with the large amount of data needed in simulations with small average signs .",
    "we calculated @xmath65 data points ( bins ) @xmath66 , and for every data point we averaged @xmath67 qmc measurements .    in fig .",
    "[ fig : hist ] we show histograms of the data distribution when the bin size is increased five times , which corresponds to an average of @xmath41 measurements per bin .",
    "both @xmath68 ( fig .",
    "[ fig : hist ] ( a ) ) and @xmath39 ( fig .  [ fig : hist ] ( b ) ) are normally distributed to a good approximation , unlike @xmath40 data points ( fig .",
    "[ fig : hist ] ( c ) ) which are strongly peaked , being characterized by a large positive kurtosis  @xcite .",
    "similar distributions of data are observed ( not shown ) for the other values of the imaginary time . in order to become",
    "gaussianly distributed the @xmath7 data require averaging over a much larger number of measurements than @xmath69 and @xmath39 data . in our case",
    "this number is about five times larger but this value is dependent on the specificity of the problem considered , being determined by both the magnitude of the correlations and the value and the distribution of the sign   we mean rebinning @xmath69 and @xmath39 and afterwards obtaining @xmath7 as the ratio of these two quantities .",
    "much worse results are obtained if successive @xmath7 data points are rebinned . ] .",
    "the way to achieve good data for mem is _ i ) _ rebinning @xmath70 until they become normal distributed , and _",
    "ii ) _ remove the correlations between data points @xmath46 and @xmath39 by a rotations in the space @xmath70 .",
    "however , the problem that arises is the calculation of @xmath36 ( eq .",
    "[ eq : chisq ] ,  [ eq : chisqr ] ) in this basis which now includes the extra sign dimension .",
    "this issue will be discussed in the next section ( sec .",
    "[ sec : newmem ] ) .",
    "denoting @xmath71 , the likelihood function is defined as @xmath72 $ ] , since the measured quantities in the qmc process are the @xmath73 points ( and not @xmath7 ) . as we showed in the previous section , for acceptable values of the bin size ,",
    "the data @xmath73 are to a good approximation gaussianly distributed .",
    "therefore , the likelihood function will have the same form as eq .",
    "[ eq : likely ] , with @xmath36 @xmath74_{ij}(\\bar{h}_j - h_j(a))\\,.\\ ] ]    the covariance matrix has now the dimension @xmath75 , @xmath76    the only problem which remains to be solved is finding an equation for @xmath77 , since eq .  [ eq : inteq ] only provides a relation for @xmath78 . in order to achieve this",
    "we do the following : first we absorb the sign into the spectrum , i.e. we define @xmath79 as @xmath80 instead of searching for a spectrum @xmath8 which satisfies eq .",
    "[ eq : inteq ] we search for @xmath79 which satisfies @xmath81 second , we consider the spectrum normalization sum - rule @xmath82 which implies @xmath83 here @xmath84 is a constant , equal to one for the the one - particle spectra and equal to @xmath85 for the two - particle case  , with @xmath86 defined in eq .",
    "[ eq : chisq ] . ] .",
    "because the sign @xmath39 was absorbed into the definition of @xmath79 we relate the sign fluctuations to the norm of the new spectrum . both eq .",
    "[ eq : ks ] and eq .",
    "[ eq : sumrules ] can be written as @xmath87 this is the basic equation which relates @xmath73 to @xmath79 and determines the likelihood function @xmath72 \\equiv p[h|{\\cal{a}}]$ ] .",
    "mem will produce the most probable spectrum @xmath79 normalized to @xmath51 which minimizes the @xmath36 function in eq .",
    "( [ eq : chisqn ] ) subject to the entropy constraint .",
    "we want to point that for the one - particle case , where @xmath88 , eq .  ( [ eq : sumrules ] ) is equivalent to @xmath89 by using eq .",
    "( [ eq : sumrules ] ) in the calculation of the likelihood function we impose @xmath90 at every measurement . since eq .",
    "[ eq : g0 gb ] results solely from the anticommutation relation of the one - particle operators it should be satisfied in every possible configuration and implicitly in every measurement . therefore , this way of implementing the normalization sum - rule is more natural than the usual way based on lagrange multipliers where the constraint is globally imposed , i.e. not at every measurement but only for the final green s function obtained at the end of the qmc process .",
    "for the two - particle case , where @xmath91 , the sum - rule eq .",
    "( [ eq : sumruleg ] ) is not an independent equation as in the one - particle case , but merely an integration over @xmath63 of eq .",
    "[ eq : inteq ] .",
    "therefore it is essential to treat @xmath84 as a constant ( equal to the final , averaged over all qmc configurations , @xmath92 ) and to disregard measurement dependent fluctuations in @xmath85 .",
    "this way we relate the norm of @xmath79 only to the fluctuation of the sign @xmath39 .",
    "calculated with different amounts of data using a ) the new method and b ) the old method.,width=316 ]    in this section we present a comparison between the spectra obtained with the old approach which does not consider the sign covariance , and the new one described in sec .",
    "[ sec : newmem ] . for calculating the one - particle spectrum at the highest temperatures , the model @xmath18 used in the entropy functional eq .",
    "( [ eq : entropy ] ) , is chosen to be a gaussian function . the model for lower temperatures",
    "is taken to be the spectrum obtained at a slightly higher temperature , a procedure called annealing .    in fig .",
    "[ fig : spectno ] ( a ) and ( b ) we show the one - particle spectra of the hubbard model at @xmath93 calculated for different amounts of data with the new and respectively with the old method . in both cases , when a large amount of data is used ( @xmath65 data points ) the spectrum ( thick continuous line ) is converged .",
    "moreover the two methods produce the same spectrum .",
    "however , it can be noticed that with the new method a reasonably good spectrum , i.e. a spectrum close to the converged one , can be obtained with an amount of data as small as @xmath94 data points ( see the double - dotted dashed line in fig .",
    "[ fig : spectno ] ( a ) ) . on the other hand ,",
    "the old method requires at least @xmath67 data points for a spectrum of comparable quality ( see the dotted line in fig .",
    "[ fig : spectno ] ( b ) ) .",
    "thus in our case we find that the new method reduces the computational cost of calculating the one - particle spectra about six times .    in general the calculation of",
    "the two - particle spectra turns out to be more difficult , because the data are more correlated and because a good default model is missing . in our case",
    "the amount of data needed for calculating the two - particle spectra is about one order of magnitude larger . at high temperature we choose a default model of the form @xmath95\\ ] ] where @xmath96 and @xmath97 are lagrange multipliers chosen to satisfy certain moment constraints , as described in ref  @xcite .",
    "again , the annealing technique is used for lower temperature calculations .     at @xmath98 calculated for different amounts of data with a ) the new method and b ) the old method.,width=316 ]     at @xmath98 calculated for different amount of data with a ) the new method and b ) the old method.,width=316 ]    the spin susceptibility spectra at @xmath98 calculated with the two methods for different amounts of data is shown in fig .",
    "[ fig : chiwcomp ] . for the two - particle case the spectra @xmath99 defined in eq .",
    "[ eq : inteq ] is in fact @xmath100 where @xmath101 is the imaginary part of the spin susceptibility . when a large amount of data is used ( @xmath65 data points )",
    "both methods produce the same spectrum ( thick continuous line in both fig .",
    "[ fig : chiwcomp ] ( a ) and ( b ) ) . however , for small amounts of data , the new method produces significantly superior results to the old method . for example the spectrum obtained with the new method for @xmath102 data points ( thin continuous line in fig .",
    "[ fig : chiwcomp ] ( a ) ) is closer to the converged spectrum than the one obtained with the old method for @xmath103 data points ( dashed line in fig .",
    "[ fig : chiwcomp ] ( b ) ) .",
    "the same conclusion can be drawn by comparing the high energy features ( @xmath104 ) visible in the plot of the imaginary part of the spin susceptibility @xmath101 in fig .",
    "[ fig : chicomp ] ( a ) and ( b ) .",
    "we showed that for qmc simulations with a severe sign problem , achieving a normal distribution of @xmath7 is extremely difficult .",
    "the problem results from the nonlinear operation which relates @xmath7 to the measured quantities @xmath69 and @xmath39 , and from the correlation between the @xmath69 points and @xmath39 .    by absorbing the sign into the definition of the spectrum",
    ", the sign fluctuations will determine the norm of the spectrum .",
    "a connection is thus established between the measured quantities @xmath105 and the renormalized spectrum @xmath106 . the likelihood function is calculated with regard to the directly measured @xmath105 data , thus no nonlinear manipulation of the data is being applied . the correlations between @xmath69 and @xmath39 can be removed by a rotation in the space determined by these vectors .",
    "we illustrated the power of this approach by a comparison of the spectra obtained with the old and the new method .",
    "when the sign is small and the correlation between the sign @xmath39 and the measured @xmath69 points is significant , the old method requires a very large amount of measurements per bin in order to produce the normally distributed and uncorrelated data points necessary for obtaining good spectra . in contrast",
    ", the new method provides good spectra for a much smaller amount of data . in our case",
    "the old method needs about six times more data than the new one , but for other problems characterized by stronger correlations this amount can be much larger .",
    "this research was supported by the nsf grants dmr-0312680 and dmr-0113574 .",
    "this research used resources of the center for computational sciences and was sponsored in part by the offices of advanced scientific computing research and basic energy sciences , u.s .",
    "department of energy .",
    "oak ridge national laboratory , where tm is a eugene p. wigner fellow , is managed by ut - battelle , llc under contract no .",
    "de - ac0500or22725 .",
    "r. n. silver , d. s. sivia and j.e .",
    "gubernatis , phys .",
    "b * 41 * , 2380 ( 1990 ) r. n. silver , j. e. gubernatis , d. s. sivia , and m. jarrell , phys . rev .",
    "lett . * 65 * , 496 ( 1990 ) j. e. gubernatis , m. jarrell , r. n. silver and d. s. sivia , phys .",
    "b * 44 * , 6011 ( 1991 ) mark jarrell and j. e. gubernatis , physics reports * 269 * , 133 , ( 1996 ) a. papoulis , _ probability and statistics_(prentice - hall , new york , 1990 ) , p. 422",
    "e. t. jaynes , in _ maximum entropy and bayesian methods _ edited by j. h. justice ( cambridge university , cambridge , 1986 ) ; s.f .",
    "maximum entropy and bayesian methods in science and engineering _ , edited by g. j. erickson and c. r. smith ( kluwer academic , dordrecht , 1988 ) d.s .",
    "sivia , los alamos science * 19 * , 180 , ( 1990 ) s. doniach and e. sondheimer _",
    "green s functions for solid state physicists _",
    "( benjamin , reading , ma , 1974 ) ; r. kubo , m. toda and n. hashitsume , _ statistical physics ii _ ( springer - verlag , new york , 1978 ) ; s. w. lovesoy , _ condensed matter physics _",
    "( benjamin / cummings , reading , ma , 1980 ) ; g. d. mahan , _ many particle physics _",
    "( kluwer academic / plenum new york , 2000 ) .",
    "j. skilling in _ maximum entropy and bayesian methods _ edited by j. skilling ( kluwer academic , dordrecht , 1989 ) , p.45 s.f gull in",
    "_ maximum entropy and bayesian methods _ edited by j. skilling ( kluwer academic , dordrecht , 1989 ) , p.53 r.k .",
    "bryan , eur .",
    "j. * 18 * , 165 , ( 1990 ) s. f. gull and j. skilling , iee proceedings * 131 * , 646 ( 1984 ) m. jarrell , th .",
    "maier , c. huscroft and s. moukouri , phys .",
    "b. * 64 * 195130 ( 2001 ) w.h . press ,",
    "teukolsky , w.t .",
    "vettering and b.p .",
    "flannery _ numerical recipes _ ( cambridge university press , 1989 ) , chap"
  ],
  "abstract_text": [
    "<S> we present a maximum entropy method ( mem ) for obtaining dynamical spectra from quantum monte carlo data which have a sign problem . by relating the sign fluctuations to the norm of the spectra </S>",
    "<S> , our method properly treats the correlations between the measured quantities and the sign . </S>",
    "<S> the method greatly improves the quality and the resolution of the spectra , enabling it to produce good spectra even for poorly conditioned data where standard mem fails . </S>"
  ]
}