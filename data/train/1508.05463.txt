{
  "article_text": [
    "deep neural networks is a branch in machine learning that has seen a meteoric rise in popularity due to its powerful abilities to represent and model high - level abstractions in highly complex data .",
    "deep neural networks have shown considerable capabilities for handling specific complex tasks such as speech recognition  @xcite , object recognition  @xcite , and natural language processing  @xcite .",
    "recent advances in improving the performance of deep neural networks have focused on areas such as network regularization  @xcite , activation functions  @xcite , and deeper architectures  @xcite .",
    "however , the neural connectivity formation of deep neural networks has remained largely the same over the past decade and thus further exploration and investigation on alternative approaches to neural connectivity formation can hold considerable promise .    to explore alternate deep neural network connectivity formation , we take inspiration from nature by looking at the way brain develops synaptic connectivity between neurons . recently , in a pivotal paper by hill _ et al . _",
    "@xcite , data of living brain tissue from wistar rats was collected and used to construct a partial map of a rat brain . based on this map , hill _",
    "et al._came to a very surprising conclusion .",
    "the synaptic formation , of specific functional connectivity in neocortical neural microcircuits , can be modelled and predicted as a random formation . in comparison , for the construction of deep neural networks , the neural connectivity formation is largely deterministic and pre - defined .    motivated by hill",
    "_ et al . _",
    "s finding of random neural connectivity formation , we aim to investigate the feasibility and efficacy of devising stochastic neural connectivity formation to construct deep neural networks . to achieve this goal",
    ", we introduce the concept of stochasticnet , where the key idea is to leverage random graph theory  @xcite to form deep neural networks via stochastic connectivity between neurons . as such",
    ", we treat the formed deep neural networks as particular realizations of a random graph",
    ". such stochastic synaptic formations in a deep neural network architecture can potentially allow for efficient utilization of neurons for performing specific tasks .",
    "furthermore , since the focus is on neural connectivity , the stochasticnet architecture can be used directly like a conventional deep neural network and benefit from all of the same approaches used for conventional networks such as data augmentation , stochastic pooling , and dropout  @xcite , and dropconnect  @xcite .",
    "while a number of stochastic strategies for improving deep neural network performance have been previously introduced  @xcite , it is very important to note that the proposed stochasticnets is fundamentally different from these existing stochastic strategies in that stochasticnets main significant contributions deals primarily with the formation of neural connectivity of individual neurons to construct efficient deep neural networks that are inherently sparse * prior * to training , while previous stochastic strategies deal with either the grouping of existing neural connections to explicitly enforce sparsity  @xcite , or removal / introduction of neural connectivity for regularization * during * training .",
    "more specifically , stochasticnets is a realization of a random graph formed prior to training and as such the connectivity in the network are * inherently sparse * , and are * permanent * and do not change during training .",
    "this is very different from dropout  @xcite and dropconnect  @xcite where the activations and connections are temporarily removed during training and put back during test for regularization purposes only , and as such the resulting neural connectivity of the network remains dense .",
    "there is no notion of dropping in stochasticnets as only a subset of possible neural connections are formed in the first place prior to training , and the resulting network connectivity of the network is sparse .",
    "stochasticnets are also very different from hashnets  @xcite , where connection weights are randomly grouped into hash buckets , with each bucket sharing the same weights , to explicitly sparsifying into the network , since there is no notion of grouping / merging in stochasticnets ; the formed stochasticnets are naturally sparse due to the formation process .",
    "in fact , stochastic strategies such as hashnets , dropout , and dropconnect can be used * in conjunction * with stochasticnets .",
    "the paper is organized as follows .",
    "first , a review of random graph theory is presented in section 2 . the theory and design considerations behind forming stochasticnet as a random graph realizations are discussed in section 3 .",
    "experimental results using four image datasets ( cifar-10  @xcite , mnist  @xcite , svhn  @xcite , and stl-10  @xcite ) to investigate the efficacy of stochasticnets with respect to different number of neural connections as well as different training set sizes is presented in section 5 .",
    "finally , conclusions are drawn in section 6 .",
    "in this study , the goal is to leverage random graph theory  @xcite to form the neural connectivity of deep neural networks in a stochastic manner . as such , it is important to first provide a general overview of random graph theory for context . in random graph theory ,",
    "a random graph can be defined as the probability distribution over graphs  @xcite .",
    "a number of different random graph models have been proposed in literature .",
    "a commonly studied random graph model is that proposed by gilbert  @xcite , in which a random graph can be expressed by @xmath1 , where all possible edge connectivity are said to occur independently with a probability of @xmath2 , where @xmath3 .",
    "this random graph model was generalized by kovalenko  @xcite , in which the random graph can be expressed by @xmath4 , where @xmath5 is a set of vertices and the edge connectivity between two vertices @xmath6 in the graph is said to occur with a probability of @xmath7 , where @xmath8 .",
    "an illustrative example of a random graph based on this model is shown in figure  [ fig : rg ] .",
    "it can be seen that all possible edge connectivity between the nodes in the graph may occur independently with a probability of @xmath7 .",
    "therefore , based on this generalized random graph model , realizations of random graphs can be obtained by starting with a set of @xmath9 vertices @xmath10 and randomly adding a set of edges between the vertices based on the set of possible edges @xmath11 independently with a probability of @xmath7 .",
    "a number of realizations of the random graph in figure  [ fig : rg ] are provided in figure  [ fig : rgrealization ] for illustrative purposes .",
    "it is worth noting that because of the underlying probability distribution , the generated realizations of the random graph often exhibit differing edge connectivity .    [ cols=\"^,^,^,^ \" , ]      motivated by the results shown in figure  [ fig : trainingvscon ] , a comprehensive experiment were done to demonstrate the performance of the proposed stochasticnets on different benchmark image datasets .",
    "stochasticnet realizations were formed with 39% neural connectivity via gaussian - distributed connectivity when compared to a conventional convnet .",
    "the stochasticnets and convnets were trained on four benchmark image datasets ( i.e. , cifar-10 , mnist , svhn , and stl-10 ) and their training and test error performances are compared to each other . since the neural connectivity of stochasticnets are realized stochastically , the performance of the stochasticnets was evaluated based on 25 trials ( leading to 25 stochasticnet realizations ) and the reported results are based on the average of the 25 trials .",
    "figure  [ fig : stochasticnetvsconvnet ] shows the training and test error results of the stochasticnets and convnets on the four different tested datasets .",
    "it can be observed that , despite the fact that there are less than half as many neural connections in the stochasticnet realizations , the test errors between convnets and the stochasticnet realizations can be considered to be the same for cifar-10 , mnist , and svhn datasets .",
    "interestingly , it was also observed that the test errors for the stochasticnet realizations is lower than that achieved using the convnet ( relative improvement in test error rate of @xmath06% compared to convnet ) for the stl-10 dataset , again despite the fact that there are less than half as many neural connections in the stochasticnet realizations .",
    "the results for the stl-10 dataset truly illustrates the particular effectiveness of stochasticnets , particularly when dealing with low number of training samples .",
    "furthermore , the gap between the training and test errors of the stochasticnets is less than that of the convnets , which would indicate reduced overfitting in the stochasticnets .",
    "the standard deviation of the 25 trials for each error curve is shown as dashed lines around the error curve .",
    "it can be observed that the standard deviation of the 25 trials is very small and indicates that the proposed stochasticnet exhibited similar performance in all 25 trials .",
    "given that the experiments in the previous sections show that stochasticnets can achieve good performance relative to conventional convnets while having significantly fewer neural connections , we now further investigate the relative speed of stochasticnets during classification with respect to the number of neural connections formed in the constructed stochasticnets . here , as with section  [ numconnections",
    "] , the neural connection probability is varied in both the convolutional layers and the hidden layer to achieve the desired number of neural connections for testing its effect on the classification speed of the formed stochasticnets .",
    "figure  [ fig : speedvscon ] demonstrates the relative classification time vs. the neural connectivity percentage relative to the baseline convnet .",
    "the relative time is defined as the time required during the classification process relative to that of the convnet .",
    "it can be observed that the relative time decreases as the number of neural connections decrease , which illustrates the potential for stochasticnets to enable more efficient classification .",
    "in this study , we introduced a new approach to deep neural network formation inspired by the stochastic connectivity exhibited in synaptic connectivity between neurons .",
    "the proposed stochasticnet is a deep neural network that is formed as a realization of a random graph , where the synaptic connectivity between neurons are formed stochastically based on a probability distribution . using this approach ,",
    "the neural connectivity within the deep neural network can be formed in a way that facilitates efficient neural utilization , resulting in deep neural networks with much fewer neural connections while achieving the same modeling accuracy .",
    "the effectiveness and efficiency of the proposed stochasticnet was evaluated using four popular benchmark image datasets and compared to a conventional convolutional neural network ( convnet ) .",
    "experimental results demonstrate that the proposed stochasticnet provides comparable accuracy as the conventional convnet with much less number of neural connections while reducing the overfitting issue associating with the conventional convnet for cifar-10 , mnist , and svhn datasets .",
    "more interestingly , a stochasticnet with much less number of neural connections was found to achieve higher accuracy when compared to conventional deep neural networks for the stl-10 dataset .",
    "as such , the proposed stochasticnet holds great potential for enabling the formation of much more efficient deep neural networks that have fast operational speeds while still achieving strong accuracy .",
    "this work was supported by the natural sciences and engineering research council of canada , canada research chairs program , and the ontario ministry of research and innovation .",
    "the authors also thank nvidia for the gpu hardware used in this study through the nvidia hardware grant program .",
    "m.s . and a.w .",
    "conceived and designed the architecture . m.s .",
    ", p.s . , and a.w",
    ". worked on formulation and derivation of architecture .",
    "m.s . implemented the architecture and performed the experiments .",
    ", p.s . , and a.w .",
    "performed the data analysis .",
    "all authors contributed to writing the paper and to the editing of the paper .",
    "he , k. et al . delving deep into rectifiers : surpassing human - level performance on imagenet classification .",
    "_ arxiv _ 1 - 11 ( 2015 ) .",
    "szegedy , c. et al .",
    "going deeper with convolutions .",
    "_ conference on computer vision and pattern recognition _ ( 2015 ) .",
    "hill , s. et al .",
    "statistical connectivity provides a sufficient foundation for specific functional connectivity in neocortical neural microcircuits .",
    "_ proceedings of national academy of sciences of the united states of america _ * 109(42 ) * 28852894 ( 2012 ) .",
    "chen , w. et al .",
    "dropout : a simple way to prevent neural networks from overfitting . _",
    "journal of machine learning research _ 1929 - 1958 ( 2014 ) .",
    "li , w. et al .",
    "regularization of neural networks using dropconnect .",
    "_ icml _ ( 2013 ) .",
    "chen , w. et al .",
    "compressing neural networks with the hashing trick .",
    "_ icml _ 2285 - 2294 ( 2015 ) .",
    "bollobs , b. , and chung , f. probabilistic combinatorics and its applications .",
    "_ american mathematical soc . _ ( 1991 ) .",
    "kovalenko , i. the structure of random directed graph .",
    "statist . _",
    "krizhevsky , a. learning multiple layers of features from tiny images .",
    "lecun , y. et al .",
    "gradient - based learning applied to document recognition . _",
    "proceedings of the ieee _ * 86(11 ) * 22782324 ( 1998 ) .",
    "netzer , y. et al .",
    "reading digits in natural images with unsupervised feature learning .",
    "_ nips workshop _ ( 2011 ) .",
    "coates , a. et al . an analysis of single layer networks in unsupervised feature learning",
    ". _ aistats _ ( 2011 ) ."
  ],
  "abstract_text": [
    "<S> deep neural networks is a branch in machine learning that has seen a meteoric rise in popularity due to its powerful abilities to represent and model high - level abstractions in highly complex data . </S>",
    "<S> one area in deep neural networks that is ripe for exploration is neural connectivity formation . </S>",
    "<S> a pivotal study on the brain tissue of rats found that synaptic formation for specific functional connectivity in neocortical neural microcircuits can be surprisingly well modeled and predicted as a random formation </S>",
    "<S> . motivated by this intriguing finding , we introduce the concept of stochasticnet , where deep neural networks are formed via stochastic connectivity between neurons . as a result </S>",
    "<S> , any type of deep neural networks can be formed as a stochasticnet by allowing the neuron connectivity to be stochastic . </S>",
    "<S> stochastic synaptic formations , in a deep neural network architecture , can allow for efficient utilization of neurons for performing specific tasks . </S>",
    "<S> to evaluate the feasibility of such a deep neural network architecture , we train a stochasticnet using four different image datasets ( cifar-10 , mnist , svhn , and stl-10 ) . </S>",
    "<S> experimental results show that a stochasticnet , using less than half the number of neural connections as a conventional deep neural network , achieves comparable accuracy and reduces overfitting on the cifar-10 , mnist and svhn dataset . </S>",
    "<S> interestingly , stochasticnet with less than half the number of neural connections , achieved a higher accuracy ( relative improvement in test error rate of @xmath06% compared to convnet ) on the stl-10 dataset than a conventional deep neural network . </S>",
    "<S> finally , stochasticnets have faster operational speeds while achieving better or similar accuracy performances . </S>"
  ]
}