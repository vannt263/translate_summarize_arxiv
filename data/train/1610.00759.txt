{
  "article_text": [
    "human action and activity understanding has been a topic of great interest in computer vision and robotics in recent years .",
    "many techniques have been developed for recognizing actions and large benchmark datasets have been proposed , with most of them focusing on full - body actions @xcite .",
    "typically , computationally approaches treat action recognition as a classification problem , where the input is a previously segmented video , and the output a set of candidate action labels .    however , there is more to action understanding , as demonstrated by biological vision . as we humans observe ,",
    "we constantly perceive , and update our belief about the observed action and about future events .",
    "we constantly recognize the ongoing action .",
    "but there is even more to it .",
    "we can understand the kinematics of the ongoing action , the limbs future positions and velocities .",
    "we also understand the observed actions in terms of our own motor - representations .",
    "that is , we are able to interpret others actions in terms of dynamics and forces , and predict the effects of these forces on objects .",
    "similarly , cognitive robots that will assist human partners will need to understand their intended actions at an early stage .",
    "if a robot needs to act , it can not have a long delay in visual processing .",
    "it needs to recognize in real - time to plan its actions .",
    "a fully functional perception action loop requires the robot to _ predict _ , so it can efficiently allocate future processes .",
    "finally , even vision processes for multimedia tasks may benefit from being predictive . interpreting human activities is a very complex task and requires both , low - level vision processes and high - level cognitive processes with knowledge about actions . @xcite .",
    "considering the challenges in state of the art visual action recognition , we argue that by integrating closely the high - level with the low - level vision processes , with the high - level modifying the visual processes @xcite , a better recognition may be achieved .",
    "prediction plays an essential component in this interaction",
    ". we can think about the action - perception loop of our cognitive system from the viewpoint of a control system .",
    "the sensors take measurements of the human activity .",
    "we then apply visual operations on this signal and extract ( possibly using additional cognitive processes ) useful information for creating the control signal in order to change the state of the cognitive system . because the processing of the signal takes time",
    ", this creates a delay for the control @xcite .",
    "it is therefore important to compute meaningful information that allows us to predict the future state of the cognitive system . in this work",
    ", we are specifically interested in manipulation actions and how visual information of hand movements can be exploited for predicting future action so that the crucial delay in the control loop can be shortened ( for an illustration see fig .",
    "[ fig : action_pred ] ) .        hand movements and actions have long been studied in computer vision to create systems for applications such as recognition of sign language @xcite .",
    "more recent applications include gesture recognition @xcite , visual interfaces @xcite , and driver analysis @xcite .",
    "different methods model the temporal evolution of actions using formalisms such as hidden markov models @xcite , conditional random fields @xcite and 3d convolutional neural networks @xcite . while in principle , some of these approaches , could be used for online prediction , they are always treated as recognition modules . in recent years",
    "a number of works have developed tools for general hand pose estimation and hand tracking , which can be building blocks for applications involving hand movement recognition .",
    "for example , building on work on full - body recognition @xcite , @xcite develops a learning - based approach using depth contrast features and random forest classifiers .",
    "@xcite in a model - based approach use a 27-degree of freedom model of the hand built from geometric primitives and gpu accelerated particle swarm optimization .",
    "so far , these trackers and pose estimators work well on isolated hands , but methods still struggle with hands in interaction with objects @xcite , although there are efforts underway to deal with such situations @xcite .",
    "inspiration for our work comes from studies in cognitive sciences on hand motion .",
    "the grasp and the movement kinematics are strongly related to the manipulation action @xcite . it has been shown that an actor s intention shapes his / her movement kinematics during movement execution , and , furthermore , observers are sensitive to this information @xcite .",
    "they can see early differences in visual kinematics and use them to discriminate between movements performed with different intentions .",
    "kinematic studies have looked at such physical differences in movement .",
    "for example , @xcite found that when subjects grasped a bottle for pouring , the middle and the ring fingers were more extended than when they grasped the bottle with the intent of displacing , throwing , or passing it .",
    "similarly , @xcite found that subjects placed their thumb and index fingers in higher positions when the bottle was grasped to pour than to lift .",
    "it appears that the visual information in the early phases of the action is often sufficient for observers to understand the intention of action . starting from this intuition , we a. ) conducted a study to evalute humans performance in recognizing manipulation actions ; b. ) implemented a computational system using state - of the art learning algorithms .",
    "the psychophysical experiment was designed to evaluate human s performance in recognizing manipulation actions in their early phases .",
    "these include : _ 1 ) _ the grasp preparation , which is the phase when the hand moves towards the object and the fingers shape to touch the object ; _ 2 ) _ the grasp , when the hand comes in contact with the object to hold it in a stable position ; and _ 3 ) _ the early actual action movement of the hand together with the object . throughout these three phases , observers judgment of the action",
    "becomes more reliable and confident .",
    "the study gives us an insight about the difficulty of the task and provides data for evaluating our computational method .",
    "our computational approach processes the sensory input as a continuous signal and formulates action interpretation as a continuous updating of the prediction of intended action .",
    "this concept is applied to two different tasks .",
    "first , from the stream of video input , we continuously predict the identity of the ongoing action .",
    "second , using as input the video stream , we predict the forces on the fingers applied to grasped objects .",
    "next , we provide a motivation for our choice of the two tasks , after which we give an overview of our approach .",
    "the first task is about action prediction from video .",
    "we humans are able to update our beliefs about the observed action , and predict it before it is completed .",
    "this capability is essential to be pro - active and react to the actions of others .",
    "robots that interact with humans also need such capability .",
    "predicting future actions of their counterpart allows them to allocate computational resources for their own reaction appropriately .",
    "for example , if a person is passing a cup to the robot , it has to understand what is happening well before the action is completed , so it can prepare the appropriate action to receive it .",
    "furthermore , vision processes have to be initiated and possibly tuned with _ predicted _ information , so the cup can be detected at the correct location , its pose estimated , and possibly other task - specific processes performed ( for example , the content of the cup may need to be recognized ) .",
    "the second task is about predicting the tactile signal of the intended action .",
    "findings of neuroscience on the mirror neuron system @xcite provide evidence for a close relationship between mechanisms of action and perception in primates .",
    "humans develop haptic perception through interaction with objects and learn to relate haptic with visual perception .",
    "furthermore , they develop the capability of hallucinating the haptic stimulus when seeing hands in certain configurations interacting with objects @xcite .",
    "this capability of hallucinating force patterns from visual input is essential for a more detailed analysis of the interaction with the physical world .",
    "it can be used to _ reason about the current interaction _ between the hand and the object , and to _ predict the action consequences _ driven by the estimated force pattern .",
    "furthermore , by associating vision with forces , we expect to obtain better computational action recognition modules .",
    "intuitively , the force vectors , whose dimensions are much lower than the visual descriptors , should provide useful compact information for classification , especially when the training data is not large . a first experiment , presented in section [ sec : predict_forces ] , confirms this idea .    most important , the force patterns may be used in robot learning .",
    "a popular paradigm in robotics is imitation learning or learning from demonstration @xcite , where the robot learns from examples provided by a demonstrator .",
    "if the forces can be predicted from images , then the force profiles together with the positional information can be used to teach the robot with video only .",
    "many researchers are trying to teach robots actions and skills that involve forces , e.g. wiping a kitchen table @xcite , pull and flip tasks @xcite , ironing or opening a door @xcite .",
    "these approaches rely on haptic devices or force and torque sensors on the robot to obtain the force profiles for the robot to learn the task .",
    "if we can predict the forces exerted by the human demonstrator , the demonstration could become vision only .",
    "this would allow us to teach robots force interaction tasks much more efficiently .    in order to solve the above two tasks , we take advantage of new developments in machine learning . specifically , we build on the recent success of recurrent neural networks ( rnns ) in conjunction with visual features from pre - trained convolutional neural networks ( cnns ) and training from a limited number of weakly annotated data .",
    "for the first task , we use an rnn to recognize the ongoing action from video input .",
    "a camera records videos of humans performing a number of manipulation actions on different objects .",
    "for example , they ` drink ' from a cup , ` pour ' from it , ` pound ' , ` shake ' , and ` move ' it ; or they ` squeeze ' a sponge , ` flip ' it , ` wash ' , ` wipe ' , and ` scratch ' with it .",
    "our system extracts patches around the hands , and feeds these patches to an rnn , which was trained offline to predict in real - time the ongoing action . for the second task , we collected videos of actions and synchronized streams of force measurements on the hand , and we used this data to train an rnn to predict the forces , using only the segmented hand patches in video input .",
    "the main contributions of the paper are : _ 1 ) _ we present the first computational study on the prediction of observed dexterous actions _ 2 ) _ we demonstrate an implementation for predicting intended dexterous actions from videos ; _ 3 ) _ we present a method for estimating tactile signals from visual input without considering a model of the object ; _ 4 ) _ we provide new datasets that serve as test - beds for the aforementioned tasks .",
    "we will focus our review on studies along the following concepts : the idea of prediction , including prediction of intention and future events ( _ a _ ) , prediction beyond appearance ( _ b _ ) , and prediction of contact forces on hands ( _ c _ ) , work on hand actions ( _ d _ ) , manipulation datasets ( _ e _ ) and action classification as a continuous process using various kinds of techniques and different kinds of inputs ( _ f _ ) .    * prediction of action intention and future events : * a small number of works in computer vision have aimed to predict intended action from visual input . for example , @xcite use a ranking svm to predict the persuasive motivation ( or the intention ) of the photographer who captured an image . @xcite",
    "seek to infer the motivation of the person in the image by mining knowledge stored in a large corpus using natural language processing techniques .",
    "@xcite propose that the grasp type , which is recognized in single images using cnns , reveals the general category of a person s intended action . in @xcite ,",
    "a temporal conditional random field model is used to infer anticipated human activities by taking into consideration object affordances .",
    "other works attempt to predict events in the future .",
    "for example , @xcite use concept detectors to predict future trajectories in a surveillance videos .",
    "@xcite learn from sequences of abstract images the relative motion of objects observed in single images .",
    "@xcite employ visual mid - level elements to learn from videos how to predict possible object trajectories in single images .",
    "more recently , @xcite learn using cnn feature representations how to predict from one frame in the video the actions and objects in a future frame .",
    "our study also is about prediction of future events using neural networks .",
    "but while the above studies attempt to learn abstract concepts for reasoning in a passive setting , our goal is to perform online prediction of specific actions from video of the recent past .    *",
    "physics beyond appearance : * many recent approaches in robotics and computer vision aim to infer physical properties beyond appearance models from visual inputs .",
    "@xcite propose that implicit information , such as functional objects , can be inferred from video .",
    "@xcite takes a task - oriented viewpoint and models objects using a simulation engine .",
    "the general idea of associating images with forces has previously been used for object manipulation .",
    "the technique is called vision - based force measurement , and refers to the estimation of forces according to the observed deformations of an object @xcite .",
    "along this idea , recently @xcite proposed a method using an rnn for the classification of forces due to tissue deformation in robotic assisted surgery .",
    "* inference of manipulation forces : * the first work in the computer vision literature to simulate contact forces during hand - object interactions is @xcite . using as input rgb data , a model - based tracker estimates the poses of the hand and a known object , from which then the contact points and the motion trajectory are derived .",
    "next , the minimal contact forces ( nominal forces ) explaining the kinematic observations are computed from the newton - euler dynamics solving a conic optimization .",
    "humans typically apply more than the minimal forces .",
    "these additional forces are learned using a neural network on data collected from subjects , where the force sensors are attached to the object .",
    "another approach on contact force simulation is due to @xcite .",
    "the authors segment the hand from rgbd data in single egocentric views and classify the pose into 71 functional grasp categories as proposed in @xcite .",
    "classified poses are matched to a library of graphically created hand poses , and theses poses are associated with force vectors normal to the meshes at contact points .",
    "thus the forces on the observed hand are obtained by finding the closest matching synthetic model .",
    "both of these prior approaches derive the forces using model based - approaches .",
    "the forces are computed from the contact points , the shape of the hand , and dynamic observations .",
    "furthermore , both use rgbd data , while ours is an end - to - end learning approach using as input only images .",
    "* dexterous actions : * the robotics community has been studying perception and control problems of dexterous actions for decades @xcite .",
    "some works have studied grasping taxonomies @xcite , how to recognize grasp types @xcite and how to encode and represent human hand motion @xcite .",
    "@xcite proposed a representation of objects in terms of their interaction with human hands .",
    "real - time visual trackers @xcite were developed , facilitating computational research with hands .",
    "recently , several learning based systems were reported that infer contact points or how to grasp an object from its appearance @xcite .",
    "* manipulation datasets : * a number of object manipulation datasets have been created , many of them recorded with wearable cameras providing egocentric views .",
    "for example , the yale grasping dataset @xcite contains wide - angle head - mounted camera videos recorded from four people during regular activities with images tagged with the hand grasp ( of 33 classes ) .",
    "similarly , the ut grasp dataset @xcite contains head - mounted camera video of people grasping objects on a table , and was tagged with grasps ( of 17 classes ) .",
    "the gtea set @xcite has egocentric videos of household activities with the objects annotated .",
    "other datasets have egocentric rgb - d videos .",
    "the uci - ego @xcite features object manipulation scenes with annotation of the 3d hand poses , and the gun-71 @xcite features subjects grasping objects , where care was taken to have the same amount of data for each of the 71 grasp types .",
    "our datasets , in contrast , are taken from the third - person viewpoint . while having less variation in the visual setting than most of the above datasets , it focuses on the dynamic aspects of different actions , which manipulate the same objects .",
    "* action recognition as an online process : * action recognition has been extensively studied .",
    "however , few of the proposed methods treat action recognition as a continuous ( in the online sense ) process ; typically , action classification is performed on _ whole _",
    "action sequences @xcite .",
    "recent works include building robust action models based on mocap data @xcite or using cnns for large - scale video classification @xcite .",
    "most methods that take into account action dynamics usually operate under a stochastic process formulation , e.g. , by using hidden markov models @xcite or semi - markov models @xcite .",
    "hmms can model relations between consecutive image frames , but they can not be applied to high - dimensional feature vectors . in @xcite",
    "the authors propose an online action recognition method by means of svm classification of sparsely coded features on a sliding temporal window .",
    "most of the above methods assume only short - time dependencies between frames , make restrictive assumptions about the markovian order of the underlying processs and/or rely on global optimization over the whole sequence .    in recent work a few studies proposed approaches to recognition of partially observed actions under the headings of _ early event detection _ or _",
    "early action recognition_. @xcite creates a representation that encodes how histograms of spatio - temporal features change over time . in a probabilistic model , the histograms",
    "are modeled with gaussian distributions , and map estimation over all subsequences is used to recognize the ongoing activity .",
    "a second approach in the paper models the sequential structure in the changing histogram representation , and matches subsequences of the video using dynamic programming .",
    "both approaches were evaluated on full body action sequences . in @xcite images are represented by spatio - temporal features and histograms of optical flow , and a hierarchical structure of video - subsegments is used to detect partial action sequences in first - person videos .",
    "@xcite perform early recognition of activities in first person - videos by capturing special sub - sequences characteristic for the onset of the main activity .",
    "@xcite propose a maximum - margin framework ( a variant of svm ) to train visual detectors to recognize partial events .",
    "the classifier is trained with all the video sub - sequences of different length . to enforce the sequential nature of the events ,",
    "additional constraints on the score function of the classifier are enforced , for example , it has to increase as more frames are matched .",
    "the technique was demonstrated in multiple applications , including detection of facial expressions , hand gestures , and activities .",
    "the main learning tools used here , the rnn and the long short term memory ( lstm ) model , were recently popularized in language processing , and have been used for translating videos to language @xcite , image description generation @xcite , object recognition @xcite , and the estimation of object motion @xcite .",
    "rnns were also used for action recognition @xcite to learn dynamic changes within the action .",
    "the aforementioned paper still performs whole video classification by using average pooling and does not consider the use of rnns for prediction . in a very recent work , however , @xcite train a lstm using novel ranking losses for early activity detection .",
    "our contribution regarding action recognition is not that we introduce a new technique .",
    "we use an existing method ( lstm ) and demonstrate it in an online prediction system .",
    "the system keeps predicting , and considers the prediction reliable , when the predicted label converges ( i.e. stays the same over a number of frames ) .",
    "furthermore , the subject of our study is novel .",
    "the previous approaches consider the classical full body action problem . here",
    "our emphasis is specifically on the hand motion , not considering other information such as the objects involved .",
    "in this section , we first review the basics of recurrent neural networks ( rnns ) and the long short term memory ( lstm ) model . then we describe the specific algorithms for prediction of actions and forces used in our approach .",
    "recurrent neural networks have long been used for modeling temporal sequences .",
    "the recurrent connections are feedback loops in the unfolded network , and because of these connections rnns are suitable for modeling time series with strong nonlinear dynamics and long time correlations .    given a sequence @xmath0 , a rnn computes a sequence of hidden states @xmath1 and outputs @xmath2 as follows : @xmath3 where @xmath4 denote weight matrices , @xmath5 denote the biases , and @xmath6 and @xmath7 are the activation functions of the hidden layer and the output layer , respectively .",
    "typically , the activation functions are defined as logistic sigmoid functions .",
    "the traditional rnn is hard to train due to the so called _ vanishing gradient _ problem , i.e. the weight updates computed via error backpropagation through time may become very small .",
    "the long short term memory model @xcite has been proposed as a solution to overcome this problem .",
    "the lstm architecture uses memory cells with gated access to store and output information , which alleviates the vanishing gradient problem in backpropagation over multiple time steps .",
    "specifically , in addition to the hidden state @xmath8 , the lstm also includes an input gate @xmath9 , a forget gate @xmath10 , an output gate @xmath11 , and the memory cell @xmath12 ( shown in figure [ fig : lstm_diagram ] ) . the hidden layer and",
    "the additional gates and cells are updated as follows : @xmath13    in this architecture , @xmath9 and @xmath10 are sigmoidal gating functions , and these two terms learn to control the portions of the current input and the previous memory that the lstm takes into consideration for overwriting the previous state .",
    "meanwhile , the output gate @xmath11 controls how much of the memory should be transferred to the hidden state .",
    "these mechanisms allow lstm networks to learn temporal dynamics with long time constants .          in this section ,",
    "we describe our proposed model for action prediction .",
    "we focus on manipulation actions where a person manipulates an object using a single hand . given a video sequence of a manipulation action ,",
    "the goal is to generate a sequence of belief distributions over the predicted actions while watching the video . instead of assigning an action label to the whole sequence",
    ", we continuously update our prediction as frames of the video are processed .",
    "[ [ visual - representation ] ] visual representation : + + + + + + + + + + + + + + + + + + + + + +    the visual information most essential for manipulation actions comes from the pose and movement of the hands , while the body movements are less important .",
    "therefore , we first track the hand using a mean - shift based tracker @xcite , and use cropped image patches centered on the hand . in order to create abstract representations of image patches ,",
    "we project each patch through a pre - trained cnn model ( shown in figure [ fig : lstm_unfolded ] ) .",
    "this provides the feature vectors used as input to the rnn .",
    "[ [ action - prediction ] ] action prediction : + + + + + + + + + + + + + + + + + +    in our model , the lstm is trained using as input a sequence of feature vectors @xmath14 and the action labels @xmath15 $ ] .",
    "the hidden states and the memory cell values are updated according to equations ( [ eq : lstm - first])-([eq : lstm - last ] ) .",
    "then logistic regression is used to map the hidden states to the label space as follows : @xmath16 then the predicted action label is obtained as : @xmath17    [ [ model - learning ] ] model learning : + + + + + + + + + + + + + + +    we follow the common approach of training the model by minimizing the negative log - likelihood over the dataset @xmath18 .",
    "the loss function is defined as @xmath19 where @xmath20 and @xmath21 denote the weight matrix and the bias term .",
    "these parameters can be learnt using the stochastic gradient descent algorithm .",
    "since we aim for the ongoing prediction rather than a classification of the whole sequence , we do not perform a pooling over the sequences to generate the outputs .",
    "each prediction is based only on the current frame and the current hidden state , which implicitly encodes information about the history . in practice",
    ", we achieve learning by performing backpropagation at each frame .",
    "we use a model similar to the one above to predict the forces on the fingers from visual input .",
    "given video sequences of actions , as well as simultaneously recorded sequences of force measurements ( see sec .  [",
    "sec : glove ] ) , we reformulate the lstm model , such that it predicts force estimates as close as possible to the ground truth values .    as before , we use as input to the lstm features from pre - trained cnns applied to image patches .",
    "in addition , the force measurements @xmath22 , @xmath23 , are used as target values , where @xmath24 is the number of force sensors attached to the hand .",
    "then the forces are estimated as : @xmath25    to train the force estimation model , we define the loss function as the least squares distance between the estimated value and the ground truth , and minimize it over the training set using stochastic gradient descent as : @xmath26",
    "[ cols=\"^,^ \" , ]      one motivation for predicting forces , is that the additional data , which we learned through association , may help increase recognition accuracy .",
    "there is evidence that human understand others actions in terms of their own motor primitives @xcite .",
    "however , so far these findings have not been modeled in computational terms .",
    "to evaluate the usefulness of the predicted forces , we applied our force estimation algorithm on the maf dataset to compute the force values .",
    "then we used the vision data together with the regressed force values as bimodal information to train a network for action predicton .",
    "table [ tbl : pred_with_force ] shows the results of the prediction accuracy with the bimodal information on different objects . referring to the table ,",
    "the overall average accuracy for the combined vision force data ( v+f ) was @xmath27 higher than for vision information only .",
    "this first attempt on predicting with bimodal data demonstrates the potential of utilizing visually estimated forces for recognition .",
    "future work will further elaborate on the idea and explore networks @xcite , which can be trained from both vision and force at the same time to learn `` hallucinate '' the forces and predict actions .    as discussed in the introduction ,",
    "the other advantage is that we will be able to teach robots through video demonstration . if we can predict forces exerted by the human demonstrator and provide the force profile of the task using vision only",
    ", this would have a huge impact on the way robots learn force interaction tasks . in future work",
    "we plan to develop and employ sensors that can also measure the tangential forces , i.e. the frictions , on the fingers .",
    "we also will expand the sensor coverage to the whole hand .",
    "with these two improvements , our method could be applied to a range of complicated task such as screwing or assembling .",
    "in this paper we proposed an approach to action interpretation , which treats the problem as a continuous updating of beliefs and predictions .",
    "the ideas were implemented for two tasks : the prediction of perceived action from visual input , and the prediction of force values on the hand .",
    "the methods were shown to run in real - time and demonstrated high accuracy performance .",
    "the action prediction was evaluated also against human performance , and shown to be nearly on par .",
    "additionally , new datasets of videos of dexterous actions and force measurements were created , which can be accessed from @xcite .",
    "the methods presented here are only a first implementation of a concept that can be further developed along a number of directions . here",
    ", we applied learning on 2d images only , and clearly , this way we also learn properties of the images that are not relevant to the task , such as the background textures . in order to become robust to these ` nuisances ' , 3d information , such as contours and depth features ,",
    "could be considered in future work .",
    "while the current implementation only considers action labels , the same framework can be applied for other aspects of action understanding .",
    "for example , one can describe the different phases of actions and predict these sub - actions since different actions share similar small movements .",
    "one can also describe the movements of other body parts , e.g. , the arms and shoulders .",
    "finally , the predicted forces may be used for learning how to perform actions on the robot .",
    "future work will attempt to map the forces from the human hands onto other actuators , for example three - fingered hands or grippers .",
    "this work was funded by the support of the national science foundation under grant sma 1540917 and grant cns 1544797 , by samsung under the gro program ( n020477 , 355022 ) , and by darpa through u.s .",
    "army grant w911nf-14 - 1 - 0384 ."
  ],
  "abstract_text": [
    "<S> looking at a person s hands one often can tell what the person is going to do next , how his / her hands are moving and where they will be , because an actor s intentions shape his / her movement kinematics during action execution . </S>",
    "<S> similarly , active systems with real - time constraints must not simply rely on passive video - segment classification , but they have to continuously update their estimates and predict future actions . in this paper , </S>",
    "<S> we study the prediction of dexterous actions . </S>",
    "<S> we recorded from subjects performing different manipulation actions on the same object , such as `` squeezing '' , `` flipping '' , `` washing '' , `` wiping '' and `` scratching '' with a sponge . </S>",
    "<S> in psychophysical experiments , we evaluated human observers skills in predicting actions from video sequences of different length , depicting the hand movement in the preparation and execution of actions before and after contact with the object . </S>",
    "<S> we then developed a recurrent neural network based method for action prediction using as input patches around the hand . </S>",
    "<S> we also used the same formalism to predict the forces on the finger tips using for training synchronized video and force data streams . </S>",
    "<S> evaluations on two new datasets show that our system closely matches human performance in the recognition task , and demonstrate the ability of our algorithms to predict real - time what and how a dexterous action is performed . </S>"
  ]
}