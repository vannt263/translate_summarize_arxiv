{
  "article_text": [
    "for many systems , temporal logic formulas are used to describe desirable system properties such as safety , stability , and liveness @xcite . given a stochastic system modeled as a , the synthesis problem is to find a policy that achieves optimal performance under a given quantitative criterion regarding given temporal logic formulas .",
    "for instance , the objective may be to find a policy that maximizes the probability of satisfying a given temporal logic formula .",
    "in such a problem , we need to keep track of the evolution of state variables that capture system dynamics as well as predicate variables that encode properties associated with the temporal logic constraints @xcite .",
    "as the number of states grows exponentially in the number of variables , we often encounter large s , for which the synthesis problems are impractical to solve with centralized methods .",
    "the insight for control synthesis of large - scale systems is to exploit the modular structure in a system so that we can solve the original problem by solving a set of small subproblems .    in literature , distributed control synthesis methods are proposed in the pioneering work for s with discounted rewards @xcite .",
    "the authors formulate a two - stage distributed reinforcement learning method : the first stage constructs and solves an abstract problem derived from the original one , and the second stage iteratively computes parameters for local problems until the collection of local problems solutions converge to one that solves the original problem .",
    "recently , is combined with a sub - gradient method into planning for average - reward problems in large mdps in @xcite .",
    "however , the method in @xcite applies only when some special conditions are satisfied on the costs and transition kernels .",
    "alternatively , hierarchical reinforcement learning introduces _ action - aggregation _ and _ action - hierarchies _ to address the planning problems with large mdps @xcite . in action - aggregation ,",
    "a micro - action is a local policy for a subset of states and the global optimal policy maps histories of states into micro - actions .",
    "however , it is not always clear how to define the action hierarchies and how the choice of hierarchies affects the optimality in the global policy .",
    "additionally , the aforementioned methods are in general difficult to implement and can not handle temporal logic specifications .    for synthesis problems in s with quantitative temporal logic constraints , centralized methods and tools",
    "@xcite are developed and applied to control design of stochastic systems and robotic motion planning @xcite . since centralized algorithms",
    "are based on either value iteration or linear programming , they inevitably hit the barrier of scalability and are not viable for large s.    in this paper , we develop a distributed optimization method for large s subject to temporal logic contraints . we first introduce a decomposition method for large s and prove a property in such a decomposition that supports the application of the proposed distributed optimization . for a subclass of s",
    "whose graph structures are planar graphs , we introduce an efficient decomposition algorithm that exploits the modular structure for the underlying caused by loose coupling between subsets of states and its constituting components . then , given a decomposition of the original system",
    ", we employ a distributed optimization method called _ block splitting algorithm _ @xcite to solve the planning problem with respect to discounted - reward objectives in large s and average - reward objectives in large ergodic s. comparing to two - stage methods in @xcite , our method concurrently solves the set of sub - problems and penalizes solutions mismatches in one step during each iteration , and is easy to implement .",
    "since the distributed control synthesis is independent from the way how a large is decomposed , any decomposition method can be used .",
    "lastly , we extend the method to solve the synthesis problems for s with two classes of quantative temporal logic objectives . through case studies",
    "we investigate the performance and effectiveness of the proposed method .",
    "let @xmath0 be a finite set .",
    "let @xmath1 be the set of finite and infinite words over @xmath0 .",
    "@xmath2 is the cardinality of the set @xmath0 .",
    "a probability distribution on a finite set @xmath3 is a function @xmath4 $ ] such that @xmath5 .",
    "the support of @xmath6 is the set @xmath7 .",
    "the set of probability distributions on a finite set @xmath3 is denoted @xmath8 .",
    "* markov decision process : * a _ markov decision process _",
    "( mdp ) @xmath9 consists of a finite set @xmath3 of states , a finite set @xmath10 of actions , an initial distribution @xmath11 of states , and a transition probability function @xmath12 that for a state @xmath13 and an action @xmath14 gives the probability @xmath15 of the next state @xmath16 .",
    "given an @xmath17 we define the set of actions _ enabled _ at state @xmath18 as @xmath19 .",
    "the cardinality of the set @xmath20 is the number of _ state - action pairs _ in the .",
    "* policies : * a _ path _ is an infinite sequence @xmath21 of states such that for all @xmath22 , there exists @xmath23 , @xmath24 .",
    "policy _ is a function @xmath25 that , given a finite state sequence representing the history , chooses a probability distribution over the set @xmath10 of actions .",
    "policy @xmath26 is memoryless if it only depends on the current state , i.e. , @xmath27 .",
    "once a policy @xmath26 is chosen , an @xmath17 is reduced to a markov chain , denoted @xmath28 .",
    "we denote by @xmath29 and @xmath30 the random variables for the @xmath31-th state and the @xmath31-th action in this chain @xmath28 . given a policy @xmath26 , for a measurable function @xmath32 that maps paths into reals ,",
    "let @xmath33 $ ] ( resp .",
    "$ ] ) be the expected value of @xmath32 when the policy @xmath26 is used given @xmath35 being the initial distribution of states ( resp .",
    "@xmath18 being the initial state ) .",
    "* rewards and objectives : * given an @xmath17 , a reward function @xmath36 and a policy @xmath26 , let @xmath37 be a discounting factor , the _ discounted - reward value _ is defined as @xmath38 where @xmath39 ; the _ average - reward value _ is defined as @xmath40 where @xmath41 $ ] . a discounted - reward ( resp .",
    "an average - reward ) problem is , for a given initial state distribution , to obtain a policy that maximizes the discounted - reward value ( resp .",
    "average - reward value ) . for discounted - reward ( average - reward )",
    "problems , the optimal value can be attained by memoryless policies @xcite . a solution to the discounted - reward problem can be found by solving the problem :    [ eq : constraintdiscounted ] @xmath42    where @xmath43 is the total number of state - action pairs in the , @xmath44 is the non - negative orthant of @xmath45 , and variable @xmath46 can be interpreted as the expected discounted time of being in state @xmath18 and taking action @xmath47 . once the problem in is solved , the optimal policy is obtained as @xmath48 and the objective function s value is the optimal discounted - reward value under policy @xmath26 given the initial distribution @xmath35 of states .    in an ergodic ,",
    "the average - reward value is a constant regardless of the initial state distribution @xcite ( see @xcite for the definition of ergodicity ) .",
    "we obtain an optimal policy for an average - reward problem by solving the problem    [ eq : averagelp ] @xmath49    where @xmath46 is understood as the long - run fraction of time that the system is at state @xmath18 and the action @xmath47 is taken .",
    "once the problem in is solved , the optimal policy is obtained as @xmath50 .",
    "the optimal objective value is the optimal average - reward value and is the same for all states .",
    "* distributed optimization : * as a prelude to the distributed synthesis method developed in section  [ sec : planning ] , now we describe the _ alternating direction method of multipliers _ ( ) @xcite for the generic convex constrained minimization problem @xmath51 where function @xmath52 is closed proper convex and set @xmath53 is closed nonempty convex . in iteration @xmath54 of the algorithm the following updates are performed :    [ eq : subeqns ] @xmath55    where @xmath56 and @xmath57 are auxiliary variables , @xmath58 is the ( euclidean ) projection onto @xmath59 , and @xmath60 is the _ proximal operator _",
    "@xcite of @xmath52 with parameter @xmath61 .",
    "the algorithm handles separately the objective function @xmath52 in and the constraint set @xmath59 in . in the dual update step coordinates these two steps and results in convergence to a solution of the original problem .",
    "* temporal logic : * formulas are defined by : @xmath62 where @xmath63 is an atomic proposition , and @xmath64 and @xmath65 are temporal modal operators for `` next '' and `` until '' .",
    "additional temporal logic operators are derived from basic ones : @xmath66 ( eventually ) and @xmath67 . given an @xmath17 , let @xmath68 be a finite set of atomic propositions , and a function @xmath69 be a labeling function that assigns a set of atomic propositions @xmath70 to each state @xmath71 that are valid at the state @xmath18 .",
    "@xmath72 can be extended to paths in the usual way , i.e. , @xmath73 for @xmath74 .",
    "a path @xmath75 satisfies a temporal logic formula @xmath76 if and only if @xmath77 satisfies @xmath76 . in an @xmath17",
    ", a policy @xmath26 induces a probability distribution over paths in @xmath78 .",
    "the probability of satisfying an formula @xmath76 is the sum of probabilities of all paths that satisfy @xmath76 in the induced markov chain @xmath28 .",
    "[ prob ] given an @xmath17 and an formula @xmath76 , synthesize a policy that optimizes a quantitative performance measure with respect to the formula @xmath76 in the @xmath17 .",
    "we consider the probability of satisfying a temporal logic formula as one quantitative performance measure .",
    "we also consider the expected frequency of satisfying certain recurrent properties specified in an formula .    by formulating a product that incorporates the underlying dynamics of a given and its temporal logic specification",
    ", it can be shown that problem  [ prob ] with different quantitative performance measures can be formulated through pre - processing as special cases of discounted - reward and average - reward problems @xcite .",
    "thus , in the following , we first introduce decomposition - based distributed synthesis methods for large s with discounted - reward and average - reward criteria .",
    "then , we show the extension for solving s with quantitative temporal logic constraints .",
    "to exploit the modular structure of a given , the initial step is to decompose the state space into small subsets of states , each of which can then be related to a small problem . in this section ,",
    "we introduce some terminologies in decomposition of s from @xcite .    given an @xmath79 , let @xmath80 be any partition of the state set @xmath3 .",
    "that is , @xmath81 , @xmath82 , @xmath83 when @xmath84 and @xmath85 . a set in @xmath80",
    "is called a _",
    "region_. the _ periphery _ of a region @xmath86 is a set of states _ outside _",
    "@xmath86 , each of which can be reached with a non - zero probability by taking some action from a state _ in _ @xmath86 .",
    "formally , @xmath87    let @xmath88 . given a region @xmath89 , we call @xmath90 the _ kernel _ of @xmath86 .",
    "we denote @xmath91 the number of state - action pairs restricted to @xmath92 , for each @xmath93 .",
    "that is , @xmath91 is the cardinality of the set @xmath94 .",
    "we call the partition @xmath95 a _ decomposition _ of @xmath17 .",
    "the following property of a decomposition is exploited in distributed optimization .",
    "[ lm : kernel ] given a decomposition @xmath96 obtained from partition @xmath97 , for a state @xmath98 where @xmath99 , if there is a state @xmath16 and an action @xmath47 such that @xmath100 , then either @xmath101 or @xmath102 .",
    "suppose @xmath103 and @xmath104 , then it must be the case that @xmath105 for some @xmath106 and @xmath107 .",
    "since from state @xmath108 , after taking action @xmath47 , the probability of reaching @xmath109 is non - zero , we can conclude that @xmath110 , which implies @xmath111 .",
    "the implication contradicts the fact that @xmath98 since @xmath112 .",
    "hence , either @xmath102 or @xmath101 .",
    "* example : * consider the in figure  [ fig : exmdp ] , which is taken from @xcite .",
    "the shaded region shows a partition @xmath113 of the state space .",
    "then , @xmath114 and @xmath115 .",
    "we obtain a decomposition of @xmath17 as @xmath116 , @xmath117 , and @xmath118 .",
    "it is observed that state @xmath119 can only be reached with non - zero probabilities by actions taken from states @xmath120 and @xmath121 .    ,",
    "actions @xmath122 , and transition probability function @xmath123 as indicated.,scaledwidth=50.0% ]      various methods are developed to derive a decomposition of an , for example , decompositions based on partitioning the state space of an according to the communicating classes in the induced graph ( defined in the following ) of that ( see a survey in @xcite ) . for the distributed synthesis method developed in this paper , it will be shown later in section  [ sec : planning ] that the number of state - action pairs and the number of states in @xmath92 are the number of variables and the number of constraints in a sub - problem , respectively .",
    "thus , we prefer a decomposition that meets one simple desirable property : for each @xmath124 , the number @xmath91 of state - action pairs in @xmath92 is small in the sense that the classical linear programming algorithm can efficiently solve an with state - action pairs of this size .",
    "next , we propose a method that generates decompositions which meet the aforementioned desirable property for a subclass of s. for an in this subclass , its induced graph is _",
    "planar_. it can be shown that s derived from classical gridworld examples , which have many practical applications in robotic motion planning , are in this subclass .",
    "we start by relating an with a directed graph .",
    "the _ labeled digraph _ induced from an @xmath125 is a tuple @xmath126 where @xmath3 is a set of nodes , and @xmath127 is a set of labeled edges such that @xmath128 if and only if @xmath129 .",
    "let @xmath130 be the total number of nodes in the graph .",
    "a partition of states in the gives rise to a partition of nodes in the graph . given a partition @xmath80 and a region @xmath131 ,",
    "a node is said to be _ contained _ in @xmath86 if some edge of the region is incident to the node .",
    "a node contained in more than one regions is called a _ boundary _ node .",
    "that is , @xmath132 is a boundary node if and only if there exists @xmath133 or @xmath134 with @xmath135 .",
    "formally , the boundary nodes of @xmath86 are @xmath136 where @xmath137 and @xmath138 .",
    "we define @xmath139 .",
    "note that since @xmath140 , @xmath141 .",
    "we use the number of boundary nodes as an upper bound on the size of the set @xmath142 of states .",
    "@xcite an _ @xmath143-division of an @xmath144-node graph _ is a partition of nodes into @xmath145 subsets , each of which have @xmath146 nodes and @xmath147 boundary nodes .",
    "reference @xcite shows an algorithm that divides a planar graph of @xmath144 vertices into an @xmath143-division in @xmath148 time .",
    "[ lma : decomp]given a partition @xmath80 of an with @xmath144 states obtained with a @xmath143-division the induced graph , the number of states in @xmath142 is upper bounded by @xmath149 and the number of states in @xmath92 is upper bounded by @xmath146 .    since each boundary node is contained in at most three regions and at least one region by the property of an @xmath143-division @xcite , the total number of boundary nodes is @xmath150 .",
    "the number of states in @xmath92 is upper bounded by the size of @xmath86 , which is @xmath146 .    to obtain a decomposition",
    ", the user specifies an approximately upper bound on the number of variables for all sub - problems .",
    "then , the algorithm decides whether there is an @xmath143-division for some @xmath143 that gives rise to a decomposition that has the desirable property .",
    "although the decomposition method proposed here is applicable for a subclass of s , the distributed synthesis method developed in this paper does not constrain the way by which a decomposition is obtained . a decomposition may be given or obtained straight - forwardly by exploiting the existing modular structure of the system .",
    "even if a decomposition does not meet the desirable property for the distributed synthesis method , the proposed method still applies as long as each subproblem derived from that decomposition ( see section  [ sec : admm ] ) can be solved given the limitation in memory and computational capacities .",
    "in this section , we show that under a decomposition , the original problem for a discounted - reward or average - reward case can be formulated into one with a sparse constraint matrix .",
    "then , we employ block - splitting algorithm based on in @xcite for solving the problem in a distributed manner .      given a decomposition @xmath151 of an , let @xmath152 be a vector consisting of variables @xmath46 for all @xmath98 with all actions enabled from @xmath18 .",
    "let @xmath153 be an index function .",
    "the constraints in can be written as : for each @xmath111 , @xmath154 and for each @xmath98 , @xmath155 , @xmath156 recall that , in lemma  [ lm : kernel ] , we have proven that each @xmath98 with @xmath157 can only be reached with non - zero probabilities from states in @xmath142 and @xmath92 . as a result , for each state @xmath18 in @xmath92 with @xmath157 and each action @xmath158 , the constraint on variable @xmath46 is only related with variables in @xmath152 and @xmath159 .",
    "let @xmath160 .",
    "we denote the number of variables in @xmath152 by @xmath91 and the number of states in the set @xmath92 by @xmath161 .",
    "let @xmath162 .",
    "the problem in is then @xmath163 where @xmath164 ,    @xmath165 and @xmath166 where @xmath167 if @xmath168 .",
    "the transformation from and to is straightforward by rewriting the constraints and we omit the detail .      for an ergodic , the constraints in the problem of maximizing the average reward , described by , can be rewritten in the way just as how is rewritten into and for the discounted - reward problem .",
    "the difference is that for the average - reward case , we let @xmath169 and replace @xmath170 with @xmath171 , for all @xmath13 , in and .",
    "an additional constraint for the average - reward case is that @xmath172 .",
    "hence , for an average - reward problem in an ergodic , the corresponding lp problem in is formulated as @xmath173 where @xmath174 is a row vector of @xmath43 ones , @xmath175 and the block - matrix @xmath10 has the same structure as of that in the discounted case with @xmath169 .",
    "we can compactly write the constraint in as @xmath176 where @xmath177 is a sparse constraint matrix similar in structure to the matrix @xmath10 in the discounted - reward case .",
    "we solve the problems in and by employing the block splitting algorithm based on in @xcite .",
    "we only present the algorithm for the discounted - reward case in .",
    "the extension to the average - reward case is straight - forward .",
    "first , we introduce new variables @xmath178 and let @xmath179 , where for a convex set @xmath180 , @xmath181 is a function defined by @xmath182 for @xmath183 , @xmath184 for @xmath185 .",
    "then , adding the term @xmath186 into the objective function enforces @xmath187 .",
    "let @xmath188 .",
    "the term @xmath189 enforces that @xmath152 is a non - negative vector .",
    "we rewrite the lp problem in as follows .",
    "@xmath190 with this formulation , we modify the block splitting algorithm in @xcite to solve in a parallel and distributed manner ( see the appendix for the details ) .",
    "the algorithm takes parameters @xmath191 , @xmath192 and @xmath193 : @xmath194 is a penalty parameter to ensure the constraints are satisfied , @xmath195 is a relative tolerance and @xmath196 is an absolute tolerance .",
    "the choice of @xmath192 and @xmath197 depends on the scale of variable values . in synthesis of s , @xmath192 and @xmath197",
    "may be chosen in the range of @xmath198 to @xmath199 .",
    "the algorithm is ensured to converge with any choice of @xmath191 and the value of @xmath191 may affect the convergence rate .",
    "we now extend the distributed control synthesis methods for s with discounted - reward and average - reward criteria to solve problem  [ prob ] in which quantitative temporal logic constraints are enforced .",
    "* preliminaries * given an formula @xmath76 as the system specification , one can always represent it by a @xmath200 where @xmath201 is a finite state set , @xmath202 is the alphabet , @xmath203 is the initial state , and @xmath204 the transition function .",
    "the acceptance condition @xmath205 is a set of tuples @xmath206 .",
    "the _ run _ for an infinite word @xmath207 is the infinite sequence of states @xmath208 where @xmath209 and @xmath210 for @xmath22 .",
    "a run @xmath211 is accepted in @xmath212 if there exists at least one pair @xmath213 such that @xmath214 and @xmath215 where @xmath216 is the set of states that appear infinitely often in @xmath191 .",
    "given an @xmath217 augmented with a set @xmath68 of atomic propositions and a labeling function @xmath218 , one can compute the product @xmath219 with the components defined as follows : @xmath220 is the set of states .",
    "@xmath10 is the set of actions .",
    "the initial probability distribution of states is @xmath221 $ ] such that given @xmath222 with @xmath223 , it is that @xmath224 .",
    "@xmath225 is the transition probability function . given @xmath226 , @xmath227 , @xmath228 and @xmath229 , let @xmath230 .",
    "the rabin acceptance condition is @xmath231 . by construction , a path @xmath232 satisfies the formula @xmath76 if and only if there exists @xmath233 , @xmath234 and @xmath235 . to maximize the probability of satisfying @xmath76 ,",
    "the first step is to compute the set of _ end components _ in @xmath236 , each of which is a pair @xmath237 where @xmath238 is non - empty and @xmath239 is a function such that for any @xmath240 , for any @xmath241 , @xmath242 and the induced directed graph @xmath243 is strongly connected . here",
    ", @xmath244 is an edge in the graph if there exists @xmath241 , @xmath245 .",
    "an end component @xmath246 is _ accepting _ if @xmath247 and @xmath248 for some @xmath249 .",
    "let the set of s in @xmath236 be @xmath250 and the set of _ accepting end states _",
    "be @xmath251 .",
    "once we enter some state @xmath252 , we can find an @xmath246 such that @xmath240 , and initiate the policy @xmath26 such that for some @xmath253 , states in @xmath254 will be visited a finite number of times and some state in @xmath255 will be visited infinitely often .",
    "* formulating the problem * an optimal policy that maximizes the probability of satisfying the specification also maximizes the probability of hitting the set of accepting end states @xmath256 .",
    "reference @xcite develops gpu - based parallel algorithms which significantly speed up the computation of end components for large s. after computing the set of s , we formulate the following problem to compute the optimal policy using the proposed decomposition and distributed synthesis method for discounted - reward cases .    given a product @xmath257 and the set @xmath256 of accepting end states , the modified product is @xmath258 where @xmath259 is the set of states obtained by grouping states in @xmath256 as a single state @xmath260 . for all @xmath14 , @xmath261 and @xmath262 .",
    "the initial distribution @xmath263 of states is defined as follows : for @xmath264 , @xmath265 , and @xmath266 . the reward function @xmath267 is defined such that for all @xmath268 that is not @xmath260 , @xmath269 where @xmath270 is the indicator function that outputs @xmath271 if and only if @xmath272 and @xmath171 otherwise . for any action @xmath273 , @xmath274 .    by definition of reward function ,",
    "the discounted reward with @xmath169 from state @xmath268 in the modified product @xmath275 is the probability of reaching a state in @xmath256 from @xmath268 under policy @xmath26 in the product @xmath236 .",
    "hence , with a decomposition of @xmath236 , the proposed distributed synthesis method for discounted - reward problems can be used to compute the policy that maximizes the probability of satisfying a given specification .",
    "* preliminaries * consider a temporal logic formula @xmath76 that can be expressed as a @xmath276 where @xmath277 are defined similar to a and @xmath278 is a set of _ accepting states_. a run @xmath191 is accepted in @xmath212 if and only if @xmath279 .",
    "given an @xmath280 and a @xmath281 , the _ product with bchi objective _ is @xmath282 where components @xmath283 are obtained similarly as in the product with rabin objective .",
    "the difference is that @xmath284 is the set of accepting states .",
    "a path @xmath232 satisfies the formula @xmath76 if and only if @xmath285 . * formulating the problem * for a product @xmath236 with bchi objective , we aim to synthesize a policy that maximizes the expected frequency of visiting an accepting state in the product @xmath286 .",
    "this type of objectives ensures some recurrent properties in the temporal logic formula are satisfied as frequently as possible .",
    "for example , one such objective can be requiring a mobile robot to maximize the frequency of visiting some critical regions .",
    "this type of objectives can be formulated as an average - reward problem in the following way : let the reward function @xmath287 be defined by @xmath288 . by definition of the reward function , the optimal policy with respect to the average - reward criterion is the one that maximizes the frequency of visiting a state in @xmath289 .",
    "if the product is ergodic , we can then solve the resulting average - reward problem by the distributed optimization algorithm with a decomposition of product @xmath236 .",
    "we demonstrate the method with three robot motion planning examples .",
    "all the experiments were run on a machine with intel xeon 4 ghz , 8-core cpu and 64 gb ram running linux .",
    "the distributed optimization algorithm is implemented in matlab .",
    "the decomposition and other operations are implemented in python .",
    "0.15   gridworld .",
    "the dash arrow represents that if the robot takes action ` n ' , there are non - zero probabilities for it to arrive at nw , n , and ne cells .",
    "( b ) a @xmath290 gridworld .",
    "a natural partition of state space using the walls gives rise to @xmath291 subsets of states .",
    "states in @xmath142 are enclosed using the squares .",
    ", title=\"fig : \" ]    0.32    [ fig : gridworld2 ]    figure  [ fig : singlestep ] shows a fraction of a gridworld .",
    "a robot moves in this gridworld with uncertainty in different terrains ( ` grass ' , ` sand ' , ` gravel ' and ` pavement ' ) . in each terrain and for robot s different action ( heading north ( ` n ' ) , south ( ` s ' ) , west ( ` w ' ) and east ( ` e ' ) ) , the probability of arriving at the correct cell is @xmath292 for pavement , @xmath293 for grass , @xmath294 for gravel and @xmath295 for sand . with a relatively small probability , the robot will arrive at the cell adjacent to the intended one .",
    "figure  [ fig : decompgw ] displays a @xmath296 gridworld",
    ". the grey area and the boundary are walls . if the robot runs into the wall , it will be bounce back to its original cell .",
    "the walls give rise to a natural partition of the state space , as demonstrated in this figure .",
    "if no explicit modular structure in the system can be found , one can compute a decomposition using the method in section  [ sec : decompmethod ] . in the following example",
    ", the wall pattern is the same as in the @xmath296 gridworld .",
    "we select a subset @xmath297 of cells as `` restricted area '' and a subset @xmath298 of cells as `` targets '' .",
    "the reward function is given : for @xmath13 , @xmath299 , @xmath300 counts for the amount of time the robot takes action @xmath47 .",
    "for @xmath301 , for all @xmath302 , @xmath303 . for @xmath304 , @xmath305 for all @xmath302",
    ". intuitively , this reward function will encourage the robot to reach the target with as fewer expected number of steps as possible , while avoiding running into a cell in the restricted area .",
    "we select @xmath306 .",
    "* case 1 : * to show the convergence and correctness of the distributed optimization algorithm , we first consider a @xmath307 gridworld example that can be solved directly with a centralized algorithm . since at each cell there are four actions for the robot to pick , the total number of variables is @xmath308 for the @xmath307 gridworld ( the wall cells are excluded from the set of states ) . in this gridworld , there is only @xmath271 target cell . the restricted area include @xmath309 cells .",
    "the resulting problem can be solved using cvx , a package for specifying and solving convex programs @xcite .",
    "the problem is solved in @xmath310 seconds , and the optimal objective value under the optimal policy given by cvx is @xmath311 .",
    "next , we solve the same problem by decomposing the state space of the along the walls into @xmath312 regions , each of which is a @xmath290 gridworld .",
    "this partition of state space yields @xmath313 states for each @xmath314 and @xmath315 states for @xmath142 .",
    "in which follows , we select @xmath316 to show the convergence of the distributed optimization algorithm .",
    "irrespective of the choices for @xmath191 , the average time for each iteration is about @xmath317 sec .",
    "the solution accuracy relative to cvx is summarized in table  [ tbl : summary ] .",
    "the ` rel .",
    "error in objval ' is the relative error in objective value attained , treating the cvx solution as the accurate one , and the infeasibility is the relative primal infeasibility of the solution , measured by @xmath318 figure  [ fig : reobjval100 ] shows the convergence of the optimization algorithm .    [",
    "cols=\"^,^,^,^,^,^,^ \" , ]     [ tbl : summary ]     gridworld with discounted reward , under @xmath319 . for clarity",
    ", we did not draw the relative error for the initial @xmath320 steps , which are comparatively large.,scaledwidth=45.0% ]    * case 2 : * for a @xmath321 gridworld , the centralized method in cvx fails to produce a solution for this large - scale problem .",
    "thus , we consider to solve it using the decomposition and distributed synthesis method . in this example , we partition the gridworld such that each region has @xmath322 cells , which results in @xmath323 regions .",
    "there are @xmath324 states in @xmath142 and about @xmath325 states in each @xmath92 , for @xmath326 . in this example",
    ", we randomly select @xmath323 cells to be the targets and @xmath323 cells to be the restricted areas . by choosing @xmath319 , @xmath327 , @xmath328 ,",
    "the optimal policy is solved within @xmath329 seconds and it takes about @xmath330 seconds for one iteration .",
    "the total number of iterations is @xmath331 . under the ( approximately)-optimal policy obtained by distributed optimization",
    ", the objective value is @xmath332 .",
    "the relative primal infeasibility of the solution is @xmath333 .",
    "figure  [ fig : objval1000 ] shows the convergence of distributed optimization algorithm",
    ".    0.45   gridworld ( the initial @xmath334 steps are omitted ) .",
    "( b ) objective value vesus iterations in @xmath335 gridworld with a bchi objective .",
    "here we only show the first @xmath336 iterations as the objective value converges to the optimal one after @xmath336 steps .",
    ", title=\"fig:\",scaledwidth=100.0% ]    0.45   gridworld ( the initial @xmath334 steps are omitted ) .",
    "( b ) objective value vesus iterations in @xmath335 gridworld with a bchi objective . here",
    "we only show the first @xmath336 iterations as the objective value converges to the optimal one after @xmath336 steps .",
    ", title=\"fig:\",scaledwidth=100.0% ]      we consider a @xmath335 gridworld with no obstacles and @xmath337 critical regions labeled `` @xmath338 '' , `` @xmath339 '' , `` @xmath340 '' and `` @xmath341 '' .",
    "the system is given a temporal logic specification @xmath342 .",
    "that is , the robot has to always eventually visit region @xmath338 and then @xmath339 , and also always eventually visit region @xmath340 and then @xmath341 .",
    "the number of states in the corresponding is @xmath343 after trimming the unreachable states , due to the fact that the robot can not be at two cells simultaneously .",
    "the quantitative objective is to maximize the frequency of visiting all four regions ( an accepting state in the ) .",
    "the formulated is ergodic and therefore our method for average - reward problems applies .    for an average - reward case",
    ", we need to satisfy the constraint @xmath344 in .",
    "this constraint leads to slow convergence and policies with large infeasibility measures in distributed optimization . to handle this issue",
    ", we approximate average reward with discounted reward @xcite : for ergodic s , the discounted accumulated reward , scaled by @xmath345 , is approximately the average reward .",
    "further , if @xmath346 is large compared to the mixing time @xcite of the markov chain , then the policy that optimizes the discounted accumulated reward with the discounting factor @xmath347 can achieve an approximately optimal average reward .",
    "given @xmath319 , @xmath348 and @xmath349 , @xmath350 , the distributed synthesis algorithm terminates in @xmath351 iteration steps and the optimal discounted reward is @xmath352 .",
    "scaling by @xmath353 , we obtain the average reward @xmath354 , which is the approximately optimal value for this average reward under the obtained policy .",
    "the convergence result is shown in figure  [ fig : reobjval50 ] and the infeasibility measure of the obtained solution is @xmath355 .",
    "for solving large markov decision process models of stochastic systems with temporal logic specifications , we developed a decomposition algorithm and a distributed synthesis method .",
    "this decomposition exploits the modularity in the system structure and deals with sub - problems of smaller sizes .",
    "we employed the block splitting algorithm in distributed optimization based on the alternating direction method of multipliers to cope with the difficulty of combining the solutions of sub - problems into a solution to the original problem .",
    "moreover , the formal decomposition - based distributed control synthesis framework established in this paper facilitates the application of other distributed and parallel large - scale optimization algorithms @xcite to further improve the rate of convergence and the feasibility of solutions for control synthesis in large s. in the future , we will develop an interface to prism toolbox @xcite with an implementation of the proposed decomposition and distributed synthesis algorithms .",
    "at the @xmath54-th iteration , for @xmath356 , @xmath357 where @xmath358 denotes the projection to the nonnegative orthant , @xmath359 denotes projection onto @xmath360 .",
    "@xmath361 is the elementwise averaging ; , @xmath362 , in the elementwise averaging , these @xmath363 will not be included . ] and @xmath364 is the exchange operator , defined as below .",
    "@xmath365 is given by @xmath366 and @xmath367 .",
    "the variables can be initialized to @xmath171 at @xmath368 .",
    "note that the computation in each iteration can be parallelized .",
    "the iteration terminates when the stopping criterion for the block splitting algorithm is met ( see @xcite for more details ) .",
    "the solution can be obtained @xmath369 .",
    "s.  thibaux , c.  gretton , j.  k. slaney , d.  price , f.  kabanza , and others , `` decision - theoretic planning with non - markovian rewards . '' _ journal of artificial intelligence research _ , vol .  25 , pp .",
    "1774 , 2006 .",
    "t.  dean and s .- h .",
    "lin , `` decomposition techniques for planning in stochastic domains , '' in _ proceedings of the 14th international joint conference on artificial intelligence - volume 2_.1em plus 0.5em minus 0.4em morgan kaufmann publishers inc .",
    ", 1995 , pp . 11211127 .",
    "m.  kwiatkowska , g.  norman , and d.  parker , `` prism 4.0 : verification of probabilistic real - time systems , '' in _ proceedings of international conference on computer aided verification _ , ser .",
    "lncs , g.  gopalakrishnan and s.  qadeer , eds .",
    "6806.1em plus 0.5em minus 0.4emspringer , 2011 , pp",
    ". 585591 .",
    "x.  c. ding , s.  l. smith , c.  belta , and d.  rus , `` mdp optimal control under temporal logic constraints , '' in _ ieee conference on decision and control and european control conference _ , 2011 ,",
    ". 532538 .",
    "m.  lahijanian , s.  andersson , and c.  belta , `` temporal logic motion planning and control with probabilistic satisfaction guarantees , '' _ ieee transactions on robotics _",
    "28 , no .  2 ,",
    "pp . 396409 , april 2012 .",
    "s.  boyd , n.  parikh , e.  chu , b.  peleato , and j.  eckstein , `` distributed optimization and statistical learning via the alternating direction method of multipliers , '' _ foundations and trends in machine learning _ ,",
    "vol .  3 , no .  1 , pp . 1122 , 2011 .",
    "t.  brzdil , v.  brozek , k.  chatterjee , v.  forejt , and a.  kucera , `` two views on multiple mean - payoff objectives in markov decision processes , '' in _ annual ieee symposium on logic in computer science _ , 2011 ,",
    ". 3342 .",
    "c.  baier , m.  gr  er , m.  leucker , b.  bollig , and f.  ciesinski , `` controller synthesis for probabilistic systems ( extended abstract ) , '' in _ exploring new frontiers of theoretical informatics _ , ser .",
    "ifip international federation for information processing , j .- j .",
    "levy , e.  mayr , and j.  mitchell , eds.1em plus 0.5em minus 0.4emspringer us , 2004 , vol . 155 , pp . 493506 .",
    "a.  wijs , j .-",
    "katoen , and d.  bonaki , `` , '' in _ _ , ser .",
    "lecture notes in computer science , a.  biere and r.  bloem , eds.1em plus 0.5em minus 0.4emspringer international publishing , 2014 , vol . 8559 , pp .",
    "310326 .",
    "g.  scutari , f.  facchinei , l.  lampariello , and p.  song , `` parallel and distributed methods for nonconvex optimization , '' in _ ieee international conference on acoustics , speech and signal processing _ , may 2014 , pp . 840844 ."
  ],
  "abstract_text": [
    "<S> optimal control synthesis in stochastic systems with respect to quantitative temporal logic constraints can be formulated as linear programming problems . however , centralized synthesis algorithms do not scale to many practical systems . to tackle this issue </S>",
    "<S> , we propose a decomposition - based distributed synthesis algorithm . by decomposing a large - scale stochastic system modeled as a markov decision process into a collection of interacting sub - systems , </S>",
    "<S> the original control problem is formulated as a linear programming problem with a sparse constraint matrix , which can be solved through distributed optimization methods . </S>",
    "<S> additionally , we propose a decomposition algorithm which automatically exploits , if exists , the modular structure in a given large - scale system . </S>",
    "<S> we illustrate the proposed methods through robotic motion planning examples . </S>"
  ]
}