{
  "article_text": [
    "the basic questions faced by extreme value analysis consist in estimating the probability of exceeding a threshold that is larger than the sample maximum and estimating a quantile of an order that is larger than 1 minus the reciprocal of the sample size . in words , they consist in making inferences on regions that lie outside the support of the empirical distribution . in order to face these challenges in a sensible framework , extreme value theory ( ) assumes that the sampling distribution @xmath0 satisfies a regularity condition .",
    "indeed , in heavy - tail analysis , the tail function @xmath1 is supposed to be regularly varying that is , @xmath2 exists for all @xmath3 .",
    "this amounts to assume the existence of some @xmath4 such that the limit is @xmath5 for all @xmath6 .",
    "in other words , if we define the _ excess distribution above the threshold @xmath7 _ by its survival function : @xmath8 for @xmath9 , then @xmath10 is regularly varying if and only if @xmath11 converges weakly towards a pareto distribution .",
    "the sampling distribution @xmath0 is then said to belong to the _ max - domain of attraction _ of a frchet distribution with index @xmath12 ( abbreviated in @xmath13 ) and @xmath14 is called the _ extreme value index_.    the main impediment to large exceedance and large quantile estimation problems alluded above turns out to be the estimation of the extreme value index . since the inception of extreme value analysis , many estimators have been defined , analysed and implemented into software .",
    "@xcite introduced a simple , yet remarkable , collection of estimators : for @xmath15 , @xmath16 where @xmath17 are the _ order statistics _ of the sample @xmath18 ( the non - increasing rearrangement of the sample ) .",
    "an integer sequence @xmath19 is said to be _ intermediate _ if @xmath20 while @xmath21 .",
    "it is well known that @xmath0 belongs to @xmath22 for some @xmath4 if and only if , for all intermediate sequences @xmath19 , @xmath23 converges in probability towards @xmath14 @xcite . under mildly stronger conditions",
    ", it can be shown that @xmath24 is asymptotically gaussian with variance @xmath25 this suggests that , in order to minimise the quadratic risk @xmath26 $ ] or the absolute risk @xmath27 , an appropriate choice for @xmath28 has to be made . if @xmath28 is too large , the hill estimator @xmath23 suffers a large bias and , if @xmath28 is too small , @xmath23 suffers erratic fluctuations .",
    "as all estimators of the extreme value index face this dilemma ( see * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* and references therein ) , during the last three decades , a variety of data - driven selection methods for @xmath28 has been proposed in the literature ( see @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite to name a few ) . a related but distinct problem is considered by @xcite : constructing uniform and adaptive confidence intervals for the extreme value index .",
    "the rationale for investigating adaptive hill estimation stems from computational simplicity and variance optimality of properly chosen hill estimators @xcite .",
    "the hallmark of our approach is to combine techniques of with tools from concentration of measure theory . as up to our knowledge",
    ", the impact of the concentration of measure phenomenon in has received little attention , we comment and motivate the use of concentration arguments .",
    "talagrand s concentration phenomenon for products of exponential distributions is one instance of a general phenomenon : concentration of measure in product spaces @xcite .",
    "the phenomenon may be summarised in a simple quote : functions of independent random variables that do not depend too much on any of them are almost constant @xcite .    the concentration approach helps to split the investigation in two steps : the first step consists in bounding the fluctuations of the random variable under concern around its median or its expectation , while the second step focuses on the expectation .",
    "this approach has seriously simplified the investigation of suprema of empirical processes and made the life of many statisticians easier @xcite .",
    "to point out the potential uses of concentration inequalities in the field of is one purpose of this paper . in statistics",
    ", concentration inequalities have proved very useful when dealing with estimator selection and adaptivity issues : sharp , non - asymptotic tail bounds can be combined with simple union bounds in order to obtain uniform guarantees of the risk of collection of estimators . using concentration inequalities to investigate adaptive choice of the number of order statistics to be used in tail index estimation is a natural thing to do .    in the present",
    "setting , tail index estimators are functions of independent random variables .",
    "talagrand s quote raises a first question : in which way are these tail functionals smooth functions of independent random variables ?",
    "we do not attempt here to revisit the asymptotic approach described by @xcite which equates smoothness with hadamard differentiability .",
    "our approach is non - asymptotic and our conception of smoothness somewhat circular , smooth functionals are these functionals for which we can obtain good concentration inequalities .    in this paper , we combine talagrand s concentration inequality for smooth functions of independent exponentially distributed random variables ( theorem [ bernstein : expo ] ) with three traditional tools of : the quantile transform , karamata s representation for slowly varying functions , and rnyi s characterisation of the joint distribution of order statistics of exponential samples .",
    "this allows us to establish concentration inequalities for the hill process @xmath29 ( theorem [ prp : hill : concentration ] ) in section [ sec : conc - ineq - hill ] .    in section [ sec",
    ": adapt - hill - estim ] , we build on these concentration inequalities to analyse the performance of a variant of lepki s rule defined in sections [ sec : lepsk - meth - adapt ] and [ sec : adapt - hill - estim ] : theorem [ thm : adapt - hill - estim ] describes an oracle inequality and corollary [ cor : adapt - hill - estim-2ndrv ] assesses the performance of this simple selection rule under a mild assumption on the so - called von mises function .",
    "note that the condition is less demanding than the regular variation condition on the von mises function that has often been assumed when looking for adaptive tail index estimators ( notable exceptions being @xcite and @xcite ) .",
    "it reveals that the performance of hill estimators selected by lepski s method matches known lower bounds ( see section [ sec : lower - bound ] ) that is , they suffer the loss of efficiency which is inherent to this problem , but not more .",
    "proofs are given in section [ sec : proofs ] .",
    "finally , in section [ sec : simulations ] , we examine the performance of this adaptive hill estimator for finite sample sizes using monte - carlo simulations .",
    "the quantile function @xmath30 is the generalised inverse of the distribution function @xmath0 .",
    "the _ tail quantile function _ of @xmath0 is a non - decreasing function defined on @xmath31 by @xmath32 , or by @xmath33    in this text , we use a variation of the quantile transform that fits : if @xmath34 is exponentially distributed , then @xmath35 is distributed according to @xmath0 .",
    "moreover , by the same argument , the order statistics @xmath17 are distributed as a monotone transformation of the order statistics @xmath36 of a sample of @xmath37 independent standard exponential random variables .",
    "@xmath38    thanks to rnyi s representation for order statistics of exponential samples , agreeing on @xmath39 , the rescaled exponential spacings @xmath40 are independent and exponentially distributed .    the quantile transform and rnyi s representation are complemented by karamata s representation for slowly varying functions .",
    "recall that a function @xmath41 is _ slowly varying at infinity _ if for all @xmath3 , @xmath42 .",
    "the von mises condition specifies the form of karamata s representation ( see * ? ? ?",
    "* corollary 2.1 ) of the slowly varying component @xmath43 of @xmath44 .",
    "[ dfn : vmises : cond ] a distribution function @xmath0 belonging to @xmath45 , satisfies the von mises condition if there exist a constant @xmath46 , a constant @xmath47 and a measurable function @xmath48 on @xmath31 such that , for @xmath49 @xmath50 with @xmath51 .",
    "the function @xmath48 is called the _",
    "von mises function_.    in the sequel , we assume that the sampling distribution @xmath52 , @xmath4 , satisfies the von mises condition with @xmath53 , von mises function @xmath48 and define the non - increasing function @xmath54 from @xmath55 to @xmath56 by @xmath57 . in the text , we assume that @xmath58 .    combining the quantile transformation , rnyi s and karamata s representations ,",
    "it is straightforward that , under the von mises condition , the sequence of hill estimators is distributed as a function of the largest order statistics of a standard exponential sample .    [ hill : rep ] the vector of hill estimators @xmath59 is distributed as the random vector @xmath60 where @xmath61 are independent standard exponential random variables while , for @xmath62 , @xmath63 is distributed like the @xmath64th order statistic of an @xmath37-sample of the exponential distribution .    for a fixed @xmath65 , a second distributional representation is available , @xmath66 where @xmath67 and @xmath68 are defined as in proposition [ hill : rep ] .",
    "this second , simpler , distributional representation stresses the fact that , conditionally on @xmath68 , @xmath69 is distributed as a mixture of sums of independent random variables approximately distributed as exponential random variables with scale @xmath14 .",
    "this distributional identity suggests that the variance of @xmath70 scales like @xmath71 , an intuition that is corroborated by analysis , see section  [ sec : conc - ineq - hill ] .",
    "the bias of @xmath70 is connected with the von mises function @xmath48 by the next formula @xmath72=   { \\ensuremath{\\mathbb{e}}}\\left[\\int_1^\\infty     \\frac{\\eta\\left({\\mathrm{e}}^{y_{(k+1 ) } } v\\right)}{v^2 } \\mathrm{d}v\\right]\\ , .\\ ] ]    henceforth , let @xmath73 be defined on @xmath31 by @xmath74 the quantity @xmath75 is the bias of the hill estimator @xmath70 given @xmath76 .",
    "the second expression for @xmath73 shows that @xmath73 is differentiable with respect to @xmath77 ( even though @xmath48 might be nowhere differentiable ) and that @xmath78 the von mises function governs both the rate of convergence of @xmath79 towards @xmath80 , or equivalently of @xmath81 towards @xmath5 , and the rate of convergence of @xmath82 towards @xmath83 .",
    "the difficulty in extreme value index estimation stems from the fact that , for any collection of estimators , for any intermediate sequence @xmath19 , and for any @xmath4 , there is a distribution function @xmath84 such that the bias @xmath85 decays at an arbitrarily slow rate .",
    "this has led authors to put conditions on the rate of convergence of @xmath79 towards @xmath80 as @xmath77 tends to infinity while @xmath3 , or equivalently , on the rate of convergence of @xmath81 towards @xmath5 .",
    "these conditions have then to be translated into conditions on the rate of decay of the bias of estimators . as we focus on hill estimators ,",
    "the connection between the rate of convergence of @xmath79 towards @xmath80 and the rate of decay of the bias is transparent and well - understood @xcite : the theory of @xmath86-regular variation provides an adequate setting for describing both rates of convergence @xcite . in words , if a positive function @xmath87 defined over @xmath55 is such that , for some @xmath88 , for all @xmath89 , @xmath90 } g(tx)/g(t ) < \\infty$ ] , @xmath87 is said to have _ bounded increase_. if @xmath87 has bounded increase , the class @xmath91 is the class of measurable functions @xmath92 on some interval @xmath93 , such that as @xmath94 @xmath95 for all @xmath96    for example , the analysis carried out by @xcite rests on the condition that , if @xmath84 , for some @xmath97 , @xmath98 and @xmath99 , @xmath100 this condition implies that @xmath101 with @xmath102 @xcite .",
    "thus , under the von mises condition , condition implies that the function @xmath103 belongs to @xmath91 with @xmath104 moreover , the abelian and tauberian theorems from @xcite assert that @xmath105 if and only if @xmath106 for any intermediate sequence @xmath19 .    in this text , we are ready to assume that if @xmath84 and satisfies the von mises condition , then , for some @xmath97 and @xmath99 and @xmath107 , @xmath108 this condition is arguably more stringent than ( [ eq : kimcarpcond ] ) .",
    "however , we do not want to assume that @xmath48 satisfies a regular variation property .",
    "this would imply that @xmath109 is @xmath110-regularly varying .    indeed , assuming as in @xcite and several subsequent papers that @xmath0 satisfies @xmath111 where @xmath112 are constants and @xmath113 , or equivalently,@xcite that @xmath114 satisfies @xmath115 ( which entails that @xmath48 is regularly varying ) makes the problem of extreme value index estimation easier ( but not easy ) .",
    "these conditions entail that , for any intermediate sequence @xmath19 , the ratio @xmath116|/(n / k_n)^\\rho$ ] converges towards a finite limit as @xmath37 tends to @xmath117 @xcite .",
    "this makes the estimation of the second - order parameter a very natural intermediate objective ( see for example * ? ? ?",
    "the necessity of developing data - driven index selection methods is illustrated in figure [ fig : riskcomp - student ] , which displays the estimated standardised root mean squared error ( rmse ) of hill estimators @xmath118^{1/2}\\ ] ] as a function of @xmath119 for four related sampling distributions which all satisfy the second - order condition with different values of the second - order parameters .",
    "for samples of size @xmath120 from student s distributions with different degrees of freedom @xmath121 .",
    "all four distributions satisfy condition with @xmath122 .",
    "the increasing parts of the lines reflect the values of @xmath110 .",
    "rmse is estimated by averaging over @xmath123 monte - carlo simulations . ]    under this second - order condition , @xcite proved that the asymptotic mean squared error of the hill estimator is minimal for sequences @xmath124 satisfying @xmath125 with @xmath126 . since @xmath97 , @xmath98 and the second - order parameter @xmath99",
    "are usually unknown , many authors have been interested in the construction of data - driven selection procedures for @xmath28 under conditions such as .",
    "a great deal of ingenuity has been dedicated to the estimation of the second - order parameters and to the use of such estimates when estimating first order parameters .",
    "as we do not want to assume a second - order condition such as condition , we resort to lepski s method which is a general attempt to balance bias and variance .    since its introduction @xcite , this general method for model selection has been proved to achieve adaptivity and to provide one with oracle inequalities in a variety of inferential contexts ranging from density estimation to inverse problems and classification @xcite .",
    "very readable introductions to lepski s method and its connections with penalised contrast methods can be found in @xcite . in",
    ", we are aware of three papers that explicitly rely on this methodology : @xcite , @xcite and @xcite .",
    "the selection rule analysed in the present paper ( see section [ sec : adapt - hill - estim ] for a precise definition ) is a variant of the preliminary selection rule introduced in @xcite @xmath127 where @xmath128 is a sequence of thresholds such that @xmath129 and @xmath130 , and @xmath131 is the hill estimator computed from the @xmath132 largest order statistics .",
    "the definition of this  stopping time \" is motivated by lemma 1 from @xcite which asserts that , under the von mises condition , @xmath133 | = o_p \\left ( \\sqrt{\\ln \\ln n } \\right ) { \\enspace .}\\ ] ] in words , this selection rule almost picks out the largest index @xmath119 such that , for all @xmath64 smaller than @xmath119 , @xmath70 differs from @xmath131 by a quantity that is not much larger than the typical fluctuations of @xmath131 .",
    "this index selection rule can be performed graphically by interpreting an alternative hill plot as shown on figure [ fig : cauchy - lepski ] ( see * ? ? ?",
    "* ; * ? ? ?",
    "* for a discussion on the merits of alt - hill plots ) .     computed on a pseudo - random sample of size @xmath134 from student distribution with 1 degree of freedom ( cauchy distribution ) .",
    "hill estimators are computed from the positive order statistics .",
    "the grey ribbon around the plain line provides a graphic illustration of lepski s method . for a given value of @xmath64 ,",
    "the width of the ribbon is @xmath135 .",
    "a point @xmath136 on the plain line corresponds to an eligible index if the horizontal segment between this point and the vertical axis lies inside the ribbon that is , if for all @xmath137 , @xmath138 .",
    "if @xmath139 were replaced by an appropriate quantile of the gaussian distribution , the grey ribbon would just represent the confidence tube that is usually added on hill plots .",
    "the triangle represents the selected index with @xmath140 .",
    "the cross represents the oracle index estimated from monte - carlo simulations , see table [ tab : ratios:1 ] . ]",
    "the goal of @xcite is not to investigate the performance of the preliminary selection rule defined in display but to design a selection rule @xmath141 , based on @xmath142 , that would asymptotically mimic the optimal selection rule @xmath143 under second - order conditions .",
    "our goal , as in @xcite , is to derive non - asymptotic risk bounds without making a second - order assumption . in both papers",
    ", the rationale for working with some special collection of estimators seems to be the ability to derive non - asymptotic deviation inequalities for @xmath70 either from exponential inequalities for log - likelihood ratio statistics or from simple binomial tail inequalities such as bernstein s inequality ( see * ? ? ?",
    "* section 2.8 ) .    in models satisfying condition , the estimators from @xcite achieve the optimal rate up to a @xmath144 factor .",
    "@xcite prove that the risk of their data - driven estimator decays at the optimal rate @xmath145 up to a factor @xmath146 in models satisfying condition .",
    "we aim at achieving optimal risk bounds under condition using a simple estimation method requiring almost no calibration effort and based on mainstream extreme value index estimators . before describing the keystone of our approach in section [ sec : talagr - conc - phen ] , we recall the recent lower risk bound for adaptive extreme value index estimation",
    ".      one of the key results in @xcite is a lower bound on the accuracy of adaptive tail index estimation .",
    "this lower bound reveals that , just as for estimating a density at a point @xcite , or point estimation in sobolev spaces @xcite , as far as tail index estimation is concerned , adaptivity has a price . using fano s lemma , and a bayesian game that extends cleanly in frameworks of @xcite and @xcite , @xcite were able to prove the next minimax lower bound .",
    "[ thm - kc - lower - bound ] let @xmath147 and @xmath148 $ ] .",
    "then , for any tail index estimator @xmath149 and any sample size @xmath37 such that @xmath150 , there exists a probability distribution @xmath151 such that    a.   @xmath152 with @xmath4 , b.   @xmath151 meets the von mises condition with von mises function @xmath48 satisfying @xmath153 for some @xmath154 , c.   @xmath155 + and @xmath156 \\geq \\frac{c_\\rho}{4(1 + 2{\\mathrm{e } } ) }   \\left(\\frac{v\\ln\\ln n}{n}\\right)^{|\\rho|/(1 + 2|\\rho| ) } \\ , , \\ ] ] with @xmath157 .",
    "using birg s lemma instead of fano s lemma , we provide a simpler , shorter proof of this theorem ( see appendix [ proof : lower : bound ] )",
    ".    the lower rate of convergence provided by theorem [ thm - kc - lower - bound ] is another incentive to revisit the preliminary tail index estimator from @xcite .",
    "however , instead of using a sequence @xmath128 of order larger than @xmath158 in order to calibrate pairwise tests and ultimately to design estimators of the second - order parameter ( if there are any ) , it is worth investigating a minimal sequence where @xmath139 is of order @xmath159 , and check whether the corresponding adaptive estimator achieves the carpentier - kim lower bound ( theorem [ thm - kc - lower - bound ] ) .    in this paper",
    ", we focus on @xmath139 of the order @xmath158 . the rationale for imposing @xmath139 of the order @xmath158",
    "can be understood by the fact that , even if the sampling distribution is a pure pareto distribution with shape parameter @xmath14 ( @xmath160 for @xmath161 ) , if @xmath162 the preliminary selection rule will , with high probability , select a small value of @xmath119 and thus pick out a suboptimal estimator .",
    "this can be justified using results from @xcite ( see appendix [ sec : calibr - prel - select ] for details ) .",
    "such an endeavour requires sharp probabilistic tools .",
    "they are the topic of the next section .      deriving authentic concentration inequalities for hill estimators",
    "is not straightforward .",
    "fortunately , the construction of such inequalities turns out to be possible thanks to general functional inequalities that hold for functions of independent exponentially distributed random variables .",
    "we recall these inequalities ( proposition [ poincare : expo ] and theorem [ bernstein : expo ] ) which have been largely overlooked in statistics .",
    "a thorough and readable presentation of these inequalities can be found in @xcite .",
    "we start by the easiest result , a variance bound that pertains to the family of poincar inequalities .",
    "[ poincare : expo ] if @xmath87 is a differentiable function over @xmath163 and @xmath164 where @xmath165 are independent standard exponential random variables , then @xmath166 \\ , .\\ ] ]    the constant @xmath167 can not be improved .",
    "the next corollary is stated in order to point the relevance of this poincar inequality to the analysis of general order statistics and their functionals .",
    "recall that the _ hazard rate _ of an absolutely continuous probability distribution with distribution @xmath0 is : @xmath168 where @xmath92 and @xmath1 are the density and the survival function associated with @xmath0 , respectively .",
    "[ var : os ] assume the distribution of @xmath169 has a positive density , then the @xmath119th order statistic @xmath170 satisfies @xmath171 \\le \\frac{c}{k } \\left(1 + \\frac{1}{k}\\right){\\ensuremath{\\mathbb{e}}}\\left [ \\frac{1}{h(x_{(k)})^2 } \\right]\\ ] ] where @xmath172 can be chosen as @xmath167 .    by smirnov s lemma @xcite , @xmath172 can not be smaller than @xmath173 .",
    "if the distribution of @xmath169 has a non - decreasing hazard rate , the factor of @xmath167 can be improved into a factor @xmath174 @xcite .",
    "@xcite show that smooth functions of independent exponential random variables satisfy bernstein type concentration inequalities .",
    "the next result is extracted from the derivation of talagrand s concentration phenomenon for product of exponential random variables in @xcite .",
    "the definition of sub - gamma random variables will be used in the formulation of the theorem and in many arguments .",
    "[ dfn : sub - gamma ] a real - valued centred random variable @xmath169 is said to be _ sub - gamma _ on the right tail with variance factor @xmath175 and scale parameter @xmath176 if @xmath177 we denote the collection of such random variables by @xmath178 .",
    "similarly , @xmath169 is said to be sub - gamma on the left tail with variance factor @xmath175 and scale parameter @xmath176 if @xmath179 is sub - gamma on the right tail with variance factor @xmath175 and tail parameter @xmath176 .",
    "we denote the collection of such random variables by @xmath180 and @xmath181 by @xmath182 .    if @xmath183 , then for all @xmath184 , with probability larger than @xmath185 @xmath186    the entropy of a non - negative random variable @xmath169 is defined by @xmath187= { \\mathbb{e}}[x \\ln x]-{\\mathbb{e}}x \\ln{\\mathbb{e}}x$ ] .",
    "[ bernstein : expo ] assume that @xmath87 is a differentiable function on @xmath163 with @xmath188 .",
    "let @xmath164 where @xmath61 are @xmath37 independent standard exponential random variables and @xmath189 .",
    "then , for all @xmath190 such that @xmath191 , @xmath192   \\leq \\frac{2\\lambda^2}{1-c } { \\ensuremath{\\mathbb{e}}}\\left [ { \\mathrm{e}}^{\\lambda ( z-{\\ensuremath{\\mathbb{e}}}z ) } \\| \\nabla g\\|^2\\right ] \\ , .\\ ] ]    let @xmath175 be the essential supremum of @xmath193 , then @xmath194 is sub - gamma on both tails with variance factor @xmath195 and scale factor @xmath196 .",
    "again , we illustrate the relevance of these versatile tools on the analysis of general order statistics .",
    "this general theorem implies that if the sampling distribution has non - decreasing hazard rate , then the order statistics @xmath170 satisfy bernstein type inequalities ( see * ? ? ?",
    "* section 2.8 ) with variance factor @xmath197 $ ] ( the poincar estimate of variance ) and scale parameter @xmath198 ) .",
    "starting back from the efron - stein - steele inequality , the authors derived a somewhat sharper inequality @xcite .",
    "[ prp : hazard : conc : ineg ] assume the distribution function @xmath0 has non - decreasing hazard rate @xmath199 that is , @xmath200 is @xmath201 and concave .",
    "let @xmath202 be distributed as the @xmath119th order statistic of a sample distributed according to @xmath0 .",
    "then , @xmath194 is sub - gamma on both tails with variance factor @xmath203 $ ] and scale factor @xmath204 .",
    "this corollary describes in which way central , intermediate and extreme order statistics can be portrayed as smooth functions of independent exponential random variables .",
    "this possibility should not be taken for granted as it is non trivial to capture in a non - asymptotic way the tail behaviour of maxima of independent gaussians @xcite . in the next section , we show in which way the hill estimator can fit into this picture .",
    "in this section , the sampling distribution @xmath0 is assumed to belong to @xmath22 with @xmath4 and to satisfy the von mises condition ( definition [ dfn : vmises : cond ] ) with bounded von mises function @xmath48 .",
    "it is well known that , under the von mises condition , if @xmath19 is an intermediate sequence , the sequence @xmath205 converges in distribution towards @xmath206 , suggesting that the variance of @xmath23 scales like @xmath207 ( see * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "proposition [ prop : bound - vari - hill ] provides us with handy non - asymptotic bounds on @xmath208   - \\gamma^2/k $ ] using the von mises function .",
    "[ prop : bound - vari - hill ] let @xmath70 be the hill estimator computed from the @xmath209 largest order statistics of an @xmath37-sample from @xmath0 .",
    "then , @xmath210 \\leq   { \\operatorname{var } } [ \\widehat{\\gamma}(k ) ]   - \\frac{\\gamma^2}{k }   \\leq    \\frac{2\\gamma}{k } { \\ensuremath{\\mathbb{e}}}\\left [ \\overline{\\eta}\\left({\\mathrm{e}}^{y_{(k+1 ) } } \\right)\\right ]   + \\frac{5}{k}{\\ensuremath{\\mathbb{e}}}\\left [ \\overline{\\eta}\\left({\\mathrm{e}}^{y_{(k+1 ) } } \\right)^2\\right ]   \\ , .\\ ] ]    the next abelian result might help in appreciating these variance bounds .",
    "[ sec : abel - vari - hill ] assuming that @xmath48 is @xmath110-regularly varying with @xmath99 , then , for any intermediate sequence @xmath19 , @xmath211    we may now move to genuine concentration inequalities for the hill estimator .",
    "the exponential representation suggests that the rescaled hill estimator @xmath212 should be approximately distributed according to a @xmath213 distribution where @xmath119 is the shape parameter and @xmath14 the scale parameter .",
    "therefore , we expect the hill estimators to satisfy bernstein type concentration inequalities that is , to be sub - gamma on both tails with variance factors connected to the tail index @xmath14 and to the von mises function .",
    "representation actually suggests more .",
    "following @xcite , we actually expect the sequence @xmath214 to behave like normalized partial sums of independent square integrable random variables that is , we believe @xmath215 to scale like @xmath158 and to be sub - gamma on both tails ( see appendix [ sec : calibr - prel - select ] ) . the purpose of this section is to meet these expectations in a non - asymptotic way .",
    "proofs use the markov property of order statistics : conditionally on the @xmath216th order statistic , the first largest @xmath217 order statistics are distributed as the order statistics of a sample of size @xmath217 of the excess distribution .",
    "they consist of appropriate invocations of talagrand s concentration inequality ( theorem [ bernstein : expo ] ) .",
    "however , this theorem generally requires a uniform bound on the gradient of the relevant function .",
    "when hill estimators are analysed as functions of independent exponential random variables , the partial derivatives depend on the points at which the von mises function is evaluated . in order to get interesting bounds",
    ", it is worth conditioning on an intermediate order statistic .    throughout this subsection ,",
    "let @xmath218 be an integer larger than @xmath219 and @xmath217 an integer not larger than @xmath37 .",
    "we denote @xmath220 , @xmath37 independent standard exponential random variables and we work on the probability space where all @xmath221 are defined , and therefore consider the hill estimators defined by representation . as we use the exponential representation of order statistics , besides hill estimators , the random variables that appear in the main statements are order statistics of exponential samples . as before , @xmath222 will denote the @xmath119th order statistic of a standard exponential sample of size @xmath37 ( we agree on @xmath39 ) .",
    "the first theorem provides an exponential refinement of the variance bound stated in proposition  [ prop : bound - vari - hill ] .",
    "however , as announced , there is a price to pay : statements hold conditionally on some order statistic .",
    "this is not an impediment to analyse lepski s rule using this theorem . indeed , when analysing lepki s rule it is sufficient to control the hill process @xmath223)\\right)_i$ ] for indices @xmath64 ranging between @xmath224 ( that should not be smaller than @xmath144 ) and some upper bound @xmath28 that achieves a certain balance between bias and standard deviation ( the bias of @xmath23 should be of order @xmath139 times the standard deviation , that is approximately @xmath225 where @xmath226 ) .",
    "the second clause of next theorem is the cornerstone in the derivation of the risk bounds presented in the next section .    in the sequel ,",
    "let @xmath227 where @xmath228 may be chosen not larger than @xmath167 and @xmath229 not larger than @xmath230 .",
    "[ prp : hill : concentration ] let @xmath231 be a shorthand for @xmath232 . for some @xmath119 such that @xmath233 where @xmath234 , let @xmath235 \\right|\\ , .\\ ] ] then , conditionally on @xmath231 ,    a.   for @xmath236 , @xmath237\\right ) \\in   \\gamma_\\pm\\left(4   \\left(\\gamma + 3\\overline{\\eta}(t ) \\right)^2,\\left(\\gamma+2\\overline{\\eta}(t)\\right)\\right ) \\ , .\\ ] ] b.   let @xmath238 be such that @xmath239 where @xmath240 with @xmath241 .",
    "assume that @xmath242 , then @xmath243 and @xmath244 \\leq \\gamma \\xi_n \\left(1 + \\frac{3r_n}{\\sqrt{j } }   \\right)\\ , .\\ ] ]    if @xmath0 is a pure pareto distribution with shape parameter @xmath4 , then @xmath245 is distributed according to a gamma distribution with shape parameter @xmath119 and scale parameter @xmath173 .",
    "tight and well - known tail bounds for gamma distributed random variables assert that @xmath246 \\right| \\geq        \\frac{\\gamma}{\\sqrt{k } } \\left ( \\sqrt{2\\ln\\left(2/\\delta \\right ) }          + \\frac{\\ln \\left(2/\\delta \\right)}{\\sqrt{k}}\\right ) \\right\\ } \\leq 2      \\delta \\ , .\\ ] ]    first part of statement ii ) reads as : conditionally on @xmath247 , with probability larger than @xmath248 , @xmath249\\right| \\leq   \\gamma ( 1 + 3r_n /\\sqrt{j } ) \\left ( \\sqrt{8 \\ln \\left(2/\\delta \\right ) } +   \\frac{\\ln \\left(2/\\delta \\right)}{\\sqrt{\\ell}}\\right ) \\ , .           \\,\\ ] ] combining both parts of statement ii ) , we also get that , conditionally on @xmath247 , with probability larger than @xmath248 , @xmath250    the reader may wonder whether resorting to the exponential representation and usual chernoff bounding would not provide a simpler argument .",
    "the straightforward approach leads to the following conditional bound on the logarithmic moment generating function , @xmath251 \\right ) \\right ) \\mid       y_{(k+1)}\\right]}\\\\ &   \\leq & \\frac{\\left(\\gamma         + \\overline{\\eta}({\\mathrm{e}}^{y_{(k+1)}})\\right)^2}{2k\\left(1-\\lambda(\\gamma         + \\overline{\\eta}({\\mathrm{e}}^{y_{(k+1 ) } } ) ) \\right ) } + \\lambda     \\left(\\overline{\\eta}({\\mathrm{e}}^{y_{(k+1)}})-b\\left (         { \\mathrm{e}}^{y_{(k+1)}}\\right )",
    "\\right ) \\ , .   \\end{aligned}\\ ] ] a similar statement holds for the lower tail .",
    "this leads to exponential bounds for deviations of the hill estimator above @xmath252 + \\overline{\\eta}({\\mathrm{e}}^{y_{(k+1)}})-b\\left (         { \\mathrm{e}}^{y_{(k+1)}}\\right)$ ] that is , to control deviations of the hill estimator above its expectation plus a term that may be of the order of magnitude of the bias .",
    "attempts to rewrite @xmath253 $ ] as a sum of martingale increments @xmath254-{\\ensuremath{\\mathbb{e}}}[\\widehat{\\gamma}(k ) \\mid y_{(i+1)}]$ ] , for @xmath255 , and to exhibit an exponential supermartingale met the same impediments .    at the expense of inflating the variance factor , theorem [ bernstein : expo ]",
    "provides a genuine ( conditional ) concentration inequality for hill estimators .",
    "as we will deal with values of @xmath119 for which bias exceeds the typical order of magnitudes of fluctuations , this is relevant to our purpose .",
    "we are now able to characterise the performance of the variant of the selection rule defined by @xcite with @xmath256 where  @xmath257 .",
    "let @xmath258 where @xmath259 is a constant to be defined below .",
    "the deterministic sequence of indices @xmath260 is defined ( for @xmath37 large enough ) by @xmath261 where @xmath262 the sequence @xmath263 is defined by choosing @xmath264 . the deterministic sequences @xmath265 and @xmath260 achieve specific balances between bias and variance . in full generality , because @xmath266 is just an upper bound on the conditional bias @xmath75 , it is difficult to precisely connect @xmath265 and @xmath260 with the oracle sequence @xmath267 .",
    "we call these two sequences the pivotal sequences . in the sequel , @xmath28 stands for @xmath268 . if the context is not clear , we specify @xmath269 or @xmath270",
    ".    let @xmath271 .",
    "recall , from section [ sec : conc - ineq - hill ] , that @xmath272 and agree on the shorthands @xmath273 and @xmath274 which is defined by replacing @xmath28 by @xmath224 in the definition of @xmath275 ( @xmath274 depends on @xmath276 but not on the sampling distribution ) . in the sequel",
    ", @xmath259 is assumed to be chosen so that @xmath277 for @xmath278 and @xmath279 ( @xmath259 may be chosen not larger than 100 ) .    the index @xmath280 is selected according to the following rule : @xmath281 where @xmath282 .",
    "the quantity @xmath283 scales like @xmath284 .",
    "the tail index estimator is @xmath285 .    as tail adaptivity has a price ( see theorem  [ thm - kc - lower - bound ] )",
    ", the ratio between the risk of the data - driven estimator @xmath285 and the risk of the pivotal index @xmath286 can not be upper bounded by a constant factor , let alone by a factor close to @xmath173 .",
    "this is why in the next theorem , we compare the risk of the empirically selected index @xmath285 with the risk of the pivotal index @xmath23 .",
    "recall , from section [ sec : conc - ineq - hill ] , that @xmath287    [ thm : adapt - hill - estim ] assume the sampling distribution @xmath288 satisfies the von mises condition with bounded von mises function @xmath48 , and @xmath289    let @xmath290 is large enough so that @xmath28 ( definition [ def : kn ] ) is well defined .",
    "then , for @xmath291 , with probability larger than @xmath292 , @xmath293 and , with probability larger than @xmath294 , @xmath295 where @xmath296    for @xmath297 , @xmath298    if the bias @xmath73 is @xmath110-regularly varying ( or equivalently , if the von mises function @xmath48 or even @xmath54 are regularly varying ) , then , elaborating on proposition 1 from @xcite , sequences @xmath299 and @xmath265 are connected by @xmath300 and their quadratic risk are related by @xmath301}{{\\ensuremath{\\mathbb{e}}}[(\\gamma-\\widehat{\\gamma}(k_n^*))^2 ] }   = \\frac{2}{2|\\rho|+1}(2|\\rho|)^{2|\\rho|/(1 + 2|\\rho| ) }   \\ , .\\ ] ] moreover , under the second - order assumption , the two pivotal sequences @xmath265 and @xmath260 are also connected .",
    "thus , if the bias is @xmath110-regularly varying , theorem [ thm : adapt - hill - estim ] provides us with a connection between the performance of the simple selection rule and the performance of the ( asymptotically ) optimal choice .",
    "recall that one of the main aims of this paper is to derive performance guarantees for the data - driven index selection method @xmath280 without resorting to second - order assumptions that is , without assuming that the von mises function is regularly varying .",
    "the next corollary upper bounds the risk of the preliminary estimator when we just have an upper bound on the bias .",
    "[ cor : adapt - hill - estim-2ndrv ] assume that , for some @xmath97 and @xmath99 , for all @xmath302 , @xmath303 then , there exists a constant @xmath304 depending on @xmath305 and @xmath110 such that , with probability larger than @xmath294 , @xmath306 where @xmath307 is defined in theorem [ thm : adapt - hill - estim ] .",
    "this meets the information - theoretic lower bound of theorem [ thm - kc - lower - bound ] .",
    "this proposition is a straightforward consequence of rnyi s representation of order statistics of standard exponential samples .",
    "as @xmath0 belongs to @xmath22 and meets the von mises condition , there exists a function @xmath48 on @xmath31 with @xmath308 such that @xmath309 and @xmath310 then , @xmath311      let @xmath312 . by the pythagorean relation , @xmath313 + { \\operatorname{var}}\\left ( { \\ensuremath{\\mathbb{e}}}[z \\mid y_{(k+1)}]\\right ) \\ , .\\ ] ] representation",
    "asserts that , conditionally on @xmath68 , @xmath194 is distributed as a sum of independent , exponentially distributed random variables .",
    "let @xmath34 be an exponentially distributed random variable .",
    "@xmath314 where we have used the cauchy - schwarz inequality and @xmath315 taking expectation with respect to @xmath68 leads to @xmath316   \\leq k{\\gamma^2 }   + 2k\\gamma{\\ensuremath{\\mathbb{e}}}\\left [ \\overline{\\eta}\\left({\\mathrm{e}}^{y_{(k+1 ) } } \\right)\\right ]   + k{{\\ensuremath{\\mathbb{e}}}\\left [ \\overline{\\eta}\\left({\\mathrm{e}}^{y_{(k+1 ) } } \\right)^2\\right ] } \\ , .\\ ] ] the last term in the pythagorean decomposition is also handled using elementary arguments .",
    "@xmath317 = k \\gamma +   k\\int_0^\\infty{\\mathrm{e}}^{-u }   \\eta\\left({\\mathrm{e}}^{u + y_{(k+1)}}\\right ) \\mathrm{d}u   \\ , .\\end{aligned}\\ ] ] as @xmath68 is a function of independent exponential random variables ( @xmath318 ) , the variance of @xmath319 $ ] may be upper bounded using poincar inequality ( proposition [ poincare : expo ] ) @xmath320\\right )      \\leq   4k { \\ensuremath{\\mathbb{e}}}\\left [ \\overline{\\eta}\\left({\\mathrm{e}}^{y_{(k+1 ) } } \\right)^2\\right]\\ , .\\ ] ]    in order to derive the lower bound , we first observe that @xmath321   \\ , .\\ ] ] now , using cauchy - schwarz inequality again , @xmath322      in the proof of theorem [ prp : hill : concentration ] , we will use the next maximal inequality ( see * ? ? ?",
    "* corollary 2.6 ) . recall the definition of @xmath178 ( definition [ dfn : sub - gamma ] ) .",
    "[ zlapcor ] let @xmath323 be real - valued random variables belonging to @xmath178 .",
    "then @xmath324 \\leq \\sqrt{2v\\ln n}+ c\\ln n~.\\ ] ]    proofs follow a common pattern . in order to check that some random variable is sub - gamma ,",
    "we rely on its representation as a function of independent exponential variables and compute partial derivatives , derive convenient upper bounds on the squared euclidean norm and the supremum norm of the gradient and then invoke theorem [ bernstein : expo ] .    at some point",
    ", we will use the next corollary of theorem [ bernstein : expo ] .",
    "[ sec : proof - prop - refprp : technique ] if @xmath87 is an almost everywhere differentiable function on @xmath325 with uniformly bounded derivative @xmath326 , then @xmath327 is sub - gamma with variance factor @xmath328 and scale factor @xmath329    we start from the exponential representation of hill estimators ( proposition [ hill : rep ] ) and represent all @xmath131 as functions of independent random variables @xmath330 where the @xmath331 , are standard exponentially distributed and @xmath332 is distributed like the @xmath216th largest order statistic of an @xmath37-sample of the standard exponential distribution .",
    "we consistently use the notation @xmath333 , for @xmath334 .    @xmath335    let @xmath336 be such that @xmath337 , let us agree on @xmath338 let @xmath339 for @xmath340 , as @xmath341 for @xmath342 and @xmath83 otherwise , @xmath343 this entails that , for @xmath344 , @xmath345 for @xmath346 , @xmath347 this is enough to entail that , for @xmath348 , @xmath349 all in all , for @xmath350 , @xmath351    [ [ proof - of - i ] ] proof of i ) + + + + + + + + + + +    an upper bound on the variance factor for @xmath352 , conditionally on @xmath231 , is obtained by specialising to the case @xmath353 and using and as well as the monotonicity of @xmath54 , @xmath354    using theorem [ bernstein : expo ] conditionally on @xmath355 , we realise that @xmath356)$ ] is sub - gamma on both sides with variance factor not larger than @xmath357 and scale factor not larger than @xmath358 .",
    "this yields    & \\ { ( i)-@xmath359 ( + ) t } 2 ^-s . & \\ { ( i)-@xmath359 ( + ) } 2 ^-s .",
    "[ [ par : proof_of_ii _ ] ] proof of ii ) + + + + + + + + + + + +    the proof of the upper bound on @xmath360 $ ] in statement ii ) from theorem [ prp : hill : concentration ] relies on standard chaining techniques from the theory of empirical processes and uses repeatedly the concentration theorem [ bernstein : expo ] for smooth functions of independent exponential random variables and the maximal inequality for sub - gamma random variables ( proposition [ zlapcor ] ) .    for general",
    "@xmath336 , the variance factor for @xmath361 is upper bounded by @xmath362 let @xmath238 be such that @xmath363 where @xmath364 with @xmath257 .",
    "now , as we assume , in the sequel , that @xmath365 , we may use the next upper bound for the variance factor of @xmath366 ( conditionally on @xmath367),@xmath368 recall that @xmath369 \\right|\\ , .\\ ] ] as it is commonplace in the analysis of normalised empirical processes ( see * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* and references therein ) , we peel the index set over which the maximum is computed .",
    "let @xmath370 and , for all @xmath371 , @xmath372 .",
    "define @xmath373 as @xmath374 \\right|\\ , .\\ ] ] then , @xmath375 & =   & { \\ensuremath{\\mathbb{e}}}[\\max_{j\\in \\mathcal{l}_n } z^a_j \\mid y_{(k+1 ) } ] \\\\",
    "& \\leq & { \\ensuremath{\\mathbb{e}}}[\\max_{j\\in \\mathcal{l}_n } ( z^a_j-{\\ensuremath{\\mathbb{e}}}[z^a_j\\mid   y_{(k+1 ) } ] ) \\mid y_{(k+1 ) } ] ] + \\max_{j\\in \\mathcal{l}_n } { \\ensuremath{\\mathbb{e } } } [ z^a_j\\mid   y_{(k+1 ) } ] ] \\ , .",
    "\\end{aligned}\\ ] ] we now derive upper bounds on both summands by resorting to the maximum inequality for sub - gamma random variables ( proposition [ zlapcor ] ) .",
    "we first bound @xmath376 $ ] , for @xmath377 .",
    "note that direct invocation of lemma [ zlapcor ] and statement i ) shows that @xmath378 \\leq 2 \\gamma(1 + 3r/\\sqrt{j})\\left ( \\sqrt{8   j \\ln(2 ) } + j \\ln(2)\\right ) \\ , .\\ ] ] this bound will be useful for handling small values of @xmath379 .",
    "for @xmath380 , @xmath381 .",
    "we now handle generic @xmath379 using chaining .",
    "fix @xmath377 , @xmath382 \\right|     \\leq \\frac{1}{2^{j/2 } } \\max_{i \\in \\mathcal{s}_j } \\ ,   i\\left| \\widehat{\\gamma}(i ) - { \\ensuremath{\\mathbb{e}}}[\\widehat{\\gamma}(i ) \\mid y_{(k+1 )",
    "} ] \\right|   \\ , .\\ ] ] in order to alleviate notation , let @xmath383 \\right)$ ] , for @xmath384 . for @xmath384 ,",
    "let @xmath385 be the binary expansion of @xmath64 .",
    "then , for @xmath386 , let @xmath387 be defined by @xmath388 so that @xmath389 , @xmath390 and @xmath391 .",
    "using the fact that @xmath392 does not depend on @xmath64 and that @xmath393=0\\ , , \\ ] ] we obtain @xmath394 \\right)\\mid y_{(k+1 ) } \\right]}\\\\ & = & { \\ensuremath{\\mathbb{e}}}\\left [ \\max_{i \\in \\mathcal{s}_j } w(i ) \\mid y_{(k+1 ) } \\right ] \\\\ & = & { \\ensuremath{\\mathbb{e}}}\\left [ \\max_{i \\in \\mathcal{s}_j } w(\\pi_j(i ) ) -w(\\pi_0(i))\\mid y_{(k+1 ) } \\right ]   \\\\ & = & { \\ensuremath{\\mathbb{e}}}\\left [ \\max_{i \\in \\mathcal{s}_j } \\sum_{h=0}^{j-1 } ( w(\\pi_{h+1}(i ) ) -w(\\pi_h(i)))\\mid y_{(k+1 ) }   \\right ] \\\\ & \\leq & \\sum_{h=0}^{j-1 } { \\ensuremath{\\mathbb{e}}}\\left [ \\max_{i \\in \\mathcal{s}_j }   ( w(\\pi_{h+1}(i ) ) -w(\\pi_h(i ) ) ) \\mid y_{(k+1 ) } \\right ] \\ , .\\end{aligned}\\ ] ] now , for each @xmath395 , the maximum is taken over @xmath396 random variables which are sub - gamma with variance factor @xmath397 and scale factor @xmath398 . by proposition  [ zlapcor ] , since @xmath399 , @xmath394 \\right)\\mid y_{(k+1 ) } \\right]}\\\\ & \\leq &    \\gamma \\sum_{h=0}^{j-1 } \\left(\\left(1 + \\frac{r_n}{\\sqrt{j}}\\right ) \\sqrt{8 h   2^{(j - h-1)}\\ln 2 } + \\sqrt{32 h \\ln 2 } r_n+ \\left(1 + \\frac{2r_n}{\\sqrt{j}}\\right ) h \\ln 2\\right ) \\\\ & \\leq &   \\gamma \\left(1 + \\frac{2r_n}{\\sqrt{j}}\\right )   \\left ( 2^{(j-1)/2 }   4.15 \\sqrt{8 \\ln 2 } + \\frac{2}{3 } \\sqrt{32 c_2 } \\ln(2 ) j^{2}+   \\frac{j(j-1)}{2 } \\ln 2\\right ) \\end{aligned}\\ ] ] where we have used @xmath400 for @xmath401 ,",
    "@xmath402 \\right)\\mid y_{(k+1 ) } \\right ] } & \\leq &    17 \\gamma \\ , 2^{j/2 }   \\left(1 + \\frac{2r_n}{\\sqrt{j}}\\right)\\ , . \\end{aligned}\\ ] ]    finally , for all @xmath403 , @xmath404 \\leq 34 \\ ,   \\gamma \\ ,   \\left(1 + \\frac{3r_n}{\\sqrt{j}}\\right )    \\,.\\ ] ]    in order to prove statement ii ) , we check that , for each @xmath403 , @xmath373 is sub - gamma on the right - tail with variance factor at most @xmath405 and scale factor not larger than @xmath406 . under the von mises condition ( definition [ dfn : vmises : cond ] ) ,",
    "the sampling distribution is absolutely continuous with respect to lebesgue measure . for almost every sample ,",
    "the maximum defining @xmath373 is attained at a single index @xmath384 .",
    "starting again from the exponential representation and repeating the computation of partial derivatives , we obtain the desired bounds .    by proposition [ zlapcor ] , @xmath407 ) \\mid y_{(k+1)}\\right ] } \\\\ & \\leq & \\left (    \\sqrt{8\\ln |\\mathcal{l}_n| }   + \\frac { \\ln |\\mathcal{l}_n|}{\\sqrt{\\ell}}\\right)\\left(\\gamma + 3 \\overline{\\eta}(t )    \\right )   \\\\   & \\leq & 4 \\sqrt{\\ln |\\mathcal{l}_n| } \\left(\\gamma + 3     \\overline{\\eta}(t )    \\right )   \\\\    & \\leq & 4 \\ , \\gamma\\ , \\sqrt{\\ln |\\mathcal{l}_n| }   \\left(1 + \\frac{3r_n}{\\sqrt{j } }    \\right )   \\ , \\end{aligned}\\ ] ] where we have used @xmath408 , for @xmath409 . combining the different bounds leads to the upper bound on @xmath360 $ ] .      throughout this proof ,",
    "let @xmath410 let us define the events @xmath411 and @xmath412 as @xmath413\\right|\\leq   \\gamma z_\\delta \\big\\}\\ , , \\\\",
    "e_2 & = & \\big\\ { t_n   \\geq \\frac{n}{k_n^\\delta } \\big\\ } \\text { with } k_n^\\delta=",
    "k_n+2{\\ln \\left(1/\\delta \\right)}+\\sqrt{2k_n \\ln ( 1/\\delta)}{\\enspace .}\\end{aligned}\\ ] ]    the fact that @xmath414 follows from the following reformulation of proposition 4.3 from @xcite ( a proof is given in appendix [ proof : prop : right : tail : order : stat ] ) .",
    "[ prop : right : tail : order : stat ] for @xmath184 , with probability larger that @xmath248 , @xmath415 where @xmath68 is the @xmath209th largest order statistic of an exponential sample of size @xmath37 .    by theorem",
    "[ prp : hill : concentration ] , @xmath416 .",
    "hence , the event @xmath417 has probability at least @xmath418 .    under @xmath419 ,    a.   @xmath420 .",
    "b.   for all @xmath421 , @xmath422|\\leq \\overline{\\eta}(t_n).$ ]    the first step of the proof consists in checking that under @xmath417 , the selected index is not smaller than @xmath28 .",
    "it suffices to check that for all @xmath423 , @xmath424 for all @xmath425 , @xmath426\\right| + \\left|\\widehat{\\gamma}(i )   -{\\ensuremath{\\mathbb{e}}}[\\widehat{\\gamma}(i)\\mid t_n]\\right|\\\\   & \\leq & \\overline{\\eta}(t_n )   + \\frac{\\gamma z_\\delta}{\\sqrt{i } }   \\\\ & \\leq & \\frac{\\gamma r_n}{\\sqrt{k_n } } + \\frac{\\gamma z_\\delta}{\\sqrt{i } } \\end{aligned}\\ ] ] so that @xmath427 meanwhile , for all @xmath428 , @xmath429\\big|}_{\\textsc{(i)}}+ \\underbrace{\\left|   { \\ensuremath{\\mathbb{e}}}[\\widehat{\\gamma}(i)-\\widehat{\\gamma}(k)\\mid t_n]\\right|}_{\\textsc{(ii ) } } + \\underbrace{\\left| \\widehat{\\gamma}(k)-{\\ensuremath{\\mathbb{e}}}[\\widehat{\\gamma}(k)\\mid t_n]\\right|}_{\\textsc{(iii ) } } \\ , .",
    "\\end{aligned}\\ ] ] under @xmath417 , for @xmath430 , @xmath431 under @xmath412 , @xmath432\\right| + \\left| { \\ensuremath{\\mathbb{e } } } [ \\gamma-\\widehat{\\gamma}(k)\\mid t_n]\\right| \\\\ & \\leq &   2 \\overline{\\eta}(t_n ) \\\\ & \\leq &   2 \\gamma r_n /\\sqrt{k_n } \\ , .",
    "\\end{aligned}\\ ] ]    plugging upper bounds on ( i ) , ( ii ) and ( iii ) , it comes that , under @xmath417 , for all @xmath433 and for all @xmath434 , @xmath435    in order to warrant that , under @xmath417 , for all @xmath436 and for all @xmath64 such that @xmath437 , @xmath438 , it is enough to have @xmath439 the last inequality holds because @xmath440 by definition of @xmath283 .",
    "hence , with probability larger than @xmath441 , @xmath417 is realised , and under @xmath417 , @xmath442 .",
    "we now check that if @xmath443 , the risk of @xmath285 is not much larger than the risk of @xmath23 .",
    "@xmath444    therefore , under @xmath445 , @xmath446    now , consider the event @xmath447 with @xmath448\\right| \\leq \\left(\\gamma+3 \\overline{\\eta}(t_n ) \\right ) \\big ( \\sqrt{8 \\ln   \\left(2/\\delta \\right ) } + \\frac{\\ln   \\left(2/\\delta \\right)}{\\sqrt{k_n}}\\big)\\bigg\\ } { \\enspace .}\\ ] ] since , @xmath449 , thanks to statement i ) from theorem [ prp : hill : concentration ] , the event @xmath447 has probability at least @xmath450 .    then , by definition of @xmath28 , under @xmath412 , @xmath451\\right|\\leq \\overline{\\eta}(t_n ) \\leq",
    "\\gamma r_n/\\sqrt{k_n }   \\ , .\\ ] ] hence , under @xmath452 , @xmath453|+ |    \\widehat{\\gamma}({k}_n ) -\\mathbb{e}[\\widehat{\\gamma}({k}_n)\\mid t_n]| \\\\    & \\leq & \\frac{\\gamma}{\\sqrt{k_n}}\\left(r_n + \\left(1 + \\frac{3r_n}{\\sqrt{k_n}}\\right)\\big ( \\sqrt{8 \\ln   \\left(2/\\delta \\right ) } + \\frac{\\ln   \\left(2/\\delta \\right)}{\\sqrt{k_n}}\\big )",
    "\\right ) \\ , . \\end{aligned}\\ ] ] therefore , plugging this bound into , with probability larger than @xmath292 , @xmath454 where @xmath455      if , for some @xmath97 and @xmath99 , @xmath456 then , by the definition of @xmath28 , @xmath457 which entails that @xmath458 solving this inequality leads to @xmath459 and finally to @xmath460 thus , for sufficiently large @xmath37 , there exists a constant @xmath176 depending on @xmath461 such that @xmath462 starting from equation of theorem [ thm : adapt - hill - estim ] , with probability @xmath292 , @xmath463 and , there exists a constant @xmath464 , depending on @xmath305 and @xmath110 , such that @xmath465 hence , with probability larger than @xmath294 , @xmath466",
    "risk bounds like theorem [ thm : adapt - hill - estim ] and corollary [ cor : adapt - hill - estim-2ndrv ] are conservative . for all practical purposes ,",
    "they are just meant to be reassuring guidelines . in this numerical section ,",
    "we intend to shed some light on the following issues :    1 .",
    "is there a reasonable way to calibrate the threshold @xmath283 used in the definition of @xmath280 ?",
    "how does the method perform if we choose @xmath283 close to @xmath467 ?",
    "how large is the ratio between the risk of @xmath285 and the risk of @xmath468 for moderate sample sizes ?",
    "the finite - sample performance of the data - driven index selection method described and analysed in section [ sec : adapt - hill - estim ] has been assessed by monte - carlo simulations .",
    "computations have been carried out in ` r ` using packages ` ggplot2 ` @xcite , ` knitr ` , ` foreach ` , ` iterators ` , ` xtable ` and ` dplyr ` ( see * ? ? ? * for a modern account of the r environment ) . to get into the details , we investigated the performance of index selection methods on samples of sizes @xmath469 and @xmath470 from the collection of distributions listed in table [ tab - risk - profiles ] .",
    "the list comprises the following distributions    a.   frchet distributions @xmath471 for @xmath3 and @xmath472 . b.   student distributions @xmath473 with @xmath474 degrees of freedom .",
    "c.   the log - gamma distribution with density proportional to @xmath475 , which means @xmath476 and @xmath477 .",
    "d.   the lvy distribution with density @xmath478 , @xmath479 and @xmath480 ( this is the distribution of @xmath481 when @xmath482 ) .",
    "e.   the @xmath483 distribution is defined by @xmath484 and von mises function equal to @xmath485 .",
    "this distribution satisfies the second - order regular variation condition with @xmath486 but does not satisfy condition .",
    "f.   two pareto change point distributions with distribution functions @xmath487 and @xmath488 , @xmath489 , and thresholds @xmath7 adjusted in such a way that they correspond to quantiles of order @xmath490 and @xmath491 , respectively .",
    "frchet , student , log - gamma distributions were used as benchmarks by @xcite , @xcite and @xcite .",
    "table  [ tab - risk - profiles ] , which is complemented by figure  [ fig : risk - comp ] , describes the difficulty of tail index estimation from samples of the different distributions .",
    "monte - carlo estimates of the standardised root mean square error ( rmse ) of hill estimators @xmath492^{1/2}\\ ] ] are represented as functions of the number of order statistics @xmath119 for samples of size @xmath470 from the sampling distributions .",
    "all curves exhibit a common pattern : for small values of @xmath119 , the rmse is dominated by the variance term and scales like @xmath493 . above a threshold that depends on the sampling distribution but that is not completely characterised by the second - order regular variation index , the rmse grows at a rate that may reflect the second - order regular variation property ( if any ) of the distribution .",
    "not too surprisingly , the three frchet distributions exhibit the same risk profile .",
    "the three curves are almost undistinguishable .",
    "the student distributions illustrate the impact of the second - order parameter on the difficulty of the index selection problem . for sample size @xmath494 ,",
    "the optimal index for @xmath495 is smaller than @xmath496 , it is smaller than the usual recommendations . for such moderate sample sizes ,",
    "distribution @xmath495 seems as hard to handle as the @xmath497-gamma distribution which usually fits in the horror hill plot gallery .",
    "the @xmath498-stable lvy distribution and the @xmath483-distribution behave very differently .",
    "even though they both have second - order parameter @xmath110 equal to @xmath499 , the @xmath483 distribution seems almost as challenging as the @xmath500 distribution while the lvy distribution looks much easier than the frchet distributions .",
    "the pareto change point distributions exhibit an abrupt transition .",
    ".estimated oracle index @xmath501 and standardised rmse @xmath502^{1/2}/\\gamma$ ] for benchmark distributions .",
    "estimates were computed from @xmath123 replicated experiments on samples of size @xmath503 [ cols=\"<,^,^,^,^\",options=\"header \" , ]     figure [ fig : pcp - risk - plot ] concisely describes the behaviour of the two index selection methods on samples from the pareto change point distribution with parameters @xmath504 and threshold @xmath7 corresponding to the @xmath490 quantile .",
    "the plain line represents the standardised rmse of hill estimators as a function of selected index .",
    "this figure contains the superposition of two density plots corresponding to @xmath505 and @xmath506 .",
    "the density plots were generated from @xmath123 points with coordinates latexmath:[$(\\widehat{k}(r_n ) ,    points with coordinates latexmath:[$(\\widehat{k}^{\\textsc{dk}}_n ,    contoured and well - concentrated density plot corresponds to the performance of @xmath285 .",
    "the diffuse tiled density plot corresponds to the performance of @xmath505 .",
    "facing pareto change point samples , the two selection methods behave differently .",
    "lepski s rule detects correctly an abrupt change at some point and selects an index slightly above that point .",
    "as the conditional bias varies sharply around the change point , this slight over estimation of the correct index still results in a significant loss as far as rmse is concerned .",
    "the drees - kaufmann rule , fed with an a priori estimate of the second - order parameter , picks out a much smaller index , and suffers a larger excess risk .     from the pareto change point distribution with parameters @xmath504 and threshold @xmath7 corresponding to the @xmath490 quantile .",
    "the concentrated density plot corresponds to points @xmath508 . ]",
    "the authors are thankful to the editor and the referees for their careful reading and valuable suggestions , which led to detect an error and to a improved version of the paper .",
    "50 [ 1]#1 [ 1]`#1 ` urlstyle [ 1]doi : # 1    j.  beirlant , y.  goegebeur , j.  teugels , and j.  segers . _",
    "statistics of extremes_. john wiley & sons , ltd . , 2004 .",
    "j.  beirlant , c.  bouquiaux , and b.  werker .",
    "semiparametric lower bounds for tail index estimation",
    ". _ journal of statistical planning and inference _ , 1360 ( 3):0 705729 , 2006 .",
    "n.  bingham , c.  goldie , and j.  teugels .",
    "_ regular variation_. cambridge university press , 1987 .",
    "l.  birg .",
    "an alternative point of view on lepski s method .",
    "in _ state of the art in probability and statistics ( leiden , 1999 ) _ , volume  36 of _ ims lecture notes monogr .",
    "_ , pages 113133 .",
    ", 2001 .",
    "l.  birg .",
    "a new lower bound for multiple hypothesis testing . _",
    "ieee trans .",
    "inform . theory _ , 51:0 16111615 , 2005 .",
    "s.  bobkov and m.  ledoux .",
    "poincar s inequalities and talagrand s concentration phenomenon for the exponential distribution .",
    ". fields _ , 107:0 383400 , 1997 .",
    "s.  boucheron and m.  thomas .",
    "concentration inequalities for order statistics .",
    "_ , 17:0 112 , 2012 .",
    "s.  boucheron , g.  lugosi , and p.  massart .",
    "_ concentration inequalities_. oxford university press , 2013 .",
    "a.  carpentier and a.  kim .",
    "adaptive confidence intervals for the tail coefficient in a wide second order class of pareto models .",
    "_ , 8:0 20662110 , 2014 .",
    "a.  carpentier and a.  kim .",
    "adaptive and minimax optimal estimation of the tail coefficient .",
    "_ statistica sinica _ , 25:0 11331144 , 2015 .",
    "s.  chatterjee .",
    "_ superconcentration and related topics_. springer - verlag , 2014 .",
    "t.  cover and j.  thomas .",
    "_ elements of information theory_. john wiley , 1991 .",
    "s.  csrg , p.  deheuvels , and d.  mason .",
    "kernel estimates of the tail index of a distribution .",
    "_ , 130 ( 3):0 10501077 , 1985 .",
    "j.  danielsson , l.  de  haan , l.  peng , and c.  g. de  vries . using a bootstrap method to choose the sample fraction in tail index estimation .",
    "_ j. multivariate anal .",
    "_ , 760 ( 2):0 226248 , 2001 .    d.  darling and p.  erds . a limit theorem for the maximum of normalized sums of independent random variables",
    "_ duke math .",
    "j _ , 23:0 143155 , 1956 .",
    "l.  de  haan and a.  ferreira .",
    "_ extreme value theory_. springer - verlag , 2006 .",
    "g.  draisma , l.  de  haan , l.  peng , and t.  pereira . a bootstrap - based method to achieve optimally in estimating the extreme value index .",
    "_ extremes _ , 2:0 367404 , 1999 .",
    "h.  drees .",
    "optimal rates of convergence for estimates of the extreme value index .",
    "_ , 260 ( 1):0 434448 , 1998 .",
    "h.  drees . on smooth statistical tail functionals .",
    "j. statist .",
    "_ , 250 ( 1):0 187210 , 1998 .",
    "h.  drees .",
    "minimax risk bounds in extreme value theory .",
    "_ , 290 ( 1):0 266294 , 2001 .",
    "h.  drees and e.  kaufmann . selecting the optimal sample fraction in univariate extreme value estimation .",
    "_ stochastic process .",
    "_ , 750 ( 2):0 149172 , 1998 .",
    "h.  drees , l.  de  haan , and s.  resnick . .",
    "_ , 280 ( 1):0 254274 , 2000 .",
    "j.  geluk , l.  de  haan , s.  resnick , and c.  stric .",
    "second - order regular variation , convolution and the central limit theorem .",
    "_ stochastic process .",
    "_ , 690 ( 2):0 139159 , 1997 .    e.  gin and v.  koltchinskii .",
    "concentration inequalities and asymptotic results for ratio type empirical processes .",
    "_ , 340 ( 3):0 11431216 , 2006 .",
    "i.  grama and v.  spokoiny .",
    "statistics of extremes by oracle estimation .",
    "_ , 360 ( 4):0 16191648 , 2008 .",
    "p.  hall and i.  weissman . on the estimation of extreme tail probabilities .",
    "_ , 250 ( 3):0 13111326 , 1997 .",
    "p.  hall and a.  welsh .",
    "adaptive estimates of parameters of regular variation .",
    "_ , 130 ( 1):0 331341 , 1985",
    ".    b.  hill . a simple general approach to inference about the tail of a distribution . _ ann .",
    "_ , 3:0 11631174 , 1975 .",
    "v.  koltchinskii . _ oracle inequalities in empirical risk minimization and sparse recovery problems .",
    "ecole det de probabilit de saint - flour xxxviii _ ,",
    "volume 2033 of _ lecture notes in math._. springer - verlag , 2008 .",
    "m.  ledoux . _",
    "the concentration of measure phenomenon_. american mathematical society , 2001 .",
    "m.  ledoux and m.  talagrand .",
    "_ probability in banach space_. springer - verlag , 1991 .",
    "o.  lepski . a problem of adaptive estimation in gaussian white noise .",
    "_ teoriya veroyatnosteui",
    "i ee primeneniya _ , 350 ( 3):0 459470 , 1990 .    o.  lepski .",
    "asymptotically minimax adaptive estimation . i. upper bounds .",
    "optimally adaptive estimates . _ teoriya veroyatnosteui",
    "i ee primeneniya _ , 360 ( 4):0 645659 , 1991 .",
    "o.  lepski .",
    "asymptotically minimax adaptive estimation .",
    "schemes without optimal adaptation .",
    "adaptive estimates .",
    "_ teoriya veroyatnosteui i ee primeneniya _ , 370 ( 3):0 468481 , 1992 .",
    "o.  lepski and a.  tsybakov .",
    "asymptotic exact nonparametric hypothesis testing in sup - norm and at a fixed point .",
    "theory rel .",
    "fields _ , 1170 ( 1):0 1748 , 2000 .    d.  mason",
    ". laws of large numbers for sums of extreme values .",
    "_ , 10:0 754764 , 1982 .",
    "p.  massart .",
    "_ concentration inequalities and model selection .",
    "ecole det de probabilit de saint - flour xxxiv _ , volume 1896 of _ lecture notes in math._. springer - verlag , 2007 .",
    "p.  math .",
    "the lepski principle revisited . _ inverse problems _ , 220 ( 3):0 l11l15 , 2006 .",
    "b.  maurey .",
    "some deviation inequalities . _ geometric and functional analysis _ , 10 ( 2):0 188197 , 1991 .",
    "s.  novak .",
    "lower bounds to the accuracy of inference on heavy tails .",
    "_ bernoulli _ , 200 ( 2):0 979989 , 2014 .",
    "s.  resnick .",
    "_ heavy - tail phenomena : probabilistic and statistical modeling _",
    ", springer - verlag , 2007 .",
    "j.  segers .",
    "abelian and tauberian theorems on the bias of the hill estimator .",
    "j. statist .",
    "_ , 290 ( 3):0 461483 , 2002 .",
    "m.  talagrand . a new isoperimetric inequality and the concentration of measure phenomenon . in _ geometric aspects of functional analysis ( 198990 )",
    "_ , volume 1469 of _ lecture notes in math .",
    "_ , pages 94124 .",
    "springer - verlag , 1991 .",
    "m.  talagrand . a new look at independence .",
    "_ , 24:0 134 , 1996 .",
    "m.  talagrand .",
    "new concentration inequalities in product spaces . _ inventiones mathematicae _ , 126:0 505563 , 1996 .",
    "m.  talagrand . _ the generic chaining_. springer - verlag , 2005 .",
    "a.  b. tsybakov .",
    "pointwise and sup - norm sharp adaptive estimation of functions on the sobolev classes .",
    "_ , 260 ( 6):0 24202469 , 1998",
    ".    s.  van de geer .",
    "_ applications of empirical process theory_. cambridge university press , 2000 .",
    "h.  wickham .",
    "_ ggplot2 : elegant graphics for data analysis_. springer - verlag , 2009 .",
    "h.  wickham .",
    "_ advanced r_. chapman & hall / crc , 2014 .",
    "let @xmath509 . then ,    @xmath510    and @xmath511 let @xmath189 , then for all @xmath512 , @xmath513\\lambda^2 } { 2(1-c ) } \\ , .\\ ] ] now , start from the first statement in theorem [ bernstein : expo ] , @xmath514   & \\leq &          \\frac{2\\lambda^2}{1-c } { \\ensuremath{\\mathbb{e}}}\\left [ { \\mathrm{e}}^{\\lambda ( z-{\\ensuremath{\\mathbb{e}}}z ) } \\| \\nabla            f\\|^2\\right ] \\\\   & = &    \\frac{4\\lambda^2}{2(1-c ) } \\frac{1}{k } \\left ( 1 + \\frac{1}{k}\\right ) { \\ensuremath{\\mathbb{e}}}\\left [ \\frac{{\\mathrm{e}}^{\\lambda ( z-{\\ensuremath{\\mathbb{e}}}z)}}{h(z)^2}\\right ] \\\\ & \\leq & \\frac{4\\lambda^2}{2(1-c ) } \\frac{1}{k } \\left ( 1 + \\frac{1}{k}\\right ) { \\ensuremath{\\mathbb{e}}}\\left [ { { \\mathrm{e}}^{\\lambda ( z-{\\ensuremath{\\mathbb{e}}}z)}}\\right ] { \\ensuremath{\\mathbb{e}}}\\left [ \\frac{1}{h(z)^2}\\right]\\end{aligned}\\ ] ] where the last inequality follows from chebychev negative association inequality .",
    "hence , @xmath515 = \\frac{\\operatorname{ent } \\left [ { \\mathrm{e}}^{\\lambda ( z-{\\ensuremath{\\mathbb{e}}}z)}\\right]}{\\lambda^2        { \\ensuremath{\\mathbb{e}}}\\left [ { \\mathrm{e}}^{\\lambda ( z-{\\ensuremath{\\mathbb{e}}}z)}\\right ] }   \\leq      \\frac{1}{2(1-c ) }   \\frac{4}{k } \\left ( 1+\\frac{1}{k}\\right )   { \\ensuremath{\\mathbb{e}}}\\left [ \\frac{1}{h(z)^2}\\right ] \\ , .\\ ] ] this differential inequality is readily solved and leads to the corollary .",
    "the proof proceeds by classical arguments . in the sequel",
    ", we use the almost sure representation argument . without loss of generality",
    ", we assume that all the random variables live on the same probability space , and that , for any intermediate sequence @xmath19 , @xmath516 converges almost surely towards a standard gaussian random variable . complemented with dominated convergence arguments",
    ", the next lemma will be the key element of the proof .",
    "[ lem : rvplus ] let @xmath517 and @xmath518 be the @xmath519th largest order statistic of a standard exponential sample , then , for any intermediate sequence @xmath19 and @xmath520 , @xmath521    note that @xmath522 then , the result follows since @xmath523 and the convergence @xmath524 is locally uniform on @xmath525 .    in order to secure dominated convergence arguments , we will use drees s improvement of potter s inequality ( see * ? ? ?",
    "for every @xmath526 , there exists @xmath527 such that , for @xmath528 , @xmath529    to prove proposition [ sec : abel - vari - hill ] , we start from representation : @xmath530 by the pythagorean relation , @xmath531\\right)+{\\ensuremath{\\mathbb{e}}}\\left [ { \\operatorname{var}}\\left ( \\widehat{\\gamma}(k_n ) \\mid y_{(k_n+1)}\\right)\\right ] \\ , , \\ ] ] so that @xmath532\\right)}{\\eta(n / k_n )     } +   k_n { \\ensuremath{\\mathbb{e}}}\\left [ \\frac{{\\operatorname{var}}\\left ( \\widehat{\\gamma}(k_n ) \\mid y_{(k_n+1)}\\right)-\\frac{\\gamma^2}{k_n}}{\\eta(n / k_n ) } \\right ] \\ , .",
    "\\end{aligned}\\ ] ] the second summand can be further decomposed using .",
    "@xmath533\\right)}{\\eta(n / k_n)}}_{\\textsc{(i ) } } \\\\ & & + \\underbrace{\\eta     ( \\tfrac{n}{k_n}){\\ensuremath{\\mathbb{e}}}\\left[{\\operatorname{var}}\\left [ \\int_0^e       \\frac{\\eta({\\mathrm{e}}^{u+y_{(k_n+1 ) } } ) } { \\eta ( n / k_n ) } { \\mathrm{d}}u \\mid y_{(k_n+1 ) }    \\right]\\right]}_{(\\textsc{ii})}\\\\ & &   + \\underbrace{2\\gamma { \\ensuremath{\\mathbb{e}}}\\left[{\\operatorname{cov}}\\left [       e,\\int_0^e \\frac{\\eta({\\mathrm{e}}^{u+y_{(k_n+1)}})}{\\eta ( n / k_n)}{\\mathrm{d}}u     \\mid y_{(k_n+1)}\\right]\\right]}_{(\\textsc{iii } ) }   .",
    "\\end{aligned}\\ ] ] we check that ( i ) and ( ii ) tend to @xmath83 and then that ( iii ) converges towards a finite limit .    fix @xmath526 and define @xmath534 .",
    "+ let @xmath535 denote the event @xmath536 . for @xmath37 such that @xmath537 , as @xmath518 is sub - gamma with variance factor @xmath538 , @xmath539    we first check that ( ii ) tends to @xmath83 .",
    "let @xmath37 be such that @xmath540 and @xmath541 denote the random variable @xmath542 .",
    "note that , for @xmath543 , @xmath544    using jensen s inequality and fubini s theorem , @xmath545\\right]}\\\\   & \\leq & { \\ensuremath{\\mathbb{e}}}\\left[{\\ensuremath{\\mathbb{e}}}\\left [ e \\int_0^e \\left (       \\frac{\\eta({\\mathrm{e}}^{u+y_{(k_n+1)}})}{\\eta ( n / k_n)}\\right)^2{\\mathrm{d}}u \\mid y_{(k_n+1 ) }     \\right]\\right ] \\\\ & = & \\int_0^\\infty { \\mathrm{e}}^{-v }   v \\int_0^v   { \\ensuremath{\\mathbb{e}}}\\left[\\left (       \\frac{\\eta({\\mathrm{e}}^{u+y_{(k_n+1)}})}{\\eta ( n / k_n)}\\right)^2   \\right ] { \\mathrm{d}}u \\mathrm{d}v \\\\ & = & \\int_0^\\infty { \\mathrm{e}}^{-v }   v \\int_0^v   { \\ensuremath{\\mathbb{e}}}\\left[\\left (       \\frac{\\eta({\\mathrm{e}}^{u+w_n}n / k_n)}{\\eta ( n / k_n)}\\right)^2   \\right ] { \\mathrm{d}}u \\mathrm{d}v \\end{aligned}\\ ] ] we now apply potter s inequality ( [ potter : drees ] ) on the event @xmath546 with @xmath547 and @xmath548 : @xmath549\\right ] } \\\\ & \\leq   & \\int_0^\\infty { \\mathrm{e}}^{-v }   v \\int_0^v   { \\ensuremath{\\mathbb{e}}}\\left [   \\mathbb{1}_{a_{n } }       { \\mathrm{e}}^{2\\rho ( u+w_n ) }    \\left (   1+\\epsilon   { \\mathrm{e}}^{\\delta ( u+|w_n| ) } \\right)^2 + \\mathbb{1}_{a_n^c } \\frac{m^2}{\\eta(n / k_n)^2 }   \\right ] { \\mathrm{d}}u \\mathrm{d}v \\\\ & \\leq & \\int_0^\\infty { \\mathrm{e}}^{-v }   v \\int_0^v   { \\ensuremath{\\mathbb{e}}}\\left [         { \\mathrm{e}}^{2\\rho w_n }    2 \\left ( 1+\\epsilon^2   { \\mathrm{e}}^{2\\delta ( u+|w_n| ) } \\right )   \\right ] { \\mathrm{d}}u \\mathrm{d}v+ \\frac{2m^2}{\\eta(n / k_n)^2}{\\ensuremath{\\mathbb{e}}}\\mathbb{1}_{a_n^c } \\ ,     . \\end{aligned}\\ ] ] the first summand has a finite limit thanks to lemma [ lem : rvplus ] .",
    "the second summand converges to @xmath83 as @xmath550 tends to @xmath83 exponentially fast while @xmath551 tends to infinity algebraically fast .",
    "bounds on ( i ) are easily obtained , using jensen s inequality and poincar inequality .",
    "@xmath552\\right)}{\\eta(n / k_n ) }   & = &   \\frac{k_n { \\operatorname{var}}\\left ( \\int_0^\\infty { \\eta \\left ( { \\mathrm{e}}^{u+y_{(k_n+1 ) } } \\right ) } { \\mathrm{e}}^{-u } { \\mathrm{d}}u\\right)}{\\eta(n / k_n ) }   \\\\ & \\le & 4 \\eta(n / k_n ) { \\mathbb{e}}\\left [ \\left ( \\int_0^\\infty \\frac{\\eta \\left (          { \\mathrm{e}}^{u+y_{(k_n+1 ) } } \\right)}{\\eta(n / k_n ) } { \\mathrm{e}}^{-u }      { \\mathrm{d}}u \\right)^2\\right ] \\\\ & \\leq &   4 \\eta(n / k_n ) { \\mathbb{e}}\\left [ \\int_0^\\infty \\left ( \\frac{\\eta \\left (          { \\mathrm{e}}^{u+y_{(k_n+1 ) } } \\right)}{\\eta(n / k_n)}\\right)^2    { \\mathrm{e}}^{-u } { \\mathrm{d}}u \\right ] \\ , . \\ ] ] using the line of arguments as for handling the limit of ( ii ) , we establish that ( i ) converges to @xmath83 .",
    "we now check that ( iii ) converges towards a finite limit .",
    "note that @xmath553\\right ] } \\\\ & = &   { \\ensuremath{\\mathbb{e}}}\\left [       ( e-1)\\int_0^e \\frac{\\eta({\\mathrm{e}}^{u+y_{(k_n+1)}})}{\\eta ( n / k_n)}{\\mathrm{d}}u     \\right ] \\ , .",
    "\\end{aligned}\\ ] ] by lemma [ lem : rvplus ] , for almost every @xmath520 , @xmath554 and @xmath555 the first term is finite as the integral of a continuous function on a compact .",
    "+ thus , @xmath556 the expected value of the last random variable is @xmath557 .",
    "we check that , for sufficiently large @xmath37 , @xmath558}\\\\   & \\leq   & { \\mathbb{e}}\\left [ | e-1|   \\int_0^e     { \\mathrm{e}}^{\\rho ( u+w_n ) } \\left(1 + \\epsilon       { \\mathrm{e}}^{\\delta ( u+|w_n|)}\\right ) + \\mathbb{1}_{a_n^c } |e-1| \\frac{m}{|\\eta(n / k_n)| } { \\mathrm{d}}u \\right ] \\\\   & \\leq & { \\ensuremath{\\mathbb{e}}}\\left[{\\mathrm{e}}^{\\rho w_n } \\left ( 2 + \\frac{\\epsilon}{\\delta(1-\\delta)^2 } { \\mathrm{e}}^{\\delta|w_n| } \\right)\\right ]   + \\frac{m}{|\\eta(n / k_n)| }   { \\ensuremath{\\mathbb{e}}}\\mathbb{1}_{a_n^c }   \\\\ & \\leq &   4 { \\mathrm{e}}^{\\frac{\\rho^2}{k_n } } +   \\frac{2 \\epsilon}{\\delta(1-\\delta)^2 } { \\mathrm{e}}^{\\frac{(\\delta-\\rho)^2}{k_n } } + \\frac{m}{|\\eta(n / k_n)| }   { \\ensuremath{\\mathbb{e}}}\\mathbb{1}_{a_n^c }   { \\enspace .}\\ ] ] we now way conclude by dominated convergence that @xmath559",
    "the proof of proposition 4.3 from @xcite yields that , with probability larger than @xmath248 , for @xmath560 , @xmath561 we may choose @xmath562 and notice that @xmath563 .",
    "this yields @xmath564",
    "lower bounds on tail index estimation error @xcite are usually constructed by defining sequences of local models around a pure pareto distribution with shape parameter @xmath565 . when deriving lower bounds for the estimation error under constraints like @xmath54 is regularly varying , the elements of the local model for sample size @xmath37 may be defined by @xmath566 where @xmath199 is square integrable over @xmath567 $ ] , @xmath568 , @xmath569 @xcite .",
    "the sequences @xmath570 and @xmath571 are chosen in such a way that @xmath572 satisfies the required constraint . if the local alternatives are pareto change point distributions as in @xcite and @xcite , @xmath573 , @xmath574 .",
    "@xcite explores a richer collection of local alternatives in order to fit into the theory of weak convergence of local experiments .    in order to explore adaptivity as in @xcite",
    ", it is necessary to handle simultaneously a collection of sequences @xmath575 corresponding to different rates of decay of the von mises function .",
    "the difficulty of estimation is connected with the difficulty of distinguishing alternatives with different tail indices that is , with the hardness of a multiple hypotheses testing problem . in order to lower bound the testing error",
    ", @xcite chose to use fano s lemma ( * ? ? ? * see ) .",
    "using fano s lemma requires bounding the kullback - leibler divergence between the different local alternatives which is not as easy as bounding the divergence between a pareto change point distribution and a pure pareto distribution .",
    "the next lemma is from @xcite .",
    "it can be used in the derivation of risk lower bounds instead of the classical fano lemma . just as fano s lemma",
    ", it states a lower bound on the error in multiple hypothesis testing .",
    "however , as it only requires computing the kullback - leibler divergence to the localisation center , in the present setting , it significantly alleviates computations and makes the proof more concise and more transparent .",
    "[ thm - kc - lower - bound : appendix ] let @xmath12 , @xmath579 , and @xmath580 .",
    "then , for any tail index estimator @xmath149 and any sample size @xmath37 such that @xmath581 , there exists a collection @xmath582 of probability distributions such that    a.   @xmath583 with @xmath584 , b.   @xmath585 meets the von mises condition with von mises function @xmath586 satisfying @xmath587 where @xmath588 , c.   @xmath589 + and @xmath590 \\geq \\frac{c_\\rho}{4(1 + 2{\\mathrm{e } } ) }   \\left(\\frac{v\\ln\\ln n}{n}\\right)^{|\\rho|/(1 + 2|\\rho| ) } \\ , , \\ ] ] with @xmath157 .",
    "the center of localisation @xmath596 is the pure pareto distribution with shape parameter @xmath4 ( @xmath597 ) .",
    "the local alternatives @xmath598 are pareto change point distributions .",
    "each @xmath585 is defined by a breakpoint @xmath599 and an ultimate pareto index @xmath600 .",
    "if @xmath601 denotes the distribution function of @xmath585 , @xmath602 karamata s representation of @xmath603 is @xmath604 with @xmath605    the kullback - leibler divergence between @xmath585 and @xmath596 is readily calculated , @xmath606 if @xmath584 , the next upper bound holds , @xmath607 the breakpoints and tail indices are chosen in such a way that all upper bounds are equal ( namely @xmath608 does not depend on @xmath64 ) , @xmath609 so that @xmath610 , for all @xmath611 .",
    "now , let @xmath149 be any tail index estimator .",
    "define region @xmath614 , as the set of samples such that @xmath600 minimises @xmath615 , for @xmath616 . then ,",
    "if the event @xmath614 is not realised , @xmath617 by birg s lemma , @xmath618 in order to make the whole construction useful , it remains to choose the `` second - order parameters '' @xmath619 s ( the true second - order parameter of each @xmath585 is infinite ! ) .",
    "we will need an upper bound on @xmath620 ( but we already have @xmath621 ) , as well as a lower bound on @xmath622 for @xmath623 that scales like @xmath624 .",
    "following @xcite , we finally choose @xmath619 as @xmath625 for @xmath611 .",
    "then , for @xmath626 , using that @xmath627 and @xmath628 , @xmath629 \\\\   & \\geq & \\frac{1}{2}\\left(\\frac{n}{v\\ln m } \\right)^{\\rho_i/(1 + 2|\\rho_i| ) } \\left [ 1- \\exp \\left ( \\frac{-(i - j)}{2(1 + 2|\\rho_i|)(1 + 2|\\rho_j|)}\\right)\\right ] \\\\   & \\geq & \\frac{c_\\rho}{2 }   \\left(\\frac{n}{v\\ln m } \\right)^{\\rho_i/(1 + 2|\\rho_i|)}\\end{aligned}\\ ] ] where @xmath630 may be chosen as @xmath631 ."
  ],
  "abstract_text": [
    "<S> this paper presents an adaptive version of the hill estimator based on lespki s model selection method . </S>",
    "<S> this simple data - driven index selection method is shown to satisfy an oracle inequality and is checked to achieve the lower bound recently derived by @xcite . in order to establish the oracle inequality , we derive non - asymptotic variance bounds and concentration inequalities for hill estimators . these concentration inequalities are derived from talagrand s concentration inequality for smooth functions of independent exponentially distributed random variables combined with three tools of extreme value theory : the quantile transform , karamata s representation of slowly varying functions , and rnyi s characterisation for the order statistics of exponential samples . </S>",
    "<S> the performance of this computationally and conceptually simple method is illustrated using monte - carlo simulations .    </S>"
  ]
}