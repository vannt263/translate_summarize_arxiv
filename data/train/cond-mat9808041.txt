{
  "article_text": [
    "neural networks and logical reasoning systems , because they both rely on turing models of computation , are equivalent in the sense that whatever is computable in one framework must also be computable in the other . how to establish the equivalence in each particular case is however a non - trivial and interesting issue , because problems that are addressed in a simple manner in one approach turn out to be intractable in the other and vice - versa",
    ". for example , combinatorial optimization problems are efficiently handled by neural networks , whereas logical systems succumb to combinatorial explosion .",
    "conversely , for highly structured problems where appropriate heuristics are known , it is simpler and faster to use a logical reasoning system , the corresponding neural network system taking a long time to tune up because rule - based information is in general not so easy to build in the network architecture .",
    "this situation led a number of authors@xcite @xcite to propose the use of hybrid systems to solve complex problems , with a logic - like program for some parts of the process and a network for the rest . on the other hand ,",
    "some authors have dealt with the identification of the network structures that might represent the logical structures of a symbolic system@xcite @xcite @xcite @xcite @xcite @xcite @xcite @xciteand conversely with the nature of the rule - based information that may be extracted from networks .",
    "extraction of rules from networks is an issue of practical importance in the construction of expert systems from example - trained networks@xcite @xcite @xcite @xcite . on the other hand when some prior rule - based information is known about a problem but nevertheless a network implementation seems appropriate",
    ", it would be useful to have some simple rules to implement the symbolic information on the architecture of the network .",
    "neural networks take advantage of parallel vlsi hardware implementations , which largely improve the processing speed and it is not so clear how to take advantage of similar implementations for symbolic processing",
    ". therefore even for problems that are naturally stated in logic terms , it might be useful to have a translation dictionary for hardware implementation purposes .",
    "the establishment of a concise way to go back and forth between symbolic and network formulations is also welcome in learning - oriented systems and in the design of multicomputer networks .",
    "different architectures make different types of learning easier or harder to design and in multicomputer networks it is essential for algorithms and architectures to fit together as well as possible . finally and independently of the practical issues , the establishing of a compact translation dictionary between the two paradigms might be a useful step in the development of a unified language for cognitive processes .",
    "most past attempts at establishing a logic - networks dictionary concern questions of the architecture required to represent some types of logical operations or the refining of the numerical part of the knowledge base . in some cases , an extension of the usual connectionist framework is required and a full correspondence is in general not established involving all the aspects of the systems ( knowledge base , rules , inference , recursion and handling of queries ) . here",
    "we describe an attempt to establish a correspondence involving all the basic elements that are present in logical and neural network systems .",
    "recall that in a neural network one has a ( distributed ) memory on the connection strengths ( synapses ) , a learning dynamics on the synapses and an activation dynamics of the node intensities . in a logical reasoning system",
    "there is some set of ground facts about the objects in the domain , a set of rules which are potential knowledge concerning relations between the objects and an inference mechanism ( backward or forward chaining ) allowing for the extraction of further information and the answering of queries .",
    "it has been argued that trying to isolate , in a network , the structures that correspond to specific logical statements or operations is a waste of time because everything in a network ( memory , rules and inference ) is distributed everywhere and forever inseparable .",
    "this may well be true for some architectures and some classes of concepts .",
    "however even the identification of the modular structures that correspond to the logic elements is already a useful step .",
    "for example one of our conclusions here is that a natural network representation of an atomic proposition is a node with n@xmath0 order synapses . because of the universal approximation properties of neural networks , that same proposition might as well be represented with first order synapses , the proposition corresponding then to a submodule of several neurons .",
    "however the identification of the kind of minimal submodule that corresponds to a specific logical element is already a useful step .",
    "for definiteness the scope of the logical systems , that is considered is a subset of the prolog logic programming language .",
    "namely the logic system is specified by a set of _ horn clauses _ which are constructed from _ terms _ which are either _ constants _ or _ variables _ or _ structures_. a constant is sensed to represent some concrete object in the domain of the problem . it is represented in the logical system by an indecomposable elementary symbol ( an _ atom _ ) .",
    "structures are restricted to be atomic propositions of the general form @xmath1 the functor is an atom and the parameter list is any list of atoms , variables or other atomic propositions . finally a variable is an entity that can at any time be bound to any atom ( constant or functor ) .",
    "small letters will be used for the atoms and capitals for the variables .",
    "the first step is to find a network representation of the basic entities of the logical system .",
    "each atom will correspond to a _",
    "node_. therefore not only constants but also functors will be assigned a node in the network .",
    "the network nodes have output zero or one and threshold one .",
    "the nodes are connected in a network , with the inputs equipped with connection strengths ( synapses ) @xmath2 of order @xmath3@xcite @xcite @xcite if @xmath3 is the number of parameters in the parameter list of the functor . in particular",
    "a simple constant has no input .",
    "the node activation is @xmath4 where the polarization @xmath5 is zero unless there is a query involving this node .",
    "1 displays the neural network representation of a _ constant node _ and fig .",
    "2 that of a _ functor node . _",
    "otherwise , if the relation @xmath8 does not hold for these particular set of parameters the connection strength vanishes .",
    "this is the straightforward way to represent in the network the _ ground facts _ of the knowledge base , that is , the relations that are explicitly expressed among the atoms . on the other hand relations and clauses that involve variables",
    "correspond to constraints on the remaining connection strengths .",
    "this is better explained through an example .",
    "let a knowledge base be @xmath9 the ground facts ( the first three headless clauses ) imply @xmath10 all other @xmath11 and @xmath12 being set to zero .",
    "let the connection strengths take only the values zero and one .",
    "then the last three clauses in ( [ 4.4 ] ) translate into the following constraints on the remaining @xmath13 and @xmath14 connection strengths @xmath15 these constraints may be realized in the network by a parallel dynamical law .",
    "define the potential function @xmath16 where @xmath17 .",
    "now the strengths ( [ 4.6 ] ) fixed by the ground statements are considered as frozen and the others evolve in parallel according to a gradient descent dynamics @xmath18 this dynamics drives the connection strengths to values zero or one satisfying the constraints ( [ 4.7 ] ) .",
    "3 shows the dynamical evolution of the connection strenghts for the potential ( [ 4.8 ] ) .",
    "the method used in this example holds in general and for each rule of the form @xmath19 a term @xmath20 is added to the potential @xmath21 that drives the connection strengths , the sum being over all the variable terms .    we have therefore established a correspondence where atomic propositions with @xmath3 parameters correspond to @xmath22 order nodes and the rules are implemented by a parallel dynamics of the synapses . because the synapse dynamics is equivalent to a forward chaining step , the network now contains on its connection strengths all the available information about the problem .",
    "therefore information may be retrieved by simply looking at the values of the connection strengths .",
    "alternatively an explicit scheme to answer queries may be established in the following way .",
    "we may consider two different types of queries , simple queries without variables and queries with variables .",
    "in the first case , for example @xmath23 the question is to check whether the relation @xmath24 holds for the atoms @xmath25 and @xmath26 . in this case",
    "it suffices to excite the polarizations @xmath27 and @xmath28 of the nodes @xmath25 and @xmath26 and see whether the node @xmath24 lights up .",
    "the same happens for more complex queries like , for example @xmath29 where now the polarizations to be excited are @xmath30 , @xmath28 , @xmath31 and @xmath32 and the answer to the query is positive if nodes @xmath24 and @xmath8 light up simultaneously .    for queries involving variables , for example @xmath33 the question is to find the instantiations of @xmath34 , if any , that make this a true statement .",
    "the product of the connection strengths @xmath35 defines a vector @xmath36 in atom space and the non - zero entries of the vector are the instantiations that positively satisfy the query .",
    "a similar scheme holds for queries involving several variables . for example , for @xmath37 ones defines a tensor @xmath38 , the non - zeros entries of which are the positive answers to the query .",
    "with all the connection strengths established in the network by the ( forward chaining - like ) dynamics of eq.([4.9 ] ) it might be an easy matter ( depending on the type of implementation used ) to look up the non - zero entries of @xmath36 or @xmath39 .",
    "alternatively one may regard the network connection strengths as potential knowledge and generate dynamically the non - zero entries of the tensors @xmath40 with @xmath41 regarding the network structure and connection strengths as potential , as opposite to explicit knowledge , the dynamics of the query tensors defined above plays a role similar to backward chaining in logical programming .",
    "m. botta , a. giordana and r. piola ; _ refining first order theories with neural networks _",
    ", in _ foundations of intelligent systems _ , z. w. ra and a. skowron ( eds . ) pp .",
    "84 - 93 , lecture notes in artificial intelligence 1325 , springer , berlin 1997 .",
    "x. nie and q. guo ; _ renaming a set of non - horn clauses _ , in _ foundations of intelligent systems _ , z. w. ra and a. skowron ( eds . ) pp .",
    "600 - 608 , lecture notes in artificial intelligence 1325 , springer , berlin 1997 ."
  ],
  "abstract_text": [
    "<S> a correspondence is established between the elements of logic reasoning systems ( knowledge bases , rules , inference and queries ) and the hardware and dynamical operations of neural networks . </S>",
    "<S> the correspondence is framed as a general translation dictionary which , hopefully , will allow to go back and forth between symbolic and network formulations , a desirable step in learning - oriented systems and multicomputer networks .    in the framework of horn clause logics </S>",
    "<S> it is found that atomic propositions with n arguments correspond to nodes with n - th order synapses , rules to synaptic intensity constraints , forward chaining to synaptic dynamics and queries either to simple node activation or to a query tensor dynamics . </S>"
  ]
}