{
  "article_text": [
    "the biplot method @xcite is a simultaneous graphical representation of the rows and columns of a data matrix . in practice ,",
    "biplot fitting occurs either by computing the singular value decomposition ( svd ) of the data matrix or by performing an alternating regressions procedure @xcite .",
    "@xcite fit the biplot by alternating a regression and a calibration step , essentially equivalent to the alternating regressions .",
    "@xcite use the term interpolation rather than calibration . for data with distributions from the exponential family , @xcite , describes `` bilinear regression '' as a method to estimate biplot parameters , but the procedure have never been implemented and the geometrical properties of the resulting representations have never been studied .",
    "method is called `` external logistic biplot '' .",
    "@xcite proposes principal components analysis for binary data based on an alternate procedure in which each iteration is performed using iterative majorization and @xcite extends the procedure for sparse data matrices , none of those describe a biplot representation for binary data .",
    "@xcite propose a biplot representation based on logistic responses called `` logistic biplot '' that is linear , the paper studies the geometry of this kind of biplots and uses a estimation procedure that is slightly different from gabriel s method .",
    "a heuristic version of the procedure for large data matrices in which scores for individuals are calculated with an external procedure as principal components analysis is described in @xcite .    when data are nominal , there are many techniques to deal with it , some of them see the problem from a factor analytic point of view to obtain latent factors that explain the correlation among variables , others as some kind of non - parametric approximations to explore the similarities among individuals ( principal coordinates analysis or multidimensional scaling ) but there is a lack of general exploratory techniques for the simultaneous representation of individuals and variables except multiple correspondence analysis , based on the chi - squared distance , that is not always adequate to describe similarities among individuals and correlations among variables . as we will see , it is possible to combine the factor analytic approach with the exploratory point of view to obtain a simultaneous representation of individuals and variables ( biplot ) that helps to explore the information provided by the data . in this paper",
    "we propose `` nominal logistic biplots '' that share characteristics from the previously mentioned techniques ; on the one hand is a procedure for dimension reduction , explaining the correlation among nominal variables with a reduced number of latent factors and on the other hand can serve as a exploratory biplot technique .",
    "nominal logistic biplots represent the rows of a data matrix as points on a reduced dimension representation ( usually 2 or 3 ) and variables as prediction regions ( convex polygons ) , in the same way as is done in @xcite for multiple correspondence analysis . for mca",
    "the category points are calculated first and then the prediction regions are obtained as regions of a voronoi diagram ; in this case the prediction regions are obtained first by nominal logistic regression that defines tessellations of the space ; the problem is then finding the voronoi diagram that is the closest to the `` logistic tessellation '' and a set of generators for such a diagram are the category points , the main advantage of doing so is that the interpretation of the biplot is done in terms of distances , for each individual the predicted category is the closest to it on the biplot .",
    "there are several candidate methods for parameter estimation :    * * alternated generalized regressions and interpolations . *",
    "( joint maximum likelihood , @xcite ) . *",
    "* marginal maximum likelihood * ( as in item response theory , @xcite ) . * * external logistic biplots * : heuristic approach for big data matrices .",
    "( logistic fits on the principal coordinates , @xcite ) .    in the context of binary logistic biplots",
    "the first two procedures are particularly useful when the number of individuals ( companies ) is higher than the number of variables ( indicators ) , being the second more stable for cases with a high number of individuals .",
    "the third method is more useful when the number of indicators is higher than the number of companies , although it can be applied in any case . in this paper",
    "we have chosen a version of the second method .",
    "final estimation of the variable parameters where calculated using an algorithm developed for this paper , standard logistic regressions on the scores provided by mirt or by principal coordinates analysis .",
    "in section 2 we describe linear and logistic biplots as a basis for the development of the nominal case .",
    "section 3 describes the model and its main geometrical characteristics .",
    "section 4 presents an algorithm to obtain the category points for each variable .",
    "section 5 applies the nominal logistic biplot to a classical set of data and section 6 concludes the paper with a discussion and some suggestions for further research .",
    "let @xmath0 be a data matrix containing the measures of @xmath1 variables ( continuous ) on @xmath2 individuals .",
    "a s - dimensional biplot is a graphical representation of a data matrix @xmath3 by means of markers ( points or vectors ) @xmath4 for its rows and markers @xmath5 for its columns , in such a way that the product @xmath6 approximates the element @xmath7 as close as possible . arranging the markers as row vectors in two matrices @xmath8 and @xmath9 , the approximation of @xmath3",
    "can be written as @xmath10 .",
    "although the classical biplot is well known , we include here a short description , in terms of alternating regressions , related to our proposal .",
    "the most typical way to obtain the biplot is from the singular value decomposition .",
    "let @xmath11 , there exists a factorization of the form @xmath12 where @xmath13 is an @xmath14 unitary matrix , @xmath15 is an @xmath16 diagonal matrix with non - negative real numbers on the diagonal , and @xmath17 an @xmath18 unitary matrix .",
    "such a factorization is called the singular value decomposition of @xmath3 .",
    "the diagonal entries @xmath19 of @xmath15 are known as the singular values of @xmath3 , and are placed in decreasing order , and the columns @xmath20 and @xmath21 of @xmath13 and @xmath17 are known as left and right singular vectors .",
    "the _ singular value decompositions _ are closely related to the _ eigen decompositions _ , the columns of @xmath13 are the eigenvectors of @xmath22 , the columns @xmath17 the eigenvectors of @xmath23 and the diagonal elements of @xmath15 are the squared roots of the non - null eigenvalues of both matrices ( that are the same ) .    it is known that the best @xmath24 approximation of @xmath3 is given by its first",
    "_ s _ singular values and vectors @xmath25    from he svd it is easy to obtain a factorization in the biplot form with the desired restriction taking @xmath26 with @xmath27 , as row and column coordinates respectively .",
    "this will be referred in the later as pca - biplot or classical biplot .",
    "for example , with @xmath28 , @xmath8 are the coordinates of individuals on the principal components and @xmath9 are the eigenvectors of the covariance matrix",
    ".    there is another way of obtaining biplots from alternated regressions .",
    "if we consider the row markers @xmath8 , as fixed , the column markers can be computed by regression @xmath29 in the same way , fixing @xmath9 , @xmath8 can be obtained as @xmath30 alternating the steps ( [ reg1 ] ) and ( [ reg2 ] ) the product converges to the svd .",
    "the algorithm can then be completed with an orthogonalization step to ensure the uniqueness of its solution .",
    "the regressions in ( [ eqsvd ] ) and ( [ eqsvd2 ] ) can be separated for each row and column of the data matrix .",
    "this symmetrical process is commonly used to adjust bilinear ( or bi - additive ) models with symmetrical roles for rows and columns . for a data matrix of individuals by variables , the roles of rows and columns are non - symmetrical , nevertheless the algorithm is still valid and is interpreted as a two - step process , alternating a regression step and an interpolation / calibration step .",
    "the regression step adjusts a separate linear regression for each column ( variable ) and the interpolation step interpolates an individual using the column markers as the reference .",
    "geometry of the interpolation step is described in @xcite .",
    "let @xmath0 be a data matrix in which the rows correspond to i individuals and the columns to j binary characters .",
    "let @xmath31 the expected probability that the character @xmath32 be present at individual @xmath33 , and @xmath34 the observed probability , either 0 or 1 , resulting in a binary data matrix .",
    "the s - dimensional logistic biplot in the @xmath35 scale is formulated as @xmath36 where @xmath37 and @xmath38 , @xmath39 , are the model parameters used as row and column markers respectively .",
    "the model is a generalized ( bi)linear model having the @xmath40 as a link function . in terms of probabilities rather than @xmath41 @xmath42 in matrix form , @xmath43 where @xmath44 is the matrix of expected probabilities",
    ", @xmath45 is a vector of ones and @xmath46 is the vector containing intercepts that have been added because it is not possible to center the data matrix in the same way as in linear biplots .",
    "the intercepts are the displacements of centroids in the same way as it is the first ordination axis in correspondence analysis .",
    "the model is a latent trait model for binary data , being the row coordinates the scores of individuals on the latent trait .",
    "although the biplot in the logit scale may be useful , it would be more interpretable in a probability scale .",
    "the points predicting different probabilities are on parallel straight lines on the biplot ; this means that predictions on the logistic biplot are made in the same way as on the linear biplots , i. e. , projecting a row marker @xmath47 onto a column marker @xmath48 .",
    "( see @xcite , @xcite ) .",
    "the model in ( [ binlogbip ] ) is also a latent trait or item response theory model , in that ordination axes are considered as latent variables that explain the association between the observed variables . in this framework",
    "we suppose that individuals respond independently to variables , and that the variables are independent for given values of the latent traits . with these assumptions",
    "the likelihood function is @xmath49 taking the logarithm of the likelihood function yields @xmath50 } }    \\label{binloglikelyhood}\\ ] ]    for @xmath8 fixed , ( [ binloglikelyhood ] ) can be separated into @xmath1 parts , one for each variable , @xmath51 } } \\right)}\\ ] ] maximizing each @xmath52 is equivalent to performing a standard logistic regression using the @xmath53 column of @xmath3 as a response and the columns of @xmath8 as regressors . in the same way the probability function can be separated into several parts , one for each row of the data matrix , @xmath54 .",
    "binary logistic biplots can be calculated using the package multbiplot @xcite .",
    "let @xmath0 be a data matrix containing the values of @xmath1 nominal variables , each with @xmath55 @xmath56 categories , for @xmath2 individuals , and let @xmath57 be the corresponding indicator matrix with @xmath58 columns .",
    "the last ( or the first ) category of each variable will be used as a baseline .",
    "let @xmath59 denote the expected probability that the category @xmath60 of variable @xmath32 be present at individual @xmath33 . a multinomial logistic latent trait model with @xmath61 latent traits , states that the probabilities are obtained as @xmath62 using the last category as a baseline in order to make the model identifiable ,",
    "the parameter for that category are restricted to be 0 , i.e. , @xmath63 , @xmath64.the model can be rewritten as @xmath65 with this restriction we assume that the log - odds of each response ( relative to the last category ) follows a linear model @xmath66 where @xmath67 and @xmath68 are the model parameters . in matrix form , @xmath69 where @xmath70 is the matrix containing the expected log - odds , defines a biplot for the odds . although the biplot for the odds may be useful , it would be more interpretable in terms of predicted probabilities and categories",
    ". this biplot will be called `` nominal logistic biplot '' , and it is related to the latent nominal models in the same way as classical linear biplots are related to factor or principal components analysis or binary logistic biplots are related to the item reponse theory or latent trait analysis for binary data .",
    "the points predicting different probabilities are no longer on parallel straight lines ( see the figure 1 with the response surfaces ) ; this means that predictions on the logistic biplot are not made in the same way as in the linear biplots , the surfaces define now prediction regions for each category as shown in the graph .",
    "suppose we have a two - dimensional representation in which the row coordinates are defined by the first two columns of @xmath71 in ( [ nominalodds ] ) , let s call @xmath72 the space generated by those columns .",
    "equations ( [ nominalprob ] ) , ( [ nominalprob2 ] ) and ( [ nominalodds ] ) define a set of probability response surfaces ( one for each category and each variable ) ( figure [ curvaslog ] ) that are no longer sigmoid as in the binary case ( @xcite ) .",
    "this means that the level curves are no longer straight lines and then , prediction of probabilities is not made by projection as in the usual linear biplots .",
    "figure  [ fig2:b ] shows the level curves for probability 0.5 and a hypothetical variable with four categories .",
    "we will show that in this case the predicted probabilities , for each variable , define a set of convex polygons that can be interpreted as `` prediction '' regions in the same way as in @xcite .",
    "for each variable there are as many regions as categories and each one is formed by the set points in with the expected probability for a category is higher than the probability for the rest of categories .",
    "let @xmath73 denote the region for category @xmath32 , then it can be defined as @xmath74 the prediction regions for a hypothetical variable with four categories are shown in figure  [ fig2:c ] .",
    "it is immediate to see that the prediction regions are closely related to the level curves .",
    "+    it has to be noted that there are some cases in which some of the categories are never predicted , those will be termed * hidden categories * and should be taken into account to construct the final representation .      in the following paragraphs",
    "we will describe a procedure to obtain the prediction regions using methods taken from the computational geometry .",
    "the set of convex polygons predicting each category form a tessellation of the plane .",
    "each cell of the tessellation is delimited by a set of straight lines that correspond to points that have equal probabilities for two of the categories of the variable ( the edges ) .",
    "we consider each variable @xmath32 ( @xmath75 ) separately .",
    "each pair of response surfaces defined by ( [ nominalprob ] ) intersect in a straight line that , projected onto the space of predictors , is the set of points in which the probability of both categories is the same .",
    "those lines are the candidates to be the edges of the convex polygons defining the prediction regions .",
    "that is , we search for the set of points @xmath76 in @xmath72 such that the pair of categories @xmath60 and @xmath77 ( @xmath78 ) , have the same expected probability @xmath79 i. e. , @xmath76 is the set of points verifying :    @xmath80    then @xmath81 or @xmath82    the above equation can be written as : @xmath83 where @xmath84 and @xmath85 are generic coordinates on the dimensions of @xmath72 .",
    "each variable @xmath32 has @xmath86 of such lines as shown in figure [ fig2:b ] for a hypothetical example with four categories .",
    "+   +    except for degenerate cases , any two lines with one index in common , @xmath76 and @xmath87 , intersect in a point @xmath88 .",
    "the @xmath89 of such points are the candidates to be the vertices of the tessellation .",
    "point @xmath88 is a vertex of the tessellation if there is not a @xmath90 such that @xmath91 for @xmath92 , where @xmath93 is the expected probability of category @xmath94 at point @xmath88 , i.e. , the expected probability for one of the categories involved is the highest .",
    "if a point is a vertex of the tessellation is termed * real point * , otherwise is a * virtual point*. degenerate cases may have parallel lines but this is extremely unlikely to occur .",
    "the prediction regions @xmath73 are delimited by all the lines @xmath76 with index @xmath60 and its vertices are all the points @xmath88 with the index @xmath60 .",
    "a category is hidden when its index is not present in any of the real points .",
    "the region of the hidden category is omitted in the representation .",
    "we now define the meaning of * join * two points @xmath88 and @xmath95 as follows ( see figure [ realvirtual ] ) :    1 .",
    "two real points should be joined , if they have two indices in common , following the line @xmath76 .",
    "2 .   two virtual points are never joined .",
    "a virtual point and a real point are joined along the line @xmath76 , starting from the real point and away from the virtual point .",
    ": real point ; red @xmath96 : virtual point , scaledwidth=50.0% ]    now it is easy to adapt the algorithm described in @xcite to construct the tessellation generated by the probability responses :    1 .",
    "compute the coordinates of all @xmath89 points @xmath88 .",
    "2 .   decide if the point is real or virtual .",
    "3 .   join all pairs of points sharing two suffices , interpreting `` join '' as described before .",
    "the procedure is different from that in @xcite in two aspects : they start from a set of points @xmath97 that they call `` category points '' arising from a multiple correspondence analysis with some modifications , and then construct the tessellation from those points using distances ; we do nt have the category points and use probabilities rather than distances . the tessellation based on distances",
    "is called a voronoi diagram and is quite a popular tool in a discipline called `` computational geometry '' ; in this diagram the space is divided into a set of polygons or regions @xmath73 in such a way that points in the region are closest to @xmath98 than to any other point .",
    "the main advantage of doing so is that it provides a simple interpretation of the representation of row and column markers of the data matrix , the predicted category for each point is the corresponding to its closest `` category point '' . representing points rather than `` regions '' produces a much cleaner and easier way to interpret the graph .",
    "we have the regions but not the points and , although from a formal point of view our problem is solved , and we have a simultaneous representation of individuals and variables , it would be more convenient to have also a set of `` category points '' to interpret the biplot in terms of distances .",
    "let s call this set of points @xmath99 .",
    "this would be a fundamental contribution of our research becase the interpretation of distances among row and column points is simple and it is not an intrinsic property of most multivariate techniques as mca , except unfolding that is designed for a completely different purpose .",
    "three problems arise :    1 .",
    "is our tessellation a voronoi diagram ? .",
    "if not , is there any way to approximate it by its closest voronoi tessellation ? .",
    "3 .   given a voronoi tessellation ,",
    "is it possible to obtain a set of generators for it ? .",
    "in the next section we describe a procedure to obtain the generators given a tessellation .      the problem of testing if any convex tessellation consists of voronoi polygons and if so , obtain a set of centers or generators of the voronoi diagram , has been studied for example by @xcite and @xcite .",
    "the first paper establishes a set of equations of slope and distance that a tesselation must hold to be voronoi in such a way that solving a linear system it is possible to obtain the set of centers(figure [ centrosvor ] ) .",
    "let s see it in more detail .",
    "first consider the following result ( we will omit the index @xmath32 of the variable for simplicity ) : a tessellation of @xmath100 polygons or convex regions @xmath101 is a voronoi diagram with centres @xmath102 iff @xmath103 , i.e. , each polygon of the tessellation is the set of points that are nearer to its center than to any center of other polygon .",
    "if we consider two adjacent polygons , @xmath104 y @xmath105 , whose common edge is @xmath106 with equation @xmath107 , and contain the vertices @xmath108 and @xmath109 , let @xmath110 and @xmath111 the voronoi centers of the regions ( our `` category points '' ) .",
    "the equations of slope and distance are : @xmath112 @xmath113 where @xmath114 and @xmath115 .",
    "and @xmath111 are equidistant from the edge they share @xmath106 ( [ arista ] ) and both lie on the line perpendicular to @xmath106 ( [ perpen]),scaledwidth=70.0% ]    those equations with , for example @xmath60 edges and @xmath116 polygons form a linear system with @xmath117 equations and @xmath118 unknowns , that can be solved by least squares . in matrix form",
    "the system is    @xmath119    with @xmath120'$ ] , @xmath121'$ ] .",
    "matrices @xmath122 y @xmath123 are sparse but that is not a problem because the number of categories is usually small .",
    "calculations to obtain a solution are based on three algorithms that can produce different centers in the case that the polygons of the tessellation are not voronoi .",
    "the three methods are :    * algorithm 1 * : minimize the conditions of distance and slope , that is , search for @xmath124 , with @xmath125 the euclidean norm .",
    "* algorithm 2 * : minimize @xmath126 , subject to @xmath127 .",
    "* algorithm 3 * : minimize @xmath128 , subject to @xmath129 .    in practice , the main problem with the linear systems is the instability of the algorithms due to the ill conditioning of the matrices .",
    "@xcite treats the problem and propose some alternatives to improve the stability of the final solution .",
    "@xcite also proposes a measure of the goodness of fit , i. e. , a measure of how near is the tessellation from a true voronoi diagram . for the hypothetical example in figure [ curvaslog ]",
    "we show the result of inverting a tessellation obtained from the logistic response in figure [ perfilplanta ] , for this case the tessellation is very close to a voronoi diagram .",
    "+      although the nominal case does nt share the geometrical properties with the binary case , the alternated algorithm described in @xcite , can be easily extended replacing the binary logistic regressions by multinomial logistic regressions .",
    "the problem with this approach is that the parameters for the individuals can not be estimated when the individual has 0 or 1 in all the variables for the binary case , or all the responses are at the baseline category for the nominal case . in this paper",
    "we use a procedure that is similar to the alternated regressions method , except that the interpolation step is `` eliminated '' by considering the row parameters as incidental .",
    "the technique assumes that the scores for individuals are random effects sampled from some larger distribution .",
    "the estimation procedure is an em - algorithm that uses the gauss - hermite quadrature to approximate the integrals , considering the individual scores as missing data .",
    "more details of similar procedures can be found in @xcite or @xcite .",
    "the likelihood function is @xmath130 where @xmath131 if individual @xmath33 chooses category @xmath60 of item @xmath32 and @xmath132 otherwise .",
    "the log - likelihood is    @xmath133    if the parameters @xmath8 for individuals where known , the log - likelihood could be separated into @xmath1 parts , one for each variable    @xmath134    where @xmath135 and @xmath136 are the submatrices of parameters for the @xmath32th variable .",
    "maximizing the log - likelihood is equivalent to maximizing each part , i.e. , obtaining the parameters for each variable separately .",
    "maximizing each @xmath137 is equivalent to performing a multinomial logistic regression using the @xmath32th column of @xmath3 as response and the columns of @xmath8 as predictors .",
    "we do not describe logistic regression here because it is as a very well known procedure .",
    "it is also well - known that when the individuals for different categories are separated ( or quasi - separated ) on the space spanned by the explanatory variables , the maximum likelihood estimators does not exist ( or are unstable ) . because we are seen the biplot as a procedure to classify the set of individuals and searching for the variables responsible for it , accounting for as much of the information as possible , it is probable that , for some variables , the individual are separated and then the procedure does not work just because the solution is good .",
    "the problem of the existence of the estimators in logistic regression can be seen in @xcite , a solution for the binary case , based on the firth s method @xcite is proposed by @xcite .",
    "the extension to nominal logistic model was made by @xcite .",
    "all the procedures were initially developed to remove the bias but work well to avoid the problem of separation .",
    "here we have chosen a simpler solution based on ridge estimators for logistic regression @xcite .    rather than maximizing @xmath138",
    "we maximize @xmath139    we do nt describe here the procedure in great detail because that is also a standard procedure . changing the values of @xmath140 we obtain slightly different solutions not affected by the separation problem .    in the same way ,",
    "if parameters for variables were known , the log - likelihood could be separated into @xmath2 parts , one for each individual .",
    "@xmath141 to maximize each part we could use newton - raphson with a penalization as before . rather than that we will use expected a posteriori estimators for the individual markers . for each individual ( or response pattern ) @xmath142 , the likelihood is @xmath143 assuming a distributional form @xmath144 ( multivariate normal , for example ) the marginal distribution becomes @xmath145 the observed likelihood is @xmath146}\\ ] ] we approximate the integral by @xmath61-dimensional gauss - hermite quadrature @xmath147 the multivariate @xmath61-dimensional quadrature , * y * , has been obtained as the product of @xmath61 unidimensional quadratures @xmath148 with @xmath149 nodes each .",
    "then the marginal expected a posteriori score for individual @xmath33 at dimension @xmath150 , @xmath37 , is    @xmath151",
    "table  [ data ] , taken from @xcite , shows the observations of four variables observed on twenty farms from the dutch island of terschelling .",
    "this table is reported in @xcite and it is part of a much larger survey . it is concerned with environmental factors and different forms of farm management . we have chosen this data because it has been previously analysed in literature and can serve as a comparison with the methods proposed here .",
    ".data on four variables observed at 20 farms on the island of terschelling [ cols=\"^,^,^,^,^ \" , ]     the four graphs could be superimposed although the resulting image would be almost unreadable ( figure  [ gower96emmixed ] ) even with only four variables ; with more variables the interpretation would be very complicated .",
    "the proposed procedure for obtaining a set of category points for each variable allows for a much simpler and easy to interpret representation .",
    "the final result is shown in figure  [ gower96em ] .",
    "we can see that farms having a `` nature management '' ( nm ) are at the areas with higher moisture ( m5 ) , zero fertilizer ( co ) and hay production ( u1 ) .",
    "farms with `` scientific management '' ( sf ) are at the region with moisture m1 and m2 , high values of fertilizer ( c4 ) and intermediate grassland use ( u2 ) .",
    "hobby farms ( hf ) are associated to dry places ( m1 ) , low use of fertilizer ( c1 ) and a tendency toward u3 . farms of type bf are hidden on the prediction model because the probability of that category is never higher than the rest .    in order to compare the proposed method with mca as in @xcite , and some alternatives for estimation described here , we have estimated the model parameters using our modification of the em algorithm and the mirt package @xcite with an additional multinomial logistic regression .",
    "the prediction regions obtained for our method produce 14 incorrect classifications against the 21 obtained by mca and the 31 by mirt ( see table  [ predictions ] ) .",
    "the table shows also true and predicted categories for all the data matrix .",
    "there are no hidden categories for variable `` manuring '' but for `` moisture '' and `` management '' , categories m4 and bf , respectively , are hidden .",
    "the last value is present in farmers 2 , 10 and 11 and none of the methods is able to predict it correctly .",
    "if we analyse the combined prediction regions for all the variables with em parameter estimation , we can observe in figure  [ gower96emmixed ] that there are 28 separate convex regions . except region containing farms",
    "13 , 18 , 19 and 20 , most of the regions are small and have less points inside , emphasizing the richness of the technique for interpreting data . in the study described by @xcite , there were 16 different regions for mca but only three were clearly populated , so we obtain a finer classification of the farms .",
    "in the preceding sections we have proposed a biplot method for nominal data in which the individual are represented as points in a low - dimensional subspace and the variables are represented as `` prediction regions '' or `` category points '' for the categories of each variable .",
    "prediction regions are convex polygons that divide the representation space into as many regions as categories of the variable , except if there is some hidden category , and then define a tessellation of the space that , conveniently approximated by a voronoi diagram , provides a set of generators that can be considered as category points .",
    "the proposed representation is interpreted in terms of distances in the sense that the category predicted for each individual is defined by the closest category points . although not described here in detail , linear biplots for the log odds of each category with the baseline .",
    "a simple adaptation of an em - algorithm is proposed for estimation of model parameters .",
    "the usual alternated em algorithm is modified to include penalized ridge estimation of the logistic model parameters in order to avoid the problems produced by the separation that makes the estimators undefined .",
    "other penalized methods are the lasso for logistic regression @xcite , the firth method @xcite applied to multinomial models by @xcite .",
    "the estimators obtained from the package mirt @xcite can also be used as a start point to construct the biplot , using the factor scores but with an additional step to refit the nominal logistic model for the variable parameters .",
    "this is so because mirt is designed for item response theory , the scores are always calculated with an additional rotation but the parameters seems not to be rotated consequently . in some examples we have tried the numerical values are strange probably due to the fact that mirt does not take into account the separation problem .",
    "both , our alternated method and mirt perform better when the number of individuals are much higher than the number of variables but there are many practical problems in which this is not so , for example , trying to classify a set of individuals with the genotypes resulting from thousands of single nucleotide polymorphisms @xcite . for those cases",
    "it is probably more efficient to estimate the individual markers by principal coordinates of the matrix @xmath152 of indicators defined previously and then fitting the nominal models on the coordinates .",
    "this is not a maximum likelihood solution but it is a good approximation when the other methods are unstable .",
    "the main advantage of using maximum likelihood is that it is possible to perform hypothesis testing to compare different models , for example to select the number of dimensions to retain .",
    "the proposed method share the characteristics of `` formal '' models as item response theory or latent traits and `` descriptive '' models as mca , could even be considered also as a graphical representation of the formal model .",
    "it has to be noted that the performance of the algorithm for approximation and inversion of the tessellation crucially depends on the goodness of fit of the nominal regression .",
    "only variables with a reasonable fit should be represented on the graph .",
    "an r package containing the procedures described by this paper has been developed by the authors @xcite .",
    "demey , j. , vicente - villardon , j.  l. , galindo , m.  p.  zambrano , a. 2008 , ` identifying molecular markers associated with classification of genotypes using external logistic biplots ' , _ bioinformatics _ * 24*(24 ) ,  28322838 ."
  ],
  "abstract_text": [
    "<S> classical biplot methods allow for the simultaneous representation of individuals ( rows ) and variables ( columns ) of a data matrix . for binary data , </S>",
    "<S> logistic biplots have been recently developed . when data are nominal , linear or even binary logistic biplots are not adequate and techniques as multiple correspondence analysis ( mca ) , latent trait analysis ( lta ) or item response theory for nominal items should be used instead .    </S>",
    "<S> in this paper we extend the binary logistic biplot to nominal data . </S>",
    "<S> the resulting method is termed nominal logistic biplot , although the variables are represented as convex prediction regions rather than vectors . using the methods from computational geometry , the set of prediction regions </S>",
    "<S> is converted to a set of points in such a way that the prediction for each individual is established by its closest `` category point '' .    then interpretation is based on distances rather than on projections . </S>",
    "<S> we study the geometry of such a representation and construct computational algorithms for the estimation of parameters and the calculation of prediction regions . </S>",
    "<S> nominal logistic biplots extend both mca and lta in the sense that gives a graphical representation for lta similar to the one obtained in mca . </S>"
  ]
}