{
  "article_text": [
    "nowadays , second order analysis attracts much attention in information theory@xcite . in this type of analysis",
    ", we focus on the second leading term with the order @xmath0 in the coding length in addition to the first leading term with the order @xmath1 when the block length is @xmath1 . to discuss the finiteness of the blocklength",
    ", we need to be careful for the second leading term as well as the first leading term .",
    "the coefficient of the order @xmath0 is given as the inverse of the cummulative distribution function of the gaussian distribution dependently of the decoding error probability @xmath2 in many existing studies for the second order except for the papers @xcite .",
    "this is because the second order analysis is deeply rooted in the central limit theorem . in channel coding",
    ", the second order coefficient is given by the gaussian distribution , whose variance is given as the variance of the information density . here",
    ", the information density is given as the logarithm of the likelihood ratio between the joint distribution of the input and output random variable and their product distribution when the expectation of the logarithm of the likelihood ratio achieve the channel capacity .",
    "however , the variance of the information density is not unique in general because plural input distributions attain the channel capacity in general .",
    "so , in such a general case , the variance of the gaussian determining the second order coefficient is chosen dependently of the sign of the decoding error probability @xmath2 .",
    "recently , the two papers @xcite extended the second order analysis to the makovian case , in which , the markovian version of the central limit theorem is employed instead of the conventional central limit theorem . in particular , the paper @xcite discussed source coding for markovian source and channel coding for additive channel whose additive noise is markovian .",
    "usually , the channel coding is discussed with the message subject to the uniform distribution . however , in the real communication , the message is not necessarily subject to the uniform distribution . to resolve this problem",
    ", we often consider the channel coding with the message subject to the non - uniform distribution .",
    "such a problem is called source - channel joint coding and has been actively studied by several researchers @xcite . as a simple case , we often assume that the message is subject to the independent and identical distribution . in this case , the capacity is given as the ratio of the conventional channel capacity to the entropy of the message .",
    "several studies @xcite derived the exponential decreasing rate of the decoding error probability in this setting .",
    "recently , while wang - ingber - kochman @xcite and kostina - verd @xcite discussed the second - order coefficient in this problem , two major open problems has been remained in this topic as follows .",
    "wang - ingber - kochman @xcite derived the second order coefficient only when the variance of the information density is unique .",
    "when the variance is not unique , kostina - verd @xcite extended the lower bound of the second - order coefficient by the same method as @xcite .",
    "however , the achievability has been open problem in the general case . also , in the above special case , wang - ingber - kochman @xcite compared their second order coefficient of the joint scheme with that with the separation scheme . based on their numerical calculation , they conjectured the lower bound of the ratio of the latter to the former while the upper bound is @xmath3 .",
    "hence , the analytical proof has been remained as another open problem .    in this paper , we tackle both open problems .",
    "firstly , we derive the second - order coefficient in this problem .",
    "the obtained coefficient is strictly larger than that by kostina - verd @xcite when the variance of the information density is not unique . to characterize the second - order coefficient",
    ", we introduce a new probability distribution as a generalization of the gaussian distribution .",
    "that is , the second - order coefficient is given as the inverse of the cummulative distribution function of the new probability distribution .",
    "further , we derive this result even when the distribution of the message is markovian .",
    "secondly , we discuss the second order coefficient with the separation scheme in the above general setting .",
    "also , we analytically determine the range of the ratio between the error probabilities with the joint and separation schemes when the variance of the information density is unique . in this way , we resolve both open problems .",
    "the remaining part of this paper is organized as follows . in section [ s2 ]",
    ", we prepare several information quantities for markovian process .",
    "section [ s3 ] introduces two new distribution families . in section [ s4 ] , we discuss the joint source - channel coding in the single shot setting .",
    "then , section [ s5 ] shows our results for markovian conditional additive channel . discusses the second order rate .",
    "section [ s6 ] discusses the case with discrete memoryless channel . in section [ s7 ] ,",
    "we compare the joint source - channel scheme with the separation scheme .",
    "in this paper , we denote the random variable by a capital letter , e.g. , @xmath4 .",
    "by @xmath5 , we denote the set that the random variable @xmath4 takes values in .",
    "then , we denote the distribution of the random variable @xmath4 by @xmath6 . when we have two distributions @xmath6 and @xmath7 , we define their product distribution @xmath8 as @xmath9 .",
    "when we have two different sets @xmath5 and @xmath10 , we denote a transition matrix from @xmath5 to @xmath10 by @xmath11 .",
    "then , we define the distribution @xmath12 as @xmath13 .",
    "when @xmath5 is the same set as @xmath10 , we do not describe the subscript @xmath14 . in this case",
    ", we define the transition matrix @xmath15 on @xmath5 as @xmath16 .",
    "a transition matrix @xmath17 on @xmath5 is called _ irreducible _ when for each @xmath18 , there exists a natural number @xmath1 such that @xmath19 .",
    "an irreducible matrix @xmath17 is called _ ergodic _ when there are no input @xmath20 and no integer @xmath21 such that @xmath22 unless @xmath1 is divisible by @xmath21 .",
    "since this paper addresses the markovian processes , we prepare several information measures given in @xcite for an ergodic and irreducible transition matrix @xmath23 on @xmath24 . for this purpose ,",
    "we employ the following assumption on transition matrices , which were introduced by the paper @xcite .",
    "we assume the following condition for a transition matrix @xmath17 : @xmath25 for every @xmath26 and @xmath27 . when this condition holds , a transition matrix @xmath28 is called non - hidden ( with respect to @xmath29 ) .    when the cardinality of @xmath10 is @xmath3 , the above non - hidden condition holds .",
    "when a transition matrix @xmath17 on @xmath30 satisfies non - hidden condition with respect to @xmath29 , we define the marginal @xmath31 by @xmath32 .",
    "by @xmath33 , we denote the perron - frobenius eigenvalue of @xmath34 for real number @xmath35 .",
    "then , we define the conditional rnyi entropy for the transition matrix @xcite as @xmath36 which is often called the lower type of conditional rnyi entropy and is denoted by @xmath37 in @xcite .    taking the limit @xmath38",
    ", we define the entropy for the transition matrix @xmath39 as @xmath40 to discuss the difference of @xmath41 from @xmath42 , we introduce the varentropy for the transition matrix @xmath43 as @xmath44 } { \\theta } .",
    "\\label{vw}\\end{aligned}\\ ] ] so , we have the approximation as @xmath45 as @xmath38 . in these definitions , when the output distribution of @xmath17 does not depend on the input element , the quantities @xmath46 , @xmath42 , and @xmath47 are the same as the conventional definitions .",
    "then , we have the following proposition .    when @xmath48 and @xmath49 are subject to the markovian process generated by @xmath17 , the random variable    @xmath50 asymptotically obeys the gaussian distribution with variance @xmath47 .",
    "to describe the second order rate in the joint source - channel coding , we introduce a new type of distribution family , so called switched gaussian convolution distributions .",
    "it is known that the convolution of two gaussian distribution is also a gaussian distribution as follows .",
    "when @xmath51 is the probability density function of the gaussian distribution with average @xmath52 and variance @xmath53 , we have @xmath54 now , we consider the case when the variance of the second probability density function is switched at @xmath55 .",
    "so , we define the function @xmath56(x)$ ] as @xmath57(x)\\nonumber \\\\ : = & \\int_{-\\infty}^{x}\\varphi_{v_1}(y ) \\varphi_{v_+}(x - y ) dy + \\int_{x}^{\\infty}\\varphi_{v_1}(y ) \\varphi_{v_-}(x - y ) dy,\\end{aligned}\\ ] ] where @xmath58 and @xmath59 .",
    "taking the integral with respect to @xmath60 , we define the function @xmath61(r):=\\int_{-\\infty}^r \\psi[v_1,v_2,v_3](x ) dx$ ] , which satisfies @xmath62(r)\\nonumber \\\\ = & \\int_{-\\infty}^{r}\\varphi_{v_1}(y ) \\phi_{v_+}(r - y ) dy + \\int_{r}^{\\infty}\\varphi_{v_1}(y ) \\phi_{v_-}(r - y ) dy \\nonumber \\\\ = & \\int_{-\\infty}^{\\infty}\\varphi_{v_1}(y )   \\min\\{\\phi_{v_2}(r - y ) , \\phi_{v_3}(r - y ) \\ } dy , \\label{12 - 24}\\end{aligned}\\ ] ] where @xmath63 .",
    "we simplify @xmath64 to @xmath65 .    since the value @xmath66 goes to @xmath52(@xmath3 ) as @xmath67 goes to @xmath68(@xmath69 ) , respectively , the rhs of goes to @xmath52(@xmath3 ) as @xmath67 goes to @xmath68(@xmath69 ) , respectively , also , the value @xmath66 is monotonically increasing with respect to @xmath67 , the rhs of also is monotonically increasing with respect to @xmath67 .",
    "these facts show that @xmath61(r)$ ] is the cummulative distribution function of a probability distribution . in the following , we call this distribution the switched gaussian convolution distribution with @xmath70 , and @xmath71 .    to see the behavior of the distribution function of the switched gaussian convolution distribution , we set @xmath72 , and change the third parameter @xmath71 .",
    "then , we obtain the graph given in fig .",
    "[ f3 ] . from the definition ,",
    "we find that the maximum @xmath73(x)$ ] is realized when @xmath74 . fig .",
    "[ f3 ] shows how much @xmath75(x)$ ] decreases unless @xmath74 .",
    "now , given two parameter @xmath77 , we define another probability distribution . for this purpose",
    ", we define the function @xmath78^{-1}$ ] as @xmath79^{-1}(\\varepsilon ) : = \\max_{\\varepsilon \\ge \\varepsilon_s * \\varepsilon_c   } \\left ( \\phi_{v_1}^{-1}(\\varepsilon_s )   +   \\phi_{v_2}^{-1}(\\varepsilon_c )   \\right ) , \\label{sep , err}\\end{aligned}\\ ] ] where the product @xmath76 is defined as @xmath80 since the function @xmath78^{-1}$ ] is strictly monotone increasing , we can define the function @xmath78 $ ] as its inverse function . since it satisfies the condition of the cummulative distribution function , we define another probability distribution by the cummulative distribution function @xmath78 $ ] .",
    "we call it @xmath76-product distribution because it is defined based on the @xmath76 product",
    ".    the cummulative distribution function has the following property .    for any @xmath81 and @xmath82 , @xmath79(r )",
    "\\ge \\phi_{v_1+v_2}(r ) .",
    "\\label{sep , so , th1}\\end{aligned}\\ ] ] the equality of ( [ sep , so , th1 ] ) is attained if and only if @xmath83 is @xmath52 or @xmath69 . when @xmath84 , we also have @xmath79(r ) \\le 2 \\phi_{2(v_1+v_2)}(r ) -\\phi_{2(v_1+v_2)}(r)^2 .",
    "\\label{sep , so , th2}\\end{aligned}\\ ] ] the equality of ( [ sep , so , th2 ] ) is attained if and only if @xmath85 .",
    "the paper ( * ? ? ?",
    "* section v ) considered the function @xmath86^{-1}(\\varepsilon)/\\sqrt{1+v_2}$ ] , and gave the same statement as in a difference form as a conjecture based on numerical calculation .",
    "this conjecture had been an open problem .",
    "we first present the problem formulation by the single shot setting .",
    "assume that the message @xmath87 takes values in @xmath88 and is subject to the distribution @xmath89 .",
    "for a channel @xmath90 with input alphabet @xmath5 and output alphabet @xmath10 , a channel code @xmath91 consists of one encoder @xmath92 and one decoder @xmath93 .",
    "the average decoding error probability is defined by @xmath94 : = \\sum_{m \\in { \\cal m } } p_m(m ) w_{y|x}(\\{b:{\\mathsf{d}}(b ) \\neq m \\}|{\\mathsf{e}}(m ) ) .",
    "\\end{aligned}\\ ] ] for notational convenience , we introduce the minimum error probability under the above condition : @xmath95 .",
    "\\end{aligned}\\ ] ]        we introduce several lemmas for the case when @xmath88 is the set of messages to be sent , @xmath89 is the distribution of the messages , and @xmath11 is the channel from @xmath5 to @xmath10 .",
    "we have the following single - shot lemma for the direct part .",
    "* lemma 3.8.1 ) for any constant @xmath96 and for any @xmath97 , there exists a code @xmath98 such that @xmath94 \\le ( p_m \\times p_x \\times w_{y|x } ) \\ { ( p_m \\times p_x \\times w_{y|x } ) ( m , x , y ) \\le c ( p_x \\times \\bar{w}_{y } ) ( x ,",
    "y ) \\}+ \\frac{1}{c } , \\label{si , di , le1}\\end{aligned}\\ ] ] where @xmath99 and @xmath100",
    ".    from above proposition , we obviously have following corollary .",
    "@xmath101      now , we proceed to the case when the channel is conditional additive .",
    "assume that @xmath5 is a module and @xmath10 is given as @xmath102 .",
    "then , the channel @xmath17 is called conditional additive @xcite when there exists a joint distribution @xmath103 such that @xmath104 then we simplify ( [ 3 ] ) of corollary [ co11 ] to the following lemma .",
    "when the channel is conditional additive channel , it follows that @xmath105    by setting that @xmath6 is the uniform distribution and choosing the random variables @xmath106 and @xmath107 to the right hand side of ( [ 3 ] ) , we have @xmath108 where @xmath109 .",
    "hence , ( [ 3 ] ) can be simplified to @xmath110        firstly , combining the idea of meta converse @xcite and ( * ? ? ?",
    "* lemma 4 ) and the general converse lemma for the joint source and channel coding ( * ? ? ?",
    "* lemma 3.8.2 ) , we obtain the following lemma for the single shot setting . the following lemma is the same as ( * ? ? ?",
    "* lemma 3.8.2 ) when @xmath111 is @xmath112 .    for any constant @xmath96 , any code @xmath98 and any distribution @xmath113 on @xmath29 , we have @xmath114    first , we set @xmath115 for each @xmath116",
    ", we define @xmath117 also , for decoder @xmath118 and each @xmath119 , we define @xmath120 in addition , we define @xmath121 so that @xmath122 using this , we define @xmath123    then , @xmath124 . \\label{sbcl1p1}\\end{aligned}\\ ] ] the last equality follows since the error probability can be written as @xmath94 = \\sum_{(m , x ) \\in { \\cal m , x } } \\sum_{y \\in { \\cal d}^c(m)}p_{mx}(m , x ) w_{y|x}(y|x ) .",
    "\\end{aligned}\\ ] ] we notice here that @xmath125 for @xmath126 . by substituting this into ( [ sbcl1p1 ] ) ,",
    "the first term of ( [ sbcl1p1 ] ) is @xmath127 which implies ( [ 2 ] ) .",
    "now , we proceed to the conditional additive case given in . applying to the conditional additive case , we obtain following lemma .    for arbitrary distribution @xmath128",
    ", we have @xmath129    for some @xmath130 , we substitute @xmath131 to . then , the first term of the right hand side of ( [ e ] ) is @xmath132 so , we obtain ( [ e ] ) .",
    "firstly , we give general notations for channel coding when the message obeys markovian process .",
    "we assume that the set of messages is @xmath133 .",
    "then , we assume that the message @xmath134 is subject to the markov process with the transition matrix @xmath135 .",
    "we denote the distribution for @xmath136 by @xmath137 .",
    "now , we consider very general sequence of channels with the input alphabet @xmath138 and the output alphabet @xmath139 . in this case , the transition matrix as @xmath140 .",
    "then , a channel code @xmath91 consists of one encoder @xmath141 and one decoder @xmath142 .",
    "then , the average decoding error probability is defined by @xmath143 : = \\sum_{m^k \\in { \\cal m}^k } p_{m^k}(m^k ) w_{y^n| x^n } ( \\{y^n:{\\mathsf{d}}(y^n ) \\neq m^k \\}|{\\mathsf{e}}(m^k ) ) .",
    "\\end{aligned}\\ ] ] for notational convenience , we introduce the error probability under the above condition : @xmath144 .",
    "\\end{aligned}\\ ] ] when there is no possibility for confusion , we simplify it to @xmath145 .",
    "instead of evaluating the error probability @xmath146 for given @xmath147 , we are also interested in evaluating @xmath148 for given @xmath149 .      in this section",
    ", we address an @xmath1-fold markovian conditional additive channel @xcite .",
    "that is , we consider the case when the joint distribution for the additive noise obeys the markov process . to formulate our channel",
    ", we prepare notations .",
    "consider the joint markovian process on @xmath102 .",
    "that is , the random variables @xmath150 and @xmath151 are assumed to be subject to the joint markovian process defined by the transition matrix @xmath152 .",
    "we denote the joint distribution for @xmath153 and @xmath154 by @xmath155 .",
    "now , we assume that @xmath5 is a module , and consider the channel with the input alphabet @xmath138 and the output alphabet @xmath156 .",
    "the transition matrix for the channel @xmath157 is given as @xmath158 for @xmath159 and @xmath160 .",
    "also , we denote @xmath161 by @xmath67 . in the following discussion ,",
    "we use the channel capacity @xmath162 , which is shown in @xcite . in this case , we denote the average error probability @xmath163 $ ] and the minimum average error probability @xmath164 by @xmath165 $ ] and @xmath166 , respectively . then , we denote the maximum size @xmath167 by @xmath168 . when we have no possibility for confusion , we simplify them to by @xmath169 $ ] , @xmath170 , and @xmath171 , respectively .    in the following discussion , we assume assumption 1 or 2 for the joint markovian process described by the transition matrix @xmath152 . the paper @xcite derives the single - letterized channel capacity under assumption 1 . among author",
    "s knowledge , the class of channels satisfying assumption 1 is the largest class of channels whose channel capacity is known .",
    "when @xmath172 is singleton and the channel is the noiseless channel given by identity transition matrix @xmath173 our problem becomes the source coding with markovian source . in this case , the memory size is equal to the cardinality @xmath174 , we denote the minimum error probability @xmath175 by @xmath176 .      for any @xmath177",
    ", it holds that @xmath178 in other words , @xmath179    theorem [ t10 ] yields the following corollary .    for @xmath180 , we have @xmath181    we assume that @xmath182 .",
    "+ ( [ a ] ) implies that @xmath183 we set @xmath184 so that @xmath185 i.e. , @xmath186 applying the central limit theorem for markovian process , we find that the rhs of goes to @xmath2 , which implies that @xmath187 hence , we have @xmath188 on the other hands , ( [ e ] ) implies that @xmath189 we set @xmath184 so that @xmath190 i.e. , @xmath191 applying the central limit theorem for markovian process , we find that the rhs of goes to @xmath2 , which implies @xmath192 thus , we have @xmath193    similar to the above two cases , we can recover the result of data compression with the second order regime .",
    "in this section , we address the @xmath1-fold discrete memoryless channel with the input system @xmath138 and the output system @xmath139 hence , we adopt the same assumptions given in section [ s5 ] for the message source .",
    "the difference from section [ s5 ] is the form of channel .",
    "given a transition matrix @xmath194 , the transition matrix for the channel @xmath195 is given as @xmath196 where @xmath197 and @xmath198 .    in this case , we denote the average error probability @xmath163 $ ] and the minimum average error probability @xmath164 by @xmath199 $ ] and @xmath200 , respectively .",
    "then , we denote the maximum size @xmath167 by @xmath201 .",
    "when we have no possibility for confusion , we simplify them to @xmath202 $ ] , @xmath203 , and @xmath204 , respectively .    for the latter discussion ,",
    "we prepare the mutual information as @xmath205 where @xmath206 . then",
    ", we define its variance version as @xmath207 and we also define channel capacity @xmath208 .      we define that @xmath209 and @xmath210 we also define the distribution achieving above maximum and minimum as : @xmath211    then , we have following theorem .    for any @xmath212",
    ", we have @xmath213 where @xmath214(r).\\end{aligned}\\ ] ] in other words , we have @xmath215(\\epsilon).\\end{aligned}\\ ] ]      the paper @xcite discussed the same problem when the message is subject to the independent and identical distribution , and @xmath216 .",
    "when the condition @xmath216 holds ,    @xmath217^{-1}(\\epsilon)$ ] becomes @xmath218 , which given by the gaussian distribution .",
    "when the message is subject to the independent and identical distribution , as a simple generalization of the direct part of @xcite , kostina - verd @xcite showed the inequality @xmath219 where @xmath220 is defined as @xmath221 hence , we call the bound @xmath220 kostina - verd bound even for a general transition matrix @xmath222 . to compare our tight bound @xmath223 and kostina - verd bound @xmath224 for a general transition matrix @xmath222 , we focus on the ratio @xmath225 .",
    "the property implies the inequality @xmath226 here , the equality is attained if and only if @xmath227 or @xmath228 , which means that if source is non - uniform , a gap between @xmath229 and @xmath230 produces the good effect for joint source - channel coding .",
    "furthermore , we will evaluate the ratio of two error probability @xmath231 . for following discussion ,",
    "we define a notation for a value of variance @xmath232 as : @xmath233(r ) : = \\int _ { r } ^ { \\infty }      \\phi_{\\sigma^2 } ( r - x )      \\varphi_{\\frac { c } { h^{w_s}(m ) } v^ { w_s } ( m ) } ( x )   dx\\\\ & \\beta[\\sigma^2](r ) : = \\int _ { -\\infty } ^ { r } \\phi_{\\sigma^2 } ( r - x ) \\varphi_{\\frac { c } { h^{w_s}(m ) } v^ { w_s } ( m ) } ( x )   dx.\\end{aligned}\\ ] ] we can find that for any @xmath234 , @xmath235(r ) $ ] is monotonically increasing function of @xmath232 , and @xmath236(r ) $ ] is monotonically decreasing function of @xmath232 .",
    "additionally , we define @xmath237(r ) = \\frac { 1 } { 2 } \\phi _ { \\frac { c } { h^{w_s}(m ) } v^ { w_s } ( m ) } ( -r)\\\\ & \\alpha_{min}(r ) : = \\lim_{\\sigma^2 \\to 0}\\alpha[\\sigma^2](r ) = 0\\\\ & \\beta_{max}(r ) : = \\lim_{\\sigma^2 \\to 0}\\beta[\\sigma^2](r ) = \\phi _ { \\frac { c } { h^{w_s}(m ) } v^ { w_s } ( m ) } ( r)\\\\ & \\beta_{min}(r ) : = \\lim_{\\sigma^2 \\to \\infty}\\beta[\\sigma^2](r ) = \\frac { 1 } { 2 } \\phi _ { \\frac { c } { h^{w_s}(m ) } v^ { w_s } ( m ) } ( r).\\end{aligned}\\ ] ]    first , we discuss the case of @xmath238 .",
    "then , we have following .",
    "@xmath239(r ) } { \\psi \\left [      \\frac { c } { h^{w_s}(m ) } v^ { w_s } ( m ) , v^*_{+ } ( w_{y|x } ) , v^*_{- } ( w_{y|x } )      \\right ] ( r ) } = \\frac { \\alpha[v^*_{- } ( w_{y|x})](r ) + \\beta[v^*_{- } ( w_{y|x})](r ) }      { \\alpha[v^*_{- } ( w_{y|x})](r ) + \\beta[v^*_{+ } ( w_{y|x})](r ) } .\\label{dis , so , re}\\end{aligned}\\ ] ] we can evaluate the right hand side of ( [ dis , so , re ] ) as : @xmath240(r ) + \\beta[v^*_{- } ( w_{y|x})](r ) } { \\alpha[v^*_{- } ( w_{y|x})](r ) + \\beta[v^*_{+ } ( w_{y|x})](r ) } \\le & \\frac { \\alpha[v^*_{- } ( w_{y|x})](r ) + \\beta[v^*_{- } ( w_{y|x})](r ) } { \\alpha[v^*_{- } ( w_{y|x})](r ) + \\beta_{min}(r ) } \\nonumber \\\\ = & 1 + \\frac { \\beta[v^*_{- } ( w_{y|x})](r ) - \\beta_{min}(r ) } { \\alpha[v^*_{- } ( w_{y|x})](r ) + \\beta_{min}(r ) } .\\label{dis , so , re1}\\end{aligned}\\ ] ] because @xmath241(r ) - \\beta_{min}(r ) > 0",
    "$ ] , the right hand side of ( [ dis , so , re1 ] ) is monotonically decreasing as to @xmath230 .",
    "hence we have @xmath240(r ) + \\beta[v^*_{- } ( w_{y|x})](r ) } { \\alpha[v^*_{- } ( w_{y|x})](r ) + \\beta_{min}(r ) } \\le \\lim_{v^*_{- } ( w_{y|x } ) \\to 0 } \\frac { \\alpha[v^*_{- } ( w_{y|x})](r ) + \\beta[v^*_{- } ( w_{y|x})](r ) } { \\alpha[v^*_{- } ( w_{y|x})](r ) + \\beta_{min}(r ) } = 2.\\label{dis , so , re2}\\end{aligned}\\ ] ] from ( [ dis , so , re ] ) , ( [ dis , so , re1 ] ) and ( [ dis , so , re2 ] ) , we can finally obtain @xmath242    on the other hands , we consider the case of @xmath243 .",
    "then we have @xmath244(r ) } { \\psi \\left [      \\frac { c } { h^{w_s}(m ) } v^ { w_s } ( m ) , v^*_{+ } ( w_{y|x } ) , v^*_{- } ( w_{y|x } )      \\right ] ( r ) } = \\frac { \\alpha[v^*_{+ } ( w_{y|x})](r ) + \\beta[v^*_{+ } ( w_{y|x})](r ) } { \\alpha[v^*_{- } ( w_{y|x})](r ) + \\beta[v^*_{+ } ( w_{y|x})](r ) } .\\label{dis , so , re3}\\end{aligned}\\ ] ] we can evaluate the right hand side of ( [ dis , so , re3 ] ) as @xmath245(r ) + \\beta[v^*_{+ } ( w_{y|x})](r ) } { \\alpha[v^*_{- } ( w_{y|x})](r ) + \\beta[v^*_{+ } ( w_{y|x})](r ) } \\le & \\frac { \\alpha[v^*_{+ } ( w_{y|x})](r ) + \\beta[v^*_{+ } ( w_{y|x})](r ) } { \\alpha_{min}(r ) + \\beta[v^*_{+ } ( w_{y|x})](r ) } \\nonumber \\\\ = & 1 + \\frac { \\alpha[v^*_{+ } ( w_{y|x})](r ) - \\alpha_{min}(r ) } { \\alpha_{min}(r ) + \\beta[v^*_{+ } ( w_{y|x})](r ) } .\\label{dis , so , re4}\\end{aligned}\\ ] ] because @xmath246(r ) - \\alpha_{min}(r ) > 0 $ ] , we can find that the right hand side of ( [ dis , so , re4 ] ) is monotonically increasing as to @xmath229 .",
    "so we have @xmath245(r ) + \\beta[v^*_{+ } ( w_{y|x})](r ) } { \\alpha_{min}(r ) + \\beta[v^*_{+ } ( w_{y|x})](r ) } \\le & \\lim_{v^*_{+ } ( w_{y|x } ) \\to \\infty } \\frac { \\alpha[v^*_{+ } ( w_{y|x})](r ) + \\beta[v^*_{+ } ( w_{y|x})](r ) } { \\alpha_{min}(r ) + \\beta[v^*_{+ } ( w_{y|x})](r ) } \\nonumber \\\\ = & \\left ( \\phi _ { \\sqrt{v^ { w_s } ( m ) } } ( r ) \\right ) ^{-1}.\\label{dis , so , re5}\\end{aligned}\\ ] ] from ( [ dis , so , re3 ] ) , ( [ dis , so , re4 ] ) and ( [ dis , so , re5 ] ) , we finally obtain @xmath247 the equality of ( [ dis , so , re6 ] ) and ( [ dis , so , re7 ] ) are both attained if and only if @xmath248 and @xmath249 .      in the direct part , we give a code composed on two parts because the conventional random coding method can not attain the bound @xmath250 .",
    "to attain the bound @xmath250 , we need to choose the distribution on @xmath138 deciding the random coding dependently of the message to be sent .",
    "hence , we divide the set of messages into two sets , and we decide our code dependently of the set the message belongs to .",
    "to realize this type code , we employ a code composed of two parts .",
    "the first part informs which set the message belongs to .",
    "the second part sends which element of the chosen set to be transmitted . using proposition [ si , di , le ]",
    ", we show that this code attain the bound @xmath250 .    _",
    "step(0 ) : _ notations . throughout this proof including the converse part ,",
    "we simplify @xmath90 as @xmath251 and @xmath252 as @xmath253 .",
    "so , @xmath254 is a random variable on @xmath255 .",
    "then , we define following random variables .",
    "@xmath256    _ step ( i ) : _ in this step , we describe our code used in this proof .",
    "this code consists of two parts as follows . in the first part ,",
    "the sender calculates a value of @xmath257 which corresponds to a message @xmath258 .",
    "then , he tells the receiver whether @xmath259 or @xmath260 . in the second part",
    ", they communicate each other by using the code depending on the result of the first part .",
    "now , we give the first part , in which , the message size of the code is @xmath261 and the number of channel use is @xmath262 .",
    "that is , the first is the code @xmath263 to tell whether @xmath264 or not . to give the first part",
    ", we define the encoder @xmath265 as @xmath266 the decoder @xmath267 is defined as @xmath268 then , we denote the error probability of the code @xmath269 by @xmath270 , which is represented as @xmath271 note that @xmath272 when @xmath273 .",
    "as the second part , we define a code to send the massage @xmath258 . from proposition [ si , di , le ] , we define a code @xmath274 so that @xmath275 \\nonumber \\\\ & \\le \\left ( p_{m^k |s(m^k ) \\le r } \\times ( p_x^+)^{\\times",
    "n } \\times w_{y^n|x^n } \\right ) \\left\\ { \\log p_{m^k |s(m^k ) \\le r } ( m^k )   + \\log \\frac { w_{x^{n}}(y^{n } ) } { w_{y^{n}}(y^{n } ) } \\le \\log c \\right\\ } + \\frac{1}{c},\\label{dis , so , di4}\\end{aligned}\\ ] ] where @xmath276 is a conditional probability distribution of @xmath277 , under the condition of @xmath278 .    on the other hands , from proposition [ si , di , le ]",
    ", we define a code @xmath279 so that @xmath280 \\nonumber \\\\ & \\le \\left ( p_{m^k |s(m^k ) > r } \\times ( p_x^-)^{\\times",
    "n } \\times w_{y^n|x^n } \\right ) \\left\\ { \\log p_{m^k |s(m^k ) > r } ( m^k )   + \\log \\frac { w_{x^{n}}(y^{n } ) } { w_{y^{n}}(y^{n } ) } \\le \\log c \\right\\ } + \\frac{1}{c},\\label{dis , so , di6}\\end{aligned}\\ ] ] where @xmath281 is a conditional probability distribution of @xmath277 , under the condition of @xmath282 .    using the above preparation",
    ", we define a code @xmath283 for whole protocol .",
    "we set @xmath284 to be @xmath0 and @xmath285 to be @xmath286 .",
    "then , we choose @xmath287 as @xmath288 which implies that @xmath289 then , for the encoder , we define @xmath290 as @xmath291 also we define the decoder @xmath292 as @xmath293    _ step ( ii ) : _ in this step , we will prove that @xmath294 \\nonumber \\\\ & \\le p_{m^k } \\",
    "{ s(m^k ) \\le r \\ }       \\big(p_{m^k |s(m^k ) \\le r } \\times ( p_x^+)^{\\times n } \\times w_{y^n|x^n } \\big )      \\left\\ {       s(m^k )      -      c(x^{n},y^{n } )      \\le r      \\right\\ } \\nonumber \\\\ & \\quad + p_{m^k } \\ { s(m^k ) > r \\ }      \\big ( p_{m^k |s(m^k ) > r } \\times ( p_x^-)^{\\times",
    "n } \\times w_{y^n|x^n } \\big ) \\left\\ {   s(m^k ) - c(x^{n},y^{n } ) \\le r \\right\\ } + \\delta_{\\sqrt{n}}+o(1).\\label{eq11 - 1}\\end{aligned}\\ ] ]    on the code @xmath295 , an error happens if an error occurs on the code @xmath296 , or an error does nt occur on the code @xmath296 and an error occurs on the code @xmath297 .",
    "so , the error probability of the code @xmath295 , i.e. , @xmath298 $ ] , is evaluated as @xmath294\\nonumber \\\\ & \\le p_{m^k } \\ { s(m^k )",
    "\\le r \\ } { { \\mathrm{p}}_{{\\mathrm{js}}}}[\\phi^+|p_{m^k |s(m^k ) \\le r } , w_{y^{n}|x^{n } } ]   + p_{m^k } \\ { s(m^k ) > r \\ } { { \\mathrm{p}}_{{\\mathrm{js}}}}[\\phi^-|p_{m^k |s(m^k ) > r } , w_{y^{n}|x^{n } } ] + \\delta_n .",
    "\\label{dis , so , di2}\\end{aligned}\\ ] ]    we set @xmath299 in ( [ dis , so , di4 ] ) .",
    "when @xmath259 , @xmath300 .",
    "so , applying the central limit theorem for markovian process ( proposition [ clt ] ) to random variable @xmath301 , we have @xmath302 which implies @xmath303 since @xmath304 , due to ( [ dis , so , di9 ] ) , we can rewrite ( [ dis , so , di4 ] ) as : @xmath305 \\nonumber \\\\ & \\le \\big ( p_{m^k |s(m^k ) \\le r } \\times ( p_x^+)^{\\times",
    "n } \\times w_{y^n|x^n } \\big ) \\left\\ {   s(m^k ) -   c(x^{n},y^{n } ) \\le r \\right\\}+o(1).\\label{dis , so , di5}\\end{aligned}\\ ] ]    on the other hands , we set @xmath299 in ( [ dis , so , di6 ] ) .",
    "when @xmath260 , we have @xmath306 .",
    "so , applying the central limit theorem for markovian process to random variable @xmath301 , we obtain @xmath307 which implies @xmath308 so , we can rewrite ( [ dis , so , di6 ] ) as : @xmath309 \\nonumber \\\\      & \\le      \\big ( p_{m^k |s(m^k ) \\le r } \\times ( p_x^-)^{\\times n } \\times w_{y|x } \\big )      \\left\\ {        s(m^k )      -       c(x^{n},y^{n } )      \\le r      \\right\\}+o(1).\\label{dis , so , di7}\\end{aligned}\\ ] ] combining ( [ dis , so , di2 ] ) , ( [ dis , so , di5 ] ) and ( [ dis , so , di7 ] ) , we obtain .    _ step ( iii ) : _ in this step , we will prove that @xmath310   \\le \\epsilon(r ) \\label{dis , so , di10},\\end{aligned}\\ ] ] which implies @xmath311 for the integer @xmath312 given in .    applying the central limit theorem for markovian process ( proposition [ clt ] )",
    ", we find the following facts . under the distribution @xmath313 , the random variable @xmath314 asymptotically obeys the normal distribution with mean @xmath315 and variance @xmath316 . under the distribution @xmath317",
    ", the random variable @xmath318 asymptotically obeys the normal distribution with mean @xmath315 and variance @xmath229 . under the distribution @xmath319 , the random variable @xmath318 asymptotically obeys the normal distribution with mean @xmath315 and variance @xmath230 .",
    "hence , taking the limit @xmath320 , we obtain @xmath321(r ) = \\epsilon(r),\\end{aligned}\\ ] ] which implies ( [ dis , so , di10 ] ) .      to show the converse part , we apply ( [ 2 ] ) of lemma [ l-11 ] to the case with the distribution @xmath322 given in step ( i ) .",
    "then , we apply the central limit theorem for markovian process ( proposition [ clt ] ) to the two random variables related to the dispersions of channel and source .",
    "since we treat two gaussian random variables , the asymptotic error probability is lower bounded by the convolution of two gaussian distributions .",
    "however , since the variance of the dispersions of channel is not unique , in general , we need to take the minimum for the gaussian distribution function .",
    "hence , the asymptotic error probability is lower bounded by the switched gaussian convolution distribution .",
    "_ step ( i ) : _ in this step , to show the converse part , we prepare several notations .",
    "we denote that @xmath323 .",
    "we focus on the set @xmath324 of empirical distributions with @xmath1 channel inputs .",
    "its cardinality @xmath325 is evaluated as @xmath326 . and",
    "in this proof , we use the distribution @xmath327 where @xmath328 we also define the sets @xmath329 where @xmath330 of is empirical distribution function of @xmath331 .",
    "_ step ( ii):_we choose the message block length @xmath287 so that @xmath332 we set the real number @xmath184 by @xmath333 by substituting @xmath334 , ( [ 2 ] ) of lemma [ l-11 ] implies that @xmath335 \\\\ \\ge & \\sum_{m^k }   p_{m^k } ( m^k)w_{{\\mathsf{e}}(m^k)}^n \\left \\ { s(m^k )      + \\sqrt{n } \\left ( \\frac { 1 } { n } \\log{\\frac{w_{{\\mathsf{e}}(m^k)}^n(y^n ) } { q^n_u ( y ) } } - c                  \\right ) \\le r \\right \\ } - e^ { - n^ { \\frac { 1 } { 4 } } } .\\end{aligned}\\ ] ]    for arbitrary @xmath336 , the first term of right hand side is evaluated as    @xmath337    _ step ( iii ) : _ for the second term of ( [ dis , so , co2 ] ) , we will show the following fact : given an arbitrary small real number @xmath338 , there exists a sufficiently large @xmath339 such that @xmath340 for @xmath341 and @xmath342 .    when @xmath343 , @xmath344\\\\ & = { { \\mathrm{v } } } _ { { \\rm ep } ( { \\mathsf{e}}(m^k ) ) , w } < \\max _ { p _ { x } } { { \\mathrm{v } } } _ { p _ { x } , w }     , \\end{aligned}\\ ] ] @xmath345\\\\ & = \\frac { 1}{\\sqrt { n } }      ( ni ( { \\rm ep } ( { \\mathsf{e}}(m^k ) ) , w_{y|x } ) + \\log{(|t_n|+1 ) } - nc ) \\nonumber \\\\ & \\le \\frac { \\log{(|t_n|+1 ) } } { \\sqrt { n } } - \\xi \\sqrt { n } , \\end{aligned}\\ ] ] where @xmath346 and @xmath347 denote the expectation and the variance under the distribution @xmath348 .",
    "thus , when @xmath349 , by using chebyshev inequality , we obtain    @xmath350 ^ 2                  } .\\end{aligned}\\ ] ]    for sufficiently large @xmath351 , we have @xmath352 ^ 2                  } . \\end{aligned}\\ ] ] since the value @xmath353 ^ 2                  } \\end{aligned}\\ ] ] asymptotically goes to @xmath354 , we obtain ( [ dis , so , pr1 ] ) .",
    "+ _ step ( iv ) : _ for the second term of ( [ dis , so , co2 ] ) , we will show the following fact : + given an arbitrary small real number @xmath338 , there exists a sufficiently large @xmath355 such that    @xmath356    for @xmath357 and @xmath358 .",
    "now , to evaluate the variance of some random variable later , we define the quantity @xmath359    when @xmath360 , the inequality @xmath361    holds . since the random variable    @xmath362    has the variance @xmath363 , applying the central limit theorem , we have    @xmath364    for sufficiently large @xmath351 . because @xmath365 is a monotonicity increasing function and the inequalities @xmath366 when @xmath367 , we have @xmath368 and when @xmath369 , we have @xmath370 hence , we obtain ( [ dis , so , pr2 ] ) .",
    "_ step ( v ) : _ we will show the following fact : given an arbitrary small real number @xmath338 , there exists a sufficiently large @xmath371 such that @xmath335\\nonumber \\\\ \\ge & \\sum _ { i= -lj } ^ { i_0 }   p_{m^k } \\left \\ { \\frac { i } { j } \\le s(m^k ) \\le \\frac { i+1 } { j } \\right \\ } \\phi \\left ( \\frac { r - \\frac { i+1 } { j } } { \\sqrt { v^*_{+ } ( w_{y|x } ) } }   \\right)\\nonumber \\\\ & + \\sum _ { i= i_0 } ^ { lj-1 }   p_{m^k } \\left \\ { \\frac { i } { j } \\le s(m^k ) \\le \\frac { i+1 } { j } \\right \\ } \\phi _ {   v^*_{- } ( w_{y|x } )   }    \\left (   r - \\frac { i+1 } { j } \\right ) - \\delta,\\end{aligned}\\ ] ] where @xmath372 , for @xmath373 and @xmath374 .    combining ( [ dis , so , pr1 ] ) and ( [ dis , so , pr2 ] ) , for sufficiently large @xmath351 , we obtain @xmath335 \\\\ \\ge & \\sum _ { i= -lj } ^ { lj-1 }      \\sum_{m^k \\in \\pi _ { n , j , i } \\cap \\omega_n }       p_{m^k } ( m^k ) w_{{\\mathsf{e}}(m^k)}^n      \\left \\ {      \\frac { i+1 } { j }      + \\sqrt{n } \\left (   \\frac { 1 } { n }                  \\log{\\frac{w_{{\\mathsf{e}}(m^k)}^n(y^n ) } { ( q_m ) ^ { \\times n } ( y^n ) } }                  + \\frac { 1 } { n } \\log{(|t_n|+1 ) } - c                  \\right ) \\le r \\right \\ }",
    "\\nonumber \\\\ & + \\sum _ { i= -lj } ^ { lj-1 }      \\sum_{m^k \\in \\pi _ { n , j , i } \\cap \\omega_n^c }       p_{m^k } ( m^k ) w_{{\\mathsf{e}}(m^k)}^n      \\left \\ {      \\frac { i+1 } { j }      + \\sqrt{n } \\left (   \\frac { 1 } { n }                  \\log{\\frac{w_{{\\mathsf{e}}(m^k)}^n(y^n ) } { w _ { { \\rm ep } ( { \\mathsf{e}}(m^k ) ) } ^ { \\times n } ( y^n ) } }                  + \\frac { 1 } { n } \\log{(|t_n|+1 ) } - c                  \\right ) \\le r \\right \\ } \\\\ \\ge & \\sum _ { i= -lj } ^ { lj-1 }      \\sum_{m^k \\in \\pi _ { n , j , i } \\cap \\omega_n }       p_{m^k } ( m^k ) w_{{\\mathsf{e}}(m^k)}^n      \\left \\ {      \\frac { i+1 } { j }      + \\sqrt{n } \\left (      \\frac { 1 } { n }      \\log{\\frac{w_{{\\mathsf{e}}(m^k)}^n(y^n ) } { ( q_m ) ^ { \\times n } ( y^n ) } }      + \\frac { 1 } { n } \\log{(|t_n|+1 ) } - c      \\right )      \\le r      \\right \\ }",
    "\\nonumber\\\\ & + \\sum _ { i= -lj } ^ { lj-1 }      \\sum_{m^k \\in \\pi _ { n , j , i } \\cap \\omega_n^c }       p_{m^k } ( m^k ) \\cdot 1 - \\delta \\\\",
    "\\ge & \\sum _ { i= -lj } ^ { lj-1 }      \\sum_{m^k \\in \\pi _ { n , j , i } }       p_{m^k } ( m^k ) w_{{\\mathsf{e}}(m^k)}^n      \\left \\ {      \\frac { i+1 } { j }      + \\sqrt{n } \\left (      \\frac { 1 } { n }      \\log{\\frac{w_{{\\mathsf{e}}(m^k)}^n(y^n ) } { ( q_m ) ^ { \\times n } ( y^n ) } }      + \\frac { 1 } { n } \\log{(|t_n|+1 ) } - c      \\right )      \\le r      \\right \\ } - \\delta \\\\",
    "\\ge & \\sum _ { i= -lj } ^ { i_0 }   p_{m^k } \\left \\ { \\frac { i } { j } \\le s(m^k ) \\le \\frac { i+1 } { j } \\right \\ } \\phi _ {   v^*_{+ } ( w_{y|x } ) }    \\left (   r - \\frac { i+1 } { j }    \\right)\\nonumber \\\\ & + \\sum _ { i= i_0 } ^ { lj-1 }   p_{m^k } \\left \\ { \\frac { i } { j } \\le s(m^k ) \\le \\frac { i+1 } { j } \\right \\ } \\phi _ {   v^*_{- } ( w_{y|x } )   }    \\left (   r - \\frac { i+1 } { j }    \\right ) - \\delta.\\end{aligned}\\ ] ]    _ step ( vi ) : _ we will show the following fact : given an arbitrary small real number @xmath375 , there exist sufficiently large numbers @xmath376 , and @xmath377 such that @xmath378 for @xmath379 .    from the central limit theorem for markov sequence ( proposition [ clt ] ) , random variable @xmath314 asymptotically obeys normal distribution with mean @xmath315 and variance @xmath380 i.e. , @xmath381 with the limit @xmath382 , we have @xmath383 so , taking the limit @xmath384 , we have @xmath385 when @xmath386 , we can compute ( [ dis , so , st6,1 ] ) as : @xmath387    furthermore , when @xmath388 , @xmath389 so , we obtain ( [ dis , so , st6 ] ) .",
    "_ step ( vi ) : _ since @xmath390 is arbitrary , combining steps ( iv ) and ( v ) , we obtain @xmath391 \\ge \\epsilon(r )",
    ". \\end{aligned}\\ ] ]",
    "in this section , we compare the result of above theorems with a performance of separation code .",
    "we assume that channel is general additive channel .",
    "firstly , we fix the input and output coding - lengths to be @xmath312 and @xmath1 .",
    "to define separation encoder , we introduce the following triplet ;    * a source encoder @xmath392 . +",
    "* a source - channel mapping @xmath393 . + * a channel encoder @xmath394 .    here",
    ", the channel encoder does not know the source distribution .",
    "so , it is natural to consider the average case with respect to the permutation on the image @xmath395 of the source encoder . to realize the average",
    ", we consider the source - channel mapping @xmath396 as the above way , which takes the value in the permutation on the set @xmath395 subject to the uniform distribution .",
    "that is , our separation encoder is given as @xmath397 .",
    "to define the separation decoder , we consider    * a source decoder @xmath398 . + * the inverse of the source - channel mapping @xmath399 + * a channel decoder @xmath400 .",
    "so , our separation decoder is given as @xmath401 .",
    "that is , our separation code is composed of @xmath402 .    here",
    ", the source code @xmath403 has the source coding rate @xmath404 and the channel code @xmath405 has the channel coding rate @xmath406    then , the decoding error probability of the code @xmath407 is given as the probability that the error occurs in the source coding or the channel coding .",
    "hence , the decoding error probability @xmath408 is defined as @xmath409 however , since the source - channel mapping @xmath396 takes the value in the permutation on the set @xmath395 subject to the uniform distribution , it is natural to take the average with respect to the choice of @xmath396 .",
    "hence , the value @xmath410 $ ] is defined as the average of @xmath408 with respect to this choice ; @xmath411   & : = e_u { \\mathrm{p}}_{\\rm sep } ( { \\mathsf{e}}^*_n , { \\mathsf{d}}^*_n ) .",
    "\\label{sep , ep } \\end{aligned}\\ ] ]",
    "let @xmath412 be the decoding error probability of the source code @xmath413 , and let @xmath414 be the decoding error probability of the channel code @xmath415 with the message subject to the uniform distribution .",
    "then , we have the following lemma .",
    "the average @xmath410 $ ] is calculated as @xmath411 = { \\mathrm{p}}_s({\\mathsf{e}}_{s , k , a } , { \\mathsf{d}}_{s , a , k } ) * { \\mathrm{p}}_c({\\mathsf{e}}_{c , a , n } , { \\mathsf{d}}_{c , n , a}).\\label{eq36}\\end{aligned}\\ ] ]    from ( [ sep , ep ] ) , we have @xmath416   \\nonumber \\\\ = & e_u { \\mathrm{p}}_{\\rm sep } ( { \\mathsf{e}}^*_n , { \\mathsf{d}}^*_n)\\nonumber \\\\ = & \\sum_{m \\in { \\cal m } ^k:{\\mathsf{d}}_{s , a , k } \\circ { \\mathsf{e}}_{s , k , a } ( m ) \\neq m } p _ { m^k } ( m ) \\nonumber\\\\ & + e_u \\sum_{m \\in { \\cal m } ^k:{\\mathsf{d}}_{s , a , k } \\circ { \\mathsf{e}}_{s , k , a } ( m ) = m }   p _ { m^k } ( m ) w_{y^n|x^n } ( \\{y : { \\mathsf{d}}_{c , n , a } ( y ) \\neq f   \\circ   { \\mathsf{e}}_{s , k , a } ( m ) \\ } | { \\mathsf{e}}_{c , a , n}\\circ f_u \\circ   { \\mathsf{e}}_{s , k , a } ( m ) ) .",
    "\\label{com , pf } \\ ] ] the second term of ( [ com , pf ] ) can be calculated as follows .",
    "@xmath417 combining ( [ com , pf ] ) and ( [ com , pf1 ] ) , we have @xmath416 \\\\ = & \\sum_{m \\in { \\cal m } ^k:{\\mathsf{d}}_{s , a , k } \\circ { \\mathsf{e}}_{s , k , a } ( m ) \\neq",
    "m } p _ { m^k } ( m ) \\\\",
    "& + \\left ( 1-\\sum_{m \\in { \\cal m } ^k:{\\mathsf{d}}_{s , a , k } \\circ { \\mathsf{e}}_{s , k , a } ( m ) \\neq m } p",
    "_ { m^k } ( m ) \\right ) \\frac { 1 } { a } \\sum _ { a \\in \\ { 1 , \\cdots , a \\ }   } w_{y^n|x^n } ( \\{y : { \\mathsf{d}}_{c , n , a } ( y ) \\neq a \\ } | { \\mathsf{e}}_{c , a , n } ( a ) ) \\\\ = & { \\mathrm{p}}_s({\\mathsf{e}}_{s , k , a } , { \\mathsf{d}}_{s , a , k } ) * { \\mathrm{p}}_c({\\mathsf{e}}_{c , a , n } , { \\mathsf{d}}_{c , n , a}).\\end{aligned}\\ ] ]    under the fixed input and output coding - lengths @xmath312 and @xmath1 , we minimize the above value @xmath410 $ ] as @xmath418 .\\end{aligned}\\ ] ] here , since @xmath419 we have @xmath420 \\nonumber \\\\ = & { { \\mathrm{p}}_{{\\mathrm{s}}}}(a ; p _ { m^k } ) * { { \\mathrm{p}}_{{\\mathrm{c}}}}(a ; w _ { y^n| x^n } ) .\\label{sep , pro}\\end{aligned}\\ ] ] note that for any two real numbers @xmath421 and @xmath422 , @xmath423    considering the minimum with given value @xmath424 , we have @xmath425    hereafter , we note the coding rate of separation scheme @xmath426 as @xmath427 .    additionally , we define @xmath428        in this section , we evaluate the second order rate of separation scheme .",
    "we have the following theorem for a conditional additive channel given by the transition matrix @xmath429 .",
    "@xmath430 ^{-1}(\\varepsilon ) .",
    "\\label{sep , so , th7}\\end{aligned}\\ ] ] in other words , @xmath431 where @xmath432 ( r ) .",
    "\\label{sep , so , th , err}\\end{aligned}\\ ] ]",
    "this theorem is markovian source and conditional additive channel version of ( * ? ? ?",
    "* section v ) .",
    "we assume that @xmath433 and intermediate set size of separation code is @xmath434 . if @xmath435 and @xmath436 then @xmath437 .    the channel and source coding theorem for markovian case with the second order ( * ? ? ?",
    "* theorems 10 and 21 ) guarantee the following relations @xmath438 hence , we have @xmath439    since @xmath440 , @xmath441 optimizing the chose of @xmath434 , we have @xmath442 hence , we have @xmath443      here , we evaluate the second order rate of separation coding in the case when the channel is discreet memoryless channel .",
    "for the discrete memoryless channel give by a transition matrix w , we have @xmath444 ^{-1}(\\varepsilon ) ,   \\tilde{\\phi } \\left [ \\frac { c } { h^{w_s}(m ) } v^ { w_s } ( m ) ,   v^*_{- } ( w_{y|x } ) \\right ] ^{-1}(\\varepsilon ) \\right \\}. \\end{aligned}\\ ] ]    the paper @xcite showed the same statement with the assumption @xmath445 and the source is independent and identical distribution .",
    "our contribution is removing the first assumption and generalizing it to markovian source .",
    "we assume that @xmath433 and intermediate set size of separation code is @xmath434 . if @xmath435 and @xmath436 then @xmath437 .",
    "the channel coding theorem with the second order @xcite ( theorem [ dis , so , th ] with uniform message of size @xmath424 ) guarantees that @xmath446 combining ( [ com , so , dis1 ] ) and ( [ com , so , dis2 ] ) , we obtain @xmath447 because @xmath448 .",
    "so , we have @xmath449      firstly , we assume that the channel is discreet memoryless channel .",
    "we set the separation bound @xmath450 as @xmath451 we compare the separation bound with the kostina - verd bound @xmath452 defined in , which is still not the tight bound in the joint source - channel scheme .",
    "the property implies the inequality @xmath453 here , the equality is attained if and only if @xmath454 , @xmath455 , or @xmath456 . when @xmath455 , there is no information to be transmitted .",
    "when @xmath456 , we can not make any information transmission .",
    "these two cases do not occur in a realistic case . when @xmath454 , the distribution of the message source is uniform , which is not discussed in the joint source - channel coding .",
    "so , we conclude that the separation scheme always has a larger decoding error probability than the joint source - channel scheme .    as the opposite evaluation",
    ", we have the following lemma .    when @xmath457 , we have @xmath458 where @xmath459 the equality holds only when @xmath460 .    therefore , when the variance of the information density is unique , i",
    ".. e , @xmath461 , lemma [ sep , so , lem2 ] analytically determines the range of the ratio between the error probabilities with the joint and separation schemes .    when the channel is conditional additive channel , above argument is more simple .",
    "we compare the bound @xmath462 and @xmath462 .",
    "the property implies the inequality @xmath463 as the opposite evaluation , we have following lemma .    when @xmath464 , we have @xmath465 where @xmath466 the equality holds only when @xmath467 .    many papers @xcite adopt the rhs of as the error probability of the separation scheme",
    "however , no existing study clarified the meaning as lemma [ l6 ] .",
    "the paper @xcite discussed a similar comparison as lemma [ sep , so , lem2 ] when the source is subject to an independent and identical distribution and @xmath468 .",
    "although the paper ( * ? ? ?",
    "* ( 33 ) ) conjectured a similar statement as lemma [ sep , so , lem ] via numerical calculation , they did not show it .",
    "hence , they could not analytically determine the range of the ratio between the error probabilities with the joint and separation schemes even when @xmath461 .",
    "mh is very grateful to professor vincent y. f. tan and professor shun watanabe for helpful discussions and comments .",
    "the works reported here were supported in part by a mext grant - in - aid for scientific research ( b ) no .",
    "16kt0017 , the okawa research grant and kayamori foundation of informational science advancement .",
    "we set @xmath469^{-1 } ( \\varepsilon ) = r $ ] .",
    "we can rewrite @xmath469^{-1 } ( \\varepsilon ) $ ] as : @xmath470^{-1 } ( \\varepsilon ) & = \\max_{\\varepsilon \\ge \\varepsilon_s * \\varepsilon_c   } \\left ( \\sqrt{v_1 } \\phi^{-1}(\\varepsilon_s ) + \\sqrt{v_2 } \\phi^{-1}(\\varepsilon_c )   \\right ) \\label{a,1 } \\\\ & = r_s + r_c,\\end{aligned}\\ ] ] where @xmath471 where @xmath472 and @xmath473 is maximizing .",
    "then , we have @xmath474 the right hand side of is represented as : @xmath475 where @xmath476 is the random variable subject to the gaussian distribution with mean @xmath315 and variance @xmath477 and @xmath478 is the random variable obeys the gaussian distribution with mean @xmath315 and variance @xmath479 . on the other hands , setting @xmath480 , we have @xmath481 because @xmath482 , we have @xmath483 combining and , we have @xmath484 i.e. , @xmath485(r )   \\ge   \\phi_{v_1+v_2 }   \\left ( r   \\right ) .",
    "\\end{aligned}\\ ] ]      for the proof , we define new function @xmath487 for @xmath488 and @xmath489 as : @xmath490 using this function , we can rewrite function @xmath469^{-1 } ( \\varepsilon ) $ ] as : @xmath491^{-1 } ( \\varepsilon ) = \\sqrt{v_1 + v_2 }   \\phi^{-1}(\\tilde{\\varepsilon}(\\varepsilon , y)),\\end{aligned}\\ ] ] where @xmath492 .",
    "we have following lemma , which guarantees ( [ sep , so , th2 ] ) .",
    "we rewrite the lhs of ( [ sep , so , th5 ] ) as @xmath502 where @xmath503 is inner product of vector .",
    "the inside of the rhs of is calculated as @xmath504 note that when @xmath505 , either @xmath506 or @xmath507 is negative .",
    "hereafter , we will consider the maximum value of ( [ sep , so , th4 ] ) under the condition @xmath508 .",
    "next , consider the case when @xmath511 , which has the above three cases .",
    "first , we consider the case when @xmath512 and @xmath513 .",
    "then , we have @xmath514 that is , the maximum value is attained when @xmath515 and @xmath516 .        for notation",
    ", we define the function for @xmath521 and @xmath522 as : @xmath523 then , we can find that @xmath524 and @xmath525 is monotonically decreasing function of @xmath526 .",
    "hence , the relation is equivalent to @xmath527 thus , it is sufficient to show that @xmath528    choosing @xmath529 , we write @xmath530 for certain @xmath531 .",
    "now , to regard @xmath532 as a function of @xmath35 , we define @xmath533 and hereafter we will find @xmath534 which minimize @xmath535 . calculating the derivative",
    ", we have @xmath536\\\\ = & -a \\sin \\theta",
    "\\phi'(a \\cos \\theta ) + a \\cos \\theta \\phi'(a \\sin \\theta ) + a \\sin \\theta",
    "\\phi'(a \\cos \\theta ) \\phi(a \\sin \\theta ) -   a \\cos \\theta \\phi(a \\cos \\theta ) \\phi'(a \\sin \\theta)\\\\ = & - a \\sin \\theta \\phi'(a \\cos \\theta ) ( 1- \\phi(a \\sin \\theta ) ) + a \\cos \\theta \\phi'(a \\sin \\theta ) ( 1 - \\phi(a \\cos \\theta)).\\end{aligned}\\ ] ] now , we define @xmath537 because @xmath538 and @xmath539 is a monotonically increasing function for @xmath540 , we find that @xmath541 is a monotonically increasing function for @xmath540 . since @xmath542 the derivative test chart of @xmath543 is given as follows .",
    "@xmath544 hence , when @xmath545 i.e. , @xmath546 , @xmath535 is minimized .",
    "therefore , when @xmath547 satisfies @xmath548 and @xmath546 , the minimum is attained .",
    "so , we have @xmath549 , which means .",
    "we assume @xmath550 .",
    "since @xmath551 setting @xmath552 for @xmath553 , we find that we can write @xmath554 and @xmath555 for @xmath556 .",
    "we set @xmath557 and we will find certain @xmath558 minimizing @xmath559 . calculating the derivative , we have @xmath560\\\\      = &      \\phi'(s ) - \\phi'(a - s ) - \\phi'(s)\\phi(a - s ) + \\phi(s ) \\phi'(a - s)\\\\      = &      \\phi'(s)(1- \\phi(a - s ) )       - \\phi'(a - s)(1-\\phi(s)).\\end{aligned}\\ ] ] here , we define @xmath561 @xmath562 is a monotonically increasing function for @xmath540 .",
    "since @xmath563 and @xmath564 , we have the derivative test chart of @xmath565 as : @xmath566 hence , @xmath567 is minimized when @xmath568 i.e. , @xmath569 .",
    "therefore , we have @xmath570 combining ( [ sep , so , th , l2,1 ] ) and lemma [ sep , so , th , l1 ] , we have the claim of lemma [ sep , so , th , l2 ]",
    ".    1 v. strassen , `` asymptotische abschtzugen in shannon s informationstheorie , '' in transactions of the third prague conference on information theory etc , czechoslovak academy of sciences , prague , pp .",
    "689 - 723 , 1962 .              v. y. f. tan , s. watanabe , and m. hayashi `` moderate deviations for joint source - channel coding of systems with markovian memory '' , _ proceedings of 2014 ieee international symposium on information theory _ , june 29 - july 4 2014 , honolulu , hi , usa , pp .",
    "1687 - 1691 .",
    "v. kostina and s. verd , `` lossy joint source - channel coding in the finite blocklength regime , '' _ proceedings of 2012 ieee international symposium on information theory _ , 1 - 6 july 2012 , cambridge , ma , usa , pp .",
    "1553 - 1557 .",
    "a. t. campo , g. vazquez - vilar , a. g. i fbregas , t. koch and a. martinez , `` achieving csiszr s source - channel coding exponent with product distributions , '' _ proceedings of 2012 ieee international symposium on information theory _ , 1 - 6 july 2012 , cambridge , ma , usa , pp .",
    "1548 - 1552 .",
    "y. zhong , f. alajaji and l. lorne campbell ,  joint source - channel coding error exponent for discrete communication systems with markovian memory , \" _ ieee trans .",
    "inf . theory _ ,",
    "12 , 4457 - 4472 ( 2007 ) .",
    "w.kumagai , and m. hayashi , `` second - order asymptotics of conversions of distributions and entangled states based on rayleigh - normal probability distributions , '' accepetd for publication in _ ieee trans .",
    "inf . theory _ ;",
    "arxiv:1306.4166    w.kumagai , and m. hayashi , `` random number conversion via restricted storage , '' _ proceedings of 2014 ieee international symposium on information theory _ , june 29 - july 4 2014 , honolulu , hi , usa , pp .",
    "2047 - 2051 .",
    "s. watanabe , m. hayashi  finite - length analysis on tail probability for markov chain and application to simple hypothesis testing , \" accepted for publication in annals of applied probability ; arxiv:1401.3801 ."
  ],
  "abstract_text": [
    "<S> we derive the second order rates of joint source - channel coding , whose source obeys the ergodic markov process by introducing new distribution family , switched gaussian convolution distribution , when the channel is a discrete memoryless . </S>",
    "<S> we also compare the joint source - channel scheme with the separation scheme in the second order regime .    </S>",
    "<S> markov chain , second order , joint source - channel coding , separation scheme </S>"
  ]
}