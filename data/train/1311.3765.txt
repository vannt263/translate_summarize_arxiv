{
  "article_text": [
    "shape constrained regression involves estimating a vector @xmath5 from observations @xmath6 where @xmath1 lies in a known convex polyhedral cone @xmath7 and @xmath8 are i.i.d .",
    "mean zero errors with finite variance .",
    "it may be recalled that convex polyhedral cones are sets of the form @xmath9 where @xmath10 is a matrix of order @xmath11 and @xmath12 means that @xmath13 for each @xmath14 .",
    "basic properties of convex polyhedral cones can be found , for example , in @xcite , chapters 7 and 8 .    in this paper , we focus on such problems when the cone @xmath15 is of the special form @xmath16 for some known integers @xmath17 and @xmath18 and nonnegative weights @xmath19 . here",
    "the integers @xmath20 and @xmath21 and the weights @xmath22 do not depend on @xmath23 .",
    "note that when @xmath24 , the condition in the definition of @xmath25 is vacuous so that @xmath26 .",
    "the dependence of the cone on the weights @xmath27 is suppressed in the notation @xmath25 .",
    "the following shape constrained regression problems are special instances of our general setup :    when @xmath28 and @xmath29 , the cone in ( [ gv ] ) consists of all nondecreasing sequences @xmath30 estimation problem ( [ eq : mdl2 ] ) then becomes the well - known isotonic regression problem .    when @xmath31 , @xmath32 and @xmath33 , the cone in ( [ gv ] ) consists of all convex sequences @xmath34 . then ( [ eq : mdl2 ] ) reduces to the usual convex regression problem with equally spaced design points .",
    "@xmath35-monotone regression corresponds to @xmath36 where @xmath37 is given by @xmath38 , and @xmath39 represents the @xmath35-times composition of @xmath40 .",
    "this is also a special case of ( [ gv ] ) .",
    "our object of interest in this paper is the least squares estimator ( lse ) for @xmath1 under the constraint @xmath41 .",
    "it is given by @xmath42 where @xmath43 is the observation vector and @xmath44 where @xmath45 denotes the usual euclidean norm in @xmath2 .",
    "a natural measure of how well @xmath42 estimates @xmath1 is @xmath46 where @xmath47 with @xmath48 and @xmath49 .",
    "as @xmath46 is random we study its expectation @xmath50 which is referred to as the risk of the estimator @xmath51 .",
    "this paper has two aims : ( 1 ) for every cone @xmath52 , we prove upper bounds for the risk @xmath53 as @xmath1 varies in @xmath54 ; ( 2 ) we isolate the risk bound for the special case of isotonic regression ( when @xmath55 ) and study its properties in more detail",
    ".      the first part of the paper will be about bounds for the risk @xmath56 .",
    "our bounds will involve the _ statistical dimension _ of the cone @xmath25 . for a cone @xmath15 , defined as in ( [ bla ] ) ,",
    "its statistical dimension is given by @xmath57 and @xmath58 is a vector whose components are independent standard normal random variables .",
    "note that the quantity @xmath59 is well defined because @xmath60 is a 1-lipschitz function of @xmath61 ; see @xcite .",
    "it was argued in @xcite that @xmath62 provides a measure of the effective dimension of the model .",
    "for example , if @xmath15 is a linear space of dimension @xmath63 , then @xmath64 , where @xmath65 is the projection matrix onto @xmath15 , and @xmath66 for all @xmath61 .",
    "it was also shown in @xcite that @xmath62 is the number of distinct values among @xmath67 for isotonic regression .",
    "the term statistical dimension for @xmath68 was first used in @xcite ; however , the definition of @xmath68 in @xcite is different from ( [ sd1 ] ) . for connections between the two definitions and more discussion on the notion of statistical dimension , see section  [ genres ] .",
    "we are now ready to describe our main result which bounds @xmath69 for @xmath70 . for each @xmath70 , let @xmath71 denote the number of inequalities among @xmath72 for @xmath73 that are strict . in theorem",
    "[ zen ] , we prove that for every @xmath70 , @xmath74 under the assumption that @xmath8 are independent normally distributed random variables with mean zero and variance @xmath75 .",
    "this bound behaves differently depending on the form of the true sequence @xmath1 and thus describes the adaptive behavior of the lse ; for more details on the inequality , see section  [ genres ] .",
    "the proof of theorem  [ zen ] uses the characterization properties of the projection operator on a closed convex cone .",
    "we prove a series of auxiliary results leading to the proof of theorem  [ zen ] ; these results hold for any polyhedral cone @xmath15 ( not necessarily of the form @xmath76 ) and are of independent interest .",
    "the second part of the paper is exclusively on isotonic regression . even in this special case ,",
    "inequality ( [ zen.eq ] ) appears to be new .",
    "we provide a reformulation of ( [ zen.eq ] ) that bounds the risk of @xmath77 using the variation of @xmath1 across subsets of @xmath78 .",
    "this results in an inequality that is more interpretable and makes comparison with previous inequalities in isotonic regression more transparent .    to state this bound",
    ", we need some notation . specializing the notation @xmath71 for @xmath79 to the cone @xmath80 ,",
    "we get @xmath71 equals the number of inequalities @xmath81 for @xmath82 that are strict . by an abuse of notation ,",
    "we extend this notation to _ interval partitions _ of @xmath23 .",
    "an interval partition @xmath83 of @xmath23 is a finite sequence of positive integers that sum to @xmath23 . in combinatorics",
    "this is called a composition of @xmath23 .",
    "let the set of all interval partitions @xmath83 of @xmath23 be denoted by @xmath84 .",
    "formally , @xmath84 can be written as @xmath85 for each @xmath86 @xmath87 , let @xmath88 .",
    "for every @xmath89 , there exist integers @xmath90 and @xmath91 with @xmath92 such that @xmath1 is constant on each set @xmath93 for @xmath94 , where @xmath95 and @xmath96 .",
    "we refer to this interval partition @xmath97 as the interval partition _ generated _ by @xmath1 .",
    "note that @xmath98 precisely equals @xmath71 , the number of inequalities @xmath81 , for @xmath82 , that are strict .    for every @xmath99 and @xmath100",
    ", we define @xmath101 where @xmath102 and @xmath96 for @xmath103 .",
    "@xmath104 can be treated as measure of variation of @xmath1 with respect to the partition @xmath83 .",
    "an important property is that @xmath105 for every @xmath99 .",
    "for the trivial partition @xmath106 , it is easy to see that @xmath107 and @xmath108 .    with this notation , our main result for isotonic regression states that @xmath109 where @xmath110 this inequality is very similar to ( [ zen.eq ] ) ; see remark  [ remp ] for the connections .",
    "the lse , @xmath77 , in isotonic regression has the explicit formula ( [ eq : charac ] ) .",
    "this formula is commonly known as the min ",
    "max formula ; see @xcite , chapter  1 . using this formula , we prove inequality ( [ pyaa ] ) in section  [ isoreg ] .",
    "we only use the fact that @xmath8 are i.i.d . with mean zero and",
    "variance @xmath75 [ normality of @xmath111 is not needed here unlike inequality ( [ zen.eq ] ) for which we require normality ] .",
    "inequality ( [ pyaa ] ) appears to be new , even though there is a huge literature on univariate isotonic regression . to place this inequality in a proper historical context",
    ", we give a brief overview of existing theoretical results on isotonic regression in section  [ isoreg ] .",
    "the strongest previous bound on @xmath112 is due to zhang  @xcite , theorem  2.2 , who showed that @xmath113 where @xmath114 with @xmath115 here , by the symbol @xmath116 we mean @xmath117 up to a multiplicative constant .",
    "the quantity @xmath118 is known as the variation of the sequence @xmath1 .    our inequality ( [ pyaa ] ) compares favorably with ( [ motw ] ) in certain cases . to see this , suppose , for example , that @xmath119 ( here @xmath120 denotes the indicator function ) so that @xmath121",
    ". then @xmath122 is essentially @xmath123 while @xmath124 is much smaller because it is at most @xmath125 as can be seen by taking @xmath126 in the definition of @xmath124 [ note that @xmath127 .    more generally by taking @xmath126 in the infimum of the definition of @xmath124",
    ", we obtain @xmath128 which is a stronger bound than ( [ motw ] ) when @xmath71 is small .",
    "the reader may observe that @xmath71 is small precisely when the differences @xmath129 are sparse .",
    "inequality ( [ pyaa ] ) can be stronger than ( [ motw ] ) even in situations when @xmath71 is not small ; see remark  [ ach ] for an example .",
    "we study properties of @xmath124 in section  [ hd ] . in theorem",
    "[ sabya2 ] , we show that @xmath124 is bounded from above by a multiple of @xmath122 that is at most logarithmic in @xmath23 .",
    "this implies that our inequality ( [ pyaa ] ) is always only slightly worse off than ( [ motw ] ) while being much better in the case of certain sequences @xmath1 .",
    "we also show in section  [ hd ] that the risk bound @xmath124 behaves differently , depending on the form of the true sequence @xmath1 .",
    "this means that bound ( [ pyaa ] ) demonstrates adaptive behavior of the lse .",
    "one gets a whole range of rates from @xmath130 ( when @xmath1 is constant ) to @xmath4 up to logarithmic factors in the worst case [ this worst case rate corresponds to the situation where @xmath131 .",
    "bound ( [ pyaa ] ) therefore presents a bridge between the two terms in the formula for @xmath122 .",
    "in addition to being an upper bound for the risk of the lse , we believe that the quantity @xmath124 also acts as a benchmark for the risk of any estimator in isotonic regression . by this , we mean that , in a certain sense , no estimator can have risk that is significantly better than @xmath124 .",
    "we substantiate this claim in section  [ lmnop ] by proving lower bounds for the _ local minimax risk _ near the `` true '' @xmath1 . for @xmath99 ,",
    "the quantity @xmath132 with @xmath133 will be called the local minimax risk at @xmath1 ; see section  [ lmnop ] for the rigorous definition of the neighborhood @xmath134 where the multiplicative constants hidden by the @xmath116 sign are explicitly given . in the above display @xmath135 is defined as @xmath136 .",
    "the infimum here is over all possible estimators @xmath137 .",
    "@xmath138 represents the smallest possible ( supremum ) risk under the knowledge that the true sequence @xmath139 lies in the neighborhood @xmath134 .",
    "it provides a measure of the difficulty of estimation of @xmath1 .",
    "note that the size of the neighborhood @xmath134 changes with @xmath1 ( and with  @xmath23 ) and also reflects the difficulty level of the problem .    under each of the two following setups for @xmath1 , and the assumption of normality of the errors ,",
    "we show that @xmath138 is bounded from below by @xmath124 up to multiplicative logarithmic factors of @xmath23 . specifically :    when the increments of @xmath1 ( defined as @xmath129 , for @xmath140 ) grow like @xmath141 , we prove in theorem  [ oval ] that @xmath142    when @xmath143 and the @xmath35 values of @xmath1 are sufficiently well - separated , we show in theorem  [ fani ] that @xmath144    because @xmath145 is an upper bound for the risk of the lse and also is a local minimax lower bound in the above sense , our results imply that the lse is near - optimal in a local nonasymptotic minimax sense .",
    "such local minimax bounds are in the spirit of cator @xcite and cai and low @xcite , who worked with the problems of estimating monotone and convex functions respectively at a point .",
    "the difference between these works and our own is that we focus on the global estimation problem . in other words ,",
    "@xcite and @xcite prove local minimax bounds for the local ( pointwise ) estimation problem while we prove local minimax bounds for the global estimation problem .",
    "we also study the performance of the lse in isotonic regression under model misspecification when the true sequence @xmath1 is not necessarily nondecreasing . here",
    "we prove in theorem  [ thm : misrafi ] that @xmath146 where @xmath147 denotes the nondecreasing projection of @xmath1 ; see section  [ misspec ] for its definition .",
    "this should be contrasted with the risk bound of zhang @xcite who proved that @xmath148 . as before our risk bound is at most , slightly worse ( by a multiplicative logarithmic factor in @xmath23 ) than @xmath149 , but is much better when @xmath150 is small .",
    "we describe two situations where @xmath150 is small : when @xmath1 itself has few constant blocks [ see ( [ eq : block ] ) and lemma  [ sabya3 ] ] and when @xmath1 is nonincreasing [ in which case @xmath151 ; see lemma  [ appr ] ] .",
    "the paper is organized as follows : in section  [ genres ] we state and prove our main upper bound for the risk @xmath152 . in section  [ isoreg ]",
    "we give a direct proof of the risk bound ( [ pyaa ] ) for isotonic regression without assuming normality of the errors .",
    "we investigate the behavior of @xmath153 , the right - hand side of ( [ pyaa ] ) , for different values of the true sequence @xmath1 and compare it with @xmath154 , the right - hand of ( [ motw ] ) , in section  [ hd ] .",
    "local minimax lower bounds for isotonic regression are proved in section  [ lmnop ] .",
    "we study the performance of the isotonic lse under model misspecification in section  [ misspec ] .",
    "the supplementary material @xcite gives the proofs of some of the results in the paper .",
    "the goal of this section is to prove inequality ( [ zen.eq ] ) .",
    "let us first review the well - known characterization of the lse under the constraint @xmath41 for an arbitrary convex polyhedral cone @xmath15 .",
    "this lse is denoted by @xmath42 and is defined in ( [ eq : lse ] ) .",
    "the function @xmath155 is well defined [ because for each @xmath61 and @xmath15 , the quantity @xmath60 exists uniquely by the hilbert projection theorem ] , nonlinear in @xmath61 ( in general ) and can be characterized by ( see , e.g. , @xcite , proposition  2.2.1 ) @xmath156 for all @xmath157 .",
    "inequality ( [ zen.eq ] ) involves the notion of statistical dimension [ defined in ( [ sd1 ] ) ] .",
    "the statistical dimension is an important summary parameter for cones , and it has been used in shape - constrained regression @xcite and compressed sensing @xcite . it is closely related to the gaussian width of @xmath15 , which is an important quantity in geometric functional analysis ( see , e.g. , @xcite , chapter  4 ) and which has also been used to prove recovery bounds in compressed sensing @xcite . see @xcite , section  10.3 , for the precise connection between the statistical dimension and the gaussian width .",
    "an alternative definition of the statistical dimension @xmath68 of an arbitrary convex polyhedral cone is given by @xmath158 where @xmath58 is a vector whose components are independent standard normal random variables .",
    "the equivalence of ( [ sd1 ] ) and ( [ sd2 ] ) was observed by meyer and woodroofe @xcite , proof of proposition  2 .",
    "it is actually an easy consequence of stein s lemma because the second identity in ( [ ctz ] ) implies @xmath159 , and therefore , stein s lemma on the right - hand side gives the equivalence of ( [ sd1 ] ) and ( [ sd2 ] ) .",
    "we are now ready to prove our main result , theorem  [ zen ] , which gives inequality  ( [ zen.eq ] ) .",
    "this theorem applies to any cone of the form ( [ gv ] ) . for the proof of theorem  [ zen ] , we state certain auxiliary results ( lemmas [ lili ] , [ hol ] and [ apps ] ) ,",
    "whose proofs can be found in the supplementary material @xcite .",
    "these supplementary results hold for any polyhedral cone ( [ bla ] ) .",
    "[ zen ] fix @xmath160 , @xmath17 and @xmath18 .",
    "consider the problem of estimating @xmath70 from ( [ eq : mdl2 ] ) for independent @xmath161 errors @xmath111 .",
    "then inequality  ( [ zen.eq ] ) holds for every @xmath162 with @xmath71 denoting the number of inequalities among @xmath163 , for @xmath164 , that are strict .",
    "before we prove theorem  [ zen ] the following remarks are in order .    from the proof of theorem",
    "[ zen ] , it will be clear that the risk of the lse satisfies a stronger inequality than ( [ zen.eq ] ) . for @xmath165 with @xmath166 ,",
    "let @xmath167 denote the values of @xmath139 for which the inequalities @xmath168 are strict .",
    "let @xmath169 the proof of theorem  [ zen ] will imply that @xmath170 the observation that @xmath171 is increasing in @xmath23 ( note that the weights @xmath22 , do not depend on @xmath23 ) implies that @xmath172 for all @xmath165 , and hence inequality ( [ zens ] ) is stronger than ( [ zen.eq ] ) .",
    "every convex polyhedral cone ( [ bla ] ) has a well - defined facial structure .",
    "indeed , a standard result ( see , e.g. , @xcite , section  8.3 ) states that a subset @xmath173 of a convex polyhedral cone @xmath15 , as defined in ( [ bla ] ) , is a face if and only if @xmath173 is nonempty and @xmath174 for some @xmath175 matrix @xmath176 whose rows are a subset of the rows of @xmath10 .",
    "the dimension of @xmath173 equals @xmath177 where @xmath178 denotes the rank of @xmath179 .",
    "it is then clear that if @xmath70 is in a low - dimensional face of @xmath54 , then @xmath71 must be small .",
    "now if @xmath171 is at most logarithmic in @xmath23 ( which is indeed the case for the case of isotonic and convex regression ; see examples [ ir ] and [ conre ] ) , then bound ( [ zen.eq ] ) implies that the risk of the lse is bounded from above by the parametric rate @xmath180 ( up to multiplicative logarithmic factors in @xmath23 ) provided @xmath1 is in a low - dimensional face of @xmath54 .",
    "therefore , the lse automatically adapts to vectors in low - dimensional faces of @xmath181 .",
    "for general @xmath1 , the risk is bounded from above by a combination of how close @xmath1 is to a @xmath35-dimensional face of @xmath52 and @xmath182 as @xmath35 varies .",
    "[ ir ] isotonic regression corresponds to @xmath28 and @xmath183 so that @xmath184 becomes @xmath80 .",
    "it turns out that the statistical dimension of this cone satisfies @xmath185 which immediately implies that @xmath186 .",
    "this can be proved using symmetry arguments formalized in the theory of finite reflection groups ; see @xcite , appendix c.4 , where the proof of ( [ frg ] ) is sketched .",
    "now let @xmath187 with @xmath188 .",
    "then there exist integers @xmath189 with @xmath190 such that @xmath191 is constant on each set @xmath93 , for @xmath192 , where @xmath102 and @xmath96 .",
    "it is easy to check then that the quantity @xmath193 defined in ( [ tde ] ) equals @xmath194 where @xmath195 is the monotone cone in @xmath196 .",
    "inequality ( [ frg ] ) then gives @xmath197 because of the concavity of @xmath198 .",
    "inequality ( [ zens ] ) therefore gives @xmath199 this inequality is closely connected to ( [ pyaa ] ) , as we describe in detail in remark  [ remp ] .",
    "note that we require normality of @xmath8 . in section  [ isoreg ] , we prove an inequality which gives a variant of inequality ( [ hh ] ) with different multiplicative constants but without the assumption of normality .    inequality ( [ hh ] ) can be restated in the following way . for each @xmath200 , let @xmath201 denote the set of all sequences @xmath202 with @xmath203 . with this notation , inequality ( [ hh ] ) can be rewritten as @xmath204.\\ ] ]    bound ( [ vk ] ) reflects adaptation of the lse with respect to the classes @xmath201 .",
    "such risk bounds are usually provable for estimators based on empirical model selection criteria ( see , e.g. , @xcite ) or aggregation ; see , for example , @xcite . specializing to the present situation , in order to adapt over @xmath201 as @xmath35 varies , one constructs lses over each @xmath201 and then either selects one estimator from this collection by an empirical model selection criterion or aggregates these estimators with data - dependent weights . in this particular situation ,",
    "such estimators are very difficult to compute as minimizing the ls criterion over @xmath201 is a nonconvex optimization problem .",
    "in contrast , the lse can be easily computed by a convex optimization problem .",
    "it is remarkable that the lse , which is constructed with no explicit model selection criterion in mind , achieves adaptive risk bound ( [ hh ] ) . in the next example",
    ", we illustrate this adaptation for the lse in convex regression .",
    "[ conre ] convex regression with equispaced design points corresponds to @xmath25 with @xmath205 and @xmath33 .",
    "it turns out that the statistical dimension of this cone satisfies @xmath206 where @xmath207 is a universal positive constant .",
    "this is proved in @xcite , theorem  3.1 , via metric entropy results for classes of convex functions .",
    "let @xmath208 with @xmath188 .",
    "let @xmath209 denote the values of @xmath139 where the inequality @xmath210 is strict . with @xmath211 , @xmath212 for @xmath213 and @xmath214 , we have , from ( [ tde ] ) and ( [ zb ] ) , @xmath215 using the fact that @xmath216 is concave for @xmath217",
    ", we have ( note that @xmath218 ) @xmath219    inequality ( [ zens ] ) then becomes @xmath220 note that the quantity @xmath221 can be interpreted as the number of affine pieces of the convex sequence @xmath191 .",
    "this risk bound is the analogue of inequality ( [ hh ] ) for convex regression , and it highlights the adaptation of the convex lse to piecewise affine convex functions . a weaker version of this inequality appeared in @xcite , theorem  2.3 .",
    "we now prove theorem  [ zen ] .",
    "we shall first state some general results ( lemmas [ lili ] , [ hol ] and [ apps ] ) , whose proofs can be found in the supplementary material @xcite , for the risk of @xmath222 , which hold for every @xmath15 of the form ( [ bla ] ) .",
    "theorem  [ zen ] will then be proved by specializing these results for @xmath223 .",
    "we begin by recalling a result of meyer and woodroofe @xcite who related the risk of @xmath42 to the function @xmath224 .",
    "specifically , @xcite , proposition  2 , proved that @xmath225 and @xmath226 these can be proved via stein s lemma ; see @xcite , proof of proposition  2 .",
    "it might be helpful to observe here that the function @xmath59 satisfies @xmath227 for every @xmath228 , and this is a consequence of the fact that @xmath229 and the characterization ( [ ctz ] ) .",
    "our first lemma below states that the risk of the lse is equal to @xmath230 for all @xmath1 belonging to the lineality space @xmath231 of @xmath15 .",
    "the lineality space @xmath232 will be crucial in the proof of theorem  [ zen ] .",
    "the lineality space of the cone for isotonic regression is the set of all constant sequences .",
    "the lineality space of the cone for convex regression is the set of all affine sequences .",
    "also , we say that two convex polyhedral cones @xmath233 and @xmath234 are orthogonal if @xmath235 for all @xmath236 and @xmath237 .",
    "[ lili ] for every @xmath0 with @xmath238 for some @xmath239 and @xmath240 ( i.e. , @xmath241 for all @xmath157 ) , we have @xmath242 .",
    "[ hol ] let @xmath15 be an arbitrary convex polyhedral cone .",
    "suppose @xmath243 are orthogonal polyhedral cones with lineality spaces @xmath244 such that @xmath245 .",
    "then @xmath246    the next lemma allows us to bound the risk of the lse at @xmath1 by a combination of the risk at @xmath191 and the distance between @xmath1 and @xmath191 .",
    "[ apps ] the risk of the lse satisfies the following inequality : @xmath247\\qquad \\qt{for every $ \\theta\\in{{\\mathcal{k}}}$}.\\ ] ]    we are now ready to prove theorem  [ zen ] .",
    "proof of theorem  [ zen ] by lemma  [ apps ] , it is enough to prove that @xmath248 fix @xmath165 , and let @xmath249 , which means that @xmath35 of the inequalities @xmath250 for @xmath251 are strict .",
    "let @xmath252 denote the indices of the inequalities that are strict .",
    "we partition the set @xmath78 into @xmath253 disjoint sets @xmath254 where @xmath255 and @xmath256 also for each @xmath257 , let @xmath258 we now apply lemma  [ hol ] with @xmath259 for @xmath260 .",
    "the lineality space of @xmath261 is , by definition , @xmath262 @xmath263 are orthogonal convex polyhedral cones because @xmath264 are disjoint .",
    "also @xmath265 because every @xmath41 can be written as @xmath266 where @xmath267 ( it is easy to check that @xmath268 for each @xmath14 ) .",
    "further , note that @xmath269 since @xmath270 for every @xmath14 .",
    "lemma  [ hol ] thus gives @xmath271 .",
    "inequality ( [ mw2 ] ) then implies that @xmath272 it is now easy to check that @xmath273 for each @xmath14 which proves ( [ zens ] ) .",
    "the proof of ( [ zen.eq ] ) is now complete by the observation @xmath274 as @xmath275 .",
    "in this section , we provide a proof of inequality ( [ pyaa ] ) using an explicit formula of the lse in isotonic regression .",
    "our proof does not require normality of @xmath8 .",
    "we also explain the similarities between inequalities ( [ zen.eq ] ) and ( [ pyaa ] ) .",
    "before we get to inequality ( [ pyaa ] ) , however , we give a brief overview of existing theoretical results in isotonic regression .",
    "usually isotonic regression is posed as a function estimation problem in which the unknown object of interest is a nondecreasing function @xmath276 , and one observes data from model ( [ eq : mdl2 ] ) with @xmath277 , where @xmath278 are fixed design points .",
    "the most natural and commonly used estimator for this problem is the monotone lse defined as any nondecreasing function @xmath279 on @xmath280 for which @xmath281 .",
    "this estimator was proposed by @xcite and @xcite ; also see @xcite for the related problem of estimating a nonincreasing density .",
    "note that @xmath77 can be computed easily using the _ pool adjacent violators algorithm _ ; see @xcite , chapter  1 .",
    "existing theoretical results on isotonic regression can be grouped into two categories : ( 1 ) results on the behavior of the lse at an interior point ( which is sometimes known as local behavior ) , and ( 2 ) results on the behavior of a global loss function measuring how far @xmath279 is from @xmath276 .",
    "results on the local behavior are proved , among others , in @xcite . under certain regularity conditions on the unknown function @xmath276 near the interior point @xmath282 , it was proved in @xcite that @xmath283 converges to @xmath284 at the rate @xmath285 and also characterized the limiting distribution of @xmath286 . in the related ( nonincreasing ) density estimation problem",
    ", the authors of @xcite demonstrated that if the interior point @xmath282 lies on a flat stretch of the underlying function , then the lse ( which is also the nonparametric maximum likelihood estimator , usually known as the grenander estimator ) converges to a nondegenerate limit at rate @xmath287 , and they characterized the limiting distribution . in @xcite , cator demonstrated that the rate of convergence of @xmath283 to @xmath284 depends on the local behavior of @xmath276 near @xmath282 , and explicitly described this rate for each @xmath276 . in this sense ,",
    "the lse @xmath279 adapts automatically to the unknown function @xmath276 . in @xcite",
    ", it was also proved that the lse is optimal for local behavior by establishing a local asymptotic minimax lower bound .    often in monotone regression ,",
    "the interest is in the estimation of the entire function @xmath276 , as opposed to just its value at one fixed point . in this sense",
    ", it is more appropriate to study the behavior of @xmath279 under a global loss function .",
    "the most natural and commonly studied global loss function in this setup is @xmath288 where @xmath289 and @xmath290 .",
    "note that under this loss function , the function estimation problem becomes exactly the same as the sequence estimation problem described in ( [ eq : mdl2 ] ) , where the goal is to estimate the vector @xmath291 under the constraint @xmath99 and the loss function ( [ eq : loss ] ) .",
    "the behavior of @xmath77 , under the loss @xmath292 , has been studied in a number of papers including @xcite .",
    "if one looks at the related ( nonincreasing ) density estimation problem , birg @xcite developed nonasymptotic risk bounds for the grenander estimator , measured with the @xmath293-loss , whereas van de geer @xcite has results on the hellinger distance .",
    "as mentioned in the , the strongest existing bound on @xmath294 is due to @xcite , theorem  2.2 .",
    "we recalled this inequality in ( [ motw ] ) and compared it to our bound ( [ pyaa ] ) in some situations .    in the following theorem , we prove inequality ( [ pyaa ] ) using explicit characterization of @xmath77 without requiring normality of @xmath8 .",
    "in fact , we prove an inequality that is slightly stronger than ( [ pyaa ] ) .",
    "we need the following notation . for simplicity ,",
    "we use @xmath295 for @xmath77 and @xmath296 for the components of @xmath77 . for any sequence @xmath297 and any @xmath298 ,",
    "let @xmath299 we will use this notation mainly when @xmath300 equals @xmath301 , @xmath1 or @xmath302 .",
    "our proof uses ideas similar to those in @xcite , section  2 ( see remark  [ annr ] for details about the connections with @xcite , section  2 ) and is based on the following explicit representation of the lse @xmath295 ( see @xcite , chapter  1 ) : @xmath303 for @xmath304 , we write @xmath305 and @xmath306 . for @xmath99 and @xmath307 , let @xmath308 where @xmath309 and @xmath96 , for @xmath310 . like @xmath104 , this quantity @xmath311",
    "can also be treated as a measure of the variation of @xmath1 with respect to @xmath83 .",
    "this measure also satisfies @xmath312 for every @xmath99 .",
    "moreover , @xmath313 when @xmath106 is the trivial partition , @xmath311 turns out to be just the standard deviation of @xmath1 .",
    "in general , @xmath314 is analogous to the within group sum of squares term in anova with the blocks of @xmath83 being the groups .",
    "below , we prove a stronger version of ( [ pyaa ] ) with @xmath311 replacing @xmath104 in the definition of @xmath124 .",
    "[ rafi ] suppose @xmath315 are observations from model ( [ eq : mdl2 ] ) with @xmath8 being i.i.d .",
    "with mean zero and variance @xmath75 . for every @xmath99",
    ", the risk of @xmath316 satisfies the following inequality : @xmath317    fix @xmath318 and @xmath319 . by ( [ eq : charac ] )",
    ", we have @xmath320 where , in the last equality , we used @xmath321 . by the monotonicity of @xmath1",
    ", we have @xmath322 for all @xmath323 .",
    "therefore , for every @xmath99 , we get @xmath324 taking positive parts , we have @xmath325 squaring and taking expectations on both sides , we obtain @xmath326 using the elementary inequality",
    "@xmath327 we get @xmath328 we observe now that , for fixed integers @xmath329 and @xmath330 , the process @xmath331 is a martingale with respect to the filtration @xmath332 where @xmath333 is the sigma - field generated by the random variables @xmath334 and @xmath335 .",
    "therefore , by doob s inequality for submartingales ( see , e.g. , theorem  5.4.3 of @xcite ) , we have @xmath336 so using the above result we get the following pointwise upper bound for the positive part of the risk : @xmath337 note that the above upper bound holds for any arbitrary @xmath330 , @xmath338 . by a similar argument",
    "we can get the following pointwise upper bound for the negative part of risk which now holds for any @xmath330 , @xmath339 : @xmath340 let us now fix @xmath307 .",
    "let @xmath102 and @xmath341 for @xmath103 .",
    "for each @xmath342 , we define two integers @xmath343 and @xmath344 in the following way : @xmath345 and @xmath346 when @xmath347 .",
    "we use this choice of @xmath348 in ( [ eq : sc1 ] ) and @xmath349 in ( [ eq : sc2 ] ) to obtain @xmath350 where @xmath351 and @xmath352 this results in the risk bound @xmath353 we shall now prove that @xmath354 and @xmath355 we give below the proof of ( [ kk1 ] ) , and the proof of ( [ kk2 ] ) is nearly identical . using the form of @xmath356 , we break up @xmath357 into two terms . for the first term , note that @xmath358 , for @xmath359 and therefore @xmath360 by lemma  11.2 in the supplementary material @xcite , we get @xmath361 for every @xmath94 . thus summing over @xmath94 , and multiplying by @xmath362 proves that the first term in @xmath357 is bounded from above by @xmath363 . to bound the second term , we write @xmath364 since the harmonic series @xmath365 is at most @xmath366 for @xmath367",
    ", we obtain @xmath368 where the last inequality is a consequence of the concavity of the logarithm function .",
    "this proves ( [ kk1 ] ) because @xmath369 .",
    "combining ( [ kk1 ] ) and ( [ kk2 ] ) proves the theorem .",
    "[ remp ] for each @xmath307 , let @xmath370 denote the set of all @xmath371 such that @xmath191 is constant on each set @xmath372 for @xmath192 .",
    "then it is easy to see that @xmath373 using this , it is easy to see that inequality ( [ rafi.eq ] ) is equivalent to @xmath374 inequality ( [ rafi.eq ] ) therefore differs from inequality ( [ hh ] ) only by its multiplicative constants . it should be noted that we proved ( [ hh ] ) assuming normality of @xmath375 while ( [ rafi.eq ] ) was proved without using normality .",
    "inequality ( [ pyaa ] ) is slightly weaker than ( [ rafi.eq ] ) because @xmath376 .",
    "we still work with ( [ pyaa ] ) in isotonic regression as opposed to ( [ rafi.eq ] ) because it is easier to compare ( [ pyaa ] ) to existing inequalities , and also , as we shall show in section  [ lmnop ] , inequality ( [ pyaa ] ) is nearly optimal .    [ annr ] bounding the infimum in the right - hand side of ( [ rafi.eq ] ) by taking @xmath126 and letting @xmath143 , we obtain @xmath377 this inequality might be implicit in the arguments of @xcite , section  2 .",
    "it might be possible to prove ( [ annr.eq ] ) by applying @xcite , theorem  2.1 , to each of the @xmath253 constant pieces of @xmath1 and by bounding the resulting quantities via arguments in @xcite , proof of theorem  2.2 .",
    "[ ach ] note that @xmath71 does not have to be small for ( [ pyaa ] ) to be an improvement of ( [ motw ] ) .",
    "one only needs that @xmath104 be small for some partition @xmath83 with small @xmath378 .",
    "equivalently , from ( [ nsv ] ) , one needs that @xmath379 is small for some @xmath371 with small @xmath380 .",
    "this is illustrated below .",
    "let @xmath381 be an arbitrary countable subset of @xmath382 $ ] , and let @xmath383 denote any probability sequence , that is , @xmath384 .",
    "fix @xmath160 , and let @xmath385 for @xmath386 .",
    "we will argue below that , for many choices of @xmath383 , inequality ( [ nsv ] ) gives a faster rate of convergence than @xmath4 , even though @xmath71 can be as large as @xmath23 .",
    "indeed , fix @xmath387 , and define @xmath388 .",
    "it is then clear that @xmath203 .",
    "also for each @xmath389 , we have @xmath390 this implies that @xmath391 .",
    "thus inequality ( [ nsv ] ) gives @xmath392.\\ ] ] when @xmath393 , it can be checked that the bound above is faster than @xmath4 .",
    "this happens , for instance , when @xmath394 for @xmath395 .",
    "in fact , when @xmath396 , ( [ alit ] ) gives the parametric rate up to logarithmic factors .    however , when , say @xmath394 for @xmath397 , ( [ alit ] ) does not give a rate that is faster than @xmath4",
    ". it might be possible here to use a different approximation vector @xmath371 which would still yield a rate of @xmath398 , but we do not have a proof of this . a result from the literature that is relevant here",
    "is @xcite , inequality ( 2.10 ) . for the vectors @xmath1 considered above ( and in certain more general situations ) , this inequality gives an asymptotic bound of @xmath398 ( without quantifying the exact order ) for the risk for all choices of @xmath399 and @xmath400 , even for @xmath401 .",
    "we prove in theorem  [ sabya2 ] in the next section that the bound given by theorem  [ rafi ] is always smaller than a logarithmic multiplicative factor of the usual cube root rate of convergence for every @xmath99 with @xmath402 . here",
    ", we shall demonstrate this in the special case of the sequence @xmath403 where the bound in ( [ rafi.eq ] ) can be calculated exactly .",
    "indeed , if @xmath404 with @xmath405 and @xmath406 , direct calculation gives @xmath407 now hlder s inequality gives @xmath408 which means that @xmath409 .",
    "therefore , for every fixed @xmath410 such that @xmath411 is an integer , @xmath314 is minimized over all partitions @xmath83 with @xmath412 when @xmath413 .",
    "this gives @xmath414 .",
    "as a consequence , theorem  [ rafi ] yields the bound @xmath415 now with the choice @xmath416 , we get the cube root rate for @xmath295 up to logarithmic multiplicative factors in @xmath23 .",
    "we generalize this to arbitrary @xmath99 with @xmath417 in theorem  [ sabya2 ] .",
    "in this section , we state some results about the quantity @xmath418 appearing in our risk bound ( [ pyaa ] ) .",
    "recall also the quantity @xmath419 that appears in ( [ motw ] ) .",
    "the first result of this section states that @xmath153 is always bounded from above by @xmath122 up to a logarithmic multiplicative factor in @xmath23 .",
    "this implies that ( [ pyaa ] ) is always only slightly worse off than ( [ motw ] ) ( by a logarithmic multiplicative factor ) while being much better when @xmath1 is well - approximated by some @xmath371 for which @xmath380 is small .",
    "recall that @xmath420 .",
    "the proofs of all the results in this section can be found in the supplementary material @xcite .",
    "[ sabya2 ] for every @xmath99 , we have @xmath421 whenever @xmath422    in the next result , we characterize @xmath153 for certain strictly increasing sequences @xmath1 where we show that it is essentially of the order @xmath423 . in some sense",
    ", @xmath124 is maximized for these strictly increasing sequences .",
    "the prototypical sequence we have in mind here is @xmath424 for @xmath425 .",
    "[ cst ] suppose @xmath426 with @xmath427 for a positive constant @xmath428 .",
    "then we have @xmath429 provided @xmath430    an important situation where ( [ lcr ] ) is satisfied is when @xmath1 arises from sampling a function on @xmath382 $ ] at the points @xmath431 for @xmath386 , assuming that the derivative of the function is bounded from below by a positive constant .",
    "next we describe sequences @xmath1 for which @xmath153 is @xmath432 , up to multiplicative factors . for these sequences",
    "our risk bound is potentially far superior to @xmath154 .",
    "[ vt ] let @xmath433 with @xmath434 where @xmath435 .",
    "then @xmath436 provided @xmath437",
    "in this section , we establish an optimality property of the lse .",
    "specifically , we show that @xmath295 is locally minimax optimal in a nonasymptotic sense .",
    "`` local '' here refers to a ball @xmath438 around the true parameter @xmath1 for a positive constant @xmath439 .",
    "the reason we focus on local minimaxity , as opposed to the more traditional notion of global minimaxity , is that the rate @xmath124 changes with @xmath1 .",
    "note that , moreover , lower bounds on the global minimax risk follow from our local minimax lower bounds .",
    "such an optimality theory based on local minimaxity has been pioneered by cai and low @xcite and cator @xcite for the problem of estimating a convex or monotone function at a point .",
    "we start by proving an upper bound for the local supremum risk of @xmath295 .",
    "recall that @xmath440 .",
    "[ lub.eq ] the following inequality holds for every @xmath99 and @xmath441 : @xmath442    inequality ( [ pyaa ] ) gives @xmath443 for every @xmath444 .",
    "fix @xmath445 . by the triangle inequality ,",
    "we get @xmath446 . as a result , whenever @xmath447 , we obtain @xmath448 as a consequence , @xmath449 this proves ( [ eq : lub.eq ] ) .",
    "we now show that @xmath124 , up to logarithmic factors in @xmath23 , is a lower bound for the local minimax risk at @xmath1 , defined as the infimum of the right - hand side of  ( [ eq : lub.eq ] ) over all possible estimators @xmath295 .",
    "we prove this under each of the assumptions ( 1 ) and ( 2 ) ( stated in the ) on @xmath1 . specifically , we prove the two inequalities ( [ inmi1 ] ) and ( [ inmi2 ] ) .",
    "these results mean that , when @xmath1 satisfies either of the two assumptions ( 1 ) or ( 2 ) , no estimator can have a supremum risk significantly better than @xmath153 in the local neighborhood @xmath450 . on the other hand ,",
    "lemma  [ lub.eq ] states that the supremum risk of the lse over the same local neighborhood is bounded from above by a constant multiple of @xmath153 .",
    "putting these two results together , we deduce that the lse is approximately locally nonasymptotically minimax for such sequences @xmath1 .",
    "we use the qualifier `` approximately '' here because of the presence of logarithmic factors on the right - hand sides of ( [ inmi1 ] ) and ( [ inmi2 ] ) .",
    "we make here the assumption that the errors @xmath111 are independent and normally distributed with mean zero and variance @xmath75 . for each @xmath99 , let @xmath451 denote the joint distribution of the data @xmath315 when the true sequence equals  @xmath1 . as a consequence of the normality of the errors , we have @xmath452 where @xmath453 denotes the kullback ",
    "leibler divergence between the probability measures @xmath454 and @xmath65 .",
    "our main tool for the proofs is assouad s lemma , the following version of which is a consequence of lemma  24.3 of @xcite , page 347 .",
    "[ suad ] let @xmath330 be a positive integer and suppose that , for each @xmath455 , there is an associated nondecreasing sequence @xmath456 in @xmath457 , where @xmath457 is a neighborhood of @xmath1 .",
    "then the following inequality holds : @xmath458 where @xmath459 denotes the hamming distance between @xmath460 and @xmath461 and @xmath462 denotes the total variation distance between probability measures .",
    "the infimum here is over all possible estimators @xmath137",
    ".    inequalities ( [ inmi1 ] ) and ( [ inmi2 ] ) are proved in the next two subsections .      in this section",
    ", we assume that @xmath1 is a strictly increasing sequence with @xmath463 and that @xmath464 for some @xmath465 $ ] and @xmath466 . because @xmath467 , assumption ( [ cr ] ) means that the increments of @xmath1 are in a sense uniform .",
    "an important example in which ( [ cr ] ) is satisfied is when @xmath468 for some function @xmath276 on @xmath382 $ ] whose derivative is uniformly bounded from above and below by positive constants .    in the next theorem",
    ", we prove that the local minimax risk at @xmath1 is bounded from below by @xmath153 ( up to logarithmic multiplicative factors ) when @xmath1 satisfies ( [ cr ] ) .",
    "[ oval ] suppose @xmath1 satisfies ( [ cr ] ) , and let @xmath469 then the local minimax risk @xmath470 satisfies the following inequality : @xmath471 provided @xmath472    theorem  [ oval ] is closely connected to minimax lower bounds for lipschitz classes of functions .",
    "indeed , using the notation @xmath473 for functions @xmath474 on @xmath382 $ ] , it can be argued that @xmath475 is a subset of @xmath134 for appropriate positive constants @xmath476 and @xmath477 .",
    "lower bound ( [ oval.eq ] ) then follows from usual lower bounds for lipschitz classes which are outlined , for example , in @xcite , chapter  2 . a direct proof of theorem  [ oval ] is included in the supplementary material @xcite .      here",
    ", we again show that the local minimax risk at @xmath1 is bounded from below by @xmath124 ( up to logarithmic multiplicative factors ) .",
    "the difference from the previous section is that we work under a different assumption from ( [ cr ] ) . specifically , we assume that @xmath143 and that the @xmath35 values of @xmath1 are sufficiently well separated and prove inequality ( [ inmi2 ] ) .",
    ". there exist integers @xmath478 with @xmath479 and @xmath480 such that @xmath1 is constant on each set @xmath481 for @xmath482 where @xmath102 and @xmath341 .",
    "also , let the values of @xmath1 on the sets @xmath483 for @xmath484 be denoted by @xmath485 .",
    "[ fani ] suppose @xmath486 for all @xmath487 for some @xmath465 $ ] and @xmath466 and that @xmath488 then , with @xmath134 defined as @xmath489 , the local minimax risk , @xmath490 , satisfies @xmath491 provided @xmath492    for notational convenience , we write @xmath493 first note that under assumption ( [ masa ] ) , theorem  [ vt ] implies that @xmath494 .",
    "let @xmath495 be a positive integer whose value will be specified later , and let @xmath496 for @xmath482 .",
    "we also write @xmath497 for @xmath498 .",
    "the elements of the finite set @xmath499 will be represented as @xmath500 where @xmath501 . for each @xmath502 , we specify @xmath503 in the following way . for @xmath504 , the quantity @xmath505 is defined as @xmath506 because @xmath1 is constant on the set @xmath507 where it takes the value @xmath508 , it follows that @xmath509 .",
    "this implies that @xmath510 for every @xmath460 as @xmath511 .    also ,",
    "because of the assumption @xmath512 , it is evident that each @xmath513 is nondecreasing",
    ". we will apply assouad s lemma to @xmath514 . for @xmath515",
    ", we have @xmath516 because @xmath517 we have @xmath518 also , from ( [ oro ] ) , we get @xmath519 the quantity @xmath520 can be easily bounded from below by noting that @xmath521 and that @xmath522 .",
    "this gives @xmath523 combining the above inequality with ( [ bas ] ) , we deduce @xmath524 this and pinsker s inequality give @xmath525 \\\\[-8pt ] \\nonumber & \\leq&\\frac{k^2 l^3 \\beta _",
    "n^2}{c_1 ^ 2 n^2 \\sigma^2}\\end{aligned}\\ ] ] whenever @xmath526 .    inequalities ( [ a1s ] ) and ( [ a2s ] ) in conjunction with assouad s lemma give @xmath527 because of ( [ ntr ] ) , we get @xmath528 , and thus @xmath529 the value of the integer @xmath530 will now be specified .",
    "we take @xmath531 because @xmath532 , we can ensure that @xmath533 by requiring that @xmath534 this gives rise to two lower bounds for @xmath23 which are collected in ( [ tie.con ] ) .    as a consequence of ( [ tar ] )",
    ", we get that @xmath535 , which ensures that the term inside the parentheses on the right - hand side of ( [ sog ] ) is at least @xmath536 .",
    "this gives @xmath537 to complete the proof , we use theorem  [ vt ] . specifically , the second inequality in  ( [ vt.eq ] ) gives @xmath538 the proof is complete by combining the above inequality with ( [ rar ] ) .",
    "we consider isotonic regression under the misspecified setting where the true sequence is not necessarily nondecreasing . specifically , consider model ( [ eq : mdl2 ] ) where now the true sequence @xmath1 is not necessarily assumed to be in @xmath80 .",
    "we study the behavior of the lse @xmath316 .",
    "the goal of this section is to prove an inequality analogous to ( [ pyaa ] ) for model misspecification .",
    "it turns out here that the lse is really estimating the nondecreasing projection of @xmath1 on @xmath80 defined as @xmath539 that minimizes @xmath540 over @xmath541 . from  @xcite , chapter",
    "1 , it follows that @xmath542 where @xmath543 is as defined in ( [ eq : shorthand ] ) .",
    "we define another measure of variation for @xmath444 with respect to an interval partition @xmath404 : @xmath544 where @xmath309 and @xmath545 for @xmath487 .",
    "it is easy to check that @xmath546 for every @xmath444 .",
    "the following is the main result of this section .",
    "the proofs of all the results in this section can be found in the supplementary material @xcite .",
    "[ thm : misrafi ] for every @xmath0 , the lse satisfies @xmath547    by theorem  [ sabya2 ] , the quantity @xmath548 is bounded from above by @xmath549 up to a logarithmic multiplicative factor in @xmath23 .",
    "therefore , theorem  [ thm : misrafi ] implies that the lse @xmath295 converges to the projection of @xmath1 onto the space of monotone vectors at least the @xmath4 rate , up to a logarithmic factor in @xmath23",
    ". the convergence rate will be much faster if @xmath550 is small or if @xmath551 is well approximated by a monotone vector @xmath191 with small @xmath380 .    by taking @xmath83 in the infimum in the upper bound of ( [ miss.eq ] ) to be the interval partition generated by @xmath551",
    ", we obtain the following result which is the analogue of ( [ amon ] ) for model misspecification .    for every arbitrary sequence @xmath1 of length @xmath23 ( not necessarily nondecreasing ) , @xmath552    in the next pair of results , we prove two upper bounds on @xmath550 .",
    "the first result shows that @xmath553 ( i.e. , @xmath551 is constant ) when @xmath1 is nonincreasing , that is , @xmath554 .",
    "this implies that the lse converges to @xmath551 at the rate @xmath555 when @xmath1 is nonincreasing .",
    "[ appr ] @xmath556 if @xmath1 is nonincreasing .    to state our next result ,",
    "let @xmath557 @xmath558 can be interpreted as the number of constant blocks of @xmath139 . for example , when @xmath559 and @xmath560 , @xmath561 .",
    "observe that @xmath562 for @xmath444 .",
    "[ sabya3 ] for any sequence @xmath0 , we have @xmath563 .    as a consequence of the above lemma",
    ", we obtain that for every @xmath564 , the quantity @xmath565 is bounded from above by @xmath566 .",
    "we thank the associate editor and the anonymous referees for constructive suggestions that significantly improved the paper ."
  ],
  "abstract_text": [
    "<S> we consider the problem of estimating an unknown @xmath0 from noisy observations under the constraint that @xmath1 belongs to certain convex polyhedral cones in @xmath2 . under this setting , we prove bounds for the risk of the least squares estimator ( lse ) . the obtained risk bound behaves differently depending on the true sequence @xmath1 which highlights the adaptive behavior of  @xmath1 . as special cases of our general result , we derive risk bounds for the lse in univariate isotonic and convex regression . </S>",
    "<S> we study the risk bound in isotonic regression in greater detail : we show that the isotonic lse converges at a whole range of rates from @xmath3 ( when @xmath1 is constant ) to @xmath4 ( when @xmath1 is _ uniformly increasing _ in a certain sense ) . </S>",
    "<S> we argue that the bound presents a benchmark for the risk of any estimator in isotonic regression by proving nonasymptotic local minimax lower bounds . </S>",
    "<S> we prove an analogue of our bound for model misspecification where the true @xmath1 is not necessarily nondecreasing .    </S>",
    "<S> ./style / arxiv - general.cfg    ,     + </S>"
  ]
}