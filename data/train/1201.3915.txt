{
  "article_text": [
    "compressive sensing ( cs ) in the presence of noise has been intensively investigated in many recent papers because any real - world device is subject to at least a small amount of noise .",
    "we refer to such problems as _ noisy compressive sensing _ ( ncs ) .",
    "let @xmath0 $ ] denote a random vector whose elements are sparsely non - zeros , called _",
    "sparse signal_. then , the ncs decoder observes a measurement vector @xmath1 \\in \\mathbb{r}^m$ ] , given as @xmath2 where @xmath3 is a deterministic sparse signal ; @xmath4 is a sensing matrix whose columns represent a possibly overcomplete basis , _",
    "i.e. _ , rank(@xmath5 , where @xmath6;and @xmath7 is an additive noise vector generated by a certain distribution .",
    "the ncs reconstruction problem has been discussed in terms of conventional @xmath8-norm approaches @xcite-@xcite . in @xcite-@xcite ,",
    "the authors assume a bounded noise and in @xcite,@xcite , an i.i.d .",
    "zero - mean gaussian noise is assumed , _",
    "i.e. _ , @xmath9 . in @xcite , candes and tao proposed an @xmath8-norm based reconstruction algorithm for the gaussian setup , called the _ dantzig selector _",
    "( l1-ds ) : @xmath10 where @xmath11 is the tolerance user defined paramter and @xmath12 denotes matrice tranposition",
    ". the reconstruction performance of l1-ds is proprtional to logarithmic factor , _",
    "i.e. _ , @xmath13 with a constant @xmath14 ( see th.1 in @xcite ) .",
    "alternatively , bayesian approaches to ncs have received attention @xcite-@xcite .",
    "this type of approach offers powerful mitigation of noise effects by using many existing statistical signal processing techniques and several statistical signal - noise models . in these approaches ,",
    "the reconstruction problem is described as the _ maximum a posteriori _ ( map ) estimation problem as follows : @xmath15 where gaussian noise is assumed and @xmath16 is a probability density function .    the most well - known bayesian approach is the _ sparse bayesian learning _ ( sbl ) algorithm @xcite-@xcite .",
    "the sbl algorithm iteratively determines the posterior density of the signal on basis of a three - layer hierarchical prior model , so the prior density is a function of certain parameters .",
    "the algorithm estimates the parameters of the prior using _ expectation maximization _ ( em ) and applies these parameters to finding the posterior .",
    "the sbl approach to sparse reconstruction was originally proposed in @xcite,@xcite .",
    "recently , ji _ et al .",
    "_ @xcite and babacan _ et al . _",
    "@xcite successfully applied the sbl approach to the ncs reconstruction problem with different prior model .",
    "another class of bayesian approaches is sparse reconstruction using sparse matrices @xcite-@xcite .",
    "the work is inspired by the success of _ low - density parity - check _",
    "( ldpc ) codes in channel coding field @xcite-@xcite .",
    "the use of the sparse matrix enables simple and fast signal acquisition that is feasible in real - world applications .",
    "in addition , these approaches can be made more attractive if they are applied in in conjunction with belief propagation ( bp ) .",
    "bp replaces the reconstruction process by iterative message - passing processes .",
    "this replacement reduces the reconstruction complexity to the @xmath17 order .",
    "baron _ et al .",
    "_ for the first time proposed the use of sparse matrices to the ncs setup and developed a bp - based algorithm , called cs - bp @xcite,@xcite .",
    "cs - bp iteratively updates the signal posterior from the two - state gaussian mixture prior via the message - passing algorithm , where the messages are the probability densities of the signal elements . in @xcite , tan _",
    "et al . _ proposed another bp - based algorithm called , bp - sbl .",
    "they applied bp to the sbl - framework in @xcite to reduce the complexity of the em algorithm .",
    "most recently , akcakaya _ et al .",
    "_ devised suprem using an idea similar to bp - sbl , but in a different framework @xcite which is based on gaussian scale mixture @xcite with a specific type of prior called the jeffreys prior @xcite .",
    "in addition , the authors restrict the story of suprem to a class of sensing matrices , called low - density frames , in which the matrices have fixed column and row weights .    in this paper , we propose a sparse reconstruction algorithm based on the bayesian approach and the use of sparse matrices .",
    "we call our algorithm as _ compressive sensing via bayesian support detection _ ( cs - bsd ) .",
    "cs - bsd has the following properties :    1 .",
    "robustness against the measurement noise effects . 2 .",
    "ability to perform as the minimum mean square error ( mmse ) estimator that has knowledge of the support set .",
    "3 .   fast convergence .",
    "cs - bsd has a _ detection - directed _ ( dd ) estimation structure which consists of signal support detection and signal value estimation , as shown in fig.[fig : fig1 - 1 ] .",
    "we consider the common procedure of first using the measurements at hand to detect the signal s support set .",
    "this detected support set is then used in the model of the sparse signal , and the value estimator is built as if the detected support set is in fact the correct set .",
    "the support detection component consists of a combination of the _ bayesian hypothesis test _ ( bht ) and bp , and signal value estimation using the detected support set is achieved via an mmse estimator .",
    "cs - bsd iterates the detection and estimation processes until the constraint in is met .",
    "the dd estimation methodology was investigated in @xcite for estimation of noisy signals and have been widely applied to wireless communication systems @xcite,@xcite . for cs ,",
    "the methodology was first reported in @xcite,@xcite ; we tailor the methodology to the ncs problem by refining that work .",
    "the complexity of cs - bsd is @xmath18 whereas that of the other bp - based algorithm is @xmath19 because cs - bsd includes the cost of mmse in addition to that of bp .",
    "however , cs - bsd converges faster than the other bp - based algorithms ; thus , its computational cost is lower in practice .",
    "in addition , cs - bsd can be much faster by converting to an parallel architecture .",
    "the rest of the paper is organized as follows .",
    "section ii introduces the sparse sensing matrix , the prior model for our system model .",
    "the details of cs - bsd are given in section iii .",
    "a few practical issues are discussed in section iv .",
    "we compare the numerical results of cs - bsd to the other recent cs algorithms in section v. section vi concludes the paper .",
    "for signal sensing , we employ sparse - bernoulli matrices @xmath21 , which have been successfully used in cs recently @xcite-@xcite . in the matrix , sparsely nonzero elements are equiprobably equal to @xmath22 or @xmath23 .",
    "we set the sparsity of @xmath20 using the fixed column weight @xmath24 .",
    "because the column weight rather than the row weight is fixed , all elements of @xmath25 have an even chance of being sensed .",
    "in addition , the fixed column weight unifies the energy of the basis of the measurement space spanned by the column vectors of @xmath20 .    with the sparse - bernoulli matrix",
    ", the linear system @xmath26 can be represented over a bipartite graph .",
    "let @xmath27 denote a set of indices corresponding to the elements of the signal vector , @xmath28 $ ] .",
    "similarly , @xmath29 denotes a set of indices corresponding to the elements of the measurement vector , @xmath30 $ ] .",
    "in addition , we define a set of edges connecting @xmath31 and @xmath32 as @xmath33 where @xmath34 is the @xmath35-th element of @xmath36 .",
    "then , a bipartite graph @xmath37 fully describes the neighboring relation in the linear system .",
    "furthermore , we define the neighbor set of @xmath31 and @xmath32 as @xmath38 for all @xmath39 and @xmath40 for all @xmath41 , respectively .",
    "note that @xmath42 for all @xmath43 under our assumption regarding @xmath20 .",
    "fig.[fig : fig4 - 1 ] depicts a simple example of the graphical representation corresponding to @xmath44 .",
    "we limit our discussion to the random vector @xmath45 whose elements are i.i.d . random variables .",
    "this assumption is commonly used in many papers @xcite-@xcite .",
    "we characterize the signal sparsity in a probabilistic manner , called _",
    "sparsity rate_. the sparsity rate @xmath46 is defined as @xmath47 for all @xmath39 .",
    "namely , each signal element independently belongs to the signal support set with the rate @xmath46 .",
    "the supportiveness of each signal element is represented by a state variable @xmath48 , defined as @xmath49 hence , we model the prior density of @xmath50 using a _ spike - and - slab _ model originating in a two - state mixture density as follows : @xmath51 where @xmath52 indicates a dirac distribution having nonzero value between @xmath53 $ ] and @xmath54 . in the prior density ,",
    "we use gaussian density @xmath55 for @xmath56 although it includes the probability mass at @xmath57 .",
    "the reason is the probability mass at @xmath57 is very small and gaussian densities are mathematically tractable .",
    "in addition , we drop the index @xmath58 from the prior density under the assumption of i.i.d .",
    "the spike - and - slab prior has been widely employed in bayesian inference problems @xcite-@xcite and was recently applied to cs @xcite as well .",
    "in this section , we discuss the details of the proposed algorithm based on the dd estimation structure .",
    "the proposed algorithm , cs - bsd , is an iterative algorithm that repeats the support detection and signal value estimation processes until @xmath59 is met .",
    "the decoder detects the signal support in each element unit .",
    "namely , the supportive state of each signal element is detected independently and converted to the support set information for the signal .",
    "first , the following simple hypothesis test can be considered for the state detection of @xmath50 : @xmath60 where @xmath61 and @xmath62 are two possible hypotheses .",
    "if we marginalize over @xmath48 , the left hand side of becomes @xmath63 where @xmath64 the right hand side of is @xmath65 where @xmath66 from and , the hypothesis test in is refined as @xmath67 here @xmath68 where the posterior density , @xmath69 , is gaussian because the signal and noise are assumed to be gaussian ( see p.326 in @xcite ) .",
    "the term of @xmath70 in right hand side of is caused by the use of gaussian density @xmath71 for the prior of nonzero @xmath50 . because the variance of @xmath72 is a function of the noise variance , the probability @xmath73 is very small , and it approaches zero as the snr increases .",
    "therefore , we suggest setting the threshold of the hypothesis test in to 1 .",
    "this implies that the hypothesis test can detect the supportive state of the signal elements with a high success probability if snr is sufficiently high .",
    "we now describe how to obtain the probability ratio , @xmath74 . by factorizing over @xmath50 ,",
    "the ratio becomes @xmath75 where @xmath76 denotes the posterior density of @xmath50 given @xmath77 .",
    "the signal elements are not i.i.d .",
    "anymore given @xmath77 . in",
    ", @xmath78 holds true since the measurements @xmath77 does not provide any additional information on the state given @xmath50 . using the bayesian rule and the prior information ,",
    "we finally obtain the hypothesis test as the following form : @xmath79 since we know the prior of the state @xmath80 from the sparsity rate , _",
    "i.e. _ , @xmath81 , we can move the prior term to the right side , and then treat it as a threshold for the hypothesis test @xmath82 .",
    "therefore , the state of each elements can be sensed from the corresponding posterior and prior densities .",
    "let @xmath83 denote the detected state of @xmath50 ; @xmath84 indicates the posterior density of @xmath50 ; , and @xmath85 denotes the conditional prior density of a signal element given the state .",
    "then , state detection for all @xmath43 is performed by choosing the hypothesis that result from @xmath86 @xmath87      the posterior density used for the bht is obtained and updated at every iteration via bp .",
    "our bp process is similar to that in @xcite,@xcite and was independently devised from @xcite,@xcite .",
    "distinctively , our bp process uses the information on the noise distributions @xmath88 under the i.i.d .",
    "zero - mean gaussian noise assumption .",
    "using bayesian rule , we can represent the posterior density of @xmath50 in the form of @xmath89 , given as @xmath90 if the sensing matrix @xmath20 is sufficiently sparse such that the corresponding bipartite graph is tree - like , we postulate that the elements of @xmath77 associated with @xmath50 are independent of each other given @xmath50 @xcite . under the tree - like assumption , we can decompose the likelihood density @xmath91 to the product of densities : @xmath92 we call each decomposition of the likelihood , @xmath93 the _ measurement density_. theorem 1 below demonstrates that the measurement density can be composed of the densities of the associated signal elements .    the measurement density @xmath93 is expressed as the linear convolution of all the associated distributions of the signal elements and the corresponding noise distribution @xmath94 as follows : @xmath95 @xmath96 , where @xmath97 and @xmath98 are the operator for linear convolution and the linear convolution of a sequence of functions , respectively + _ proof _ : see appendix a.    therefore , the essence of the bp - process is to update the signal and measurement densities by exchanging probability density messages , associated with the neighboring relation in the bipartite graph .",
    "let @xmath99 denote the message from the @xmath58-th signal element to the @xmath100-th measurement element , called the signal message ; @xmath101 is the message from the @xmath100-th measurement element to the @xmath58-th signal element , called the measurement message .",
    "the signal message is an approximation of the density of the signal element , _",
    "i.e. _ , @xmath102 and it is obtained from simply by replacing the measurement density with the measurement message of the previous iteration . note that in bp - process the message coming from the @xmath100-th measurement is excluded in the calculation of @xmath99 .",
    "thus , the signal message at the @xmath103-th iteration is expressed as @xmath104\\end{aligned}\\ ] ] @xmath96 , where @xmath105 $ ] is the normalization function to make @xmath106 .",
    "similarly , the measurement message approximates the measurement density , _",
    "i.e. _ , @xmath107 , and it is obtained from the expression of by replacing the associated densities of signal elements @xmath108 with the signal messages for the purpose of iteration , that is , @xmath109    the convolution operations in can be efficiently computed by using the _ fast fourier transform _ ( fft ) .",
    "therefore , we express for the measurement message calculation as @xmath110\\end{aligned}\\ ] ] where @xmath111 denotes a fourier matrix of size @xmath112 .",
    "in fact , the use of the fft brings a small calculation gap between this result and that of since the fft - based calculation performs a circular convolution that produces output having a heavy tail , as shown in fig.[fig : fig4 - 3 ] .",
    "the heaviness increases as the corresponding row weights in @xmath20 increase .",
    "however , the difference is can be ignored , especially when the densities are bell - shaped distributions .    finally , the update of the posterior density of @xmath50 at the @xmath103-th iteration is provided as given in definition 2 .",
    "[ def1]*definition *    [ def2 ] let @xmath113 denote a measurement message at the @xmath103-th iteration for all @xmath114 .",
    "then , the posterior density of @xmath50 at the @xmath103-th iteration is calculated by @xmath115,\\end{aligned}\\ ] ] where @xmath105 $ ] is the normalization function that makes @xmath116 .",
    "we now describe signal value estimation based on the dd estimation structure .",
    "the dd estimator is basically an estimator that determines how to act on the input data directed by the information from the detector . in cs - bsd ,",
    "the detector provides the support information @xmath117 , and the value estimator then finds the signal values as if the detected support set is the correct set at each iteration .",
    "that is , @xmath118 where the estimator decides that @xmath119 for all @xmath120 . from the argument in",
    ", the dd estimate converges to the true signal @xmath25 since the detected support set becomes the correct set as snr and the number of iterations @xmath103 increases .",
    "this dd methodology makes no general claim regarding optimality of the solution ; however , it is common and often successful .",
    "let @xmath121 denote a random vector consisting of the elements with @xmath122 .",
    "then , the problem in is reduced to @xmath123 since @xmath124 and the noise elements are assumed to be zero - mean i.i.d .",
    "gaussian random variables with variance @xmath125 and @xmath126 respectively , the map estimation in is recast as @xmath127 where @xmath128 denotes a submatrix of @xmath20 corresponding to @xmath129 .",
    "in addition , the map and mmse estimates are identical , assuming the signal and noise are gaussian ( see p.358 in @xcite ) .",
    "therefore , the estimate @xmath130 can be obtained by the mmse estimator @xmath131 to combine the support information @xmath117 and the estimated values @xmath132 , we define an index set @xmath133 corresponding to the elements @xmath134 $ ] and a bijective mapping function @xmath135 . then , the reconstruction at each iteration is readily obtained from @xmath136 for all @xmath39 .",
    "cs - bsd is summarized in algorithm [ alg : cs - bsd ] .",
    "we implement the bp process in cs - bsd based on the sampled - message approach in @xcite .",
    "the density messages @xmath137 are vectors of size @xmath112 where @xmath112 is chosen to be power of two for efficient use of fft .",
    "next , we analyze the complexity of cs - bsd by considering each part seperately .",
    "let us consider the complexity of bp first . since the matrix @xmath138 has the fixed column weight @xmath24 and the size for a density vector is @xmath112 , the decoder requires @xmath139 flops per iteration to calculate the signal message @xmath99 in , and @xmath140 flops per iteration to calculate the measurement message @xmath101 in , since the row weight is @xmath141 on average and the cost of the fft - based convolution is @xmath142 .",
    "hence , the per - iteration cost for all probability messages is @xmath143 flops . for the bht in",
    ", the decoder requires @xmath144 flops to calculate a likelihood ratio .",
    "the cost for the hypothesis test is much smaller than that of bp ; therefore , it is ignored .",
    "let us fix the signal sparsity as the expected value of the cardinality of the support set , _",
    "i.e. _ , @xmath145=nq$ ] , for purpose of comparison .",
    "then , the complexity of the mmse estimation in depends strongly upon @xmath146 such that conventionally it requires @xmath147 flops if qr decomposition is used @xcite .",
    "thus , the total complexity of cs - bsd is @xmath148 flops where @xmath149 denotes the number of iterations .",
    "if @xmath24 and @xmath112 are fixed , the complexity of cs - bsd can be simplified to @xmath150 flops .",
    "the bp process is known to converge within @xmath151 @xcite such that its complexity is @xmath152 .",
    "if we fix the number of iteration @xmath149 empirically , we can remove the mmse operation from the iterations . in that case , the complexity is reduced to @xmath153 .      the bp process for finding the posterior finding can be implemented using a parallel architecture .",
    "indeed , many parallelized bp algorithms , with applications to ldpc codes , have demonstrated superior performance in @xcite-@xcite .",
    "the graph representation of the sparse sensing matrix shows that the dependencies of the message calculations for any signal elements ( or measurement elements ) depend only upon the corresponding measurement elements ( or signal elements ) .",
    "this allows all messages in bp to be computed in a parallel manner .",
    "therefore , implementing bp on a parallel architecture for bp yields low power consumption , high - speed decoding , and simple logic @xcite .",
    "we demonstrate the advantages of cs - bsd using simulation results in several different settings . to show its average performance , we take 200 monte carlo trials for each point in the simulation .",
    "in each trial , we generate the deterministic sparse signal @xmath25 with @xmath154 and @xmath155 whose values are represented with finite precision .",
    "the finite precision is provided by 6-bit quantization such that each signal value has 64 levels .",
    "this assumption of finite precision for the signal values is reasonable in terms of digital signal processing and implementation .",
    "in addition , we restrict the magnitude level of the signal elements to @xmath156 for the same reason .",
    "we define the snr as @xmath157 and @xmath158 as the undersampling ratio for signal acquisition .      to determine the performance of the support detector in cs - bsd , we defined the _ state error rate _ ( ser ) as : @xmath159 is the state variable corresponding to the true signal value @xmath160 .",
    "we simulate the ser performance as a function of the snr for a variety of undersampling ratio @xmath158 . in this simulation",
    ", we set @xmath161 , @xmath162 , and @xmath163 .",
    "in addition .",
    "we compare the ser performance to a theoretical limit on the support recovery given by fletcher _",
    "they found a necessary condition for _ maximum - likelihood _ ( ml ) estimation to asymptotically recover the support set if the sensing matrix has i.i.d .",
    "gaussian entries .",
    "the ml estimation is described as @xmath164 where the signal sparsity @xmath165 is assumed to be known , @xmath166 is a subset of the index set of the signal , and @xmath167 denotes the orthogonal projection of @xmath77 onto the subspace spanned by columns of @xmath20 corresponding to @xmath168 .",
    "namely , the ml estimate is a subset of @xmath31 such that the subspace spanned by the corresponding columns of @xmath20 contain the maximum energy of @xmath77 .",
    "we rewrite the necessary condition in terms of snr such that @xmath169 where minimum - to - average ratio ( mar ) is defined as @xmath170 in this comparison , we used 200 monte carlo trials to find the average snr@xmath171 , _",
    "i.e. _ , @xmath172 $ ] . in fig.[fig : fig5 - 1 ] , the ser curves show a waterfall behavior ; the curves decline rapidly to less than @xmath173 beyond a certain threshold snr .",
    "this behavior supports the argument in that the bht achieves successful support detection in the high snr regime .",
    "we consider the ser=@xmath173 bound as an almost error - free bound since it is much less than the rate of one state error @xmath174 when @xmath154 .",
    "the threshold snr for the error - free bound is roughly 34.8 db for @xmath158=0.3 , 32.9 db for @xmath175 , and 31.1 db for @xmath176 .",
    "remarkably , this threshold snr approaches to@xmath177 as @xmath158 increases .",
    "for example , the gap between the limit and the simulation result is 0.58 db for @xmath178 ; however , the gap is only 0.2 db for @xmath176 . for @xmath179 , since the sensing matrix @xmath20 is not sufficiently sparse , the tree - like assumption regarding @xmath20 is rarely satisfied .",
    "such a fact occasionally causes the bp - process to diverge , leading to severe errors in support detection .",
    "we consider the reconstruction performance in terms of normalized _ means square error _",
    "( mse ) , which is defined as @xmath180.\\end{aligned}\\ ] ] we compare our algorithm to several recent cs reconstruction algorithms : 1 ) cs - bp @xcite,@xcite , 2 ) l1-ds via linear programming @xcite , 3 ) bayesian cs ( bcs ) @xcite , 4 ) cosamp @xcite , and 5 ) suprem ( reweighted version ) @xcite .",
    "for bcs and suprem , we obtained the source code from each author s webpage ; for cosamp we used stephen becker s code ( available at http://www.ugcs.caltech.edu/~srbecker/algorithms.shtml ) .",
    "l1-ds is provided by the l1-magic package ( available at http://users.ece.gatech.edu/~justin/l1magic/).we implemented cs - bp algorithm by using the sampled - message approach and upgrading the original algorithm to use the noise information .",
    "for cs - bp , we used the sparse - bernoulli sensing matrix with @xmath163 ; for suprem , we use a sensing matrix generated from a low - density frame @xcite with the same parameters ( @xmath181 , @xmath182 , @xmath24 ) .",
    "l1-ds , cosamp and bcs were used with a gaussian sensing matrix having the same column energy as the sparse - bernoulli matrix , for fairness , _",
    "i.e. _ , @xmath183 .",
    "the sparsity of an input parameter in cosamp and suprem was set according to the expectation of the cardinality of the support set @xmath184=nq$ ] .",
    "those algorithms are summarized in table [ table1 ] , with respect to thier complexity , type of sensing matrix , prior type , and algorithm type .      in fig.[fig : fig5 - 2 ] , we show the mse performance as a function of snr where @xmath176 , @xmath161 , and @xmath162 . in the high snr regime ,",
    "the advantage of cs - bsd becomes remarkable .",
    "as the snr increases , the mse of cs - bsd approaches to that of an mmse estimator that has knowledge of the support set , defined as @xmath185}}{{\\left\\| { { \\bf{x}}_{{{0,supp } } } } \\right\\|_2 ^ 2 } } , \\end{aligned}\\ ] ] where tr@xmath186 $ ] denote the matrix trace operation . beyond snr=31 db ,",
    "since the ser of cs - bsd is almost error - free , the mse performance achieves @xmath187 at @xmath188 .",
    "surprisingly , this result is superior to that of the @xmath8 norm based approach , which is known as an optimal algorithm in the noiseless case .",
    "the gap between the two algorithms is caused by the reconstruction error over the non - supporting elements .",
    "cs - bsd completely removes the error from the non - supporting elements whereas the @xmath8 norm based approach leaves a certain amount of the reconstruction error on the non - supporting elements .    in the low snr regime , it is noteworthy that cs - bsd works well although the proposed algorithm was originally targeted at a reasonable system having high snr .",
    "for example , cs - bsd achieves mse=@xmath189 at snr=14 db in fig.[fig : fig5 - 2 ] , which provides 3 db snr gain from l1-ds ; 2 db gain from cosamp ; 1 db gain from cs - bp and suprem .",
    "to support this result , we present fig.[fig : fig4 - 2 ] which describes the iterative behavior to find the posterior of @xmath50 given @xmath77 at snr=10db . if @xmath190 , most of the probability mass in the posterior stays at the zero - spike as shown in fig.[fig : fig4 - 2]-(a ) ; if @xmath191 , the probability mass gradually shifts toward an estimated value as shown in fig.[fig : fig4 - 2]-(b ) , over the iteration .",
    "since the snr is low , the probability mass spreads considerbly over the neighbored values due to the noise effect ; thus , it can lead to difficulty in detecting the state of the signal element using the simple map criterion . in cs - bsd ,",
    "the use of the bht nicely compensates for this weakness of the map by scanning the probability mass over the entire range of values .      in fig.[fig :",
    "fig5 - 5 ] , we examine the mse performance of the bp - based algorithms , cs - bp and suprem , as a function of a fixed number of iterations where @xmath192 , @xmath162 , and snr = 50 db . in this simulation",
    ", we used the non - reweighted version of suprem since the reweighted version requires more than 10 iterations .",
    "the figure demonstrates that cs - bsd converges faster than cs - bp and suprem .",
    "the convergence of cs - bsd is achieved within 2 to 3 iterations with cs - bp , whereas suprem require more than 10 iterations .",
    "the theoretical and empirical research in this paper demonstrated that cs - bsd is a powerful algorithm for sparse signal reconstruction in ncs . in cs - bsd , we employed the dd estimation structure , which consists of support detection and signal value estimation . in the support detection process",
    ", bp provides the signal posterior densities , and then bht detects the support based on the posteriors . in the signal value estimation process , an mmse estimator provides the signal values using the detected support set . these detection and estimation process",
    "are iterated until the constraint @xmath193 is met .",
    "the evaluated ser performance showed that the support detection of cs - bsd is almost error - free beyond a certain threshold snr according to the undersampling ratio @xmath158 . on the basis of the ser result",
    ", we argued that cs - bsd achieves the performance of an mmse estimator that has the knowledge of the support set beyond the threshold snr .",
    "we supported the argument by evaluating the mse performance .",
    "the complexity of cs - bsd is @xmath194 , which includes the cost of mmse @xmath147 , in addition to that of bp , @xmath195 .",
    "although our algorithm incurs an additional cost for mmse estimation , it converges faster than other bp - based algorithms , so the computational cost is lower in practice .",
    "_ proof _ : we define a random vector @xmath196 $ ] consisting of the signal elements associated with @xmath197 and the corresponding index set @xmath198 , where @xmath199 . with a bijective mapping function @xmath200 , each element of @xmath201 corresponds to @xmath202 by marginalizing over @xmath203 to @xmath204 , we obtain @xmath205 where @xmath206 since @xmath207 is independent of @xmath45 . by further marginalizing over elements of @xmath201 , we rewrite the expression in as @xmath208 where we assume @xmath209 without loss of generality .",
    "in addition , @xmath210 holds true since knowing @xmath201 is equivalent to knowing @xmath211 ; thus , there is no uncertainty in @xmath212 . since the elements of @xmath45 are assumed be independent , we replace @xmath213 in with the product of the probability densities . @xmath214 the expression in",
    "can be represented by a sequence of convolutions of probability densities , as given in . @xmath215",
    "this work was supported by the world - class university program ( r31 - 10026 ) , haek - sim research program ( no .",
    "2011 - 0027682 ) , do - yak research program ( no.2011 - 0016496 ) , and leading foreign research institute recruitment program ( k20903001804 - 11e0100 - 00910 ) through the national research foundation of korea funded by the ministry of education , science , and technology ( mest ) .",
    "shihao ji , ya xue , and lawrence carin , `` bayesian compressive sensing , '' _ ieee trans .",
    "signal process .",
    "56 , no . 6 , pp . 2346 - 2356 , june .",
    "( the matlab code is available at http://people.ee.duke.edu/~lcarin/bcs.html . )",
    "m. akcakaya , j. park , and v. tarokh , `` a coding theory approach to noisy compressive sensing using low density frame , '' accepted to _ ieee trans .",
    "signal process .",
    "( the matlab code is available at http://people.fas.harvard.edu/~akcakaya/suprem.html )            d. mackay , `` good error - correcting codes based on very sparse matrices,''__ieee trans .",
    "inf . theory _",
    "_ , vol . 45 , no .",
    "399 - 431 , mar",
    "d. middleton and r. esposito , `` simultaneous optimum detection and estimation of signal in noise , '' _ ieee trans .",
    "inform . theory _",
    "434 - 444 , may . 1968 .",
    "jaewook kang , heung - no lee , and kiseon kim , `` message passing aided least square recovery for compressive sensing , '' in _ proc",
    ". signal processing with adaptive sparse structured representation ( spars ) _ , pp .",
    "124 , jun . 2011 .",
    "k. shimizu , t. ishikawa , n. togawa , t. ikenaga , and s. goto , `` a parallel lsi architecture for ldpc decoder improving message - passing schedule , '' in _ proc .",
    "symp . on circ . and syst .",
    "( iscas ) _ , pp .",
    "5099 - 5102 , may 2006"
  ],
  "abstract_text": [
    "<S> in this paper , we investigate a bayesian sparse reconstruction algorithm called compressive sensing via bayesian support detection ( cs - bsd ) . this algorithm is quite robust against measurement noise and achieves the performance of an minimum mean square error ( mmse ) estimator that has support knowledge beyond a certain snr thredhold . </S>",
    "<S> the key idea behind cs - bsd is that reconstruction takes a detection - directed estimation structure consisting of two parts : support detection and signal value estimation . </S>",
    "<S> belief propagation ( bp ) and a bayesian hypothesis test perform support detection and an mmse estimator finds the signal values belonging to the support set . </S>",
    "<S> cs - bsd converges faster than other bp - based algorithms and it can be converted to an parallel architecture to become much faster . </S>",
    "<S> numerical results are provided to verify the superiority of cs - bsd , compared to recent algorithms .    </S>",
    "<S> compressive sensing , sparse signal reconstruction , support detection , belief propagation , detection - directed estimation </S>"
  ]
}