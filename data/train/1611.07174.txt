{
  "article_text": [
    "speech recognition(asr ) is designed to transcript human speech into spoken phonemes .",
    "asr has been investigated for several decades .",
    "traditionally , a statistical model of maximum likelihood decoding and maximum mutual information estimation are used for speech recognition @xcite , thus the use of hidden markov model ( hmm ) for speech recognition had also become predominant several years ago @xcite . with the spring - up of deep learning ,",
    "deep neural network ( dnn ) with hmm states has been shown to outperform the traditional gaussian mixture models @xcite , thus many new training tricks have been proposed to improve the performance of dnns , such as powerful non - linear activation functions , layer - wise mini - batch training , dropout and fast gradient descent method @xcite .",
    "recurrent neural network ( rnn ) is a powerful tool for sequential modeling , and dnns are gradually replaced by rnns which have been successfully applied in asr in the last several years .",
    "meanwhile , deep long short - term memory rnns and deep bidirectional rnns are purposed to exploit long time memory in asr @xcite . besides , sequence training of rnns with connectionist temporal classification ( ctc ) has shown great performance in end - to - end asr @xcite",
    "traditional frame - wise cross entropy training needs pre - segmented data by hand , but ctc is an end - to - end training method for rnns which decodes the output probability distribution into phoneme sequences without requiring pre - segmented training data .",
    "rnn has been widely used in asr , but rnn ca nt depict very long time dependency because of its vanishing gradient problem , and deeper rnn seems to have little improvement when the number of layers reaches a limit . although lstm improves the performance of rnn , an disadvantage of lstm is that it requires too much computation and stores multiple gating neural responses at each time - step , which will become a computational bottleneck .",
    "very recently , some other novel neural networks structures have been proposed , of which convolutional neural network ( cnn ) is one of the most attractive models .",
    "cnn is an older deep neural network architecture @xcite , and has enjoyed the great popularity as a efficient approach in character recognition .",
    "human speech , which is also a sequential signal , can be transformed into a feature map that we can take similarly as an image .",
    "human speech signals are highly variable because of different speaking accents , different speaking styles and uncertain noises from the environment . for speech recognition",
    ", cnn has several advantages : ( ) human speech signal has local correlations in both time and frequency , cnn is well suited to exploit these correlations explicitly through a local connectivity .",
    "( ) cnn has the ability to capture the frequency shift in human speech signal .",
    "some researchers proposed cnn can be used in speech recognition , and it has been proved that deep cnn has better performance over general feed - forward neural network or gmm - hmm in several speech recognition tasks @xcite .",
    "most of previous application of cnns in speech recognition only used fewer convolutional layers .",
    "for example , abdel - hamid  et  al .",
    "@xcite used one convolutional layer , one pooling layer and a few full - connected layers .",
    "amodei  et  al .",
    "@xcite also used only three convolutional layers as the feature preprocessing layers .",
    "some researcher has shown that cnn - based speech recognition which uses raw speech as input can be more robust @xcite , and very deep cnns has also been proved to show great performance in noisy speech recognition and large vocabulary continuous speech recognition ( lvcsr ) tasks @xcite .",
    "generally , very small filters with 3 * 3 kernels have recently been successfully applied in acoustic modeling in hybrid nn - hmm speech recognition system , and pooling layer has been proved to be replaced by full - connected convolutional layers and pooling has no highlights for lvcsr tasks @xcite .",
    "cnn has the ability to exploit the internal dependency of speech sequences while having the advantage of fewer parameters than rnn .",
    "this is to say , we can use less complex computational cost to achieve the same performance as rnn . with the datasets of human speech becoming larger ,",
    "it will come true that overfitting will be less important but the convergence speed is what we always care about .",
    "meanwhile , we find there are few work that discusses the convergence speed of different configurations in deep cnns for asr . typical architecture of deep cnns for asr usually contains some fully - connected convolutional layers at the bottom , followed by several recurrent layers and fully - connected feedforward layers , but we find that , in practice , it s too slow to train this type of architecture for acoustic modeling . to explore",
    "how can we speed up the convergence , we build another two architectures based on deep cnns for acoustic modeling : ( ) deep recurrent convolutional networks , and ( ) applying residual learning framework in deep cnns . deep residual learning framework can obtain compelling accuracy and good convergence performance in computer vision @xcite , which attributes to its identity mapping as the skip connection .",
    "we compare the convergence speed of above three different architectures , besides , we further present the evaluation results by per of three architectures .",
    "our work has meaningful reference for who are also applying deep cnns for asr to attain a fast convergence speed .",
    "this paper is organized as follows .",
    "we first review the basics of commonly used models in asr including recurrent neural network and convolutional neural network ( section ) , then we explain how the end - to - end training method of connectionist temporal classification can be used to decode the output phonemes probability distribution ( section ) . besides , we propose a bi - directional statistical n - gram language model to rectify the output sequences of acoustic model in section .",
    "section explains the experiment setup and some training details .",
    "section presents the comparison of convergence speed between traditional deep convolutional recurrent networks , our novel deep recurrent convolutional networks and those applied with deep residual learning .",
    "further , we show the evaluation results by minimum per of all experimental architectures in section .",
    "the most common functions applied to a neuron s output is relu @xcite as a function of its input with @xmath0 .",
    "relu behaves better than traditional non - linear functions such as sigmoid or tanh , since the mean value of relu s activations is not zero , some neurons in practice always become dead during backpropagation .",
    "exponential linear unit ( elu ) was introduced in @xcite , in contrast to relu , elu has negative values which pushes the mean of activations close to zero , this is to say , elu can decrease the gap between the normal gradient and unit natural gradient and , therefore , speed up training .",
    "the expression of elu nonlinearity is shown below in equation  [ qg1 ] .",
    "@xmath1    since our work is based on deep cnns , we can take elu as the non - linear function to faster our network s convergence .",
    "fast learning has a great impact on performance of training large datasets .",
    "general forward neural network ca nt depict the time dependency for sequence modeling problems well , such as automatic speech recognition .",
    "one type of special forward neural network is recurrent neural network(rnn ) .",
    "when rnn is folded out in time , it can be considered as a dnn with many sequential layers .",
    "in contrast to forward neural network , rnn is used to build time dependency of input features with internal memory units .",
    "rnn can receive input and produce output at every layer , the general architecture of rnn is shown in figure  [ rnn ] .        as shown in figure  [ rnn ] , a very simple rnn is composed of an input layer , a hidden layer and an output layer .",
    "the recurrent hidden layer is designed to pass the forward information to backward time - steps .",
    "we can depict internal relationship of a general rnn in equation  ( [ qg_rnn ] ) .",
    "@xmath2    where @xmath3 is the weight matrix between adjacent hidden units , @xmath4 is the hidden unit of previous time - step , @xmath5 is the weight matrix between input layer and hidden layer , @xmath6 is the input at time t , and the bias @xmath7 is added and finally an activation function @xmath8 , typically sigmoid , tanh , relu or elu , will be applied to generate the output of the recurrent layer . if several recurrent layers are stacked , the output of previous layer becomes the input of next layer .",
    "compared to standard fully - connected neural networks and rnns , cnns which are proposed in @xcite have much fewer parameters so that they are easier to train .",
    "cnns are pretty similar to ordinary neural networks , they are made of trainable weights and bias and they can also be stacked to a deep depth , which has been successfully applied in imagenet competition @xcite .    a typical architecture of cnn is composed of a convolutional layer and a pooling layer which is shown in figure  [ cnn ] . in most cases ,",
    "a typical convolutional layer contains several feature maps , each of which is a kind of filter with shared parameters .",
    "these filters are spatial and extend through the full depth of input volume .",
    "pooling layer is designed for dimensionality reduction and full - connected layer can output the probability distribution of all different classes .        in our experimented model ,",
    "we replace the pooling layer with the full convolutional layer .",
    "especially , we pad the input layer with zeros in both dimensions , since that the output layer can be the same size as the input layer if we set the stride to be 1 .",
    "the details of padding is shown in figure  [ cnn_pad ] .",
    "we also replace the traditional hmm decoder for output sequences .",
    "labelling unsegmented sequential data is very common in speech recognition , and ctc is good at achieving this .",
    "the basic idea of ctc is to interpret the outputs of network as a probability distribution over all possible phenomes . given this distribution , we can derive the objective function of sequence labeling . since the objective function is differentiable",
    ", we can train it by backpropagation through time algorithm .    using the probability distributions learned by deep cnns",
    ", we would then use a ctc loss layer to finally output the phenome sequence . for a given input sequence ,",
    "the goal of ctc is to minimize the average edit distance from the output sequence to actual sequence .",
    "suppose @xmath9 is an alphabet of all output phonemes , the ctc network has one more units than there are labels in @xmath9 , and the activations of first @xmath10 labels are interpreted as the probability of observing the corresponding labels , the activation of the extra unit is the probability of the blank label . from the output probability distribution , we can compute probability of any sequence by summing all its different alignments .    in detail ,",
    "if we define @xmath11 as the probability of outputting label @xmath12 at time @xmath13 , which defines a distribution over the set @xmath14 of length t sequences over the alphabet @xmath15 : @xmath16    in ( [ qg2 ] ) , we take the elements of @xmath14 as different paths , which is denoted as @xmath17 .",
    "given a labelling @xmath18 and a mapping function @xmath19 which removes all blanks and repeated labels , we can compute its probability by summing all its possible paths : @xmath20 therefore , the output sequence should be the most probable labelling , it s a decoding problem about how to find the output sequence .",
    "we require an efficient way of calculating the probabilities @xmath21 of each labelling .",
    "since from ( [ qg3 ] ) we may feel that it s very difficult to handle because there are many paths corresponding to a giving labelling .",
    "however , the problem can be converted to a dynamic programming problem , like what we use in hmms .",
    "the key point of dynamic algorithm is to break down the sum over all paths into forward and backward variables .    to account for the blanks",
    ", ctc considers to add blanks between every pair of labels and beginning and end .",
    "therefore , the length of modified label @xmath22 is @xmath23 . since the state transition space for dynamic programming should nt be too large for computational efficiency , we assume that state transition only occurs between blank and non - blank label or two distinct non - blank labels . besides , we allow all prefixes begins with a blank or the first symbol in @xmath18 and ends with a blank or the last symbol in @xmath18 .",
    "thus , we can draw the expressions of dynamic program formulation from figure  [ ctc ] .",
    "define the forward variable @xmath24    we can conclude that if the current label is blank or the current blank is the same as it was two steps ago , then the current forward variable is defined @xmath25    otherwise , the current forward variable is defined @xmath26    since the last label must only be a blank or the last label in @xmath18 , the final probability of a given label is calculated by forward variables as @xmath27    the forward variables are defined previously , and the backward variables can be defined similarly . in practice , in order to avoid any underflows on any digital computer , we must normalize both the forward variables and backward variables . finally , we can calculate the maximum likelihood error as @xmath28    for forward and backward variables defined above , we can calculate the probability of any labels occurred at any time t given a labelling l as @xmath29    since @xmath21 can be obtained by summing over all s , differentiating this w.r.t @xmath30 , we only consider paths that go through label k at time t.    @xmath31    @xmath32 represents the set of positions where k occurs in l , so the objective function s gradient is    @xmath33    our prediction of acoustic modeling is tied to ctc loss , and the loss takes the output of deep convolutional neural network as input . using back propagation algorithm , we can update all parameters of acoustic model to reach a minimum of loss .",
    "statistical language modeling and neural network have both been successfully used in speech recognition  @xcite .",
    "traditional n - gram model always makes an assumption that the probability of the current word depends only on the probability of the previous n-1 words , we propose a new bidirectional n - gram model which considers context probability of two sides .",
    "computation of our bidirectional n - gram model is composed of two parts .",
    "first , we define forward n - gram going left to right in a sentence and we obtain the forward probability for each phoneme given previous phoneme phrase .",
    "@xmath34    second , we reverse the whole training sentence to obtain the backward probability for each phoneme given future phoneme phrase . @xmath35",
    "our model takes as input bidirectional n - gram phoneme counts , thus it combines the bidirectional context information capacity and simple computation complexity . besides , to make our model more robust , we perform bigram , trigram and four - gram including both forward probability and backward probability  based on timit corpus .",
    "we shows an example how our language model rectify mislabeled phonemes of acoustic model in figure  [ n - gram ] .",
    "we perform our speech recognition experiments on a public commonly used speech dataset : timit .",
    "timit is composed of 630 speakers with 6300 utterances of different sexes and dialects .",
    "every audio clip is followed by phoneme transcriptions and sentence transcriptions , and we take the phoneme transcriptions as ground labels . the output is probability distribution of 63 labels including 61 non - blank phonemes , one space label and one blank label for ctc .",
    "since a typical machine learning dataset contains training set , validation set and test set , we design to split 6300 utterances of timit into 5000 training utterances , 1000 validation utterances and 300 test utterances . for the generalization of our model ,",
    "we randomly choose six different partitions of timit and then select the best partition by cross validation .",
    "we build a simply baseline model of deep neural networks to evaluate the performance of six different partitions , and we choose the best partition whose test cost curve is going down both stably and fast .      at the stage of pre - processing ,",
    "each piece of speech is analyzed using a 25-ms hamming window with a fixed overlap of 10-ms .",
    "each feature vector of a frame are calculated by fourier - transform - based filter - bank analysis , which includes 39 log energy coefficients distributed on 13 mel frequency cepstral coefficients ( mfccs ) , along with their first and second temporal derivatives . besides",
    ", all feature data are normalized so that each vector dimension has a zero mean and unit variance .",
    "since the feature matrix of each audio speech differs in time length , we pad each feature matrix with zeros to a max length .",
    "we build our deep neural network models based on open - source library @xmath36 , which is a lightweight library to build and train neural networks in theano .",
    "we take our experiment on gpu tesla k80 to speed up our training . for ctc part",
    ", we choose to use a c++ implementation of baidu research and we write some python code to wrap it in lasagne . besides , all cross validation , feature generation , data loading , result analysis , visualization are implemented in python code by ourselves .",
    "all our experimental models are trained by end - to - end stochastic gradient descent algorithm with a mini - batch of 32 . in detail , since the adam optimization method @xcite is computationally efficient , requires little memory allocation and is well suitable for training deep learning models with large data , we adopt the adam method with a learning rate of 0.00005 at the start of training . instead of setting learning rate to be 0.001",
    "as the paper said , we find that a learning rate of 0.00005 can make the divergence more stable in practice .",
    "as our experimental models are very deep , we would like to adopt some regulations to avoid overfitting .",
    "recetnly , batch normalization @xcite has shown a better regularization performance , however it would add extra parameters and needs heavy data augmentaion , which we would like to avoid .",
    "instead , we add a dropout layer after the recurrent layers and after the first full - connected feedforward layer to prevent it from overfitting . to keep the sequential information for better acoustic modeling , we reserve the whole context information for end - to - end training instead of splitting each audio into frames of same length for frame - wise cross - entropy training . besides , on the top layer of our experimental models , we set the activation function to be linear , for that ctc decoding has wrapped the softmax layer inside .",
    "since our proposed model is end - to - end and phoneme - level , we use phoneme error rate ( per ) to evaluate the result .",
    "the per is computed after the ctc network had decoded the whole output sentence into a sequence of phonemes .",
    "we then compute the damerau - levenshtein distance between our predicted sequence and the truth sequence to obtain the mistake we made .",
    "the average number of mistakes over the length of the phoneme sequence is just our per .",
    "we evaluate our model on the test set finally .",
    "to explore the convergence properties of deep convolutional networks with recurrent networks , we have conducted experiments with different configurations . in this section",
    ", we ll discuss three typical experiments we have tried , which are novel deep recurrent convolutional networks , traditional deep convolutional recurrent networks and residual networks .",
    "we ll compare the convergence speed of them according to different configurations including number of layers , number of parameters , number of feature maps and applying residual learning .",
    "traditional training approach of acoustic modeling is based on frame - wise cross - entropy of predicted output and true label , which needs handy alignment between input frames and output labels .",
    "to avoid such high labor cost , our approach exploits the dynamic decoding method based on ctc which can perform supervised learning on sequence data and avoid alignment between input data and output label .",
    "we choose a commonly used dataset timit to work on our acoustic model .",
    "timit contains 6300 utterances of 630 speakers in 8 dialects , each audio contains phoneme transcription , word transcription and the whole sentence .",
    "we choose the phoneme transcription as labels for phoneme - level training .",
    "since many recent asr systems use deep cnns for acoustic modeling as part of asr , especially , deep cnns are used for feature preprocessing followed by rnn ctc decoding network .",
    "conventional deep cnns contain convolution and pooling , but we find that pooling can be replaced by convolution with fewer feature maps in practice .",
    "inspired by this , we also build four deep convolutional recurrent networks , which are composed of deep convolutional layers , four recurrent layers and two full - connected feedforward layers .",
    "they are distinguished by different number of feature maps at convolutional layer .",
    "as shown in figure  [ fig : all_models_2 ] , the two deep fully - connected convolutional recurrent networks `` cr1 '' and `` cr2 '' differ in number of feature maps in bottom convolutional layers .",
    "we set the number of feature maps to go down gradually , which means that the number of feature maps becomes narrower from bottom layers to up layers .",
    "both `` cr3 '' and `` cr4 '' have narrower deep convolutional layers , but they have different bottom convolutional layers . the parameters of each model are shown in table  [ tab : all_models_2 ] .",
    ".parameters of convolutional recurrent models [ cols=\"^,^,^,^,^\",options=\"header \" , ]      to investigate the convergence of different networks in figure  [ fig : model1 ] , we present the comparison of cost curves in figure  [ fig : cost1 ] .",
    "the horizontal axis represents time , of which the unit is minute .",
    "the vertical axis represents training cost .",
    "we totally train each model for 42 epochs , and each marker denotes an epoch in figure  [ fig : cost1 ] .",
    "results show that `` rc1 '' converges the slowest at both the beginning and the end , which is probably caused by too many parameters and too many convolutional layers of different feature maps . differently",
    ", `` rc2 '' performs much better than `` rc1 '' , since both finished 42 epochs at same time , but there is a great gap between their cost curves . comparing ",
    "rc1  with `` rc2 '' , `` rc2 '' has fewer parameters and more continuous full - connected convolutional layers .",
    "`` rc3 '' has fewer recurrent layers than `` rc1 '' , and `` rc3 '' also behaves much better than `` rc1 '' . similarly , `` rc4 '' has fewer recurrent layers than `` rc2 '' , and it behaves much better than ",
    "`` rc4 '' finishes 42 epochs in only 900 minutes , but ",
    "rc2  takes around 1100 minutes . since `` rc4 '' , `` rc5 '' and `` rc6 ''",
    "have the similar number of parameters , we find that `` rc4 '' behaves best among three , which probably attributes to the different structure of `` rc4 '' .",
    "based on `` rc5 '' , `` rc6 '' has more convolutional feature maps , but we find this degrades the performance of `` rc5 '' .",
    "totally , our experiments show that narrower fully - connected convolutional layers can help improve model to converge , as `` rc2 '' and `` rc4 '' . besides , deep fully - connected convolutional layers with same number of feature maps have slightly inferior performance to narrower ones , as `` rc5 '' and `` rc6 '' . for convergence ,",
    "narrower recurrent convolutional layers behave better .          to investigate the difference of convergence between deep recurrent convolutional network and deep convolutional recurrent network , we present a comparison of convergence curves between both .",
    "recurrent convolutional network is our novel proposed network for acoustic modeling , and convolutional recurrent network is the conventional one , they have the similar number of parameters .",
    "since we have discussed some different models of them previously , we especially select two better models `` rc2 '' and `` cr2 '' to be shown in figure  [ fig : comparison_12 ] .",
    "as shown in figure  [ fig : comparison_12 ] , we count 88 epochs for both `` cr2 '' and `` rc2 '' . observing the training cost curves along time",
    ", we can draw some conclusions .",
    "first , `` rc2 '' finishes 88 epoches in less than 3000 minutes , while `` cr2 '' finishes in nearly 4000 minutes , even though `` cr2 '' has the same number of parameters as `` rc2 '' , but it converges one quarter faster than `` rc2 '' . besides , in the first 2000 minutes , `` rc2 '' behaves much better than `` cr2 '' , but in the last half of training , `` cr2 '' catches up with `` rc2 '' .",
    "therefore , our proposed deep recurrent convolutional network has the faster convergence performance at the beginning , and reach the same performance as conventional deep convolutional recurrent network later .      generally",
    ", deeper convolutional neural networks can have larger capacity for feature representation , however , it has been shown that deeper convolutional neural networks are more difficult to train for their degradation problem , although there are some modern optimization methods .",
    "recently , a new residual learning framework has been proposed to ease the training of very deep convolutional neural networks , and deep residual networks  @xcite have been proved to improve convergence and higher accuracy in image classification with no more extra parameters .",
    "general deep residual networks ( resnets ) are composed of many stacked `` residual blocks '' , and each residual block can be expressed in following form :    @xmath37    @xmath38    where @xmath39 and @xmath40 are input and output of the @xmath18-th residual block , and @xmath41 is a residual mapping function . in general",
    ", @xmath42 is an identity mapping and @xmath43 is an activation function , which we set to be @xmath44  @xcite here . with the presence of short - cut connection",
    ", residual networks can ease the degradation problem of deeper convolutional neural networks because the additional layers can only simply perform an identity mapping .",
    "although resnets with over 100 layers have shown great accuracy for several challenging image classification tasks on imagenet competitions  @xcite and ms coco competitions  @xcite , we also want to explore how the residual blocks behave in our experimental models .        to explore how resnets behave in asr",
    ", we propose a novel architecture which combines deep fully convolutional network with residual learning framework in asr .",
    "we build two resnets based on both `` cr2 '' and `` rc2 '' , which are shown in figure  [ fig : residual_models ] . to avoid extra parameters , we build residual blocks only with layers of same dimension .",
    "`` cr2 '' and `` rc2 '' are two models with best performance among all discussed models above .",
    "for `` res - rc2 '' , we build four residual blocks based on `` rc2 '' , and each residual block contains several layers with the same number of feature maps . in contrast , `` res - cr2 '' contains two residual blocks based on `` cr2 '' .",
    "figure  [ fig : convergence_rescrnn ] gives the comparison of cost curves between two plain networks and their residual versions .",
    "`` res - rc2 '' shows the best performance , it finishes 88 epochs in only 1500 minutes , and its cost also goes down very quickly . comparing `` res - rc2 '' with `` rc2 '' , we can draw a conclusion that `` res - rc2 '' converges twice as fast as  rc2  , which mainly attributes to the four shortcuts connection of identity mapping in `` res - rc2 '' , thus ,",
    "the difficulty of training `` res - cr2 '' can be eased .",
    "however , compared to `` cr2 '' , `` res - cr2 '' with only two residual blocks converges much slower . concretely",
    ", `` cr2 '' finishes 88 epochs in less than 4000 minutes , while `` res - cr2 '' finished 88 epochs in nearly 7000 minutes .",
    "although `` res - cr2 '' has two residual blocks based on `` cr2 '' , but the degradation problem has been exposed out of expectation . for this degradation problem ,",
    "we propose two possible reasons .",
    "( ) the bottom residual block of `` res - cr2 '' has ten convolutional layers , we think it s so deep that the power of deep residual learning may be restricted .",
    "( ) `` res - cr2 '' has two recurrent layers on top of convolutional layers , so the convergence may be mainly influenced by the top recurrent layers and residual blocks make no difference here .",
    "timit is a small 16 khz speech corpus with 6300 utterances , from which the validation set of 300 utterances and the test set of 300 utterances are derived .",
    "we use 63 phonemes as output labels , including 61 non - blank labels , one space label and one blank label for ctc . after acoustic modeling",
    ", a probability distribution of 63 labels will be decoded by the ctc network into final phoneme sequences . then , our proposed bidirectional hybrid n - gram language model over phonemes , estimated from the whole training set , is used to rectify the final sequence .",
    "c | c | c | c type & model structure & validation per & test per +    & cr1 & 21.56% & 20.08% + & cr2 & * 20.59% * & * 18.73% * + & cr3 & 23.89% & 22.32% + & cr4 & 20.56% & 19.31% +    & rc1 & 32.17% & 30.91% + & rc2 & * 21.36% * & * 20.71% * + & rc3 & 25.66% & 24.34% + & rc4 & 27.44% & 25.54% + & rc5 & 25.76% & 24.11% + & rc6 & 26.60% & 24.87% +    & res - rc2 & * 18.77% * & * 17.33% * + & res - cr2 & 20.13% & 18.90% +    since our acoustic models are phoneme - level , we evaluate them by per on the test set .",
    "each experiment of an architecture has been conducted several times , we present the minimum validation per and minimum test per for every architecture in table  [ tab : per_evaluation ] . according to evaluation result ,",
    "our novel deep recurrent convolutional network `` rc2 '' obtains a 20.71% per , with accuracy competitive to traditional deep belief network acoustic model .",
    "although , deep convolutional recurrent network `` cr2 '' achieves a test per of 18.73% , which is a 2% relative improvement over the novel `` rc2 '' .",
    "however , when we apply the residual learning framework to them , we find that `` res - rc2 '' obtains a test per of 17.33% , which is an obvious improvement over `` rc2 '' . furthermore ,",
    "`` res - cr2 '' attains a per of 18.90% with slightly improvement over `` cr2 '' , which probably is caused by the heavy residual block as we discussed previously .",
    "our paper presents a detailed experimental comparison of three different acoustic models in speech recognition , they are traditional deep convolutional recurrent networks , deep recurrent convolutional networks and deep residual networks . traditional application of deep cnns are used for feature preprocessing , followed by recurrent layers and ctc decoding layer , but in practice it takes too much time to converge .",
    "our proposed deep recurrent convolutional network takes the recurrent networks as feature preprocessing , and deep cnns are designed to depict high - level feature representation .",
    "our experiments show that , compared to traditional deep convolutional recurrent networks , our novel recurrent convolutional network can converge in less time in the first half and also attain a comparable per .",
    "besides , we try to apply residual learning framework in our acoustic models .",
    "we build an identity mapping as shortcut connection as residual block for each model , and experiments show that our proposed novel deep recurrent convolutional networks can benefit a lot from residual blocks both in accuracy and convergence , however , residual blocks seem to have some negative impacts on the traditional deep convolutional recurrent networks .",
    "we present a detailed analysis about their performance according to different training cost curves , and our proposed `` res - rc2 '' attains the best per of 17.33% .",
    "our experiments verify that the novel deep recurrent convolutional networks can take place of traditional deep convolutional recurrent networks in asr with less training time , in particular , residual learning framework can also be applied in deep recurrent convolutional neural network to make great improvement in both convergence speed and recognition accuracy .",
    "the author would like to thank chengyou xie and qionghaofeng wu for helpful discussions on automatic speech recognition .",
    "ieee , lalit r. bahl member , f. j. f. ieee , and r. l. mercer .",
    "_  a maximum likelihood approach to continuous speech recognition . \" _",
    "pattern analysis & machine intelligence ieee transactions on 5.2(1983 ) , pp .",
    "179190 .",
    "levinson , s. e. , l. r. rabiner , and m. m. sondhi .",
    "_  an introduction to the application of the theory of probabilistic functions of a markov process to automatic speech recognition . \"",
    "_ bell system technical journal 62.4(1983 ) , pp .",
    "10351074 .",
    "amodei , dario and anubhai , rishita and battenberg , eric and case , carl and casper , jared and catanzaro , bryan and chen , jingdong and chrzanowski , mike and coates , adam and diamos , greg and others , _  deep speech 2 : end - to - end speech recognition in english and mandarin \" _ , arxiv preprint arxiv:1512.02595,2015 .",
    "hannun , awni and case , carl and casper , jared and catanzaro , bryan and diamos , greg and elsen , erich and prenger , ryan and satheesh , sanjeev and sengupta , shubho and coates , adam and others , _  deep speech : scaling up end - to - end speech recognition \" _ , arxiv preprint arxiv:1412.5567 , 2014 .",
    "le cun , b boser and denker , john s and henderson , d and howard , richard e and hubbard , w and jackel , lawrence d , _  handwritten digit recognition with a back - propagation network \" _ , advances in neural information processing systems , 1990 .",
    "mikolov , tomas , et al .",
    "recurrent neural network based language model \" _ , interspeech 2010 , conference of the international speech communication association , makuhari , chiba , japan , september 2010 , pp .  10451048 ."
  ],
  "abstract_text": [
    "<S> performance of end - to - end automatic speech recognition ( asr ) systems can significantly be improved by the increasing large speech corpus and deeper neural network . given the arising problem of training speed and recent success of deep convolutional neural network in asr , we build a novel deep recurrent convolutional network for acoustic modeling and apply deep residual learning framework to it , our experiments show that it has not only faster convergence speed but better recognition accuracy over traditional deep convolutional recurrent network . </S>",
    "<S> we mainly compare convergence speed of two acoustic models , which are novel deep recurrent convolutional networks and traditional deep convolutional recurrent networks . with faster convergence speed , our novel deep recurrent convolutional networks can reach the comparable performance . </S>",
    "<S> we further show that applying deep residual learning can boost both convergence speed and recognition accuracy of our novel recurret convolutional networks . </S>",
    "<S> finally , we evaluate all our experimental networks by phoneme error rate ( per ) with newly proposed bidirectional statistical language model . </S>",
    "<S> our evaluation results show that our model applied with deep residual learning can reach the best per of 17.33% with fastest convergence speed in timit database .    </S>",
    "<S> speech recognition , convergence speed , end - to - end , recurrent neural network , convolutional neural network , residual learning framework , phoneme error rate </S>"
  ]
}