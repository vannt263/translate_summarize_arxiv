{
  "article_text": [
    "the human visual system efficiently recognizes and localizes objects within cluttered scenes . for artificial systems , however , this is still difficult , due to viewpoint - dependent object variability , and the high in - class variability of many object types .",
    "deep hierarchical neural models roughly mimick the nature of mammalian visual cortex , and by community consensus are among the most promising architectures for such tasks .",
    "the most successful hierarchical object recognition systems all extract localized features from input images , convolving image patches with filters .",
    "filter responses are then repeatedly sub - sampled and re - filtered , resulting in a deep feed - forward network architecture whose output feature vectors are eventually classified .",
    "one of the first hierarchical neural systems was the neocognitron @xcite which inspired many of the more recent variants .",
    "unsupervised learning methods applied to patches of natural images tend to produce localized filters that resemble off - center - on - surround filters , orientation - sensitive bar detectors , gabor filters @xcite .",
    "these findings in conjunction with experimental studies of the visual cortex justify the use of such filters in the so - called _ standard model _ for object recognition @xcite , whose filters are fixed , in contrast to those of convolutional neural networks ( cnns ) @xcite , whose weights ( filters ) are randomly initialized and changed in a supervised way using back - propagation ( bp ) .    despite the hardware progress of the past decades ,",
    "computational speed is still a limiting factor for cnn architectures characterized by many building blocks typically set by trial and error . to systematically test the impact of various architectures on classification performance , we present a fast cnn implementation on graphics processing units ( gpus ) .",
    "previous gpu implementations of cnns @xcite were hard - coded to satisfy gpu hardware constraints , whereas our implementation is flexible and fully online ( i.e. , weight updates after each image ) .",
    "it allows for training large cnns within days instead of months , such that we can investigate the influence of various structural parameters by exploring large parameter spaces @xcite and performing error analysis on repeated experiments .",
    "we evaluate various networks on the handwritten digit benchmark mnist @xcite and two image classification benchmarks : norb @xcite and cifar10 @xcite .",
    "cnns are hierarchical neural networks whose convolutional layers alternate with subsampling layers , reminiscent of simple and complex cells in the primary visual cortex @xcite .",
    "cnns vary in how convolutional and subsampling layers are realized and how the nets are trained . the cnn architecture considered in this study differs from the one of @xcite in the sense that after each cnn - layer an optional max - pooling layer @xcite can be used . here",
    "we give a complete description of this independent implementation ( fig .",
    "[ fig : cnn - architecture ] ) .",
    "the image processing layer is an optional pre - processing layer of predefined filters that are kept fixed during training .",
    "thus additional information besides the raw input image can be provided to the network , such as edges and gradients . in particular , we find that a contrast - extracting layer @xcite helps to improve the recognition rate for norb .",
    "a convolutional layer is parametrized by the size and the number of the maps , kernel sizes , skipping factors , and the connection table .",
    "each layer has @xmath0 maps of equal size ( @xmath1 , @xmath2 ) .",
    "a kernel ( blue rectangle in fig  [ fig : cnn - architecture ] ) of size ( @xmath3 , @xmath4 ) is shifted over the valid region of the input image ( i.e. the kernel has to be completely inside the image ) .",
    "the skipping factors @xmath5 and @xmath6 define how many pixels the filter / kernel skips in x- and y - direction between subsequent convolutions .",
    "the size of the output map is then defined as : @xmath7 where index @xmath8 indicates the layer .",
    "each map in layer @xmath9 is connected to at most @xmath10 maps in layer @xmath11 .",
    "neurons of a given map share their weights but have different receptive fields .      the biggest architectural difference between our implementation and the cnn of @xcite is the use of a max - pooling layer instead of a sub - sampling layer .",
    "no such layer is used by @xcite who simply skips nearby pixels prior to convolution , instead of pooling or averaging .",
    "@xcite found that max - pooling can lead to faster convergence , select superior invariant features , and improve generalization .",
    "the output of the max - pooling layer is given by the maximum activation over non - overlapping rectangular regions of size ( @xmath3 , @xmath4 ) .",
    "max - pooling enables position invariance over larger local regions and downsamples the input image by a factor of @xmath3 and @xmath4 along each direction .",
    "kernel sizes of convolutional filters and max - pooling rectangles as well as skipping factors are chosen such that either the output maps of the last convolutional layer are downsampled to 1 pixel per map , or a fully connected layer combines the outputs of the topmost convolutional layer into a 1d feature vector .",
    "the top layer is always fully connected , with one output unit per class label .",
    "the latest generation of nvidia gpus , the 400 and 500 series ( we use gtx 480 & gtx 580 ) , has many advantages over older gpus , most notably the presence of a r / w l2 global cache for device memory .",
    "this permits faster programs and simplifies writing the code .",
    "in fact , the corresponding transfer of complexity into hardware alleviates many software and optimization problems .",
    "our experiments show that the cnn program becomes 2 - 3 times faster just by switching from gtx 285 to gtx 480 .",
    "manual optimization of cuda code is very time - consuming and error prone .",
    "we optimize for the new architecture , relying on the l2 cache for many of the device memory accesses , instead of manually writing code that uses textures and shared memory .",
    "code obtained by this pragmatic strategy is fast enough .",
    "we use the following types of optimization : pre - computed expressions , unrolled loops within template kernels , strided matrices to obtain coalesced memory accesses and registers wherever possible .",
    "additional manual optimizations are possible in case future image classification problems will require even more computing power .",
    "both outputs @xmath12 and deltas @xmath13 of layer @xmath9 are 2d strided .",
    "their original size is @xmath14 , but they are horizontally strided with a pitch of 32 floats ( we use this stride for all 2d data ) , resulting in coalesced memory accesses .",
    "the vertical stride avoids additional bounding tests in cuda kernels .",
    "all connections between maps of consecutive layers @xmath11 and @xmath9 are stored in matrix @xmath15 .",
    "each row of @xmath15 contains all connections that feed into a particular map in layer @xmath9 . because we aim for a flexible architecture with partially connected layers , in the first column we store the number of previous connections .",
    "this index is useful for forward propagation ( fp ) and adjusting weights ( aw ) cuda kernels .",
    "the second column stores the number of connections , followed by corresponding indices of maps in @xmath11 connected to the current map .    for bp and fp ,",
    "analogous information about connections is needed . we therefore store backward connections in @xmath16 .",
    "aw requires a list of all map connections ( see subsection  [ sub : wa ] ) , stored as an array of map index pairs .",
    "dealing with biases in bp kernel requires to know where the weights of particular connections start ; this information is stored in a 2d array @xmath17 of size @xmath18 .",
    "a straightforward way of parallelizing fp is to assign a thread block to each map that has to be computed . for maps bigger than 1024 neurons ,",
    "the job is further split into smaller blocks by assigning a block to each line of the map , because the number of threads per block is limited ( 1024 for gtx 480 ) .",
    "a one to one correspondence between threads and the map s neurons is assumed . because of weight sharing , threads inside a block can access data in parallel , in particular the same weights and inputs from the previous layer .",
    "each thread starts by initializing its sum with the bias , then loops over all map connections , convolving the appropriate patch of the input map with the corresponding kernel .",
    "the output is obtained by passing the sum through a scaled tanh activation function , and then written to device memory .",
    "bp of deltas can be done in two ways : by pushing or by pulling",
    ". pushing deltas means taking each delta from the current layer and computing the corresponding deltas for the previous layer . for an architecture with shared weights",
    "this has the disadvantage of being hard to code .",
    "each delta from the current layer contributes to many deltas in the previous layer , which translates into a lot of programming .",
    "there are two ways of avoiding this : either writing partial deltas to a separated block of memory and then putting everything together by calling another kernel ( slow because of a tremendous increase in the number of memory accesses , and the need of another kernel ) , or using atomic writes ( to avoid data hazards ) to update deltas ( very slow because many writings are serialized ) .",
    "we implement pulling deltas , which has almost none of the above speed - limiting drawbacks , but is a bit more complicated .",
    "the ( uni- or bi - dimensional ) thread grid assigns a ( bi- or uni - dimensional ) thread block to each map in the previous layer and a thread to each neuron in every map . similar to fp , for maps with more than 1024 neurons , the 2d grid is further split into smaller 1d blocks by assigning a 2d block to each row of the map .",
    "each thread computes the delta of its corresponding neuron by pulling deltas from the current layer .",
    "for every neuron in the previous layer we have to determine the list of neurons in the current layer which are connected to it .",
    "let us consider neuron @xmath19 from a map in layer @xmath11 , and then assume that @xmath20 are the coordinates of neurons in maps of @xmath9 that contribute to the delta of neuron @xmath19 .",
    "the @xmath20 neuron is connected to kernel size number neurons ( @xmath21 ) from each connected map in the previous layer .",
    "the indices in @xmath11 of the neurons connected through a kernel to the @xmath20 neuron are : @xmath22 we can now compute the inequalities for @xmath20 : @xmath23 because @xmath20 has to be inside the map , the final inequalities are : @xmath24 the above inequalities state that the delta of neuron @xmath19 from @xmath11 is computed from deltas of neurons in a rectangular area in maps of @xmath9 ( fig .",
    "[ fig : bpdeltas ] ) . after summing up the deltas",
    ", each thread multiplies the result by the derivative of the activation function .     has 29 x 29 neurons ; the map in @xmath9 has 13 x 13 neurons .",
    "they are linked through a 5 x 5 kernel @xmath25 . skipping factors of @xmath26 and @xmath27",
    "are assumed .",
    "arrows and colors depict the correspondence between neurons in @xmath11 and their sources in @xmath9 . ]",
    "fp and bp have a grid on the list of maps , but the aw thread grid is on the list of kernels ( filters ) between maps of two consecutive layers .",
    "the 1d grid has a block for each connection between two maps .",
    "thread blocks are 2d , with a corresponding thread for every kernel weight .",
    "the bias weight is included as an entire row of threads , thus requiring thread blocks to have @xmath28 threads .",
    "most of the time these additional @xmath4 threads will do nothing , thread ( 0,0 ) being activated only for blocks that have to process the bias .",
    "[ sub : wa ]",
    "we use a system with a core i7 - 920 ( 2.66ghz ) , 12 gb ddr3 and four graphics cards : 2 x gtx 480 and 2 x gtx 580 .",
    "the correctness of the cpu version is checked by comparing the analytical gradient with its finite difference approximation . on gpu",
    "this is not possible because all computations are performed with single precision floating point numbers .",
    "hence the gpu implementation s correctness is checked by comparing its results to those of a randomly initialized net after training it for several epochs on the more accurate cpu version .",
    "obtaining identical results after trillions of operations is a strong indication of correctness .",
    "the implemented cnn s plain feed - forward architecture is trained using on - line gradient descent .",
    "all images from the training set are used for training and also for validation .",
    "if deformations are enabled , only the images from the training set will be deformed .",
    "weights are initialized according to a uniform random distribution in the range @xmath29 $ ] .",
    "each neuron s activation function is a scaled hyperbolic tangent : @xmath30 @xcite .",
    "we pick the trained cnn with the lowest validation error , and evaluate it on the test set ( test for best validation - tfbv ) . the best test error ( bt )",
    "is also listed for all experiments .",
    "the reported computation times per epoch include training , validation and testing as well as all data transfers .",
    "for the mnist dataset the networks are trained on deformed images , continually generated in on - line fashion .",
    "affine ( translation , rotation , scaling , horizontal shearing ) and elastic deformations @xcite are combined .",
    "we use a variable learning rate that shrinks by a multiplicative constant after each epoch , from @xmath31 down to @xmath32 after 500 epochs .",
    ".error rates on mnist test set for randomly connected cnns with 2 to 6 convolutional layers with m maps and an optional fully connected layer with n neurons .",
    "various kernel sizes and skipping factors were used . [ cols=\"^,^,^ \" , ]     to see if bigger nets are better , we increase the number of maps per layer from 100 to 200 , 300 and 400 , respectively ( last three rows in tab .",
    "[ table_cifar10 ] ) .",
    "training time increases exponentially , but the test error decreases , reaching a minimum for nets with 300 maps per layer . our 19.51% error rate is better than the previous state of the art for this dataset , 20.40% @xcite and 25.50% @xcite . unlike @xcite , however , we use the original images without any particular input normalization .",
    "note that the error rate standard deviations are smaller than those obtained on norb , that is , different initializations yield consistent results .",
    "the gpu code scales well with network size . for small nets",
    "the speedup is small ( but still over 10 ) since they fit better inside the cpu cache , and gpu resources are underutilized . for huge nets ( ex : table  [ table_norb ] )",
    "the gpu implementation is more than 60 times faster than a compiler - optimized cpu version . given the flexibility of our gpu version , this is a significant speedup .",
    "one epoch takes 35 gpu minutes but more than 35 cpu hours .",
    "we presented high - performance gpu - based cnn variants trained by on - line gradient descent , with sparse random connectivity , computationally more efficient and biologically more plausible than fully connected cnns .",
    "principal advantages include state - of - the - art generalization capabilities , great flexibility and speed .",
    "all structural cnn parameters such as input image size , number of hidden layers , number of maps per layer , kernel sizes , skipping factors and connection tables are adaptable to any particular application .",
    "we applied our networks to benchmark datasets for digit recognition ( mnist ) , 3d object recognition ( norb ) , and natural images ( cifar10 ) .",
    "on mnist the best network achieved a recognition test error rate of 0.35% , on norb 2.53% and on cifar10 19.51% .",
    "our results are raising the bars for all three benchmarks .",
    "currently the particular cnn types discussed in this paper seem to be the best adaptive image recognizers , provided there is a labeled dataset of sufficient size .",
    "no unsupervised pretraining is required .",
    "good results require big and deep but sparsely connected cnns , computationally prohibitive on cpus , but feasible on current gpus , where our implementation is 10 to 60 times faster than a compiler - optimized cpu version .",
    "this work was partially funded by the swiss commission for technology and innovation ( cti ) , project n. 9688.1 iff : intelligent fill in form .",
    "p.  simard , d.  steinkraus , and j.  platt .",
    "best practices for convolutional neural networks applied to visual document analysis . in _",
    "seventh international conference on document analysis and recognition _ , pages 958963 , 2003 .",
    "r.  uetz and s.  behnke .",
    "large - scale object recognition with cuda - accelerated hierarchical neural networks . in _",
    "ieee international converence on intelligent computing and intelligent systems ( icis ) _ , 2009 ."
  ],
  "abstract_text": [
    "<S> we present a fast , fully parameterizable gpu implementation of convolutional neural network variants . </S>",
    "<S> our feature extractors are neither carefully designed nor pre - wired , but rather learned in a supervised way . </S>",
    "<S> our deep hierarchical architectures achieve the best published results on benchmarks for object classification ( norb , cifar10 ) and handwritten digit recognition ( mnist ) , with error rates of 2.53% , 19.51% , 0.35% , respectively . </S>",
    "<S> deep nets trained by simple back - propagation perform better than more shallow ones . </S>",
    "<S> learning is surprisingly rapid . </S>",
    "<S> norb is completely trained within five epochs . </S>",
    "<S> test error rates on mnist drop to 2.42% , 0.97% and 0.48% after 1 , 3 and 17 epochs , respectively . </S>"
  ]
}