{
  "article_text": [
    "derivative - free optimization ( dfo ) , the mathematical study of the optimization algorithms that do not use derivatives , has become an important aspect in modern optimization research .",
    "one of the most common uses of dfo is for the optimization of an objective function that results from a computer simulation . as",
    "simulation - based research becomes more prevalent , the need for dfo methods is likely to continue to increase .",
    "it is important to note that , while dfo algorithms do not use derivatives , this is not the same as saying the objective function is not differentiable .",
    "for example , if numerical integration is being used , then the objective function is very likely to be differentiable , but derivatives may be very difficult to obtain analytically . this insight has inspired a collection dfo methods that are based on approximating the objective function with a model function .",
    "the gradients , or even second derivatives , of the model function can then be used to guide the optimization algorithm .",
    "examples of such methods include @xcite @xcite @xcite @xcite",
    "@xcite @xcite @xcite @xcite ( among many others ) .",
    "intuitively , in order to prove convergence for a model - based dfo method , one requires that the model functions are sufficiently accurate approximations of the true objective function . in recent research",
    ", this notion has been mathematically captured in the definition of _ fully linear models _",
    "@xcite @xcite .",
    "fully linear models are models that approximate the true objective function ( near the incumbent solution ) in a manner similar to a first - order taylor expansion ( an exact definition appears in section [ ssec : notation ] ) .",
    "to define fully linear models , we must assume that the true objective function is smooth , @xmath1 .",
    "thus , a standard assumption in model - based dfo research is smoothness of the objective function .",
    "if the objective function is smooth , and the gradient mapping is locally lipschitz , then a number of methods have been developed for constructing fully linear models @xcite @xcite @xcite @xcite ( and references therein ) .",
    "it is quite remarkable that some research has moved away from assuming @xmath1 .",
    "for example , in @xcite , it is assumed that the true objective function takes the form @xmath2 , where each @xmath3 . clearly , in this situation @xmath4 .",
    "however , if it is possible to provide fully linear models for each @xmath5 , then it is still possible to create a convergent algorithm @xcite .",
    "( an application in seismic retrofitting design that fulfills this assumption is examined in @xcite . )",
    "another example is @xcite , where the objective function takes the form @xmath6 .",
    "again , while @xmath4 , if it is possible to provide fully linear models for each @xmath5 , then a convergent algorithm can still be created .",
    "notice , in both @xcite and @xcite , the objective function is a composition of a convex lower semi - continuous ( lsc ) function with a smooth vector - valued function . in this paper",
    ", we examine the error bounds for the composition of a convex lsc function with a smooth vector - valued function under the assumption that it is possible to provide _ fully linear models _ for each component of the vector - valued function .",
    "this follows a similar vein to the recent work by regis , @xcite , which explores calculus rules for the simplex gradient .",
    "( regis s work includes , sum rules , product rules , and quotient rules ; however composition rules are unexplored . )",
    "the results herein encompass the error bound results of @xcite and @xcite , and also makes way for other composition functions , such as an augmented lagrangian penalty function , @xmath7 .",
    "we find that , under mild assumptions , the error bound in the function value retains the same order , over the same neighbourhood , as the original error bound ( section [ functionvalueapproximations ] ) .",
    "we also find that , under mild assumptions , the error bound for the first order information ( subdifferentials ) also retains the same order as the original error bound , but only at the focal point of the fully linear models ( section [ subdifferentialapproximations ] ) .",
    "in addition , we provide some simple examples that demonstrate what can go wrong if the assumptions are removed .",
    "the principal focus on this paper is the behaviour of _ fully linear models _ , formally defined as follows .",
    "[ definition : fullylinearatx ] given @xmath1 , @xmath8 , and @xmath9 , we say that @xmath10 are _ fully linear models of @xmath11 at @xmath12 _ if there exists scalars @xmath13 and @xmath14 such that for all @xmath15 @xmath16 we say @xmath10 are fully linear models of @xmath11 at @xmath12 _ with constants @xmath13 and @xmath14 _ , if conditions are satisfied for these constants .",
    "recall a function is convex if its epi - graph ( @xmath17 ) is a convex set ( * ? ? ?",
    "* def 2.1 , prop 2.4 ) , and a function is lower semi - continuous ( lsc ) if its epi - graph is a closed set ( * ? ? ?",
    "* def 1.5 , thm 1.6 ) .",
    "as we are concerned with the composition of a convex lsc and a smooth vector - valued function @xmath18 , we must also define fully linear models for vector - valued functions .",
    "[ definition : fullylinearvector ] given @xmath19 , @xmath9 , and @xmath20 such that @xmath21 is @xmath22 for each @xmath23 , we say that @xmath24 , with @xmath25 are _ fully linear models of @xmath26 at @xmath27 _",
    "( _ with constants @xmath28 and @xmath29 _ ) if @xmath30 are fully linear models of @xmath5 at @xmath27 ( with constants @xmath28 and @xmath29 ) for each @xmath31 .",
    "henceforth , we will use the notation @xmath32 and + @xmath33 .    for a vector - valued function ,",
    "fully linear implies that for each @xmath31 , there exists scalars @xmath34 and @xmath35 , satisfying conditions . using the phrase ` with constants @xmath28 and @xmath29 '",
    "is accomplished by setting @xmath36 and @xmath37 .",
    "other notation in this paper will follow that of @xcite .",
    "in particular , for a function @xmath38 at a point @xmath39 , we define the _ ( regular ) subdifferential",
    "_ @xmath40 by @xmath41 where @xmath42 is the ` little - oh ' function ( see @xcite ) .",
    "the subdifferential represents a set of generalized gradients and plays an important role in optimization . for example",
    ", the subdifferential can be used to check first order optimality ( * ? ? ?",
    "* thm 10.1 ) , and determine descent directions ( * ? ? ?",
    "* ex 8.23 ) .    of interest to this paper is the _ chain rule _ for computing subdifferentials . in particular ,",
    "if @xmath11 is the composition of a convex lsc function @xmath43 and a smooth vector - valued function @xmath26 , @xmath44 , and @xmath45 , then the chain rule applies at @xmath27 , @xmath46 see ( * ? ? ?",
    "* thm 10.6 ) .",
    "our first result examines the error bounds on the function values of @xmath44 , where @xmath43 is convex lsc function , @xmath26 is a smooth vector - valued function , and @xmath26 is approximated via fully linear models .",
    "[ thm : fvalues ] suppose @xmath47 where @xmath48 is convex lsc and @xmath18 is a smooth vector - valued function .",
    "suppose @xmath19 with @xmath45 .",
    "suppose @xmath9 and @xmath24 are fully linear models of @xmath26 at @xmath27 with constants @xmath28 and @xmath29 .",
    "define @xmath49 .",
    "then , there exists @xmath50 and @xmath51 such that @xmath52    * proof : * as @xmath45 and @xmath26 is continuous , there exists @xmath53 such that @xmath54 for all @xmath55 .",
    "define @xmath56 .    by the continuity of @xmath26",
    ", @xmath57 is compact . as such",
    ", @xmath43 is lipschitz continuous relative to this set ( * ? ? ? * cor 8.32 ) .",
    "let @xmath58 be the lipschitz constant of @xmath43 relative to @xmath59",
    ".    given any @xmath60 and @xmath61 , we find that @xmath62 thus @xmath63 and @xmath64 defined above , provide the desired scalars .    the scalar @xmath50 in theorem [ thm : fvalues ] depends on the locally lipschitz constant of @xmath43 , and on the dimension @xmath65 .",
    "it is not difficult to create examples showing that these values are required to properly define @xmath50 .",
    "( indeed , if @xmath66 and @xmath67 , then the proof becomes tight . ) however , in the case where @xmath43 is the max - function , the local lipschitz constant reduces to @xmath68 , and it is possible to remove the dependence on the dimension @xmath65 . in doing this , a cleaner bound is achieved .",
    "suppose @xmath69 where @xmath18 is a smooth vector - valued function .",
    "suppose @xmath19 .",
    "suppose @xmath9 and @xmath24 are fully linear models of @xmath26 at @xmath27 with constants @xmath28 and @xmath29",
    ". then @xmath70    * proof : * select any @xmath15 and @xmath61 .",
    "let @xmath71 and @xmath72 .",
    "notice that @xmath73 where the last inequality results from @xmath74 being fully linear models of @xmath75 .",
    "similarly , @xmath76 combined , these yield equation .",
    "we end this section with an example demonstrating the importance of the assumption @xmath45 in theorem [ thm : fvalues ] .",
    "[ ex : importanceofint1 ] consider @xmath77 where @xmath78 and @xmath79 .",
    "clearly @xmath43 is convex lsc and @xmath26 is smooth .",
    "let @xmath80 , and notice that @xmath81 are fully linear models of @xmath26 at @xmath82 with constants @xmath83 , @xmath84 , and @xmath85 .",
    "however , @xmath86 .    define @xmath87 , and notice that @xmath88 hence , the bound in equation can never be achieved .",
    "we now turn our attention to the error bounds on the subgradient vectors of @xmath44 , where @xmath43 is convex lsc function , @xmath26 is a smooth vector - valued function , and @xmath26 is approximated via fully linear models . notice that theorem [ thm : gvalues ] makes one additional assumption , @xmath89 , and the results of theorem [ thm : gvalues ] are focused only at the point @xmath27 ( instead of all points within @xmath90 of @xmath27 .",
    "the need for these assumptions is explored in examples [ ex : importancefequaltildef ] and [ ex : importantofbeingatxbar ] .",
    "[ thm : gvalues ] suppose @xmath47 where @xmath48 is convex lsc and @xmath18 is a smooth vector - valued function .",
    "suppose @xmath19 and @xmath45 .",
    "suppose @xmath9 and @xmath24 are fully linear models of @xmath26 at @xmath27 with constants @xmath28 and @xmath29 .",
    "suppose @xmath89 .",
    "define @xmath49 .",
    "then , there exists @xmath91 such that for all @xmath15 :    1 .   given any @xmath92 there",
    "exists @xmath93 such that @xmath94 , and 2 .   given any @xmath93 there",
    "exists @xmath92 such that @xmath94 .",
    "* proof : * since @xmath43 is convex lsc , @xmath26 is smooth , and @xmath45 , by ( * ? ? ?",
    "* thm 10.6 ) we have that @xmath95 similarly , @xmath96 applying @xmath89 , we have that @xmath97    since @xmath45 , the set @xmath98 is bounded ( * ? ? ? * prop 16.14 ) .",
    "let @xmath99 .    selecting any @xmath92 ,",
    "there exist @xmath100 such that @xmath101 .",
    "define @xmath102 , and notice that @xmath103\\right\\|\\\\      & = & \\left ( \\sum_{i=1}^m ( f_m(\\bar{x})^\\top w - \\nabla \\tilde{f}_{m,\\delta}(\\bar{x})^\\top w)^2   \\right)^{\\frac{1}{2 } } \\\\      & \\leq & \\left ( \\sum_{i=1}^m \\|f_m(\\bar{x } ) - \\nabla \\tilde{f}_{m,\\delta}(\\bar{x})\\|^2 \\|w\\|^2   \\right)^{\\frac{1}{2 } } \\\\      & \\leq & \\left ( \\sum_{i=1}^m ( \\kappa_g \\delta)^2 m ^2   \\right)^{\\frac{1}{2 } } \\\\      & \\leq & m \\sqrt{m } \\kappa_g \\ , \\delta .",
    "\\end{array}\\ ] ] conversely , selecting any @xmath93 , there exist @xmath100 such that @xmath104 .",
    "define @xmath105 , and notice that the sequence of inequalities again holds .",
    "therefore , @xmath106 provides the desired scalar .",
    "similar to theorem [ thm : fvalues ] , the scalar @xmath91 in theorem [ thm : fvalues ] depends on the dimension @xmath65 , and on a constant _ related _ to the lipschitz constant of @xmath43 .",
    "indeed , the _ lipschitz modulus _ of @xmath43 at @xmath107 is defined @xmath108 when @xmath43 is convex and @xmath109 , then the lipschitz modulus can be found via @xmath110 see ( * ? ? ? * thm 9.13 ) , which is exactly the constant used in the proof of theorem [ thm : gvalues ] ( with @xmath111 ) .    in the case where @xmath43 is the max - function ,",
    "the maximum size of a subgradient vector is @xmath112 and it is possible to remove the dependence on the dimension @xmath65 .",
    "this can be found in ( * ? ? ?",
    "* lem 2.1 ) , so we do not repeat the result here .    in the case where @xmath43 is the @xmath0 norm , the maximum size of a subgradient vector is @xmath112 and the results reconstruct ( * ? ? ?",
    "* lem 2 ) ( although it should be noted that their notation differs slightly from the notation herein ) .",
    "the importance of @xmath45 to theorem [ thm : gvalues ] is clear from the fact that if @xmath113 , then @xmath98 is either empty or unbounded ( * ? ? ?",
    "* prop 16.14 ) . if @xmath98 is empty , then the theorem becomes moot ; while if @xmath98 is unbounded , the next example shows what can go wrong .",
    "[ ex : importanceofint2 ] consider @xmath11 , @xmath43 , and @xmath26 as defined in example [ ex : importanceofint1 ] . at @xmath114 , we have @xmath115 , @xmath116 , and @xmath86 .",
    "let @xmath117 , and notice that @xmath81 are fully linear models of @xmath26 at @xmath82 with constants @xmath83 , @xmath84 , and @xmath85 .",
    "define @xmath87 , and notice that @xmath118 as such , @xmath119 . in particular , given any @xmath120 , the vector @xmath121 .",
    "moreover , this vector has @xmath122 which does not converge to @xmath123 as @xmath124 .",
    "theorem [ thm : gvalues ] also introduced the new condition that @xmath89 .",
    "most common methods for constructing fully linear models satisfy this assumption : for example , linear interpolation @xcite , quadratic interpolation @xcite , and minimum frobenius norm models @xcite , all satisfy this assumption .",
    "however , note that models based on linear regression @xcite may not satisfy this assumption .",
    "the next example explores what can go wrong when this assumption is removed .",
    "[ ex : importancefequaltildef ] consider @xmath77 where @xmath125 and @xmath79 .",
    "clearly @xmath43 is convex lsc , @xmath26 is smooth , and @xmath126 so @xmath127 for any @xmath128 . at @xmath114 , we have @xmath115 , @xmath129 .",
    "let @xmath130 , and notice that @xmath81 are fully linear models of @xmath26 at @xmath82 with constants @xmath83 , @xmath84 , and @xmath85 .",
    "however , @xmath131 .    define @xmath87 , and notice that @xmath132 in particular , @xmath133 is actually differentiable at @xmath134 with @xmath135 .",
    "hence , for @xmath136 , @xmath137 which does not converge to @xmath123 as @xmath124 .",
    "finally , we point out that theorem [ thm : gvalues ] restricts its analysis to approximate subgradients at the point @xmath27 .",
    "if @xmath138 , then the error bounds no longer apply .",
    "[ ex : importantofbeingatxbar ] consider @xmath11 , @xmath43 , and @xmath26 as in example [ ex : importancefequaltildef ] .",
    "let @xmath139 , and notice that @xmath81 are fully linear models of @xmath26 at @xmath82 with constants @xmath83 , @xmath84 , and @xmath85 .",
    "note @xmath140 .",
    "define @xmath87 , and notice that @xmath141 notice that @xmath142 is differentiable at the point @xmath143 for any @xmath144 , with @xmath145 .",
    "however , @xmath11 is not differentiable at @xmath146 , and instead has @xmath147\\}$ ] .",
    "hence , for @xmath148 , @xmath149 which does not converge to @xmath123 as @xmath124 .",
    "this paper has derived error bounds resulting from the composition of a convex lsc function with a smooth vector - valued function , under the assumption that fully linear models are available for each component of the vector - valued function .",
    "if the convex lsc function is the max - function , then the results recreate results from @xcite . while , if the convex lsc function is the @xmath0-norm , then the results recreate results from @xcite .",
    "in theorem [ thm : fvalues ] , we see that the error bound in the function value retains the same order , over the same neighbourhood , as the original error bound . in theorem",
    "[ thm : gvalues ] , we see that the error bound for the first order information retains the same order as the original error bound , but only at the single focal point for the fully linear models . in examining these theorems and proofs , it should be immediately clear that if _ fully linear models _ are replaced by _ fully quadratic models _",
    "@xcite @xcite , then the results are trivially adaptable",
    ". in particular , if the error bounds for the fully linear models of the vector - valued function are replaced with @xmath150 then the resulting error bounds in theorems [ thm : fvalues ] and [ thm : gvalues ] will become @xmath151 and @xmath152 @xmath153                a.  conn , k.  scheinberg , and p.  toint . on the convergence of derivative - free methods for unconstrained optimization . in _",
    "approximation theory and optimization ( cambridge , 1996 ) _ , pages 83108 .",
    "cambridge univ . press ,",
    "cambridge , 1997 ."
  ],
  "abstract_text": [
    "<S> derivative - free optimization ( dfo ) is the mathematical study of the optimization algorithms that do not use derivatives . </S>",
    "<S> one branch of dfo focuses on model - based dfo methods , where an approximation of the objective function is used to guide the optimization algorithm . proving convergence of such methods often applies an assumption that the approximations form _ fully linear models _  an assumption that requires the true objective function to be smooth . </S>",
    "<S> however , some recent methods have loosened this assumption and instead worked with functions that are compositions of smooth functions with simple convex functions ( the max - function or the @xmath0 norm ) . in this paper </S>",
    "<S> , we examine the error bounds resulting from the composition of a convex lower semi - continuous function with a smooth vector - valued function when it is possible to provide fully linear models for each component of the vector - valued function . </S>",
    "<S> we derive error bounds for the resulting function values and subgradient vectors .    </S>",
    "<S> * keywords : * derivative - free optimization , fully linear models , subdifferential , numerical analysis    * ams subject classification : * primary , 65k10 , 49j52 ; secondary , 49m25 , 90c56 . </S>"
  ]
}