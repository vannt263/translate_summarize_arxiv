{
  "article_text": [
    "low - rank matrix approximation is an important ingredient of modern machine learning methods .",
    "numerous learning tasks rely on multiplication and inversion of matrices , operations that scale cubically in the number of data points @xmath0 , and therefore quickly become a bottleneck for large data . in such cases ,",
    "low - rank matrix approximations promise speedups with a tolerable loss in accuracy .",
    "a notable instance is the _ nystrm method _",
    "@xcite , which takes a positive semidefinite matrix @xmath1 as input , selects from it a small subset @xmath2 of columns @xmath3 , and constructs the approximation @xmath4 .",
    "the matrix @xmath5 is then used in place of @xmath6 , which can decrease runtimes from @xmath7 to @xmath8 , a huge savings ( since typically @xmath9 ) .    since its introduction into machine learning ,",
    "the nystrm method has been applied to a wide spectrum of problems , including kernel ica @xcite , kernel and spectral methods in computer vision @xcite , manifold learning  @xcite , regularization  @xcite , and efficient approximate sampling @xcite .",
    "recent work  @xcite shows risk bounds for nystrm applied to various kernel methods .",
    "the most important step of the nystrm method is the selection of the subset @xmath2 , the so - called _",
    "landmarks_. this choice governs the approximation error and subsequent performance of the approximated learning methods  @xcite .",
    "the most basic strategy is to sample landmarks uniformly at random  @xcite .",
    "more sophisticated non - uniform selection strategies include deterministic greedy schemes @xcite , incomplete cholesky decomposition @xcite , sampling with probabilities proportional to diagonal values  @xcite or to column norms  @xcite , sampling based on leverage scores  @xcite , via k - means  @xcite , or using submatrix determinants  @xcite .",
    "we study landmark selection using _ determinantal point processes ( dpp ) _ , discrete probability models that allow tractable sampling of diverse non - independent subsets  @xcite .",
    "our work generalizes the determinant based scheme of  @xcite .",
    "we refer to our scheme as dpp - nystrm , and analyze it from several perspectives .",
    "a key quantity in our analysis is the error of the nystrmapproximation .",
    "suppose @xmath10 is the target rank ; then for selecting @xmath11 landmarks , nystrm s error is typically measured using the frobenius or spectral norm relative to the best achievable error via rank-@xmath10 svd @xmath12 ; i.e. , we measure @xmath13 several authors also use additive instead of relative bounds . however , such bounds are very sensitive to scaling , and become loose even if a single entry of the matrix is large .",
    "thus , we focus on the above relative error bounds .    first , we analyze this approximation error .",
    "previous analyses  @xcite fix a cardinality @xmath14 ; we allow the general case of selecting @xmath15 columns .",
    "our relative error bounds rely on the properties of characteristic polynomials .",
    "empirically , dpp - nystrmobtains approximations competitive to state - of - the - art methods .",
    "second , we consider its impact on kernel methods . specifically , we address the impact of nystrm - based kernel approximations on kernel ridge regression .",
    "this task has been noted as the main application in  @xcite .",
    "we show risk bounds of dpp - nystrmthat hold in expectation .",
    "empirically , it achieves the best performance among competing methods .",
    "third , we consider the efficiency of dpp - nystrm ; specifically , its tradeoff between error and running time . since its proposal , determinantal sampling has so far not been used widely in practice due to valid concerns about its scalability .",
    "we consider a gibbs sampler for @xmath10-dpp , and analyze its mixing time using a _ path coupling _  @xcite argument . we prove that under certain conditions the chain is fast mixing , which implies a _ linear _ running time for dppsampling of landmarks .",
    "empirical results indicate that the chain yields favorable results within a small number of iterations , and the best efficiency - accuracy traedoffs compared to state - of - art methods ( figure  [ fig : tradeoff ] ) .",
    "throughout , we are approximating a given positive semidifinite ( psd ) matrix @xmath1 with eigendecomposition @xmath16 and eigenvalues @xmath17 .",
    "we use @xmath18 for the @xmath19-th row and @xmath20 for the @xmath21-th column , and , likewise , @xmath22 for the rows of @xmath6 and @xmath3 for the columns of @xmath6 indexed by @xmath23 $ ] . finally , @xmath24 is the submatrix of @xmath6 with rows and columns indexed by @xmath2 . in this notation , @xmath25}\\lambda_{[k],[k]}u_{\\cdot,[k]}^\\top$ ] is the best rank-@xmath10 approximation to @xmath6 in both frobenius and spectral norm .",
    "we write @xmath26 for the rank and @xmath27 for the pseudoinverse , and denote a decomposition of @xmath6 by @xmath28 , where @xmath29 .    *",
    "the nystrm method .",
    "* the _ standard nystrm _ method selects a subset @xmath23 $ ] of @xmath30 _ landmarks _ , and approximates @xmath6 with @xmath31 .",
    "the actual set of landmarks affects the approximation quality , and is hence the subject of a substantial body of research @xcite . besides various landmark selection methods , there exist variations of the standard nystrm method .",
    "ensemble nystrm method _",
    "@xcite , for instance , uses a weighted combination of approximations .",
    "the _ modified nystrm method _ constructs an approximation @xmath32 @xcite . in this paper , we focus on the standard nystrm method .",
    "* determinantal point processes . * a _ determinantal point process _ @xmath33 is a distribution over all subsets of a ground set @xmath34 of cardinality @xmath0 that is determined by a psd kernel @xmath1 .",
    "the probability of observing a subset @xmath23 $ ] is proportional to @xmath35 , that is , @xmath36 when conditioning on a fixed cardinality , one obtains a @xmath10-dpp  @xcite . to avoid confusion with the target rank @xmath10 , and",
    "since we use cardinality @xmath30 , we will refer to this distribution as @xmath37-dpp , and note that @xmath38 where @xmath39 is the @xmath37-th coefficient of the characteristic polynomial @xmath40 .",
    "sampling from a ( @xmath37-)dppcan be done in polynomial time , but requires a full eigendecomposition of @xmath6  @xcite , which is prohibitive for large @xmath0 .",
    "a number of approaches have been proposed for more efficient sampling @xcite .",
    "we follow an alternative approach based on gibbs sampling and show that it can offer fast polynomial - time dppsampling and nystrm approximations .",
    "next , we consider sampling @xmath37 landmarks @xmath23 $ ] from @xmath37-dpp(@xmath6 ) , and use the approximation @xmath41 .",
    "we call this approach dpp - nystrm .",
    "it was essentially introduced in  @xcite , but without making the explicit connection to dpps .",
    "our analysis builds on this connection and subsumes existing results that only apply to @xmath37 being the rank @xmath10 of the target approximation .",
    "we begin with error bounds for matrix approximations :    [ thm : nys ] if @xmath42-dpp(@xmath6 ) , then dpp - nystrmsatisfies the relative error bounds    @xmath43 & \\le \\left(\\frac{c+1}{c+1-k}\\right)\\sqrt{n - k } , \\\\    \\mathbb{e}_c\\left[{\\|k - k_{\\cdot c } ( k_{c , c})^\\dagger k_{c\\cdot}\\|_2 \\over \\|k - k_k\\|_2}\\right ] & \\le \\left({c+1\\over c+1-k}\\right)(n - k).\\end{aligned}\\ ] ]    these bounds hold in expectation . an additional argument based on @xcite yields high probability bounds , too ( appendix  [ append : sec : proof ] ) .    to show theorem  [ thm : nys ] , we exploit a property of characteristic polynomials observed in  @xcite .",
    "but first recall that the coefficients of characteristic polynomials satisfy @xmath44 .",
    "[ lem : char ] for any @xmath45 , it holds that @xmath46    with lemma  [ lem : char ] in hand , we are ready to prove theorem  [ thm : nys ] .",
    "we begin with the frobenius norm error , and then show the spectral norm result . using the decomposition @xmath47 , it holds that @xmath48 = \\mathbb{e}_c \\left[\\|b^\\top b - b^\\top b_{\\cdot , c}(b_{\\cdot , c}^\\top b_{\\cdot , c})^\\dagger b_{\\cdot , c}^\\top b\\|_f\\right]\\\\      & = \\mathbb{e}_c \\left[\\|b^\\top ( i - b_{\\cdot , c}(b_{\\cdot , c}^\\top b_{\\cdot , c})^\\dagger b_{\\cdot , c}^\\top ) b\\|_f\\right]= \\mathbb{e}_c \\left[\\|b^\\top ( i - u^c ( u^c)^\\top ) b\\|_f\\right ] ,      \\end{aligned}\\ ] ] where @xmath49 is the svd of @xmath50 .",
    "next , we extend @xmath51 to an orthogonal basis @xmath52\\in\\mathbb{r}^{r(k)\\times r(k)}$ ] of @xmath53 . using that @xmath54 and applying cauchy - schwartz yields    @xmath55= \\mathbb{e}_c \\left[\\|b^\\top ( u^c)^\\perp ( ( u^c)^\\perp)^\\top b\\|_f\\right]\\\\      & = \\mathbb{e}_c \\left[\\sqrt{{\\sum\\nolimits}_{i , j } ( b_i^\\top ( u^c)^\\perp ( ( u^c)^\\perp)^\\top b_j)^2}\\right]\\le \\mathbb{e}_c \\left[\\sqrt{({\\sum\\nolimits}_{i , j } \\|b_i^\\top ( u^c)^\\perp\\|_2 ^ 2 \\|b_j^\\top ( u^c)^\\perp\\|_2 ^ 2)}\\right]\\\\      & = \\mathbb{e}_c \\left[{\\sum\\nolimits}_i \\|b_i^\\top ( u^c)^\\perp\\|_2 ^ 2\\right]= { 1\\over e_c(k)}{\\sum\\nolimits}_{|c| = c}{\\sum\\nolimits}_{i } \\det(b_{\\cdot , c}^\\top b_{\\cdot , c } ) \\|b_i^\\top ( u^c)^\\perp\\|_2 ^ 2\\\\      & \\overset{(a)}= { 1\\over e_c(k)}{\\sum\\nolimits}_{|c| = c}{\\sum\\nolimits}_{i\\notin c } \\det(b_{\\cdot , c\\cup\\{i\\}}b_{\\cdot , c\\cup\\{i\\}}^\\top)\\\\      & \\overset{(b)}{= } ( c+1 ) { e_{c+1}(k)\\over e_c(k)}.    \\end{aligned}\\ ] ]    in @xmath56 , we use that @xmath57 projects vectors onto the null ( column ) space of @xmath58 , and @xmath59 uses the definition of @xmath60 .",
    "with lemma  [ lem : char ] , it follows that @xmath61    the bound on the frobenius norm immediately implies the bound on the spectral norm :    @xmath62 \\;\\;\\le \\mathbb{e}_c \\left[\\|k - k_{\\cdot c } k_{c , c}^\\dagger k_{c\\cdot}\\|_f \\right]\\\\      & \\;\\;\\le { c+1\\over c+1-k}\\sqrt{n - k } \\|k - k_k\\|_f \\;\\;\\le { c+1\\over c+1-k}(n - k ) \\|k - k_k\\|_2 \\qedhere    \\end{aligned}\\ ] ]    [ [ remarks . ] ] remarks .",
    "+ + + + + + + +    compared to previous bounds ( e.g. ,  @xcite on uniform and leverage score sampling ) , our bounds seem somewhat weaker asymptotically ( since as @xmath63 they do not converge to 1 ) .",
    "this suggests that there is an opportunity for further tightening our bounds , which may be worthwhile , given than in section  sec .",
    "[ sec : exp : app ] our extensive experiments on various datasets with dpp - nystrmshow that it attains superior accuracies compared with various state - of - art methods .",
    "our theoretical ( section [ sec : dppnys ] ) and empirical ( section  [ sec : exp : app ] ) results suggest that dpp - nystrmis well - suited for scaling kernel methods . in this section",
    ", we analyze its implications on kernel ridge regression .",
    "the experiments in section  [ sec : exp ] confirm our results empirically .",
    "we have @xmath0 training samples @xmath64 , where @xmath65 are the observed labels under zero - mean noise with finite covariance .",
    "we minimize a regularized empirical loss @xmath66 over an rkhs @xmath67 .",
    "equivalently , we solve the problem @xmath68 for the corresponding kernel matrix @xmath6 . with the squared loss @xmath69 ,",
    "the resulting estimator is @xmath70 and the prediction for @xmath71 is given by @xmath72 .",
    "denoting the noise covariance by @xmath73 , we obtain the risk @xmath74 observe that the bias term is matrix - decreasing ( in @xmath6 ) while the variance term is matrix - increasing . since the estimator   requires expensive matrix inversions , it is common to replace @xmath6 in by an approximation @xmath5 . if @xmath5 is constructed via nystrmwe have @xmath75 , and it directly follows that the variance shrinks with this substitution , while the bias increases . denoting the predictions from @xmath5 by @xmath76 , theorem  [ thm : krr ] completes the picture of how using @xmath5 affects the risk .",
    "[ thm : krr ] if @xmath5 is constructed via dpp - nystrm , then @xmath77\\le 1 + { ( c+1)\\over n\\gamma } { e_{c+1}(k)\\over e_c(k)}.\\end{aligned}\\ ] ]    again , using  @xcite , we obtain bounds that hold with high probability ( appendix  [ append : sec : proof ] ) .",
    "we build on  @xcite .",
    "knowing that @xmath78 as @xmath75 , it remains to bound the bias .",
    "using @xmath47 and @xmath79 , we obtain    @xmath80    where @xmath81 .",
    "since @xmath82 and @xmath83 commute , we have    @xmath84    it follows that    @xmath85    hence ,    @xmath86    finally , this inequality implies that    @xmath87    taking the expectation over @xmath88-dpp(@xmath6 ) yields    @xmath89 \\le 1 + \\mathbb{e}_c\\left[{\\nu_c \\over n\\gamma}\\right]= 1 + { ( c+1)\\over n\\gamma } { e_{c+1}(k)\\over e_c(k)}.\\end{aligned}\\ ] ]    together with the fact that @xmath90 , we obtain    @xmath91 & = \\mathbb{e}_c \\left[\\sqrt{\\mathrm{bias}(\\tilde{k } ) + \\mathrm{var}(\\tilde{k})\\over \\mathrm{bias}(k ) + \\mathrm{var}(k)}\\right]\\\\ & \\le 1 + { ( c+1)\\over n\\gamma } { e_{c+1}(k)\\over e_c(k)}\\end{aligned}\\ ] ]    for any @xmath92 .    [ [ remarks.-1 ] ] remarks .",
    "+ + + + + + + +    theorem  [ thm : krr ] quantifies how the learning results depend on the decay of the spectrum of @xmath6 .",
    "in particular , the ratio @xmath93 closely relates to the effective rank of @xmath6 : if @xmath94 and @xmath95 , this ratio is almost zero , resulting in near - perfect approximations and no loss in learning .",
    "there exist works that consider nystrmmethods in this scenario  @xcite .",
    "our theoretical bounds could also be tightened in this setting , possibly by a tighter bound on the elementary symmetric polynomial ratio .",
    "this theoretical exercise may be worthwhile given our extensive experiments comparing dpp - nystrmagainst other state - of - art methods in  sec .",
    "[ sec : exp : krr ] that reveal the superior performance of dpp - nystrm .",
    "despite its excellent empirical performance and strong theoretical results , determinantal sampling for nystrmhas rarely been used in applications due to the computational cost of @xmath7 for directly sampling from a dpp , which involves an eigendecomposition .",
    "instead , we follow a different route : an mcmc sampler , which offers a promising alternative if the chain mixes fast enough .",
    "recent empirical results provide initial evidence @xcite , but without a theoretical analysis ; other recent works  @xcite do not apply to our cardinality - constrained setting .",
    "we offer a theoretical analysis that confirms fast mixing ( i.e. , polynomial or even _",
    "linear_-time sampling ) under certain conditions , and connect it to our empirical results .",
    "the empirical results in section  [ sec : exp ] illustrate the favorable performance of dpp - nystrmin trading off time and error . concurrently with this paper , @xcite derived a different , general analysis of fast mixing that also confirms our observations .",
    "algorithm  [ algo : mcdpp ] shows a gibbs sampler for @xmath10-dpp . starting with a uniformly random set @xmath96 , at iteration @xmath97",
    ", we try to swap an element @xmath98 with an element @xmath99 , according to @xmath100 and @xmath101 .",
    "the stationary distribution of this chain is exactly the desired @xmath10-dpp(@xmath6 ) .    *",
    "input : * @xmath6 the kernel matrix , @xmath102 $ ] the ground set * output : * @xmath103 sampled from exact @xmath37-dpp(@xmath6 ) randomly initialize @xmath104 , @xmath105 sample @xmath106 from uniform bernoulli distribution pick @xmath107 and @xmath108 uniformly randomly + @xmath109 @xmath110 with prob . @xmath111",
    "the _ mixing time _",
    "@xmath112 of the chain is the number of iterations until the distribution over the states ( subsets ) is close to the desired one , as measured by total variation : @xmath113 .",
    "we bound @xmath112 via coupling techniques .",
    "given a markov chain @xmath114 on a state space @xmath115 with transition matrix @xmath116 , a _ coupling _ is a new chain @xmath117 on @xmath118 such that both @xmath114 and @xmath119 , if considered marginally , are markov chains with the same transition matrix @xmath116 . the key point of coupling is to construct such a new chain to encourage @xmath120 and @xmath121 to _ coalesce _ quickly . if , in the new chain , @xmath122 for some fixed @xmath97 regardless of the starting state @xmath123 , then @xmath124  @xcite .",
    "such coalescing chains can be difficult to construct .",
    "_ path coupling _",
    "@xcite relieves this burden by reducing the coupling to adjacent states in an appropriately constructed state graph .",
    "the coupling of arbitrary states follows by aggregation over a path between the states .",
    "path coupling is formalized in the following lemma .",
    "@xcite [ lem : pathcoupling ] let @xmath125 be an integer - valued metric on @xmath118 where @xmath126 .",
    "let @xmath127 be a subset of @xmath128 such that for all @xmath129 there exists a path @xmath130 between @xmath120 and @xmath121 where @xmath131 for @xmath132 $ ] and @xmath133 .",
    "suppose a coupling @xmath134 of the markov chain is defined on all pairs in @xmath127 such that there exists an @xmath135 such that @xmath136\\le \\alpha \\delta(r , t)$ ] for all @xmath137 , then we have @xmath138    the lemma says that if we have a contraction of the two chains in expectation ( @xmath135 ) , then the chain mixes fast .",
    "with the path coupling lemma , we obtain a bound on the mixing time that can be _ linear _ in the data set size @xmath0 .",
    "the actual mixing time depends on three quantities that relate to how sensitive the transition probabilities are to swapping a single element in a set of size @xmath37 .",
    "consider an arbitrary set @xmath139 of columns , @xmath140 , and complete it to two @xmath37-sets @xmath141 and @xmath142 that differ in exactly one element .",
    "our quantities are , for @xmath143 , and @xmath144 : @xmath145    [ thm : mix ] let the contraction coefficient @xmath146 be given by    @xmath147\\backslash s , r\\neq t}\\sum_{u_3\\in s , u_4\\notin s\\cup\\{r , t\\}}p_3(s , r , t , u_3,u_4)- \\sum_{u_1\\notin s\\cup\\{r , t\\}}p_1(s , r , t , u_1)-\\sum_{u_2\\in s}p_2(s , r , t , u_2).\\end{aligned}\\ ] ]    when @xmath135 , the mixing time for the gibbs sampler in algorithm  [ algo : mcdpp ] is bounded as @xmath148    we bound the mixing time via path coupling .",
    "let @xmath149 be half the hamming distance on the state space , and define @xmath127 to consist of all state pairs @xmath150 in @xmath128 such that @xmath151 .",
    "we intend to show that for all states @xmath137 and next states @xmath152 , we have @xmath136\\le \\alpha \\delta(r , t)$ ] for an appropriate @xmath146 .",
    "since @xmath151 , the sets @xmath153 and @xmath154 differ in only two entries .",
    "let @xmath155 , so @xmath156 and @xmath157 and @xmath158 . for a state transition",
    ", we sample an element @xmath159 and @xmath160\\backslash r$ ] as switching candidates for @xmath153 , and elements @xmath161 and @xmath162\\backslash t$ ] as switching candidates for @xmath154 .",
    "let @xmath163 and @xmath164 be the bernoulli random variables indicating whether we try to make a transition . in our coupling",
    "we always set @xmath165 .",
    "hence , if @xmath166 then both chains will not transition and the distance of states remains . for @xmath167 ,",
    "we distinguish four cases :    [ [ case - c1 ] ] case c1 + + + + + + +    if @xmath168 and @xmath169 , we let @xmath170 and @xmath171 . as a result ,",
    "@xmath172 .",
    "[ [ case - c2 ] ] case c2 + + + + + + +    if @xmath168 and @xmath173 , we let @xmath170 and @xmath174 . in this case , if both chains transition , then the resulting distance is zero , otherwise it remains one . with probability @xmath175 both chains transition .",
    "[ [ case - c3 ] ] case c3 + + + + + + +    if @xmath176 and @xmath169 , we let @xmath177 and @xmath171 . again , if both chains transition , then the resulting distance is @xmath178 , otherwise it remains one . with probability @xmath179 both chains transition .",
    "[ [ case - c4 ] ] case c4 + + + + + + +    if @xmath180 and @xmath181 , we let @xmath182 and @xmath183 .",
    "if both chains make the same transition ( both move or do not move ) , the resulting distance is one , otherwise it increases to 2 .",
    "the distance increases with probability @xmath184 .    with those four cases , we can now bound @xmath136 $ ] . for all @xmath185 , i.e. , @xmath186 :    @xmath187\\over \\mathbb{e}[\\delta(r , t ) ] } = { 1\\over 2 } + \\text{pr}(c2 ) \\mathbb{e}[\\delta(r',t')| c2 ] + \\text{pr}(c3 ) \\mathbb{e}[\\delta(r',t')| c3 ] + \\text{pr}(c4 ) \\mathbb{e}[\\delta(r',t')| c4]\\\\ & = \\frac12 + { 1\\over 2c(n - c)}\\big(\\sum_{u_1\\notin s\\cup\\{r , t\\}}(1 - p_1(u_1 ) ) + \\sum_{u_2\\in s}(1 - p_2(u_2 ) ) +   \\sum_{\\substack{u_3\\in s,\\\\ u_4\\notin s\\cup\\{r , t\\}}}(1 + p_3(u_3,u_4))\\big ) \\\\ & = { 1\\over 2c(n - c)}\\big(2c(n-1)+\\sum_{\\substack{u_3\\in s,\\\\",
    "u_4\\notin s\\cup\\{r , t\\}}}p_3(u_3,u_4 ) -",
    "\\sum_{u_1\\notin s\\cup\\{r , t\\}}p_1(u_1)-\\sum_{u_2\\in s}p_2(u_2 ) -1\\big),\\end{aligned}\\ ] ]    where we did not explicitly write the arguments @xmath188 to @xmath189 . for    @xmath190\\backslash s,\\\\r\\neq t}}&\\sum_{\\substack{u_3\\in s,\\\\ u_4\\notin s\\cup\\{r , t\\}}}p_3(u_3,u_4)-\\sum_{u_1\\notin s\\cup\\{r , t\\}}p_1(u_1)-\\sum_{u_2\\in s}p_2(u_2)\\end{aligned}\\ ] ]    and",
    "@xmath135 the path coupling lemma  [ lem : pathcoupling ] implies that @xmath191    [ [ remarks.-2 ] ] remarks .",
    "+ + + + + + + +    if @xmath192 is fixed , then the mixing time ( running time ) depends only linearly on @xmath0 .",
    "the coefficient @xmath146 itself depends on our three quantities .",
    "in particular , fast mixing requires @xmath193 ( the difference between transition probabilities ) to be very small compared to @xmath194 , @xmath195 , at least on average .",
    "the difference @xmath193 measures how exchangeable two points @xmath196 and @xmath97 are .",
    "this notion of symmetry is closely related to a symmetry that determines the complexity of submodular maximization  @xcite ( indeed , @xmath197 is a submodular function ) .",
    "this symmetry only needs to hold for most pairs @xmath196 , @xmath97 , and most swapping points @xmath198 , @xmath199 .",
    "it holds for kernels with sufficiently fast - decaying similarities , similar to the conditions in  @xcite for unconstrained sampling .",
    "one iteration of the sampler can be implemented efficiently in @xmath200 time using block inversion @xcite .",
    "additional speedups via quadrature are also possible @xcite . together with the analysis of mixing time , this leads to fast sampling methods for @xmath10-dpps .",
    "in our experiments , we evaluate the performance of dpp - nystrmon both kernel approximation and kernel learning tasks , in terms of running time and accuracy .",
    "we use 8 datasets : abalone , ailerons , elevators , compact , compact(s ) , bank32nh , bank8fm and california housing .",
    "we subsample 4,000 points from each dataset ( 3,000 training and 1,000 test ) . throughout our experiments",
    ", we use an rbf kernel and choose the bandwidth @xmath201 and regularization parameter @xmath202 for each dataset by 10-fold cross - validation .",
    "we initialize the gibbs sampler via kmeans++ and run for 3,000 iterations .",
    "results are averaged over 3 random subsets of data .",
    "we first explore dpp - nystrm(`kdpp`in the figures ) for approximating kernel matrices .",
    "we compare to uniform sampling  ( ` unif ` ) and leverage score sampling ( ` lev ` ) @xcite as baseline landmark selection methods",
    ". we also include adapfull  ( ` adapfull ` ) @xcite that performs quite well in practice but scales poorly , as @xmath203 , with the size of dataset .",
    "although sampling with regularized leverage scores ( ` reglev ` ) @xcite is not originally designed for kernel approximations , we include its results to see how regularization affects leverage score sampling .",
    "figure  [ fig : app_ailerons_mc ] shows example results on the ailerons data ; further results may be found in the appendix .",
    "dpp - nystrmperforms well , achieving the lowest error as measured in both spectral and frobenius norm .",
    "the only method that is on par in terms of accuracy is ` adapfull ` , which has a much higher running time .        for a different perspective",
    ", figure  [ fig : rel_app_mc ] shows the improvement in error over ` unif ` .",
    "relative improvements are averaged over all data sets .",
    "again , the performance of dpp - nystrmalmost always dominate those of other methods , and achieves an up to 80% reduction in error .",
    "next , we apply dpp - nystrmto kernel ridge regression , comparing against uniform sampling ( ` unif ` ) @xcite and regularized leverage score sampling ( ` reglev ` ) @xcite which have theoretical guarantees for this task .",
    "figure  [ fig : krr_ailerons ] illustrates an example result : non - uniform sampling greatly improves accuracy , with ` kdpp`improving over regularized leverage scores in particular for a small number of landmarks , where a single column has a larger effect .",
    "figure  [ fig : rel_krr ] displays the average improvement over ` unif ` , averaged over 8 data sets .",
    "again , the performance of ` kdpp`dominates ` reglev`and ` unif ` , and leads to gains in accuracy . on average ` kdpp`consistently achieves more than @xmath204 improvement over ` unif ` .",
    ".4     .4       in the next experiment , we empirically study the mixing of the gibbs chain with respect to matrix approximation errors , the ultimate measure that is of interest in our application of the sampler .",
    "we use @xmath205 and choose @xmath0 as 1,000 and 4,000 . to exclude impacts of the initialization , we pick the initial state @xmath96 uniformly at random .",
    "we run the chain for 5,000 iterations , monitoring how the error changes with the number of iterations .",
    "example results on the ailerons data are shown in figure  [ fig : conv_ailerons_50_fnorm ] .",
    "empirically , the error drops very quickly and afterwards fluctuates only little , indicating a fast convergence of the approximation error . other error measures and larger @xmath37 , included in the appendix , confirm this trend .",
    "notably , our empirical results suggest that the mixing time does not increase much as @xmath0 increases greatly , suggesting that the gibbs sampler remains fast even for large @xmath0",
    ".    in theorem  [ thm : mix ] , the mixing time depends on the quantity @xmath146 . by subsampling 1,000 random sets",
    "@xmath139 and column indices @xmath206 , we approximately computed @xmath146 on our data sets .",
    "we find that , as expected , @xmath135 in particular for kernels with a smaller bandwidth , and in general @xmath146 increases with @xmath10 . in accordance with the theory",
    ", we found that the mixing time ( in terms of error ) too increases with @xmath10 . in practice",
    ", we observe a fast drop in error even for cases where @xmath207 , indicating that theorem  [ thm : mix ] is conservative and that the iterative mcmc approach is even more widely applicable .",
    ".4     .4     iterative methods like the gibbs sampler offer tradeoffs between time and error .",
    "the longer the markov chain runs , the closer the sampling distribution is to the desired dpp , and the higher the accuracy obtained by nystrm .",
    "we hence explicitly show the time and accuracy trade - off of the sampler on ailerons ( of size 4,000 ) for up to 200 and california housing ( of size 12,000 ) for up to 100 iterations .",
    "a similar tradeoff occurs with leverage scores . for the experiments in the other sections",
    ", we computed the ( regularized ) leverage scores for ` lev`and ` reglev`exactly .",
    "this requires a full , computationally expensive eigendecomposition . for a fast , rougher approximation",
    ", we here compare to an approximation mentioned in  @xcite .",
    "concretely , we sample @xmath208 elements with probability proportional to the diagonal entries of kernel matrices @xmath209 , and then use a nystrm - like method to construct an approximate low - rank decomposition of @xmath6 , and compute scores based on this approximation .",
    "we vary @xmath208 from 20 to 340 on ailerons and 20 to 140 on california housing to show the tradeoff for approximate leverage score sampling  ( ` applev ` ) and regularized leverage score sampling  ( ` appreglev ` ) .",
    "we also include adappartial ( ` adappart ` )  @xcite that approximates ` adapfull`and is much more efficient , and kmeans nystrm(`kmeans ` )  @xcite that empirically perform very well in kernel approximation .",
    "figure  [ fig : tradeoff ] summarizes and compares the tradeoffs offered by these different methods on the ailerons and california housing datasets .",
    "the @xmath210 axis indicates time , the @xmath211 axis error , so the lower left is the preferred corner .",
    "we see that ` adapfull ` , ` lev`and ` reglev`are expensive and perform worse than ` kdpp ` .",
    "the approximate variants ` adappart ` , ` applev`and ` appreglev`have comparable efficiency but higher error .",
    "on the smaller data , ` kmeans`is accurate but needs more time than ` kdpp ` , while on the larger data it is dominated in both accuracy and time by ` kdpp ` .",
    "overall , on the larger data , dpp - nystrmoffers the best tradeoff of accuracy and efficiency .",
    "in this paper , we revisited the use of @xmath10-determinantal point processes for sampling good landmarks for the nystrmmethod .",
    "we theoretically and empirically observe its competitive performance , for both matrix approximation and ridge regression , compared to state - of - the - art methods .    to make this accurate method scalable to large matrices",
    ", we consider an iterative approach , and analyze it theoretically as well as empirically .",
    "our results indicate that the iterative approach , a gibbs sampler , achieves good landmark samples quickly ; under certain conditions even in a number of iteratons linear in @xmath0 , for an @xmath0 by @xmath0 matrix . finally , our empirical results demonstrate that among state - of - the - art methods , the iterative sampler yields the best tradeoff between efficiency and accuracy .",
    "this research was partially supported by an nsf career award 1553284 , nsf grant iis-1409802 , and a google research award .",
    "we also thank xixian chen for discussions .",
    "44 [ 1]#1 [ 1]`#1 ` urlstyle [ 1]doi : # 1    r.  h. affandi , a.  kulesza , e.  fox , and b.  taskar .",
    "ystrm approximation for large - scale determinantal processes . in _ aistats _ , pages 8598 , 2013 .",
    "a.  e. alaoui and m.  w. mahoney .",
    "fast randomized kernel methods with statistical guarantees . _ nips _ , 2015 .",
    "d.  j. aldous .",
    "some inequalities for reversible markov chains . _ journal of the london mathematical society _ ,",
    "pages 564576 , 1982 .",
    "n.  anari , s.  o. gharan , and a.  rezaei .",
    "onte carlo markov chain algorithms for sampling strongly rayleigh distributions and determinantal point processes . in _ colt _ , 2016 .",
    "f.  r. bach .",
    "sharp analysis of low - rank kernel matrix approximations . _",
    "colt _ , 2013 .",
    "f.  r. bach and m.  i. jordan .",
    "kernel independent component analysis .",
    "_ jmlr _ , pages 148 , 2003 .",
    "f.  r. bach and m.  i. jordan .",
    "predictive low - rank decomposition for kernel methods . in _ icml _ , pages 3340 , 2005 .",
    "belabbas and p.  j. wolfe . on landmark selection and sampling in high - dimensional data analysis .",
    "_ philosophical transactions of the royal society of london a : mathematical , physical and engineering sciences _ , pages 42954312 , 2009 .",
    "belabbas and p.  j. wolfe .",
    "spectral methods in machine learning and new strategies for very large datasets",
    ". _ proceedings of the national academy of sciences _ , pages 369374 , 2009 .",
    "j.  borcea , p.  brndn , and t.  liggett .",
    "negative dependence and the geometry of polynomials . _ journal of the american mathematical society _ ,",
    "pages 521567 , 2009 .",
    "r.  bubley and m.  dyer .",
    "path coupling : a technique for proving rapid mixing in markov chains . in",
    "_ focs _ , pages 223231 , 1997 .    c.  cortes , m.  mohri , and a.  talwalkar . on the impact of kernel approximation on learning accuracy . in _ aistats _ ,",
    "pages 113120 , 2010 .",
    "a.  deshpande , l.  rademacher , s.  vempala , and g.  wang .",
    "matrix approximation and projective clustering via volume sampling . in _ soda _ ,",
    "pages 11171126 , 2006 .",
    "p.  drineas and m.  w. mahoney . on the nystrm method for approximating a gram matrix for",
    "improved kernel - based learning .",
    "_ jmlr _ , pages 21532175 , 2005 .",
    "p.  drineas , r.  kannan , and m.  w. mahoney .",
    "fast monte carlo algorithms for matrices ii : computing a low - rank approximation to a matrix .",
    "_ siam journal on computing _ ,",
    "pages 158183 , 2006 .",
    "m.  dyer and c.  greenhill . a more rapidly mixing markov chain for graph colorings .",
    "_ random structures and algorithms _ , pages 285317 , 1998 .",
    "s.  fine and k.  scheinberg .",
    "efficient svm training using low - rank kernel representations .",
    "_ jmlr _ , pages 243264 , 2002 .    c.  fowlkes , s.  belongie , f.  chung , and j.  malik .",
    "spectral grouping using the nystrm method .",
    "_ tpami _ , pages 214225 , 2004 .",
    "a.  gittens and m.  w. mahoney . revisiting the nystrm method for improved large - scale machine learning .",
    "_ icml _ , 2013 .",
    "g.  h. golub and c.  f. van  loan . _ matrix computations_. jhu press , 2012 .",
    "a.  gotovos , h.  hassani , and a.  krause .",
    "sampling from probabilistic submodular models . in _ nips _ , pages 19361944 , 2015 .",
    "v.  guruswami and a.  k. sinop .",
    "optimal column - based low - rank matrix reconstruction . in _ soda _ , pages 12071214 , 2012 .",
    "j.  b. hough , m.  krishnapur , y.  peres , and b.  virg .",
    "determinantal processes and independence .",
    "_ probability surveys _ , pages 206229 , 2006 .",
    "b.  kang .",
    "fast determinantal point process sampling with application to clustering . in _ nips _ , pages 23192327 , 2013 .",
    "a.  kulesza and b.  taskar .",
    "k - dpps : fixed - size determinantal point processes . in",
    "icml _ , pages 11931200 , 2011 .",
    "a.  kulesza and b.  taskar .",
    "determinantal point processes for machine learning .",
    "_ arxiv preprint arxiv:1207.6083 _",
    ", 2012 .",
    "s.  kumar , m.  mohri , and a.  talwalkar .",
    "ensemble nystrm method . in _ nips _ , pages 10601068 , 2009 .",
    "s.  kumar , m.  mohri , and a.  talwalkar .",
    "sampling methods for the nystrm method .",
    "_ the journal of machine learning research _ , pages 9811006 , 2012 .    c.  li , s.  jegelka , and s.  sra",
    "efficient sampling for k - determinantal point processes .",
    "_ aistats _ , 2016 .    c.  li , s.  sra , and s.  jegelka .",
    "gaussian quadrature for matrix inverse forms with applications . in _",
    "icml _ , 2016 .",
    "o.  macchi .",
    "the coincidence approach to stochastic point processes .",
    "_ advances in applied probability _ , pages 83122 , 1975 .",
    "e.  j. nystrm .",
    "ber die praktische auflsung von integralgleichungen mit anwendungen auf randwertaufgaben .",
    "_ acta mathematica _ , pages 185204 , 1930 .",
    "r.  pemantle and y.  peres .",
    "concentration of lipschitz functionals of determinantal and other strong rayleigh measures .",
    "_ combinatorics , probability and computing _ , pages 140160 , 2014 .",
    "p.  rebeschini and a.  karbasi . fast mixing for discrete point processes .",
    "_ colt _ , 2015 .",
    "a.  rudi , r.  camoriano , and l.  rosasco .",
    "less is more : nystrm computational regularization . _",
    "nips _ , 2015 .",
    "h.  shen , s.  jegelka , and a.  gretton .",
    "fast kernel - based independent component analysis .",
    "_ ieee transactions on signal processing _ , pages 34983511 , 2009 .",
    "a.  j. smola and b.  schlkopf .",
    "sparse greedy matrix approximation for machine learning .",
    "_ icml _ , 2000 .",
    "s.  sun , j.  zhao , and j.  zhu .",
    "a review of nystrm methods for large - scale machine learning .",
    "_ information fusion _ ,",
    "pages 3648 , 2015 .",
    "a.  talwalkar , s.  kumar , and h.  rowley . large - scale manifold learning . in",
    "cvpr _ , 2008 .",
    "a.  talwalkar , s.  kumar , m.  mohri , and h.  rowley .",
    "large - scale svd and manifold learning .",
    "_ jmlr _ , pages 31293152 , 2013 .",
    "j.  vondrk .",
    "symmetry and approximability of submodular maximization problems .",
    "_ siam journal on computing _ , 420 ( 1):0 265304 , 2013 .",
    "s.  wang , c.  zhang , h.  qian , and z.  zhang . using the matrix ridge approximation to speedup determinantal point processes sampling algorithms . in _",
    "aaai _ , 2014 .",
    "c.  williams and m.  seeger . using the nystrm method to speed up kernel machines . in _ nips _ , pages 682688 , 2001 .",
    "k.  zhang , i.  w. tsang , and j.  t. kwok .",
    "improved nystrm low - rank approximation and error analysis . in _ icml _ , pages 12321239 , 2008 .",
    "to show high probability bounds we employ concentration results on homogeneous strongly rayleigh measures .",
    "specifically , we use the following theorem .",
    "[ thm : concentration ] let @xmath212 be a @xmath10-homogeneous strongly rayleigh probability measure on @xmath213 and @xmath214 an @xmath215-lipschitz function on @xmath213 , then @xmath216\\ge a\\ell ) \\le \\exp\\{-a^2/8k\\}.\\ ] ]    it is known that a @xmath10-dppis a homogeneous strongly rayleigh measure on @xmath213  @xcite , thus theorem  [ thm : concentration ] applies to results obtained with @xmath10-dpp . concretely , for the bound in theorem  [ thm : nys ] that holds in expectation , we have the following bound that holds with high probability :    [ cor : prob ] when sampling @xmath217-dpp(@xmath6 ) , for any @xmath218 , with probability at least @xmath219 we have    @xmath220    where @xmath221 are the eigenvalues of @xmath6 .",
    "the lipschitz constants of the relative errors are upper bounded by @xmath222 and @xmath223 , respectively .",
    "applying theorem  [ thm : concentration ] yields the results .    for the bound in theorem  [ thm : krr ] that holds in expectation ,",
    "we have the following bound that holds with high probability :    if @xmath5 is constructed via dpp - nystrm , then with probability at least @xmath224 , @xmath225 is upper - bounded by    @xmath226    consider the function @xmath227 . since @xmath228 , it follows that the lipschitz constant for @xmath229 is at most @xmath230 .",
    "thus when @xmath217-dppand @xmath218 , by applying theorem  [ thm : concentration ] we see that the inequality @xmath231 + \\sqrt{8c\\log(1/\\delta ) } \\text{tr}(k)$ ] holds with probability at least @xmath224 .",
    "hence @xmath232\\le 1 + \\mathbb{e}\\left[{\\nu_c\\over n\\gamma}\\right ] + \\sqrt{8c\\log(1/\\delta ) } { \\text{tr}(k)\\over n\\gamma}\\\\ & \\;\\;\\;\\;= 1 + { 1\\over   n\\gamma } \\left({(c+1 ) e_{c+1}(k)\\over e_c(k ) } + \\sqrt{8c\\log(1/\\delta ) } \\text{tr}(k)\\right)\\end{aligned}\\ ] ] holds with probability at least @xmath224 .",
    "we first show the mixing of the gibbs dpp - nystrmwith 50 landmarks with different performance measures : relative spectral norm error , training error and test error of kernel ridge regression in  fig .",
    "[ append : fig : conv_ailerons_50 ] .",
    "we also show corresponding results with respect to 100 and 200 landmarks in  fig .",
    "[ append : fig : conv_ailerons_100 ] and  fig .",
    "[ append : fig : conv_ailerons_200 ] , so as to illustrate that for varying number of landmarks the chain is indeed fast mixing and will give reasonably good result within a small number of iterations .",
    "we next show time - error trade - offs for various sampling methods on small and larger datasets with respect to fnorm and 2norm errors .",
    "we sample 20 landmarks from ailerons dataset of size 4,000 and california housing of size 12,000 .",
    "the result is shown in figure  [ append : fig : ailerons_tradeoff_large ] and figure  [ append : fig : calhousing_tradeoff_large ] and similar trends as the example results in the main text could be spotted : on small scale dataset ( size 4,000 ) ` kdpp`get very good time - error trade - off .",
    "it is more efficient than ` kmeans ` , though the error is a bit larger . while on larger dataset ( size 12,000 ) the efficiency is further enhanced while the error is even lower than ` kmeans ` .",
    "it also have lower variances in both cases compared to ` applev`and ` appreglev ` .",
    "overall , on larger dataset we obtain the best time - error trade - off with ` kdpp ` ."
  ],
  "abstract_text": [
    "<S> the nystrmmethod has long been popular for scaling up kernel methods . </S>",
    "<S> its theoretical guarantees and empirical performance rely critically on the quality of the _ landmarks _ selected . </S>",
    "<S> we study landmark selection for nystrmusing determinantal point processes ( dpps ) , discrete probability models that allow tractable generation of _ diverse _ samples . </S>",
    "<S> we prove that landmarks selected via dpps guarantee bounds on approximation errors ; subsequently , we analyze implications for kernel ridge regression . contrary to prior reservations due to cubic complexity of dppsampling , we show that ( under certain conditions ) markov chain dppsampling requires only _ linear _ time in the size of the data . </S>",
    "<S> we present several empirical results that support our theoretical analysis , and demonstrate the superior performance of dpp - based landmark selection compared with existing approaches . </S>"
  ]
}