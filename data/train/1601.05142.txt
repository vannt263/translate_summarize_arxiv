{
  "article_text": [
    "as the web grows as the primary medium for publication , communication , and other services , so grows the importance of preserving the web ( as evidenced by recent articles in the new yorker @xcite , npr @xcite , and _ the atlantic _",
    "web resources are ephemeral , existing in the perpetual _ now _ ; important historical events frequently disappear from the web without being preserved or recorded .",
    "we may miss pages because we are not aware they should be saved or because the pages themselves are hard to archive .    on july 17 , 2015",
    ", ukrainian separatists announced via social media , with video evidence , that they shot down a military cargo plane in ukrainian airspace ( figure [ shotdown ] ) .",
    "however , the downed plane was actually the commercial malaysian airlines flight 17 ( mh17 ) .",
    "the ukrainian separatists removed from social media their claim of shooting down what we now know was a non - military passenger plane .",
    "the internet archive @xcite , using the heritrix web crawler @xcite , was crawling and archiving the social media site twice daily and archived the claimed credit for downing the aircraft ; this is now the only definitive evidence that ukrainian separatists shot down mh17 @xcite .",
    "this is an example of the importance of high - fidelity web archiving to record history and establish evidence of information published on the web .        however , not all historical events are archived as fortuitously as the mh17 example . in an attempt to limit online piracy and theft of intellectual property , the u.s .",
    "government proposed the widely unpopular stop online piracy act ( sopa ) @xcite .",
    "while the attempted passing of sopa may be a mere footnote in history , the overwhelming protest in response is significant . on january 18 , 2012 , many prominent websites organized a world - wide blackout in protest of sopa @xcite .",
    "wikipedia blacked out their site by using javascript to load a `` splash page '' that prevented access to wikipedia s content ( figure [ wiki1 ] ) .",
    "the internet archive , using heritrix , archived the wikipedia site during the protest .",
    "however , the archived january 18 , 2012 page , as replayed in the wayback machine @xcite , does not include the splash page ( figure [ wikisopa2 ] ) @xcite .",
    "because archival crawlers such as heritrix are not able to execute javascript , they neither discovered nor archived the splash page .",
    "wikipedia s protest as it appeared on january 18 , 2012 has been lost from the archives and , without human efforts , would be potentially lost from human history .",
    "the sopa protest , like mh17 , is an example of an important historical event . unlike the mh17 example ( which establishes our need to archive with high fidelity ) ,",
    "the sopa example is not well represented in the archives . in this work ,",
    "we present a method to improve the fidelity of javascript - dependent archival copies .",
    "specifically , we show that archival crawlers that use phantomjs can interact just two levels deep into a representation and uncover 15.6 times more embedded resources ( 70.9% of which are available via onclick events ) .",
    "[ [ problem ] ] problem description + + + + + + + + + + + + + + + + + + +    the current rate of browsers implementing ( and content authors adopting ) client - side technologies such as javascript is much faster than crawlers abilities to develop tools to crawl web resources that leverage the technologies .",
    "this leads to a difference between the web that crawlers can discover and the web that human users experience ",
    "a challenge impacting the archives as well as other web - scale crawlers ( e.g. , those used by search engines ) . over time , live web resources have been more heavily leveraging javascript ( i.e. , ajax ) to load embedded resources @xcite . because javascript - dependent representations are not accessible to web - scale archival crawlers , their representations are not fully archived .",
    "when the representation is replayed from the archive , the javascript will execute and may issue ajax requests for a resource that is on the live web , which leads to one of two possible outcomes : the live web `` leaking '' into the archive leading to an incorrect representation @xcite , or missing embedded resources ( i.e. , returns a 400 or 500 class http response ) in the archived resource leading to an incomplete representation , both of which result in reduced archival quality @xcite .",
    "when an archived deferred representation loads embedded resources from the live web via leakage , it is a _ zombie _ resource , leaving the representation incorrect , and potentially _ prima facie violative _ @xcite .",
    "we refer to the ease of archiving a web resource as _ archivability _ , and have shown that resources that rely on javascript to construct their representations have lower archivability than resources that avoid javascript @xcite .",
    "heritrix archives pages by beginning with an initial seed list of universal resource identifiers ( uris ) , dereferencing a uri from the list , archiving the returned representation , extracting embedded uris to add to its crawl frontier , and repeating until the crawl frontier is exhausted .",
    "heritrix does not execute any client - side scripts or use headless or headful browsing technologies @xcite .",
    "we define _ deferred representations _ as representations of resources that rely on javascript and other client - side technologies to initiate requests for embedded resources after the initial page load .",
    "we use the term _ deferred _ because the representation is not fully realized and constructed until _",
    "after _ the javascript code is executed on the client .",
    "note that the mere presence of javascript does not indicate that a representation will be deferred .",
    "a deferred representation may be interactive , but its reliance on ajax and javascript to initiate http requests for post - load resources makes the representation deferred .",
    "http transactions are stateless , meaning the representation is often indexable and identified by a combination of a timestamp and uri @xcite .",
    "however , client - side technologies such as javascript have made representations state__ful _ _ , allowing representations and their states to change independent of the uri and the time at which the representation was received from the server . in a resource with a deferred representation and multiple descendants . ] , user or client - side interactions generate requests for additional embedded resources .    [ [ contributions ] ] contributions + + + + + + + + + + + + +    in this paper , we define a representation constructed as a result of user interaction or other client - side event without a subsequent request for the resource s uri as a _ _ descendant _ _",
    "( i.e. , a member of the client - side event tree below the root ) .",
    "client - side events may also trigger a request for additional resources to be included in the representation , which leads to deferred representations .",
    "we explore the number and characteristics of descendants as they pertain to web archiving and explore the cost - benefit trade - off of actively crawling and archiving descendants en route to a higher quality , more complete archive .",
    "dincturk et al .",
    "@xcite constructed a model for crawling rich internet applications ( rias ) by discovering all possible descendants and identifying the simplest possible state machine to represent the states .",
    "we explore the archival implementations of their hypercube model by discovering client - side states ( represented as a tree ) and their embedded resources to understand the impact that deferred representations have on the archives .",
    "we evaluate the performance impacts of exploring descendants ( i.e. , crawl time , depth , and breadth ) against the improved coverage of the crawler ( i.e. , frontier size ) along with the presence of embedded resources unique to descendants in the internet archive , using heritrix as our case study of a web - scale crawling tool .",
    "we show that the vast majority ( 92% in @xmath0 and 96% in @xmath1 ) and @xmath1 are defined in section 3 .",
    "] of embedded resources loaded as a result of user interactions are not archived , and that there are two levels in the interaction trees of our uri - rs .    throughout this paper",
    "we use memento framework terminology .",
    "memento @xcite is a framework that standardizes web archive access and terminology .",
    "original ( or live web ) resources are identified by uri - r , and archived versions of uri - rs are called _ mementos _ and are identified by uri - m .",
    "banos et al . @xcite created an algorithm to evaluate archival success based on adherence to standards for the purpose of assigning an archivability score to a uri - r . in our previous work @xcite",
    ", we studied the impact of accessibility standards on archivability and memento completeness .",
    "we also measured the correlation between the adoption of javascript and the number of missing embedded resources in the archives @xcite .",
    "spaniol @xcite measured the quality of web archives based on matching crawler strategies with resource change rates .",
    "ben saad and ganarski @xcite performed a similar study regarding the importance of changes on a page .",
    "gray and martin @xcite created a framework for high quality mementos and assessed their quality by measuring the missing embedded resources . in previous work @xcite , we assigned a quantitative metric to a previously qualitative measurement of memento quality and measured a reduction in memento quality caused by javascript .",
    "these works study quality , helping us understand what is missing from mementos .    google has made efforts toward indexing deferred representations @xcite  a step in the direction of solving the archival challenges posed by javascript .",
    "google s indexing focuses on rendering an accurate representation for indexing and discovering new uris , but does not fully solve archival the challenges caused by javascript .",
    "archiving web resources and indexing representations are different activities that have differing goals and processes .",
    "browsertrix @xcite and webrecorder.io @xcite are page - at - a - time archival tools for deferred representations and descendants , but they require human interaction and are not suitable for web - scale archiving .",
    "@xcite handles deferred representations well , but is a page - at - a - time archival tool and strips out embedded javascript from the memento .",
    "stripping the embedded javascript leads to potentially reduced functionality in the memento and an inability to perform a post - mortem analysis of a page s intended behavior using the memento .",
    "we proposed a two - tiered crawling approach for archiving deferred representations at web - scale that uses heritrix and phantomjs @xcite .",
    "we measured the performance impact of incorporating a headless browsing utility in an archival workflow .",
    "our work demonstrates that phantomjs @xcite and its headless browsing approach can be used in conjunction with heritrix to grow heritrix s crawl frontier by 1.75 times and better archive deferred representations , but crawls 12.15 times slower than heritrix alone .",
    "we build on this effort by enhancing the phantomjs branch of the archival workflow to learn and execute interactions on the client with the hypercube model .",
    "note that phantomjs can not be used for all crawl targets because of the unacceptably slow crawl speed as compared to heritrix .",
    "we use a classifier to identify which representations are deferred and require phantomjs for complete archiving .",
    "several efforts have studied client - side state .",
    "mesbah et al .",
    "performed several experiments regarding crawling and indexing representations of web pages that rely on javascript @xcite focusing mainly on search engine indexing and automatic testing @xcite .",
    "singer et al . developed a method for predicting how users interact with pages to navigate within and between web resources @xcite .",
    "rosenthal spoke about the difficulty of archiving representations reliant on javascript @xcite .",
    "rosenthal et al .",
    "extended their lockss work to include a mechanism for handling ajax @xcite . using crawljax and selenium to click on dom elements with onclick events attached and",
    "monitor the http traffic , they capture the ajax - specific resources . while rosenthal et al .",
    "measure performance based on the audits and repairs required , we focus on wall - clock time and frontier sizes to measure performance .",
    "further , we omit form - filling , a feature that the lockss enhancements provide .",
    "we extend this work to all interactions and study the depth of the interaction trees and best policies for crawling deferred representations .",
    "these prior works investigate the archiving and crawling challenges that client - side technologies like javascript have introduced . in this work",
    ", we build on these past investigations to understand the multiple states that can be discovered on the client by mapping interactions and the additional resources required to build the descendants .",
    "dincturk et al .",
    "@xcite present a model for crawling rias by constructing a graph of descendants .",
    "a ria is a resource with descendants and potentially a deferred representation .",
    "the work by dincturk et al . focuses on ajax requests for additional resources initiated by client - side events which leads to deferred representations with descendants .",
    "their work , which serves as the mathematical foundation for our work , identifies the challenges with crawling ajax - based representations and uses a _ hypercube strategy _ to efficiently identify and navigate between all client - side states of a deferred representation .",
    "their model defines a client - side state as a state reachable from a url through client - side events and is uniquely identified by the state s dom .",
    "that is , two states are identified as equivalent if their dom ( e.g. , html ) is directly equivalent .",
    "the hypercube model is defined by the finite state machine ( fsm ) +   + @xmath2 +   + and defined further in equation [ statedef ] , where    * @xmath3 is the finite set of client states * @xmath4 is the initial state reached by dereferencing the uri - r and executing the initial on - load events * @xmath5 defines the client - side event @xmath6 as a member of the set of all events @xmath7 * @xmath8 is the transition function in which a client - side event is executed and leads to a new state    @xmath9    dincturk et al . define a graph @xmath10 in which @xmath11 is the set of vertices @xmath12 where @xmath13 represents an `` ajax state '' @xmath14 .",
    "edges represent the transitions , or events , @xmath6 such that @xmath15 iff @xmath16 .",
    "a path @xmath17 is a series of adjacent edges that constitute a series of transitions from @xmath18 to @xmath14 via @xmath19 . in effect",
    ", @xmath17 is a series of descendants derived from @xmath18 with one descendant at each level of the tree .",
    "we adopt the fsm presented by dincturk et al . nearly in its entirety . because our application of this fsm is web archiving , our goal is to identify all of the embedded resources required by the representation to build any descendant as a result of user interactions or client - side events , archive them , and be able to replay them when a user interacts with the memento .",
    "the representation returned by simply dereferencing a uri - r is defined as @xmath20-@xmath21 .",
    "subsequent descendants @xmath20-@xmath22 and @xmath20-@xmath23 are derived from @xmath20-@xmath21 through a series of events @xmath24 .",
    "we define a descendant @xmath20-@xmath22 as a client - side state originating at @xmath20-@xmath21 as transitioning via events @xmath6 such that @xmath25 .",
    "additionally , we define our paths through @xmath26 as the set of embedded resources required to move from @xmath18 to @xmath14 .",
    "we present a generic interaction tree of descendants in figure [ diagram ] .",
    "when we dereference a uri - r , we get a representation from the server ; this is @xmath18 .",
    "if there are two interactions available from @xmath18 , we can execute the interactions to get to @xmath27 or @xmath28 from our root @xmath18 ( note that the onclick event required an external image to be retrieved ) . in this example , @xmath27 and @xmath28 are descendants of @xmath18 and are both @xmath0 in @xmath17 from @xmath18",
    ". if new interactions are available from @xmath27 , we can reach @xmath29 and @xmath30 , which are both @xmath1 in @xmath17 from @xmath18 ( similarly , we can reach @xmath31 and @xmath32 from @xmath28 , peers of @xmath29 and @xmath30 ) .        because of the differences between our model and the hypercube model ( section [ paths ] ) , our focus on web archiving , and to ensure we have omniscient knowledge of all interactions , state dependencies , equivalences , and available interactions , we organize the states as a tree rather than a hypercube .",
    "because new interactions lead to states @xmath33 deeper in the tree , we generalize the levels of the trees as @xmath33 and refer to new states by their vertices @xmath34 .",
    "due to the archival focus of this study , we have a different concept of state equivalence than the hypercube model .",
    "while dincturk establishes state equivalence based on the dom ( using strict equivalence based on string comparison ) , we consider the embedded resources required to construct a descendant .",
    "we consider states to be equivalent if they rely on the same embedded resources . as such",
    ", we define the set of embedded resources for a descendant @xmath33 as @xmath35 .",
    "any two states with identical unordered sets of embedded resources are defined as equivalent , effectively a bijection between the two states .",
    "@xmath17 between @xmath18 to @xmath14 is isomorphic if , over the course of @xmath17 , the embedded resources required are identical , and each state within @xmath17 are bijections .",
    "note that in figure [ diagram ] , @xmath27 , @xmath29 , and @xmath30 are equivalent because they require the same set of embedded resources , even if @xmath29 and @xmath30 are reached through an additional @xmath36 and @xmath37 from @xmath27 .",
    "two paths are identical if , over the course of each @xmath33 @xmath38 @xmath17 , the cumulative set of embedded resources required to render each descendant is identical .",
    "we define the set of embedded resources over the entire path as @xmath39 in equation [ resourcepaths ] .",
    "@xmath40    we present our process for traversing paths in algorithm [ algo1 ] .",
    "we traverse all states within the interaction tree to understand what embedded resources are required by each state .",
    "if a state @xmath41 requires a new embedded resource that has not yet been added to the crawl frontier , it is added as part of path @xmath17 . from @xmath39 , we identify the archival coverage ( using memento ",
    "line 10 ) .",
    "we also identify the duplicate uri - rs by canonicalizing , trimming fragment identifiers from the uri - r , and using string comparisons to determine equality .",
    "identify @xmath39 find uri - ms of @xmath39 to determine coverage de - duplicate @xmath39 to determine overlap    as an example , we present the state tree of the top level page at bloomberg.com in figure [ realstate ] . at @xmath18 , the page has a menu at the top of the page with a mouseover event listener ( figure [ realstatel ] ) .",
    "mousing over the labels initiates ajax requests for json data , and the data is used to populate a sub - menu ( @xmath0 ) .",
    "the sub - menu has another mouseover menu that requests images and other json data to display new information , such as stock market data and movie reviews ( @xmath1 ) .",
    "note that @xmath0 and @xmath1 are very broad given the number of menu items .",
    "this is an example of @xmath17 through two levels of mouseover interactions that leads to new json and image embedded resources .",
    "the page also has onclick events ( figure [ realstater ] ) .",
    "these onclick events also lead to descendants at @xmath0 , but not @xmath1 .",
    "however , the onclick events lead to equivalent descendants that we identify as equivalent .",
    "finally , note that the comments section has a form that users can fill out to enter a comment .",
    "this is beyond the scope of this work ; this type of action changes the representation itself and can lead to infinite states , changing the nature of our investigation from identifying archival targets to altering the live web record .",
    "to measure descendants , we needed to construct a tool that can crawl , uncover , and understand descendants and deferred representations .",
    "we have previously shown that phantomjs is an effective utility for crawling deferred representations @xcite .",
    "we constructed a phantomjs - based utility that dereferences a uri - r , identifies the interactive portions of the dom ( i.e. , the dom elements with event listeners ) , and constructs a tree of descendants , reached via initiating interactions and client - side events ( just as in the hypercube model ) .",
    "phantomjs records the set of embedded resources requested by the client ; in a production system , this would be the set of resources added to the heritrix crawl frontier .    because phantomjs is closely tied to the dom and client s javascript engine , race conditions and other event listener assignments prevents phantomjs from understanding the entirety of events available on a representation .",
    "as such , we leveraged visualevent , a bookmarklet that is designed to visually display the dom elements that have event listeners and javascript functions attached @xcite , to understand which events and interactions can be executed on the client @xcite .",
    "our phantomjs tool uses the list of events identified by visualevent to construct a set of interactions @xmath42 that may lead to descendants .",
    "phantomjs performs an exhaustive depth - first traversal of all possible combinations of interactions .",
    "post - mortem , we perform state equivalence and identify the number of unique paths @xmath17 , states @xmath33 , and embedded resources @xmath39 that a crawler would have to visit in order to comprehensively archive the resources needed to provide full functionality in a memento .",
    "we use the same 440 uri - r dataset from our prior investigation of crawling deferred representations @xcite .",
    "we generated uri - rs by randomly generating bitly strings and identifying their redirection targets .",
    "we used phantomjs to identify each uri - r as having a deferred or nondeferred representation and identify the number and type of descendants and interactions available on the representations of uri - rs in this set , along with the descendants within the interaction tree and embedded resources required to build the descendant representations .",
    "note that a @xmath34 is reached by a series of interactions @xmath19 .",
    "we consider a descendant that is a candidate to add to the tree identical to another descendant within the tree if the set of interactions to reach the descendant are identical .",
    "if we encounter a potential descendant that is reachable by the same interactions as another descendant within the tree , we do not add the descendant to the tree because the descendant already exists within the tree ) . ] .",
    "we present our algorithm for crawling the descendants in algorithm [ algo2 ] .",
    "we begin by using phantomjs to dereference a uri - r ( line 4 ) at @xmath18 , and use visualevent to extract the interactive elements ( line 5 ) .",
    "we identify all possible combinations of interactions and use them as an interaction frontier ( line 7 ) , and iterate through the interaction frontier to crawl @xmath0 . from @xmath0 , we extract all possible interactions available and add them to the interaction frontier .",
    "we iterate through the interaction frontier until we have exhausted all possible combinations of interactions at each @xmath33 . at the end of each @xmath33 construction , we run state deduplication ( line 14 ) .",
    "we deem two interaction scripts as equivalent if they perform identical actions in identical order : +   + @xmath43 = @xmath44",
    "the approach that we identify in section [ approach ] is suitable for most of the deferred representations that a web user may encounter while browsing . however , deferred representations with certain conditions are not handled by our approach .",
    "some representations use a div overlayed on the entire window area and identify interactions and events according to the pixel the user clicks .",
    "this creates an interaction frontier of @xmath45 or 2,073,600 ! for a screen size of @xmath46 pixels . due to this massive frontier size",
    ", we omit such interactions .",
    "mapping ( e.g. , google maps ) and similar applications that might have a near - infinite descendants are outside the scope of this work .    for these style of deferred representations ,",
    "a _ canned _ set of interactions ( e.g. , pan once , zoom twice , right click , pan again ) would be more useful @xcite",
    ". with enough of these canned interactions , a sizable portion of the descendants can be identified by a crawler over time , with coverage scaling with the number of executions performed .",
    "this is the archival equivalent of the halting problem  it is difficult to recognize when the crawler has captured _ enough _ of the embedded resources , when it should stop , or when it has captured everything .",
    "during our crawl of the 440 uri - rs , we classified each as having a deferred or nondeferred representation .",
    "as previously discussed , uri - rs with deferred representations will have an event that causes the javascript and ajax in the representation to request additional resources .",
    "we dereferenced each of our 440 uri - rs and identified 137 uri - rs with nondeferred representations and 303 uri - rs with deferred representations .",
    ".the average distribution of descendants within the deferred representation uri - r set . [ cols= \" <",
    ", > , > , > , > \" , ]     we present a summary of the storage requirements for descendants ( using _ max coverage _ ) in table [ storages ] . if we write out the json describing the states , transitions , rendered content , and other information , it would add , on average , 16.45 kb per descendant or memento . with 8,691 descendants",
    ", a total of 143 mb of storage space for the warcs will be required just for the metadata , along with the storage space for the representations of the 54,378 new embedded resources .",
    "the embedded resources discovered in our crawl average 2.5 kb in size .",
    "the embedded resources at @xmath18 were 2.6 kb on average , and the newly discovered embedded resources , as a result of deferred representations , were 2.4 kb in size on average .",
    "we estimate that nondeferred representations , which have 31.02 embedded resources on average , would require 80.7 kb per uri - r , or 11.1 mb of storage for the 137 uri - rs in the collection .",
    "the storage requirement increases to 13.4 mb with the additional metadata .",
    "deferred representations have 25.4 embedded resources at @xmath18 , or 70.0 kb per uri - r . for the 303 uri - rs in the collection , @xmath18 would require 21.2 mb of storage . in @xmath0 , the crawl discovered 45,015 embedded resources which requires 108.0 mb of additional storage , and 9,363 embedded resources at @xmath1 , or an additional 22.5 mb of storage . in all , the 303 deferred representations require 156.7 mb of storage for the entire collection and all crawl levels ( 8,691 descendants ) .",
    "this is 11.3 times more storage than is required for the nondeferred representations , or 5.12 times more storage per uri - r crawled .",
    "if we consider the july 2015 common crawl @xcite as representative of what an archive might be able to crawl in one month ( 145 tb of data for 1.81 billion uris ) , an archive would require 597.4 tb of additional storage for descendants ( 29.9 tb of which is additional storage for metadata ) for a total of 742.4 tb to store descendants and metadata for a one - month crawl .",
    "if we assume that the july crawl is a representative monthly sample , an archive would need 8.9 pb of storage for a year - long crawl .",
    "this is an increase of 7.17 pb per year ( including 358.8 tb of storage for metadata ) to store the resources from deferred representations .",
    "alternatively , can also say that an archive will miss 6.81 pb of embedded resources per year because of deferred representations .",
    "p5 cm | p2 cm * storage target * & * size * + json metadata per descendant / memento & 16.5 kb + json metadata of all descendants & 143 mb +   + average embedded resource & 2.5 kb + embedded resources per uri & 80.7 + total embedded resource storage & 11.1 mb + total json metadata storage & 2.3 mb + total with json metadata & 13.4 mb +   + average embedded resource & 2.6 kb + embedded resources per uri & 70.0 + embedded resource storage @xmath18 & 21.2 mb + embedded resource storage @xmath0 & 108.0 mb + embedded resource storage @xmath1 & 22.5 mb + total json metadata storage & 5.0 mb + total with json metadata & 156.7 mb +",
    "in this paper , we present a model for crawling deferred representations by identifying interactive portions of pages and discovering descendants . we adapt prior work by dincturk et al . and present a fsm to describe descendants and propose a warc storage model for the descendants .",
    "we show that , despite high standard deviations in our sample , deferred representations have 38.5 descendants per uri - r , and that deferred representations are surprisingly shallow , only reaching a depth of 2 levels .",
    "this means that deferred representations are shallower than originally anticipated ( but also very broad ) and therefore it is more feasible to completely archive deferred representations using automated methods than previously thought .",
    "archives that do not execute javascript during archiving are incomplete ; 69% of uris have descendants and 96% of the embedded resources in those descendants are not archived .    crawling all descendants ( which we defined as the _ max coverage _",
    "policy ) is 38.9 times slower than crawling with only heritrix , but adds 15.60 times more uri - rs to the crawl frontier than heritrix alone . using the _ max roi _",
    "policy is 27.04 times slower than heritrix , but adds 13.40 times more uri - rs to the crawl frontier than heritrix alone .",
    "we do not recommend one policy over the other since the policy selection depends on the archival goals of coverage or speed .",
    "however , both will help increase the ability of the crawlers to archive deferred representations .",
    "most of @xmath47 ( newly discovered by traversing the paths ) are unarchived ( 92% unarchived at @xmath0 and 96% at @xmath1 ) .",
    "however , 22.4% of the newly discovered uri - rs match one of the top 10 occurring uri - rs , indicating a high amount of overlap within @xmath39 ; mostly , these are ad servers and data - services like google analytics .",
    "we will work to incorporate phantomjs into a web - scale crawler to measure the actual benefits and increased archival coverage realized when crawling deferred representations .",
    "because a large portion of the embedded resources we discovered originated at data services ( e.g. , google analytics , table [ counts ] ) , we will investigate the importance ( using our algorithm for memento damage @xcite ) of the new embedded resources in @xmath0 and @xmath1 . we will also work to develop an approach to solve our current edge cases ( section [ edge ] ) , including a way to handle applications like mapping applications using our automated approach along with an approach using `` canned interactions '' .",
    "our goal is to understand how many executions of canned interactions are necessary to uncover an acceptable threshold of embedded resources ( e.g. , how many pans and zooms are needed to get all google maps tiles for all of norfolk , va , usa ? ) .",
    "we will also investigate filling out forms similar to rosenthal et al .",
    "@xcite .",
    "our work presented in this paper establishes an understanding of how much web archives are missing by not accurately crawling deferred representations and presents a process for better archiving descendants .",
    "we demonstrate that archiving deferred representation is a less daunting task with regards to crawl time than previously thought , with fewer levels of interactions required to discover all descendants .",
    "the increased frontier size and associated metadata will introduce storage challenges with deferred representations requiring 5.12 times more storage .",
    "this work was supported in part by nsf 1526700 and neh hk-50181 .",
    "a.  mesbah and a.  van deursen . migrating multi - page web applications to single - page ajax interfaces . in",
    "_ proceedings of the 11th european conference on software maintenance and reengineering _ , pages 181190 , 2007 ."
  ],
  "abstract_text": [
    "<S> the web is today s primary publication medium , making web archiving an important activity for historical and analytical purposes . </S>",
    "<S> web pages are increasingly interactive , resulting in pages that are increasingly difficult to archive . </S>",
    "<S> client - side technologies ( e.g. , javascript ) enable interactions that can potentially change the client - side state of a representation . </S>",
    "<S> we refer to representations that load embedded resources via javascript as _ deferred representations_. it is difficult to archive all of the resources in deferred representations and the result is archives with web pages that are either incomplete or that erroneously load embedded resources from the live web .    we propose a method of discovering and crawling deferred representations and their _ descendants _ ( representation states that are only reachable through client - side events ) . </S>",
    "<S> we adapt the dincturk et al . </S>",
    "<S> hypercube model to construct a model for archiving descendants , and we measure the number of descendants and requisite embedded resources discovered in a proof - of - concept crawl . </S>",
    "<S> our approach identified an average of 38.5 descendants per seed uri crawled , 70.9% of which are reached through an onclick event . </S>",
    "<S> this approach also added 15.6 times more embedded resources than heritrix to the crawl frontier , but at a rate that was 38.9 times slower than simply using heritrix . </S>",
    "<S> we show that our dataset has two levels of descendants . </S>",
    "<S> we conclude with proposed crawl policies and an analysis of the storage requirements for archiving descendants . </S>"
  ]
}