{
  "article_text": [
    "deep learning networks ( dln ) inspired from the hierarchical organization of neurons and synapses in human brain are an important class of machine learning algorithms and have redefined the state - of - the - art for many cognitive applications @xcite .",
    "however , dlns involve data - intensive computations that lead to high power and memory bandwidth requirements on von - neumann machines . as a result ,",
    "the power budget they thrive on is multiple orders of magnitude greater than the human brain .",
    "for instance , alexnet @xcite that won the imagenet challenge in 2012 consisted of 650k neurons and 60 m synapses , and thrives on 2 - 4 gops of compute power per classification .",
    "such power and memory bottlenecks have inspired the research in neuromorphic computing to build efficient architectures for accelerating neural networks by overcoming the von - neumann bottlenecks .",
    "to this effect , several works have shown dln implementations using graphic processing units , multi - core processors and hardware accelerators @xcite .",
    "while dlns are being successfully used in many recognition applications , there is a growing shift in the research community towards a more biologically plausible and energy - efficient computing paradigm , spiking neural networks ( snn ) @xcite . driven by brain - like spike based computations , snn involves event - driven data processing making them the emerging choice for energy - efficient recognition applications .",
    "additionally , recent researches have shown deep snns to exhibit high accuracy on various complex recognition tasks @xcite . however , cmos implementations of neuromorphic systems to accelerate snns suffer from power and area inefficiencies that stem from the realization of neuron and synapse functionality using primitives namely instructions and boolean logic , resulting in dozens of transistors to mimic a single neuron / synapse @xcite .    the limitations of cmos can be addressed with emerging technologies , such as memristive devices that realize synaptic functionalities with very high efficacy @xcite .",
    "crossbars made up of these devices at the cross - points have been studied for energy - efficient inner - product engines @xcite .",
    "this has furthered the efforts to realize in - memory processing based architectures using memristive crossbar arrays ( mca ) for neuromorphic applications .",
    "however , mca size is a strong function of technology , for example , phase change memories ( pcm ) @xcite , ag - si @xcite , spintronic devices @xcite etc .",
    "large crossbars allow more flexibility in directly mapping an snn onto it .",
    "this can also reduce peripheral overheads and thereby improve overall energy consumption .",
    "however , large crossbars are infeasible as they suffer from non - idealities like sneak - paths , process variations and parasitic voltage drops @xcite which lead to erroneous computations .",
    "this necessitates the design of reconfigurable platforms for snns that can utilize the mca energy benefits as well as address the limitations posed by mca size .    in this work ,",
    "we introduce resparc - a novel reconfigurable neuromorphic architecture built on mcas for efficient implementation of snn applications .",
    "the post - cmos technology based mcas provide efficient realization of synapses @xcite .",
    "additionally , crossbars store the network weights thereby enabling `` in - memory processing '' .",
    "this circumvents the problems associated with frequent and large volumes of data transfer between cpu and memory for implementing dlns on conventional computing systems @xcite .",
    "we translate the event - driven nature of snn computations to architectural techniques ( discussed in section 3.2 ) in order to achieve higher energy - efficiency .",
    "hence , resparc aims at synergically combining the benefits of snn and the design space of emerging technology using mcas .",
    "organizationally , resparc is a three - tiered reconfigurable platform designed to incorporate the data - flow patterns in any neural network in a scalable fashion . each tier is targeted to bring in a specific variety of reconfigurability with respect to the snn morphology .",
    "the three tiers are namely :    1 .   *",
    "macro processing engine - reconfigurable compute unit * to map neurons with variable fan - in",
    "* neurocell - reconfigurable datapath * to map snns with varying inter and intra layer connectivities namely multi - layer perceptrons ( mlps ) and convolutional neural networks ( cnns ) .",
    "* resparc - reconfigurable core * to map snns with varying size ( number of layers ) .",
    "resparc is a spatially scalable architecture .",
    "it enumerates the synapses across mcas on different mpes , with mpes spread across different neurocells thereby , using more mcas for mapping a larger spiking neural network .",
    "additionally , resparc s reconfigurability enables the usage of variable mca sizes for mapping a given snn topology .",
    "hence , for any given mca technology , a size which is permissible by the technology constraints for proper operation can be chosen thereby , enabling `` technology - aware '' mapping of snns on resparc .",
    "prior work on mcas for snn implementations have primarily focused on device and circuit optimizations and do not involve architecture - level analysis @xcite .",
    "the benefits at device - level need to be preserved at system - level .",
    "our work proposes a full - fledged mca based reconfigurable neuromorphic architecture that can implement a wide variety of snns with varying complexity and topology , as required by an application .",
    "it also helps to perform system - level analysis of mcas , as mcas are not a drop - in replacement for existing computation cores in the cmos snn implementations .",
    "post - cmos based architectures were also explored in @xcite . while these works propose architectures for artificial neural networks , resparc targets snn and utilizes its event - drivenness for added energy benefits .",
    "additionally , resparc is distinct micro - architecturally as it explores a spatially scalable design based on reconfigurable hierarchies .",
    "moreover , our design obviates the use of energy hungry analog - digital conversions unlike @xcite thereby leading to energy reductions .",
    "there has also been prior work on snn acceleration using cmos technologies .",
    "for instance , akopyan et al .",
    "@xcite proposed truenorth which uses low power 28 nm cmos technology and asynchronous circuit designs . while our work is complementary to the effects of @xcite , we explore post - cmos technology for snn acceleration .",
    "moreover , to analyze the benefits of resparc with respect to cmos accelerators , we implement an optimized cmos based baseline . additionally , other techniques such as asynchronous computation will complement the snn acceleration on resparc .    in summary",
    "the key contributions of this work are :    1 .",
    "* an efficient memristive crossbar based architecture for spiking neural networks * is designed to harness the energy - efficiency from in - memory processing and event - driven computation .",
    "* different spiking network topologies ( mlp , cnn ) from different recognition applications * namely digit recognition , house number recognition and object classification are mapped onto resparc and analyzed for performance and energy benefits with respect to their digital cmos implementations .",
    "* different mca sizes for different snn topologies based on the limitations posed by the memristive technology * are explored to determine the optimum crossbar size for mapping a given network .",
    "snn is regarded as the third generation neural network .",
    "snns require the input to be encoded as spike trains and involve spike - based ( 0/1 ) information transfer between neurons . at a particular instant ,",
    "each spike is propagated through the layers of the network while the neurons accumulate the spikes over time causing the neuron to fire or spike .",
    "the deep snn topologies used in this work are mlps and cnns .",
    "an mlp , shown in fig .",
    "1(a ) , is a multi - layered snn in which all neurons in a layer are connected to all neurons in the previous layer .",
    "a deep cnn , shown in fig .",
    "1(b ) , is also a multi - layered snn composed of alternating convolution and sub - sampling layers . as shown in fig .",
    "1(c ) , a typical spiking neuron does an accumulation operation followed by thresholding operation .",
    "the spiking neuron model used in this work is the integrate - and - fire ( if ) model .",
    "note that , our work focuses on the testing / computation of the snn and assumes that resparc has been trained offline using supervised training algorithms @xcite .",
    "2(a ) shows a 2-layer fully connected snn .",
    "2(b ) shows the connectivity structure / matrix ( from fig .",
    "2(a ) ) map - ped onto an mca .",
    "the memristive devices at its cross - points encode the synaptic weights of the snn .",
    "an mca receives voltage inputs at its rows and the resulting current output at any column is the weighted summation of the encoded weights at that column and the input voltage .",
    "this is a direct consequence of the kirchhoff s law as the current output into a column from any cross - point will be the product of the conductance at that cross - point and the voltage across it .",
    "thus , mca is an analog `` inner - product '' computation unit .",
    "the mca outputs are interfaced with neurons .",
    "the neurons receive the input current that results in its membrane potential accumulating over time .",
    "when the membrane potential reaches a threshold , the neuron spikes ( `` 1 '' ) thus mirroring the function of an if neuron .",
    "resparc ( shown in fig .",
    "3 ) is the reconfigurable core and the topmost level among the three reconfigurable hierarchies . as shown in fig .",
    "3 , resparc is composed of pool of neurocells which are the second level in the reconfigurable hierarchy .",
    "4 shows a macro processing engine ( denoted as mpe in fig .",
    "3 ) which is the lowest level in the hierarchy .",
    "next , we will discuss the organization and the logical dataflow in each hierarchy starting from the lowest level and moving towards the higher levels .",
    "resparc is the scalable extension of a neurocell ( nc ) and enables mapping of an snn ( that exceeds an nc s size ) across multiple ncs .",
    "the ncs share a global `` io_bus '' that connects to an sram ( input memory ) .",
    "thus , data transfer between different ncs go through the sram .",
    "each nc in the nc - array is associated with a `` tag ( x , y ) '' which facilitates input broadcast from the sram to a variable number of ncs ( that map to a given layer ) within a single cycle . to monitor the completion of an nc s computation , the global control unit consists of an event - flag , dedicated to every nc which gets set when the nc completes .",
    "7 illustrates the logical dataflow involved across hierarchies for snn computation . within an nc",
    ", parallel data transfer occurs between layers of the snn through the switch network .",
    "data transfer occurs serially through the shared bus between layers mapped across multiple ncs to compute the final output .",
    "we leverage the energy - efficiency of mca ( weight storage and inner - product computation ) for energy savings . additionally ,",
    "as mentioned in section 3.1 , the reconfigurability in mpe enables optimized mapping that reduces the peripheral energy per mca thereby resulting in overall energy reductions . within a neurocell , event - drivenness in snn computations",
    "is utilized by adding `` zero - check logic '' in each programmable switch to prevent data transfers resulting from insignificant spike - packets ( for instance , all bits in the spike packet being zero ) .",
    "additionally , at the topmost level in the hierarchy , resparc exploits snn data statistics ( event - drivenness ) to prevent unnecessary broadcasts to neurocells by checking the data read from sram with a `` zero - check logic '' .",
    "thus , the reconfigurability and event - driven computation further complement the benefits observed with mcas for energy - efficient snn acceleration on resparc .",
    "a macro processing engine ( mpe ) is composed of multiple mcas tied together to the local control unit . the mpe shown in fig .",
    "4 includes four mcas , each of which is associated to its neurons and a set of buffers namely ( 1 ) input buffer ( ibuff ) , ( 2 ) output buffer ( obuff ) and ( 3 ) target buffer ( tbuff ) .",
    "the ibuff buffers the input spike packets received until the required data needed by the mca is available .",
    "similarly , the obuff buffers the output spike packets computed by the neuron until the required data to be sent to a target neuron is available .",
    "the tbuff stores the address of the target neuron(s ) .",
    "although , we consider if neurons in this work , any spiking neuron can be interfaced with the mca .",
    "the mcas contain the synapses corresponding to the neurons being computed in an mpe .",
    "this is realized by mapping the connectivity matrix on the mcas as shown in fig .",
    "2 . however , for memristive technology , mca sizes which ensure reliable operation are much smaller for instance 64 rows and 64 columns ( 64 @xmath064 ) in comparison to a typical neural network s fan - in that is of the order of several hundreds @xcite .",
    "this necessitates partitioning the connectivity matrix to map it across multiples mcas .",
    "subsequently , the neuron output is computed by time - multiplexing the mca outputs onto the neuron as shown in fig .",
    "5 . an mpe can be configured to support time - multiplexed computation of multiple degrees to map neurons with variable fan - in . in case a neuron s fan - in",
    "exceeds the fan - in support an mpe provides locally , the connectivity matrix is mapped across multiple mpes .    for sparser connectivity matrices , which is typical of cnns , different output neurons have different inputs along with some input sharing .",
    "hence , a column ( column maps to an output neuron ) in an mca will consist of synapses at only certain sparse locations ( rows ) that correspond to its inputs leading to incompletely utilized mca .",
    "further , mapping the connectivity matrix of a cnn directly to a large mca results in higher non - utilization due to large number of unused cross - points ( synapses ) .",
    "however , enumerating the connectivity matrix across multiple smaller mcas facilitates enhanced input - sharing that improves mca utilization .",
    "consequently , this reduces the number of mpes required for the mapping .",
    "this improves overall energy consumption by reducing the peripheral energy per mca .",
    "hence , mpe s reconfigurability enables optimized mca utilization for sparse connectivity .",
    "as shown in fig .",
    "3 , a neurocell is composed of multiple mpes and programmable switches .",
    "the switch network enables spike - packet transfers within the neurocell .",
    "a switch connects to its four neighboring mpes .",
    "additionally , each switch has a dedicated connection to the switches in the same row and same column .",
    "this enables low - latency ( one - hop ) spike - packet transfers between the connected mpes .",
    "essentially , a neurocell is a pool of mpes coupled with dense local connections that enables high throughput digital data transfer within it .",
    "each switch can be configured to serve one or multiple mpes it connects to thereby , realizing a reconfigurable datapath within the neurocell .",
    "this enables to optimize the datapath for the given snn s connectivity .",
    "consequently , this reduces the load on each switch and simplifies the overall traffic management within the neurocell .",
    "6 shows the programmable switch design .",
    "each input and output line is associated with data and address buffers to synchronize data transfer between the receiver and target mpe .",
    "further , depending on the switch configuration , it arbitrates between the sender mpes .    as mentioned before",
    ", a connectivity matrix can span across multiple mpes . to compute the neuron output , mca current(s ) from one mpe",
    "is transmitted to another mpe ( consisting the neuron ) followed by their time - multiplexed integration .",
    "such analog signal transmission is facilitated by gated wires connecting the neighboring mpes ( dashed lines in fig .",
    "we implemented the dataflow proposed in @xcite for our cmos baseline and aggressively optimized it for snns .",
    "we augmented the implementation with event - driven optimizations to prevent unnecessary memory fetches and computations .",
    "additionally , we added buffers to optimize the temporal and spatial data reuse patterns to minimize the memory fetches and thereby , optimizing the overall energy consumption .",
    "note that our cmos baseline enables to decouple the circuit and network - on - chip driven optimizations in other cmos based snn accelerators in order to rigorously analyze the mca centric memory and computation benefits in resparc .",
    "resparc is composed of different technologies namely crossbar technology , technology of the interfaced neurons and the cmos peripherals . for the memristive devices , we used a resistance range of `` 20k@xmath1  200k@xmath1 '' with 16 levels ( 4 bits ) for weight - discretization , that is typical of memristive technologies such as pcm , ag - si @xcite .",
    "we considered an operating voltage of `` vdd/2 '' for the mca as it is interfaced with cmos neurons @xcite .",
    "the peripheral circuit consisting of buffers , communication and control logic was implemented at the register transfer level in verilog hdl and mapped to ibm 45 nm technology using synopsys design compiler .",
    "synopsys power compiler was used to estimate the energy consumption .",
    "the input memory ( sram ) was modelled using cacti @xcite .",
    "8 lists the simulation parameters and the implementation metrics for one neurocell .",
    "please note that the same methodology was also used to estimate the energy consumption of our cmos baseline .",
    "9 shows the simulation parameters and implementation metrics for the baseline .",
    "our benchmark comprises of six snn designs from different recognition applications namely , house number recognition ( svhn dataset @xcite ) , digit recognition ( mnist dataset@xcite ) and object classification ( cifar-10 dataset @xcite ) .",
    "we use one mlp and one cnn from each application .",
    "the snns were trained using supervised learning algorithm proposed in @xcite .",
    "10 shows the benchmark details . as mentioned before",
    ", we do not consider the training phase of the snn and hence , do not consider the energy expended in programming the mcas . also , in typical use case of recognition applications , the training process is performed once or very infrequently .",
    "on the other hand , the testing or evaluation phase , in which the actual classification is performed using snns , extends for much longer periods of time .",
    "hence , we evaluate resparc for the more critical testing phase .",
    "in this section , we present the results of various experiments that demonstrate the benefits of resparc and underscore the effectiveness of the proposed architecture in exploring the design space of post - cmos based mcas for snn applications .",
    "11 compares the energy savings and performance spee - dups obtained _ per classification _ for resparc over the cmos baseline for various snn applications with cnn and mlp topologies .",
    "the energy consumptions are normalized to the energy consumption of mnist on resparc and the performance speedups are normalized to cifar-10 on cmos baseline .",
    "the mca size used is 64 i.e. , 64 rows and 64 columns . as shown in figs .",
    "11 ( a ) and ( c ) , resparc provides significant energy benefits between 10@xmath0  15@xmath0 ( 12@xmath0 on average ) at a performance speedup of 33@xmath0  95@xmath0 ( 60@xmath0 on average ) for the cnn benchmarks . for mlps , ( shown in figs .",
    "11(b ) and ( d ) ) energy benefits on resparc increase to 331@xmath0  549@xmath0 ( 513@xmath0 on average ) at a performance speedup of 360@xmath0  415@xmath0 ( 382@xmath0 on average ) .",
    "hence , resparc efficiently accelerates both cnn and mlp based snn applications .    the lower efficiency ( both energy and speedup ) for cnns stems from the incomplete utilization of mcas in resparc as discussed in subsection 3.1.1 .",
    "the incomplete utilization leads to higher peripheral energy consumption per mca there - by , decreasing the overall energy improvement .",
    "additionally , incompletely utilized mcas lead to lesser gain in performance speedup as lesser number of mca outputs ( columns ) are utilized .",
    "in contrast , mlps have fully utilized mcas that result in higher throughput ( number of outputs computed per unit time ) as all the columns of the mca are being used for output computation .",
    "the graphs in fig .",
    "12 show the breakdown of energy from fig .",
    "11 into 3 key components for resparc : ( i ) neuron ( ii ) crossbar ( iii ) peripherals and 3 key components for the cmos baseline : ( i ) core ( ii ) memory access ( iii ) memory leakage .",
    "we present the energy distribution for mlp and cnn benchmarks on different mca ( crossbar ) sizes namely ( i ) resparc-128 ( ii ) resparc-64 ( iii ) resparc-32 .",
    "12 ( a ) shows resparc energy consumption for mlps .",
    "the energy consumption decreases with increasing mca size .",
    "this is due to the fact that for larger mcas the synapses would be mapped across less number of mpes that decreases the peripheral energy per mca reducing the overall energy consumption . on the other hand , for cnns ( shown in fig .",
    "12(c ) ) , resparc-64 is the most energy - efficient .",
    "we observe a decrease in energy from resparc-32 to resparc-64 with cnns due to decrease in peripheral energy .",
    "however , increasing mca size from 64 to 128 increases the mca non - utilization ( due to sparser connectivity in cnns as discussed in section 3.1.1 ) that dominates the overall energy consumption .",
    "hence , unlike mlps , an increase in mca size from 64 to 128 does not result in a corresponding decrease in the peripheral energy per mca as the number of mpes being used does not decrease commensurately .    as shown in fig .",
    "12 ( b ) , the energy consumption in mlps on the cmos baseline is dominated by the memory component ( access and leakage ) .",
    "this implies that the energy savings for mlps on resparc results from efficient memory storage ( weight storage in mcas ) . on the other hand , fig .",
    "12 ( d ) shows that the computation core ( which includes the buffers and the computation units ) dominates the energy consumption in cnns .",
    "this suggests that the energy efficiency for cnns on resparc results from the efficient inner - product computation in the mcas .",
    "the graphs in fig .",
    "13 show the energy savings for mnist dataset on resparc due to snn s event - driven processing nature .",
    "the energy benefits are highest on resparc with the smallest mca size .",
    "this is a consequence of the fact that the probability of finding zeros with smaller run - lengths ( zeros with run length of 32 refers to a 32-bit spike - packet with all bits being zero ) is significantly higher than that with larger run - lengths .",
    "we also obtained similar energy improvements with event - driven optimizations on the other two datasets .",
    "as discussed before , smaller mcas are preferred because of reliability but they suffer from increased peripheral energy consumption .",
    "however , resparc with its event - drivenness enables using mcas of smaller sizes for efficient acceleration of snns",
    ".    the benefits observed with cnns are lesser than mlps .",
    "this is due to the fact that cnns process two - dimensional spatial windows of the input image that typically comprises of foreground ( white ) pixels .",
    "in contrast , mlps process one - dimensional vectors that can easily find zero run - lengths for background ( black ) pixels .      here",
    ", we analyze the memristor bit - precision on accuracy and energy consumption of resparc and cmos baseline . as illustrated in fig .",
    "14 ( a ) , the classification accuracy increases continuously with increasing weight precision ( higher bit - discretization ) .",
    "however , the accuracy with 4-bits is comparable to the accuracy with 8 bits .",
    "hence , we used 4-bit weight precision for our energy comparisons between resparc and cmos baseline .",
    "however , other complex applications may necessitate the usage of higher bit - discretization for weight storage .",
    "a noteworthy observation here is that the energy consumption in resparc ( from fig .",
    "14 ( b ) ) is fairly independent of the weight precision .",
    "however , the area of the memristive device will increase with increasing precision that will increase the mca area resulting in an area overhead .",
    "we also observe from fig . 14 ( b ) that the energy consumption of the cmos baseline increases with increasing bit - discretization .",
    "this is due to the fact that a higher precision demands bigger memory , buffers and compute units resulting in an increase in both the core power ( buffer and computation units ) and memory power ( access and leakage ) .",
    "the intrinsic compatibility of post - cmos technologies with biological primitives provides new opportunities to develop efficient neuromorphic systems . in this work we proposed resparc a memristive crossbar based architecture for energy - efficient acceleration of deep spiking neural networks ( snn ) .",
    "we developed a reconfigurable hierarchy that efficiently implements snns of different connectivities given a memristive crossbar size and technology .",
    "additionally , resparc synergically combines the energy benefits of post - cmos technologies and the event - drivenness of bio - inspired snns to address the power and memory bottlenecks in modern computing systems .",
    "our results on a range of recognition applications suggest that resparc is a promising architecture to implement snns providing favorable tradeoffs between energy and crossbar size ."
  ],
  "abstract_text": [
    "<S> neuromorphic computing using post - cmos technologies is gaining immense popularity due to its promising abilities to address the memory and power bottlenecks in von - neumann computing systems . in this paper </S>",
    "<S> , we propose resparc - a reconfigurable and energy efficient architecture built - on memristive crossbar arrays ( mca ) for deep spiking neural networks ( snns ) . </S>",
    "<S> prior works were primarily focused on device and circuit implementations of snns on crossbars . </S>",
    "<S> resparc advances this by proposing a complete system for snn acceleration and its subsequent analysis . </S>",
    "<S> resparc utilizes the energy - efficiency of mcas for inner - product computation and realizes a hierarchical reconfigurable design to incorporate the data - flow patterns in an snn in a scalable fashion . </S>",
    "<S> we evaluate the proposed architecture on different snns ranging in complexity from 2k230k neurons and 1.2m5.5 m synapses . </S>",
    "<S> simulation results on these networks show that compared to the baseline digital cmos architecture , resparc achieves 500@xmath0 ( 15@xmath0 ) efficiency in energy benefits at 300@xmath0 ( 60@xmath0 ) higher throughput for multi - layer perceptrons ( deep convolutional networks ) . furthermore , resparc is a technology - aware architecture that maps a given snn topology to the most optimized mca size for the given crossbar technology .    </S>",
    "<S> < ccs2012 > < concept > < concept_id>10010583.10010786.10010787.10010788</concept_id > < concept_desc > hardware  emerging architectures</concept_desc > < concept_significance>300</concept_significance > < /concept > </S>",
    "<S> < /ccs2012 > </S>"
  ]
}