{
  "article_text": [
    "one of the most challenging problems in modern statistics is to find efficient methods for treating high - dimensional data sets . in various practical situations",
    "the problem of predicting or explaining a scalar response variable @xmath2 by @xmath3 scalar predictors @xmath4 arises . for solving this problem one should first specify an appropriate mathematical model and",
    "then find an algorithm for estimating that model based on the observed data . in the absence of a priori information on the relationship between @xmath2 and @xmath5 ,",
    "complex models are to be preferred .",
    "unfortunately , the accuracy of estimation is in general a decreasing function of the model complexity .",
    "for example , in the regression model with additive noise and two - times continuously differentiable regression function @xmath6 , the most accurate estimators of @xmath7 based on a sample of size @xmath8 have a quadratic risk decreasing as @xmath9 when @xmath8 becomes large .",
    "this rate deteriorates very rapidly with increasing @xmath3 leading to unsatisfactory accuracy of estimation for moderate sample sizes .",
    "this phenomenon is called `` curse of dimensionality '' , the latter term being coined by bellman ( 1961 ) .    to overcome the `` curse of dimensionality '' , additional restrictions on the candidates @xmath7 for describing the relationship between @xmath2 and @xmath10 are necessary .",
    "one popular approach is to consider the multi - index model with @xmath11 indices : for some linearly independent vectors @xmath12 , @xmath13 , @xmath14 and for some function @xmath15 , the relation @xmath16 holds for every @xmath17 . here and in the sequel the vectors are understood as one column matrices and @xmath18 denotes the transpose of the matrix @xmath19 . of course , such a restriction is useful only if @xmath20 and the main argument in favor of using the multi - index model is that for most data sets the underlying structural dimension @xmath11 is substantially smaller than @xmath3",
    ". therefore , if the vectors @xmath12 , @xmath13 , @xmath14 are known , the estimation of @xmath7 reduces to the estimation of @xmath21 , which can be performed much better because of lower dimensionality of the function @xmath21 compared to that of @xmath7 .",
    "another advantage of the multi - index model is that it assesses that only few linear combinations of the predictors may suffice for `` explaining '' the response @xmath2 .",
    "considering these combinations as new predictors leads to a much simpler model ( due to its low dimensionality ) , which can be successfully analyzed by graphical methods , see @xcite , @xcite for more details .",
    "thus , throughout this work we assume that we are given @xmath8 observations @xmath22 from the model @xmath23 where @xmath24 are unobserved errors assumed to be mutually independent zero mean random variables , independent of the design @xmath25 .    since it is unrealistic to assume that @xmath26 are known , estimation of these vectors from the data is of high practical interest . when the function @xmath21 is unspecified , only the linear subspace @xmath27 spanned by these vectors may be identified from the sample .",
    "this subspace is usually called _ index space _ or _ dimension - reduction ( dr ) subspace_. clearly , there are many dr subspaces for a fixed model @xmath7 . even if @xmath7 is observed without error , only the smallest dr subspace , henceforth denoted by @xmath28 , can be consistently identified .",
    "this smallest dr subspace , which is the intersection of all dr subspaces , is called _ effective dimension - reduction _ ( edr ) subspace @xcite or _ central mean subspace _ @xcite .",
    "we adopt in this paper the former term , in order to be consistent with @xcite and @xcite , which are the closest references to our work .",
    "the present work is devoted to studying a new algorithm for estimating the edr subspace.we call it structural adaption via maximum minimization ( samm ) .",
    "it can be regarded as a branch of the structure - adaptive ( sa ) approach introduced in @xcite , @xcite .",
    "note that a closely related problem is the estimation of the central subspace ( cs ) , see @xcite for its definition . for model ( [ model ] ) with i.i.d .",
    "predictors , the cs coincides with the edr subspace .",
    "hence , all the methods developed for estimating the cs can potentially be applied in our set - up .",
    "we refer to @xcite for background on the difference between the cs and the central mean subspace and to @xcite for a discussion of the relationship between different algorithms estimating these subspaces .",
    "there are a number of methods providing an estimator of the edr subspace in our set - up .",
    "these include ordinary least square @xcite , sliced inverse regression @xcite , sliced inverse variance estimation @xcite , principal hessian directions @xcite , graphical regression @xcite , parametric inverse regression @xcite , sa approach @xcite , iterative hessian transformation @xcite , minimum average variance estimation ( mave ) @xcite and minimum discrepancy approach @xcite .",
    "all these methods , except sa approach and mave , are related to the principle of inverse regression ( ir ) .",
    "therefore they inherit its well known limitations .",
    "first , they require a hypothesis on the probabilistic structure of the predictors usually called linearity condition .",
    "second , there is no theoretical justification guaranteeing that these methods estimate the whole edr subspace and not just a part thereof ( cf .",
    "* section 3.1 ) and the comments on the third example in ( * ? ? ?",
    "* section 4 ) ) . in the same time",
    ", they have the advantage of being simple for implementation and for inference .",
    "the two other methods mentioned above  sa approach and mave  have much wider applicability including even time series analysis .",
    "the inference for these methods is more involved than that of ir based methods , but sa approach and mave are recognized to provide more accurate estimates of the edr subspace .",
    "these arguments , combined with the empirical experience , indicate the complementarity of different methods designed to estimate the edr subspace .",
    "it turns out that there is no procedure among those cited above that outperforms all the others in plausible settings .",
    "therefore , a reasonable strategy for estimating the edr subspace is to execute different procedures and to take a decision after comparing the obtained results . in the case of strong contradictions , collecting additional data or using extra - statistical arguments is recommended .",
    "the algorithm samm we introduce here exploits the fact that the gradient @xmath29 of the regression function @xmath7 evaluated at any point @xmath17 belongs to the edr subspace .",
    "the estimation of the gradient being an ill - posed inverse problem , it is better to estimate some linear combinations of @xmath30 , which still belong to the edr subspace .",
    "let @xmath31 be a positive integer .",
    "the main idea leading to the algorithm proposed in @xcite is to iteratively estimate @xmath31 linear combinations @xmath32 of vectors @xmath33 and then to recover the edr subspace from the vectors @xmath34 by running a principal component analysis ( pca ) .",
    "the resulting estimator is proved to be @xmath0-consistent provided that @xmath31 is chosen independently on the sample size @xmath8 .",
    "unfortunately , if @xmath31 is small with respect to @xmath8 , the subspace spanned by the vectors @xmath32 may cover only a part of the edr subspace .",
    "therefore , the empirical experience advocates for large values of @xmath31 , even if the desirable feature of @xmath35-consistency fails in this case .",
    "the estimator proposed in the present work is designed to provide a remedy for this dissension between the theory and the empirical experience .",
    "this goal is achieved by introducing a new method of extracting the edr subspace from the estimators of the vectors @xmath32 . if we think of pca as the solution to a minimization problem involving a sum over @xmath31 terms ( see ( [ pca ] ) in the next section ) then",
    ", to some extent , our proposal is to replace the sum by the maximum .",
    "this motivates the term structural adaptation via maximum minimization .",
    "the main advantage of samm is that it allows us to deal with the case when @xmath31 increases polynomially in @xmath8 and yields an estimator of the edr subspace which is consistent under a very weak identifiability assumption .",
    "in addition , samm provides a @xmath0-consistent estimator ( up to a logarithmic factor ) of the edr subspace when @xmath36 .    if @xmath37 , the corresponding model is referred to as _ single - index _ regression .",
    "there are many methods for estimating the edr subspace in this case ( see @xcite , @xcite and the references therein ) .",
    "note also that the methods for estimating the edr subspace have often their counterparts in the partially linear regression analysis , see for example @xcite and @xcite .",
    "some aspects of the application of dimension reduction techniques in bioinformatics are studied in  @xcite and @xcite .",
    "the rest of the paper is organized as follows .",
    "we review the structure - adaptive approach and introduce the samm procedure in section  [ strad ] .",
    "theoretical features including @xmath35-consistency of the procedure are stated in section  [ theor ] .",
    "section  [ sim ] contains an empirical study of the proposed procedure through monte carlo simulations .",
    "the technical proofs are deferred to the appendix .",
    "introduced in @xcite , the structure - adaptive approach is based on two observations . first , knowing the structural information helps better estimate the model function .",
    "second , improved model estimation contributes to recovering more accurate structural information about the model .",
    "these advocate for the following iterative procedure . start with the null structural information , then iterate the above - mentioned two steps ( estimation of the model and extraction of the structure ) several times improving the quality of model estimation and increasing the accuracy of structural information during the iteration .",
    "when no structural information is available , one can only proceed in a fully nonparametric way .",
    "a proper estimation method is based on local linear smoothing ( cf .",
    "@xcite for more details ) : estimators of the function @xmath7 and its gradient @xmath38 at a point @xmath39 are given by @xmath40 x_{ij } \\end{matrix}\\biggr)}{\\biggl(\\begin{matrix } 1 \\\\[-3pt ] x_{ij } \\end{matrix}\\biggr)}^{\\t }      k\\bigg(\\frac{|x_{ij}|^{2}}{b^{2}}\\bigg)\\bigg\\}^{-1 }      \\sum_{j=1}^{n}y_{j } { \\biggl(\\begin{matrix } 1 \\\\[-3pt ] x_{ij } \\end{matrix}\\biggr)}\\,k\\bigg(\\frac{|x_{ij}|^{2}}{b^{2 } }      \\bigg),\\end{aligned}\\ ] ] where @xmath41 , @xmath42 is a _ bandwidth _ and @xmath43 is a univariate kernel supported on @xmath44 $ ] . the bandwidth @xmath42 should be selected so that the ball with the radius @xmath42 and the center at the point of estimation @xmath39 contains at least @xmath45 design points . for large value of @xmath3",
    "this leads to a bandwidth of order one and to a large estimation bias .",
    "the goal of the structural adaptation is to diminish this bias using an iterative procedure exploiting the available estimated structural information .    in order to transform these general observations to a concrete procedure ,",
    "let us describe in the rest of this section how the knowledge of the structure can help to improve the quality of the estimation and how the structural information can be obtained when the function or its estimator is given .",
    "let us start with the case of known @xmath28 .",
    "the function @xmath7 has the same smoothness as @xmath21 in the directions of the edr subspace @xmath28 spanned by the vectors @xmath46 , whereas it is constant ( and therefore , infinitely smooth ) in all the orthogonal directions .",
    "this suggests to apply an anisotropic bandwidth for estimating the model function and its gradient .",
    "the corresponding local - linear estimator can be defined by @xmath47 x_{ij } \\end{matrix}\\biggr ) }          { \\biggl(\\begin{matrix } 1 \\\\[-3pt ] x_{ij } \\end{matrix}\\biggr)}^{\\t }          w_{ij}^ * \\bigg\\}^{-1 }        \\sum_{j=1}^{n }        y_{j } { \\biggl(\\begin{matrix } 1 \\\\[-3pt ] x_{ij } \\end{matrix}\\biggr ) } \\ ,        w_{ij}^ * \\mbox { } , \\end{aligned}\\ ] ] with the weights @xmath48 , where @xmath49 is some positive real number and @xmath50 is the orthogonal projector onto the edr subspace @xmath28.this choice of weights amounts to using infinite bandwidth in the directions lying in the orthogonal complement of the edr subspace .",
    "if only an estimator @xmath51 of @xmath28 is available , the orthogonal projector @xmath52 onto the subspace @xmath51 may replace @xmath50 in the expression ( [ hatfff ] ) .",
    "this rule of defining the local neighborhoods is too stringent , since it definitely discards the directions belonging to @xmath53 .",
    "being not sure that our information about the structure is exact , it is preferable to define the neighborhoods in a softer way .",
    "this is done by setting @xmath54 and by redefining @xmath55 x_{ij } \\end{matrix}\\biggr ) }          { \\biggl(\\begin{matrix } 1 \\\\[-3pt ] x_{ij } \\end{matrix}\\biggr)}^{\\t }          w_{ij } \\bigg\\}^{-1 }        \\sum_{j=1}^{n }        y_{j } { \\biggl(\\begin{matrix } 1 \\\\[-3pt ] x_{ij } \\end{matrix}\\biggr ) } \\ ,        w_{ij } \\mbox { } .\\end{aligned}\\ ] ] here , @xmath56 is a real number from the interval @xmath44 $ ] measuring the importance attributed to the estimator @xmath52 . if we are very confident in our estimator @xmath52 , we should choose @xmath56 close to zero .",
    "suppose first that the values of the function @xmath38 at the points @xmath39 are known.then @xmath28 is the linear subspace of @xmath57 spanned by the vectors @xmath58 , @xmath59 . for classifying the directions of @xmath57 according to the variability of @xmath7 in each direction and , as a by - product identifying @xmath28 , the principal component analysis ( pca ) can be used .",
    "the pca method is based on the orthogonal decomposition of the matrix @xmath60 : @xmath61 with an orthogonal matrix @xmath62 and a diagonal matrix @xmath63 with diagonal entries @xmath64 .",
    "clearly , for the multi - index model with @xmath11-indices , only the first @xmath11 eigenvalues of @xmath65 are positive .",
    "the first @xmath11 eigenvectors of @xmath65 ( or , equivalently , the first @xmath11 columns of the matrix @xmath62 ) define an orthonormal basis in the edr subspace .",
    "let @xmath31 be a positive integer . in @xcite ,",
    "a `` truncated '' matrix @xmath66 is considered , which coincides with @xmath65 if @xmath31 equals @xmath8 .",
    "let @xmath67 be a system of functions on @xmath68 satisfying the conditions @xmath69 for every @xmath70 , with @xmath71 being the kronecker symbol .",
    "define @xmath72 and@xmath73 . by the bessel inequality",
    ", it holds @xmath74 .",
    "moreover , since @xmath75 , any eigenvector of @xmath65 is an eigenvector of @xmath66 .",
    "finally , by the parseval equality , @xmath76 if @xmath77 .",
    "note that the estimation of @xmath65 has been treated in @xcite .",
    "the reason of considering the matrix @xmath78 instead of @xmath65 is that @xmath78 can be estimated much better than @xmath65 .",
    "in fact , estimators of @xmath65 have poor performance for samples of moderate size because of the sparsity of high dimensional data , ill - posedness of the gradient estimation and the non - linear dependence of @xmath65 on @xmath29 . on the other hand , estimation of @xmath78",
    "reduces to the estimation of @xmath31 linear functionals of @xmath29 and may be done with a better accuracy .",
    "the obvious limitation of this approach is that it recovers the edr subspace entirely only if the rank of @xmath78 coincides with the rank of @xmath65 , which is equal to @xmath11 . to enhance our chances of seeing the condition @xmath79 fulfilled",
    ", we have to choose @xmath31 sufficiently large . in practice , @xmath31 is chosen of the same order as @xmath8 .    in the case",
    "when only an estimator of @xmath29 is available , the above described method of recovering the edr directions from an estimator of @xmath78 have a risk of order @xmath80 ( see ( * ? ? ? * theorem 5.1 ) ) .",
    "this fact advocates against using very large values of @xmath31 .",
    "we desire nevertheless to use many linear combinations in order to increase our chances of capturing the whole edr subspace . to this end",
    ", we modify the method of extracting the structural information from the estimators @xmath81 of vectors @xmath34 .",
    "let @xmath82 be an integer .",
    "observe that the estimator @xmath83 of the projector @xmath50 based on the pca solves the following quadratic optimization problem : @xmath84 where the minimization is carried over the set of all symmetric @xmath85-matrices .",
    "the value @xmath86 can be estimated by looking how many eigenvalues of @xmath83 are significant .",
    "let @xmath87 be the set of @xmath88-matrices defined as follows : @xmath89 from now on , for two symmetric matrices @xmath90 and",
    "@xmath91 , @xmath92 means that @xmath93 is semidefinite positive .",
    "define @xmath94 as a minimizer of the maximum of the @xmath95 s instead of their sum : @xmath96 this is a convex optimization problem that can be effectively solved even for a large @xmath3 although a closed form solution is not known .",
    "moreover , as we will show below , the incorporation of ( [ maxopt ] ) in the structural adaptation yields an algorithm having good theoretical and empirical performance .",
    "throughout this section the true dimension @xmath11 of the edr subspace is assumed to be known .",
    "thus , we are given @xmath8 observations @xmath22 from the model @xmath97 where @xmath24 are independent centered random variables .",
    "the vectors @xmath98 are assumed to form an orthonormal basis of the edr subspace entailing thus the representation @xmath99 . in what follows",
    ", we mainly consider deterministic design .",
    "nevertheless , the results hold in the case of random design as well , provided that the errors are independent of @xmath100 .",
    "henceforth , without loss of generality we assume that @xmath101 for any @xmath59 , where @xmath102 denotes the euclidian norm of the vector @xmath103 .",
    "the structure - adaptive algorithm with maximum minimization consists of following steps .",
    "* specify positive real numbers @xmath104 , @xmath105 , @xmath106 and @xmath107 .",
    "choose an integer @xmath31 and select a set @xmath108 of vectors from @xmath109 verifying @xmath110 .",
    "set @xmath111 .",
    "* initialize the parameters @xmath112 , @xmath113 and @xmath114 . *",
    "define the estimators @xmath115 for @xmath59 by formula ( [ hatfff1 ] ) with the current values of @xmath116 and @xmath52 .",
    "set @xmath117 where @xmath118 is the @xmath119th coordinate of @xmath120 . * define the new value @xmath121 as the solution to ( [ maxopt ] ) . * set @xmath122 , @xmath123 and increase @xmath124 by one .",
    "* stop if @xmath125 or @xmath126 , otherwise continue with the step c ) .",
    "let @xmath127 be the total number of iterations .",
    "the matrix @xmath128 is the desired estimator of the projector @xmath50 .",
    "we denote by @xmath129 the orthogonal projection onto the space spanned by the eigenvectors of @xmath128 corresponding to the @xmath11 largest eigenvalues .",
    "the estimator of the edr subspace is then the image of @xmath129 .",
    "equivalently , @xmath129 is the estimator of the projector onto @xmath28 .",
    "the described algorithm requires the specification of the parameters @xmath106 , @xmath107 , @xmath104 and @xmath105 , as well as the choice of the set of vectors @xmath130 . in what follows we use the values",
    "@xmath131 this choice of input parameters is up to some minor modifications the same as in @xcite , @xcite and @xcite , and is based on the trade - off between the bias and the variance of estimation .",
    "it also takes into account the fact that the local neighborhoods used in ( [ hatfff ] ) should contain enough design points to entail the consistency of the estimator .",
    "the choice of @xmath31 and that of vectors @xmath120 will be discussed in section  [ sim ] .      prior to stating rigorous theoretical results we need to introduce a set of assumptions . from now on",
    ", we use the notation @xmath132 for the identity matrix of dimension @xmath3 , @xmath133 for the largest eigenvalue of @xmath134 and @xmath135 for the sum of squares of all elements of the matrix @xmath90 .",
    "( a1 ) : :    there exists a positive real @xmath136 such that    @xmath137 and    @xmath138    for every @xmath139 .",
    "unlike the smoothness assumption , the assumptions on the identifiability of the model and the regularity of design are more involved and specific for each algorithm .",
    "the formal statements read as follows .",
    "( a2 ) : :    let the vectors @xmath140 be defined by    ( [ betal ] ) and let    @xmath141 .",
    "there exist vectors    @xmath142 and    constants @xmath143 such that    @xmath144    we denote @xmath145 .    assumption ( a2 ) implies that the subspace @xmath146 is the smallest dr subspace , therefore it is the edr subspace .",
    "indeed , for any dr subspace @xmath147 , the gradient @xmath58 belongs to @xmath147 for every @xmath119 .",
    "therefore @xmath148 for every @xmath149 and @xmath150 .",
    "thus , for every @xmath151 from the orthogonal complement @xmath152 , it holds @xmath153 . therefore @xmath154 implying thus the inclusion @xmath155 .",
    "[ lempsi ] if the family @xmath130 spans @xmath109 , then assumption ( a2 ) is always satisfied with some @xmath156 ( that may depend on @xmath8 ) .",
    "set @xmath157 , @xmath158 and write the @xmath159 matrix @xmath160 in the form @xmath161 . recall that if @xmath162 are two matrices such that @xmath163 is well defined and the rank of @xmath164 coincides with the number of lines in @xmath164 , then @xmath165 .",
    "this implies that @xmath166 provided that @xmath167 , which amounts to @xmath168 .",
    "let now @xmath169 be a linearly independent subfamily of @xmath170 .",
    "then the @xmath11th largest eigenvalue @xmath171 of the matrix @xmath172 is strictly positive .",
    "moreover , if @xmath173 are the eigenvectors of @xmath174 corresponding to the eigenvalues @xmath175 , then @xmath176 hence , ( [ pi * ] ) is fulfilled with @xmath177 for every @xmath178 .",
    "these arguments show that ( a2 ) is a fairly weak identifiability assumption .",
    "in fact , since we always choose @xmath130 so that @xmath168 , ( a2 ) amounts to requiring that the value @xmath179 remains bounded when @xmath8 increases .",
    "let us proceed with the assumption on the design regularity .",
    "define @xmath180 , @xmath181 and for any @xmath182 matrix @xmath183 set @xmath184 , @xmath185 , @xmath186 and @xmath187 z_{ij}^{(k ) }",
    "\\end{matrix}\\biggr ) } { \\biggl(\\begin{matrix } 1 \\\\[-3pt ] z_{ij}^{(k ) } \\end{matrix}\\biggr)}^{\\t } \\,w_{ij}^{(k)}(u).\\ ] ]    ( a3 ) : :    for some positive constants @xmath188 and for    some @xmath1890,1/2]$ ] , the inequalities    @xmath190 hold for every    @xmath191 and for every @xmath182 matrix    @xmath183 verifying @xmath192 .",
    "( a4 ) : :    the errors @xmath193 are centered    gaussian with variance @xmath194 .",
    "we assume that the kernel @xmath195 used in ( [ hatfff1 ] ) is chosen to be continuous , positive and vanishing outside the interval @xmath44 $ ] .",
    "the vectors @xmath120 are assumed to verify @xmath196 for some constant @xmath197 independent of @xmath8 . in the sequel , we denote by @xmath198 some constants depending only on @xmath199 and @xmath197 .",
    "[ thm1 ] assume that assumptions * ( a1)-(a4 ) * are fulfilled",
    ". there exists a constant @xmath200 such that for any @xmath2010,2\\sqrt{\\log(nl)}]$ ] and for sufficiently large values of @xmath8 , it holds @xmath202 where @xmath203 , @xmath204 and @xmath205 .    under the assumptions of theorem  [ thm1 ] , for sufficiently large @xmath8",
    ", it holds @xmath206    easy algebra yields @xmath207 the equality @xmath208 and the linearity of the trace operator complete the proof of the first inequality .",
    "the second inequality can be derived from the first one by standard arguments in view of the inequality @xmath209 .",
    "these results assess that for @xmath36 , the estimator of @xmath28 provided by the samm procedure is @xmath210-consistent up to a logarithmic factor .",
    "this rate of convergence is known to be optimal for a broad class of semiparametric problems , see @xcite for a detailed account on the subject .",
    "the inspection of the proof of theorem  [ thm1 ] shows that the factor @xmath211 multiplying the `` bias '' term @xmath212 disappears when @xmath213 .",
    "[ noise ] the same rate of convergence remains valid in the case when the errors are not necessarily identically distributed gaussian random variables , but have ( uniformly in @xmath8 ) a bounded exponential moment .",
    "this can be proved along the lines of proposition  [ prop1 ] , see appendix .",
    "note that in ( a3 ) we implicitly assumed that the matrices @xmath214 are invertible , which may be true only if any neighborhood @xmath215 contains at least @xmath3 design points different from @xmath216 .",
    "the parameters @xmath107 , @xmath106 , @xmath104 and @xmath105 are chosen so that the volume of ellipsoids @xmath217 is a non - decreasing function of @xmath124 and @xmath218 .",
    "therefore , from theoretical point of view , if the design is random with positive density on @xmath44^d$ ] , it is easy to check that for a properly chosen constant @xmath219 , assumption ( a3 ) is satisfied with a probability close to one . in applications ,",
    "we define @xmath107 as the smallest real such that @xmath220 and add to @xmath221 a small full - rank matrix to be sure that the resulting matrix is invertible , see section [ sim ] .    in the case when @xmath222 is integer ,",
    "an example of deterministic design satisfying ( a3 ) is as follows .",
    "choose @xmath3 functions @xmath223\\to [ 0,\\infty[$ ] such that @xmath224 } h_k(x)>0 $ ] and @xmath225 } h_k(x ) < \\infty$ ] .",
    "define the design points @xmath226 by @xmath227 , where @xmath228 range over @xmath229 .",
    "this definition guarantees that the number of design points lying in an ellipsoid @xmath230 is asymptotically of the same order as @xmath231 , as @xmath232 .",
    "this suffices for ( a3 ) .",
    "of course , it is unlikely to have such a design in practice , since even for small @xmath233 and moderate @xmath3 it leads to an unrealistically large sample size .",
    "the aim of this section is to demonstrate on several examples how the performance of the algorithm samm depends on the sample size @xmath8 , the dimension @xmath3 and the noise level @xmath234 .",
    "we also show that our procedure can be successfully applied in autoregressive models .",
    "many unreported results show that in most situations the performance of samm is comparable to the performance of sa approach based on pca and to that of mave . a thorough comparison of the numerical virtues of these methods being out of scope of this paper",
    ", we simply show on some examples that samm may substantially outperform mave in the case of large `` bias '' .",
    "the computer code of the procedure samm is distributed freely , it can be downloaded from _ http://www.proba.jussieu.fr / pageperso / dalalyan/_. it requires the matlab packages sdpt3 and yalmip.we are grateful to professor yingcun xia for making the computer code of mave available to us .    to obtain higher stability of the algorithm",
    ", we preliminarily standardize the response @xmath2 and the predictors @xmath235 .",
    "more precisely , we deal with @xmath236 and @xmath237 , where @xmath238 is the empirical variance of @xmath2 , @xmath239 is the empirical covariance matrix of @xmath10 and @xmath240 is the @xmath182 matrix obtained from @xmath239 by replacing the off - diagonal elements by zero . to preserve consistency , we set @xmath241 , where @xmath242 is the last - step estimate of @xmath34 , and define @xmath128 as the solution to ( [ maxopt ] ) with @xmath81 replaced by @xmath243 .",
    "furthermore , we add the small full - rank matrix @xmath244 to @xmath245 x_{ij } \\end{matrix}\\biggr ) } { \\biggl(\\begin{matrix } 1 \\\\[-3pt ] x_{ij } \\end{matrix}\\biggr)}^{\\t}w_{ij}$ ] in ( [ hatfff1 ] ) .    in all examples presented below the number of replications is @xmath246 .",
    "the mean loss @xmath247 and the standard deviation @xmath248 are reported , where @xmath249 with @xmath250 being the estimator of @xmath50 for @xmath251th replication .",
    "the set @xmath130 plays an essential role in the algorithm .",
    "the optimal choice of this set is an important issue that needs further investigation .",
    "we content ourselves with giving one particular choice which agrees with theory and leads to nice empirical results .",
    "let @xmath253 , @xmath254 , be the permutation of the set @xmath255 satisfying @xmath256 .",
    "let @xmath257 be the inverse of @xmath253 , i.e.@xmath258 for every @xmath259 .",
    "define @xmath130 as the set of vectors @xmath260 \\big(\\sin\\big(\\frac{2\\pi k\\mathfrak s_j^{-1}(1)}{n}\\big),\\ldots , \\sin\\big(\\frac{2\\pi k\\mathfrak s_j^{-1}(n)}{n}\\big)\\big)^\\t \\end{matrix } , k\\le [ n/2],\\ j\\le d\\bigg\\}\\ ] ] normalized to satisfy @xmath261 for every @xmath262 .",
    "it is easily seen that these vectors satisfy conditions ( [ asspsi ] ) and @xmath168 , so the conclusion of lemma  [ lempsi ] holds .",
    "above , @xmath263 $ ] is the integer part of @xmath264 and @xmath124 and @xmath251 are positive integers .",
    "we set @xmath265 and @xmath266 with @xmath267 we run samm and mave procedures on the data generated by the model @xmath268 where the design @xmath10 is such that the coordinates @xmath269 are i.i.d .  uniform on @xmath270 $ ] , and the errors @xmath271 are i.i.d .",
    "standard gaussian independent of the design .",
    "table [ table1 ] contains the average loss for different values of the sample size @xmath8 for the first step estimator by samm , the final estimator provided by samm and the estimator based on mave .",
    "we plot in figure  [ fig1 ] ( a ) the average loss normalized by the square rood of the sample size @xmath8 versus @xmath8 .",
    "it is clearly seen that the iterative procedure improves considerably the quality of estimation and that the final estimator provided by samm is @xmath210-consistent . in this example",
    ", mave method often fails to recover the edr subspace .",
    "however , the number of failures decreases very rapidly with increasing @xmath8 .",
    "this is the reason why the curve corresponding to mave in figure  [ fig1 ] ( a ) decreases with a strong slope .",
    "@xmath272    .__average loss @xmath273 of the estimators obtained by samm and mave procedures in example 1 .",
    "the standard deviation is given in parentheses .",
    "_ _ [ cols=\"^,>,>,>,>,>\",options=\"header \" , ]",
    "since the proof of the main result is carried out in several steps , we give a short road map for guiding the reader throughout the proof .",
    "the main idea is to evaluate the accuracy of the first step estimators of @xmath34 and , given the accuracy of the estimator at the step @xmath124 , evaluate the accuracy of the estimators at the step @xmath274 .",
    "this is done in subsections  [ ss2 ] and [ ss3 ] .",
    "these results are based on a maximal inequality proved in subsection [ ss5 ] and on some properties of the solution to ( [ maxopt ] ) proved in subsection [ ss6 ] .",
    "the proof of theorem  [ thm1 ] is presented in subsection [ ss4 ] , while some technical lemmas are postponed to subsection  [ ss7 ] .      since at the first step no information about",
    "the edr subspace is available , we use the same bandwidth in all directions , that is the local neighborhoods are balls ( and not ellipsoids ) of radius @xmath49 .",
    "therefore the first step estimator @xmath275 of the vector @xmath276 is the same as the one used in @xcite .            in order that the kernel estimator of @xmath291 be consistent ,",
    "the ball centered at @xmath292 with radius @xmath107 should contain at least @xmath3 points from @xmath293 .",
    "if the design is regular , this means that @xmath107 is at least of order @xmath294 .",
    "the optimization of the risk of @xmath275 with respect to @xmath107 verifying @xmath295 leads to the choice @xmath296 .      at the @xmath297th step of iteration",
    ", we have at our disposal a symmetric matrix @xmath298 belonging to the set @xmath299 thus the matrix @xmath300 is the @xmath297th step approximation of the projector @xmath301 onto the edr subspace @xmath302 . using this approximation , we construct the new matrix @xmath303 in the following way : set @xmath304 , @xmath305 and define the estimator of the regression function and its gradient at the design point @xmath306 as follows : @xmath307 \\widehat{{\\nabla \\ ! f}}_{\\pi}(x_{i } ) \\end{pmatrix } = v_{i}(\\pi)^{-1}\\sum_{j=1}^{n } y_{j } { \\biggl(\\begin{matrix } 1 \\\\[-3pt ] x_{ij } \\end{matrix}\\biggr ) } w_{ij}(\\pi),\\ ] ] where @xmath308 and @xmath309 x_{ij } \\end{matrix}\\biggr ) } { \\biggl(\\begin{matrix } 1 \\\\[-3pt ] x_{ij } \\end{matrix}\\biggr)}^{\\t } \\,w_{ij}(\\pi).\\ ] ] to state the next result , we need some additional notation . set @xmath310 , @xmath311 and @xmath312 , where @xmath313 . in this notation",
    ", we obtain @xmath314 p_\\rho^*\\widehat{{\\nabla \\ !",
    "f}}_{\\pi}(x_{i } ) \\end{pmatrix } & = \\begin{pmatrix }      h^{-1 } & 0\\\\      0 & p_{\\rho}^ { * } \\end{pmatrix } v_{i}(\\pi)^{-1}\\sum_{j=1}^{n } y_j { \\biggl(\\begin{matrix } 1 \\\\[-3pt ] x_{ij } \\end{matrix}\\biggr)}\\,w_{ij}(\\pi)\\\\ & = \\frac1h \\begin{pmatrix }      1 & 0\\\\      0 & hp_{\\rho}^ { * } \\end{pmatrix } v_{i}(\\pi)^{-1}\\begin{pmatrix }      1 & 0\\\\      0 & hp_{\\rho}^ { * } \\end{pmatrix } \\sum_{j=1}^{n } y_j { \\biggl(\\begin{matrix } 1 \\\\[-3pt ] z_{ij } \\end{matrix}\\biggr)}\\,w_{ij}(\\pi)\\\\ & = h^{-1 } \\v_{i}(u)^{-1 } \\sum_{j=1}^{n } y_j { \\biggl(\\begin{matrix } 1 \\\\[-3pt ] z_{ij } \\end{matrix}\\biggr)}\\,w_{ij}(u)\\end{aligned}\\ ] ] where @xmath315 and @xmath316 z_{ij } \\end{matrix}\\biggr ) } { \\biggl(\\begin{matrix } 1 \\\\[-3pt ] z_{ij } \\end{matrix}\\biggr)}^{\\t } \\,w_{ij}(u).\\ ] ] set @xmath317 and @xmath318",
    ".    [ p5 ] if ( a1)-(a4 ) are fulfilled then there exist gaussian vectors @xmath319 such that @xmath320\\leq c_0 ^ 2\\sigma^2 $ ] and @xmath321 where the @xmath322 is taken over @xmath323 , @xmath324 and we used the notation @xmath325 , @xmath203 and @xmath326 .    let us start with evaluating the bias term @xmath327 .",
    "according to the cauchy - schwarz inequality , it holds @xmath328-{\\nabla \\ !",
    "f}(x_{i})\\bigr )      \\psi_{\\ell}(x_{i } ) \\bigg|^2 \\nonumber\\\\ & \\leq \\frac1{n^{2 } }      \\sum_{i=1}^{n } \\big|p_{\\rho}^{*}\\bigl(\\esp[\\widehat{{\\nabla \\ !",
    "f}}_{\\pi}(x_{i})]-      { \\nabla \\ ! f}(x_{i})\\bigr)\\big|^2      \\sum_{i=1}^{n } \\psi_{l}^{2}(x_{i } ) \\nonumber\\\\ & \\leq \\max_{i=1,\\ldots , n}\\big|p_{\\rho}^{*}\\bigl(\\esp[\\widehat{{\\nabla \\ !",
    "f}}_{\\pi}(x_{i})]- { \\nabla \\ ! f}(x_{i})\\bigr)\\big|^2.\\end{aligned}\\ ] ] simple computations show that @xmath329&- { \\nabla \\ ! f}(x_{i})\\bigr)\\big|\\\\ & \\leq \\bigg|\\esp \\begin{pmatrix }      h^{-1}\\hat f_{\\pi}(x_{i})\\\\[3pt ]      p_{\\rho}^{*}\\widehat{{\\nabla \\ !",
    "f}}_{\\pi}(x_{i } ) \\end{pmatrix } - \\begin{pmatrix }      h^{-1}f(x_{i})\\\\[3pt ]      p_{\\rho}^{*}{\\nabla \\ !",
    "f}(x_{i } ) \\end{pmatrix}\\bigg|\\\\ & = h^{-1}\\bigg| \\v_{i}^{-1 } \\sum_{j=1}^{n } f(x_j ) { \\biggl(\\begin{matrix } 1 \\\\[-3pt ] z_{ij } \\end{matrix}\\biggr)}\\,w_{ij}(u)- \\begin{pmatrix }      h^{-1}f(x_{i})\\\\[3pt ]      p_{\\rho}^{*}{\\nabla \\ !",
    "f}(x_{i } ) \\end{pmatrix}\\bigg|\\\\ & = h^{-1 } \\big|\\v_{i}^{-1 } \\sum_{j=1}^{n } r_{ij } { \\biggl(\\begin{matrix } 1 \\\\[-3pt ] z_{ij } \\end{matrix}\\biggr ) } w_{ij}(u)\\big|:=b(x_i),\\end{aligned}\\ ] ] where @xmath330 .",
    "define @xmath331 and @xmath332 z_{ij } \\end{matrix}\\biggr)}\\sqrt{w_{ij}(u)}$ ] . then @xmath333 the identity @xmath334 implies @xmath335 where the maximum of @xmath336 is taken over the indices @xmath337 satisfying @xmath338 . since the weights @xmath339 are defined via the kernel function @xmath340 vanishing on the interval @xmath341 , we have@xmath342 .",
    "by corollary  [ cellipt ] @xmath343 implies @xmath344 .",
    "let us denote by @xmath345 the @xmath346 matrix having @xmath347 as @xmath124th column .",
    "then @xmath348 and therefore @xmath349 these estimates yield @xmath350 , and consequently , @xmath351 let us treat now the stochastic term @xmath352 .",
    "it can be bounded as follows @xmath353 where @xmath354 z_{ij } \\end{matrix}\\biggr ) } w_{ij}(u)\\psi_{\\ell}(x_{i}).\\ ] ] let us define @xmath355)$ ] . in view of lemma  [ lem4.1 ] , we have @xmath320\\leq nh^2\\sigma^2\\sum_j|c_{j,\\ell}(u^*)|^2\\leq c_0 ^ 2\\sigma^2 $ ] .",
    "one checks that for any @xmath356 and for any @xmath357 such that @xmath358 , it holds @xmath359 ) -\\frac{\\xi_\\ell^*}{h\\sqrt",
    "n}\\big|\\leq \\sup_{\\|u - u^*\\|_2\\leq \\alpha } \\bigg|      \\sum_{j=1}^{n } \\big(c_{j,\\ell}(u)-c_{j,\\ell}(u^*)\\big)\\,\\varepsilon_{j } \\bigg|.\\ ] ] set @xmath360 .",
    "lemma  [ lem4.2 ] implies that proposition  [ prop1 ] can be applied with @xmath361 and @xmath362 .",
    "setting @xmath363 we get that the probability of the event @xmath364 is less than @xmath365 .",
    "this completes the proof of the proposition .",
    "[ cor1 ] if @xmath366 and the assumptions of proposition  [ p5 ] are fulfilled , then @xmath367 in particular , if @xmath289 , the probability of the event @xmath368 does not exceed @xmath369 , where @xmath322 is taken over all @xmath370 , @xmath324 and @xmath371 , @xmath372 , @xmath373 are defined in proposition  [ p5 ] and in theorem  [ thm1 ] .",
    "recall that at the first step we use the following values of parameters : @xmath376 , @xmath377 and @xmath378 .",
    "let us denote @xmath379 and introduce the event @xmath380 . according to corollary  [ cor2 ] the probability of the event @xmath381 is at least @xmath382 .",
    "in view of proposition  [ p2 ] , we get @xmath383 .    for any integer",
    "@xmath384 $ ] ( where @xmath127 is the total number of iterations ) , we define @xmath385 here @xmath386 with @xmath387 \\widehat{{\\nabla \\ ! f}}^{(k)}(x_{i } ) \\end{pmatrix } = \\bigg(\\sum_{j=1}^{n}{\\biggl(\\begin{matrix } 1 \\\\[-3pt ] x_{ij } \\end{matrix}\\biggr)}{\\biggl(\\begin{matrix } 1 \\\\[-3pt ] x_{ij } \\end{matrix}\\biggr)}^{\\t}\\,w_{ij}^{(k)}\\bigg)^{-1 } \\sum_{j=1}^{n } y_{j}{\\biggl(\\begin{matrix } 1 \\\\[-3pt ] x_{ij } \\end{matrix}\\biggr)}w_{ij}^{(k)},\\ ] ] and @xmath388 .",
    "combining lemmas  [ lem5.3 ] and  [ lem5.4 ] , we obtain @xmath389 and therefore , using corollary  [ cor1 ] , we get @xmath390 since @xmath391 , it holds @xmath392 and @xmath393 .",
    "lemma  [ lem5.4 ] implies that @xmath394 according to lemma  [ lem5.3 ] , we have @xmath395 , @xmath396 and @xmath397 . consequently , for @xmath8 sufficiently large , we have @xmath398 and @xmath399 $ ] . since @xmath400 and @xmath401 , we infer that @xmath402 therefore @xmath403 tends to zero as @xmath232 at least as fast as @xmath404 and the assertion of the theorem follows from the definition of @xmath405 and lemma  [ le4.2 ] ( see below ) .",
    "[ prop1 ] let @xmath406 be a positive number and let @xmath407 be a finite set .",
    "let functions @xmath408 obey the conditions @xmath409 if the @xmath410 s are independent @xmath411-distributed random variables , then @xmath412 where @xmath413 .",
    "let @xmath414 be the ball @xmath415 and @xmath416 be the @xmath417-net on @xmath414 such that for any @xmath418 there is an element @xmath419 such that @xmath420 .",
    "it is easy to see that such a net with cardinality @xmath421 can be constructed . for every @xmath418 we denote@xmath422 . since @xmath423 for any @xmath424 and for any @xmath425",
    ", we have @xmath426 thus we get @xmath427 hence , if @xmath428 , then @xmath429 on the other hand , for any @xmath430 the cauchy - schwarz inequality yields @xmath431 since @xmath432 is certainly less than @xmath433 , we have @xmath434 and the assertion of proposition follows .",
    "we collect below some simple facts concerning the solution to the optimization problem ( [ maxopt ] ) . by classical arguments , it is always possible to choose a measurable solution @xmath52 to ( [ maxopt ] ) .",
    "this measurability will be assumed in the sequel .    in proposition  [ p1 ]",
    "the case of general @xmath233 ( not necessarily equal to @xmath11 ) is considered . as we explain below",
    ", this generality is useful for further developments of the method extending it to the case of unknown structural dimension @xmath11 .",
    "the vectors @xmath34 are assumed to belong to a @xmath11-dimensional subspace @xmath28 of @xmath57 , but in this subsection we do not assume that @xmath34s are defined by ( [ betal ] ) .",
    "in fact , we will apply the results of this subsection to the vectors @xmath435 .        for every @xmath450 , we have @xmath451",
    "since @xmath452 minimizes @xmath453 over @xmath454 , we have @xmath455 denote @xmath456 . from definition @xmath457 .",
    "therefore , for every @xmath458 @xmath459 the second inequality of the proposition follows now from @xmath460 for every @xmath461 .    to prove the last assertion , remark that according to the definition of @xmath462 , for every matrix @xmath463 there exists an index @xmath458 such that @xmath464 .",
    "in particular , @xmath465 for some @xmath262 and hence @xmath466 .    proposition  [ p1 ] can be used for estimating the structural dimension @xmath233 .",
    "indeed , @xmath467 for @xmath468 and the results mean that @xmath469 for @xmath448 .",
    "therefore , it is natural to search for the smallest value @xmath470 of @xmath471 such that the function @xmath472 does not significantly decrease for @xmath473 .        in view of the relations",
    "@xmath478 and @xmath479 , we have @xmath480 note also that the equality @xmath481 implies that @xmath482 . now condition ( [ pi * ] ) and proposition  [ p1 ] imply @xmath483 and the assertion follows .    [ lab ]",
    "let @xmath484 for some @xmath485 .",
    "then for any @xmath486 @xmath487 .",
    "it obviously holds @xmath488 and @xmath489\\cdot |x|^{2}.\\end{aligned}\\ ] ] for every @xmath490 , it obviously holds @xmath491 , and hence , @xmath492 .",
    "therefore , @xmath493 yielding @xmath494 as required .",
    "[ le4.2 ] let @xmath484 for some @xmath501 and let @xmath474 be the orthogonal projection matrix in @xmath57 onto the subspace spanned by the eigenvectors of @xmath52 corresponding to its largest @xmath11 eigenvalues .",
    "then @xmath502 .",
    "let @xmath503 and @xmath504 , @xmath505 be respectively the eigenvalues and the eigenvectors of @xmath52 .",
    "assume that @xmath506",
    ". then @xmath507 and @xmath508 .",
    "moreover , @xmath509 since @xmath510 is an orthonormal basis of @xmath57 , therefore , on the one hand , @xmath511&\\le \\sum_{j\\le m^ * } \\hat\\lambda_j \\tr[\\hat\\theta_j\\hat\\theta_j^\\t \\pi^*]+ \\hat\\lambda_{m^*}\\sum_{j > m^*}\\tr[\\hat\\theta_j\\hat\\theta_j^\\t \\pi^*]\\\\ & = \\sum_{j\\le m^ * } ( \\hat\\lambda_j-\\hat\\lambda_{m^*})\\tr[\\hat\\theta_j\\hat\\theta_j^\\t \\pi^*]+ \\hat\\lambda_{m^*}\\tr\\big[\\sum_{j=1}^d\\hat\\theta_j\\hat\\theta_j^\\t \\pi^*\\big]\\\\ & = \\sum_{j\\le m^ * } ( \\hat\\lambda_j-\\hat\\lambda_{m^ * } ) \\tr[\\hat\\theta_j\\hat\\theta_j^\\t \\pi^*]+m^*\\hat\\lambda_{m^*}.\\end{aligned}\\ ] ] since @xmath512=|\\pi^*\\hat\\theta_j|^2\\le 1 $ ] , we get @xmath513\\le \\sum_{j\\le m^*}\\hat\\lambda_{j}$ ] . taking into account the relations @xmath514 , @xmath208 and @xmath515 , we get @xmath516\\le\\delta^2 $ ] and therefore @xmath517\\le \\delta^2/(1-\\hat\\lambda_{m^*+1})\\le \\delta^2/(1-\\delta^2)$ ] .",
    "the inequality @xmath520 implies that @xmath521 since @xmath522 for any matrix @xmath90 , it holds @xmath523 by similar arguments one checks that @xmath524 thus we get @xmath525 .",
    "the assumption @xmath518 yields the assertion of the lemma .",
    "simple computations yield @xmath527 z_{ij } \\end{matrix}\\biggr ) } \\bigg|^2w_{ij } & = \\tr(\\v_i^{-1})\\leq \\frac{d c_v}{n_i}.\\qquad\\label{dcv}\\end{aligned}\\ ] ] hence , we have @xmath528 z_{ij } \\end{matrix}\\biggr ) } w_{ij}\\,\\psi_{\\ell}(x_{i})\\bigg|^2\\\\ & \\le \\frac{\\bar\\psi^2}{h^2n^2}\\ : \\sum_{j=1}^n\\bigg(\\sum_{i=1}^n\\frac{w_{ij}}{n_i}\\bigg ) \\bigg(\\sum_{i=1}^n\\bigg|\\v_{i}^{-1 } { \\biggl(\\begin{matrix } 1 \\\\[-3pt ] z_{ij } \\end{matrix}\\biggr)}\\bigg|^2 n_iw_{ij}\\bigg)\\\\ & \\le   \\frac{c_k\\bar\\psi^2}{h^2n^2}\\ : \\sum_{j=1}^n\\sum_{i=1}^n\\bigg|\\v_{i}^{-1 } { \\biggl(\\begin{matrix } 1 \\\\[-3pt ] z_{ij } \\end{matrix}\\biggr)}\\bigg|^2 n_iw_{ij}.\\end{aligned}\\ ] ] interchanging the order of summation and using inequality ( [ dcv ] ) we get the desired result .",
    "we have @xmath534 z_{ij } \\end{matrix}\\biggr)}\\bigg ] w_{ij}(u)\\psi_{\\ell}(x_{i})\\bigg\\|_2 ^ 2\\\\ & + 2\\bigg\\|\\frac{1}{hn}\\sum_{i=1}^{n}\\e^{\\t}\\v_{i}^{-1}(u ) { \\biggl(\\begin{matrix } 1 \\\\[-3pt ] z_{ij } \\end{matrix}\\biggr)}\\frac{d w_{ij}(u)}{du}\\;\\psi_{\\ell}(x_{i } ) \\bigg\\|_2",
    "^ 2\\\\ & = \\delta_1+\\delta_2.\\end{aligned}\\ ] ] one checks that @xmath535 , where we used the notation @xmath536 and the inequality @xmath537 which follows from lemma  [ lab ] .",
    "we get @xmath538 z_{ij } \\end{matrix}\\biggr)}w'_{ij}(u)\\big| \\bigg)^2\\leq \\frac{c \\bar\\psi^2 c_v^2 c_{k'}^2}{n^2h^2}.\\end{aligned}\\ ] ] in order to estimate the term @xmath539 , remark that the differentiation ( with respect to @xmath540 ) of the identity @xmath541 yields @xmath542 simple computations show that @xmath543 z_{ij } \\end{matrix}\\biggr ) } { \\biggl(\\begin{matrix } 1 \\\\[-3pt ] z_{ij } \\end{matrix}\\biggr)}^{\\t } \\frac{\\partial \\ } { \\partial u_{pq}}w_{ij}(u)\\\\ & = \\sum_{j=1}^{n } { \\biggl(\\begin{matrix } 1 \\\\[-3pt ] z_{ij } \\end{matrix}\\biggr ) } { \\biggl(\\begin{matrix } 1 \\\\[-3pt ] z_{ij } \\end{matrix}\\biggr)}^{\\t}w'_{ij}(u ) ( z_{ij})_p(z_{ij})_q.\\end{aligned}\\ ] ] hence , for any @xmath544 , @xmath545 z_{ij } \\end{matrix}\\biggr ) } { \\biggl(\\begin{matrix } 1 \\\\[-3pt ] z_{ij } \\end{matrix}\\biggr)}^{\\t}\\v_i^{-1}(u)a_2\\;w'_{ij}(u)z_{ij}z_{ij}^{\\t}.\\end{aligned}\\ ] ] this relation combined with the estimate @xmath546 for all @xmath547 such that @xmath548 , implies the norm estimate @xmath549 z_{ij } \\end{matrix}\\biggr)}{\\biggl(\\begin{matrix } 1 \\\\[-3pt ] z_{ij } \\end{matrix}\\biggr)}^{\\t}\\v_i^{-1}(u)a_2\\ ; w'_{ij}(u)\\bigg|\\\\ & \\leq   150|a_1|\\,|a_2|\\sum_{j=1}^{n } \\big\\|\\v_i^{-1}(u)\\big\\|^2|w'_{ij}(u)|\\\\ & \\le   150 c_w c_v^2|a_1|\\,|a_2| n_i(u)^{-1}.\\end{aligned}\\ ] ] it leads to the estimate @xmath550 , and the assertion of the lemma follows .",
    "we do now an induction on @xmath124 .",
    "since @xmath560 as @xmath232 and @xmath561 , the inequality @xmath562 is true for sufficiently large values of @xmath8 .",
    "let us prove the implication @xmath563 since @xmath564 we infer that @xmath553 and therefore @xmath565 . by our choice of @xmath105 and @xmath104",
    ", we have @xmath566 . therefore , @xmath567 thus , for @xmath8 large enough , @xmath555 and @xmath568 .",
    "this implies that @xmath569 . by induction",
    "we infer that @xmath570 and @xmath555 for any @xmath571 .",
    "this completes the proof of the lemma .",
    "let us denote @xmath575 , then @xmath576 and under @xmath577 we have @xmath578 set @xmath579 and @xmath580 , where @xmath581 if @xmath582 . since @xmath583 and @xmath584 .",
    "therefore @xmath585 , @xmath586 .",
    "this inequality implies that @xmath587 and , in view of proposition  [ p2 ] we obtain the assertion of the lemma",
    ".    * acknowledgment . *",
    "much of this work has been carried out when the first author was visiting the weierstrass institute for applied analysis and stochastics . the financial support from the institute and the hospitality of professor spokoiny are gratefully acknowledged ."
  ],
  "abstract_text": [
    "<S> the statistical problem of estimating the effective dimension - reduction ( edr ) subspace in the multi - index regression model with deterministic design and additive noise is considered . </S>",
    "<S> a new procedure for recovering the directions of the edr subspace is proposed . under mild assumptions , </S>",
    "<S> @xmath0-consistency of the proposed procedure is proved ( up to a logarithmic factor ) in the case when the structural dimension is not larger than @xmath1 . </S>",
    "<S> the empirical behavior of the algorithm is studied through numerical simulations .    , </S>"
  ]
}