{
  "article_text": [
    "in our setting , a stochastic particle system is a system of _ coupled _ @xmath3-dimensional stochastic differential equations ( sdes ) each modeling the state of a `` particle '' .",
    "such particle systems are versatile tools that can be used to model the dynamics of various complicated phenomena using relatively simple interactions , e.g. , pedestrian dynamics @xcite , collective animal behavior @xcite and interactions between cells @xcite . one common goal of the simulation of these particle systems is to average some quantity of interest computed on _ all _ particles , e.g. , the average velocity , average exit time or average number of particles in a specific region .    under certain conditions , most importantly the exchangeability of particles and sufficient regularity of the sde coefficients , the stochastic particle system approaches a mean - field limit as the number of particles tends to infinity @xcite .",
    "exchangeability of particles refers to the assumption that all permutations of the particles have the same joint distribution .",
    "if the initial states of particles are independently sampled , then all particles behave independently in the mean - field limit , due to propagation of chaos , despite the fact that the dynamics of the particles are coupled . moreover , in the mean - field limit , each particle follows a single mckean - vlasov sde where the advection and/or diffusion coefficients depend on the distribution of the solution to the sde @xcite . in many cases ,",
    "the objective is to approximate the expected value of a quantity of interest ( qoi ) in the mean - field limit as the number of particles tend to infinity , subject to some error tolerance , @xmath4 . while it is possible to approximate the expectation of these qois by estimating the solution to a nonlinear pde using traditional numerical methods , such methods usually suffer from the curse of dimensionality .",
    "indeed , the cost of these method is usually of @xmath5 for some constant @xmath6 that depends on the particular numerical method .",
    "using sparse numerical methods alleviates the curse of dimensionality but requires increasing regularity as the dimensionality of the state space increases . on the other hand ,",
    "monte carlo methods do not suffer from this curse with respect to the dimensionality of the state space .",
    "this work explores different variants and extensions of the monte carlo method when the underlying stochastic particle system satisfies certain crucial assumptions .",
    "we motivate the validity of these assumptions by verifying them numerically on a simple stochastic particle system , leaving theoretical justification to a future work .",
    "generally , the sdes that constitute a stochastic particle system can not be solved exactly and their solution must instead be approximated using the euler - maruyama or milstein scheme with a number of time steps , @xmath7 .",
    "this approximation parameter and a finite number of particles , @xmath8 , are the two approximation parameters that are involved in approximating a finite average of the qoi computed for all particles in the system .",
    "then , to approximate the expectation of this average , we use a monte carlo method .",
    "in such a method , multiple _ independent _ and _ identical _ stochastic particle systems , approximated with the same number of time steps , @xmath7 , are simulated and the average qoi is computed from each and an overall average is then taken . using this method , reducing the variance of the estimator",
    "is achieved by increasing the number of simulations of the stochastic particle system or increasing the number of particles in the system .",
    "section  [ ss : mc ] presents the monte carlo method more precisely in the setting of stochastic particle systems .",
    "particle methods that are not based on monte carlo were also discussed in @xcite . in these methods ,",
    "single _ simulation of the stochastic particle system is carried out and only the number of particles is increased to reduce the variance .    as an improvement of monte carlo methods , the multilevel monte carlo ( mlmc )",
    "method was first introduced in @xcite for parametric integration and in @xcite for sdes ; see @xcite and references therein for an overview .",
    "mlmc improves the efficiency of monte carlo method when only an approximation , controlled with a single discretization parameter , of the solution to the underlying system can be computed .",
    "the basic idea is to reduce the number of required samples on the finest , most accurate but most expensive discretization by reducing the variability of this approximation by using a _ correlated _ coarser and cheaper discretization as a control variate .",
    "more details are given in section  [ ss : mlmc ] for the case of stochastic particle systems .",
    "the application of mlmc to particle systems has been investigated in many works @xcite .",
    "the same concepts have also been applied to nested expectations @xcite .",
    "more recently , a particle method applying the mlmc methodology to stochastic particle systems was also introduced in @xcite achieving , for a linear system with a diffusion coefficient that is independent of the state variable , a work complexity of @xmath9 .",
    "recently , the multi - index monte carlo ( mimc ) method @xcite was introduced to tackle high dimensional problems with more than one discretization parameter .",
    "mimc is based on the same concepts as mlmc and improves the efficiency of mlmc even further but requires mixed regularity with respect to the discretization parameters .",
    "more details are given in section  [ ss : mimc ] for the case of stochastic particle systems . in this section ,",
    "we demonstrate the improved work complexity of mimc compared with the work complexity of mc and mlmc , when applied to a stochastic particle system . more specifically ,",
    "we show that , when using a naive simulation method for the particle system with quadratic complexity and an euler - maruyama time stepping scheme , the optimal work complexity of mimc is @xmath10 when the diffusion coefficient is independent of the state variable and @xmath11 otherwise .",
    "finally , in section  [ s : res ] , we provide numerical verification for the assumptions that are made throughout the current work and the derived rates of the work complexity .    in what follows",
    ", the notation @xmath12 means that there exists a constant @xmath13 that is independent of @xmath14 and @xmath15 such that @xmath16 .",
    "consider a system of @xmath8 exchangeable stochastic differential equations ( sdes ) where for @xmath17 , we have the following equation for @xmath18 @xmath19 for some ( possibly stochastic ) functions , @xmath20 and @xmath21 . here",
    "@xmath22 and @xmath23 is called the empirical measure . in this setting , @xmath24 are mutually independent @xmath3-dimensional wiener processes . if , moreover , @xmath25 are independent and identically distributed ( i.i.d . ) , then under certain conditions on the smoothness and form of @xmath26 and @xmath27 @xcite , as @xmath28 for any @xmath29 , the @xmath30 stochastic process satisfies @xmath31 where @xmath32 is the corresponding marginal law . due to , @xmath33 are i.i.d . ; hence , unless we want to emphasize the particular path , we drop the @xmath34-dependence in @xmath30 and refer to the random process @xmath35 instead . in any case , we are interested in computing @xmath36{\\psi\\op*{x_{\\infty}(t)}}}}$ ] for some given function , @xmath37 , and some final time , @xmath38 .    throughout this work ,",
    "we focus on a simple , one - dimensional example of . for @xmath39 ,",
    "we seek @xmath40 that satisfies @xmath41 where @xmath42 are i.i.d . and independent from the set of i.i.d . random variables @xmath43 and the wiener processes @xmath44 .",
    "we are interested in @xmath45{\\cos(x_{\\infty}(t))}}}}^2 +    \\op*{{{{\\ensuremath{\\mathrm{e}}}\\mspace{-2mu}\\en*[]{\\sin(x_{\\infty}(t))}}}}^2,\\ ] ] a real number between zero and one that measures the level of synchronization in the system with an infinite number of oscillators  @xcite . in this case , we need two estimators : one where we take @xmath46 and the other where we take @xmath47 .    while it is computationally efficient to approximate @xmath36{\\psi(x_\\infty(t))}}}$ ] by solving the integro - differential fokker - planck pde of @xmath48 when the state dimensionality , @xmath3 , is small ( cf . ,",
    "e.g. , @xcite ) , the cost of a standard full tensor approximation increases exponentially as the dimensionality of the state space increases . on the other hand , using sparse approximation techniques to solve",
    "the pde requires increasing regularity assumptions as the dimensionality of the state space increases . instead , in this work , we focus on approximating the value of @xmath36{\\psi\\op{x_\\infty}}}}$ ] by simulating the sde system in .",
    "let us now define @xmath49 here , due to exchangability , @xmath50 are identically distributed but they are not independent since they are taken from the same realization of the particle system . nevertheless , we have @xmath36{\\phi_p } } } = { { { \\ensuremath{\\mathrm{e}}}\\mspace{-2mu}\\en*[]{\\psi(x_{p|p}(t))}}}$ ] for any @xmath34 and @xmath8 .",
    "moreover , we can show that @xmath51{l^2 }   { { { \\ensuremath{\\mathrm{e}}}\\mspace{-2mu}\\en*[]{\\psi(x_\\infty(t))}}}$ ] using the results in @xcite for a class of stochastic particle systems with constant diffusion coefficients . in this case , with respect to the number of particles , @xmath8 , the cost of a naive calculation of @xmath52 is @xmath53 due to the linear sum in for every particle in the system .",
    "it is possible to take @xmath54 in as i.i.d .",
    ", i.e. , for each @xmath55 , @xmath56 is taken from a different independent realization of the system . in this case , the usual law of large numbers applies , but the cost of a naive calculation of @xmath52 is @xmath57 . for this reason , we focus in this work on the former method of taking identically distributed but not independent @xmath54 .    following the setup in @xcite ,",
    "our objective is to build a random estimator , @xmath58 , approximating @xmath59{\\psi(x_\\infty(t))}}}$ ] with minimal work , i.e. , we wish to satisfy the constraint @xmath60}}\\      \\le \\epsilon\\ ] ] for a given error tolerance , @xmath4 , and a given confidence level determined by @xmath61 .",
    "we instead impose the following , more restrictive , two constraints : @xmath62{\\mathcal{a}-\\phi_\\infty}}}| \\leq ( 1-\\theta){{\\mathrm{tol } } } , \\\\",
    "\\notag \\text{\\bf statistical constraint : } & & { { \\ensuremath{\\mathbb{p}}\\mspace{-2mu}\\left[|\\mathcal{a } - { { { \\ensuremath{\\mathrm{e}}}\\mspace{-2mu}\\en*[]{\\mathcal{a}}}}| \\geq \\theta { \\mathrm{tol}}\\right ] } } \\le \\epsilon ,    \\end{aligned}\\ ] ] for a given tolerance splitting parameter , @xmath63 , possibly a function of @xmath4 . assuming ( at least asymptotic ) normality of the estimator , @xmath58",
    ", we replace the statistical constraint by a constraint on the variance and write : @xmath64 } } \\leq                                         \\op * { \\frac{\\theta { \\mathrm{tol}}}{c_\\epsilon } } ^2 .",
    "\\end{aligned}\\ ] ] here , @xmath65 is such that @xmath66 , where @xmath67 is the cumulative distribution function of a standard normal random variable .",
    "the asymptotic normality of the estimator is usually shown using some form of the central limit theorem ( clt ) or the lindeberg - feller theorem ( see , e.g. , @xcite for clt results for the mlmc and mimc estimators and figure  [ fig : error_vs_tol]-_right _ ) .",
    "as previously mentioned , we wish to approximate the values of @xmath35 by using with a finite number of particles , @xmath8 . for a given number of particles , @xmath8 ,",
    "a solution to is not readily available .",
    "instead , we have to discretize the system of sdes using , for example , the euler - maruyama time - stepping scheme with @xmath7 time steps . for @xmath68 , @xmath69",
    "where @xmath70 are i.i.d . for the remainder of this work",
    ", we use the notation @xmath71 at this point , we make the following assumptions : @xmath72{\\phi_p^n -    \\psi(x_{\\infty } ) } } } } \\leq \\abs*{{{{\\ensuremath{\\mathrm{e}}}\\mspace{-2mu}\\en*[]{\\psi(x_{\\cdot | p}^{n|n } ) - \\psi(x_{\\cdot | p } ) } } } }    + \\abs*{{{{\\ensuremath{\\mathrm{e}}}\\mspace{-2mu}\\en*[]{\\psi(x_{\\cdot | p } ) - \\psi(x_{\\infty } ) } } } } & \\lesssim n^{-1 } + p^{-1 } , \\\\ \\label{eq",
    ": p_var_assumption } \\tag{\\textbf{p2 } }   { { \\ensuremath{\\mathrm{var}}\\mspace{-2mu}\\left[\\phi_p^n\\right ] } } & \\lesssim p^{-1}.\\end{aligned}\\ ] ] the first assumption relates to weak convergence .",
    "indeed , the weak convergence of the euler - maruyama method with respect to the number of time steps is a standard result shown , for example , in @xcite by assuming 4-time differentiablity of @xmath26 and @xmath73 . showing that the constant multiplying @xmath74 is bounded for all @xmath8",
    "is straightforward by extending the standard proof of weak convergence the euler - maruyama method in ( * ? ? ?",
    "* chapter 14 ) and assuming boundedness of @xmath26 and @xmath73 . on the other hand",
    ", the weak convergence with respect to the number of particles and assumption   will be verified numerically later . in general",
    "these assumptions also translate to smoothness and boundedness assumptions on @xmath26 and @xmath73 .",
    "finally , we remark that , with a naive method , the total cost to compute a single sample of @xmath75 is @xmath76 .",
    "the quadratic power of @xmath8 can be reduced by using , for example , a multipole algorithm . in general",
    ", we consider the work required to compute one sample of @xmath77 as @xmath78 for a positive constant , @xmath79 .",
    "in this section we study different monte carlo methods that can be used to estimate the previous quantity , @xmath80 . in the following ,",
    "we use the notation @xmath81 where , for each @xmath82 , @xmath83 denotes the @xmath84th sample of the set of underlying random variables that are used in calculating @xmath85 , i.e. , the wiener path , @xmath86 , the initial condition , @xmath87 , and any random variables that are used in @xmath26 or @xmath73 .",
    "moreover , we sometimes write @xmath88 to emphasize the dependence of the @xmath89th sample of @xmath77 on the underlying random variables .      the first estimator that we look at is a monte carlo estimator . for a given number of samples , @xmath90 , number of particles , @xmath8 , and number of time steps , @xmath7 , we can write the mc estimator as follows : @xmath91 here , @xmath92{\\mathcal a_{\\text{mc}}(m , p , n ) } } } = { { { \\ensuremath{\\mathrm{e}}}\\mspace{-2mu}\\en*[]{\\phi_p^n } } } = \\frac{1}{p}\\sum_{p=1}^p { { { \\ensuremath{\\mathrm{e}}}\\mspace{-2mu}\\en*[]{\\psi(x_{p|p}^{n|n } ) } } } = { { { \\ensuremath{\\mathrm{e}}}\\mspace{-2mu}\\en*[]{\\psi(x_{\\ \\cdot|p}^{n|n})}}},\\\\ \\text{and}\\quad & { { \\ensuremath{\\mathrm{var}}\\mspace{-2mu}\\left[\\mathcal a_{\\text{mc}}(m , p , n)\\right ] } } = \\frac{{{\\ensuremath{\\mathrm{var}}\\mspace{-2mu}\\left[\\phi_p^n\\right]}}}{m},\\\\ \\text{while the total work is}\\quad & { \\textnormal{work}\\left[\\mathcal a_{\\text{mc}}(m ,    p , n)\\right ] } = m p^{{{\\ensuremath{\\widehat \\gamma } } } } n. \\endaligned\\ ] ] hence , due to",
    ", we must have @xmath93 and @xmath94 to satisfy , and , due to , we must have @xmath95 to satisfy .",
    "based on these choices , the total work to compute @xmath96 is @xmath97 } = { { \\ensuremath { \\mathcal o\\left ( { \\mathrm{tol}}^{-2 -{{\\ensuremath{\\widehat \\gamma } } } } \\right ) } } } .\\ ] ]    using a naive calculation method of @xmath75 ( i.e. , @xmath98 ) gives a work complexity of @xmath99 .",
    "see also table  [ tbl : rates ] for the work complexities for different common values of @xmath100 .      for a given @xmath101 ,",
    "define two hierarchies , @xmath102 and @xmath103 , satisfying @xmath104 and @xmath105 for all @xmath106 .",
    "then , we can write the mlmc estimator as follows : @xmath107 where we later choose the function @xmath108 such that @xmath109 and @xmath110{\\varphi_{p_{\\ell-1}}^{n_{\\ell-1}}}}}= { { { \\ensuremath{\\mathrm{e}}}\\mspace{-2mu}\\en*[]{\\phi_{p_{\\ell-1}}^{n_{\\ell-1}}}}},$ ] so that @xmath36{\\mathcal a_{\\textnormal{mlmc } } } } } = { { { \\ensuremath{\\mathrm{e}}}\\mspace{-2mu}\\en*[]{\\phi_{p_l}^{n_l}}}}$ ] due to the telescopic sum . for mlmc to have better work complexity than that of monte carlo",
    ", @xmath111 and @xmath112 must be correlated for every @xmath106 and @xmath84 , so that their difference has a smaller variance than either @xmath111 or @xmath112 for all @xmath113 .",
    "given two discretization levels , @xmath114 and @xmath115 , with the same number of particles , @xmath8 , we can generate a sample of @xmath116 that is correlated to @xmath117 by taking @xmath118 that is , we use the same samples of the initial values , @xmath119 , the same wiener paths , @xmath120 , discretized with @xmath115 steps , and , in case they are random as in , the same samples of the advection and diffusion coefficients , @xmath26 and @xmath73 , respectively . in case the diffusion coefficient , @xmath73 , is not a constant , we can improve the correlation by using an antithetic sampler as detailed in @xcite or by using a higher - order scheme like the milstein scheme @xcite . in the kuramoto example , the euler - maruyama and the milstein schemes are equivalent because the diffusion is constant . on the other hand , given two different sizes of the particle system , @xmath121 and @xmath122 , with the same discretization level , @xmath7 , we can generate a sample of @xmath123 that is correlated to @xmath124 by taking @xmath125 in other words , we use the same @xmath122 noise paths out of the total @xmath121 noise paths to run an independent simulation of the stochastic system with @xmath122 particles .",
    "we also consider another estimator that is more correlated with @xmath124 .",
    "the `` antithetic '' estimator was first independently introduced in ( * ? ? ?",
    "* chapter 5 ) and @xcite and subsequently used in other works on particle systems @xcite and nested simulations @xcite . in this work",
    ", we call this estimator a `` partitioning '' estimator to clearly distinguish it from the antithetic estimator in @xcite .",
    "we assume that @xmath126 for all @xmath106 and some positive integer @xmath127 and take @xmath128 that is , we split the underlying @xmath121 sets of random variables into @xmath129 identically distributed and independent groups , each of size @xmath122 , and independently simulate @xmath129 particle systems , each of size @xmath122 .",
    "finally , for each particle system , we compute the quantity of interest and take the average of the @xmath129 quantities .    in the following subsections ,",
    "we look at different settings in which either @xmath121 or @xmath114 depends on @xmath106 while the other parameter is constant for all @xmath106 .",
    "we begin by recalling the optimal convergence rates of mlmc when applied to a generic random process , @xmath130 , with a trivial generalization to the case when there are two discretization parameters : one that is a function of the level , @xmath106 , and the other , @xmath131 , that is fixed for all levels .",
    "[ thm : mlmc_complexity ] let @xmath132 be an approximation of @xmath130 for every @xmath133 .",
    "consider the mlmc estimator @xmath134 with @xmath135 and for @xmath136 where @xmath137 and @xmath138 , assume the following    1 .",
    "@xmath139{y - y_{\\mathscr l , \\ell } } } } } \\lesssim \\exp(-\\widetilde w    \\mathscr l ) + \\exp(-w \\ell ) $ ] 2 .",
    "@xmath140 } } \\lesssim    \\exp(-\\widetilde c",
    "\\mathscr l ) \\exp(-s \\ell)$ ] 3 .",
    "@xmath141 } \\lesssim   \\exp(\\widetilde",
    "\\gamma \\mathscr l ) \\exp(\\gamma \\ell)$ ] .    then , for any @xmath142 , there exists @xmath143 and a sequence of @xmath144 such that and are satisfied and @xmath145 } \\lesssim \\begin{cases }      { \\mathrm{tol}}^{- 2 - \\p*{\\frac{\\widetilde \\gamma}{\\widetilde w}-\\widetilde c } } & \\text{if } s > \\gamma      \\\\      { \\mathrm{tol}}^{- 2 - \\p*{\\frac{\\widetilde \\gamma}{\\widetilde w}-\\widetilde c } } \\log\\op*{{\\mathrm{tol}}^{-1}}^2 & \\text{if } s = \\gamma      \\\\      { \\mathrm{tol}}^{- 2 - \\p*{\\frac{\\widetilde \\gamma}{\\widetilde w}-\\widetilde c } - \\frac{\\gamma - s}{w } } & \\text{if } s < \\gamma .",
    "\\end{cases}\\ ] ]    straightforward derivation from the proof of ( * ? ? ? * theorem 1 ) .      in this setting",
    ", we take @xmath146 for some @xmath147 and @xmath148 for all @xmath106 , i.e. , the number of particles is a constant , @xmath149 , on all levels .",
    "we make an extra assumption in this case , namely : @xmath150 } } \\lesssim p_l^{-1 } n_{\\ell}^{-{{\\ensuremath{\\overline s } } } } =    p_l^{-1 } \\op{{{\\ensuremath{\\overline \\beta}}}}^{-{{\\ensuremath{\\overline s}}}\\ell}.\\ ] ] the factor @xmath151 is the usual assumption on the variance convergence of the level difference in mlmc theory @xcite and is a standard result for the euler - maruyama scheme @xcite . on the other hand ,",
    "the factor @xmath152 can be motivated from , which states that the variance of each term in the difference converges at this rate . due to theorem  [ thm : mlmc_complexity ] ,",
    "we can conclude that the work complexity of mlmc is @xmath153 } \\lesssim \\begin{cases }      { \\mathrm{tol}}^{- 1   - { { \\ensuremath{\\widehat \\gamma } } } } & \\text{if } { { \\ensuremath{\\overline s } } } > 1      \\\\",
    "{ \\mathrm{tol}}^{- 1 - { { \\ensuremath{\\widehat \\gamma } } } } \\log\\op*{{\\mathrm{tol}}^{-1}}^2 & \\text{if } { { \\ensuremath{\\overline s}}}= 1      \\\\      { \\mathrm{tol}}^{- 2 - { { \\ensuremath{\\widehat \\gamma}}}+ { { \\ensuremath{\\overline s } } } }      & \\text{if } { { \\ensuremath{\\overline s } } } < 1 . \\end{cases}\\ ] ]    in this example , since the diffusion coefficient is constant , we have @xmath154 ( see figure  [ fig : mlmc_rates ] ) , and a naive calculation method of @xmath75 ( @xmath98 ) gives a work complexity of @xmath155 .",
    "see also table  [ tbl : rates ] for the work complexities for different common values of @xmath156 and @xmath100 .      in this setting , we take @xmath157 for some @xmath158 and @xmath159 for all @xmath106 , i.e. , we take the number of time steps to be a constant , @xmath160 , on all levels .",
    "we make an extra assumption in this case : @xmath161 } } \\lesssim p_{\\ell}^{-{{\\ensuremath{\\widehat s}}}-1 }    = { { \\ensuremath{\\widehat \\beta}}}^{-\\ell({{\\ensuremath{\\widehat s}}}+1)},\\ ] ] for some constant @xmath162 .",
    "the factor @xmath163 is the usual assumption on the variance convergence of the level difference in mlmc theory @xcite and is shown in , e.g. , @xcite for a certain class of particle systems . on the other hand ,",
    "the factor @xmath164 can be motivated from , since the variance of each term in the difference is converging at this rate . due to theorem  [ thm : mlmc_complexity ]",
    ", we can conclude that the work complexity of mlmc in this case is @xmath153 } \\lesssim \\begin{cases }      { \\mathrm{tol}}^{- 3 } & \\text{if } { { \\ensuremath{\\widehat s}}}+1 > { { \\ensuremath{\\widehat \\gamma}}}\\\\      { \\mathrm{tol}}^{- 3 } \\log\\op*{{\\mathrm{tol}}^{-1}}^2 & \\text{if } { { \\ensuremath{\\widehat s}}}+1 = { { \\ensuremath{\\widehat \\gamma}}}\\\\      { \\mathrm{tol}}^{- 2 - { { { \\ensuremath{\\widehat \\gamma}}}+{{\\ensuremath{\\widehat s } } } } }      & \\text{if } { { \\ensuremath{\\widehat s}}}+1 < { { \\ensuremath{\\widehat \\gamma}}}. \\end{cases}\\ ] ]    using a naive calculation method of @xmath75 ( @xmath98 ) , we distinguish between the two samplers :    * using the sampler @xmath165 in  , we verify numerically that @xmath166 ( cf .",
    "figure  [ fig : mlmc_rates ] ) .",
    "hence , the work complexity is @xmath167 which is the same work complexity as a monte carlo estimator .",
    "this should be expected since using the `` correlated '' samples of @xmath168 and @xmath169 do not reduce the variance of the difference , as figure  [ fig : mlmc_rates ] shows . * using the partitioning estimator , @xmath170 , in  , we have @xmath171 ( see figure  [ fig : mlmc_rates ] ) . hence , the work complexity is @xmath172 . here",
    "the samples of @xmath173 have higher correlation to corresponding samples of @xmath169 , thus reducing the variance of the difference .",
    "still , using mlmc with hierarchies based on the number of times steps ( fixing the number of particles ) yields better work complexity .",
    "see also table  [ tbl : rates ] for the work complexities for different common values of @xmath156 and @xmath100 .      in this case , we vary both the number of particles and the number of time steps across mlmc levels .",
    "that is , we take @xmath157 and @xmath146 for all @xmath106 . in this case , a reasonable assumption is    @xmath174 } } \\lesssim    { { \\ensuremath{\\widehat \\beta}}}^{-\\ell } \\op*{\\max({{\\ensuremath{\\widehat \\beta}}}^{-{{\\ensuremath{\\widehat s } } } } , { { \\ensuremath{\\overline \\beta}}}^{-{{\\ensuremath{\\overline s}}}})}^\\ell.\\ ] ]    the factor @xmath175 can be motivated from   since the variance of each term in the difference is converges at this rate . on the other hand , @xmath176 is the slower convergence of   and  .",
    "due to theorem  [ thm : mlmc_complexity ] and defining @xmath177 we can conclude that the work complexity of mlmc is @xmath153 } \\lesssim \\begin{cases }      { \\mathrm{tol}}^{- 2 } & \\text{if }       s > \\gamma      \\\\      { \\mathrm{tol}}^{- 2 }   \\log\\op*{{\\mathrm{tol}}^{-1}}^2 & \\text{if } s = \\gamma      \\\\      { \\mathrm{tol}}^{- 2- \\frac{\\gamma - s}{w } }      & \\text{if } s < \\gamma .",
    "\\end{cases}\\ ] ]    we choose @xmath178 and use a naive calculation method of @xmath75 ( yielding @xmath98 ) and the partitioning sampler ( yielding @xmath179 ) .",
    "finally the constant diffusion coefficient gives @xmath180 .",
    "refer to figure  [ fig : mlmc_rates ] for numerical verification .",
    "based on these rates , we have @xmath181 and @xmath182 .",
    "the mlmc work complexity in this case is @xmath183 see also table  [ tbl : rates ] for the work complexities for different common values of @xmath156 and @xmath100 .",
    "following @xcite , for every @xmath184 , let @xmath185 and @xmath186 and define the first - order mixed - difference operator in two dimensions as @xmath187 with @xmath188 and @xmath189 .",
    "the mimc estimator is then written for a given @xmath190 as @xmath191 at this point , similar to the original work on mimc @xcite , we make the following assumptions on the convergence and cost of @xmath192 , namely @xmath193{{{\\ensuremath{{\\boldsymbol \\delta } } } } \\phi_{p_{\\alpha_1}}^{n_{\\alpha_2 } } } } } \\lesssim    p_{\\alpha_1}^{-1 } n_{\\alpha_2}^{-1}\\\\ \\label{eq : mip_var_assumption}\\tag{\\textbf{mimc2}}{{\\ensuremath{\\mathrm{var}}\\mspace{-2mu}\\left[{{\\ensuremath{{\\boldsymbol \\delta } } } } \\phi_{p_{\\alpha_1}}^{n_{\\alpha_2}}\\right ] } } \\lesssim p_{\\alpha_1}^{-{{\\ensuremath{\\widehat s}}}-1 } n_{\\alpha_2}^{-{{\\ensuremath{\\overline s}}}}.\\ ] ] assumption is motivated from by assuming that the mixed first order difference , @xmath194 , gives a product of the convergence terms instead of a sum .",
    "similarly , is motivated from and . to the best of our knowledge , there are currently no proofs of these assumptions for particle systems , but we verify them numerically for   in figure  [ fig : mimc_rates ] .",
    "henceforth , we will assume that @xmath195 for easier presentation .",
    "following ( * ? ? ?",
    "* lemma 2.1 ) and recalling the assumption on cost per sample , @xmath196 } \\lesssim p_{\\alpha_1}^{{{\\ensuremath{\\widehat \\gamma } } } } n_{\\alpha_2}^ { } $ ] , then , for every value of @xmath197 , the optimal set can be written as @xmath198 and the optimal computational complexity of mimc is @xmath199 , where @xmath200    here again , we use a naive calculation method of @xmath75 ( yielding @xmath98 ) and the partitioning sampler ( yielding @xmath179 ) .",
    "finally the constant diffusion coefficient gives @xmath180 .",
    "hence , @xmath201 , @xmath202 and @xmath203 } = { { \\ensuremath { \\mathcal o\\left ( { \\mathrm{tol}}^{-2 }      \\log\\op*{{\\mathrm{tol}}^{-1}}^2 \\right ) } } } . $ ] see also table  [ tbl : rates ] for the work complexities for different common values of @xmath156 and @xmath100 .",
    ".the work complexity of the different methods presented in this work in common situations , encoded as @xmath204 to represent @xmath205 .",
    "when appropriate , we use the partitioning estimator ( i.e. , @xmath206 ) . in general",
    ", mimc has always the best complexity .",
    "however , when @xmath207 mimc does not offer an advantage over an appropriate mlmc method . [ cols=\"<,^,^,^,^\",options=\"header \" , ]",
    "in this section we provide numerical evidence of the assumptions and work complexities that were made in the section  [ s : mc ] .",
    "this section also verifies that the constants of the work complexity ( which are not tracked ) are not significant for reasonable error tolerances .",
    "the results in this section were obtained using the ` mimclib ` software library @xcite .",
    "we compare the mlmc @xcite method in the setting that was presented in section  [ sec : mlmc_np ] and the mimc @xcite method that was presented in section  [ ss : mimc ] , using the corresponding algorithms that were outlined in their original work .",
    "these particular settings provide a straightforward way to check the bias of the estimator by checking the absolute value of the level differences in mlmc or the multi - index differences in mimc . on the other hand , checking the bias in the settings outlined in sections  [ ss : mc ] , [ sec : mlmc_n ] and  [ sec : mlmc_p ] is not as straightforward since determining the number of times steps and/or the number of particles estimator is not trivial .",
    "this makes a fair numerical comparison difficult .",
    "as such , we only verify the assumptions that are required for these methods .    in the results outlined below , we focus on the kuramoto example in , with the following choices : @xmath208 , @xmath209 , @xmath210 and @xmath211 for all @xmath34 .",
    "we also make the choice @xmath212 and @xmath213 for mlmc and @xmath214 and @xmath215 for mimc .",
    "figure  [ fig : mlmc_rates ] shows the absolute expectation and variance of the level differences for the different mlmc settings that were outlined in section  [ ss : mlmc ] .",
    "these figures verify assumptions , and  for the values @xmath180 and @xmath216 for the @xmath217 sampler in or the value @xmath179 for the @xmath218 sampler in .",
    "for the same parameter values , figure  [ fig : mimc_rates ] provides numerical evidence for assumptions   and .",
    "figure  [ fig : error_vs_tol]-_left _ shows the exact errors of both mlmc and mimc versus the prescribed tolerance .",
    "this plot shows that both methods estimate the quantity of interest up to the same error tolerance . while figure  [ fig : error_vs_tol]-_right _",
    "show that our assumption in section  [ sec : prob_set ] of the asymptotic normality of these estimators is well founded .",
    "the comparison of the work complexity in figure  [ fig : work_vs_tol ] is thus fair .",
    "the latter figure shows the performance improvement of mimc over mlmc and shows that the complexity rates that we derived in this work are reasonably accurate .     and",
    "@xmath213 , this plot shows , for , numerical evidence for assumption   ( _ left _ ) and assumptions  , ",
    "( _ right _ ) .",
    "from the _ right _ plot , we can confirm that @xmath219 .",
    "we can also deduce that using the @xmath217 sampler in yields @xmath216 ( i.e. , no variance reduction compared to @xmath220}}$ ] ) while using the @xmath218 sampler in yields @xmath179 . ]    ) and the partitioning sample of particle systems ( yielding @xmath206 ) . here",
    ", the number of particles is taken to be @xmath221 , while the number of time steps is taken to be @xmath222 . when considering a mixed difference ( i.e , @xmath223 ) , a higher rate of convergence is observed . ]     and  [ ss : mimc ] , respectively ) when applied to .",
    "_ left _ : the exact errors of the estimators .",
    "this plot shows that , up to the prescribed 95% confidence level , both methods approximate the quantity of interest to the same required tolerance , @xmath4 .",
    "_ right _ : a qq plot of the cumulative distribution function ( cdf ) of the value of both estimators with with @xmath224 , shifted by their mean and scaled by their standard deviation showing that both estimators , when appropriately shifted and scaled , are well approximated by a standard normal random variable.,title=\"fig : \" ]    and  [ ss : mimc ] , respectively ) when applied to .",
    "_ left _ : the exact errors of the estimators .",
    "this plot shows that , up to the prescribed 95% confidence level , both methods approximate the quantity of interest to the same required tolerance , @xmath4 .",
    "_ right _ : a qq plot of the cumulative distribution function ( cdf ) of the value of both estimators with with @xmath224 , shifted by their mean and scaled by their standard deviation showing that both estimators , when appropriately shifted and scaled , are well approximated by a standard normal random variable.,title=\"fig : \" ]    ) and mimc ( detailed in section  [ ss : mimc ] ) when applied to . for sufficiently small tolerances , the running time closely follows the predicted theoretical rates ( also plotted ) and shows the performance improvement of mimc.,title=\"fig : \" ] ) and mimc ( detailed in section  [ ss : mimc ] ) when applied to . for sufficiently small tolerances",
    ", the running time closely follows the predicted theoretical rates ( also plotted ) and shows the performance improvement of mimc.,title=\"fig : \" ]",
    "this work has shown both theoretically and numerically under certain assumptions , that could be verified numerically , the improvement of mimc over mlmc when used to approximate a quantity of interest computed on a particle system as the number of particles goes to infinity .",
    "the application to other particle systems ( or equivalently other mckean - vlasov sdes ) is straightforward and similar improvements are expected .",
    "the same machinery was also suggested for approximating nested expectations in @xcite and the analysis here applies to that setting as well .",
    "moreover , the same machinery , i.e. , multi - index structure with respect to time steps and number of particles coupled with a partitioning estimator , could be used to create control variates to reduce the computational cost of approximating quantities of interest on stochastic particle systems with a finite number of particles .",
    "future work includes analyzing the optimal level separation parameters , @xmath127 and @xmath225 , and the behavior of the tolerance splitting parameter , @xmath226 .",
    "another direction could be applying the mimc method to higher - dimensional particle systems such as the crowd model in @xcite . on the theoretical side ,",
    "the next step is to prove the assumptions that where postulated in this work for certain classes of particle systems , namely : the bias assumption  , the variance assumption  , the second order convergence of the variance of the partitioning estimator   with @xmath206 and the mimc rates for mixed differences   and   .",
    "finally , the same methodology with a partitioning estimator could be applied to other multi - index samplers such as multi - index quasi monte carlo ( miqmc ) @xcite .",
    "r. tempone is a member of the kaust strategic research initiative , center for uncertainty quantification in computational sciences and engineering .",
    "r. tempone received support from the kaust crg3 award ref : 2281 and the kaust crg4 award ref:2584 .",
    "improved multilevel monte carlo convergence using the milstein scheme . in _",
    "monte carlo and quasi - monte carlo methods 2006 _ , a.  keller , s.  heinrich , and h.  niederreiter , eds .",
    "springer berlin heidelberg , 2008 , pp ."
  ],
  "abstract_text": [
    "<S> we address the approximation of functionals depending on a system of particles , described by stochastic differential equations ( sdes ) , in the mean - field limit when the number of particles approaches infinity . </S>",
    "<S> this problem is equivalent to estimating the weak solution of the limiting mckean - vlasov sde . to that end </S>",
    "<S> , our approach uses systems with finite numbers of particles and an euler - maruyama time - stepping scheme . in this case </S>",
    "<S> , there are two discretization parameters : the number of time steps and the number of particles . </S>",
    "<S> based on these two parameters , we consider different variants of the monte carlo and multilevel monte carlo ( mlmc ) methods and show that , in the best case , the optimal work complexity of mlmc to estimate the functional in one typical setting with an error tolerance of @xmath0 is @xmath1 . we also consider a method that uses the recent multi - index monte carlo method and show an improved work complexity in the same typical setting of @xmath2 when using a partitioning estimator . </S>",
    "<S> our numerical experiments are carried out on the so - called kuramoto model , a system of coupled oscillators .    </S>",
    "<S> * keywords : * multi - index monte carlo , multilevel monte carlo , monte carlo , particle systems , mckean - vlasov , mean - field , stochastic differential equations , weak approximation , sparse approximation , combination technique     * class : * 65c05 ( monte carlo methods ) , 65c30 ( stochastic differential and integral equations ) , 65c35 ( stochastic particle methods ) </S>"
  ]
}