{
  "article_text": [
    "owing to a combination of an improved toolset of simulational machinery and methods of data analysis and the exponential increase in available computer power observed over the past four decades , computer simulations such as the monte carlo method have at least drawn level with the more traditional perturbative approaches for studying a plethora of problems in statistical physics @xcite , ranging from critical phenomena @xcite over the physics of disordered systems @xcite to soft matter and biological problems @xcite .",
    "this success notwithstanding , a range of notoriously hard problems appear to create an insatiable appetite for more powerful computational devices to finally settle a number of long - standing questions . among such problems",
    "are , for instance , the quest of understanding the nature of the spin glass phase @xcite or the protein folding problem . to achieve results beyond the reach of the available standard computational resources of the time",
    ", there has been a tradition of designing special purpose computers , e.g. , for calculations in lattice field theory @xcite or the simulation of spin models @xcite .",
    "since the design and programming of such dedicated machines regularly requires a large effort in terms of monetary and human resources , recently scientists have started to adopt the use of graphics processing units for general purpose computational tasks in the hope of harvesting their nominally vast computational power , on par with some devices based on fpgas , without the need of time - consuming work at and near the hardware level @xcite . by design , gpus",
    "are optimized for manipulating a large number of graphics primitives in parallel , which often amounts to simple , floating - point matrix calculations .",
    "in contrast to current cpus , they are not designed to cope with `` unexpected '' branches in the code , or for executing a single - threaded program as fast as possible . while this makes gpus not well suited as drop - in replacements for cpus for interactive computing , their highly parallel architecture",
    "might well be taken advantage of in scientific calculations with an often high degree of vectorizable or parallelizable code .",
    "their original design for graphics calculations , however , entails certain design features which are not necessarily optimal for scientific computational tasks , such as a special hierarchy of memory organization or a restriction to ( efficient ) floating - point calculations only in single precision arithmetics , which only has been alleviated in the very latest generation of cards .",
    "while the first applications of general purpose computing on gpus were performed directly in graphics programming languages such as opengl @xcite , access to these devices for scientific applications has been considerably simplified with the advent of language extensions such as nvidia cuda @xcite and opencl @xcite for performing general purpose computing on gpus .",
    "the application presented here was coded on the nvidia architecture using the cuda framework , which is a high - level extension to the c language family .",
    "figure [ fig : hardware ] shows a schematic representation of the nvidia gpus used in the work presented here .",
    "a gpu consists of a number of multiprocessors , each composed of a number of single processing units which concurrently work with the same code on different parts of a common data set . of utmost importance to the efficient performance of gpu programs",
    "is the organization of gpu memory , which comes in a number of flavors :    * _ registers _ : each multiprocessor is equipped with several thousand registers , access to which is local to each processing unit and extremely fast . * _ shared memory _ : the processors combined in a multiprocessor have access to a small amount ( @xmath0 kb for tesla cards and @xmath1 kb for the fermi architecture ) of shared memory , which serves as a means of synchronization and communication between the threads in a block .",
    "this memory resides on - chip and can be accessed essentially without significant memory latency . * _ global memory _ :",
    "this large amount of memory ( currently up to @xmath2 gb ) is on separate dram chips and can be accessed by each thread on each multiprocessor .",
    "access suffers from a latency of several hundred clock cycles . * _ constant and texture memory _ : these memory areas are of the same speed as global memory , but they are cached such that read access can be very fast . from device perspective",
    "they are essentially read - only . *",
    "_ host memory _ : the memory of the host cpu unit can not be accessed from inside gpu calculations .",
    "memory transfers between global and device memory are important for communication with the `` outside world '' .",
    "additionally , the recent fermi architecture provides certain cache memories , but since the previous tesla architecture is used in the present work , i do not discuss them here . in the cuda framework ,",
    "calculations are organized to match the layout of the hardware : each multiprocessor executes ( part of ) a _ block _ of threads concurrently , while the different blocks of a _ grid _ are assigned to separate multiprocessors . to alleviate the large latency ( in terms of clock cycles ) of global memory accesses , in an ideal setup",
    "there are many more threads in total than available processors , such that a different ( part of a ) thread block can be scheduled for execution while the threads of a given block wait for memory fetches or writes .    for maximum performance ,",
    "implementations of scientific calculations have to take these characteristics into account and , in particular , should ideally meet the following design goals :    1 .   a large degree of locality of the calculations , reducing the need for communication between threads 2 .   a large coherence of calculations with a minimum occurrence of divergence of the execution paths of different threads 3 .   a total number of threads significantly exceeding the number of available processing units 4 .",
    "a large overhead of arithmetic operations and shared memory accesses over global memory accesses",
    "as a typical application in statistical physics , i studied the single - spin flip metropolis @xcite simulation of a nearest - neighbor , ferromagnetic ising model with hamiltonian @xmath3 on square and simple cubic lattices of edge length @xmath4 , using periodic boundary conditions .",
    "a proposed flip of spin @xmath5 is accepted with the metropolis probablity @xmath6,\\ ] ] such that the updating decision can be drawn solely upon examining the states of spin @xmath5 and its four ( in 2d ) resp .",
    "six ( in 3d ) neighbors .",
    "hence , the necessary calculations can be made local and highly parallel by using lattice decompositions of the checkerboard type .",
    "the authors of ref .",
    "@xcite used a single checkerboard decomposition , working on strips or columns of the lattice . since",
    "this setup does not take the hierarchical memory organization into account , the spin field needs to reside in global memory at all times , such that memory accesses are very costly . here",
    ", instead , i suggest to use a double checkerboard decomposition , whose organization is in line with the hierarchic layout of gpu memory : for the square - lattice system , on a first , `` coarse '' level , the lattice is divided into @xmath7 blocks . on a second , `` fine '' level , each block is decomposed , again in a checkerboard fashion , into @xmath8 sub - blocks .",
    "this is illustrated in fig .",
    "[ fig : checker ] . as a consequence of this decomposition ,",
    "each large tile of one of the two sub - lattices ( `` even '' and `` odd '' ) of the coarse decomposition can be updated independently , and for each tile under consideration all sites of one sub - lattice are again independent of each other .",
    "it is thus possible to load the configuration of spins of one of the coarse tiles into shared memory , including an extra surface layer of neighboring spins needed for calculating the local energy of spins in the considered tile , cf .  the shaded area in fig .",
    "[ fig : checker ] .",
    "this loading operation is distributed over the threads of a block , arranging memory accesses to achieve coalescence @xcite .    in total",
    ", the simulation thus proceeds as follows :    1 .",
    "a kernel is launched assigning all @xmath9 _ even _ tiles of the coarse checkerboard to a separate thread block , all of which are ( depending on the number of multiprocessors available in hardware ) executed in parallel . 2 .",
    "the @xmath10 threads of each thread block cooperatively load the spin configuration of their tile plus a boundary layer into shared memory .",
    "3 .   the threads of each block perform a metropolis update of each _ even _ lattice site in their tile in parallel .",
    "the threads of each block are synchronized , ensuring that all of them have completed the previous step .",
    "the threads of each block perform a metropolis update of each _ odd _ lattice site in their tile in parallel .",
    "the threads of each block are again synchronized .",
    "a second kernel is launched working on the @xmath9 _ odd _ tiles of the coarse checkerboard in the same fashion as for the even tiles .    in practice",
    ", the kernels for even and odd sub - lattices can be implemented as calls to the same kernel , using an extra offset parameter to distinguish sub - lattices . to leverage the effect of loading a tile s spin configuration into shared memory , a generalized multi - hit technique @xcite",
    "is employed for performing the simulations , where steps @xmath11@xmath12 above are repeated @xmath13 times . in this way",
    ", one sub - lattice of the coarse checkerboard is updated several times before updating the other sub - lattice .",
    "close to criticality , the generalized multi - hit approach leads to somewhat increased autocorrelation times @xcite , which reduces the overall efficiency of the implementation presented here in the vicinity of a critical point . in view of the existence of efficient cluster algorithms for this case @xcite",
    ", however , single - spin flip metropolis simulations are not the algorithm of choice for this situation , anyway .",
    "the code for the metropolis kernel formulated here is extremely simple , taking up only around 60 lines ( vs.  around 300 lines in the implementation presented in ref .",
    "it can be downloaded from the author s website @xcite .",
    "square lattice for parallel metropolis simulations on gpu .",
    "each of the @xmath14 big tiles is assigned as a thread block to a multiprocessor , whose individual processors work on one of the two sub - lattices of all @xmath15 sites of the tile in parallel.,scaledwidth=40.0% ]",
    "to actually perform the metropolis updates , a stream of pseudo - random numbers is required .",
    "it is clear that , for reasonable efficiency , each thread needs to have access to an independent ( sub-)stream of random numbers . for simplicity and the sake of comparison ,",
    "i here use an array of simple 32-bit linear congruential generators ( lcg ) with identical multipliers , but randomly chosen initial seeds for each thread @xcite .",
    "it is clear that in view of the short period @xmath16 of the generators , most of the different sequences will have significant overlap and , e.g. , in a simulation with @xmath17 monte carlo sweeps of a @xmath18 system about @xmath19 random numbers are used , significantly exceeding the period of the generator , and even more dramatically exceeding the value @xmath20 considered to be safe when using lcgs @xcite .",
    "somewhat surprisingly , for the 2d model all simulation data are consistent with the exact results for the internal energy and specific heat @xcite with this setup . on the contrary , when using an actually cleaner setup with _",
    "sub - sequences of the 32-bit lcg , and even when using disjoint sequences of an analogous 64-bit lcg with period @xmath21 , highly significant deviations are encountered . for high - precision real - world applications , therefore , i suggest to use different pseudo - random number generators , for instance of the lagged fibonacci type @xcite .",
    "the corresponding implementations will be discussed elsewhere @xcite .     using the double checkerboard decomposition and @xmath13-fold generalized multi - hit updates .",
    "gpu times are for a tesla c1060 device and cpu times for 3.0 ghz intel core 2 quad processors with 4 mb and 6 mb of cache , respectively.,scaledwidth=40.0% ]    for the 2d model , in fig .",
    "[ fig : times ] the times for performing a single spin flip are presented as a function of the linear system size @xmath4 .",
    "the time required for the measurement of elementary quantities such as the energy and magnetization is not included in these figures , since pure spin - flip times over the years have developed into a standard unit for comparing different architectures and implementations and thus allow to compare to a host of previous calculations .",
    "gpu calculations have been performed here on a tesla c1060 device with 4 gb of ram . by experimentation",
    ", for the considered system sizes @xmath22 , the optimal tile sizes are found to be @xmath23 for @xmath24 , @xmath25 for @xmath26 and @xmath27 for @xmath28 .",
    "using shared memory and the multi - hit technique , single spin flip times down to about @xmath29 ns can be achieved , significantly exceeding the performance reported in ref .",
    "when comparing these results to cpu calculations , the question arises whether multiple cpu cores should be taken into account @xcite .",
    "i refrain her from doing so , and use serial cpu code as the _ de facto _ standard of code used in most simulations on single cpus .",
    "the cpu code used in ref .",
    "@xcite was a one - to - one copy of the gpu code .",
    "just replacing it by code more suitable for serial execution already results in a speed - up by a factor of two .",
    "this observation , as well as the cache effect clearly visible in fig .",
    "[ fig : times ] as the size of the spin field of @xmath30 bytes reaches the size of the cache , indicate that speed - up factors are a rather fragile measure of gpu vs.cpu performance . trying a relatively fair comparison , using the somewhat optimized code on a cpu with sufficiently large cache , results in the speed - up factors presented in fig .",
    "[ fig : speedup ] . whereas compared to the cpu code used in ref .",
    "@xcite speed - ups of up to @xmath31 are observed , for the more realistic comparison used here , a maximal speed - up of around @xmath32 is reached ( vs.  a speed - up of around @xmath33 for the gpu code of ref .",
    "the double checkerboard decomposition proceeds in a completely analogous way for the case of the 3d ising model , and in this case we achieve a maximum performance of around @xmath34 ns per single spin flip with maximal speed - ups of almost @xmath35 compared to the corresponding cpu code for a @xmath36 system ( for this lattice size , the spin configuation is significantly larger than the cache memory if using regular integer variables for the spins ) .",
    "it is obvious that the chosen problem and implementation come rather close to meeting the design goals set out in sec .",
    "[ sec : hardware ] and thus constitute a quite ideal application .",
    "indeed , we achieve a total throughput in excess of 100 gflop / s from the chosen implementation which is at least of the same order of magnitude as the theoretical peak performance of @xmath37 gflop / s for the tesla c1060 card .",
    "the outlined approach easily generalizes to simulations of more general spin models , in particular models with continuous spins such as the heisenberg model , where the large efficieny of gpu devices with ( single - precision ) floating - point calculations comes into play . for the case of disordered models , parallelism is also possible by working on many disorder realizations concurrently . combining such approaches with ( asynchronous )",
    "multi - spin coding , we achieve a performance of around @xmath38 ps per single spin flip for the edwards - anderson ising spin glass . these and further extensions will be discussed in a separate publication @xcite .",
    "22 natexlab#1#1[2]#2 , , , , , edition , .",
    ", , , ( ) .",
    "( ed . ) , , , , .",
    ", ( eds . ) , , volume , , , . , , , in : ( ed . ) , , , , , p. . , , , , , , , , , , , , , , , , , , , , , , , , , ( ) . , , , , ( ) . , , , , , , , , , , , , , , , , , , , , , ( ) .",
    ", , , , , , ( ) .",
    ", , , , , , ( ) .",
    ", , , , , ( ) . , . , . , , , , , , ( ) . , , , ,",
    ", , , ( ) . . , , , , edition , .",
    ", , , ( ) .",
    ", , in : , , pp . . , , , , . ."
  ],
  "abstract_text": [
    "<S> over the last couple of years it has been realized that the vast computational power of graphics processing units ( gpus ) could be harvested for purposes other than the video game industry . </S>",
    "<S> this power , which at least nominally exceeds that of current cpus by large factors , results from the relative simplicity of the gpu architectures as compared to cpus , combined with a large number of parallel processing units on a single chip . to benefit from this setup for general computing purposes , the problems at hand need to be prepared in a way to profit from the inherent parallelism and hierarchical structure of memory accesses . in this contribution </S>",
    "<S> i discuss the performance potential for simulating spin models , such as the ising model , on gpu as compared to conventional simulations on cpu .    </S>",
    "<S> monte carlo simulations , gpu , spin models </S>"
  ]
}