{
  "article_text": [
    "in this paper , we consider the gaussian linear regression model , given by @xmath1 where @xmath2 is a @xmath3 vector of response variables , @xmath4 is a @xmath5 matrix of predictor variables , @xmath6 is a @xmath7 vector of slope coefficients , and @xmath8 is a @xmath3 vector of iid @xmath9 random errors .",
    "recently , there has been considerable interest in the high - dimensional case , where @xmath0 , driven primarily by challenging applications . indeed , in genetic studies , where the response variable corresponds to a particular observable trait , the number of subjects , @xmath10 , may be of order @xmath11 , while the number of genetic features , @xmath12 , in consideration can be of order @xmath13 . despite the large number of features ,",
    "usually only a few have a genuine association with the trait .",
    "for example , the @xcite has confirmed that only seven genes have a non - negligible association with type  i diabetes .",
    "therefore , it is reasonable to assume that @xmath6 is sparse , i.e. , only a few non - zero entries .",
    "given the practical importance of the high - dimensional regression problem , there is now a substantial body of literature on the subject . in the frequentist",
    "setting , a variety of methods are available based on minimizing loss functions , equipped with a penalty on the complexity of the model .",
    "this includes the lasso @xcite , the smoothly clipped absolute deviation @xcite , the adaptive lasso @xcite , and the dantzig selector @xcite .",
    "@xcite give a selective overview of these and other frequentist methods .",
    "from a bayesian perspective , popular methods for variable selection in high - dimensional regression include stochastic search variable selection @xcite and the methods based on spike - and - slab priors @xcite .",
    "these methods and others are reviewed in @xcite and @xcite .",
    "more recently , @xcite , @xcite , and @xcite propose bayesian variable selection methods and establish model selection consistency .",
    "any bayesian approach to the regression problem yields a posterior distribution on the high - dimensional parameter @xmath6 .",
    "it is natural to ask under what conditions will the @xmath6 posterior distribution concentrate around the true value at an appropriate or optimal rate .",
    "recently , @xcite show that , with a suitable laplace - like prior for @xmath6 , similar to those in @xcite , and under conditions on the design matrix @xmath4 , the posterior distribution concentrates around the truth at rates that match those for the corresponding lasso estimator ( e.g. , * ? ? ?",
    "these results leave room for improvement in at least two directions ; first , the rates associated with the lasso estimator are not optimal , so a break from the laplace priors ( and perhaps even the standard bayesian setup itself ) is desirable ; second , and perhaps most importantly , posterior computation with these inconvenient non - conjugate priors is expensive and non - trivial . in this paper , we develop a new approach , motivated by computational considerations , which leads to improvements in both directions , simultaneously .    towards a model that leads to more efficient computation ,",
    "it is natural to consider a conjugate normal prior for @xmath6 .",
    "however , theorem  2.8 in @xcite says that if the prior has normal tails , then the posterior concentration rates can be suboptimal , motivating a departure from the somewhat rigid bayesian framework .",
    "following @xcite , we consider a new empirical bayes approach , motivated by the very simple idea that the tails of the prior are irrelevant as long as its center is chosen informatively .",
    "so , our proposal is to use the data to provide an informative center for the normal prior for @xmath6 , along with an extra regularization step to prevent the posterior from tracking the data too closely .",
    "details of our proposed empirical bayes model are presented in section  [ s : model ] .",
    "it turns out that this new empirical bayes posterior is both easy to compute and has desirable asymptotic concentration properties .",
    "section  [ s : theory ] presents a variety of concentration rate results for our empirical bayes posterior .",
    "for example , under almost no conditions on the model or design matrix , a concentration rate relative to prediction error loss is obtained which is , at least in some cases , minimax optimal ; the optimal rate can be achieved in all cases , but at a cost ( see remark  [ re : minimax ] ) .",
    "furthermore , we provide a model selection consistency result which says , under optimal conditions , the empirical bayes posterior can asymptotically identify those truly non - zero coefficients in the linear model .",
    "our approach has some similarities with the exponential weighting methods in , e.g. , @xcite and @xcite ; in fact , ours can be viewed as a generalization of these approaches , defining a full posterior that , when suitably summarized , corresponds essentially to their estimators . in section  [",
    "s : numerical ] we propose a simple and efficient markov chain monte carlo method to sample from our empirical bayes posterior , and we present several simulation studies to highlight both the computational speed the superior finite - sample performance of our method compared to several others in terms of model selection .",
    "finally section  [ s : discuss ] gives a brief a discussion , the key message being that we get provable posterior concentration results , optimal in a minimax sense in some cases , fast and easy computation , and strong finite - sample performance .",
    "lengthy proofs and some auxiliary results are given in the appendix .",
    "here , and in the theoretical analysis in section  [ s : theory ] , we take the error variance @xmath14 to be known , as is often done ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "techniques for estimating @xmath14 in the high - dimensional case are available ; see section  [ s : numerical ] . to specify a prior for @xmath6 that incorporates sparsity , we decompose @xmath6 as @xmath15 , where @xmath16 denotes the `` active set '' of variables , @xmath17 , and @xmath18 is the @xmath19-vector containing the particular non - zero values .",
    "based on this decomposition , we can specify the prior for @xmath6 in two steps : a prior for @xmath20 and then a prior for @xmath18 , given @xmath20 .",
    "first , the prior @xmath21 for the model @xmath20 decomposes as follows : @xmath22 where @xmath23 is a probability mass function on the size @xmath19 of @xmath20 .",
    "that is , we assign a prior distribution @xmath23 on the model size and then , given the size , put a uniform prior on all models of the given size .",
    "some conditions on @xmath23 will be required for suitable posterior concentration . in particular , we assume that @xmath23 is supported on @xmath24 , not on @xmath25 , where @xmath26 is the the rank of the matrix @xmath4 ; see , also , @xcite , @xcite , @xcite , and @xcite .",
    "that is , @xmath27 our primary motivation for imposing this constraint is that in practical applications , the true value of @xmath28 ; i.e. @xmath29 , is typically much smaller than @xmath30 . even in the ideal case where @xmath31 is known , if @xmath32 , then quality estimation of the corresponding parameters is not possible .",
    "moreover , models containing a large number of variables can be difficult to interpret .",
    "therefore , since having no more variables than samples in the fixed - model case is a reasonable assumption , we do not believe that restricting the support of our prior for the model size is a strong condition .",
    "second , for the conditional prior on @xmath18 , given @xmath20 that satisfies @xmath33 , we propose to employ the available distribution theory for the least squares estimator @xmath34 .",
    "specifically , we take the prior for @xmath18 , given @xmath20 , as @xmath35 here , @xmath36 is the columns of @xmath4 corresponding to @xmath20 , and @xmath37 is a tuning parameter , to be specified .",
    "this is reminiscent to zellner s @xmath38-prior ( e.g. , * ? ? ?",
    "* ) , except that it is centered at the least squares estimator ; see section  [ ss : likelihood ] for more on this data - dependent prior centering . to summarize , our proposed prior @xmath39 for @xmath6 is given by @xmath40 following @xcite , we refer to this data - dependent prior as an empirical prior ; see section  [ ss : posterior ] . by restricting @xmath33",
    ", we can be sure that the least squares estimator @xmath34 is available , along with the usual distribution theory . in our implementation",
    ", @xmath41 will be large , which means that the conditional prior for @xmath18 is rather diffuse , so the dependence on the data , through @xmath34 , is not overly strong .",
    "obviously , to properly define the conditional prior for @xmath18 , we implicitly assume that @xmath42 is non - singular for all subsets @xmath20 with @xmath33 .",
    "this is only for simplicity , however , since the theory in section  [ s : theory ] goes through without this assumption at the cost of making computations more difficult .      for the likelihood function ,",
    "write @xmath43 as the @xmath10-dimensional gaussian density at @xmath2 , with mean @xmath44 , covariance matrix proportional to the identity matrix , and treated as a function of @xmath6 .",
    "one unique feature of our approach so far is the centering of the ( conditional ) prior on the least squares estimator , which is greedy , in some sense . to prevent the posterior from tracking the data too closely",
    ", the second feature of our proposed approach is that we introduce a fractional power @xmath45 on the likelihood .",
    "that is , instead of @xmath46 , our likelihood will be @xmath47 ; see @xcite .",
    "other authors have advocated the use of a fractional likelihood , including @xcite , @xcite , @xcite , @xcite , @xcite , and @xcite , but these papers have different foci and none include a data - dependent ( conditional ) prior centering .",
    "in fact , we feel that this combination of centering and fractional likelihood regularization ( see section  [ ss : posterior ] ) is a powerful tool that can be used for a variety of high - dimensional problems .",
    "our analysis in what follows does not go through for the genuine bayes case , corresponding to @xmath48 , but @xmath49 can be arbitrarily close to 1 .",
    "clearly , for finite - samples , the numerical differences between results for @xmath50 and for @xmath48 are negligible .      given the prior @xmath39 for @xmath6 and the fractional likelihood , we form an empirical bayes posterior distribution , denoted by @xmath51 , for @xmath6 using the standard bayesian update .",
    "that is , for @xmath52 a measurable subset of @xmath53 , we have @xmath54 computation of this empirical bayes posterior will be discussed in section  [ s : numerical ] .",
    "we interpret `` empirical bayes '' loosely  if the prior depends on data , then the corresponding posterior is empirical bayes .",
    "the combination of a prior , data - dependent or not , with a fractional likelihood via bayes formula can also be understood from this empirical bayes point of view .",
    "indeed , @xmath55 i.e. , the bayes combination of a fractional likelihood with a prior is equivalent to a bayes combination of the original likelihood function with a data - dependent prior .",
    "as @xcite explain , rescaling the prior by a portion of the likelihood helps to protect from possible inconsistencies by penalizing those parameter values that `` track the data too closely . ''",
    "before getting into details about the concentration rates , we first want to clarify what is meant by asymptotics in this context . there is an implicit triangular array setup , i.e. , for each @xmath10 , the response vector @xmath56 is modeled according to with the @xmath5 design matrix @xmath57 , of rank @xmath26 , which we take to be deterministic but depending on @xmath10 , and vector of coefficients @xmath58 .",
    "when @xmath10 is increased , more data is available so , even though there are more variables to contend with ( since @xmath0 ) , there is hope that something about the true @xmath59 can be learnt , provided that it is sufficiently sparse . in",
    "what follows , we will use the standard notation in which is less cumbersome but hides the triangular array formulation .",
    "it is important to keep in mind , however , that , throughout our analysis , @xmath12 , @xmath30 , and @xmath60 depend implicitly on @xmath10 .",
    "we make some minimal standing assumptions .",
    "first , without loss of generality , we can assume that @xmath61 .",
    "no other assumptions concerning @xmath10 , @xmath12 , @xmath30 , and @xmath60 will be required .",
    "the results below also hold for all fixed tuning parameters @xmath45 and @xmath37 ; see section  [ ss : implementation ] for guidance on the practical choice of @xmath62 . for the design matrix @xmath4",
    ", there is a standing simplifying assumption that we shall make .",
    "in particular , we assume that @xmath36 is full - rank for each @xmath20 satisfying @xmath33 .",
    "this assumption holds , for example , if @xmath4 satisfies the `` sparse riesz condition with rank @xmath10 '' discussed in @xcite and @xcite .",
    "it is possible , however , to remove this condition , but it requires a modification of the empirical bayes model . indeed ,",
    "if the prior @xmath63 for @xmath20 only puts positive mass on those @xmath20 such that @xmath36 is full - rank , and if @xmath64 is full - rank , then the theoretical results presented below follow similarly .",
    "the drawback for adjusting the prior for @xmath20 in this way is additional computational cost , i.e. , the less - than - full - rank models must be identified and removed by zeroing out the prior mass .",
    "we opt here to keep things simple by making the full - rank assumption .",
    "let @xmath52 be a generic event for @xmath65 .",
    "our empirical bayes posterior probability of the event @xmath52 in can be rewritten as @xmath66 where @xmath67 is the likelihood ratio .",
    "let @xmath68 denote the denominator in the above display , i.e. , @xmath69 .",
    "the next result , which will be useful throughout our analysis , gives a sure lower bound on @xmath68 .",
    "[ lem : denominator ] there exists @xmath70 such that @xmath71 .",
    "@xmath68 is an average of a non - negative @xmath20-dependent quantity with respect to @xmath21 .",
    "this average is clearly greater than the quantity for @xmath72 times @xmath73 .",
    "that is , @xmath74 direct calculation shows that the lower bound above equals @xmath75 using the trivial bound @xmath76 on the norm in the exponent , the proof is complete if we let @xmath77 , which is clearly positive .",
    "we now present a result characterizing the concentration rate of the posterior distribution for the mean @xmath44 .",
    "set @xmath78 where @xmath79 is a positive sequence to be specified .",
    "since this loss involves the @xmath4 matrix , the notion of convergence we are considering here is related to prediction .",
    "different loss functions will be considered in section  [ ss : other ] .",
    "as discussed in @xcite , e.g. , their equation ( 2.8 ) , @xmath79 proportional to @xmath80 corresponds to the convergence rate for the lasso estimator .",
    "intuitively , if @xmath31 were known , then the best rate for the prediction error would be @xmath60 , so the logarithmic term acts as a penalty for having to also deal with the unknown model .",
    "let @xmath81 be the numerator for the posterior probability of @xmath82 , as in , i.e. , @xmath83 .",
    "we have the following bound on @xmath81 .",
    "[ lem : numerator ] there exists @xmath84 and @xmath85 such that @xmath86 , uniformly in @xmath87",
    ".    see appendix  [ ss : proof.num ] .    to bound the posterior probability of @xmath82 ,",
    "let @xmath88 .",
    "since @xmath89 , surely , by lemma  [ lem : denominator ] , we have @xmath90 taking expectation and plugging in the bound in lemma  [ lem : numerator ] gives @xmath91 which holds uniformly in @xmath87 with @xmath92 .",
    "then the empirical bayes concentration rate @xmath93 is such that the above upper bound vanishes .",
    "a first conclusion is that @xmath79 must satisfy @xmath94 .",
    "more precisely , if we set @xmath95 then the rate @xmath79 satisfies @xmath96 this amounts to a condition on the prior @xmath97 for @xmath19 .",
    "indeed , requires that @xmath97 should be sufficiently concentrated near @xmath60 , so that @xmath98 is not too small and the expectation of @xmath99 with respect to @xmath97 is not too big .",
    "compare this to the prior support conditions in @xcite , @xcite , and @xcite .",
    "[ thm : mean.concentration ] for any @xmath100 , if the prior @xmath97 on @xmath19 admits @xmath101 such that holds with @xmath79 , then there exists a constant @xmath102 such that @xmath103 as @xmath104 , uniformly in @xmath87 with @xmath105 .    by lemmas  [ lem : denominator ] and [ lem : numerator ] , and the growth condition",
    ", we have that , for large @xmath10 , @xmath106 the first term inside the parentheses vanishes since @xmath94 . next , under , there exists a @xmath107 such that @xmath108 .",
    "so , if we take @xmath109 such that @xmath110 , then the upper bound above goes to @xmath111 as @xmath104 .",
    "this implies the result .",
    "[ re : minimax ] what rates @xmath79 are desirable / attainable ?",
    "the minimax rate for estimation under this prediction error loss is @xmath112 ; see , e.g. , @xcite .",
    "note the phase transition between the ordinary [ @xmath113 and the ultra high - dimensional [ @xmath114 regimes . according to remark  [ re : geometric ]",
    ", an empirical bayes posterior concentration rate equal to @xmath115 obtains for a class of priors on @xmath20 , which is minimax optimal but only in the ordinary high - dimensional regime ; this rate is slightly better than those obtained in @xcite and @xcite . by picking a prior outside this class , in particular , one that puts a little mass on an overly - complex model , the minimax rate can be achieved in both the ordinary and ultra high - dimensional regimes .",
    "there is a price to be paid , however , for this complete minimax rate : the little piece of extra prior mass on the complex model is large enough to cause problems with the proofs of marginal posterior concentration properties for @xmath20 .",
    "justification of these claims can be found in appendix  [ s : claims ] .",
    "based on these observations , we conjecture that the priors on @xmath20 that lead to minimax concentration rate under prediction error loss do not lead to desirable model selection properties .",
    "this is intuitively reasonable , since good prediction generally does not require a correctly specified model , but more work is needed to confirm this . since we prefer to have a single prior that does well in all aspects , we will not concern ourselves here with attaining the optimal minimax rate in the ultra high - dimensional regime , though we do know how to obtain it .    [",
    "re : geometric ] the growth condition holds with @xmath79 proportional to @xmath115 , the minimax rate in the ordinary high - dimensional case , if there exists constants @xmath116 , @xmath117 , @xmath118 , @xmath119 , @xmath120 , and @xmath121 such that @xmath97 satisfies @xmath122 the proof of this claim follows from calculations similar to those in example  [ ex : complexity ] below .",
    "assumption  1 in @xcite implies , but our restriction , @xmath33 , allows us to get rates for priors that may not satisfy .    [ re : phi ] consider the expectation term @xmath123 .",
    "the trivial bound @xmath124 could be used in the ultra high - dimensional case where @xmath125 .",
    "more generally , if @xmath97 satisfies , then the formulas for partial sums of a geometric series reveal that this expectation term is bounded as @xmath104 .",
    "in fact , in all three examples discussed below , it is easy to confirm that the expectation term is bounded .",
    "therefore , the rate is determined completely by the prior concentration around @xmath31 .",
    "next we identify the rate @xmath79 corresponding to several choices of prior @xmath97 .",
    "the complexity prior in example  [ ex : complexity ] , which is simple and has good properties , will be our choice of prior in what follows ; our proofs sections  [ ss : dimension][ss : selection ] can be easily modified to cover any @xmath97 that satisfies .",
    "[ ex : complexity ] the complexity prior for the model size @xmath19 in equation  ( 2.3 ) of @xcite is given by @xmath126 where @xmath127 and @xmath128 are positive constants .",
    "this prior clearly satisfies the condition in remark  [ re : geometric ] .",
    "we claim that this complexity prior satisfies with @xmath129 . to see this , note that @xmath130 is lower bounded by @xmath131 the ratio inside the parentheses above vanishes since @xmath132 .",
    "similarly , by stirling s formula , we have that @xmath133 .",
    "putting these two bounds together , and using the result in remark  [ re : phi ] , we can conclude that the complexity prior above yields a posterior concentration rate @xmath115 .",
    "[ ex : beta.binom ] convergence rates can be obtained for other priors @xmath97 . first , consider a beta ",
    "binomial prior for @xmath19 , i.e. , @xmath134 which corresponds to a @xmath135 prior for @xmath136 and a conditional @xmath137 prior for @xmath19 , given @xmath138 . for @xmath139 , for a constant @xmath140 , it can be shown that the corresponding rate @xmath79 is proportional to @xmath115 .",
    "if , on the other hand , @xmath97 is a @xmath141 mass function , then similar calculations show that the concentration rate is @xmath142 , which agrees with the lasso rate in @xcite , but falls short of the rates discussed previously .      under our proposed prior",
    ", the empirical bayes posterior distribution for @xmath6 is concentrated on an @xmath30-dimensional subspace of the full @xmath12-dimensional parameter space . in the sparse case , where the true @xmath87 has effective dimension @xmath143 , it is interesting to ask if the posterior distribution is actually concentrated on a space of dimension close to @xmath60 . below we give an affirmative answer to this question under some conditions .",
    "such considerations will also be useful in sections  [ ss : other ] and [ ss : selection ] .",
    "for a given @xmath144 , let @xmath145 be those @xmath6 vectors with no less than @xmath144 non - zero entries .",
    "we say that the effective dimension of @xmath51 is bounded by @xmath146 if the expected posterior probability of @xmath147 vanishes as @xmath104 .",
    "next write @xmath148 for the numerator of the posterior probability of @xmath147 .",
    "[ lem : dim ] @xmath149 for all @xmath87 .",
    "see appendix  [ ss : proof.dim ] .",
    "we can combine lemma  [ lem : dim ] and lemma  [ lem : denominator ] to conclude that @xmath150 \\leq e^{cs^\\star } \\frac{\\binom{p}{s^\\star}}{f_n(s^\\star ) } \\sum_{s = \\delta}^r { \\varphi}^s f_n(s),\\ ] ] uniformly in @xmath87 with @xmath105 . since @xmath151",
    ", we have @xmath152 and , therefore , @xmath153 \\leq e^{cs^\\star + \\log \\zeta_n } \\sum_{s=\\delta}^r { \\varphi}^s f_n(s).\\ ] ] so , if the tail of the prior @xmath97 on the model size is sufficiently light , then the posterior probability assigned to models with complexity of order greater than @xmath60 will be small . under the conditions of theorem  [ thm : mean.concentration ] ,",
    "we know the magnitude of @xmath154 , but here we need additional control on the tails of @xmath97 .",
    "[ thm : dim ] let @xmath100 .",
    "if @xmath97 is of the form , then @xmath155 \\to 0 $ ] , holds with @xmath156 , uniformly in @xmath87 with @xmath92 ,",
    "i.e. , the effective dimension @xmath51 is bounded by @xmath157 .",
    "recall that , for this @xmath97 , @xmath154 is of the order @xmath158 .",
    "moreover , for a generic @xmath144 , the summation @xmath159 is bounded by a partial sum of a geometric series .",
    "in particular , the bound is @xmath160 , where @xmath161 and @xmath162 are in . in that case ,",
    "@xmath163}.\\ ] ] so , if @xmath144 is a suitable multiple of @xmath60 , then clearly the @xmath164 term dominates the @xmath165 term .",
    "in particular , if @xmath166 with @xmath167 , then the product on the right - hand side of vanishes , proving the claim .    to summarize ,",
    "our prior is such that the posterior distribution is supported on models of size no more than @xmath30 .",
    "however , a good prior is one such that the posterior ought to be able to learn the size of the true model that generated the data , which is possibly much less than @xmath30 .",
    "theorem  [ thm : dim ] shows that , indeed , if the prior @xmath97 on the model size has sufficiently light tails , then the posterior will concentrate on models of size proportional to @xmath60 , the true model size .",
    "note , also , that we can not take a @xmath168 to replace @xmath60 in @xmath157 , since we would need @xmath169 which confirms this particular point .",
    "theorem  [ thm : mean.concentration ] concerns the empirical bayes posterior probability of sets of @xmath6 which are near the true @xmath87 relative to a distance depending on the design matrix @xmath4 .",
    "a natural question is if the empirical bayes posterior concentrates on neighborhoods of @xmath87 with respect to more other metrics , such as @xmath170- and @xmath171-norms .",
    "an affirmative answer will require further conditions on @xmath4 to separate @xmath6 from @xmath44 .",
    "in the low - dimensional case , with @xmath172 , we have @xmath173 where @xmath174 is the minimum eigenvalue of @xmath175 , which is positive if @xmath175 is non - singular .",
    "when @xmath0 , @xmath4 is not full rank and , therefore , the smallest eigenvalue of @xmath176 is zero , making the above inequality trivial and not useful .",
    "however , it is still possible to get something like the displayed inequality . towards this , define the function @xmath177 the quantity @xmath178 is called the `` smallest scaled sparse singular value of @xmath4 of dimension @xmath28 , '' similar to the quantity in equation  ( 11 ) of @xcite and that in definition  2.3 of @xcite .",
    "its main purpose is to facilitate conversion of @xmath171-norm concentration results for the mean vector @xmath44 to @xmath171-norm concentration results for @xmath6 itself .",
    "indeed , a result shown in ( * ? ? ?",
    "* lemma  1 ) is that a true @xmath179 with @xmath92 is identifiable if and only if @xmath180 consequently , @xmath181 is an important quantity and will appear in theorem  [ thm : beta.concentration ] below .",
    "one can define quantities analogous to @xmath181 in order to get concentration results relative to the @xmath170- or @xmath182-norm of @xmath6 ; see ( * ? ? ?",
    "* section  2 ) .",
    "the result presented below will follow almost immediately from theorem  [ thm : mean.concentration ] and the definition of @xmath181 . indeed , for any @xmath6 , we have @xmath183 for example , if @xmath184 is lower - bounded , then so is @xmath185 , for suitable @xmath181 , so a posterior concentration result for the @xmath171-norm on @xmath6 should follow from an analogous result for the @xmath171 prediction error as in theorem  [ thm : mean.concentration ] .",
    "the only obstacle is that the @xmath181 term on the right - hand depends on the particular @xmath6 .",
    "the following result leads to the observation that @xmath186 can be controlled by a term that depends only on @xmath60 .",
    "[ lem : kappa ] for any @xmath6 and @xmath87 , @xmath187 .",
    "this follows since @xmath181 is non - increasing and @xmath188 .    under our prior formulation ,",
    "we know that the posterior puts probability  1 on those @xmath6 for which @xmath189 .",
    "so , if @xmath105 , then , trivially , @xmath190 .",
    "for better control on the @xmath181 term in , recall that theorem  [ thm : dim ] says that the posterior probability of the event @xmath191 vanishes as @xmath104 .",
    "therefore , for @xmath192 , @xmath193 holds for all @xmath6 in a set with posterior probability approaching 1 .",
    "compare this to theorem  1 of @xcite , and also to the corresponding model selection results for frequentist point estimators in , e.g. , ( * ? ? ?",
    "* chap .  7 ) .",
    "we are now ready for the concentration rate result with respect to the @xmath171-norm loss on the parameter @xmath6 itself .",
    "this time , set @xmath194 where @xmath195 is a positive sequence to be specified .",
    "[ thm : beta.concentration ] for @xmath196 , suppose that the prior @xmath97 satisfies with exponent @xmath197 , so that theorem  [ thm : mean.concentration ] holds with @xmath79 equal to @xmath198 and theorem  [ thm : dim ] holds with @xmath199 , for @xmath167 .",
    "then there exists a constant @xmath109 such that @xmath200 as @xmath104 , uniformly in @xmath87 with @xmath92 , where @xmath201 provided that @xmath202 , where @xmath203 .",
    "it follows immediately from that @xmath204 implies @xmath205 by definition of @xmath195 and the inequality , this last inequality implies @xmath206 if we take @xmath109 as in theorem  [ thm : mean.concentration ] , then the event in the above display is exactly @xmath207 .",
    "we have shown that @xmath208 . by theorem  [ thm : mean.concentration ] , the expectation of the upper bound vanishes uniformly in @xmath87 as @xmath104 , so the proof is almost complete .",
    "the remaining issue to deal with is an extra term in the upper bound for @xmath209 coming from using @xmath210 in place of @xmath186 above .",
    "however , this extra term is @xmath211 by theorem  [ thm : dim ] , and , therefore , does not actually impact the proof .",
    "compare this result to the third in theorem  2 of @xcite .",
    "first , our rate is slightly better , @xmath115 compared to the lasso rate @xmath80 .",
    "second , our bound does not depend on a `` compatibility number '' ( e.g. , * ? ? ?",
    "* definition  2.1 ) , which also improves the rate and makes interpretation of our result easier .",
    "a referee has indicated that the improved results are as a direct consequence of the @xmath212 term that appears in the prior for @xmath18 .",
    "also , the condition @xmath213 , with @xmath214 and @xmath215 , agrees with the condition , roughly , @xmath216 for some @xmath217 , in @xcite ; that is , just a little more than identifiability , as in is needed .",
    "interest here is on the model @xmath20 and not directly on the regression coefficients . in this case , it is convenient to work with the marginal posterior distribution for @xmath20 which , thanks to the simple conjugate structure in the conditional prior , we can write explicitly as @xmath218 where @xmath219",
    ". then @xmath220 from this bound , we can show that the posterior concentrates on models contained in @xmath31 , i.e. , asymptotically , it will not charge any models with unnecessary variables .",
    "furthermore , this conclusion requires no conditions on the @xmath4 matrix or true @xmath87 . for simplicity",
    ", we will focus on the particular complexity prior @xmath97 in shown previously to yield desirable posterior concentration properties .",
    "[ thm : selection1 ] let the constant @xmath140 in the complexity prior be such that @xmath221 .",
    "then @xmath222 , uniformly over @xmath87 .",
    "see appendix  [ proof : selection1 ] .",
    "theorem  [ thm : selection1 ] says that , asymptotically , our empirical bayes posterior will not include any unnecessary variables .",
    "it remains to say what it takes for the posterior to asymptotically identify all the important variables .",
    "the first condition is one on the @xmath4 matrix , specifically , if @xmath60 is the true model size , then we require @xmath223 ; this is implied by monotonicity of @xmath181 and the identifiability condition in section  [ ss : other ] . for our second assumption",
    ", we consider the magnitudes of the non - zero entries in a @xmath60-sparse @xmath87 .",
    "intuitively , we can not hope to be able to distinguish between an actual zero and a very small non - zero , but defining what is `` very small '' requires some care . here , we define this cutoff by @xmath224 where @xmath102 is a constant to be determined .",
    "in particular , coefficients of magnitude greater than @xmath225 are large enough to be detected .",
    "the so - called _ beta - min _",
    "condition assumes that all the non - zero coefficients are sufficiently far from zero .",
    "the cutoff @xmath225 in is better than that appearing in equation  ( 2.18 ) in @xcite for the lasso model selector but comparable to that in theorem  1 of @xcite and in the third part of theorem  5 in @xcite , where the latter requires additional assumptions on @xmath4 .",
    "[ thm : selection2 ] for any @xmath100 , let @xmath87 be such that @xmath105 and @xmath226 with @xmath227 , where @xmath140 is in the complexity prior , with @xmath221 . assuming the condition of theorem  [ thm : dim ] holds , if @xmath223 , then @xmath228 .",
    "see appendix  [ proof : selection2 ] .",
    "to compute our empirical bayes posterior distribution , we employ a markov chain monte carlo method . to start , recall from",
    "that we can write the marginal posterior mass function , @xmath229 , for the model @xmath20 can be written down explicitly , i.e. , @xmath230 where @xmath231 is the least - squares prediction for model @xmath20 .",
    "intuitively , there are three contributing factors to the posterior distribution for @xmath20 , namely , the prior probability of the model , a measure of how well the model fits the data , and an additional penalty on the complexity of the model .",
    "so , clearly , the posterior distribution will favor models with smaller number of variables that provide adequate fit to the observed @xmath2 .",
    "this provides further intuition about theorems presented in section  [ s : theory ] .    besides this intuition , the formula @xmath229 provides a convenient way to run a rao  blackwellized metropolis  hastings method to sample from the posterior distribution of @xmath20 . indeed , if @xmath232 is a proposal function , then a single iteration of our proposed metropolis ",
    "hastings sampler goes as follows :    1 .",
    "given a current state @xmath20 , sample @xmath233 .",
    "move to the new state @xmath234 with probability @xmath235 otherwise , stay at state @xmath20 .    repeating this process @xmath109 times",
    ", we obtain a sample of models @xmath236 from the posterior @xmath229 .",
    "monte carlo approximations of , say , the inclusion probabilities ( section  [ ss : simulations ] ) of individual variables can then easily be computed based on this sample . in our case , we use a symmetric proposal distribution @xmath232 , i.e. , one that samples @xmath234 uniformly from those models that differ from @xmath20 in exactly one position , which simplifies the acceptance probability above since the @xmath237-ratio is identically 1 .",
    "also , we initialize our markov chain monte carlo search at the model selected by lasso .    to implement this procedure , some additional tuning parameters need to be specified .",
    "first , recall that @xmath238 corresponds to the genuine bayes model with a flat prior for @xmath18 .",
    "our theory does not cover this case , but we can mimic it by picking something close . here",
    "we consider @xmath239 and @xmath240 ; in our experience , the performance is not sensitive to the choice of @xmath62 in a neighborhood of @xmath241 .",
    "second , for the prior on the model size , we employ the complexity prior with @xmath242 and @xmath243 , i.e. , @xmath244 .",
    "the choice of small @xmath127 makes the prior sufficiently spread out , allowing the posterior to move across the model space and , in particular , helping the markov chain for @xmath20 to mix reasonably well .",
    "third , in practice , the error variance @xmath14 is seldom known , so some procedure to handle unknown @xmath14 is needed .",
    "we proposed to modify our empirical bayes posterior by plugging in an estimate of @xmath14 .",
    "in particular , we use a residual mean square error based on a lasso fit @xcite .    finally ,",
    "if samples from the @xmath6 posterior are desired , then these can easily be obtained , via conjugacy , after a sample of @xmath20 is available . in particular , the conditional posterior distribution for @xmath18 , given @xmath20 , is normal with mean @xmath245 and variance @xmath246 .",
    "r code to implement our procedure is available at www.math.uic.edu/~rgmartin .      in this section ,",
    "we reconsider some of the simulation experiments performed by @xcite , which are related to experiments presented in @xcite . in each setting , the error variance is @xmath247 ; the covariate matrix is obtained by sampling from a multivariate normal distribution with zero mean , unit variance , and constant pairwise correlation @xmath248 ; and the true model @xmath31 has @xmath249 . the particular correlation structure among the covariates",
    "is given practical justification in @xcite . under this setup",
    ", we consider three different settings :    @xmath250 , @xmath251 , and @xmath252 ;    @xmath253 , @xmath254 , and @xmath255 the same as in setting  1 ;    @xmath250 , @xmath251 , and @xmath256 .",
    "our settings  12 correspond to the two @xmath257 configurations in case  2 of @xcite and our setting  3 is the same as their case  3 .    we carry out model selection by retaining those variables whose inclusion probability @xmath258 , @xmath259 , exceeds 0.5 ; this is the so - called median probability model , shown to be optimal , in a certain sense , by @xcite .",
    "alternatively , one could select the model with the largest posterior probability , but this is more expensive computationally compared to the median probability model ",
    "only @xmath12 inclusion probabilities instead of up to @xmath260 model probabilities . in all cases ,",
    "the posterior almost immediately concentrates on the true model .",
    "our markov chain monte carlo required only 5000 iterations to reach convergence , which took only a few seconds on an ordinary laptop computer : about 10 seconds for setting  1 and about 25 seconds for setting  2 .    to summarize the performance , we consider five different measures .",
    "first , we consider the mean inclusion probability for those variables in and out of the active set @xmath31 , respectively , i.e. , @xmath261 we expect the former to be close to 1 and the latter to be close to 0 .",
    "next , we consider the probability that the model selected by our empirical bayes method , denoted by @xmath262 is equal to or contains the true model @xmath31 . finally , we also compute the false discovery rate of our selection procedure . a summary of these quantities for our empirical bayes method , denoted by _ eb _ , across the three settings is given in tables  [ table : setting1][table : setting3 ] .    for comparison ,",
    "we consider those methods discussed in @xcite , including their two bayesian methods , denoted by basad and basad.bic .",
    "two other bayesian methods considered are the credible region approach of @xcite , denoted by bcr.joint , and the spike - and - slab method of @xcite , denoted by spikeslab .",
    "we also consider three penalized likelihood methods , all tuned with bic , namely , the lasso @xcite , the elastic net @xcite , and the smoothly clipped absolute deviation @xcite , denoted by lasso.bic , en.bic , and scad.bic , respectively .",
    "the results for these methods are taken from tables  23 in @xcite , which were obtained based on 200 samples taken from the models described in settings  13 described above .",
    "our selection method based on our empirical bayes posterior is the overall the best among those being compared in terms of selecting the true model and false discovery rate .",
    "in addition to the strong finite - sample performance of our model selection procedure , our theory is arguably stronger than that available for the other methods in this comparison .",
    "take , for example , the basad method of @xcite , the next - best - performer in the simulation study .",
    "their method produces a posterior distribution for @xmath6 but since their prior has no point mass , this posterior can not concentrate on a lower - dimensional subspace of @xmath53 .",
    "so , it is not clear if their posterior distribution for @xmath6 can attain the minimax concentration rate without tuning the prior using knowledge about the underlying sparsity level .",
    ".simulation results for setting  1 .",
    "first seven rows taken from table  2 ( top ) in @xcite ; the _ eb _ row corresponds to our empirical bayes procedure . [ cols=\"<,^,^,^,^,^\",options=\"header \" , ]",
    "we have presented an empirical bayes model for the sparse high - dimensional regression problem .",
    "though the proposed approach has some unusual features , such as a data - dependent prior , we characterize the posterior concentration rate , which agrees with the optimal minimax rate in some cases . to our knowledge , this is the only available minimax concentration rate result for a full posterior distribution in the sparse high - dimensional linear model .",
    "moreover , our formulation allows for relatively simple posterior computation , via markov chain monte carlo , and simulation studies show that model selection by thresholding the posterior inclusion probabilities outperforms a variety of existing methods .",
    "the general strategy proposed here goes as follows .",
    "suppose we have a high - dimensional parameter , and different models @xmath20 identify a set of parameters @xmath263 .",
    "suppose further that @xmath264 is sparse in the sense that only a few of its entries are non - null .",
    "then an empirical bayes model is obtained by specifying a prior for @xmath265 as @xmath266 , where @xmath267 would be allowed to depend on data through , say , the maximum likelihood estimator @xmath268 of @xmath263 . intuitively , the idea is to center the conditional prior on a data - dependent point , say @xmath268 , and then use the fractional likelihood to prevent the posterior to track the data too closely .",
    "we believe this is a general tool that can be used in high - dimensional problems , and one possible application of this approach , which we plan to explore , is a mixture model where @xmath20 represents the number of mixture components , and @xmath263 is the set of parameters associated with a mixture model with @xmath20 mixture components .",
    "the authors are grateful for the valuable comments provided by the editor , associate editor , and three anonymous referees .",
    "this work is partially supported by the u.  s.  national science foundation , grants dms1506879 and dms1507073 .",
    "write @xmath269 . rewrite the numerator @xmath81 of the posterior as @xmath270 where the sum is over all @xmath20 with @xmath271 ,",
    "@xmath272 is a @xmath12-vector made by augmenting @xmath18 with @xmath273 for all @xmath274 , and @xmath275 is the set of all @xmath18 such that @xmath276 .",
    "focus on a single @xmath20 .",
    "taking expectation of the inner integral with respect to @xmath277 gives @xmath278 \\,d\\beta_s.\\ ] ] apply hlder s inequality to the inside expectation , i.e. , for @xmath279 and @xmath280 , @xmath281 \\notag \\\\ & \\leq { \\mathsf{e}}^{1/h } \\bigl [ \\bigl\\{\\frac{{\\mathsf{n}}(y \\mid x\\beta_{s+ } , \\sigma^2 i)}{{\\mathsf{n}}(y \\mid x \\beta^\\star , \\sigma^2 i ) } \\bigr\\}^{h\\alpha } \\bigl ] \\ , { \\mathsf{e}}^{1/q } \\bigl [ { \\mathsf{n}}^q(\\beta_s \\mid \\hat\\beta_s , \\gamma^{-1 } ( x_s^\\top x_s)^{-1 } ) \\bigr ] .",
    "\\label{eq : holder}\\end{aligned}\\ ] ] if @xmath282 , then a renyi divergence formula is available for the first term , giving @xmath283 = e^{-\\frac{\\alpha ( 1-h\\alpha)}{2\\sigma^2 } \\|x(\\beta_{s+ } - \\beta^\\star)\\|^2}.\\ ] ] for the second term in the product above , recall that @xmath284 . then @xmath285 and",
    ", therefore , since @xmath286 is idempotent of rank @xmath19 , we get that @xmath287 is distributed as a non - central chi - square with @xmath19 degrees of freedom and non - centrality parameter @xmath288 . then @xmath289 \\notag \\\\ & = \\frac{\\gamma^{|s|/2}|x_s^\\top x_s|^{1/2}}{(2\\pi)^{|s|/2 } } { \\mathsf{e}}^{1/q } ( e^{-\\frac{q\\gamma}{2 } z } ) \\notag \\\\ & = \\frac{\\gamma^{|s|/2}|x_s^\\top x_s|^{1/2}}{(2\\pi)^{|s|/2 } } ( 1 + q\\gamma)^{-\\frac{|s|}{2q } } e^{-\\frac{\\gamma}{2(1+q\\gamma ) } \\lambda } \\notag \\\\ & = \\frac{\\gamma^{|s|/2}|x_s^\\top x_s|^{1/2}}{(2\\pi)^{|s|/2 } } ( 1 + q\\gamma)^{-\\frac{|s|}{2q } } e^{-\\frac{\\gamma}{2\\sigma^2(1+q\\gamma ) } \\|x_s(\\beta_s - ( x_s^\\top x_s)^{-1}x_s^\\top x \\beta^\\star)\\|^2 } , \\label{eq : holder2 } \\end{aligned}\\ ] ] where the second equality is from the standard formula for the moment generating function of a non - central chi - square random variable .",
    "now we must integrate the upper bound over @xmath290 with respect to @xmath18 .",
    "it is clear from the definition of @xmath275 that the quantity in is bounded on @xmath275 , i.e. , @xmath291 it is also clear that the expression resembles a normal density in @xmath18 , modulo some multiplicative factors .",
    "the algebra is tedious , but the integral of with respect to @xmath18 is bounded above by @xmath292 putting everything together , we have that @xmath293 taking @xmath294 completes the proof .",
    "the proof is an application of ideas used in the proof of lemma  [ lem : numerator ] . in particular",
    ", @xmath295 equals @xmath296 take expectation with respect to @xmath277 as in the proof of lemma  [ lem : denominator ] and move expectation to the inside of the integral . working with each @xmath20 term separately , apply hlder s inequality to bound the expectation of the product .",
    "this upper bound consists of a product of three terms just like in the previous proof .",
    "the first is bounded by 1 ; the second is @xmath99 ; and the third is a probability density function in @xmath18 .",
    "then the integral over @xmath18 is bounded by @xmath99 and the claim follows .",
    "fix @xmath87 and write @xmath297 as usual .",
    "write @xmath298 for the @xmath299 matrix projecting onto the column space of @xmath36 .",
    "if @xmath300 , then @xmath301 and , since @xmath302 is idempotent of rank @xmath303 , this quantity is distributed as a non - central chi - square with @xmath303 degrees of freedom and non - centrality parameter @xmath304 by definition of @xmath298 , it turns out that the non - centrality parameter in the above display is zero , so it is actually an ordinary / central chi - square . from the chi - square moment generating function we immediately get @xmath305 where @xmath306 is a constant that depends only on @xmath307 .",
    "then @xmath308 plug in our complexity prior and simplify the upper bound : @xmath309 from @xmath310 the upper bound becomes @xmath311 so , if @xmath127 is such that @xmath221 , the upper bound vanishes , completing the proof .      in light of theorem  [ thm : selection1",
    "] , it suffices to show that @xmath312 . to start ,",
    "take a generic @xmath313 .",
    "then , from , we have @xmath314 the exponent @xmath315 is a chi - square random variable with @xmath316 degrees of freedom and non - centrality parameter @xmath317 the algebra is a bit tedious , but we can simplify @xmath318 as @xmath319 from the non - central chi - square moment generating function we have @xmath320 where @xmath321 .",
    "the irrepresentability result in lemma  5 of @xcite gives a lower bound on @xmath318 : @xmath322 monotonicity of @xmath181 implies that @xmath323 and , furthermore , by the beta - min condition , @xmath324 putting everything together , including the definition of @xmath225 , we get @xmath325 if we can show that the sum of our upper bound above , over all @xmath313 , vanishes , then we are done .",
    "plugging in our complexity prior , we need to bound @xmath326 where @xmath327 is a constant that depends only on @xmath328 .",
    "note that @xmath329 then the summation can be bounded above by @xmath330 where the inequality follows from the formula for partial sums of a geometric series . since @xmath227 , the upper bound vanishes , completing the proof",
    "consider a prior @xmath331 for the model @xmath20 of the form @xmath332 where @xmath63 is a prior on models of size @xmath33 , for @xmath333 , @xmath334 is a fixed model with @xmath335 and @xmath336 , and @xmath337 for @xmath338 to be determined . with the prior @xmath331 , it is easy to see that the denominator @xmath68 of the posterior satisfies @xmath339 for sufficiently large @xmath10 ( so that @xmath340 , say ) , where @xmath341 for the case @xmath72 , in the first term of the maximum , we have @xmath342 just like in the proof of lemma  [ lem : denominator ] . hence , @xmath343 for the second term , since @xmath33 , we have @xmath344 since the span of @xmath345 contains that of @xmath64 , by assumption , we have that @xmath346 and , consequently , the term in the exponent above is bigger than @xmath347 , which is obviously positive",
    ". therefore , the second term in the maximum is @xmath348 where @xmath349 is as in lemma  [ lem : denominator ] . finally , for @xmath350",
    ", we have @xmath351 with probability  1 , for large @xmath10 , as desired .",
    "we claim that , with this new prior @xmath352 , the posterior can achieve the minimax rate for the prediction loss under both the ordinary and ultra high - dimensional regimes .",
    "that is , we get the optimal rate @xmath353 this follows easily from the denominator bound discussed above , so long as our current numerator bound from lemma  [ lem : numerator ] also holds for the new prior .",
    "the majority of the proof of lemma  [ lem : numerator ] has nothing to do with the model prior , so we can immediately jump to the following conclusion : @xmath354 where @xmath355 and @xmath356 are as in the proof of lemma  [ lem : numerator ] .",
    "now , for the weighted average part , we have @xmath357 the first term in this upper bound is just like that in the proof of lemma  [ lem : numerator ] , so we have a handle on this .",
    "we need to choose @xmath358 in such a way that the second term is also controlled . since @xmath359 for some @xmath338",
    ", it follows that we need @xmath360 . with this choice",
    ", the optimal minimax rate can be achieved in both ordinary and ultra high - dimensional regimes .",
    "we claimed in remark  [ re : minimax ] that there is a price to be paid , in terms of model selection performance , if one uses the prior @xmath352 discussed above .",
    "the problem is that the weight @xmath358 assigned to the large model @xmath334 , with @xmath361 , is considerably larger than the weight @xmath362 assigned to the true model @xmath31 .",
    "then the corresponding posterior mass assigned to @xmath334 is too large , large enough to pull the posterior away from the true model , leading to inconsistency ."
  ],
  "abstract_text": [
    "<S> we propose a new empirical bayes approach for inference in the @xmath0 normal linear model . </S>",
    "<S> the novelty is the use of data in the prior in two ways , for centering and regularization . under suitable sparsity assumptions , </S>",
    "<S> we establish a variety of concentration rate results for the empirical bayes posterior distribution , relevant for both estimation and model selection . </S>",
    "<S> computation is straightforward and fast , and simulation results demonstrate the strong finite - sample performance of the empirical bayes model selection procedure .    </S>",
    "<S> _ keywords and phrases : _ data - dependent prior ; fractional likelihood ; minimax rate ; regression ; variable selection . </S>"
  ]
}