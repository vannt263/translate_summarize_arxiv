{
  "article_text": [
    "in this paper , we consider the problem of robust and sparse estimation for linear regression models . in modern regression analysis , sparse and high - dimensional estimation scenarios where ratio of the number of predictor variables to the number of observations , say @xmath5 ,",
    "is high , but the number of actually relevant predictor variables to the number of observations , say @xmath6 , is low , have become increasingly common in areas such as bioinformatics and chemometrics . in this type of regression scenarios , due to the high - dimensional nature of the data , it is difficult to discover outlying observations using simple criteria .",
    "traditional robust regression estimators do not produce sparse models and can have a bad behaviour with regards to robustness and efficiency when @xmath5 is high , see @xcite .",
    "moreover , they can not be calculated for @xmath7 .",
    "thus , robust regression methods for high - dimensional data are in need .",
    "modern approaches to estimation in sparse and high - dimensional linear regression models include penalized least squares estimators , e.g. the ls - bridge estimator of @xcite and the ls - scad estimator of @xcite .",
    "ls - bridge estimators are penalized least squares estimators in which the penalization function is proportional to the @xmath0 norm with @xmath2 .",
    "they include as special cases the ls - lasso of @xcite ( @xmath8 ) and the ls - ridge of @xcite ( @xmath9 ) .",
    "the ls - scad estimator is a penalized least squares estimator in which the penalization function , the scad , is a non - concave function with several interesting theoretical properties .",
    "the theoretical properties of penalized least squares estimators have been extensively studied in the past years .",
    "of special note is the so called _ oracle property _ defined in @xcite : an estimator is said to have the _ oracle property _",
    "if the estimated coefficients corresponding to zero coefficients of the true regression parameter are set to zero with probability tending to one , while at the same time the coefficient corresponding to non - zero coefficients of the true regression parameter are estimated with the same asymptotic efficiency as if we knew the correct model in advance .",
    "@xcite derive the asymptotic distribution of ls - bridge estimators in the classical regression scenario of fixed @xmath10 and prove that for @xmath3 these estimators can have the _ oracle property_. they also show that for @xmath8 , the ls - lasso sets the estimated coefficients corresponding to zero coefficients of the true regression to zero with positive probability .",
    "the ls - lasso estimator is not variable selection consistent unless rather stringent conditions are imposed on the design matrix , and thus in general does not posses the _ oracle property _",
    "; see @xcite and @xcite for details .",
    "moreover , the ls - lasso estimator has a bias problem : it can excessively shrink large coefficients . to remedy this issue",
    ", @xcite introduced the adaptive ls - lasso , where adaptive weights are used for penalizing different coefficients of the @xmath11 norm of the coefficients , and showed that the adaptive lasso can have the _ oracle property_.    we note that whereas there exist extremely efficient algorithms to calculate the ls - lasso , see @xcite , ls - bridge estimators with @xmath3 seem to be somewhat difficult to calculate . an algorithm to calculate ls - bridge estimators with @xmath3",
    "is described in @xcite . as @xcite points out",
    ", adaptive ls - lasso estimators can be calculated using any of the algorithms available to calculate ls - lasso estimators .",
    "penalized least squares estimators are not robust and may be highly inefficient under heavy tailed errors . in an attempt to remedy this issue , penalized m - estimators defined using a convex loss function have been proposed ; see for example @xcite and @xcite .",
    "unfortunately , these estimators are not robust with respect to contaminations in the predictor variables .",
    "@xcite proposed the sparse - lts estimator , a least trimmed squares estimator with a @xmath11 penalization . in a simulation study , @xcite show that the sparse - lts can be robust with respect to contamination in both the response and predictor variables .",
    "the sparse - lts estimator can be calculated for @xmath7 .",
    "however , @xcite do not provide any asymptotic theory for their estimator .",
    "@xcite propose a robust version of the lars procedure , see @xcite , and in extensive simulations show that the rlars procedure produces well behaved estimators under diverse contamination models .",
    "however , since the rlars procedure is not based on the minimization of a clearly defined objective function , a theoretical analysis of its properties is difficult .",
    "@xcite proposed a penalized regression estimator based on an exponential squared loss function .",
    "they prove that a _ local minimum _ of the objective function used to define their estimator can have the _ oracle property_. on the other hand , their proposed estimators can not be calculated in regression scenarios with @xmath7 . @xcite introduced s - ridge and mm - ridge estimators : @xmath12-penalized s- and mm - estimators of regression .",
    "in extensive simulation studies he shows that these estimators can be robust in a variety of contamination scenarios .",
    "however , @xmath12-penalized regression estimators do not produce sparse models .",
    "@xcite does not provide any asymptotic theory for these estimators .    in this paper",
    ", we study the robust and asymptotic properties of mm - bridge and adaptive mm - bridge estimators : @xmath0-penalized mm - estimators of regression and mm - estimators with an adaptive @xmath1 penalty .",
    "we obtain lower bounds on the breakdown points of mm - bridge and adaptive mm - bridge estimators . for the case of a fixed number of covariates , we prove the strong consistency of mm - bridge and adaptive mm - bridge estimators under general conditions .",
    "we derive the asymptotic distribution of mm - bridge estimators for all @xmath13 and prove that for @xmath3 they can have the _ oracle property_. for the special case of @xmath8 we show that the coordinates of the mm - bridge estimator corresponding to null coefficients of the true regression parameter will be set to zero with positive probability .",
    "see the comments following theorem [ dist - asin knight ] .",
    "we show that adaptive mm - bridge estimators can have the _ oracle property _ for all @xmath4 .",
    "we propose an algorithm to calculate both mm - bridge estimators with @xmath8 , which we call mm - lasso estimators , and adaptive mm - bridge estimators with @xmath14 , which we call adaptive mm - lasso estimators .",
    "our algorithm uses the s - ridge estimator of @xcite as an initial estimator and iteratively solves a weighted - lasso type problem .",
    "even though we derive our asymptotic results for fixed @xmath10 , mm - lasso and adaptive mm - lasso estimators can be calculated for @xmath7 . in extensive simulations ,",
    "we study the performance with regards to stability in the presence of high - leverage outliers , and prediction accuracy and variable selection properties for uncontaminated samples of the mm - lasso and adaptive mm - lasso estimators .",
    "finally , we apply our proposed estimators to a real high - dimensional data set .",
    "the rest of this paper is organized as follows . in section [ sec - mm",
    "y s ] we review the definition and some of the most important properties of mm and s - estimators of regression . in section [ sec - mm - bridge ]",
    "we define s - bridge , mm - bridge and adaptive mm - bridge estimators , we study their robust and asymptotic theoretical properties and we describe an algorithm to compute mm - lasso and adaptive mm - lasso estimators . in section [ sec - sim ]",
    "we conduct an extensive simulation . in section [ sec - real ] we apply the aforementioned estimators to a real high - dimensional data set .",
    "conclusions are provided in section [ conclusions ] .",
    "finally , the proof of all our results are given in the appendix .",
    "we consider a linear regression model with random carriers : we observe @xmath15 @xmath16 i.i.d .",
    "@xmath17-dimensional vectors , where @xmath18 is the response variable and @xmath19 is a vector of random carriers , satisfying @xmath20 where @xmath21 and @xmath22 are to be estimated and @xmath23 is independent of @xmath24 . for @xmath25 let @xmath26 , where @xmath27 .",
    "some of the coefficients of @xmath28 may be zero , and thus the corresponding carriers do not provide relevant information to predict @xmath29 .",
    "we do not know in advance the set of indices corresponding to coefficients that are zero , and it may be of interest to estimate it . for simplicity , we will assume @xmath30 , where @xmath31 , @xmath32 , all the coordinates of @xmath31 are non - zero and all the coordinates of @xmath32 are zero .",
    "let @xmath33 be the distribution of the errors @xmath23 , @xmath34 the distribution of the carriers @xmath24 and @xmath35 the distribution of @xmath36 .",
    "then @xmath35 satisfies@xmath37 let @xmath38 stand for the first @xmath39 coordinates of @xmath40 and let @xmath41 be its distribution .",
    "for @xmath42 and @xmath2 we note @xmath43 and @xmath44 . throughout this paper",
    ", @xmath45-function will refer to a bounded @xmath45-function , in the sense of @xcite .",
    "a popular choice of @xmath45-functions is tukey s bisquare family of functions given by @xmath46 where @xmath47 is a tuning constant .",
    "given a sample @xmath48 from some distribution @xmath49 and @xmath50 the corresponding m - estimate of scale @xmath51 is defined by @xmath52 it is easy to prove that @xmath53 if and only if @xmath54 and in this case@xmath55    the robustness of an estimator is measured by its stability when a small fraction of the observations are arbitrarily replaced by outliers that may not follow the assumed model .",
    "a robust estimator should not be much affected by a small fraction of outliers .",
    "a quantitative measure of an estimator s robustness , introduced by @xcite , is the finite - sample replacement breakdown point . loosely speaking ,",
    "the finite - sample replacement breakdown point of an estimator is the minimum fraction of outliers that may take the estimator beyond any bound . for a regression estimator",
    ", this measure is defined as follows .",
    "given a sample @xmath56 , @xmath57 , let @xmath58 and let @xmath59 be a regression estimator .",
    "the finite - sample replacement breakdown point of @xmath60 is then defined as @xmath61 where @xmath62 and @xmath63 is the set of all datasets with at least @xmath64 elements in common with @xmath65 .",
    "a breakdown point equal to @xmath66 only guarantees that for any given contamination fraction @xmath67 , there exists a compact set such that the estimator in question remains in that compact set whenever a fraction of @xmath68 observations is arbitrarily modified .",
    "however , this compact set may be very large .",
    "thus , although a high breakdown point is always a desirable property , an estimator that has a high breakdown point can still be largely affected by a small fraction of contaminated observations .    given a sample @xmath15 , @xmath57 from the model given in ( [ h0 ] ) ,",
    "@xcite define the s - estimator of regression as @xmath69 where @xmath70 is a m - estimate of scale .",
    "@xcite derive the asymptotic distribution of s - estimators of regression under very general conditions .",
    "s - estimators can always be tuned so as to attain the maximum possible finite - sample replacement breakdown point for regression equivariant estimators .",
    "however , s - estimators can not combine high breakdown point with high efficiency at the normal distribution , see @xcite .",
    "let @xmath15 , @xmath16 be a sample satisfying , and @xmath71 and @xmath72 be two @xmath45-functions satisfying @xmath73 .",
    "then @xcite defines the mm - estimator of regression as @xmath74 where @xmath75 is a consistent and high breakdown point estimate of @xmath76 and @xmath77 is the m - estimate of scale of the residuals of @xmath75 , calculated using @xmath71 and @xmath78 .",
    "@xcite , proves that under general conditions , mm - estimators are strongly consistent for @xmath76 , and furthermore @xmath79 where @xmath80 , @xmath81 is defined by @xmath82 @xmath83 and @xmath84    besides , he shows that @xmath85 can be chosen so that the resulting mm - estimator has simultaneously the two following properties :    * normal asymptotic efficiency as close to one as desired .",
    "* breakdown point greater than or equal to that of the initial estimator .",
    "@xcite recommend to take @xmath86 and @xmath87 .",
    "the tuning constant for @xmath71 , @xmath88 , should be chosen so that the resulting m - estimate of scale be consistent for the error standard deviation in the case of normal errors .",
    "the choice of @xmath89 should aim at striking a balance between robustness and efficiency .",
    "@xcite recommend to choose @xmath90 so that the mm - estimator has an asymptotic efficiency of 85% at the normal distribution .",
    "the reason for choosing an 85% asymptotic efficiency at the normal distribution , is that at this level of the efficiency the mm - estimator has the same maximum asymptotic bias as the initial s - estimator of regression for the case of normal errors and normal carriers .",
    "given a sample @xmath15 , @xmath57 , @xmath91 , @xmath92 , a @xmath45-function @xmath71 and @xmath93 , we define the @xmath94-penalized s - bridge estimator of regression following @xcite as @xmath95 where @xmath96 is the residual scale estimate defined using @xmath71 and @xmath78 . if the model contains an intercept , then it is not penalized .",
    "it is easy to see that @xmath97 where @xmath98 is the s - estimator calculated using @xmath71 and @xmath78 .    given another @xmath45-function @xmath72 which satisfies @xmath73 and @xmath99 we define the @xmath0-penalized mm - bridge estimator of regression as @xmath100 where @xmath75 is a consistent initial estimate of @xmath28 .",
    "clearly , the robustness of the mm - bridge estimator will depend heavily on the robustness of the initial estimate .",
    "if the model contains an intercept , then it is not penalized . for the case",
    "@xmath8 we will call the resulting estimator mm - lasso .",
    "note that our definition of a mm - bridge estimator with @xmath101 and @xmath9 , is not exactly the same as the definition of mm - ridge estimators of @xcite . for a given @xmath102 ,",
    "the mm - ridge of @xcite is equal to our mm - ridge estimator calculated with @xmath103 .",
    "nonetheless , our asymptotic results can be very easily adapted to cover the mm - ridge estimators as defined by @xcite .",
    "however , this is not the case for our results concerning the finite - sample breakdown point of mm - bridge estimators .",
    "@xcite points out that the finite - sample breakdown point of mm - ridge estimators is , for a fixed penalization parameter and according to his definition of mm - ridge estimators , greater than or equal to the breakdown point of the residual scale @xmath70 . in theorem [ theo - break - br ] , we show that for a fixed penalization parameter and according to our definition of mm - bridge estimators , the breakdown point of any mm - bridge estimator is greater than @xmath104 .",
    "given @xmath105 , @xmath106 and @xmath107 we define the adaptive mm - bridge estimator of regression as @xmath108 where @xmath109 is a consistent initial estimate of @xmath28 . clearly if @xmath110 for some @xmath111 , then @xmath112 . if the model contains an intercept , then it is not penalized . for the case",
    "@xmath14 we will call the resulting estimator adaptive mm - lasso . note that",
    "for coefficients corresponding to large coefficients of @xmath109 , the adaptive mm - lasso employs a small penalty ; this ameliorates the bias issues associated with the @xmath11 penalty .",
    "@xcite prove that their estimator can have the highest possible breakdown point among regression equivariant estimators , but it must be noted that their estimator is not regression equivariant .",
    "@xcite show that the breakdown point of the sparse - lts estimator is @xmath113 , where @xmath114 is the number of trimmed observations , and prove that the breakdown point of the ls - lasso estimator is @xmath115 .",
    "note that it follows immediately from that for any @xmath116 , the finite - sample breakdown point of @xmath117 is at least as high as that of @xmath98 . in theorem [ theo - break - br ] , we prove that for any fixed @xmath118 , the breakdown point of @xmath119 is equal to @xmath104 . in theorem",
    "[ theo - break - ad ] , we prove that for any fixed @xmath120 , the breakdown point of @xmath121 is greater than or equal to the breakdown point of @xmath109 .",
    "however , one could argue that since @xmath117 , @xmath119 and @xmath121 are not regression equivariant , these results are rather vacuous",
    ". see @xcite .",
    "[ theo - break - br ] if @xmath118 is fixed , then @xmath122    [ theo - break - ad ] if @xmath120 is fixed , then @xmath123 .",
    "note that if @xmath124 , then @xmath125 whenever @xmath126 . in practice , @xmath116 , @xmath102 and @xmath107",
    "may be chosen via some data - driven procedure such as cross - validation . in this case",
    ", the breakdown point of the resulting mm - bridge and adaptive mm - bridge estimators may be lower than @xmath104 .",
    "the robustness of the resulting estimators will depend sorely on the robustness of the cross - validation scheme , and hence the use of robust residual scales as objective functions , instead of the classical root mean squared error , is crucial .",
    "we now describe the set - up to study the asymptotic properties of s - bridge , mm - bridge and adaptive mm - bridge estimators of regression .",
    "we will assume that    * @xmath71 and @xmath72 are twice continuously differentiable and eventually constant .",
    "* @xmath127 for all non - zero @xmath128 .",
    "* @xmath33 has an even continuous density , @xmath129 , that is a monotone decreasing function of @xmath130 and a strictly decreasing function of @xmath130 in a neighborhood of 0 .",
    "a family of @xmath45-functions that satisfies [ b1 ] is tukey s bisquare family of functions , given in .",
    "condition [ b2 ] is needed in the proof of the consistency of s - bridge estimators .",
    "note that condition [ b3 ] does not require finite moments from @xmath131 .",
    "thus , extremely heavy tailed error distributions , such as cauchy s distribution , can be easily seen to satisfy [ b3 ] .",
    "however , [ b3 ] does impose a rather stringent symmetry assumption on the error distribution .",
    "this requirement greatly simplifies the asymptotic treatment of the estimators and is usual in robust statistics .",
    "the following theorem proves the strong consistency of s - bridge , mm - bridge and adaptive mm - bridge estimators of regression whenever @xmath132 , @xmath133 and @xmath134 respectively .",
    "[ cons ] let @xmath15 , @xmath16 be i.i.d observations with distribution @xmath35 , which satisfies .",
    "assume [ b1]-[b3 ] hold",
    ". then    1 .   if @xmath132 , @xmath135 .",
    "[ s - cs ] 2 .   if @xmath133 , @xmath136 .",
    "[ mm - cs ] 3 .",
    "if @xmath134 , @xmath137 .",
    "[ amm - cs ]    in practice , we will use the s - ridge estimator of @xcite as the initial estimate @xmath75 in and . note that according to theorem [ cons ] and the remarks above theorem [ theo - break - br ] , the s - ridge is a high breakdown point and consistent estimate of @xmath28 , as long as the penalization parameter satisfies",
    "@xmath138 .    in order to obtain the rate of convergence of mm - bridge and adaptive mm - bridge estimators we will have to make the following additional assumption :    * @xmath34 has finite second moments and",
    "@xmath139 is non - singular .    in the next theorem , we prove the @xmath140-consistency of mm - bridge and adaptive mm - bridge estimators",
    ".    [ rate ] let @xmath15 , @xmath16 be i.i.d observations with distribution @xmath35 , which satisfies .",
    "assume [ b1]-[b4 ] hold",
    ". then    1 .   if @xmath141 , then @xmath142 .",
    "2 .   if @xmath143 , then @xmath144 .    from now on",
    ", we will assume that the initial estimator used to define the penalty weights for the adaptive mm - bridge estimator , @xmath109 , is @xmath140-consistent .",
    "for example , according to theorem [ rate ] , we could take @xmath109 to be some mm - bridge estimator calculated with @xmath145 .",
    "let @xmath146 stand for the first @xmath39 coordinates of @xmath121 and @xmath147 for the remaining @xmath148 .",
    "let @xmath149 stand for the first @xmath39 coordinates of @xmath119 and @xmath150 for the remaining @xmath148 .",
    "the following theorem shows that , as long as @xmath151 and @xmath152 , adaptive mm - bridge estimators can be variable selection consistent , and that if @xmath3 , then mm - bridge estimators can be variable selection consistent as well . in particular ,",
    "taking @xmath14 , we prove the variable selection consistency of adaptive mm - lasso estimators .",
    "[ spars ] let @xmath15 , @xmath16 be i.i.d observations with distribution @xmath35 , which satisfies .",
    "assume [ b1]-[b4 ] hold .    1 .",
    "suppose @xmath3 , @xmath153 and @xmath154 .",
    "then @xmath155 .",
    "2 .   suppose @xmath156 , @xmath157 and @xmath158 .",
    "then @xmath159    next we derive the asymptotic distribution of @xmath149 and @xmath146 .    [ orac ] let @xmath15 , @xmath16 be i.i.d observations with distribution @xmath35 , which satisfies .",
    "assume [ b1]-[b4 ] hold .    1 .",
    "suppose @xmath3 , @xmath160 and @xmath161 . then @xmath162 2 .",
    "suppose @xmath152 , @xmath163 and @xmath164 .",
    "then @xmath165    here @xmath166 and @xmath167 are as in and @xmath168 .",
    "theorem [ spars ] together with theorem [ orac ] prove that @xmath121 and @xmath119 can have the _ oracle property _ as long as @xmath169 and @xmath152 , and @xmath3 respectively .",
    "that is : the estimated coefficients corresponding to null coordinates of the true regression parameter are set to zero with probability tending to 1 , while at the same time the coefficients corresponding to non - null coordinates of the true regression parameter are estimated with the same asymptotic efficiency as if we had applied a non penalized mm - estimators to the relevant carriers only .    in theorem [ dist - asin knight ]",
    "we derive the asymptotic distribution of @xmath119 for @xmath170 .",
    "our theorem is analogous to theorem 2 of @xcite .",
    "[ dist - asin knight ] let @xmath15 , @xmath16 be i.i.d observations with distribution @xmath35 , which satisfies .",
    "let @xmath171 .",
    "assume [ b1]-[b4 ] hold and @xmath172 .",
    "then @xmath173 where @xmath174 for @xmath175 , @xmath176 for @xmath8 and @xmath177 .",
    "note that if @xmath178 , @xmath179 has the same asymptotic distribution as the corresponding non - penalized mm - estimator .",
    "if @xmath180 and @xmath8 , the coordinates of @xmath119 corresponding to null coefficients of @xmath76 will be set to zero with positive probability , the proof is essentially the same as the one that appears in pages 1361 - 1362 of @xcite .",
    "however , one can show that @xmath181 where @xmath182 depends on @xmath183 , @xmath184 and @xmath76 .",
    "the proof is essentially the same as the proof of proposition 1 of @xcite .    if @xmath175 the amount of shrinkage of the estimated regression coefficients increases with the magnitude of the true regression coefficients .",
    "hence , for `` large '' parameters , the bias introduced by mm - bridge estimators with @xmath175 may be unacceptably large , at least for the fixed @xmath10 scenario .",
    "for the case @xmath9 we can calculate the asymptotic distribution of the estimator explicitly .",
    "it follows easily from theorem [ dist - asin knight ] that the asymptotic distribution of the mm - ridge estimator is @xmath185    in the next theorem we derive the asymptotic distribution of @xmath119 for @xmath3 when @xmath186 .    [ dist - asin knight2 ] let @xmath15 , @xmath16 be i.i.d observations with distribution @xmath35 , which satisfies .",
    "let @xmath187 .",
    "assume [ b1]-[b4 ] hold and @xmath186 .",
    "then @xmath173 where @xmath188 and @xmath189 .",
    "it follows from theorem [ dist - asin knight2 ] that for @xmath3 , if @xmath180 , the coordinates of @xmath119 corresponding to null coefficients of @xmath76 will be set to zero with positive probability .",
    "moreover , in this case the shrinkage only affects the coordinates of the estimators corresponding to null coefficients of @xmath28 , and hence no asymptotic bias is introduced .      in this section",
    ", we describe an algorithm to obtain approximate solutions of for @xmath8 , i.e. mm - lasso estimators . through out",
    "this section we will assume that our model , , contains an intercept , and that the first coordinate of each @xmath190 equals 1 .",
    "let @xmath191 be the matrix with @xmath190 as rows .",
    "prior to any calculations , all the columns of @xmath191 , except the first one , are centered and scaled using the median and the normalized median absolute deviation respectively .",
    "the response vector @xmath192 is centered using the median . at the end",
    ", the final estimates are expressed in the original coordinates .",
    "we take the s - ridge estimator of @xcite , which we note @xmath117 , as the initial estimate in .",
    "the penalization parameter for s - ridge estimator , @xmath116 , is chosen via robust 5-fold cross - validation , as described in @xcite .",
    "let @xmath193 .",
    "let @xmath194 , where @xmath195 is the derivative of @xmath72 . for a given @xmath196 ,",
    "let @xmath197 .",
    "suppose @xmath102 is given . and let @xmath198 be the diagonal matrix formed by @xmath199 .",
    "let @xmath200 and @xmath201 .",
    "let @xmath119 be the mm - lasso estimator .",
    "it is easy to show that @xmath119 satisfies @xmath202 where @xmath203 stands for a change of sign .",
    "note that the first column of @xmath204 equals @xmath205 . for each @xmath206",
    "let @xmath207 be the @xmath111-th column of @xmath204 and let @xmath208 then @xmath207 can be decomposed as the sum of two vectors : @xmath209 , in the direction of @xmath210 , and @xmath211 , orthogonal to @xmath210 .",
    "let @xmath212 be the matrix with columns @xmath213 .",
    "it is easy to show that @xmath119 satisfies @xmath214    we note that if @xmath215 and @xmath216 where known , @xmath217 , ... , @xmath218 could be estimated by solving equation using some algorithm to solve lasso - type problems , e.g. the lars procedure or coordinate descent optimization , without including an intercept .",
    "then @xmath219 could be solved easily from .",
    "the fact that @xmath215 and @xmath216 depend on @xmath119 suggests an iterative procedure , as is usual in robust statistics .",
    "starting from @xmath117 we iteratively solve equation using the lars algorithm without including an intercept and then solve for the intercept in .",
    "call @xmath220 the estimate at the @xmath221-th iteration .",
    "convergence is declared when @xmath222 where @xmath223 is some fixed tolerance parameter . in our simulations we took @xmath224 .",
    "regarding the calculation of adaptive mm - lasso estimators , we note that solving is equivalent to solving @xmath225 where @xmath226 for @xmath227 and taking @xmath228 .",
    "hence , our procedure to calculate mm - lasso estimators can be used to calculate adaptive mm - lasso estimators , simply applying the routine to the data with weighed carriers . to calculate our proposed adaptive mm - lasso estimator , we take @xmath229 , @xmath230 and @xmath231 .    in practice , we chose the @xmath45-functions used to calculate the initial s - ridge estimator , the mm - lasso estimator and the adaptive mm - lasso estimator of the form @xmath232 and @xmath233 where @xmath234 and @xmath235 is as in .",
    "the tuning constants @xmath88 and @xmath89 are chosen as in @xcite .    the penalization parameter for @xmath119 , @xmath102 , is chosen over a set of candidates via robust 5-fold cross validation , using a @xmath236-scale of the residuals as the objective function .",
    "the @xmath236-scale was introduced by @xcite to measure in a robust and efficient way the largeness of the residuals in a regression model .",
    "the set of candidate lambdas is taken as 30 equally spaced points between 0 and @xmath237 , where @xmath237 is approximately the minimum @xmath238 such that all the coefficients of @xmath119 except the intercept are zero . to estimate @xmath237 we",
    "first robustly estimate the maximal correlation between @xmath192 and the columns of @xmath191 using bivariate winsorization as advocated by @xcite .",
    "we use this estimate as an initial guess for @xmath237 and then improve it using a binary search . if @xmath7 , then 0 is excluded from the candidate set .",
    "the penalization parameter for @xmath121 , @xmath107 , is chosen using the same scheme used to choose @xmath102 .",
    "the initial s - ridge estimate is calculated using our own adaption of maronna s matlab code to c++ . to solve equation we use the fastlasso ( ) function from the robusthd r package ( @xcite ) .",
    "we use the foreach r ( @xcite ) package for parallel computations when it comes to finding optimal penalization parameters via cross - validation .",
    "this provided a significant reduction in computing times in computers with several cores .",
    "extensive parts of our computer code are written in c++ and interfaced with r using the rcpparmadillo package ( @xcite ) .",
    "an r package that includes the functions to calculate the estimators we propose is available at http://esmucler.github.io / mmlasso/.",
    "in this section , we compare the performance with regards to prediction accuracy and variable selection properties of    * the mm - lasso estimator described in the previous section . *",
    "the adaptive mm - lasso estimator described in the previous section . * the sparse - lts of @xcite .",
    "the penalization parameter for this estimator is chosen using a bic - type criterion as advocated by the authors .",
    "the estimator was calculated using the sparselts ( ) function from the robusthd r package . *",
    "the ls - lasso estimator .",
    "the penalization parameter for this estimator was chosen using 5-fold cross validation using the sum of the squared residuals as the objective function .",
    "the estimator was calculated using the lars ( ) function from the lars r package ( @xcite ) . *",
    "the adaptive ls - lasso estimator .",
    "the weights used were the reciprocal of an initial ls - lasso estimator , calculated as above .",
    "both the initial and the final penalization parameters were chosen using 5-fold cross validation using the sum of the squared residuals as the objective function . the estimator was calculated using the lars ( ) function from the lars r package . * the maximum likelihood oracle estimator , that is , the maximum likelihood estimator applied to the relevant carriers only .",
    "when the errors follow a normal distribution , this is the least squares estimators applied to the relevant carriers only .",
    "note that in any case , this is not a feasible estimator , and is included for benchmarking purposes only .",
    "* for the contaminated scenarios , we will also include the oracle mm estimator : an mm - estimator , calculated with tukey s bisquare function and tuned to have @xmath239 normal efficiency , applied to the relevant carriers only .",
    "the estimator was calculated using the lmrob ( ) function from the robust r package ( @xcite ) .",
    "once again , note that this is not a feasible estimator , and is included for benchmarking purposes only .      to evaluate the estimators we generate two independent samples of size @xmath240 of the model @xmath241 . the first sample , called the training sample , is used to fit the estimates and the second sample , called the testing sample , is used to evaluate the prediction accuracy of the estimates .",
    "we considered three possible distributions for the errors : a zero mean normal distribution , student s t - distribution with three degrees of freedom ( @xmath242 ) and student s t - distribution with one degree of freedom ( @xmath243 ) .",
    "the first case corresponds to the classical scenario of normal errors , the second case has heavy - tailed errors and the third case has extremely heavy - tailed errors .",
    "for the first two cases we use the prediction root mean squared error ( rmse ) to evaluate the prediction accuracy of the estimates . for the third case , since student s t - distribution with one degree of freedom does not have a finite first moment , we use the median of the absolute value ( mad ) of the prediction residuals as a measure of the the estimators prediction accuracy .",
    "we also evaluate the variable selection performance of the estimators by calculating the false negative ratio ( fnr ) , that is , the fraction of coefficients erroneously set to zero , and the false positive ratio ( fpr ) , the fraction of coefficient erroneously not set to zero .",
    "we consider the following five scenarios for the sample size , the number of covariates , @xmath28 and the distribution of the carriers .    1 .",
    "we take @xmath244 , @xmath245 and @xmath28 given by : component 1 is 3 , component 2 is 1.5 , component 6 is 2 and the rest of the coordinates are set to zero .",
    "we take @xmath246 with @xmath247 with @xmath248 . for the case of normally distributed errors , we take the standard deviation of the errors to be @xmath249 .",
    "2 .   the same as the last one , but with @xmath250 and @xmath251 .",
    "we take @xmath252 , @xmath253 and @xmath28 given by : components 1 - 5 are 2.5 , components 6 - 10 are 1.5 , components 11 - 15 are 0.5 and the rest are zero .",
    "we take @xmath246 with @xmath247 with @xmath254 . for the case of normally distributed errors , we take the standard deviation of the errors to be @xmath255 .",
    "we take @xmath256 , @xmath253 and @xmath28 given by : components 1 - 5 are 2.5 , components 6 - 10 are 1.5 , components 11 - 15 are 0.5 and the rest are zero .",
    "the first @xmath257 covariates @xmath258 and the remaining @xmath259 covariates @xmath260 are independent .",
    "the first 15 covariates have a zero mean multivariate normal distribution .",
    "the pairwise correlation between the @xmath221th and @xmath111th components of @xmath258 is @xmath261 with @xmath262 for @xmath263 .",
    "the final 185 covariates have a zero mean multivariate normal distribution .",
    "the pairwise correlation between the @xmath221th and @xmath111th components of @xmath260 is @xmath261 with @xmath262 for @xmath264 . for the case of normally distributed errors , we take the standard deviation of the errors to be @xmath255",
    "the same as the last one , but with @xmath265 .",
    "the same as scenario 1 , but with @xmath266 and @xmath267 .",
    "in scenario 1 we have a moderately high @xmath5 ratio . in scenario 2",
    "we have a relatively low @xmath5 ratio . in scenario 3 we have @xmath268 and high @xmath5 ratio and in scenarios 4 , 5 and 6 we have @xmath7 .",
    "scenarios 1 and 2 were analysed in @xcite and @xcite .",
    "scenarios 3 , 4 and 5 were analysed in @xcite .",
    "to evaluate the robustness of the estimators for the case of high - leverage outliers , we introduce contaminations in all six scenarios , for the case of normal errors .",
    "note that we only contaminate the training sample and not the testing sample .",
    "we take @xmath269 $ ] and for @xmath270 we set @xmath271 and @xmath272 .",
    "we moved @xmath273 in an uniformly spaced grid between 0 and 3 with step 0.1 and then between 3 and 10 with step 1 .",
    "to summarize the results for the contaminated scenarios we report for each estimator the maximum rmse , fnr and fpr over all outlier sizes @xmath273 .",
    "we note that the rmses of the ls - lasso and the adaptive ls - lasso are unbounded as a function of the outlier size and thus the range of outlier sizes considered aims at finding the maximum rmse of the mm - lasso , the adaptive mm - lasso and the sparse - lts .",
    "the number of montecarlo replications for the uncontaminated scenarios was @xmath274 .",
    "the number of montecarlo replications for contaminated scenarios was reduced to @xmath275 , to keep computation times reasonably low .",
    "we now present the results of our simulation study .",
    "all results are rounded to two decimal places .",
    "table [ tab : sincon ] shows the results for scenarios 1 through 6 without contamination .    regarding the prediction accuracy of the estimators , for the case of normal errors ,",
    "the mm - lasso and the adaptive mm - lasso have a rmse of the same order , and at times even lower than that of lasso and the adaptive lasso .",
    "the sparse - lts shows a good behaviour in scenarios 1 , 2 and 5 , but its rmse is much larger than that of the other estimators for the remaining scenarios . for the case of errors with t(3 ) or t(1 ) distribution , the mm - lasso and the adaptive mm - lasso show the best overall performance .",
    "we were surprised by the fact that for t(3 ) errors , the lasso and the adaptive lasso have a reasonably low rmse when compared with the maximum likelihood oracle .",
    "as expected , the lasso and the adaptive lasso lose all predictive power when the errors have a t(1 ) distribution .",
    "except for scenarios 3 , 4 and 6 , the sparse - lts shows a reasonably good performance . regarding the variable selection properties of the estimators , we note that the fpr and the fnr of the mm - lasso are comparable to that of the lasso , and the fpr and fnr of the adaptive mm - lasso are comparable to that of the adaptive lasso for the case of normal errors . for errors with t(3 ) or t(1 )",
    "distribution , the mm - lasso and the adaptive mm - lasso generally show the best behaviour .",
    "the fpr of the adaptive mm - lasso is lower than that of the mm - lasso , but the price to pay for this improvement is an increase in the fnr .",
    "note that for scenarios 1 , 2 and 3 the sparse - lts has a rather high fpr , always greater than 0.5 .    in table [",
    "tab : con ] we show the results for scenarios 1 through 6 under high - leverage contamination .",
    "the mm - lasso and the adaptive mm - lasso show the best overall behaviour .",
    "the sparse - lts shows a good behaviour for scenarios 1 , 2 , and the best behaviour for scenario 5 , but its maximum rmse is much larger than that of the mm - lasso and the adaptive mm - lasso for the rest of the scenarios . as expected",
    ", the maximum rmse of the lasso and the adaptive lasso is very large in all cases . in figure",
    "[ graf : curvahuang ] we show the rmses of the estimators as a function of the outlier size for scenario 3 .",
    "the mm - lasso has the overall best behaviour , followed closely by the adaptive mm - lasso .",
    "note that the rmse curves of the lasso and of the adaptive lasso are unbounded as a function of the outlier size : by taking larger outlier sizes the maximum rmses of the mm - lasso , the adaptive mm - lasso and the sparse - lts would not change , but those of the lasso and the adaptive lasso would increase without bound . regarding the variable selection properties of the estimators , the mm - lasso and the adaptive mm - lasso show the best overall balance between a low fnr and a low fpr .",
    "note that for scenarios 1 , 2 , 3 the maximum fpr of the sparse - lts is very high .",
    "= 0.11 cm    p3cmp1cmp0.7cmp0.7cmp1cmp0.7cmp0.7cmp1cmp0.7cmp0.7 cm scenario & normal & & & @xmath242 & & & @xmath243 & & + & rmse & fnr & fpr & rmse & fnr & fpr&mad & fnr & fpr + 1 @xmath276 & + mm - lasso & 3.42 & 0.04 & 0.52 & 1.77 & 0 & 0.52 & 1.36 & 0.01 & 0.50 + adaptive mm - lasso & 3.43 & 0.09 & 0.27 & 1.75 & 0 & 0.20 & 1.32 & 0.02 & 0.21 + sparse - lts & 3.92 & 0.03 & 0.82 & 1.91 & 0 & 0.85 & 1.44 & 0 & 0.69 + lasso & 3.33 & 0.02 & 0.43 & 1.84 & 0 & 0.46 & 9.9 & 0.38 & 0.28 + adaptive lasso & 3.28 & 0.06 & 0.26 & 1.82 & 0.01 & 0.29 & 10 & 0.46 & 0.19 + oracle & 3.15 & 0 & 0 & 1.69 & 0 & 0 & 1.16 & 0 & 0 + 2 @xmath277 & + mm - lasso & 1.09 & 0 & 0.53 & 1.77 & 0 & 0.51 & 1.21 & 0 & 0.46 + adaptive mm - lasso & 1.07 & 0 & 0.21 & 1.75 & 0 & 0.18 & 1.18 & 0 & 0.16 + sparse - lts & 1.16 & 0 & 0.69 & 1.76 & 0 & 0.67 & 1.24 & 0 & 0.57 + lasso & 1.07 & 0 & 0.48 & 1.82 & 0 & 0.46 & 5.38 & 0.39 & 0.29 + adaptive lasso & 1.07 & 0 & 0.31 & 1.73 & 0 & 0.31 & 5.54 & 0.47 & 0.19 + oracle & 1.04 & 0 & 0 & 1.72 & 0 & 0 & 1.10 & 0 & 0 + 3 @xmath278 & + mm - lasso & 1.69 & 0.13 & 0.21 & 1.75 & 0.10 & 0.27 & 1.28 & 0.15 & 0.17 + adaptive mm - lasso & 1.77 & 0.26 & 0.09 & 1.80 & 0.21 & 0.09 & 1.39 & 0.29 & 0.06 + sparse - lts & 2.25 & 0 & 1 & 2.14 & 0 & 1 & 1.78 & 0.01 & 0.97 + lasso & 1.74 & 0.11 & 0.22 & 1.90 & 0.12 & 0.27 & 10.7 & 0.55 & 0.21 + adaptive lasso & 1.74 & 0.21 & 0.13 & 1.94 & 0.22 & 0.14 & 10.7 & 0.72 & 0.09 + oracle & 1.63 & 0 & 0 & 1.73 & 0 & 0 & 1.27 & 0 & 0 + 4 @xmath279 & + mm - lasso & 1.90 & 0.02 & 0.12 & 2.02 & 0.02 & 0.1 & 1.90 & 0.08 & 0.09 + adaptive mm - lasso & 1.78 & 0.08 & 0.02 & 1.89 & 0.07 & 0.01 & 1.71 & 0.15 & 0.03 + sparse - lts & 3.32 & 0.14 & 0.11 & 3.17 & 0.12 & 0.01 & 2.43 & 0.13 & 0.12 + lasso & 1.96 & 0.02 & 0.23 & 2.19 & 0.02 & 0.23 & 7.79 & 0.51 & 0.1 + adaptive lasso & 2.16 & 0.04 & 0.15 & 2.41 & 0.05 & 0.16 & 9.10 & 0.56 & 0.07 + oracle & 1.64 & 0 & 0 & 1.77 & 0 & 0 & 1.28 & 0 & 0 + 5 @xmath279 & + mm - lasso & 1.92 & 0.16 & 0.08 & 1.89 & 0.11 & 0.06 & 1.42 & 0.16 & 0.05 + adaptive mm - lasso & 1.94 & 0.29 & 0.03 & 1.89 & 0.22 & 0.02 & 1.47 & 0.31 & 0.01 + sparse - lts & 1.89 & 0.13 & 0 & 1.98 & 0.11 & 0 & 1.47 & 0.15 & 0 + lasso & 1.88 & 0.11 & 0.21 & 2.12 & 0.12 & 0.22 & 6.33 & 0.57 & 0.10 + adaptive lasso & 2.06 & 0.18 & 0.13 & 2.29 & 0.19 & 0.14 & 6.75 & 0.70 & 0.10 + oracle & 1.64 & 0 & 0 & 1.77 & 0 & 0 & 1.29 & 0 & 0 + 6 @xmath280 & + mm - lasso & 4.05 & 0.12 & 0.07 & 1.99 & 0 & 0.06 & 2.05 & 0.08 & 0.05 + adaptive mm - lasso & 3.99 & 0.18 & 0.03 & 1.80 & 0.01 & 0.01 & 1.79 & 0.12 & 0.02 + sparse - lts & 4.72 & 0.26 & 0.12 & 2.60 & 0.04 & 0.09 & 2.22 & 0.07 & 0.11 + lasso & 3.67 & 0.05 & 0.07 & 2.04 & 0.01 & 0.07 & 30.5 & 0.62 & 0.03 + adaptive lasso & 3.97 & 0.06 & 0.06 & 2.26 & 0.01 & 0.06 & 31.3 & 0.64 & 0.02 + oracle & 3.13 & 0 & 0 & 1.67 & 0 & 0 & 1.12 & 0 & 0 +    p3cmp2cmp1.5cmp1.5 cm scenario & max .",
    "rmse & max .",
    "fnr & max .",
    "fpr + 1 @xmath276 & + mm - lasso & 4.38 & 0.11 & 0.57 + adaptive mm - lasso & 4.43 & 0.25 & 0.32 + sparse - lts & 4.92 & 0.07 & 0.95 + lasso & 5.78 & 0.27 & 0.49 + adaptive lasso & 6.14 & 0.36 & 0.33 + oracle mm & 3.71 & 0 & 0 + 2 @xmath277 & + mm - lasso & 1.39 & 0 & 0.59 + adaptive mm - lasso & 1.38 & 0.01 & 0.36 + sparse - lts & 1.42 & 0 & 0.92 + lasso & 4.89 & 0.19 & 0.56 + adaptive lasso & 5.13 & 0.25 & 0.38 + oracle mm & 1.21 & 0 & 0 + 3 @xmath281 & + mm - lasso & 2.02 & 0.20 & 0.35 + adaptive mm - lasso & 2.11 & 0.36 & 0.21 + sparse - lts & 3.18 & 0 & 1 + lasso & 3.05 & 0.25 & 0.26 + adaptive lasso & 3.24 & 0.41 & 0.15 + oracle mm&2.09 & 0 & 0 + 4 @xmath279 & + mm - lasso & 4.14 & 0.13 & 0.24 + adaptive mm - lasso & 4.02 & 0.21 & 0.12 + sparse - lts & 5.25 & 0.28 & 0.15 + lasso & 6.74 & 0.31 & 0.21 + adaptive lasso & 7.96 & 0.40 & 0.14 + oracle mm&2.09 & 0 & 0 + 5 @xmath279 & + mm - lasso & 2.48 & 0.21 & 0.15 + adaptive mm - lasso & 2.72 & 0.37 & 0.05 + sparse - lts & 2.14 & 0.22 & 0 + lasso & 20.25 & 0.64 & 0.15 + adaptive lasso & 13.03 & 0.79 & 0.06 + oracle mm&2.09 & 0 & 0 + 6 @xmath280 & + mm - lasso & 4.97 & 0.36 & 0.08 + adaptive mm - lasso & 5.08 & 0.45 & 0.04 + sparse - lts & 5.40 & 0.47 & 0.11 + lasso & 6.04 & 0.42 & 0.07 + adaptive lasso & 7.89 & 0.45 & 0.06 + oracle mm&3.68 & 0 & 0 +    finally , we calculated the computing times of the adaptive mm - lasso , the mm - lasso and the sparse - lts for several of the considered scenarios , for the case of normal errors and no contamination .",
    "since the computing times for the adaptive mm - lasso and the mm - lasso were very similar , we only report the results for the adaptive mm - lasso .",
    "computing times were averaged over 5 replications and calculations were performed on r 3.0.2 on a 3.07x4 ghz intel core i7 pc .",
    "we see that in scenarios 1 , 3 and 5 the sparse - lts is considerably faster than the adaptive mm - lasso .",
    "however , in scenario 6 , the adaptive mm - lasso is 3 times faster than the sparse - lts .",
    "p3cmp3cmp3 cm scenario&adaptive mm - lasso & sparse - lts + 1 @xmath276&3.33&0.7 + 3 @xmath281&7.35&1.71 + 5 @xmath279&41.75&28.51 + 6 @xmath280&8.05&25.89 +",
    "in this section , we analyse a data set corresponding to electron - probe x - ray microanalysis of archaeological glass vessels , where each of @xmath282 glass vessels is represented by a spectrum on 1920 frequencies . for each vessel",
    "the contents of thirteen chemical compounds are registered .",
    "this data set appears in @xcite , and was previously analysed in @xcite .",
    "we fit a linear model where the response variable is the content of the @xmath283 chemical compound ( pbo ) and the carriers are the 1920 frequencies measures on each glass vessel . since for frequencies below 15 and above 500",
    "the values of @xmath284 are almost null and show very little variability , we keep frequencies 15 to 500 , so that we have @xmath285 .",
    "we apply the mm - lasso , the adaptive mm - lasso , the sparse - lts , the lasso and the adaptive lasso estimators to the data .",
    "the mm - lasso selects seven variables : the @xmath286 , @xmath287 , @xmath288 , @xmath289 , @xmath290 , @xmath291 and @xmath292 frequencies .",
    "the adaptive mm - lasso selects four variables : the @xmath286 , @xmath287 , @xmath288 and @xmath291 frequencies .",
    "thus , the adaptive mm - lasso drops three of the variables selected by the mm - lasso .",
    "the sparse - lts selects three variables : the @xmath289 and @xmath292 and @xmath293 frequencies .",
    "the lasso selects 70 variables , the adaptive lasso selects 49 .",
    "hence , all three robust estimators produce models that are sparser and easier to interpret .",
    "to asses the prediction accuracy of the estimators , we used 5-fold cross - validation .",
    "the criterion used was a @xmath236-scale of the residuals , calculated as in @xcite .",
    "the adaptive mm - lasso and the lasso show the best behaviour by far , followed by the lasso , the adaptive lasso and the sparse - lts , in that order .",
    "p3cmp2cmp1.5 cm & @xmath236-scale + mm - lasso & 0.086 + adaptive mm - lasso & 0.083 + sparse - lts & 0.329 + lasso & 0.131 + adaptive lasso & 0.138 +",
    "we have studied the robust and asymptotic properties of mm - bridge and adaptive mm - bridge regression estimators .",
    "we proved that , for the case of a fixed number of covariates , mm - bridge estimators can have the _ oracle property _ defined in @xcite whenever @xmath3 .",
    "we proved that adaptive mm - bridge estimators can have the _ oracle property _ for all @xmath152 .",
    "we also derived the asymptotic distribution of the mm - ridge estimator of @xcite .",
    "we proposed an algorithm to calculate both the mm - lasso and the adaptive mm - lasso .",
    "our simulation study suggests that , at least for the scenarios considered , the proposed mm - lasso and adaptive mm - lasso estimators provide the best balance between prediction accuracy and sparse modelling for uncontaminated samples , and stability in the presence of outliers .",
    "the adaptive mm - lasso reduces the false positive ratio of the mm - lasso , with the unpleasant , and foreseeable , side effect of an increase in the false negative ratio .",
    "we note that even though we derived our asymptotic results for the case of a fixed number of covariates , the mm - lasso and adaptive mm - lasso estimators can be calculated for @xmath7 .",
    "the study of the asymptotic properties of the these estimators for regression models with a diverging number of parameters is part of our future work .",
    "take @xmath294 such that @xmath295 and a sequence @xmath296 , such that @xmath297 for @xmath298 and all @xmath299 .",
    "let @xmath300 and @xmath301denote the estimators @xmath302 and @xmath303 computed in @xmath296 . since there are a finite number of sets included in @xmath304 , to prove the theorem it will be enough to show that @xmath305 is bounded .",
    "suppose that this is not so , then eventually passing to a subsequence we can assume that @xmath306 when @xmath307 .",
    "since @xmath72 is bounded , for sufficiently large @xmath308 we have that @xmath309 which contradicts the definition of @xmath300 .",
    "let @xmath310 .",
    "take @xmath294 such that @xmath311 and a sequence @xmath296 , such that @xmath297 for @xmath298 and all @xmath299 .",
    "let @xmath312 , @xmath313 and @xmath301 denote the estimators @xmath314 , @xmath315 and @xmath303 computed in @xmath296 .",
    "note that since @xmath311 , @xmath313 is bounded . since there are a finite number of sets included in @xmath304 , to prove the theorem it will be enough to show that @xmath316 is bounded .",
    "suppose that this is not so , then eventually passing to a subsequence we can assume that for some @xmath317 , @xmath318 when @xmath307 .",
    "hence , there exists @xmath319 , such that for @xmath320 , @xmath321 .",
    "it follows that @xmath322 .",
    "it can be readily verified that @xmath328 is continuous and positive .",
    "lemma 4.2 of @xcite shows that @xmath328 has a unique minimum at @xmath329 , and hence proves the fisher consistency of s - estimators of regression .",
    "theorem 6 of @xcite , shows that @xmath330 has a unique minimum at @xmath331 , and hence proves the fisher consistency of mm - estimators of regression .",
    "theorem 4.1 of @xcite shows that @xmath340 converges almost surely to @xmath76 and so follows from .",
    "note that the second term in @xmath341 converges uniformly to zero over compact sets , and hence lemma [ lemma s unif ] and the continuity of @xmath328 show that holds .",
    "thus is proved .",
    "note that by definition of @xmath314 @xmath342 the second term in is @xmath343 since @xmath344 is consistent by assumption .",
    "since @xmath303 is consistent by assumption , by lemma [ lemma s unif ] , @xmath345 .",
    "thus , the law of large numbers and the bounded convergence theorem imply that the right hand side of converges almost surely to @xmath346      one can easily show that the graphs of the family of functions @xmath348 form a vc class of sets with a constant envelope .",
    "the proof of this is essentially the same as the one that appears on page 29 of @xcite .",
    "it follows that @xmath349 is a glivenko - cantelli class of functions , i.e. @xmath350 hence , it follows from and theorem 6 of @xcite that for any @xmath351 @xmath352                    then , since @xmath314 is strongly consistent for @xmath28 and the first @xmath39 coordinates of @xmath76 are non zero , for large enough @xmath240 the first @xmath39 coordinates of @xmath314 stay away from zero with arbitrarily high probability . applying the mean value theorem",
    "we get that @xmath365 for some @xmath366 such that @xmath367 .",
    "since @xmath368 and @xmath314 and @xmath315 are consistent , we have that for some @xmath369 , for large enough @xmath240 , with arbitrarily high probability                    then for large enough @xmath240 , with arbitrarily high probability , @xmath379 is obtained by minimizing @xmath380 over @xmath381 .",
    "we will show that if @xmath381 and @xmath382 then , for large enough @xmath240 , @xmath383 with arbitrarily high probability and the theorem will follow .      applying the mean value theorem",
    "we get @xmath385 where @xmath386 for some @xmath387 $ ] . applying the mean value theorem",
    "once more we get @xmath388 where @xmath389 . by lemma 5.1 of @xcite and lemma [ lemma",
    "s unif ] , the first term in the last equation is @xmath390 .",
    "the second term is also @xmath390 , by lemma [ lemma s unif ] , the fact that by [ b1 ] @xmath391 is bounded , [ b4 ] and the law of large numbers .    on the other hand @xmath392 since @xmath315 is @xmath140-consistent by assumption .",
    "note also that @xmath393 .",
    "hence , for some @xmath394 , for sufficiently large @xmath240 that does not depend on @xmath395 , with arbitrarily high probability , we have that @xmath396        for @xmath399 let @xmath400 .",
    "note that by theorem [ cons ] , @xmath314 is strongly consistent for @xmath76 and hence with probability 1 all the coordinates of @xmath401 stay away from zero for a sufficiently large @xmath240 .",
    "also , by theorem [ spars ] , @xmath402 with probability tending to one .",
    "then for large enough @xmath240 , with arbitrarily high probability the partial derivatives for the first @xmath39 coordinates of @xmath403 at @xmath314 exist , and hence @xmath404              we define for @xmath412 @xmath413 so that @xmath414 .",
    "we will show that for each compact set @xmath415 , @xmath416 converges weakly to @xmath417 in @xmath418 . to do so , we will verify conditions ( i ) and ( ii ) of theorem 2.3 of @xcite .",
    "then the finite - dimensional convergence follows from , , , lemma [ lemma s unif ] , lemmas 4.2 and 5.1 of @xcite , slutzky s theorem and the cramer - wold device .",
    "see the proof of theorem [ orac ] for more details .",
    "a second order taylor expansion shows that @xmath425 with @xmath426 . applying the mean value theorem to the first term in the taylor expansion we get @xmath427 with @xmath428 .",
    "then if @xmath429 , by lemma [ lemma s unif ] and lemmas 4.2 and 5.1 of @xcite and the fact that by [ b1 ] @xmath391 is bounded , we have @xmath430 let @xmath431 stand for outer probability .",
    "then it follows from that for sufficiently small @xmath223 @xmath432 where the supremum runs over @xmath433 . recalling and we see that we have proven condition ( ii ) .",
    "konis , k. , maechler , m. , marazzi , a. , maronna , r. , martin , d.r . , rocke d. , salibian - barrera , m. , wang , j. , yohai v.j . , zamar r. , zivot e. ( 2014 ) .",
    "robust : robust library .",
    "r package version 0.4 - 16 .",
    "http://cran.r-project.org/package=robust [ http://cran.r-project.org/package=robust ] .",
    "yohai , v.j .",
    "high breakdown point and high efficiency robust estimates for regression .",
    "technical report no.66 , department of statistics , university of washington , seattle , washington , usa . available at http://www.stat.washington.edu/research/reports/1985/tr066.pdf .",
    "yohai , v.j . and zamar , r.h .",
    "high breakdown point estimates of regression by means of the minimization of an efficient scale .",
    "technical report no.84 , department of statistics , university of washington , seattle , washington , usa . available at https://www.stat.washington.edu/research/reports/1986/tr084.pdf ."
  ],
  "abstract_text": [
    "<S> penalized regression estimators are a popular tool for the analysis of sparse and high - dimensional data sets </S>",
    "<S> . however , penalized regression estimators defined using an unbounded loss function can be very sensitive to the presence of outlying observations , especially high leverage outliers . </S>",
    "<S> moreover , it can be particularly challenging to detect outliers in high - dimensional data sets . </S>",
    "<S> thus , robust estimators for sparse and high - dimensional linear regression models are in need . in this paper , we study the robust and asymptotic properties of mm - bridge and adaptive mm - bridge estimators : @xmath0-penalized mm - estimators of regression and mm - estimators with an adaptive @xmath1 penalty . for the case of a fixed number of covariates , </S>",
    "<S> we derive the asymptotic distribution of mm - bridge estimators for all @xmath2 . </S>",
    "<S> we prove that for @xmath3 mm - bridge estimators can have the _ oracle property _ defined in @xcite . </S>",
    "<S> we prove that for all @xmath4 adaptive mm - bridge estimators can have the _ oracle property_. the advantages of our proposed estimators are demonstrated through an extensive simulation study and the analysis of a real high - dimensional data set .    and </S>"
  ]
}