{
  "article_text": [
    "randomized algorithms have established themselves as some of the most competitive methods for rapid low - rank matrix approximation , which is vital in many areas of scientific computing , including principal component analysis  @xcite and face recognition  @xcite , large scale data compression  @xcite and fast approximate algorithms for pdes and integral equations  @xcite . in this paper",
    ", we consider randomized algorithms for low - rank approximations and singular value approximations within the subspace iteration framework , leading to results that simultaneously retain the reliability of randomized algorithms and the typical faster convergence of subspace iteration methods .    given any @xmath2 matrix @xmath3 with @xmath4 , its singular value decomposition ( svd ) is described by the equation @xmath5 where @xmath6 is an @xmath2 column orthogonal matrix ; @xmath7 is an @xmath8 orthogonal matrix ; and @xmath9 with @xmath10 .",
    "writing @xmath6 and @xmath7 in terms of their columns , @xmath11 then @xmath12 and @xmath13 are the left and right singular vectors corresponding to @xmath14 , the @xmath15-th largest singular value of @xmath3 . for any @xmath16 ,",
    "we let @xmath17 be the ( rank-@xmath18 ) truncated svd of @xmath3 .",
    "the matrix @xmath19 is unique only if @xmath20 .",
    "the assumption that @xmath21 will be maintained throughout this paper for ease of exposition .",
    "our results still hold for @xmath22 by applying all the algorithms on @xmath23 .",
    "similarly , all our main results are derived under the assumption that @xmath24 .",
    "but they remain unchanged even if @xmath25 , and hence remain valid by a continuity argument .",
    "all our analysis is done without consideration of round - off errors , and thus need not hold exactly true in finite precision , especially when the user tolerances for the low - rank approximation are close to machine precision levels .",
    "additionally , we assume throughout this paper that all matrices are real . in general",
    ", @xmath19 is an ideal rank-@xmath18 approximation to @xmath3 , due to the following celebrated property of the svd :    [ thm : truncsvd ] ( eckart and young  @xcite , golub and van loan  @xcite ) @xmath26    while there are results similar to theorem  [ thm : truncsvd ] for all unitarily invariant matrix norms , our work on low - rank matrix approximation bounds will only focus on the two most popular of such norms : the 2-norm and the frobenius norm .",
    "theorem  [ thm : truncsvd ] states that the truncated svd provides a rank-@xmath18 approximation to @xmath3 with the smallest possible 2-norm error and frobenius - norm error . in the 2-norm",
    ", any rank-@xmath18 approximation will result in an error no less than @xmath27 , and in the frobenius - norm , any rank-@xmath18 approximation will result in an error no less than @xmath28 . additionally , the singular values of @xmath19 are exactly the first @xmath18 singular values of @xmath3 , and the singular vectors of @xmath19 are the corresponding singular vectors of @xmath3 .",
    "note , however , that while the solution to problem  ( [ eqn : truncsvdf ] ) must be @xmath19 , solutions to problem  ( [ eqn : truncsvd2 ] ) are not unique and include , for example , the rank-@xmath18 matrix @xmath29 defined below for any @xmath30 : @xmath31 this subtle distinction between the 2-norm and frobenius norm will later on become very important in our analysis of randomized algorithms ( see remark  [ rem : fvs2 ] . ) in theorem  [ thm : fto2 ] we prove an interesting result related to theorem  [ thm : truncsvd ] for rank-@xmath18 approximations that only solve problems  ( [ eqn : truncsvd2 ] ) and  ( [ eqn : truncsvdf ] ) approximately .    to compute a truncated svd of a general @xmath2 matrix @xmath3 ,",
    "one of the most straightforward techniques is to compute the full svd and truncate it , with a standard linear algebra software package like the lapack  @xcite .",
    "this procedure is stable and accurate , but it requires @xmath32 floating point operations , or _",
    "flops_. this is prohibitively expensive for applications such as data mining , where the matrices involved are typically sparse with huge dimensions . in other practical applications involving the truncated svd , often",
    "the very objective of computing a rank-@xmath18 approximation is to avoid excessive computation on @xmath3 .",
    "hence it is desirable to have schemes that can compute a rank-@xmath18 approximation more efficiently . depending on the reliability requirements ,",
    "a good rank-@xmath18 approximation can be a matrix that is accurate to within a constant factor from the optimal , such as a rank - revealing factorization ( more below ) , or it can be a matrix that closely approximates the truncated svd itself .",
    "many approaches have been taken in the literature for computing low - rank approximations , including rank - revealing decompositions based on the qr , lu , or two - sided orthogonal ( aka utv ) factorizations  @xcite .",
    "recently , there has been an explosion of randomized algorithms for computing low - rank approximations  @xcite .",
    "there is also software package available for computing interpolative decompositions , a form of low - rank approximation , and for computing the pca , with randomized sampling  @xcite .",
    "these algorithms are attractive for two main reasons : they have been shown to be surprisingly efficient computationally ; and like subspace methods , the main operations involved in many randomized algorithms can be optimized for peak machine performance on modern architectures . for a detailed analysis of randomized algorithms and an extended reference list ,",
    "see  @xcite ; for a survey of randomized algorithms in data analysis , see  @xcite .",
    "the subspace iteration is a classical approach for computing singular values .",
    "there is extensive convergence analysis on subspace iteration methods  @xcite and a large literature on accelerated subspace iteration methods  @xcite . in general , it is well - suited for fast computations on modern computers because its main computations are in terms of matrix - matrix products and qr factorizations that have been highly optimized for maximum efficiency on modern serial and parallel architectures  @xcite .",
    "there are two well - known weaknesses of subspace iteration , however , that limit its practical use . on one hand",
    ", subspace iteration typically requires very good separation between the wanted and unwanted singular values for good convergence . on the other hand",
    ", good convergence also often critically depends on the choice of a good start matrix  @xcite .",
    "another classical class of approximation methods for computing an approximate svd are the krylov subspace methods , such as the lanczos algorithm ( see , for example  @xcite . )",
    "the computational cost of these methods depends heavily on several factors , including the start vector , properties of the input matrix and the need to stabilize the algorithm .",
    "one of the most important part of the krylov subspace methods , however , is the need to do a matrix - vector product at each iteration .",
    "in contrast to matrix - matrix products , matrix - vector products perform very poorly on modern architectures due to the limited data reuse involved in such operations , in fact , one focus of krylov subspace research is on effective avoidance of matrix - vector operations in krylov subspace methods ( see , for example  @xcite . )    this work focuses on the surprisingly strong performance of randomized algorithms in delivering highly accurate low - rank approximations and singular values .",
    "to illustrate , we introduce algorithm  [ alg : randsami ] , one of the basic randomized algorithms ( see  @xcite . )",
    "[ alg : randsami]*basic randomized algorithm * +    [ cols= \" < , < \" , ]     given a set of terms ( a query ) , lsi attempts to find the document that best matches it in some semantical sense . to do so",
    ", lsi computes a rank-@xmath18 truncated svd of the term - document matrix so that @xmath33 .    for any query vector @xmath34 ,",
    "compute the feature vector @xmath35 .",
    "the document that most matches @xmath34 is the row of @xmath36 that is the most parallel to @xmath37 .",
    "we use the tdt2 text data  @xcite .",
    "the tdt2 corpus consists of data collected during the first half of 1998 and taken from 6 sources , including 2 newswires ( apw , nyt ) , 2 radio programs ( voa , pri ) and 2 television programs ( cnn , abc ) .",
    "it consists of 11201 on - topic documents which are classified into 96 semantic categories .",
    "what is available at  @xcite is a subset of this corpus , with a total of 9,394 documents and over @xmath38 terms .",
    "[ fig : lsi ]    we performed @xmath39 random queries with the truncated svd for different values of @xmath18 .",
    "then we repeat the same queries with the low - rank approximation computed by algorithm  [ alg : randsiad ] for @xmath40 and a decreasing set of @xmath41 values . for each @xmath34 and @xmath41 , algorithm  [ alg : randsiad ]",
    "automatically stops once @xmath42 column samples have been reached in computing the low - rank approximation .",
    "table  [ tab : lsi ] clearly indicates that better accuracy in randomized algorithms leads to more agreement with the truncated svd in terms of query matches .",
    "due to the nature of this experiment , an agreement does not always mean a better match .",
    "however , table  [ tab : lsi ] does give some indication that better accuracy in the low - rank approximation is probably better for lsi .",
    "since @xmath43 looks significantly better than @xmath44 , this example indicates that for lsi , it may be necessary to use algorithm  [ alg : randsiad ] with a small but positive @xmath34 value for best performance .",
    "we begin with the following probability tool .",
    "( chen and dongarra  @xcite ) [ lem : chendon ] let @xmath45 be an @xmath46 standard gaussian random matrix with @xmath47 , and let @xmath48 denote the probability density function of @xmath49 , then @xmath48 satisfies : @xmath50    the following classical result , the _ law of the unconscious statistician _",
    ", will be very helpful to our analysis .",
    "[ prop : law ] let @xmath51 be a non - negative continuously differentiable function with @xmath52 , and let @xmath45 be a random matrix , we have @xmath53    we also need to define the following functions @xmath54 where @xmath55 and @xmath56 are constants to be specified later on .",
    "it is easy to see tht @xmath52 and @xmath57 , and @xmath58    * proof of proposition  [ lem : omega2bound ] : * define a function @xmath59 . then by proposition  [ lem : fnormexpv ]",
    ", we have @xmath60 @xmath61 is a lipschitz function on matrices with lipschitz constant @xmath62 ( see theorem  [ thm : gaussfun ] ) : @xmath63 for equation  ( [ eqn : svab1 ] ) , we can rewrite , by way of function @xmath64 in  ( [ eqn : gghat ] ) and proposition  [ prop : law ] , @xmath65 by theorem  [ thm : gaussfun ] , we have @xmath66 for @xmath67 .",
    "putting it all together , @xmath68 where in the last equation we have used the fact that @xmath69 and that @xmath70 .    comparing equations  ( [ eqn : svab1 ] ) and  ( [ eqn : svb1a ] ) ,",
    "it is clear that we need to seek a @xmath71 so that @xmath72 for all values of @xmath55 .",
    "this is equivalent to @xmath73 or @xmath74 which becomes @xmath75 for @xmath76 , the right hand side reaches its maximum as @xmath77 approaches @xmath78 .",
    "hence it suffices to choose @xmath79 such that @xmath80 which solves to @xmath81 for @xmath82 and @xmath83 , we have @xmath84 .",
    "the last equation for @xmath79 is easily satisfied when we choose @xmath85 .",
    "we will now take a similar approach to prove equation  ( [ eqn : svab2 ] ) .",
    "we rewrite , by way of function @xmath86 in  ( [ eqn : gghat ] ) , @xmath87 since @xmath88 for @xmath67 , we now have @xmath89    comparing equations  ( [ eqn : svab2 ] ) and  ( [ eqn : svb1b ] ) , we now must seek a @xmath71 so that @xmath90 for all values of @xmath55 .",
    "equivalently , @xmath91 or @xmath92 which is the same as @xmath93 the right hand side approaches the maximum value as @xmath94 approaches @xmath78 .",
    "hence @xmath79 must satisfy @xmath95 which solves to @xmath96 again the choice @xmath97 satisfies this equation .",
    "* q.e.d . *    the proof for proposition  [ lem : omega1bound ] will follow a similar track . however , due to the complications with @xmath98 , we will seek help from lemma  [ lem : largdev ] instead of theorem  [ thm : gaussfun ] to shorten the estimation process .",
    "* proof of proposition  [ lem : omega1bound ] : * as in the proof of proposition  [ lem : omega2bound ] , we can write @xmath99 by lemma  [ lem : largdev ] we have for any @xmath100 , @xmath101 following arguments similar to those in the proof of proposition  [ lem : omega2bound ] , we have for a constant @xmath102 to be later determined , @xmath103 below we will derive lower bounds on  ( [ eqn : bnd1 ] ) for the three difference cases of @xmath104 in proposition  [ lem : omega1bound ] .",
    "for @xmath105 , equation  ( [ eqn : bnd1 ] ) can be simplified as @xmath106 we now seek a @xmath71 so that @xmath107 for all values of @xmath55 .",
    "this condition is very similar to equation  ( [ eqn : condelta ] ) .",
    "arguments similar to those used to solve  ( [ eqn : condelta ] ) lead to @xmath108 the choice @xmath109 satisfies this equation for @xmath110    now we consider the case @xmath111 .",
    "we rewrite equation  ( [ eqn : bnd1 ] ) in light of equation  ( [ eqn : calfac1 ] ) in  [ sec : apprelim ] : @xmath112 to prove proposition  [ lem : omega1bound ] , we just need to find a constant @xmath113 so that @xmath114 where the asymptotic term @xmath115 behaves like @xmath116 when @xmath77 is tiny and like @xmath117 when @xmath77 is very large .",
    "equation  ( [ eqn : bnd1a ] ) is equivalent to @xmath118 all the extra terms involving the @xmath119 function have added much complexity to the above expression .",
    "we cut it down with equations  ( [ eqn : calfac5 ] ) and  ( [ eqn : calfac6 ] ) in appendix  [ sec : apprelim ] by replacing all relevant expressions involving @xmath120 by their corresponding calculus upper bounds .",
    "this gives @xmath121 or @xmath122 , which holds for @xmath123 and @xmath124    the last case for our lower bound in proposition  [ lem : omega1bound ] is @xmath125 . with equation  ( [ eqn : calfac2 ] ) in  [ sec : apprelim ]",
    ": and the choice @xmath126 , equation  ( [ eqn : bnd1 ] ) reduces to @xmath127 for @xmath128    it is now time to prove equation  ( [ eqn : svab4 ] ) .",
    "our approach for @xmath105 is similar .",
    "we rewrite , by way of function @xmath86 in  ( [ eqn : gghat ] ) , @xmath129 since for any @xmath100 , @xmath130 we now have @xmath131 similarly , we seek a @xmath132 so that @xmath133 this last equation is very similar to equation  ( [ eqn : svb1c ] ) , with the only difference being the coefficients in the second term on the left hand side .",
    "thus its solution similarly satisfies @xmath134 again , the value @xmath109 satisfies this equation for @xmath135    the special cases @xmath125 and @xmath136 lead to some involved calculations with lemma  [ lem : largdev ] .",
    "instead , we will appeal to lemma  [ lem : chendon ] , an upper bound on the probability density function of smallest eigenvalue of the wishart matrix @xmath137 .",
    "it is a happy coincidence that this upper bound is reasonably tight for @xmath138 . by lemma  [ lem : chendon ] ,",
    "@xmath139 the integral in equation  ( [ eqn : chendon2 ] ) can be bounded as @xmath140    below we further simplify equation  ( [ eqn : chendon3 ] ) . for @xmath136 , the integral in  ( [ eqn : chendon3 ] )",
    "becomes , according to equation  ( [ eqn : calfac3 ] ) in  [ sec : apprelim ] : @xmath141 replacing the integral in equation  ( [ eqn : chendon3 ] ) , and plugging the resulting upper bound into equation  ( [ eqn : chendon2 ] ) , we obtain the desired equation  ( [ eqn : svab4 ] ) for @xmath111 .    finally we consider the case @xmath142 .",
    "the integral in equation  ( [ eqn : chendon3 ] ) can be rewritten as @xmath143 where we have used the substitution @xmath144 . applying the inequality @xmath145 to both factors in the denominator above , and utilizing the identity  ( [ eqn : calfac4 ] ) from  [ sec : apprelim ]",
    ", we bound the integral from above as @xmath146 which leads to the desired equation  ( [ eqn : svab4 ] ) for @xmath147 .",
    "* q.e.d . *",
    "here we list the facts we have used from calculus .",
    "their proofs have been left out , since they do not provide any additional insight into our analysis .",
    "we start with @xmath148 definite integrals : @xmath149 where @xmath77 , @xmath150 are all positive constants .",
    "we will also list the following inequalities for any @xmath151 : @xmath152                                                p.  drineas , m.  w. mahoney , and s.  muthukrishnan .",
    "subspace sampling and relative - error matrix approximation : column - based methods . in j.",
    "diaz and _ et al .",
    "_ , editors , _ approximation , randomization , combinatorial optimization _ , volume 4110 of _ lncs _ , pages 321326 , berlin , 2006 .",
    "springer .",
    "e.  liberty , n.  ailon , and a.  singer .",
    "dense fast random projections and lean walsh transforms . in a.",
    "goel , k.  jansen , j.  rolim , and r.  rubinfeld , editors , _ approximation and randomization and combinatorial optimization _ , volume 5171 of _ lecture notes in computer science _ , pages 512522 , berlin , 2008 . springer ."
  ],
  "abstract_text": [
    "<S> a classical problem in matrix computations is the efficient and reliable approximation of a given matrix by a matrix of lower rank . </S>",
    "<S> the truncated singular value decomposition ( svd ) is known to provide the best such approximation for any given fixed rank . however , the svd is also known to be very costly to compute . among the different approaches in the literature for computing low - rank approximations , </S>",
    "<S> randomized algorithms have attracted researchers recent attention due to their surprising reliability and computational efficiency in different application areas . typically , such algorithms are shown to compute with very high probability low - rank approximations that are within a constant factor from optimal , and are known to perform even better in many practical situations . in this paper </S>",
    "<S> , we present a novel error analysis that considers randomized algorithms within the subspace iteration framework and show with very high probability that highly accurate low - rank approximations as well as singular values can indeed be computed quickly for matrices with rapidly decaying singular values . </S>",
    "<S> such matrices appear frequently in diverse application areas such as data analysis , fast structured matrix computations and fast direct methods for large sparse linear systems of equations and are the driving motivation for randomized methods . </S>",
    "<S> furthermore , we show that the low - rank approximations computed by these randomized algorithms are actually rank - revealing approximations , and the special case of a rank-@xmath0 approximation can also be used to correctly estimate matrix @xmath1-norms with very high probability . </S>",
    "<S> our numerical experiments are in full support of our conclusions .    </S>",
    "<S> * key words : * low - rank approximation , randomized algorithms , singular values , standard gaussian matrix . </S>"
  ]
}