{
  "article_text": [
    "a complete characterization of the achievable rate distortion region for the classical lossy multiterminal source coding problem depicted in fig .",
    "[ fig : mtsc ] has remained an open problem for over three decades .",
    "several special cases have been solved :    * the lossless case where @xmath0 .",
    "slepian and wolf solved this case in their seminal work@xcite . * the case where one source is recovered losslessly : i.e. , @xmath1 .",
    "this case corresponds to the source coding with side information problem of ahlswede - krner - wyner @xcite,@xcite . *",
    "the wyner - ziv case @xcite where @xmath2 is available to the decoder as side information and @xmath3 should be recovered with distortion at most @xmath4 . * the berger - yeung case ( which subsumes the previous three cases )",
    "@xcite where @xmath4 is arbitrary and @xmath5 .    despite the apparent progress ,",
    "other seemingly fundamental cases , such as when @xmath4 is arbitrary and @xmath6 , remain unsolved except perhaps in very special cases .      in this paper , we give the achievable rate region for two cases subject to a particular choice of distortion measure @xmath7 , defined in section [ sec : probstatement ] .",
    "specifically , for our particular choice of @xmath7 , we give the achievable rate distortion region for the following two cases :    * the situation when @xmath8 and @xmath9 are subject to a joint distortion constraint given a reproduction @xmath10 : @xmath11\\leq d.\\end{aligned}\\ ] ] * the case where @xmath8 is subject to a distortion constraint given a reproduction @xmath12 : @xmath13\\leq d_x,\\end{aligned}\\ ] ] and there is no distortion constraint on the the reproduction of @xmath9 ( i.e. , @xmath14=@xmath15 ) .",
    "the regions depend critically on our choice of @xmath7 , which can be interpreted as a natural measure of the soft information the reproduction @xmath10 symbol provides about the source symbols @xmath8 and @xmath9 ( resp .",
    "the information @xmath12 provides about @xmath8 ) .",
    "the remainder of this paper is organized as follows . in section",
    "[ sec : probstatement ] we formally define the problem and provide our main results . in section [ sec : proofs ] , we discuss the properties of @xmath7 and provide the proofs of our main results .",
    "section [ sec : conc ] delivers the conclusions and a brief discussion regarding further directions .",
    "in this paper , we consider two cases of the lossy multiterminal source coding network presented in fig . [",
    "fig : jdxd ] .    in the first case ,",
    "we study the achievable rates @xmath16 subject to the joint distortion constraint @xmath11\\leq d,\\end{aligned}\\ ] ] where @xmath10 is the joint reproduction symbol computed at the decoder from the messages @xmath17 and @xmath18 received from the @xmath8- and @xmath9-encoders respectively .    in the second case ,",
    "we study the achievable rates @xmath16 subject to a distortion constraint on @xmath8 : @xmath13\\leq d_x,\\end{aligned}\\ ] ] where @xmath12 is the reproduction symbol computed at the decoder from the messages @xmath17 and @xmath18 received from the @xmath8- and @xmath9-encoders respectively . in this second case",
    ", there is no distortion constraint on @xmath9 .    to simplify terminology",
    ", we refer to the first and second cases described above as the joint distortion ( jd ) network and x - distortion ( xd ) network respectively .",
    "formally , define the source alphabets as @xmath19 and @xmath20 .",
    "we consider the discrete memoryless source sequences @xmath3 and @xmath2 drawn i.i.d . according to the joint distribution @xmath21 .",
    "let @xmath3 be available at the @xmath8-encoder and @xmath2 be available at the @xmath9-encoder as depicted in fig .",
    "[ fig : jdxd ] .",
    "( we will informally refer to probability mass functions as distributions throughout this paper . )    for the case of joint distortion , we consider the reproduction alphabet @xmath22 , where @xmath23 denotes the set of probability distributions on @xmath24 points . in other words , for @xmath25 , @xmath26 where @xmath27 and @xmath28 . with @xmath29 defined in this way , it will be convenient to use the notation @xmath30 for @xmath31 .",
    "note that the restriction of the reproduction alphabet to the probability simplex places constraints on the function @xmath32 .",
    "for example , one can not choose @xmath33 .",
    "define the joint distortion measure @xmath34 by @xmath35 and the corresponding distortion between the sequences @xmath36 and @xmath37 as @xmath38    as we will see in section [ sec : proofs ] , the distortion measure @xmath7 measures the amount of soft information that the reproduction symbols provide about the source symbols in such a way that the expected distortion can be described as an entropy .",
    "for example , given the output from a discrete memoryless channel , the minimum distortion between the channel input and output is the conditional entropy .",
    "for this reason , we refer to @xmath7 as an entropy - based distortion measure .",
    "the function @xmath7 is a natural distortion measure for practical scenarios .",
    "a similar distortion measure has appeared previously in the image processing literature @xcite and in the study of the information bottleneck problem @xcite .",
    "however , it does not appear to have been studied in the context of multiterminal source coding .",
    "a @xmath39-rate distortion code for the jd network consists of encoding functions , @xmath40 and a decoding function @xmath41    a vector @xmath42 with nonnegative components is achievable for the jd network if there exists a sequence of @xmath39-rate distortion codes satisfying @xmath43\\leq d.\\end{aligned}\\ ] ]    the achievable rate distortion region , @xmath44 , for the jd network is the closure of the set of all achievable vectors @xmath42 .    in a similar manner , we can also consider the case when there is only a distortion constraint on @xmath8 rather than a joint distortion constraint on @xmath45 . for this , we consider the reproduction alphabet @xmath46",
    ". with @xmath47 defined in this way , it will be convenient to use the notation @xmath48 for @xmath49 .",
    "we define the distortion measure @xmath50 by @xmath51 and the corresponding distortion between the sequences @xmath52 and @xmath53 as @xmath54    identical to the case for the jd network , we can define a @xmath39-rate distortion code for the xd network , with the exception that the range of the decoding function @xmath55 is the reproduction alphabet @xmath56 .",
    "a vector @xmath57 with nonnegative components is achievable for the xd network if there exists a sequence of @xmath39-rate distortion codes satisfying @xmath58\\leq d_x.\\end{aligned}\\ ] ]    the achievable rate distortion region , @xmath59 , for the xd network is the closure of the set of all achievable vectors @xmath57 .",
    "our main results are stated in the following theorems :    [ thm : achievableregion ] @xmath60    [ thm : achievableregionx ] @xmath61    since the distortion measure is reminiscent of discrete entropy , we can think of the units of distortion as  bits \" of distortion . thus , theorem [ thm : achievableregion ] states that for every bit of distortion we allow for @xmath45 jointly , we can remove exactly one bit of required rate from the constraints defining the slepian - wolf achievable rate region .",
    "indeed , we prove the theorem by demonstrating a correspondence between a modified slepian - wolf network and the multiterminal source coding problem in question .    similarly ,",
    "when we only consider a distortion constraint on @xmath8 , theorem [ thm : achievableregionx ] states that for every bit of distortion we tolerate , we can remove one bit of rate required by the @xmath8-encoder in the ahlswede - krner - wyner region .",
    "the proofs of theorems [ thm : achievableregion ] and [ thm : achievableregionx ] are given in the next section .",
    "we choose to prove theorems [ thm : achievableregion ] and [ thm : achievableregionx ] by showing a correspondence between schemes that achieve a prescribed distortion constraint and the well - known lossless distributed source coding scheme of slepian and wolf , and the source coding with side - information scheme of ahlswede , krner , and wyner .",
    "this provides a great deal of insight into how the various distortions are achieved .    in each case",
    ", the proof relies on a peculiar property of the distortion measure @xmath7 .",
    "namely , the ability to convert expected distortions to entropies that are easily manipulated . in the following subsection",
    ", we discuss the properties of the distortion measure @xmath7 .",
    "[ lem : expecteddistortion ] given any @xmath62 arbitrarily correlated with @xmath63 , the estimator @xmath64 $ ] produces the expected distortion @xmath65 \\geq \\frac{1}{n}\\sum_{i=1}^nh(x_i , y_i|u).\\end{aligned}\\ ] ] moreover , this lower bound can be achieved by setting @xmath66(x , y):=\\pr\\left(x_i = x , y_i = y|u = u\\right)$ ] .    given",
    "any @xmath62 arbitrarily correlated with @xmath63 , denote the reproduction of @xmath63 from @xmath62 as @xmath64\\in \\hat{\\mathcal{z}}^n$ ] . by definition of the reproduction alphabet",
    ", we can consider the estimator @xmath64 $ ] to be some probability distribution on @xmath67 conditioned on @xmath62 .",
    "then , we obtain the following lower bound on the expected distortion conditioned on @xmath68 : @xmath69 \\\\&= \\frac{1}{n}\\sum_{i=1}^n \\sum_{x , y\\in\\mathcal{x}\\times\\mathcal{y } } p_i(x , y|u ) \\log\\left(\\frac{1}{\\hat{z}_i[u](x , y ) } \\right)\\\\ & = \\frac{1}{n}\\sum_{i=1}^n d\\left(p_i(x , y|u)||\\hat{z}_i[u](x , y)\\right)+h(x_i , y_i|u = u)\\\\ & \\geq \\frac{1}{n}\\sum_{i=1}^n h(x_i , y_i|u = u),\\end{aligned}\\ ] ] where @xmath70 is the true conditional distribution . averaging both sides over all values of @xmath62 , we obtain the desired result .",
    "note that the lower bound can always be achieved by setting @xmath66(x , y):=p_i(x , y|u)$ ] .",
    "let @xmath71 be drawn i.i.d . and let @xmath72 be given .",
    "the rate distortion function with side information is @xmath73 where the minimization is over all functions @xmath74 and conditional distributions @xmath75 such that @xmath76\\leq d$ ] .        in both examples",
    ", we make the surprising observation that the distortion function @xmath7 yields a rate distortion function that is a multiple of the rate distortion function obtained using the  erasure \" distortion measure @xmath81 defined as follows : @xmath82 this is somewhat counter - intuitive given the fact that an estimator is able to pass much more  soft \" information to the distortion measure @xmath7 compared to @xmath81",
    ". it would be interesting to understand whether or not this relationship holds for general multiterminal networks , however this issue remains open .",
    "we have defined @xmath7 to be a joint distortion measure on @xmath67 , however it is possible to decompose it in a natural way .",
    "we can define the marginal and conditional distortions for @xmath8 and @xmath83 respectively by decomposing @xmath66(x , y)=\\hat{z}_i(x|u)\\hat{z}_i(y|x , u)$ ] ( note the slight abuse of notation ) .",
    "thus , if the total expected distortion is less than @xmath84 , we define the marginal and conditional distortions @xmath4 , and @xmath85 as follows : @xmath86 \\\\ & = \\mathbb{e}\\left[d_x(x^n,\\hat{z}^n ) \\right ] + \\mathbb{e}\\left[d_{y|x}(y^n,\\hat{z}^n ) \\right]\\\\ & : = d_x+d_{y|x}\\\\ & \\geq \\frac{1}{n } \\sum_{i=1}^nh(x_i|u)+h(y_i|u , x_i).\\end{aligned}\\ ] ] in a complimentary manner , we can decompose the expected distortion into @xmath87 satisfying @xmath88 .",
    "suppose a length @xmath91 code satisfies the expected distortion constraint @xmath92<d+\\epsilon/2 $ ] . by repeating the code @xmath93 times",
    ", we obtain @xmath93 i.i.d .",
    "realizations of @xmath94 . by the weak law of large numbers : @xmath95 for @xmath93 sufficiently large .        for each @xmath99 , we can rearrange ( [ eqn : dx_expression ] ) to obtain @xmath100 by the definition of @xmath37 , observe that @xmath101 is a valid probability measure on @xmath102 .",
    "thus , for any subset @xmath103 , we have @xmath104 combining ( [ eqn : dx_expressionforcontradiction ] ) and ( [ eqn : subset ] ) gives the desired result : @xmath105        the basic idea is to let the @xmath8-encoder send @xmath166 ( requiring rate @xmath167 ) and have the @xmath9-encoder send @xmath168 ( requiring rate @xmath169 ) . by lemma",
    "[ lem : conditionaldistortionsetsize ] , the number of @xmath3 sequences that lie in @xmath170 is less than @xmath171 .",
    "therefore , if the @xmath8-encoder performs a random binning of the @xmath3 sequences into @xmath128 and sends the bin index corresponding to the observed sequence @xmath3 ( incurring an additional rate of @xmath172 ) , the decoder can recover @xmath3 losslessly with high probability .",
    "since @xmath165 is an achievable rate vector , there exists some conditional distribution @xmath173 so that @xmath174 and @xmath175 .",
    "wlog , reduce @xmath167 and @xmath169 if necessary so that @xmath176 and @xmath177 .",
    "now , we construct a sequence of codes that achieve that point in the standard way .",
    "in particular , generate @xmath178 different @xmath179 sequences independently i.i.d . according to @xmath180 . upon observing @xmath2 ,",
    "the @xmath9-encoder finds a jointly typical @xmath179 and sends the corresponding index to the decoder . at the @xmath8-encoder ,",
    "bin the @xmath3 sequences into @xmath181 bins and , upon observing the source sequence @xmath3 , send the corresponding bin index to the decoder . with high probability , the decoder can reconstruct @xmath3 losslessly .      1 .",
    "use the lossless code described above with probability @xmath182 . in this case , the distortion on @xmath8 can be made arbitrarily small .",
    "note that we can assume w.l.o.g . that @xmath183 since distortion @xmath184 can be achieved when the decoder only receives the sequence @xmath179 .",
    "2 .   with probability @xmath185 ,",
    "the @xmath8-encoder sends nothing , while the @xmath9-encoder continues to send @xmath179 . in this case",
    ", the distortion on @xmath8 is @xmath186 .",
    "r. ahlswede and j. krner .",
    "source coding with side information and a converse for the degraded broadcast channel .",
    "ieee trans .",
    "theory , it-21:629 - 637 , 1975 .",
    "a. wyner . on source coding with side information at the decoder .",
    "ieee trans .",
    "theory , it-21:294 - 300 , 1975 .",
    "i. csiszr and j. krner . towards a general theory of source networks .",
    "ieee trans .",
    "theory , it-26:155 - 165 , 1980 . i. csiszr and j. krner .",
    "information theory : coding theorems for discrete memoryless systems . academic press , new york , 1981 ."
  ],
  "abstract_text": [
    "<S> in this paper , we consider a class of multiterminal source coding problems , each subject to distortion constraints computed using a specific , entropy - based , distortion measure . </S>",
    "<S> we provide the achievable rate distortion region for two cases and , in so doing , we demonstrate a relationship between the lossy multiterminal source coding problems with our specific distortion measure and ( 1 ) the canonical slepian - wolf lossless distributed source coding network , and ( 2 ) the ahlswede - krner - wyner source coding with side information problem in which only one of the sources is recovered losslessly . </S>"
  ]
}