{
  "article_text": [
    "feature selection is a dimensionality reduction technique for a wide range of search problems .",
    "a common practice used in machine learning is to find subset of available features for a learning algorithm .",
    "the best subset contains the least number of dimensions that most contribute to accuracy  @xcite .",
    "feature selection differs from standard dimensionality reduction .",
    "both methods seek to reduce the number of attributes in the dataset , but a dimensionality reduction method does so by creating new combinations of attributes , where as feature selection methods includes and excludes attributes in the data without changing them .",
    "the central premise when using a feature selection technique is that the data contains many features that are either _ redundant _ or _ irrelevant _ , and can thus be removed without incurring much loss of information  @xcite .",
    "redundant features refer to those having information which is already contained in other features , while irrelevant features are those with no useful information related to the class variable .",
    "feature selection mostly acts as a filter , muting out features that are not useful in addition to the existing features , and thus avoid over - fitting , speed up computation , or improve the interpretability of the results .",
    "feature selection techniques can be broadly classified into two groups : classifier dependent ( _ wrapper _",
    "@xcite , _ embedded _",
    "@xcite ) and classifier independent _ filter _",
    "classifier dependent methods exploit the label information as a measure to guide the feature selection process .",
    "examples are backward elimination and forward selection . in forward selection",
    ", variables are progressively incorporated into larger subsets , whereas in backward elimination one starts with the set of all variables and progressively eliminates the least promising ones  @xcite .",
    "the inherent advantage of this class of methods are that multi - collinearity issues are automatically handled .",
    "however , no prior knowledge about the actual relationship between the variables can be inferred .",
    "classifier independent methods , on the other hand , define a scoring function between features and labels in the selection process .",
    "they are based only on general features like the correlation with the variable to predict .",
    "filter methods suppress the least interesting variables and are particularly effective in computation time and robust to over - fitting .",
    "a very simple _ filter _ type feature selection algorithm is a basic correlation analysis  @xcite , where only attributes which are correlated to the label are be chosen in a predictive analytics model .",
    "correlation analysis helps to identify these  features \" right at the outset and allows a better understanding of how the attribute affects the predicted variable .",
    "however correlation table needs to be generated and collinear variables must be eliminate  by hand \" .",
    "the mutual information mi ( also called cross entropy or information gain ) is a widely used information theoretic measurement for the stochastic dependency of discrete random variables .",
    "mi based feature selection works similar to a standard correlation analysis , except that it is more robust when the variables are noisy or non - linearly related to the predicted or target variable .",
    "heuristic greedy algorithms are adopted to find out the most relevant and least redundant feature at each step .",
    "brown  @xcite show that the redundant term should refer to conditional mutual information and summarizes a uniform framework for information theoretic feature selection .",
    "however , due to the computational cost , the joint probability density functions ( pdf ) are often approximated as multiplication of single variable distribution functions .",
    "this is based on two inconsistent independence assumptions in gao  @xcite , which can be seen as first order approximation of information gain , leading to the miss of multi - way feature combinations .",
    "this feature independence assumption produces singleton feature elements , thereby failing to capture he joint mi gain over a collection of correlated variables .",
    "a higher order approximation is thus needed to counter this scenario and get a better comprehension of the inter - feature dependence .",
    "the ica ( independent component analysis ) algorithm , based on infomax or maximum likelihood estimation , tries to represent an input vector @xmath0 as a linear combination of independent signal vector @xmath1 e.g. @xmath2 , where each @xmath3 and @xmath4 represents a random variable and @xmath5 is called the mixing matrix  @xcite .",
    "the connection between ica algorithm and information theory has been well known for many years  @xcite .",
    "however , to our knowledge , most of work concerns extracting signal vector @xmath6 by finding out the optimal unmixing matrix ( @xmath7 ) , that can minimize the mutual information(mi ) between @xmath4 , which leads to large running time .    to address all these issues ,",
    "this paper presents a feature selection method , termed higher order feature selection ( hofs ) .",
    "this integrates ica to approximate the mi in higher order dimension . instead of producing a single list of features ,",
    "we provide several subsets of features that maximizes mi , giving better comprehension as to which features work best when selected together due to their underlying interdependent structure .",
    "this addresses feature ranking , by scoring the feature subsets based on their combined predictive power , which is a significant advantage over lower order models .",
    "our model also considers simplifying the computational cost of calculating multi - variable mi using ica .",
    "we compare the results of our experiments against well known feature selection algorithms on publicly available datasets .",
    "observations show that we perform at par if not better than them in classification error and global mi captured using the subset of selected features , while keeping similar running times and computational complexity .",
    "rest of the paper is organized as follows : sec .",
    "[ sec2 ] sets up the basic framework of mi based feature selection approach with notations , definitions and prior work . sec .",
    "[ sec3 ] discusses the lo - order and ica issues which we are trying to address in this paper . in sec .",
    "[ sec4 ] , we introduce the proposed higher order feature selection ( hofs ) approach with its computation analysis and running times . all experiments on publicly available real datasets and their comparison results are illustrated in sec .",
    "experiments performed on synthetic data ( numerical graphical models and heterogeneous feature models ) for model checking are tabulated in sec .",
    "we conclude our findings and discuss some of the future problems that can be addressed in sec .",
    "let @xmath8 denote a possible value of the discrete random variable ( r.v . )",
    "@xmath9 which is drawn from the alphabet set @xmath10 .",
    "also , let @xmath11 define the probability density function ( pdf ) of @xmath9 , and @xmath12 denote the probability that r.v . @xmath9 takes value @xmath8 .",
    "when @xmath9 is discrete , the probability can be estimated as a fraction of observations @xmath13 , taking on value @xmath8 from the total @xmath14 .",
    "an event @xmath15 is indicated as a draw of a value from the set .",
    "also let @xmath16 be the total number of features in @xmath9 , with @xmath17)$ ] as the current feature which is considered for selection , @xmath18 denote its label and @xmath19 denote the sparse set of selected features with elements @xmath20 for @xmath21 $ ] , where @xmath22 denotes its cardinality .",
    "entropy and mutual information are two well - known concepts in information theory , which are used to measure the information provided by random variables .    * entropy @xmath23 : * it is a measure of uncertainty of the random variable @xmath9 , or the average amount of information that we receive with every event . the more certain @xmath9 is , higher the value of @xmath23 , @xmath24    * joint entropy @xmath25 : * it is the measure of uncertainty of a joint variable , which consists of two random variables @xmath9 and @xmath26 .",
    "the joint entropy of a set of variables is greater than or equal to all of the individual entropies of the variables in the set . @xmath27 where @xmath28 .",
    "* conditional entropy @xmath29 : * it quantifies the amount of information needed to describe the outcome of a random variable , given that the value of another random variable .",
    "@xmath30 where @xmath31 .    * mutual information ( mi ) @xmath32 :",
    "* it is a measure of shared information between two random variables .",
    "mi between two r.v .",
    "@xmath9 and @xmath26 can be defined as in eq .",
    "[ eq : mi ] : @xmath33 which can be regarded as the certainty gain of random variable @xmath9 when conditioned of label @xmath26 . in this case",
    ", the higher value of @xmath34 means the stronger relation between @xmath9 and @xmath26 .",
    "lewis  @xcite and duch  @xcite coined the term mutual information maximization ( mim ) as a feature scoring criteria , which measures the usefulness of a feature subset when used for classification .",
    "this heuristic considers a score for each feature independently of others , which is a limitation by itself , yet is known to be suboptimal in those cases .",
    "@xmath35 battiti  @xcite defined the mutual information feature selection ( mifs ) criteria which ensures feature relevance and forces a penalty to ensure low correlations within the set of selected features , in a sequential manner .",
    "setting the penalty term @xmath36 to zero would result in mim scores .",
    "@xmath37 yang and moody  @xcite and meyer  @xcite proposed an alternate version of mifs , called the joint mutual information ( jmi ) , which increases complementary information between features .",
    "this is able to capture the relevance - redundancy trade - off with various heuristic terms .",
    "@xmath38 peng  @xcite gave the minimum - redundancy maximum - relevance ( mrmr ) criteria which omits the conditional relevance term completely .",
    "the penalty term is inversely proportional to the size of current feature set .",
    "as the set grows the penalty term will diminish , hence all selected features will become strongly pairwise independent .",
    "@xmath39 fleuret  @xcite introduced the conditional mutual information maximization ( cmim ) criteria , which assumes that selected features are independent and class - conditionally independent given the unselected feature .",
    "this criterion ensures a good trade - off between independence and discrimination.@xmath40\\end{aligned}\\ ] ] nguyen  @xcite proposed a global mi - based feature selection via spectral relaxation ( spec    cmi    ) approach .",
    "they have the ability to handle second - order feature dependency , by favoring features having large total pairwise conditional relevance .",
    "they also show that for large data , low rank approximation can be applied to gain computational advantage to our global algorithm over its greedy counterpart .",
    "@xmath41 gao  @xcite produced variational information maximization ( vmi ) which can be applied for feature selection over any general class of distributions and provide tractable lower bounds for mutual information .",
    "their model is optimal if the data is generated according to tree graphical models .",
    "they also outperform previous methods in speed .",
    "@xmath42    .notation [ cols=\"^,^\",options=\"header \" , ]",
    "mutual information ( mi ) defines a measurement of how informative the features are .",
    "feature selection based on mi has been developed a lot over the past decade .",
    "however , the computation of global mi is a np - hard problem .",
    "thus , most of those algorithms are forced to do a lower order estimation .",
    "we introduce an improved method ( hofs ) to estimate the global mi by integrating incremental ica algorithm .",
    "our method clearly performs better than most owing to its ability to select feature subsets that jointly maximize mi , while keeping similar running times and computational complexity as the current approaches .",
    "we would like to extend our work to provide minimal cardinality feature subsets for a wide range of datasets including geometric and hyperspectral .",
    "m.  l. bermingham , r.  pong - wong , a.  spiliopoulou , c.  hayward , i.  rudan , h.  campbell , a.  f. wright , j.  f. wilson , f.  agakov , p.  navarro , et  al .",
    "application of high - dimensional feature selection : evaluation for genomic prediction in man .",
    ", 5:10312 , 2015 .",
    "d.  d. lewis .",
    "feature selection and feature extraction for text categorization . in _ proceedings of the workshop on speech and natural language _ , pages 212217 .",
    "association for computational linguistics , 1992 .",
    "x.  v. nguyen , j.  chan , s.  romano , and j.  bailey .",
    "effective global approaches for mutual information based feature selection . in _ proceedings of the 20th acm sigkdd international conference on knowledge discovery and data mining _ , pages 512521 .",
    "acm , 2014 ."
  ],
  "abstract_text": [
    "<S> feature selection is a process of choosing a subset of relevant features so that the quality of prediction models can be improved . </S>",
    "<S> an extensive body of work exists on information - theoretic feature selection , based on maximizing mutual information ( mi ) between subsets of features and class labels . </S>",
    "<S> the prior methods use a lower order approximation , by treating the joint entropy as a summation of several single variable entropies . </S>",
    "<S> this leads to locally optimal selections and misses multi - way feature combinations . </S>",
    "<S> we present a higher order mi based approximation technique called higher order feature selection ( hofs ) . instead of producing a single list of features , our method produces a ranked collection of feature subsets that maximizes mi , giving better comprehension ( feature ranking ) as to which features work best together when selected , due to their underlying interdependent structure . </S>",
    "<S> our experiments demonstrate that the proposed method performs better than existing feature selection approaches while keeping similar running times and computational complexity . </S>"
  ]
}