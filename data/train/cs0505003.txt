{
  "article_text": [
    "in the beginning of the 1980s , hopfield  @xcite and his colleges published two scientific papers on  neuron \" computation .",
    "hopfield showed that highly interconnected networks of nonlinear analog neurons are extremely effective in solving optimization problems . from that time on , people has being applying the hopfield network to solve a wide class of combinatorial optimization problems ( see a survey  @xcite ) .    in a discrete - time version ,",
    "the hopfield network implemented local search . in a continuous - time version",
    ", it implemented gradient decent .",
    "both algorithms suffer the local minimum problem and many optimization problems in practice have lots of local minima .",
    "furthermore , the hopfield - tank formulation of the energy function of the network causes infeasible solutions to occur most of the time  @xcite .",
    "people also found that those valid solutions were only slightly better than randomly chosen ones .    to guarantee the feasibility of the solutions , the most important breakthrough came from the valid subspace approaches of aiyer et al  @xcite and gee  @xcite .",
    "however , it requires researchers to design a constraint energy function to make solution feasible , add it to the original energy function , and recalculate the energy function to obtain new connection weights .",
    "it is not simple and is unlikely that biological neural networks also implement such a process . to escape from local minima ,",
    "many variations of the hopfield network have been proposed based on the principles of simulated annealing  @xcite .",
    "three major approaches are boltzmann  @xcite , cauchy  @xcite , and gaussian machines  @xcite . in theory",
    ", simulated annealing can approach the global optimal solution in exponential time .",
    "however , it is not guaranteed and is very slow to make it effective in practice . like local search",
    ", it does nt know how long it will take to converge .",
    "it also does not know if a solution is a global optimum so that the search process can be stopped .",
    "those improvements make the hopfield network competitive with conventional optimization algorithms , such as simulated annealing .",
    "however , it can not be more powerful than those algorithms because it is just the implementations of those algorithms using interconnected networks of computing units , such as neurons .",
    "its capability is restricted due to the limitations of the network structure and the theoretical limitations of the optimization algorithms it implements .",
    "those conventional optimization algorithms have both performance problems and convergence problems , far from satisfactory in solving problems in practice . for example",
    ", stereo matching is an important problem in computer vision , and one of the most active research areas in that field  @xcite .",
    "compared with many specialized optimization algorithms , such as graph cuts  @xcite , simulated annealing has the worst performance in both the solution quality and computing time .",
    "people in the computer vision community even do not need to put it in the comparison list in the evaluation of different optimization algorithms for stereo matching  @xcite .    in this paper",
    ", we present a hopfield network with a new set of difference equations to fix those problems . in solving large scale optimization problems in computer vision ,",
    "it significantly outperform general optimization algorithms , such as simulated annealing and local search with multi - restarts , as well as specialized algorithms .",
    "one of the most popular energy functions used in computer vision and other areas has the following general form : @xmath0 where each variable @xmath1 , for @xmath2 , has a finite domain @xmath3 of size @xmath4 ( @xmath5 ) .",
    "@xmath6 is a real - valued function on variable @xmath1 , called the unary constraint on @xmath1 and @xmath7 is a real - valued function on variable @xmath1 and @xmath8 , called the binary constraint on @xmath1 and @xmath8 .",
    "the optimization of the above energy function is np - hard .",
    "it is called the binary constraint optimization problem in computer science , a special case of constraint model where each constraint involves at most two variables .    without loss of generality , we assume all constraints are nonnegative functions through out this paper . also , we focus on the minimization of the function  ( [ binary_cost_function ] ) because minimization and maximization are logically equivalent .",
    "following the hopfield network formulation , we use a number of neurons , one for each value of each variable . if variable @xmath1 has @xmath4 values , we have @xmath4 neurons , one for each value .",
    "this set of neurons is called a neuron group . in total",
    ", we have @xmath9 neurons organized as @xmath10 groups for @xmath10 variables .",
    "the state of the neuron is denoted as @xmath11 for the value @xmath1 of the @xmath12th variable . because we are dealing with the minimization problem , for the convenience of the mathematical manipulation , we make @xmath13 and use lower values to indicate higher activation . if @xmath14 , the corresponding neuron has the highest activation level .",
    "different from the hopfield network , we use a new set of difference equations shown as follows : @xmath15 where @xmath16 is a threshold function with the threshold @xmath17 .",
    "the threshold function @xmath18 is defined as @xmath19 and @xmath20 is defined as @xmath21 where @xmath22 is the iteration step , @xmath23 are non - negative weights satisfying @xmath24 .",
    "parameter @xmath25 is a weight satisfying @xmath26    to ensure the feasibility of solutions , we follow the winner - take - all principle by using the threshold function to progressively inhibit more and more neurons in the same group .",
    "eventually , we let only one neuron be active in one group .",
    "those values that the remaining active neurons corresponding to constitute a solution to the original combinatorial optimization problem  ( [ binary_cost_function ] ) .",
    "this approach is different from the one used by the hopfield network .",
    "the hopfield network adds a new constraint energy function to ensure the feasibility of solutions .",
    "this set of difference equations is a direct neural network implementation of a new optimization algorithm ( detail in the following subsection ) for energy function minimization in a general form .",
    "this algorithm is based a principle for optimization , called cooperative computation , completely different from many existing ones .",
    "given an optimization problem instance , the computation always has a unique equilibrium and converges to it with an exponential rate regardless of initial conditions and perturbations .",
    "it is important to note that , different from the hopfield network in a discrete - time version , our set of difference equations does not require to update one state at one time .",
    "all states for all values of all variables can be updated simultaneously in parallel .",
    "it is another advantage of this neural network over the classical hopfield network .      to solve a hard combinatorial optimization problem",
    ", we follow the divide - and - conquer principle .",
    "we first break up the problem into a number of sub - problems of manageable sizes and complexities . following that",
    ", we assign each sub - problem to an agent , and ask those agents to solve the sub - problems in a cooperative way .",
    "the cooperation is achieved by asking each agent to compromise its solution with the solutions of others instead of solving the sub - problems independently .",
    "we can make an analogy with team playing , where the team members work together to achieve the best for the team , but not necessarily the best for each member . in many cases ,",
    "cooperation of this kind can dramatically improve the problem - solving capabilities of the agents as a team , even when each agent may have very limited power .    to be more specific ,",
    "let @xmath27 be a multivariate objective function , or simply denoted as @xmath28 , where each variable @xmath1 has a finite domain @xmath3 of size @xmath4 ( @xmath5 ) .",
    "we break the function into @xmath10 sub - objective functions @xmath29 ( @xmath30 ) , such that @xmath29 contains at least variable @xmath1 for each @xmath12 , the minimization of each objective function @xmath29 ( the sub - problem ) is computational manageable in sizes and complexities , and @xmath31    for example , a binary constraint optimization problem  ( [ binary_cost_function ] ) has a straight - forward decomposition : @xmath32    the @xmath10 sub - problems can be described as : @xmath33 where @xmath34 is the set of variables that sub - objective function @xmath29 contains .    because of the interdependence of the sub - objective functions , as in the case of the binary constraint - based function  ( see eq .",
    "( [ binary_cost_function ] ) ) , minimizing those sub - objective functions in such an independent way can hardly yield a consensus in variable assignments . for example , the assignment for @xmath1 that minimizes @xmath29 can hardly be the same as the assignment for the same variable that minimizes @xmath35 if @xmath35 contains @xmath1 .",
    "we need to solve those sub - problems in a cooperative way so that we can reach a consensus in variable assignments .    to do that",
    ", we can break the minimization of each sub - objective function  ( see ( [ sub - problem ] ) ) into two steps , @xmath36 where @xmath37 denotes the set @xmath34 minuses @xmath38 .",
    "that is , first we optimize @xmath29 with respect to all variables that @xmath29 contains except @xmath1 .",
    "this gives us the intermediate solution in optimizing @xmath29 , denoted as @xmath11 , @xmath39 second , we optimize @xmath11 with respect to @xmath1 , @xmath40    the intermediate solutions of the optimization , @xmath41 , is an unary constraint on @xmath1 introduced by the algorithm , called the assignment constraint on variable @xmath1 . given a value of @xmath1 , @xmath41 is the minimal value of @xmath29 . to minimize @xmath29 , those values of @xmath1 which have smaller assignment constraint values @xmath41",
    "are preferred more than those of higher ones .",
    "to introduce cooperation in solving the sub - problems , we add the unary constraints @xmath42 , weighted by a real value @xmath43 , back to the right side of ( [ unary - constraint0 ] ) and modify the functions  ( [ unary - constraint0 ] ) to be iterative ones : @xmath44 where @xmath22 is the iteration step , @xmath23 are non - negative weight values satisfying @xmath24 . it has been found  @xcite that such a choice of @xmath23 makes sure the iterative update functions converge .",
    "the function at the right side of the equation is called the modified sub - objective function , denoted as @xmath45 .    by adding back @xmath42 to @xmath29",
    ", we ask the optimization of @xmath29 to compromise its solution with the solutions of the other sub - problems . as a consequence , the cooperation in the optimization of all the sub - objective functions ( @xmath29s )",
    "is achieved .",
    "this optimization process defined in ( [ cooperative_optimization ] ) is called the cooperative optimization of the sub - problems .",
    "parameter @xmath25 in ( [ cooperative_optimization ] ) controls the level of the cooperation at step @xmath22 and is called the cooperation strength , satisfying @xmath26 a higher value for @xmath25 in ( [ cooperative_optimization ] ) will weigh the solutions of the other sub - problems @xmath42 more than the one of the current sub - problem @xmath29 . in other words ,",
    "the solution of each sub - problem will compromise more with the solutions of other sub - problems . as a consequence , a higher level of cooperation in the optimization",
    "is reached in this case .",
    "the update functions  ( [ cooperative_optimization ] ) are a set of difference equations of the assignment constraints @xmath11 .",
    "unlike conventional difference equations used by probabilistic relaxation algorithms  @xcite , cooperative computations  @xcite , and hopfield networks  @xcite , this set of difference equations always has one and only one equilibrium given @xmath43 and @xmath23 .",
    "the computation converges to the equilibrium with an exponential rate , @xmath43 , regardless of initial conditions of @xmath46 .",
    "those computational properties will be shown in theorems in the next section and their proofs are provided in @xcite .    by minimizing the linear combination of @xmath29 and @xmath42 , which are the intermediate solutions for other sub - problems",
    ", we can reasonably expect that a consensus in variable assignments can be reached .",
    "when the cooperation is strong enough , i.e. , @xmath47 , the difference equations  ( [ cooperative_optimization ] ) are dominated by the assignment constraints @xmath42 , it appears to us that the only choice for @xmath8 is the one that minimizes @xmath42 for any @xmath29 that contains @xmath8 .",
    "that is a consensus in variable assignments .",
    "theory only guarantees the convergence of the computation to the unique equilibrium of the difference equations .",
    "if it converges to a consensus equilibrium , the solution , which is consisted of the consensus assignments for variables , must be the global optimum of the objective function @xmath28 , guaranteed by theory ( detail in the next section ) .",
    "however , theory does nt guarantee the equilibrium to be a consensus , even by increasing the cooperation strength @xmath43 .",
    "otherwise , np = p .",
    "in addition to the cooperation scheme for reaching a consensus in variable assignments , we introduce another important operation of the algorithm , called variable value discarding , at each iteration .",
    "a certain value for a variable , say @xmath1 , can be discarded if it has a assignment constraint value , @xmath11 that is higher than a certain threshold , @xmath48 , because they are less preferable in minimizing @xmath29 as explained before . there",
    "do exist thresholds from theory for doing that ( detail in the next section ) .",
    "those discarded values are those that can not be in any global optimal solution . by discarding values",
    ", we can trim the search space .",
    "if only one value is left for each variable after a certain number of iterations using the thresholds provided theory , they constitute the global optimal solution , guaranteed by theory  @xcite .",
    "however , theory does not guarantee that one value is left for each variable in all cases .",
    "otherwise , np = p .",
    "this value discarding operation can be interpreted as neuron inhibition following the winner - take - all principle if we implement this algorithm using neural networks .    by discarding values",
    ", we increase the chance of reaching a consensus equilibrium for the computation . in practice",
    ", we progressively tighten the thresholds to discard more and more values as the iteration proceeds to increase the chance of reaching a consensus equilibrium . in the end",
    ", we leave only one value for each variable .",
    "then , the final solution is a consensus equilibrium .",
    "however , by doing that , such a final solution is not guaranteed to be the global optimum .",
    "nevertheless , in our experiments in solving large scale combinatorial optimization problems , we found that the solution quality of this algorithm is still satisfactory , significantly better than that of other conventional optimization methods , such as simulated annealing and local search  @xcite .      in the previous sub - section ,",
    "we choose @xmath23 such that it is non - zero if @xmath8 is contained by @xmath29 . for a binary constraint optimization problem using the decomposition  ( [ decomposition_binary ] )",
    ", it implies that we choose @xmath23 be non - zero if and only if @xmath8 is a neighbor of @xmath1 .",
    "however , theory tells us that this is too restrictive . to make the algorithm to work",
    ", we only need to choose @xmath49 to be a propagation matrix defined as follows :    a propagation matrix @xmath50 is a irreducible , nonnegative , real - valued square matrix and satisfies @xmath51 [ definition_propagation_matrix ]    a matrix @xmath52 is called reducible if there exists a permutation matrix @xmath53 such that @xmath54 has the block form @xmath55    the system is called reaching a consensus solution if , for any @xmath12 and @xmath56 where @xmath35 contains @xmath1 , @xmath57 where @xmath45 is defined as the function to be minimized at the right side of eq .  ( [ cooperative_optimization ] ) .",
    "[ consensus ]    an equilibrium of the system is a solution to @xmath11 , @xmath58 , that satisfies the difference equations  ( [ cooperative_optimization ] ) .    to simplify the notations in the following discussions ,",
    "let @xmath59 let @xmath60 , the favorable value for assigning variable @xmath1 .",
    "let @xmath61 , a candidate solution at iteration @xmath22 .",
    "the following theorem shows that @xmath62 for @xmath63 have a direct relationship to the lower bound on the objective function @xmath28 .",
    "given any propagation matrix @xmath52 and the general initial condition @xmath64 or @xmath65 , @xmath66 is a lower bound function on @xmath67 , denoted as @xmath68 .",
    "that is @xmath69 in particular , let @xmath70 , then @xmath71 is a lower bound on the optimal cost @xmath72 , that is @xmath73 .",
    "[ theorem_1 ]    here , subscript `` - '' in @xmath71 indicates that it is a lower bound on @xmath72 .",
    "this theorem tells us that @xmath74 provides a lower bound on the objective function @xmath75 .",
    "we will show in the next theorem that this lower bound is guaranteed to be improved as the iteration proceeds .    given any propagation matrix @xmath52 , a constant cooperation strength @xmath43 , and the general condition @xmath64",
    ", @xmath76 is a non - decreasing sequence with upper bound @xmath72 .",
    "[ theorem_3 ]    if a consensus solution is found at some step or steps , then we can find out the closeness between the consensus solution and the global optimum in cost .",
    "if the algorithm converges to a consensus solution , then it must be the global optimum also .",
    "the following theorem makes these points clearer .",
    "given any propagation matrix @xmath52 , and the general initial condition @xmath64 or @xmath77 .",
    "if a consensus solution @xmath78 is found at iteration step @xmath79 and remains the same from step @xmath79 to step @xmath80 , then the closeness between the cost of @xmath78 , @xmath81 , and the optimal cost , @xmath72 , satisfies the following two inequalities , @xmath82 @xmath83 where @xmath84 is the difference between the optimal cost @xmath72 and the lower bound on the optimal cost , @xmath85 , obtained at step @xmath86 . when @xmath87 and @xmath88 for @xmath89 , @xmath90 .",
    "[ theorem_2 ]      the performance of the cooperative algorithm further depends on the dynamic behavior of the difference equations  ( [ cooperative_optimization ] ) .",
    "its convergence property is revealed in the following two theorems .",
    "the first one shows that , given any propagation matrix and a constant cooperation strength , there does exist a solution to satisfy the difference equations ( [ cooperative_optimization ] ) .",
    "the second part shows that the cooperative algorithm converges exponentially to that solution .",
    "given any symmetric propagation matrix @xmath52 and a constant cooperation strength @xmath43 , then difference equations  ( [ cooperative_optimization ] ) have one and only one solution , denoted as @xmath91 or simply @xmath92 .",
    "[ theorem_7 ]    given any symmetric propagation matrix @xmath52 and a constant cooperation strength @xmath43 , the cooperative algorithm , with any choice of the initial condition @xmath93 , converges to @xmath94 with an exponential convergence rate @xmath43 .",
    "that is @xmath95 [ theorem_8 ]    this theorem is called the convergence theorem .",
    "it indicates that our cooperative algorithm is stable and has a unique attractor , @xmath94 .",
    "hence , the evolution of our cooperative algorithm is robust , insensitive to perturbations , and the final solution of the algorithm is independent of initial conditions . in contrast ,",
    "conventional algorithms based on iterative improvement have many local attractors due to the local minima problem .",
    "the evolutions of these algorithms are sensitive to perturbations , and the final solutions of these algorithms are dependent on initial conditions .",
    "the two necessary conditions provides in this subsection allows us to discard variable values that can not be in any global optimum .",
    "given a propagation matrix @xmath52 , and the general initial condition @xmath64 or @xmath65 . if value @xmath96",
    "( @xmath97 ) is in the global optimum , then @xmath98 , for any @xmath99 , must satisfy the following inequality , @xmath100 where @xmath71 is , as defined before , a lower bound on @xmath72 obtained by the cooperative system at step @xmath22 .",
    "[ necessary_condition_1 ]    given a symmetric propagation matrix @xmath52 and the general initial condition @xmath64 or @xmath77 . if value @xmath96",
    "( @xmath97 ) is in the global optimum , then @xmath98 must satisfy the following inequality , @xmath101 here @xmath102 is computed by the following recursive function : @xmath103 where @xmath104 is the second largest eigenvalue of the propagation matrix @xmath52 .    for the particular choice of w=@xmath105 , @xmath106 and @xmath107 [ necessary_condition_2 ]    inequality  ( [ eqn_necessary_condition_1 ] ) and inequality  ( [ eqn_necessary_condition_2 ] ) provide two criteria for checking if a value can be in some global optimum .",
    "if either of them is not satisfied , the value can be discarded from the value set to reduce the search space .",
    "both thresholds in ( [ eqn_necessary_condition_1 ] ) and ( [ eqn_necessary_condition_2 ] ) become tighter and tighter as the iteration proceeds . therefore ,",
    "more and more values can be discarded and the search space can be reduced . with the choice of the general initial condition @xmath64 , the right hand side of ( [ eqn_necessary_condition_1 ] ) decreases as the iteration proceeds because of the property of @xmath71 revealed by theorem  [ theorem_3 ] . with the choice of a constant cooperation strength @xmath43 , and suppose @xmath108 , then @xmath109 and @xmath110 is a monotonic decreasing sequence satisfying @xmath111 this implies that the right hand side of ( [ eqn_necessary_condition_2 ] ) monotonically decreases as the iteration proceeds .",
    "the proposed algorithm has outperformed many well - known optimization algorithms in solving real optimization problems in computer vision@xcite , image processing@xcite , and data communications .",
    "these experiment results give strong evidence of the algorithm s considerable potential .",
    "we provides in this section the performance comparison of the new hopfield networks with cooperative optimization and boltzmann machine network for stereo matching  @xcite .",
    "the boltzmann machine is simply a discrete time hopfield network in which the dynamic function of each neuron is defined by simulated annealing  @xcite .",
    "simulated annealing is a well - known optimization method which is based on stochastic local optimization .",
    "stereo vision is an important process in the human visual perception . as of now",
    ", there is still a lack of satisfactory computational neural model for it . to understand such an important process",
    ", people treat stereo vision as stereo matching .",
    "stereo matching is to use a pair of 2-d images of the same scene taken at the same time but two different locations to recover the depth information of the scene ( see fig .",
    "[ twoimages ] ) .    instead of using toy problems , we tested both types of neural networks with real problems .",
    "four pairs of images including the one shown in fig .",
    "[ twoimages ] are used in our experiments .",
    "the ground truth , the depth images obtained by boltzmann machine and by the new hopfield network with cooperative optimization are shown in fig .  [ groundtruth ] . clearly , the results of new hopfield with cooperative optimization are much cleaner , much smoother , and much better than the results of boltzmann machine .    table  [ table_1 ] lists the minimal energies found by the two types of neural networks .",
    "those found the new hopfield network are much lower those found by boltzmann machine .",
    ".the minimal energies found by neural networks with different dynamics . [ cols=\"^,^,^ \" , ]     the iteration time is @xmath112 for the new hopfield network and @xmath113 for boltzmann network . in each iteration , all neurons are updated once . on average ,",
    "the new hopfield network is three times faster than boltzmann machine in our simulation .",
    "another big advantage of the new hopfield network over boltzmann machine is its inherited parralism . in each iteration , all neurons in the new hopfield network can be updated fully in parallel .",
    "this feature , together with the excellent performance of the new hopfield network offer us commerial pontential in implementing stereo vision capability for robots and unmanned vehicles .",
    "this paper presented a neural network implementation of a new powerful cooperative algorithm for solving combinatorial optimization problems .",
    "it fixes many problems of the hopfield network in theoretical , performance , and implementation perspectives .",
    "its operations are based on parallel , local iterative interactions .",
    "the proposed algorithm has many important computational properties absent in existing optimization methods .",
    "given an optimization problem instance , the computation always has a unique equilibrium and converges to it with an exponential rate regardless of initial conditions and perturbations .",
    "there are sufficient conditions  @xcite for identifying global optimum and necessary conditions for trimming search spaces . in solving large scale optimization problems in computer vision",
    ", it significantly outperformed classical optimization methods , such as simulated annealing and local search with multi - restarts .",
    "one of the key processes of cooperative computation is value discarding .",
    "this is the same in principle as the inhibition process used by marr and poggio in @xcite , and lawrence and kanade in @xcite .",
    "the inhibition process makes the cooperative computation fundamentally different from the most known optimization methods . as steven pinker pointed out in his book `` how the mind works ''",
    ", the cooperative optimization captures the flavor of the brain s computation of stereo vision .",
    "it has many important computational properties not possessed by conventional ones .",
    "they could help us in understanding cooperative computation possibly used by human brains in solving early vision problems .",
    "x.  huang , `` cooperative optimization for solving large scale combinatorial problems , '' in _ theory and algorithms for cooperative systems _ , ser .",
    "series on computers and operations research.1em plus 0.5em minus 0.4emworld scientific , 2004 , pp",
    ". 117156 .",
    "r.  szeliski and r.  zabih , `` an experimental comparison of stereo algorithms , '' in _ vision algorithms : theory and practice _ , ser .",
    "lncs , b.  triggs , a.  zisserman , and r.  szeliski , eds .",
    "1883.1em plus 0.5em minus 0.4emcorfu , greece : springer - verlag , sept .",
    "1999 , pp . 119 ."
  ],
  "abstract_text": [
    "<S> the hopfield network has been applied to solve optimization problems over decades . </S>",
    "<S> however , it still has many limitations in accomplishing this task . </S>",
    "<S> most of them are inherited from the optimization algorithms it implements . </S>",
    "<S> the computation of a hopfield network , defined by a set of difference equations , can easily be trapped into one local optimum or another , sensitive to initial conditions , perturbations , and neuron update orders . </S>",
    "<S> it does nt know how long it will take to converge , as well as if the final solution is a global optimum , or not . in this paper </S>",
    "<S> , we present a hopfield network with a new set of difference equations to fix those problems . </S>",
    "<S> the difference equations directly implement a new powerful optimization algorithm </S>",
    "<S> .    [ section ] [ section ] [ section ] [ section ] </S>"
  ]
}