{
  "article_text": [
    "given a data matrix @xmath3 , principal component analysis ( pca ) can be regarded as a ` denoising ' technique that replaces @xmath3 by its closest rank - one approximation .",
    "this optimization problem can be solved efficiently , and its statistical properties are well - understood .",
    "the generalization of pca to tensors is motivated by problems in which it is important to exploit higher order moments , or data elements are naturally given more than two indices .",
    "examples include topic modeling @xcite , video processing , collaborative filtering in presence of temporal / context information , community detection @xcite , spectral hypergraph theory and hyper - graph matching @xcite .",
    "further , finding a rank - one approximation to a tensor is a bottleneck for tensor - valued optimization algorithms using conditional gradient type of schemes . while tensor factorization is np - hard @xcite",
    ", this does not necessarily imply intractability for natural statistical models . over the last ten years , it was repeatedly observed that either convex optimization or greedy methods yield optimal solutions to statistical problems that are intractable from a worst case perspective ( well - known examples include sparse regression @xcite and low - rank matrix completion @xcite ) .    in order to investigate the fundamental tradeoffs between computational resources and statistical power in tensor pca",
    ", we consider the simplest possible model where this arises , whereby an unknown unit vector @xmath4 is to be inferred from noisy multilinear measurements .",
    "namely , for each unordered @xmath0-uple @xmath5 $ ] , we measure @xmath6 with @xmath7 gaussian noise ( see below for a precise definition ) and wish to reconstruct @xmath4 . in tensor notation ,",
    "the observation model reads ( see the end of this section for notations ) @xmath8 this is analogous to the so called ` spiked covariance model ' used to study matrix pca in high dimensions @xcite .",
    "it is immediate to see that maximum - likelihood estimator @xmath9 is given by a solution of the following problem @xmath10 solving it exactly is in general np hard @xcite .",
    "we next summarize our results .",
    "note that , given a completely observed rank - one symmetric tensor @xmath11 ( i.e. for @xmath12 ) , it is easy to recover the vector @xmath13 .",
    "it is therefore natural to ask the question _ for which signal - to - noise ratios one can one still reliably estimate @xmath4 ? _ the answer appears to depend dramatically on the computational resources if there exists a constant @xmath14 independent of @xmath15 ( but possibly dependent on @xmath0 ) , such that @xmath16 .",
    "ideal estimation .",
    ": :    assuming unbounded computational resources , we can solve the [ eq : tpca ]    optimization problem and hence implement the maximum likelihood    estimator @xmath17 .",
    "we use recent results in    probability theory to show that this approach is successful for    @xmath18 ( here @xmath19 is a constant    given explicitly below , with    @xmath20 ) . in particular , above    this threshold even ,    @xmath4 can only be recovered modulo sign . for the sake of    simplicity , we assume here that this ambiguity is correctly resolved . ]",
    "we have , with high probability , @xmath21 we use an information - theoretic argument to show that no approach    can do significantly better , namely no procedure can estimate    @xmath4 accurately for @xmath22    ( for @xmath14 a universal constant ) .",
    "tractable estimators : unfolding . : :    we consider two approaches to estimate @xmath4 that can be    implemented in polynomial time .",
    "the first approach is based on tensor    unfolding : starting from the tensor    @xmath23 , we produce a matrix    @xmath24 of dimensions @xmath25 .",
    "we then perform matrix pca on @xmath24 .",
    "we show that this    method is successful for @xmath26 ( provided we choose    @xmath27 ) .",
    "+    a heuristics argument suggests that the necessary and sufficient    condition for tensor unfolding to succeed is indeed    @xmath28 ( which is below the rigorous    bound by a factor @xmath29 for @xmath0 odd ) .",
    "we    can indeed confirm this conjecture for @xmath0 even and under    an asymmetric noise model .",
    "numerical simulations confirm the    conjecture for @xmath30 .",
    "tractable estimators : power iteration . : :    we then consider a simple tensor power iteration method , that proceeds    by repeatedly applying the tensor to a vector . we prove that ,    initializing this iteration uniformly at random , it converges very    rapidly to an accurate estimate provided @xmath31 .",
    "a heuristic argument suggests that the correct    necessary and sufficient threshold is given by    @xmath32 . in other words ,",
    "power iteration is substantially less    powerful than unfolding .",
    "tractable estimators : warm - start power iteration . : :    motivated by the last observation , we consider a ` warm - start ' power    iteration algorithm , in which we initialize power iteration with the    output of tensor unfolding . this approach appears to have the same    threshold signal - to - noise ratio as simple unfolding , but significantly    better accuracy above that threshold .",
    "+    we also study a number of variations on this , with improved unfolding    methods .",
    "tractable estimators : approximate message passing . : :    finally we consider an approximate message passing ( amp ) algorithm    @xcite .",
    "such algorithms proved effective in compressed sensing    and several other estimation problems .",
    "we show that the behavior of    amp is qualitatively similar to the one of naive power iteration . in    particular",
    ", amp fails for any @xmath1 bounded as    @xmath33 .",
    "side information .",
    ": :    given the above computational complexity barrier , it is natural to    study weaker version of the original problem .",
    "here we assume that    extra information about @xmath4 is available .",
    "this can be    provided by additional measurements or by approximately solving a    related problem , for instance a matrix pca problem as in @xcite .",
    "we    model this additional information as @xmath34 ( with @xmath35 an independent gaussian    noise vector ) , and incorporate it in the initial condition of amp    algorithm .",
    "we characterize exactly the threshold value    @xmath36 above which amp converges to an    accurate estimator .",
    "the thresholds for various classes of algorithms are summarized below .    [ cols=\"^,^,^\",options=\"header \" , ]     for instance , this table indicates that a large order-@xmath37 gaussian tensor should have @xmath38 , while a large order @xmath39 tensor has @xmath40 . as a simple consequence of lemma [ lem : benarous ] , we establish an upper bound on the error incurred by the maximum likelihood estimator , see section [ sec : proofml ] for a proof .",
    "[ th : nonconvexopt ] let @xmath19 be the sequence of real numbers introduced above .",
    "letting @xmath17 denote the maximum likelihood estimator ( i.e. the solution of [ eq : tpca ] ) , we have for @xmath15 large enough , and all @xmath41 @xmath42 with probability at least @xmath43 .",
    "the following upper bound on the value of the problem [ eq : tpca ] is proved using sudakov - fernique inequality . while it is looser than lemma [ lem : benarous ] ( corresponding to the case @xmath44 ) , we expect it to become sharp for @xmath45 a suitably large constant .",
    "we refer to appendix [ sec : proofupperboundx ] for its proof .",
    "[ lem : upperboundx ] under [ eq : symmetricmodel ] model , we have @xmath46 further , for any @xmath47 , @xmath48      at this point it is useful to pause , in order to provide some further background .",
    "the random cost function @xmath49 ( defined on the unit sphere @xmath50 ) was studied in the context of statistical physics under the name of ` spherical p - spin model . ' in particular , crisanti and smmers @xcite used the non - rigorous replica method from spin glass theory to compute the asymptotic value @xmath19 .",
    "their results were confirmed rigorously by talagrand @xcite .",
    "the most striking prediction from statistical physics is that the function @xmath51 has an exponential number of local maxima on the unit sphere @xcite .",
    "furthermore , there exists @xmath52 such that , for each @xmath53 the number of local maxima with value @xmath54 is @xmath55 . in @xcite rigorous evidence is developed to this support picture .    in the [ eq : symmetricmodel ] these local maxima translate into _ undesired _ local maxima of @xmath56 .",
    "it is natural to guess that these local maxima affect local iterative algorithms , and that these do not converge to a good estimate of @xmath4 unless they are initialized within thee ` basin of attraction ' of @xmath4 .",
    "the analysis in the next sections confirms this intuition .",
    "a simple and popular heuristics to obtain tractable estimators of @xmath4 consists in constructing a suitable matrix with the entries of @xmath3 , and performing principal component analysis on this matrix . since the number of distinct entries of @xmath3 is of order @xmath57 , the resulting matrix @xmath58 has dimension @xmath59 .",
    "this operation is variously referred as matricization , unfolding , flattening . while the details of this construction can vary",
    ", we do not expect them to affect qualitatively our results , that we summarize for the sake of convenience :    1 .",
    "the best way to unfold @xmath3 amounts to form a matrix as square as possible .",
    "2 .   setting @xmath60 ( in particular @xmath61 for @xmath62 ) , the unfolding approach succeeds when @xmath1 is larger than @xmath63 .",
    "this is to be compared with @xmath64 that is sufficient for the maximum likelihood estimator ( see previous section ) .",
    "+ based on heuristic arguments , we believe that the tight threshold is @xmath28 ( i.e. that @xmath28 is both necessary and sufficient modulo constants ) .",
    "a sharper analysis is possible when the symmetric noise tensor @xmath7 in our [ eq : symmetricmodel ] is replaced by non - symmetric gaussian noise , and @xmath0 is even . in particular",
    ", we can confirm the above conjecture in this case .",
    "( as mentioned , we expect similar results to hold more generally . )",
    "+ in this case , if @xmath65 , then the estimator from unfolding is essentially orthogonal to the signal @xmath4 . on the other hand ,",
    "if @xmath66 , we construct an estimator with @xmath67 .",
    "4 .   we achieves the remarkable behavior at the last point by a _",
    "recursive unfolding _ method . in a nutshell",
    "we perform principal component analysis on @xmath58 , construct a matrix out of the principal vector , and then perform again principal component analysis .      for an integer @xmath68",
    ", we introduce the unfolding ( also referred to as matricization or reshape ) operator @xmath69 as follows . for any indices",
    "@xmath70 $ ] , we let @xmath71 and @xmath72 , and define @xmath73_{a , b}= \\bx_{i_1,i_2,\\cdots , i_k}~~.~~ \\ ] ] standard convex relaxations of low - rank tensor estimation problem compute factorizations of @xmath58@xcite .",
    "not all unfoldings ( choices of @xmath74 ) are equivalent .",
    "it is natural to expect that this approach will be successful only if the signal - to - noise ratio exceeds the operator norm of the unfolded noise @xmath75 .",
    "the next lemma suggests that the latter is minimal when @xmath76 is ` as square as possible ' .",
    "a similar phenomenon was observed in a different context in @xcite .",
    "[ lem : upperboundmatricized ] for any integer @xmath77 we have , for some universal constant @xmath78 , @xmath79 for all @xmath15 large enough , both bounds are minimized for @xmath80 .",
    "further @xmath81    the concentration bound ( [ eq : concentrationmatricized ] ) follows because , for @xmath82 of norm @xmath83 , the function @xmath84 is a lipschitz function of the gaussian vector @xmath85 with modulus at most @xmath86 .",
    "hence the same holds for @xmath87 , and the claim follows from gaussian concentration of measure .    for the upper bound in eq .",
    "( [ eq : normmatricized ] ) , note that @xmath88 has i.i.d .",
    "standard normal entries .",
    "the proof follows from the definition ( [ eq : symnoisedefinition ] ) together with triangular inequality and standard bounds on the norm of the random gaussian matrices @xmath89 @xcite : @xmath90    for the upper bound in eq .",
    "( [ eq : normmatricized ] ) , note that @xmath91 where the last inequality is proved by considering @xmath92 the identity permutation ( all the terms in the sum are positive ) .",
    "the desired lower bound follows since the concentration inequality ( [ eq : concentrationmatricized ] ) implies @xmath93 , and we therefore have @xmath94    the last lemma suggests the choice @xmath95 , which we shall adopt in the following , unless stated otherwise .",
    "we will drop the subscript from @xmath96 .",
    "let us recall the following standard result derived directly from wedin perturbation theorem @xcite , and stated in the context of the spiked model .",
    "[ th : wedin ] let @xmath97 be a matrix with @xmath98 .",
    "let @xmath99 denote the right singular vector of @xmath100 . if @xmath101 , then @xmath102    note @xmath103 is the only singular value of @xmath104 , while the second singular value of @xmath105 is at most @xmath106 .",
    "wedin theorem states that , for all @xmath107 , we have @xmath108 in particular @xmath109 for @xmath110 .",
    "hence the claim ( [ eq : wedinub ] ) follows from @xmath111    [ th : unfold ] letting @xmath112 denote the top right singular vector of @xmath24 , we have the following , for some universal constant @xmath113 , and @xmath114 .    if @xmath115 then , with probability at least @xmath116 , we have @xmath117    by definition we have @xmath118 where @xmath119 , @xmath120 .",
    "we know by lemma [ lem : upperboundmatricized ] that @xmath121 with the claimed probability . the loss upper bound ( [ eq : lossub ] ) follows immediately from this upper bound and wedin s theorem eq .",
    "( [ eq : wedinub ] ) .",
    "a technical complication in analyzing the random matrix @xmath58 lies in the fact that its entries are not independent , because the noise tensor @xmath7 is assumed to be symmetric . in the next theorem",
    "we consider the case of non - symmetric noise and even @xmath0 .",
    "this allows us to leverage upon known results in random matrix theory @xcite to obtain : @xmath122",
    "asymptotically sharp estimates on the critical signal - to - noise ratio ; @xmath123 a lower bound on the loss below the critical signal - to - noise ratio .",
    "namely , we consider observations @xmath124 where @xmath125 is a standard gaussian tensor ( i.e. a tensor with i.i.d .",
    "standard normal entries ) .",
    "let @xmath126 denote the top right singular vector of @xmath24 .",
    "for @xmath127 even , and define @xmath128 , as above . by (",
    "* theorem 4 ) , or ( * ? ? ?",
    "* theorem 2.3 ) , we have the following almost sure limits @xmath129 in other words @xmath130 is a good estimate of @xmath131 if and only if @xmath1 is larger than @xmath63 .",
    "we can use @xmath132 to estimate @xmath4 as follows . construct the matricization @xmath133 ( slight abuse of notation ) of @xmath134 by letting , for @xmath135 $ ] , and @xmath136 $ ] , @xmath137",
    "we then let @xmath138 to be the left principal vector of @xmath139 .",
    "we refer to this algorithm is a power of two one could construct a square matricization and repeat the same process . for analysis purposes",
    ", we prefer the version described here . ] as to _ recursive unfolding_.    [ th : unfoldnonsymm ] let @xmath140 be distributed according to the non - symmetric model ( [ eq : nonsymm ] ) with @xmath127 even , define @xmath128 . and let @xmath138 be the estimate obtained by two - steps recursive unfolding .    if @xmath141 then , almost surely @xmath142    for the sake of simplicity , we assume @xmath143 .",
    "the limit along other sequences follows from a standard subsequence argument .",
    "it follows from the invariance of the noise distribution in eq .",
    "( [ eq : nonsymm ] ) that @xmath144 where @xmath145 .",
    "it follows from eq .",
    "( [ eq : tightphtr2 ] ) , together with the almost sure limits @xmath146 and @xmath147 that ( almost surely ) @xmath148 using the definition ( [ eq : m1w ] ) , we then have ( recall that @xmath149 ) @xmath150 where @xmath151 and @xmath152 is a matrix with i.i.d standard normal entries . using @xcite",
    ", we have , with probability @xmath153 , @xmath154 since @xmath155 is bounded away from zero as @xmath33 , wedin s theorem implies @xmath156 , and therefore the claim ( [ eq : lossubnons ] ) .",
    "we conjecture that the weaker condition @xmath157 is indeed sufficient also for our original symmetric noise model model , both for @xmath0 even and for @xmath0 odd .",
    "iterating over ( multi- ) linear maps induced by a ( tensor ) matrix is a standard method for finding leading eigenpairs , see @xcite and references therein for tensor - related results . in this section",
    "we will consider a simple power iteration , and then its possible uses in conjunction with tensor unfolding .",
    "finally , we will compare our analysis with results available in the literature .",
    "approximate message passing ( amp ) provides a different iterative strategy and will be discussed in section [ sec : amp ] . while the qualitative behavior is the same as for naive power iteration , a sharper asymptotic analysis is possible for amp .",
    "the simplest iterative approach is defined by the following recursion @xmath158 the following result establishes convergence criteria for this iteration , first for generic noise @xmath7 and then for standard normal noise ( using lemma [ lem : benarous ] ) .",
    "[ lem : poweriterationpositiveresult ] assume @xmath159^{1/(k-1)}\\ , , \\label{eq : initpoweriteration } \\ ] ] then for all @xmath160 , the power iteration estimator satisfies @xmath161 if @xmath7 is a standard normal noise tensor , then conditions ( [ eq : snr_poweriteration ] ) , ( [ eq : snr_poweriteration ] ) are satisfied with high probability provided @xmath162^{1/(k-1 ) } = \\beta^{-1/(k-1 ) } \\ , \\big(1+o_k(1)\\big)\\ , .",
    "\\label{eq : initpoweriteration_bis } \\ ] ]    we next discuss two aspects of this result : @xmath122 the requirement of a positive correlation between initialization and ground truth ; @xmath123 possible scenarios under which the assumptions of theorem [ lem : poweriterationpositiveresult ] are satisfied .",
    "notice that we require a positive correlation of the initialization @xmath163 with the ground truth @xmath4 .",
    "the underlying reason is that , if @xmath164 is small , then @xmath165 remains small at all subsequent iterations . in order to clarify this point , it is instructive to compute the distribution of @xmath166 for standard gaussian noise @xmath7 .",
    "we let @xmath167 using eq .",
    "( [ eq : xmultiplication ] ) and the fact that @xmath168 is independent of @xmath7 , we get @xmath169 where @xmath170 , and @xmath171 is a vector with @xmath172 in probability as @xmath33 .",
    "in particular @xmath173 in particular @xmath174 only if @xmath175 , or , equivalently , @xmath176 .",
    "this suggest that the condition in eq .",
    "( [ eq : initpoweriteration_bis ] ) is not too far from being tight ( in the sense that the exponent @xmath177 can at best replaced by @xmath178 ) .",
    "in general we can not assume that an initialization satisfying the conditions of theorem [ lem : poweriterationpositiveresult ] .",
    "hence , unlike for ordinary matrix factorization , _ power iteration is not a practical solution to the tensor principal component problem_. there are however circumstances under which a sufficiently good initialization exists .    extremely low noise .",
    ": :    if @xmath179 is a uniformly random vector on the unit sphere ,    then @xmath180 is approximately normal with mean zero    and variance @xmath181 .",
    "for instance    @xmath182 with probability roughly    @xmath183 .    +    comparing this with condition ( [ eq : initpoweriteration ] ) , we obtain    that a random initialization succeed with positive probability if    @xmath184 for standard gaussian noise , this amounts to requiring    @xmath185 .",
    "the above heuristic analysis suggests that the    correct condition should be @xmath186 .",
    "additional side information .",
    ": :    additional information might be available about the vector    @xmath4 .",
    "this information can be used for initiating the    power iteration . in the next section",
    "we consider the special case in    which tensor unfolding is used for initializing power iteration .",
    "it is instructive to compare the result of the previous section with the ones for tensor unfolding , cf .",
    "section [ sec : unfolding ] . summarizing , for standard gaussian noise    * tensor unfolding is guaranteed to succeed provided @xmath187 , with @xmath188 .",
    "we conjecture that a necessary and sufficient condition is in fact @xmath28 ( e.g. @xmath189 for order @xmath37 tensors ) .",
    "* power iteration , with random initialization requires @xmath190 .",
    "our heuristic calculation suggests that a necessary and sufficient condition is in fact @xmath32 ( e.g. @xmath191 for order @xmath37 tensors ) ..    in other words , tensor unfolding is successful under a signal - to - noise ratio that is order of magnitudes smaller than power iteration .",
    "this suggests the following _ warm start _ procedure : @xmath122 compute a first estimate @xmath192 of @xmath4 using tensor unfolding ; @xmath123 use this as initialization for the power iteration , hence setting @xmath193 .",
    "we will explore this approach numerically in section [ sec : num ] .      as mentioned above",
    ", power iteration is a natural approach to tensor factorization and was studied in several earlier papers .",
    "most recently , interest within machine learning was spurred by @xcite .",
    "our theorem [ lem : poweriterationpositiveresult ] is analogous to the main result of @xcite although incomparable :    * in @xcite the ` signal ' part of the tensor @xmath3 is assumed to have an orthogonal decomposition @xmath194 with @xmath195 bounded away from zero . here , the signal part has rank one ( equivalently , all the @xmath196 s but one vanish ) . * in @xcite",
    "only the case of third order tensors ( @xmath30 ) is considered .",
    "we characterize power iteration for general @xmath0 . *",
    "we establish convergence in a number of iterations @xmath197 that is _ independent of the dimensions _ @xmath15 .",
    "in @xcite the number of iterations is bounded by a polynomial in @xmath15 .",
    "* we evaluate our bounds in the case of gaussian noise .",
    "this allows a comparison with other methods , such as tensor unfolding .",
    "approximate message passing ( amp ) algorithms @xcite proved successful in several high - dimensional estimation problems including compressed sensing , low rank matrix reconstruction , and phase retrieval @xcite .",
    "an appealing feature of this class of algorithms is that their high - dimensional limit can be characterized exactly through a technique known as ` state evolution . ' here we develop an amp algorithm for tensor data , and its state evolution analysis focusing on the fixed @xmath1 , @xmath33 limit .",
    "proofs follows the approach of @xcite and will be presented in a journal publication .    in a nutshell",
    ", our amp for tensor pca can be viewed as a sophisticated version of the power iteration method of the last section . with the notation @xmath198",
    ", we define the amp iteration over vectors @xmath199 by @xmath200 , @xmath201 , and @xmath202 ( note that , unlike in power iteration , we normalize @xmath203 ` before ' multiplying it by @xmath3 .",
    "this choice is equivalent but yields slightly simpler expression . )",
    "our main conclusion is that _ the behavior of amp is qualitatively similar to the one of power iteration .",
    "_ however , we can establish stronger results in two respects :    1 .",
    "we can prove that , unless side information is provided about the signal @xmath4 , the amp estimates remains essentially orthogonal to @xmath4 , for any fixed number of iterations .",
    "this corresponds to a converse to theorem [ lem : poweriterationpositiveresult ] .",
    "since state evolution is asymptotically exact , we can prove sharp phase transition results with explicit characterization of their locations .",
    "we assume that the additional information takes the form of a noisy observation @xmath204 , where @xmath205 .",
    "our next results summarizes the state evolution analysis .",
    "its proof is deferred to a journal publication .",
    "[ propo : amp_state ] let @xmath206 be a fixed integer .",
    "let @xmath207 be a sequence of unit norm vectors @xmath208 .",
    "let also @xmath209 denote a sequence of tensors @xmath210 generated following [ eq : symmetricmodel ] .",
    "finally , let @xmath203 denote the @xmath197-th iterate produced by [ eq : amp ] , and consider its orthogonal decomposition @xmath211 where @xmath212 is proportional to @xmath4 , and @xmath213 is perpendicular .",
    "then @xmath213 is uniformly random , conditional on its norm .",
    "further , almost surely @xmath214 where @xmath215 is given recursively by letting @xmath216 and , for @xmath217 ( we refer to this as to _ state evolution _ ) : @xmath218    note that state evolution coincides with the equation that we derived for the first iteration of power iteration , cf .",
    "( [ eq : firstiterationpoweriteration ] ) ( apart from the different scaling ) .",
    "it is important to notice that for subsequent iterations @xmath219 , state evolution ( [ eq : se ] ) does not correctly describe naive power iteration .",
    "the reason is that @xmath203 depends on @xmath3 , and hence the same argument does not apply .",
    "the amp iteration differ from naive power iteration because of the ` memory term ' , @xmath220 .",
    "as shown in @xcite , this memory term approximately cancels dependencies . as a consequence , the resulting algorithm obeys state evolution .",
    "the following result characterizes the minimum required additional information @xmath221 to allow [ eq : amp ] to escape from those undesired local optima .",
    "we will say that @xmath222 converges almost surely to a _ desired local optimum _ if , almost surely , @xmath223    [ thm : ampisgood ] consider the [ eq : tpca ] problem with @xmath224 and @xmath225    then [ eq : amp ] converges almost surely to a desired local optimum if and only if @xmath226 where @xmath227 is the largest solution of @xmath228 ,    in the special case @xmath229 , and @xmath230 , assuming @xmath231 , [ eq : amp ] tends to a desired local optimum .",
    "numerically @xmath232 is enough for [ eq : amp ] to achieve @xmath233 if @xmath234 .    as a final remark",
    ", we note that the methods of @xcite can be used to show that , under the assumptions of theorem [ thm : ampisgood ] , for @xmath235 a sufficiently large constant , amp asymptotically solves the optimization problem [ eq : tpca ] .",
    "formally , we have , almost surely , @xmath236",
    "let us emphasize two practical suggestions that arise from our work :    * tensor unfolding is superior to tensor power iteration under our spiked model .",
    "for instance , for @xmath30 , we expect tensor power iteration to require @xmath189 and unfolding to require @xmath191 . * for smaller values of @xmath1 , iterative methods (",
    "tensor power iteration or approximate message passing ) only produce a good estimate if the initialization has a scalar product with the ground truth @xmath4 that is bounded away from zero . * as a consequence of the above , side information about the unknown vector @xmath4 can greatly improve performances .",
    "+ a special case , we will study the behavior of warm start algorithms that first perform a singular value decomposition of @xmath24 , and then apply an iterative method ( tensor power iteration or approximate message passing ) .",
    "in this section we will illustrate these suggestions through numerical simulations .",
    "section [ sec : coneconstrained ] describes a refinement of tensor unfolding that provides a tighter relaxation .",
    "section [ sec : numericalcomparison ] compares different algorithms .",
    "finally , section [ sec : sideinfo ] provides additional illustration of how side information can dramatically simplify the estimation problem .",
    "note that , for @xmath237 , the outer product @xmath238 ( regarded as an @xmath239 matrix ) is positive semi - definite ( psd ) .",
    "considering the case @xmath30 , we have @xmath240 this remark suggest to perform a cone - constrained principal component analysis of @xmath24 , where the left singular vector ( viewed as a matrix ) belongs to the pds cone . in order to write this formally , it is convenient to introduce the operator @xmath241 that matricizes vectors as @xmath242 .",
    "the psd - cone - constrained principal component of @xmath24 , is defined by @xmath243 this optimization problem is np hard , since it includes copositive programming as a special case .",
    "however @xcite provides rigorous and empirical evidence that problems of this type can be solved efficiently by a projected power iteration , under statistical model dor @xmath3 .",
    "denoting @xmath244 the orthogonal projector onto the psd cone , we iterate the following for @xmath217 , using random initialization of @xmath245 , @xmath246      in fig .  [ fig : comparisonamppowerunfold ] we compare different algorithms on data generated following [ eq : symmetricmodel ] with @xmath30 , and @xmath247 and for a range of values of @xmath248 $ ] .",
    "the plots represent measured values of the absolute correlation @xmath249 versus @xmath1 , averaged over @xmath250 samples ( except for @xmath251 , where we used @xmath252 samples ) .    the main findings are consistent with the theory developed above :    * tensor power iteration ( with random initialization ) performs poorly with respect to other approaches that use some form of tensor unfolding .",
    "the gap widens as the dimension @xmath15 increases . *",
    "psd - constrained principal component analysis ( described in the last section ) is slightly superior to plain unfolding . *",
    "all algorithms based on initial unfolding have essentially the same threshold . above that threshold ,",
    "those that process the singular component ( either by recursive unfolding or by tensor power iteration ) have superior performances over simpler one - step algorithms .",
    "in addition , we noted that the two iterative algorithms ( [ eq : tensorpiter ] and [ eq : amp ] ) show very close behaviors in our experiments .    in figure",
    "[ fig : scalingcomparison ] we compare the scaling with @xmath15 of the threshold signal - to - noise ratio for different type of algorithms .",
    "our heuristic arguments suggest that tensor power iteration with random initialization will work for @xmath191 , while unfolding only requires @xmath189 ( our theorems guarantee this for , respectively , @xmath253 and @xmath191 ) .",
    "we plot the average correlation @xmath249 versus ( respectively ) @xmath254 and @xmath255 .",
    "the curve superposition confirms that our prediction captures the correct behavior already for @xmath15 of the order of @xmath250 .",
    "our next experiment concerns a simultaneous matrix and tensor pca task : we are given a tensor @xmath256 of [ eq : symmetricmodel ] with @xmath229 and the signal to noise ratio @xmath257 is fixed .",
    "in addition , we observe @xmath258 where @xmath259 is a symmetric noise matrix with upper diagonal elements @xmath260 iid @xmath261 and the value of @xmath262 $ ] varies .",
    "this experiment mimics a rank-1 version of topic modeling method presented in @xcite where @xmath100 is a matrix representing pairwise co - occurences and @xmath3 triples .",
    "the analysis in previous sections suggest to use the leading eigenvector of @xmath100 as the initial point of amp algorithm for tensor pca on @xmath3 .",
    "we performed the experiments on @xmath263 randomly generated instances with @xmath264 and report in figure [ fig : simpca ] the mean values of @xmath265 with confidence intervals .",
    "random matrix theory predicts @xmath266 @xcite .",
    "thus we can set @xmath267 and apply the theory of the previous section .",
    "in particular , proposition [ propo : amp_state ] implies @xmath268 and @xmath269 otherwise simultaneous pca appears vastly superior to simple pca .",
    "our theory captures this difference quantitatively already for @xmath270 .",
    "this work was partially supported by the nsf grant ccf-1319979 and the grants afosr / darpa fa9550 - 12 - 1 - 0411 and fa9550 - 13 - 1 - 0036 .",
    "introduce the operator @xmath271 where for indices @xmath272 , we have @xmath273 with @xmath274 .",
    "let @xmath275 denote the kullback - leiber divergence where @xmath276 is the law of @xmath277 conditional on @xmath278 .",
    "[ lem : kullback ] for any pairs of vectors @xmath279 we have @xmath280    first note that for any @xmath281 , @xmath276 is a gaussian probability distribution @xmath282 on the other hand for any symmetric tensor @xmath283 we have @xmath284 .",
    "therefore we have @xmath285    we are now in position to prove theorem [ th : infotheoretic ] .",
    "let @xmath286 denote the class of estimators @xmath138 with unit norm : @xmath287    recall that the packing number @xmath288 of @xmath289 is the maximum cardinality of any set @xmath290 such that @xmath291 for any @xmath292 . by a standard argument , letting @xmath293 the corresponding covering number such that @xmath294 for any @xmath295 .",
    "] , we have , for @xmath296 a point on the unit sphere , @xmath297 ( here @xmath298 denotes the @xmath299-dimensional volume , and @xmath300 the ball of radius @xmath301 centered at @xmath302 . )    let @xmath303 denote an @xmath301-packing with cardinality @xmath304 .",
    "let @xmath4 be uniformly distributed in the set @xmath303 .",
    "for an estimator @xmath305 , we define @xmath306 .",
    "consider the _ error _",
    "event @xmath307 . by definition of @xmath308 ,",
    "the event @xmath309 implies @xmath310 . by markov inequality",
    "we have : @xmath311 by fano s inequality @xcite we have that : @xmath312 where @xmath313 , and in the second inequality we used @xcite @xmath314    using eq .",
    "( [ eq : boundpacking ] ) and lemma [ lem : kullback ] , in eq .",
    "( [ eq : fanoineq ] ) , we get @xmath315 choosing @xmath316 , we get , @xmath317 for @xmath318 and @xmath319 . in particular @xmath320 provided @xmath321 . by eq .",
    "( [ eq : perrbound ] ) this implies @xmath322",
    "let @xmath323 be a symmetric standard normal tensor , and consider the associated objective function @xmath324 while the function @xmath325 is obviously non - convex , it turns out that for random data @xmath7 it is dramatically so .",
    "namely , it has an exponential number of local maximum , whose value is typically only a fraction of the value of the global maximum .     defined in eq .",
    "( [ eq : complexityformula ] ) .",
    "as proved in @xcite , the expected number of local maxima of the objective function @xmath51 is to leading exponential order @xmath326.,width=302 ]    in order to quantify this phenomenon , for @xmath327 , let @xmath328 denote the number of local maxima of @xmath325 over @xmath289 , that have value larger or equal than @xmath329 .",
    "the next lemma from @xcite characterizes the growth rate of the number of local minima .    for any @xmath330",
    ", we have @xmath331 where , for @xmath332 @xmath333 further , for @xmath334 , @xmath335 .",
    "the function @xmath336 is monotone decreasing for @xmath337 , and non - negative if and only if @xmath338 $ ] for some @xmath339 ( strictly positive for @xmath53 ) . in figure",
    "[ fig : complexityfunction ] , we plot @xmath336 for @xmath340 .",
    "informally , this means that the function @xmath51 has exponentially many local maxima with value @xmath341 for any @xmath342 . to leading exponential order ,",
    "the number of such maxima is given by @xmath343 .",
    "the value @xmath19 can be determined as the unique solution to the equation @xmath344 .",
    "it is immediately to do this numerically , obtaining the values in section [ sec : infotheoryandupperbounds ] .",
    "the last result implies that the global maximum of @xmath51 is ( asymptotically ) at least @xmath19 .",
    "indeed the global maximum is necessarily a local maximum as well .",
    "the next result implies that indeed the global maximum converges to @xmath19 .",
    "[ cor : mukprobabilist ] let @xmath19 denote the unique non - negative root of the equation @xmath345 , for @xmath332 .",
    "then @xmath346    in order to derive the large-@xmath0 asymptotics of @xmath19 , we rewrite eq .  (",
    "[ eq : complexityformula ] ) in terms of the variable @xmath347 .",
    "we get @xmath348 , where @xmath349 further @xmath350 $ ] .",
    "the claimed asymptotics follows by showing that the only solution of @xmath351 in this interval obeys @xmath352 .",
    "this in turns can be showed by using the bounds @xmath353 and showing that the solution of @xmath354 for large @xmath355 is @xmath356 .",
    "finally , the norm @xmath357 concentrates tightly around its expectation .    for any @xmath47",
    ", we have @xmath358    note that @xmath359 is a lipschitz function with lipschitz modulus @xmath86 ( with respect to euclidean norm ) of the gaussian vector ( tensor ) @xmath85 .",
    "hence @xmath357 is lipchitz continuous with the same modulus .",
    "the claim follows from gaussian isoperimetry @xcite .",
    "note that to make the connection with the notations used in @xcite , one has to use the proper scaling @xmath360 ( @xmath361is the objective function considered in @xcite ) .",
    "the upper bound on the tensor operator norm obtained from sudakov - fernique inequality is not tight .",
    "in fact taking @xmath362 in lemma [ lem : upperboundx ] gives the loose upper bound @xmath363 .",
    "except in the case of random matrices ( @xmath364 ) , this is loose roughly by a factor @xmath365 .      by optimality of @xmath138",
    ", we have @xmath366 whence @xmath367 note that @xmath368 is lipchitz continuous in the gaussian random variables @xmath85 , with modulus bounder by @xmath369 .",
    "hence , by gaussian isoperimetry , with probability at least @xmath370 , we have ( since @xmath7 and @xmath4 are independent , @xmath371 ) @xmath372 using @xmath373 which holds for @xmath374 $ ] , and rescaling @xmath375 , we get @xmath376 with probability at least @xmath377 for all @xmath15 large enough .      [",
    "lem : fginnerproducts ] for each @xmath378 , let @xmath379 and @xmath380 be a vector with @xmath381 .",
    "then there exists a sequence @xmath382 independent from @xmath329 , such that @xmath383 and the following happens . with probability one ,",
    "there exists ( a random ) @xmath384 such that , for all @xmath385 , @xmath386 } \\big|~ \\|\\bg + \\tau \\bvz\\|_2 - \\sqrt{1+\\tau^2 } \\big | \\leq \\delta_n\\ , . \\ ] ]    since @xmath387 is uniformly continuous on bounded intervals @xmath388 $ ] , it is sufficient to prove @xmath386 } \\big|~ \\|\\bg + \\tau \\bvz\\|_2 ^ 2- ( 1+\\tau^2 ) \\big | \\leq \\delta_n\\ , . \\ ] ] for all @xmath385 , and an eventually different sequence @xmath382 . by triangular inequality and",
    "using @xmath389 , @xmath390 } \\big|~ \\|\\bg + \\tau \\bvz\\|_2 ^ 2- ( 1+\\tau^2 ) \\big | \\le \\big|\\|\\bg\\|^2 - 1\\big|+2\\tau_{\\rm    max}\\big|\\<\\bvz,\\bg\\>\\big| \\ ] ] next we have @xmath391 almost surely by the strong law of large numbers , and @xmath392 whence @xmath393 by borel - cantelli .    for @xmath394 $ ] , we define @xmath395 note that @xmath396}m_{\\bx}(\\kappa ) .",
    "\\label{eq : sigmaplus } \\ ] ] the function @xmath397 is a lipschitz continuous function with lipschitz constant @xmath86 of the standard gaussian tensor @xmath85 ( namely @xmath398 ) . hence , by gaussian isoperimetry , we have @xmath399 further we claim that @xmath400 is uniformly continuous for @xmath394 $ ] . in order to prove this , let @xmath401 where @xmath402 .",
    "we have , for @xmath403 $ ] , and by letting @xmath404 and @xmath405 denote the perpendicular components of @xmath406 and @xmath407 , we have for some constant @xmath408 @xmath409 where eq .",
    "( [ ineq : kchooseqformula ] ) was obtained by exploiting the symmetry of the tensor @xmath3 and eq .",
    "( [ ineq : sqrtkappa1kappa2term ] ) was derived using the norm of the vector @xmath410 . using eq .",
    "( [ eq : isoperimetry ] ) over a grid @xmath411 , and the fact and triangular inequality , or from a standard @xmath301-net argument . ] that @xmath412 for some constant @xmath413 with probability @xmath153 , we have for all @xmath414 and some constant @xmath408 @xmath415 } \\big|m_{\\bx}(\\kappa ) - \\om(\\kappa)\\big| \\ge t \\big\\ }    \\le 2\\ , n    e^{-n c t^2/2 } + 2\\ , e^{-cn}\\ , .   \\ ] ] in particular , by borel - cantelli we have , almost surely , @xmath416 } \\big|m_{\\bx}(\\kappa ) - \\om(\\kappa)\\big| = 0\\ , .",
    "\\label{eq : mom } \\ ] ]    in order to upper bound @xmath417 , we apply sudakov - fernique inequality for non - centered gaussian processes ( * ? ? ?",
    "* theorem 1 ) to the two processes @xmath418 , @xmath419 indexed by @xmath420 defined as follows : @xmath421 for random a vector @xmath422 .",
    "it is easy to see that @xmath423 and @xmath424 ^ 2\\big\\ } &   = \\left \\ { \\e\\cx_\\bv - \\e \\cx_\\bw\\right \\}^2 +   \\frac{2 ~k } { n } \\big(1-\\<\\bv,\\bw\\>^k\\big)\\ , , \\\\ \\e\\big\\{\\big[\\cy_\\bv-\\cy_\\bw\\big]^2\\big\\ } &   =    \\left \\ { \\e\\cy_\\bv - \\e\\cy_\\bw \\right \\ } ^2 + \\frac{2~k^2 } { n } \\big(1-\\<\\bv,\\bw\\>\\big)\\ , . \\ ] ] hence @xmath425 ^ 2\\big\\}\\le \\e\\big\\{\\big[\\cy_\\bv-\\cy_\\bw\\big]^2\\big\\}$ ] ( this follows from @xmath426 for @xmath427 $ ] ) .",
    "we conclude that @xmath428 where @xmath429 and @xmath382 satisfies @xmath430 uniformly over @xmath431 $ ] , by lemma [ lem : fginnerproducts ] .",
    "we finally conclude that @xmath432 concentration around the expectation follows by gaussian isoperimetry as in the proof of lemma [ lem : benarous ] .",
    "let @xmath433 and @xmath434 . let @xmath435 , @xmath436 $ ]",
    "be the two solutions of @xmath437 we will show below that our assumptions imply @xmath438 .",
    "further @xmath439 implies @xmath440 .    by definition of @xmath3",
    ", we have @xmath441 which implies , by triangular inequality , @xmath442 we will prove the first inequality @xmath443 by induction .",
    "it is true at @xmath444 by assumption .",
    "assume it is true at @xmath197",
    ". then @xmath445 using eq .",
    "( [ eq : mothereq2 ] ) .",
    "hence we can divide the two inequalities above obtaining @xmath446 which implies @xmath447 in particular @xmath448 where the latter sequence is defined by @xmath449 , @xmath450 , for @xmath451 .",
    "the function @xmath452 is concave and monotone increasing , and maps @xmath453 $ ] into itself , with @xmath454 , @xmath455 . by standard calculus argument @xmath456 exponentially",
    "fast , which implies @xmath457 to conclude the proof of eq .",
    "( [ eq : boundpi ] ) , we notice that , for @xmath458 @xmath459^{1/(k-1)}\\ , , \\ ] ] where we recall that @xmath435 , @xmath460 are the two solutions of @xmath461 in the interval @xmath462 $ ] . for the first inequality ,",
    "note that , in the interval @xmath463 $ ] , @xmath336 is decreasing with @xmath464 .",
    "this implies @xmath465 i.e. @xmath466 as long as @xmath467 , which is implied by @xmath458 .    for the second inequality , note that",
    ", in the interval @xmath468 $ ] , we have @xmath336 increasing with @xmath469 .",
    "this implies @xmath470 as long as @xmath471^{1/k-1}\\le 1-(k-1)^{-1}$ ] , which follows , again , by our assumptions .",
    "finally , conditions ( [ eq : snr_poweriteration_bis ] ) , ( [ eq : initpoweriteration_bis ] ) follow directly by applying lemma [ lem : benarous ] .",
    "let us recall the state evolution recursion ( [ eq : se ] ) @xmath472 notice that @xmath473 is strictly positive and monotone increasing on @xmath474 .",
    "the theorem follows by proving that the following claims hold for @xmath475    1 .   the fixed point equation @xmath476 has two strictly positive solutions @xmath477 .",
    "the smallest fixed point is given by @xmath478 as in the statement .",
    "the largest fixed point satisfies @xmath479 .      in order to prove the above statements , it is convenient to use the monotone parametrization @xmath481 that maps @xmath482 onto the interval @xmath483 . after some algebra ( and discarding the solution at @xmath484 ) ,",
    "fixed point equation then reads @xmath485 now , the function @xmath486 is continuously differentiable and strictly positive in the interval @xmath487 , with @xmath488 .",
    "further , simple calculus shows it has a unique stationary point ( a maximum ) at @xmath489 with @xmath490 .",
    "this implies that , for @xmath475 , eq .  ( [ eq : fixedpointsimpler ] ) has two fixed points @xmath491 thus implying points 1 and 2 above ( the latter immediately follows from inverting the re - parametrization ) .",
    "florent benaych - georges and raj  rao nadakuditi , _ the singular values and vectors of low rank perturbations of large rectangular random matrices _ , journal of multivariate analysis * 111 * ( 2012 ) , 120135 .",
    "d.  l. donoho and m.  elad , _ optimally sparse representation in general ( nonorthogonal ) dictionaries via @xmath497 minimization _ , proceedings of the national academy of sciences * 100 * ( 2003 ) , no .  5 , 21972202 .",
    "p.  schniter and s.  rangan , _ compressive phase retrieval via generalized approximate message passing _ , communication , control , and computing ( allerton ) , 2012 50th annual allerton conference on , ieee , 2012 , pp .  815822 ."
  ],
  "abstract_text": [
    "<S> we consider the principal component analysis problem for large tensors of arbitrary order @xmath0 under a single - spike ( or rank - one plus noise ) model . on the one hand , </S>",
    "<S> we use information theory , and recent results in probability theory , to establish necessary and sufficient conditions under which the principal component can be estimated using unbounded computational resources . </S>",
    "<S> it turns out that this is possible as soon as the signal - to - noise ratio @xmath1 becomes larger than @xmath2 ( and in particular @xmath1 can remain bounded as the problem dimensions increase ) .    on the other hand , we analyze several polynomial - time estimation algorithms , based on tensor unfolding , power iteration and message passing ideas from graphical models . </S>",
    "<S> we show that , unless the signal - to - noise ratio diverges in the system dimensions , none of these approaches succeeds . </S>",
    "<S> this is possibly related to a fundamental limitation of computationally tractable estimators for this problem .    </S>",
    "<S> we discuss various initializations for tensor power iteration , and show that a tractable initialization based on the spectrum of the matricized tensor outperforms significantly baseline methods , statistically and computationally . </S>",
    "<S> finally , we consider the case in which additional side information is available about the unknown signal . </S>",
    "<S> we characterize the amount of side information that allows the iterative algorithms to converge to a good estimate . </S>"
  ]
}