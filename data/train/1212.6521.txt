{
  "article_text": [
    "training neural networks for reinforcement learning tasks ( i.e.  as value - function approximators ) is problematic because the non - stationarity of the error gradient can lead to poor convergence , especially if the networks are recurrent . the data which the agent learns from is dependent on the agent s own policy which changes over time .",
    "an alternative to training by gradient - descent is to search the space of neural networks policy directly via evolutionary computation . in this _ neuroevolutionary _ framework , networks are encoded either directly or indirectly as strings of values or _ genes _ , called _ chromosomes _ , and then evolved in the standard way ( genetic algorithm , evolution strategies , etc . )    direct encoding schemes employ a one - to - one mapping from genes to network parameters ( e.g.  connectivity pattern , synaptic weights ) , so that the size of the evolved networks is proportional to the length of the chromosomes",
    ".    in indirect schemes , the mapping from chromosome to network can in principle be any computable function , allowing chromosomes of fixed size to represent networks of arbitrary complexity .",
    "the underlying motivation for this approach is to scale neuroevolution to problems requiring large networks such as vision  @xcite , since search can be conducted in relatively low - dimensional gene space . theoretically , the optimal or most _ compressed _ encoding is the one in which each possible network is represented by the shortest program that generates it , i.e.  the one with the lowest kolmogorov complexity  @xcite . while the lowest kolmogorov complexity encoding is generally not computable , but it can be approximated from above through a search in the space of network - computing programs @xcite written in a universal programming language .",
    "less general but more practical encodings @xcite often lack _ continuity in the genotype - phenotype mapping _",
    ", such that small changes to a genotype can cause large changes in its phenotype .",
    "for example , using cellular automata  @xcite or graph - based encodings  @xcite to generate connection patterns can produce large networks but violates this continuity condition .",
    "hyperneat  @xcite , which evolves weight - generating networks using neuro - evolution of augmenting topologies ( neat ;  @xcite ) provides continuity while changing weights , but adding a node or a connection to the weight - generating network causes a discontinuity in the phenotype space .",
    "these discontinuities occur frequently when e.g.  replacing neat in hyperneat with genetic programming - constructed expressions @xcite .",
    "furthermore , these representations do not provide an _ importance ordering _ on the constituent genes .",
    "for example , in the case of graph encodings , one can not gradually cut of less important parts of the graph ( gp expression , neat network ) that constructs the phenotype .    here",
    "we present an indirect encoding scheme in which genes represent fourier series coefficients , and genomes are decoded into weight matrices via an _ inverse _ fourier - type transform .",
    "this means that the search is conducted in the frequency domain rather than the weight space ( i.e. the spatio - temporal domain ) . due to the equivalence between the two , this",
    "encoding is both _ complete _ and _ closed _ : all valid networks can be represented and all representations are valid networks  @xcite .",
    "the encoding also provides continuity ( small changes to a frequency coefficient cause small changes to the weight matrix ) , allows the complexity of the weight matrix to be controlled by the number of coefficients ( importance ordering ) , and makes the size of the genome independent of the size of the network it generates .",
    "the intuition behind this approach is that because real world tasks tend to exhibit strong regularity , the weights near each other in the weight matrix of a successful network will be correlated , and therefore can be represented in the frequency domain by relatively few , low - frequency coefficients .",
    "for example , if the input to a network is raw video , it is very likely the input weights corresponding to adjacent pixels will have a similar value .",
    "this is the same concept used in lossy image coding where high - frequency coefficients containing very little information are discarded to achieve compression .",
    "this `` compressed '' encoding was first introduced by  @xcite where a version of practical universal search  @xcite was used to discover minimal solutions to well - known rl benchmarks . subsequently  @xcite it was used with the cosyne  @xcite neuroevolution algorithm where the correlation between weights was restricted to a 2d topology . in this paper ,",
    "the encoding is generalized to higher dimensional correlations that can potentially better capture the inherent regularities in a given environment , so that fewer coefficients are needed to represent successful networks ( i.e. higher compression ) .",
    "the encoding is applied to the scalable octopus arm using a variant of natural evolution strategies ( nes ;  @xcite ) , called separable nes ( snes ;  @xcite ) which is efficient for optimizing high - dimensional problems .",
    "our experiments show that while the task requires networks with thousands of weights , it contains a high degree of redundancy that the frequency domain encoding can exploit to reduce the dimensionality of the search dramatically .",
    "the next section provides a short tutorial on the fourier transform .",
    "section  [ sec : dctrep ] describes the dct network encoding and the procedure for decoding the networks .",
    "the experimental results appear in section  [ sec : exp ] , where we show how the compressed network representation both can accelerate learning and provide more robust solutions .",
    "section  [ sec : discussion ] discusses the main contributions of the paper , and provide some ideas for future research .",
    "any periodic function @xmath0 can be uniquely represented by an infinite sum of cosine and sine functions , i.e.  its _ fourier series _ :    @xmath1,\\ ] ]    where @xmath2 is time and @xmath3 is the frequency , and @xmath4 .",
    "the coefficients @xmath5 and @xmath6 specify how much of the corresponding function is in @xmath0 , and can be obtained by multiplying both sides of eq .",
    "( [ eq : fourier ] ) by the band frequency , integrating , and dividing by @xmath7 .",
    "so for the coefficient , @xmath8 , of the cosine with frequency @xmath9 : @xmath10 \\ , \\mathrm{d } t\\\\ & = &   \\frac{a_\\theta}{\\pi}\\int^{\\pi}_{-\\pi } \\ !",
    "\\cos(\\theta t)\\cos(\\omega t)\\ , \\mathrm{d } t,\\ \\ \\theta=\\omega\\\\ & = &   \\frac{a_\\theta}{\\pi}\\int^{\\pi}_{-\\pi } \\ !",
    "\\cos^2(\\theta t ) \\",
    ", \\mathrm{d } t\\\\ & = &   a_\\theta\\end{aligned}\\ ] ] ( 2 ) simplifies to ( 3 ) because all sinusoidal functions with different frequencies are orthogonal and therefore cancel out , @xmath11 , leaving only the frequency of interest , and ( 4 ) simplifies to ( 5 ) because @xmath12 .",
    "the fourier series can be extended to complex coefficients :    @xmath13    for a function periodic in @xmath14 $ ] the equations become : @xmath15 the _ fourier transform _ is then a generalization of complex fourier series as @xmath16 .",
    "the discrete @xmath5 is replaced with a continuous @xmath17 , while @xmath18 and the sum is replaced with an integral : @xmath19    in the case where there are @xmath20 uniformly - spaced samples of @xmath0 , the _ discrete fourier transform _",
    "( dft ) @xmath21 and the _ inverse discrete fourier transform _",
    "@xmath22 are defined .",
    "chromosomes , one for each of the weight matrices specified by the network architecture , @xmath23 .",
    "each chromosome is mapped , by algorithm  1 , into a coefficient array of a dimensionality specified by @xmath24 . in this example , an rnn with two inputs and four neurons is encoded as 8 coefficients .",
    "there are @xmath25@xmath24@xmath26 , chromosomes and @xmath24 @xmath27 .",
    "the second step is to apply the inverse dct to each array to generate the weight values , which are mapped into the weight matrices in the last step.,title=\"fig : \" ] @xmath24    the most widely used transform in image compression , is the _ discrete cosine transform _ ( dct ) which considers only the real part of the dft .",
    "the dct is an invertible function @xmath28 that computes a sequence of coefficients ( @xmath29 ) from a sequence of real numbers ( @xmath30 . there are four types of dct transforms based on how the boundary conditions are handled . in this paper , the type iii dct , dct(iii ) ,",
    "is used to transform coefficients into weight matrices .",
    "dct(iii ) is the inverse of the standard , forward dct(ii ) used in e.g. jpeg , and is defined as :    @xmath31\\right),\\quad k=0,\\dots , n-1\\ ] ]    where @xmath32 is the @xmath33-th weight and @xmath34 is the @xmath35-th frequency coefficient .",
    "the dct can be performed on signals of arbitrary dimension by applying a one - dimensional transform along each dimension of the signal .",
    "for example , in a 2d image a 1d transform is first applied to the columns and then , a second 1d transform is applied to the rows of the coefficient matrix resulting from the first transform .",
    "when a signal , such as a natural image , is transformed into the frequency domain , the power in the upper frequencies tends be low ( i.e.  the corresponding coefficients have small values ) since pixel values tend change gradually across most of the image .",
    "compression can be achieved by discarding these coefficients , meaning fewer bits need to be stored , and replacing them with zeros during decompression .",
    "this is the idea behind the network encoding described in the next section : if a problem can be solved by a neural network with smooth weight matrices , then , in the frequency domain , the matrices can be represented using only some of the frequencies , and therefore fewer parameters compared to the number of weights in the network .",
    "networks are encoded as a string or _ genome _ , @xmath36 , consisting of @xmath33 substrings or _ chromosomes _ of real numbers representing dct coefficients . the number of chromosomes is determined by the choice of network architecture , @xmath23 , and data structures used to decode the genome , specified by @xmath24 = @xmath37 , where @xmath38 , @xmath39 , is the dimensionality of the coefficient array for chromosome @xmath40 .",
    "the total number of coefficients , @xmath41 , is user - specified ( for a compression ratio of @xmath42 ) , and the coefficients are distributed evenly over the chromosomes . which frequencies should be included in the encoding is unknown .",
    "the approach taken here restricts the search space to _ band - limited _ neural networks where the power spectrum of the weight matrices goes to zero above a specified limit frequency , @xmath43 , and chromosomes contain all frequencies up to @xmath43 , @xmath44 .",
    "figure  [ fig : overview ] illustrates the procedure used to decode the genomes . in this example , a fully - recurrent neural network ( on the right ) is represented by @xmath45 weight matrices , one for the input layer weights , one for the recurrent weights , and one for the bias weights .",
    "the weights in each matrix are generated from a different chromosome which is mapped into its own @xmath38-dimensional array with the same number of elements as its corresponding weight matrix ; in the case shown , @xmath24 @xmath46 : 3d arrays for both the input and recurrent matrices , and a 2d array for the bias weights .    in previous work",
    "@xcite , the coefficient matrices were 2d , where the simplexes are just the secondary diagonals ; starting in the top - left corner , each diagonal is filled alternately starting from its corners ( see figure  [ fig : importance ] ) .",
    "however , if the task exhibits inherent structure that can not be captured by low frequencies in a 2d layout , more compression can potentially be gained by organizing the coefficients in higher - dimensional arrays .",
    "each chromosome is mapped to its coefficient array according to algorithm  1 ( figure  [ fig : simplex ] ) which takes a list of array dimension sizes , @xmath47 and the chromosome , @xmath48 , to create a total ordering on the array elements , @xmath49 . in the first loop",
    ", the array is partitioned into @xmath50-simplexes , where each simplex , @xmath51 , contains only those elements @xmath52 whose cartesian coordinates , @xmath53 , sum to integer @xmath54 .",
    "the elements of simplex @xmath51 are ordered in the while loop according to their distance to the corner points , @xmath55 ( i.e.  those points having exactly one non - zero coordinate ; see example points for a 3d - array in figure  [ fig : simplex ] ) , which form the rows of matrix @xmath56^t$ ] , sorted in descending order by their sole , non - zero dimension size . in each loop iteration ,",
    "the coordinates of the element with the smallest euclidean distance to the selected corner is appended to the list @xmath57 , and removed from @xmath51 .",
    "the loop terminates when @xmath51 is empty .",
    "after all of the simplexes have been traversed , the vector @xmath57 holds the ordered element coordinates . in the final loop ,",
    "the array is filled with the coefficients from low to high frequency to the positions indicated by @xmath57 ; the remaining positions are filled with zeroes .",
    "finally , a @xmath58dimensional inverse dct transform is applied to the array to generate the weight values , which are mapped to their position in the corresponding 2d weight matrix .",
    "once the @xmath33 chromosomes have been transformed , the network is complete .",
    ", is shown decoded into three different networks .",
    "the genome is first mapped into an @xmath59 matrix which is transformed into a weight matrix via the 2d inverse dct .",
    "the right column shows the resulting networks corresponding to each matrix .",
    "note that the size of the network is independent of the genome length .",
    "the squares denote input units ; the circles are neurons , arrow thickness denotes the magnitude of a connection weight and its color the polarity ( black = positive , red = negative).,title=\"fig:\"][fig : decode - different - size - nets ]    the dct network representation is not restricted to a specific class of networks but most of the conventional perceptron - type neural networks can be represented as a special case of a fully - connected recurrent neural networks ( frnn ) .",
    "this architecture is general enough to represent e.g. feed - forward and jordan / elman networks since they are just sub - graphs of the frnn .",
    "the compressed weight space encoding was tested on evolving neural network controllers for the octopus arm problem , introduced by  @xcite .",
    "the octopus arm was chosen because its complexity can scaled by increasing the arm length .",
    "the octopus arm ( see figure  [ fig : octopus ] ) consists of @xmath60 compartments floating in a 2d water environment .",
    "each compartment has a constant volume and contains three controllable muscles ( dorsal , transverse and ventral ) .",
    "the state of a compartment is described by the @xmath61-coordinates of two of its corners plus their corresponding @xmath62 and @xmath63 velocities .",
    "together with the arm base rotation , the arm has @xmath64 state variables and @xmath65 control variables .",
    "the goal of the task to reach a goal position with the tip of the arm , starting from three different initial positions , by contracting the appropriate muscles at each @xmath66s step of simulated time .",
    "while initial positions 2 and 3 look symmetrical , they are actually quite different due to gravity .    the number of control variables is typically reduced by aggregating them into @xmath67 `` meta''actions : contraction of all dorsal , all transverse , and all ventral muscles in first ( actions 1 , 2 , 3 ) or second half of the arm ( actions 4 , 5 , 6 ) plus rotation of the base in either direction ( actions 7 , 8) . in the experiments , both meta - actions and",
    "_ raw _ actions are used .",
    "consists of @xmath67 fully - connected neurons that control the arm through the meta - actions .",
    "the network is connected to @xmath68 inputs ( @xmath35 stands for the number of compartments , e.g. @xmath69 ) .",
    "the network for raw actions has @xmath70 neurons ( in the case of @xmath69 compartments ) organized in @xmath71 grid .",
    ", title=\"fig : \" ] + @xmath72@xmath73      networks were evolved to control a @xmath74 compartment arm using two different fully - connected recurrent neural network architectures : @xmath72 , with @xmath67 neurons controlling the meta - actions , and @xmath73 , having @xmath70 neurons , one for each primitive , non - aggregated ( raw ) action ( see figure  [ fig : psi ] ) .",
    "architecture @xmath72 has @xmath75 input weight matrix , @xmath76 recurrent weight matrix and bias vector of length @xmath67 , for a total of @xmath77 weights .",
    "architecture @xmath73 has @xmath78 input weight matrix , @xmath79 recurrent weight matrix and bias vector of length @xmath70 , for a total of @xmath80 weights .",
    "the following three schemes were used to map the genomes in the coefficient arrays , see figure  [ fig : decodingfunctions ] .    1 .",
    "@xmath81 : the genome is mapped into a single matrix ( i.e. @xmath82 ) , the inverse dct is performed , and the matrix is split into a @xmath83(8@xmath60 + 2 ) matrix of input weights , a @xmath84 weight matrix of recurrent connections and a bias vector of length @xmath35 , where @xmath35 is the number of neurons in the network , and @xmath60 is the number of arm compartments . 2 .",
    "@xmath85 : the genome is partitioned into @xmath45 chromosomes , mapped into three arrays : ( 1 ) a 3d , @xmath86(@xmath87)@xmath88 array , where 8 refers to the number of state variables per compartment , ( 2 ) an @xmath89 array for the recurrent weights of the @xmath35 neurons controlling the meta - actions , and ( 3 ) a bias vector of length @xmath35 .",
    "@xmath90 : the genome is partitioned into @xmath45 chromosomes , mapped into three arrays : ( 1 ) a 4d @xmath91(@xmath87)@xmath92(@xmath87 ) array that contains input weights for a @xmath93(@xmath87 ) grid of neurons , one for each raw action , ( 2 ) a @xmath94(@xmath87)@xmath95(@xmath87 ) recurrent weight array , and ( 3 ) and a @xmath94(@xmath87 ) bias array .",
    "the dimension size of 3 in these arrays refers to the number of muscles per compartment .",
    "schemes @xmath81 and @xmath85 were used to generate @xmath72 networks ; @xmath81 and @xmath90 were used to generate @xmath73 networks .",
    "coefficient arrays are filled using algorithm  1 , and weights for each compartment are placed next to weights for the adjacent compartments in the physical arm .",
    "scheme @xmath81 was used by  @xcite , and is included here for the purpose of comparison .",
    "this is the simplest mapping that forces a single set of coefficients ( chromosome ) to represent all of the network weight matrices .",
    "scheme @xmath85 tries to capture 3d correlations between input weights , so that fewer coefficients may be required to represent the similarity between not only weights with similar function ( i.e.  affecting state variables near each other on the arm ) _ within _ a given arm compartment ( as in @xmath81 ) , but also _ across _ compartments . the input , recurrent and bias weights are compressed separately .",
    "@xmath90 arranges the weights such that correlations between the all four dimensions that uniquely specify a weight a can be exploited . for example , this data structure places next to each other input weights affecting : muscles with the same function in adjacent compartments , muscles in the same compartment with different functions , the same muscle from adjacent compartments , etc .",
    "gcomposed5 ( 75,320)@xmath81 ( 270,320)@xmath85    @xmath90      indirect encoded networks were evolved with a fixed number of coefficients @xmath96 , and using an incremental procedure describe below , for the four configurations @xmath72@xmath81 , @xmath72@xmath85 , @xmath73@xmath81 , and @xmath73@xmath90 , where for example @xmath73@xmath90 denotes the architecture that uses raw actions and is decoded using scheme @xmath90 . each of the @xmath97 ( compression ratios ) @xmath98 @xmath99 ( @xmath23@xmath24 configurations ) = @xmath100 setups consisted of 20 runs .",
    "for comparison direct encoded networks were also evolved where the genomes explicitly encode the weights , for a total of @xmath77 and @xmath80 genes ( weights ) , for @xmath72 and @xmath73 , respectively .",
    "networks were evolved using separable natural evolution strategies ( snes ; @xcite ) , an efficient variant in the nes @xcite family of black - box optimization algorithms . in each generation",
    "the algorithm samples a population of @xmath101 individuals , computes a monte carlo estimate of the fitness gradient , transforms it to the natural gradient and updates the search distribution parameterized by a mean vector , @xmath102 , and covariance matrix , @xmath103 .",
    "adaptation of the full covariance matrix is costly because it requires computing the matrix exponential , which becomes intractable for large problems ( e.g. more than 1000 parameters  network weights or dct coefficients ) .",
    "snes combats this by restricting the class of search distributions to be gaussian with a diagonal covariance matrix , so that the search is performed in predefined coordinate system .",
    "this restriction makes snes scale linearly with the problem dimension ( see  @xcite for a full description of nes ) .",
    "the population size @xmath104 is calculated based on the number of coefficients , @xmath105 , being evolved , @xmath106 , the learning rates are @xmath107 .",
    "each snes run is limited to @xmath108 fitness evaluations .",
    "the fitness was computed as the average of the following score over three trials : @xmath109 , \\label{eqn : fit}\\ ] ] where @xmath2 is the number of time steps before the arm touches the goal , @xmath110 is a number of time steps in a trial , @xmath111 is the final distance of the arm tip to the goal and @xmath112 is the initial distance of the arm tip to the goal .",
    "each of the three trials starts with the arm in a different configuration ( see figure  [ fig : octopus ] ) .",
    "this fitness measure is different to the one used in @xcite , because minimizing the integrated distance of the arm tip to the goal causes greedy behaviors . in the viscous fluid environment of the octopus arm ,",
    "a greedy strategy using the shortest length trajectory does not lead to the fastest movement : the arm has to be compressed first , and then stretched in the appropriate direction .",
    "our fitness function favors behaviors that reach the goal within a small number of time steps .    in all of the experiments described so far",
    ", the encoding stays fixed throughout the evolutionary run .",
    "therefore it depends on correctly guessing the best number of coefficients . in an attempt to automatically determine the best number of coefficients , a set of 20 simulations",
    "were run , using configuration @xmath73@xmath90 , where the networks are initially encoded by 10 coefficients and then the number of coefficients incremented by 10 every 6000 evaluations .",
    "if the performance does not improve after @xmath97 successive coefficient additions , the algorithm ends and the best number of coefficients is reported . adding a coefficient to the network encoding means incrementing the number of dimensions in the mean , @xmath102 , and covariance , @xmath103 , vectors of the snes search distribution .",
    "when coefficients are added the complexity of all @xmath33 weight matrices increases . for example , a genome consisting of @xmath113 coefficients is distributed into @xmath45 chromosomes : @xmath114 , @xmath115 and @xmath116 .",
    "additional coefficients would then be appended one at a time cycling through the chromosomes , adding the first to @xmath117 ( the first shortest chromosome ) , the second to @xmath118 , the next to @xmath119 , and so on , until all 10 new coefficients are added , resulting in chromosomes of length @xmath120 .",
    "if a chromosome reaches a length equal to the number of weights in its corresponding weight matrix , then it can not take on any more coefficients , and any additional coefficients are distributed the same way over the other chromosomes .",
    "in most tasks , not all input or control variables can be organized in such way ( such as the base rotation in the octopus arm task ) . in such case",
    ", one can either use a separate weight array , or place the weights together in a large array and decode them separately .",
    "in such a case , some values that result from the inverse dct are not used .",
    "convergencegrid3b ( 30,256)@xmath72@xmath81 ( 235,256)@xmath72@xmath85 ( 30,125)@xmath73@xmath81 ( 235,125)@xmath73@xmath90    barchartperformance3b ( 55,113)@xmath72@xmath81 ( 158,113)@xmath72@xmath85 ( 260,113)@xmath73@xmath81 ( 363,113)@xmath73@xmath90    weightmatrices2    @xmath72@xmath81@xmath73@xmath90    figure  [ fig : convergence ] summarizes the experimental results .",
    "each of the three log - log plot shows performance of each encoding for one of the three @xmath23@xmath24 configurations ; each curve denotes the best fitness in each generation ( averaged over 20 runs ) .",
    "the bar - graph shows the number of evaluations required on average for each set to reach a fitness of 0.75 .    for the @xmath72@xmath81 , controllers encoded indirectly by 40 coefficients or less ( @xmath121 ) reach high fitness more quickly than the direct encoded controllers",
    "however , the final fitness after 6000 evaluations is similar across all encodings . because the networks are relatively small ( 728 weights ) when meta - actions are used , direct search in weight space is still efficient .",
    "when architecture @xmath72 is decoded using @xmath85 , surprisingly the advantage of the indirect encoding is lost .",
    "while the 3d coefficient input array would seem to offer higher compression , it turns out that the number of coefficients required properly set the weights in this structure is so close to the number of weight in the network that nothing is gained .    for raw action control , @xmath73 , where the networks now have 3680 weights , the simple @xmath81 scheme again works well",
    ", converging 60% faster while using less than @xmath122% as many parameters ( @xmath123 ) as the direct encoding .",
    "however , much higher compression comes from @xmath90 where correlations in all four dimensions of the arm can be captured .",
    "the direct encoding only outperforms @xmath113 , which does not offer enough complexity to represent successful weight matrices .",
    "but , with just @xmath124 dct coefficients , the compression ratio goes to @xmath125:@xmath66 ; reaching a fitness of @xmath126 in only @xmath127 evaluations , more than @xmath128 times faster than the direct encoding .",
    "figure  [ fig : matrices ] shows examples of weight matrices evolved for the two most successful configurations .",
    "notice how regular the weight values are compared to the direct encoded networks .",
    "the evolved controllers exhibit quite natural looking behavior .",
    "for example , when starting with the arm hanging down ( initial state 3 ) , the controller employs a _ whip_-like motion to stretch the arm tip toward the goal , and overcome gravity and the resistance from the fluid environment ( figure  [ fig : film ] ) .",
    "runs ) achieved for a given number of coefficients in incremental evolution of @xmath73@xmath90 networks .",
    "the median number of coefficients for which adding more coefficients does not improve the solution is @xmath129 . ]",
    "figure  [ fig : increm ] contains box - plots showing the median , maximum and minimum ( out of @xmath124 independent runs ) fitness found during the progress of the incremental coefficient evolution . with the initial 10 coefficients the runs reach a median fitness of @xmath130 , but with very high variance .",
    "as coefficients are added the median improves peaking at @xmath131 , and the variance narrows to a minimum at @xmath132 .",
    "to @xmath133 coefficients ( box - plots ) are compared to directly encoded controllers ( horizontal lines ) .",
    "data points are the median of 20 runs , the boxes indicate the lower and upper quartiles , and the bars the minimum and maximum values . ]    in this section the best controllers from the two most successful indirect encodings , @xmath72@xmath81 and @xmath73@xmath90 , are tested in two ways to measure both the generality of the evolved behavior , and that of the underlying frequency - based representation .",
    "controllers were re - evaluated on the task using two new starting positions , with the arm oriented in the @xmath134 and @xmath135 directions instead of the three positions ( @xmath136 , @xmath137 , @xmath138 ) used during evolution ( see figure  [ fig : octopus ] ) .",
    "figure  [ fig : posedfn3 ] shows the results of this test comparing direct and indirect encoded controllers .",
    "each data point is the median fitness of the best controller from each of the 20 runs for a given number of coefficients ; the boxes indicate the upper / lower quantiles and the bars the min / max values .",
    "the solid straight line is the median fitness for the direct encoded controllers , the dashed lines correspond to the upper / lower quantiles . for @xmath139",
    "the generalization is comparable to that of direct encoding , but with significantly lower variance , and networks encoded with @xmath140 generalize better that the direct nets , again with lower variance .",
    "the performance of @xmath141 yields the best generalization , very consistently performing nearly as well as on the original three starting positions .",
    "the networks with lower compression ( @xmath142 ) better capture the general behavior required to reach the goal from new starting positions .      in this test ,",
    "the arm length is changed from 10 compartments to between 3 and 20 .",
    "different arm lengths mean different numbers of inputs , and consequently require different size weight matrices .",
    "for the dct encoded nets , the size of the network is independent of the number of coefficients , so that different arm lengths can be accommodated by modifying the size of the coefficient matrix appropriately ( see figure  [ fig : decode - different - size - nets ] ) .",
    "however , for the direct encoded nets , there is no straightforward way to add or remove structure to the network meaningfully .    in order to be able to compare direct and indirect nets ,",
    "the direct nets were transformed into the frequency domain by reversing the procedure depicted in figure  [ fig : overview ] .",
    "first , the network weights are mapped to the appropriate positions in the correct number of multi - dimensional arrays .",
    "the _ forward _ dct is applied to each array , and the network is then `` re - generated '' to the appropriate size for the specified arm by adjusting the size of the coefficient matrix ( padding with zeros if the matrix is enlarged ) , and applying the inverse dct .",
    "the best network from each run was re - evaluated on each of the arm lengths ( 3 - 20 ) .",
    "the number of time steps allowed to control arms longer than 10 compartments was increased linearly up to @xmath143 time steps for @xmath124 compartment arm .",
    "the closest position of the arm tip and the time step when the goal was reached were used to compute the fitness .",
    "arms that moved the arm tips further away from the goal were assigned zero fitness because the closest position ( which is in fact the initial arm position ) was reached in zero time .",
    "the results of this test are summarized in figure  [ fig : gen - length ] .",
    "the surface plots show the difference between the indirect and direct encoding for each compression level and number of compartments for ( a ) @xmath72@xmath81 ( meta - actions ) , and ( b ) @xmath73@xmath90 ( raw actions ) .",
    "the elevation of the surface above the @xmath144 plane indicates how much better or worse the indirect encoding is in generalizing to different arm lengths than the direct encoding ( with networks resized as described above ) .",
    "while the convergence speed of the indirect and direct encoding was very similar for @xmath72 ( figure  [ fig : convergence ] ) , the indirect encodings are less sensitive to changes in the network size .",
    "the deep trough at 10 compartments in graph ( a ) is due to the fact that , for this length arm ( the same as used to evolve the nets ) , the direct encoding is slightly better on average than the indirect encoding ( see final fitness in @xmath72@xmath81 plot in figure  [ fig : convergence ] ) , but can not generalize well to even small changes to the arm length ",
    "the direct encoded solutions are overspecialized .    as with the test in section  [ sec : gen - pos ]",
    ", the best generalization performance is obtained with @xmath124 and @xmath145 coefficients for both @xmath72@xmath81 and @xmath73@xmath90 . for larger numbers of coefficients ,",
    "the generalization declines gradually for arm length of around 10 , and more rapidly for shorter arms",
    ".     compartments .",
    "we can see that the network scales well ( except an arm with lengths of @xmath146 and @xmath124 ) and produces a smooth behavior transition for arms having from @xmath99 to @xmath147 compartments .",
    "the movement starts to be different in case of the long arms @xmath141 and the generalization performance degrades . ]",
    "the experimental results revealed that searching in the `` compressed '' space of fourier coefficients can improve search efficiency over the standard , direct search in weight space .",
    "the frequency domain representation exploits the correlation between weight values that are spatially proximal in the weight matrix , thereby reducing the number of parameters required to encode successful networks . both fixed and incremental search in coefficient space discovered solutions that required an order of magnitude fewer parameters than the direct encoding for the octopus arm task , and a similar improvement in learning speed .",
    "perhaps more importantly , it also produced controllers that were more general with respect to initial states , and more robust to changes in the environment ( the arm length ) .",
    "this supports the idea that band - limited networks are in some sense simpler , and therefore less prone to overfitting .",
    "the choice of encoding scheme , @xmath24 , proved decisive in determining the amount of compression attainable for the two network architectures .",
    "there are many possible ways to organize the coefficients as input to the decompressor ( idct ) , but the fact that even the most naive approach , @xmath81 ( where one set of coefficients is used to represent all of the weight matrices ) worked well , is encouraging . the slightly more complex @xmath85 illustrates how adding higher dimensional correlations does not necessarily lead to better compression .",
    "so , how to choose a good @xmath24 ?",
    "a useful default strategy may be to first identify the _ high - level dimensions _ of the environment that partition the weights qualitatively ( e.g.  for input weights : the compartment from which its connection originates , the compartment where it terminates , the muscle it affects , and which of the eight state variables it is associated with ) , and assume that these dimensions are all correlated by arranging the coefficients in data structures with the same number of dimensions , as was done in @xmath73@xmath90 .",
    "this strategy , though the most complex , yielded by far the most compression , with solutions having thousands of weights being discovered by searching a space of only 20 coefficients .",
    "it might be possible to achieve even higher compression by switching to a different basis altogether , such gaussian kernels  @xcite or wavelets .",
    "one potential limitation of a fourier - type basis is that if the frequency content needs to vary across the matrix , then many coefficients will be required to represent it .",
    "this is the reason for using multiple chromosomes per genome in our experiments .",
    "in contrast , wavelets are designed to deal with this spatial locality , and could therefore provide higher compression by allowing all network matrices to by represented compactly by a single set of coefficients ; for example , a simple scheme like @xmath73@xmath81 could possibly compress as well as @xmath73@xmath90 while requiring less domain knowledge .    in the current implementation",
    ", the network topology ( number of neurons ) is simply specified by the user .",
    "however , given the fact that the size of the weight matrices is independent of number of coefficients , it may be possible to optimize the topology by decoding genomes into networks whose size is drawn from probability mass function that is updated each generation according to relative performance of each topology .",
    "future work will begin in this direction to not only search for parsimonious representation of large network , but also to determine their complexity .",
    "this work was supported by the snf grants 200020 - 125038/1 and 200020 - 140399/1 .",
    "gauci , j. and stanley , k. ( 2007 ) . generating large - scale neural networks through discovering geometric regularities . in _ proceedings of the conference on genetic and evolutionary computation _ , pages 9971004 , new york , ny , usa . acm .          kassahun , y. , edgington , m. , metzen , j.  h. , sommer , g. , and kirchner , f. ( 2007 ) . a common genetic encoding for both direct and indirect encodings of networks . in _ proceedings of the conference on genetic and evolutionary computation ( gecco-07 )",
    "_ , pages 10291036 , new york , ny , usa . acm .",
    "schaul , t. , glasmachers , t. , and schmidhuber , j. ( 2011 ) .",
    "high dimensions and heavy tails for natural evolution strategies . in _ proceedings of the genetic and evolutionary computation conference ( gecco-2011 , dublin)_.      schmidhuber , j. ( 1995 ) . discovering solutions with low kolmogorov complexity and high generalization capability . in prieditis ,",
    "a. and russell , s. , editors , _ proceedings of the twelfth international conference on machine learning ( icml ) _ , pages 488496 .",
    "morgan kaufmann publishers , san francisco , ca .",
    "woolley , b.  g. and stanley , k.  o. ( 2010 ) . evolving a single scalable controller for an octopus arm with a variable number of segments . in schaefer , r. , cotta ,",
    "c. , kolodziej , j. , and rudolph , g. , editors , _ ppsn ( 2 ) _ , volume 6239 of _ lecture notes in computer science _ , pages 270279 ."
  ],
  "abstract_text": [
    "<S> neuroevolution has yet to scale up to complex reinforcement learning tasks that require large networks . </S>",
    "<S> networks with many inputs ( e.g.raw video ) imply a very high dimensional search space if encoded directly . </S>",
    "<S> indirect methods use a more compact genotype representation that is transformed into networks of potentially arbitrary size . in this paper </S>",
    "<S> , we present an indirect method where networks are encoded by a set of fourier coefficients which are transformed into network weight matrices via an inverse fourier - type transform . </S>",
    "<S> because there often exist network solutions whose weight matrices contain regularity ( i.e.  adjacent weights are correlated ) , the number of coefficients required to represent these networks in the frequency domain is much smaller than the number of weights ( in the same way that natural images can be compressed by ignore high - frequency components ) . </S>",
    "<S> this `` compressed '' encoding is compared to the direct approach where search is conducted in the weight space on the high - dimensional octopus arm task . </S>",
    "<S> the results show that representing networks in the frequency domain can reduce the search - space dimensionality by as much as two orders of magnitude , both accelerating convergence and yielding more general solutions . </S>"
  ]
}