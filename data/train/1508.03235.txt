{
  "article_text": [
    "in large scale chip multicore ( lcmp ) , on - chip cache management and interconnection network have significant impact on performance , power consumption of the system . as the core count of chip multicore increase , the pressure on on - chip cache ( in particularly the last level cache ( llc ) l2 cache )",
    "increase significantly .",
    "single shared cache ( physically shared ) is not good for performance in terms of access latency and interference among cores .",
    "any how , there are many level of caches in this kind of system , first level cache ( l0 an l1 ) must be private , but the last level cache ( the l2 cache ) which must be bigger and need to be managed efficiently .",
    "also the completely distributed ( physically distributed ) cache may not be good for many cases where a core requires a larger portion of cache . distributed cache suffers from increased local cache pressure and eviction .",
    "so logically shared and physically distributed model ( lspd ) capture both performance in terms of access time and share effectively . among various last level cache management models ,",
    "lspd model of last level cache is promising in terms of cache utilization and overall system performance .",
    "the performance of lspd model depends on effective policy for the cache block placement , eviction , migration and directory management .",
    "mesh interconnection network to connect the cores in lcmp is widely used as it provides a good trade off between simplicity , scalability and maintainability . as stated in @xcite ,",
    "the interconnection network in lcmp occupies significant amount of area and consumes around 40% of total power , so bufferless network is promising alternative design to reduce hardware cost and power consumption where overall network traffic is low to medium range .",
    "it is good idea to explore all the design spaces of different cache management policies , which suite to large multicore system connected using bufferless interconnection network .",
    "so in this work , we have designed an efficient simulator to simulate on chip cache management of lcmp where cores are connected using bufferless network .",
    "as most of available simulators ( targeted to simulate large scale multicore architecture ) are very slow , and also our designed one is very slow , so we have parallelized our designed simulator and ported onto gpu platform using cuda programming @xcite to speed up the simulation . in terms of cost of the system , a personal computer with an additional gpu card is cheaper than traditional higher performance server computer .",
    "this also motivates us to design our gpu based simulator .",
    "rest of the paper is organized as follows : section [ prevwork ] gives the survey of previous work .",
    "section [ cachearch ] present the assumed cache architecture model and management policy for our designed simulator .",
    "section [ buffnet ] describes about the bufferless on chip interconnection methodology used in our simulator design .",
    "we have presented a short introduction to gpu and cuda programming model in section [ cudagpu ] .",
    "section [ gpuimpl ] describes about both serial and parallel version implementation of simulator and its design . in section",
    "[ result ] , we present our experimental result with analysis , and finally we briefly conclude the paper and discuss about future work in section [ concl ] .",
    "performance of most commonly used shared cache ( physically shared ) is pathetic in terms of access latency and interference , and also as core counts of chip multicore have direct impact on pressure on the shared on - chip cache .",
    "many researchers proposed technique to reduce interference either by partition efficiently or managing efficiently @xcite to maximized the performance of multicore system .    distributed cache reduces interference and latency , but may not be good where a core requires a larger portion of cache @xcite . and in that case a distributed cache slice may suffer from increased local cache pressure and eviction .",
    "so lspd model provide an effective trade - off between cache access time and share .",
    "these papers @xcite describes about various variation of lspd cache .",
    "also they proposed many logically sharing policy , placement , eviction , replication and management of physically distributed cache to improve the performance of multicore system . in @xcite , they used a mechanism called cooperative caching , which put a locally evicted block in another on - chip l2 cache that may have spare capacity , rather than evict it from the on - chip hierarchy entirely . in @xcite , authors proposed architecture with each core having a cache with configurable or adoptable boundary between private and shared portion . in @xcite , author classified cache access patterns of a range of server and scientific workloads into distinct classes , where each class is amenable to different block placement policies .",
    "there are many noc simulators and they have different features , and takes different configurable parameters .",
    "a short list of the current network on chip simulators includes noxim @xcite , nirgam @xcite and nostrum nsse @xcite .",
    "these simulator are cycle accurate noc simulator developed using systemc . in noxim",
    "@xcite , user can customize many parameters likes network size , buffer size , packet size distribution , routing algorithm , packet injection rate , traffic time distribution , traffic pattern , and etc to simulate noc .",
    "nirgam @xcite is a simulator similar to noxim for noc routing and application modeling .",
    "both noxim and nirgam uses randomly generated traffic as input .",
    "nnse takes topology , routing policy , and traffic patterns and based on these configuration parameters a simulator is built and executed .",
    "nnse uses both randomly generated traffic and core generated trace traffic .",
    "these simulators mostly design to simulate network on chip target to mainly buffered network and without supporting memory hierarchy .",
    "jantsch et .",
    "@xcite explain performance different memory architecture and management in an noc platform . as most of the available multicore simulators like multi2sim @xcite , sesc @xcite , simicgems @xcite , and , etc are very slow . and",
    "noc simulators written in systemc are extremely slow .",
    "so we ca nt simulate a bigger design with more than 100 cores .",
    "many researchers propose to enhance noc simulation using multi - threaded parallel core or accelerated using fpgas and these are hasim , protoflex and fast @xcite . in simflex @xcite ,",
    "another group of researcher uses statistical sampling of system to speed up the simulation of targeted multicore system .",
    "darsim @xcite , hornet @xcite and graphite @xcite are cycle accurate parallel simulator targeted to explore multicore architecture",
    ". these simulator can be executed either in network only mode by using synthetic traces or in full system multicore mode using a build in mips core simulator .",
    "these simulator target to run simulation on multicore ( power full xeon like octa or hexa core ) server system . also both the simulator uses several techniques to speed up the simulation .",
    "these techniques are direct execution , seamless multicore and multi - machine distribution , and lax or periodic synchronization .",
    "others parallel simulators that target multicore architecture are cotson @xcite , bigsim @xcite , fastmp @xcite and wisconsin wind tunnel ( wwt ) @xcite",
    ".    in @xcite authors presented first gpu - based noc simulator without considering bufferless , that is the fastest noc simulators built till date . to the best of our knowledge ,",
    "our simulator is the first attempt on building gpu based simulator for lspd cache system large chip multicore connected with bufferless noc .",
    "lspd cache model on - chip l2 cache is well suited for large chip multicore . for simplicity and scalability , mesh type interconnections are popular in large scale chip multicore .",
    "figure [ noc ] shows an example of 16 core connected in mesh fashion and with each core having its own local l1 cache and logically shared l2 cache slices .",
    "l1 caches are private per core due to their tight timing requirement and l1 caches are write through cache . each core of the system have equal amount of l2 cache slices locally",
    ". the local l2 cache slice can be access very fast where as remote l2 slice access takes longer time .",
    "there is no replication of l2 cache block , means only one copy of l2 cache block will be present in the system at a time .    as off - chip bandwidth",
    "is limited , it is essential to utilize the on - chip cache space efficiently and effectively . also accessing a off - chip memory location is slower than accessing an on - chip remote cache location .",
    "so efficient placement , eviction , replacement and migration are very much important to manage the cache space resource in chip multicore .    in lspd cache model , directory contains the information of cache blocks .",
    "if there is a local l2 cache miss , then directory is consulted for on chip availability of the requested cache block in the l2 cache system .",
    "if cache block is present at some other remote core then it get accessed from there over network .",
    "if a core is accessing a particular remote cache block very frequently then migrating that cache block to frequently accessing core reduces the future cache access latency for that cache block , and also reduces network traffic . if a cache block gets migrated from a core then upon receiving a request for migrated cache block , core redirect the request packet to current location of migrated cache block till update of the migration in the directory .        in fig .",
    "show an extended view of cache arrangement for one core .",
    "configuration of all core are same , each core have their own l1 cache and shared l2 cache slices .",
    "each cache block contains tag , address , statistics counter and data block .",
    "tag is required for identifying cache block .",
    "when a cache block is evicted then entry of the cache block must be deleted from directory .",
    "a directory is a place where we store information about all the cache blocks present in the whole on - chip l2 .",
    "so , directory maintains a list of cache block and their locations .",
    "statistics counter maintained for keeping track of accesses from different node and it helps in migration .",
    "for example , statistics counter holds information about last @xmath0 ( say 10 ) accesses .",
    "[ lspdir ]    fig [ lspdir ] .",
    "shows an example of logically shared and physically distributed cache , and it maintain a directory to keep track of location on chip cache blocks of l2 cache system .",
    "content of directory is shown in right part of figure [ lspdir ] .",
    "location of l2 cache block slice with address is ( 2,4 ) , which means the cache block with tag value 100 is available on node at row 2 and column 4 of the assumed 2d mesh .",
    "each node have equal size l2 cache slice that is locally accessible to the node and also remotely accessible to all other nodes . for simplicity",
    ", we can assume a global directory .",
    "directory maintains the location information about all l2 cache blocks present on chip at the moment .",
    "when a node get a local l2 cache miss then the node will first consult with directory to check availability of requested cache block at other nodes .",
    "if the requested cache block is present at a remote node , then directory will send reply with the address of node which is current owner of that block .",
    "then a new request will made by the requesting node to local l1 cache from the remote node .",
    "also we need to write back the evicted l1 block to the corresponding l2 slices of evicted l1 block , the location of evicted block is necessary to write back and this location can be stored in cache block it self or directory need to be consulted . if there is no any information about the requested block found in directory then directory will send negative reply to requested node indicating block not found .",
    "in that case requested block will make a new request to next higher level memory for that particular block .",
    "one can think of distributed directory , in this case directory is distributed across all the nodes .",
    "the disadvantage of centralized directory is that it may become a hot spot or bottleneck point in system .",
    "we have not considered cache coherence here .",
    "if we assume l1 is write through cache then l1 is responsible for coherence .",
    "clearly there are many techniques available to manage on chip last level cache data block to improve performance of lspd cache system .",
    "there are efficient ways of placement and eviction methods for lspd l2 cache systems .",
    "but certainly all approaches have their own advantages and disadvantages .",
    "a simple and good approach is to place the block in requester cache after that migrate to other core based on frequency of access for nodes .",
    "migration of a cache block may improve performance of lspd cache system . if a node is frequently requesting to a specific cache block of a remote node , then that remote cache block should be moved to requesting node .",
    "this placement approach is local placement approach .",
    "this may lead to problem of not getting proper required shared of l2 cache space for some core .",
    "so we may need to design policy so that system should globally decide the placement , eviction and migration of the l2 cache blocks .",
    "in centralized directory organization global management is easier .",
    "from implementation point of view we can think , within l2 cache , for each cache block have a statistics counter which contains the information about the number of access from the remote nodes .",
    "also it records the number of access from all remote nodes in a list for last @xmath0 accesses .",
    "if a remote node have accessed it mostly , then migration get triggered . if a local node have accessed it most time than there is no need of migration .",
    "if migration is required , whole cache block will be sent / migrate to that remote node .",
    "while migration of cache blocks is in transit , source node will continue to provide service for migrated cache block till migrated cache block reached its destination .",
    "when migrated cache block reaches at its destination , source packet invalidates its copy and directory get updated .",
    "a statistics counter need to be placed in every cache block to implement the migration .",
    "the check for a local cache migration may get triggered when a request comes from remote nodes .",
    "if a node is servicing a request for a particular l2 cache block then that block will not get migrated till current request is serviced .",
    "if a node , say @xmath1 , make a request for particular l2 cache block at time @xmath2 ( say ) to node @xmath3 . while request is in transit , in mean time if desired cache block gets migrated from node @xmath3 to @xmath4 .",
    "then upon receiving the request from node @xmath1 , node @xmath3 will make a new request on behalf of node @xmath1 .",
    "it will redirect the request by changing destination address , since directory has information of all l2 cache block .",
    "it prevents from dropping of request cache block .",
    "we have assumed local victim selection for replacement . if a chosen victim cache block position in directory already contains a valid cache block then that cache block entry is deleted since it is no longer accessible after replacement .",
    "replacement routine gets triggered at the following time :    * when a new cache block comes to l2 cache from main memory .",
    "* when a migrated cache block from remote node comes in to a local l2 cache .",
    "* a l1 cache block replacement happens when a new block comes in to local l1 cache either from local l2 cache or from remote l2 cache .",
    "the evicted block need to be written back to the memory or corresponding l2 block if it is a l1 block replacement .",
    "when a node make a request for remote l2 cache block in network then it will not send another request packet till it receives the cache block from remote node or from memory .",
    "in the mean time it can serve to all local requests .",
    "that means miss under a miss is not allowed , but a hit under a miss is allowed .",
    "as in bufferless network data flits may arrive in out of order , it is necessary to keep a re - order buffer to order the flits of a cache request / access to make a complete cache block or request .      in our to be simulated target architecture",
    ", we assume each node has receiving buffers of l2 block size .",
    "data from the network get received and is stored in receiving buffer .",
    "these data may be l2 cache block or l1 cache block or any other control messages .",
    "after completion of data reception for a particular event , the data block from the buffer is transferred to computer system ( or written to l1 or l2 cache ) .",
    "similarly when a node sends or migrates a block to other node through network , it placed the data block into sending buffer before starts sending .",
    "since for a request / cache block transfer many flits are send , and each flit routed independently .",
    "so these flits may follow different paths and take different travel time to reach its destination . and",
    "these flits may reach in out of order at receiving node .",
    "receiving node will put all the flits in buffer according to its number .",
    "when all flits of a access or cache block migration arrived at destination then it get delivered to local core of destination router @xcite .",
    "table [ ff ] shows typical flits division of different request , access and migration of cache blocks .",
    ".flit division of request / access [ cols=\"<,^,^,<,^,^ \" , ]     in our simulator , we store data for all the routers , cores and caches on the global memory of gpu . global memory",
    "is limited in size .",
    "data structure to simulate cache takes significant amount of memory of gpu .",
    "so with the varying of cache size , more number of threads can be executed . in the table",
    "[ cacheproc ] , the upper limit of threads with different cache size has been shown .",
    "also implementation of migration requires some amount of memory for each cache blocks .",
    "if to be simulated system is large , where huge number of cache blocks are present then migration implementation consumes significant amount of memory to store access history of each cache blocks .",
    "so by removing migration strategy , we were able simulate more number of cores . in the third row of the table [ cacheproc ] , for given cache configuration",
    ", we were able to run 30,000 core with migration and 43,000 core without migration .",
    "simulation of large scale cmp with on chip interconnection have huge scope of parallelism and we have exploited this parallelism using a modern gpu to achieve significant speedup as compared to serial execution of our designed simulator .",
    "we have achieved 25 times speedup in the trace based simulation . by simulating the more cores in to be targeted system",
    ", we can achieve more speedup in our gpu based parallel simulator .",
    "this simulator can be used for various scientific and research purpose .",
    "we have used centralized directory and also local cache replacement policy , which will not give good performance in reality . by using global cache replacement policy using centralized directory ,",
    "performance can be improved of the targeted system . in future",
    ", we are planning to implement distributed directory for more realistic case .",
    "it will improve the performance and hence speedup .",
    "as we have not considered cache coherence in our simulator , it can be very good extension .",
    "also , modeling global placement , migration and eviction in simulator may add substantial value to simulator ."
  ],
  "abstract_text": [
    "<S> _ last level cache management and core interconnection network play important roles in performance and power consumption in multicore system . </S>",
    "<S> large scale chip multicore uses mesh interconnect widely due to scalability and simplicity of the mesh interconnection design . as interconnection network occupied significant area and consumes significant percent of system power , </S>",
    "<S> bufferless network is an appealing alternative design to reduce power consumption and hardware cost . </S>",
    "<S> we have designed and implemented a simulator for simulation of distributed cache management of large chip multicore where cores are connected using bufferless interconnection network . also , we have redesigned and implemented the our simulator which is a gpu compatible parallel version of the same simulator using cuda programming model . </S>",
    "<S> we have simulated target large chip multicore with up to 43,000 cores and achieved up to 25 times speedup on nvidia geforce gtx 690 gpu over serial simulation . _ </S>"
  ]
}