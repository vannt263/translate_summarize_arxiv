{
  "article_text": [
    "the ever - increasing importance of modern big - data analytics brings with it the imperative to understand fusion and inference on multiple and massive disparate , distributed data sets .",
    "what processing can be profitably done separately , for subsequent joint inference ? in the case where each data set consists of measurements on the same objects , and combining the full data sets is prohibitively expensive , it seems reasonable to separately project each large , high - dimensional collection to a low - dimensional space , and to then combine the representations .",
    "unfortunately , this model can lead to undesirable incommensurability with significant deleterious effects on fusion and inference . in this manuscript",
    ", we quantify an appearance of this phenomenon .    in section [ caut ]",
    "we begin with an idealized tale of two scientists and its accompanying theorem [ main ] , in order to pave the way for our main result , theorem [ realmain]stated and proved in section [ general]wherein , under more general conditions , an asymptotic relationship is given between the procrustean fitting - error of the two lower dimensional data sets and a distance between the projections .",
    "then , in section [ sims ] , we perform simulation experiments to illustrate and support our main result theorem [ realmain ] , and we use these simulations to explore the implications of theorem [ realmain ] ; in particular , when there is an insufficient spectral gap in the covariance structure at the projection - dimension cutoff , then large projection distance may result between the projections for the two data sets , and inordinately large procrustean fitting - error then follows .",
    "this  incommensurability phenomenon \" was named in priebe et al . @xcite .",
    "for this section only , we explore an idealized scenario for the purpose of illustration ; the general setting will be treated in section [ general ] . for this entire manuscript , a general background reference for matrix analysis tools that we employ ( eg procrustes fitting",
    ", singular value decomposition , spectral and norm identities and inequalities such as weyl s theorem for hermitian matrices and interlacing inequalities for hermitian matrices ) is the classical text @xcite , background on the grassmannian ( eg principal angles , hausdorff distance ) useful for our particular work is easily accessible in @xcite , and background on principal components analysis ( henceforth  pca \" ) can be found in @xcite .",
    "a classical and broad textbook on the grassmannian is @xcite .",
    "suppose that two scientists each take daily measurements of @xmath0 features of a random process , where @xmath0 is a large , positive integer .",
    "for each day @xmath1 , the first scientist records her daily measurements as @xmath2 , where @xmath3 is her measurement of the @xmath4th feature , and the second scientist records his daily measurements as @xmath5 , where @xmath6 is his measurement of the @xmath4th feature , for @xmath7 .",
    "although the two scientists want to record the same process , suppose that their measurements are made with some error , which we model in the following manner .",
    "there are three collections of random variables @xmath8 , @xmath9 , and @xmath10 , each over indices @xmath1 and @xmath7 , such that these random variables are all collectively independent and identically distributed , and their common distribution has finite variance @xmath11 .",
    "suppose that the random variables @xmath8 are the signal feature values associated with the process that the scientists would like to record , and the random variables @xmath9 and @xmath10 are confounding noise .",
    "let a real - valued  measurement - accuracy \" parameter @xmath12 be fixed in the interval @xmath13 $ ] .",
    "one scenario is that for each day @xmath1 and feature @xmath7 , the first scientist s measurement @xmath14 is a mixture of @xmath15 and @xmath16 with respective probabilities @xmath12 and @xmath17 , and the second scientist s measurement @xmath18 is a mixture of @xmath15 and @xmath19 with respective probabilities @xmath12 and @xmath17 .",
    "a second scenario is that , instead , for each day @xmath1 and feature @xmath7 ,  @xmath20 and @xmath21 .",
    "the main result of section  [ caut ] is theorem [ main ] , which will hold in either of these two scenarios . at one extreme ,",
    "if @xmath22 , then the two scientists measurements are perfectly accurate and @xmath23 for all @xmath24 . at the other extreme ,",
    "if @xmath25 , then the two scientists measurements are independent of each other .    for each positive integer @xmath26 , denote by @xmath27 the matrix @xmath28 \\in { \\mathbb{r}}^{m \\times n}$ ] consisting of the first scientist s measurements over the first @xmath26 days , and denote by @xmath29 the matrix @xmath30 \\in { \\mathbb{r}}^{m \\times n}$ ] consisting of the second scientist s measurements over the  first  @xmath26  days .    because the measurement vectors are in high - dimensional space  @xmath31 , suppose the scientists project their respective measurement vectors to the lower - dimensional space @xmath32 for some smaller , positive integer  @xmath33 .",
    "this is done in the following manner .",
    "let @xmath34 denote the centering matrix ( @xmath35 and @xmath36 are , respectively , the @xmath37 identity matrix and the matrix of all ones ) .",
    "suppose that the first scientist chooses a sequence @xmath38 of random ( or deterministic ) elements of the grassmannian @xmath39 ( the space of all @xmath33-dimensional subspaces of @xmath31 ) , and suppose that the second scientist chooses a sequence @xmath40 of random ( or deterministic ) elements of the grassmannian @xmath39 .",
    "no assumptions are made on the distributions of these elements of the grassmannian or on their dependence / independence , but one example of interest is where , for @xmath41 , @xmath42 denote the respective @xmath33-dimensional subspaces to which principal components analysis ( pca ) projects @xmath43 and @xmath44 , respectively .",
    "let @xmath45 denote the projection operator from @xmath31 onto @xmath46 . on each day @xmath26 ,",
    "the first scientist reports the scaled matrix @xmath47 to the governing board of scientists , and the second scientist reports the scaled matrix @xmath48 to the governing board of scientists .",
    "( nota bene : the specific choice of @xmath49 in the scaling @xmath50 is an innocuous notational convenience . )",
    "now , the governing board of scientists wants to perform its own check that the two scientists are indeed taking measurements reflecting the same process .",
    "so the governing board of scientists computes the _ procrustean fitting - error _ @xmath51 .",
    "it will later be seen ( from ( [ scn ] ) ) that the square procrustean fitting - error satisfies @xmath52 ; the governing board of scientists reasons that this square procrustean fitting - error should be small ( negligible compared to @xmath53 ) if indeed @xmath12 is close to @xmath54 .",
    "is this reasoning valid ?    in the following , @xmath55 denotes the hausdorff distance on the grassmannian @xmath39 ; in particular , for any @xmath56 , @xmath57 where @xmath58 are the principal angles between @xmath59 and @xmath60 . note that the square hausdorff distance satisfies @xmath61 .",
    "[ main ] almost surely , @xmath62 \\rightarrow 0 $ ] as @xmath63 .",
    "the proof of theorem [ main ] is given later , in section [ secmain ] , as a special case of the more general theorem [ realmain ] .",
    "theorem [ main ] says that @xmath64 asymptotically becomes this convex combination ( via @xmath65 ) of @xmath53 and @xmath66 .",
    "in particular , if @xmath12 is close to @xmath67 , which implies that the two scientists measurements are independent of each other , then indeed @xmath64 is close to its maximum possible value @xmath53 , but if @xmath12 is close to @xmath54 , meaning that the scientists measurements are close to being the same as each other , we then have @xmath64 close to @xmath66 . is",
    "this square hausdorff distance close to zero when @xmath12 is close to @xmath54 ?    in section [ sims ] we show that , in fact , if the principal components analysis projections are used then this may not be the case , and the square hausdorff distance @xmath66 might even be close to its maximum possible value of @xmath53 .",
    "by contrast , if the two scientists both used the simple - minded projection consisting of just taking the first @xmath33 coordinates of @xmath31 and ignoring the rest of the coordinates , then @xmath68 , in which case @xmath12 close to @xmath54 would indeed yield @xmath64 close to @xmath67 .",
    "the main result of this section is the statement and proof of theorem [ realmain ] .",
    "we begin with a description of a general setting and a list of basic facts that will be used in the proof of theorem  [ realmain ] .      from this point and on , we will consider a much more general setting than the idealistic setting of section  [ caut ] .",
    "suppose now that @xmath69 and @xmath70 are random vectors ( for convenience , let us denote @xmath71 ) such that the stacked random vectors @xmath72 , \\bigl [ \\begin{smallmatrix } { \\bf x}^{(2 ) } \\\\ { \\bf y}^{(2 ) } \\end{smallmatrix } \\bigr ] , \\bigl [ \\begin{smallmatrix } { \\bf x}^{(3 ) } \\\\ { \\bf y}^{(3 ) } \\end{smallmatrix } \\bigr ] , \\ldots   \\in { \\mathbb{r}}^{2m}$ ] are independent , identically distributed , with covariance matrix @xmath73 = \\left [ \\begin{array}{cc } \\textup{cov}({\\bf x } ) &    \\textup{cov}({\\bf x},{\\bf y } )   \\\\",
    "\\textup{cov}({\\bf y},{\\bf x } ) & \\textup{cov}({\\bf y } ) \\end{array } \\right ]    \\in { \\mathbb{r}}^{2 m \\times 2m}.\\ ] ] ( we no longer require , in the manner of section [ caut ] , that @xmath74 and @xmath75 have independent , nor identically distributed components , nor that they arise as a mixture of other random variables in any particular way . ) assume that cov@xmath76 and cov@xmath77 are both nonzero matrices .",
    "then define , for each positive integer @xmath26 , random matrices @xmath78 \\in { \\mathbb{r}}^{m \\times n}$ ] and @xmath79 \\in { \\mathbb{r}}^{m \\times n}$ ] .",
    "let @xmath42 denote the respective @xmath33-dimensional subspaces to which principal components analysis ( pca ) projects @xmath43 and @xmath44 .",
    "in the special case where cov@xmath76 and cov(@xmath80 are scalar multiples of @xmath81 , then we will explicitly allow @xmath82 to be any sequences of elements in @xmath39 whatsoever , deterministic  or  random .",
    "it is useful to consider the projections @xmath45 and @xmath83 as @xmath84 symmetric , idempotent matrices ( i.e. ,  keep the ambient coordinate system @xmath31 for the projection s range ) and , for each @xmath85 , define @xmath86 and @xmath87 .",
    "( there is no difference for our results and for the procrustean fitting - error if , as in section  [ caut ] , we instead treated @xmath45 and @xmath83 as functions @xmath88 with the coordinate systems of @xmath46 and @xmath89 , respectively , in which case we have @xmath90 and @xmath91 in @xmath92 instead of @xmath93 . )    for any matrix @xmath94 with only real - valued eigenvalues ( eg , symmetric matrices ) , let @xmath95 denote the eigenvalues of @xmath96 . for any matrix @xmath97 ,",
    "let @xmath98 denote the singular values of @xmath96 .",
    "recall that if @xmath96 is symmetric and positive semidefinite ( e.g. , a covariance matrix ) then @xmath99 for all @xmath100 , and recall that , for any @xmath101 , the nonzero eigenvalues of @xmath102 are the same as the nonzero eigenvalues of @xmath103 . for any @xmath56 ( with projection matrices @xmath104 ) and all @xmath100 , we thus have @xmath105 .",
    "in fact , @xmath106 has at most @xmath33 positive eigenvalues and at most @xmath33 positive singular values ( the rest of the eigenvalues and the rest of the singular values are all zero ) and , for all @xmath107 , @xmath108 where @xmath58 are the principal angles between @xmath59 and @xmath60 .    for each @xmath85 ,",
    "the _ hausdorff distance _",
    "@xmath109 is the nonnegative square root of @xmath110 it is clear that @xmath111 .",
    "we also define , for each @xmath112 , the quantity @xmath113 later , in proposition [ fbnd ] , we will prove it always holds that @xmath114 .",
    "note that if cov@xmath115 is a nonzero scalar multiple of @xmath81 then @xmath116 is equal to @xmath117 and , in fact , if cov@xmath115 is the zero matrix then we will define @xmath118 ( because , indeed , @xmath119 is not defined ) .",
    "for this reason , we like to view @xmath116 as a weighted form of the square hausdorff distance .    for each @xmath120 the procrustean fitting - error",
    "is defined to be @xmath121 in fact , it holds that @xmath122      within the setting of section [ setting ] , we now state and prove the main result of section [ general ] :    [ realmain ] in the setting of section [ setting ] , it holds almost surely that @xmath123 \\rightarrow 0\\ ] ] as @xmath63 , where @xmath124 is defined as @xmath125    in proposition [ sbnd ] we prove that @xmath126 . to prove theorem [ realmain ] we first establish lemmas  [ tech2 ]  and  [ next ] :    [ tech2 ] almost surely , @xmath127 as @xmath63 .",
    "* proof of lemma [ tech2 ] : * for each @xmath41 , let us consider a singular value decomposition @xmath128 where @xmath129 is orthogonal , @xmath130 is a  diagonal \" matrix , with nonnegative diagonals non - increasing along its main diagonal , and @xmath131 is orthogonal . by the definition of pca , @xmath132 where @xmath133 is the diagonal matrix with its first @xmath33 diagonals @xmath54 and its remaining diagonals  @xmath67 .",
    "thus , the matrix @xmath134 and the matrix @xmath135 share their @xmath33 largest eigenvalues , and the remaining @xmath136 eigenvalues of the latter matrix are  @xmath67 . by the strong law of large numbers , almost surely @xmath137 ,",
    "hence we have @xmath138 as @xmath63 .",
    "lastly , if cov@xmath139 for some @xmath140 then recall that we explicitly allow @xmath141 to be any elements of @xmath39 ; in this case note that by the boundedness of @xmath142 and the strong law of large numbers that , as @xmath63 , @xmath143 as desired .",
    "@xmath144    [ next ] for @xmath100 , almost surely @xmath145 as @xmath63 , where @xmath146 .",
    "* proof of lemma [ next ] : * for each @xmath85 , expand the expression @xmath147 by the definitions to write it as @xmath148 where @xmath149 and @xmath150 are defined by @xmath151 and @xmath152 define @xmath153 ; by the strong law of large numbers , almost surely @xmath154 as @xmath63 .",
    "thus , by the subadditivity and submultiplicativity of the norm , and by the boundedness of @xmath155 and @xmath156 , we have almost surely that @xmath157 as @xmath63 .",
    "now , by lemma [ tech2 ] and the definition of @xmath149 , almost surely @xmath158 as @xmath63 , hence by ( [ biggs ] ) and the boundedness of @xmath155 and @xmath156 , we have almost surely that @xmath159 as @xmath160 .",
    "thus , by weyl s theorem for hermitian matrices , for each @xmath100 we have almost surely that @xmath161 - \\lambda_i \\big [   \\delta \\cdot \\big ( p_{{{\\mathcal a}}^{(n ) } } \\textup{cov}({\\bf x},{\\bf y } )   p_{{{\\mathcal b}}^{(n)}}^t \\big)^t p_{{{\\mathcal a}}^{(n ) } } \\textup{cov}({\\bf x},{\\bf y } )   p_{{{\\mathcal b}}^{(n)}}^t   \\big ] \\big| \\rightarrow 0\\end{aligned}\\ ] ] as @xmath160 , from which lemma [ next ] follows , after noting that @xmath83 is symmetric .",
    "@xmath144 + we are now able to prove the main result of this section , theorem [ realmain ] . + * proof of theorem [ realmain ] : * let @xmath162 be as defined in lemma [ next ] . note that for any nonnegative , bounded real sequences @xmath163 and @xmath164 , it holds and @xmath165 are bounded , and since @xmath166 implies @xmath167 .",
    "( without the boundedness assumption this implication may not hold . )",
    "conversely , if @xmath168 , then there exists @xmath169 such that @xmath170 for a subsequence , in which case @xmath171 . ]",
    "that @xmath172 if and only if @xmath173 , as @xmath174 .",
    "thus , by lemma [ next ] , and noting that the rank of @xmath175 is at most @xmath33 , we have almost surely that , as @xmath63 , @xmath176 but the expression in ( [ point ] ) can be simplified , by ( [ scn ] ) and ( [ ethest ] ) , as @xmath177    \\\\ & = &   \\epsilon^2 ( { \\mathcal x}^{(n)},{\\mathcal y}^{(n ) }   ) - \\big [ 2k- 2 \\rho \\sum_{i=1}^k \\left ( \\frac{1}{\\frac{1}{k}\\sum_{j=1}^k \\sigma_j \\left ( \\textup{cov}({\\bf x},{\\bf y } ) \\right ) }   \\sigma_i \\big ( p_{{{\\mathcal a}}^{(n ) } } \\textup{cov}({\\bf x},{\\bf y})p_{{{\\mathcal b}}^{(n ) } } \\big )   \\right ) \\big ]    \\\\ & = & \\epsilon^2 ( { \\mathcal x}^{(n)},{\\mathcal y}^{(n ) }   ) - \\big [ ( 1-\\rho ) \\cdot 2k + \\rho \\cdot \\sum_{i=1}^k 2 \\left ( 1-\\frac{1}{\\frac{1}{k}\\sum_{j=1}^k \\sigma_j \\left ( \\textup{cov}({\\bf x},{\\bf y } ) \\right ) }   \\sigma_i \\big ( p_{{{\\mathcal a}}^{(n ) } } \\textup{cov}({\\bf x},{\\bf y})p_{{{\\mathcal b}}^{(n ) } } \\big )   \\right ) \\big]\\\\ & = & \\epsilon^2 ( { \\mathcal x}^{(n)},{\\mathcal y}^{(n ) }   ) - \\big [ ( 1-\\rho ) \\cdot 2k + \\rho \\cdot \\eth^2 ( { { \\mathcal a}}^{(n ) } , { { \\mathcal b}}^{(n ) } ) \\big],\\end{aligned}\\ ] ] which establishes theorem [ realmain ] .",
    "@xmath144 + there is a special case of theorem [ realmain ] that deserves attention :    [ special ] in the setting of section [ setting ] , if @xmath178 and @xmath179 for a real number @xmath180 , then it holds almost surely that @xmath181 \\rightarrow 0\\ ] ] as @xmath63 , where @xmath182 .",
    "theorem [ special ] is an immediate consequence of theorem [ realmain ] , since we previously pointed out that when @xmath183 is a scalar multiple of the identity then @xmath184 .",
    "@xmath144 + finally , theorem [ main ] from section [ caut ] is an immediate consequence of theorem [ special ] , after noting that the setting of section [ caut ] is a special case of the setting of section [ setting ] , with ( recall the definitions of @xmath185 and @xmath12 from section [ caut ] ) @xmath73 = \\left [ \\begin{array}{rr } \\alpha \\cdot i_m \\ \\ \\ &    \\gamma^2 \\cdot \\alpha \\cdot i_m   \\\\",
    "\\gamma^2 \\cdot \\alpha \\cdot i_m   \\ \\ \\   & \\alpha \\cdot i_m \\end{array } \\right ]    \\in { \\mathbb{r}}^{2 m \\times 2m}.\\ ] ] so @xmath186 and @xmath187 of theorem [ special ] are , respectively , @xmath188 and @xmath185 , thus in theorem [ special ] we have @xmath189 .",
    "this proves theorem [ main ] .",
    "@xmath144      [ fbnd ] for @xmath116 as defined in ( [ ethest ] ) , it holds that @xmath191 .",
    "* proof of proposition [ fbnd ] : * the upper bound is trivial . to prove the lower bound , first we re - express ( [ ethest ] ) as @xmath192 and",
    "we show that each summand in the summation of ( [ boon ] ) will be nonnegative . indeed , for any @xmath193 and @xmath194",
    ", we have that @xmath195 and @xmath196 ; this is seen as follows .",
    "say @xmath197 is such that @xmath198 is orthogonal and @xmath199 is diagonal with @xmath54 s and @xmath67 s on its diagonal .",
    "then @xmath200 , the inequality holding by the interlacing theorem for hermitian matrices . by a similar argument @xmath201 , and applying these in succession yields that @xmath202 .",
    "@xmath144    for @xmath124 , as defined in theorem [ realmain ] , it holds that @xmath126 .",
    "[ sbnd ]    * proof of proposition [ sbnd ] : * let @xmath203 be a singular value decomposition ; i.e. @xmath204 are orthogonal and @xmath205 is diagonal , with nonincreasing nonnegative diagonal entries .",
    "define @xmath206 by @xmath207 \\left [ \\begin{array}{cc } \\textup{cov}({\\bf x } ) &    \\textup{cov}({\\bf x},{\\bf y } )   \\\\",
    "\\textup{cov}^t({\\bf x},{\\bf y } ) & \\textup{cov}({\\bf y } ) \\end{array } \\right ] \\left [ \\begin{array}{cc } u^t &   0_m   \\\\ 0_m & v^t \\end{array } \\right ] = \\left [ \\begin{array}{cc } u   \\textup{cov}({\\bf x } ) u^t &   \\lambda   \\\\ \\lambda & v \\textup{cov}({\\bf y } ) v^t \\end{array } \\right ] \\ ] ] where @xmath208 is the matrix of zeros .",
    "a covariance matrix is positive semidefinite , thus @xmath209 is positive semidefinite , as well as all of its principal submatrices . for each @xmath210 , the two - by - two submatrix consisting of the @xmath4th and @xmath211th rows and columns of @xmath209 has nonnegative diagonals and a nonnegative determinant , thus @xmath212 , i.e. @xmath213 now , summing ( [ lat ] ) over @xmath210 and applying the cauchy - schwartz inequality to the resulting right - hand side , we obtain @xmath214 for any hermitian matrix , the vector of its diagonals always majorizes the vector of its eigenvalues , thus @xmath215 and proposition [ sbnd ] follows from ( [ lata ] ) , ( [ latb ] ) , and ( [ latb ] ) applied to @xmath216 and @xmath217 . @xmath144      suppose that @xmath218 is an orthogonal matrix such that @xmath219 = \\left [ \\begin{array}{cc } \\textup{cov}({\\bf x } ) &   \\beta \\cdot i_m   \\\\",
    "\\beta \\cdot i_m & \\textup{cov}({\\bf x } ) \\end{array } \\right ]    \\in { \\mathbb{r}}^{2 m \\times 2m},\\ ] ] where @xmath220 is nonzero ; this might arise in situations similar to the cautionary tale of two scientists in section [ caut]wherein two scientists are taking measurements of the same random process  except that the second scientist permutes the order of the features ( i.e. , @xmath221 is a permutation matrix ) .",
    "define @xmath222 .",
    "in this situation , the quantity @xmath223 may be more interesting  than  the  quantity @xmath66 , since @xmath46 might be viewed as being more comparable to @xmath224 then to @xmath89 .",
    "indeed , if the eigenvalues of cov@xmath76 are distinct and @xmath26 is large and @xmath221 is not @xmath81 , then @xmath223 would be small , in contrast to @xmath66 .    [",
    "corrective ] in the case of the previous paragraph , we have @xmath225 .",
    "proposition [ corrective ] will be illustrated in section [ illu ] .",
    "+ * proof of proposition [ corrective ] : * here we have @xmath226 = \\left [ \\begin{array}{cc } i_m & 0_m \\\\ 0_m & w^t   \\end{array } \\right ] \\left [ \\begin{array}{cc } \\textup{cov}({\\bf x } ) &   \\beta \\cdot i_m   \\\\ \\beta \\cdot i_m & \\textup{cov}({\\bf x } ) \\end{array } \\right ] \\left [ \\begin{array}{cc } i_m & 0_m \\\\ 0_m & w   \\end{array } \\right ] = \\left [ \\begin{array}{cc } \\textup{cov}({\\bf x } ) &   \\beta \\cdot w   \\\\ \\beta \\cdot w^t & \\textup{cov}({\\bf y } ) \\end{array } \\right ]   , \\ ] ] thus for all @xmath100 @xmath227 because @xmath228 , and by ( [ thd ] ) , ( [ ethest ] ) , ( [ ft ] ) , and ( [ fs ] ) it follows that + @xmath229 . @xmath144",
    "in this section we use simulations to illustrate and support the theorems which we stated and proved in the previous sections , and we then use these simulations to illustrate how the  incommensurability phenomenon \" can arise as a consequence . what is meant by this phenomenon is the occurrence of an inordinately large procrustean fitting - error between projected data that was originally highly - correlated .",
    "( this phenomenon was named in priebe et al @xcite . )      our first illustration of theorem [ realmain ] and theorem [ special ] is with @xmath74 and @xmath75 distributed multivariate normal ( with mean vector consisting of all zeros ) such that cov@xmath230cov@xmath231 and cov@xmath232 for assorted values of @xmath180 .",
    "note that @xmath124 as defined in theorem [ realmain ] is @xmath180 here , note that @xmath187 and @xmath180 as defined in theorem [ special ] are , respectively , @xmath54 and @xmath180 here , and note that here @xmath233 because cov@xmath115 is a scalar multiple of the identity .",
    "also , this example may be seen as an illustration of theorem  [ main]in the tale of two scientists  with @xmath65 there being @xmath180 here .      for each of @xmath236 , and for each of @xmath237 and @xmath238 we obtained @xmath239 realizations of @xmath90 and @xmath91 and used pca to obtain @xmath46 and @xmath89 . in figure",
    "[ fig1 ] , we plotted the values of @xmath64 against the respective values of @xmath240 , in colors blue , green , red , cyan , magenta , blue , green , red , cyan , magenta , blue for the respective values of @xmath236 . for reference , we also included  in figure [ fig1]lines with y - intercept @xmath241 and slope @xmath180 , for each of the above - specified values of @xmath180 ; basically , theorem  [ main ] , theorem  [ realmain ] , and theorem  [ special ] state that the scatter plots will adhere to these respective lines in the limit as @xmath26 goes to @xmath242 .",
    "indeed , notice in figure [ fig1 ] that the scatter plots adhere very closely to their respective lines , and such adherence substantially improves as @xmath237 is raised to @xmath243 , which supports / illustrates the claims of theorem [ main ] , theorem [ realmain ] , and theorem  [ special ] .",
    "the above was done using pca to generate @xmath46 and @xmath89 .",
    "what if we instead took @xmath46 and @xmath89 to ( each ) be the span of the first two standard - basis vectors in @xmath244 ? we will call this the  trivial \" choice of @xmath46 and @xmath89 . of course",
    ", the value of @xmath240 would always be identically zero , and note that theorem [ main ] , theorem [ realmain ] , and theorem  [ special ] still apply with this choice of @xmath46 and @xmath89 because cov@xmath76 and cov@xmath77 are scalar multiples of the identity .",
    "thus , the scatter plots from these above experiments when they are performed instead for the trivial choice of @xmath46 and @xmath89 would land in the far left of figure [ fig1 ] ( along the y - axis at @xmath245 ) , clustered about their respective lines .",
    "indeed , we then performed the above experiments for the trivial choice of @xmath46 and @xmath89 ; the sample mean and sample standard deviation of @xmath64 for the @xmath239 monte carlo replicates when @xmath243 were as follows : @xmath246 indeed , besides the notable exception when @xmath247 ( where there is no correlation anyway between @xmath74 and @xmath75 ) , the values of @xmath64 were substantially larger when pca was used to generate @xmath46 and @xmath89 than for the trivial choice of @xmath46 and @xmath89 .",
    "this is the incommensurability phenomenon , a situation where use of pca has the consequence of inordinately large procrustean fitting - error .",
    "let us call the values @xmath248 $ ] _ residuals_. it is noteworthy that in the above experiments the sample standard deviation of the residuals when pca was used to generate @xmath46 and @xmath89 is very close to the sample standard deviation of @xmath64 for the trivial choice of @xmath46 and @xmath89 .",
    "specifically , we computed : @xmath249",
    "so , it seems empirically here that the variation in @xmath64 not explained by @xmath66 when pca generates @xmath46 and @xmath89 is approximately the same as the variation in @xmath64 for the trivial choice of @xmath46 and @xmath89 ( in which @xmath68 identically ) and , as such , @xmath66 explains all of the rest of the variation here in @xmath64 when pca is used .",
    "our next illustration of theorem [ realmain ] and theorem [ special ] is with @xmath74 and @xmath75 multivariate normal ( with mean vector of all zeros ) such that cov@xmath230cov@xmath250 the diagonal matrix in @xmath251 with @xmath252 on all diagonals except for the first diagonal , which has the value @xmath54 , and such that cov@xmath253 .",
    "so we are using @xmath254 here .",
    "as above , @xmath233 because cov@xmath115 is a scalar multiple of the identity .",
    "we will use three different projection dimensions , each of @xmath255 .",
    "when @xmath256 the formula in theorem [ realmain ] yields @xmath257 , when @xmath235 the formula yields @xmath258 , and when @xmath259 the formula yields @xmath260 .",
    "using pca to generate @xmath46 and @xmath89 , we obtained @xmath261 realizations of @xmath90 and @xmath91 when @xmath243 , for each projection dimension @xmath256 , @xmath235 , and @xmath259 ; the values of @xmath64 are plotted against the respective values of @xmath240 in the left figure of figure [ fig2 ] , with @xmath256 in blue , @xmath235 in red , and @xmath262 in green .",
    "as before , lines are drawn on the figure to indicate the limiting relationship between @xmath64 and @xmath240 that is predicted by theorem  [ realmain ] and theorem  [ special ] ; indeed , the scatter plots adhere very closely to these respective lines . in the right hand side of figure [ fig2 ]",
    "is @xmath263 monte carlo simulations when @xmath235 for each of @xmath264 ( yellow ) , @xmath265 ( blue ) , @xmath266 ( magenta ) and @xmath267 ( black ) . as @xmath26 is getting larger ,",
    "these are seen to get increasingly closer to the corresponding limiting relationship between @xmath64 and @xmath240 .",
    "all of this supports the claims of theorem [ realmain ] and theorem [ special ] .    in the experiments for",
    "the left figure in figure [ fig2 ] , the sample mean and sample standard deviation of @xmath268 were as follows : @xmath269 ( we normalize @xmath64 with division by @xmath53 since the range of @xmath64 is @xmath270 $ ] . as @xmath33 increases , the correlation @xmath124 increases ,",
    "so it would seem at first thought that the normalized procrustean fitting - error @xmath268 should decrease .",
    "indeed , the leftmost green points in ( the left figure of ) figure [ fig2 ] are below the leftmost red points , which are below the leftmost blue points .",
    "however , overall , the normalized procrustean fitting - error is seen in the table above to be much higher in the case of @xmath235 than the case of @xmath256 .",
    "this is explained by noting a substantial gap between the first eigenvalue of cov@xmath74 and the second eigenvalue of cov@xmath74 ( @xmath54 vs @xmath252 ) whereas there is no gap between the second eigenvalue of of cov@xmath74 and the third eigenvalue of cov@xmath74 ( both are @xmath252 ) .",
    "thus when @xmath256 the pca projection has little variance whereas when @xmath235 the pca projection has much variance , often causing much larger hausdorff distance between @xmath46 and @xmath89 , which results in larger procrustean fitting - error by theorem [ realmain ] .",
    "as such , the case of @xmath235 is an example of the incommensurability phenomenon of inordinately large procrustean fitting - error . but then observe that when @xmath259 we find that the normalized procrustean fitting - error is competitive with the @xmath256 case ; even though the tenth and eleventh eigenvalues of cov@xmath74 are the same , nonetheless the correlation @xmath124 has increased , and the variance of the pca projection has decreased enough to improve the normalized procrustean fitting - error to be competitive with the case of @xmath256 .    not only may the incommensurability phenomenon occur when there is no spectral gap in the covariance structure at the projection dimension , but the incommensurability phenomenon may occur when this spectral gap is positive but small . indeed , repeating the experiments performed for the left figure in figure [ fig2 ] , and just changing the second diagonal of cov@xmath230cov@xmath75 from @xmath252 to @xmath271 for each of @xmath272 but otherwise the experiments are the same , we got a very similar - looking scatter plot as the left figure in figure [ fig2 ] , and the sample mean and sample standard deviation of @xmath268 were as follows : @xmath273 in the case of @xmath235 , the spectral gap in the covariance structure at the projection dimension is @xmath274 , and note that as this gap grows to @xmath275 there is a lessening of the incommensurability phenomenon , but the phenomenon is still very much present . indeed",
    "( from the table above ) , when @xmath276 , the sample mean of @xmath277 when @xmath235 ( see table above ) is below the sample mean when @xmath256 , but it is only lower by less than a half of the sample standard deviation of @xmath268 when @xmath235 and , in fact , notice that the sample standard deviation of @xmath268 when @xmath235 is more than @xmath278 times the sample standard deviation when @xmath256 .",
    "thus there is a significant probability of an inordinately high procrustean fitting error in the case of @xmath235 with @xmath276 .",
    "[ [ a - modification - of - the - second - illustration - to - illustrate - the - isometry - corrective - property - of - eth2-illu ] ] a modification of the second illustration to illustrate the isometry - corrective property of @xmath190 [ illu ] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~      @xmath226 = \\left [ \\begin{array}{cccccccccccc } 1 & 0 & 0 & \\hdots & 0 & 0    & 0 & 0 & \\hdots & 0 & 0 & .6 \\\\ 0 & .7 & 0 & \\hdots & 0 & 0 & 0 & 0 & \\hdots & 0 & .6 & 0\\\\ 0 & 0 & .7 & \\hdots & 0 & 0 & 0 & 0 & \\hdots & .6 & 0 & 0\\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\vdots &   & \\vdots & \\vdots & \\vdots \\\\ 0 & 0 & 0 & \\hdots & .7 & 0 & 0 & .6 & \\hdots & 0 & 0 & 0\\\\ 0 & 0 & 0 & \\hdots & 0 & .7 & .6 & 0 & \\hdots & 0 & 0 & 0",
    "\\\\ 0 & 0 & 0 & \\hdots & 0 & .6 & .7 & 0 & \\hdots & 0 & 0 & 0 \\\\ 0 & 0 & 0 & \\hdots & .6 & 0 & 0 & .7 & \\hdots & 0 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots &   & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots \\\\ 0 & 0 & .6 & \\hdots & 0 & 0 & 0 & 0 & \\hdots & .7 & 0 & 0 \\\\ 0 & .6 & 0 & \\hdots & 0 & 0 & 0 & 0 & \\hdots & 0 & .7 & 0 \\\\ .6 & 0 & 0 & \\hdots & 0 & 0 & 0 & 0 & \\hdots & 0 & 0 & 1 \\end{array }   \\right ]     \\in { \\mathbb{r}}^{40 \\times 40}.\\ ] ]    of course , this is exactly the illustration in the beginning of section [ secillu ] , with the only exception that the coordinates of @xmath279 have been permuted into reverse order .",
    "performing the very same experiments from the beginning of section [ secillu ] , the scatter plots of @xmath64 vs @xmath240 will * not * look like the scatter plots in figure [ fig2 ] .",
    "however , since the permutation transformation is an isometry , we then have by proposition [ corrective ] in section [ corrsect ] , that the scatter plots of @xmath64 vs @xmath280 * will indeed * look like the scatter plots in figure [ fig2 ] .",
    "the use of @xmath190 automatically accounts for isometrical transformations of @xmath74 and/or @xmath75 from a common frame , in the manner of this example .",
    "it should also be mentioned that , for the illustration of this section ( with the covariance matrix above ) , if @xmath46 and @xmath89 were not generated with pca , but instead @xmath46 and @xmath89 were selected to be ( the same as each other by setting them to be ) the span of any number of standard - basis vectors in @xmath281 then the procrustes fitting - error would be disasterously large .",
    "the fact that such a naive choice of @xmath46 and @xmath89 was successful in the illustration in section [ firill ] was just a byproduct of the good fortune that @xmath74 and @xmath75 did not have permuted coordinates or any other isometrical transformation applied to them .",
    "+ * acknowledgements : * the work of all authors was partially supported by national security science and engineering faculty fellowship ( nsseff ) , johns hopkins university human language technology center of excellence ( jhu hlt coe ) , and the xdata program of the defense advanced research projects agency ( darpa ) administered through air force research laboratory contract fa8750 - 12 - 2 - 0303 ."
  ],
  "abstract_text": [
    "<S> suppose that two large , high - dimensional data sets are each noisy measurements of the same underlying random process , and principle components analysis is performed separately on the data sets to reduce their dimensionality . in some circumstances it may happen that the two lower - dimensional data sets have an inordinately large procrustean fitting - error between them . </S>",
    "<S> the purpose of this manuscript is to quantify this  incommensurability phenomenon . \" in particular , under specified conditions , the square procrustean fitting - error of the two normalized lower - dimensional data sets is ( asymptotically ) a convex combination ( via a correlation parameter ) of the hausdorff distance between the projection subspaces and the maximum possible value of the square procrustean fitting - error for normalized data . </S>",
    "<S> we show how this gives rise to the incommensurability phenomenon , and we employ illustrative simulations to explore how the incommensurability phenomenon may have an appreciable  impact . </S>",
    "<S> + _ keywords : _ procrustes fitting , high - dimensional data , principal components analysis , grassmannian , hausdorff distance , incommensurability phenomenon . </S>"
  ]
}