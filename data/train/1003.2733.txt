{
  "article_text": [
    "linear least - squares problems , in the form of statistical regression analyses , are a basic tool of investigation in both the physical and the social sciences , and consequently they are an important computation .",
    "this paper develops a single methodology that determines tight lower and upper estimates of condition numbers for several problems involving linear least - squares .",
    "the condition numbers are with respect to the matrices in the problems and scaled @xmath0-norms .",
    "the problems are : orthogonal projections and least - squares residuals @xcite , minimum @xmath0-norm solutions of underdetermined equations @xcite , and in the present case , the solution of overdetermined equations @xmath1 where @xmath2 is an @xmath3 matrix of full column rank ( hence , @xmath4 ) .",
    "some presentations of error bounds contain formulas that can severely overestimate the condition number , including the siam documentation for the lapack software .",
    "this introduction provides some background material .",
    "section [ sec : definition ] discusses definitions of condition numbers .",
    "section [ sec : brief ] describes the estimate and provides an example ; this material is appropriate for presentation in class .",
    "section [ sec : derivation ] proves that the condition number varies from the estimate within a factor of @xmath5 .",
    "the derivation relies on a formula for the nuclear norm of a matrix .",
    "( this norm is the sum of the singular values including multiplicities , and is also known as the trace norm . )",
    "section [ sec : comparison ] examines overestimates in the literature .",
    "section [ sec : advanced ] evaluates the nuclear norm of rank @xmath0 matrices ( lemma [ lem : rank2 ] ) .",
    "ever since @xcite and @xcite invented the method of least - squares , the problems had been solved by applying various forms of elimination to the normal equations , @xmath6 in equation ( [ eqn : lls ] ) .",
    "instead , @xcite suggested applying householder transformations directly to @xmath2 , which removed the need to calculate @xmath7 .",
    "however , reported that @xmath8 was still `` relevant to some extent '' to the accuracy of the calculation because they found that @xmath8 appears in a bound on perturbations to @xmath9 that are caused by perturbations to @xmath2 .",
    "their discovery was `` something of a shock '' .",
    "the original error bound of was difficult to interpret because of an assumed scaling for the problem .",
    "derived a bound by the augmented matrix approach that was suggested to him by golub .",
    "re - derived the bound from his study of the matrix pseudoinverse and exhibited a perturbation to the matrix that attains the leading term .",
    "van der sluis ( 1975 , p.  251",
    "5.8 ) also derived bjrck s bound and introduced a simplification of the formula and a geometric interpretation of the leading term .",
    "later followed wedin in basing the derivation of his bound on the pseudoinverse .",
    "derived a lower bound for the condition number thereby proving that his formula and the coefficient in bjrck s bound are quantifiably tight estimates of the spectral condition number .",
    "in contrast , condition numbers with respect to frobenius norms have exact formulas that have been given in various forms by @xcite , @xcite , and @xcite .",
    "the oldest way to derive perturbation bounds is by differential calculus .",
    "if @xmath10 is a vector valued function of the vector @xmath9 whose partial derivatives are continuous , then the partial derivatives give the best estimate of the change to @xmath11 for a given change to @xmath9 @xmath12 where @xmath13 is the jacobian matrix of the partial derivatives of @xmath11 with respect to @xmath9 . the magnitude of the error in the first order approximation ( [ eqn : approximation-1 ] ) is bounded by landau s little @xmath14 for all sufficiently small @xmath15 . ) being @xmath14 .",
    "the order of the error terms is independent of the norm because all norms for finite dimensional spaces are equivalent . ]",
    "thus @xmath16 is the unique linear approximation to @xmath17 in the vicinity of @xmath9 .",
    "differs from @xmath17 by @xmath18 and therefore does not provide a @xmath14 approximation . ] taking norms produces a perturbation bound , @xmath19 equation ( [ eqn : calculus-1 ] ) is the smallest possible bound on @xmath20 in terms of @xmath15 provided the norm for the jacobian matrix is induced from the norms for @xmath17 and @xmath21 . in this case for each @xmath9 there is some @xmath21 , which is nonzero but may be chosen arbitrarily small , so the bound ( [ eqn : calculus-1 ] ) is attained to within the higher order term , @xmath22",
    ". there may be many ways to define condition numbers , but because equation ( [ eqn : calculus-1 ] ) is the smallest possible bound , any definition of a condition number for use in bounds equivalent to ( [ eqn : calculus-1 ] ) must arrive at the same value , @xmath23 .",
    "are given by , , , and .",
    "] the matrix norm may be too complicated to have an explicit formula , but tight estimates can be derived as in this paper .",
    "many problems depend on two parameters @xmath24 , @xmath25 which may consist of the entries of a matrix and a vector ( for example ) . in principle",
    "it is possible to treat the parameters altogether . , @xcite derived a joint condition number of the least - squares solution with respect to a frobenius norm of the matrix and vector that define the problem . ]",
    "a condition number for @xmath11 with respect to joint changes in @xmath24 and @xmath25 requires a common norm for perturbations to both .",
    "such a norm is @xmath26 a single condition number then follows that appears in an optimal error bound , @xmath27 the value of the condition number is again @xmath28 where the matrix norm is induced from the norm for @xmath17 and the norm in equation ( [ eqn : joint - norm ] ) .",
    "because @xmath24 and @xmath25 may enter into the problem in much different ways , it is customary to treat each separately .",
    "this approach recognizes that the jacobian matrix is a block matrix @xmath29}\\ ] ] where the functions @xmath30 and @xmath31 have @xmath25 and @xmath24 fixed , respectively .",
    "the first order differential approximation ( [ eqn : approximation-1 ] ) is unchanged but is rewritten with separate terms for @xmath24 and @xmath25 , @xmath32 bound ( [ eqn : single ] ) then can be weakened by applying norm inequalities , @xmath33 the coefficients @xmath34 and @xmath35 are the separate condition numbers of @xmath11 with respect to @xmath24 and @xmath25 , respectively .",
    "these two different approaches lead to error bounds ( [ eqn : single ] , [ eqn : double ] ) that differ by at most a factor of @xmath0 because it can be shown @xcite @xmath36 thus , for the purpose of deriving tight estimates of joint condition numbers , it suffices to consider @xmath37 and @xmath38 separately .",
    "the linear least - squares problem ( [ eqn : lls ] ) does not have an unique solution when @xmath2 does not have full column rank .",
    "a specific @xmath9 can be chosen such as the one of minimum norm .",
    "however , small changes to @xmath2 can still produce large changes to @xmath9 .",
    "does not have full column rank , then for every nonzero vector @xmath39 in the right null space of the matrix , @xmath40 .",
    "thus , a change to the matrix of norm @xmath41 changes the solution from @xmath42 to @xmath43 . ] in other words , a condition number of @xmath9 with respect to rank deficient @xmath2 does not exist or is `` infinite . ''",
    "perturbation bounds in the rank deficient case can be found by restricting changes to the matrix , for which see and .",
    "that theory is beyond the scope of the present discussion .",
    "this section summarizes the results and presents an example .",
    "proofs are in section [ sec : derivation ] . it is assumed that @xmath2 has full column rank and the solution @xmath9 of the least - squares problem ( [ eqn : lls ] ) is not zero .",
    "the solution is proved to have a condition number @xmath44 with respect to @xmath2 within the limits , @xmath45 ^ 2 + 1 } $ } \\ ; \\le \\ ; \\chi_x ( a ) \\ ; \\le \\ ; \\fbox { $ { { \\boldsymbol \\kappa^{}_{\\boldsymbol 2}}}[{\\mbox { \\bfseries \\scshape",
    "v}}\\tan ( { { \\boldsymbol \\theta } } ) + 1]$ } \\ , , \\",
    "] ] where @xmath46 , @xmath47 , and @xmath48 are defined below ; they are bold to emphasize they are the values in the tight estimates of the condition number .",
    "there is also condition number with respect to @xmath49 , @xmath50 these condition numbers @xmath44 and @xmath51 are for measuring the perturbations to @xmath2 , @xmath49 , and @xmath9 by the following scaled @xmath0-norms , @xmath52 like equation ( [ eqn : double ] ) , the two condition numbers appear in error bounds of the form , and @xmath53 could be discarded from the @xmath54 terms because only the order of magnitude of the terms is pertinent . ]",
    "@xmath55 where @xmath56 is the solution of the perturbed problem , @xmath57    the quantities @xmath46 , @xmath47 , and @xmath48 in the formulas ( [ eqn : chixa ] , [ eqn : chixb ] ) are @xmath58 where @xmath46 is the spectral matrix condition number of @xmath2 ( @xmath59 is the smallest singular value of @xmath2 ) , @xmath47 is van der sluis s ratio between @xmath60 and @xmath46 , , which is the present @xmath47 . ]",
    "@xmath48 is the angle between @xmath49 and @xmath61 , is the column space of @xmath2 . ] and @xmath62 is the least - squares residual .    1 .",
    "@xmath46 depends only on the extreme singular values of @xmath2 .",
    "@xmath48 depends only on the `` angle of attack '' of @xmath49 with respect to @xmath61 .",
    "3 .   if @xmath2 is fixed , then @xmath47 depends on the orientation of @xmath49 to @xmath61 but not on @xmath48 .",
    "has full column rank , @xmath63 and @xmath9 can only vary proportionally when their directions are fixed . ]",
    "please refer to figure [ fig : schematic ] as needed .",
    "if @xmath61 is fixed , then @xmath46 and @xmath47 depend only on the singular values of @xmath2 , and @xmath48 depends only on the orientation of @xmath49 .",
    "thus , @xmath46 and @xmath48 are separate sources of ill - conditioning for the solution . if @xmath63 has comparatively large components in singular vectors corresponding to the largest singular values of @xmath2 , then @xmath64 and the condition number @xmath65 depends on @xmath66 which was the discovery of @xcite .",
    "otherwise , @xmath66 `` plays no role '' .    , and the angle @xmath48 between @xmath63 and @xmath49 . ]",
    "this example illustrates the independent effects of @xmath46 , @xmath47 , and @xmath48 on @xmath65 .",
    "it is based on the example of .",
    "let @xmath67 , \\quad   b = \\left [ \\begin { array } { c } \\beta \\cos ( \\phi)\\\\ \\beta \\sin ( \\phi ) \\\\ 1 \\end { array } \\right ] \\",
    ", , \\quad   { \\delta a}= \\left [ \\begin { array } { c c } 0 & 0\\\\ 0 & 0\\\\ 0 & \\epsilon \\end { array } \\right ] .\\ ] ] where @xmath68 . in this example , @xmath69 , \\quad   r = \\left [ \\begin { array } { c } 0\\\\ 0\\\\ 1 \\end { array } \\right ] , \\quad   x + { \\delta x}= \\left [ \\begin { array } { c } \\beta \\cos ( \\phi ) \\\\[0.1ex ] { \\epsilon + \\alpha \\beta \\sin ( \\phi ) \\vphantom { ( } \\over \\vphantom { ( } \\alpha^2 + \\epsilon^2 } \\\\",
    "\\end { array } \\right ] \\ , .\\ ] ] the three terms in the condition number are @xmath70 ^ 2 + [ \\sin ( \\phi)]^2 } } \\ , , \\qquad \\tan ( { { \\boldsymbol \\theta } } ) = { 1 \\over \\beta } \\ , .\\ ] ] these values can be independently manipulated by choosing @xmath71 , @xmath72 , and @xmath73 .",
    "the tight upper bound for the condition number is @xmath74 ^ 2 + [ \\sin ( \\phi)]^2 } } + 1 \\right ) \\ , .\\ ] ] the relative change to the solution of the example @xmath75 ^ 2 + [ \\sin ( \\phi)]^2 } } + { \\mathcal o } ( \\epsilon^2 ) \\ , .\\ ] ] is close to the bound given by the condition number estimate and the relative change to @xmath2 .",
    "the formula for the jacobian matrix @xmath76 of the solution @xmath77 with respect to @xmath49 is clear .",
    "would introducing a name , @xmath78 , for the function by which @xmath9 varies with @xmath49 when @xmath2 is held fixed , @xmath79 , so that the notation for the jacobian matrix is then @xmath80 .",
    "this pedantry will be discarded here to write @xmath81 with @xmath2 held fixed ; and similarly for @xmath82 with @xmath49 held fixed . ] for derivatives with respect to the entries of @xmath2 , it is necessary to use the `` @xmath83 '' construction to order the matrix entries into a column vector ; @xmath84 is the column of entries @xmath85 with @xmath86 in co - lexicographic order .",
    "the approximation is then @xmath87}\\ , { \\mbox { vec}}({\\delta a } ) + { j_x(b)}\\ , { \\delta b}+ \\mbox { higher order terms}\\ ] ] and upon taking norms @xmath88}\\|}_{\\displaystyle \\chi_x ( a ) } \\ , \\| { \\delta a}\\| + \\underbrace { \\displaystyle \\| { j_x(b)}\\|}_{\\displaystyle \\chi_x ( b ) } \\ , \\| { \\delta b}\\| + o ( \\max \\ { \\| { \\delta a}\\| , \\ , \\| { \\delta b}\\| \\ } ) \\ , , \\ ] ] where it is understood the norms on the two jacobian matrices are induced from the following norms for @xmath89 , @xmath90 , and @xmath21",
    ".      equation ( [ eqn : differential - bound ] ) applies for any choice of norms . in theoretical numerical analysis especially for least - squares problems",
    "the spectral norm is preferred . for @xmath0-norms",
    "the matrix condition number of @xmath7 is the square of the matrix condition number of @xmath2 .",
    "the norms used in this paper are thus , @xmath91 where @xmath92 , @xmath93 , @xmath94 are constant scale factors .",
    "these formulas define norms for @xmath3 matrices , for @xmath95 vectors , and for @xmath96 vectors .",
    "the scaling makes the size of the changes relative to the problem of interest .",
    "the scaling used in equations ( [ eqn : chixa][eqn : specific - scaled - norms ] ) is @xmath97      from @xmath98 follows @xmath99 and then for the scaling of equation ( [ eqn : scale - factors ] ) @xmath100      the jacobian matrix @xmath101}$ ] is most easily calculated from the total differential of the identity @xmath102 with respect to @xmath2 and @xmath9 , which is @xmath103 \\ , { \\mbox { vec}}(da ) + j_f ( x ) \\ , dx = 0 $ ] . hence @xmath104^{-1 } j_f [ { \\mbox { vec}}(a)]}_{\\displaystyle j_x[{\\mbox { vec}}(a ) ] } { \\mbox { vec}}(da)\\ ] ] where @xmath105 and where @xmath106   = \\left [   \\setlength { \\arraycolsep } { 0.25em } \\begin { array } { c c c } r^t\\\\ & \\ddots\\\\ & & \\hspace { 0.5em } r^t \\end { array } \\right ] -   \\left [   \\setlength { \\arraycolsep } { 0.33em } \\begin { array } { c c c c c } x_1 a^t & \\cdots & x_n a^t \\end { array } \\right ] , \\ ] ] in which @xmath107 is the least - squares residual , and @xmath108 is the @xmath109-th entry of @xmath9 .",
    "the desired condition number is the norm induced from the norms in equation ( [ eqn : norms ] ) .",
    "@xmath110}\\| & = & \\displaystyle \\max_{{\\delta a } } { \\| { j_x [ { \\mbox { vec}}(a)]}\\ , { \\mbox { vec}}({\\delta a } ) \\|_{\\mathcal { x}}\\over \\| { \\mbox { vec}}({\\delta a } ) \\|_{\\mathcal { a } } } \\\\ \\noalign { \\bigskip } & = & \\displaystyle { { \\mathcal { a}}\\over { \\mathcal { x } } } \\max_{{\\delta a } } { \\| { j_x [ { \\mbox { vec}}(a)]}\\ , { \\mbox { vec}}({\\delta a } ) \\|_2 \\over \\|",
    "{ \\delta a}\\|_2 }   \\end { array}\\ ] ] the numerator and denominator are vector and matrix @xmath0-norms , respectively . if @xmath2 is an @xmath3 matrix , then the maximization in equation ( [ eqn : induced ] ) has many degrees of freedom .",
    "an identity for operator norms can be applied to avoid this large optimization problem .",
    "suppose @xmath111 and @xmath112 have the norms @xmath113 and @xmath114 , respectively .",
    "if a problem with data @xmath115 has a solution function @xmath116 , then the condition number is the induced norm of the @xmath117 jacobian matrix , @xmath118 this optimization problem has @xmath119 degrees of freedom .",
    "an alternate expression is the norm for the transposed operator represented by the transposed matrix , ) is stated by , , , and .",
    "the name of the transposed operator varies .",
    "see for `` transpose , '' dunford and schwartz ( loc .",
    "cit . )  and rudin ( loc .",
    "cit . )  for `` adjoint , '' and for `` conjugate '' or `` dual . ''",
    "some parts of mathematics use `` adjoint '' in the restricted context of hilbert spaces , for example in linear algebra see .",
    "that concept is actually a `` slightly different notion '' from the banach space transpose used here . ]",
    "\\|^ * = \\max_{{\\delta s } } { \\| [ j_f ( d)]^t { \\delta s}\\|_n^ * \\over \\| { \\delta s}\\|_m^ * } \\ , .\\ ] ] the norm is induced from the dual norms @xmath121 and @xmath122 which must be determined .",
    "this optimization problem has @xmath123 degrees of freedom .",
    "equation ( [ eqn : transpose ] ) might be easier to evaluate , especially when the problem has many more data values than solution variables , @xmath124 , as is often the case .",
    "applying the formula ( [ eqn : transpose ] ) for the norm of the transpose matrix to the equation ( [ eqn : induced ] ) results in the simpler optimization problem , @xmath125}\\| = { { \\mathcal { a}}\\over { \\mathcal { x } } } \\max_{{\\delta x } } { \\| \\ { { j_x [ { \\mbox { vec}}(a)]}\\}^t { \\delta x}\\|^*_2 \\over \\| { \\delta x}\\|^*_2}\\ ] ] the norm for the transposed jacobian matrix is induced from the duals of the @xmath0-norms for matrices and vectors .",
    "the vector @xmath0-norm in the denominator is its own dual .",
    "so as not to interrupt the present discussion , some facts needed to evaluate the numerator are proved in section [ sec : advanced ] : the dual of the matrix @xmath0-norm is the nuclear norm ( section [ app : spectral ] ) , and a formula for the nuclear norm is given ( section [ app : rank2 ] ) .",
    "the application of equation ( [ eqn : simpler ] ) requires evaluating the matrix - vector product in the numerator . continuing the derivation of section [ sec :",
    "begin ] from equation ( [ eqn : where ] ) , the vector - matrix product @xmath126 $ ] for some @xmath25 can be evaluated by straightforward multiplication , @xmath127   = \\left [   \\setlength { \\arraycolsep } { 0.25em } \\begin { array } { c c c } v_1 r^t   & \\cdots & v_n r^t   \\end { array } \\right ] -   \\left [   \\setlength { \\arraycolsep } { 0.33em } \\begin { array } { c c c c c } x_1 v^t a^t &",
    "\\cdots & x_n v^t a^t   \\end { array } \\right ] .\\ ] ] this row vector , when transposed , is expressed more simply using @xmath83 notation : the first part is @xmath128 scaled by each entry of @xmath25 , @xmath129 , the second part is @xmath130 scaled by each entry of @xmath9 , @xmath131 .",
    "altogether @xmath132^t v = { \\mbox { vec}}(rv^t - avx^t ) \\ , .\\ ] ] substituting @xmath133^{-1}\\}^t { \\delta x}= ( a^t a)^{-1 } { \\delta x}$ ] for some @xmath21 gives , by equation ( [ eqn : hence ] ) , @xmath134}^t { \\delta x}=   { \\mbox { vec}}\\left\\ { r \\ , [ ( a^t a)^{-1 } { \\delta x}]^t - a \\ , [ ( a^t a)^{-1 } { \\delta x } ] \\ , x^t \\right\\ } \\ , , \\ ] ] or equivalently , @xmath135}^t { \\delta x}\\ }   = u_1^ { } v_1^t + u_2^ { } v_2^t\\ ] ] where `` @xmath136 '' is the inverse of `` @xmath83 , '' and @xmath137 the matrix on the right side of equation ( [ eqn : jxadx ] ) has rank @xmath0 .",
    "moreover , the two rank @xmath60 pieces are mutually orthogonal because the least - squares residual @xmath128 is orthogonal to the coefficient matrix @xmath2 . with these replacements equation ( [ eqn : simpler ] ) becomes @xmath138}\\big\\| = { { \\mathcal { a}}\\over { \\mathcal { x } } } \\max_{\\| { \\delta x}\\|_2 = 1 } \\| u_1^ { } v_1^t + u_2^ { } v_2^t \\|^*_2 \\ , .\\ ] ]    lemma [ lem : kahan ] shows that the dual of the spectral matrix norm is the matrix norm that sums the singular values of the matrix , which is called the nuclear norm .",
    "lemma [ lem : rank2 ] then evaluates this norm for rank @xmath0 matrices to find that the objective function of equation ( [ eqn : inducedtransposed ] ) is @xmath139 where @xmath140 is the angle between @xmath141 and @xmath142 , and @xmath143 is the angle between @xmath144 and @xmath145 , and both angles should be taken from @xmath146 to @xmath147 . since @xmath141 is orthogonal to @xmath142 therefore @xmath148 and then @xmath149 so @xmath150 is not negative .",
    "this means the maximum lies between the lower and upper limits @xmath151 with @xmath152 restricted to @xmath60 , the lower bound and also the upper bound attain their maxima when @xmath21 is a right singular unit vector for the smallest singular value of @xmath2 , @xmath153 some value of @xmath154 lies between the limits when @xmath21 is a right singular unit vector for the smallest singular value of @xmath2 .",
    "because these are the largest possible limits , the maximum value must lie between them as well .",
    "these limits must be multiplied by the coefficient @xmath155 in equation ( [ eqn : inducedtransposed ] ) to obtain bounds for the norm of the jacobian matrix .",
    "[ spectral condition numbers ] [ thm : condition - numbers ] for the full column rank linear least - squares problem with solution @xmath156 , and for the scaled norms of equation ( [ eqn : norms ] ) with scale factors @xmath92 , @xmath93 , and @xmath94 , @xmath157 where @xmath158 and @xmath159 are the singular values of the rank @xmath0 matrix @xmath160 for @xmath161 the value of @xmath65 lies between the lower limit of malyshev and the upper limit of bjrck , @xmath162 the upper bound exceeds @xmath65 by at most a factor @xmath5 .",
    "the formula for @xmath163 and the limits for @xmath44 simplify to equations ( [ eqn : chixa ] , [ eqn : chixb ] ) for the scale factors in equation ( [ eqn : scale - factors ] ) .",
    "section [ sec : conditionwrtb ] derives @xmath163 , and sections [ sec : begin][sec : conditionwrtafinished ] derive @xmath44 and the bounds .",
    "the condition number @xmath65 in theorem [ thm : condition - numbers ] can lie strictly between the limits of bjrck and malyshev . for the example of section [ sec : example ] ,",
    "the rank @xmath0 matrix in the theorem is @xmath164 .\\ ] ] for the specific values @xmath165 ,",
    "@xmath166 , @xmath167 , the sum of the singular values of this matrix can be numerically maximized over @xmath168 to evaluate the condition number with the following results .",
    "@xmath169 ^ 2 + 1\\big)^{1/2}$ & $ = $ & 32.505\\ldots & lower limit of malyshev\\\\ \\end { tabular}\\ ] ] these calculations were done with mathematica @xcite .",
    "@xmath170 \\right \\|{_{\\mbox { \\scriptsize \\rm f } } } & \\| { \\delta x}\\|_2 & \\displaystyle { 1 \\over { \\sigma_{\\min}}^ { } } \\sqrt { { \\| r \\|_2 ^ 2 \\over \\alpha^2 { \\sigma_{\\min}}^2 } + { \\| x \\|_2 ^ 2 \\over \\alpha^2 } + { 1 \\over \\beta^2 } } & \\mbox { exact } \\\\ { \\begin { minipage}{10em } \\vrule depth0ex height4.0ex width{0pt}malyshev\\\\ \\scriptsize \\nocite { malyshev2003}(2003 , p.\\ 1187 , eqn.\\ 1.8)\\vrule depth3.0ex height0ex width{0pt}\\end { minipage } } & \\displaystyle { \\| { \\delta a}\\|{_{\\mbox { \\scriptsize \\rm f } } } \\over \\| a \\|_2 } & \\displaystyle { \\| { \\delta x}\\|_2 \\over \\| x \\|_2 } & & \\mbox { exact } \\\\ { \\begin { minipage}{11.5em } \\vrule depth0ex height4.0ex width{0pt}malyshev and theorem \\ref { thm : condition - numbers}\\\\ \\scriptsize \\nocite { malyshev2003}(2003 , p.\\ 1189 , eqn.\\ 2.4 and line --6)\\vrule depth3.0ex height0ex width{0pt}\\end { minipage } } & \\displaystyle { \\| { \\delta a}\\|_2 \\over \\| a \\|_2 } & \\displaystyle { \\| { \\delta x}\\|_2 \\over \\| x \\|_2 } & \\mbox{\\hspace { -1.5em } \\raisebox{4.0ex}[0pt][0pt]{$\\displaystyle \\left .",
    "\\vrule depth7ex height0ex width 0pt \\right\\ } \\hspace { 0.5em } { \\| a \\|_2 \\over { \\sigma_{\\min}}^ { } } \\sqrt { { \\| r \\|_2 ^ 2 \\over { \\sigma_{\\min}}^2 \\ , \\| x \\|_2 ^ 2 } + 1}$ } } & \\mbox { approx } \\\\ \\hline \\end { array}\\ ] ]    table [ tab : lls ] lists several condition numbers or approximations to condition numbers for least - squares solutions .",
    "the three exact values measure changes to @xmath2 by the frobenius norm , while the two approximate values are for the spectral norm .",
    "the difference can be attributed to the ease or difficulty of solving the maximization problem in equation ( [ eqn : inducedtransposed ] ) .",
    "the dual spectral norm of the rank @xmath0 matrix involves a trigonometric function , @xmath171 in equation ( [ eqn : numerator ] ) , whose value only can be estimated . if a frobenius norm were used instead , then lemma [ lem : rank2 ] shows the dual norm of the rank @xmath0 matrix involves an expression , @xmath172 , whose value is zero , which greatly simplifies the maximization problem .",
    "many error bounds in the literature combine @xmath173 in the manner of equation ( [ eqn : double ] ) , @xmath174 \\epsilon + o ( \\epsilon ) \\qquad \\mbox { overestimate by at most $ \\times 2 $ }",
    "\\\\ \\noalign { \\smallskip } \\label { eqn : bestbound-2 } & \\le & \\big [ { { \\boldsymbol \\kappa^{}_{\\boldsymbol 2}}}({\\mbox { \\bfseries \\scshape",
    "v}}\\tan ( { { \\boldsymbol \\theta } } ) + 1 ) + { \\mbox { \\bfseries \\scshape v}}\\sec ( { { \\boldsymbol \\theta } } ) \\big ] \\epsilon + o ( \\epsilon ) \\quad \\mbox { further at most $ \\times \\sqrt 2 $ } \\\\ \\noalign { \\smallskip }   \\label { eqn : bestbound-3 } & =   & \\left ( { \\| a \\|_2 \\| r \\|_2 \\over { \\sigma_{\\min}}^2 \\| x \\|_2 } + { \\| a \\|_2 \\over { \\sigma_{\\min } } } + { \\| b \\|_2 \\over { \\sigma_{\\min}}\\| x \\|_2 } \\right ) \\epsilon + o ( \\epsilon ) \\ , , \\end{aligned}\\ ] ] where @xmath175 . bounds ( [ eqn : bestbound-1 ] , [ eqn : bestbound-2 ] ) are larger than the attainable bound by at most factors @xmath0 and @xmath176 , respectively , by equation ( [ eqn : sum-6 ] ) and theorem [ thm : condition - numbers ] .",
    "some bounds are yet larger .",
    "reports @xmath177 this bound is an overestimate in comparison to equation ( [ eqn : bestbound-3 ] ) .",
    "an egregious overestimate occurs in an error bound that appears to have originated in the 1983 edition of the popular textbook of .",
    "the overestimate is restated by in the lapack documentation , and by , @xmath178 \\epsilon + { \\mathcal o } ( \\epsilon^2 ) \\ , .\\ ] ] in comparison with equation ( [ eqn : bestbound-2 ] ) this bound replaces @xmath47 by @xmath46 and replaces @xmath60 by @xmath179 .",
    "an overestimate by a factor of @xmath46 occurs for the example of section [ sec : example ] with @xmath180 , @xmath166 , and @xmath181 . in this case",
    "the ratio of equation ( [ eqn : gvl ] ) to equation ( [ eqn : bestbound-2 ] ) is @xmath182",
    "this section describes the dual norms in the formulas of sections [ sec : transpose ] and [ sec : conditionwrtacontinued ] .",
    "the actual mathematical concept is a norm for the dual space .",
    "however , linear algebra `` identifies '' a space with its dual , so the concept becomes a `` dual norm '' for the same space .",
    "this point of view is appropriate for hilbert spaces , but it omits an important level of abstraction . as a result , the linear algebra literature lacks a complete development of finite dimensional normed linear ( banach ) spaces . rather than make functional analysis a prerequisite for this paper , here the identification approach is generalized to give dual norms for spaces other than column vectors ( which is needed for data in matrix form ) , but only as far as the dual norm itself in section [ app : duals ] .",
    "section [ app : spectral ] gives the formula for the dual of the spectral matrix norm .",
    "section [ app : rank2 ] evaluates the norm for matrices of rank @xmath0 .",
    "_ banach spaces are needed in this paper because the norms used in numerical analysis are not necessarily those of a hilbert space . _",
    "the space of @xmath3 matrices viewed as column vectors has been given the spectral matrix norm in equation ( [ eqn : norms ] ) . if the norm were to make the space a hilbert space , then the norm would be given by an inner product .",
    "there would be an @xmath183 symmetric matrix , @xmath184 , so that for every @xmath3 matrix @xmath185 , @xmath186^t \\ ; s \\ ; { \\mbox { vec}}(b ) } \\ , , \\ ] ] which is impossible .",
    "if @xmath187 is a finite dimensional vector space over @xmath188 , then the dual space @xmath189 consists of all linear transformations @xmath190 , called functionals .",
    "if @xmath187 has a norm , then @xmath189 has the usual operator norm given by @xmath191 one notation is used for both norms because whether a norm is for @xmath187 or @xmath189 can be decided by what is inside .    for a finite dimensional @xmath187 with a basis @xmath192 , @xmath193 , @xmath194 , @xmath195 ,",
    "the dual space has a basis @xmath196 , @xmath197 , @xmath194 , @xmath198 defined by @xmath199 where @xmath200 is kronecker s delta function . in linear algebra for finite dimensional spaces ,",
    "it is customary to represent the arithmetic of @xmath189 in terms of @xmath187 under the transformation @xmath201 defined on the bases by @xmath202 .",
    "_ this transformation is not unique because it depends on the choices of bases . _",
    "usually @xmath187 has a favored or `` canonical '' basis whose @xmath203 is said to `` identify '' @xmath189 with @xmath187 . under this identification",
    "the norm for the dual space then is regarded as a norm for the original space .",
    "[ dual norm ] let @xmath204 have norm @xmath205 and let @xmath203 identify @xmath187 with the dual space @xmath189 .",
    "the dual norm for @xmath187 is @xmath206 where the right side is the norm in equation ( [ eqn : operatornorm ] ) for the dual space .",
    "the notation @xmath207 avoids confusing the two norms for @xmath187 .",
    "there seems to be no standard notation for the dual norm ; others are @xmath208 , @xmath209 , and @xmath210 which are used respectively by , , and .",
    "the space @xmath211 of real @xmath3 matrices has a canonical basis consisting of the matrices @xmath212 whose entries are zero except the @xmath86 entry which is @xmath60 .",
    "this basis identifies a matrix @xmath2 with the functional whose value at a matrix @xmath185 is @xmath213 .",
    "[ dual of the spectral matrix norm ] [ lem : kahan ] the dual norm of the spectral matrix norm with respect to the aforementioned canonical basis for @xmath211 is given by @xmath214 , where @xmath215 is the vector of @xmath2 s singular values including multiplicities .",
    "that is , @xmath216 is the sum of the singular values of @xmath2 with multiplicities , which is called the nuclear norm or the trace norm .    _",
    "( supplied by @xcite . ) _ let @xmath217 be a `` full '' singular value decomposition of @xmath2 , where both @xmath218 and @xmath219 are orthogonal matrices , and where @xmath220 is an @xmath3 `` diagonal '' matrix whose diagonal entries are those of @xmath221 .",
    "the trace of a square matrix , @xmath123 , is invariant under conjugation , @xmath222 , so @xmath223 since @xmath224 , the entries of @xmath225 are at most @xmath60 in magnitude , and therefore @xmath226 .",
    "this upper bound is attained for @xmath227 where @xmath228 is the @xmath3 `` identity '' matrix .",
    "an alternate proof is offered by the work of von @xcite .",
    "he studied a special class of norms for @xmath211 .",
    "a symmetric gauge function of order @xmath229 is a norm for @xmath230 that is unchanged by every permutation and sign change of the entries of the vectors .",
    "such a function applied to the singular values of a matrix always defines a norm on @xmath211 .",
    "for example , @xmath231 where as in lemma [ lem : kahan ] @xmath221 is the length @xmath232 column vector of singular values for @xmath2 .",
    "the dual of this norm is given by the dual norm for the singular values vector , see .",
    "[ frobenius and nuclear norms of rank 2 matrices . ]",
    "[ lem : rank2 ] if @xmath234 and @xmath235 , then ( frobenius norm ) @xmath236 and ( nuclear norm , or trace norm ) @xmath237 where @xmath140 is the angle between @xmath141 and @xmath142 , and @xmath143 is the angle between @xmath144 and @xmath145 .",
    "both angles should be taken from @xmath146 to @xmath147 .    if any of the vectors vanish , then the formulas are clearly true , so it may be assumed that the vectors are nonzero .",
    "the strategy of the proof is to represent the rank @xmath0 matrix as a @xmath238 matrix whose singular values can be calculated . since singular values are wanted , it is necessary that the bases for the @xmath238 representation be orthonormal .",
    "to that end , let @xmath239 and @xmath240 be orthogonal unit vectors with @xmath241 and @xmath242 .",
    "the coefficients signs are indeterminate , so without loss of generality assume @xmath243 and @xmath244 , in which case @xmath245 similarly , let @xmath246 and @xmath247 be mutually orthogonal unit vectors with @xmath248 and @xmath249 . again without loss of generality @xmath250 and @xmath251 so that @xmath252 notice that @xmath253 let @xmath254 .",
    "a straightforward calculation shows that , with respect to the orthonormal basis consisting of @xmath246 and @xmath247 , the matrix @xmath255 is represented by the matrix @xmath256 \\ , .\\ ] ] the desired norms are now given in terms of the eigenvalues of @xmath123 , @xmath257 , @xmath258    the expression for @xmath259 requires further analysis . for any @xmath238 matrix @xmath123 , @xmath260 in the present case these eigenvalues are real because the @xmath123 of interest is symmetric , and @xmath261 because it is also positive semidefinite .",
    "altogether @xmath262 ^ 2 \\ge 4 \\det ( m ) \\ge 0 $ ] , so @xmath263 .",
    "these bounds prove the following quantities are real , and it can be verified they are the square roots of the eigenvalues of @xmath123 , @xmath264 thus @xmath265    in summary , the desired quantities @xmath259 and @xmath266 have been expressed in terms of @xmath267 and @xmath268 which the expression for @xmath123 expands into formulas of @xmath269 , @xmath72 , @xmath270 , and @xmath271 .",
    "these in turn expand to expressions of @xmath272 and @xmath273 .",
    "it is remarkable that the ultimate expressions in terms of @xmath272 and @xmath273 are straightforward , @xmath274 where @xmath140 is the angle between @xmath141 and @xmath142 , and similarly for @xmath143 .",
    "the formula for @xmath275 is established . the formula for @xmath259 simplifies , using the difference formula for cosine , to the one in the statement of the lemma . since the positive root of @xmath276",
    "is wanted , the angles should be chosen from @xmath146 to @xmath147 so the squares of the sines can be removed without inserting a change of sign .",
    "these calculations have been verified with mathematica @xcite ."
  ],
  "abstract_text": [
    "<S> the condition number of solutions to full rank linear least - squares problem are shown to be given by an optimization problem that involves nuclear norms of rank 2 matrices . </S>",
    "<S> the condition number is with respect to the least - squares coefficient matrix and 2-norms . </S>",
    "<S> it depends on three quantities each of which can contribute ill - conditioning . </S>",
    "<S> the literature presents several estimates for this condition number with varying results ; even standard reference texts contain serious overestimates . </S>",
    "<S> the use of the nuclear norm affords a single derivation of the best known lower and upper bounds on the condition number and shows why there is unlikely to be a closed formula .    </S>",
    "<S> linear least - squares , condition number , applications of functional analysis , nuclear norm , trace norm    65f35 , 62j05 , 15a60 </S>"
  ]
}