{
  "article_text": [
    "hidden markov models provide a flexible description of a wide variety of real - life phenomena ; see @xcite for an overview .",
    "an hmm  is a pair of discrete - time stochastic processes , @xmath8 and @xmath9 , where @xmath10 is an unobserved process and @xmath11 is observed .",
    "the hidden process @xmath12 is a  markov chain with initial density @xmath13 at time @xmath14 and transition density @xmath15 , with @xmath16 i.e. @xmath17 where @xmath18 denotes probability , @xmath19 and @xmath20 is lebesgue measure .",
    "in addition , the observations @xmath21conditioned upon @xmath12 are statistically independent and have marginal density @xmath22 , i.e.@xmath23 with @xmath24 . the hmm is given by equations ( [ eq : evol])-([eq : obs ] ) and is often referred to in the literature as a state - space model .",
    "here @xmath4 is a static parameter , which is to be estimated in using mle and online as the data arrive ; this problem has a large range of real applications such as financial modelling or weather prediction .",
    "statistical inference from the class of hmms described above is typically non - trivial . in most scenarios of practical interest",
    "one can not calculate the likelihood : @xmath25 where @xmath26 and @xmath27 is the predictor ; see e.g.  @xcite for the standard filtering recursions .",
    "hence as the likelihood is not analytically tractable , one must resort to numerical methods , not only to compute it , but to maximize @xmath28 w.r.t .",
    "when @xmath4 is known , a popular collection of techniques for both estimating the likelihood as well as performing filtering and smoothing are sequential monte carlo methods e.g.  @xcite .",
    "smc techniques simulate a collection of @xmath29 samples in parallel , sequentially in time and combine importance sampling and resampling to approximate a sequence of probability distributions of increasing state - space known pointwise up - to a multiplicative constant .",
    "these techniques provide a natural estimate of the likelihood .",
    "the estimate is quite well understood and is known to be unbiased @xcite and , in addition , the relative variance is known to increase linearly with @xmath3 @xcite , @xmath3 being the number of data .",
    "when @xmath4 is unknown , as is the case here , estimation of @xmath4 is complicated by the path - degeneracy problem of smc methods ; e.g.  @xcite . however , there are still many specialized smc techniques which can successfully be used for online parameter estimation of hmms in a wide variety of contexts , such as @xcite .",
    "most of these techniques require the evaluation of @xmath30 and potentially the gradient vectors as well .    in this article",
    ", we consider the scenario where @xmath31 is either intractable , in the sense that one can not calculate it or an unbiased estimator of it , or one does not want to calculate the density , potentially due to the high - dimensionality of @xmath32 .",
    "it is assumed that one can sample from @xmath33 . in this case",
    ", one can not use standard or the more advanced smc methods that are mentioned above ( or indeed many other techniques ) and hence exact online parameter estimation is difficult to achieve .",
    "one approach which is designed to deal with this problem are abc techniques ; see e.g.  @xcite .",
    "whilst there are a number of other competitors @xcite , we focus upon abc ideas ; see @xcite for some discussion of the relative merits of abc against competing methods . in the context of hmms",
    "there has been some work on the construction of abc approximations of hmms @xcite , computational techniques for filtering and smoothing @xcite and their statistical consistency for parameter estimation @xcite .",
    "abc approximations of hmms are biased , but the bias can be controlled to arbitrary precision via a parameter @xmath0 ; the bias typically goes to zero as @xmath34 . at present",
    "there is not a methodology which can achieve our objective of online parameter estimation . in this article",
    "we do the following :    1 .   investigate the bias in the log - likelihood and the gradient of the log - likelihood that is induced by the abc approximation for a fixed data set .",
    "2 .   develop an smc approach with cost @xmath35 that allows one to estimate the static parameters in an online fashion .    in order to estimate the parameters one must obtain numerical estimates of the log - likelihood and gradient of this quantity .",
    "it is then important to understand what happens to the bias of the abc approximation of these latter quantities , as the time parameter ( number of data , @xmath3 ) grows .",
    "we establish , under some assumptions , that this abc bias , for both quantites is no worse than @xmath2 ; this result is associated to the theoretical work in @xcite .",
    "these former results indicate that the abc approximation is amenable to numerical implementation : parameter estimation will not necessarily be dominated by the bias ; we discuss why this is the case in remarks [ rem : abc_approx_error ] and [ rem : abc_approx_error1 ] . for 2 .",
    "we introduce an smc approach based upon spsa @xcite to estimate the parameters in an online manner ( see also @xcite in the context of hmms ) .",
    "this methodology can be expected to ` work well ' when :    * @xmath36 is large and @xmath37 , @xmath38 are small to moderate .    whilst these statements are somewhat delicate ( e.g.  what is large ) , in the scenario of high - dimensional states , it has been established in @xcite that the _ simulation _ error does not explode in the dimension .",
    "as a result , the ideas here can be seen as principled competitors ( and related to - see @xcite ) to ensemble kalman filter - based algorithms such as in @xcite .",
    "this paper is structured as follows . in section",
    "[ sec : model ] we discuss the model and abc approximation .",
    "our bias result is also given . in section [ sec : comp ] our computational strategy is outlined . in section [ sec : numer ] the method is investigated from a computational perspective . in section [ sec : summ ] the article is concluded with some discussion of future work . the proofs of our results can be found in the appendix .",
    "consider first the joint filtering or smoothing density of the hmm given by @xmath39 where @xmath16 is the static parameter , @xmath40 are the hidden states and @xmath41 the observations .",
    "this quantity can be computed recursively using @xmath42 with the _ recursive likelihood _",
    "being @xmath43",
    "furthermore we write the log- ( marginal ) likelihood at time @xmath3 : @xmath44 in the context of mle one is usually interested computing @xmath45 note that this is a batch or off - line method , which means that one needs to wait first to collect the complete dataset and then compute the ml estimate . for a long observation sequence",
    "the computation of the gradient at each iteration of the algorithm can be prohibitive .",
    "therefore , one uses on - line methods whereby the estimate of the parameter is updated sequentially as the data arrives . a practical alternative would be to consider the following update scheme at time @xmath46 , for some sequence @xmath47 @xmath48 upon receiving @xmath49 , the parameter estimate is updated in the direction of ascent of the conditional density of this new observation . the algorithm in the present form",
    "is not suitable for on - line implementation due to the need to evaluate the gradient of @xmath50 at the current parameter estimate which would require computing the filter from time @xmath14 to time @xmath46 using the current parameter value @xmath51 .",
    "a recursive ml ( rml ) algorithm bypassing this problem has been proposed in the literature when @xmath52 is finite in @xcite .",
    "it relies on the following update scheme @xmath53 where the positive non - increasing step - size sequence @xmath54 satisfies @xmath55 and @xmath56 @xcite ; e.g. @xmath57 for @xmath58 .",
    "the quantity @xmath59 is defined as @xmath60 where the notation @xmath61 indicates that at each time @xmath46 the quantities in ( [ eq : filter1])-([eq : filter3 ] ) are computed using the parameter estimate @xmath51 .",
    "the asymptotic properties of this algorithm  ( i.e. the behavior of @xmath51  in the limit as @xmath46  goes to infinity ) have been studied in @xcite for a finite state - space hmm .",
    "it is shown that under regularity conditions this algorithm converges towards a local maximum of the average log - likelihood ; this average log - likelihood being maximized at the ` true ' parameter value .    in this article",
    ", we would like to implement approximate versions of these on - line and off - line ml schemes when both the following cases hold :    * case 1 : we can sample from the conditional distribution of @xmath62 , for any fixed @xmath4 and @xmath63 . * case 2 : we can not or do not want to evaluate the conditional density of @xmath62 , @xmath30 and do not have access to an unbiased estimate of it .",
    "apart from using likelihoods which do not admit computable densities such as some stable distributions , this context might appear relevant to the context when one is interested to use smc methods and evaluate @xmath30 when @xmath64 is large .",
    "smc methods for _ filtering _ do not always scale well with the dimension of the hidden state @xmath36 , often requiring a computational cost @xmath65 , with @xmath66 ; see e.g.  @xcite .",
    "a more detailed discussion on the difficulties of using smc methods in high dimensions is far beyond the scope of this article , but we remark the ideas in this paper can be relevant in this context .      to facilitate statistical inference , we consider an abc approximation of the joint smoothing density ( e.g.  @xcite ) : @xmath67 where @xmath68 are pseudo observations , @xmath0 @xmath69 is some kernel function that has bandwidth that depends upon a precision parameter @xmath0 .",
    "examples include : @xmath70 where @xmath71 is the indicator function , @xmath72 is the @xmath73norm , @xmath74 is normal density on @xmath75dimensions with mean @xmath76 and covariance @xmath77 and @xmath78 is the @xmath75dimensional identity matrix .    consider the quantity , to be used below : @xmath79 throughout the article we _ critically _ choose @xmath80 such that the denominator of does not depend upon @xmath81 or @xmath4 . as noted in @xcite , after integrating out the @xmath82",
    ", this representation leads to a new ( or perturbed ) hmm with transitions @xmath83 and likelihoods @xmath84 .",
    "parameter estimation associated to the smoother @xmath85 just considers the function : @xmath86 where @xmath87 we term the maximizer of @xmath88 as the abc - mle .",
    "one can then define a rml procedure for the abc - hmm as in section [ sec : model1 ] : @xmath89 in practice , one can consider an estimation of @xmath88 including factors independent of @xmath90 ; this is discussed in section [ sec : comp ] .",
    "results on associated to the asymptotics of the abc - mle ( i.e.  as @xmath3 grows ) can be found in @xcite ; there is an asymptotic bias . in addition , in the case of noisy abc , where the data become corrupted , there is no asymptotic bias and one can recover the true parameter .",
    "we remark that the methodology that is considered in this article can _ easily _ incorporate noisy abc .",
    "however , there may be several reasons why one may not want to use noisy abc : ( 1 ) the consistency results ( currently ) depend upon the data originating from the original hmm ; ( 2 ) the current simulation - based methodology may not be able to push @xmath91 towards zero . for ( 1 ) ,",
    "if the data do not originate from the hmm of interest , it has not been studied what happens with regards to the asymptotics of noisy abc for hmms .",
    "it may be that some investigators might be uncomfortable with assuming that the data originate from the exactly the hmm being fitted . for ( 2 ) the asymptotic bias ( which is under assumptions either @xmath92 or @xmath93 @xcite ) could be less than the asymptotic variance ( under assumptions @xmath93 @xcite ) as @xmath91 could be much bigger than 1 when using current simulation methodology .",
    "we do not use noisy abc in this article , but acknowledge its fundamental importance with regards to parameter estimation associated to abc for hmms ; our approach is pragmatic , taking into account points ( 1)-(2 ) .",
    "we now prove an upper - bound on the bias induced by the abc approximation on the log - likelihood and gradient of the log - likelihood .",
    "the latter is more relevant for parameter estimation , but the mathematical arguments are considerably more involved for this quantity , in comparison to the abc bias of the log - likelihood . hence the log - likelihood is considered as a simple preliminary result .",
    "these results are to be taken in the context of abc ( not noisy abc ) and help to provide some guarantees associated to the numerics .",
    "we consider the scenario @xmath94 where the set @xmath95 is specified below . throughout @xmath72",
    "is understood to be an @xmath96norm .",
    "the hidden - state is assumed to lie on a _ compact _ set , i.e.  @xmath52 is compact .",
    "we use the notation @xmath97 to denote the class of probability measures on @xmath52 and @xmath98 the collection of finite and signed measures on @xmath52 .",
    "@xmath99 denotes the total variation distance .",
    "the initial distribution of the hidden markov chain is written as @xmath100 .",
    "in addition , we condition on the observed data and do not mention them in any mathematical statement of results ( due to the assumptions below ) .",
    "we do not consider the instance of whether the data originate , or not , from a hmm .",
    "for the control of the bias of the gradient of the log - likelihood ( theorem [ theo : grad_ll_bias ] ) , we assume that @xmath101 .",
    "this is not restrictive as one can use the arguments to prove analgous results when @xmath102 , by considering componentwise arguments for the gradient .",
    "in addition , for the gradient result , the derivative of @xmath103 is written @xmath104 .",
    "we make the following assumptions , which are extremely strong .",
    "they are made to keep the proofs as short as possible .",
    "[ hyp : like_cont ] _ lipschitz continuity of the likelihood_. there exist @xmath105 such that for any @xmath106 , @xmath107 , @xmath108 @xmath109    [ hyp : stat ] _ statistic and metric_. the set @xmath110 is : @xmath111    [ hyp : like_bound ] _ boundedness of likelihood and transition_. there exist @xmath112 such that for all @xmath113 , @xmath114 , @xmath108 @xmath115    [ hyp : like_grad_cont ] _ lipschitz continuity of the gradient of the likelihood_. @xmath116 , @xmath117 are differentiable in @xmath4 for each @xmath113 , @xmath114 .",
    "in addition , there exist @xmath105 such that for any @xmath106 , @xmath107 , @xmath108 @xmath118    [ hyp : like_grad_bound ] _ boundedness of gradients of the likelihood and transition_. there exist @xmath112 such that for all @xmath113 , @xmath114 , @xmath108 @xmath119    we first have the result on the abc bias of the log - likelihood .",
    "the proof is in appendix [ app : log_like ] .",
    "[ prop : ll_bias ] assume ( a1 - 3 ) . then there exist a @xmath120 such that for any @xmath121 , @xmath100 , @xmath0 , @xmath108 we have : @xmath122    [ rem : abc_approx_error ] the above proposition gives some simple guarantees on the bias of the abc log - likelihood .",
    "when using smc algorithms to approximate @xmath123 , the overall error will be decomposed into the deterministic bias that is present from the abc approximation ( that in proposition [ prop : ll_bias ] ) and the numerical error of approximating the log - likelihood . under some assumptions ,",
    "the @xmath124error of the smc estimate of the log - likelihood should not deteriorate any faster than linearly in time ; this is due to the results cited previously .",
    "thus , as the time parameter increases , the abc bias of the log - likelihood will not necessarily dominate the simulation - based error that would be present even if @xmath125 is evaluated .",
    "proposition [ prop : ll_bias ] is reasonably straight - forward to prove , but , is of less interest in the context of parameter estimation , as one is interested in the gradient of the log - likelihood .",
    "we now have the result on the abc bias of the gradient of the log - likelihood . the proof in appendix [ app : log_like_grad ] .",
    "[ theo : grad_ll_bias ] assume ( a1 - 5 ) .",
    "then there exist a @xmath120 such that for any @xmath121 , @xmath100 , @xmath104 , @xmath0 , @xmath108 we have : @xmath126    [ rem : abc_approx_error1 ] the above theorem again provides some explicit guarantees when using an abc approximation along with smc - based numerical methods .",
    "for example , if one can consider approximating gradients in an abc context ( see @xcite ) , then from the results of @xcite , one expects that the variance of the smc estimates to increase only linearly in time . again , as time increases the abc bias does not necessarily dominate the variance that would be present even if @xmath125 is evaluated ( i.e.  one uses smc on the true model ) .",
    "the result in theorem [ theo : grad_ll_bias ] can be found in eq .",
    "( 72 ) of @xcite and direct limit ( as @xmath127 ) in @xcite .",
    "however , we adopt a new ( and fundamentally different ) proof technique , with a substantially clearer proof and an additional result of independent interest is proved .",
    "we derive the stability w.r.t .",
    "time of the bias of the abc approximation of the filter derivative ; see theorem [ theo : filt_deriv_bias ] in appendix [ app : filt_deriv ] .",
    "in order to perform online parameter estimation , we will need to use a smc algorithm to approximate @xmath128 for @xmath4 fixed ; this is a critical quantity that we will use below .",
    "an algorithm which can do this is the smc approach in @xcite which is detailed in figure [ fig : abc_smc ] , with proposals @xmath129 with density w.r.t .  lebesgue measure .    on the basis of figure [ fig : abc_smc ]",
    ", one can approximate @xmath88 , _ up - to a constant that is independent of _",
    "@xmath4 , as follows . in an abuse of notation",
    ", we denote this smc estimate ( which does not include factors that do not depend on @xmath4 ) as @xmath130 .",
    "the smc estimate is @xmath131 with @xmath132 these estimates are unbiased for any @xmath133 ( see @xcite ) . in practice , we are interested in the log - likelihoods ; taking logarithms of the above estimates generally leads to a biased approximation of @xmath134 and @xmath135 . one can implement a form of bias correction , using the taylor series expansion ideas in @xcite . throughout",
    ", we use the bias - corrected estimates : @xmath136    the parameter @xmath91 can be computed adaptively ; see @xcite .",
    "it is remarked that a drawback of this algorithm is that when @xmath38 grows with @xmath137 fixed , one can not expect the algorithm to work well for every @xmath91 ; typically one must increase @xmath91 to yield reasonable algorithmic results and this is at the cost of increasing the bias . to maintain @xmath91 at a reasonable level ,",
    "one must consider more advanced strategies which are not investigated here .",
    "one final point , which is often useful in practice .",
    "one can modify the abc approximation to : @xmath138f_{\\theta}(x_k|x_{k-1})\\ ] ] which yields the same bias as the original abc approximation ( on integrating the @xmath139 variables ) but can yield substantial computational improvements .",
    "this is because as @xmath140 grows one approximates a marginal smc that does not sample the auxiliary @xmath139 variables .",
    "* @xmath141 @xmath142 i.i.d .  from @xmath143 .",
    "@xmath144 @xmath145 . @xmath146 * @xmath29@xmath147 @xmath148@xmath149@xmath150@xmath151 * @xmath152@xmath153 @xmath154@xmath155@xmath156 . @xmath157",
    "[ rem : collapsed_abc ] we note that , suppressing @xmath4 , if the hmm can be written in the form : @xmath158 where @xmath159 is known , @xmath160 , @xmath161 with @xmath162 i.i.d .",
    "@xmath163 with @xmath164 i.i.d .  and independent of @xmath162 and @xmath165 ,",
    "suppose that :    * one can evaluate the densities of @xmath167 and @xmath168 and sample from the associated distributions .",
    "* one can evaluate @xmath169 ( resp .",
    "@xmath170 ) pointwise , for each @xmath121 and @xmath171 ( resp .",
    "@xmath172 ) .",
    "one can construct a ` collapsed ' ( see @xcite ) abc approximation ( assuming @xmath173 , @xmath174 , with @xmath175 a distance metric on @xmath176 ) @xmath177 hence a version of the smc algorithm in figure [ fig : abc_smc ] can be derived which does not need to sample from the dynamics of the data . in additon",
    "one does not need access to the transition density of the hidden markov chain .",
    "this representation , however , does not always apply .      recall the rml procedure in section [ sec : model1 ] , where @xmath30 is not intractable : @xmath178 for @xmath179 a sequence of step - sizes . in practice",
    ", one does not know the gradient and must resort to ( e.g. ) smc techniques to approximate it ; see for example @xcite . in our abc context one can run the algorithm in figure [ fig : abc_smc ] to approximate the abc filter . to recursively update @xmath4 , at least using the ideas in @xcite ,",
    "one has to evaluate @xmath180 which we will not have access to .",
    "we propose the following computational scheme ; the idea is to use spsa , which does not require the quantities in .",
    "introduce a decreasing sequence of positive numbers @xmath181 .",
    "suppose , with @xmath182 as in the update , , we have @xmath183    start with some initial guess @xmath184 and perform the standard smc update ( i.e.  as in figure [ fig : abc_smc ] ) for two sets of particles .",
    "one with parameter : @xmath185 and the other with parameter : @xmath186 where @xmath187 is a @xmath188dimensional vector with each entry @xmath189 bernoulli distributed ( see @xcite ) . for both algorithms",
    "compute @xmath190 and @xmath191 respectively , where the estimates are the bias - corrected versions as in equation . to obtain the next parameter estimate , in the @xmath192dimension ,",
    "take @xmath193    at any subsequent time - point , with @xmath194 and perform the standard smc update for two sets of particles .",
    "one with parameter : @xmath195 and the other with parameter : @xmath196 for both algorithms compute @xmath197 and @xmath198 to obtain the next parameter estimate , in the @xmath192dimension , take @xmath199 this algorithm does not require one to evaluate @xmath200 or its gradient .",
    "we refer the reader to @xcite and @xcite for a theoretical justification of this procedure .",
    "we consider two numerical examples that are designed to investigate the accuracy and behaviour of our numerical algorithms . in order to do this , we do not consider scenarios where @xmath125 is intractable .",
    "we consider the following linear gaussian hmm , with @xmath201 : @xmath202 with @xmath203 independent and @xmath204 , @xmath205 . in the subsequent examples",
    ", we will use a simulated dataset obtained with @xmath206 .",
    "we begin by considering a small data set , of @xmath207 data points .",
    "the offline scenario is the one for which we can expect the best possible performance of the abc - smc ; if we can not obtain reasonable parameter estimates in this scenario we would not expect abc to be useful in practice .",
    "we are concerned with obtaining offline abc - smc estimates @xmath208 where @xmath209 is the iteration , @xmath210 is the parameter estimate in the @xmath211-dimension , and @xmath212 is the @xmath211-entry of the bernoulli distributed vector . for the spsa stepsizes , we chose @xmath213 , @xmath214 for @xmath215 , and @xmath216 for @xmath217 .",
    "the iteration consists of running the abc - smc algorithm for 1000 data - points , with the current value of @xmath4 . in figure",
    "[ fig : lg_off ] , we compare offline estimates of the following cases :    a.   kalman filter ( kf ) with spsa b.   smc on the true model using @xmath218 , with spsa c.   abc - smc using @xmath219 , @xmath220 , @xmath221 , with spsa d.   maximum likelihood estimates ( mle ) from an offline grid search optimization .    in this particular test case , we can observe good relative performance of the abc - smc procedure , with regards to estimating parameters .",
    "this strong performance allows us to investigate a slightly more challenging scenario .",
    "we now consider a larger data set with @xmath222 data points , simulated with the previously indicated parameter values .",
    "we use the online spsa method described in section [ sec : spsa ] .",
    "the smc ( i.e.  on the true model ) and abc - smc algorithms were employed with the same @xmath29 ( and @xmath140 , @xmath91 for abc - smc ) as in the offline case , and the spsa sequences are similar to their offline forms , in section [ sec : lg_off ] .",
    "we ran fifty independent runs of the each algorithm considered in the previous section . in figure",
    "[ fig : lg_on ] , we plot the medians and credible intervals for the 25 - 75% and 5 - 95% percentiles of the parameter estimates ( across the independent runs ) .",
    "the @xmath223 converge after @xmath224 time steps , with the kf and smc yielding similarly valued estimates .",
    "we observe increased variance from left to right in figure [ fig : lg_on ] , which we attribute to the randomness of smc and abc - smc respectively .",
    "in particular , the expected reduced accuracy of abc - smc against smc is apparent , but , the bias does not appear to be substantial ( for abc - smc ) in this particular example .",
    "we now consider the following non - linear state - space model with @xmath225 .",
    "the original model is such that hidden process evolves deterministically according to the lorenz 63 system of ordinary differential equations , @xmath226 where we recall that the arguments @xmath227 are the @xmath228dimension at time @xmath229 ; where @xmath229 is continuous here .",
    "we modify the model to one such that the hidden process is a discrete - time markov chain with stochastic dynamics : @xmath230 where @xmath231 is the @xmath232-order approximation runge kutta solution to the lorenz 63 system , @xmath233 and @xmath234 is taken as known . here @xmath235 is used to represent the time - discretization .    for the observations",
    ": @xmath236 where @xmath237 , @xmath167 is independent of @xmath168 and @xmath238 is the cholesky root of a toeplitz matrix defined by the parameters @xmath239 and @xmath240 as follows : @xmath241 and @xmath242 when @xmath243 , @xmath244 and @xmath245 , a visualisation of the lorenz 63 ( hidden ) dynamics is shown in figure [ fig : state ] and the associated simulated dataset in [ fig : data ] .    for the simulated dataset in figure [ fig : data ]",
    ", we use abc - smc to obtain online parameter estimates for @xmath4 and we study the performance of these estimates under different settings .",
    "we will use @xmath246 to denote the estimate of @xmath4 at time @xmath3 , that was estimated using @xmath29 particles , @xmath140 pseudo - observations and a gaussian kernel with covariance @xmath247 . we will compare the behaviour of the algorithm as each of @xmath248 varies .",
    "we now examine the performance of the algorithm with @xmath249 .",
    "for each value of @xmath29 , we ran fifty independent runs of abc - smc , using @xmath220 and @xmath250 . in figures [ fig : l63_boxplot_kappa_n]-[fig : l63_boxplot_rho_n ] we plot boxplots of the terminal parameter estimates , @xmath251 , against their true values marked by dotted green lines . in figures [ fig : l63_biasvar_kappa_n]-[fig : l63_biasvar_rho_n ] we plot the absolute value of the monte carlo ( mc ) bias ( that is , the absolute difference between the estimate and true value ) , in red , and the mc standard deviation , in blue .",
    "the mc bias and standard deviation points are fitted with least - squares curves proportional to @xmath252 , the standard mc rates with which the accuracy of the estimates is expected to improve . with regards to the variability of the estimates one sees the expected reduction in variability as @xmath29 increases .",
    "the bias is harder to quantify ; it will not necessarily be the case that as @xmath29 grows the bias falls .",
    "this is because there is a monte carlo bias ( from the smc ) , an optimization bias ( from the spsa ) , an approximation bias ( from the abc ) and the fact that the data have been generated from the model ( so the true static parameters might not be exact ) .",
    "increasing @xmath29 can only deal with the smc bias ( which for estimates with parameters fixed is @xmath253 ) , but the addition of parameter estimation again does not make it easy to understand what happens here .",
    "the main point is simply as expected ; one obtains significantly more reproducible / consistent results as @xmath29 grows .",
    "+    next we look at the influence of the pseudo - observations . for @xmath254",
    ", we show in figures [ fig : l63_boxplot_kappa_m]-[fig : l63_boxplot_rho_m ] the boxplots of the terminal estimates @xmath255 from fifty independent runs of abc - smc , using @xmath256 and @xmath250 .",
    "the dotted green lines marks the true @xmath4 values which generate the data . in figures [ fig : l63_biasvar_kappa_m]-[fig : l63_biasvar_rho_m ] , the mc biases and the mc standard deviations of the @xmath255 are plotted point - wise , in red and blue , with lines of least squared - error fit to them .",
    "as @xmath140 increases , we see reductions in the mc variance .",
    "this reduction in variance can be attributed to the fact that the abc - smc algorithm approximates an algorithm that does not simulate the pseudo data ; hence by a rao - blackwellization argument , one expects a reduction in variance .",
    "these results are consistent with  @xcite .",
    "for this example , after @xmath257 , there seems to be little impact on the accuracy of the estimates ; it is not clear whether such performance occurs for other examples .",
    "+    we now vary @xmath3 ; for @xmath258 .",
    "we ran fifty independent runs of abc - smc using @xmath219 , @xmath220 , and @xmath250 , and plotted boxplots of the terminal estimates @xmath259 , in figures [ fig : l63_boxplot_kappa_n]-[fig : l63_boxplot_rho_n ] , against the true values of @xmath4 marked in dotted green lines .",
    "recall that recursive maximum likelihood estimation tries to maximise @xmath260 , so we expect @xmath3 not to have a great effect on the bias nor the variance ( also due to the bias results in section [ sec : result ] and the subsequent consistency results in @xcite ) .",
    "this is confirmed in figures [ fig : l63_biasvar_kappa_n]-[fig : l63_biasvar_rho_n ] , where the absolute value of the mc biases and the mc standard deviations have been plotted in red and blue , and fitted with linear lines of least squared - error .",
    "+    finally , we investigate the influence of @xmath261 . for each @xmath91",
    ", we again ran fifty independent runs of abc - smc with @xmath219 and @xmath220 , for the dataset @xmath244 .",
    "the boxplot of the parameter estimates are plotted , in figures [ fig : l63_boxplot_kappa_e]-[fig : l63_boxplot_rho_e ] , against dotted green lines which indicate the true @xmath4 .",
    "figures [ fig : l63_biasvar_kappa_e]-[fig : l63_biasvar_rho_e ] show the absolute value of mc biases in red , and the mc standard deviations in blue .",
    "fitted to the mc biases is a non - linear least squares curve proportional to @xmath262 .",
    "the result we presented in section [ sec : result ] states that as @xmath91 increases , the bias will increase on @xmath92 , hence the term proportional to @xmath91 of the fitted curve .",
    "however , the abc - smc algorithm becomes less stable for @xmath91 too small ( in the sense that , for example , the variance of the weights will become larger as @xmath91 grows ) , incurring more varied estimates and affected biases ; thus the term proportional to @xmath263 .",
    "fitted to the mc standard deviations is a non - linear least squares curves proportional to @xmath263 .",
    "for this example , the mc standard deviation decreases at this rate as @xmath91 increases .",
    "in this article we have presented a technique to perform online parameter estimation using abc - smc and spsa for hmms .",
    "this is useful for models where the state - dimension is high and the parameter and observations are of moderate dimension .",
    "in addition , it is required when the conditional density of the observations given the hidden state is intractable .",
    "some future work is as follows .",
    "the representation in remark [ rem : collapsed_abc ] can be potentially useful for alternative online parameter estimation techniques , other than using spsa .",
    "in @xcite we are investigating the use of the online em algorithm @xcite and any potential benefit that it may have over the ideas in this paper .",
    "we have remarked that one drawback of the smc algorithm implemented is its inability to deal with small @xmath91 .",
    "two potential ways to proceed are as follows .",
    "one is to introduce a further approximation by the expectation - propagation algorithm ( as in @xcite ) and potentially removing smc altogether .",
    "the other is to consider more advanced smc approaches such as @xcite and how this might help one reduce @xmath91 ; this is an area of ongoing research .",
    "we are also considering abc approximations in the scenario of deterministic dynamics for the hidden state ; these models have wide application in applied mathematics as filtering initial conditions of partial differential equations .",
    "the second author was funded by an moe grant and acknowledges useful conversations with david nott .",
    "we also acknowledge useful conversations with sumeetpal singh .",
    "we introduce a round of notations . as our analysis will rely upon that in @xcite our notations will follow that article . it is remarked that under our assumptions , one can establish the same assumptions as in @xcite .",
    "moreover , the time - inhomogenous upper - bounds in that paper can be made time - homogenous ( albeit less tight ) under our assumptions .",
    "in addition , our proof strategy follows ideas in @xcite .",
    "@xmath264 is the class of bounded and real - valued measurable functions on @xmath52 . throughout , for @xmath265 , @xmath266 . for @xmath265 and any operator @xmath267 , @xmath268 .",
    "in addition for @xmath269 , @xmath270 .",
    "we introduce the non - negative operator : @xmath271 with the abc equivalent @xmath272 , @xmath273 . to keep consistency with @xcite and to allow the reader to follow the proofs",
    ", we note that the filter at time @xmath274 , @xmath275 ( resp .",
    "abc filter , at time @xmath3 , @xmath276 ) is exactly , with initial distribution @xmath100 and test function @xmath265 @xmath277 resp .",
    "@xmath278 where @xmath279 , @xmath280 .",
    "in addition , we write the filter derivatives as @xmath281 , @xmath282 where the second argument is the gradient of the initial measure .",
    "the following operators will be used below , for @xmath121 : @xmath283\\label{eq : gtdefn}\\\\ \\widetilde{h}^n(\\mu_{\\theta})(\\varphi ) & : = &   f_{\\theta}^{n-1}(\\mu_{\\theta } ) r_{n,\\theta}(1)^{-1}[f_{\\theta}^{n-1}(\\mu_{\\theta})\\widetilde{r}_{n,\\theta}(\\varphi ) -   f_{\\theta}^{n-1}(\\mu_{\\theta})\\widetilde{r}_{n,\\theta}(1 ) f_{\\theta}^n(\\mu_{\\theta})(\\varphi ) ] \\label{eq : htdefn}\\end{aligned}\\ ] ] with the convention @xmath284 .",
    "in addition , we set @xmath285.\\ ] ] where @xmath286 .",
    "finally , an important notational convention is as follows . throughout",
    "we use @xmath287 to denote a constant whose value may change from line - to - line in the calculations .",
    "this constant will typically not depend upon important parameters such as @xmath91 and @xmath3 and any important dependencies will be highlighted .",
    "we begin with the equality @xmath288 with , for @xmath289 @xmath290 we will consider each summand in .",
    "the case @xmath291 is only considered ; the scenario @xmath292 will follow a similar and simpler argument .    using the inequality @xmath293 for every @xmath294 we have @xmath295 note that @xmath296 @xmath297 where we have applied ( a[hyp : like_bound ] ) and @xmath287 does not depend upon @xmath91 .",
    "thus we consider @xmath298 @xmath299 the r.h.s",
    ".  can be upper - bounded by the sum of @xmath300f_{\\theta}(x_k|x_{k-1 } ) f_{\\theta}^{k-1}(\\mu_{\\theta})(dx_{k-1})dx_k |\\ ] ] and @xmath301)dx_k|.\\ ] ] the first expression can be dealt with by using ( a[hyp : like_cont ] ) , which implies @xmath302 the second expression can be controlled by ( * ? ? ?",
    "* theorem 2 ) : @xmath303 to yield that @xmath304 one can thus conclude .",
    "we have that @xmath305 it then follows that @xmath306 @xmath307}{p_{\\theta}(y_k|y_{1:k-1 } ) } +   \\frac{\\nabla p_{\\theta,\\epsilon}(y_k|y_{1:k-1})}{p_{\\theta}(y_k|y_{1:k-1})p_{\\theta,\\epsilon}(y_k|y_{1:k-1})}[p_{\\theta,\\epsilon}(y_k|y_{1:k-1})-p_{\\theta}(y_k|y_{1:k-1 } ) ] \\bigg ) .",
    "\\label{eq : ll_main_decomp}\\ ] ] we will deal with the two terms on the r.h.s .  of in turn .",
    "the scenario @xmath291 is only considered ; the case @xmath292 follows a similar and simpler argument .    first starting with summand @xmath308}{p_{\\theta}(y_k|y_{1:k-1})}.\\ ] ] noting , we need only upper - bound the @xmath309 norm of the following expression",
    "@xmath310 @xmath311 @xmath312 we start with . using ( a[hyp :",
    "like_grad_cont ] ) we can establish that for each @xmath313 @xmath314 where @xmath287 does not depend upon @xmath315 . hence @xmath316f_{\\theta}(x_k|x_{k-1 } ) f_{\\theta}^{k-1}(\\mu_{\\theta})(dx_{k-1})dx_k| \\leq c \\epsilon.\\ ] ] then we note that by ( * ? ? ?",
    "* theorem 2 ) ( see ) and ( a[hyp : like_grad_bound ] ) @xmath317dx_k| \\leq c\\epsilon\\ ] ] thus we have shown that @xmath318 now , moving onto , by we have @xmath319\\nabla\\{f_{\\theta}(x_k|x_{k-1})\\ } f_{\\theta}^{k-1}(\\mu_{\\theta})(dx_{k-1})dx_k| \\leq c \\epsilon.\\ ] ] and can again use ( * ? ? ?",
    "* theorem 2 ) ( i.e.  ) to deduce that @xmath320dx_k| \\leq c\\epsilon\\ ] ] and thus that latexmath:[\\[| \\int_{\\mathsf{x}^2 } g_{\\theta}(y_k|x_k ) \\nabla\\{f_{\\theta}(x_k|x_{k-1})\\ } f_{\\theta}^{k-1}(\\mu_{\\theta})(dx_{k-1})dx_k -   \\int_{\\mathsf{x}^2 } g_{\\theta,\\epsilon}(y_k|x_k ) \\nabla\\{f_{\\theta}(x_k|x_{k-1})\\ } f_{\\theta,\\epsilon}^{k-1}(\\mu_{\\theta})(dx_{k-1})dx_k    onto , which upper - bounded by @xmath319f_{\\theta}(x_k|x_{k-1 } ) \\widetilde{f}_{\\theta}^{k-1}(\\mu_{\\theta},\\widetilde{\\mu_{\\theta}})(dx_{k-1})dx_k| + \\ ] ] @xmath322dx_k|.\\ ] ] for the first expression , we can write : @xmath323}{(\\sup_{x\\in\\mathsf{x}}|g_{\\theta}(y_k|x ) - g_{\\theta,\\epsilon}(y_k|x)|)}f_{\\theta}(x_k|x_{k-1})dx_k\\bigg ) \\widetilde{f}_{\\theta}^{k-1}(\\mu_{\\theta},\\widetilde{\\mu_{\\theta}})(dx_{k-1})|.\\ ] ] then we can apply and , noting that @xmath324}{(\\sup_{x\\in\\mathsf{x}}|g_{\\theta}(y_k|x ) - g_{\\theta,\\epsilon}(y_k|x)|)}f_{\\theta}(x_k|x_{k-1})dx_k\\bigg ) \\leq 1\\ ] ] one can also use lemma [ lem : filter_deriv_upper ] to deduce that @xmath319f_{\\theta}(x_k|x_{k-1 } ) \\widetilde{f}_{\\theta}^{k-1}(\\mu_{\\theta},\\widetilde{\\mu_{\\theta}})(dx_{k-1})dx_k| \\leq c(1+\\|\\widetilde{\\mu_{\\theta}}\\|)\\epsilon.\\ ] ] then , one can easily apply theorem [ theo : filt_deriv_bias ] to show that @xmath322dx_k| \\leq c(2+\\|\\widetilde{\\mu_{\\theta}}\\|)\\epsilon.\\ ] ] thus we have upper - bounded the @xmath73norm of the sum of the expressions - and we have established that @xmath308}{p_{\\theta}(y_k|y_{1:k-1 } ) } \\leq c(2+\\|\\widetilde{\\mu_{\\theta}}\\|)\\epsilon .",
    "\\label{eq : first_ll_bound}\\ ] ]    moving onto the second summand on the r.h.s .  of ,",
    "@xmath325 by , we need only consider upper - bounding , in @xmath309 , @xmath326 .",
    "this can be decomposed into the sum of three expressions : @xmath327 @xmath328 and @xmath329 as @xmath330 and @xmath331 are upper - bounded as well as @xmath52 being compact the first two expressions are upper - bounded in @xmath309 .",
    "in addition as @xmath332 is upper - bounded , we can apply lemma [ lem : filter_deriv_upper ] to see that the third expression is upper - bounded in @xmath309 .",
    "hence , we have shown that @xmath333\\bigg| \\leq c(1+\\|\\widetilde{\\mu_{\\theta}}\\|)\\epsilon\\label{eq : second_ll_bound}.\\ ] ] combining the results - and noting we can conclude .",
    "[ theo : filt_deriv_bias ] assume ( a1 - 5 ) .",
    "then there exist a @xmath120 such that for any @xmath121 , @xmath100 , @xmath104 , @xmath334 , @xmath108 : @xmath335    we have the following telescoping sum decomposition ( e.g.  @xcite ) for the differences in the filters , with @xmath265 : @xmath336\\ ] ] where we are using the notation @xmath337 , for @xmath338 .",
    "hence , taking gradients and swapping the order of summation and differentiation we have and omitting the second arguments of @xmath339 on the r.h.s .",
    "( to reduce the notational burden ) @xmath340,\\widetilde{f}_{\\theta}^{(n - p+1)}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})])(\\varphi ) -   \\nonumber \\\\ & &   \\widetilde{f}_{\\theta}^{n - p+2,n}(f_{\\theta,\\epsilon}^{(n - p+1)}[f_{\\theta,\\epsilon}^{(n - p)}(\\mu_{\\theta } ) ] , \\widetilde{f}_{\\theta,\\epsilon}^{(n - p+1)}[f_{\\theta,\\epsilon}^{(n - p)}(\\mu_{\\theta})])(\\varphi ) \\bigg ] \\label{eq : filter_deriv_main_decomp}.\\end{aligned}\\ ] ] to continue with the proof we will adopt ( * ? ? ? * lemma 6.4 ) : @xmath341 with @xmath342 and @xmath343 defined in - and @xmath344 similar extension to the notation as for the filter @xmath345 and the convention @xmath346 . returning to and again omitting the second arguments of @xmath339 on the r.h.s . : @xmath347 @xmath348,\\widetilde{f}_{\\theta,\\epsilon}^{(n - p+1)}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})]\\}(\\varphi)~+\\ ] ] @xmath349 , \\widetilde{h}_{\\theta}^{n - p+2,q}[f_{\\theta}^{(n - p+1)}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta } ) ) ] \\}(\\varphi)~-\\ ] ] @xmath350 , \\widetilde{h}_{\\theta}^{n - p+2,q}[f_{\\theta,\\epsilon}^{(n - p+1)}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))]\\}(\\varphi ) \\bigg\\ }",
    "\\bigg]\\label{eq : main_decomp_filter_+1}.\\ ] ]    we start first with the summand on the r.h.s .  of the second line of , which we compactly denote as : @xmath351,\\widetilde{f}_{\\theta}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})]\\}(\\varphi ) -   \\widetilde{g}_{\\theta}^{p-1}\\{f_{\\theta,\\epsilon}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})],\\widetilde{f}_{\\theta,\\epsilon}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})]\\}(\\varphi).\\ ] ] this can be decomposed further into the sum of @xmath351,\\widetilde{f}_{\\theta}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})]\\}(\\varphi ) -   \\widetilde{g}_{\\theta}^{p-1}\\{f_{\\theta,\\epsilon}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})],\\widetilde{f}_{\\theta}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})]\\}(\\varphi ) \\label{eq : filt_grad_new1}\\ ] ] and @xmath352,\\widetilde{f}_{\\theta}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})]\\}(\\varphi ) -   \\widetilde{g}_{\\theta}^{p-1}\\{f_{\\theta,\\epsilon}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})],\\widetilde{f}_{\\theta,\\epsilon}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})]\\}(\\varphi ) \\label{eq : filt_grad_new2}.\\ ] ] beginning with , by ( * ? ? ?",
    "* lemma 6.7 ) , equation ( 43 ) we have @xmath353,\\widetilde{f}_{\\theta}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})]\\}(\\varphi ) -   \\widetilde{g}_{\\theta}^{p-1}\\{f_{\\theta,\\epsilon}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})],\\widetilde{f}_{\\theta}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})]\\}(\\varphi)|\\ ] ] @xmath354-f_{\\theta,\\epsilon}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})]\\| \\|\\widetilde{f}_{\\theta}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})]\\|\\ ] ] where @xmath355 and @xmath287 do not depend upon @xmath356 or @xmath357 .",
    "applying lemma [ lem : abc_perturbation_filter ] we have @xmath353,\\widetilde{f}_{\\theta}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})]\\}(\\varphi ) -   \\widetilde{g}_{\\theta}^{p-1}\\{f_{\\theta,\\epsilon}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})],\\widetilde{f}_{\\theta}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})]\\}(\\varphi)|\\ ] ] @xmath358\\|\\ ] ] where @xmath287 does not depend upon @xmath103 , @xmath91 or @xmath357 .",
    "then by remark [ rem : filt_deriv_contr ] and lemma [ lem : filter_deriv_upper ] @xmath359\\|\\leq c(2+\\|\\widetilde{\\mu_{\\theta}}\\|)$ ] and thus the upper - bound on the @xmath73norm of : @xmath353,\\widetilde{f}_{\\theta}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})]\\}(\\varphi ) -   \\widetilde{g}_{\\theta}^{p-1}\\{f_{\\theta,\\epsilon}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})],\\widetilde{f}_{\\theta}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})]\\}(\\varphi)| \\leq c\\|\\varphi\\|_{\\infty}\\epsilon\\rho^{p-1}(2+\\|\\widetilde{\\mu_{\\theta}}\\| ) \\label{eq : filt_grad_new3}.\\ ] ] now , moving onto , by ( * ? ? ?",
    "* lemma 6.7 ) , equation ( 42 ) : @xmath360,\\widetilde{f}_{\\theta}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})]\\}(\\varphi ) -   \\widetilde{g}_{\\theta}^{p-1}\\{f_{\\theta,\\epsilon}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})],\\widetilde{f}_{\\theta,\\epsilon}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})]\\}(\\varphi)|\\ ] ] @xmath361 - \\widetilde{f}_{\\theta,\\epsilon}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})]\\|.\\ ] ] applying lemma [ lem : abc_filter_deriv_perturb ] @xmath360,\\widetilde{f}_{\\theta}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})]\\}(\\varphi ) -   \\widetilde{g}_{\\theta}^{p-1}\\{f_{\\theta,\\epsilon}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})],\\widetilde{f}_{\\theta,\\epsilon}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})]\\}(\\varphi)|\\ ] ] @xmath362 then by lemma [ lem : filter_deriv_upper ] , we deduce that @xmath360,\\widetilde{f}_{\\theta}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})]\\}(\\varphi ) -   \\widetilde{g}_{\\theta}^{p-1}\\{f_{\\theta,\\epsilon}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})],\\widetilde{f}_{\\theta,\\epsilon}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})]\\}(\\varphi)|   \\leq   c\\|\\varphi\\|_{\\infty}\\epsilon \\rho^{p-1 } ( 2 + \\|\\widetilde{\\mu_{\\theta}}\\|)\\label{eq : filt_grad_new4}.\\ ] ] combining and @xmath353,\\widetilde{f}_{\\theta}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})]\\}(\\varphi ) -   \\widetilde{g}_{\\theta}^{p-1}\\{f_{\\theta,\\epsilon}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})],\\widetilde{f}_{\\theta,\\epsilon}[f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta})]\\}(\\varphi)| \\leq c\\|\\varphi\\|_{\\infty}\\epsilon \\rho^{p-1}(2 + \\|\\widetilde{\\mu_{\\theta}}\\| ) \\label{eq : filt_grad_new5}.\\ ] ]    we now consider the summands over @xmath363 in the second and third lines of .",
    "again , adopting the compact notation above we can decompose the summands over @xmath363 into the sum of @xmath364,\\widetilde{h}_{\\theta}^{s}[f_{\\theta}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))]\\}(\\varphi ) -   \\widetilde{g}_{\\theta}^{n - q}\\{f_{\\theta}^s [ f_{\\theta,\\epsilon}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))],\\widetilde{h}_{\\theta}^{s}[f_{\\theta}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))]\\}(\\varphi ) \\label{eq : g1_filt_deriv}\\ ] ] and @xmath365,\\widetilde{h}_{\\theta}^{s}[f_{\\theta}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))]\\}(\\varphi )   -   \\widetilde{g}_{\\theta}^{n - q}\\{f_{\\theta}^s [ f_{\\theta,\\epsilon}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))],\\widetilde{h}_{\\theta}^s[f_{\\theta,\\epsilon}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))]\\}(\\varphi ) \\label{eq : g2_filt_deriv}\\ ] ] where @xmath366 .",
    "we start with ; by ( * ? ? ?",
    "* lemma 6.7 ) equation ( 43 ) , we have @xmath367,\\widetilde{h}_{\\theta}^{s}[f_{\\theta}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))]\\}(\\varphi ) -   \\widetilde{g}_{\\theta}^{n - q}\\{f_{\\theta}^s [ f_{\\theta,\\epsilon}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))],\\widetilde{h}_{\\theta}^{s}[f_{\\theta}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))]\\}(\\varphi)|\\ ] ] @xmath368-f_{\\theta}^s [ f_{\\theta,\\epsilon}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))]\\| \\|\\widetilde{h}_{\\theta}^{s}[f_{\\theta}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))\\|.\\ ] ] then we will use the stability of the filter ( e.g.  ( * ? ? ? * theorem 3.1 ) ) @xmath369-f_{\\theta}^s [ f_{\\theta,\\epsilon}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))]\\| \\leq c\\rho^s \\|f_{\\theta}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))-f_{\\theta,\\epsilon}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))\\|.\\ ] ] by lemma [ lem : abc_perturbation_filter ] @xmath370 and thus @xmath367,\\widetilde{h}_{\\theta}^{s}[f_{\\theta}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))]\\}(\\varphi ) -   \\widetilde{g}_{\\theta}^{n - q}\\{f_{\\theta}^s [ f_{\\theta,\\epsilon}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))],\\widetilde{h}_{\\theta}^{s}[f_{\\theta}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))]\\}(\\varphi)|\\ ] ] @xmath371\\|.\\ ] ] by ( * ? ? ?",
    "* lemma 6.8 ) we have @xmath372\\|\\leq c$ ] , where @xmath287 does not depend upon @xmath373 or @xmath91 and hence @xmath367,\\widetilde{h}_{\\theta}^{s}[f_{\\theta}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))]\\}(\\varphi ) -   \\widetilde{g}_{\\theta}^{n - q}\\{f_{\\theta}^s [ f_{\\theta,\\epsilon}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))],\\widetilde{h}_{\\theta}^{s}[f_{\\theta}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))]\\}(\\varphi)| \\leq c\\|\\varphi\\|_{\\infty}\\epsilon \\rho^{p-1}.\\ ] ] now , turning to and applying ( * ? ? ?",
    "* lemma 6.7 ) ( 42 ) we have @xmath374,\\widetilde{h}_{\\theta}^{s}[f_{\\theta}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))]\\}(\\varphi )   -   \\widetilde{g}_{\\theta}^{n - q}\\{f_{\\theta}^s [ f_{\\theta,\\epsilon}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))],\\widetilde{h}_{\\theta}^s[f_{\\theta,\\epsilon}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))]\\}(\\varphi)|\\ ] ] @xmath375-\\widetilde{h}_{\\theta}^s[f_{\\theta,\\epsilon}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))]\\|\\label{eq : filt_grad_new_new}.\\ ] ] then by ( * ? ? ?",
    "* lemma 6.8 ) we have @xmath376-\\widetilde{h}_{\\theta}^s[f_{\\theta,\\epsilon}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))]\\| \\leq c\\rho^{s } \\|f_{\\theta}(f_{\\theta,\\epsilon}^{n - p})(\\mu_{\\theta } ) - f_{\\theta,\\epsilon}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))\\|\\ ] ] and then on applying lemma [ lem : abc_perturbation_filter ] we thus have that @xmath377 returning to , it follows by the above calculations that : @xmath367,\\widetilde{h}_{\\theta}^{s}[f_{\\theta}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))]\\}(\\varphi ) -",
    "\\widetilde{g}_{\\theta}^{n - q}\\{f_{\\theta}^s [ f_{\\theta,\\epsilon}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))],\\widetilde{h}_{\\theta}^{s}[f_{\\theta}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))]\\}(\\varphi)| \\leq c\\|\\varphi\\|_{\\infty}\\epsilon \\rho^{p-1}.\\ ] ] thus we have proved that @xmath367,\\widetilde{h}_{\\theta}^{s}[f_{\\theta}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))]\\}(\\varphi)- \\widetilde{g}_{\\theta}^{n - q}\\{f_{\\theta}^s [ f_{\\theta,\\epsilon}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))],\\widetilde{h}_{\\theta}^s[f_{\\theta,\\epsilon}(f_{\\theta,\\epsilon}^{n - p}(\\mu_{\\theta}))]\\}(\\varphi)|| \\leq c\\|\\varphi\\|_{\\infty}\\epsilon \\rho^{p-1}\\label{eq : filt_grad_new_new1}.\\ ] ]            * control of * @xmath382 .",
    "we will use the hahn - jordan decomposition : @xmath384 .",
    "it is assumed that both @xmath385 .",
    "the scenario with either @xmath386 or @xmath386 is straightforward and omitted for brevity .",
    "we can write : @xmath387   +   \\frac{\\widetilde{\\mu_{\\theta}}^- r_{n,\\theta}(1)}{\\mu_{\\theta }",
    "r_{n,\\theta}(1 ) } [ f_{\\theta}^{(n)}(\\widetilde{\\bar{\\mu_{\\theta}}}^-)(\\varphi ) - f_{\\theta}^{(n)}(\\mu_{\\theta})(\\varphi)]\\ ] ] where @xmath388 and @xmath389 .",
    "thus we have @xmath390 [ f_{\\theta}^{(n)}(\\widetilde{\\bar{\\mu_{\\theta}}}^+)(\\varphi ) - f_{\\theta}^{(n)}(\\mu_{\\theta})(\\varphi)]\\nonumber\\\\ & & + \\frac{\\widetilde{\\mu_{\\theta}}^+r_{n,\\theta,\\epsilon}(1)}{\\mu_{\\theta } r_{n,\\theta,\\epsilon}(1 ) } [ f_{\\theta}^{(n)}(\\widetilde{\\bar{\\mu_{\\theta}}}^+)(\\varphi ) - f_{\\theta}^{(n)}(\\mu_{\\theta})(\\varphi ) - f_{\\theta,\\epsilon}^{(n)}(\\widetilde{\\bar{\\mu_{\\theta}}}^+)(\\varphi ) + f_{\\theta,\\epsilon}^{(n)}(\\mu_{\\theta})(\\varphi ) ] \\nonumber\\\\ & & +   \\bigg[\\frac{\\widetilde{\\mu_{\\theta}}^-r_{n,\\theta}(1)}{\\mu_{\\theta } r_{n,\\theta}(1 ) } -   \\frac{\\widetilde{\\mu_{\\theta}}^-r_{n,\\theta,\\epsilon}(1)}{\\mu_{\\theta } r_{n,\\theta,\\epsilon}(1)}\\bigg ] [ f_{\\theta}^{(n)}(\\widetilde{\\bar{\\mu_{\\theta}}}^-)(\\varphi ) - f_{\\theta}^{(n)}(\\mu_{\\theta})(\\varphi)]\\nonumber\\\\ & & + \\frac{\\widetilde{\\mu_{\\theta}}^-r_{n,\\theta,\\epsilon}(1)}{\\mu_{\\theta } r_{n,\\theta,\\epsilon}(1 ) } [ f_{\\theta}^{(n)}(\\widetilde{\\bar{\\mu_{\\theta}}}^-)(\\varphi ) - f_{\\theta}^{(n)}(\\mu_{\\theta})(\\varphi ) - f_{\\theta,\\epsilon}^{(n)}(\\widetilde{\\bar{\\mu_{\\theta}}}^-)(\\varphi ) + f_{\\theta,\\epsilon}^{(n)}(\\mu_{\\theta})(\\varphi)]\\label{eq : f - mu - epsilon - main}.\\end{aligned}\\ ] ] by symmetry , we need only consider the terms including @xmath391 ; one can treat those with @xmath392 by using similar arguments .",
    "first dealing with term on the first line of the r.h.s .",
    "we have that @xmath393 [ f_{\\theta}^{(n)}(\\widetilde{\\bar{\\mu_{\\theta}}}^+)(\\varphi ) - f_{\\theta}^{(n)}(\\mu_{\\theta})(\\varphi ) ]   = \\ ] ] @xmath394 [ f_{\\theta}^{(n)}(\\widetilde{\\bar{\\mu_{\\theta}}}^+)(\\varphi ) - f_{\\theta}^{(n)}(\\mu_{\\theta})(\\varphi)]\\ ] ] now by ( a[hyp : like_cont ] ) , for any @xmath3 @xmath395 thus @xmath394 \\leq \\frac{c\\epsilon\\widetilde{\\mu_{\\theta}}^+(1)}{\\mu_{\\theta } r_{n,\\theta}(1 ) } + c\\epsilon\\frac{\\widetilde{\\mu_{\\theta}}^+r_{n,\\theta,\\epsilon}(1)}{\\mu_{\\theta } r_{n,\\theta,\\epsilon}(1)\\mu_{\\theta } r_{n,\\theta}(1)}.\\ ] ] now one can show that there exist a @xmath120 such that for any @xmath396 @xmath397 then it follows that @xmath398 hence we have shown that @xmath399 [ f_{\\theta}^{(n)}(\\widetilde{\\mu_{\\theta}}^+)(\\varphi ) - f_{\\theta}^{(n)}(\\mu_{\\theta})(\\varphi ) ] \\leq c\\|\\varphi\\|_{\\infty}\\epsilon\\widetilde{\\mu_{\\theta}}^+(1).\\ ] ] second , the second line of the r.h.s .  of .",
    "by lemma [ lem : abc_perturbation_filter ] , for any @xmath100 , @xmath400 , with @xmath287 independent of @xmath103 , and in addition using we have @xmath401 \\leq c\\|\\varphi\\|_{\\infty}\\epsilon \\widetilde{\\mu_{\\theta}}^+(1).\\ ] ] thus we have shown : @xmath402 = c\\epsilon\\|\\widetilde{\\mu_{\\theta}}\\|.\\label{eq : g_cont_ineq}\\ ] ]    * control of * @xmath383 .",
    "we have @xmath403 + \\bigg [ \\frac{\\mu_{\\theta } \\widetilde{r}_{n,\\theta,\\epsilon}(1 ) f_{\\theta,\\epsilon}^{(n)}(\\mu_{\\theta})(\\varphi)}{\\mu_{\\theta } r_{n,\\theta,\\epsilon}(1 ) } -   \\frac{\\mu_{\\theta } \\widetilde{r}_{n,\\theta}(1 ) f_{\\theta}^{(n)}(\\mu_{\\theta})(\\varphi)}{\\mu_{\\theta } r_{n,\\theta}(1 ) } \\bigg]\\label{eq : h_main_decomp}.\\ ] ] we start with the first bracket on the r.h.s .  of .",
    "we first note that @xmath404 dx ' \\leq c \\|\\varphi\\|_{\\infty}\\epsilon \\label{eq : rtilde_ineq}\\ ] ] where we have applied .",
    "then we have @xmath405 by using on the first term on the r.h.s .  of the above equation and by using in the numerator for the second , along with in the denominator , we have @xmath406.\\ ] ] then as @xmath407dx ' \\leq c \\|\\varphi\\|_{\\infty}\\int_{\\mathsf{x}}dx ' \\leq c \\|\\varphi\\|_{\\infty } \\label{eq : bound_r_tilde}\\ ] ] where the compactness of @xmath52 and ( a[hyp : like_grad_bound ] ) have been used , we have the upper - bound @xmath408 moving onto the second bracket on the r.h.s .  of , this is equal to @xmath409 f_{\\theta,\\epsilon}^{(n)}(\\mu_{\\theta})(\\varphi ) + \\frac{\\mu_{\\theta } \\widetilde{r}_{n,\\theta}(1)}{\\mu_{\\theta } r_{n,\\theta}(1 ) } [ f_{\\theta,\\epsilon}^{(n)}(\\mu_{\\theta})(\\varphi)-f_{\\theta}^{(n)}(\\mu_{\\theta})(\\varphi)]\\ ] ] by using the inequality , we have @xmath409 f_{\\theta,\\epsilon}^{(n)}(\\mu_{\\theta})(\\varphi ) \\leq c\\epsilon |f_{\\theta,\\epsilon}^{(n)}(\\mu_{\\theta})(\\varphi)| \\leq c\\|\\varphi\\|_{\\infty}\\epsilon.\\ ] ] using lemma [ lem : abc_perturbation_filter ] and in addition using in the denominator and in the numerator we have @xmath410 \\leq c\\|\\varphi\\|_{\\infty}\\epsilon\\ ] ] where @xmath287 does not depend upon @xmath103 and @xmath91 .",
    "thus we have established that @xmath411 one can put together the results of and and establish that @xmath412 on combining the results and and noting we conclude the proof .",
    "we will consider only @xmath419 as the abc filter derivative will follow similar calculations , for any @xmath0 ( with upper - bounds that are independent of @xmath91 ) . by",
    "* lemma 6.4 ) we have for @xmath265 @xmath420 by ( * ? ?",
    "* lemma 6.6 ) we have the upper - bound @xmath421 with @xmath355 .",
    "then by ( * ? ? ?",
    "* lemma 6.8 ) , it follows that @xmath422 from which one concludes .",
    ", p. , li , b. & bengtsson , t.  ( 2008 ) .",
    "sharp failure rates for the bootstrap particle filter in high dimensions . in _ pushing the limits of contemporary statistics _ , b. clarke & s. ghosal , eds , 318329 , ims ."
  ],
  "abstract_text": [
    "<S> in this article we focus on maximum likelihood estimation ( mle ) for the static parameters of hidden markov models ( hmms ) . we will consider the case where one can not or does not want to compute the conditional likelihood density of the observation given the hidden state because of increased computational complexity or analytical intractability . </S>",
    "<S> instead we will assume that one may obtain samples from this conditional likelihood and hence use approximate bayesian computation ( abc ) approximations of the original hmm . </S>",
    "<S> abc approximations are biased , but the bias can be controlled to arbitrary precision via a parameter @xmath0 ; the bias typically goes to zero as @xmath1 . </S>",
    "<S> we first establish that the bias in the log - likelihood and gradient of the log - likelihood of the abc approximation , for a fixed batch of data , is no worse than @xmath2 , @xmath3 being the number of data ; hence , for computational reasons , one might expect reasonable parameter estimates using such an abc approximation . turning to the computational problem of estimating @xmath4 , </S>",
    "<S> we propose , using the abc - sequential monte carlo ( smc ) algorithm in @xcite , an approach based upon simultaneous perturbation stochastic approximation ( spsa ) . </S>",
    "<S> our method is investigated on two numerical examples . + </S>",
    "<S> * key - words * : approximate bayesian computation , hidden markov models , parameter estimation , sequential monte carlo    * static parameter estimation for abc approximations of hidden markov models *    by elena ehrlich@xmath5 , ajay jasra@xmath6 & nikolas kantas@xmath7    @xmath5department of mathematics , imperial college london , london , sw7 2az , uk . </S>",
    "<S> + e-mail:`elena.ehrlich05@ic.ac.uk ` + @xmath6department of statistics & applied probability , national university of singapore , singapore , 117546 , sg . </S>",
    "<S> + e-mail:`staja@nus.edu.sg ` + @xmath7department of statistical science , university college london , london , wc1e 6bt , uk . </S>",
    "<S> + e-mail:`n.kantas@ucl.ac.uk ` </S>"
  ]
}