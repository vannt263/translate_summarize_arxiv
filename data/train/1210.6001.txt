{
  "article_text": [
    "binary classification is one of the most well - understood problems of machine learning and statistics : a wealth of efficient classification algorithms has been developed and applied to a wide range of applications .",
    "perhaps one of the reasons for this is that binary classification is conceptually one of the simplest statistical learning problems .",
    "it is thus natural to try and use it as a building block for solving other , more complex , newer or just different problems ; in other words , one can try to obtain efficient algorithms for different learning problems by reducing them to binary classification .",
    "this approach has been applied to many different problems , starting with multi - class classification , and including regression and ranking @xcite , to give just a few examples .",
    "however , all of these problems are formulated in terms of independent and identically distributed ( i.i.d . )",
    "this is also the assumption underlying the theoretical analysis of most of the classification algorithms .    in this work we consider learning problems that concern time - series data for which independence assumptions do not hold",
    ". the series can exhibit arbitrary long - range dependence , and different time - series samples may be interdependent as well .",
    "moreover , the learning problems that we consider   the three - sample problem , time - series clustering , and homogeneity testing   at first glance seem completely unrelated to classification .",
    "we show how the considered problems can be reduced to binary classification methods .",
    "the results include asymptotically consistent algorithms , as well as finite - sample analysis . to establish the consistency of the suggested methods , for clustering and the three - sample problem",
    "the only assumption that we make on the data is that the distributions generating the samples are stationary ergodic ; this is one of the weakest assumptions used in statistics . for homogeneity testing",
    "we have to make some mixing assumptions in order to obtain consistency results ( this is indeed unavoidable @xcite ) .",
    "mixing conditions are also used to obtain finite - sample performance guarantees for the first two problems .",
    "the proposed approach is based on a new distance between time - series distributions ( that is , between probability distributions on the space of infinite sequences ) , which we call _ telescope distance_. this distance can be evaluated using binary classification methods , and its finite - sample estimates are shown to be asymptotically consistent .",
    "three main building blocks are used to construct the telescope distance .",
    "the first one is a distance on finite - dimensional marginal distributions .",
    "the distance we use for this is the following : @xmath0 where @xmath1 are distributions and @xmath2 is a set of functions .",
    "this distance can be estimated using binary classification methods , and thus can be used to reduce various statistical problems to the classification problem .",
    "this distance was previously applied to such statistical problems as homogeneity testing and change - point estimation @xcite . however",
    ", these applications so far have only concerned i.i.d .",
    "data , whereas we want to work with highly - dependent time series .",
    "thus , the second building block are the recent results of @xcite , that show that empirical estimates of @xmath3 are consistent ( under certain conditions on @xmath2 ) for arbitrary stationary ergodic distributions .",
    "this , however , is not enough : evaluating @xmath3 for ( stationary ergodic ) time - series distributions means measuring the distance between their finite - dimensional marginals , and not the distributions themselves .",
    "finally , the third step to construct the distance is what we call _ telescoping_. it consists in summing the distances for all the ( infinitely many ) finite - dimensional marginals with decreasing weights .",
    "we show that the resulting distance ( telescope distance ) indeed can be consistently estimated based on sampling , for arbitrary stationary ergodic distributions .",
    "further , we show how this fact can be used to construct consistent algorithms for the considered problems on time series .",
    "thus we can harness binary classification methods to solve statistical learning problems concerning time series . to illustrate the theoretical results in an experimental setting , we chose the problem of time - series clustering , since it is a difficult unsupervised problem which seems most different from the problem of binary classification .",
    "experiments on both synthetic and real - world data are provided .",
    "the real - world setting concerns brain - computer interface ( bci ) data , which is a notoriously challenging application , and on which the presented algorithm demonstrates competitive performance . a related approach to address the problems considered here , as well some related problems about stationary ergodic time series ,",
    "is based on ( consistent ) empirical estimates of the distributional distance , see @xcite and @xcite about the distributional distance .",
    "the empirical distance is based on counting frequencies of bins of decreasing sizes and `` telescoping . ''",
    "a similar telescoping trick is used in different problems , e.g.  sequence prediction @xcite . another related approach to time - series analysis involves a different reduction , namely , that to data compression @xcite .",
    "* organisation .",
    "* section  [ s : def ] is preliminary . in section  [ s : dist ]",
    "we introduce and discuss the telescope distance .",
    "section  [ s : red ] explains how this distance can be calculated using binary classification methods .",
    "sections  [ s : tsc ] and  [ s : clust ] are devoted to the three - sample problem and clustering , respectively . in section",
    "[ s : speed ] , under some mixing conditions , we address the problems of homogeneity testing , clustering with unknown @xmath4 , and finite - sample performance guarantees .",
    "section  [ s : exp ] presents experimental evaluation .",
    "let @xmath5 be a measurable space ( the domain ) , and denote @xmath6 and @xmath7 the product probability space over @xmath8 and the induced probability space over the one - way infinite sequences taking values in @xmath9 .",
    "time - series ( or process ) distributions are probability measures on the space @xmath10 .",
    "we use the abbreviation @xmath11 for @xmath12 .",
    "a set @xmath2 of functions is called _ separable _ if there is a countable set @xmath13 of functions such that any function in @xmath2 is a pointwise limit of a sequence of elements of @xmath13 .",
    "a distribution @xmath14 is stationary if @xmath15 for all @xmath16 , @xmath17 .",
    "a stationary distribution is called ( stationary ) ergodic if @xmath18 @xmath14-a.s . for every @xmath16 , @xmath19 .",
    "( this definition , which is more suited for the purposes of this work , is equivalent to the usual one expressed in terms of invariant sets , see , e.g. , @xcite . )",
    "we start with a distance between distributions on @xmath9 , and then we will extend it to distributions on @xmath20 . for two probability distributions @xmath21 and @xmath22  on @xmath23 and a set @xmath2 of measurable functions on @xmath9 , one can define the distance @xmath24 this metric has been studied since at least @xcite ; its special cases include kolmogorov - smirnov @xcite , kantorovich - rubinstein @xcite and fortet - mourier @xcite metrics .",
    "note that the distance function so defined may not be measurable ; however , it is measurable under mild conditions which we assume when necessary .",
    "in particular , separability of @xmath2 is a sufficient condition ( separability is required in most of the results below )",
    ".    we will be interested in the cases where @xmath25 implies @xmath26 .",
    "note that in this case @xmath3 is a metric ( the rest of the properties are easy to see ) . for reasons that will become apparent shortly ( see remark below )",
    ", we will be mainly interested in the sets @xmath2 that consist of indicator functions . in this case",
    "we can identify each @xmath27 with the indicator set @xmath28 and ( by a slight abuse of notation ) write @xmath29 in this case it is easy to check that the following statement holds true .",
    "@xmath3 is a metric on the space of probability distributions over @xmath9 if and only if @xmath2 generates  @xmath30 .",
    "the property that @xmath2 generates  @xmath30 is often easy to verify directly .",
    "first of all , it trivially holds for the case where @xmath2 is the set of halfspaces in a euclidean @xmath9 .",
    "it is also easy to check that it holds if @xmath2 is the set of halfspaces in the feature space of most commonly used kernels ( provided the feature space is of the same or higher dimension than the input space ) , such as polynomial and gaussian kernels .",
    "based on @xmath3 we can construct a distance between time - series probability distributions . for two time - series distributions @xmath31",
    "we take the @xmath3 between @xmath4-dimensional marginal distributions of @xmath32 and @xmath33 for each @xmath19 , and sum them all up with decreasing weights .",
    "[ d : tele ] for two time series distributions @xmath32 and @xmath33 on the space @xmath34 and a sequence of sets of functions @xmath35 define the _ telescope distance _",
    "@xmath36 where @xmath37 , @xmath19 is a sequence of positive summable real weights ( e.g. , @xmath38 or @xmath39 ) .    [",
    "th : m ] @xmath40 is a metric if and only if @xmath41 is a metric for every @xmath19 .",
    "the statement follows from the fact that two process distributions are the same if and only if all their finite - dimensional marginals coincide .",
    "[ d : etele ] for a pair of samples @xmath42 and @xmath43 define _ empirical telescope distance _ as @xmath44    all the methods presented in this work are based on the empirical telescope distance .",
    "the key fact is that it is an asymptotically consistent estimate of the telescope distance , that is , the latter can be consistently estimated based on sampling .",
    "[ th : cons ] let @xmath45 be a sequence of separable sets @xmath46 of indicator functions ( over @xmath8 ) of finite vc dimension such that @xmath46 generates @xmath47 .",
    "then , for every stationary ergodic time series distributions @xmath48 and @xmath49 generating samples @xmath42 and @xmath43 we have @xmath50    note that @xmath51 is a biased estimate of @xmath52 , and , unlike in the i.i.d .",
    "case , the bias may depend on the distributions ; however , the bias is @xmath53 .",
    "[ rem : ind ] the condition that the sets @xmath46 are sets of indicator function of finite vc dimension comes from @xcite , where it is shown that for any stationary ergodic distribution @xmath14 , under these conditions , @xmath54 is an asymptotically consistent estimate of @xmath55 .",
    "this fact implies that @xmath41 can be consistently estimated , from which the theorem is derived .    as established in @xcite , under the conditions of the theorem we have @xmath56 for all @xmath19 , and likewise for @xmath49 .",
    "fix an @xmath57 .",
    "we can find a @xmath58 such that @xmath59 note that @xmath60 depends only on @xmath61 .",
    "moreover , as follows from  ( [ eq : ada ] ) , for each @xmath62 we can find an @xmath63 such that @xmath64 let @xmath65 and define analogously @xmath66 for  @xmath49 .",
    "thus , for @xmath67 , @xmath68 we have @xmath69 where the first inequality follows from the definition   of @xmath70 and from  ( [ eq : t ] ) , and the last inequality follows from  ( [ eq : ada3 ] ) .",
    "since @xmath61 was chosen arbitrary the statement follows .",
    "the methods for solving various statistical problems that we suggest are all based on @xmath70 .",
    "the main appeal of this approach is that @xmath70 can be calculated using binary classification methods .",
    "here we explain how to do it .",
    "the definition  ( [ eq : ets ] ) of @xmath71 involves calculating @xmath72 summands ( where @xmath73 ) , that is @xmath74 for each @xmath75 . assuming that @xmath76 are indicator functions , calculating each of the summands amounts to solving the following @xmath4-dimensional binary classification problem .",
    "consider @xmath77 , @xmath78 as class-1 examples and @xmath79 , @xmath80 as class-0 examples .",
    "the supremum  ( [ eq : sup ] ) is attained on @xmath76 that minimizes the empirical risk , with examples weighted with respect to the sample size .",
    "indeed , we can define the weighted empirical risk of any @xmath76 as @xmath81 which is obviously minimized by any @xmath76 that attains  ( [ eq : sup ] ) .",
    "thus , as long as we have a way to find @xmath76 that minimizes empirical risk , we have a consistent estimate of @xmath82 , under the mild conditions on @xmath83 required by theorem  [ th : cons ] .",
    "since the dimension of the resulting classification problems grows with the length of the sequences , one should prefer methods that work in high dimensions , such as soft - margin svms @xcite .",
    "a particularly remarkable feature is that _ the choice of @xmath46 is much easier _ for the problems that we consider _ than in the binary classification _ problem . specifically , if ( for some fixed @xmath4 ) the classifier that achieves the minimal ( bayes ) error for the classification problem is not in @xmath46 , then obviously the error of an empirical risk minimizer will not tend to zero , no matter how much data we have .",
    "in contrast , all we need to achieve asymptotically 0 error in estimating @xmath84 ( and therefore , in the learning problems considered below ) is that the sets @xmath46 generate @xmath85 and have a finite vc dimension ( for each @xmath4 ) .",
    "this is the case already for the set of half - spaces in @xmath86 . in other words ,",
    "the _ approximation _ error of the binary classification method ( the classification error of the best @xmath87 in @xmath46 ) is not important .",
    "what is important is the estimation error ; for asymptotic consistency results it has to go to 0 ( hence the requirement on the vc dimension ) ; for non - asymptotic results , it will appear in the error bounds , see section  [ s : speed ] .",
    "thus , we have the following statement .",
    "the approximation error @xmath88 , and thus the error of the algorithms below , can be much smaller than the error of classification algorithms used to calculate @xmath89 .",
    "we can conclude that , beyond the requirement that @xmath46 generate @xmath85 for each @xmath19 , the choice of @xmath90 ( or , say , of the kernel to use in svm ) is entirely up to the needs and constraints of specific applications .    finally",
    ", we remark that while in the definition of the empirical distributional distance  ( [ eq : ets ] ) the number of summands is @xmath72 ( the length of the shorter of the two samples ) , it can be replaced with any @xmath91 such that @xmath92 , without affecting any asymptotic consistency results . in other words ,",
    "theorem  [ th : cons ] , as well as all the consistency statements below , hold true for @xmath72 replaced with any function @xmath93 that increases to infinity .",
    "a practically viable choice is @xmath94 ; in fact , there is no reason to choose faster growing @xmath95 since the estimates for higher - order summands will not have enough data to converge .",
    "this is also the value we use in the experiments .",
    "we start with a conceptually simple problem known in statistics as the three - sample problem ( some times also called time - series classification ) .",
    "we are given three samples @xmath96 , @xmath97 and @xmath98 .",
    "it is known that @xmath99 and @xmath100 were generated by different time - series distributions , whereas @xmath101 was generated by the same distribution as either @xmath99 or @xmath100 .",
    "it is required to find out which one is the case .",
    "both distributions are assumed to be stationary ergodic , but no further assumptions are made about them ( no independence , mixing or memory assumptions ) .",
    "the three sample - problem for dependent time series has been addressed in @xcite for markov processes and in @xcite for stationary ergodic time series .",
    "the latter work uses an approach based on the distributional distance . indeed , to solve this problem it suffices to have consistent estimates of some distance between time series distributions .",
    "thus , we can use the telescope distance .",
    "the following statement is a simple corollary of theorem  [ th : cons ] .",
    "[ th : cl ] let the samples @xmath96 , @xmath97 and @xmath98 be generated by stationary ergodic distributions @xmath102 and @xmath103 , with @xmath104 and either ( i ) @xmath105 or ( ii ) @xmath106 . let the sets @xmath46 , @xmath19 be separable sets of indicator functions over @xmath8 .",
    "assume that each set @xmath46 , @xmath19 has a finite vc dimension and generates @xmath47 .",
    "a test that declares that ( i ) is true if @xmath107 and that ( ii ) is true otherwise , makes only finitely many errors with probability  1 as @xmath108 .",
    "it is straightforward to extend this theorem to more than two classes ; in other words , instead of @xmath99 and @xmath100 one can have an arbitrary number of samples from different stationary ergodic distributions",
    ". a further generalization of this problem is the problem of time - series clustering , considered in the next section .",
    "we are given @xmath109 time - series samples @xmath110 , and it is required to cluster them into @xmath111 groups , where , in different settings , @xmath111 may be either known or unknown . while there may be many different approaches to define what should be considered a good clustering , and , thus , what it means to have a consistent clustering algorithm , for the problem of clustering time - series samples",
    "there is a natural choice , proposed in @xcite : assume that each of the time - series samples @xmath110 was generated by one out of @xmath111 different time - series distributions @xmath112 .",
    "these distributions are unknown .",
    "the _ target clustering _ is defined according to whether the samples were generated by the same or different distributions : the samples belong to the same cluster if and only if they were generated by the same distribution .",
    "a clustering algorithm is called _ asymptotically consistent _ if with probability 1 from some @xmath113 on it outputs the target clustering , where @xmath113 is the length of the shortest sample @xmath114 .    again , to solve this problem",
    "it is enough to have a metric between time - series distributions that can be consistently estimated .",
    "our approach here is based on the telescope distance , and thus we use  @xmath84 .",
    "the clustering problem is relatively simple if the target clustering has what is called the _ strict separation property _",
    "@xcite : every two points in the same target cluster are closer to each other than to any point from a different target cluster .",
    "the following statement is an easy corollary of theorem  [ th : cons ] .",
    "[ th : ss ] let the sets @xmath46 , @xmath19 be separable sets of indicator functions over @xmath8 .",
    "assume that each set @xmath46 , @xmath19 has a finite vc dimension and generates @xmath47 .",
    "if the distributions @xmath112 generating the samples @xmath110 are stationary ergodic , then with probability 1 from some @xmath115 on the target clustering has the strict separation property with respect to  @xmath51 .    with the strict separation property at hand , if the number of clusters @xmath111 is known , it is easy to find asymptotically consistent algorithms .",
    "here we give some simple examples , but the theorem below can be extended to many other distance - based clustering algorithms .",
    "the _ average linkage _ algorithm works as follows .",
    "the distance between clusters is defined as the average distance between points in these clusters .",
    "first , put each point into a separate cluster .",
    "then , merge the two closest clusters ; repeat the last step until the total number of clusters is @xmath111 .",
    "the _ farthest point _ clustering works as follows .",
    "assign @xmath116 to the first cluster . for @xmath117 ,",
    "find the point @xmath118 , @xmath119 that maximizes the distance @xmath120 ( to the points already assigned to clusters ) and assign @xmath121 to the cluster @xmath122 . then assign each of the remaining points to the nearest cluster .",
    "the following statement is a corollary of theorem  [ th : ss ] .",
    "[ th : clt ] under the conditions of theorem  [ th : ss ] , average linkage and farthest point clusterings are asymptotically consistent , provided the correct number of clusters @xmath111 is given to the algorithm .",
    "note that we do not require the samples to be independent ; the joint distributions of the samples may be completely arbitrary , as long as the marginal distribution of each sample is stationary ergodic .",
    "these results can be extended to the online setting in the spirit of @xcite .    for the case of unknown number of clusters ,",
    "the situation is different : one has to make stronger assumptions on the distributions generating the samples , since there is no algorithm that is consistent for all stationary ergodic distributions @xcite ; such stronger assumptions are considered in the next section .",
    "the results established so far are asymptotic out of necessity : they are established under the assumption that the distributions involved are stationary ergodic , which is too general to allow for any meaningful finite - time performance guarantees . while it is interesting to be able to establish consistency results under such general assumptions , it is also interesting to see what results can be obtained under stronger assumptions .",
    "moreover , since it is usually not known in advance whether the data at hand satisfies given assumptions or not , it appears important to have methods that have _ both _ asymptotic consistency in the general setting and finite - time performance guarantees under stronger assumptions .",
    "it turns out that this is possible : for the methods based on @xmath84 one can establish both the asymptotic performance guarantees for all stationary ergodic distributions and finite - sample performance guarantees under stronger assumptions , namely the uniform mixing conditions introduced below .",
    "another reason to consider stronger assumptions on the distributions generating the data is that some statistical problems , such as homogeneity testing or clustering when the number of clusters is unknown , are provably impossible to solve under the only assumption of stationary ergodic distributions , as shown in  @xcite .",
    "thus , in this section we analyse the speed of convergence of @xmath84 under certain mixing conditions , and use it to construct solutions for the problems of homogeneity and clustering with an unknown number of clusters , as well as to establish finite - time performance guarantees for the methods presented in the previous sections .",
    "a stationary distribution on the space of one - way infinite sequences @xmath7 can be uniquely extended to a stationary distribution on the space of two - way infinite sequences @xmath123 of the form @xmath124 .    for a process distribution @xmath14 define the mixing coefficients @xmath125 where @xmath126 denotes the sigma - algebra of the random variables in brackets .",
    "when @xmath127 the process @xmath14 is called uniformly @xmath128-mixing ( with coefficients @xmath129 ) ; this condition is much stronger than ergodicity , but is much weaker than the i.i.d .  assumption .",
    "assume that a sample @xmath42 is generated by a distribution @xmath14 that is uniformly @xmath128-mixing with coefficients @xmath129 assume further that @xmath46 is a set of indicator functions with a finite vc dimension @xmath130 , for each @xmath19 .",
    "since in this section we are after finite - time bounds , we fix a concrete choice of the weights @xmath37 in the definition  [ d : tele ] of @xmath84 , @xmath131    the general tool that we use to obtain performance guarantees in this section is the following bound that can be obtained from the results of @xcite .",
    "@xmath132 where @xmath133 are any integers in @xmath134 and @xmath135 .",
    "the parameters @xmath133 should be set according to the values of @xmath128 in order to optimize the bound .",
    "one can use similar bounds for classes of finite pollard dimension @xcite or more general bounds expressed in terms of covering numbers , such as those given in @xcite . here",
    "we consider classes of finite vc dimension only for the ease of the exposition and for the sake of continuity with the previous section ( where it was necessary ) .",
    "furthermore , for the rest of this section we assume geometric @xmath128-mixing distributions , that is , @xmath136 for some @xmath137 . letting @xmath138 the bound  ( [ eq : mixtl ] )",
    "becomes @xmath139    [ th : mix ] let two samples @xmath42 and @xmath43 be generated by stationary distributions @xmath48 and @xmath49 whose @xmath128-mixing coefficients satisfy @xmath140 for some @xmath137 .",
    "let @xmath46 , @xmath19 be some sets of indicator functions on @xmath8 whose vc dimension @xmath130 is finite and non - decreasing with @xmath4 .",
    "then @xmath141 where @xmath142 , the probability is with respect to @xmath143 and @xmath144    from   we have @xmath145 . using this and the definitions  ( 1 ) and ( 2 ) of @xmath52 and @xmath51 we obtain @xmath146 which , together with  ( 6 ) implies the statement .      given two samples @xmath42 and @xmath43 generated by distributions @xmath48 and @xmath49 respectively , the problem of homogeneity testing ( or the two - sample problem )",
    "consists in deciding whether @xmath147 .",
    "a test is called ( asymptotically ) consistent if its probability of error goes to zero as @xmath148 goes to infinity . as mentioned above , in general , for stationary ergodic time series distributions",
    "there is no asymptotically consistent test for homogeneity @xcite ( even for binary - valued time series ) ; thus , stronger assumptions are in order .",
    "homogeneity testing is one of the classical problems of mathematical statistics , and one of the most studied ones .",
    "vast literature exits on homogeneity testing for i.i.d .  data , and for dependent processes as well .",
    "we do not attempt to survey this literature here .",
    "our contribution to this line of research is to show that this problem can be reduced ( via the telescope distance ) to binary classification , in the case of strongly dependent processes satisfying some mixing conditions .",
    "it is easy to see that under the mixing conditions of lemma  1 a consistent test for homogeneity exists , and finite - sample performance guarantees can be obtained .",
    "it is enough to find a sequence @xmath149 such that @xmath150 ( see ) .",
    "then the test can be constructed as follows : say that the two sequences @xmath42 and @xmath43 were generated by the same distribution if @xmath151 ; otherwise say that they were generated by different distributions .",
    "[ th : hom ] under the conditions of lemma  [ th : mix ] the probability of type  i error ( the distributions are the same but the test says they are different ) of the described test is upper - bounded by @xmath152 . the probability of type  ii error ( the distributions are different but the test says they are the same ) is upper - bounded by @xmath153 where @xmath154 .    the statement is an immediate consequence of lemma  [ th : mix ] .",
    "indeed , for the type  i error , the two sequences are generated by the same distribution , so the probability of error of the test is given by   with @xmath155 .",
    "the probability of type  ii error is given by @xmath156 , which is upper - bounded by @xmath157 as follows from  .",
    "the optimal choice of @xmath158 may depend on the speed at which @xmath130 ( the vc dimension of @xmath46 ) increases ; however , for most natural cases ( recall that @xmath46 are also parameters of the algorithm ) this growth is polynomial , so the main term to control is @xmath159 .",
    "for example , if @xmath46 is the set of halfspaces in @xmath160 then @xmath161 and one can chose @xmath162 .",
    "the resulting probability of type  i error decreases as @xmath163 .",
    "if the distributions generating the samples satisfy certain mixing conditions , then we can augment theorems  [ th : ss ] and  [ th : clt ] with finite - sample performance guarantees .",
    "let the distributions @xmath164 generating the samples @xmath110 satisfy the conditions of lemma  [ th : mix ] .",
    "define @xmath165 and @xmath166 . then with probability at least @xmath167 the target clustering of the samples has the strict separation property . in this case single linkage and farthest point algorithms output the target clustering .    note",
    "that a sufficient condition for the strict separation property to hold is that for every pair @xmath168 of samples generated by the same distribution we have @xmath169 , and for every pair @xmath168 of samples generated by different distributions we have @xmath170 . using lemma  [ th : mix ] , the probability of such an even ( for each pair ) is upper - bounded by @xmath171 , which , multiplied by the total number @xmath172 of pairs gives the statement .",
    "the second statement is obvious .    as with homogeneity testing , while in the general case of stationary ergodic distributions it is impossible to have a consistent clustering algorithm when the number of clusters @xmath4 is unknown , the situation changes if the distributions satisfy certain mixing conditions . in this case",
    "a consistent clustering algorithm can be obtained as follows .",
    "assign to the same cluster all samples that are at most @xmath158-far from each other , where the threshold @xmath158 is selected the same way as for homogeneity testing : @xmath149 and @xmath150 .",
    "the optimal choice of this parameter depends on the choice of @xmath46 through the speed of growth of the vc dimension @xmath130 of these sets .",
    "given @xmath109 samples generated by @xmath4 different stationary distributions @xmath173",
    ", @xmath174 ( unknown @xmath4 ) all satisfying the conditions of lemma  [ th : mix ] , the probability of error ( misclustering at least one sample ) of the described algorithm is upper - bounded by @xmath175 where @xmath176 and @xmath177 , with @xmath178 , @xmath179 being lengths of the samples .",
    "the statement follows from theorem  [ th : hom ] .",
    "for experimental evaluation we chose the problem of time - series clustering .",
    "average - linkage clustering is used , with the telescope distance between samples calculated using an svm , as described in section  [ s : red ] . in all experiments ,",
    "svm is used with radial basis kernel , with default parameters of libsvm @xcite .",
    "the parameters @xmath37 in the definition of the telescope distance ( definition  [ d : tele ] ) are set to @xmath180 .      for the artificial setting we have chosen highly - dependent time series distributions which have the same single - dimensional marginals and which can not be well approximated by finite- or countable - state models .",
    "the distributions @xmath181 , @xmath182 , are constructed as follows .",
    "select @xmath183 $ ] uniformly at random ; then , for each @xmath184 obtain @xmath185 by shifting @xmath186 by @xmath187 to the right , and removing the integer part .",
    "the time series @xmath188 is then obtained from @xmath185 by drawing a point from a distribution law @xmath189 if @xmath190 and from @xmath191 otherwise .",
    "@xmath189 is a 3-dimensional gaussian with mean of 0 and covariance matrix @xmath192 .",
    "@xmath191 is the same but with mean @xmath193 .",
    "if @xmath187 is irrational then the distribution @xmath181 is stationary ergodic , but does not belong to any simpler natural distribution family @xcite .",
    "the single - dimensional marginal is the same for all values of @xmath187 .",
    "the latter two properties make all parametric and most non - parametric methods inapplicable to this problem . in our experiments",
    ", we use two process distributions @xmath194 , with @xmath195 .",
    "the dependence of error rate on the length of time series is shown on figure  [ fig : errlen ] .",
    "one clustering experiment on sequences of length 1000 takes about 5 min .  on a standard laptop .      to demonstrate the applicability of the proposed methods to realistic scenarios , we chose the brain - computer interface data from bci competition iii @xcite .",
    "the dataset consists of ( pre - processed ) bci recordings of mental imagery : a person is thinking about one of three subjects ( left foot , right foot , a random letter ) .",
    "originally , each time series consisted of several consecutive sequences of different classes , and the problem was supervised : three time series for training and one for testing .",
    "we split each of the original time series into classes , and then used our clustering algorithm in a completely unsupervised setting .",
    "the original problem is 96-dimensional , but we used only the first 3 dimensions ( using all 96 gives worse performance ) .",
    "the typical sequence length is 300 .",
    "the performance is reported in table  [ fig : errlen ] , labeled @xmath196 .",
    "all the computation for this experiment takes approximately 6 minutes on a standard laptop .",
    "the following methods were used for comparison .",
    "first , we used dynamic time wrapping ( dtw ) @xcite which is a popular base - line approach for time - series clustering .",
    "the other two methods in table  1 are from @xcite .",
    "the comparison is not fully relevant , since the results in @xcite are for different settings ; the method kcpa was used in change - point estimation method ( a different but also unsupervised setting ) , and svm was used in a supervised setting .",
    "the latter is of particular interest since the classification method we used in the telescope distance is also svm , but our setting is unsupervised ( clustering ) .",
    "this research was funded by the ministry of higher education and research , nord - pas - de - calais regional council and feder ( contrat de projets etat region cper 2007 - 2013 ) , anr projects explo - ra ( anr-08-cosi-004 ) , lampada ( anr-09-emer-007 ) and coadapt , and by the european community s fp7 program under grant agreements n@xmath197216886 ( pascal2 ) and n@xmath197270327 ( complacs ) .",
    "maria - florina balcan , nikhil bansal , alina beygelzimer , don coppersmith , john langford , and gregory sorkin .",
    "robust reductions from ranking to classification . in nader bshouty and claudio gentile , editors ,",
    "_ learning theory _ ,",
    "volume 4539 of _ lecture notes in computer science _ , pages 604619 .",
    "balcan , a.  blum , and s.  vempala . a discriminative framework for clustering via similarity functions . in _ proceedings of the 40th annual acm symposium on theory of computing _ , pages 671680 .",
    "acm , 2008 .",
    "daniel kifer , shai ben - david , and johannes gehrke .",
    "detecting change in data streams . in _ proceedings of the thirtieth international conference on very large data bases - volume 30 _ , vldb04 ,",
    "pages 180191 , 2004 ."
  ],
  "abstract_text": [
    "<S> we show how binary classification methods developed to work on i.i.d .  </S>",
    "<S> data can be used for solving statistical problems that are seemingly unrelated to classification and concern highly - dependent time series . specifically , the problems of time - series clustering , homogeneity testing and the three - sample problem are addressed . </S>",
    "<S> the algorithms that we construct for solving these problems are based on a new metric between time - series distributions , which can be evaluated using binary classification methods . </S>",
    "<S> universal consistency of the proposed algorithms is proven under most general assumptions . </S>",
    "<S> the theoretical results are illustrated with experiments on synthetic and real - world data . </S>"
  ]
}