{
  "article_text": [
    "one disadvantage of the original kohonen learning rule is that it has no potential ( or objective , or cost ) function valid for the general case of continuously distributed input spaces .",
    "ritter , martinetz and schulten @xcite gave for the expectation value of the learning step ( @xmath6 denotes the voronoi cell of @xmath0 ) @xmath7 the following potential function @xmath8 this expression is however only a valid potential in cases like the end - phase of learning of travelling - salesman - type optimization procedures , i.e. input spaces with a discrete input probability distribution and only as long as the borders of the voronoi tesselation @xmath9 are not shifting across a stimulus vector ( fig .",
    "[ clfig1 ] ) , which results in discontinuities .",
    ".,height=188 ]    as kohonen pointed out @xcite , for differentiation of ( [ cl_wrkpot ] ) w.r . to a stimulus vector @xmath10",
    ", one has to take into account the movement of the borders of the voronoi tesselation , leading to corrections to the oringinal som rule by an additional term for the winner ; so ( [ cl_wrkpot ] ) is a potential function for the winner relaxing kohonen rather than for som .",
    "this approach has been generalized @xcite to obtain infomax maps , as will be discussed in section [ clau_sec4 ] .",
    "for a gaussian neighbourhood kernel @xmath11 it is illustrative to look at limiting cases for the kernel width @xmath12 .",
    "the limit of @xmath13 implies @xmath4 to be constant , which means that all neurons receive the same learning step and there is no adaptation at all . on the other hand",
    ", the limit @xmath14 gives @xmath15 which coincides with the so - called vector quantization ( vq ) @xcite which means there is no neighbourhood interaction at all .",
    "the interesting case is where @xmath12 is small , which corresponds to the parameter choice for the end phase of learning . defining @xmath16 , we can expand ( @xmath17-expansion of the kohonen algorithm @xcite )",
    "@xmath18 here we have written the sum over the two next neighbours in the one - dimensional case ( neural chain ) , but the generalization to higher dimensions is straightforward .",
    "( note that instead of evaluating the gaussian for each learning step , one saves a considerable amount of computation by storing the powers of @xmath17 in a lookup table for a moderate kernel size , and neglecting the small contributions outside . using the @xmath19 kernel instead of",
    "the `` original '' @xmath20 kohonen kernel reduces fluctuations and preserves the magnification law better ; see @xcite for the corrections for a non - gaussian kernel . )",
    "if we now restrict to a travelling salesman setup ( periodic boundary 1d - chain ) with the case that the number of neurons equals the number of stimuli ( cities ) , then the potential reduces to @xmath21 this coincides with the @xmath22 limit ( the limit of high input resolution , or low temperature ) of the durbin and willshaw elastic net @xcite which also has a local ( universal ) magnification law ( see section [ clau_sec3 ] ) in the 1d case @xcite , that however astonishingly ( it seems to be the only feature map where it is no power law ) is not a power law ; low magnification is delimited by elasticity ) . as the elastic net is troublesome concerning parameter choice @xcite for stable behaviour esp . for serial presentation ( as a feature map model would require ) , the connection between elastic net and wrk should be taken more as a motivation to study the wrk map . apart from ordering times and reconstruction errors , one quantitative measure for feedforward structures",
    "is the transferred mutual information , which is related to the magnification law , as described in the following section .",
    "information theory @xcite gives a quantitative framework to describe information transfer through feedforward systems .",
    "mutual information between input and output is maximized when output and input are always identical , then the mutual information is maximal and equal to the information entropy of the input . if the output is completely random , the mutual information vanishes . in noisy systems ,",
    "maximization of mutual information e.g. leads to the optimal number of redundant transmissions .",
    "linsker @xcite was the first who applied this `` infomax principle '' to a self - organizing map architecture ( one should note he used a slightly different formulation of the neural dynamics , and the algorithm itself is computationally very costly ) .",
    "however , the approach can be used to quantify information preservation even for other algorithms .",
    "this can be done in a straightforward manner by looking at the magnification behavoiur of a self - organizing map .",
    "the magnification factor is defined as the number of neurons ( per unit volume in input space ) .",
    "this density is equal to the inverse of the jacobian of the input - output mapping .",
    "the remarkable property of many self - organizing maps is that the magnification factor ( at least in 1d ) becomes a function of the local input probability density ( and is therefore independent of the desity elsewhere , apart from the normalization ) , and in most cases even follows a power law .",
    "while the self - organizing map shows a power law with exponent @xmath23 @xcite , an exponent of 1 would correspond ( for a map without added noise ) to maximal mutual information , or on the other hand to the case that the firing probability of all neurons is the same .",
    "in higher dimensions , however , the stationary state of the weight vectors will in general not decouple , so the magnification law is no longer only of local dependence of the input probability density ( fig .",
    "[ clfig2 ] ) .",
    "networks , the neural density in general is not a local function of the stimulus density , but depends also on the density in neighbouring regions .",
    ", height=222 ]",
    "as pointed out in @xcite , the prefactor @xmath24 in the wrk learning rule can be replaced by a free parameter , giving the generalized winner relaxing kohonen algorithm @xmath25 the @xmath26 restricts the modification to the winner update only ( the first term is the classical kohonen som ) . the subtracted ( @xmath27 , winner relaxing case ) resp . added ( @xmath28 , winner enhancing case ) sum corresponds to some center - of - mass movement of the rest of the weight vectors .",
    "the magnification law has been shown to be a power law @xcite with exponent @xmath29 for the winner relaxing kohonen ( associated with the potential ) and @xmath30 for the generalized winner relaxing kohonen ( see fig .",
    "[ clfig3 ] ) . as stability for serial update",
    "can be acheived only within @xmath31 , the magnification exponent can be adjusted by an _ a priori _ choice of parameter @xmath32 between @xmath24 and @xmath33 .    , including the special cases som ( @xmath34 ) and wrk ( @xmath35 ) .",
    ", height=215 ]    although kohonen reported the wrk to have a larger fraction of initial conditions ordered after finite time @xcite , one can on the other hand ask how fast a rough ordering is reached from a random initial configuration . here the average ordering time can have a minimum @xcite for negative @xmath32 corresponding to a near - infomax regime . as in many optimization problems",
    ", this seems mainly to be a question whether the average , the maximal ( worst case ) , or the minimal ( parallel evaluation ) ordering time is to be minimized .",
    "cortical receptor fields of an adult animal show plasticity on a long time - scale which is very well separated from the time - scale of the signal processing dynamics .",
    "therefore , for constant input space distributions ( which in principle can be measured ) the magnification law could be accessed experimentally by measuring the neural activity ( or the number of active neurons ) by any electrical or noninvasive technique .",
    "especially the auditory cortex is well suitable for a direct comparison of mathematical modelling due to its 1-d input space .",
    "owls and bats have a large amplitude variation in their input probability distribution ( their own `` echo '' frequency is heard most often ) and are therefore pronounced candidates for experiments .    in a refined step ,",
    "experimental and theoretical investigations on the nonlinear modifications of the learning rules have to be done . on the experimental side",
    ", it has to be clarified which refinements to long - term - potentiation and long - term - depletion have to be found for the weight vectors in a neural map architecture",
    ". mechanisms that can be included as modifications are modified winner updates ( as for gwrk ) , probabilistic winner selection @xcite , or a local learning rate , depending on averaged firing rates and reconstruction errors @xcite . on the theoretical side , it is obvious that the same magnification exponent can be obtained by quite different algorithms .",
    "the relation between them and the transfer to more realistic models should be investigated further .",
    "j. c. claussen , winner relaxing and winner - enhancing kohonen maps : maximal mutual information from enhancing the winner , preprint ( 2002 ) .",
    "t. graepel , m. burger , and k. obermayer , phys .",
    "e 56 , 3876 ( 1997 ) ."
  ],
  "abstract_text": [
    "<S> self - organizing maps are models for unsupervised representation formation of cortical receptor fields by stimuli - driven self - organization in laterally coupled winner - take - all feedforward structures . </S>",
    "<S> this paper discusses modifications of the original kohonen model that were motivated by a potential function , in their ability to set up a neural mapping of maximal mutual information . enhancing the winner update , instead of relaxing it , results in an algorithm that generates an infomax map corresponding to magnification exponent of one . </S>",
    "<S> despite there may be more than one algorithm showing the same magnification exponent , the magnification law is an experimentally accessible quantity and therefore suitable for quantitative description of neural optimization principles .    </S>",
    "<S> self - organizing maps are one of most successful paradigms in mathematical modelling of special aspects of brain function , despite that a quantitative understanding of the neurobiological learning dynamics and its implications on the mathematical process of structure formation are still lacking . a biological discrimination between models may be difficult , and it is not completely clear @xcite what optimization goals are dominant in the biological development for e.g. skin , auditory , olfactory or retina receptor fields . </S>",
    "<S> all of them roughly show a self - organizing ordering as can most simply be described by the self - organizing feature map @xcite defined as follows :    every stimulus in input space ( receptor field ) is assigned to a so - called winner ( or center of excitation ) @xmath0 where the distance @xmath1 to the stimulus is minimal . according to kohonen , </S>",
    "<S> all weight vectors are updated by @xmath2 this can be interpreted as a hebbian learning rule ; @xmath3 is the learning rate , and @xmath4 determines the ( pre - defined ) topology of the neural layer . while kohonen chose @xmath5 to be 1 for a fixed neighbourhood , and 0 elsewhere , a gaussian kernel ( with a width decreasing in time ) is more common . </S>",
    "<S> the self - organizing map concept can be used with regular lattices of any dimension ( although 1 , 2 or 3 dimensions are preferred for easy visualization ) , with an irregular lattice , none ( vector quantization ) @xcite , or a neural gas @xcite where the coefficients @xmath5 are determined by rank of distance in input space . in all cases , learning can be implemented by serial ( stochastic ) , batch , or parallel updates . </S>"
  ]
}