{
  "article_text": [
    "this paper focuses on _ counting _ , which is among the most fundamental operations in almost every field of science and engineering .",
    "computing the sum @xmath18 $ ] is the simplest counting ( @xmath3 denotes time ) . counting the @xmath0th moment @xmath18^\\alpha$ ]",
    "is more general . when @xmath19 , @xmath20^\\alpha$ ] counts the total number of non - zeros in @xmath2 .",
    "when @xmath21 , @xmath18^\\alpha$ ] counts the `` energy '' or `` power '' of the signal @xmath2 .",
    "if @xmath2 actually outputs the power of an underlying signal @xmath22 , counting the sum @xmath23 is equivalent to computing @xmath18^{1/2}$ ] .    here",
    ", @xmath2 denotes a time - varying signal , for example , _ data streams_@xcite . in the literature , the @xmath0th frequency moment of a data stream @xmath2",
    "is defined as    counting @xmath24 for massive data streams is practically important , among many challenging issues in data stream computations .",
    "in fact , the general theme of `` scaling up for high dimensional data and high speed data streams '' is among the `` ten challenging problems in data mining research . '' because the elements , @xmath7 $ ] , are time - varying , a nave counting mechanism requires a system of @xmath25 counters to compute @xmath24 exactly .",
    "this is not always realistic when @xmath25 is large and we only need an approximate answer . for example , @xmath25 may be @xmath26 if @xmath2 records the arrivals of ip addresses . or , @xmath25 can be the total number of checking / savings accounts .    * _ compressed counting ( cc ) _ * is a new scheme for approximating the @xmath0th frequency moments of data streams ( where @xmath27 ) using low memory .",
    "the underlying technique is based on what we call _ skewed stable random projections_.      we consider the popular _ turnstile _ data stream model @xcite .",
    "the input stream @xmath28 , @xmath29 $ ] arriving sequentially describes the underlying signal @xmath30 , meaning @xmath31 = a_{t-1}[i_t ] + i_t$ ] .",
    "the increment @xmath32 can be either positive ( insertion ) or negative ( deletion ) . restricting @xmath33 results in the _ cash register _ model . restricting @xmath7\\geq 0 $ ] at all @xmath3 ( but @xmath32 can still be either positive or negative ) results in the _ strict turnstile _",
    "model , which suffices for describing most ( although not all ) natural phenomena . for example@xcite , in a database , a record can only be deleted if it was previously inserted .",
    "another example is the checking / savings account , which allows deposits / withdrawals but generally does not allow overdraft .",
    "_ compressed counting ( cc ) _ is applicable when , at the time @xmath3 for the evaluation , @xmath7\\geq 0 $ ] for all @xmath34 .",
    "this is more flexible than the _ strict turnstile _ model , which requires @xmath7\\geq 0 $ ] at all @xmath3 .",
    "in other words , cc is applicable when data streams are ( a ) insertion only ( i.e. , the _ cash register _ model ) , or ( b ) always non - negative ( i.e. , the _ strict turnstile _ model ) , or ( c ) non - negative at check points .",
    "we believe our model suffices for describing most natural data streams in practice .    with the realistic restriction that @xmath7\\geq 0 $ ] at @xmath3 , the definition of the @xmath0th frequency moment becomes @xmath35^\\alpha ;",
    "\\end{aligned}\\ ] ]    and the case @xmath36 becomes trivial , because @xmath37= \\sum_{s=1}^t i_s\\end{aligned}\\ ] ]    in other words , for @xmath38 , we need only a simple counter to accumulate all values of increment / decrement @xmath32 .    for @xmath5",
    ", however , counting ( [ eqn_def_f2 ] ) is still a non - trivial problem .",
    "intuitively , there should exist an intelligent counting system that performs almost like a simple counter when @xmath8 with small @xmath9 .",
    "the parameter @xmath9 may bear a clear physical meaning .",
    "for example , @xmath9 may be the `` decay rate '' or `` interest rate , '' which is usually small .    the proposed _",
    "compressed counting ( cc ) _ provides such an intelligent counting systems . because its underlying technique is based on _ skewed stable random projections _",
    ", we provide a brief introduction to _ skewed stable distributions_.      a random variable @xmath39 follows a @xmath40-skewed @xmath0-stable distribution if the fourier transform of its density is@xcite @xmath41    where @xmath42 and @xmath43 is the scale parameter .",
    "we denote @xmath44 . here",
    "when @xmath46 , the inverse fourier transform is unbounded ; and when @xmath47 , the inverse fourier transform is not a probability density .",
    "this is why _ compressed counting _ is limited to @xmath6 .",
    "consider two independent variables , @xmath48 .",
    "for any non - negative constants @xmath49 and @xmath50 , the `` @xmath0-stability '' follows from properties of fourier transforms : @xmath51 however , if @xmath49 and @xmath50 do not have the same signs , the above `` stability '' does not hold ( unless @xmath52 or @xmath21 , @xmath53 ) . to see this , we consider @xmath54 , with @xmath55 and @xmath56 . then , because @xmath57 , @xmath58    which does not represent a stable law , unless @xmath52 or @xmath59 , @xmath53 .",
    "this is the fundamental reason why _ compressed counting _ needs the restriction that at the time of evaluation , elements in the data streams should have the same signs .      given @xmath60 with each element @xmath61 i.i.d . , then    meaning @xmath62 represents one sample of the stable distribution whose scale parameter @xmath24 is what we are after .    of course , we need more than one sample to estimate @xmath24 . we can generate a matrix @xmath63 with each entry @xmath64 .",
    "the resultant vector @xmath65 contains @xmath66 i.i.d .",
    "samples : @xmath67 , @xmath68 to @xmath66 .    note that this is a linear projection ; and recall that the _ turnstile _ model is also linear .",
    "thus , _ skewed stable random projections _ can be applicable to dynamic data streams . for every incoming @xmath28 , we update @xmath69 for @xmath68 to @xmath66 . this way , at any time @xmath3 , we maintain @xmath66 i.i.d .",
    "stable samples .",
    "the remaining task is to recover @xmath24 , which is a statistical estimation problem .",
    "the _ method of moments _ is often convenient and popular in statistical parameter estimation .",
    "consider , for example , the three - parameter generalized gamma distribution @xmath70 , which is highly flexible for modeling positive data , e.g. , @xcite .",
    "if @xmath71 , then the first three moments are @xmath72 , @xmath73 , @xmath74 .",
    "thus , one can estimate @xmath75 , @xmath76 and @xmath77 from @xmath25 i.i.d .",
    "samples @xmath78 by counting the first three empirical moments from the data . however , some moments may be ( much ) easier to compute than others if the data @xmath79 s are collected from data streams . instead of using integer moments ,",
    "the parameters can also be estimated from any three _ fractional _",
    "moments , i.e. , @xmath80 , for three different values of @xmath0 .",
    "because @xmath25 is very large , any consistent estimator is likely to provide a good estimate .",
    "thus , it might be reasonable to choose @xmath0 mainly based on the computational cost .",
    "see appendix [ app_moments ] for comments on the situation in which one may also care about the relative accuracy caused by different choices of @xmath0 .",
    "the logarithmic norm @xmath81 arises in statistical estimation , for example , the maximum likelihood estimators for the pareto and gamma distributions .",
    "since it is closely connected to the moment problem , section [ sec_log ] provides an algorithm for approximating the logarithmic norm , as well as for the logarithmic distance ; the latter can be quite useful in machine learning practice with massive heavy - tailed data ( either dynamic or static ) in lieu of the usual @xmath82 distance .",
    "entropy is also an important summary statistic .",
    "recently @xcite proposed to approximate the entropy moment @xmath83 using the @xmath0th moments with @xmath8 and very small @xmath9",
    ".      pioneered by@xcite , there have been many studies on approximating the @xmath0th frequency moment @xmath24 .",
    "@xcite considered integer moments , @xmath84 , 1 , 2 , as well as @xmath47 .",
    "soon after , @xcite provided improved algorithms for @xmath27 .",
    "@xcite proved the sample complexity lower bounds for @xmath85 .",
    "@xcite proved the optimal lower bounds for all frequency moments , except for @xmath36 , because for non - negative data , @xmath38 can be computed essentially error - free with a counter@xcite .",
    "@xcite provided algorithms for @xmath85 to ( essentially ) achieve the lower bounds proved in @xcite .",
    "note that an algorithm , which `` achieves the optimal bound , '' is not necessarily practical because the constant may be very large . in a sense , the method based on _ symmetric stable random projections_@xcite is one of the few successful algorithms that are simple and free of large constants .",
    "@xcite described the procedure for approximating @xmath38 in data streams and proved the bound for @xmath4 ( although not explicitly ) . for @xmath5",
    ", @xcite provided a conceptual algorithm .",
    "@xcite proposed various estimators for _ symmetric stable random projections _ and provided the constants explicitly for all @xmath27 .",
    "none of the previous studies , however , captures of the intuition that , when @xmath36 , a simple counter suffices for computing @xmath38 ( essentially ) error - free , and when @xmath86 with small @xmath9 , the sample complexity ( number of projections , @xmath66 ) should be low and vary continuously as a function of @xmath9 .    *",
    "_ compressed counting ( cc ) _ * is proposed for @xmath45 and it works particularly well when @xmath8 with small @xmath9",
    ". this can be practically very useful .",
    "for example , @xmath9 may be the `` decay rate '' or the `` interest rate , '' which is usually small ; thus cc can count the total value in the future taking into account the effect of decaying or interest accruement .",
    "in parameter estimations using the _ method of moments _ , one may choose the @xmath0th moments with @xmath0 close 1 .",
    "also , one can approximate the entropy moment using the @xmath0th moments with @xmath8 and very small @xmath9@xcite .",
    "our study has connections to the johnson - lindenstrauss lemma@xcite , which proved @xmath87 at @xmath21 .",
    "an analogous bound holds for @xmath45@xcite .",
    "the dependency on @xmath88 may raise concerns if , say , @xmath89 .",
    "we will show that cc achieves @xmath90 in the neighborhood of @xmath36 .",
    "recall that _",
    "compressed counting ( cc ) _ boils down to a statistical estimation problem .",
    "that is , given @xmath66 i.i.d .",
    "samples @xmath91 , estimate the scale parameter @xmath24 .",
    "section [ sec_gm ] will explain why we fix @xmath92 .",
    "part of this paper is to provide estimators which are convenient for theoretical analysis , e.g. , tail bounds .",
    "we provide the _ geometric mean _ and the _ harmonic mean _ estimators , whose",
    "asymptotic variances are illustrated in figure [ fig_comp_var_factor ] .",
    "* the * geometric mean * estimator , @xmath93 @xmath93 is unbiased .",
    "we prove the sample complexity explicitly and show @xmath94 suffices for @xmath0 around 1 .",
    "* the * harmonic mean * estimator , @xmath95 , for @xmath96 it is considerably more accurate than @xmath93 and its sample complexity bound is also provided in an explicit form . here",
    "@xmath97 is the usual gamma function .",
    "section [ sec_gm ] begins with analyzing the moments of skewed stable distributions , from which the _ geometric mean _ and _ harmonic mean _",
    "estimators are derived .",
    "section [ sec_gm ] is then devoted to the detailed analysis of the _ geometric mean _ estimator .",
    "section [ sec_hm ] analyzes the _ harmonic mean _ estimator .",
    "section [ sec_log ] addresses the application of cc in statistical parameter estimation and an algorithm for approximating the logarithmic norm and distance .",
    "the proofs are presented as appendices .",
    "we first prove a fundamental result about the moments of skewed stable distributions .    [ lem_moments ] if @xmath98 , then for any @xmath99 , @xmath100 which can be simplified when @xmath92 , to be @xmath101 for @xmath96 , and @xmath102 , @xmath103    * proof : * see appendix [ proof_lem_moments ] .",
    "@xmath104 +    recall that _",
    "compressed counting _",
    "boils down to estimating @xmath24 from these @xmath66 i.i.d .",
    "samples @xmath105 . setting @xmath106 in lemma [ lem_moments ]",
    "yields an unbiased estimator : @xmath107^k.\\end{aligned}\\ ] ]    the following lemma shows that the variance of @xmath108 decreases with increasing @xmath109 $ ] .",
    "the variance of @xmath108 @xmath110^k } { \\left[\\frac{2}{\\pi}\\sin\\left(\\frac{\\pi\\alpha}{2k}\\right ) \\gamma\\left(1-\\frac{1}{k}\\right)\\gamma\\left(\\frac{\\alpha}{k}\\right)\\right]^{2k}}-1,\\vspace{-0.1in}\\end{aligned}\\ ] ] is a decreasing function of @xmath111 $ ] .",
    "* proof : *  the result follows from the fact that @xmath112    is a deceasing function of @xmath109 $ ] .",
    "@xmath104    therefore , for attaining the smallest variance , we take @xmath113 . for brevity , we simply use @xmath93 instead of @xmath114 .",
    "in fact , the rest of the paper will always consider @xmath113 only .",
    "we rewrite @xmath93 ( i.e. , @xmath115 ) as @xmath116^k.\\end{aligned}\\ ] ] here , @xmath117 , if @xmath118 , and @xmath119 if @xmath120 .",
    "lemma [ lem_gm_moments ] concerns the asymptotic moments of @xmath93 .",
    "[ lem_gm_moments ] as @xmath121 @xmath122^k \\\\\\label{eqn_gm_asymp } \\rightarrow & \\exp\\left(-\\gamma_e\\left(\\alpha-1\\right)\\right),\\end{aligned}\\ ] ] * monotonically * with increasing @xmath66 ( @xmath123 ) , where @xmath124 is euler s constant .",
    "for any fixed @xmath3 , as @xmath121 , @xmath125^{k } } { \\cos^{kt}\\left(\\frac{\\kappa(\\alpha)\\pi}{2k}\\right ) \\left[\\frac{2}{\\pi}\\sin\\left(\\frac{\\pi\\alpha}{2k}\\right ) \\gamma\\left(1-\\frac{1}{k}\\right)\\gamma\\left(\\frac{\\alpha}{k}\\right)\\right]^{kt } } \\\\\\notag = & f_{(\\alpha)}^t\\exp\\left ( \\frac{1}{k}\\frac{\\pi^2(t^2-t)}{24}\\left(\\alpha^2 + 2 - 3\\kappa^2(\\alpha)\\right)+o\\left(\\frac{1}{k^2}\\right)\\right).\\end{aligned}\\ ] ] @xmath126 * proof : * see appendix [ proof_lem_gm_moments ] .",
    "@xmath104    in ( [ eqn_f_gm ] ) , the denominator @xmath127 depends on @xmath66 for small @xmath66 .",
    "for convenience in analyzing tail bounds , we consider an asymptotically equivalent _ geometric mean _ estimator : @xmath128    lemma [ lem_gm_bounds ] provides the tail bounds for @xmath129 and figure [ fig_g_gm ] plots the tail bound constants .",
    "one can infer the tail bounds for @xmath93 from the monotonicity result ( [ eqn_gm_asymp ] ) .",
    "[ lem_gm_bounds ] the right tail bound : and the left tail bound : @xmath130    @xmath131 and @xmath132 are solutions to here @xmath133 is the `` psi '' function .",
    "* proof : * see appendix [ proof_lem_gm_bounds ] . @xmath104    it is important to understand the behavior of the tail bounds as @xmath134 .",
    "( @xmath135 if @xmath118 ; and @xmath136 if @xmath120 . )",
    "see more comments in appendix [ app_moments ] .",
    "lemma [ lem_g_gm_rate ] describes the precise rates of convergence .",
    "[ lem_g_gm_rate ] for fixed @xmath137 , as @xmath138 ( i.e. , @xmath139 ) , @xmath140 * proof : *   see appendix [ proof_lem_g_gm_rate ] .",
    "@xmath104    figure [ fig_g_gm_approx ] plots the constants for small values of @xmath9 , along with the approximations suggested in lemma [ lem_g_gm_rate ] . since we usually consider @xmath137 should not be too large , we can write , as @xmath141 , @xmath142 and @xmath143 if @xmath120 ; both at the rate @xmath144 .",
    "however , if @xmath118 , @xmath145 , which is extremely fast .",
    "+    the sample complexity bound is then straightforward .",
    "[ lem_jl ] using the geometric mean estimator , it suffices to let @xmath146 so that the error will be within a @xmath147 factor with probability @xmath148 , where @xmath149 . in the neighborhood of @xmath36",
    ", @xmath150 only .",
    "for @xmath96 , the _ harmonic mean _ estimator can considerably improve @xmath93 .",
    "unlike the _ harmonic mean _ estimator in @xcite , which is useful only for small @xmath0 and has no exponential tail bounds except for @xmath151 , the _ harmonic mean _ estimator in this study has very nice tail properties for all @xmath152 .",
    "the _ harmonic mean _",
    "estimator takes advantage of the fact that if @xmath153 , then @xmath154 exists for all @xmath102 .",
    "[ lem_hm ] assume @xmath66 i.i.d .",
    "samples @xmath155 , define the harmonic mean estimator @xmath156 , @xmath157 and the bias - corrected harmonic mean estimator @xmath95 , @xmath158 the bias and variance of @xmath95 are @xmath159    the right tail bound of @xmath156 is , for @xmath160 , @xmath161 where @xmath162 is the solution to @xmath163    the left tail bound of @xmath156 is , for @xmath164 , @xmath165 where @xmath166 is the solution to @xmath167    * proof : * see appendix [ proof_lem_hm ] . @xmath104 .",
    "the logarithmic norm and distance can be important in practice . consider estimating the parameters from @xmath25 i.i.d .",
    "samples @xmath168 .",
    "the density function is @xmath169 , and the likelihood equation is @xmath170 if instead , @xmath171 , @xmath172 to @xmath25 , then the density is @xmath173 , @xmath174 , and the likelihood equation is @xmath175    therefore , the logarithmic norm occurs at least in the content of maximum likelihood estimations of common distributions .",
    "now , consider the data @xmath79 s are actually the elements of data streams @xmath7 $ ] s .",
    "estimating @xmath16 $ ] becomes an interesting and practically meaningful problem .",
    "our solution is based on the fact that , as @xmath19 , @xmath176^\\alpha\\right)\\rightarrow \\sum_{i=1}^d\\log a_t[i],\\end{aligned}\\ ] ] which can be shown by lhpital s rule .",
    "more precisely , @xmath177^\\alpha\\right ) - \\sum_{i=1}^d\\log a_t[i]\\right| \\\\\\notag = & o\\left(\\frac{\\alpha}{d}\\left(\\sum_{i=1}^d\\log a_t[i]\\right)^2\\right ) + o\\left(\\alpha\\sum_{i=1}^d\\log^2a_t[i]\\right),\\end{aligned}\\ ] ] which can be shown by taylor expansions .",
    "therefore , we obtain one solution to approximating the logarithmic norm using very small @xmath0 . of course , we have assumed that @xmath7>0 $ ] strictly .",
    "in fact , this also suggests an approach for approximating the logarithmic distance between two streams @xmath178 - b_t[i]|$ ] , provided we use _ symmetric stable random projections_.    the logarithmic distance can be useful in machine learning practice with massive heavy - tailed data ( either static or dynamic ) such as image and text data .",
    "for those data , the usual @xmath82 distance would not be useful without `` term - weighting '' the data ; and taking logarithm is one simple weighting scheme .",
    "thus , our method provides a direct way to compute pairwise distances , taking into account data weighting automatically .",
    "one may be also interested in the tail bounds , which , however , can not be expressed in terms of the logarithmic norm ( or distance ) .",
    "nevertheless , we can obtain , e.g. , @xmath179\\geq ( 1+\\epsilon ) \\left[\\frac{d}{\\alpha}\\log\\left(\\frac{1}{d}f_{(\\alpha)}\\right)\\right]\\right)\\\\\\notag \\leq & \\exp\\left(-k\\frac{\\left(\\left(f_{(\\alpha)}/d\\right)^{\\epsilon}-1\\right)^2}{g_{r , hm}}\\right ) , \\hspace{0.5 in } \\epsilon>0,\\\\\\notag & \\mathbf{pr}\\left(\\left[\\frac{d}{\\alpha}\\log\\left(\\frac{1}{d}\\hat{f}_{(\\alpha),hm}\\right)\\right]\\leq ( 1-\\epsilon ) \\left[\\frac{d}{\\alpha}\\log\\left(\\frac{1}{d}f_{(\\alpha)}\\right)\\right]\\right)\\\\\\notag \\leq & \\exp\\left(-k\\frac{\\left(1-\\left(d / f_{(\\alpha)}\\right)^{\\epsilon}\\right)^2}{g_{l , hm}}\\right ) , \\hspace{0.5 in } 0<\\epsilon<1\\end{aligned}\\ ] ] if @xmath93 is used , we just replace the corresponding constants in the above expressions . if we are interested in the logarithmic distance , we simply apply _ symmetric stable random projections _ and use an appropriate estimator of the distance ; the corresponding tail bounds will have same format .",
    "counting is a fundamental operation . in data streams",
    "@xmath7 $ ] , @xmath180 $ ] , counting the @xmath0th frequency moments @xmath181^\\alpha$ ] has been extensively studied .",
    "our proposed _ compressed counting ( cc )",
    "_ takes advantage of the fact that most data streams encountered in practice are non - negative , although they are subject to deletion and insertion .",
    "in fact , cc only requires that at the time @xmath3 for the evaluation , @xmath7\\geq 0 $ ] ; at other times , the data streams can actually go below zero .",
    "_ compressed counting _ successfully captures the intuition that , when @xmath36 , a simple counter suffices , and when @xmath15 with small @xmath9 , an intelligent counting system should require low space ( continuously as a function of @xmath9 ) .",
    "the case with small @xmath9 can be practically important .",
    "for example , @xmath9 may be the `` decay rate '' or `` interest rate , '' which is usually small .",
    "cc can also be very useful for statistical parameter estimation based on the _ method of moments_. also , one can approximate the entropy moment using the @xmath0th moments with @xmath8 and very small @xmath9 .    compared with previous studies , e.g. , @xcite , _ compressed counting",
    "_ achieves , in a sense , an `` infinite improvement '' in terms of the asymptotic variances when @xmath12 .",
    "two estimators based on the geometric mean and the harmonic mean are provided in this study , including their variances , tail bounds , and sample complexity bounds .",
    "we analyze our sample complexity bound @xmath182 at the neighborhood of @xmath36 and show @xmath183 at small @xmath9 .",
    "this implies that our bound at small @xmath9 is actually @xmath184 instead of @xmath185 , which is required in the johnson - lindenstrauss lemma and its various analogs .",
    "finally , we propose a scheme for approximating the logarithmic norm and the logarithmic distance , useful in statistical parameter estimation and machine learning practice .",
    "+ we expect that new algorithms will soon be developed to take advantage of _ compressed counting_. for example , via private communications , we have learned that a group is vigorously developing algorithms using projections with @xmath15 very close to 1 , where @xmath9 is their important parameter .",
    "we provide a ( somewhat contrived ) example of the _ method of moments_. suppose the observed data @xmath79 s are from data streams and suppose the data follows a gamma distribution @xmath186 , i.i.d . here ,",
    "we only consider one parameter @xmath75 so that we can analyze the variance easily .",
    "suppose we estimate @xmath75 using the @xmath0th moment . because @xmath187 , we can solve for @xmath188 from @xmath189 by the `` delta method '' ( i.e. , @xmath190 ) and using the implicit derivative of @xmath188 , we obtain @xmath191    one can verify @xmath192 increases monotonically with increasing @xmath193 . because @xmath79 s are from data streams , we apply _ compressed counting _ for the @xmath0th moment .",
    "suppose we consider the difference in the estimation accuracy at different @xmath0 is not important ( because @xmath25 is large ) .",
    "then we simply let @xmath4 . in case",
    "we need to estimate two parameters , we might choose @xmath194 and another @xmath0 close to 1 .",
    "now suppose we actually care about both the estimation accuracy ( which favors smaller @xmath0 ) and the computational efficiency ( which favors @xmath194 ) , we then need to balance this trade - off by choosing @xmath0 .",
    "to do so , we need to know the precise behavior of _ compressed counting _ in the neighborhood of @xmath4 , as well as the precise behavior of @xmath188 , i.e. , its tail bounds ( not just variance ) .",
    "thus , our analysis on the convergence rates in lemma [ lem_g_gm_rate ] will be very useful .",
    "assume @xmath98 . to prove @xmath195 for @xmath196 , (",
    "* theorem 2.6.3 ) provided only a partial answer : @xmath197 where we denote @xmath198 and according to the parametrization used in ( * ? ? ?",
    "* i.19 , i.28 ) : @xmath199 note that @xmath200 therefore , for @xmath99 , @xmath201        note that when @xmath118 and @xmath92 , @xmath39 is always non - negative . as shown in the proof of ( * ? ? ?",
    "* theorem 2.6.3 ) , @xmath208 the only thing we need to check is that in the proof of ( * ? ? ? * theorem 2.6.3 ) , the condition for fubini s theorem ( to exchange order of integration ) still holds when @xmath209 , @xmath92 , and @xmath210 .",
    "we can show @xmath211 provided @xmath210 ( @xmath212 ) and @xmath213 , i.e. , @xmath118 .",
    "note that @xmath214 always and euler s formula : @xmath215 is frequently used to simplify the algebra .",
    "once we show that fubini s condition is satisfied , we can exchange the order of integration and the rest follows from the proof of ( * ? ? ?",
    "* theorem 2.6.3 ) .",
    "because of continuity , the `` singularity points '' @xmath216 do not matter .",
    "we first show that , for any fixed @xmath3 , as @xmath121 , @xmath217^{k } } { \\cos^{kt}\\left(\\frac{\\kappa(\\alpha)\\pi}{2k}\\right ) \\left[\\frac{2}{\\pi}\\sin\\left(\\frac{\\pi\\alpha}{2k}\\right ) \\gamma\\left(1-\\frac{1}{k}\\right)\\gamma\\left(\\frac{\\alpha}{k}\\right)\\right]^{kt } } \\\\\\notag = & f_{(\\alpha)}^t\\exp\\left ( \\frac{1}{k}\\frac{\\pi^2(t^2-t)}{24}\\left(\\alpha^2 + 2 - 3\\kappa^2(\\alpha)\\right)+o\\left(\\frac{1}{k^2}\\right)\\right).\\end{aligned}\\ ] ]    in @xcite , it was proved that , as @xmath121 , @xmath218^{k } } { \\left[\\frac{2}{\\pi}\\sin\\left(\\frac{\\pi\\alpha}{2k}\\right ) \\gamma\\left(1-\\frac{1}{k}\\right)\\gamma\\left(\\frac{\\alpha}{k}\\right)\\right]^{kt } } \\\\\\notag = & 1+\\frac{1}{k}\\frac{\\pi^2(t^2-t)}{24}\\left(\\alpha^2 + 2\\right)+o\\left(\\frac{1}{k^2}\\right)\\\\\\notag = & \\exp\\left ( \\frac{1}{k}\\frac{\\pi^2(t^2-t)}{24}\\left(\\alpha^2 + 2\\right)+o\\left(\\frac{1}{k^2}\\right)\\right).\\end{aligned}\\ ] ] using the infinite product representation of cosine@xcite @xmath219 we can rewrite @xmath220 which , combined with the result in @xcite , yields the desired expression .",
    "+ the next task is to show @xmath221^k \\rightarrow \\exp\\left(-\\gamma_e\\left(\\alpha-1\\right)\\right),\\end{aligned}\\ ] ] monotonically as @xmath121 , where @xmath222 , is euler s constant . in @xcite",
    ", it was proved that , as @xmath121 , @xmath223^k \\rightarrow \\exp\\left(-\\gamma_e\\left(\\alpha-1\\right)\\right),\\end{aligned}\\ ] ] monotonically . in this study , we need to consider instead @xmath224^k \\\\ = & \\left[2\\cos\\left(\\frac{\\kappa(\\alpha)\\pi}{2k}\\right)\\frac{\\gamma\\left(\\frac{\\alpha}{k}\\right)\\sin\\left(\\frac{\\pi\\alpha}{2k}\\right)}{\\gamma\\left(\\frac{1}{k}\\right)\\sin\\left(\\frac{\\pi}{k}\\right ) } \\right]^k\\end{aligned}\\ ] ] ( note euler s reflection formula @xmath225 . ) the additional term @xmath226^k = 1 - o\\left(\\frac{1}{k}\\right)$ ]",
    ". therefore , @xmath122^k \\rightarrow \\exp\\left(-\\gamma_e\\left(\\alpha-1\\right)\\right).\\end{aligned}\\ ] ]    to show the monotonicity , however , we have to use some different techniques from @xcite .",
    "the reason is because the additional term @xmath226^k$ ] increases ( instead of decreasing ) monotonically with increasing @xmath66 .",
    "first , we consider @xmath120 , i.e. , @xmath227 . for simplicity , we take logarithm of ( [ eqn_proof_gm_coefficent ] ) and replace @xmath228 by @xmath3 , where @xmath229 ( recall @xmath123 ) .",
    "it suffices to show that @xmath230 increases with increasing @xmath231 $ ] , where @xmath232 because @xmath233 , to show @xmath234 in @xmath231 $ ] , it suffices to show @xmath235 one can check that @xmath236 and @xmath237 , as @xmath238 .",
    "@xmath239 here @xmath240 is the `` psi '' function .",
    "therefore , to show @xmath241 , it suffices to show that @xmath242 is an increasing function of @xmath243 $ ] , i.e. , @xmath244    using series representation of @xmath245 @xcite , we show @xmath246 because we consider @xmath247 .",
    "thus , it suffices to show that @xmath248 to show @xmath249 , we can treat @xmath250 as a function of @xmath0 ( for fixed @xmath3 ) . because both @xmath251 and @xmath252 are convex functions of @xmath253 $ ] , we know @xmath250 is a concave function of @xmath0 ( for fixed @xmath3 ) .",
    "it is easy to check that @xmath254 because @xmath250 is concave in @xmath255 $ ] , we must have @xmath249 ; and consequently , @xmath256 and @xmath257 . therefore , we have proved that ( [ eqn_proof_gm_coefficent ] ) decreases monotonically with increasing @xmath66 , when @xmath258 .    for @xmath118 ( i.e. , @xmath259 )",
    ", we prove the monotonicity by a different technique .",
    "first , using infinite - product representations @xcite , @xmath260 we can rewrite ( [ eqn_proof_gm_coefficent ] ) as @xmath261^k%=\\left[\\frac{\\gamma\\left(\\frac{\\alpha}{k}\\right)\\sin\\left(\\frac{\\pi\\alpha}{k}\\right)}{\\gamma\\left(\\frac{1}{k}\\right)\\sin\\left(\\frac{\\pi}{k}\\right ) } \\right]^k = \\exp\\left(-\\gamma_e(\\alpha-1)\\right)\\times\\\\\\notag & \\left ( \\prod_{s=1}^\\infty\\exp\\left(\\frac{\\alpha-1}{sk}\\right)\\left(1+\\frac{\\alpha}{ks}\\right)^{-1}\\left(1+\\frac{1}{ks}\\right ) \\left(1-\\frac{\\alpha^2}{k^2s^2}\\right)\\left(1-\\frac{1}{s^2k^2}\\right)^{-1}\\right)^k.\\end{aligned}\\ ] ] to show its monotonicity , it suffices to show for any @xmath262 @xmath263 decreases monotonically , which is equivalent to show the monotonicity of @xmath230 with increasing @xmath3 , for @xmath264 , where @xmath265 it is straightforward to show that @xmath266 is monotonically decreasing with increasing @xmath3 ( @xmath267 ) , for @xmath118 .",
    "we first find the constant @xmath268 in the right tail bound @xmath269 for @xmath270 , the markov moment bound yields @xmath271^k}{(1+\\epsilon)^{t}\\exp\\left(-t\\gamma_e ( \\alpha -1)\\right)}.\\end{aligned}\\ ] ] we need to find the @xmath3 that minimizes the upper bound . for convenience ,",
    "we consider its logarithm , i.e. , @xmath272 whose first and second derivatives ( with respect to @xmath3 ) are @xmath273        since we have proved that @xmath278 , i.e. , @xmath230 is a convex function , one can find the optimal @xmath3 by solving @xmath279 : @xmath280 we let the solution be @xmath281 , where @xmath131 is the solution to @xmath282    alternatively , we can seek a `` sub - optimal '' ( but asymptotically optimal ) solution using the asymptotic expression for @xmath283 in lemma [ lem_gm_moments ] , i.e. , the @xmath3 that minimizes @xmath284 whose minimum is attained at @xmath285 this approximation can be useful ( e.g. , ) for serving the initial guess for @xmath131 in a numerical procedure .        from lemma [ lem_gm_moments ] , we know that , for any @xmath3 , where @xmath289 if @xmath120 and @xmath290 if @xmath118 , @xmath291^k } { \\exp\\left(t\\gamma_e(\\alpha-1)\\right)}\\\\\\notag = & ( 1-\\epsilon)^t\\exp\\left(-t\\gamma_e(\\alpha-1)\\right)\\frac{\\left [ \\cos\\left(\\frac{\\kappa(\\alpha)\\pi}{2k}t\\right)\\gamma\\left(1+\\frac{t}{k}\\right ) \\right]^k}{\\left [ \\gamma\\left(1+\\frac{\\alpha t}{k}\\right)\\cos\\left(\\frac{\\pi\\alpha t}{2k}\\right ) \\right]^k}\\\\\\notag = & ( 1-\\epsilon)^t\\exp\\left(-t\\gamma_e(\\alpha-1)\\right)\\frac{\\left [ \\cos\\left(\\frac{\\kappa(\\alpha)\\pi}{2k}t\\right)\\gamma\\left(\\frac{t}{k}\\right ) \\right]^k}{\\left[\\alpha \\gamma\\left(\\frac{\\alpha t}{k}\\right)\\cos\\left(\\frac{\\pi\\alpha t}{2k}\\right ) \\right]^k}\\end{aligned}\\ ] ] whose minimum is attained at @xmath292 ( we skip the proof of convexity ) such that @xmath293 thus , we show the left tail bound      first , we consider the right bound . from lemma [ lem_gm_bounds ] , @xmath294 and @xmath131 is the solution to @xmath295 , @xmath296 using series representations in @xcite @xmath297 we rewrite @xmath298 as @xmath299 we show that , as @xmath141 , i.e. , @xmath300 , the term @xmath301 from lemma [ lem_gm_bounds ] , we know @xmath302 has a unique well - defined solution for @xmath303 . we need to analyze this term @xmath304 which , when @xmath138 ( i.e. , @xmath305 ) , must approach a finite limit . in other words , @xmath306 , at the rate @xmath144 , i.e. , @xmath307    by eulerr reflection formula and series representations , @xmath308 @xmath309 taking logarithm of which yields @xmath310",
    "if @xmath96 , i.e. , @xmath311 , then @xmath312 thus , for @xmath96 , as @xmath313 , we obtain @xmath314    if @xmath247 , i.e. , @xmath315 and @xmath316 , then @xmath317 also @xmath318 and @xmath319 therefore , for @xmath247 , we also have @xmath320 in other words , as @xmath138 , the constant @xmath268 converges to @xmath321 at the rate @xmath144 .",
    "+ next , we consider the left bound . from lemma [ lem_gm_bounds ] , @xmath322 where @xmath323 and @xmath132 is the solution to @xmath324 , @xmath325      we first consider @xmath328 . in order for @xmath329 to have a meaningful solution",
    ", we must make sure that @xmath330 converges to a finite value as @xmath141 , i.e. , @xmath331 also .",
    "this provides an approximate solution for @xmath132 when @xmath120 : @xmath332 using series representations , we obtain @xmath333 therefore , for @xmath247 @xmath334    finally , we need to consider @xmath96 . in this case , @xmath335 using properties of riemann s zeta function and bernoulli numbers@xcite @xmath336 using the integral relation@xcite and treating @xmath132 as a positive integer ( which does not affect the asymptotics ) @xmath337 thus , the solution to @xmath338 can be approximated by @xmath339 again , using series representations , we obtain @xmath340 combining the results , we obtain , when @xmath118 and @xmath12 , @xmath341        we can then estimate @xmath24 by @xmath347 , i.e. , @xmath348 which is biased at the order @xmath349 . to remove the @xmath349 term of the bias , we recommend a bias - corrected version obtained by taylor expansions ( * ? ? ?",
    "* theorem 6.1.1 ) : @xmath350 from which we obtain the bias - corrected estimator @xmath351 whose bias and variance are @xmath159"
  ],
  "abstract_text": [
    "<S> * counting * is a fundamental operation . </S>",
    "<S> for example , counting the @xmath0th frequency moment , @xmath1^\\alpha$ ] , of a streaming signal @xmath2 ( where @xmath3 denotes time ) , has been an active area of research , in theoretical computer science , databases , and data mining . when @xmath4 , the task ( i.e. , counting the sum ) can be accomplished using a counter . when @xmath5 , however , it becomes non - trivial to design a small space ( i.e. , low memory ) counting system .    _ </S>",
    "<S> compressed counting ( cc ) _ is proposed for efficiently computing the @xmath0th frequency moment of a data stream @xmath2 , where @xmath6 . </S>",
    "<S> cc is applicable if the streaming data follow the _ turnstile _ model , with the restriction that at the time @xmath3 for the evaluation , @xmath7\\geq 0 , \\forall i\\in[1,d]$ ] , which includes the _ strict turnstile _ model as a special case . for data streams in practice , </S>",
    "<S> this restriction is minor .    </S>",
    "<S> the underlying technique is _ skewed stable random projections _ , which captures the intuition that , when @xmath4 a simple counter suffices , and when @xmath8 with small @xmath9 , the sample complexity should be low ( continuously as a function of @xmath9 ) . </S>",
    "<S> we show the sample complexity ( number of projections ) @xmath10 , where @xmath11 as @xmath12 . </S>",
    "<S> in other words , for small @xmath9 , @xmath13 instead of @xmath14 .    </S>",
    "<S> the case @xmath12 is practically very important . </S>",
    "<S> it is now well - understood that one can obtain good approximations to the entropies of data streams using the @xmath0th moments with @xmath15 and very small @xmath9 . for statistical inference using the _ method of moments </S>",
    "<S> _ , it is sometimes reasonable use the @xmath0th moments with @xmath0 very close to 1 . as another example </S>",
    "<S> , @xmath9 might be the `` decay rate '' or `` interest rate , '' which is usually small . </S>",
    "<S> thus , _ compressed counting _ will be an ideal tool , for estimating the total value in the future , taking in account the effect of decaying or interest accruement .    </S>",
    "<S> finally , our another contribution is an algorithm for approximating the logarithmic norm , @xmath16 $ ] , and the logarithmic distance , @xmath17 - b_t[i]\\right|$ ] . </S>",
    "<S> the logarithmic norm arises in statistical estimations . </S>",
    "<S> the logarithmic distance is useful in machine learning practice with heavy - tailed data . </S>"
  ]
}