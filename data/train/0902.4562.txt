{
  "article_text": [
    "needless to say , the problem of finding the zeroes of a function is indeed one of the central issues in every branch of science , since many others can be reduced to it .",
    "scalar equations of the form @xmath4 , where @xmath5 is an @xmath1-dimensional vector , can be rewritten as @xmath6 .",
    "nonlinear systems of equations @xmath7 can be rewritten as @xmath8 , where @xmath9 is a vector valued function , and then reduced to a scalar equation by taking @xmath10 .",
    "when talking about finding the root of a function the dimension of the space plays a crucial role . for functions in one variable",
    "well estabilished and efficient algorithms exist , like the bisection , secant , newton - raphson methods and many others .",
    "these methods can either be higly efficient in terms of rate of convergence but may fail to converge in certain cases or can ensure convergence at the cost of a slower rate .",
    "the key feature in one dimension is that it is always possible to bracket the solution between two points , use such points as an estimate for the root and then improve above them .",
    "this is what allows to develop convergent algorithms . in two or more dimensions",
    "the problem is completely different because the boundary of any domain containing the solution is made up of an infinity of points rather than only two .",
    "thus multidimensional root - finding problems are usually deterministically approached by methods involving local quantities , such as a guess of the solution and the derivatives of the function around it .",
    "these are extensions of the newton - raphson method@xcite and may fail to converge if the function is not sufficiently well behaved in the domain of interest : rapidly oscillating or not - differentiable functions pose significant problems in the application of such methods .    as an alternative to deterministic methods , stochastic root finding algorithms",
    "provide an efficient way to cope with bad bahaved functions , and are the only available tool when the value of the function itself can not be known exactly without a high computational cost@xcite . another promising alternative to root finding and other optimization problems",
    "is represented by genetic algorithms and evolutionary programming @xcite , in which the `` survival of the fittest '' paradigm is used to select the solution among a population of initial candidates which are progressively mutated and discarded basing on a suitable fitness function .",
    "one of the key problems affecting all the above - mentioned methods is that at least one initial guess of the solution is required , from which the method starts .",
    "such initial guess is what ultimately determines if the method will converge to the correct solution or will get trapped inside a local minumum or other bad behaved regions .",
    "this weakness is due to the intrinsic locality of such methods , which try to improve on the available solution basing on informations regarding only a neighborhood of the current estimate .    in the present paper an exact analytical result that allows to directly compute the position of the root of a function",
    "is presented .",
    "basing on this result a numerical method to find the solution is developed , which is intrinsically global and does not need an initial guess of the solution .",
    "let us consider a scalar function @xmath3 defined over an @xmath1-dimensional domain @xmath2 having a unique zero at @xmath11 .",
    "we will further assume that there exists a simply - connected open set @xmath12 , of arbitrarily small characteristic size @xmath13 , such that @xmath14 and over which @xmath3 is differentiable .",
    "in practice we require @xmath3 and @xmath2 to be well behaved just over an arbitrarily small domain containing the root on its boundary . given @xmath3 and @xmath2 satisfying the above conditions let us consider the family of functions @xmath15 where @xmath16 is a real parameter and @xmath17 is integer . since by hypotesis @xmath18 is the only root of @xmath3 , these functions have an absolute maximum at that point , where they reach the value @xmath19 .",
    "we can now switch to the problem of finding the position of the maximum of @xmath20 , which is indeed equivalent to finding the zero of @xmath3 . to solve this problem",
    "we borrow a very well known concept from classical mechanincs : the center of mass of a body with nonuniform mass density tends to lie near the points where the mass density is higher . with this in mind",
    "we can intuitively think that if a body has a mass density which diverges at a point , the center of mass should reasonably be near that point .",
    "thus we regard the function @xmath20 as the mass density of an @xmath1-dimensional object , whose shape is the domain @xmath2 , and compute the position of its center of mass when the parameter @xmath16 becomes infinitely small .",
    "we recall that the center of mass @xmath21 of an @xmath1-dimensional non - uniform body is computed as @xmath22 where @xmath23 is the mass density . in our case",
    "we take @xmath24 in the limit @xmath25 , thus we write @xmath26 the key point is that the integrals in ( [ b ] ) can always be made divergent in the limit @xmath25 for sufficiently high values of @xmath17 , depending on the dimension of @xmath2 and the multiplicity of the root . thus assuming a suitable value for @xmath17 we can restrict the integration to any domain over which the integrals diverge without affecting the result , in partucular we choose to integrate over @xmath27 and then perform a variable change @xmath28 , obtaining @xmath29 where @xmath30 is the @xmath13-set having the point @xmath31 on its boundary . at this stage we can already see that the result of the computation is the searched solution @xmath18 plus an @xmath16-dependent error term @xmath32 which we shall show to go to zero as @xmath16 vanishes .",
    "since @xmath30 is arbitrarily small we can subsitutue @xmath33 with its lowest order taylor expansion , which will depend on the multiplicity of the root . without loss of generality",
    "we will assume that @xmath34 over @xmath35 .",
    "in doing so we notice that , in the limit of vanishing @xmath16 , at the denominator we are integrating a function with a singularity of the type @xmath36 , while at the numerator the integrand has a @xmath37 singularity .",
    "the limit of the ratio of the two integrals is then zero and we can conclude that @xmath38 thus the position of the root of @xmath3 can be directly computed and corresponds to the center of mass of an @xmath2-shaped object having a singular mass density .",
    "equation ( [ a ] ) is exact and valid under broad assumptions and many numerical methods could be derived from it depending on how the integrals are actually handled .",
    "any practical numerical implementation of ( [ a ] ) will be confronted with the fact that the integrals are divergent in the limit @xmath25 , a feature which is a necessary condition for the result to hold .",
    "even when using finite values of @xmath16 we are left with the problem of computing @xmath39 @xmath1-dimensional integrals , a task whose computational cost could make the proposed method not competitive with other approaches , in fact , any deterministic discretization of the integrals would imply a computational cost growing very rapidly with the number of dimensions of @xmath2 . on the other hand statistical methods , such as monte carlo sampling ,",
    "are much more effective for high dimensional problems because their rate of convergence is always @xmath40 for @xmath41 samplings , regardless of the dimension@xcite .",
    "although a simple monte carlo sampling would then suffice , its rate of convergence would still be poor confronted with , e.g. , the bisection method which has a rate of convergence of @xmath42 for @xmath41 iterations , or newton - raphson methods which are even faster , provided that the initial guess is close enough to the solution to make them converge .",
    "in computing the integrals in ( [ a ] ) we can exploit a feature that will help speeding up things .",
    "we note that the two integrals to be computed are very similar , the only difference being that the function @xmath43 is multiplied by @xmath5 in the numerator .",
    "moreover we know that the functions to be integrated reach very high values on a little region around the solution , while they are almost null everywhere else .",
    "both features can be taken into account by developing an adeguate monte carlo sampling procedure in which the two integrals are computed in parallel : every time the numerator is sampled at the point @xmath44 , the denominator is sampled at the same point .",
    "thus after @xmath41 samplings the estimated solution @xmath45 is computed as @xmath46 where @xmath44 is a sequence of points in @xmath2 , generated according to the probability density @xmath47 , which is used for the evaluation of both the numerator and the denominator .",
    "thus if during the sampling the numerator has a sudden jump due to @xmath44 lying near the singularity , the denominator will jump too , and this will reduce the fluctuations of the ratio , causing @xmath45 to jump towards the exact value .",
    "this parallel - sampling choice has no formal justification besides the fact that it turns out to work well , in fact , even if we know that both integrals will converge to the respective solutions as @xmath40 , we can not in principle tell anything about the rate of convergence of their ratio , because the two samplings are higly correlated in way that depends on the particular problem .",
    "another useful property of ( [ mc ] ) derives from the fact that we are computing the ratio of two integrals and thus the final result is independent from the normalization of @xmath48 , which is a major issue in monte carlo sampling techniques .",
    "the simplest choice is to adopt a constant probability density @xmath49 to generate the sampling points . in the following",
    "we shall show that in this case the method converges even in the case @xmath50 , and its rate of convergence is @xmath51 .",
    "to do so we evaluate ( [ c ] ) using the parallel - sampling method with uniform probability density , after setting @xmath50 .",
    "we obtain @xmath52 where @xmath44 is a sequence of uniformly distributed points inside @xmath53 and @xmath54 is the error after @xmath41 samplings .",
    "if we now consider the quantity @xmath55 we can write @xmath56 since @xmath57 , for some @xmath58 , on an arbitraryly small domain around @xmath31 .    we can rearrange the elements of the sequence @xmath59 in order to @xmath60 to be the smallest value and then write @xmath61 now substituting ( [ e ] ) in ( [ d ] ) we obtain @xmath62 since the points @xmath44 are uniformly distributed over the @xmath1-dimensional domain @xmath2 the average distance between two points , in the limit @xmath63 , will be of the order of @xmath64 and thus @xmath60 will be of that order too .",
    "then , in the limit of large @xmath41 , the term @xmath65 in ( [ e ] ) will diverge while the rest of the summation will remain finite , we can then say that @xmath66 and thus the sequence @xmath45 converges to @xmath18 with a rate of convergence of @xmath64 .",
    "thus in the case of uniform sampling the method is ( almost surely ) convergent but its rate of convergence is poor , depends on the dimension of the space and in particular becomes lower for higher dimensional problems .",
    "the efficiency of monte carlo sampling techniques is highly enhanced when the distribution of sampling points resembles the function to be integrated , thus we developed an adaptive - sampling method in which the probability density is varied during the process in order to maximize the efficiency .",
    "the key feature that allows such a strategy is again the fact that we are computing the ratio of two integrals and thus we are allowed to use non - normalized probability densities for the generation of sampling points . in particular , the parallel - sampling tecnique allows us to change the probability during the process : a peaked probability distribution is chosen for the generation of the sampling points , and its shape is gradually varied in order to concentrate the sampling around the solution .",
    "the method proceeds as follows : a gaussian probability density of the form @xmath67 is used for the distribution of sampling points , where @xmath68 is the @xmath69th - component of @xmath5 , @xmath70 is the estimation of the solution at step @xmath71 and @xmath72 is a suitable measure of the fluctuations of @xmath73 up to step @xmath71 . when the process starts , @xmath74 is set to a value such that the gaussian is flat over @xmath2 .",
    "the estimation @xmath75 loses all meaning in this case and can be set to an arbitrary point .",
    "after a few samplings , a new estimation of the solution is available , as well as an estimation of its fluctuations .",
    "these values are then used to update the probability density @xmath76 .",
    "the key point is that as the sampling proceeds , the estimation comes closer to the exact solution and the fluctuations are reduced , thus @xmath76 is gradually sharpened and centered around @xmath18 .",
    "this makes @xmath76 resemble the functions to be integrated , making the sampling more efficient and further reducing the fluctuations , which in turn make @xmath76 more peaked around the solution .",
    "this adaptive - sampling technique must be tuned carefully .",
    "if the variance of the gaussian is reduced too fast , it can happen that the region around @xmath18 is never sampled and thus the process converges to a random point and fails to find the correct solution .",
    "conversely , if the variance is reduced too slowly , ther is no real advantage in using an adaptive algorithm and the results are the same as using a constant uniform probability density . in practice",
    "the key parameters are the number of samplings between two successive updates of @xmath76 and the way @xmath77 is computed , a suitable choice has been found to be @xmath78 where the number of samplings @xmath41 over wich the mean is performed determines the speed at which the variance of the gaussian is reduced , which in turn affects the rate of convergence .    .",
    "figures on the bottom row show the corresponding convergence tests for the adaptive monte carlo sampling method : dashed lines show the the fluctuation @xmath77 of the solution , the continuous line is the actual error @xmath79 . ]",
    "figure ( [ fig1 ] ) shows three one - dimensional test problems ( top figures ) against which the method has been tested in order to check its convergence . in these examples",
    "@xmath47 is updated every @xmath80 samplings and @xmath81 the graphs at the bottom show typical convergence curves from which it can be seen that the rate is approximatively exponential ( continuous line ) .",
    "it can also be noted that the parameter @xmath77 ( dashed line ) usually overestimates the actual error and can thus be taken as a conservative estimate .",
    "actually , figs .",
    "[ fig1]a and [ fig1]c show exceptions to this behaviour : this is caused by the finite value of @xmath16 which introduces an intrinsic uncertainty in the determination of the solution and thus the error curves level at a certain value ; anyway such error can be made arbitrarily small by reducing @xmath16 .",
    "a similar test has been performed for multidimensional problems in fig .",
    "[ fig2 ] where the error @xmath82 is plotted against the number of monte carlo samplings .",
    "the function considered is the @xmath1-dimensional equivalent of fig .",
    "it can be seen that the exponential rate of convergence is mantained in every dimension , differing only in the slope of the function .",
    "this is due to the fact that as the number of dimensions is increased , the parameter @xmath41 in ( [ sigma ] ) has to be increased in order to ensure convergence . in this case",
    "@xmath83 for the 1d problem and @xmath84 for the 5d problem .",
    "we have shown that the adaptive sampling method dramatically increases the rate of convergence with respect to the uniform sampling , at the cost of introducing the possibility that it could fail to converge in some cases .",
    "this weakness can actually be effectively turned into a benefit .",
    "the failure of the adaptive - sampling tecnique derives from having introduced some locality in the method , indeed , using a gaussian probability density corresponds to limiting the domain considered for the computation of integrals in ( [ a ] ) .",
    "this locality can then drive the method to converge to local minima or to points inside regions where @xmath3 reaches very low values .",
    "actually the worst case would be that of a function @xmath3 which has two roots in @xmath2 : in this case the function @xmath85 would have two singularities that would make the uniform - sampling method not converge to any of the two . on the other side ,",
    "the adaptive - sampling method , due to its tendency to drive the probability density around points where @xmath3 has very low values , will converge to one of the two solutions .",
    "this feature can be exploited to develop an automated algorithm that sequentially discovers all the roots of @xmath3 .",
    "consider a function @xmath3 defined on @xmath2 which has a number of roots at points @xmath86 .",
    "the adaptive - sampling method then the method will converge to one of the solutions , say @xmath87 . a this point",
    "if the region surrounding @xmath87 is removed from the domain @xmath2 and the process is repeated , the method will converge to another solution , say @xmath88 .",
    "this loop can be iterated in order to find all the roots of @xmath3 .",
    "we have proposed a global root - finding algorithm which allows to compute the position of the root of a scalar function over an @xmath1-dimensional domain under very broad assumptions .",
    "an exact analytical result is given which transforms the inverse problem of finding the root of a function to a direct computation of its position , interpreted as the center of mass of an @xmath1-dimensional @xmath2-shaped object with singular mass density .",
    "in view of this result a parallel monte carlo sampling method has been developed and proven to converge with a rate of @xmath64 .",
    "the method is then extended in order to employ adaptive sampling of the integrals , providing a numerical method with exponential rate of convergence which can be further extended to an automated root - search algoritm applicable to functions with multiple roots .",
    "the proposed methods , being based on integral quantities , are global in nature and applicable to a wide variety of cases in which the function is bad behaved ( not differentiable , singular , rapidly oscillating , ... ) or the domain @xmath2 is not regular ( not simply connected , irregular shape , ... ) .",
    "this paper is a demonstration of the possibilities of the proposed method , further work is in progress in order to improve its convergence rate and range of applicability .",
    "a wide variety of methods can be derived from equation ( [ a ] ) , depending on the specific choice of the @xmath85 function and the specific method used to compute the integrals , and this could open the way towards more efficient methods ."
  ],
  "abstract_text": [
    "<S> a method to solve the problem @xmath0 efficiently on any @xmath1-dimensional domain @xmath2 under very broad hypoteses is proposed . </S>",
    "<S> the position of the root of @xmath3 , assumed unique , is found by computing the center of mass of an @xmath2-shaped object having a singular mass density . </S>",
    "<S> it is shown that although the mass of the object is infinite , the position of its center of mass can be computed exactly and corresponds to the solution of the problem . </S>",
    "<S> the exact analytical result is implemented numerically by means of an adaptive monte carlo sampling technique which provides an exponential rate of convergence . </S>",
    "<S> the method can be extended to functions with multiple roots , providing an efficient automated root finding algorithm . </S>"
  ]
}