{
  "article_text": [
    "affine space models are simple to compute with a principal component analysis ( pca ) but are not appropriate to approximate signal classes that include complex forms of variability .",
    "image classes are often invariant to rigid transformations such as translations or rotations , and include elastic deformations , which define highly non - linear manifolds .",
    "textures may also be realizations of strongly non - gaussian processes that can not be discriminated with linear models either .",
    "kernel methods define distances @xmath0 , with operators @xmath1 which address these issues by mapping @xmath2 and @xmath3 into a space of much higher dimension .",
    "however , invariance properties and learning requirements on small training sets , rather suggest to implement a dimensionality reduction .",
    "if @xmath1 has appropriate invariants and linearizes small deformations then this paper shows that affine spaces become powerful classification models in the transformed domain .",
    "suppose that @xmath4 is translated and deformed into @xmath5 .",
    "let @xmath6 be sup matrix norm of the deformation tensor @xmath7 . to linearize small deformations , we impose the following lipschitz regularity condition : @xmath8 a small signal deformation yields a metric modification which is bounded by the deformation amplitude .",
    "a fourier transform modulus @xmath9 is invariant to translation but does not satisfy ( [ lipdefosns ] ) because high frequencies are severely modified by small deformations .",
    "existing invariant representations do not seem either to satisfy this property .",
    "localized transforms such as wavelet transforms are stable relatively to local deformations but are not translation invariant .",
    "scattering operators constructed in @xcite , are invariant to translations and lipschitz continuous relatively to local deformations , up to a log term .",
    "these scattering operators @xmath1 create invariants by representing signal high frequencies with interference coefficients . this paper models complex signal classes with low - dimensional affine spaces in the scattering domain , which are computed with a pca .",
    "the classification is performed by a penalized model selection .",
    "scattering operators may also be invariant to any compact lie subgroup of @xmath10 , such as rotations , but we concentrate on translation invariance , which carries the main difficulties and already covers a wide range of classification applications .",
    "section [ sec2 ] reviews the construction of scattering operators with a cascade of wavelet transforms and modulus operators , which defines a non - linear convolution network @xcite .",
    "section [ section_classif ] shows that learning affine scattering model spaces has a linear complexity in the number of training samples .",
    "section [ sec_num_results ] describes state of the art classification results obtained from limited number of training samples in the mnist hand - written digit database , and for texture classification in the curet database .",
    "softwares are available in `` www.cmap.polytechnique.fr/scattering '' .",
    "in order to build a representation which is lipschitz continuous to deformations , a scattering transform begins from a wavelet representation .",
    "translation invariance is obtained by progressively mapping high frequency wavelet coefficients to lower frequencies , with modulus operators described in section [ waveletmodulu ] . scattering operators",
    "iterate over wavelet modulus operators .",
    "section [ scaopern ] shows that it defines a translation invariant representation , which is lipschitz continuous to deformation , up to a log term .",
    "a fast computational algorithm is given in section [ fastalgo ] .",
    "this section explains how to represent signal high frequencies with lower frequency interference terms , computed with a wavelet transform modulus .",
    "a wavelet transform extracts information at different scales and orientations by convolving a signal @xmath2 with dilated bandpass wavelets @xmath11 having a spatial orientation angle @xmath12 : @xmath13 at the largest scale @xmath14 , low - frequencies are carried by a lowpass scaling function @xmath15 : @xmath16 , with @xmath17 and @xmath18 .",
    "the resulting wavelet representation is @xmath19 the norm of the wavelet operator is defined by @xmath20 with @xmath21 and it satisfies @xmath22 if and only if for all @xmath23 , @xmath24    we consider families of complex wavelets @xmath25 where @xmath26 are low - pass envelops .",
    "separable complex wavelets may be constructed from the analytical part of wavelets defining orthonormal bases @xcite , in which case @xmath27 and the wavelet transform is unitary .",
    "oriented gabor functions are other examples of complex wavelets , obtained with a modulated gaussian @xmath28 , which is rotated with @xmath29 by an angle @xmath30 : @xmath31 . in numerical experiments , we set @xmath32 , @xmath33 , @xmath34 , and @xmath15 is also a gaussian with @xmath35",
    ". it satisfies ( [ littlewood2 ] ) only over a finite range of scales .    if @xmath36 then @xmath37 if and only if @xmath38 , because @xmath39 has derivatives of amplitude proportional to @xmath40 .",
    "high frequencies corresponding to fine scales are thus highly sensitive to translations .",
    "translation invariance is improved by mapping high frequencies to lower frequencies with a complex modulus operator . since @xmath41",
    ", we verify that @xmath42 where @xmath43 , @xmath44 and @xmath45 .",
    "wavelet coefficients @xmath46 are located at high frequencies because of the @xmath47 term .",
    "these oscillations are removed by a modulus operator @xmath48 is now mostly concentrated in the low frequency domain covered by the envelop @xmath49 .",
    "it may however also include some high frequencies produced by the modulus singularities where @xmath50 .",
    "using complex wavelets is important to reduce the number of such singularities and thus concentrate the information at low frequencies .",
    "if @xmath51 then one can verify that @xmath52 where @xmath53 is an interference term .",
    "it is a combination of the @xmath54 , for all @xmath55 and @xmath56 in the support of @xmath57 .",
    "the modulus yields interferences that depend upon frequency intervals , but it loses the exact frequency locations @xmath55 in each octave .    a complex modulus is applied to all wavelet coefficients , but not to the low frequencies @xmath58 , which defines a wavelet modulus propagator : @xmath59 since @xmath60 and the wavelet transform is contractive , it results that @xmath61 and @xmath62 if @xmath27 in ( [ littlewood1 ] ) .",
    "a scattering operator iterates on the propagation operator @xmath63 @xcite , and defines a convolutional network .",
    "it is contractive , translation invariant and lipschitz continuous to deformations , up to a log factor .",
    "a scattering operator is defined along a path @xmath64 which is a family of wavelet indices .",
    "it computes @xmath65 wavelet convolutions and modulus along this path : @xmath66 with @xmath67 and @xmath68 .",
    "the scattering output at the scale @xmath14 is @xmath69    one can verify that scattering coefficients for paths of length @xmath70 are computed by applying the wavelet modulus propagator @xmath63 to scattering coefficients for all paths @xmath71 of length @xmath72 : @xmath73 a scattering transform is thus obtained by cascading the propagator @xmath63 , which defines a convolution network illustrated in figure [ scatfigure ] .",
    "computing @xmath74 generates a first layer of transformed signals @xmath75 for @xmath76 , and transforms @xmath2 into @xmath77 .",
    "suppose that all scattered signals @xmath78 have already been computed for @xmath79 . according to ( [ cascadeop ] ) , the next layer @xmath70 is calculated by applying @xmath63 to each @xmath78 on the @xmath80 layer where @xmath72 .",
    "it also transforms the @xmath80 layer of @xmath78 into @xmath81 .    the scattering operator @xmath82 is thus computed along all possible paths with a convolutional network .",
    "convolution networks are general computational architectures introduced by lecun @xcite , that involve convolutions and non - linear operators .",
    "they are usually constructed with deep - learning back - propagation algorithms @xcite to learn the filter coefficients .",
    "they have been succesfully applied to number of recognition tasks @xcite and proposed as models for visual perception @xcite . in this case , the filters are dilated wavelets which are not learned , and the non - linearities are modulus operators .    as opposed to standard layered neural networks which output the last layer , all nodes of a scattering network output a signal @xmath82 which is used for classification . depending upon @xmath65",
    ", @xmath82 gives different type of information on @xmath2 .",
    "for @xmath83 , @xmath84 averages the signal . for @xmath85",
    ", @xmath86 is an averaged wavelet modulus , which tends to the @xmath87 norm of wavelet coefficients when @xmath88 increases . for @xmath89",
    ", it provides higher order interferences between the signal components along different orientations and frequency octaves .    for appropriate complex wavelets",
    ", one can prove @xcite that the energy @xmath90 of a scattering layer @xmath91 tends to zero as @xmath91 increases .",
    "this decay is fast .",
    "numerically the maximum network depth is typically limited to @xmath92 .    .",
    "]    the scattering metric for @xmath93 is obtained with a summation over all paths @xmath71 : @xmath94 where @xmath95 . since @xmath96 is calculated in ( [ cascadeop ] ) by iterating on the contractive propagator @xmath63 , it results that it is also contractive @xcite @xmath97 scattering operators are not only contractive but also preserve the norm . for appropriate complex wavelets which satisfy ( [ littlewood2 ] ) for @xmath27",
    ", one can prove @xcite that @xmath98 .",
    "when a signal is translated @xmath99 , the scattering transform is also translated @xmath100 because it is computed with convolutions and modulus .",
    "however , when @xmath88 increases , @xmath101 tends to a constant because of the convolutions with @xmath102 .",
    "it thus becomes translation invariant and one can verify @xcite that the asymptotic scattering metric is translation invariant : @xmath103    for classification the key scattering property is its lipschitz continuity to deformations @xmath104 .",
    "let @xmath105 and @xmath106 , where @xmath6 is the matrix sup norm of @xmath7 .",
    "along paths of length @xmath107 , one can prove @xcite that for all @xmath108 the scattering metric satisfies latexmath:[\\[\\label{lipschitz } \\|s_{j } d_\\tau f- s_{j}f\\| \\leq c \\,m_0 \\,\\|f\\|\\ ,    the scattering operator is thus lipschitz continuous to deformations , up to a log term .",
    "it shows that for sufficiently large scales @xmath14 , the signal translations and deformations are locally linearized by the scattering operator .",
    "fast scattering computations are possible because the scattering energy @xmath110 is highly concentrated along a small set of paths .",
    "a scattering transform is calculated along these paths with an @xmath111 algorithm .",
    "section [ waveletmodulu ] explains that @xmath112 has an energy mostly located over lower frequencies . as",
    "a result @xmath113 is negligible if @xmath114 .",
    "the scattering energy is thus concentrated on progressive paths @xmath115 which satisfy @xmath116 . since @xmath117",
    ", progressive paths have a length @xmath118 .",
    "since @xmath119 tends to zero as @xmath65 increases , progressive paths are computed up to a maximum length @xmath107 , which is typically @xmath120 in numerical applications .",
    "if there are @xmath121 different mother wavelets @xmath11 then the number of progressive paths @xmath71 of length @xmath107 is @xmath122 .",
    "the scattering transform is implemented along progressive paths for @xmath107 , by iterating on wavelet transforms and modulus operators , with the layered network illustrated in figure [ scatfigure ] .",
    "each @xmath123 is computed with wavelet convolutions and is sampled at intervals @xmath124 where @xmath125 is the scale of the last wavelet on the path .",
    "the oversampling factor is typically @xmath126 to avoid aliasing .",
    "the averaged signal @xmath127 is sampled at intervals @xmath128 .",
    "the addition of a new element @xmath129 at the end of a path @xmath71 is written @xmath130 .",
    "set @xmath131 = f[n]$ ] , @xmath132 @xmath133= |s(p)f \\star \\psi_{j,\\gamma}[n\\delta 2^{j - j_{|p|}}]|$ ] @xmath134 = s(p)f \\star \\phi_j [ n\\delta 2^{j - j_{|p|}}]$ ] @xmath134 = s(p)f \\star \\phi_j [ n\\delta 2^{j - j_{|p|}}]$ ] output : @xmath135    one can verify that the scattering signals @xmath82 along all progressive paths of length @xmath107 include @xmath136 coefficients .",
    "the computational cost is driven by the number of operations to compute the subsampled wavelet transform convolutions along @xmath121 orientations . with a fast filter bank algorithm",
    ", it requires @xmath137 operations for a signal of size @xmath138 .",
    "the overall complexity of this algorithm is thus @xmath139 .",
    "direct convolution calculations with ffts bring in an extra factor @xmath140 .",
    "translation invariance and lipschitz regularity to local deformations linearize small deformations .",
    "signal classes can thus be approximated with low - dimensional affine spaces in the scattering domain .",
    "although the scattering representation is implemented with a potentially deep convolution network , learning is not deep and it is reduced to pca computations .",
    "the classification is implemented with a penalized model selection .",
    "a signal class @xmath141 can be modeled as a realization of a random process @xmath142 .",
    "there are multiple sources of variability , due to the reflectivity of the material as in textures , due to deformations or to various illuminations .",
    "illumination variability is often low - frequency and can be approximated in linear spaces of dimension close to @xmath143 @xcite .",
    "this property remains valid in the scattering domain .",
    "a scattering operator also linearizes local deformations and reduces the variance of large classes of stationary processes .",
    "one can thus build a linear affine space approximation of @xmath144 .",
    "a scattering transform @xmath144 along progressive paths of length @xmath107 is a vector of size @xmath111 , which may be much smaller then @xmath138 if @xmath88 is large .",
    "the affine space @xmath145 of dimension @xmath146 which minimizes the expected projection error @xmath147 is @xmath148 where @xmath149 and @xmath150 is the space generated by the first @xmath146 eigenvectors of the covariance operator of @xmath151",
    ". the space dimension @xmath146 is limited to a maximum value @xmath152 .",
    "these affine space models are estimated by computing the empirical average and the empirical covariance of @xmath101 , for all training signals @xmath153 .",
    "the empirical covariance is diagonalized to estimate the @xmath152 eigenvectors of largest eigenvalues . under mild conditions @xcite",
    ", the sample covariance matrix @xmath154 converges in norm to the true covariance when the number of training signals is of the order of the dimensionality of the space where @xmath144 belongs .",
    "dimensionality reduction is thus important to learn affine space models from few training signals .",
    "compute @xmath155 compute the empirical average @xmath156 and covariance @xmath157 compute with a thin svd the @xmath152 eigenvectors of @xmath154 of largest eigenvalues .    the computational complexity to estimate affine space models @xmath158 is dominated by eigenvectors calculations . to compute the first @xmath152 eigenvectors",
    ", a thin svd algorithm requires @xmath159 operations , where @xmath160 is the number of training signals .",
    "let us consider a classification problem with several classes @xmath161 .",
    "we introduce a classification algorithm which selects affine space models by minimizing a penalized approximation error .    each class @xmath162",
    "is represented by a family of embedded affine spaces @xmath163 , where @xmath164 is the space generated by the first @xmath146 eigenvectors @xmath165 of the empirical covariance matrix @xmath166 . for a fixed dimension @xmath146 ,",
    "a space @xmath167 is discriminative for @xmath168 if the projection error of @xmath155 in @xmath167 is smaller than its projection in the other spaces @xmath169 : @xmath170 with latexmath:[\\[\\|s_j f - p_{\\mathbf{a}_{k , i}}(s_{j } f)\\|^2   =   \\|s_j f - \\hat \\mu_i \\|^2   -    \\sum_{l=1}^k     model selection for classification is not about finding an accurate approximation model as in model selection for regression but looks for a discriminative model @xcite .",
    "if @xmath155 for @xmath168 is close to the class centroid @xmath172 then low - dimensional affine spaces @xmath167 are highly discriminative even if the remaining error is not negligible , because it is unlikely that any other low - dimensional affine space @xmath169 yields a comparable error .",
    "if @xmath2 is an `` outlier '' which is far from the centroid @xmath172 then a higher dimensional approximation space @xmath167 is needed for discrimination .",
    "it is therefore necessary to adjust the dimensionality of the discrimination space to each signal @xmath2 . the class index @xmath173 of @xmath2 is estimated by adjusting the dimension @xmath146 of the space @xmath167 that yields the best approximation , with a penalization proportional to the space dimension @xmath146 @xcite : @xmath174    compute @xmath175 @xmath176 @xmath177    the minimum penalized energy @xmath178 is computed from the @xmath179 inner products of @xmath155 with the eigenvectors @xmath180 that generate the embedded affine spaces @xmath181 for @xmath182 .",
    "the overall computational complexity is thus @xmath183 .",
    "this classification algorithm depends upon the penalization factor @xmath184 and the scale @xmath14 of the scattering transform .",
    "these two parameters are optimized with a cross - validation mechanism .",
    "it minimizes a classification error computed on a validation subset of the training samples , which does not take part in the affine model learning .    *",
    "increasing the scale @xmath14 reduces the intra - class variability of the representation by building invariance , but it can also reduce the distance across classes .",
    "the optimal size @xmath14 is thus a trade - off between both . *",
    "the penalization parameter @xmath184 is similar to a threshold on @xmath185 .",
    "the model increases the dimension @xmath146 of the approximation space if the inner product is above @xmath184 . increasing @xmath184",
    "thus reduces the dimension of the affine model spaces , which is needed when the training sequence is small .",
    "this section presents classification results for handwritten digit recognition , and for texture discrimination with illumination variations .",
    "the scattering transform is implemented with the same gabor wavelets along @xmath186 orientations for both problems , and the maximum scattering length is limited to @xmath187 .",
    "the mnist hand - written digit database provides a good example of classification with important deformations .",
    "table [ mnist ] compares scattering classification results for training sets of variable size , with results obtained with deep - learning convolutional networks @xcite , which currently have the best results .",
    "table [ mnist ] compares the pca model selection algorithm applied on scattering coefficients and an svm classifier with polynomial kernel whose degree was optimized , also applied on scattering coefficients .",
    "cross validation finds an optimal scattering scale @xmath188 , which corresponds to translations and deformations of amplitude about @xmath189 pixels , which is compatible with observed deformations on digits .    below @xmath190 training examples",
    ", a pca scattering classifier provides state of the art results .",
    "it yields smaller errors than deep - learning convolution network which require large training sets to optimize all network parameters with backpropagtion algorithms . for @xmath191 training samples ,",
    "the deep - learning convolution network error @xcite is below the scattering classifier error .",
    "table [ mnist ] shows that applying a linear svm classifier over the scattering transform degrades the results relatively to a pca classifier up to large training sets , and it requires much more computations .",
    "this is an indirect validation of the linearization properties of the scattering transform .",
    ".percentage of error as a function of the training size for mnist .",
    "minimum errors are in bold .",
    "the last column gives the average model space dimension @xmath192 . [ cols=\"^,^,^,^,^\",options=\"header \" , ]     curet textures have a strong variability due to illumination .",
    "for a given three - dimensional surface , grey level variability belong to a linear space of dimension of the order of @xmath143 @xcite , mostly generated by low - dimensional functions when the surface is regular .",
    "textures are not regular three - dimensional surfaces , however scattering operators seem to preserve this low - dimensional approximation capabilities , which may partly explain the large improvements of linear pca model selections relatively to svm classifications over scattering coefficients .",
    "learning adjusts the scattering scale to @xmath193 by cross - validation , which is much larger than for hand - written digit recognition . choosing a large scale is necessary to reduce the variance of scattering estimators .",
    "indeed , for a given illumination and pose , a texture can be modeled as a realization of a stationary process @xmath142 .",
    "if @xmath194 is stationary , since @xmath195 is obtained through convolution and modulus operators , it remains stationary .",
    "moreover @xmath196 , so @xmath197 which does not depend upon @xmath88 and @xmath198 .",
    "the average scattering coefficients @xmath199 provide descriptors which discriminate stationary processes including processes having the same fourier power spectrum .",
    "for a large class of processes , a single texture realization has a variance @xmath200 which decreases exponentially with @xmath88 @xcite .",
    "figure [ curedecay ] shows the exponential decay of @xmath201 as a function of @xmath88 , averaged over several texture classes .",
    "all textures @xmath142 are normalized with a zero - mean and a unit variance .",
    "this exponential variance reduction is due to the scattering averaging by @xmath102 and to the iterated removal of random phase fluctuations by scattering modulus operators .",
    "the cross - validation choice of a large @xmath88 results from the need to reach a sufficiently small variance @xmath201 to optimize classification results .     in log - scale as a function of @xmath88 .",
    "as a result of their translation invariance and lipschitz regularity to deformations , scattering operators provide appropriate representations to model complex signal classes with affine spaces calculated with a pca .",
    "classification with model selection provides state of the art results with limited training size sequences , for handwritten digit recognition and textures .",
    "as opposed to discriminative classifiers such as svm and deep - learning convolution networks , these algorithms learn a model for each class independently from the others , which leads to fast learning algorithms .    for signal classes including large rotations or scaling deformations",
    ", it is necessary to use scattering operators which are invariant to these large deformations .",
    "this is done with a combined scattering operator which implements a wavelet transform propagator in space but also along angle parameters to become rotation invariant @xcite .",
    "classification of images but also of audio signals signals is then possible with linear affine space models on combined scattering representations .",
    "k. jarrett , k. kavukcuoglu , m. ranzato and y. lecun :  what is the best multi - stage architecture for object recognition ? \" , proc .",
    "international conference on computer vision ( iccv09 ) , ieee , 2009 .",
    "f. lauer , c. suen , g. bloch ;  a trainable feature extractor for handwritten digit recognition \" , pattern recognition 40 , 6 ( 2007 ) 1816 - 1824 .",
    "m.varma , a. zisserman :  a statistical approach to material classification using image patch exemplars \" .",
    "ieee transactions on pattern analysis and machine intelligence , 31(11):20322047 , november 2009 ."
  ],
  "abstract_text": [
    "<S> a scattering transform defines a signal representation which is invariant to translations and lipschitz continuous relatively to deformations . </S>",
    "<S> it is implemented with a non - linear convolution network that iterates over wavelet and modulus operators . </S>",
    "<S> lipschitz continuity locally linearizes deformations . </S>",
    "<S> complex classes of signals and textures can be modeled with low - dimensional affine spaces , computed with a pca in the scattering domain . </S>",
    "<S> classification is performed with a penalized model selection . </S>",
    "<S> state of the art results are obtained for handwritten digit recognition over small training sets , and for texture classification . </S>"
  ]
}