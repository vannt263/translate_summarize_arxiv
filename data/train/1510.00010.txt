{
  "article_text": [
    "* stationarity , synchronization and determinism . *",
    "if a process is _ stationary _ , it follows that @xmath58 for all @xmath59 , @xmath60 and hence @xmath61 for all @xmath59 , @xmath60 .",
    "any prescient states derived from this sequence then also satisfy @xmath62 for all @xmath59 , @xmath60 : the memory required to store the current position in the pattern does not change with time .",
    "if we can _ synchronize _ our memory , it means that we can determine which prescient state we are in from a sufficiently long observation of the past ( at least as many observations as the _ crypticity _ of the pattern  @xcite ) . expressed in terms of conditional entropy , @xmath63 , which reflects no uncertainty in the state @xmath33 if the entire past is known .",
    "closely related is the concept of _ determinism _ , which is when the state of the internal memory at the end of any action is completely fixed by the initial state of the memory and the symbols observed in the pattern .",
    "entropically expressed , @xmath64 .",
    "once a process has been synchronized to deterministic memory , it will remain synchronized .",
    "causal states are automatically deterministic  @xcite .",
    "* work cost of writing a pattern to a tape .",
    "* let us specifically examine the cost of writing @xmath28 parts of some pattern to a tape using a generator with prescient states @xmath21 .",
    "initially , all @xmath28 positions on the tape will be in their default uncorrelated state @xmath65 with associated entropy @xmath66 . for an outside observer who is not aware of the particular choice of symbols written by the machine ( but who knew the machine s initial internal state ) after the pattern has been written , the entropy of the tape will have changed to @xmath67 .",
    "this value can be expanded to @xmath68 .",
    "as @xmath21 is prescient , @xmath69 is exactly as useful as @xmath70 for predicting the future of @xmath36 .",
    "hence , the entropy of the tape can be re - written as @xmath71 .",
    "using stationarity , the entropy of the tape can then be rewritten as @xmath72 .",
    "finally , we note that all prescient states are as good as each other for predicting the future , and so we can replace @xmath73 with the equivalent entropy conditioned on causal states @xmath74 .",
    "the work cost of writing this pattern is hence proportional to this change in entropy : @xmath75    * work cost of updating the generator s memory .",
    "* the above generator must update its internal state from @xmath33 to @xmath34 before it can produce the next part of the pattern . as with the extractor",
    ", we admit an ancilla system of the same size as the internal memory @xmath21 , which starts and ends in the pure state @xmath40 .",
    "( see [ fig : updatestate ] . ) for deterministic memory @xmath64 and the ancilla can be set to state @xmath34 by a reversible operation with no associated work exchange . for indeterministic memory , the ancilla is slightly randomised and its entropy increases by @xmath76 ( as with the extractor ) .",
    "let us consider the cost of resetting the previous internal state @xmath33 back to @xmath40 .",
    "again , this does not cost the full entropy @xmath77 , since there may be some mutual information @xmath78 between the old state , and the new state and tape .",
    "[ constructively one could imagine this mutual information as being used to reduce the entropy of @xmath33 without work cost , transforming @xmath33 into a new state @xmath79 with entropy @xmath80 .",
    "] remarking that , as before , the increased entropy of @xmath33 due to indeterminism is completely negated by the work that could have been extracted by randomizing the ancilla in the first place , we see that the work cost of resetting @xmath33 to @xmath40 is @xmath81    we first remark that in the limit of @xmath82 , the information @xmath83 , the excess entropy , because @xmath21 is prescient and so contains all the information that could be known about the future . hence the work dissipated converges on @xmath84 in the limit of large  @xmath28 ( in agreement with @xcite for causal states ) .          * work extractable from a pattern . *",
    "consider an extractor whose internal memory begins in state @xmath33 .",
    "for ease of calculation , we admit an ancillary variable of the same size as the internal memory , that is initially in a pure state @xmath40 . if the device is deterministic , upon the input of @xmath28 values on the tape , the joint system can be put into the state @xmath89 reversibly and hence without any work cost . if the device is not - deterministic , but still prescient for @xmath36 , then the final @xmath34 has some additional randomness , constrainted by the fact @xmath33 is a fine - graining of the causal state .",
    "as such , the entropy of the ancilla is increased to @xmath90 .",
    "we can rewrite this value as @xmath76 , since the causal state is itself determined by @xmath91 , and no other parts of the past are useful for determining @xmath34 .",
    "we can now evaluate the entropy of the previous internal state and the tape , noting its mutual information with the new internal state : @xmath92 .",
    "@xmath93 we have used that for a stationary process",
    "we have also used prescience , such that conditioning the future of @xmath36 on @xmath70 is equivalent to conditioning it on the causal state @xmath95 . since the terms in @xmath95 are shifted in time with respect to each other , from stationarity they all represent the same value , and so each addend contributes the same entropy .",
    "the extractor takes each value on the tape back to the default uncorrelated states @xmath96 , with entropy @xmath66 .",
    "the final state of the ancilla must also be pure and uncorrelated with the reset tape , such that the total final entropy of the tape - ancilla system is also @xmath66 .",
    "this corresponds to a change in entropy @xmath97 -   { { h}\\small(r^{t+k } \\,|\\ , r^t x^{t+1}\\ldots x^{t+k}\\small)}$ ] .",
    "the final term , arising only when there is indeterminism , is equal in magnitude but opposite in sign to the amount of randomness we introduced to the tape from indeterminism in the first place .",
    "as such , these terms perfectly cancel out , and do not affect the final value of the work exchange . by landauer s principal ,",
    "we hence see that the work extracted when resetting @xmath28 symbols on the tape and advancing the internal state from @xmath33 to @xmath34 is @xmath98.\\ ] ]"
  ],
  "abstract_text": [
    "<S> living organisms capitalize on their ability to predict their environment to maximize their available free energy , and invest this energy in turn to create new complex structures . </S>",
    "<S> is there a preferred method by which this manipulation of structure should be done ? </S>",
    "<S> our intuition is `` simpler is better , '' but this is only a guiding principal . here , we substantiate this claim through thermodynamic reasoning </S>",
    "<S> . we present a new framework for the manipulation of patterns  structured sequences of data  by predictive devices . </S>",
    "<S> we identify the dissipative costs and how they can be minimized by the choice of memory in these predictive devices . for pattern generation </S>",
    "<S> , we see that simpler is indeed better . </S>",
    "<S> however , contrary to intuition , when it comes to extracting work from a pattern , any device capable of making statistically accurate predictions can recover all available energy .    </S>",
    "<S> * simpler is better*an idea that has resonated with culture throughout history . amongst many examples , </S>",
    "<S> the value of simplicity has been recognized in the minimalist movement in 1960s art and music , _ shibui _ in japanese ceramics , productivism in post - revolutionary russian , and the guiding principles of twenty - first century apple product design  @xcite . </S>",
    "<S> our predilection for simplicity even permeates natural philosophy , as most - famously formalized by william of ockham s razor . </S>",
    "<S> this notion , `` everything should be made as simple as possible but no simpler , ''  </S>",
    "<S> @xcite suggests that satisfying explanations for reality must avoid unnecessary complications .    </S>",
    "<S> the virtue of simplicity in art is subjective , but in science this can be quantified , provided that we explain what is meant both by `` simpler '' and by `` better '' . in thermodynamics , </S>",
    "<S> a subject whose original purpose relates to the design of steam - engines , `` better '' suggests efficiency and thrift ( e.g.  more power for less coal ) . </S>",
    "<S> this carries through to modern thermodynamics : the best approach for a given task minimizes the expenditure of a limited resource  @xcite ( e.g.  work or tricky - to - create states ) .    </S>",
    "<S> meanwhile in computational mechanics  @xcite , we can formalize `` simpler '' in the context of pattern generation and manipulation . everything we observe in the environment </S>",
    "<S> can be considered to be pattern  a temporal sequence of data that exhibits statistical structure . </S>",
    "<S> much of science then deals with the construction of machines that take information from past observations , and use it to generate future statistics . here , the _ simplest _ machine stores this information about the past using the minimal amount of internal structure .     </S>",
    "<S> * cycle of pattern generation and extraction . </S>",
    "<S> * a tape moves from left to right . </S>",
    "<S> a generator expends work to write a pattern to the tape . </S>",
    "<S> the extractor then uses the pattern on this tape to extract work . </S>",
    "<S> to run cyclically , each device maintains _ prescient _ memory that keeps track of the pattern . in this article </S>",
    "<S> , we identify the dissipative work costs . </S>",
    "<S> we find that simpler is thermodynamically better _ only _ when it comes to generating patterns . , </S>",
    "<S> scaledwidth=42.5% ]    to answer `` when is simpler thermodynamically better ? ''  </S>",
    "<S> we present a thermodynamic framework for pattern manipulation . </S>",
    "<S> we evaluate the steady - state operation of a full thermal cycle , consisting of a _ generator _ that work to accurately produce a pattern and an _ extractor _ that anticipates the pattern to exploit it for free energy ( see  [ fig : simextcycle ] ) . </S>",
    "<S> we examine the work exchange in these devices , and how they use internal memory to retain their predictive ability . </S>",
    "<S> we then identify dissipative work costs that worsen their thermodynamic performance . in this context , </S>",
    "<S> a _ </S>",
    "<S> simpler _ device uses less internal memory , and a _ </S>",
    "<S> better _ one wastes less work maintaining this memory .    </S>",
    "<S> ockham s razor is a guiding philosophical principle , rather than a cast - in - stone physical law ; and so there is no a priori physical motivation to assume that using the smallest , simplest , memory is always thermodynamically better . by minimizing the dissipative work costs , we identify the best choice of internal memory . </S>",
    "<S> in the generator , using the simplest possible internal memory ( the causal states associated with the pattern s _ statistical complexity _  @xcite ) minimizes dissipation . for generating a pattern </S>",
    "<S> : simpler is thermodynamically better . for the extractor , defying our intuition , it transpires that the choice of memory has no thermodynamic consequence .    </S>",
    "<S> * patterns are a resource . * </S>",
    "<S> knowing a system s internal state has thermodynamic consequence : this knowledge can be used to perform work ( drive a mechanical task ) , as illustrated by the szilrd engine thought experiment  @xcite . </S>",
    "<S> a box has a single particle inside , on the left- or right - hand - side . </S>",
    "<S> a movable barrier inserted in the center acts as a piston that expands as the particle pushes against it . </S>",
    "<S> if an agent knows which side of the barrier the particle is on , she can couple the barrier ( e.g.  via a pulley ) to raise a weight . </S>",
    "<S> as the piston expands it generates an amount of work @xmath0 ( at temperature @xmath1 ) by drawing in the same amount of heat from its surroundings .    </S>",
    "<S> knowledge about patterns may also be exploited . </S>",
    "<S> suppose an agent attempts to extract work from a series of szilrd engines ( labelled sequentially by index @xmath2 ) , prepared such that the particle in engine @xmath2 is on the same side of the barrier as in engine @xmath3 with probability @xmath4 . </S>",
    "<S> an agent unware of this pattern would only be able to correctly predict the particle s location half of the time , and hence will extract less work than an agent who knows the pattern and couples her pulley accordingly . </S>",
    "<S> as such , the ability to predict grants thermodynamic advantage .    </S>",
    "<S> this is a manifestation of maxwell s dmon  an apparently paradoxical conversion of heat into work that is only resolved by accepting that information is physical and hence subject to the laws of thermodynamics  @xcite . </S>",
    "<S> for the single szilrd engine , we must also account for the cost of resetting the agent s memory about the particle s location  this knowledge must be thought of as a resource . </S>",
    "<S> likewise , since it is more thermodynamically useful that the sequence follows a pattern than be uncorrelated , the pattern itself must also be considered as a resource . producing a pattern hence requires an investment of work . </S>",
    "<S> moreover , any physical device that generates ( or exploits ) a pattern contains some memory about what has happened in the pattern so far , in order to accurately generate ( or anticipate ) upcoming parts of the pattern . </S>",
    "<S> any thermodynamic costs of maintaining this internal memory must also be accounted for .    </S>",
    "<S> the quantitative link between information , entropy and heat dissipation is given by landauer s principle  @xcite : the minimum work cost of any information - processing task is proportional to the change in information entropy . </S>",
    "<S> using a base 2 logarithm gives units of _ bits_. ] ( just like macroscopic thermodynamics , where the work required to slowly change between two states of the same internal energy is proportional to the change in thermodynamic entropy ) .    * the mathematics of patterns . </S>",
    "<S> * we approach this mathematically using the language of statistical complexity  @xcite . </S>",
    "<S> the outputs of a physical process are represented using a bi - infinite sequence of random variables @xmath5 , where @xmath6 describes the random variable at time @xmath2 . </S>",
    "<S> ( in our example above , @xmath6 is whether engine @xmath2 has a particle on the left or the right ) . </S>",
    "<S> these are partitioned such that variables up to @xmath6 are in the past @xmath7 , and from @xmath8 onwards are in the future @xmath9 . a particular _ sequence _ </S>",
    "<S> is specified by the values that each random variable ultimately takes ( @xmath10 , @xmath11 etc . ) . a statistical description of the _ pattern _ is given by the probability distribution over these sequences @xmath12 .    when the outputs follow a pattern , the past and future are correlated . </S>",
    "<S> if a particular sequence @xmath13 is observed in the past , the statistics of the future are given by @xmath14 . </S>",
    "<S> information theory quantifies how useful this past knowledge is for predicting the future : the difference in the entropy with and without knowledge of the past , @xmath15 . </S>",
    "<S> this value is exactly the _ mutual information _ between the past and future @xmath16 ( also known as the _ excess entropy _ @xmath17 ) .    </S>",
    "<S> we make the simplifying assumption that the pattern is a _ stationary process _  </S>",
    "<S> @xcite the statistics @xmath12 are invariant under time translation . </S>",
    "<S> this does not mean that every output @xmath18 in the sequence is identical , or that the pattern is markovian , but rather that the statistics of @xmath8 onwards , given a past sequence , have no explicit time dependence : @xmath19 for all @xmath2 , @xmath20 and @xmath13 . </S>",
    "<S> hence , we can often omit the superscript  @xmath2 .    </S>",
    "<S> the notion of an agent is formalised by constructing a _ prescient device _ : </S>",
    "<S> a machine whose knowledge of future statistics is as good as is causally possible ( i.e.  without knowledge about the outcomes of random events in the future ) </S>",
    "<S> . it would be cumbersome to store the entire infinite past string in this device s memory , since this requires an unbounded amount of storage . </S>",
    "<S> instead , one uses a ( many - to - one ) function to map multiple past sequences to the device s internal memory @xmath21 , which will itself be a random variable over a finite alphabet . </S>",
    "<S> two different pasts are only mapped to the same internal memory state if the future statistics conditional on them are the same . for the purpose of predicting future statistics , knowing @xmath21 is as useful as knowing the entire past and @xmath22 . </S>",
    "<S> such internal states @xmath21 are known as _ </S>",
    "<S> prescient states_.    storing these states requires an amount of memory given by the entropy @xmath23 over their probability distribution . </S>",
    "<S> we define states that use less memory , and hence have lower entropy , as _ simpler_. the simplest prescient states requiring the least memory are known as _ causal states _ </S>",
    "<S> ( written  @xmath24 ) . </S>",
    "<S> these correspond to the equivalence classes of the pasts that predict the same future statistics . as </S>",
    "<S> any given pattern has a unique set of causal states , the memory @xmath25 needed to store these states quantifies how difficult it is to describe that pattern , and so is a _ statistical measure of complexity _  </S>",
    "<S> @xcite . </S>",
    "<S> unlike kolmogorov complexity  @xcite which concerns the generation of a specific string , the statistical complexity avoids classifying structureless but highly - random processes as complicated . </S>",
    "<S> statistical complexity has been applied to analysing structure in diverse contexts , such as neural networks  @xcite , financial markets  @xcite and    the intuition bestowed upon us by ockham s razor suggests that in any realization of a predictive device , we should use these simplest causal states . in the remainder of this article , we shall see where this intuition holds , and substantiate the penalties incurred by deviation from it .    * a new framework for pattern manipulation . * </S>",
    "<S> the classical expressions of thermodynamic laws ( e.g.  kelvin s statement that a device can not convert heat into work with no other effect  @xcite ) concern cyclic behaviour processes that leave the system in a state allowing for repetition with the same thermodynamic consequence . without a full cycle in mind </S>",
    "<S> , there is the danger that the thermal benefit of a process may come at the expense of consuming an unaccounted - for resource . </S>",
    "<S> a cycle does not require the _ microstate _ of the system to return to its original value . consider a piston of gas expanding and compressing </S>",
    "<S> : it does not matter if the individual molecules have moved to new locations by the end of the cycle , as long as the important thermodynamic variables  the pressure and volume  return to their original values .    </S>",
    "<S> in the same way , we present a cycle of generating and consuming a pattern ( see [ fig : simextcycle ] ) . </S>",
    "<S> visualise the pattern as a series of symbols on a tape ( each position labelled by @xmath2 ) . </S>",
    "<S> these symbols are initially distributed according to uncorrelated high - entropy , so long as we use the same statistical states for the generator s input and extractor s output . ] </S>",
    "<S> random variables @xmath26 . </S>",
    "<S> the tape is acted on by two machines , which are in contact with a thermal reservoir ( i.e.  heat bath at inverse temperature @xmath27 ) and a battery for storing free energy ( e.g.  a raising weight ) . </S>",
    "<S> the first machine , a _ generator _ , writes @xmath28 steps of the pattern onto the tape with an associated work cost . </S>",
    "<S> the second machine , an _ extractor _ , resets @xmath28 symbols on the patterned tape back to the uncorrelated default states , outputting work as it does so </S>",
    "<S> . an _ instantaneous _ device with @xmath29 acts on one symbol at a time ; whereas a machine with a larger _ stride _ @xmath28 processes a larger section of the pattern at once .    </S>",
    "<S> since the generator and extractor require accurate statistical knowledge of the future ( the generator to know what it must produce next ; the extractor to anticipate the upcoming inputs ) , they must both maintain prescient internal memory . </S>",
    "<S> as such they are realisations of prescient devices . </S>",
    "<S> we shall assume that these devices begin with their internal memories synchronized , such that the part of the pattern the generator is about to produce matches with the part of the pattern the extractor anticipates acting upon [ i.e.  the machines have internal states @xmath30 and </S>",
    "<S> @xmath31 respectively such that @xmath32 . ] to run continually in an cycle , the generator and extractor must remain synchronized after they have generated and extracted the same number of symbols . </S>",
    "<S> this ensures that their performance on the next part of the pattern remains the same .    </S>",
    "<S> thus , after each device has processed @xmath28 symbols , the prescient state must be updated from @xmath33 to @xmath34 . </S>",
    "<S> how this update is performed can be deterministic or otherwise . </S>",
    "<S> the behaviour is _ deterministic _ if given an initial memory state @xmath33 and the next @xmath28 symbols of the pattern @xmath35 , the final state @xmath34 is completely defined ( i.e.  the randomness is entirely confined to the outputs @xmath35 ) . </S>",
    "<S> the alternative is that there is some additional randomness in the final value of @xmath34 . </S>",
    "<S> however , this randomness can only be between choices of @xmath34 that predict exactly the same future statistics of @xmath36 , as otherwise the devices will cease to be prescient ( and may desynchronize ) .     </S>",
    "<S> * writing a pattern to a tape . * the various choices of symbols on a tape can each be associated with a different energy level of a system ( drawn as a black horizontal lines whose relative height indicates relative energy ) . </S>",
    "<S> the statistical state of the symbol is a probability distribution ( drawn as grey bars ) over these configurations . by changing the hamiltonian of the tape whilst remaining in contact with a thermal reservoir </S>",
    "<S> , the statistics can be altered to @xmath37 . at this point , </S>",
    "<S> the system is isolated from the heat reservoir and the hamiltonian is adiabatically removed . </S>",
    "<S> the whole procedure requires an investment of work proportional to the reduction in the state s entropy . </S>",
    "<S> , scaledwidth=46.0% ]    * investing work to generate a pattern . </S>",
    "<S> * for a given pattern , there is a family of generators , characterized by the prescient states @xmath33 used as internal memory , and by the the number of steps @xmath28 of the pattern that are generated at once . </S>",
    "<S> the work cost of generating a pattern consists of two contributions : one from changing the entropy of the tape , and one from updating the memory .    from landauer </S>",
    "<S> s principle , the minimal work investment required to write @xmath28 symbols of the pattern onto the tape ( at inverse temperature @xmath38 ) is given by @xmath39,\\ ] ] [ see [ fig : updatetape ] , and _ </S>",
    "<S> appendix_. ] this value relates to change in entropy of the tape itself , and so is an intrinsic property of the pattern rather than of the machine generating it . as such </S>",
    "<S> it can be expressed in terms of the causal states  @xmath24 ( which are unique for any given pattern  @xcite ) rather than the arbitrary device - dependent internal states @xmath21 .     </S>",
    "<S> * updating the generator s memory . * ( the deterministic case is shown . ) a blank ancilla state @xmath40 is _ updated _ at no cost to @xmath34 , conditioned on the values of the initial internal state @xmath41 and outputs @xmath35 . </S>",
    "<S> the old state @xmath41 is then _ decorrelated _ from @xmath42 and @xmath35 , and the mutual information used to reduce the entropy of the internal state ( transforming it into @xmath43 ) . </S>",
    "<S> finally @xmath43 is _ reset _ back to the blank ancilla state at work cost @xmath44 , so that the generator s internal state is ready to produce the next part of the pattern . , </S>",
    "<S> scaledwidth=44.0% ]    the second contribution , the cost of updating the internal memory into @xmath34 ( so that the generator is ready to produce the next part of the pattern ) is given by @xmath45 [ see [ fig : updatestate ] and _ appendix_. ] the first term is the cost of erasing the previous state @xmath33 , offset by the mutual information that is contained within the new state of the memory @xmath34 and the patterned outputs @xmath35 . </S>",
    "<S> the second term ( which is zero for deterministic updates ) reflects the fact that if the update is indeterministic , according to landauer s principle we can recover a portion of the work cost associated with the memory s change in entropy . </S>",
    "<S> however , by expanding the first term ( details in _ appendix _ ) , we arrive at @xmath46 where the indeterministic term has been eliminated . </S>",
    "<S> whatever work might have been gained by introducing randomness into @xmath34 is entirely cancelled out by the cost of resetting this randomness in the previous state @xmath33 . hence determinism does not in fact play a role in deciding the thermodynamic advantage of prescient memory .    rather , we see that the dissipative cost is proportional to the difference between @xmath33 s predictive power to guess the next @xmath28 symbols , and @xmath34 s _ retrodictive _ power to guess the preceding @xmath28 symbols . failure to predict increases the first entropy , and hence the amount of dissipation . on the other hand , failure to retrodict increases the second entropy and lessens the total dissipation .    </S>",
    "<S> what choice of prescient memory @xmath21 minimizes this cost ? </S>",
    "<S> the first term @xmath47 is the same for _ all _ choices of prescient states , since they all must reduce the entropy of the future statistics by the largest possible amount . </S>",
    "<S> the second term @xmath48 takes its largest value when @xmath34 contains the least information about the preceding @xmath28 symbols . </S>",
    "<S> the causal state @xmath49 contain the least information about the past required to predict the future , so all alternative prescient states must contain this amount or more . as such , @xmath50 . out of all the possible prescient states </S>",
    "<S> @xmath21 to use as our internal memory , the simplest ones  the causal states @xmath24hence dissipate the least heat . </S>",
    "<S> we thus conclude that for generating a pattern , _ simpler is better_.    * extracting work from a pattern . * </S>",
    "<S> let us now evaluate how much work we can extract from a pattern , by considering the prescient _ extractor _ , this device is a sophisticated szilrd engine  @xcite that maintains an up - to - date internal memory ( the prescient state @xmath33 ) allowing it to anticipate the upcoming @xmath28 symbols in the pattern . </S>",
    "<S> work is extracted by converting @xmath35 into uncorrelated output states @xmath51 .    in order to fully account for all the changes in entropy in our system , again we must consider both the tape on which the pattern is written and the internal memory of the extractor . </S>",
    "<S> we do this in a single step ; the total average work released by taking @xmath28 symbols to the default state @xmath52 , and updating the internal memory from @xmath33 to @xmath34is given by @xmath53\\ ] ] [ proof in _ appendix_. ]    the value here is entirely proportional to the change in entropy of the @xmath28 symbols on the tape , and is the maximum amount we would expect from landauer s principle . </S>",
    "<S> the curious result here is that although we have accounted for the cost of updating , there is absolutely no dependence on the choice of internal memory @xmath33 , and the scaling with the number of symbols processed @xmath28 is trivial . unlike with the generator , </S>",
    "<S> it does not matter what sort of memory is used for extraction .    </S>",
    "<S> moreover , we remark that there may always be dissipation in generation since even causal states can contain superfluous information about the past for some patterns . </S>",
    "<S> however , this is not the case for extraction , even though we would expect larger @xmath33 to perform worse since there should be more about the past to `` clean up '' when updating the internal state .    </S>",
    "<S> this apparent paradox may be explained by carefully considering the difference between the generator and the extractor . </S>",
    "<S> crucially , the output pattern must be undisturbed at the end of the generation procedure but not for extraction . </S>",
    "<S> thus , the difference is between a device that moves information , and a device that copies information . </S>",
    "<S> moving information is a logically reversible process , whereas copying information is not  and it is _ logical irreversibility _ that lies behind dissipative costs in computation  @xcite . at the end of the extraction , there is only one copy of this information ( encoded in @xmath34 ) , whereas in generation , the information is present both in the internal memory and in the tape . </S>",
    "<S> thus the extractor can move the information from @xmath54 into @xmath34 , but the generator must copy it . if more information is stored in @xmath33 , then more has to be copied . </S>",
    "<S> this subtle , but important , distinction reveals to us why updating the memory must dissipate heat in pattern generation , but not for pattern extraction .    </S>",
    "<S> * relation to existing results . * </S>",
    "<S> we first observe that if we consider only updating the generator s memory , and restrict ourselves to causal states and the limit @xmath55 , then the dissipative work cost of updating the generator s memory [ [ eq : centnost ] ] converges on @xmath56 , recovering the result of wiesner et al .  </S>",
    "<S> @xcite .    </S>",
    "<S> secondly , we note that the dissipative cost [ [ eq : centnost ] ] may be re - expressed in terms of mutual information : @xmath57 in the opposite , instantaneous , case @xmath29 , this recovers a mathematical quantity similar to that introduced by still et al .  </S>",
    "<S> @xcite as the _ useless instantaneous nostalgia_. that this coincides with our result in certain limits , despite very different derivations , demonstrates the universality of the information - theoretic concepts underlying the thermal costs of prediction . in both cases , </S>",
    "<S> the need to clean up unnecessary information about a process s past has been identified as a source of dissipation .    </S>",
    "<S> our work extends this by examining the thermodynamics of a full cycle of pattern generation and extraction ( see [ fig : simextcycle ] ) . by doing so </S>",
    "<S> , we have identified that penalty for storing more than is necessary depends very much on how the prediction is used . </S>",
    "<S> any prescient extractor can recover the entire free energy of the pattern  such a device has no concept of useless nostalgia , and no dissipation . </S>",
    "<S> it is not unnecessary storage in and of itself that induces the dissipative cost , but rather logical irreversibility . </S>",
    "<S> only when the pattern manipulation is irreversible ( such as in our generator , or the system in  @xcite ) does storing too much information incur a thermal penalty .    </S>",
    "<S> * the thermodynamics of patterns . </S>",
    "<S> * the two devices we have presented in this article are building blocks that can be combined into complex pattern manipulators . </S>",
    "<S> the simplest example is the cycle of generation followed by extraction at the same temperature ( [ fig : simextcycle ] ) . </S>",
    "<S> this configuration could be considered as charging a battery that stores energy in the form of a pattern , to be later released by the extractor . since the contributions from writing and consuming the pattern cancel out , the net cost is from the work dissipation whilst updating the generator s memory . </S>",
    "<S> using the simplest internal memory ensures that the least work is wasted .    </S>",
    "<S> an alternative configuration is for a generator and extractor to act in parallel on different patterns . here </S>",
    "<S> , the extractor consumes one pattern , and uses the energy released to power a generator that writes a different pattern . </S>",
    "<S> this approximates the action of all living organisms : for example , a lion metabolizes the structure of an antelope ( destroying it in the process ) , and uses the energy released to build more lion . using the simplest internal memory for generation grants the advantage that less antelope needs to be consumed in order to produce the same amount of lion </S>",
    "<S> .    * outlook . * in this article , we have treated the thermodynamics of patterns , and identified the dissipative work costs in a cycle of producing and consuming a pattern . for generation , simpler is thermodynamically better ; the cost is minimized when using the simplest internal states possible : the causal states associated with the pattern s statistical complexity . on the other hand , for extraction we found that any prescient device has the ability to recover the entire free energy of the pattern , because the internal memory could be updated in a logically reversible way . </S>",
    "<S> the answer to  `` when is simpler thermodynamically better ? ''  is that simpler is better when it helps us avoid logical irreversibility .    </S>",
    "<S> our discussion has thus far been restricted to the average behaviour of classical systems operating in the quasistatic regime . by taking the quasistatic limit , </S>",
    "<S> we do not account for additional dissipation that occurs when writing to the tape in finite time . </S>",
    "<S> the additional costs of this are highly implementation - dependent : typically , faster processes dissipate more heat . by considering only the average behaviour , </S>",
    "<S> we have avoided addressing the fluctuations in the work costs . </S>",
    "<S> fluctuations are intrinsically related to dissipation and dissipative work  @xcite , but it is not yet clear what the associated fluctuation relation is for the dissipative work discussed here .    </S>",
    "<S> recent research indicates that by using quantum states , less memory is required than the best classical alternative for the same predictive power  @xcite . in a random process </S>",
    "<S> , there is a limit to how distinct the futures may be : two different pasts may in some ( but not all ) cases lead to exactly the same future . </S>",
    "<S> there is hence no need to perfectly distinguish between the two internal states , but this is unavoidable when using classical memory . </S>",
    "<S> quantum memory does not require this , and hence has a lower entropy for the same level of prescience as the classical case . </S>",
    "<S> it is therefore feasible that by storing less unnecessary information about the past , quantum machines could be simpler devices and hence dissipate less heat . </S>",
    "<S> this would suggest that there are certain pattern - processing tasks where the quantumness of a device could yield thermodynamic advantage . </S>",
    "<S> could quantum pattern generators , through their increased simplicity , be better than the classical alternatives ?    </S>",
    "<S> * acknowledgements . </S>",
    "<S> * we are grateful for funding from the john templeton foundation grant 53914 _ `` occam s quantum mechanical razor : can quantum theory admit the simplest understanding of reality ? '' _ ; the national research foundation ; the ministry of education in singapore , the academic research fund tier 3 moe2012-t3 - 1 - 009 ; the national basic research program of china grants 2011cba00300 and 2011cba00302 ; the national natural science foundation of china grants 11450110058 , 61033001 and 61361136003 ; the 1000 talents program of china ; and the oxford martin school . </S>",
    "<S> we thank , , and for illuminating .    </S>",
    "<S> 20 [ 1]#1 urlstyle [ 1]doi : # 1    s.  richmond . . </S>",
    "<S> _ the daily telegraph ( uk ) _ , may 2012 . </S>",
    "<S> issn 0307 - 1235 . </S>",
    "<S> http://www.telegraph.co.uk / technology / apple/9283706/jonathan - ive - interv% iew - simplicity - isnt - simple.html[http://www.telegraph.co.uk / technology / apple/9283706/jonathan - ive - interv% iew-simplicity-isnt-simple.html ] .    attributed to  a. </S>",
    "<S> einstein  via r.  sessions . </S>",
    "<S> everything should be made as simple as possible , but not simpler , 2011 . </S>",
    "<S> - simple/.    f.  g. s.  l. brando , m.  horodecki , j.  oppenheim , j.  m. renes , and r.  w. spekkens . . </S>",
    "<S> _ physical review letters _ , 1110 ( 25):0 250404 , december 2013 . </S>",
    "<S> issn 0031 - 9007 . </S>",
    "<S> doi : 10.1103/physrevlett.111.250404 . </S>",
    "<S> http://link.aps.org/doi/10.1103/physrevlett.111.250404 .    </S>",
    "<S> j.  p. crutchfield and k.  young . . </S>",
    "<S> _ physical review letters _ , pages 105108 , 1989 . </S>",
    "<S> issn 00319007 . </S>",
    "<S> doi : 10.1103/physrevlett.63.105 . </S>",
    "<S> http://link.aps.org/doi/10.1103/physrevlett.63.105 .    </S>",
    "<S> c.  r. shalizi and j.  p. crutchfield . . </S>",
    "<S> _ journal of statistical physics _ , 1040 ( 3 - 4):0 817879 , 2001 . </S>",
    "<S> issn 00224715 . </S>",
    "<S> doi : 10.1023/a:1010388907793 . </S>",
    "<S> .    h.  leff and a.  f. rex . _ maxwell s demon 2 : entropy , classical and quantum information , computing_. taylor & francis , 2002 . </S>",
    "<S> isbn 9780750307598 .    c.  h. bennett . . </S>",
    "<S> _ international journal of theoretical physics _ , 210 ( 12):0 905940 , december 1982 . </S>",
    "<S> issn 0020 - 7748 . </S>",
    "<S> doi : 10.1007/bf02084158 . </S>",
    "<S> http://www.springerlink.com / content / jn7x3365386phn46/.    r.  landauer . . </S>",
    "<S> _ physics letters a _ , 2170 ( 4):0 188193 , 1996 . </S>",
    "<S> issn 0375 - 9601 . </S>",
    "<S> doi : 10.1016/0375 - 9601(96)00453 - 7 . </S>",
    "<S> http://www.sciencedirect.com/science/article/pii/0375960196004537 .    </S>",
    "<S> j.  m.  r. parrondo , j.  m. horowitz , and t.  sagawa . . _ nature physics _ , 110 ( 2):0 131139 , february 2015 . </S>",
    "<S> issn 1745 - 2473 . </S>",
    "<S> doi : 10.1038/nphys3230 . </S>",
    "<S> http://dx.doi.org/10.1038/nphys3230 .    </S>",
    "<S> a.  n. kolmogorov . . </S>",
    "<S> _ sankhya : the indian journal of statistics , series a _ , 250 ( 4):0 369376 , 1963 . </S>",
    "<S> issn 0976 - 836x .    </S>",
    "<S> r.  haslinger , k.  l. klinkner , and c.  r. shalizi . </S>",
    "<S> _ neural computation _ , 220 ( 1):0 12157 , january 2010 . </S>",
    "<S> issn 1530 - 888x . </S>",
    "<S> doi : 10.1162/neco.2009.12 - 07 - 678 . </S>",
    "<S> / doi / abs/10.1162/neco.2009.12 - 07 - 678#.v% ay3c_mqqko[http://www.mitpressjournals.org / doi / abs/10.1162/neco.2009.12 - 07 - 678#.v% ay3c_mqqko ] .    j.  b. park , j.  won lee , j .- s . </S>",
    "<S> yang , h .- h . </S>",
    "<S> jo , and h .- </S>",
    "<S> t . moon . . </S>",
    "<S> _ physica a : statistical mechanics and its applications _ , 3790 ( 1):0 179187 , june 2007 . </S>",
    "<S> issn 03784371 . </S>",
    "<S> doi : 10.1016/j.physa.2006.12.042 . </S>",
    "<S> http://www.sciencedirect.com/science/article/pii/s0378437107000271 .    </S>",
    "<S> c.  lu and r.  r. brooks . . in _ proceedings of the 2012 workshop on learning from authoritative security experiment results - laser 12 _ , pages 4146 , new york , new york , usa , july 2012 . </S>",
    "<S> acm press . </S>",
    "<S> isbn 9781450311953 . </S>",
    "<S> doi : 10.1145/2379616.2379622 . </S>",
    "<S> http://dl.acm.org/citation.cfm?id=2379616.2379622 .    </S>",
    "<S> w.  thomson . . </S>",
    "<S> _ transactions of the royal society of edinburgh _ , xx0 ( ii):0 261288 , 1851 . http://www.biodiversitylibrary.org/item/126047#page/291 .    </S>",
    "<S> k.  wiesner , m.  gu , e.  rieper , and v.  vedral . . </S>",
    "<S> _ proceedings of the royal society a : mathematical , physical and engineering sciences _ , 468:0 40584066 , 2012 . </S>",
    "<S> issn 1364 - 5021 . </S>",
    "<S> doi : 10.1098/rspa.2012.0173 . </S>",
    "<S> .    s.  still , d.  a. sivak , a.  j. bell , and g.  e. crooks . . </S>",
    "<S> _ physical review letters _ , </S>",
    "<S> 1090 ( 12):0 120604 , september 2012 . </S>",
    "<S> issn 0031 - 9007 . </S>",
    "<S> doi : 10.1103/physrevlett.109.120604 . </S>",
    "<S> http://link.aps.org/doi/10.1103/physrevlett.109.120604 .    </S>",
    "<S> g.  e. crooks . . </S>",
    "<S> _ physical review e _ </S>",
    "<S> , 600 ( 3):0 27212726 , september 1999 . </S>",
    "<S> issn 1063 - 651x . </S>",
    "<S> doi : 10.1103/physreve.60.2721 . </S>",
    "<S> http://link.aps.org/doi/10.1103/physreve.60.2721 .    </S>",
    "<S> a.  gomez - marin , j.  m.  r. parrondo , and c.  van den broeck . . </S>",
    "<S> _ epl ( europhysics letters ) _ , 820 ( 5):0 50002 , june 2008 . </S>",
    "<S> issn 0295 - 5075 . </S>",
    "<S> doi : 10.1209/0295 - 5075/82/50002 . </S>",
    "<S> http://stacks.iop.org/0295-5075/82/i=5/a=50002 .    </S>",
    "<S> m.  gu , k.  wiesner , e.  rieper , and v.  vedral . . </S>",
    "<S> _ nature communications _ , 30 ( 762 ) , 2012 . </S>",
    "<S> issn 2041 - 1723 . </S>",
    "<S> doi : 10.1038/ncomms1761 . </S>",
    "<S> http://dx.doi.org/10.1038/ncomms1761m3 .    </S>",
    "<S> j.  r. mahoney , c.  j. ellison , and j.  p. crutchfield . . </S>",
    "<S> _ journal of physics a : mathematical and theoretical _ , 420 ( 36):0 362002 , september 2009 . </S>",
    "<S> issn 1751 - 8113 . </S>",
    "<S> doi : 10.1088/1751 - 8113/42/36/362002 . </S>",
    "<S> http://stacks.iop.org/1751-8121/42/i=36/a=362002 . </S>"
  ]
}