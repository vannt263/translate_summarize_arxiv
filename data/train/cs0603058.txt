{
  "article_text": [
    "an optimization problem that is characterized by a set @xmath0 and a hypergraph @xmath1 .",
    "there are @xmath2 decision variables ; each is associated with a vertex @xmath3 and takes values in a set @xmath0 .",
    "the set @xmath4 is a collection of subsets ( or , `` hyperedges '' ) of the vertex set @xmath5 ; each hyperedge @xmath6 is associated with a real - valued `` component function '' ( or , `` factor '' ) @xmath7 .",
    "the optimization problem takes the form @xmath8 where @xmath9 here , @xmath10 is the vector of variables associated with vertices in the subset @xmath11 .",
    "we refer to an optimization program of this form as a _ graphical model_. while this formulation may seem overly broad  indeed , almost any optimization problem can be cast in this framework  we are implicitly assuming that the graph is sparse and that the hyperedges are small .    over the past few years , there has been significant interest in a heuristic optimization algorithm for graphical models .",
    "we will call this algorithm the min - sum message passing algorithm , or the min - sum algorithm , for short .",
    "this is equivalent to the so - called max - product algorithm , also known as belief revision , and is closely related to the sum - product algorithm , also known as belief propagation .",
    "interest in such algorithms has to a large extent been triggered by the success of message passing algorithms for decoding low - density parity - check codes and turbo codes @xcite .",
    "message passing algorithms are now used routinely to solve np - hard decoding problems in communication systems .",
    "it was a surprise that this simple and efficient approach offers sufficing solutions .",
    "the majority of literature has been focused on the case where the set @xmath0 is discrete and the resulting optimization problem is combinatorial in nature .",
    "we , however , are interested in the the case where @xmath12 and the optimization problem is continuous . in particular , many continuous optimization problems that are traditionally approached using methods of linear programming , convex programming , etc .",
    "also possess graphical structure , with objectives defined by sums of component functions .",
    "we believe the min - sum algorithm leverages this graphical structure in a way that can complement traditional optimization algorithms , and that combining strengths will lead to algorithms that are able to scale to larger instances of linear and convex programs .",
    "one continuous case that has been considered in the literature is that of pairwise quadratic graphical models . here , the objective function is a positive definite quadratic function @xmath13 this function is decomposed in a pairwise fashion according to an undirected graph @xmath14 , so that @xmath15 where the functions @xmath16 are quadratic .",
    "it has been shown that , if the min - sum algorithm converges , it computes the global minimum of the quadratic @xcite .",
    "the question of convergence , however , has proved difficult .",
    "sufficient conditions for convergence have been established @xcite , but these conditions are abstract and difficult to verify . convergence has also been established for classes of quadratic programs arising in certain applications @xcite .    in recent work , johnson ,",
    "@xcite have introduced the notion of _ walk - summability _ for pairwise quadratic graphical models .",
    "they establish convergence of the min - sum algorithm for walk - summable pairwise quadratic graphical models when the particular set of component functions @xmath17 is employed by the algorithm and the algorithm is initialized with zero - valued messages .",
    "further , they give examples outside this class for which the min - sum algorithm does not converge .",
    "note that there may be many ways to decompose a given objective function into component functions .",
    "the min - sum algorithm takes the specification of component functions as an input and exhibits different behavior for different decompositions of the same objective function .",
    "alternatively , the choice of a decomposition can be seen to be equivalent to the choice of initial conditions for the min - sum algorithm @xcite . a limitation of the convergence result of johnson , et al .",
    "@xcite is that it requires use of a particular decomposition of the objective function of the form and with zero - valued initial messages .",
    "the analysis presented does not hold in other situation .",
    "for example , the result does not establish convergence of the min - sum algorithm in the applied context considered in @xcite .",
    "we will study the convergence of the min - sum algorithm given a convex decomposition :    * ( convex decomposition ) * + a _ convex decomposition _ of a quadratic function @xmath18 is a set of quadratic functions @xmath19 such that @xmath15 each function @xmath20 is strictly convex , and each function @xmath21 is convex ( although not necessarily strictly so ) .",
    "we will say that a quadratic objective function is _ convex decomposable _ if there exists a convex decomposition .",
    "this condition implies strict convexity of the quadratic objective function , however , not all strictly convex , quadratic functions are convex decomposable .",
    "the primary contribution of this paper is in establishing that the min - sum algorithm converges given _ any _ convex decomposition or even decompositions that are in some sense `` dominated '' by convex decompositions .",
    "this result can be equivalently restated as a sufficient condition on the initial messages used in the min - sum algorithm .",
    "convergence is established under both synchronous and asynchronous models of computation .",
    "we believe that this is the most general convergence result available for the min - sum algorithm with a quadratic objective function .",
    "the walk - summability condition of johnson , et al .",
    "is equivalent to the existence of a convex decomposition @xcite . in this way",
    ", our work can be viewed as a generalization of their convergence results to a broad class of decompositions or initial conditions .",
    "this generalization is of more than purely theoretical interest .",
    "the decentralized and asynchronous settings in which such optimization algorithms are deployed are typically dynamic .",
    "consider , for example , a sensor network which seeks to estimate some environmental phenomena by solving an optimization problem of the form .",
    "as sensors are added or removed from the network , the objective function in will change slightly . reinitializing the optimization algorithm after each such change would require synchronization across the entire network and a large delay to allow the algorithm to converge .",
    "if the change in the objective function is small , it is likely that the change in the optimum of the optimization problem is small also .",
    "hence , using the current state of the algorithm ( the set of messages ) as an initial condition may result in much quicker convergence . in this way , understanding the robustness of the min - sum algorithm over different initial conditions is important to assessing it s practical value .    beyond this",
    ", however , our work suggests path towards understanding the convergence of the min - sum algorithm in the context of general convex ( i.e. , not necessarily quadratic ) objective functions .",
    "the notion of a convex decomposition is easily generalized , while it is not clear how to interpret the walk - summability condition or a decomposition of the form in the general convex case . in follow - on work @xcite ,",
    "we have been able to establish such a generalization and develop conditions for the convergence of the min - sum algorithm in a broad range of general convex optimization problems . when specialized to the quadratic case",
    ", however , those results are not as general as the results presented herein .",
    "the optimization of quadratic graphical models can be stated as a problem of inference in gaussian graphical models . in this case , the min - sum algorithm is mathematically equivalent to sum - product algorithm ( belief propagation ) , or the max - product algorithm .",
    "our results therefore also apply to gaussian belief propagation .",
    "however , since gaussian belief propagation , in general , computes marginal distributions that have correct means but incorrect variances , we believe that the optimization perspective is more appropriate than the inference perspective . as such , we state our results in the language of optimization .",
    "finally , note that solution of quadratic programs of the form is equivalent to the solution of the sparse , symmetric , positive definite linear system @xmath22 .",
    "this is a well - studied problem with an extensive literature .",
    "the important feature of the min - sum algorithm in this context is that it is decentralized and totally asynchronous .",
    "the comparable algorithms from the literature fall into the class of classical iterative methods , such as the jacobi method or the gauss - seidel method @xcite . in an optimization context , these methods can be interpreted as local search algorithms , such as gradient descent or coordinate descent . while these methods are quite robust , they suffer from a notoriously slow rate of convergence .",
    "our hope is that message - passing algorithms will provide faster decentralized solutions to such problems than methods based on local search . in application contexts where a comparison can be made @xcite , preliminary results show that this may indeed be the case .",
    "consider a connected undirected graph with vertices @xmath23 and edges @xmath24 .",
    "let @xmath25 denote the set of neighbors of a vertex @xmath26 .",
    "consider an objective function @xmath27 that decomposes according to pairwise cliques of @xmath14 ; that is @xmath28    the min - sum algorithm attempts to minimize @xmath18 by an iterative , message passing procedure .",
    "in particular , at time @xmath29 , each vertex @xmath26 keeps track of a `` message '' from each neighbor @xmath30 .",
    "this message takes the form of a function @xmath31 .",
    "these incoming messages are combined to compute new outgoing messages for each neighbor . in particular ,",
    "the message @xmath32 from vertex @xmath26 to vertex @xmath33 evolves according to @xmath34 here , @xmath35 represents an arbitrary offset term that varies from message to message . only the relative values of the function",
    "@xmath36 matter , so @xmath35 does not influence relevant information . its purpose is to keep messages finite .",
    "one approach is to select @xmath35 so that @xmath37 . the functions @xmath38 are initialized arbitrarily ; a common choice is to set @xmath39 for all messages .    at time @xmath29",
    ", each vertex @xmath40 forms a local objective function @xmath41 by combining incoming messages according to @xmath42 the vertex then generates a running estimate of the @xmath40th component of an optimal solution to the original problem according to @xmath43 by dynamic programming arguments , it is easy to see that this procedure converges and is exact given a convex decomposition when the graph @xmath14 is a tree .",
    "we are interested in the case where the graph has arbitrary topology .",
    "an alternative way to view iterates of the min - sum algorithm is as a series of `` reparameterizations '' of the objective function @xmath18 @xcite .",
    "each reparameterization corresponds to a different decomposition of the objective function . in particular , at each time @xmath29 , we define a function @xmath44 , for each vertex @xmath45 , and a function @xmath46 , for each edge @xmath47 , so that @xmath48 the functions evolve jointly according to @xmath49 @xmath50 they are initialized at time @xmath51 according to @xmath52 @xmath53 in the common case , where the functions @xmath54 are all set to zero , the initial component functions @xmath55 are identical to @xmath56 , modulo constant offsets .",
    "a running estimate of the @xmath40th component of an optimal solution to the original problem is generated according to @xmath57    the message passing interpretation and the reparameterization interpretation can be related by @xmath58 @xmath59 @xmath60 these relations are easily established by induction on @xmath29 .",
    "as they indicate , the message passing interpretation and the reparameterization interpretation are completely equivalent in the sense that convergence of one implies convergence of the other , and that they compute the same estimates of an optimal solution to the original optimization problem .",
    "reparameterizations are more convenient for our purposes for the following reason : note that the decomposition of the objective @xmath18 is not unique .",
    "indeed , many alternate factorizations can be obtained by moving mass between the single vertex functions @xmath61 and the pairwise functions @xmath62 .",
    "since the message passing update depends on the factorization , this would seem to suggest that the each choice of factorization results in a different algorithm .",
    "however , in the reparameterization interpretation , the choice of factorization only enters via the initial conditions .",
    "moreover , it is clear that the choice of factorization is equivalent to the initial choice of messages @xmath54 .",
    "our results will identify sufficient conditions on these choices so that the min - sum algorithm converges .",
    "we are concerned with the case where the objective function @xmath63 is quadratic , i.e. @xmath64 here , @xmath65 is a symmetric , positive definite matrix and @xmath66 is a vector . since @xmath63 must decompose relative to the graph @xmath14 according to , we must have the non - diagonal entries satisfy @xmath67 if @xmath68 . without loss of generality",
    ", we will assume that @xmath69 for all @xmath47 ( otherwise , each such edge @xmath70 can be deleted from the graph ) and that @xmath71 for all @xmath72 ( otherwise , the variables can be rescaled so that this is true ) .",
    "let @xmath73 be the set of directed edges .",
    "that is , @xmath47 iff @xmath74 and @xmath47 iff @xmath75 .",
    "( we use braces and parentheses to distinguish directed and undirected edges , respectively . ) quadratic component functions @xmath76 that sum to @xmath18 can be parameterized by two vectors of parameters , @xmath77 and @xmath78 , according to @xmath79 @xmath80 given such a representation , we will refer to the components of @xmath81 as the quadratic parameters and the components of @xmath82 as the linear parameters .",
    "iterates @xmath83 of the min - sum algorithm can be represented by quadratic parameters @xmath84 and linear parameters @xmath85 . by explicit computation of the minimizations involved in the reparameterization update ,",
    "we can rewrite the update equations in terms of the parameters @xmath84 and @xmath85 .",
    "in particular , if @xmath86 , then @xmath87 @xmath88 if , on the other hand , @xmath89 then the minimization @xmath90 is unbounded and the update equation is ill - posed .",
    "further , the estimate of the @xmath40th component of the optimal solution , defined by , becomes @xmath91 when @xmath92 , and is ill - posed otherwise .",
    "we define a generalization to the notion of a convex decomposition .    *",
    "( convex - dominated decomposition ) * + a convex - dominated decomposition of a quadratic function @xmath18 is a set of quadratic functions @xmath93 that form a decomposition of @xmath18 , such that for some convex decomposition @xmath94 , @xmath95 is convex , for all edges @xmath47 .",
    "note that any convex decomposition is also convex - dominated .",
    "the following theorem is the main result of this paper .",
    "[ th : main - convergence ] * ( quadratic min - sum convergence ) * + if @xmath18 is convex decomposable and @xmath96 is a convex - dominated decomposition , then the quadratic parameters @xmath84 , the linear parameters @xmath85 , and the running estimates @xmath97 converge . moreover , @xmath98    this result is more general than required to capture the `` typical '' situation . in particular , consider a situation where a problem formulation gives rise to component functions @xmath99 that form a convex decomposition of an objective function @xmath63 . then , initialize the min - sum algorithm with @xmath100 . since the initial iterate is a convex decomposition",
    ", it certifies that @xmath18 is convex decomposable , and it is also a convex - dominated decomposition",
    ".    we will prove theorem  [ th : main - convergence ] in section  [ se : overall ] . before doing so",
    ", we will study the parameter sequences @xmath84 and @xmath85 independently .",
    "the update for the the quadratic parameters @xmath84 does not depend on the linear parameters @xmath85 .",
    "hence , it is natural to study their evolution independently , as in @xcite . in this section ,",
    "we establish existence and uniqueness of a fixed point of the update .",
    "further , we characterize initial conditions under which @xmath84 converges to this fixed point .    whether or not a decomposition is convex depends on quadratic parameters but not the linear ones .",
    "let @xmath101 be the set of quadratic parameters @xmath102 that correspond to convex decompositions .",
    "we have the following theorem establishing convergence for the quadratic parameters .",
    "the proof relies on certain monotonicity properties of the update , and extends the method developed in @xcite .",
    "[ th : gammaconv ] * ( quadratic parameter convergence ) * + assume that @xmath18 is convex decomposable .",
    "the system of equations @xmath103 has a solution @xmath104 such that @xmath105 moreover , @xmath104 is the unique such solution .    if we initialize the min - sum algorithm so that @xmath106 , for some @xmath107 , then @xmath108 for all @xmath109 , and @xmath110    see appendix  [ ap : gammaconv ] .",
    "the key condition for the convergence is that the initial quadratic parameters @xmath111 must be dominated by those of a convex decomposition .",
    "such initial conditions are easy to find , for example @xmath112 or @xmath113 satisfy this requirement .",
    "note that we should not expect the algorithm to converge for arbitrary @xmath111 . for the update to even be well - defined at time @xmath29 , we require that @xmath114 the condition on @xmath111 in theorem  [ th : gammaconv ] guarantees this at time @xmath51 , and the theorem guarantees that it continue to hold for all @xmath115 . similarly , the computation of the estimate @xmath97 requires that @xmath116 the theorem guarantees that this is true for all @xmath117 , given suitable choice of @xmath111 .",
    "in this section , we will assume that the quadratic parameters @xmath84 are set to the fixed point @xmath104 , and study the evolution of the linear parameters @xmath85 .",
    "in this case , the update for the linear parameters takes the particularly simple form @xmath118 this linear equation can be written in vector form as @xmath119 where @xmath120 is a vector with @xmath121 @xmath122 is a diagonal matrix with @xmath123 and @xmath124 is a matrix such that @xmath125 if the spectral radius of @xmath126 is less than 1 , then we have convergence of @xmath85 independent of the initial condition @xmath127 by @xmath128 we will show that existence of a convex decomposition of @xmath18 is a sufficient condition for this to be true . in order to proceed ,",
    "we first introduce the notion of walk - summability .",
    "note that the optimization problem we are considering , @xmath129 has the unique solution @xmath130 define @xmath131 , so @xmath132 and @xmath133 , if @xmath134 .",
    "if we assume that the matrix @xmath135 has spectral radius less than 1 , we can express the solution @xmath136 by the infinite series @xmath137 the idea of walk - sums , introduced by johnson , et al .",
    "@xcite , allows us to interpret this solution as a sum of weights of walks on the graph .    to be precise ,",
    "walk _ of length @xmath138 to be a sequence of vertices @xmath139 such that @xmath140 , for all @xmath141 . given a walk @xmath142",
    ", we can define a weight by the product @xmath143 ( we adopt the convention that @xmath144 for walks of length @xmath145 , which consist of a single vertex . ) given a set of walks @xmath146 , we define the weight of the set to be the sum of the weights of the walks in the set , that is @xmath147 define @xmath148 to be the ( infinite ) set of all walks from vertex @xmath26 to vertex @xmath40 . if the quantity @xmath149 was well - defined , examining the structure of @xmath135 and , we would have @xmath150    * ( walk - summability ) * + given a matrix @xmath151 with @xmath71 , define @xmath152 by @xmath153_{ij}|$ ] .",
    "we say @xmath154 is walk - summable if the spectral radius of @xmath152 is less than 1 .",
    "walk - summability of @xmath154 guarantees the the function @xmath155 is well - defined even for infinite sets of walks , since in this case , the series @xmath156 is absolutely convergent .",
    "it is not difficult to see that existence of a convex decomposition of @xmath18 implies walk - summability @xcite .",
    "more recent work @xcite shows that these two conditions are in fact equivalent .",
    "we introduce a different weight function @xmath157 defined by @xmath158 @xmath157 can be extends to sets of walks as before .",
    "however , we interpret this function only over _ non - backtracking _ walks , where a walk @xmath142 is non - backtracking if @xmath159 , for @xmath160 .",
    "denote by @xmath161 the set of non - backtracking walks .",
    "the following combinatorial lemma establishes a correspondence between @xmath157 on non - backtracking walks and @xmath155 .",
    "[ le : nuwalksum ] assume that @xmath18 is convex decomposable . for each @xmath162",
    ", there exists a set of walks @xmath163 , all terminating at the same vertex as @xmath142 , such that @xmath164 further , if @xmath165 and @xmath166 , then @xmath167 and @xmath168 are disjoint",
    ".    see appendix  [ ap : walksum ] .",
    "the above lemma reveals that @xmath157 is well - defined on infinite sets of non - backtracking walks .",
    "indeed , if @xmath169 , @xmath170 and the latter sum is finite since @xmath154 is walk - summable .",
    "we can make the correspondence between @xmath157 and @xmath155 stronger with the following lemma .",
    "[ le : nuexact ] assume that @xmath18 is convex decomposable .",
    "if we define @xmath171 to be the set of all non - backtracking walks from vertex @xmath26 to vertex @xmath172 , we have @xmath173    see appendix  [ ap : walksum ]",
    ".      examining the structure of the matrix @xmath126 from , it is clear that if @xmath174 is defined to be the set of all length @xmath29 non - backtracking walks @xmath142 with @xmath175 and @xmath176 , then @xmath177_{ij , uk } = \\nu({{\\cal w}}^{nb , t+1}_{uk\\rightarrow ij}).\\ ] ] thus , if @xmath178 is the set of all non - backtracking walks @xmath142 of length at least 1 satisfying @xmath175 and @xmath176 , @xmath179_{ij , uk } & = \\sum_{t=0}^\\infty \\nu({{\\cal w}}^{nb , t+1}_{uk\\rightarrow ij } ) = \\nu({{\\cal w}}^{nb,1+}_{uk\\rightarrow ij } ) \\\\ & = \\sum_{w\\in{{\\cal w}}^{nb,1+}_{uk\\rightarrow ij } } \\nu(w ) .",
    "\\end{split}\\ ] ] lemma  [ le : nuwalksum ] and assure us that the later sum must be absolutely convergent .",
    "then , we have established the following lemma .    [",
    "le : atconv ] assume that @xmath18 is convex decomposable .",
    "the spectral radius of @xmath180 is less than 1 .      from lemma  [ le : atconv ] , we have @xmath181 for each vertex @xmath40 , define the quantity @xmath182 in this case , the estimate @xmath183 for each vertex @xmath40 , defined by , converges to @xmath184_{ij } \\right ) \\\\ & = \\bar{\\gamma}_j \\left ( h_j +   \\sum_{i \\in n(j ) } \\sum_{\\{u , k\\}\\in\\vec{e } } \\nu({{\\cal w}}^{nb,1+}_{uk\\rightarrow ij } ) h_u \\right ) \\\\ & = \\bar{\\gamma}_j \\left ( h_j +   \\sum_{u\\in v } \\nu({{\\cal w}}^{nb,1+}_{u\\rightarrow j } ) h_u \\right ) .",
    "\\end{split}\\ ] ] here , we define @xmath185 is the set of non - backtracking walks of length at least @xmath186 starting at @xmath187 and ending at @xmath40 . note that if @xmath188 , then a non - backtracking walk from @xmath187 to @xmath40 must have length at least 1 .",
    "thus , @xmath189 if @xmath190 , there is a single non - backtracking walk of length 0 from @xmath40 to @xmath40 , namely @xmath191 , and @xmath192 .",
    "thus , @xmath193 hence , @xmath194 comparing with lemma  [ le : nuexact ] , and , we have @xmath195 thus , @xmath196 .",
    "putting together the results in this section , we have the following theorem .",
    "[ th : zconv ] * ( linear parameter convergence ) * + assume that @xmath18 is convex decomposable and that @xmath197 .",
    "then , for arbitrary initial conditions @xmath127 , the linear parameters @xmath85 converge .",
    "further , the corresponding estimates @xmath97 converge to the global optimum @xmath136 .",
    "in section  [ se : quad ] , we established the convergence of the quadratic parameters @xmath84 . in section  [",
    "se : lin ] , we established the convergence of the linear parameters @xmath85 assuming the quadratic parameters were set to their fixed point . here",
    ", we will combine these results in order to prove theorem  [ th : main - convergence ] , which establishes convergence of the full min - sum algorithm , where the linear parameters evolve jointly with the quadratic parameters .",
    "it suffices to establish convergence of the linear parameters @xmath85 .",
    "define the matrix @xmath198 by @xmath199 define the diagonal matrix @xmath200 by @xmath201 .",
    "then , the min - sum update becomes @xmath202 where @xmath203 is defined by . from theorem",
    "[ th : gammaconv ] , it is clear that @xmath204 and @xmath205 ( where @xmath126 and @xmath206 are defined by and , respectively ) .    from lemma  [ le : atconv ] ,",
    "the spectral radius of @xmath180 is less than 1 .",
    "hence , there is a vector norm @xmath207 on @xmath208 and a corresponding induced operator norm such that @xmath209 , for some @xmath210 @xcite .",
    "pick @xmath211 sufficiently large so that @xmath212 for all @xmath213 .",
    "then , the series @xmath214 converges for @xmath213 . set @xmath215 @xmath216 then , for @xmath213 , @xmath217 since @xmath218 , for any @xmath219 we can pick @xmath220 so that if @xmath221 , @xmath222",
    ". then , for @xmath221 , @xmath223 repeating over @xmath29 , @xmath224 thus , @xmath225 since @xmath226 is arbitrary , it is clear that @xmath85 converges to @xmath227 .",
    "the fact that @xmath97 converges to @xmath136 follows from the same argument as in theorem  [ th : zconv ] .",
    "the work we have presented thus far considers the convergence of a synchronous variation of the min - sum algorithm . in that case , every component of each of the parameter vectors @xmath84 and @xmath85 is update at every time step .",
    "however , the min - sum algorithm has a naturally parallel nature and can be applied in distributed contexts .",
    "in such implementations , different processors may be responsible for updating different components of the parameter vector .",
    "further , these processors may not be able to communicate at every time step , and thus may have insufficient information to update the corresponding components of the parameter vectors .",
    "there may not even be a notion of a shared clock .",
    "as such , it is useful to consider the convergence properties of the min - sum algorithm under an _ asynchronous _ model of computation .    in such a model",
    ", we assume that a processor associated with vertex @xmath26 is responsible for updating the parameters @xmath228 and @xmath229 for each neighbor @xmath230 .",
    "we define the @xmath231 to be the set of times at which these parameters are updated .",
    "we define @xmath232 to be the last time the processor at vertex @xmath40 communicated to the processor at vertex @xmath26 . then , the parameters evolve according to @xmath233 @xmath234 note that the processor at vertex @xmath26 is not computing its updates with the most recent values of the other components of the parameter vector .",
    "it uses the values of components from the last time it communicated with a particular processor .",
    "we will make the assumption of _ total asynchronism _",
    "@xcite : we assume that each set @xmath235 is infinite , and that if @xmath236 is a sequence in @xmath235 tending to infinity , then @xmath237 , for each neighbor @xmath238 .",
    "this mild assumption guarantees that each component is updated infinitely often , and that processors eventually communicate with neighboring processors .",
    "it allows for arbitrary delays in communication , and even the out - of - order arrival of messages between processors .",
    "we can extend the convergence result of theorem  [ th : main - convergence ] to this setting .",
    "the proof is straightforward given the results we have already established and standard results on asynchronous algorithms ( see @xcite , for example ) .",
    "we will provide an outline here .",
    "for the convergence of the quadratic parameters , note that the synchronous iteration is a monotone mapping ( see lemma  [ le : fprop ] in appendix  [ ap : gammaconv ] ) .",
    "for such monotone mappings , synchronous convergence implies totally asynchronous convergence by proposition  6.2.1 in @xcite . the linear parameter update equation for the synchronous algorithm has the form @xmath239 for @xmath29 sufficiently large , by the convergence of the quadratic parameters , the matrix @xmath240 becomes arbitrarily close to @xmath126 . from lemma  [ le : atconv ] , the matrix @xmath180 has spectral radius less than one . in this case",
    ", by corollary  2.6.2 in @xcite , it must correspond to a weighted maximum norm contraction .",
    "then , one can establish asynchronous convergence of the linear parameters by appealing again to proposition  6.2.1 in @xcite .",
    "the following corollary is a restatement of theorem  [ th : main - convergence ] in terms of message passing updates of the form .",
    "[ co : conv]*(convergence of message passing updates ) * + let @xmath94 be a convex decomposition of @xmath18 , and let @xmath99 be a decomposition of @xmath18 into quadratic functions such that @xmath241 is a convex function of @xmath242 , for all @xmath47 . then , using the decomposition @xmath93 and quadratic initial messages @xmath243 , the running estimates @xmath97 generated by the min - sum algorithm converge .",
    "further , @xmath244    the work of johnson , et al .",
    "@xcite identifies existence of convex decomposition of the objective as a important condition for such convergence results and also introduces the notion of walk - summability .",
    "however , the convergence analysis presented there only establishes a special case of the above corollary , where @xmath245 @xmath246 in addition , they present a quadratic program that is not convex decomposable , and where the min - sum algorithm fails to converge .    the prior work of the current authors in @xcite considers a case that arises in distributed averaging applications .",
    "there , convergence is established when @xmath247 @xmath248 this is also a special case of corollary  [ co : conv ] .",
    "the work in @xcite further develops complexity bounds on the rate of convergence in certain special cases .",
    "study of the rate of convergence of the min - sum algorithm in more general cases remains an open issue .",
    "note that the main convexity condition of corollary  [ co : conv ] can also be interpreted in the context of general convex objectives .",
    "while our analysis is very specific to the quadratic case , the result may be illuminating in the broader context of convex programs .",
    "finally , although every quadratic program can be decomposed over pairwise cliques , as we assume in this paper , there may also be decompositions involving higher order cliques .",
    "our analysis does not apply to that case , and this is an interesting question for future consideration .",
    "define the domain @xmath249 and the operator @xmath250 by @xmath251 this operator corresponds to a single min - sum update of the quadratic parameters .",
    "we will first establish some properties of this operator .",
    "[ le : fprop ] the following hold :    * the operator @xmath252 is continuous . *",
    "the operator @xmath252 is monotonic .",
    "that is , if @xmath253 and @xmath254 , @xmath255 . *",
    "the operator @xmath252 is positive .",
    "that is , if @xmath256 , @xmath257 . * if @xmath107 and @xmath258 , @xmath259 * if @xmath107 , @xmath260 .    parts  ( i)-(iii ) follow from the corresponding properties of the function @xmath261 for @xmath262 .",
    "part  ( v ) follows from setting @xmath263 in part  ( iv ) .",
    "part  ( iv ) remains .",
    "for notational convenience , define @xmath264 @xmath265 we have @xmath266 denote the numerator of the last expression by @xmath267 .",
    "since the denominator is positive , it suffices to show that @xmath268 .",
    "define @xmath269 @xmath270 note that @xmath271 @xmath272 since @xmath107 , we have @xmath273 , for each @xmath74 .",
    "then , we can derive the chain of inequalities @xmath274    we are now ready to prove theorem  [ th : gammaconv ] .",
    "assume that @xmath18 is convex decomposable .",
    "the set of system of equations @xmath103 has a solution @xmath104 such that @xmath105 moreover , @xmath104 is the unique such solution .    if we initialize the min - sum algorithm so that @xmath106 , for some @xmath107 , then @xmath108 for all @xmath109 , and @xmath110    pick some @xmath107 .",
    "then , @xmath260 from part  ( v ) of lemma  [ le : fprop ] .",
    "thus , we have @xmath275 , for all @xmath115 , by monotonicity .",
    "( here , @xmath276 denotes @xmath29 applications of the operator @xmath252 . )",
    "then , the sequence @xmath277 is a monotonically decreasing sequence , which by the positivity of @xmath252 , is bounded below by zero .",
    "hence , the limit @xmath278 exists . by continuity",
    ", it must be a fixed point of @xmath252 .",
    "now , note that , by positivity , @xmath279 .",
    "thus , by monotonicity , @xmath280 , for all @xmath115 . since @xmath281",
    ", we have @xmath282 , for all @xmath115 , and this sequence converges to a fixed point @xmath283 .",
    "we wish to show that @xmath284 .",
    "assume otherwise .",
    "define @xmath285 since @xmath286 , the set in the above infimum is not empty . since @xmath287 and @xmath288 , we must have @xmath289 .",
    "then , we have @xmath290 applying @xmath252 and using part  ( iv ) of lemma  [ le : fprop ] , @xmath291 this contradicts the definition of @xmath292 .",
    "thus , we must have @xmath284",
    ".    set @xmath293 . from the above argument , we have @xmath294 , for all @xmath295 .",
    "thus , @xmath104 satisfies the conditions of the lemma .",
    "assume there is some other fixed point @xmath296 satisfying the conditions of the lemma .",
    "positivity implies @xmath297 .",
    "then , since @xmath298 for some @xmath107 , by repeatedly applying @xmath252 , we have @xmath299 for all @xmath109 .",
    "taking a limit as @xmath300 , it is clear that @xmath301 .",
    "it remains to prove the final statement of the lemma .",
    "consider @xmath111 , with @xmath106 , for some @xmath107 .",
    "note that @xmath302",
    ". then , @xmath303 for all @xmath109 .",
    "taking limits , @xmath110",
    "for the balance of this section , we assume that @xmath18 admits a convex decomposition .    in order to prove lemma  [ le : nuwalksum ] , we first fix an arbitrary vertex @xmath172 , and consider an infinite computation tree rooted at a vertex @xmath304 corresponding to @xmath172 .",
    "such a tree is constructed in an iterative process , first starting with a single vertex @xmath304 . as each step",
    ", vertices are added to leaves on the tree corresponding to the neighbors of the leaf in the original graph other than its parent .",
    "hence , the tree s vertices consist of replicas of vertices in the original graph , and the local structure around each vertex is the same as that in the original graph .",
    "we can extend both functions @xmath155 and @xmath157 to walks on the computation tree by defining weights on edges in the computation tree according to the weights of the corresponding edges in the original graph .",
    "we will use the tilde symbol to distinguish vertices and subsets of the computation tree from those in the underlying graph .",
    "we begin with a lemma .",
    "[ le : srminus ] given connected vertices @xmath305 in the computation tree , with labels @xmath26,@xmath40 , respectively , let @xmath306 be the set of walks starting at @xmath307 and returning to @xmath307 but never crossing the edge @xmath308 .",
    "then , @xmath309    first , note that walks in @xmath310 can be mapped to disjoint walks on the original graph .",
    "hence , by walk - summability , the infinite sum @xmath311 converges absolutely .",
    "now , define the set @xmath312 to be the set of walks in @xmath306 that travel at most a distance @xmath313 away from @xmath307 in the computation tree .",
    "a walk @xmath314 can be decomposed into a series of traversals to neighbors @xmath315 , self - returning walks from @xmath316 to @xmath316 that do not cross @xmath317 and travel at most distance @xmath318 from @xmath316 , and then returns to @xmath307 . letting @xmath29 index the total number of such traversals , we have the expression @xmath319 by walk - summability , this infinite sum must converge .",
    "thus , @xmath320 by the symmetry of the computation tree , the quantity @xmath321 depends only on the labels of @xmath307 and @xmath322 in the original graph . set @xmath323 and @xmath324 , for each @xmath325 and integer @xmath326 .",
    "then , we have @xmath327 by theorem  [ th : gammaconv ] , we have @xmath328 then , since @xmath329 , and @xmath330 we have @xmath331    we call a walk on the computation tree a _ shortest - path _ walk if it is the unique shortest path between its endpoints . given a shortest - path walk @xmath332 define @xmath333 to be the set of all walks of the form @xmath334 where @xmath335 , for @xmath336 .",
    "intuitively , these walks proceed along the path @xmath337 , but at each point @xmath338 , they may also take a self - returning walk from vertex @xmath338 to vertex @xmath338 that does not cross the edge @xmath339 .",
    "[ le : tildep ] given a shortest - path walk @xmath332 , @xmath340    @xmath341    we are now ready to prove lemma  [ le : nuwalksum ] .",
    "assume that @xmath18 is convex decomposable . for each @xmath162",
    ", there exists a set of walks @xmath163 , all terminating at the same vertex as @xmath142 , such that @xmath164 further , if @xmath165 and @xmath166 , then @xmath167 and @xmath168 are disjoint .",
    "take a vertex @xmath26 in the original graph . given a walk from @xmath26 to @xmath172 in the original graph",
    ", there is a unique corresponding walk from a replica of @xmath26 to @xmath304 in the computation tree . also notice that non - backtracking walks in the original graph that terminate at @xmath172 correspond uniquely to shortest - path walks in the computation tree that terminate at @xmath304 .",
    "now , assume that @xmath342 terminates at @xmath172 .",
    "let @xmath332 be the corresponding shortest - path walk in the computation tree , and consider the set @xmath343 .",
    "we will define @xmath167 to be the set of walks in the original graph corresponding to @xmath343 . from lemma  [ le : tildep ] , @xmath344    now , consider another walk @xmath165 , @xmath166 , that also terminates at @xmath172 .",
    "we would like to show that @xmath167 and @xmath168 are disjoint .",
    "let @xmath345 be the shortest - path walk corresponding to @xmath346 .",
    "equivalently , we can show @xmath343 and @xmath347 are    disjoint .",
    "assume there is some walk @xmath348 .",
    "then , both @xmath332 and @xmath345 must be the shortest - path from the origin of @xmath316 to @xmath304 .",
    "since shortest - paths between a pair of vertices on the computation tree are unique , we must have @xmath349 and this @xmath350 , which is a contradiction .",
    "note that we only considered non - backtracking walks terminating at a fixed vertex @xmath172 .",
    "however , our choice or @xmath172 was arbitrary hence we can repeat the construction for each @xmath351 .",
    "moreover , if @xmath142 and @xmath346 terminate at different vertices @xmath172 and @xmath352 , respectively , the sets @xmath167 and @xmath168 will contain only walks that terminate at @xmath172 and @xmath352 , respectively , thus they will be disjoint .    using similar arguments as above , we can prove lemma  [ le : nuexact ] .",
    "assume that @xmath18 is convex decomposable .",
    "if we define @xmath171 to be the set of all non - backtracking walks from vertex @xmath26 to vertex @xmath172 , we have @xmath173    consider a walk @xmath353 , and let @xmath354 be the unique corresponding walk in the computation tree terminating at @xmath304 .",
    "let @xmath332 be the unique shortest - path walk corresponding to @xmath354 .",
    "note that @xmath332 will originate at a replica of @xmath26 , and end at @xmath304 .",
    "thus , @xmath332 uniquely corresponds to a non - backtracking walk @xmath355 .",
    "now , @xmath354 can be uniquely decomposed according to @xmath356 where @xmath335 , for @xmath336 , and @xmath357 is a self - returning walk from @xmath304 to @xmath304 . applying lemma  [ le : tildep ]",
    ", we have @xmath358 where @xmath359 is the set of self - returning walks from @xmath304 to @xmath304 .",
    "however , a walk @xmath360 can be uniquely decomposed into a series of traversals to neighbors @xmath361 , self - returning walks from @xmath316 to @xmath316 that do not cross @xmath362 , and then returns to @xmath307 . letting @xmath29 index the total number of such traversals , we have the expression @xmath363 from lemma  [ le : srminus ] , @xmath364 thus , @xmath365",
    "the first author wishes to thank jason johnson for a helpful discussion .",
    "the first author was supported by a benchmark stanford graduate fellowship .",
    "this material is based upon work supported by the national science foundation under grant no .",
    "iis-0428868 .",
    "p.  rusmevichientong and b.  van  roy , `` an analysis of belief propagation on the turbo decoding graph with gaussian densities , '' _ ieee transactions on information theory _ , vol .",
    "47 , no .  2 ,",
    "pp . 745765 , 2001 .    m.  j. wainwright , t.  jaakkola , and a.  s. willsky , `` tree - based reparameterization framework for analysis of sum - product and related algorithms , '' _ ieee transactions on information theory _",
    "49 , no .  5 ,",
    "pp . 11201146 , 2003 .",
    "m.  j. wainwright , t.  jaakkola , and a.  s. willsky , `` tree consistency and bounds on the performance of the max - product algorithm and its generalizations , '' _ statistics and computing _ , vol .",
    "14 , pp . 143166 , 2004 .",
    "c.  c. moallemi and b.  van  roy , `` convergence of the min - sum algorithm for convex optimization , '' management science & engineering department , stanford university , tech . rep . , 2007 , url : http://moallemi.com/ciamac/papers/cc-2007.pdf .",
    "ciamac c. moallemi ciamac c. moallemi is an assistant professor at the graduate school of business of columbia university , where he has been since 2007 .",
    "he received sb degrees in electrical engineering & computer science and in mathematics from the massachusetts institute of technology ( 1996 ) .",
    "he studied at the university of cambridge , where he earned a certificate of advanced study in mathematics , with distinction ( 1997 ) .",
    "he received a phd in electrical engineering from stanford university ( 2007 ) .",
    "he is a member of the ieee and informs .",
    "he is the receipient of a british marshall scholarship ( 1996 ) and a benchmark stanford graduate fellowship ( 2003 ) .",
    "benjamin van roy benjamin van roy is an associate professor of management science and engineering , electrical engineering , and , by courtesy , computer science , at stanford university .",
    "he has held visiting positions as the wolfgang and helga gaul visiting professor at the university of karlsruhe and as the chin sophonpanich foundation professor of banking and finance at chulalongkorn university .",
    "he received the sb ( 1993 ) in computer science and engineering and the sm ( 1995 ) and phd ( 1998 ) in electrical engineering and computer science , all from mit .",
    "he is a member of informs and ieee .",
    "he has served on the editorial boards of discrete event dynamic systems , machine learning , mathematics of operations research , and operations research .",
    "he has been a recipient of the mit george c. newton undergraduate laboratory project award ( 1993 ) , the mit morris j. levin memorial master s thesis award ( 1995 ) , the mit george m. sprowls doctoral dissertation award ( 1998 ) , the nsf career award ( 2000 ) , and the stanford tau beta pi award for excellence in undergraduate teaching ( 2003 ) .",
    "he has been a frederick e. terman fellow and a david morgenthaler ii faculty scholar ."
  ],
  "abstract_text": [
    "<S> we establish the convergence of the min - sum message passing algorithm for minimization of a quadratic objective function given a convex decomposition . </S>",
    "<S> our results also apply to the equivalent problem of the convergence of gaussian belief propagation .    </S>",
    "<S> moallemi , van roy : convergence of the min - sum message passing algorithm + for quadratic optimization    message - passing algorithms , decentralized optimization </S>"
  ]
}