{
  "article_text": [
    "text recognition in natural images , _ scene text recognition _ , is a challenging but wildly useful task .",
    "text is one of the basic tools for preserving and communicating information , and a large part of the modern world is designed to be interpreted through the use of labels and other textual cues .",
    "this makes scene text recognition imperative for many areas in information retrieval , in addition to being crucial for human - machine interaction .",
    "while the recognition of text within scanned documents is well studied and there are many document ocr systems that perform very well , these methods do not translate to the highly variable domain of scene text recognition . when applied to natural scene images , traditional ocr techniques",
    "fail as they are tuned to the largely black - and - white , line - based environment of printed documents , while text occurring in natural scene images suffers from inconsistent lighting conditions , variable fonts , orientations , background noise , and imaging distortions .",
    "to effectively recognise scene text , there are generally two stages : word detection and word recognition .",
    "the detection stage generates a large set of word bounding box candidates , and is tuned for speed and high recall .",
    "previous work uses sliding window methods  @xcite or region grouping methods  @xcite very successfully for this .",
    "subsequently , these candidate detections are recognised , and this recognition process allows for filtering of false positive word detections .",
    "recognition is therefore a far more challenging problem and it is the focus of this paper .",
    "while most approaches recognize individual characters by pooling evidence locally , goodfellow  @xcite do so from the image of the whole character string using a convolutional neural network ( cnn )  @xcite .",
    "they apply this to street numbers and synthetic captcha recognition obtaining excellent results .",
    "inspired by this approach , we move further in the direction of holistic word classification for scene text , and make two important contributions .",
    "firstly , we propose a state - of - the - art cnn text recogniser that also pools evidence from images of entire words .",
    "crucially , however , we regress all the characters simultaneously , formulating this as a classification problem in a large lexicon of 90k possible words  ( sect .",
    "[ sec : dictionary ] ) . in order to do so , we show how cnns can be efficiently trained to recognise a very large number of words using",
    "_ incremental training_. while our lexicon is restricted , it is so large that this hardly constitutes a practical limitation .",
    "secondly , we show that this state - of - the - art recogniser can be trained _ purely from synthetic data_. this result is highly non - trivial as , differently from captcha , the classifier is then applied to real images .",
    "while synthetic data was used previously for ocr , it is remarkable that this can be done for scene text , which is significantly less constrained .",
    "this allows our framework to be seamlessly extended to larger vocabularies and other languages without any human - labelling cost .",
    "in addition to these two key contributions , we study two alternative models  a character sequence encoding model with a modified formulation to that of  @xcite  ( sect .",
    "[ sec : characters ] ) , and a novel bag - of - n - grams encoding model which predicts the unordered set of n - grams contained in the word image  ( sect .",
    "[ sec : ngrams ] ) .",
    "a discussion of related work follows immediately and our data generation system described after in  sect .",
    "[ sec : synth ] .",
    "our deep learning word recognition architectures are presented in  sect .",
    "[ sec : architectures ] , evaluated in  sect .",
    "[ sec : eval ] , and conclusions are drawn in  sect .",
    "[ sec : conclusions ] .",
    "[ [ sec : related ] ] related work .",
    "+ + + + + + + + + + + + +    traditional text recognition methods are based on sequential character classification by either sliding windows  @xcite or connected components  @xcite , after which a word prediction is made by grouping character classifier predictions in a left - to - right manner .",
    "the sliding window classifiers include random ferns  @xcite in wang  @xcite , and cnns in  @xcite .",
    "both  @xcite and  @xcite use a small fixed lexicon as a language model to constrain word recognition .",
    "more recent works such as  @xcite make use of over - segmentation methods , guided by a supervised classifier , to generate candidate proposals which are subsequently classified as characters or false positives .",
    "for example , photoocr  @xcite uses binarization and a sliding window classifier to generate candidate character regions , with words recognised through a beam search driven by classifier scores followed by a re - ranking using a dictionary of 100k words .",
    "@xcite uses the convolutional nature of cnns to generate response maps for characters and bigrams which are integrated to score lexicon words .",
    "in contrast to these approaches based on character classification , the work by  @xcite instead uses the notion of holistic word recognition .",
    "@xcite still rely on explicit character classifiers , but construct a graph to infer the word , pooling together the full word evidence .",
    "rodriguez  @xcite use aggregated fisher vectors  @xcite and a structured svm framework to create a joint word - image and text embedding .",
    "@xcite use whole word - image features to recognize words by comparing to simple black - and - white font - renderings of lexicon words .",
    "goodfellow  @xcite had great success using a cnn with multiple position - sensitive character classifier outputs ( closely related to the character sequence model in  sect .",
    "[ sec : characters ] ) to perform street number recognition .",
    "this model was extended to captcha sequences ( up to 8 characters long ) where they demonstrated impressive performance using synthetic training data for a synthetic problem ( where the generative model is known ) , but we show that synthetic training data can be used for a real - world data problem ( where the generative model is unknown ) .",
    "> m10ptm0.8 ( a ) &   + ( b ) & +    this section describes our scene text rendering algorithm .",
    "as our cnn models take whole word images as input instead of individual character images , it is essential to have access to a training dataset of cropped word images that covers the whole language or at least a target lexicon .",
    "while there are some publicly available datasets from icdar  @xcite , the street view text ( svt ) dataset  @xcite and others , the number of full word image samples is only in the thousands , and the vocabulary is very limited .",
    "these limitations have been mitigated before by mining for data or having access to large proprietary datasets  @xcite , but neither of these approaches are wholly accessible or scalable .    here",
    "we follow the success of some synthetic character datasets  @xcite and create a synthetic word data generator , capable of emulating the distribution of scene text images .",
    "this is a reasonable goal , considering that much of the text found in natural scenes is computer - generated and only the physical rendering process (  printing , painting ) and the imaging process (  camera , viewpoint , illumination , clutter ) are not controlled by a computer algorithm .",
    "[ fig : synthdata ] illustrates the generative process and some resulting synthetic data samples .",
    "these samples are composed of three separate image - layers  a background image - layer , foreground image - layer , and optional border / shadow image - layer  which are in the form of an image with an alpha channel .",
    "the synthetic data generation process is as follows :    1 .   _",
    "font rendering _  a font is randomly selected from a catalogue of over 1400 fonts downloaded from google fonts .",
    "the kerning , weight , underline , and other properties are varied randomly from arbitrarily defined distributions . the word is rendered on to the foreground image - layer s alpha channel with either a horizontal bottom text line or following a random curve .",
    "border / shadow rendering _  an inset border , outset border or shadow with a random width may be rendered from the foreground .",
    "base coloring _",
    " each of the three image - layers are filled with a different uniform color sampled from clusters over natural images .",
    "the clusters are formed by k - means clustering the three color components of each image of the training datasets of  @xcite into three clusters .",
    "projective distortion _ ",
    "the foreground and border / shadow image - layers are distorted with a random , full - projective transformation , simulating the 3d world .",
    "natural data blending _",
    " each of the image - layers are blended with a randomly - sampled crop of an image from the training datasets of icdar 2003 and svt .",
    "the amount of blend and alpha blend mode (  normal , add , multiply , burn , max , _ etc .",
    "_ ) is dictated by a random process , and this creates an eclectic range of textures and compositions .",
    "the three image - layers are also blended together in a random manner , to give a single output image .",
    "noise _  gaussian noise , blur , and jpeg compression artefacts are introduced to the image .",
    "the word samples are generated with a fixed height of 32  pixels , but with a variable width .",
    "since the input to our cnns is a fixed - size image , the generated word images are rescaled so that the width equals 100 pixels .",
    "although this does not preserve the aspect ratio , the horizontal frequency distortion of image features most likely provides the word - length cues .",
    "we also experimented with different padding regimes to preserve the aspect ratio , but found that the results are not quite as good as with resizing .",
    "the synthetic data is used in place of real - world data , and the labels are generated from a corpus or dictionary as desired . by creating training datasets much larger than what has been used before , we are able to use data - hungry deep learning algorithms to train richer , whole - word - based models .",
    "in this section we describe three models for visual recognition of scene text words .",
    "all use the same framework of generating synthetic text data ( sect .  [",
    "sec : synth ] ) to train deep convolutional networks on whole - word image samples , but with different objectives , which correspond to different methods of reading .",
    "[ sec : dictionary ] describes a model performing pure word classification to a large dictionary , explicitly modelling the entire known language .",
    "[ sec : characters ] describes a model that encodes the character at each position in the word , making no language assumptions to naively predict the sequence of characters in an image .",
    "[ sec : ngrams ] describes a model that encodes a word as a bag - of - n - grams , giving a compositional model of words as not only a collection of characters , but of 2-grams , 3-grams , and more generally , n - grams .",
    "this section describes our first model for word recognition , where words @xmath0 are constrained to be selected in a pre - defined dictionary @xmath1 .",
    "we formulate this as multi - class classification problem , with one class per word . while the dictionary @xmath1 of a natural language may seem too large for this approach to be feasible , in practice an advanced english vocabulary , including different word forms , contains only around 90k words , which is large but manageable .    in detail",
    ", we propose to use a cnn classifier where each word @xmath2 in the lexicon corresponds to an output neuron .",
    "we use a cnn with four convolutional layers and two fully connected layers .",
    "rectified linear units are used throughout after each weight layer except for the last one . in forward order ,",
    "the convolutional layers have 64 , 128 , 256 , and 512 square filters with an edge size of 5 , 5 , 3 , and 3 .",
    "convolutions are performed with stride 1 and there is input feature map padding to preserve spatial dimensionality .",
    "@xmath3 max - pooling follows the first , second and third convolutional layers .",
    "the fully connected layer has 4096 units , and feeds data to the final fully connected layer which performs classification , so has the same number of units as the size of the dictionary we wish to recognize .",
    "the predicted word recognition result @xmath4 out of the set of all dictionary words @xmath1 in a language @xmath5 for a given input image @xmath6 is given by @xmath7 . since @xmath8 and with the assumptions that @xmath6 is independent of @xmath5 and that prior to any knowledge of our language all words are equally probable , our scoring function reduces to @xmath9 .",
    "the per - word output probability @xmath10 is modelled by the softmax scaling of the final fully connected layer , and the language based word prior @xmath11 can be modelled by a lexicon or frequency counts .",
    "a schematic of the network is shown in  fig .",
    "[ fig : dictnet ]  ( a ) .",
    "[ [ training . ] ] training .",
    "+ + + + + + + + +    we train the network by back - propagating the standard multinomial logistic regression loss with dropout  @xcite , which improves generalization .",
    "optimization uses stochastic gradient descent ( sgd ) , dynamically lowering the learning rate as training progresses . with uniform sampling of classes in training data , we found the sgd batch size must be at least a fifth of the total number of classes in order for the network to train .    for very large numbers of classes (  over 5k classes ) ,",
    "the sgd batch size required to train effectively becomes large , slowing down training a lot .",
    "therefore , for large dictionaries , we perform _ incremental training _ to avoid requiring a prohibitively large batch size .",
    "this involves initially training the network with 5k classes until partial convergence , after which an extra 5k classes are added .",
    "the original weights are copied for the original 5k classes , with the new classification layer weights being randomly initialized . the network is then allowed to continue training , with the extra randomly initialized weights and classes causing a spike in training error , which is quickly trained away .",
    "this process of allowing partial convergence on a subset of the classes , before adding in more classes , is repeated until the full number of desired classes is reached . in practice for this network ,",
    "the cnn trained well with initial increments of 5k classes , and after 20k classes is reached the number of classes added at each increment is increased to 10k .",
    "\\(a )     [ cols=\"^,^,^,^ \" , ]     [ fig : ngramresults ]",
    "in this paper we introduced a new framework for scalable , state - of - the - art word recognition  synthetic data generation followed by whole word input cnns .",
    "we considered three models within this framework , each with a different method for recognising text , and demonstrated the vastly superior performance of these systems on standard datasets .",
    "in addition , we introduced a new synthetic word dataset , orders of magnitude larger than any released before .",
    "this work was supported by the epsrc and erc grant visrec no .",
    "we gratefully acknowledge the support of nvidia corporation with the donation of the gpus used for this research ."
  ],
  "abstract_text": [
    "<S> in this work we present a framework for the recognition of natural scene text . our framework does not require any human - labelled data , and performs word recognition on the whole image holistically , departing from the character based recognition systems of the past . </S>",
    "<S> the deep neural network models at the centre of this framework are trained solely on data produced by a synthetic text generation engine  </S>",
    "<S> synthetic data that is highly realistic and sufficient to replace real data , giving us infinite amounts of training data . </S>",
    "<S> this excess of data exposes new possibilities for word recognition models , and here we consider three models , each one `` reading '' words in a different way : via 90k - way dictionary encoding , character sequence encoding , and bag - of - n - grams encoding . in the scenarios of language based and completely unconstrained text recognition we greatly improve upon state - of - the - art performance on standard datasets , using our fast , simple machinery and requiring zero data - acquisition costs .    </S>",
    "<S> visual geometry group , university of oxford + ` { max,karen,vedaldi,az}@robots.ox.ac.uk ` </S>"
  ]
}