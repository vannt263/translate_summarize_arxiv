{
  "article_text": [
    "numerous examples of power  law distributions ( pld ) are well - known in different fields of science and human activity @xcite .",
    "power laws are considered @xcite as one of signatures of complex self - organizing systems .",
    "they are sometimes called zipf  pareto law or fractal distributions .",
    "we can mention here the zipf ",
    "pareto law in linguistics @xcite , economy @xcite and in the science of sciences @xcite , gutenberg - richter law in geophysics @xcite , pld in critical phenomena @xcite , pld of avalanche sizes in sandpile model for granulated media @xcite and fragment masses in the impact fragmentation @xcite , etc .",
    "according to the well - known maximum entropy principle developed by jaynes @xcite for a boltzmann - gibbs statistics an equilibrium distribution of probabilities must provide maximum of the boltzmann information entropy @xmath7 upon additional conditions of normalization @xmath8 and a fixed average energy @xmath9 .    then , the gibbs canonical distribution @xmath10 is determined from the extremum of the functional @xmath11 where @xmath12 and @xmath13 are lagrange multipliers and @xmath14 is the thermodynamic temperature .",
    "however , when investigating complex physical systems ( for example , fractal and self - organizing structures , turbulence ) and a variety of social and biological systems , it appears that the gibbs distribution does not correspond to observable phenomena . in particular ,",
    "it is not compatible with a power - law distribution that is typical @xcite for such systems .",
    "introducing of additional constraints on a sought distribution in the form of conditions of true average values @xmath15 of some physical parameters of the system @xmath16 gives rise to a generalized gibbs distribution with additional terms in the exponent but does not change its exponential form .",
    "montroll and shlesinger @xcite investigated this problem and found that maximum entropy principle applied to the gibbs ",
    "shannon entropy could give rise to the power ",
    "law distribution under only very special constraint that `` has not been considered as a natural one for use in auxiliary conditions . ''",
    "the well  known boltzmann formula defines a statistical entropy , as a logarithm of a number of states @xmath17 attainable for the system @xmath18 here and below the entropy is written as dimensionless value without the boltzmann constant @xmath19 . besides",
    ", we will use a natural logarithm instead of binary logarithm accepted in information theory .",
    "this definition is valid not only for physical systems but for much more wide class of social , biological , communication and other systems described with the use of statistical approach .",
    "the only but decisive restriction on the validity of this equation is the condition that all @xmath17 states of the system have equal probabilities ( such systems are described in statistical physics by a microcanonical ensemble ) .",
    "it means that probabilities @xmath20 ( for all @xmath21 ) that permits to rewrite the boltzmann formula as @xmath22 when the probabilities @xmath23 are not all equal we can introduce an ensemble of microcanonical subsystems in such a manner that all @xmath24 states of the @xmath25-th subsystem have equal probabilities @xmath23 and its boltzmann entropy is @xmath26 .",
    "the simple averaging of the boltzmann entropy @xmath27 leads to the gibbs ",
    "shannon entropy @xmath28 just such derivation of @xmath29 is used in some textbooks ( see , e. g. @xcite )    this entropy is generally accepted in equilibrium and non - equilibrium statistical thermodynamics @xcite and communication theory but needs in modification for complex systems . to seek out a direction of modification of the gibbs ",
    "shannon entropy we consider first extremal properties of an equilibrium state in thermodynamics .",
    "a direct calculation of an average energy of a system gives the internal energy @xmath30 , its extremum is characteristic of an equilibrium state of rest for a mechanical system , other than a thermodynamic system that can change heat with a heat bath of a fixed temperature @xmath14 .",
    "an equilibrium state of the latter system is characterized by extremum of the helmholtz free energy @xmath31 . to derive it statistically from the hamiltonian @xmath32 without use of thermodynamics we can following balescu @xcite",
    "introduce a moment - generating function @xmath33 where @xmath34 is the arbitrary constant , and construct a cumulant - generating function @xmath35 that becomes the helmholtz free energy @xmath31 when divided by @xmath34 that is chosen as @xmath36 .",
    "such a choice of the pre - factor @xmath37 ensures a limiting passing of the helmholtz free energy @xmath31 into the internal energy @xmath30 when @xmath38 ( @xmath39 ) .",
    "now we return to the problem of a generalized entropy for open complex systems .",
    "exchange by both energy and entropy ( or information ) with the environment is inherent in such systems ( see e. g. the book @xcite devoted to this subject ) .",
    "it is pertinent to introduce the noun of an _ entropy bath _ ( or _ information bath _ ) .",
    "coupling with the entropy bath can be regarded as a necessary condition for self - organization of a complex system .    as a result of such coupling",
    "the system under consideration can not reach a state of thermodynamic equilibrium that is characterized by maximum of the gibbs ",
    "shannon entropy , derived by the simple averaging of the boltzmann entropy .",
    "it is necessary to look for any other function to characterize its steady state resulted from the coupling with the entropy bath .",
    "an effort may be made to find a `` free entropy '' of a sort by the same way that was used above for derivation of the helmholtz free energy for a system coupled with a heat bath .",
    "the moment - generating function is introduced as @xmath40 then the cumulant - generating function is @xmath41 to obtain the desired generalization of the entropy we are to find an @xmath34-dependent numerical pre - factor which ensures a limiting pass of the new entropy into the gibbs ",
    "shannon entropy .",
    "such the coefficient is @xmath42 . indeed , the new @xmath34-family of entropies @xmath43 includes the gibbs  shannon entropy as a particular case when @xmath44 .",
    "thus , it has appeared that the desired `` free entropy '' coincides with the known renyi entropy @xcite .",
    "it is conventional to write it with the parameter @xmath45 in the form @xmath46 on the other hand , we can represent eq .",
    "( [ 6 ] ) as @xmath47 then the renyi entropy can be represented as a particular case of the kolmogorov ",
    "nagumo @xcite generalized averages @xmath48 if we put there the kolmogorov  nagumo function in the form @xmath49 , @xmath50 and choose @xmath51 .",
    "renyi introduced his entropy just in such a manner .",
    "renyi wanted to find the most general class of entropies which preserved the additivity for statistically independent systems and was compatible with the kolmogorov ",
    "nagumo generalized average . by this way he found the exponential kolmogorov  nagumo function @xmath52 .",
    "physically , such a choice of @xmath52 on its own appears accidental until it is not pointed to the fact that the same exponential function of the hamiltonian provides derivation of the free energy which is extremal in an equilibrium state of a thermodynamic system exchanging heat with a heat bath .",
    "this fact permits us to suppose that the renyi entropy derived in the same manner should be extremal at a steady state of a complex system which exchange entropy with its surroundings actively .",
    "note that for linear @xmath53 the kolmogorov ",
    "nagumo generalized average turns out to be the ordinary linear mean and hence the gibbs - shannon entropy follows as an average entropy in the usual sense .",
    "in view of the way of the renyi entropy derivation we can suppose that it is maximal at a steady state of a complex system being in contact with the entropy bath .",
    "such a supposing is justified by the shore  johnson theorem @xcite .",
    "they considered a procedure of updating of a distribution function when a new information related to the system had appeared in a form of an additional constraint @xmath54 .",
    "shore and johnson gave five `` consistency axioms '' for this updating operation @xcite : 1 ) uniqueness : the result should be unique .",
    "2 ) invariance : the choice of coordinate system should not matter ( for continuous probability densities).3 ) system independence : it should not matter whether one accounts for independent information about independent systems separately in terms of different densities or in terms of a joint density .",
    "4 ) subset independence : it should not matter whether one treats disjoint subsets of system states in terms of separate conditional densities or in terms of the full density .",
    "5 ) in the absence of new information , we should not change the prior .",
    "the following theorem is proven on the base of these axioms ( here it is for the particular case of a homogeneous prior distribution @xmath55 ( for all @xmath25 ) ) : + * theorem * _ an updating procedure satisfies the five consistency axioms above if and only if it is equivalent to the rule + maximize @xmath56 under the constraint @xmath54 .",
    "_ + or + _ maximize any monotonous function @xmath57 under the constraint @xmath54 . _    the most evident choice of the monotonous function is @xmath58 , that is the renyi entropy @xmath59 for @xmath60 .",
    "such a choice of @xmath61 ensures the limit @xmath62 when @xmath63 and passage of @xmath59 ( for all @xmath0 ) to @xmath64 in the case of absence of new information when @xmath65 ( for all @xmath25 ) .",
    "both these properties should be considered as necessary conditions for a choice of @xmath66 .",
    "in particular , the function @xmath67 leading to the tsallis entropy fails because it does not satisfy the second of these conditions .",
    "thus , the shore - johnson theorem provide quite conclusive foundation of the renyi entropy as itself and the maximum entropy principle for it and in doing so it justifies the above proposal that the renyi entropy as the free entropy is maximal at a steady state of a complex system .    in the light of this theorem the khinchin s uniqueness theorem @xcite for the gibbs ",
    "shannon entropy should be reconsidered .",
    "khinchin based on the next three axioms : + ( 1 ) @xmath68 is a function of the probabilities @xmath23 only and has to take its maximum value for the uniform distribution of probabilities @xmath69 : @xmath70 , where @xmath71 is any other distribution .",
    "+ ( 2 ) the second axiom refers to a composition @xmath72 of a master subsystem @xmath73 and subordinate subsystem @xmath74 for which probability of a composed state is @xmath75 where @xmath76 is the conditional probability to find the subsystem @xmath74 in the state @xmath77 if the master subsystem @xmath78 is in the state @xmath25 .",
    "then the axiom requires that @xmath79 where @xmath80 is the conditional entropy and @xmath81 is the partial conditional entropy of the subsystem @xmath74 when the subsystem @xmath78 is in the @xmath25-th state .",
    "+ ( 3 ) @xmath68 remains unchanged if the sample set is enlarged by a new , impossible event with zero probability : @xmath82    while proving his uniqueness theorem khinchin enlarged the second axiom .",
    "he supposed that all @xmath30 states of the composite system @xmath72 were equally probable , that is , @xmath83 for all @xmath25 and @xmath77 ; whence he got @xmath84 besides , khinchin supposed that @xmath85 states of the subsystem @xmath74 corresponding to each @xmath25-th state of the master subsystem @xmath73 are equally probable , as well .",
    "so , he took @xmath86 for @xmath87 and @xmath88 ; whence from eqs .",
    "( [ 4.1 ] ) and ( [ 4.3 ] ) he obtained @xmath89 and @xmath90 substituting eqs .",
    "( [ 4.4 ] ) , ( [ 4.5 ] ) into eq .",
    "( [ 4.2 ] ) khinchin got the gibbs ",
    "shannon entropy for the master subsystem @xmath73 @xmath91 .",
    "it should be noted that the last khinchin s supposition of equal probabilities of states of the subsystem @xmath74 is the most questionable and should be abandoned .",
    "indeed , the probability distribution for @xmath74 coupled with the master subsystem should be rather canonical distribution than equally probable one .",
    "on the other hand , the abandonment of this supposition destroys all the proof of the khinchin theorem .",
    "so , we are left with only the shore ",
    "johnson theorem where the gibbs ",
    "shannon entropy is no more than a particular case of the renyi entropy .",
    "the maximum entropy principle for the renyi entropy @xmath92 under additional constraints of fixed value @xmath93 , and normalization of @xmath94 gives rise @xcite to the renyi distribution function @xmath95 at @xmath63 the distribution @xmath96 becomes the gibbs canonical distribution in which the constant @xmath97 .",
    "substituting the renyi distribution ( [ rd ] ) into the renyi entropy definition ( [ 9 ] ) , we find the thermodynamic entropy in the renyi thermostatistics as @xmath98 where the boltzmann constant @xmath19 is introduced .    when @xmath63 this entropy passes into thermodynamic entropy in the gibbs thermostatistics @xmath99 on the other hand , the gibbs thermostatistics is based on the gibbs distribution for energy @xmath100 but not fluctuations of energy @xmath101 .",
    "so , the renyi distribution ( [ rd ] ) should be represented as a distribution for energy @xmath100 , as well . dividing both the expression in the brackets in eq .",
    "( [ rd ] ) and @xmath102 by @xmath103 we get the alternative equivalent form of the renyi distribution @xmath104 @xmath105 the thermodynamic renyi entropy can be represented in the alternative form , as well @xmath106    a physical @xmath0-dependent temperature in the renyi superstatistics should be defined in a standard manner as @xmath107 it was shown @xcite that @xmath108 at least for the power - law hamiltonian , so the physical @xmath0-dependent temperature becomes @xmath109 where @xmath14 is the temperature of a heat bath . according to ref .",
    "@xcite , the fact that @xmath110 @xmath111 says in favor of greater ordering of states with lower @xmath0 .",
    "the helmholtz free energy is defined in the renyi thermostatistics as @xmath112 on the other hand , the thermodynamic definition of the free energy should be @xmath113 it is not difficult to ensure that both definitions coinside in the limit @xmath63 . for arbitrary @xmath0 , their equivalence can be checked for the particular case of the power - law hamiltonian . with the use of the relation",
    "@xmath114 @xcite we get @xmath115 where @xmath116/(1-q).\\ ] ] such a difference can not provoke objections because of the free energy is determined in thermodynamics with an accuracy of an addent @xmath117 .",
    "thus , we have gibbs and renyi thermostatistics based on different microscopic entropy definitions .",
    "each of them provides an adequate description of corresponding class of systems and we need in a rigorous formulation of conditions of transfer from one thermostatistics to another .",
    "transfer from the gibbs distribution describing a state of dynamic chaos @xcite to power ",
    "law renyi distributions that are characteristic for ordered self - organized systems @xcite corresponds to an increase of an `` order parameter '' @xmath118 from zero at @xmath119 up to @xmath120 .    in accordance to the landau theory @xcite of phase transitions an entropy derivative with respect to the order parameter undergoes a jump at a point of the phase transition .    here",
    "we deal with the transfer from the gibbs thermostatistics to the renyi thermostatistics corresponding to non - zeroth values of the order parameter @xmath121 .",
    "let us consider a variation of the entropy at this transition .",
    "now it is not difficult to calculate the limiting value at @xmath122 of the derivative of the entropy @xmath123 with respect to @xmath121 .",
    "we get @xmath124 according to a fluctuation theory for the gibbs equilibrium ensemble we have @xmath125 whence @xmath126 where @xmath127 is the heat capacity at a constant volume .",
    "thus , the derivative of the entropy gain with respect to the order parameter exhibits the jump ( equal to @xmath128 ) at @xmath129 .",
    "this permits us to consider the transfer to the renyi thermostatistics as a peculiar kind of a phase transition into a more organized state",
    ". we can give this transition the name _ entropic phase transition_.    as a result of the entropic phase transition the system passes into an ordered state with the order parameter @xmath130 .",
    "in contrast to the usual phase transition that takes place at the temperature of phase transition , conditions of the entropic phase transition are likely to be determined partially for each concrete system .",
    "for example , a threshold of emergence of turbulence ( see @xcite ) as an ordered structure is determined by a critical reynolds number and an emergence of benard cells is determined by a critical rayleigh number ( see @xcite ) .",
    "social , economical and biological systems are realized as a rule in ordered self ",
    "organized forms .",
    "this is the reason why power  law and closely related distributions are characteristic for them but not canonical gibbs distribution .    for the particular case of a power",
    " law hamiltonian @xmath131 problem of a value of the order parameter was discussed in ref .",
    "@xcite where maximum maximorum of the thermodynamic renyi entropy was found at @xmath132 , that is , at @xmath133 .",
    "the renyi distribution for such @xmath6 becomes a pure power ",
    "law distribution that agrees with observable data for self - organized systems .",
    "the landscapes of this entropy @xmath134 $ ] is illustrated in fig . 1 ( left ) .",
    "the landscape of the usual thermodynamic gibbs entropy @xmath135 $ ] for the same renyi distribution is illustrated in fig .",
    "1 ( right ) .",
    "it is seen that in contrast to @xmath136 $ ] the gibbs entropy @xmath135 $ ] decreases with the gain of @xmath3 and attains its maximum at @xmath137 , that is in the most disordered state when the renyi distribution becomes the gibbs canonical one .",
    "it should be stressed that thermodynamic laws are irrelevant to microscopic interpretations of thermodynamic functions . on the other hand , according to the boltzmann  gibbs microscopic interpretation of entropy , its gain is accompanied by evolution of a system to an homogeneous equilibrium state of thermal chaos . in contrast , the renyi thermodynamic entropy increases as a system ordering ( departure of the order parameter @xmath3 from zero ) increases ( see fig .",
    "so , it can be considered as a kind of potential that drives the system to self - organized state .",
    "transfer from the usual gibbs thermostatistics to the renyi thermostatistics takes the form of a phase transition of ordering with the order parameter @xmath3 .",
    "as soon as the system passes into this new phase state of the renyi thermostatistics , a spontaneous development of self  organization to a more ordered state begins accompanied with gain of thermodynamic entropy . in doing",
    "so the well - known contradiction between observable spontaneous self  organization and the second law is eliminated when we use the renyi entropy as a microscopic definition of the thermodynamic entropy instead of the gibbs ",
    "shannon one .",
    "moreover , it may be supposed that biological evolution or development are governed by the extremal principle of the renyi thermostatistics .",
    "+ * acknowledgement . *",
    "it is pleasure to thank a. v. vityazev for many fruitful discussions and supporting this work .",
    "i must to pay homage to late prof .",
    "d. n. zubarev , my teacher who payed my attention to the renyi entropy many years ago .",
    "mantegna r. physica a * 277 * , 136 ( 2000 ) .",
    "_ how nature works : the science of self - organized criticality .",
    "_ n .- y . , copernicus , 1996 .",
    "zipf g. k. _ human behavior and the principle of least efforts .",
    "_ cambridge , addison - wesley , 1949 .",
    "mandelbrot b. b. journ .",
    "business , * 36 * , 394 ( 1963 ) .",
    "price d. _ little science , big science .",
    "columbia univ .",
    "1963 kasahara k. _ earthquake mechanics .",
    "_ cambridge univ . press , 1981 .",
    "baxter r. j. _ exactly solved models in statistical mechanics .",
    "_ london , ac . press , 1982 .",
    "bak p. , tang c. and wiesenfeld k. phys .",
    "rev . a * 38 * , 364 , ( 1988 ) .",
    "fujiwara a. , kamimoto g. and tsukamoto a. icarus , * 31 , * 277 ( 1977 ) .",
    "bashkirov a.g . ,",
    "vityazev a.v .",
    "space sci . * 44 * , 909 ( 1996 ) .",
    "jaynes e.t .",
    "phys.rev . * 106 * , 620 ; * 107 * , 171 ( 1957 ) .",
    "e. w. montroll and m. f. shlesinger , _ journ .",
    "phys . _ * 32*:209230 ( 1983 ) .",
    "haken h ( 1988 ) information and self - organization .",
    "springer , berlin nicolis gs ( 1986 ) dynamics of hierarchial systems .",
    "an evolutionary approach springer , berlin zubarev d. n. _ nonequilibrium statistical thermodynamic . _ plinum press , 1972 .",
    "zubarev d. , morozov v. , ropke g. _ statistical mechanics of nonequilibrium processes .",
    "_ berlin , akademie verlag , 1997 .",
    "balescu r ( 1975 ) equilibrium and nonequilibrium statistical mechanics .",
    "john wiley and sons inc , new york .",
    "renyi a ( 1970 ) probability theory .",
    "north - holland , amsterdam kolmogorov an ( 1930 ) atti r accad naz lincei 12:388396 nagumo m ( 1930 ) japan j math 7:7179 shore",
    "je , johnson rw ( 1980 ) ieee trans inform theory it-26:2637 johnson rw , shore",
    "je ( 1983 ) ieee trans inform theory it-29:942943 uffink j ( 1995 ) studies hist philos mod phys 26b:223253 khinchin ai ( 1953 ) uspekhi mat nauk 8:320 ; khinchin ai ( 1957 ) mathematical foundations of information theory .",
    "dover , new york bashkirov a ( 2004 ) phys rev lett 93:13061 bashkirov ag ( 2004 ) physica a 340:153 - 162 klimontovich yul ( 1994 ) statistical theory of open systems .",
    "kluwer academic publishers , dordrecht landau ld , lifshitz em ( 1980 ) course of theoretical physics , vol .",
    "5 : statistical physics .",
    "pergamon , oxford , part 2 .",
    "zubarev dn , morozov vg , troshkin ov ( 1992 ) theor math phys 92:896908"
  ],
  "abstract_text": [
    "<S> taking into account extremum of a helmholtz free energy in the equilibrium state of a thermodynamic system the renyi entropy is derived from the boltzmann entropy by the same way as the helmholtz free energy from the hamiltonian . </S>",
    "<S> the application of maximum entropy principle to the renyi entropy gives rise to the renyi distribution . </S>",
    "<S> the @xmath0-dependent renyi thermodynamic entropy is defined as the renyi entropy for renyi distribution . </S>",
    "<S> a temperature and free energy are got for a renyi thermostatistics . </S>",
    "<S> transfer from the gibbs to renyi thermostatistics is found to be a phase transition at zero value of an order parameter @xmath1 . </S>",
    "<S> it is shown that at least for a particular case of the power - law hamiltonian @xmath2 this entropy increases with @xmath3 . </S>",
    "<S> therefore in the new entropic phase at @xmath4 the system tends to develop into the most ordered state at @xmath5 . </S>",
    "<S> the renyi distribution at @xmath6 becomes a pure power - law distribution . </S>",
    "<S> + key words : entropy bath , renyi entropy , maximum entropy principle , order parameter , phase transition , self - organization .     </S>",
    "<S> +     + institute dynamics of geospheres , ras , + leninskii prosp . </S>",
    "<S> 38 ( bldg.1 ) , 119334 , moscow , russia + </S>"
  ]
}