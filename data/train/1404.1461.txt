{
  "article_text": [
    "the question of trying to understand what _",
    "information is hidden in the _ shape _ of the word - frequency distribution has a long tradition .",
    "it goes back to the first part of the twentieth century when it was discovered that the word - frequency distribution of a text typically has a broad  fat - tailed \" shape , which often can be well approximated with a power law over a large range @xcite .",
    "this led to the empirical concept of zipf s law which states that the number of words that occur @xmath4-times in a text , @xmath5 , is proportional to @xmath6 @xcite .",
    "the question is then what special principle or property of a language causes this power law distribution of word - frequencies and this is still an ongoing research @xcite .    in middle of the twentieth century simon in ref .",
    "@xcite instead suggested that , since quite a few completely different systems also seemed to follow zipf s law in their corresponding frequency distributions , the explanation of the law must be more general and stochastic in nature and hence independent of any specific information of the language itself .",
    "instead he proposed a random stochastic growth model for a book written one word at a time from beginning to end .",
    "this became a very influential model and has served as a starting point for much later works @xcite . in the ` simon - view '",
    "the shape of the word - frequency distribution does not reflect any specific property of a language but is shaped by a random stochastic element .",
    "an extreme random model was proposed in the middle of the twentieth century by miller in ref .",
    "@xcite : the resulting text can be described as being produced by a monkey randomly typing away on a typewriter .",
    "however the properties of the monkey book are quite unrealistic and different from a real text @xcite .",
    "this ` randomness - view ' was recently developed further in a series of paper in terms of concepts like random group formation , random book transformation and the meta - book @xcite .",
    "a crucial difference , compared to the ` zips - view ' , is that the ` randomness - view ' is based on the notion that the shape of the word - frequency distribution is a general consequence of randomness which carries no specific information of the language .",
    "in other words the  zipf - view  is leaning more on the idea that a language is a special system and that as a consequence the functional form of the word - frequency distribution reflects some specific property of the language , whereas the  randomness - view  maintains that very little specific language information can be extracted from this distribution .",
    "the concept of randomness in a text dates back to at least 1913 and a. markov @xcite : markov demonstrated that even an exquisitely crafted poem like pushkin s eugene onegin , when viewed as a string of letters , contained random features like e.g. how often a randomly chosen letter is followed by a consonant or a vowel .",
    "this was at the beginning of what developed into the fundamental statistical concept of markov chains .",
    "this begs the conceptual question of how something crafted with such an amount of intention , purpose and meaning could possibly contain something entirely random .",
    "a somewhat related question is hidden within the decimal tail of the number @xmath7 : the decimal tail of @xmath8 has a definite cause since it is the ratio between the circumference and diameter of a circle .",
    "thus every decimal in the expansion is solidly given .",
    "however , if you pick a decimal place randomly and read off its value and ask yourself what the value of the next decimal might be , then it is with equal probability any of the numbers 0,1, .. ,9 .",
    "thus the poem eugene onegin and the number @xmath8 both display some randomness in spite of their perfectly deterministic cause .    from a statistical point of view",
    "the decimal tail of @xmath8 is pseudo - random and equivalent to a number - series created by throwing a dice with ten fair outcomes .",
    "however , if the only thing you know is that the decimal tail of @xmath8 is equivalent to a pseudo - random series , throwing the dice will not give you any information as to the ratio between the circumference and the diameter of a circle .",
    "words in a text are random in an analogous fashion ; a specific word occurs @xmath4 times in the text and @xmath5 specific words occur the same number of times .",
    "suppose you randomly pick a word in the text and that this word occurs @xmath9 times .",
    "what is the total number of occurrence in the text of the following word ?",
    "the randomness view argues that this occurrence is random and given by a probability proportional to @xmath5 .",
    "the dice @xmath5 itself can be estimated using the maximum entropy principle @xcite .",
    "the fact that frequency distributions of possible outcomes for some sufficiently complex deterministic systems reduce to equivalent random distributions is not restricted to words@xcite .",
    "deterministic systems which display random features are termed _ pseudo - random_. in the discussion section some more examples are mentioned . however , in the present paper we analyze the consequences for words in a text .",
    "the general point is that the ideas of scale - freeness ingrained into the various zipf s law approaches are superseded by the inherent randomness , which we argue is a very basic property of a written text .    in order to be concrete",
    "we will focus on the difference between , on the one hand , a generalized scaling law for word - frequency distributions proposed by font - clos et al in ref .",
    "@xcite and suggesting a bona fide specific property of a language , and , on the other hand , the general predictions from the  randomness - view  @xcite .",
    "we will in the present paper use the following notation : @xmath10 ( @xmath11 ) is the number of distinct words which occur @xmath4-times ( @xmath4-times or more ) in a text which in total contains @xmath12 words .",
    "the scaling law proposed in ref .",
    "@xcite can be cast into the form @xmath13 .    in section [ sec:2 ] , we first demonstrate directly from raw data that @xmath11 does indeed change shape with text - length in a very systematic manner such that the proposed scaling - form @xmath13 can not be conceptually valid .",
    "this means that this scaling function can not be a true specific feature of the word - frequency distribution . in section [ sec:3 ]",
    ", we then compare the systematic length dependence of @xmath11 with the predictions from the ` randomness - view ' and indeed find consistent agreement .",
    "we elucidate just how little information you need about the language in order to _ predict _ the characteristic features of the data for the word - frequency .",
    "this has a crucial and more far reaching consequence : whenever you need very little information to describe a particular feature , then indeed very little specific information about the system can be extracted from this characteristic feature . in section [ sec:4 ] ,",
    "we discuss and show that for a distribution @xmath14 the shape is indeed length - invariant under the randomness ( more precisely under the random book transformation assumption @xcite ) . in ref .",
    "@xcite it was observed that the limit of a very large text by an author seems to approach the limit @xmath14 .",
    "this suggests that this approximate scaling should work better the longer the text is .",
    "some concluding remarks are added in section [ sec:5 ] , in particular on the applicability of the ` randomness view ' to a much broader spectrum of complex systems .",
    "the first issue is the factual situation . does or does",
    "nt the word - frequency distribution , @xmath10 change shape when shortening the text - lengths @xmath12 ? note that , in the present context , two curves have the same shape provided their log - log - plots can be slid on top of each other , so that one is entirely on top of the other .",
    "figure [ fig:1](a ) gives a first illustration : the number of distinct words in a text , @xmath15 , increases with the total number of words in the text @xmath12 .",
    "@xmath15 as a function of @xmath16 is determined for two novels , _ moby dick _ by h. melville and _ harry potter 1 - 7 _ by j.k .",
    "rowling , by taking averages over fixed text - length @xmath12 .",
    "the data is plotted as @xmath15 against @xmath16 in a log - log scale . in the limit",
    "@xmath17 is @xmath18 ( the first word is always a distinct word ) , so that the curves in fig . [",
    "fig:1](a ) overlap at this point ( which is the right - most point in fig .",
    "[ fig:1](a ) ) . however as @xmath12 increases the two curves start to systematically deviate .",
    "this demonstrates that these two curves do have different shapes .",
    "what conclusions can be drawn from this fact ?",
    "the first conclusion is that the function @xmath19 is _ not _ a universal function of a language . other possibilities are a ) a property of a given language at a particular time - period ( a language evolves slowly with time@xcite ) , b ) a unique property of an author writing a text or c ) just a property of a particular text . as argued in ref .",
    "@xcite , where the complete production of three different authors are compared , it is to good approximation a property of the author . in ref . @xcite it was argued that this is related to the concept of a meta book for an author , reminiscent of an author - fingerprint .",
    "next we investigate the number of distinct words , @xmath11 , which occur more or equal to @xmath4 times in a text of length @xmath12 .",
    "first one may note that since any distinct word must occur at least one time it follows that @xmath20 .",
    "this means that if we instead change the number of occurrences @xmath4 to the relative number of occurrences @xmath21 , then @xmath11 becomes @xmath22 and @xmath23 .",
    "this latter form should , according the scaling form @xmath13 be scale - invariant . to check this",
    "we again start with moby dick .",
    "first one notes that @xmath24 , because the most common word ` the ' is a single word .",
    "this means that if one plots @xmath22 , as a function of @xmath4 , in the same plot as @xmath15 , as a function of @xmath25 , then these two curves will coalesce at the left end points by definition .",
    "next one takes the occurrence - randomness of the words in a text into account , which means that the ratio between the number of ` the ' and the total number of words @xmath26 is to good approximation constant ( @xmath27 for moby dick ( see inset in fig . [",
    "fig:2](a ) and also fig . 3 in ref .",
    "this means that , on the average , approximately every @xmath28 word in the text is a ` the ' . or , in other words , a text - part of length @xmath29 on the average contains one ` the ' , which means that @xmath30 for moby dick .",
    "since @xmath31 is not equal to @xmath32 , this shows that @xmath22 is not a scaling function in the complete range of the variable @xmath21 .",
    "this inequality is illustrated in fig .",
    "[ fig:1](b ) for moby dick and in fig .",
    "[ fig:1](c ) for potter 1 - 7 .",
    "however , as explained above , the randomness view implies that it is a valid scaling for the most frequent word so that @xmath31 irrespective of @xmath12 .",
    "the randomness view implies that this is the only point where the scaling is expected to be strictly valid @xcite .    in fig .",
    "[ fig:2 ] we investigate this discrepancy in more detail : the novels are divided into text - lengths of given sizes @xmath12 and the @xmath22 is obtained as the average over such fixed text - lengths .",
    "a first observation is that @xmath23 , which means that the left - most point for each text - length falls on the respective @xmath19 curve shown in fig .",
    "[ fig:1 ] .",
    "[ fig:2](a ) shows the result for moby dick .",
    "the complete moby dick contains @xmath33 words and corresponds to the lowest curve in fig . [",
    "fig:2](a ) .",
    "the texts parts correspond to @xmath34 , @xmath35 , @xmath36 , @xmath37 , and @xmath38 , respectively .",
    "one notes that all these text parts to good approximation starts from the same right - most point @xmath39 .",
    "the reason for this is the following : the most frequent word in an english text is the word _ the_. to good approximation the density of _ the _ s is independent of where in the novel you are . in moby",
    "dick the density of _ the _ is @xmath40 and is to good approximation constant and independent of the text lengths ( compare inset in fig . [",
    "fig:2](a ) ) @xcite .     by comparing different text - lengths : ( a ) data for parts of moby dick , where @xmath41 denotes the average over an n@xmath42-part of the novel",
    "all these n@xmath42-part curves to good approximation starts from the same right - most point because @xmath39 is to good approximation independent of the text length , as shown by the inset .",
    "however all curves by definition ends on the corresponding @xmath19-curve ( same curve as in fig . [ fig:1](b ) ) .",
    "clearly all these n@xmath42-part curves have different shapes and are not connected by a scaling relation ; ( b ) same as fig . [",
    "fig:2](a ) for potter 1 - 7 .",
    "( c ) power law approximation for the n@xmath42-parts for moby dick : @xmath3 systematically increases with shortening the text - length . ;",
    "( d ) shows this systematic increase of @xmath3 for moby dick and potter .",
    "note that @xmath3 diverges , as the limit text - length corresponding to one @xmath43 on the average is approached ( compare figs .",
    "[ fig:1 ] ( a ) and ( b ) ) .",
    "the length - independent @xmath3 predicted by invoking a scaling relation corresponds to the horizontal lines in fig . [ fig:2](d).,scaledwidth=50.0% ]    _ if _ the scaling @xmath13 was entirely correct , _ then _ all the data in fig .",
    "[ fig:2](a ) should fall on the full @xmath19 ( highest curve in fig .",
    "[ fig:1](a ) ) .",
    "this is clearly not the case . next you can ask if the data for the parts of moby dick can be slid so that the overlap with the data for the full moby dick",
    "however this is not possible because the curves describing the data do in fact have different shapes .",
    "thus the scaling @xmath13 is very approximate and limited to values close to @xmath44 ( see fig . [",
    "fig : a3](a ) in appendix [ apdx ] for a estimate of relative errors ) .",
    "figure [ fig:2](b ) contains the same analysis as fig . [ fig:2](a ) but for the harry potter novels 1 - 7 .",
    "combined into one text these novels contain @xmath45 words and is hence about five times larger than moby dick .",
    "however , the conclusions are just the same as for moby dick . yet",
    ", one notes that the full harry potter and a twentieth - part of harry potter , for larger values of @xmath21 , overlap to good approximation in fig . [",
    "fig:2](b ) .",
    "we will come back to the issue of what might be implied by this particular overlap .    as pointed out in ref .",
    "@xcite , the implication from the scaling function is that , if the full text can be approximated by a power law , then all the text - parts should be well approximated with the same power - law . in other words",
    "the implication is that the power - law index @xmath3 does not change with text - size , in direct contradiction to the prediction from the ` randomness - view ' @xcite .",
    "figure [ fig:2](c ) shows that the full moby dick can indeed be approximated by a straight - line ( over a range , see appendix [ apdx ] ) .",
    "the slope of this line is @xmath46 .",
    "however , also the text - parts can to good approximation be approximated by such lines , but these lines become steeper the smaller the text part .",
    "thus the power law index @xmath3 does systematically increase with smaller text size .",
    "figure [ fig:2](d ) shows this systematic increase in the power - law index with decreasing text length .",
    "note that as the text - length limit @xmath29 is approached @xmath3 diverges ( see figs .",
    "[ fig:1](b ) and ( c ) ) .",
    "the prediction , based on invoking the scaling assumption@xcite , is that @xmath3 is constant ( the horizontal line in fig . [ fig:2](d ) ) which has no support by the data .    in this section",
    "we have demonstrated that the scaling law @xmath13 is not borne out by data .",
    "this conclusion was reach by carefully analyzing the consequences of such a scaling law , rather than just trying to fit it to the data over a limited range . before discussing the possible implications , we will in the next section widen the perspective and describe what the ` randomness - view ' predicts and implies .",
    "the ` randomness view ' of word - frequency distributions is based on two assumptions .",
    "the first is that a text written by an author is homogeneous ( i.e. the chance of randomly picking a word of occurance @xmath4 is equal over the text ) and the second randomness assumption enters through the maximum entropy principle . more precisely the first means that if you enumerate the word positions , @xmath12 , in the text by @xmath47 then the probability to find a word which occurs @xmath4 times in the text is to good approximation independent of the position @xmath48 within the text .",
    "for example you do not find more rare words at the end of the text than in the beginning and the most common word ` the ' is to good approximation evenly spread through the text .",
    "this assumption can be expressed by a mathematical transformation , the random book transformation(rbt ) @xcite , which will be described and discussed below in the present section .",
    "however , the basic consequence of the homogeneous assumption is that if you take an n@xmath42-part of the text the _ word - frequency distribution _ is to good approximation the same as if you just randomly deleted words from the text until only an n@xmath42-part remain .",
    "this means that the homogeneous assumption immediately leads to a _ prediction _ for the word - frequency of the n@xmath42-part of the text .",
    "this prediction for moby dick is given in fig .",
    "[ fig:3](a ) .",
    "the point is that the homogeneous assumption to good approximation _ predicts",
    "_ how the word - frequency changes when you take a part of the text .",
    "it also predicts that the power - law index @xmath3 increases with decreasing text size . the conclusion inferred from fig .",
    "[ fig:3](a ) , is that the randomness implied by the homogeneous assumption gives both a qualitative and a quantitative agreement with the data ( fig .",
    "[ fig : a3 ] in appendix [ apdx ] gives the relative error for the predictions and compared to the same error obtained from the scaling assumption ) .",
    "-parts of moby dick ( full curves ) and the randomness prediction following from the homogeneous assumption ( dashed curves ) .",
    "the near perfect agreement suggests that world - frequency distributions for the @xmath49-parts follow from simple statistics .",
    "( b ) goes one step further . here",
    "the starting knowledge is just @xmath12 , @xmath15 , and @xmath44 for the full text of moby dick .",
    "the @xmath49-parts are predicted by first using randomness in the form of the maximum entropy principle and rgf to obtain @xmath50 for the full text and then subsequently using randomness in the form of rbt - transformation ( eqs ( [ eq_rbt1],[eq_rbt2],[eq_rbt3 ] ) to get the parts .",
    "the @xmath49-parts predictions are again given by the dashed curve and the agreement is nearly as good as in ( a ) ( see appendix [ apdx ] for a plot of relative errors ) .",
    "this suggests that the shapes of word - frequency distributions contain basically no explicit linguistic information .",
    "( c ) and ( d ) give the corresponding results for the harry potter data.,scaledwidth=50.0% ]    the second randomness assumption enters through the maximum entropy principle .",
    "this is the same principle which in physics gives rise to the ideal gas law by assuming that the collisions between the gas - molecules in a container are random or the gauss - distribution by assuming that the deviations around some average are random . in the present context",
    "it can be formulated as the random group formation ( rgf ) @xcite .",
    "rgf predicts the word - frequency distribution from the sole knowledge of the total number of words @xmath12 , the number of distinct words @xmath15 and the frequency of the most common word @xmath44 @xcite .",
    "the point is that the rgf - prediction is a general prediction where the randomness is incorporated into the maximum entropy principle : it predicts the probability @xmath50 that an object belongs to a group containing @xmath4 objects provided that you know that the total number of objects is @xmath12 , the total number of groups is @xmath15 and that the number of objects in the largest group is @xmath44 @xcite .",
    "thus the rgf - prediction involves no linguistic information other than the identification between objects and words and between groups and distinct words .",
    "what is reflected in the word - frequency distribution is some general property , which texts share with many other completely unrelated phenomena @xcite .",
    "thus rgf predicts the probability @xmath51 for the full text without any explicit linguistic information and the homogeneity assumption transforms this expectation into the text - parts of length @xmath52 using the random book transformation ( rbt ) inherit in the text homogeneity assumption @xcite @xmath53 where @xmath54 and @xmath55 are column matrices corresponding to @xmath56 and @xmath57 .",
    "the transformation matrix @xmath58 is given by @xmath59 where @xmath60 is binomial coefficient .",
    "@xmath61 is given by the normalization condition @xmath62 thus , by combining the maximum entropy randomness with the homogeneity randomness , the word - frequency for parts of moby dick can be entirely predicted from the sole knowledge of @xmath12 , @xmath15 and @xmath44 for the full text .",
    "these predictions are given by the dashed curves in fig .",
    "[ fig:3](b ) .",
    "the agreement with the data ( full drawn curves in fig .",
    "[ fig:3](c ) ) is striking ( see fig .",
    "[ fig : a3](b ) in appendix [ apdx ] for the relative errors ) .",
    "the fact that you to good accuracy can _ predict _ the features of the word - frequency from two very general assumptions of randomness and without any specific linguistic information conversely suggests that you can extract basically no linguistic information from the shape of the word - frequency distribution .",
    "the rbt - transformation given in eqs .",
    "( [ eq_rbt1],[eq_rbt2],[eq_rbt3 ] ) predicts how the probability distribution @xmath51 for the full text changes into @xmath63 for the n@xmath42 part when assuming text - homogeneity . at the same time",
    "the rgf - function @xmath64 with @xmath3 typically in the range @xmath65 $ ] gives both a qualitatively and quantitative account of word - frequency distributions in real texts @xcite .",
    "this means that one can use the functional form @xmath64 in order to understand how the general shape of the frequency distribution influences what happens when one takes an n@xmath42-part of the text .",
    "this leads to two exact mathematical results which helps clarifying the situation .",
    "the first result is that @xmath66 under the rbt transforms as @xmath67 this means that the limit case @xmath68 and @xmath69 is invariant under the transformation .",
    "however , @xmath70 is not normalizable , so @xmath71 has to be larger than zero . if @xmath71 is small enough then eq . (",
    "[ eq : gamma1 ] ) reduces to @xmath72 .",
    "the average @xmath73 then becomes @xmath74 , so that @xmath75 for small @xmath76 .",
    "it follows that @xmath77 .",
    "consequently , in this special case and provided @xmath41 is small enough , @xmath78 obeys the scaling proposed by font - clos et al in ref .",
    "[ fig:4](a ) shows the result for eq .",
    "( [ eq : gamma1 ] ) for the same @xmath12 and @xmath71 as for moby dick .",
    "as seen from the figure , in this special case of @xmath0 , the approximate scaling is according to the  randomness - view \" predicted to hold to good approximation down to a tenth of the original text - length .",
    "however , for larger @xmath41 the situation shown in figs .",
    "[ fig:4](b ) and ( c ) is recovered , as it must from general considerations .    .",
    "( a ) is the exact results for the special frequency probability function @xmath79 given by eq .",
    "( [ eq : gamma1 ] ) . here",
    "@xmath12 and @xmath71 are the same as for moby dick . in this case",
    "the @xmath80-curves are almost collapsed for @xmath81 with @xmath82 .",
    "however for @xmath83 the deviations become significant .",
    "( b ) shows the case for @xmath84 again with the same @xmath12 and @xmath71 as for moby dick . in this case",
    "a book consistent with the distribution is created and partitioned .",
    "one notes that the deviations are significant already for @xmath85.,scaledwidth=50.0% ]    the second analytical solution to eqs .",
    "( [ eq_rbt1],[eq_rbt2],[eq_rbt3 ] ) is @xmath86 which transforms into @xmath87 one notes that this form diverges at @xmath88 .",
    "the point is that if you start with @xmath64 and @xmath89 then you approach this divergent form with increasing @xmath41 .",
    "this tendency is illustrated in fig .",
    "[ fig:4](b ) for the case @xmath84 with the same @xmath12 and @xmath71 as for moby dick . in this case",
    "the deviation from the approximate scaling form is significant already for @xmath90 .",
    "the point is that since a real text corresponds to @xmath91 , it follows that it will always change shape whenever @xmath41 becomes large enough .",
    "it also follows that the discrepancy between a scaling curve and the data for a given @xmath41 will depend on the starting shape .",
    "the larger @xmath3 the starting shape has , the larger discrepancy for a given @xmath41 .",
    "this explains why the discrepancy in case of potter 1 - 7 for @xmath92 is smaller than for moby dick ( compare figs .",
    "[ fig:2](a ) and ( b ) ) .",
    "another aspect , which is to some extent reflected in the difference of the 20@xmath42-parts for moby dick and potter 1 - 7 , is related to the meta - book concept discussed in ref .",
    "the meta - book of an author is all novels written by an author added together to a single text .",
    "the larger part of this text you analyze , the smaller the power law index @xmath3 @xcite .",
    "it was suggested that , in the limit of an infinite text , @xmath3 approaches 1 and the distribution @xmath93 approaches the limit form @xmath70 @xcite .",
    "this means that the longer the text , the smaller the @xmath3 and consequently also the smaller the difference in functional form , when taking an n@xmath42-part .",
    "thus the fact that potter 1 - 7 is about five times longer than moby dick suggests that the discrepancy between the 20@xmath42-part and the full text should be larger for moby dick than for potter 1 - 7 , in accordance with figs .",
    "[ fig:2](a ) and ( b ) .",
    "in conclusion we find that an approximate scaling form in a special case indeed emerges from the randomness assumption that words with a given frequency are equally likely to appear anywhere in a text .",
    "this paper discusses both general and specific issues in connection with analyzing word - frequency distributions .",
    "the first general issue concerns the two seemingly not compatible views on word - frequencies i.e. the ` zipf s view ' attributing specific properties to the distribution and the ` randomness view ' which emphasizes the non - specific character of the distribution .",
    "we have here shown that the ` randomness view ' _ predicts _ word - frequency distributions both qualitatively and quantitatively .",
    "the conclusion drawn from this is that the shapes of word - frequency - distributions are general features , which word - distribution share with a multitude of totally unrelated phenomena and that in fact the linguistic content , which can be drawn from these distribution , is basically nil .",
    "this is contrary to the ` zipf s view ' , which often presumes that the shapes of word - frequency distributions carry specific linguistic information . as a concrete example we discussed a generalized scaling which",
    "could well have reflected such a specific feature @xcite .",
    "however carefully analysis showed that this particular scaling is not borne out by the data and does not suggest any specific feature beyond the randomness .",
    "the second general issue is the conceptual difference between _ predicting _ and _ fitting _ : the ` randomness view ' _ predicts _ word - frequency distributions from general assumptions , whereas _",
    "fitting _ to particular curve - forms is a common procedure within this field and a successful fit to the data over a limited range is often furthermore taken as evidence of the correctness of the assumptions underlying the curve - form . in the concrete example discussed here , it was proposed that the suggested scaling function could be _ approximately _ parameterized by a single curve - form with two free parameters @xcite .",
    "however , as shown , such a fitting does not imply anything beyond what is contained in the randomness - view .",
    "more generally from the point of the present work , an analysis based on fitting word - frequency data to power - laws has but little information value , in spite of its long history stemming from zipf s early work in 1932 @xcite .",
    "the questions of how a power - law fitted to data for @xmath94 ( usually called heaps law ) relates to a power - law fitted to word - frequency distribution @xmath50 ( usually called zipf s law ) is discussed in ref .",
    "@xcite . from the randomness - perspective such power - law fittings carries no additional information : @xmath94 and @xmath50 carry the same random - information and the @xmath94 can be directly obtained from @xmath50 @xcite .",
    "the relation between heaps law and zipf s law in the present context is further illustrated in appendix [ apdx ] .",
    "another general point is that that @xmath50 is never quite a power - law as can be seen from fig .",
    "[ fig:1](b ) and ( c ) and fig .",
    "[ fig : a1 ] in appendix [ apdx ] .",
    "the @xmath50 in a log - log plot bends for higher values of @xmath4 in accordance with rgf - form from the maximum entropy principle .",
    "sometimes this bend is , in accordance with the scale - freeness ideas , associated with yet a second power - law as in the case of language corpora in ref .",
    "@xcite . in appendix [ apdx ]",
    "we show that such corpora are also well described by the randomness - view and the rgf - form ( compare fig .",
    "[ fig : a2 ] ) . thus fitting the data by two power - laws",
    "does by itself not imply any specific language feature .",
    "word - frequencies are not the only example for which outcomes for some complex deterministic systems reduce to random distributions .",
    "other examples are e.g. distribution of species into taxa in biology @xcite , chemical reaction networks @xcite , family names @xcite , sizes of population centers @xcite and travel distance distributions @xcite .",
    "thus the question of random versus specific is relevant in a much broader context than just word - frequencies .",
    "estroup , les gammes stnographiques , fourth ed .",
    ", institut stenographique de france , paris , 1916 .",
    "zipf , selective studies of the principle of relative frequency in language , harvard university press , cambridge , 1932 .",
    "zipf , the psycho - biology of language : an introduction to dynamic philology , mifflin , boston , 1935 .",
    "zipf , human bevavior and the principle of least effort , addison - wesley , reading , 1949 .",
    "b. mandelbrot , an informational theory of the statistical structure of languages , butterworth , woburn , 1953 .",
    "w. li , random texts exhibit zipfs - law - like word frequency distribution , ieee t. inform .",
    "theory 38 ( 1992 ) 1842 - 1845 .",
    "baayen , word frequency distributions , kluwer academic , dordrecht , 2001 .",
    "i cancho , r.v .",
    "sol , least effort and the origins of scaling in human language , proc .",
    "natl . acad .",
    "100 ( 2003 ) 788 - 791 .",
    "montemurro , beyond the zipf - mandelbrot law in quantitative linguistics , physica a 300 ( 2001 ) 567 - 578 .",
    "f , font - close , g. boleda , a. corral , a scaling law beyond zipf s law and its relation to heaps law , new j. phys 15 ( 2013 ) 093033 .",
    "h. simon , on a class of skew distribution functions , biometrika 42 ( 1955 ) 425 - 440 .",
    "i. kanter , d.a .",
    "kessler , markov processes : linguistics and zipf s law , phys .",
    "74 ( 1995 ) 4559 - 4562 .",
    "dorogovtsev , j.f.f .",
    "mendes , languague as an evolving word web , proc .",
    "b 268 ( 2001 ) 2603 - 2606 .",
    "a. masucci , g. rodgers , networks properties of written human language , phys .",
    "e 74 ( 2006 ) 26102 . c. cattuto , v. loreto , v.d.p .",
    "servedio , a yule - simon process with memory , europhys .",
    "76 ( 2006 ) 208 - 214 .",
    "l. l , z .- k .",
    "zhang , t. zhou , deviation of zipf s and heaps laws in human languages with limited dictionary sizes , sci .",
    "3 ( 2013 ) 1082 .",
    "miller , some effects of intermittance silence , am . j. psychol",
    "70 ( 1957 ) 311 - 314 .",
    "s. bernhardsson , s.k .",
    "baek , p. minnhagen , a paradoxical property of the monkey book , j. stat .",
    "( 2011 ) po7013 .",
    "s. bernhardsson , l.e.c .",
    "da rocha , p. minnhagen , size dependent word frequencies and the translational invariance of books , physica a 389 ( 2010 ) 330 - 341 .",
    "s. bernhardsson , l.e.c .",
    "da rocha , p. minnhagen , the meta book and size - dependent properties of written language , new j. phys",
    ". 11 ( 2009 ) 123015 .",
    "baek , s. bernhardsson , p. minnhagen , zipf s law unzipped , new j. phys . 13",
    "( 2011 ) 043004 .",
    "x. yan , p. minnhagen , maximum entropy , word - frequency , chinese characters , and multiple meanings , plos one 10 ( 2015 ) e0125592 .",
    "zipf s law is often expressed in terms of the probability function @xmath95 where @xmath96 is the total number of specific words .",
    "a log - log plot of @xmath50 is given fig .",
    "[ fig : a1](a ) .",
    "the data is the same potter text as in fig . [",
    "fig:1](a ) and is represented by dots in binned form in the figure . zipf s law means , in its original form , that this data should fall on a straight line with slope -2 . as seen in the figure the left part of the data can we well approximated with straight line of slope -1.6 ( full line in fig .",
    "[ fig : a1](a ) ) .",
    "this is often taken as evidence that a language can be associated with the specific property of scale - freeness , although with a different exponent than the -2 stipulated by zipf s law .",
    "however , since the data actually follows a bent curve the right part of the data fits better to a straight line with a steeper slope ( dashed line in the fig . [",
    "fig : a1](a ) ) .",
    "this means that the data can be rather well _ fitted _ by two straight - lines of different slopes .",
    "the fact that you need two straight - lines to fit the data was discussed in ref .",
    "@xcite in case of large language corpora .",
    "two examples from the google 1-grams data @xcite are shown in fig .",
    "[ fig : a2 ] . as seen both sets",
    "can be well represented by _ fitting _ to two straight lines in accordance with ref .",
    "the full drawn curve in figs .",
    "[ fig : a2 ] ( a ) and ( b ) are _ predictions _ from the randomness view .",
    "the randomness prediction maintains that if you know @xmath12 , @xmath15,and @xmath44 you can regardless of language predict @xmath50 very well . furthermore the randomness predictions can also be well _ fitted _ to two straight - lines .",
    "thus there is little evidence that you can extract any additional information by @xmath97 to two straight - lines than what is already contained in the randomness view .",
    "[ fig : a1 ] ( c ) gives the corresponding randomness prediction of the potter text .",
    "versus @xmath4 in a log - log plot with two straight lines . however , the data is already well _ predicted _ from the sole knowledge of @xmath98 , @xmath15 and @xmath99 using the randomness property.,scaledwidth=50.0% ]    ( a ) , [ fig:3](a ) and [ fig:3](b ) .",
    "the relative error is given by @xmath100 where @xmath4(@xmath101 ) is the measured ( predicted ) value , respectively .",
    "( a ) is the scaling prediction that the data for the n@xmath42-part should fall on on the full text data n@xmath102 .",
    "the error becomes large for towards smaller @xmath4 values ( upto 200 - 300% ) and progressively worse with decreasing text - length .",
    "( b ) a blow up showing where the error becomes larger than 50% for the various text - lengths .",
    "( c ) shows the randomness prediction which has much smaller relative errors in the range of 10% over the complete range .",
    "( d ) is the prediction based on the sole knowledge of @xmath12 , @xmath15 , and @xmath44 for the complete text .",
    "the relative errors are only slightly larger than for ( c ) .",
    "it is the small relative errors in ( c ) and ( d ) which are reflected in the near overlaps between prediction and data in fig .",
    "[ fig:3].,scaledwidth=50.0% ]    figure [ fig : a1 ] ( b ) shows @xmath94 for the potter text i.e. the number of specific words as a function of the length of the text .",
    "also this curve can to some extent be _ fitted _ to a power law ( straight full line in fig .",
    "[ fig : a1 ] ( b ) ) .",
    "this is in approximate agreement with heaps law which states that @xmath94 has a power - law character .",
    "however , since the first words are specific the slope of the power - law is @xmath103 in the left - most part and it then decreases because the curve is apparently bent . from the random - view perspective , @xmath50 and @xmath94",
    "are simply related and contain the same information @xcite .",
    "in particular the randomness view predicts that if @xmath50 is a power - law then the corresponding heaps law can only be approximate @xcite .",
    "nevertheless one can derive approximate relations between the approximate power - laws @xcite . in the simplest approximation",
    "it is just @xmath104 provided @xmath105 and @xmath106 .",
    "this relation holds pretty well for the full drawn lines in fig .",
    "[ fig : a1 ] ( a ) and ( b ) . in fig .",
    "[ fig:2 ] ( c ) and ( d ) we have in , correspondence with fig .",
    "[ fig : a1 ] ( a ) , used the left part of the data to extract an approximate power - law .",
    "figure [ fig : a3 ] supplies a measure of the precision in figs .",
    "[ fig:2](a ) , [ fig:3](a ) , and [ fig:3](b ) .",
    "[ fig:2 ] ( a ) compares @xmath41-parts of moby dick with the full text @xmath107 .",
    "the relative error made is given in figs .",
    "[ fig : a3](a ) and ( b ) : fig .",
    "[ fig : a3](a ) shows that the relative error becomes very large ( upto 300% ) for the smaller values . fig .",
    "[ fig : a3](b ) shows the error for larger values in a blown up scale .",
    "this should then be compared to the relative error between the data and the randomness prediction shown in fig .",
    "[ fig : a3](c ) which corresponds to fig .",
    "[ fig:3](a ) . by comparing figs .",
    "[ fig : a3](a ) , ( b ) and ( c ) , one finds that the randomness prediction is a better prediction than the scaling prediction .",
    "[ fig:3](c ) corresponds to prediction you get from the sole knowledge of @xmath12 , @xmath15 and @xmath44 for the full moby dick .",
    "thus without any knowledge of the author or what language the text is written in . from this point of view",
    "the smallness in the relative error is notable ."
  ],
  "abstract_text": [
    "<S> the text - length - dependence of real word - frequency distributions can be connected to the general properties of a random book . </S>",
    "<S> it is pointed out that this finding has strong implications , when deciding between two conceptually different views on word - frequency distributions , _ i.e. _ the specific ` zipfs - view ' and the non - specific ` randomness - view ' , as is discussed . </S>",
    "<S> it is also noticed that the text - length transformation of a random book does have an exact scaling property precisely for the power - law index @xmath0 , as opposed to the zipf s exponent @xmath1 and the implication of this exact scaling property is discussed . </S>",
    "<S> however a real text has @xmath2 and as a consequence @xmath3 increases when shortening a real text . </S>",
    "<S> the connections to the predictions from the rgf(random group formation ) and to the infinite length - limit of a meta - book are also discussed . </S>",
    "<S> the difference between ` curve - fitting ' and ` predicting ' word - frequency distributions is stressed . </S>",
    "<S> it is pointed out that the question of randomness versus specifics for the distribution of outcomes in case of sufficiently complex systems has a much wider relevance than just the word - frequency example analyzed in the present work . </S>"
  ]
}