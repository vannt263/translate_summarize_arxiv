{
  "article_text": [
    "often semiparametric estimators are asymptotically equivalent to a sample average .",
    "the object being averaged is referred to as the influence function .",
    "the influence function is useful for a number of purposes .",
    "its variance is the asymptotic variance of the estimator and so it can be used for asymptotic efficiency comparisons .",
    "also , the form of remainder terms follow from the form of the influence function so knowing the influence function should be a good starting point for finding regularity conditions . in addition , estimators of the influence function can be used to reduce bias of a semiparametric estimator .",
    "furthermore , the influence function approximately gives the influence of a single observation on the estimator .",
    "indeed this interpretation is where the influence function gets its name in the robust estimation literature , see hampel ( 1968 , 1974 ) .",
    "we show how the influence function of a semiparametric estimator can be calculated from the functional given by the limit of the semiparametric estimator .",
    "we show that the influence function is the limit of the gateaux derivative of the functional with respect to a smooth deviation from the true distribution , as the deviation approaches a point mass .",
    "this calculation is similar to that of hampel ( 1968 , 1974 ) , except that the deviation from the true distribution is restricted to be smooth .",
    "smoothness of the deviation is necessary when the domain of the functional is restricted to smooth functions .",
    "as the deviation approaches a point mass the derivative with respect to it approaches the influence function .",
    "this calculation applies to many semiparametric estimators that are not defined for point mass deviations , such as those that depend on nonparametric estimators of densities and conditional expectations .",
    "we also consider regularity conditions for validity of the influence function calculation .",
    "the conditions involve frechet differentiability as well as convergence rates for nonparametric estimators .",
    "they also involve stochastic equicontinuity and small bias conditions .",
    "when estimators depend on nonparametric objects like conditional expectations and pdf s , the frechet differentiability condition is generally satisfied for intuitive norms , e.g. as is well known from goldstein and messer ( 1992 ) .",
    "the situation is different for functionals of the empirical distribution where frechet differentiability is only known to hold under special norms , dudley ( 1994 ) .",
    "the asymptotic theory here also differs from functionals of the empirical distribution in other ways as will be discussed below .",
    "newey ( 1994 ) previously showed that the influence function of a semiparametric estimator can be obtained by solving a pathwise derivative equation .",
    "that approach has proven useful in many settings but does require solving a functional equation in some way .",
    "the approach of this paper corresponds to specifying a path so that the influence can be calculated directly from the derivative .",
    "this approach eliminates the necessity of finding a solution to a functional equation .",
    "regularity conditions for functionals of nonparametric estimators involving frechet differentiability have previously been formulated by ait - sahalia ( 1991 ) , goldstein and messer ( 1992 ) , newey and mcfadden ( 1994 ) , newey ( 1994 ) , chen and shen ( 1998 ) , chen , linton , and keilegom ( 2003 ) , and ichimura and lee ( 2010 ) , among others .",
    "newey ( 1994 ) gave stochastic equicontinuity and small bias conditions for functionals of series estimators .",
    "in this paper we update those using belloni , chernozhukov , chetverikov , and kato ( 2015 ) .",
    "bickel and ritov ( 2003 ) formulated similar conditions for kernel estimators .",
    "andrews ( 2004 ) gave stochastic equicontinuity conditions for the more general setting of gmm estimators that depend on nonparametric estimators .    in section 2",
    "we describe the estimators we consider .",
    "section 3 presents the method for calculating the influence function . in section 4",
    "we outline some conditions for validity of the influence function calculation .",
    "section 5 gives primitive conditions for linear functionals of kernel density and series regression estimators .",
    "section 6 outlines additional conditions for semiparametric gmm estimators .",
    "section 7 concludes .",
    "the subject of this paper is estimators of parameters that depend on unknown functions such as probability densities or conditional expectations .",
    "we consider estimators of these parameters based on nonparametric estimates of the unknown functions .",
    "we refer to these estimators as semiparametric , with the understanding that they depend on nonparametric estimators",
    ". we could also refer to them as `` plug in estimators '' or more precisely as `` plug in estimators that have an influence function . ''",
    "this terminology seems awkward though , so we simply refer to them as semiparametric estimators .",
    "we denote such an estimator by @xmath0 , which is a function of the data @xmath1 where @xmath2 is the number of observations . throughout the paper",
    "we will assume that the data observations @xmath3 are i.i.d .",
    "we denote the object that @xmath0 estimates as @xmath4 , the subscript referring to the parameter value under the distribution that generated the data .",
    "some examples can help fix ideas .",
    "one example with a long history is the integrated squared density where @xmath5 @xmath3 has pdf @xmath6 and @xmath7 is @xmath8-dimensional .",
    "this object is useful in certain testing settings . a variety of different estimators of @xmath4 have been suggested .",
    "one estimator is based on a kernel estimator @xmath9 of the density given by @xmath10where @xmath11 is a bandwidth and @xmath12 is a kernel .",
    "an estimator @xmath0 can then be constructed by plugging in @xmath13 in place of @xmath14 in the formula for @xmath4 as @xmath15this estimator of @xmath4 and other estimators have been previously considered by many others .",
    "we use it as an example to help illustrate the results of this paper .",
    "it is known that there are other estimators that are better than @xmath16 one of these is@xmath17where @xmath12 is a symmetric kernel .",
    "gine and nickl ( 2008 ) showed that this estimator converges at optimal rates while it is well known that @xmath18 does not .",
    "our purpose in considering @xmath0 is not to suggest it as the best estimator but instead to use it to illustrate the results of this paper .",
    "another example is based on the bound on average consumer surplus given in hausman and newey ( 2015 ) . here",
    "a data observation is @xmath19 where @xmath20 is quantity of some good , @xmath21 is price , and @xmath22 is income . for @xmath23",
    "the object of interest is @xmath24.\\]]from hausman and newey ( 2015 ) it follows that this object is a bound on the weighted average over income and individuals of average equivalent variation for a price change from @xmath25 to @xmath26 when there is general heterogeneity .",
    "it is an upper ( or lower ) bound for average surplus when @xmath27 is a lower ( or upper ) bound for individual income effects . here",
    "@xmath28 is a known weight function that is used to average across income levels .",
    "one estimator of @xmath4 can be obtained by plugging - in a series nonparametric regression estimator of @xmath29 in the formula for @xmath30 . to describe a series estimator let @xmath31 be a vector of approximating functions such as power series or regression splines .",
    "also let @xmath32^{t}$ ] and @xmath33 be the matrix and vector of observations on the approximating functions and on quantity .",
    "a series estimator of @xmath34 $ ] is given by @xmath35where @xmath36 will be nonsingular with probability approaching one under conditions outlined below .",
    "we can then plug in this estimator to obtain @xmath37we use this estimator as a second example .",
    "this paper is about estimators that have an influence function .",
    "we and others refer to these as asymptotically linear estimators .",
    "an asymptotically linear estimator is one satisfying@xmath38=0,e[\\psi ( z_{i})^{t}\\psi ( z_{i})]<\\infty .",
    "\\label{inf}\\]]the function @xmath39 is referred to as the influence function , following terminology of hampel ( 1968,1974 ) .",
    "it gives the influence of a single observation in the leading term of the expansion in equation ( [ inf ] ) .",
    "it also quantifies the effect of a small change in the distribution on the limit of @xmath0 as we further explain below .    in the integrated squared density example the influence function is well known to be @xmath40.\\]]this formula holds for the estimators mentioned above and for all other asymptotically linear estimators of the integral of the square of an unrestricted pdf . in the consumer surplus example the influence function is@xmath41,\\delta ( x)=f_{0}(x)^{-1}w(x).\\]]as will be shown below .",
    "in this section we provide a method for calculating the influence function . the key object on which the influence function depends",
    "is the limit of the estimator when @xmath3 has cdf @xmath42 we denote this object by @xmath43 .",
    "it describes how the limit of the estimator varies as the distribution of a data observation varies .",
    "formally , it is mapping from a set @xmath44 of cdf s into the real line,@xmath45 in the integrated squared density example @xmath46 where all elements of the domain @xmath44 are restricted to be continuous distributions with pdfs that are square integrable . in the average surplus example @xmath47dx$ ] where the domain is restricted to distributions where @xmath48 $ ] and @xmath43 exist and @xmath49 is continuously distributed with pdf @xmath50 that is positive where @xmath51 is positive .",
    "we use how @xmath43 varies with @xmath52 to calculate the influence function .",
    "let @xmath53 denote a cdf such that @xmath54 is in the domain @xmath44 of @xmath43 for small enough @xmath55 and @xmath53 approaches a point - mass at @xmath7 as @xmath56 .",
    "for example , if @xmath44 is restricted to continuous distributions then we could take @xmath53 to be continuous with pdf @xmath57 for @xmath12 a bounded pdf with bounded support and @xmath58 denoting a possible value of @xmath59 . under regularity conditions given below the influence function",
    "can be calculated as@xmath60 .",
    "\\label{inf func}\\]]the derivative in this expression is the gateaux derivative of the functional @xmath43 with respect to `` contamination '' @xmath53 to the true distribution @xmath61  thus this formula says that the influence function is the limit of the gateaux derivative of @xmath43 as the contamination distribution @xmath53 approaches a point mass at @xmath7 .",
    "for example , consider the integrated squared density where we let the contamination distribution @xmath53 have a pdf  @xmath57 for a bounded kernel @xmath12",
    ". then @xmath62^{2}d\\tilde{z}\\right\\ } |_{t=0 } \\\\ & = & \\int 2[f_{0}(\\tilde{z})-\\beta _ { 0}]g_{z}^{h}(\\tilde{z})d\\tilde{z}.\\end{aligned}\\]]assuming that @xmath63 is continuous at @xmath64 the limit as @xmath56 is given by@xmath65 = 2\\lim_{h\\longrightarrow 0}\\int f_{0}(\\tilde{z})g_{z}^{h}(\\tilde{z})d\\tilde{z}-2\\beta _ { 0}=2[f_{0}(z)-\\beta _ { 0}].\\]]this function is the influence function at @xmath7 of semiparametric estimators of the integrated squared density . thus equation ( [ inf func ] ) holds in the example of an integrated squared density . as we show below , equation ( [ inf func ] ) , including the gateaux differentiability , holds for any asymptotically linear estimator satisfying certain mild regularity conditions .",
    "equation ( [ inf func ] ) can be thought of as a generalization of the influence function calculation of hampel ( 1968 , 1974 ) .",
    "that calculation is based on contamination @xmath66 that puts probability one on @xmath67 .",
    "if @xmath68 is the domain @xmath44 of @xmath43 then the influence function is given by the gateaux derivative @xmath69the problem with this calculation is that @xmath70 @xmath71will not be in the domain @xmath44 for many semiparametric estimators .",
    "it is not defined for the integrated squared density , average consumer surplus , nor for any other @xmath43 that is only well defined for continuous distributions .",
    "equation ( [ inf func ] ) circumvents this problem by restricting the contamination to be in @xmath44 . the influence function",
    "is then obtained as the limit of a gateaux derivative as the contamination approaches a point mass , rather than the gateaux derivative with respect to a point mass .",
    "this generalization applies to most semiparametric estimators .",
    "we can relate the influence function calculation here to the pathwise derivative characterization of the influence function given in van der vaart ( 1991 ) and newey ( 1994 ) .",
    "consider @xmath72 as a path with parameter @xmath55 passing through the truth at @xmath73 it turns out that this path is exactly the right one to get the influence function from the pathwise derivative .",
    "suppose that @xmath61 has pdf @xmath14 and @xmath53 has density @xmath74 so that the likelihood corresponding to this path is @xmath75 .",
    "the derivative of the corresponding log - likelihood at zero , i.e. the score , is @xmath76 where we do not worry about finite second moment of the score for the moment . as shown by van der vaart ( 1991 )",
    ", the influence function will solve the equation @xmath77 \\\\ & = & \\int \\psi ( \\tilde{z})\\left [ \\frac{g_{z}^{h}(\\tilde{z})}{f_{0}(\\tilde{z})}-1\\right ] f_{0}(\\tilde{z})d\\tilde{z}=\\int \\psi ( \\tilde{z})g_{z}^{h}(\\tilde{z})d\\tilde{z}.\\end{aligned}\\]]taking the limit as @xmath56 then gives the formula ( [ inf func ] ) for the influence function when the influence function is continuous at @xmath7 . in this way @xmath78",
    "can be thought of as a path where the pathwise derivative converges to the influence function as @xmath79 approaches a point mass at @xmath7 .",
    "we give a theoretical justification for the formula in equation ( [ inf func ] ) by assuming that an estimator is asymptotically linear and then showing that equation ( [ inf func ] ) is satisfied under a few mild regularity conditions .",
    "one of the regularity conditions we use is local regularity of @xmath0 along the path @xmath80 this property is that for any @xmath81 , when @xmath1 are i.i.d . with distribution @xmath82@xmath83\\overset{d}{\\longrightarrow } n(0,v),v = e[\\psi ( z_{i})\\psi ( z_{i})^{t}].\\]]that is , under a sequence of local alternatives , when @xmath0 is centered at @xmath84 then @xmath0 has the same limit in distribution as for @xmath61 .",
    "this is a very mild regularity condition .",
    "many semiparametric estimators could be shown to be uniformly asymptotically normal for @xmath55 in a neighborhood of @xmath85which would imply this condition .",
    "furthermore , it turns out that asymptotic linearity of @xmath0 and gateaux differentiability of @xmath86 at @xmath87 are sufficient for local regularity .",
    "for these reasons we view local regularity as a mild condition for the influence function calculation .    for simplicity",
    "we give a result for cases where @xmath61 is a continuous distribution with pdf @xmath14 and @xmath44 includes paths @xmath88 where @xmath53 has pdf @xmath89 and @xmath12 is a bounded pdf with bounded support .",
    "we also show below how this calculation can be generalized to cases where the deviation need not be a continuous distribution .",
    "theorem 1 : _ suppose that _ @xmath0 _ _  is asymptotically linear with influence function _",
    "_ @xmath90__that is continuous at _ _",
    "@xmath7 _ _  and _ _ @xmath3 _ _  is continuously distributed with pdf _",
    "_ @xmath63 _ _  that is bounded away from zero on a neighborhood of _ _ @xmath7__. if _",
    "_ @xmath0 _",
    "_  is locally regular for the path _",
    "_ @xmath54 _ _  then equation ( [ inf func ] ) is satisfied .",
    "furthermore , if _ _ @xmath91 _",
    "_  is differentiable at _ _",
    "@xmath87 _ _  with derivative _",
    "_ @xmath92__then _ _ @xmath0 _ _  is locally regular . _ _",
    "this result shows that if an estimator is asymptotically linear and certain conditions are satisfied then the influence function satisfies equation ( _ [ inf func ] _ ) , justifying the calculation of the influence function .",
    "furthermore , the process of that calculation will generally show differentiability of @xmath93 and so imply local regularity of the estimator , confirming one of the hypotheses that is used to justify the formula . in this way this result provides a precise link between the influence function of an estimator and the formula in equation ( _ [ inf func ] _ ) .",
    "this result is like van der vaart ( 1991 ) in showing that an asymptotically linear estimator is regular if an only if its limit is pathwise differentiable .",
    "it differs in some of the regularity conditions and in restricting the paths to have the mixture form @xmath54 with kernel density contamination @xmath53 .",
    "such a restriction on the paths actually weakens the local regularity hypothesis because @xmath0 only has to be locally regular for a particular kind of path rather than a general class of paths .",
    "although theorem 1 assumes @xmath7 is continuously distributed the calculation of the influence function will work for combinations of discretely and continuously distributed variables . for such cases the calculation can proceed with a deviation that is a product of a point mass for the discrete variables and a kernel density for the continuous variables . more generally , only the variables that are restricted to be continuously distributed in the domain @xmath44 need be continuously distributed in the deviation .",
    "we can illustrate using the consumer surplus example .",
    "consider a deviation that is a product of a point mass @xmath94 at some @xmath20 and a kernel density @xmath95 centered at @xmath23 .",
    "the corresponding path is @xmath96where @xmath97 is the distribution corresponding to @xmath98 .",
    "let @xmath99 be the marginal pdf for @xmath49 along the path .",
    "multiplying and dividing by @xmath100 and using iterated expectations we find that @xmath101d\\tilde{x}=\\int f_{t}(\\tilde{x})^{-1}w(\\tilde{x})e_{f_{t}}[q|\\tilde{x}]f_{t}(\\tilde{x})dx = e_{f_{t}}[f_{t}(x_{i})^{-1}w(x_{i})q_{i}].\\]]differentiating with respect to @xmath55 gives @xmath102w(\\tilde{x})e[q|\\tilde{x}]f_{0}(\\tilde{x})d\\tilde{x } \\\\ &",
    "= & q\\int \\delta ( \\tilde{x})g^{h}(\\tilde{x})d\\tilde{x}-\\int \\delta ( \\tilde{x})e[q|\\tilde{x}]g^{h}(\\tilde{x})d\\tilde{x}.\\end{aligned}\\]]therefore , assuming that @xmath103 is continuous at @xmath49 we have@xmath104).\\]]this result could also be derived using the results for conditional expectation estimators in newey ( 1994 ) .",
    "the fact that local regularity is necessary and sufficient for equation ( _ [ inf func ] _ ) highlights the strength of the asymptotic linearity condition .",
    "calculating the influence function is a good starting point for showing asymptotic linearity but primitive conditions for asymptotic linearity can be complicated and strong .",
    "for example , it is known that asymptotic linearity can require some degree of smoothness in underlying nonparametric functions , see bickel and ritov ( 1988 ) .",
    "we next discuss regularity conditions for asymptotic linearity .",
    "one of the important uses of the influence function is to help specify regularity conditions for asymptotic linearity .",
    "the idea is that once @xmath105 has been calculated we know what the remainder term for asymptotic linearity must be . the remainder term",
    "can then be analyzed in order to formulate conditions for it to be small and hence the estimator be asymptotically linear . in this section",
    "we give one way to specify conditions for the remainder term to be small .",
    "it is true that this formulation may not lead to the weakest possible conditions for asymptotic linearity of a particular estimator .",
    "it is only meant to provide a useful way to formulate conditions for asymptotic linearity .    in this section",
    "we consider estimators that are functionals of a nonparametric estimator taking the form @xmath106where @xmath107 is some nonparametric estimator of the distribution of @xmath108 .",
    "both the integrated squared density and the average consumer surplus estimators have this form , as discussed below .",
    "we consider a more general class of estimators in section 6 .    since @xmath109 adding and subtracting the term @xmath110",
    "gives@xmath111if @xmath112 and @xmath113 both converge in probability to zero then @xmath0 will be asymptotically linear . to the best of our knowledge little is gained in terms of clarity or relaxing conditions by considering @xmath114 rather than @xmath115 and @xmath116 separately , so we focus on the individual remainders .",
    "the form of the remainders @xmath115 and @xmath116 are motivated by @xmath39 being a derivative of @xmath43 with respect to @xmath52 .",
    "the derivative interpretation of @xmath39 suggests a linear approximation of the form @xmath117where the equality follows by @xmath118=0.$ ] plugging in @xmath107 in this approximation gives @xmath119 as a linear approximation to @xmath120 .",
    "the term @xmath121 is then the remainder from linearizing @xmath122 around @xmath123 the term @xmath124 is the difference between the linear approximation @xmath125 evaluated at the nonparametric estimator @xmath107 and at the empirical distribution @xmath126 , with @xmath127 @xmath128 .",
    "it is easy to fit the kernel estimator of the integrated squared density into this framework .",
    "we let @xmath107 be the cdf corresponding to a kernel density estimator @xmath129 then for @xmath46 the fact that @xmath130 gives an expansion as in equation ( [ exp ] ) with @xmath131^{2}dz.\\]]applying this framework to a series regression estimator requires formulating that as an estimator of a distribution @xmath52 .",
    "one way to do that is to specify a conditional expectation operator conditional on @xmath49 and a marginal distribution for @xmath49 , since a conditional expectation operator implies a conditional distribution . for a series estimator",
    "we can take @xmath107 to have a conditional expectation operator such that @xmath132=\\frac{1}{n}\\sum_{i=1}^{n}a(q_{i},x)p^{k}(x_{i})^{t}\\hat{\\sigma}^{-1}p^{k}(x).\\]]then it will be the case such that @xmath133dx=\\int w(x)\\hat{d}(x)dx=\\hat{\\beta},\\]]which only depends on the conditional expectation operator , leaving us free to specify any marginal distribution for @xmath49 that is convenient .",
    "taking @xmath107 to have a marginal distribution which is the true distribution of the data we see that @xmath134dx=\\int e_{\\hat{f}}[\\psi ( z)|x]f_{0}(x)dx=\\int \\psi ( z)\\hat{f}(dz).\\]]in this case @xmath135 and@xmath136f_{0}(x)dx-\\frac{1}{n}\\sum_{i=1}^{n}\\psi ( z_{i}).\\ ] ]    next we consider conditions for both of the remainder terms @xmath124 and @xmath121 to be small enough so that @xmath0 is asymptotically linear .",
    "the remainder term @xmath137 is the difference between a linear functional of the nonparametric estimator @xmath107 and the same linear functional of the empirical distribution @xmath126 .",
    "it will shrink with the sample size due to @xmath107 and @xmath126 being nonparametric estimators of the distribution of @xmath138 meaning that they both converge to @xmath61 as the sample size grows",
    ". this remainder will be the only one when @xmath43 is a linear functional of @xmath139    this remainder often has an important expectation component that is related to the bias of @xmath0 .",
    "often @xmath107 can be thought of as a result of some smoothing operation applied to the empirical distribution .",
    "the @xmath107 corresponding to a kernel density estimator is of course an example of this . an expectation of @xmath124",
    "can then be thought of as a smoothing bias for @xmath0 , or more precisely a smoothing bias in the linear approximation term for @xmath16 consequently , requiring that @xmath140 will include a requirement that @xmath141 times this smoothing bias in @xmath0 goes to zero .",
    "also @xmath141 times the deviation of @xmath124 from an expectation will need to go zero in order for @xmath140 .",
    "subtracting an expectation from @xmath142 will generally result in a stochastic equicontinuity remainder , which is bounded in probability for fixed @xmath52 and converges to zero as @xmath52 approaches the empirical distribution . in the examples the resulting remainder goes to zero under quite weak conditions .    to formulate a high level condition we will consider an expectation conditional on some sigma algebra @xmath143 that can depend on all of the observations .",
    "this set up gives flexibility in the specification of the stochastic equicontinuity condition .",
    "assumption 1 : @xmath144=o_{p}(n^{-1/2})$ ] and @xmath145=o_{p}(n^{-1/2}).$ ]    we illustrate this condition with the examples .",
    "for the integrated square density let @xmath143 be a constant so that the conditional expectation in assumption 1 is the unconditional expectation .",
    "let @xmath146 and note that by a change of variables @xmath147 we have @xmath148 then@xmath149 & = & e[\\psi ( z_{i},h)]=\\int [ \\int \\psi ( z+hu)f_{0}(z)dz]k(u)du ,   \\label{kernelr1 } \\\\ \\hat{r}_{1}(\\hat{f})-e[\\hat{r}_{1}(\\hat{f } ) ] & = & \\frac{1}{n}\\sum_{i=1}^{n}\\left\\ { \\psi ( z_{i},h)-e[\\psi ( z_{i},h)]-\\psi ( z_{i})\\right\\ } .",
    "\\notag\\end{aligned}\\]]here @xmath150 $ ] is the kernel bias for the convolution @xmath151 of the influence function and the true pdf .",
    "it will be @xmath152 under smoothness , kernel , and bandwidth conditions that are further discussed below .",
    "the term @xmath153 $ ] is evidently a stochastic equicontinuity term that is @xmath154 as long as @xmath155=0.$ ]    for the series estimator for consumer surplus let @xmath156^{t}\\hat{\\sigma}^{-1}p^{k}(x)$ ] and note that @xmath157 here we take @xmath158 then we have @xmath159 & = & \\frac{1}{n}\\sum_{i=1}^{n}\\hat{\\delta}(x_{i})d_{0}(x_{i})-\\beta _ { 0 } ,   \\label{seriesr } \\\\",
    "\\hat{r}_{1}(\\hat{f})-e[\\hat{r}_{1}(\\hat{f})|\\chi _ { n } ] & = & \\frac{1}{n}\\sum_{i=1}^{n}[\\hat{\\delta}(x_{i})-\\delta ( x_{i})][q_{i}-d_{0}(x_{i } ) ] .",
    "\\notag\\end{aligned}\\]]here @xmath144 $ ] is a series bias term that will be @xmath154 under conditions discussed below .",
    "the term @xmath145 $ ] is a stochastic equicontinuity term that will be @xmath154 as @xmath160 gets close to @xmath161 . in particular , since @xmath160 depends only on @xmath162 , the expected square of this term conditional on @xmath143 will be @xmath163^{2}var(q_{i}|x_{i}),$ ] which is @xmath164 when @xmath165 is bounded and @xmath166^{2}=o_{p}(1).$ ]    turning now to the other remainder @xmath167 we note that this remainder results from linearizing around @xmath61 .",
    "the size of this remainder is related to the smoothness properties of @xmath43 .",
    "we previously used gateaux differentiability of @xmath43 along certain directions to calculate the influence function .",
    "we need a stronger smoothness condition to make the remainder @xmath121 small .",
    "frechet differentiability is one helpful condition .",
    "if the functional @xmath43 is frechet differentiable at @xmath61 then we will have @xmath168for some norm @xmath169 unfortunately frechet differentiability is generally not enough for @xmath170 this problem occurs because @xmath43 and hence @xmath171 may depend on features of @xmath52 which can not be estimated at a rate of @xmath172 for the integrated squared error @xmath173^{2}dz\\right\\ } ^{1/2}$ ] is the root integrated squared error .",
    "consequently @xmath174 is not bounded in probability and so @xmath113 does not converge in probability to zero .",
    "this problem can be addressed by specifying that @xmath175 converges at some rate and that @xmath43 satisfies a stronger condition than frechet differentiability .",
    "one condition that is commonly used is that @xmath176 . this condition will be satisfied if @xmath43 is twice continuously differentiable at @xmath61 or if the first frechet derivative is lipschitz .",
    "if it is also assumed that @xmath107 converges faster than @xmath177 then assumption a1 will be satisfied .",
    "a more general condition that allows for larger @xmath116 is given in the following hypothesis .",
    "assumption 2 : for some @xmath178 @xmath179 _ _  and _ _ @xmath180 .",
    "this condition separates nicely into two parts , one about the properties of the functional and another about a convergence rate for @xmath107 . for the case @xmath181",
    "assumption 2 has been previously been used to prove asymptotic linearity , e.g. by ait - sahalia ( 1991 ) , andrews ( 1994 ) , newey ( 1994 ) , newey and mcfadden ( 1994 ) , chen and shen ( 1998 ) , chen , linton , and keilegom ( 2003 ) , and ichimura and lee ( 2010 ) among others .    in the example of the integrated squared density @xmath182^{2}dz = o(\\left\\vert f - f_{0}\\right\\vert ^{2})$ ] for @xmath183^{2}dz\\}^{1/2}.$ ] thus assumption 2 will be satisfied with @xmath181 when @xmath13 converges to @xmath14 faster than @xmath177 in the integrated squared error norm .",
    "the following result formalizes the observation that assumptions 1 and 2 are sufficient for asymptotic linearity of @xmath0 .",
    "theorem 2 : _ if assumptions 1 and 2 are satisfied then _ @xmath0 _ _  is asymptotically linear with influence function _ _ @xmath39 .    an alternative set of conditions for asymptotic normality of @xmath184",
    "was given by ait - sahalia ( 1991 ) .",
    "instead of using assumption 1 ait - sahalia used the condition that @xmath185 converged weakly as a stochastic process to the same limit as the empirical process .",
    "asymptotic normality of @xmath186 then follows immediately by the functional delta method .",
    "this approach is a more direct way to obtain asymptotic normality of the linear term in the expansion .",
    "however weak convergence of @xmath185 requires stronger conditions on the nonparametric bias than does the approach adopted here .",
    "also , ait - sahalia s ( 1991 ) approach does not deliver asymptotic linearity , though it does give asymptotic normality .",
    "these conditions for asymptotic linearity of semiparametric estimators are more complicated than the functional delta method outlined in reeds ( 1976 ) , gill ( 1989 ) , and van der vaart and wellner ( 1996 ) .",
    "the functional delta method gives asymptotic normality of a functional of the empirical distribution or other root - n consistent distribution estimator under just two conditions , hadamard differentiability of the functional and weak convergence of the empirical process .",
    "that approach is based on a nice separation of conditions into smoothness conditions on the functional and statistical conditions on the estimated distribution .",
    "it does not appear to be possible to have such simple conditions for semiparametric estimators .",
    "one reason is that they are only differentiable in norms where @xmath174 is not bounded in probability .",
    "in addition the smoothing inherent in @xmath107 introduces a bias that depends on the functional and so the weakest conditions are only attainable by accounting for interactions between the functional and the form of @xmath139 in the next section we discuss this bias issue .",
    "in this section we consider primitive conditions for assumption 1 to be satisfied for kernel density and series estimators .",
    "we focus on assumption 1 because it is substantially more complicated than assumption 2 .",
    "assumption 2 will generally be satisfied when @xmath43 is sufficiently smooth and @xmath107 converges at a fast enough rate in a norm .",
    "such conditions are quite well understood .",
    "assumption 1 is more complicated because it involves both bias and stochastic equicontinuity terms .",
    "the behavior of these terms seems to be less well understood than the behavior of the nonlinear terms .",
    "assumption 1 being satisfied is equivalent to the linear functional @xmath187 being an asymptotically linear estimator .",
    "thus conditions for linear functionals to be asymptotically linear are also conditions for assumption 1 . for that reason it suffices to confine attention to linear functionals in this section . also , for any linear functional of the form @xmath188 we can renormalize so that @xmath189 for @xmath190.$ ] then without loss of generality we can restrict attention to functionals @xmath191 with @xmath192=0.$ ]      conditions for a linear functional of a kernel density estimator to be asymptotically linear were stated though ( apparently ) not proven in bickel and ritov ( 2003 ) . here we give a brief exposition of those conditions and a result .",
    "let @xmath7 be an @xmath193 vector and @xmath107 have pdf @xmath194 .",
    "as previously noted , for @xmath195 we have @xmath196 to make sure that the stochastic equicontinuity condition holds we assume :    assumption 3 : @xmath12 _ is bounded with bounded support , _ @xmath197 _ _  _ _ @xmath39 _ _  is continuous almost everywhere , and for some _ _ @xmath198 , @xmath199<\\infty .$ ]    from bickel and ritov ( 2003 , pp",
    ". 1035 - 1037 ) we know that the kernel bias for linear functionals is that of a convolution . from equation ( [ kernelr1 ] ) we see that@xmath200-\\beta _ { 0}=\\int \\rho ( hu)k(u)du,\\rho ( t)=\\int \\psi ( z+t)f_{0}(z)dz=\\int \\psi ( \\tilde{z})f_{0}(\\tilde{z}-t)d\\tilde{z}.\\]]since @xmath201 the bias in @xmath0 is the kernel bias for the convolution @xmath202 a convolution is smoother than the individual functions involved . under quite general conditions the number of derivatives of @xmath203 that exist will equal the sum of the number of derivatives @xmath204 of @xmath205 that exist and the number of derivatives @xmath206 of @xmath39 that exist .",
    "the idea is that we can differentiate the first expression for @xmath203 with respect to @xmath55 up to @xmath206 times , do a change of variables @xmath207 , and then differentiate @xmath204 more times with respect to @xmath55 to see that @xmath203 is @xmath208 times differentiable .",
    "consequently , the kernel smoothing bias for @xmath0 behaves like the kernel bias for a function that is @xmath208 times differentiable .",
    "if a kernel of order @xmath209 is used the bias of @xmath0 will be of order @xmath210 that is smaller than the bias order @xmath211 for the density .",
    "intuitively , the integration inherent in a linear function is a smoothing operation and so leads to bias that is smaller order than in estimation of the density .",
    "some papers have used asymptotics for kernel based semiparametric estimators based on the supposition that the bias of the semiparametric estimator is the same order as the bias of the nonparametric estimator .",
    "instead the order of the bias of @xmath0 is the product of the order of kernel bias for @xmath205 and @xmath39 when the kernel is high enough order .",
    "this observations is made in bickel and ritov ( 2003 ) .",
    "newey , hsieh , and robins ( 2004 ) also showed this result for a twicing kernel , but a twicing kernel is not needed , just any kernel of appropriate order .",
    "as discussed in bickel and ritov ( 2003 ) a bandwidth that is optimal for estimation of @xmath14 may also give asymptotic linearity . to see this note that the optimal bandwidth for estimation of @xmath14 is @xmath212 plugging in this bandwidth to a bias order of @xmath210 gives a bias in @xmath0 that goes to zero like @xmath213 this bias will be smaller than @xmath214 for @xmath215 thus , root - n consistency of @xmath0 is possible with optimal bandwidth for @xmath13 when the number of derivatives of @xmath39 is more than half the dimension of @xmath7 .",
    "such a bandwidth will require use of a @xmath208 order kernel , which is higher order than is needed for optimal estimation of @xmath216 bickel and ritov ( 2003 ) refer to nonparametric estimators that both converge at optimal rates and for which linear functionals are root - n consistent as plug in estimators , and stated @xmath217 as a condition for existence of a kernel based plug in estimator .",
    "we now give a precise smoothness condition appropriate for kernel estimators .",
    "let @xmath218 denote a vector of nonnegative integers and @xmath219 let @xmath220 denote the @xmath221partial derivative of @xmath222 with respect to the components of @xmath223    assumption 4 : @xmath205 _ is continuously differentiable of order _",
    "@xmath204 _ _ , _ _ @xmath39 _ _  is continuously differentiable of order _",
    "_ @xmath224 _ _  _ _ @xmath12 _ _  is a kernel of order _ _ @xmath225 _ _  _ _ @xmath226 , _ and there is _",
    "_  such that for all _ _ @xmath227 with @xmath228 _ _  _ _ @xmath229 and @xmath230 @xmath231    here is a result on asymptotic linearity of kernel estimators of linear functionals .",
    "theorem 3 : _",
    "if assumptions 3 and 4 are satisfied then _",
    "@xmath232    there are many previous results on asymptotic linearity of linear functionals of kernel density estimators .",
    "newey and mcfadden ( 1994 ) survey some of these .",
    "theorem 3 differs from many of these previous results in assumption 4 and the way the convolution form of the bias is handled .",
    "we follow bickel and ritov ( 2003 ) in this .",
    "conditions for a linear functional of series regression estimator to be asymptotically linear were given in newey ( 1994 ) .",
    "it was shown there that the bias of a linear functional of a series estimator is of smaller order than the bias of the series estimator . here",
    "we provide an update to those previous conditions using belloni , chernozhukov , chetverikov , and kato ( 2015 ) on asymptotic properties of series estimators .",
    "we give conditions for asymptotic linearity of a linear functional of a series regression estimator of the form@xmath37we give primitive conditions for the stochastic equicontinuity and bias terms from equation ( [ seriesr ] ) to be small .",
    "let @xmath233^{t}\\hat{\\sigma}^{-1}p^{k}(x)=e[\\delta ( x)p^{k}(x)^{t}]\\hat{\\sigma}^{-1}p^{k}(x)$ ] and @xmath234 as described earlier .",
    "the stochastic equicontinuity term will be small if @xmath235^{2}/n\\overset{p}{\\longrightarrow } 0.$ ] let @xmath236 $ ] and @xmath237 $ ] be the coefficients of the population regression of @xmath238 on @xmath239 then the bias term from equation ( [ seriesr ] ) satisfies@xmath240/n+e[\\delta ( x_{i})\\{p^{k}(x_{i})^{t}\\gamma -d_{0}(x_{i})\\ } ] , \\label{serbias}\\]]the first term following the equality is a stochastic bias term that will be @xmath154 under relatively mild conditions from belloni et .",
    "2015 ) . for the coefficients",
    "@xmath241 $ ] of the population projection of @xmath242 on @xmath243 the second term satisfies @xmath244=-e[\\{\\delta ( x_{i})-\\gamma _ { \\delta } ^{t}p^{k}(x_{i})\\}\\{d_{0}(x_{i})-p^{k}(x_{i})^{t}\\gamma \\}]\\]]where the equality holds by @xmath245 being orthogonal to @xmath243 in the population .",
    "as pointed out in newey ( 1994 ) , the size of this bias term is determined by the product of series approximation errors to @xmath246 and to @xmath238 .",
    "thus , the bias of a series semiparametric estimator will generally be smaller than the nonparametric bias for a series estimate of @xmath247 for example , for power series if @xmath29 and @xmath161 are continuously differentiable of order @xmath248 and @xmath249 respectively , @xmath49 is r - dimensional , and the support of @xmath49 is compact then by standard approximation theory , @xmath250\\right\\vert \\leq ck^{-(s_{d}+s_{\\delta } ) /r}\\ ] ]    as discussed in newey ( 1994 ) it may be possible to use a @xmath251 that is optimal for estimation of @xmath252 and also results in asymptotic linearity .",
    "if @xmath253 and @xmath251 is chosen to be optimal for estimation of @xmath252 then @xmath254 thus , root - n consistency of @xmath0 is possible with optimal number of terms for @xmath252 when the number of derivatives of @xmath161 is more than half the dimension of @xmath7 .",
    "turning now to the regularity conditions for asymptotic linearity , we follow belloni et . al .",
    "( 2015 ) and impose the following assumption that takes care of the stochastic equicontinuity condition and the random bias term . :",
    "assumption 5 : @xmath255 _ is bounded _ , @xmath256<\\infty , $ ] _ _  the eigenvalues of _ _",
    "@xmath236 $ ] _",
    "_  are bounded and bounded away from zero uniformly in _",
    "_ @xmath251 _ _ , there is a set _ _ @xmath257 _ with _ @xmath258 _ _  and _ _ @xmath259 _ and _",
    "@xmath260__such that__@xmath261}\\leq c_{k},$ ] _",
    "_  _ _ @xmath262 _ _  and for _ _ @xmath263 _ _  we have _ _",
    "@xmath264    the next condition takes care of the nonrandom bias term .    assumption 6 : @xmath265}\\leq c_{k}^{\\delta } ,",
    "$ ] @xmath266 , _ and _",
    "@xmath267    belloni et .",
    "2015 ) give an extensive discussion of the size of @xmath268 @xmath260 , and @xmath269 for various kinds of series approximations and distributions for @xmath270 . for power series assumptions 5 and 6",
    "are satisfied with @xmath271 @xmath272 @xmath273 , @xmath274 and @xmath275for tensor product splines of order @xmath276 , assumptions 5 and 6 are satisfied with @xmath277 @xmath278 @xmath279 , @xmath280 and@xmath281    theorem 4 : _ if assumptions 5 and 6 are satisfied then for _",
    "@xmath282 $ ] _ we have _",
    "@xmath283    turning now to the consumer surplus bound example , note that in this case @xmath51 is not even continuous so that @xmath161 is not continuous .",
    "this generally means that one can not assume a rate at which @xmath284 goes to zero . as long as @xmath285 can provide arbitrarily good mean - square approximation to any square integrable function , then @xmath286 as @xmath251 grows .",
    "then assumption 6 will require that @xmath287 is bounded .",
    "therefore for power series it suffices for asymptotic linearity of the series estimator of the bound that @xmath288for this condition to hold it suffices that @xmath29 is three times differentiable , @xmath289 and @xmath290 is bounded away from zero . for regression splines",
    "it suffices that    @xmath291    for this condition to hold it suffices that the splines are of order at least @xmath292 , @xmath29 is twice differentiable , @xmath293 and @xmath294 is bounded away from zero . here",
    "we find weaker sufficient conditions for a spline based estimator to be asymptotically linear than for a power series estimator .",
    "a more general class of semiparametric estimators that has many applications is the class of generalized method of moment ( gmm ) estimators that depend on nonparametric estimators .",
    "let @xmath295 denote a vector of functions of the data observation @xmath64 parameters of interest @xmath296 , and a distribution @xmath52 .",
    "a gmm estimator can be based on a moment condition where @xmath4 is the unique parameter vector satisfying @xmath297=0.\\]]that is we assume that this moment condition identifies @xmath298 .",
    "semiparametric single index estimation provides examples . for the conditional mean restriction ,",
    "the model assumes the conditional mean function to only depend on the index , so that @xmath299 . with normalization imposed ,",
    "first regressor coefficient is @xmath300 so that @xmath301 .",
    "let @xmath302 .",
    "ichimura ( 1993 ) showed that under some regularity conditions , @xmath303^{2}\\}\\]]identifies @xmath4 .",
    "thus in this case , @xmath304 and @xmath305^{2}\\}}{\\partial \\beta } .\\ ] ]    for the conditional median restriction , the model assumes the conditional median function @xmath306 to only depend on the index , so that @xmath307 .",
    "ichimura and lee ( 2010 ) showed that under some regularity conditions , @xmath308identifies @xmath4 .",
    "thus in this case , @xmath309    let @xmath310 . note that at @xmath311 , the derivative of @xmath312 with respect to @xmath296 equals @xmath313.\\]]thus the target parameter @xmath4 satisfies the first order condition @xmath314[y - e(y|x^{t}\\theta _ { 0})]\\}.\\ ] ]    analogously , at @xmath315 , the derivative of @xmath316 with respect to @xmath298 equals @xmath317/f_{y|x}(m(y|x^{t}\\theta_{0})|x).\\ ] ] thus the target parameter @xmath318 satisfies the first order condition @xmath319 [ 2\\cdot1\\{y < m(y|x^{t}\\theta_{0})\\}-1]/f_{y|x}(m(y|x^{t}\\theta_{0})|x)\\}.\\ ] ]    estimators of @xmath4 can often be viewed as choosing @xmath0 to minimize a quadratic form in sample moments evaluated at some estimator @xmath107 of @xmath61 . for @xmath320 and",
    "@xmath321 a positive semi - definite weighting matrix the gmm estimator is given by @xmath322 in this section we discuss conditions for asymptotic linearity of this estimator .    for this type of nonlinear estimator showing consistency",
    "generally precedes showing asymptotic linearity .",
    "conditions for consistency are well understood .",
    "for differentiable @xmath323 asymptotic linearity of @xmath0 will follow from an expansion of @xmath324 around @xmath4 in the first order conditions .",
    "this gives @xmath325with probability approaching one , where @xmath326 , @xmath327 , and @xmath328 is a mean value that actually differs from row to row of @xmath329 .",
    "assuming that @xmath330 for positive semi - definite @xmath331 , and that @xmath332 $ ] and @xmath333 it will follow that @xmath334 then asymptotic linearity of @xmath0 will follow from asymptotic linearity of @xmath335 .    with an additional stochastic equicontinuity condition like that of andrews ( 1994 ) , asymptotic linearity of @xmath335 will follow from asymptotic linearity of functionals of @xmath139 for @xmath336 let @xmath337 $ ] and @xmath338note that @xmath339 is the difference of two objects that are bounded in probability ( by @xmath340=0)$ ] and differ only when @xmath52 is different than @xmath61 . assuming that @xmath341 is continuous in @xmath52 in an appropriate sense we would expect that @xmath339 should be close to zero when @xmath52 is close to @xmath123 as long as @xmath107 is close to @xmath61 in large samples in that sense , i.e. is consistent in the right way , then we expect that the following condition holds .",
    "assumption 7 : @xmath342",
    ".    this condition will generally be satisfied when the nonparametrically estimated functions are sufficiently smooth with enough derivatives that are uniformly bounded and the space of function in which @xmath52 lie is not too complex ; see andrews ( 1994 ) and van der vaart and wellner ( 1996 ) . under assumption",
    "7 asymptotic linearity of @xmath343 will suffice for asymptotic linearity of @xmath344 . to see this",
    "suppose that @xmath343 is asymptotically linear with influence function @xmath345 then under assumption 7 and by @xmath346=0,$ ] @xmath347+o_{p}(1).\\]]thus assumption 7 and asymptotic linearity of @xmath343 suffice for asymptotic linearity of @xmath335 with influence function @xmath348 . in turn these conditions and others will imply that @xmath0 is asymptotically linear with influence function@xmath349.\\ ] ]    the influence function @xmath350 of @xmath337 $ ] can be viewed as a correction term for estimation of @xmath61 .",
    "it can be calculated from equation ( [ inf func ] ) applied to the functional @xmath351 .",
    "assumptions 1 and 2 can be applied with @xmath352 for regularity conditions for asymptotic linearity of @xmath353 here is a result doing so    theorem 5 : _ if _ @xmath354 _ _ , _ _ @xmath330 _ _ , _ _ @xmath323 _ _  is continuously differentiable in a neighborhood of _ _ @xmath4 _ _  with probability approaching _ _ @xmath355 _ _  for any _ _",
    "_  we have _",
    "_ @xmath357 _ _  _ _ @xmath358 _ _  is nonsingular , assumptions 1 and 2 are satisfied for _ _ @xmath359 $ ] _ _  and _ _ @xmath360 _ _  and assumption 7 is satisfied then _ _",
    "_  is asymptotically linear with influence function _ _ @xmath361 $ ] .",
    "alternatively , assumption 7 can be used to show that the gmm estimator is asymptotically equivalent to the estimator studied in section 4 .    for brevity we do not give a full set of primitive regularity conditions for the general gmm setting .",
    "they can be formulated using the results above for linear functionals as well as frechet differentiability , convergence rates , and primitive conditions for assumption 7 .",
    "in this paper we have given a method for calculating the influence function of a semiparametric estimator .",
    "we have also considered ways to use that calculation to formulate regularity conditions for asymptotic linearity .",
    "we intend to take up elsewhere the use of the influence function in bias corrected semiparametric estimation .",
    "shen ( 1995 ) considered optimal robust estimation among some types of semiparametric estimators . further work on robustness of the kinds of estimators considered here may be possible . other work on the influence function of semiparametric estimators",
    "may also be of interest .",
    "* proof of theorem 1 * :  note that in a neighborhood of @xmath362 @xmath363 ^{1/2}$ ] is continuously differentiable and we have @xmath364 ^{1/2}=\\frac{1}{2}\\frac{g_{z}^{h}(\\tilde{z})-f_{0}(\\tilde{z})}{\\left [ tg_{z}^{h}(\\tilde{z})+(1-t)f_{0}(\\tilde{z})\\right ]",
    "^{1/2}}\\leq c\\frac{g_{z}^{h}(\\tilde{z})+f_{0}(\\tilde{z})}{f_{0}(\\tilde{z})^{1/2}}.\\]]by @xmath63 bounded away from zero on a neighborhood of @xmath7 and the support of @xmath365 shrinking to zero as @xmath366 it follows that there is a bounded set @xmath367 with @xmath368 for @xmath11 small enough .",
    "therefore , it follows that @xmath369then by the dominated convergence theorem @xmath370 ^{1/2}$ ] is mean - square differentiable and @xmath371 is continuous in @xmath55 on a neighborhood of zero for all @xmath11 small enough .",
    "also , by @xmath372 for all @xmath373 and @xmath374 on a neighborhood of it follows that @xmath375 for all @xmath55 and @xmath11 small enough and hence @xmath376 .",
    "then by theorem 7.2 and example 6.5 of van der vaart ( 1998 ) it follows that for any @xmath81 a vector of @xmath2 observations @xmath377 that is i.i.d . with pdf @xmath378 is contiguous to a vector of @xmath2 observations with pdf @xmath63 .",
    "therefore , @xmath379holds when @xmath377 are i.i.d . with pdf @xmath380 .    next by @xmath90 continuous at @xmath7 , @xmath90 is bounded on a neighborhood of @xmath223 therefore for small enough @xmath11 , @xmath381 and hence @xmath382 is continuous in @xmath55 in a neighborhood of @xmath87 . also , for @xmath383 note that @xmath384    suppose @xmath377 are i.i.d . with pdf @xmath385",
    "let @xmath386 and @xmath387 .",
    "adding and subtracting terms , @xmath388note that @xmath389 . also , for large enough @xmath2",
    ", @xmath390so the lindbergh - feller condition for a central limit theorem is satisfied .",
    "furthermore , it follows by similar calculations that @xmath391 therefore , by the lindbergh - feller central limit theorem , @xmath392 .",
    "therefore we have @xmath393 if and only if@xmath394suppose that @xmath395 is differentiable at @xmath87 with derivative @xmath396 .",
    "then@xmath397by @xmath398 bounded .",
    "next , we follow the proof of theorem 2.1 of van der vaart ( 1991 ) , and suppose that eq .",
    "( [ mean converge ] ) holds for all @xmath399 consider any sequence @xmath400 .",
    "let @xmath401 be the subsequence such that @xmath402let @xmath403 for @xmath404 and @xmath405 for @xmath406 by construction , @xmath407 so that eq ( [ mean converge ] ) holds .",
    "therefore it also holds along the subsequence @xmath401 , so that @xmath408\\longrightarrow 0.\\]]by construction @xmath409 is bounded away from zero , so that @xmath410 /r_{m}\\longrightarrow 0 $ ] .",
    "since @xmath411 is any sequence converging to zero it follows that @xmath395 is differentiable at @xmath87 with derivative @xmath412 .",
    "we have now shown that eq .",
    "( [ mean converge ] ) holds for all sequences @xmath81 if and only if @xmath395 is differentiable at @xmath87 with derivative @xmath413 furthermore , as shown above eq .",
    "( [ mean converge ] )  holds if and only if @xmath0 is regular .",
    "thus we have shown that @xmath0 is regular if and only if @xmath395 is differentiable at @xmath87 with derivative @xmath412 .    finally note that as @xmath56 it follows from continuity of @xmath90 at @xmath7 , @xmath12 bounded with bounded support , and the dominated convergence theorem that @xmath414    * proof of theorem 2 * : this follows as outlined in the text from assumptions 1 and 2 and eq .",
    "( [ exp ] ) and the fact that if several random variables converge in probability to zero then so does their sum . q.e.d .",
    "* proof of theorem 3 * : by the first dominance condition of assumption 4 , @xmath415 is continuously differentiable with respect @xmath55 up to order @xmath416 in a neighborhood of zero and for all @xmath417 with @xmath418@xmath419for any @xmath417 with @xmath420 it follows by a change of variables @xmath207 and the second dominance condition that@xmath421is continuously differentiable in @xmath422up to order @xmath204 in a neighborhood of zero and that for any @xmath423 with @xmath424@xmath425therefore @xmath426 is continuously differentiable of order @xmath427 in a neighborhood of zero . since @xmath201 and @xmath12 has bounded support and is order @xmath427 the usual expansion for kernel bias gives@xmath200-\\beta _ { 0}=\\int \\rho ( hu)k(u)du = o(h^{s_{\\zeta } + s_{f}}).\\]]therefore , @xmath428\\longrightarrow 0.$ ]    next , by continuity almost everywhere of @xmath39 in assumption 3 it follows that @xmath429 as @xmath56 with probability one ( w.p.1 ) .",
    "also , by assumption 3 @xmath430 is finite w.p.1 , so that by @xmath12 having bounded support and the dominated convergence theorem , w.p.1 , @xmath431furthermore , for @xmath11 small enough @xmath432so it follows by the dominated convergence theorem that @xmath433\\longrightarrow 0 $ ] as @xmath434 therefore , @xmath435\\longrightarrow 0.\\]]since the expectation and variance of @xmath112 converges to zero it follows that assumption 1 is satisfied .",
    "assumption 2 is satisfied because @xmath43 is a linear functional , so the conclusion follows by theorem 2 . _",
    "_    * proof of theorem 4 : * since everything in the remainders is invariant to nonsingular linear transformations of @xmath285 it can be assumed without loss of generality that @xmath436=i.$ ] let @xmath437 so that by assumption 6 , @xmath438\\longrightarrow 0.$ ] note that by @xmath165 bounded and the markov inequality,@xmath439where the last equality follows as in step 1 of the proof of lemma 4.1 of belloni et .",
    "al . ( 2015 ) .",
    "we also have@xmath440\\sigma ^{-1}e[\\delta ( x)p^{k}(x_{i})]=e[\\{\\gamma _ { \\delta } ^{t}p^{k}(x_{i})\\}^{2}].\\]]by @xmath441 it follows that @xmath442\\longrightarrow e[\\delta ( x_{i})^{2}]>0 $ ] , so that @xmath443 .",
    "let @xmath444 so that @xmath445 .",
    "note that @xmath446/n=\\bar{\\gamma}^{t}(\\tilde{\\gamma}-\\gamma ) , \\tilde{\\gamma}=\\hat{\\sigma}^{-1}\\sum_{i=1}^{n}p^{k}(x_{i})d_{0}(x_{i})/n\\]]let @xmath447 and @xmath448 be defined by the equations@xmath449/\\sqrt{n}+r_{1n}(\\bar{\\gamma})=r_{1n}(\\gamma ) + r_{2n}(\\bar{\\gamma}).\\]]by eqs .",
    "( 4.12 ) and ( 4.14 ) of lemma 4.1 of belloni et .",
    "al . ( 2015 ) and by assumption 5 we have@xmath450noting that @xmath451=o(1),$ ] we have@xmath452/n=(\\gamma ^{t}\\gamma ) ^{1/2}\\bar{\\gamma}^{t}(\\gamma -\\gamma ) = o(1)o_{p}(1)\\overset{p}{\\longrightarrow } 0.\\]]also , note that @xmath453=0 $ ] , so that by the cauchy - schwarz inequality , @xmath454\\right\\vert = \\sqrt{n}\\left\\vert e[\\{\\delta ( x_{i})-p^{k}(x_{i})^{t}\\gamma _ { \\delta } \\}\\{d_{0}(x_{i})-p^{k}(x_{i})^{t}\\gamma \\}]\\right\\vert \\leq \\sqrt{n}c_{k}^{\\delta } c_{k}\\longrightarrow 0.\\]]then the conclusion follows by the triangle inequality and eq .",
    "( [ serbias ] ) . _",
    "_    * proof of theorem 5 : * as discussed in the text it suffices to prove that @xmath335 is asymptotically linear with influence function @xmath455 . by assumption 7",
    "it follows that@xmath456also , by the conclusion of theorem 1 and @xmath457 we have @xmath458by the triangle inequality it follows that@xmath459+o_{p}(n^{-1/2}).q.e.d.\\ ] ]",
    "ait - sahalia , y. ( 1991 ) : `` nonparametric functional estimation with applications to financial models , '' mit economics ph .",
    "d. thesis ."
  ],
  "abstract_text": [
    "<S> often semiparametric estimators are asymptotically equivalent to a sample average . </S>",
    "<S> the object being averaged is referred to as the influence function . </S>",
    "<S> the influence function is useful in formulating primitive regularity conditions for asymptotic normality , in efficiency comparions , for bias reduction , and for analyzing robustness . </S>",
    "<S> we show that the influence function of a semiparametric estimator can be calculated as the limit of the gateaux derivative of a parameter with respect to a smooth deviation as the deviation approaches a point mass . </S>",
    "<S> we also consider high level and primitive regularity conditions for validity of the influence function calculation . </S>",
    "<S> the conditions involve frechet differentiability , nonparametric convergence rates , stochastic equicontinuity , and small bias conditions . </S>",
    "<S> we apply these results to examples .    </S>",
    "<S> * jel classification : * c14 , c24 , h31 , h34 , j22    * keywords : * influence function , semiparametric estimation , bias correction . </S>"
  ]
}