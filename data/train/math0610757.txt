{
  "article_text": [
    "in multivariate analysis there are several statistical procedures whose output is a partition of the space .",
    "typical examples of this situation are cluster analysis and classification rules . in cluster analysis ( or un  supervised classification ) we look for a partition of the space into homogeneous groups or clusters ( with small dispersion within groups ) , that help us to understand the structure of the data .",
    "several cluster methods have been proposed , such as hierarchical clustering ( hartigan , 1975 ) , k - means ( macqueen , 1967 ) , k - mediods ( kaufman and rousseeuw , 1987 ) , kurtosis based clustering ( pea and prieto , 2001 ) . from most of them we get a partition of the space in disjoint subsets .",
    "pattern recognition or classification is about guessing or predicting the unknown nature of an observation , a discrete quantity such as black or white , one or zero , sick or healthy .",
    "an observation is a collection of numerical measurements such as an image ( which is a sequence of bits , one per pixel ) , a vector of weather data , or an electrocardiogram . in classification rules",
    ", we have in addition a training sample for each group , from which we know together with the observation of the random vector of variables , a label that indicates to which subpopulation it belongs .",
    "then a _ classifier _ is any map that represents for each new data our guess of the class , given its associated vector .",
    "the map produces a classification rule , that is also a partition of the space . according to which subset of the partition a new data belongs ,",
    "is classified in that class .",
    "there is also an extensive literature on classification rules , such as fisher s linear discrimination ( fisher , 1936 ) , nearest neighbor rules ( fix and hodges , 1951 ) , regression trees - cart ( breiman et al . , 1984 ) , or reduced kernel discriminant analysis ( hernndez and velilla , 2005 ) .",
    "a general problem in cluster or classification is to find structures in a high dimensional variable space but with small data sets .",
    "it is common that in many practical cases , the amount of variables ( that should not be confused with the amount of information ) is too high .",
    "this may be due to the presence of several `` noisy '' non  informative variables , and/or redundant information from strongly correlated variables that may produce multicolinearity .",
    "then the information contained in the data set could be extracted from a reduced subset of the original variables .",
    "a difficult task is to find out which variables are `` important '' , where the concept of `` important '' should be related to the statistical procedure we are dealing with . if we are interested in cluster analysis , we would like to find the variables that explains the groups we have found . in this way , a ( small ) subset of variables should `` explain ''",
    "as best as possible the statistical procedure in the original space ( the high dimensional space ) .",
    "dimension reduction techniques ( like principal component analysis ) will produce linear combinations of the variables which are difficult to interpret unless most of the coefficients of the linear combination are negligent .",
    "the variable selection method of fowlkes , et al .",
    "( 1988 ) shifts the problem to a reduced variable space and looks for new clusters with less variables .",
    "tandesse , et al .",
    "( 2005 ) propose a bayesian approach for simultaneously selecting variables and identifying cluster structures without knowing the number of clusters .",
    "the bayesian model with latent variables is very useful in cluster analysis since it produces the most complete output : number of clusters , data allocation and informative variables . to solve the model",
    "it is necessary to use mcmc methods , in particular metropolis - hastings with reversible - jump ( green , 1995 ) , that introduce an important complexity to the users that are not familiar with computer programming .    in this paper",
    "we propose consistent statistical methods for variable selection that are easy to use .",
    "the variables that explain better the procedure on the original space help us to understand better the cluster output , and as a by - product , we find a dimension reduction procedure that can be used in a new data set for the same problem .",
    "we consider two different proposals based on the idea of `` blinding '' unnecessary variables . to cancel the effect of one variable",
    ", we substitute all the values of that variable by it s marginal mean in the first proposal and by the conditional mean in the second proposal .",
    "the marginal mean approach is mainly oriented to detect the `` noisy '' non  informative variables , while the conditional mean approach is more related to deal also with multicolinearity .",
    "the first one is simpler and does not require large sample size as the second one . in practice , we will also need an algorithm to solve the optimization problem .    in section 2",
    "we define in precise terms what we understand for a subset of variables that explains a multivariate partition procedure .",
    "next we define our objective function and provide a strongly consistent estimate of the optimal subset .",
    "a small simulation study is also performed . in section 3",
    "we introduce the proposal based on the conditional mean and show the performance in a simulated data set . in section 4 we describe a forward  backward selection algorithm that looks for the minimum subset that explains a fixed percentage of the data assignation to the clusters .",
    "section 5 is devoted to the analysis of two real data examples with medium and large dimensional variable spaces .",
    "section 6 includes some final remarks and the proofs are given in the appendix .",
    "let @xmath2 be a random vector with distribution @xmath3 .",
    "we consider any statistical procedure whose output is a partition of the space @xmath4 .",
    "for instance , this is the case of the population target for most clustering methods or classification rules . to fix ideas",
    "we will concentrate in cluster methods . for a fix number of clusters @xmath5",
    ", we have a function @xmath6 which determines to which cluster each single point belongs .",
    "we denote the space partition by @xmath7 , that satisfies @xmath8 for instance , if we consider @xmath9 ( with k=2 ) , and @xmath10 are the cluster centers , i.e. the values that minimize @xmath11 the set @xmath12 is given by @xmath13 , while @xmath14 .",
    "if @xmath15 is large , typically some of the components of vector @xmath16 are strongly correlated or might be almost irrelevant for the cluster procedure .",
    "then , if the information from the noisy variables is removed from our data , we should expect that their cluster allocations does not change .",
    "these means that the data are kept in the same group as in the original partition .",
    "the key point is to notice that the partition is defined in the original @xmath15-dimensional space and the input data requires information from all the variables , included the noisy ones .",
    "we propose to look for the subset of indices @xmath17 for which the original partition rule applied to a new  less informative ",
    "vector @xmath18 built up from @xmath16 , behaves as close as possible to the procedure when is applied to the `` full information '' vector @xmath16 .",
    "the vector @xmath19 contains the variables from @xmath16 that are index by @xmath20 , and the rest of the variables with index outside the set @xmath20 are `` blinded '' .",
    "a noisy variable means that its probability distribution is almost the same at all the clusters .",
    "this suggests to substitute the information in the `` blinded '' variables by their mean value .",
    "it will depend on the problem ( the distribution @xmath3 of @xmath16 ) and on how many variables @xmath21 we select , the percentage of cluster allocations explained by them . in practice",
    ", we can choose @xmath22 in order to explain at least a fixed percentage , for instance , @xmath23 , @xmath24 or @xmath25 of the data .",
    "we now put our purpose in a precise setup . given a subset of indices",
    "@xmath26 we define the vector @xmath27 , where @xmath28 if @xmath29 and @xmath30 otherwise .",
    "note that instead of the expectation @xmath31 we can use the median of @xmath32 or any other location parameter for the @xmath33coordinate like m - estimates or trimmed  means .",
    "the results will still holds provided we have a strong consistent estimate of the location parameter .    for a fixed integer @xmath21 ,",
    "the population target is the set @xmath34 , @xmath35 = d , for which the _ population objective function _ , given by @xmath36 attains it maximum .    in this way",
    ", we look for the subset @xmath20 for which the original partition rule applied to the less informative random vector @xmath19 behaves as close as possible to the procedure when is applied to the `` full information '' vector @xmath16 .",
    "all components with index outside the set @xmath20 are blinded in the sense that are constant .    in practice ,",
    "the empirical version consist on the application of the next steps :    1 .",
    "given iid data @xmath37 , apply the partition procedure to the data set and obtain the empirical cluster allocation function , @xmath38 where now @xmath39 is data dependent .",
    "the associated space partition will be denoted by @xmath40 , for @xmath41 .",
    "2 .   for a fixed value @xmath21 , given a subset of indices @xmath42 , with @xmath43 , define the random vectors @xmath44 verifying @xmath45 = x_j[i ] \\mbox { if } i \\in i , \\mbox { and } x^*_j[i ] = \\bar{x}[i ]   \\mbox { otherwise},\\ ] ] where @xmath46 $ ] stands for the @xmath33coordinate of the vector @xmath16 , and @xmath47 $ ] stands for the @xmath33coordinate of the average vector .",
    "+ if we have used instead of the expected value other location parameter in the population version ( like the median ) , we substitute the average by the empirical version ( the sample median ) .",
    "3 .   calculate the _ empirical objective function _",
    "@xmath48 where @xmath49 stands for the indicator function of the set @xmath50 .",
    "4 .   look for a subset @xmath51 , with @xmath52 , that maximizes the empirical objective function @xmath53 .",
    "as expected , the consistency of our variable selection procedure is linked to the properties of the cluster partition method .",
    "we now give some conditions under which our procedure is consistent .    _ assumption 1 : _",
    "\\a ) the partition procedure is strongly consistent , i.e. , given @xmath54 , there exists a set @xmath55 with @xmath56 , such that for all @xmath57 @xmath58 where @xmath59 stands for the intersection of the set @xmath60 and the closed ball centered at zero of radius @xmath61 , @xmath62 .",
    "\\b ) @xmath63 where @xmath64 stands for the distance from @xmath16 to the frontier of @xmath65 .",
    "_ assumption 2 : _",
    "@xmath66    assumption 1a holds typically for cluster and classification rules , where the set @xmath60 is the complement of an @xmath61neighborhood (  outer parallel set \" ) of the partition boundaries as shown in figure  [ dibujito ] , i.e. @xmath67 where @xmath68 denotes the ball with center @xmath69 and radius @xmath61 .",
    "to @xmath70 over compact sets ( the color area @xmath60).,width=132 ]     * ( strong consistency ) * let @xmath71 be iid random vectors with distribution @xmath72 . given @xmath22 , @xmath73 ,",
    "let @xmath74 be the family of all subsets of @xmath75 with cardinal @xmath22 , and @xmath76 the family of subsets where the maximum of @xmath77 is attained , for @xmath78 . then , under assumptions 1 and 2 we have that there exists @xmath79 , such that @xmath80    the proof is given in the appendix .      in order to analyze our method performance ,",
    "we carry out a monte carlo study for some simulated date sets . in all of them we generated 100 observations in a three dimensional variable space .",
    "the underlying distributions are mixtures of three multivariate normals ,    @xmath81    where @xmath82 and @xmath83 .",
    "the cluster structure is defined through @xmath84 and @xmath85 and , to simplify , we consider they are independent in all the cases , with distributions given by @xmath86 for the distribution of @xmath87 we consider two different scenarios .",
    "_ case i : _ @xmath88 is an independent `` noise '' variable with distribution given by @xmath89 where @xmath90 takes different values , @xmath91 , @xmath92 and @xmath93 .",
    "figure  [ 3clusterplots ] shows a simulated data set from these distributions with @xmath94 .",
    "the three clusters are perfectly distinguish when plotting the pairs @xmath95 , however only two clusters are appreciated in the scatter plots that consider @xmath88 , as it is the case of the @xmath84 and @xmath96 histograms .    ,",
    "width=302 ]    _ case ii : _ @xmath88 is not an independent variable and is given by @xmath97    in table  [ tablasimul ] we report the proportion of times where the information in only one , two or three variables is enough to explain all the cluster allocations .",
    "we also consider the effect of a possible reduction in the efficiency to only 95% , or 90% , of correct allocations . in all the cases we carried out 1,000 replications and follow the next steps",
    ".    1 .   generate @xmath98 observations .",
    "2 .   split the data into three cluster using the @xmath99means algorithm .",
    "3 .   search the optimal subset of variables for 100% , 95% and 90% efficiencies .",
    "in the first case our variable selection method is very successful and selects only the two variables @xmath100 and @xmath101 in almost all the simulations , for 100% , 95% and 90% efficiencies .",
    "a different scenario appears with case ii , where the third variable is a linear combination of the first two variables . only in the 14.6% of the times",
    "the two variable subset explains all the cluster allocations .",
    "this changes dramatically when we allow for a 5% or 10% of miss ",
    "classified observations , now 97% of the times the method selects only two variables , instead of three .    case ii shows and interesting feature of the selection variable procedure , it is able to eliminate noise variables , but it is unable to detect redundant information from co  linear variables .",
    "this effect can be more clearly seen with the simulated example proposed by tadesse , sha and vannucci ( 2005 ) .",
    "the data consists on the 15 three - dimensional observations displayed in figure  [ tsv05]a .",
    "the first four observations come from independent normals with mean @xmath102 and variance @xmath103 .",
    "the next three data come from independent normals with mean @xmath104 and variance @xmath105 .",
    "the following six data come from independent normals with mean @xmath106 and variance @xmath107 , while the last two come from independent normals with mean @xmath108 and variance @xmath109 . despite in tadesse _",
    "( 2005 ) the data set was generated with twenty  dimensional observations instead of three  dimensional , we call tsv05 to this data set .",
    "-means centers , b ) results of blinding the vertical coordinate with the mean value.,title=\"fig:\",width=151,height=132 ] -means centers , b ) results of blinding the vertical coordinate with the mean value.,title=\"fig:\",width=151,height=132 ]    we first run the @xmath99-mean algorithm with @xmath110 , which classified correctly the whole data set .",
    "then , we run the variable selection procedure based on the mean value ( dropping out noisy non  informative variables ) . a closer look to this data",
    "generating mechanism indicates that one should expect to attain a 100% efficiency with only one variable , since we have the same cluster structure at the three coordinates .",
    "however , the procedure was unable to find the cluster structure blinding all variables except one .",
    "the efficiencies in table  [ tablatsv05a ] show that only the subset with the three variables classify all the data in their original clusters .",
    "this result is expected since all the variables contain information about the cluster , they are not noisy variables . however , as in case ii , these colinear variables are redundant and would be interesting to develop a variable selection method able to detect them .",
    "figure  [ tsv05]b helps us to understand the main problem that appears when we blind one variable by substituting all the data for the mean value .",
    "we observe the case of blinding the vertical coordinate , that means a projection of all the data in the shadow mean plane . as the mean is not a representative value for data generated from a cluster structure , the allocations will be by chance to any of the clusters . for instance , we point out the correct center for one projected data with a discontinuous arrow , however in this case the closer center is a different one .",
    "this data is wrongly allocated with the variable selection method .",
    "remember that we blind the variable but not the corresponding coordinate of the @xmath99-mean centers . then to eliminate not only noisy variables but also colinear variables the idea is to blind with local information , instead of using the mean .",
    "this would not be a problem for noisy variables and we will see in the next section that it is crucial for multicolinearity .",
    "the previous procedure is mainly designed to find `` noisy '' non - informative variables , however as the simulated data set highlighted , it may fail in the presence of colinearity . in order to deal with this problem",
    ", we consider a quite natural extension , changing the definition of the `` less informative '' vector @xmath19 .",
    "recall that we defined @xmath111 , if @xmath29 , and @xmath112 otherwise .",
    "thus , for indices in the complement of the set @xmath20 , @xmath113 is defined as the best constant predictor .",
    "now the idea appears clearly , to change means by conditional means .",
    "we define the less informative vector @xmath114 for indices @xmath33 in the complement of the set @xmath20 as the conditional expectation of @xmath32 given the set of variables @xmath115 , i.e. the best predictor of @xmath32 based on those variables",
    ". this procedure will be able to deal with both kinds of problems .",
    "however , at a first look , a shortcoming is that it will require a large sample size in order to estimate the conditional expectation .",
    "also the computational effort is quite bigger .",
    "the choice of the smoothing parameter is also challenging , since it must involve not more data than the size of the smaller cluster ( if we think for instance in local averages ) . if @xmath116 is the size of the smallest group for the partition procedure , and for each @xmath22 and @xmath117 , @xmath118 is the number of nearest neighbor s we will need to require that @xmath119 , together with the standard conditions @xmath120 , and @xmath121 , as @xmath122 .",
    "we now describe briefly the proposal in a precise setup .      given a subset of indices @xmath123 let @xmath124 = : ( x_{i_1 } , \\ldots , x_{i_d } ) , \\hspace{2 mm } \\mbox{for } \\hspace{2 mm } i_1 < i_2 <",
    "\\ldots < i_d.\\ ] ]    we define the vector @xmath125 , where @xmath126 if @xmath29 and @xmath127)$ ] otherwise . instead of the conditional expectation @xmath128)$ ] in order to attain robustness we can use local medians , or local m - estimates ( see for instance , stone , 1977 , truong , 1989 or boente and fraiman , 1995 ) .    for a fix integer @xmath21 , now the _ population objective function _",
    "is the set @xmath129 , @xmath130 , for which the function @xmath131 attains it maximum .    in practice ,",
    "the empirical version consists on the same steps than in the method based on using the mean , except the second , that is substitute by the next step :    1 .   for a fixed value of @xmath21 , given a subset of indices @xmath132 , with @xmath43 , fix an integer value @xmath133 ( the number of nearest neighbor to be used ) . for each @xmath134 , find the set of indices @xmath135 of the @xmath133-nearest neighbor s of @xmath136 $ ] among @xmath137 , \\ldots , x_n[i ] \\}$ ] . + now define the random vectors @xmath138 verifying @xmath45 = x_j[i ] \\hspace{2 mm } \\mbox{if } \\hspace{2 mm } i \\in i , \\hspace{2 mm } \\mbox{and } \\hspace{2 mm } x^*_j[i ] = \\frac{1}{r } \\sum _ { m \\in c_j } x_m [ i ] \\hspace{2 mm } \\mbox{otherwise},\\ ] ] where @xmath46 $ ] stands for the @xmath33coordinate of the vector @xmath16 .",
    "a resistant procedure would take the local median instead of the local mean for @xmath139 , i.e. @xmath140 = median(\\{x_m [ i ] : m \\in c_j\\})$ ] .      as we have seen before the consistency of the variable selection method relays on the properties of the cluster partition methods .",
    "moreover , regularity conditions on the boundary of the partitioning sets , in order to carry out the nonparametric regression , are requested .",
    "now , we give the conditions under which our procedure is consistent . together with assumption 1 we will need :    _ assumption 3 : _ @xmath141    _ assumption 4 : _",
    "@xmath142 where @xmath143 is the corresponding non ",
    "parametric regression functions and @xmath144 is a consistent estimate of @xmath145 .",
    "assumption 4 allows to use any uniformly consistent estimate of the regression function , although we have only describe above the case of @xmath133nearest neighbor estimates .    *",
    "( strong consistency ) * let @xmath71 be iid random vectors with distribution @xmath72 . given @xmath22 , @xmath146 , let @xmath147 be the family of all subsets of @xmath148 with cardinal @xmath22 , and @xmath149 the family of subsets where the maximum of @xmath77 is attained , for @xmath150 . then ,",
    "under assumptions 1 , 3 and 4 we have that there exists @xmath151 , such that @xmath152    the proof of theorem 2 is very similar to that of theorem 1 and we omit it in full detail .",
    "we only point out the differences at the appendix .",
    "when we apply the conditional mean selection variable method to the 15 three dimensional data of tsv05 , we now obtain that with only one variable we attain a 100% efficiency .",
    "this is the case of the second or the third variable , while with the first one we obtain a 93.3% efficiency .",
    "a slightly different version of this example includes three new variables .",
    "the additional noisy coordinates are generated from independent standard normal distributions .",
    "the two variable selection procedures applied to the six  dimensional data set produce exactly the same results as for the three dimensional data .",
    "the noisy variables are not necessary to reach 100% efficiency .",
    "a well known feature of the variable selection problem is the great number of subsets that should be considered even for moderate values of @xmath15 .",
    "an exhaustive search guarantees to find the smaller subset of variables to achieve , at least , a fixed percentage on the empirical objective function , however this procedure is non feasible when many variables are considered .",
    "for instance , if @xmath153 we should check among more than @xmath154 combinations .",
    "alternatively , we propose a computationally less expensive forward - backward search algorithm .",
    "we run the search meanly in the forward mode and include the last step in the backward mode .",
    "the algorithm starts from a one variable set and , progressively , includes new variables with an iterative revision of the inclusions in each step .",
    "in general , the backward search is less costly , but the leave - one - out strategy will make difficult to find a small subset .",
    "when a set provides a percentage of good classifications over the fixed percentage , the backward process starts the search of a more parsimonious solution . to compute the objective function",
    "we can blind the variables either replacing them by the mean or the conditional mean ( in this case the conditional distribution is towards the chosen subset up to that step ) .",
    "the estimation of the conditional mean is done by nearest neighbors .",
    "we distinguish three parts in the algorithm design , that are sequentially implemented .",
    "part 1 : : :    select the most `` influential '' variable    @xmath155 ( the data assignation is more    affected by its absence ) , blinding one by one all the variables and    selecting the one with minimum value of the objective function ,    @xmath156    where @xmath157 denotes all the variables except the    @xmath158 , that is blinded .",
    "part 2 : : :    sequential increment of variables one by one ( forward search ) . in each    step",
    "we look for the accompanying variable , such that the new subset    maximizes the number of successfully data allocations .",
    "we also    consider replacement of previously introduced variables following the    iterative scheme described by miller ( 1984 ) as a variation of the    classical forward - backward methods .",
    "the increment continues until the    fixed percentage of well classified data is reached .",
    "part 3 : : :    the subset is revised for unnecessary variables ( backward search ) .",
    "the    previously introduced variables are questioned one by one whether they    are necessary .",
    "the algorithm stops when no further reduction can be    found without loss of efficiency .",
    "this procedure strongly depends on the order in the variable vector . to avoid ( or minimize ) the label effect we run the algorithm for a random sample of the permuted variables .",
    "we finally select the solutions that use the minimum number of variables .    in the following section we illustrate the algorithm performance in real data examples .",
    "the matlab codes are available upon request to the authors .",
    "we have survey data concerning education quality from 98 schools in the city and suburbs of buenos aires ( argentine ) .",
    "the survey and posterior data analysis was developed by llach _",
    "an important objective on this study was to find homogeneous groups of schools and the characterization of the clusters .",
    "the selection variable method is a powerful tool to separate all the variables with real influence from those that are non - informative .    at each school ,",
    "a questionnaire with fifteen items was fulfilled by the headmaster and the teachers .",
    "the questions regard on the human and didactic resources , the relationships between all the involved agents and the building physical condition .",
    "all the answers range in a discrete scale between 1 and 100 .",
    "the items @xmath159 to @xmath160 are answered by the headmasters and refer to their experience , aptitude , school general knowledge , evaluation of the building conservation , evaluation of the didactic resources , relationships with teachers , parents and students .",
    "the items @xmath161 to @xmath162 are answered by the teachers and the questions are the same @xmath159 to @xmath160 , except @xmath163 ( school general knowledge ) that is only answered by the headmasters .    in llach _",
    "( 2006 ) , a @xmath99means cluster procedure was performed using the 98 fifteen dimensional vectors .",
    "the data were split into three clusters of sizes 45 , 21 and 19 respectively .",
    "the relationship between the clusters and the mean scores in a general knowledge exam ( gke ) and the mean socioeconomic level of the students ( sel ) are shown in table [ tabla gke ] .",
    "both the gke results and sel are significatively different among clusters , with anova @xmath164 .",
    "the clusters with higher mean level of student knowledge correspond with those with also higher mean socioeconomic level .",
    "the question now is which variables have relevant information to establish this school grouping .",
    ".means and standard deviations for the student general knowledge exams ( gke ) and the student socioeconomic levels ( sel ) . [ cols=\"^,^,^,^,^ \" , ]     the use of the conditional mean algorithm , instead of the faster mean algorithm , reduces in all the cases the number of time  intervals that provides enough information to characterize the two electric power home  consumer typologies .",
    "the results show that the choice of the number of nearest neighbor s is also important , although the method seems to be less sensitive than non ",
    "parametric regression .",
    "however , it is an important problem to be solved . in our case ,",
    "the results for 5-nn are quite satisfactory : for a 100% of efficiency , there is only one solution with 9 variables ; for a 95% of efficiency we found 15 different solutions with six variables ; while for a 90% of efficiency we found 5 different solutions with four variables .",
    "we choose one of them to illustrate in figure  [ sol5nn ] the `` window  times '' ( non - shadow areas ) which seems relevant .",
    "the most informative consume registers are confined to a few `` window  times '' ( see figure  [ meanand5nn ] ) and the two types of electric power consumers are mainly characterized by their different aptitudes at some time ",
    "intervals in the morning ( 7:00 to 11:00 ) , evening ( 15:00 to 19:00 ) , night ( 21:00 to 24:00 ) and early morning ( 3:00 to 4:00 ) . comparing the mean and the 5-nn conditional solutions we observe that the redundant information , specially at evening and night , is summarized in the smaller subset of variables found by the mean conditional algorithm .",
    "when we reduce the degree of efficiency and accept a number of missclassifications , the importance of the early morning behavior diminished .",
    "we propose two variable selection procedures particularly design for partition rules ( typically supervised and un - supervised classification methods ) that help to understand the results for high - dimensional data .",
    "both methods are strongly consistent .",
    "the second procedure , based on conditional means , is much more flexible and takes into account general dependence structures within the data .",
    "the performance of our proposals in simulated and real data examples is quite impressive .    for low or moderate dimensional data an exhaustive search is possible for even the case of 100% efficiency . however , it is unfeasible for high - dimensional data and we propose a forward - backward algorithm .",
    "we compare the algorithm performance with the exhaustive search in some of the examples and the results are very positive since they provide the same subsets .",
    "however , it will demand a considerable computational effort in time that suggest that some additional research should be consider in this aspect .",
    "ricardo fraiman and ana justel have been partially supported by the spanish ministerio de educacin y ciencia , grant mtm2004 - 00098 .",
    "_ proof of theorem 1 _    to simplify the proof , we assume that there exists a unique subset @xmath165 that maximize @xmath77 for @xmath166",
    ". then we point out the differences with the case of more than one subset along the lines of the proof .",
    "as the optimization is over all the @xmath22 combinations of the @xmath15 variable indices , a finite number , it suffices to show that @xmath167 indeed , since there exist a unique set @xmath168 that maximizes @xmath77 , there exists @xmath169 such that @xmath170 we have from ( [ conv ] ) that for all @xmath171 @xmath172 which entails @xmath173 since we also have @xmath174 we conclude that @xmath175 maximizes @xmath176 if @xmath177 a.s .",
    "if there exists more than one subset in @xmath178 , the argument is the same by replacing @xmath175 by @xmath179 .",
    "now , it remains to show ( [ conv ] ) , which reduces to prove that @xmath180 for @xmath181    finally , the equation ( [ ec1 ] ) will follow if we show that , for all fixed @xmath99 , @xmath182 and @xmath183 = p(f(x)=k , f(y)=k ) \\mbox { a.s.}\\end{aligned}\\ ] ]    first we show that by the assumption 1a we have @xmath184    the left hand side of ( [ ec111 ] ) is majorized by    @xmath185 @xmath186",
    "the first term converges to zero for any @xmath61 and @xmath133 by assumption 1a , while the second term is dominated by @xmath187 which converges a.s . to @xmath188",
    "since this last limit can be made arbitrarily small choosing @xmath61 and @xmath133 adequately , ( [ ec111 ] ) holds . finally from the law of large numbers we get @xmath189 which concludes the proof of ( [ ec11 ] ) .    for the proof of the equation ( [ ec12 ] ) , the way in which the random vectors @xmath190 and @xmath191 have been defined , for a fixed subset @xmath20 , implies that all the @xmath33coordinates of @xmath192 are zero for @xmath193 , while the rest of them ( for @xmath194 ) are given by @xmath195 - e(x[i]).\\ ] ] we recall that @xmath46 $ ] stands for the @xmath33coordinate of the vector @xmath16 , and @xmath47 = \\frac{1}{n } \\sum_{j=1}^n x_j[i]$ ] .",
    "the vectors @xmath192 are all the same ( i.e. the difference do not depend on @xmath196 ) , and are given by @xmath197 = ( \\bar{x}[i ] - e(x[i ] ) ) \\mathcal{i}_{\\{i \\notin i\\ } } ,   \\mbox { for } j=1,\\dots n.\\end{aligned}\\ ] ] > from the law of large numbers we get @xmath198 the proof of ( [ ec12 ] ) will be complete if we show that @xmath199 and @xmath200 we now define the sets b and c as follows : @xmath201    @xmath202 and @xmath203 by the assumption 1b we have that @xmath204 . therefore , given @xmath205 and @xmath206 , there exists @xmath207 such that @xmath208 given @xmath206 , we also have the following inclusions : @xmath209 which imply that the left hand side of ( [ ec122 ] ) is majorized by @xmath210 which converges , as @xmath211 , to @xmath212 finally , from the assumption 2 we get that @xmath213 which concludes the proof of ( [ ec122 ] ) .",
    "the proof of ( [ ec123 ] ) is completely analogous .",
    "_ proof of theorem 2 _    the proof goes on the same lines as the proof of theorem 1 .",
    "the only difference is that now @xmath214 follows from assumption 4 .",
    ".5cm-.5 cm                          macqueen , j.b .",
    "( 1967 ) , `` some methods for classification and analysis of multivariate observations '' . in _ proceedings of 5-th berkeley symposium on mathematical statistics and probability _ , berkeley , university of california press , 281297 ."
  ],
  "abstract_text": [
    "<S> in this paper we introduce two procedures for variable selection in cluster analysis and classification rules . </S>",
    "<S> one is mainly oriented to detect the `` noisy '' non  informative variables , while the other deals also with multicolinearity . </S>",
    "<S> a forward  backward algorithm is also proposed to make feasible these procedures in large data sets . a small simulation is performed and some real data examples are analyzed . * keywords : * cluster analysis , selection of variables , forward - backward algorithm . _ </S>",
    "<S> a.m.s . </S>",
    "<S> 1980 subject classification : _ _ primary : _ 62h35    * selection of variables for cluster analysis and classification rules *    _ @xmath0departamento de matemtica y ciencias , universidad de san andrs , argentina _ </S>",
    "<S> + _ @xmath1departamento de matemticas , universidad autnoma de madrid , spain _ </S>"
  ]
}