{
  "article_text": [
    "the purpose of this lecture note is to illustrate a route for the definition of entropy using our experience with computers . in the process",
    "the connection between statistical physics and computations comes to the fore .",
    "this is a question that plagues almost all especially beginning physics students .",
    "there are several correct ways to answer this .    1 .",
    "it is the perfect differential that one gets by dividing the heat transfered by a quantity @xmath0 that gives us the hot - cold feeling ( i.e. temperature ) .",
    "it is the log of the number of states available .",
    "it is something proportional to @xmath1 where @xmath2 is the probability that the system is in state i. ( ) 4 .",
    "it is just an axiom that there exists an extensive quantity @xmath3 , obeying certain plausible conditions , from which the usual thermodynamic rules can be obtained .",
    "( )    but the colloquial link between disorder or randomness and entropy remains unexpressed though , agreeably , making a formal connection is not easy .",
    "our plan is to establish this missing link _ a la _ kolmogorov .    besides these conceptual questions",
    ", there is a practical issue that bugs many who do computer simulations where different configurations are generated by some set of rules . in the end one wants to calculate various thermodynamic quantities which involve both energy and entropy .",
    "now , each configuration generated during a simulation or time evolution has an energy associated with it . _ but does it have an entropy ? _ the answer is of course blowing in the wind .",
    "all thermodynamic behaviours ultimately come from a free energy , say , @xmath4 where @xmath5 , the energy , generally known from mechanical ideas like the hamiltonian , enters as an average , denoted by the angular brackets , _ but no such average for @xmath3_. as a result , one can not talk of `` free energy '' of a configuration at any stage of the simulation .",
    "all the definitions mentioned above associate @xmath3 to the ensemble , or distributions over the phase space .",
    "they simply forbid the question  what is the entropy of a configuration \" .",
    "too bad !      over the years we have seen the size of computers shrinking , speed increasing and power requirement going down .",
    "centuries ago a question that tickled scientists was the possibility of converting heat to work or finding a perfect engine going in a cycle that would completely convert heat to work . a current version of the same problem would be : can we have a computer that does computations but at the end does not require any energy . or , we take a computer , draw power from a rechargeable battery to do the computation , then do the reverse operations and give back the energy to the battery .",
    "such a computer is in principle a perpetual computer . _",
    "is it possible ? _",
    "what we mean by a computer is a machine or an object that implements a set of instructions without any intelligence .",
    "it executes whatever it has been instructed to do without any decision making at any point . at the outset , without loss of generality ,",
    "we choose binary ( 0,1 ) as the alphabet to be used , each letter to be called a bit .",
    "the job of the computer is to manipulate a given string as per instructions .",
    "just as in physics , where we are interested in the thermodynamic limit of infinitely large number of particles , volumes etc , we would be interested in infinitely long strings .",
    "the question therefore is  can bit manipulations be done without cost of energy ? \"",
    "the problem that a configuration can not have an entropy has its origin in the standard statistical problem that a given outcome of an experiment can not be tested for randomness .",
    "e.g. , one number generated by a random number generator can not be tested for randomness .    for concreteness ,",
    "let us consider a general model system of a magnet consisting of spins @xmath6 arranged on a square lattice with @xmath7 representing a lattice site .",
    "if necessary , we may also use an energy ( or hamiltonian ) @xmath8 where the sum is over nearest neighbours ( i.e. bonds of the lattice ) .",
    "suppose the temperature is so high that each spin can be in anyone of the two states @xmath9 with equal probability .",
    "we may generate such a configuration by repeated tossing of a fair coin . if we get @xmath10 ( @xmath11:h,@xmath12:t )",
    "is it a random configuration ? or can the configurations of spins as shown in fig .",
    "[ fig:1 ] be considered random ?     are represented by arrows pointing up or down .",
    "( a ) a ferromagnetic state , ( b ) an antiferromagnetic state , and ( c ) a seemingly random configuration . ]    with @xmath13 spins ( or bits ) , under tossing of a fair coin , the probability of getting fig .",
    "[ fig:1](a ) is @xmath14 and so is the probability of ( b ) or ( c ) .",
    "therefore , the fact that a process is random can not be used to guarantee randomness of the sequence of outcomes .",
    "still , we do have a naive feeling .",
    "all heads in @xmath13 coin toss experiments or strings like 1111111 ... ( ferro state of fig . [ fig:1](a ) ) or 10101010 ... ( anti - ferro state of fig [ fig:1](b ) ) are never considered random because one can identify a pattern , but a string like 110110011100011010001001 ... ( or configuration of fig [ fig:1](c ) ) may be taken as random .",
    "_ but what is it that gives us this feeling ? _      the naive expectation can be quantified by a different type of arguments , not generally emphasized in physics . suppose i want to describe the string by a computer programme ; or rather by an algorithm .",
    "of course there is no unique  programming \" language nor there is  a `` computer - but these are not very serious issues .",
    "we may choose , arbitrarily , one language and one computer and transform all other languages to this language ( by adding ' ' translators \" ) and always choose one particular computer .",
    "the two strings , the ferro and the anti - ferro states , can then be obtained as outputs of two very small programmes ,    .... ( a ) print 1    5 million times ( ferro state ) ( b ) print 10   2.5 million times ( antiferro state ) ....    in contrast , the third string would come from    .... ( c ) print 110110011100 ...   ( disordered state ) ....",
    "so that the size of the programme is same as the size of the string itself .",
    "this example shows that the size of the programme gives an expression to the naive feeling of randomness we have .",
    "we may then adopt it for a quantitative measure of randomness .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ definition : let us define _ randomness _ of a string as the size of the _ minimal _ programme that generates the string .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the crucial word is  minimal \" . in computer parlance",
    "what we are trying to achieve is a compression of the string and the minimal programme is the best compression that can be achieved .",
    "another name given to what we called `` randomness '' is _ complexity _ , and this particular measure is called kolmogorov algorithmic complexity . the same quantity , randomness , is also called information , because the more we can compress a string the less is the information content .",
    "information and randomness are then two sides of the same coin : the former expressing a positive aspect while the 2nd a negative one !",
    "let @xmath15 be a programme for the string of configuration @xmath16 and let us denote the length of any string by @xmath17 .",
    "the randomness or complexity is @xmath18 we now define _ a string as random _ , if its randomness or complexity is similar to the length of the string , or , to be quantitative , if randomness is larger than a pre - chosen threshold , e.g , say , @xmath19 .",
    "the choice of @xmath20 is surely arbitrary here and any number would do .",
    "a few things need to be mentioned here . _",
    "( i ) _ by definition , a minimal programme is random , because its size can not be reduced further . _",
    "( ii ) _ it is possible to prove that a string is _ not _ random by explicitly constructing a small programme , but it is not possible to prove that a string _ is _ random .",
    "this is related to gdel s incompleteness theorem .",
    "for example , the digits of @xmath21 may look random ( and believed to be so ) until one realizes that these can be obtained from an efficient routine for , say , @xmath22 .",
    "we may not have a well - defined way of constructing minimal algorithms , but we agree that such an algorithm exists . _ ( iii ) _ the arbitrariness in the choice of language leads to some indefiniteness in the definition of randomness which can be cured by agreeing to add a translator programme to all other programmes .",
    "this still leaves the differences of randomness of two strings to be the same .",
    "in other words , randomness is defined upto an arbitrary additive constant .",
    "entropy in classical thermodynamics also has that arbitrariness . _",
    "( iv ) _ such a definition of randomness satisfies a type of subadditivity condition @xmath23 , where the @xmath24 term can not be ignored .",
    "accepting that this kolmogorovian approach to randomness makes sense and since we connect randomness in a physical system with entropy , let us associate this randomness @xmath25 with the entropy of that string or configuration @xmath26 . for an ensemble of strings or configurations with probability @xmath2 for the @xmath7-th string or configuration @xmath27",
    ", the average entropy will be defined by @xmath28 ( taking the boltzmann constant @xmath29 ) .",
    "we shall claim that this is the thermodynamic entropy we are familiar with .    since the definition of entropy in eq .",
    "( [ eq:2 ] ) looks ad hoc , let us first show that this definition gives us back the results we are familiar with . to complete the story , we then establish the equivalence with the gibbs definition of entropy .",
    "consider the ising problem .",
    "let us try to write the free energy of a state with @xmath30 + spins and @xmath31 spins with @xmath32 .",
    "the number of such configurations is @xmath33 an ordered list ( say lexicographical ) of all of these @xmath34 configurations is then made .",
    "if all of these states are equally likely to occur then one may specify a state by a string that identifies its location in the list of configurations .",
    "the size of the programme is then the number of bits required to store numbers of the order of @xmath34 .",
    "let @xmath3 be the number of bits required . for general @xmath35 ,",
    "@xmath3 is given by @xmath36 stirling s approximation then gives @xmath37 , \\end{aligned}\\ ] ] with @xmath38 , the probability of a spin being up .",
    "resemblance of eq .",
    "( [ eq:4 ] ) with the boltzmann formula for entropy ( sec .",
    "[ sec : introduction ] ) should not go unnoticed here .",
    "( [ eq:1 ] ) is the celebrated formula that goes under the name of entropy of mixing for alloys , solutions etc .",
    "it is important to note that no attempt has been made for  minimalizations \" of the algorithm or in other words we have not attempted to compress @xmath34 . for example , no matter what the various strings are , all of the n spin configurations can be generated by a loop ( algorithm represented schematically )    ....       i = 0 10    i = i+1        l = length of i in binary       print 0   ( n - l ) times , then   \" i \" in binary       if   ( i < n ) go to 10       stop ....    by a suitable choice of @xmath13 ( _ e.g. _ , @xmath39 ) the code for representation of @xmath13 can be shortened enormously by compressing @xmath13 .",
    "this shows that one may generate all the spin configurations by a small programme though there are several configurations that would require individually much bigger programmes .",
    "this should not be considered a contradiction because it produces much more than we want .",
    "it is fair to put a restriction that the programmes we want should be self delimiting ( meaning it should stop without intervention ) and should produce just what we want , preferably no extra output .",
    "such a restriction then automatically excludes the above loop .",
    "secondly , many of the numbers in the sequence from @xmath40 to @xmath34 can be compressed enormously .",
    "however , what enumeration scheme we use , can not be crucial for physical properties of a magnet , and therefore , we do need @xmath3 bits to convey an arbitrary configuration .",
    "it is also reassuring to realize that there are random ( i.e. incompressible ) strings in @xmath41 possible @xmath13-bit strings .",
    "the proof goes as follows .",
    "if an @xmath13-bit string is compressible , then the compressed length would be @xmath42 .",
    "but there are only @xmath43 such strings . now",
    "the compression procedure has to be one to one ( unique ) or otherwise decompression will not be possible .",
    "hence , for every @xmath13 , there are strings which are not compressible and therefore random .",
    "a related question is the time required to run a programme .",
    "what we have defined so far is the `` space '' requirement .",
    "it is also possible to define a `` time complexity '' defined by the time required to get the output . in this note",
    "we avoid this issue of time altogether .      in the kolmogorov approach",
    "we can now write the free energy of any configuration , @xmath27 as @xmath44 with the thermodynamic free energy coming from the average over all configurations , @xmath45 if we now claim that @xmath3 obtained in eq .",
    "( [ eq:1 ] ) is the entropy of any configuration , and since no compression is used , it is the same for all ( this is obviously an approximation ) , we may use @xmath46 .",
    "the average energy may be approximated by assuming random mixture of up and down spins with an average value @xmath47 . if @xmath48 is the number of nearest neighbours ( @xmath49 for a square lattice ) , the free energy is then given by @xmath50.\\ ] ] note that we have not used the boltzmann or the gibbs formula for entropy . by using the kolmogorov definition",
    "what we get back is the mean field ( or bragg - williams ) approximation for the ising model .",
    "as is well - known , this equation on minimization of @xmath51 with respect to @xmath52 , gives us the curie - weiss law for magnetic susceptibility at the ferro - magnetic transition .",
    "no need to go into details of that because the purpose of this exercise is to show that the kolmogorov approach works .",
    "a more elementary example is the sckur - tetrode formula for entropy of a perfect gas .",
    "we use cells of small sizes @xmath53 such that each cell may contain at most one particle . for n particles we need @xmath54 numbers to specify a configuration , because each particle can be in one of @xmath55 cells .",
    "the size in bits is @xmath56 so that the change in randomness or entropy as the volume is changed from @xmath57 to @xmath58 is @xmath59 the indistinguishability factor can also be taken into account in the above argument , but since it does not affect eq .",
    "( [ eq:6 ] ) , we do not go into that . similarly momentum contribution can also be considered .",
    "it may be noted here that the work done in isothermal expansion of a perfect gas is @xmath60 where @xmath61 is the pressure satisfying @xmath62 and @xmath63 is defined in eq .",
    "( [ eq:6 ] ) . both eqs .",
    "( [ eq:6 ] ) and ( [ eq:7 ] ) are identical to what we get from thermodynamics .",
    "the emergence of @xmath64 is because of the change in base from @xmath65 to @xmath66 .",
    "it seems logical enough to take this route to the definition of entropy and it would remove much of the mist surrounding entropy in the beginning years of a physics student .",
    "for the computer problem mentioned in the introduction , one needs to ponder a bit about reality . in thermodynamics ,",
    "one considers a reversible engine which may not be practical , may not even be implementable . but a reversible system without dissipation can always be justified .",
    "can one do so for computers ?      to implement an algorithm ( as given to it ) , one needs logic circuits consisting of say and and nand gates ( all others can be built with these two ) each of which requires two inputs ( a , b ) to give one output ( c ) . by construction ,",
    "such gates are irreversible : given c , one can not reconstruct a and b. however it is possible , at the cost of extra signals , to construct a reversible gate ( called a toffoli gate ) that gives and or nand depending on a third extra signal .",
    "the truth table is given in appendix [ sec : toffoli - gate ] .",
    "reversibility is obvious .",
    "a computer based on such reversible gates can run both ways and therefore , after the end of manipulations , can be run backwards because the hardware now allows that . just like a reversible engine",
    ", we now have a reversible computer .",
    "all our references to computers will be to such reversible computers .",
    "let us try to formulate a few basic principles applicable to computers .",
    "these are rephrased versions of laws familiar to us .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ * law i * : it is not possible to have perpetual computation . _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    in other words , we can not have a computer that can read a set of instructions and carry out computations to give us the output _ without any energy requirement_. proving this is not straight forward but this is not inconsistent with our intuitive ideas .",
    "we wo nt pursue this .",
    "this type of computer may be called perpetual computer of type i. first law actually forbids such perpetual computers .    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ * law ii * : it is not possible to have a computer whose sole purpose is to draw energy from a reversible source , execute the instructions to give the output and run backward to deliver the energy back to source , and yet leave the memory at the end in the original starting state . _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    a computer that can actually do this will be called a perpetual computer of second kind or type ii .      in order to see the importance of the second law",
    ", we need to consider various manipulations on a file ( which is actually a string ) .",
    "our interest is in long strings ( length going to infinity as in thermodynamic limit in physics ) .",
    "now suppose we want to edit the file and change one character , say , in the 21st position .",
    "we may then start with the original file and add an instruction to go to that position and change the character . as a result",
    "the edit operation is described by a programme which is almost of the same length ( at least in the limit of long strings ) as the original programme giving the string .",
    "therefore there is no change in entropy in this editing process .",
    "suppose we want to copy a file .",
    "we may attach the copy programme with the file .",
    "the copy programme itself is of small size .",
    "the copy process therefore again does not change the entropy .",
    "one may continue with all the possible manipulations on a string and convince oneself that all ( but one ) can be performed at constant entropy .",
    "the exceptional process is _ delete or removal of a file_. there is no need of elaboration that this is a vital process in any computation . when we remove a file , we are replacing the entire string by all zeros - a state with negligible entropy .",
    "it is this process that would reduce the entropy by @xmath13 for @xmath13 characters so that in conventional units the heat produced at temperature @xmath0 is @xmath67 ( see eq .",
    "( [ eq:7 ] ) ) . we know from physics that entropy reduction does not happen naturally ( we can not cool a system easily ) .",
    "we can have a reversible computer that starts by taking energy from a source to carry out the operations but to run it backward ( via toffoli gates ) it has to store many redundant information in memory .",
    "even though the processes are iso - entropic and can be reversed after getting the output to give back the energy to the source ,",
    "we * no longer * have the memory in the same `` blank '' state we started with .",
    "to get back to that `` blank '' state , we have to clear the memory ( remove the strings ) .",
    "this last step lowers the entropy , a process that can not be carried out without help from outside .",
    "if we do not want to clear the memory , the computer will stop working once the memory is full .",
    "this is the second law that prohibits perpetual computer of second kind .",
    "the similarity with thermodynamic rules is apparent . to complete the analogy ,",
    "a computer is like an `` engine '' and memory is the fuel . from a practical point of view",
    ", this loss of entropy is given out as heat ( similar to latent heat on freezing of water ) .",
    "landauer in 1961 pointed out that the heat produced due to this loss of entropy is @xmath68 per bit or @xmath69 for @xmath13 bits .",
    "for comparison , one may note that @xmath70 is the total amount of entropy lost when an ising ferromagnet is cooled from a very high temperature paramagnetic phase to a very low temperature ferromagnetic phase .",
    "if the process of deletion on a computer occurs very fast in a very small region of space , this heat generation can create problem .",
    "it therefore puts a limit on miniaturization or speed of computation .",
    "admittedly this limit is not too realistic because other real life processes would play major roles in determining speed and size of a computer .",
    "see appendix [ sec : heat - generated - chip ] for an estimate of heat generated .",
    "let us now look at another aspect of computers namely transmission of strings ( or files ) or communication .",
    "this topic actually predates computers . to be concrete ,",
    "let us consider a case where we want to transmit images discretized into small cells of four colours , with probabilities @xmath71 the question in communication is : `` what is the minimal length of string ( in bits ) required to transmit any such image ? ''",
    "there are two possible ways to answer this question .",
    "the first is given by the kolmogorov entropy (= randomness = complexity ) while the second is given by a different powerful theorem called shannon s noiseless coding theorem . given a long string @xmath72 of say @xmath73 characters , if we know its kolmogorov entropy @xmath74 then that has to be the smallest size for that string .",
    "if we now consider all possible @xmath73 character strings with @xmath75 as the probability of the @xmath76th string , then @xmath77 is the average number we are looking for .",
    "unfortunately it is not possible to compute @xmath74 for all cases .",
    "here we get help from shannon s theorem .",
    "the possibility of transmitting a signal that can be decoded uniquely is guaranteed with probability 1 , if the average number of bits per character @xmath78 where @xmath2 s are the probabilities of individual characters .",
    "a proof of this theorem is given in appendix [ sec : proof - shann - theor ] .",
    "since the two refer to the same object , they are the same with probability 1 , _",
    "i.e. _ , @xmath79      the applicability of the shannon theorem is now shown for the above example . to choose a coding scheme , we need to restrict ourselves to _ prefix _ codes ( i.e. codes that do not use one code as the `` prefix '' of another code .",
    "as an example , if we choose @xmath80 , decoding can not be unique .",
    "e.g. what is 010 ? or ?",
    "nonuniqueness here came from the fact that ( @xmath81 ) has the code of ( @xmath40 ) as the first string or prefix .",
    "a scheme which is prefix free is to be called a prefix code .    for our original example",
    ", we may choose @xmath82 as a possible coding scheme to find that the average length required to transmit a colour is @xmath83 it is a simple exercise to show that any other method would only increase the average size .",
    "what is remarkable is that @xmath84 an expression we are familiar with from the gibbs entropy and also see in the shannon theorem .    in case",
    "the source changes its pattern and starts sending signals with equal probability @xmath85 we may adopt a different scheme with @xmath86 for which the average length is @xmath87 this is less than what we would get if we stick to the first scheme . such simple schemes may not work for arbitrary cases as , e.g. , for @xmath88 in the first scheme we get @xmath89 while the second scheme would give @xmath90 . in the limit of @xmath91 , we can opt for a simpler code @xmath92 one way to reduce this length is then to make a list of all possible @xmath93 strings , where @xmath94 in some particular order and then transmit the item number of the message .",
    "this can not require more than @xmath3 bits per character .",
    "we see the importance of the gibbs formula but it is called the shannon entropy .      it is to be noted that the shannon theorem looks at the ensemble and not at each string independently . therefore the shannon entropy @xmath95 is ensemble based , but as the examples of magnet or noninteracting gas showed , this entropy can be used to get the entropy of individual strings .    given a set , like the colours in the above example , we can have different probability distributions for the elements .",
    "the shannon entropy would be determined by that distribution . in the kolmogorov case",
    ", we are assigning an `` entropy '' @xmath74 to the @xmath76th long string or state but @xmath96 is determined by the probabilities @xmath75 s of the long strings which are in turn determined by the @xmath52 s of the individual characters . since both refer to the best compression on the average , they have to be equivalent . it should however be noted that this equivalence is only in the limit and is a probability 1 statement meaning that there are configurations which are almost not likely to occur and they are not counted in the shannon entropy . instead of the full list to represent all the configurations ( as we did in eqs .",
    "( [ eq:3 ] ) and ( [ eq:4 ] ) ) , it suffices to consider a smaller list consisting of the relevant or typical configurations .",
    "they are @xmath97 in number ( see appendix [ sec : proof - shann - theor ] for details ) , typically requiring @xmath3 bits per character .",
    "a physical example may illustrate this . even though all configuration of molecules in a gas are allowed and should be taken into account , it is known that not much harm is done by excluding those configurations where all the molecules are confined in a small volume in one corner of a room .",
    "in fact giving equal weightage to all the configurations in eq .",
    "( [ eq:4 ] ) is one of the sources of approximations of meanfield theory .",
    "we now try to argue that statistical mechanics can also be developed with the above entropy picture . to do so",
    ", we consider the conventional canonical ensemble , i.e. , a system defined by a hamiltonian or energy @xmath98 in contact with a reservoir or bath with which it can exchange only energy . in equilibrium",
    ", there is no net flow of energy from one to the other but there is exchange of energy going on so that our system goes through all the available states in phase space .",
    "this process is conventionally described by appropriate equations of motions but , though not done generally , one may think of the exchange as a communication problem . in equilibrium ,",
    "the system is in all possible states with probability @xmath2 for the @xmath7th state and is always in communication with the reservoir about its configuration .",
    "the communication is therefore a long string of the states of the system each occurring independently and identically distributed ( that s the meaning of equilibrium ) .",
    "it seems natural to make the hypothesis that nature picks the optimal way of communication .",
    "we of course assume that the communication is noiseless .",
    "the approach to equilibrium is just the search for the optimal communication .",
    "while the approach process has a time dependence where the `` time '' complexity would play a role , it has no bearing in equilibrium and need not worry us . with that in mind , we may make the following postulates :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \\(1 ) in equilibrium , the energy @xmath99 remains constant .",
    "+ ( 2 ) the communication with the reservoir is optimal with entropy @xmath95 .",
    "+ ( 3 ) for a given average energy , the entropy is maximum to minimize failures in communication . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the third postulate actually assures that the maximum possible number of configurations ( @xmath100 ) are taken into account in the communication process .",
    "no attempt has been made to see if these postulates can be further minimized .    with these sensible postulates ,",
    "we have the problem of maximizing @xmath3 with respect to @xmath2 s keeping @xmath101=constant and @xmath102 .",
    "a straight forward variational calculation shows that @xmath103 with @xmath104 being the standard partition function .",
    "the parameter @xmath105 is to be chosen properly such that one gets back the average energy .",
    "the usual arguments of statistical mechanics can now be used to identify @xmath105 with the inverse temperature of the reservoir .",
    "we have tried to show how the kolmogorov approach to randomness may be fruitfully used to define entropy and also to formulate statistical mechanics .",
    "once the equivalence with conventional approach is established , all calculations can then be done in the existing framework .",
    "what is gained is a conceptual framework which lends itself to exploitation in understanding basic issues of computations .",
    "this would not have been possible in the existing framework .",
    "this also opens up the possibility of replacing `` engines '' by `` computers '' in teaching of thermodynamics .",
    "* acknowledgments *    this is based on the c. k. majumdar memorial talks given in kolkata on 22nd and 23rd may 2003 .",
    "i was fortunate enough to have a researcher like prof .",
    "chanchal kumar majumdar as a teacher in science college .",
    "i thank the ckm memorial trust for organizing the memorial talk in science college , kolkata .",
    "the truth table of the toffoli gate is given below . with three inputs a , b , c ,",
    "the output in c@xmath106 is the and or nand operation of a and b depending on c=0 or 1 .",
    "a1 : toffoli gate    [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]",
    "the statement of shannon s noiseless coding theorem is :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ if @xmath107 is the minimal average code length of an optimal code , then @xmath108 where @xmath109 .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the adjective `` noiseless '' is meant to remind us that there is no error in communication .",
    "a more verbose statement would be    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ if we use @xmath110 bits to represent strings of @xmath13 characters with shannon entropy @xmath3 , then a reliable compression scheme exists if @xmath111 .",
    "conversely , if @xmath112 , no compression scheme is reliable .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the equivalence of the two statements can be seen by recognizing that @xmath3 need not be an integer but @xmath107 better be .",
    "let us first go through a heuristic argument to motivate shannon s coding theorem .",
    "suppose a source is emitting signals @xmath113 independently and identically distributed with two possible values @xmath114 with probability @xmath115 , and @xmath116 with probability @xmath117 .",
    "for a long enough string @xmath118 the probability is    @xmath119},\\end{aligned}\\ ] ]    because for large @xmath13 the number of expected @xmath120 is @xmath121 and @xmath40 is @xmath122 .",
    "this expression shows that the probability of a long string is determined by @xmath123,\\ ] ] the `` entropy '' for this particular problem .",
    "note the subtle change from eq .",
    "( [ eq:11 ] ) to eq .",
    "( [ eq:11b ] ) .",
    "this use of expectation values for large @xmath13 led to the result that most of the strings , may be called the `` typical '' strings , belong to a subset of @xmath93 strings ( out of total @xmath41 strings ) .",
    "let us define a typical string more precisely for any distribution .",
    "a string of @xmath13 symbols @xmath124 will be _ called typical _ ( or better @xmath125-typical ) if @xmath126 for any given @xmath127 .",
    "( [ eq:12 ] ) may also be rewritten as @xmath128 - s \\le \\epsilon\\ ] ]      now , for random variables @xmath27 , @xmath129 s , defined by @xmath130 , are also independent identically distributed random variables . it",
    "is then expected that @xmath131 , the average value of @xmath129 s , averaged over the string for large @xmath13 , should approach the ensemble average , namely , @xmath132 .",
    "this expectation comes from the law of large numbers that @xmath133    { \\buildrel{n\\rightarrow\\infty}\\over\\longrightarrow } \\",
    "1,\\ ] ] for any @xmath134 .",
    "this means that given an @xmath125 we may find a @xmath135 so that the above probability in eq .",
    "[ eq:14 ] is greater than @xmath136 . recognizing that @xmath137 eq .",
    "( [ eq:14 ] ) implies @xmath138 \\ge 1 - \\delta.\\ ] ] we conclude that the probability that a string is typical as defined in eqs .",
    "( [ eq:12 ] ) and ( [ eq:16 ] ) is @xmath136 .",
    "let us now try to estimate the number @xmath139 , the total number of typical strings .",
    "let us use a subscript @xmath140 for the typical strings with @xmath140 going from @xmath40 to @xmath139 .",
    "the sum of probabilities @xmath141 s of the typical strings has to be less than or equal to one , and using the definition of eq .",
    "( [ eq:12 ] ) , we have one inequality @xmath142 this gives @xmath143 .",
    "let us now get a lower bound for @xmath139 .",
    "we have just established that the probability for a string to be typical is @xmath136 . using the other limit from eq .",
    "( [ eq:12 ] ) we have @xmath144 which gives @xmath145 .",
    "the final result is that the total number of typical strings satisfies @xmath146 where @xmath147 can be chosen small for large @xmath13 .",
    "hence , in the limit @xmath148      now let us choose a coding scheme that requires @xmath149 number of bits for the string of @xmath13 characters .",
    "our aim is to convert a string to a bit string and decode it - the whole process has to be unique . representing the coding and decoding by ``",
    "operators '' @xmath150 and @xmath151 respectively , and any string by @xmath152 , what we want can be written in a familiar form @xmath153 where the last line is the equivalent `` pipeline '' in a unix or gnu / linux system .",
    "let s take @xmath154 .",
    "we may choose an @xmath125 such that @xmath155 .",
    "it is a trivial result that @xmath156 . here",
    "@xmath157 is the total number of possible bit strings .",
    "hence all the typical strings can be encoded .",
    "nontypical strings occur very rarely but still they may be encoded .    if @xmath158 , then @xmath159 and obviously all the typical strings can not be encoded .",
    "hence no coding is possible .",
    "this completes the proof of the theorem .",
    "as per a report of 1988 , the energy dissipation per logic operation has gone down from @xmath160 joule in 1945 to @xmath161 joule in 1980 s .",
    "( ref : r. w. keyes , ibm j. res .",
    "devel . * 32 * , 24 ( 1988 ) url : http://www.research.ibm.com/journal/rd/441/keyes.pdf ) for comparison , thermal energy @xmath162 at room temperature is of the order of @xmath163 joule .      a more recent example . for a pentium 4 at 1.6ghz ,",
    "if the cpu fan ( that cools the cpu ) is kept off , then during operations the cpu temperature may reach @xmath167 ( yes celsius ) as monitored by standard system softwares on an hcl made pc ( used for preparation of this paper ) ."
  ],
  "abstract_text": [
    "<S> a definition of entropy via the kolmogorov algorithmic complexity is discussed . as examples , </S>",
    "<S> we show how the meanfield theory for the ising model , and the entropy of a perfect gas can be recovered . </S>",
    "<S> the connection with computations are pointed out , by paraphrasing the laws of thermodynamics for computers . </S>",
    "<S> also discussed is an approach that may be adopted to develop statistical mechanics using the algorithmic point of view . </S>"
  ]
}