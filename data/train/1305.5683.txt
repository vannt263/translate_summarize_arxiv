{
  "article_text": [
    "astronomy is increasingly becoming a computationally intensive field due to the ever larger datasets delivered by observational efforts to map ever larger volumes and provide ever finer details of the universe . in consequence ,",
    "conventional methods are often inadequate , requiring the development of new data reduction techniques .",
    "the x/@xmath0-ray spectrometer , aboard the observatory , perfectly illustrates this trend .",
    "the telescope is dedicated to the analysis of both point - sources and diffuse emissions , with a high energy resolution  @xcite .",
    "its imaging capabilities rely on a coded - mask aperture and a specific observation strategy based on a dithering procedure  @xcite .",
    "after several years of operation , it also becomes important to be able to handle simultaneously all the data , in order , for example , to get a global view of the galaxy emission and to determine the contribution of the various emission components .",
    "the sky imaging with is not direct .",
    "the standard data analysis consists in adjusting a model of the sky and instrumental background to the data through a chi - square function minimization or a likelihood function maximization .",
    "the related system of equations is then solved for the intensities of both sources and background .",
    "the corresponding sky images are very incomplete and contain only the intensities of some selected sky sources but not the intensities in all the pixels of the image .",
    "hence , images obtained by processing small subsets of data simultaneously can not always be combined together ( co - added ) to produce a single image .",
    "instead , in order to retrieve the low signal - to - noise ratio sources or to map the low surface brightness `` diffuse '' emissions  @xcite , one has to process simultaneously several years of data and consequently to solve a system of equations of large dimension .",
    "grouping all the data containing a signal related to a given source of the sky allows to maximize the amount of information on this specific source and to enhance the contrast between the sky and the background .",
    "ideally , the system of equations that connects the data to the sky model ( where the unknown parameters are the pixels intensities ) should be solved for both source intensities and variability timescales .",
    "this problem , along with the description and treatment of sources variability , is the subject of another paper  @xcite .",
    "it is mandatory , for example when studying large - scale and weak structures in the sky , to be able to process large amounts of data simultaneously .",
    "the spatial ( position ) and temporal ( variability ) description of sources leads to the determination of several tens of thousands of parameters , if @xmath16 years of  data are processed at the same time .",
    "consequently , without any optimization , the systems to be solved can exceed rapidly the capacities of most conventional machines . in this paper",
    "we describe a technique for handling such large datasets .",
    "is a spectrometer provided with an imaging system sensitive both to point - sources and extended source / diffuse emission .",
    "the instrument characteristics and performance can be found in  @xcite and  @xcite .",
    "data are collected thanks to 19 high purity ge detectors illuminated by the sky through a coded - mask .",
    "the resulting field - of - view ( fov ) is @xmath1@xmath2 and the energy ranges from 20 kev to 8 mev .",
    "the instrument can locate intense sources with an accuracy of a few arc minutes  @xcite .",
    "the coded mask consists of elements which are opaque ( made of tungsten ) or transparent to the radiation .",
    "photons coming from a certain direction cast a shadow of the mask onto the detectors plane .",
    "the shadowgram depends on the direction of the source ( figure  [ fig : shadowgram ] ) .",
    "the recorded counts rate in each detector of the camera is the sum of the contribution from all the sources in the fov .",
    "the deconvolution consists of solving a system of equation which relates a sky model to the data through a transfer function . in the case of",
    ", the imaging properties rely on the coded aperture , but also on a specific observing strategy : the dithering .",
    "the reconstruction of all the pixels of the sky image enclosed in the fov is not possible from a single exposure .",
    "indeed , dividing the sky into @xmath1@xmath3 pixels ( the angular resolution ) , we obtain , for a @xmath2 fov,@xmath1@xmath4 unknowns . however , a single exposure contains only 19 data values which are the number of observed counts in the 19 ge detectors and does not permit us to obtain the parameters necessary to determine the model of the sky and background .",
    "the related system of equations is thus undetermined .",
    "the dithering observation technique aims to overcome this difficulty .    by introducing multiple exposures for a given field that are shifted by an offset that is small compared to the size of the fov , it is possible to increase the number of equations , by grouping exposures , until the system becomes determined and thus solvable .",
    "an appropriate dithering strategy  @xcite has been used where the spacecraft continuously follows a dithering pattern throughout an observation . in general",
    ", the pointing direction varies around a target by steps of @xmath3 within a five - by - five square or a seven - point hexagonal pattern .",
    "a pointing ( exposure ) lasts between 30 and 60 minutes .",
    "thus , the dithering allows to construct a solvable system of equations .",
    "however , in addition to the variable instrumental background , sources are also variable on various timescales ranging from hours ( roughly the duration of an exposure ) to years .",
    "this is not a major problem at high energy ( e @xmath5 100 kev ) , since there are only few emitting sources , whose intensities are rather stable in time with respect to the statistics . at lower energies ( e @xmath6 100 kev ) and in most cases , the source intensities vary during the time spanned by the all the exposures .",
    "the chi - square , of the associated least - square problem , for this group can be relatively high , if sources intensity variations are not taken into account . in spite of this",
    ", it is possible to include a model of the source intensity variations in the formulation of the problem and to re - optimize the system of equations accordingly  @xcite . nevertheless , including sources variability in the system of equations increases the number of unknowns to determine ( [ sec : material : foundation : problem ] ) since intensities , in each `` time - bin '' ( a segment of time where the intensity of a given source does not change statistically ) , are to be determined simultaneously along with the parameters which model the instrumental background .",
    "it is impossible from a single exposure ( 19 data values ) to obtain the sky image in the @xmath2 fov ; only a coarse image containing at most 19 sources can be obtained .",
    "this coarse image is under - sampled and contains information on only 19 pixels ( there is no information on the other pixels ) .",
    "hence , images can not always be combined together ( co - added ) to produce a single image .",
    "furthermore , crowded regions like the galactic center contain hundreds of sources and thus a single exposure can not provide the amount of information needed , even to build only a coarse image .",
    "the grouping of the exposures , by selecting all those that contain signal on a specific sky target , can provide the necessary information .",
    "the fov spanned by these exposures is large ( @xmath2 to @xmath7 ) and contains numerous sources .",
    "the signal ( counts and energies ) recorded by the camera on the 19 ge detectors is composed of contributions from each source in the fov plus the background . for @xmath8 sources located in the field of view",
    ", the data @xmath9 obtained from detector @xmath10 during an exposure ( pointing ) @xmath11 , for a given energy band , can be expressed by the relation : @xmath12 where @xmath13 is the response of the instrument for source @xmath14 ( function of the source direction relative to the telescope pointing axis ) , @xmath15 is the flux of source @xmath14 during pointing @xmath11 and @xmath16 the background both recorded during the pointing @xmath11 for detector @xmath10 .",
    "@xmath17 are the measurement errors on the data @xmath9 , they are assumed to have zero mean , to be independent and normally distributed with a known variance @xmath18 ( @xmath19 ) $ ] and @xmath20 ) .    for a given pointing @xmath11 ,",
    "@xmath9 , @xmath13 , and @xmath16 are vectors of @xmath21 ( say @xmath22 detectors or 19 for single events and up to 141 , when all the multiple events are used in addition to the single events  @xcite . ] ) elements . for a given set of @xmath23 exposures",
    ", we have a system of @xmath24 equations ( eq .",
    "[ eqn : expression ] ) . to reduce the number of free parameters related to background , we take advantage of the stability of relative counts rate between detectors and rewrite the background term as : @xmath25    where @xmath26 is a normalization coefficient per pointing related to the background intensity",
    ", @xmath27 is a background count rate pattern ( uniformity map ) on the camera for detector @xmath10 , and @xmath28 the effective observation time for pointing @xmath11 and detector @xmath10 . the number of parameters necessary to model the background reduces to @xmath23 if @xmath29 is assumed to be known .",
    "however , in some cases it can be determined while processing the data ( [ app : two : autoflfi ] ) . + the two extreme cases , in terms of number of parameters to be determined , are    * first , when the sources and background intensities are assumed to be constant throughout all the observation ( time spanned by the exposures ) , the relation between the data and the sky model can be written , omitting the detector indices , as @xmath30 the aim is to compute the intensities @xmath31 of the @xmath8 sources and the background relative intensity @xmath32 .",
    "therefore , the relation can be written in matrix form , as @xmath33 we can rewrite the system in a more compact form as @xmath34 where @xmath35 ( elements @xmath36 ) is an @xmath37 matrix and @xmath38 .",
    "the parameters to be determined , @xmath39 is a vectors of length @xmath40 . the data @xmath41 and the associated statistical errors @xmath42 are vectors of length @xmath43 .",
    "* second , if the background or the sources are variable on the exposure timescale , the number of unknowns ( free parameters ) of the set of @xmath24 equations is then @xmath44 ( for the @xmath8 sources and the background intensities , namely @xmath45 and @xmath32).this leads , unless the number of sources is small , to an underdetermined system of equations .",
    "fortunately , in real applications , many sources vary on timescales larger than the exposure .",
    "this leads to a further reduction of the number of parameters compared to the case where all sources vary on the exposure timescale .",
    "in addition , many point sources are weak enough to be considered as having constant flux within the statistical errors , especially for higher energies ( e @xmath5 100 kev )",
    ". then the @xmath46 parameters related to sources will reduce into @xmath47 parameters and , similarly , @xmath48 for the background .",
    "as these parameters have also a temporal connotation , they will hereafter be referred to as .",
    "if the source named or numbered @xmath49 is variable , then the total duration covered by the @xmath23 exposures is divided into @xmath50 sub - intervals where the source intensity can be considered as stable / constant regarding the data statistics .",
    "the solution @xmath51 is expanded in @xmath50 segments , it takes the value @xmath52 in segment k , and can be written in compact notation @xmath53 actually the instants @xmath54 correspond to the exposure acquisition time ( exposure number ) , with @xmath55=1 and @xmath56 .",
    "there is at least one and at most @xmath23 time segments for each source @xmath49 ( @xmath57 $ ] becoming a vector of length @xmath50 ) .",
    "the matrix @xmath35 ( eq .",
    "[ eqn : h0 ] ) is to be modified accordingly .    when expanding matrix @xmath35 , column @xmath49",
    "is expanded in @xmath50 new columns , hence the number of intensities ( unknowns ) increases .",
    "schematically @xmath35 ( @xmath37 ) is mapped into a matrix @xmath58 ( @xmath59 ) , @xmath60 being the sum of all sources intervals ( @xmath61 ) , that is the number of ( the index j=0 correspond to the background ) .",
    "matrix @xmath62 is related to the background while @xmath63 is related to the sources response .",
    "parameters @xmath64 and @xmath65 are related to background and source intensity variations with the time ( number of exposures ) .",
    "box i illustrates schematically how the matrix @xmath58 is derived from the matrix @xmath35 .",
    "@xmath66    finally , the relation between the data and the sky model , similarly as in eq .",
    "[ eqn : h0 ] , is @xmath67 physically , @xmath58 corresponds to the transfer function or matrix , @xmath68 to the data and @xmath69 to the unknown intensities ( sources plus background ) to be determined ( a vector of length n ) .",
    "taking into account the variability of sources and instrumental background increases the size of the system of equation and the number of unknowns , but also increases the sparsity of the matrix related to the system of equations , which means that the underlying matrices have very few non - zero entries . in our application ,",
    "the matrix @xmath35 is sparse , thus matrix @xmath58 is even sparser .",
    "objective methods to construct the matrix @xmath58 from @xmath35 are described in  @xcite .",
    "to give an idea , for the dataset which corresponds to ( @xmath16 years of data , the number of free parameters @xmath70 to be determined are between @xmath71 and @xmath72 depending on the energy band considered and hypotheses made on sources and background variability timescale ( [ sec : material : material ] ) .",
    "the material is related to the analysis of data accumulated between 2003 and 2009 with the spectrometer .",
    "the astrophysical application is the study of diffuse emission of the galaxy .",
    "the details can be found in  @xcite .",
    "the goal is to disentangle the `` diffuse '' emission ( modeled with 3 spatially extended components ) from the point - sources emission and instrumental background .",
    "we need to sample these large - scale structures efficiently over the entire sky and consequently use the maximum amount of data simultaneously , since a single exposure covers only one - hundredth of the total sky area .",
    "the datasets consist of 38699 exposures that yield @xmath73 data points . in most cases considered here ,",
    "the background intensity is considered to be quite stable on a @xmath16 hours timescale , which corresponds to @xmath74 unknowns .",
    "a.   the highest energy bands ( @xmath75 kev ) are less problematic in terms of number of parameters to determine , as illustrated by the 200 - 600 kev band .",
    "the sky model contains only 29 sources which are essentially not variable in time ( given the instrument sensitivity ) .",
    "the number of unknowns is @xmath76 .",
    "b.   the lowest energy bands ( @xmath77 kev ) are more problematic .",
    "we use the 25 - 50 kev band .",
    "the sky model contains 257 sources variable on different timescales .",
    "when the background intensity is assumed to vary on @xmath16 hours timescale , @xmath78  intensity are to be determined .",
    "+ in some configurations , essentially used to assess the results , background intensity and/or strongest variable sources vary on the exposure timescale , and the number of unknowns could be as high as @xmath79 to @xmath80 .",
    "nevertheless , the matrices associated with these problems remain relatively structured . c.   to avoid excessively structured matrices , we generate also matrices @xmath58 , with a variable number of columns , the number of segments @xmath50 for a particular source being a random number between 1 and @xmath23 .",
    "this results in a different number of parameters @xmath60 .",
    "another astrophysical application is the study of a particular source or sky region , here the crowded central region of the galaxy . in this case , it is possible to use a smaller number of exposures .",
    "we use 7147 exposures which cover a sky region of radius @xmath2 around the galactic center .",
    "we measure the intensity variations of a set of 132 sources .",
    "the number of parameters to determine @xmath81 is relatively small .",
    "details can be found in  @xcite . a second matrix , used for verification purposes , has @xmath82 .",
    "it corresponds to the case where some sources are forced to vary on shorter timescales .",
    "the material consists of rectangular matrices @xmath58 and symmetric square matrices @xmath83 ( @xmath84 ) related to the above physical problems ( [ sec : material : foundation : problem ] ) .",
    "the characteristics of some of these matrices are described in table  [ table : sparsity ] .",
    "the system we use in the experiments consists of an intel i7 - 3517u processor with 8  gb main memory .",
    "we ran the experiments on a single core , although our algorithms are amenable to parallelism .    [ table : sparsity ]      the mathematical problem described in section  [ sec : material : foundation : problem ] and developed in  [ sec : theory : methods : lsq ] requires the solution of several algebraic equations .",
    "first , if the chi - square statistic is used , a linear least - squares problem has to be solved to estimate the parameters of the model .",
    "second , elements ( entries ) of the inverse of a matrix have to be computed in order to determine the error bars ( variances of these parameters ) .",
    "third , in some cases , the parameters are adjusted to the data through a multi - component algorithm based on likelihood tests ( poisson statistics ) ; this problem leads to a non - linear system of equations ( [ app : two : mle ] ) .",
    "these three problems can be reduced to solving a linear system with a square matrix : a linear least - squares problem @xmath85 can be transformed into a square system @xmath86 by use of the normal equations ] ( @xmath87 and @xmath88 ) . similarly , computing entries of the inverse of a matrix amounts to solving many linear systems , as described in detail in section  [ sec : theory : mumpsinv ] . for the above mentioned non - linear problem",
    ", we chose a newton - type method ; this involves solving several linear systems as well .",
    "our problems are large , but sparse ( cf .",
    "table  [ table : sparsity ] ) , which justifies the use of sparse linear algebra techniques . in section  [ sec : theory : largesystem ] , we describe how we selected a method suitable for our application .      the system is , in most cases , overdetermined ( there are more equations - or measures here - than unknowns ) , therefore there is ( generally ) no exact solution , but a `` best '' solution , motivated by statistical reason , obtained by minimizing the following merit function , which is the chi - square :    @xmath89 ^ 2\\ ] ]    @xmath90 is vector of length @xmath43 representing the data , @xmath91 $ ] a diagonal matrix of order @xmath43 whose diagonal is ( @xmath92 ) , where @xmath93 is the measurement error ( standard deviation ) corresponding to the data point @xmath94 .",
    "these quantities are assumed to be known ( formally @xmath95 ) .",
    "@xmath96 is a matrix of size @xmath59 .",
    "the least - square solution @xmath97 is obtained by solving the following normal equation :    @xmath98 h)x = h^t[\\sigma^{-2 } ] y \\mathrm{~or~as~ } ax = b\\ ] ]    once the solution has been computed , the uncertainties on the estimated solution @xmath69 are needed as well .",
    "the corresponding variance can be obtained by computing the diagonal of @xmath99 : @xmath100",
    "sparse matrices appear in numerous industrial applications ( mechanics , fluid dynamics ,  ) , and the solution of sparse linear systems has been an active field of research since the 1960s .",
    "many challenges still arise nowadays , because industrial applications involve larger and larger number of unknowns ( up to a few billions nowadays ) , and because hardware architectures are increasingly complex ( multi - core , multi - gpu , etc . ) .    exploiting sparsity can significantly reduce the number of operations and the amount of memory needed to solve a linear system .",
    "let us take the example of a partial differential equation to be solved on a 2d physical domain ; the domain can be discretized on a @xmath101 2d grid and using , say , finite differences , the equation can be transformed into a sparse linear system with @xmath102 unknowns . without exploiting sparsity ,",
    "this system would be solved in @xmath103 operations ( using an exact method ) , with a memory usage in @xmath104 .",
    "it has been shown that , for this particular case , the number of arithmetic operations can be reduced to @xmath105 , and space complexity to @xmath106 by exploiting the sparsity of the matrix @xcite .",
    "many methods exist for solving sparse linear systems  @xcite .",
    "two main classes can be distinguished : _ direct methods _ , that rely on a matrix factorization ( e.g. , @xmath107 ) , and _ iterative methods _ , that build a sequence of iterates that hopefully converges to the solution .",
    "direct methods are known to be numerically robust but often have large memory and computational requirements , while iterative methods can be less memory - demanding and often faster but are less robust in general .",
    "iterative methods often need to be _ preconditioned _ , i.e. , to be applied to a modified system @xmath108 for which the method will converge more easily ; a trade - off has to be found between the cost of computing and using the preconditioner @xmath43 and how the preconditioner improves the convergence .",
    "the choice of a method is often complicated and strongly depends on the application . in our case , we choose to use a direct method for the following reasons :    * memory usage is often a bottleneck that prevents the use of direct methods , but with the matrices arising from our application , direct and iterative methods have roughly the same memory footprint .",
    "this is explained in the next section . *",
    "the matrices from our application are numerically challenging ; we found that unpreconditioned iterative methods ( we tried gmres ) have difficulties converging and that a direct method that does not implement robust numerical features is also likely to fail ( we illustrate this in section  [ sec : results ] ) .",
    "* we need to compute error bars , which amounts to solving a large number ( @xmath109 ) of linear systems with different right - hand sides but the same matrix .",
    "this is particularly suitable for direct methods ; indeed , once the matrix of the system is factored ( e.g. , @xmath107 ) , the factors can be reused to solve for different right - hand sides at a relatively inexpensive cost .",
    "we describe this in section  [ sec : theory : mumpsinv ] .    in this work ,",
    "we use the _ mumps _ ( multifrontal massively parallel solver ) package .",
    "mumps  @xcite aims at solving large problems on parallel architectures .",
    "it is known to be very robust numerically , by offering a large variety of numerical processing operations , and provides a large panel of features . in the following section ,",
    "we briefly describe how sparse direct methods work .",
    "we introduce the basic material needed to understand the algorithm used for the computation of error bars ( described in section  [ sec : theory : mumpsinv ] ) .",
    "direct methods are commonly based on gaussian elimination , with the aim to factorize the sparse matrix , say @xmath83 , of the linear system @xmath86 into a product of  `` simpler ''  matrices called _ factors_. typically , @xmath83 can be factored into @xmath107 where @xmath110 and @xmath29 are lower and upper triangular matrices respectively , or @xmath111 , where @xmath112 is a diagonal matrix if @xmath83 is symmetric ( which is the case in our application ) .",
    "sparse direct methods depend on the non - zero pattern of the matrix and are optimized in that sense ; specialized mathematical libraries for tridiagonal , banded , cyclic matrices are common .",
    "if the pattern is more complex , then the method usually consists of three phases : _ analysis _ , _ factorization _ and _",
    "solution_.      the _ analysis _ phase applies numerical and structural preprocessing to the matrix , in order to optimize the subsequent phases .",
    "one of the main preprocessing operations , called _ reordering _ , aims at reducing the _ fill - in _ ,",
    "i.e. , the number of non - zero elements which appear in the factors but do not exist in the initial matrix ; this step consists in permuting the rows and columns of the initial matrix in such a way that less fill - in will occur in the permuted matrix .",
    "table  [ tab : fillin ] shows the amount of fill - in for different problems coming from our astrophysical application when the matrices are permuted using the nested - dissection method . for each matrix",
    ", the number of non - zero elements in the original matrix @xmath83 and in the @xmath110 factor of the @xmath113 factorization of @xmath83 are reported .",
    "note that in our application , the fill - in is not very large : the number of non - zero elements in the factors is of the same order of magnitude as in the original matrix . as a result ,",
    "the use of sparse , direct methods is likely to provide a good scalability with respect to the size of the matrix produced by the application .",
    "moreover , this implies that , for our application , direct and iterative methods will have roughly the same memory requirements ; indeed , in an unpreconditioned iterative method , the memory footprint is mainly due to the storage of the matrix @xmath83 , while the major part of memory requirements of direct methods comes from the factors .",
    "note that , while our application exhibit low amount of fill - in , this not the case in other applications ; in many problems , especially those involving pdes on 3d physical domains , the number of non - zero coefficients in the factors can be as big as one hundred times more than in the original matrix . in this case , using an iterative method can be interesting memory - wise .",
    "an important step of the analysis phase is the _ symbolic factorization _ : this operation computes the non - zero pattern of the factors , on which the numerical factorization and the solution will rely .",
    "the symbolic factorization computes the structure of the factors by manipulating graphs , and also a structure called the _ elimination tree _ , a tree - shaped graph with @xmath60 vertices .",
    "this tree represents tasks dependencies for the factorization and the solution phases .",
    "we describe in more details the elimination tree since it is a key structure to explain and understand ( see section  [ sec : theory : mumpsinv ] ) how to accelerate the solution phase since computing entries in the inverse of the matrix corresponds to incomplete traversals of the elimination tree .",
    "figure  [ fig : etreea ] shows an elimination tree and we use it to illustrate some definitions : one of the nodes is designated to be the _ root _ ; in the figure , this is node @xmath114 . for our purpose ,",
    "the root is the node corresponding to the variable of the linear system that is eliminated last .",
    "ancestor _ of a vertex @xmath115 is a vertex on the path from @xmath115 to the root .",
    "the _ parent _ ( or _ father _ ) of @xmath115 is its first ancestor ; all the nodes but the root have a parent .",
    "for example , on the figure , nodes @xmath114 and @xmath116 are ancestors of @xmath117 ; @xmath116 is the parent of @xmath117 .",
    "a _ child _ of a vertex @xmath115 is a vertex of which @xmath115 is the parent .",
    "for example , @xmath117 and @xmath118 are the children of @xmath116 .",
    "a vertex without children is called a _ leaf _ ; @xmath119 and @xmath120 are leaves .",
    "_ descendants _ of a vertex @xmath115 are all the nodes in the subtree rooted at @xmath115 ; for example , @xmath119 , @xmath120 , @xmath118 and @xmath117 are descendants of @xmath116 .    in the following subsections ( _ factorization _ and _ solution phase _ )",
    ", we describe briefly how a sparse direct solver uses elimination trees ; we will also rely on this notion in section  [ sec : theory : mumpsinv ] for the computation of error bars .",
    "further details about the construction and the role of elimination trees in sparse solvers are given in  @xcite .",
    "after the preprocessing performed during the analysis phase , the numerical factorization takes place and the matrix @xmath83 is transformed into a product of factors ( e.g. , @xmath121 ) .",
    "the factorization consists in traversing the elimination tree following a _ postorder _ , that is a _ topological ordering _",
    "( i.e. each parent is visited after its children ) where the nodes in each subtree are visited consecutively . in figure",
    "[ fig : etreea ] , 1 - 4 - 2 - 3 - 5 - 6 is , for example , a postorder . at each node",
    ", a partial factorization of a dense matrix is performed .",
    "note that nodes that belong to different branches can be processed independently , which is especially useful in a parallel setting .",
    "the factorization phase tries to follow as much as possible the preparation from the analysis phase , but sometimes , because of numerical issues ( typically , division by a `` bad pivot '' , i.e. a very small diagonal entry that could imply round - off errors ) , it has to adapt dynamically : the structure of the factors and the scheduling of the tasks can be modified on the fly .      once the matrix has been factored , the linear system is solved .",
    "for example , in the case of the @xmath121 factorization , the system @xmath86 becomes @xmath122 and is solved in two steps ( two solutions of triangular systems ) :    @xmath123    the forward substitution follows a bottom - up traversal of the elimination tree as in the factorization , while the backward substitution follows a top - down traversal of the tree . at each node , one component of the solution is computed , and some updates are performed on the dependent variables ( ancestor nodes for the forward phase , descendant nodes for the backward phase ) .      in our astrophysical application , once the solution , either for the linear or non - linear problem , has been found , it is necessary to compute the variances of the parameters of the fitted function . in the case of multiple regressions such as least squares problems ,",
    "the standard deviation of the solution can be obtained by inverting the hessian or covariance matrix . however , since the inverse of a sparse matrix is structurally full , it is impractical to compute or store it  @xcite . in our case , the whole inverse of the covariance matrix is not required : since we only want the variances of the parameters ( not their covariances ) , we only need to compute the diagonal elements of the inverse .",
    "some work has been done since the 1970s in order to compute a subset of elements of the inverse of a sparse matrix .",
    "one of the first works is @xcite which has been extended in  @xcite ; this approach relies on a direct method ( i.e. on a factorization ) .",
    "an iterative method has been proposed in  @xcite for matrices with a decay property .",
    "some methods have also been developed for matrices arising from specific applications ; a more detailed survey is given in @xcite .",
    "many of these methods provide sophisticated ideas and interesting performance on specific problems , but no software package is publicly available , with the exception of the approach implemented within mumps solver , that we describe in the next section .      the @xmath99 feature in mumps has been described in @xcite and was motivated by the application , among other applications that require the computation of inverse entries , or , more generally , applications that involve sparse right - hand sides ( as explained in this section ) .",
    "this feature is able to compute any set of entries of @xmath99 , relying on a traditional solution phase , i.e. by computing every required entry @xmath124 as :    @xmath125    using the @xmath126 factors of @xmath83 , this amounts to solving two triangular systems : @xmath127    the first triangular system in the equation above is particular because its right - hand side @xmath128 is very sparse ( only one non - zero entry ) .",
    "furthermore , we do not need the whole solution of the second triangular system , but only one component .",
    "this information can be exploited to reduce the traversal of the elimination tree ; while a regular solution phase would consist in visiting the whole elimination tree twice ( a bottom - up traversal followed by a top - down traversal ) , computing @xmath124 consists in two partial traversals of the tree : the first triangular system is solved by following the path from node @xmath14 to the root node , and the second triangular system is solved by following the path from the root node to node @xmath129 ; this is referred to as _ pruning _ the elimination tree . since each node of the tree corresponds to operations to be performed ( arithmetic operations , or expensive accesses to the factors in the out - of - core case ) , this leads to significant improvements in computation time .",
    "moreover , since we do not have to manipulate dense solution vectors , this also leads to significant savings in memory usage .",
    "we illustrate this technique in figure  [ fig : ainv23 ] : entry @xmath130 is required , thus the only nodes of tree that have to be visited lie on the path from node @xmath118 to the root node ( @xmath114 ) and on the path from the root node to node @xmath120 .",
    "therefore , one does not have to perform operations at nodes @xmath117 and @xmath119 .    .",
    "the traversal of the tree is reduced to the path from @xmath118 to @xmath114 and the path from @xmath114 to @xmath120 ; no computation is performed at nodes @xmath119 and @xmath117.,scaledwidth=25.0% ]    when many entries of the inverse are requested , they can not generally be computed all at once ( mainly because of memory usage ) , but they can be computed by blocks , which allows to take advantage of efficient dense linear algebra kernels . work has been performed in order to find optimal ways to form the blocks in different contexts @xcite and to improve the parallel efficiency .",
    "a substantial time is spent in computing @xmath87 with a basic algorithm .",
    "the use of an appropriate algorithm to perform the operation @xmath87 helps to reduce the computation time ( see section  [ sec : calculation : hess ] ) .",
    "the mumps solver is used to solve the system of equations as described in section  [ sec : calculation : mumpspackage ] .",
    "finally , the error bars on the solution are computed , which means the calculation of the diagonal elements of inverse matrix .",
    "the new @xmath99 feature of mumps is compared with several algorithms , in terms of computation time in section  [ sec : calculation : errbars ] .",
    "the computation of the normal equation @xmath87 is of paramount importance in many problems , yet is a very challenging operation due to the considerable amount of symbolic operations needed to compute the sparsity structure of @xmath83 .",
    "for this reason efficient algorithms have been developed in the past . to perform this operation we decided to use part of a larger code developed by  @xcite for computing the qr factorization of sparse matrices .",
    "the used part was originally developed to compute only the structure of the @xmath83 matrix and , thus , we had to extend it in order to compute the coefficients values .",
    "this was possible thanks to the help of the original code developer .",
    "one important feature of this code offers the possibility to update the elements of @xmath83 that are changed after modification of some numerical values of the columns of @xmath58 without recomputing the whole matrix ( the technique used to compute simultaneously the solution and the background pattern in the algorithm is described in  [ sec : material : foundation : problem ] and  [ app : two : autoflfi ] ) .",
    "lrrr matrix & & + @xmath87 & algorithm@xmath131 & algorithm@xmath132 +   + full matrix & 28.2 & 5779 + 5000 h columns modified & 0.43 & 258 +   + full matrix & 41.2 & 18585 + 5000 h columns modified & 0.13 & 31.7 +    table  [ table : simpleops ] shows the time reduction achieved for both the computation of @xmath87 and its update after the modifications of some columns of @xmath58 .",
    "the results in the first column are obtained with the code extracted from the software package by  @xcite and improved as suggested by the author .",
    "the results on the second column , instead , are obtained by computing @xmath60 matrix vector products where , for each product , a dense vector of length @xmath43 ( with many zero elements ) corresponding to a column of @xmath58 is built in place .",
    "the gain over a simple basic algorithm is significant ( a factor @xmath1300 ) and demonstrates the interest of using specialized libraries dedicated to sparse matrix computations .",
    "here we briefly illustrate the interest of exploiting sparsity of the matrix when solving a linear system . in table",
    "[ table : solvetime ] , we compare the time for solving linear systems arising from our application using a dense solver ( lapack  @xcite ) and a sparse solver ( mumps ) .",
    "times are in seconds and include the @xmath113 factorization of a symmetric matrix @xmath83 of order @xmath133 and the computation of the solution @xmath69 of the system @xmath86 ( @xmath69 and @xmath134 are vectors of length @xmath135 ) . in the results related to mumps , the time for the analysis phase is included . in the second row of the table , instead , the matrix is treated as dense , hence its full storage is used and no analysis phase is performed . for the largest two problems , the dense algorithm can not be used as the memory requirements are roughly 23  gb and 167  gb respectively .",
    "we can extrapolate that on this system , the run time would be around 22 hours for the largest problem ( instead of 6.7 seconds using a sparse algorithm ) .",
    ".[table : solvetime]times ( in seconds ) for the computation of the solution . [ cols=\"<,>,>,>,>,>\",options=\"header \" , ]     the results in table  [ table : solvetime ] confirm that sparse , direct solvers achieved a good scalability on the problems of our target application whereas dense linear algebra kernels quickly exceed the limit of modern computing platforms .      in this section we present experimental results related to the computation of error bars or , equivalently , of the diagonal entries of the inverse matrix @xmath99 .",
    "our approach that relies on the pruned tree , presented in section  [ sec : theory : mumpsinv ] , is compared to the basic , left - looking approach described in @xcite . in the case of a symmetric matrix , this approach computes the diagonal entries of the inverse matrix as @xmath136 where we denoted with @xmath124 and @xmath137 the coefficients of @xmath99 and @xmath138 , respectively .",
    "this amounts to computing , one at a time , the columns of @xmath139 and then summing the corresponding contribution onto the @xmath140 coefficients . in this algorithm ,",
    "the sparsity of the right - hand side and of the factor matrix @xmath110 is exploited but not completely , and the experimental results discussed below show that this results in a higher execution time .",
    "furthermore , because of memory issues , this simple algorithm does not allow to simultaneously compute many diagonal entries of @xmath99 ; clearly this is also a limiting factor for performance .",
    "our implementation of this method is based on the ldl package  @xcite . as a second term of comparison we also provide experimental results for a brute force approach with no exploitation of sparsity of the right - side and solution vectors . for this purpose , we use directly the mumps package and solve several systems of equations in order to compute the inverse matrix . in addition , we analyze the influence of grouping the computation of the diagonal entries ( 1 right - hand - side ( rhs ) at a time or 128 at a time ) .",
    "the experimental results for the three methods described above are reported in table  [ table : computeinverse ] . for the sake of this comparison ,",
    "all these methods are executed in sequential mode although the code of the brute force approach and of the mumps @xmath99 feature are parallel .",
    "the experiments were carried out on the above - mentioned system .",
    "these results show that the brute force algorithm becomes competitive with respect to the simple algorithm when the entries are processed by blocks .",
    "the mumps  @xmath99 feature described in  [ sec : theory : mumpsinv ] is significantly faster than all other approaches and the gain increases with the size of the problem .",
    "pruning leads to clear gains over a strict traditional solution phase .",
    "the gain is even larger for the largest problem due to the good scalability of the @xmath99 algorithm with respect to the problem size .",
    "the simple , left - looking approach shows reasonable performance for small problems , but could not be tested on our largest matrix because numerical pivoting , not available in ldl package , is needed during factorization to obtain an accurate solution .",
    "[ table : computeinverse ]",
    "the mumps solver and its  @xmath99 functionality are the core tools to solve systems of equations related to the measurements of the sources intensity .",
    "figure  [ fig : diffuse ] shows the application to the determination of the different components of the galaxy spectrum .",
    "the related analysis is performed in 24 consecutive energy bands in order to extract counts spectra .",
    "the counts spectra are then converted into photon spectra .",
    "the details are given in  @xcite .",
    "another application is the study of the intensity variations of a peculiar source or sky region .",
    "figure  [ fig : datah1743fov ] shows the intensity in function of the time ( exposure ) of some of the sources located in the central crowded region of the galaxy . for this application , the end and start of the",
    "are determined by a segmentation algorithm , which is based on the efficient @xmath113 factorization of symmetric matrix provided by mumps , details can be found in  @xcite .",
    "we have demonstrated that even for the basic operation such as sparse matrix product , it is better to use dedicated algorithms or libraries ( [ sec : calculation : hess ] ) .",
    "+ the mumps solver is very effective on the sparse matrix structure arising from astrophysical problems encountered with .",
    "this solver is robust and the matrix factorization is stable against rounding errors .",
    "it provides many numerical preprocessing options and implements robust pivoting strategies , which make it one of the most numerically stable solvers available .",
    "the matrices arising from the   application are symmetric and indefinite ; they are not    extremely challenging numerically , but they do require two - by - two numerical pivoting for stability ( in table [ table : computeinverse ] the ldl package could not provide an accurate solution on our largest matrix ) .",
    "the proposed approach not only leads to substantial time reduction but also enables the resolution of large sparse system of equations which could not be solved using basic algorithms .",
    "other sparse linear systems solvers exists and have been used to validate the performance reported in the experimental section ; for an exhaustive list see for example  @xcite , but they all lack a function to compute also the error bars on the solution quickly , which is mandatory in our astrophysical application .",
    "the @xmath99 feature in mumps ( computation of selected inverse entries ) did not exist before the beginning of this study , the application was actually one of the motivating applications for developing techniques for the computation of inverse entries , and for releasing a publicly available code .",
    "this functionality allows to compute easily and rapidly the error bars on the solution .",
    "the gain in time over already optimized algorithms is important .    among other methods to solve the problem completely , solution and error bars , one should mention alternative methods such as monte carlo markov chains  @xcite or simulated annealing @xcite .",
    "such advanced statistical tools can provide the best fit and the variances of the solution of both linear and non - linear systems of equations .",
    "in particular mcmc methods could be useful when computing error bars , in case of complex constraints on the function",
    ". however , these methods may be very prohibitive in time , especially if high precision on the parameters is required ; they have in general a weak or non - guaranteed convergence and are not the best suited for our needs , given the complexity of our problem .",
    "we have developed algorithms to process years of data and to enhance the production of hard x / soft @xmath0-ray survey catalogs .",
    "these methods have been successfully applied to a set of @xmath16 years of data  @xcite .",
    "we have shown that , for processing efficiently and accurately years of data , it is critical to use algorithms that take advantage of the sparse structure of the transfer function ( matrix ) , such as those implemented in the mumps software .",
    "it was also demonstrated that error bars can be obtained at a relatively inexpensive cost ( the same order of magnitude as a simple problem solution ) thanks to a recently developed algorithmic feature that efficiently computes selected entries of the inverse of a matrix . in addition , thanks to many efforts in optimization , gains are achieved both in memory usage and in computation time .",
    "hence , it is possible to process large datasets in a reasonable time and to compute the diagonal of the covariance matrix , and thus error bars , in a rather short time .",
    "more generally , the ideas described here can be applied to a large variety of problems .",
    "finally , we are today able to solve sparse linear systems with millions , sometimes billions , of unknowns .",
    "although we have not used explicitly parallel computing but instead performed many sequential computations at the same time ( for each energy band , etc .. ) , the proposed approach can also be used directly in a parallel setting on massively parallel machines .    in the near future",
    ", instruments will commonly create datasets of a few tens to a few hundreds of terabytes for a single observation project .",
    "many of the current tools and techniques for managing large datasets will not scale easily to meet this challenge . surveys of the sky already require parallel computing in order to be performed .",
    "new surveys will demand orders of magnitude increases in the available data and therefore in data processing capabilities .",
    "it is also a challenge for scientists who need to extract a maximum of science from the data .",
    "exciting scientific breakthroughs remain to be achieved as astronomers manipulate and explore massive datasets , but they will require advanced computing capabilities , infrastructure and algorithms .",
    "the project has been completed under the responsibility and leadership of cnes .",
    "we are thankful to asi , cea , cnes , dlr , esa , inta , nasa and ostc for support .",
    "we are very grateful to chiara puglisi , research engineer at inpt - irit , for her contribution to improve the performance of the @xmath141 product in section  [ sec : calculation : hess ] .",
    "in the case of a low number of counts , it is recommended to use the mle of the solution instead of the @xmath142 solution . following  @xcite , we maximize the likelihood function ,    @xmath143    where @xmath144 is the model of the data obtain through the relation @xmath145 .",
    "to optimize this non - linear problem , potentially with bound constraints ( such as positivity of the solution ) , there are at least four approaches :    a.   newton type methods ( or modified newton methods ) : they use the hessian matrix to define a search direction and hence need the solution of a large linear system of equations at least at each few iterations .",
    "they are the most powerful methods available and can find the solution in a few iterations .",
    "b.   quasi - newton methods : they build an approximation of the hessian at each iteration .",
    "they optimize a quadratic function in at most @xmath146 iterations ( @xmath146 being the number of unknowns ) . c.   conjugate - gradient methods : unlike the newton - type and quasi - newton methods , conjugate gradients methods do not require the storage of an @xmath146 by @xmath146 hessian matrix and thus are suited to solve large problems .",
    "the gradient of the function ( a vector of length @xmath146 ) is used to define the search direction .",
    "they are not usually as reliable or efficient as newton - type methods and often a relatively large number of iterations has to be performed before obtaining an acceptable solution .",
    "d.   simplex  @xcite , simulated annealing  @xcite or monte carlo markov chain ( mcmc )  @xcite can also be considered , but they are often prohibitive in time .",
    "methods ( a ) and ( b ) are known as order-2 optimization methods ( gradient and hessian used ) , ( c ) as an order-1 optimization method ( gradient used ) , while method ( d ) can use only the function value .    to use a newton type method ( order-2 )",
    ", we need to compute the gradient @xmath147 and the hessian @xmath148 of the function    @xmath149 h \\end{split}\\ ] ]    @xmath150 $ ] is a diagonal matrix of order @xmath43 whose diagonal is ( @xmath151,  ,@xmath152 ) . as for the lsq case , the variance of the solution is obtained thanks to the inversion of the hessian matrix ( note that in the limit @xmath153 , the likelihood ( @xmath148 ) and chi - square ( @xmath83 ) hessian matrices are the same ) . a guess solution to this non - linear optimization problem is the lsq solution .",
    "the fitting algorithm , based on the likelihood test statistic , is a non - linear optimization problem . to optimize a non - linear problem ,",
    "potentially with bound constraints , a newton type method , known for its efficiency and reliability can be used , as we already have a solver for large sparse systems at hand . a software package for large - scale non - linear optimization such as ipopt ( interior point optimizer ) can be used .",
    "ipopt uses a linear solver such as mumps or ma57  @xcite as a core algorithm . for more details on this large - scale non - linear algorithm ,",
    "see @xcite .",
    "a few similar software packages for large - scale non - linear optimization exist , among them lancelot  @xcite , minos  @xcite and snopt  @xcite .",
    "sometimes the `` empty - field '' or `` uniformity map '' @xmath29 has to be computed with the solution . in order to preserve the linearity of the problem , we have adopted the algorithm described below .",
    "we consider that if the solution @xmath69 is known , @xmath154 coming back to the detector and pointing number    @xmath155    in the above formula @xmath156 is the counts due to the sources , assumed to be known .",
    "@xmath157 is the background contribution , @xmath32 is assumed to be known and @xmath29 is to be estimated . at this stage , using the model of the sky described by  [ eqn : a4 ] , a rough estimate of the pattern is @xmath158 .",
    "for the lsq statistic , we wish to minimize the following quantities for each of the working detectors ,    @xmath159    the lsq solution @xmath160 is    @xmath161    for the mle statistic , we do not have to preserve the linearity of the problem and hence the computation of the improved `` empty - field '' pattern can be done during the non - linear optimization process . on another side ,",
    "the algorithm is simplified if we proceed similarly as in the lsq case .",
    "then , we wish to maximize the following quantities for each of the working detectors ,    @xmath162 \\right)\\\\ & \\mathrm{~for~d=1, ... ,n_p } \\end{split}\\ ] ]    the mle solution @xmath163 is @xmath164    one should mention that it is possible to compute , similarly , an `` empty - field '' pattern on some restricted time interval instead of the whole dataset ; the best `` empty field '' for pointing intervals @xmath165 to @xmath166 is then ,    @xmath167      a sub - optimal algorithm to obtain both the sources and the background fluxes , as well as the improved `` empty - field '' pattern is described in algorithm  [ table : algo_lsq_iterback ] .",
    "we start with an approximation @xmath168 and apply some iterative refinement . in practice",
    ", the algorithm converges in a few iterations .",
    "@xmath169 , compute the structure of the hessian ( @xmath83 or @xmath148 ) compute lsq or mle solution compute a new approximation of @xmath29 by minimizing again lsq or maximizing mle statistics update @xmath58 ( the first @xmath170 columns of @xmath58 and update the new hessian matrix ( sec .",
    "[ sec : calculation : hess ] ) ) if @xmath142 stops decreasing or the likelihood function stops increasing then go to step 8 compute @xmath58 at the solution ( if not already done ) and the diagonal of @xmath171 to obtain the uncertainties on the solution    99    amestoy , p. r. , duff , i. s. , koster , j. & lexcellent , j .- y .",
    ",  2001 , `` a fully asynchronous multifrontal solver using distributed dynamic scheduling '' , siam journal of matrix analysis and applications 23 ( 1 ) : 1541 .",
    "amestoy , p. r. , guermouche , a. , lexcellent , j .- y .",
    "& pralet , s. ,  2006 , `` hybrid scheduling for the parallel solution of linear systems '' , parallel computing 32 ( 2 ) : 136156 .",
    "amestoy , p. r. , duff , i. s. , lexcellent , j .- y . , robert , y. , rouet , f .- h . & uar , b. ,  2012 , `` on computing inverse entries of a sparse matrix in an out - of - core environment '' , siam journal on scientific computing 34 ( 4 ) : 1975 - 1999 .",
    "anderson , e. , bai , z. , dongarra , j. , greenbaum , a. , mckenney , a. , du croz , j. , hammerling , s. , demmel , j. , bischof , c & and sorensen , d. ,  1990 , `` lapack : a portable linear algebra library for high - performance computers '' , proceedings of the 1990 acm / ieee conference on supercomputing , 211 .",
    "bai , z. , demmel , j. , dongarra , j. , ruhe , a. & van der vorst , h. 2000 , `` templates for the solution of eigenvalue problems : a practical guide '' , siam , philadelphia , 2000 .",
    "bobin j. , starck j .- l . & ottensamer r. ,  2008 , ieee sel .",
    "signal proc .",
    ", 2 , 718    bouchet , l. , roques , j.  p. & jourdain , e. , 2010 , , 720 , 177 .",
    "bouchet , l. , strong , a. , porter , t.a , et al .",
    ",  2011 , , 739 , 29 .",
    "bouchet , l. , amestoy , p. r. , buttari , a. , rouet , f .- h . & chauvin , m. ,  2013 , , in press ( http://dx.doi.org/10.1051/0004-6361/201219605 )    campbell , y. e. & davis , t. a. ,  1995 , `` computing the sparse inverse subset : an inverse multifrontal approach '' , tech . rep .",
    "tr-95 - 021 , cise dept .",
    ", univ . of florida .",
    "cash , w. ,  1979 , , 228 , 939 .",
    "conn , a. r. , gould , i. m. , & toint , p. l. ,  1996 , `` numerical experiments with the lancelot package ( release a ) for large - scale nonlinear optimization '' , mathematical programming , 73(1 ) , 73110 .",
    "davis , t. a. ,  2005 , `` user guide for ldl , a concise sparse cholesky package '' , tech .",
    ", cise dept .",
    ", univ . of florida .",
    "dubath , p. , kndlseder , j. , skinner , g. k. , et al . ,  2005 , , 357 , 420 .",
    "duff , i. s. , erisman , a. m. , gear , c. w. & reid , j. k. ,  1988 , `` sparsity structure and gaussian elimination '' , acm signum newsletter 23 ( 2 ) , 2 - 8 .",
    "duff , i. s. , erisman a. m. & reid , j. k. ,  1989 , `` direct methods for sparse matrices '' .",
    "oxford university press .",
    "duff , i. s. & reid , j. k. , 2004 , `` ma57 - a code for the solution of indefinite sparse symmetric linear systems '' , acm transactions on mathematical software , 30 , 118 .",
    "gill , p. e. , murray , w. , & saunders , m. a. ,  1997 , `` snopt : an sqp algorithm for large - scale constrained optimization '' .",
    "technical report sol97 - 3 , department of eesor , stanford university , stanford , california 94305 , usa , 1997 .",
    "hastings , w. k. ,  1970 , biometrika 57 ( 1 ) : 97109 .",
    "hoffman , a. j. , martin , m. s. & rose , d. j. ,  1973 , `` complexity bounds for regular finite difference and finite element grids '' , siam journal on numerical analysis 10 ( 2 ) : 364369 .",
    "jensen , p.  l. , clausen , k. , cassi , c. , et al .",
    ",  2003 , , 411 , l7 .",
    "kirkpatrick , s. , gelatt , c. d. & vecchi , m. p. ,  1983",
    ", science , new series , 220 ( 4598 ) , 671680 .",
    "liu , j. w. h. ,  1990 , `` the role of elimination trees in sparse factorization '' , siam journal on matrix analysis and applications , 11 ( 1 ) : 134172 .",
    "metropolis , n. , rosenbluth , a.w . ,",
    "rosenbluth , m.n . ,",
    "teller , a.h . & teller , e. ,  1953 ) , journal of chemical physics 21 ( 6 ) : 10871092 .",
    "murtagh , b. a. , & saunders , m. a. ,  1982 , `` a projected lagrangian algorithm and its implementation for sparse nonlinear constraints '' , mathematical programming study 16 ( constrained optimization ) , 84 - 117 .",
    "neal , r. m. ,  1993 , `` probabilistic inference using markov chain monte carlo methods '' , technical report crg - tr-93 - 1 , department of computer science , university of toronto .",
    "nelder , j. , & mead , r. ,  1965 , computer journal , vol .",
    "7 , no 4 , 308313 .",
    "puglisi , c. ,  1993 , `` qr factorization of large sparse matrix overdetermined and square matrices with the multifrontal method in a multiprocessing environment '' , phd thesis , institut national polytechnique de toulouse , toulouse , france .",
    "roques , j.  p. , schanne , s. , von kienlin , a. , et al . ,  2003 , , 411 , l91 .",
    "saad , y. ,  1996 , `` iterative methods for sparse linear systems '' : pws publications .",
    "slavova , t. ,  2009 , `` parallel triangular solution in an out - of - core multifrontal approach for solving large sparse linear systems '' , phd thesis , institut national polytechnique de toulouse , france .",
    "stewart , g. w. ,  1998 , `` matrix algorithms'',siam press , pennslyvania , 1998 .",
    "takahashi , k. , fagan , j. , and chen , m.s .",
    ",  1973 , `` formation of a sparse bus impedance matrix and its application to short circuit study '' , in power industry computer applications conference , 6369 .",
    "tang , j. , & saad , a. ,  2009 , `` a probing method for computing the diagonal of the matrix inverse '' , tech .",
    "report umsi-2010 - 42 , minesota supercomputer institute , university of minnesota , minneapolis , mn , 2009 .",
    "ubertini , p. , lebrun , f. , di cocco , g. , et al . ,  2003 , , 411 , l131 .",
    "vedrenne , g. , roques , j.  p. , schonfelder , v. , et al . ,",
    "2003 , , 411 , l63 .",
    "wchter , a. and biegler . , l.  t. ,  2006 , mathematical programming , 106(1),2557 .",
    "wiaux y. , jacques l. , puy g.m scaife a. m. m. , vandergheynst p ,  2009 , , 395 , 1733",
    "* x/@xmath0-ray spectrometer data analysis * large astronomical data sets arising from the simultaneous analysis of years of data .",
    "* resolution of a large sparse system of equations ; solution and its variance . *",
    "the multifrontal massively parallel solver ( mumps ) to solve the equations . *",
    "mumps @xmath99 feature to compute selected inverse entries ( variance of the solution ,  ) ."
  ],
  "abstract_text": [
    "<S> nowadays , analyzing and reducing the ever larger astronomical datasets is becoming a crucial challenge , especially for long cumulated observation times . </S>",
    "<S> the x/@xmath0-ray spectrometer is an instrument for which it is essential to process many exposures at the same time in order to increase the low signal - to - noise ratio of the weakest sources . in this context , </S>",
    "<S> the conventional methods for data reduction are inefficient and sometimes not feasible at all . </S>",
    "<S> processing several years of data simultaneously requires computing not only the solution of a large system of equations , but also the associated uncertainties . </S>",
    "<S> we aim at reducing the computation time and the memory usage . </S>",
    "<S> since the transfer function is sparse , we have used some popular methods for the solution of large sparse linear systems ; we briefly review these methods . </S>",
    "<S> we use the multifrontal massively parallel solver ( mumps ) to compute the solution of the system of equations . </S>",
    "<S> we also need to compute the variance of the solution , which amounts to computing selected entries of the inverse of the sparse matrix corresponding to our linear system . </S>",
    "<S> this can be achieved through one of the latest features of the mumps software that has been partly motivated by this work . in this paper </S>",
    "<S> we provide a brief presentation of this feature and evaluate its effectiveness on astrophysical problems requiring the processing of large datasets simultaneously , such as the study of the entire emission of the galaxy . </S>",
    "<S> we used these algorithms to solve the large sparse systems arising from data processing and to obtain both their solutions and the associated variances . in conclusion , thanks to these newly developed tools , processing large datasets arising from is now feasible with both a reasonable execution time and a low memory usage .    </S>",
    "<S> methods : data analysis , methods : numerical , techniques : imaging spectroscopy , techniques : miscellaneous , gamma - rays : general </S>"
  ]
}