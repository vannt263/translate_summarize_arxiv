{
  "article_text": [
    "from a purely theoretical point of view , given the underlying conditional probability distribution of a dependent variable @xmath0 and a set of features @xmath1 , the bayes decision rule can be applied to construct the optimum induction algorithm . however , in practice learning machines are not given access to this distribution , @xmath2 .",
    "therefore , given a feature vector or variables @xmath3 , the aim of most machine learning algorithms is to approximate this underlying distribution or estimate some of its characteristics .",
    "unfortunately , in most practically relevant data mining applications , the dimensionality of the feature vector is quite high making it prohibitive to learn the underlying distribution .",
    "for instance , gene expression data or images may easily have more than tens of thousands of features . while , at least in theory , having more features should result in a more discriminative classifier , it is not the case in practice because of the computational burden and curse of dimensionality .",
    "high - dimensional data poses different challenges on induction and prediction algorithms . essentially , the amount of data to sustain the spatial density of the underlying distribution increases exponentially with the dimensionality of the feature vector , or alternatively , the sparsity increases exponentially given a constant amount of data .",
    "normally in real - world applications , a limited amount of data is available and obtaining a sufficiently good estimate of the underlying high - dimensional probability distribution is almost impossible unless for some special data structures or under some assumptions ( independent features , etc ) .",
    "thus , dimensionality reduction techniques , particularly feature extraction and feature selection methods , have to be employed to reconcile idealistic learning algorithms with real - world applications .    in the context of feature selection ,",
    "two main issues can be distinguished .",
    "the first one is to define an appropriate measure function to assign a score to a set of features .",
    "the second issue is to develop a search strategy that can find the optimal ( in a sense of optimizing the value of the measure function ) subset of features among all feasible subsets in a reasonable amount of time .",
    "different approaches to address these two problems can roughly be categorized into three groups : wrapper methods , embedded methods and filter methods .",
    "wrapper methods @xcite use the performance of an induction algorithm ( for instance a classifier ) as the measure function .",
    "given an inducer @xmath4 , wrapper approaches search through the space of all possible feature subsets and select the one that maximizes the induction accuracy .",
    "most of the methods of this type require to check all the possible @xmath5 subsets of features and thus , may rapidly become prohibitive due to the so - called combinatorial explosion . since the measure function is a machine learning ( ml ) algorithm ,",
    "the selected feature subset is only optimal with respect to that particular algorithm , and may show poor generalization performance over other inducers .",
    "the second group of feature selection methods are called embedded methods @xcite and are based on some internal parameters of the ml algorithm .",
    "embedded approaches rank features during the training process and thus simultaneously determine both the optimal features and the parameters of the ml algorithm .",
    "since using ( accessing ) the internal parameters may not be applicable in all ml algorithms , this approach can not be seen as a general solution to the feature selection problem .",
    "in contrast to wrapper methods , embedded strategies do not require to run the exhaustive search over all subsets since they mostly evaluate each feature individually based on the score calculated from the internal parameters .",
    "however , similar to wrapper methods , embedded methods are dependent on the induction model and thus the selected subset is somehow tuned to a particular induction algorithm .",
    "filter methods , as the third group of selection algorithms , focus on filtering out irrelevant and redundant features in which irrelevancy is defined according to a predetermined measure function . unlike the first two groups , filter methods do not incorporate the learning part and thus show better generalization power over a wider range of induction algorithms .",
    "they rely on finding an optimal feature subset through the optimization of a suitable measure function .",
    "since the measure function is selected independently of the induction algorithm , this approach decouples the feature selection problem from the following ml algorithm .",
    "the first contribution of this work is to analyze the popular mutual information measure in the context of the feature selection problem .",
    "we will expand the mutual information function in two different series and show that most of the previously suggested information - theoretic criteria are the first or second order truncation - approximations of these expansions .",
    "the first expansion is based on generalization of mutual information and has already appeared in literature while the second one is new , to the best of our knowledge . the well - known minimal redundancy maximal relevance ( mrmr ) score function can be immediately concluded from the second expansion .",
    "we will discuss the consistency and accuracy of these approximations and experimentally investigate the conditions in which these truncation - approximations may lead to high estimation errors .",
    "alternatively , feature selection methods can be categorized based on the search strategies they employ .",
    "popular search approaches can be divided into four categories : exhaustive search , greedy search , projection and heuristic .",
    "a trivial approach is to exhaustively search in the subset space as it is done in wrapper methods .",
    "however , as the number of features increases , it can rapidly become infeasible . hence , many popular search approaches use greedy hill climbing , as an approximation to this np - hard combinatorial problem .",
    "greedy algorithms iteratively evaluate a candidate subset of features , then modify the subset and evaluate if the new subset is an improvement over the old one .",
    "this can be done in a forward selection setup which starts with an empty set and adds one feature at a time or with a backward elimination process which starts with the full set of features and removes one feature at each step .",
    "the third group of the search algorithms are based on targeted projection pursuit which is a linear mapping algorithm to pursue an optimum projection of data onto a low dimensional manifold that scores highly with respect to a measure function @xcite . in heuristic methods , for instance genetic algorithms ,",
    "the search is started with an initial subset of features which gradually evolves toward better solutions .",
    "recently , two convex quadratic programing based methods , qpfs in @xcite and soss in @xcite have been suggested to address the search problem .",
    "qpfs is a deterministic algorithm and utilizes the nystrm method to approximate large matrices for efficiency purposes .",
    "soss on the other hand , has a randomized rounding step which injects a degree of randomness into the algorithm in order to generate more diverse feature sets .",
    "developing a new search strategy is another contribution of this paper . here",
    ", we introduce a new class of search algorithms based on semi - definite programming ( sdp ) relaxation .",
    "we reformulate the feature selection problem as a ( 0 - 1)-quadratic integer programming and will show that it can be relaxed to an sdp problem , which is convex and hence can be solved with efficient algorithms @xcite .",
    "moreover , there is a discussion about the approximation ratio of the proposed algorithm in subsection 3.2 .",
    "we show that it usually gives better solutions than greedy algorithms in the sense that its approximate solution is more probable to be closer to the optimal point of the criterion .",
    "let us consider an @xmath6 dimensional feature vector @xmath7 $ ] and a dependent variable @xmath0 which can be either a class label in case of classification or a target variable in case of regression .",
    "the mutual information function is defined as a distance from independence between @xmath1 and @xmath0 measured by the kullback - leibler divergence @xcite .",
    "basically , mutual information measures the amount of information shared between @xmath1 and @xmath0 by measuring their dependency level .",
    "denote the joint pdf of @xmath1 and @xmath0 and its marginal distributions by @xmath8 , @xmath9 and @xmath10 , respectively .",
    "the mutual information between the feature vector and the class label can be defined as follows : @xmath11 it reaches its maximum value when the dependent variable is perfectly described by the feature set . in this case",
    "mutual information is equal to @xmath12 , where @xmath12 is the shannon entropy of @xmath0 .",
    "mutual information can also be considered a measure of set intersection @xcite .",
    "namely , let @xmath13 and @xmath14 be event sets corresponding to random variables @xmath15 and @xmath16 , respectively .",
    "it is not difficult to verify that a function @xmath17 defined as : @xmath18 satisfies all three properties of a formal measure over sets @xcite @xcite , i.e. , non - negativity , assigning zero to empty set and countable additivity .",
    "however , as we see later , the generalization of the mutual information measure to more than two sets will no longer satisfy the _ non - negativity _ property and thus can be seen as a signed measure which is the generalization of the concept of measure by allowing it to have negative values .",
    "there are at least three reasons for the popularity of the use of mutual information in feature selection algorithms .",
    "most of the suggested non information - theoretic score functions are not formal set measures ( for instance correlation function ) .",
    "therefore , they can not assign a score to a set of features but rather to individual features . however , mutual information as a formal set measure is able to evaluate all possible informative interactions and complex functional relations between features and as a result , fully extract the information contained in a set of features .",
    "the relevance of the mutual information measure to misclassification error is supported by the existence of bounds relating the probability of misclassification of the bayes classifier , @xmath19 , to the mutual information .",
    "more specifically , fano s weak lower bound @xcite on @xmath19 , @xmath20 where @xmath21 is the number of classes and the hellman - raviv @xcite upper bound , @xmath22 on @xmath19 , provide somewhat a performance guarantee .",
    "as it can be seen in and , maximizing the mutual information between @xmath1 and @xmath0 decreases both upper and lower bounds on misclassification error and guarantees the goodness of the selected feature set . however , there is somewhat of a misunderstanding of this fact in the literature .",
    "it is sometimes wrongly claimed that maximizing the mutual information results in minimizing the @xmath19 of the optimal bayes classifier .",
    "this is an unfounded claim since @xmath19 is not a monotonic function of the mutual information .",
    "namely , it is possible that a feature vector @xmath23 with less relevant information - content about the class label @xmath0 than a feature vector @xmath24 yields a lower classification error rate than @xmath24 .",
    "the following example may clarify this point .",
    "* example 1 * : consider a binary classification problem with equal number of positive and negative training samples and two binary features @xmath25 and @xmath26 . the goal is to select the optimum feature for the classification task .",
    "suppose the first feature @xmath25 is positive if the outcome is positive .",
    "however , when the outcome is negative , @xmath25 can take both positive and negative values with the equal probability .",
    "namely , @xmath27 and @xmath28 . in the same manner ,",
    "the likelihood of @xmath26 is defined as @xmath29 and @xmath30 .",
    "then , the bayes classifier with feature @xmath25 yields the classification error : @xmath31 similarly , the bayes classifier with @xmath26 yields @xmath32 meaning that , @xmath26 is a better feature than @xmath25 in the sense of minimizing the probability of misclassification .",
    "however , unlike their error probabilities , @xmath33 , is greater than @xmath34 .",
    "that is , @xmath25 conveys more information about the class label in the sense of shannon mutual information than @xmath26 .",
    "a more detailed discussion can be found in @xcite . however , it is worthwhile to mention that although using mutual information may not necessarily result in the highest classification accuracy , it guarantees to reveal a salient feature subset by reducing the upper and lower bounds of @xmath19 .",
    "3- by adapting classification error as a criterion , most standard classification algorithms fail to correctly classify the instances from minority classes in imbalanced datasets .",
    "common approaches to address this issue are to either assign higher misclassification costs to minority classes or replace the classification accuracy criterion with the area under the roc curve which is a more relevant criterion when dealing with imbalanced datasets .",
    "either way , the features should also be selected by an algorithm which is insensitive ( robust ) with respect to class distributions ( otherwise the selected features may not be informative about minority classes , in the first place ) .",
    "interestingly , by internally applying unequal class dependent costs , mutual information provides some robustness with respect to class distributions .",
    "thus , even in an imbalanced case , a mutual information based feature selection algorithm is likely ( though not guaranteed ) to not overlook the features that represent the minority classes . in citebao:11 ,",
    "the concept of the mutual information classifier is investigated .",
    "specifically , the internal cost matrix of the mutual information classifier is derived to show that it applies unequal misclassification costs when dealing with imbalanced data and showed that the mutual information classifier is an optimal classifier in the sense of maximizing a weighted classification accuracy rate .",
    "the following example shows this robustness .",
    "* example 2 * : assume an imbalanced binary classification task where @xmath35 .",
    "as in example 1 , there are two binary features @xmath25 and @xmath26 and the goal is to select the optimum feature .",
    "suppose @xmath27 and @xmath36 .",
    "unlike the first feature , @xmath26 can much better classify the minority class @xmath37 and @xmath38 .",
    "it can be seen that the bayes classifier with @xmath25 results in 100% classification rate for the majority class while only 50% correct classification for the minority .",
    "on the other hand , using @xmath26 leads to 100% correct classification for the minority class and 80% for the majority . based on the probability of error ,",
    "@xmath25 should be preferred since its probability of error is @xmath39 while @xmath40 .",
    "however , by using @xmath25 the classifier can not learn the rare event ( 50% classification rate ) and thus randomly classifies the minority class which is the class of interest in many applications .",
    "interestingly , unlike the bayesian error probabilities , mutual information prefers @xmath26 over @xmath25 , since @xmath41 is greater than @xmath42 .",
    "that is , mutual information is to some extent robust against imbalanced data .",
    "unfortunately , despite the theoretical appeal of the mutual information measure , given a limited amount of data , an accurate estimate of the mutual information would be impossible . because to calculate mutual information , estimating the high - dimensional joint probability @xmath8 is inevitable which is , in turn , known to be an np hard problem @xcite .",
    "as mutual information is hard to evaluate , several alternatives have been suggested @xcite , @xcite , @xcite .",
    "for instance , the max - relevance criterion approximates with the sum of the mutual information values between individual features @xmath43 and @xmath0 : @xmath44 since it implicitly assumes that features are independent , it is likely that selected features are highly redundant . to overcome this problem ,",
    "several heuristic corrective terms have been introduced to remove the redundant information and select mutually exclusive features . here",
    ", it is shown that most of these heuristics are derived from the following expansions of mutual information with respect to @xmath43 .",
    "the first expansion of mutual information that is used here , relies on the natural extension of mutual information to more than two random variables proposed by mcgill @xcite and abramson @xcite . according to their proposal ,",
    "the three - way mutual information between random variables @xmath45 is defined by : @xmath46 where `` , '' between variables denotes the joint variables .",
    "note that , similar to two - way mutual information , it is symmetric with respect to @xmath45 variables , i.e. , @xmath47 .",
    "generalizing over @xmath6 variables : @xmath48 unlike 2-way mutual information , the generalized mutual information is not necessarily nonnegative and hence , can be interpreted as a signed measure of set intersection @xcite .",
    "consider and assume @xmath49 is class label @xmath0 , then positive @xmath50 implies that @xmath51 and @xmath52 are redundant with respect to @xmath0 since @xmath53 .",
    "however , the more interesting case is when @xmath50 is negative ,",
    "i.e. , @xmath54 .",
    "this means , the information contained in the interactions of the variables is greater than the sum of the information of the individual variables @xcite .",
    "an artificial example for this situation is the binary classification problem depicted in figure [ fig1 ] , where the classification task is to discriminate between the ellipse class ( class samples depicted by circles ) and the line class ( star samples ) by using two features : values of @xmath55 axis and values of @xmath56 axis .",
    "as can be seen , since @xmath57 and @xmath58 , there is no way to distinguish between these two classes by just using one of the features .",
    "however , it is obvious that employing both features results in almost perfect classification , i.e. , @xmath59 .     and @xmath56 features .",
    "while information of each individual feature about the class label ( ellipse or line ) is almost zero , their joint information can almost completely remove the class label ambiguity . ]    the mutual information in can be expanded out in terms of generalized mutual information between the features and the class label as : @xmath60 from the definition in it is straightforward to infer this expansion .",
    "however , the more intuitive proof is to use the fact that mutual information is a measure of set intersection , i.e. , @xmath61 , where @xmath62 is the corresponding event set of the @xmath45 variable .",
    "now , expanding the @xmath6-variable measure function results in : @xmath63 where the last equation follows directly from the addition law or sum rule in set theory .",
    "the proof is complete by recalling that all measure functions with the set intersection arguments in the last equation can be replaced by the mutual information functions according to the definition of mutual information in .",
    "the second expansion for mutual information is based on the _ chain rule of information _",
    "@xcite : @xmath64 the chain rule of information leaves the choice of ordering quite flexible . for example , the right side can be written in the order @xmath65 or @xmath66 .",
    "in general , it can be expanded over @xmath67 different permutations of the feature set @xmath68 .",
    "taking the sum over all possible expansions yields , @xmath69 dividing both sides by @xmath70 , and using the following equation @xmath71 to replace @xmath72 terms , our second expansion can be expressed as @xmath73 ignoring the unimportant multiplicative constant @xmath74 on the left side of equation , the right side can be seen as a series expansion form of mutual information ( up to a known constant factor ) .      in",
    "the both proposed expansions and , mutual information terms with more than two features represent higher - order interaction properties .",
    "neglecting the higher order terms yields the so - called truncated approximation of the mutual information function .",
    "if we ignore the constant coefficient in , the truncated forms of suggested expansions can be written as : @xmath75 where @xmath76 is the truncated approximation of and @xmath77 is for .",
    "interestingly , despite the very similar structure of the expressions in , they have intrinsically different behaviors . this difference seems to be rooted in different functional forms they employ to approximate the underlying high - order pdf with lower order distributions ( i.e. , how they combine these lower order terms ) .",
    "for instance , the functional form that mifs employs to approximate @xmath78 is shown in . while @xmath76 is not necessarily a positive value , @xmath77 is guaranteed to be a positive approximation since all terms in are positive .",
    "however , @xmath77 may highly underestimate the mutual information values since it may violate the fact that is always greater than or equal to @xmath79 .",
    "several known criteria including joint mutual information ( jmi ) @xcite , minimal redundancy maximal relevance ( mrmr ) @xcite and mutual information feature selection ( mifs ) @xcite can immediately be derived from @xmath76 and @xmath77 .    using the identity : @xmath80 in @xmath77",
    "reveals that @xmath77 is equivalent to jmi .",
    "@xmath81    using @xmath82 and ignoring the terms containing more than two variables , i.e. , @xmath83 , in the second approximation @xmath77 , one may immediately recognize the popular score function @xmath84 introduced by peng et al . in @xcite .",
    "that is , mrmr is a truncated approximation of mutual information and not a heuristic approximation as suggested in @xcite .",
    "the same line of reasoning as for mrmr can be applied to @xmath76 to achieve mifs with @xmath85 equal to 1 .",
    "@xmath86    * observation * : a constant feature is a potential danger for the above measures . while adding an informative but correlated feature may reduce the score value ( since @xmath87 can be negative ) , adding a non - informative constant feature @xmath88 to a feature set does not reduce its score value since both @xmath89 and @xmath90 terms are zero , that is , constant features may be preferred over informative but correlated features .",
    "therefore , it is essential to remove constant features by some preprocessing before using the above criteria for feature selection .      a natural question arising in this context with respect to the proposed truncated approximations is : under what probabilistic assumptions do the proposed approximations become valid mutual information functions ? that is , which structure should a joint pdf admit , to yield mutual information in the forms of @xmath76 or @xmath77 ?",
    "for instance , if we assume features are mutually and class conditionally independent , i.e. , @xmath91 and @xmath92 , then it is easy to verify that mutual information has the form of max - relevance introduced in .",
    "these two assumptions , define the adapted _ independence - map _ of @xmath8 where the independence - map of a joint probability distribution is defined as follows .    *",
    "definition 1 * : _ an independence - map ( i - map ) is a look up table or a set of rules that denote all the conditional and unconditional independence between random variables .",
    "moreover , an i - map is consistent if it leads to a valid factorized probability distribution_.    that is , given a consistent i - map , a high - order joint probability distribution is approximated with product of low - order pdfs and the obtained approximation is a valid pdf itself ( e.g. , @xmath93 is an approximation of the high - order pdf @xmath9 and it is also a valid probability distribution ) .    the question regarding the implicit consistent i - map that mifs adopts has been investigated in @xcite .",
    "however , the assumption set ( i - map ) suggested in their work is inconsistent and leads to the incorrect conclusion that mifs upper bounds the bayesian classification error via the inequality . as we show in the following theorem , unlike the max - relevance case",
    ", there is no i - map that can produce mutual information in the forms of mrmr of mifs ( ignoring the trivial solution that reduces mrmr or mifs to max - relevance )",
    ".    * theorem 1 . * _ ignoring the trivial solution , i.e. , the i - map indicating that random variables are mutually and class conditionally independent , there is no consistent i - map that can produce mutual information functions in the forms of mrmr or mifs for arbitrary number of features_.    * proof * : the proof is by contradiction .",
    "suppose there is a consistent i - map , where its corresponding joint pdf @xmath94 ( which is the approximation of @xmath8 ) can generate mutual information in the forms of or .",
    "that is , if this i - map is adopted , by replacing @xmath94 in we get mrmr or mifs .",
    "this implies that mrmr and mifs are _ always _ valid set measures for all datasets regardless of their true underlying joint probability distributions .",
    "now , if we show ( by any example ) that they are not valid mutual information measures , i.e. , they are not always positive and monotonic , then we have contradicted our assumption that @xmath94 exists and is a valid pdf .",
    "it is not so difficult to construct an example in which mrmr or mifs can get negative values . consider the case where features are independent of class label , @xmath95 , while they have nonzero dependencies among themselves , @xmath96 . in this case , both mrmr and mifs generate negative values which is not allowed by a valid set measure .",
    "this contradicts our assumption that they are generated by a valid distribution , so we are forced to conclude that there is no consistent i - map that results in mutual information in the mrmr or mifs forms.@xmath97    the same line of reasoning can be used to show that @xmath76 and @xmath77 are also not valid measures .    however",
    ", despite the fact that no valid pdf can produce mutual information of those forms , it is still valid to ask for which low - order approximations of the underlying high - order pdfs , mutual information reduces to a truncated approximation form .",
    "that is , we do not restrict an approximation to be a valid distribution anymore .",
    "any functional form of low - order pdfs may be seen as an approximation of the high - order pdfs and may give rise to mifs or mrmr . in the next subsection",
    "we reveal these assumptions for the mifs criterion .",
    "it is shown in @xcite that truncation of the joint entropy @xmath98 at the @xmath99th - order is equivalent to approximating the full - dimensional pdf @xmath9 using joint pdfs with dimensionality of @xmath99 or smaller .",
    "this approximation is called @xmath99th order kirkwood approximation . the truncation order that we choose , partially determines our belief about the structure of the function that we are going to estimate the exact @xmath9 with",
    ".    the 2nd order kirkwood approximation of @xmath9 , can be denoted as follows @xcite : @xmath100^{n-2}}\\ ] ] now , assume the following two assumptions hold :    * assumption 1 * : features are class conditionally independent , that is : @xmath101    * assumption 2 * : @xmath9 is well approximated by a 2nd order kirkwood superposition approximation in .",
    "then , writing the definition of mutual information and applying the above assumptions yields the mifs criterion @xmath102 in the above equation , ( a ) follows the second assumption by substituting the 2nd order kirkwood approximation inside the logarithm of the entropy integral and ( b ) is an immediate consequence of the first assumption .",
    "the first assumption has already appeared in previous works @xcite @xcite .",
    "however , the second assumption is novel and , to the best of our knowledge , the connection between the kirkwood approximation and the mifs criterion has not been explored before .",
    "it is worth to mention that , in reality , both assumptions can be violated . specifically , the kirkwood approximation may not precisely reproduce dependencies we might observe in real - world datasets .",
    "moreover , it is important to remember that the kirkwood approximation is not a valid probability distribution .      from our experiments , which we omit because of space constraints , @xmath77 tends to underestimate the mutual information while @xmath76 shows a large overestimation for independent features and a large underestimation ( even becoming negative ) in the presence of dependent features . in general",
    ", @xmath77 shows more robustness than @xmath76 .",
    "the same results can be observed for mrmr which is derived from @xmath77 and mifs derived from @xmath76 .",
    "previous work also arrived to the same results and reported that mrmr performs better and more robustly than mifs especially when the feature set is large .",
    "therefore , in the following sections we use @xmath77 as the truncated approximation . for simplicity ,",
    "its subscript is dropped and it is rewritten as follows : @xmath104 note that although @xmath105 in is not a formal set measure any more , it still can be seen as a score function for sets .",
    "however , it is noteworthy that unlike formal measures , the suggested approximations are no longer monotonic where the monotonicity merely means that a subset of features should not be better than any larger set that contains the very same subset",
    ". therefore , as explained in @xcite the branch and bound based search strategies can not be applied to them .",
    "a very similar approach has been applied @xcite ( by using @xmath76 approximation ) to derive several known criteria like mifs @xcite and mrmr @xcite . however , in @xcite and most of other previous works , the set score function in is immediately reduced to an individual - feature score function by fixing @xmath106 features in the feature set .",
    "this will let them to run a greedy selection search method over the feature set which essentially is a one - feature - at - a - time selection strategy .",
    "it is clearly a naive approximation of the optimal np - hard search algorithm and may perform poorly under some conditions . in the following ,",
    "we investigate a convex approximation of the binary objective function appearing in feature selection inspired by the goemans - williamson maximum cut approximation approach @xcite .",
    "given a measure function @xmath105 , the subset selection problem ( ssp ) can be defined as follows :    * definition 2 * : given @xmath6 features @xmath43 and a dependent variable @xmath0 , select a subset of @xmath107 features that maximizes the measure function .",
    "here it is assumed that the cardinality @xmath108 of the optimal feature subset is known .    in practice",
    ", the exact value of @xmath108 can be obtained by evaluating subsets for different values of cardinality @xmath108 with the final induction algorithm .",
    "note that it is intrinsically different than wrapper methods . while in wrapper methods @xmath5",
    "subsets have to be tested , here at most @xmath6 runs of the learning algorithm are needed to evaluate all possible values of @xmath108 .",
    "a search strategy is an algorithm trying to find a feature subset in the feature subset space with @xmath5 members , the size of the feature subset space reduces to @xmath109 . ]",
    "that optimizes the measure function .",
    "the wide range of proposed search strategies in the literature can be divided into three categories : 1- exponential complexity methods including exhaustive search @xcite , branch and bound based algorithms @xcite .",
    "2- sequential selection strategies with two very popular members , forward selection and backward elimination methods .",
    "3- stochastic methods like simulated annealing and genetic algorithms @xcite , @xcite .    here",
    ", we introduce a fourth class of search strategies which is based on the convex relaxation of the 0 - 1 integer programming and explore its approximation ratio by establishing a link between ssp and an instance of the maximum - cut problem in graph theory . in the following ,",
    "we briefly discuss the two popular sequential search methods and continue with the proposed solution : a close to optimal polynomial - time complexity search algorithm and its evaluation on different datasets .      the forward selection ( fs ) algorithm selects a set @xmath110 of size @xmath108 iteratively as follows :    1 .   initialize @xmath111 .",
    "2 .   in each iteration @xmath112 , select the feature @xmath113 maximizing @xmath114 , and set @xmath115 .",
    "output @xmath116 .",
    "similarly , backward elimination ( be ) can be described as :    1 .",
    "start with the full set of feature @xmath117 .",
    "2 .   iteratively remove a variable @xmath113 maximizing @xmath118 , and set @xmath119 , where removing @xmath120 from @xmath110 is denoted by @xmath121 .",
    "3 .   output @xmath116 .",
    "an experimentally comparative evaluation of several variants of these two algorithms has been conducted in @xcite . from an information theoretical standpoint ,",
    "the main disadvantage of the forward selection method is that it only can evaluate the utility of a single feature in the limited context of the previously selected features .",
    "the artificial binary classifier in figure [ fig1 ] may illustrate this issue . since the information content of each feature ( @xmath55 and @xmath56 ) is almost zero , it is highly probable that the forward selection method fails to select them in the presence of some other more informative features .",
    "contrary to forward selection , backward elimination can evaluate the contribution of a given feature in the context of all other features .",
    "perhaps this is why it has been frequently reported to show superior performance than forward selection .",
    "however , its overemphasis on feature interactions is a double - edged sword and may lead to a sub - optimal solution .",
    "* example 3 * : imagine a four dimensional feature selection problem where @xmath25 and @xmath26 are class conditionally and mutually independent of @xmath122 and @xmath123 , i.e. , @xmath124 and @xmath125 .",
    "consider @xmath126 and @xmath127 are equal to zero , while their interaction is informative .",
    "that is , @xmath128 . moreover , assume @xmath129 , @xmath130 and @xmath131 .",
    "the goal is to select only two features out of four . here",
    ", backward elimination will select @xmath132 rather than the optimal subset @xmath133 because , removing any of @xmath25 or @xmath26 features will result in @xmath134 reduction of the mutual information value @xmath135 , while eliminating @xmath122 or @xmath123 deducts at most @xmath136 .",
    "one may draw the conclusion that backward elimination tends to sacrifice the individually - informative features in favor of the merely cooperatively - informative features .",
    "as a remedy , several hybrid forward - backward sequential search methods have been proposed .",
    "however , they all fail in one way or another and more importantly can not guarantee the goodness of the solution .",
    "alternatively , a sequential search method can be seen as an approximation of the combinatorial subset selection problem . to propose a new approximation method",
    ", the underlying combinatorial problem has to be studied .",
    "to this end , we may formulate the ssp defined in the beginning of this section as : @xmath137 where @xmath138 is a symmetric mutual information matrix constructed from the mutual information terms in : @xmath139 where @xmath140 and @xmath141 $ ] is a binary vector where the variables @xmath142 are set - membership binary variables indicating the presence of the corresponding features @xmath43 in the feature subset .",
    "it is straightforward to verify that for any binary vector @xmath143 , the objective function in is equal to the score function @xmath144 where @xmath145 .",
    "note that , for mrmr @xmath146 terms have to be replaced with @xmath147 .",
    "the ( 0,1)-quadratic programming problem has attracted a great deal of theoretical study because of its importance in combinatorial problems ( * ? ? ?",
    "* and references therein ) .",
    "this problem can simply be transformed to a programming problem , @xmath148 via @xmath149 transformation , where @xmath150 is an all ones vector .",
    "additionally @xmath151 in the above formulation is a constant equal to @xmath152 and it can be ignored because of its independence of @xmath153 . in order to homogenize the objective function in , define an @xmath154 matrix @xmath155 by adding a 0-th row and column to @xmath138 so that : @xmath156 ignoring the constant factor @xmath157 in , the equivalent homogeneous form of can be written as : @xmath158 note that @xmath153 is now an @xmath159 dimensional vector with the first element @xmath160 as a reference variable .",
    "given the solution of the problem above , i.e. , @xmath153 , the optimal feature subset is obtained by @xmath161 .",
    "the optimization problem in can be seen as an instance of the maximum - cut problem @xcite with an additional cardinality constraint , also known as the k - heaviest subgraph or maximum partitioning graph problem . the two main approaches to solve this combinatorial problem are either to use the linear programming relaxation by linearizing the product of two binary variables @xcite , or the semidefinite programming ( sdp ) relaxation suggested in @xcite .",
    "the sdp relaxation has been proved to have exceptionally high performance and achieves the approximation ratio of 0.878 for the original maximum - cut problem .",
    "the sdp relaxation of is : @xmath162 where @xmath163 is an unknown @xmath164 positive semidefinite matrix and @xmath165 denotes its trace .",
    "obviously , any feasible solution @xmath153 for is also feasible for its sdp relaxation by @xmath166 .",
    "furthermore , it is not difficult to see that any rank one solution , @xmath167 , of is a solution of .",
    "the problem can be solved within an additive error @xmath168 of the optimum by for example interior point methods @xcite whose computational complexity are polynomial in the size of the input and @xmath169 . however , since its solution is not necessarily a rank one matrix , we need some more steps to obtain a feasible solution for .",
    "the following three steps summarize the approximation algorithm for which in the following will be referred to as convex based relaxation approximation ( cobra ) algorithm .",
    "* cobra algorithm * :    1 .",
    "sdp : solve and obtain @xmath170 .",
    "repeat the following steps many times and output the best solution .",
    "2 .   randomized rounding : using the multivariate normal distribution with a zero mean and a covariance matrix @xmath171 to sample @xmath172 from distribution @xmath173 and construct @xmath174 .",
    "select @xmath175 .",
    "size adjusting : by using the greedy forward or backward algorithm , resize the cardinality of @xmath176 to @xmath108 .",
    "the randomized rounding step is a standard procedure to produce a binary solution from the real - valued solution of and is widely used for designing and analyzing approximation algorithms @xcite .",
    "the third step is to construct a feasible solution that satisfies the cardinality constraint .",
    "generally , it can be skipped since in feature selection problems the exact satisfaction of the cardinality constraint is not required .",
    "we use the sdp - nal solver @xcite with the yalmip interface @xcite to implement this algorithm in matlab .",
    "sdp - nal uses the newton - cg augmented lagrangian method to efficiently solve sdp problems .",
    "it can solve large scale problems ( @xmath6 up to a few thousand ) in an hour on a pc with an intel core i7 cpu .",
    "even more efficient algorithms for low - rank sdp have been suggested claiming that they can solve problems with the size up to @xmath177 in a reasonable amount of time @xcite .",
    "here we only use the sdp - nal for our experiments .      [ cols=\"<,^,^,^,^,^,^,^ \" , ]",
    "a convex based parallel search strategy for feature selection , cobra , was suggested in this work .",
    "its approximation ratio was derived and compared with the approximation ratio of the backward elimination method .",
    "it was experimentally shown that cobra outperforms sequential search methods especially in the case of sparse data .",
    "moreover , we presented two series expansions for mutual information , and showed that most mutual information based score functions in the literature including mrmr and mifs are truncated approximations of these expansions . furthermore , the underlying connection between mifs and the kirwood approximation was explored , and it was shown that by adopting the class conditional independence assumption and the kirkwood approximation for @xmath9 , mutual information reduces to the mifs criterion .",
    "this work has partly been supported by swiss national science foundation ( snsf ) .",
    "tofigh naghibi received his msc .",
    "degree in electrical engineering from sharif university of technology , tehran , iran in 2009 .",
    "he then joined the speech processing group at eth zurich where he is currently working toward a ph.d .",
    "degree in electrical engineering .",
    "his research interests include signal processing , pattern recognition and machine learning topics such as boosting methods and online learning .",
    "sarah hoffmann received her msc . in computer science from dresden technical university in germany . from 2003 to 2007 , she worked for a research group of stmicroelectronics in rousset , france .",
    "she has recently finished her ph.d .",
    "studies in speech processing at eth zurich .",
    "her research interests focus on prosody generation in speech synthesis by means of statistical models and semi - supervised speech recognition .",
    "beat pfister received his diploma in electrical engineering and his ph.d . from eth zurich . since 1981",
    ", he has been the head of the speech processing at the eth zurich .",
    "he acquired and guided numerous research projects in speech coding , text - to - speech synthesis , speech recognition , and speaker verification .",
    "his research interests include text - to - speech synthesis and speech recognition with emphasis on interdisciplinary approaches including signal processing , statistical modelling , knowledge - based systems and linguistics ."
  ],
  "abstract_text": [
    "<S> feature selection , mutual information , convex objective , approximation ratio . </S>"
  ]
}