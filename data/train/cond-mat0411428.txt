{
  "article_text": [
    "the optimization of system with many degrees of freedom with respect to some cost function is a frequently encountered task in physics and beyond .",
    "one special class of algorithms used for finding the high - quality solutions to those np - hard optimization problems is the so - called nature inspired algorithms , including simulated annealing(sa)@xcite , genetic algorithms(ga)@xcite , genetic programming(gp)@xcite , and so on .    in recent years ,",
    "a novel nature inspired algorithm named extremal optimization(eo ) is proposed by boettcher and percus@xcite , which is very sententious and competitive comparing with some well known algorithms like sa , ga , gp etc .. to make the underlying mechanism of eo more concrete , let s focus on the natural selection of biological system . in nature ,",
    "highly specialized , complex structure often emerge when their most inefficient elements are selectively driven to extinction . for example , evolution progresses by selecting against the few most poorly adapted species , rather than by expressly breeding those species best adapted to their environment .",
    "the principle that the least fit elements are progressively eliminated has been applied successfully in the bak - sneppen model@xcite , where each individual corresponding a certain species is characterized by a fitness value , and the least fit one with smallest fitness value and its closest dependent species are successively selected for adaptive changes .",
    "the extremal optimization algorithm draws upon the bak - sneppen mechanism , yielding a dynamic optimization procedure free of selection parameters .",
    "here we consider a general optimization problem , where the system consists of @xmath0 elements , and we wish to minimize the cost function @xmath1 depending on the system configuration @xmath2 .",
    "the eo algorithm proceeds as follows : + ( 1 ) choose an initial configuration @xmath2 of the system at will ; set @xmath3 .",
    "+ ( 2 ) evaluate the fitness value @xmath4 for each individual @xmath5 and rank each individual according to its fitness value so as to the least fit one is in the top .",
    "use @xmath6 to denote the individual @xmath5 s rank , clearly , the least fit one is of rank 1 .",
    "choose one individual @xmath7 that will be changed with the probability @xmath8 , and then , only randomly change the state of @xmath7 and keep all other individuals state unaltered . accept the new configuration @xmath9 unconditionally @xmath10 , and if @xmath11 then set @xmath3 .",
    "+ ( 3 ) repeat at step ( 2 ) as long as desired .",
    "+ ( 4 ) return @xmath12 and @xmath13 .",
    "the efficiency of eo algorithm is sensitive to the probability function @xmath14 . in basic eo , @xmath15 and for any @xmath16 , @xmath17 .",
    "a more efficient algorithm , the so - called @xmath18-eo , can be obtained through a slight modification from basic eo . in @xmath18-eo , @xmath19 where @xmath20 . of course , aiming at idiographic optimization problems , one can design various forms of @xmath14 to improve the performance of basic eo .",
    "for example , middleton has proposed the jaded extremal optimization(jeo ) method for ising spin glass system by reducing the probability of flipping previously selected spins , which remarkably improved the efficiency of eo@xcite .",
    "the previous studies indicate that eo algorithm can often outperform some far more complicated or finely tuned algorithm , such as sa or ga , on some famous np - hard@xcite discrete optimization problems , including graph partitioning@xcite , travelling salesman problem@xcite , three - coloring problem@xcite , finding lowest energy configuration for iring spin glass system@xcite , and so on .",
    "however , many practical problems can not be abstracted to discrete form , thus to investigate eo s efficiency on continuous optimization problems@xcite is not only of theoretic interest , but also of prominent practical worthiness .    in this paper , a so - called continuous extremal optimization(ceo )",
    "algorithm aiming at continuous optimization problem will be introduced , which can be considered as a mixing algorithm consisting of two components , one is with responsibility for global searching and the other is with responsibility for local searching . with only one adjustable parameter , the ceo s",
    "performance proves competitive with more elaborate stochastic optimization procedures .",
    "we demonstrate it on a well known continuous optimization problem : the lennerd - jones(lj ) clusters optimization problem .    this paper is organized as follows : in section 2",
    ", the lj clusters optimization problem will be briefly introduced . in section 3",
    ", we will give the algorithm proceeds of ceo .",
    "next , we give the computing results about the performance of ceo on lj clusters optimization problem . finally , in section 5 , the conclusion is drawn and the relevances of the ceo to the real - life problems are discussed .",
    "continuous optimization problem is ubiquitous in materials science : many situation involve finding the structure of clusters and the dependence of structure on size is particularly complex and intriguing . in practice",
    ", we usually choose a potential function to take the most steady structure since it s considered to be in possession of the minima energy .",
    "however , in all but the simplest cases , these problem are complicated due to the presence of many local minima .",
    "such problem is encountered in many area of science and engineering , for example , the notorious protein folding problem@xcite .    [ 0.8 ] the details of @xmath18-ceo for @xmath21 $ ] .",
    "figure 1*a * shows the average energies obtained by ceo over 200 runs , and figure 1*b * exhibits the success rate of hitting the global minima in 200 runs@xcite . for both figure 1*a * and 1*b * , the main plot and inset represent the case @xmath22 and @xmath23 respectively .",
    "one can find that , the best @xmath18 corresponding lowest average energy and highest success rate is approximate to 1.5.,title=\"fig : \" ] [ 0.9 ] the details of @xmath18-ceo for @xmath21 $ ] . figure 1*a * shows the average energies obtained by ceo over 200 runs , and figure 1*b * exhibits the success rate of hitting the global minima in 200 runs@xcite . for both figure 1*a * and 1*b * , the main plot and inset represent the case @xmath22 and @xmath23 respectively .",
    "one can find that , the best @xmath18 corresponding lowest average energy and highest success rate is approximate to 1.5.,title=\"fig : \" ]    as one of the simplest models that exhibits such behavior @xcite one may consider the problem of finding the ground - state structure of nanocluster of atoms interacting through a classical lennerd - jones pair potential , in reduced units given by @xmath24 where @xmath25 is the distance between two atoms .",
    "this potential has a single minimum at @xmath26{2}$ ] , which is the equilibrium distance of two atoms .",
    "it can , of course , easily be reduced to an arbitrary lj - potential by a simple rescaling of length and energy units .",
    "the @xmath5th atom has energy @xmath27 and the total energy for _ n _ atoms is @xmath28 the optimization task is to find the configuration with minimum total potential energy of a system of @xmath0 atoms , each pair interacting by potential of the form ( 1 ) .",
    "clearly , a trivial lower bound for the total energy is @xmath29 , obtained when one assumes that all pairs are at their equilibrium separation . for @xmath30",
    "the lower bound can actually be obtained in three - dimensional space , corresponding respectively to a dimer , equilateral triangle , and regular tetrahedron , with all interatomic distance equal to 1 . however , from @xmath31 onwards it is not possible to place all the atoms simultaneously at the potential minimum of all others and the ground - state energy is strictly larger than the trivial lower bound .",
    "this system has been studied intensely @xcite and is known to have an exponential increasing number of local minima , growing roughly as @xmath32 near @xmath33 , at which point there are already at least 988 minima @xcite .",
    "if this scaling continues , more than @xmath34 local minima exist when @xmath0 approach 100 .",
    "the continuous extremal optimization algorithm is consisted of two components , one is the classical eo algorithm with responsibility for global searching , and the other is a certain local searching algorithm .",
    "we give the general form of ceo algorithm by way of the lj clusters optimization problem as follows : + ( 1 ) choose an initial state of the system , where all the atoms are placed within a spherical container with radius@xcite @xmath35,\\ ] ] where @xmath36{2}$ ] is the equilibrium distance and @xmath0 denotes the number of atoms .",
    "set the minimal energy @xmath37 .",
    "+ ( 2 ) use a certain local searching algorithm to find the local minimum from the current configuration of system .",
    "if the local minimal energy is lower than @xmath38 , then replace @xmath38 by the present local minimum .",
    "+ ( 3 ) rank each atom according to its energy obtained by equ.(2 ) . here",
    ", the atom who has highest energy is the least fit one and is arranged in the top of the queue .",
    "choose one atom @xmath7 that will be changed with the probability @xmath8 where @xmath39 denotes the rank of atom @xmath7 , and then , only randomly change the coordinates of @xmath7 and keep all other atoms positions unaltered .",
    "accept the new configuration unconditionally .",
    "+ ( 4 ) repeat at step ( 2 ) as long as desired .",
    "+ ( 5 ) return the minimal energy @xmath38 and the corresponding configuration .",
    "for an idiographic problem , one can attempt various local searching algorithms and pitch on the best one . in this paper , for the lj clusters optimization problem",
    ", we choose limited memory bfgs method(lbfgs ) qua the local searching algorithm .",
    "the bfgs method is an optimization technique based on quasi - newton method proposed by broyden , fletcher , goldfard and shanno .",
    "lbfgs proposed by liu and nocedal@xcite is especially effective on problems involving a large number of variables . in this method ,",
    "an approximation @xmath40 to the inverse of the hessian is obtained by applying @xmath41 bfgs updates to a diagonal matrix @xmath42 , using information from the previous @xmath41 steps .",
    "the number @xmath41 determines the amount of storage required by the routine , which is specified by the user , usually @xmath43 and in our computation @xmath41 is fixed as 4 .",
    "[ 0.8 ] the performance of ceo algorithm on lj clusters optimization problem . in figure 2*a",
    "* , the red circles represent the average energies obtained by ceo over 200 runs , where the black squares represent the global minima .",
    "figure 2*b * shows the success rate of hitting the global minima in 200 runs , the inset is the success rate for @xmath44 that may be unclear in the main plot.,title=\"fig : \" ] [ 0.9 ] the performance of ceo algorithm on lj clusters optimization problem . in figure 2*a * , the red circles represent the average energies obtained by ceo over 200 runs , where the black squares represent the global minima .",
    "figure 2*b * shows the success rate of hitting the global minima in 200 runs , the inset is the success rate for @xmath44 that may be unclear in the main plot.,title=\"fig : \" ]    similar to @xmath18-eo , we use @xmath18-ceo algorithm for the lj clusters optimization problem , where the probability function of ceo is @xmath19 .",
    "since there are @xmath45 pairs of interactional atoms in a lj cluster of size @xmath0 , we require @xmath46 updates where @xmath47 is a constant and fixed as 100 in the following computation . in order to avoid falling into the same local minimum too many times , before running lbfgs",
    ", we should make the system configuration far away from last local minimum , thus we run lbfgs every 20 time steps .",
    "that is to say , for a lj cluster of size @xmath0 , the present algorithm runs eo @xmath48 times and lbfgs @xmath49 times in total .",
    "we have carried out the @xmath18-ceo algorithm so many times for different @xmath18 and @xmath0 , and find that the algorithm performs better when @xmath18 is in the interval [ 1,2 ] . in figure 1 , we report the details for @xmath50 , where figure 1*a * shows the average energies obtained by ceo over 200 runs , and figure 1*b * exhibits the success rate @xmath51 of hitting the global minima@xcite . for both figure 1*a * and 1*b * , the main plot and inset represent the case @xmath22 and @xmath23 respectively .",
    "the readers should note that , although the difference of average energies between two different @xmath18 is great in the plot , it is very very small in fact .",
    "for the case @xmath22 , the best value of @xmath18 is @xmath52 corresponding the lowest average energy and highest success rate , however , the performance of ceo for @xmath53 is almost the same as @xmath52 in this case but obviously better than @xmath52 in the case @xmath23 .",
    "therefore , in the following computation , we set @xmath53 . we have also compared the performance of ceo on larger lj clusters for @xmath53 and @xmath52 , the two cases are pretty much the same thing and @xmath53 is a little better .    [ 0.8 ] the average cpu time(ms ) over 200 runs va the size of lj clusters . in the log - log plot ,",
    "the data can be well fitted by a straight line with slope @xmath54 , which indicates that the increasing tendency of cpu time @xmath55 vs cluster size is approximate to power - law form as @xmath56.,title=\"fig : \" ]    we demonstrate that for all the lj clusters of size @xmath0 not more than 100 , the global minima can be obtained by using ceo algorithm . in figure 2 , we report the performance of ceo on lj clusters optimization problem according to 200 independent runs . in figure 2*a * , the red circles represent the average energies obtained by ceo over 200 runs , where the black squares represent the global minima .",
    "one can see that the deviation from global minimum becomes greater and greater when the cluster size getting larger and larger , which indicates that for very large lj cluster , ceo may be a poor algorithm .",
    "figure 2*b * shows the success rate of hitting the global minima in 200 runs , the inset is the success rate for @xmath44 that may be unclear in the main plot .",
    "for both the case @xmath57 and @xmath58 , the global optimal solutions appears only once in 200 runs .",
    "although ceo is not a all - powerful algorithm and it may perform poorly for very large lj clusters , we demonstrate that it is competitive or even superior over some more elaborate stochastic optimization procedures like sa@xcite , ga@xcite in finding the most stable structure of lj clusters with minimal energy .",
    "finally , we investigate the average cpu time over 200 runs vs the size of lj clusters .",
    "the computations were carried out in a single pentiumiii processor(1ghz ) . from figure 3 , in the log - log plot ,",
    "the data can be well fitted by a straight line with slope @xmath59 , which indicates that the increasing tendency of cpu time @xmath55 vs cluster size is approximate to power - law form as @xmath56 .",
    "that means the ceo is a polynomial algorithm of order @xmath60 .",
    "in this paper , we explore a general - purpose heuristic algorithm for finding high - quality solutions to continuous optimization problems .",
    "the computing results indicate that this simple approach is competitive and sometimes can outperform some far more complicated or finely tuned nature inspired algorithm including simulated annealing and genetic algorithm , on a well - known np - hard continuous optimization problem for lj clusters(see reference@xcite for comparison ) . according to eo s updating rule ,",
    "it is clear that eo has very high ability in global searching , thus to combine eo and a strong local searching algorithm may produce a high efficient algorithm for continuous optimization problems .",
    "recently , several novel algorithms aiming at lj clusters optimization problem have been proposed , such as fast annealing evolutionary algorithm@xcite , conformational space annealing method@xcite , adaptive immune optimization algorithm@xcite , cluster similarity checking method@xcite , and so forth .",
    "these algorithms consider more about the special information about lj clusters and perform better than ceo .",
    "however , we have not found a compellent evidence indicating that there exists a general - purpose algorithm like sa or ga entirely preponderate over ceo on lj cluster optimization problem .",
    "it is worthwhile to emphasize that , in this paper , we do not want to prove that the ceo is an all - powerful algorithm , even do not want to say that the ceo is a good choice for chemists on lj cluster optimization problem since a general - purpose method often perform poorer than some special methods aiming at an idiographic problem .",
    "the only thing we want to say is the ceo , an extension of nature inspired algorithm eo , is a competitive algorithm and needs more attention .",
    "this work is supported by the outstanding youth fund from the national natural science foundation of china ( nnsfc ) under grant no .",
    "20325517 , and the teaching and research award program for outstanding young teacher ( trapoyt ) in higher education institutions of the ministry of education ( moe ) of china , the state key development programme of basic research of china ( 973 project ) , the nnsfc under grant no . 10472116 , 70471033 and 70271070 , and the specialized research fund for the doctoral program of higher education ( srfdp no.20020358009 ) .",
    "s. kirkpatrick , c. d. gelatt and m. p. vecchi , science * 220 * , 671(1983 ) .",
    "e. h. l. aarts and j. h. m. korst , _ simulated annealing and boltzmann machines _ ( john wiley and sons , 1989 )",
    ". j. holland , _ adaption in natural and artificial systems _",
    "( the university of michigan press , ann arbor , mi , 1975 ) .",
    "d. g. bounds , nature * 329 * , 215(1987 ) .",
    "d. e. goldberg , _ genetic algorithms in search , optimization , and machine learning _ ( addison - wesly , ma , 1989 ) .",
    "w. banzhaf , p. nordin , r. keller and f. francone , _",
    "genetic programming - an introduction _",
    "( morgan kaufmann , san francisco , ca , 1998 ) . s. boettcher and a. g. percus , artificial intelligence * 199 * , 275(2000 ) .",
    "s. boettcher , j. phys .",
    "a * 32 * , 5201(1999 ) .",
    "s. boettcher , comput .",
    "* 2 * , 6(2000 ) .",
    "s. boettcher , comput .",
    "* 2 * , 75(2000 ) .",
    "s. boettcher , a. g. percus and m. grigni , lect . notes . comput .",
    "1917 * , 447(2000 ) .",
    "p. bak and k. sneppen , phys .",
    "lett . * 71 * , 4083(1993 ) .",
    "k. sneppen , p. bak , h. flyvbjerg and m. h. jensen , proc .",
    "92 , 5209(1995 ) .",
    "a. a. middleton , phys .",
    "e * 69 * , 055701(r)(2004 ) .",
    "m. r. garey and d. s. johnson , _ computers and intractability : a guide to the theory of np - completeness _",
    "( freeman , new york , 1979 ) .",
    "s. boettcher and a. g. percus , phys .",
    "e * 64 * , 026114(2001 ) .",
    "s. boettcher and a. g. percus , phys .",
    "* 86 * , 5211(2001 ) .",
    "s. boettcher and a. g. percus , phys .",
    "e * 69 * , 066703(2004 ) .",
    "s. boettcher and m. grigni , j. phys .",
    "a * 35 * , 1109(2002 ) . in discrete optimization problem ,",
    "the set of all the states of system is denumerable , while in continuous optimization problem , the corresponding set is a continuum .",
    "t. p. martin , t.bergmann , h.ghilich and t. lange , chem .",
    "* 172*,209 ( 1990 )",
    ". l. t. wille , in : d. stauffer(ed . ) , annual reviewsof computational physics vii , world scientific , singnapore , 2000 .",
    "m. r. hoare , advan .",
    "* 40 * ( 1979 ) 49 , and references therein .",
    "b. xia , w. cai , x. shao , q. guo , b. maigret and z. pan , j. mol .",
    "struct.(theochem ) * 546 * , 33(2001 ) .",
    "w. cai , y. feng , x. shao and z. pan , j. mol .",
    "struct.(theochem ) * 579 * , 229(2002 ) .",
    "d. c. liu and j. nocedal , math .",
    "b * 45 * , 503(1989 ) .",
    "the complete up - to - date list of the global minima of lj clusters can be download from the web http://brian.ch.cam.ac.uk/. l. t. wille , computational materials science * 17 * , 551(2000 ) .",
    "d. m. deaven , n. tit , j. r. morris and k. m. ho , chem .",
    "* 256 * , 195(1996 ) .",
    "j. lee , i. -h .",
    "lee and j. lee , phys .",
    "lett . * 91 * , 080201(2003 ) .",
    "x. shao , l. cheng and w. cai , j. chem . phys . * 120 * , 11401(2004 ) .",
    "l. cheng , w. cai and x. shao , chem .",
    "* 389 * , 309(2004 ) ."
  ],
  "abstract_text": [
    "<S> in this paper , we explore a general - purpose heuristic algorithm for finding high - quality solutions to continuous optimization problems . the method , called continuous extremal optimization(ceo ) , can be considered as an extension of extremal optimization(eo ) and is consisted of two components , one is with responsibility for global searching and the other is with responsibility for local searching . with only one adjustable parameter , the ceo s </S>",
    "<S> performance proves competitive with more elaborate stochastic optimization procedures . </S>",
    "<S> we demonstrate it on a well known continuous optimization problem : the lennerd - jones clusters optimization problem . </S>"
  ]
}