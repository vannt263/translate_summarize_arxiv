{
  "article_text": [
    "branching processes ( bp ) are stochastic models used in population dynamics .",
    "the theory of such processes could be found in a number of books ( @xcite , @xcite , @xcite ) , and application of branching processes in biology is discussed in @xcite , @xcite , @xcite , @xcite .",
    "statistical inference in bp depends on the kind of observation available , whether the whole family tree has been observed , or only the generation sizes at given moments .",
    "some estimators considering different sampling schemes could be found in @xcite and @xcite .",
    "the problems get more complicated for multitype branching procesess ( mtpb ) where the particles are of different types ( see @xcite ) . yakovlev and yanev @xcite",
    "develop some statistical methods to obtain ml estimators for the offspring characteristics , based on observation on the relative frequencies of types at time @xmath0 .",
    "other approaches use simulation and monte carlo methods ( see @xcite , @xcite ) .    when the entire tree was not observed , but only the objects existing at given moment , an expectation maximization ( em ) algorithm could be used , regarding the tree as the hidden data .",
    "guttorp @xcite presents an em algorithm for the single - type process knowing generation sizes and in @xcite an em algorithm is used for parametric estimation in a model of y - linked gene in bisexual bp .",
    "such algorithms exist for strictures , called stochastic context - free grammars ( scfg ) .",
    "a number of sources point out the relation between mtbps and scfgs ( see @xcite , @xcite ) .",
    "this relation has been used in previous work @xcite to propose a computational scheme for estimating the offspring distribution of mtbp using the inside - outside algorithm for scfg ( @xcite ) .",
    "a new method , related to this , but constructed entirely for bp will be presented here .",
    "the em algorithm specifies a sequence that is guaranteed to converge to the ml estimator under certain regularity conditions . the idea is to replace one difficult ( sometimes impossible ) maximization of the likelihood with a sequence of simpler maximization problems whose limit is the desired estimator . to define an em algorithm two different likelihoods",
    "are considered  for the `` incomplete - data problem '' and for `` complete - data problem '' . when the incomplete - data likelihood is difficult to work with , the complete - data could be used in order to solve the problem .",
    "the conditions for convergence of the em sequence to the incomplete - data ml estimator are known and should be considered when such an algorithm is designed .",
    "more about the theory and applications of the em algorithm could be found in @xcite .",
    "the paper is organized as follows . in section 2 the model of mtbp with terminal types",
    "is introduced .",
    "section 3 shows the derivation of an em algorithm for estimating the offspring probabilities in general , and then proposes a recurrence scheme that could be used to ease the computations .",
    "a comprehensive example is given in section 4 and the results of a simulation study are shown in section 5 .",
    "a mtbp could be represented as @xmath1 , where @xmath2 denotes the number of objects of type @xmath3 at time @xmath0 , @xmath4 .",
    "an individual of type @xmath5 has offspring of different types according to a @xmath6-variate distribution @xmath7 and every object evolves independently . if @xmath8 , this is the bienaym - galton - watson ( bgw ) process . for the process with continuous time @xmath9 , define the _ embedded generation process _ as follows ( see @xcite ) .",
    "let @xmath10 _ number of objects in the @xmath11-th generation of _ @xmath12 . if we take the sample tree @xmath13 and transform it to a tree @xmath14 having all its branches of unit length but otherwise identical to @xmath13 , then @xmath15 , where @xmath16 is a bgw process .",
    "we call @xmath17 the embedded generation process for @xmath12 .",
    "the trees associated either with bgw process , or the embedded generation bgw process will be used to estimate the offspring probabilities .",
    "now we consider mtbp where certain _ terminal types _ of objects , once created , neither die nor reproduce ( see @xcite ) . if @xmath18 is the set of non - terminal types and @xmath19 is the set of terminal types , then an object of type @xmath20 produces offspring of any type and an object of type @xmath21 does not reproduce any more .",
    "here each @xmath22 constitutes a _ final group _ ( see @xcite ) . this way we model a situation where `` dead '' objects do not disappear , but are registered and present as `` dead '' through the succeeding generations .",
    "the process described above is reducible , because once transformed into a terminal type , an object remains in it s final group . for irreducible processes",
    "some statistical theory has been developed ( see @xcite for example ) , but for reducible ones statistical inference is case - dependent .",
    "we are interested in estimation of the offspring probabilities .",
    "if the whole tree @xmath13 is observed the natural ml estimator for the offspring probabilities is @xmath23 where @xmath24 is the number of times a node of type @xmath25 appears in the tree @xmath13 and @xmath26 is the number of times a node of type @xmath25 produces offspring @xmath27 in @xmath13 .",
    "it is not always possible to observe the whole tree though , often we have the following sampling scheme @xmath28 , for some @xmath29 .",
    "let @xmath30 consists of 1 object of some type .",
    "suppose we are able to observe a number of independent instances of the process , meaning that they start with identical objects and reproduce according to the same offspring distribution .",
    "such observational scheme is common in biological laboratory experiments .",
    "if @xmath0 is discrete @xmath12 is the number of objects in the @xmath0-th generation . for continuous time",
    "it is a `` generation '' in the embedded generation bgw process .",
    "a simple illustration is presented in fig",
    ". 1 a)c ) , where `` alive '' objects are grey , `` dead '' ones are white and numbers represent the different types .        here",
    "the notion that `` dead '' objects present and could be observed in succeeding generations as terminal types is crucial .",
    "if `` dead '' particles disappeared somewhere inside the `` hidden '' tree structure , estimation would be impossible ( see fig . 2 for an example ) .",
    "in the first tree information about the reproduction has been lost and we have the same observation as in the second tree.,width=321 ]",
    "the em algorithm was explained and given its name in a paper by dempster , laird , and rubin in 1977 @xcite .",
    "it is a method for finding maximum likelihood estimates of parameters in statistical models , where the model depends on unobserved latent variables .",
    "let a statistical model be determined by parameters @xmath31 , @xmath32 is the observation and @xmath33 is some `` hidden '' data , which determines the probability distribution of @xmath32 . then the density of the `` complete '' observation is @xmath34 and the density of the `` incomplete '' observation is the marginal one @xmath35 .",
    "we can write the likelihoods and the conditional density of @xmath33 given @xmath32 and @xmath31 this way : @xmath36 the aim is to maximize the log likelihood @xmath37 as @xmath38 is not observed the right side is replaced with its expectation under @xmath39 : @xmath40 - e_{\\theta'}[\\log k(y|\\theta , x)].\\ ] ] let @xmath41.$ ] it has been proved that @xmath42 with equality only if @xmath43 , or if @xmath44 for some other @xmath45 . choosing @xmath46 will make the difference positive and so , the likelihood could only increase in each step .",
    "expectation maximization algorithm _ is usually stated formally like this :    _ e - step : _ calculate function @xmath47 .",
    "_ m - step : _ maximize @xmath47 with respect to @xmath31 .",
    "the convergence of the em sequence @xmath48 depends on the form of the likelihood @xmath49 and the expected likelihood @xmath47 functions .",
    "the following condition , due to wu ( @xcite ) , guarantees convergence to a stationary point .",
    "it is presented here as cited in @xcite .",
    "_ if the expected complete - data log likelihood @xmath50 $ ] is continuous both in @xmath31 and @xmath51 , then all limit points of an em sequence @xmath48 are stationary points of @xmath49 , and @xmath52 converges monotonically to @xmath53 for some stationary point @xmath54 .",
    "_      let @xmath32 be the observed set of particles , @xmath13 is the unobserved tree structure and @xmath55 is the set of parameters  the offspring probabilities @xmath56 ( the probability that a particle of type @xmath25 produces the set of particles @xmath27 )",
    ". then the likelihood of the `` complete '' observation is : @xmath57 where @xmath58 is a counting function ",
    "@xmath59 is the number of times a particle of type @xmath25 produces the set of particles @xmath27 in the tree @xmath13 , observing @xmath32 .",
    "the probability of the `` incomplete '' observation is the marginal probability @xmath60 . for the em algorithm",
    "we need to compute the function    [ ndaskalova : qfun ] & q(|^(i ) ) = e_^(i)(p(,x| ) ) = _ p(|x,^(i))p(,x| ) + & = _",
    "p(|x,^(i))_t_v c(t_v ; , x)p(t_v ) + & = _ t_v _",
    "p(|x,^(i))c(t_v ; , x)p(t_v ) + & = _ t_v",
    "e_^(i)c(t_v ) p(t_v ) .",
    "we need to maximize the function ( [ ndaskalova : qfun ] ) under the condition @xmath61 , @xmath62 for every @xmath63 and @xmath64 .",
    "the lagrange method requires to introduce the function @xmath65 taking partial derivatives with respect to @xmath56 and obtaining the lagrangian multiplier @xmath66 , we get to the result that the re - estimating parameters are the normalized expected counts , which look like the ml estimators in the `` complete '' observation case ( [ ndaskalova : mle ] ) , but the observed counts are replaced with their expectations .",
    "@xmath67 where the expected number of times a particle of type @xmath25 appears in the tree @xmath13 is : @xmath68 and the expected number of times a particle of type @xmath25 gives offspring @xmath27 in the tree @xmath13 is : @xmath69    it is easy to check that in this case the convergence condition stated above is fulfilled .",
    "we consider the representation @xmath70 where @xmath71 is a polynomial function of @xmath72-s  the offspring probabilities , estimated on the @xmath73-th step .",
    "then @xmath74 is a sum of continuous functions in all the parameters @xmath75 and @xmath72 , so it is also continuous .",
    "this way we are sure to converge to a stationary value , though it might not always be a global maximizer .",
    "it is a case of em where the m - step is explicitly solved , so the computational effort will be on the e - step .",
    "the problem is that in general enumerating all possible trees @xmath13 is of exponential complexity .",
    "the method proposed below is aimed to reduce complexity .      we have shown that the e - step of the algorithm consists of determining the expected number of times a given type @xmath25 or a given production @xmath76 appears in a tree @xmath13 .",
    "a general method will be proposed here for computing these counts .",
    "the algorithm consists of three parts  calculation of the inner probabilities , the outer probabilities and em re - estimation , which are shown below .",
    "let us define the * inner * probability @xmath77 of a subtree rooted at particle of type @xmath25 to produce outcome @xmath78 where @xmath79 is the number of objects of type @xmath5 ( fig .",
    "3 ) . from the basic branching property of the process",
    "we get the following recurrence :    & ( i , v ) = _ w p(t_v \\{t_w_1 ,  , t_w_k } ) _ i_1+  +i_k = i ( i_1,t_w_1)  (i_k , t_w_k )    where @xmath80 are all possible sets of particles that @xmath25 can produce .",
    "the * outer * probability @xmath81 is the probability of the entire tree excluded a subtree , rooted at particle of type @xmath25 and producing outcome @xmath78 ( fig .",
    "the recurrence here is :    & ( i , v ) = _ w _",
    "p(t_w \\{t_v , t_v_(2 ) ,  , t_v_(m ) } ) + & _ j - i ( i+j , w ) _",
    "+j_m = j ( j_2,v_(2))  (j_m , v_(m ) )    where @xmath82 are all possible sets of particles that @xmath83 can produce and @xmath84 is the observed set of objects .        for every @xmath85 the expected number of times @xmath24 that @xmath25 is used in the tree @xmath13 could be presented as follows :    & e_c(t_v ) = _ p(|x , ) c(t_v;,x ) = _ c(t_v;,x ) + & = _",
    "p(,x|)c(t_v;,x ) = _ : t_v p(,x| ) + & = _ i(i , v)(i , v ) ,    where @xmath77 and @xmath81 are the inner and outer probabilities for all @xmath86 .",
    "similarly , the expected number of times a production @xmath87 is used could be calculated :    & e_^(i)c(t_v \\{t_w_1 ,  , t_w_k } ) + & = _ i _ i_1+  +i_k = i ( i , v)(i_1,w_1) ",
    "(i_k , w_k)p^(i)(t_v \\{t_w_1 ,  , t_w_k } )",
    ".    dividing the expectations above , we obtain the * em re - estimation * of the parameters :    & p^(i+1)(t_v \\{t_w_1 ,  , t_w_k } ) + & =    for several observed sets of objects the expected numbers in the nominator and denominator are summed for all sets .",
    "such generally stated the algorithm still has high complexity . in practical applications",
    "though , often there are small number of types and not all of the productions in the offspring distributions are allowed .",
    "thus , for a number of cases , a specific dynamic programming schemes based on the above recurrence could be proposed , which will be less complex .",
    "we consider a mtbp with four types of particles  two nonterminal @xmath88 , @xmath89 and two terminal @xmath90 , @xmath91 , and the productions that are allowed ( with nonzero probability ) are : @xmath92 to cover the possibility of observing particles of types @xmath88 and @xmath89 the additional productions @xmath93 and @xmath94 are introduced .    now , let @xmath95 and the process has started with one particle of type @xmath88 .",
    "we do nt have further information about the distribution , so an uniform one is assumed for every type : @xmath96 and @xmath97 .",
    "let @xmath98 be the probability of a tree rooted at a particle of type @xmath25 to produce the number of particles of types @xmath99 respectively .",
    "the initialization of @xmath100 should be as follows : @xmath101    for the level of two particles we are interested in all possible subsets of @xmath102 containing two @xmath103-s .",
    "the values of @xmath100 are calculated below :    & _",
    "( 1,0,1,0)^1 = p_11 ^ 1_(1,0,0,0)^1_(0,0,1,0)^1 + p_12 ^ 1_(1,0,0,0)^1_(0,0,1,0)^2 + p_12 ^ 1_(0,0,1,0)^1_(1,0,0,0)^2 + & = 1/4.1/4.1/4 + 1/4.1/4.0 + 1/4.1/4.0 = 1/64 ,    similarly , @xmath104 also @xmath105 finally    & _ ( 1,0,1,1)^1 = p_11 ^ 1[_(1,0,1,0)^1_(0,0,0,1)^1 + _ ( 1,0,0,1)^1_(0,0,1,0)^1 + _ ( 1,0,0,0)^1_(0,0,1,1)^1 ]",
    "+ p_12 ^ 1[_(1,0,1,0)^1_(0,0,0,1)^2 + & + _ ( 1,0,0,1)^1_(0,0,1,0)^2 + _",
    "( 1,0,0,0)^1_(0,0,1,1)^2 + _",
    "( 1,0,1,0)^2_(0,0,0,1)^1 + _",
    "( 1,0,0,1)^2_(0,0,1,0)^1 + _ ( 1,0,0,0)^2_(0,0,1,1)^1 ] + & = 1/4.(1/64.0 + 1/48.1/4 + 1/4.1/48)+ 1/4.(1/64.1/3 + 1/48.0 + 1/4.0 + 0.0 + 0.1/4 + 0.1/48 ) + & = 1/256 .",
    "next follow the calculations of the outer probabilities @xmath106 .",
    "the initial values are @xmath107 and @xmath108 .",
    "then :    & _",
    "( 1,0,1,0)^1 = _",
    "( 1,0,1,1)^1[p_11 ^ 1_(0,0,0,1)^1 + p_12 ^ 1_(0,0,0,1)^2 ] = 1.(1/4.0 + 1/4.1/3 ) = 1/12 , + & _",
    "( 1,0,0,1)^1 = 1/16 , _",
    "( 0,0,1,1)^1 = 1/16 , + & _",
    "( 1,0,1,0)^2 = _ ( 1,0,1,1)^1p_12 ^ 1_(0,0,0,1)^1 + _",
    "( 1,0,1,1)^2p_22 ^ 2_(0,0,0,1)^2 = 1.1/4.0 + 0.1/3.1/3 = 0 , + & _",
    "( 1,0,0,1)^2 = 1/16 , _",
    "( 0,0,1,1)^2 = 1/16 , +    & _ ( 1,0,0,0)^1 = _ ( 1,0,1,0)^1[p_11 ^ 1_(0,0,1,0)^1 + p_12 ^ 1_(0,0,1,0)^2 ] + & + _ ( 1,0,0,1)^1[p_11 ^ 1_(0,0,0,1)^1 + p_12 ^ 1_(0,0,0,1)^2 ] + _ ( 1,0,1,1)^1[p_11 ^ 1_(0,0,1,1)^1 + p_12 ^ 1_(0,0,1,1)^2 ] + & = 1/12.(1/4.1/4 + 1/4.0 ) + 1/16.(1/4.0 + 1/4.1/3 ) + 1.(1/4.1/48 + 1/4.0 ) = 1/64 , + & _",
    "( 0,0,1,0)^1 = 1/64 _ ( 0,0,0,1)^1 = 3/256 ,    & _",
    "( 1,0,0,0)^2 = _ ( 1,0,1,0)^1p_12 ^ 1_(0,0,1,0)^1 + _ ( 1,0,1,0)^2p_22 ^ 2_(0,0,1,0)^2 + _ ( 1,0,0,1)^1[p_12 ^ 1_(0,0,0,1)^1 + & + _ ( 1,0,0,1)^2p_22 ^ 2_(0,0,0,1)^2 ] + _ ( 1,0,1,1)^1[p_12 ^ 1_(0,0,1,1)^1 + _",
    "( 1,0,1,1)^1p_22 ^ 2_(0,0,1,1)^2 ] + & = 1/12.1/4.1/4 + 0.1/3.0 + 1/16.1/4.0 + 1/16.1/3.1/3 + 1.1/4.1/48 + 1.1/3.0 = 5/288 , + & _",
    "( 0,0,1,0)^2 = 5/288 , _",
    "( 0,0,0,1)^2 = 1/256 .    using that @xmath109 , we are able to compute the expected values we need :    & e_c(t_1 ) = _ i_i^1_i^1 = ( 1/4.1/64 + 1/4.1/64 + 0 + & + 1/64.1/12 + 1/48.1/16 + 1/48.1/16 + 1/256 ) = = 4 ,    & e_c(t_1 \\{t_1 , t_1 } ) = _ i _ i_1+i_2=i _ i^1_i_1 ^ 1_i_2 ^ 1p(t_1 \\{t_1 , t_1 } ) = = 1    & e_c(t_1 \\{t_1 , t_2 } ) = _ i _ i_1+i_2=i _ i^1_i_1 ^ 1_i_2 ^ 2p(t_1 \\{t_1 , t_2 } ) = = 1    & e_c(t_1 \\{t_1^t } ) = _",
    "t_1^t^1p(t_1 \\{t_1^t } ) = 1/4.1/64 = = 1 ,    & e_c(t_1 \\{t_1 } ) = _ t_1",
    "^ 1p(t_1 \\{t_1 } ) = 1/4.1/64 = = 1 .",
    "thus , the estimated distribution for @xmath88 is    & _ 11 ^ 1 = = 1/4 , _ 12 ^ 1 = = 1/4 , + & _ 1 ^ 1 = = 1/4 , _",
    "t^1 = = 1/4 ,    which is the same as the initially chosen one , so this would be the final estimation .    for the offspring distribution of @xmath89 similar computations lead to the result : @xmath110 so the estimation is : @xmath111 which converges on the next iteration also .",
    "simulation experiment has been performed to study behaviour of the estimates obtained via the algorithm .",
    "observations have been simulated according to the model ( [ ndaskalova : simmod ] ) in the previous section with offspring probabilities @xmath112 and @xmath113 .",
    "estimation has been performed using different sample sizes both for the number of observations , and the tree sizes as well .",
    "all the computations were made in r @xcite .",
    "it is important to investigate how the size of the tree , which corresponds to the size of the `` hidden '' part of the observation , affects the estimates . in table",
    "[ ndaskalova : tabl1 ] are shown the result for small tree sizes and sample size 20 .",
    "the most accurate estimates are obtained through averaging these results .",
    "it can be seen that there is great variation in the estimate of the individual distribution of type 2 , thought the mean is close to the real values . for larger sample sizes",
    "the variance of the estimates is reduced , but there is some bias in the estimate for type 2 ( table [ ndaskalova : tabl2 ] ) .",
    "larger sample trees also lead to biased estimates for the individual distribution for type 2 ( table [ ndaskalova : tabl3 ] ) .",
    "using larger trees is also computationally more expensive .",
    ".estimation obtained using small tree samples of size 20 . [ cols=\"<,^,^,^,^,^\",options=\"header \" , ]     the bias in the estimate for type 2 is due to the greater uncertainty in the process for type 2 : these particles could be generated by a particle of their own type , as well as , by a particle of type 1 .",
    "for example , production of one particle of type 1 and two of type 2 could happen in two ways : once @xmath114 and then @xmath115 , or twice @xmath114 .",
    "so , in general , productions @xmath114 take part more often in the estimation than productions @xmath115 .",
    "as the branching is hidden and all possible generations have to be taken in account , this results in underestimation of @xmath116 and some overestimation of @xmath117 when that hidden part gets larger .",
    "a general em algorithm has been proposed to find ml estimation of the offspring probabilities of mtbp with terminal types when only an observation of the generation at given moment is available .",
    "the example presented shows that the algorithm is straightforward and convenient to apply for a particular model .",
    "simulation study shows that better estimates are obtained using smaller samples .",
    "such algorithms would be useful in biological models like cell proliferation , genetics , genomics , evolution , and wherever a model of mtbp with terminal types is suitable .",
    "geman , s. , m. johnson ( 2004 ) _ probability and statistics in computational linguistics , a brief review _ mathematical foundations of speech and language processing .",
    "johnson , m. ; khudanpur , s.p . ; ostendorf , m. ; rosenfeld , r. ( eds . )    gonzlez m. , j. martn , r. martnez , m. mota ( 2008 ) _ non - parametric bayesian estimation for multitype branching processes through simulation - based methods _ , computational statistics & data analysis , * 52(3 ) * , 12811291 .",
    "gonzlez , m. and gutirrez , c. and martnez , r. ( 2010 ) _ parametric inference for y - linked gene branching model : expectation - maximization method _ , proceeding of workshop on branching processes and their applications , lecture notes in statistics , * 197 * , 191204 ."
  ],
  "abstract_text": [
    "<S> multitype branching processes ( mtbp ) model branching structures , where the nodes of the resulting tree are objects of different types . </S>",
    "<S> one field of application of such models in biology is in studies of cell proliferation . a sampling scheme that appears </S>",
    "<S> frequently is observing the cell count in several independent colonies at discrete time points ( sometimes only one ) . </S>",
    "<S> thus , the process is not observable in the sense of the whole tree , but only as the `` generation '' at given moment in time , which consist of the number of cells of every type . </S>",
    "<S> this requires an em - type algorithm to obtain a maximum likelihood ( ml ) estimation of the parameters of the branching process . </S>",
    "<S> a computational approach for obtaining such estimation of the offspring distribution is presented in the class of markov branching processes with terminal types .    </S>",
    "<S> * msc : * primary 60j80 , 62g05 ; secondary 60j80 , 62p10 + * * multitype branching processes , terminal types , offspring distribution , maximum likelihood estimation , expectation maximization , inside - outside algorithm . </S>"
  ]
}