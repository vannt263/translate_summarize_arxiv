{
  "article_text": [
    "a non trivial problem that appears in several applications , e.g. in multivariate regression and bayesian statistics , is the estimation of the mean of a truncated normal distribution .",
    "the problem arises in cases where a random vector @xmath0^t$ ] follows a normal distribution with mean @xmath1 and covariance matrix @xmath2 , denoted by @xmath3 , but @xmath4 is restricted to a closed subset @xmath5 of @xmath6 .",
    "however , the restrictions considered in most of the cases are of the form @xmath7 , with @xmath8 , @xmath9 . if @xmath10 and @xmath11 ( @xmath12 and @xmath13 ) , we have a _",
    "one - sided truncation _ from the left ( right ) , while in the case where both @xmath14 and @xmath15 are reals , we have _ two - sided truncation_.    the problem has attracted the attention of several researchers from the sixties . since then , several deterministic approaches have been proposed with some of them trying to estimate not only the mean but also other moments ( such as the variance ) of a multivariate truncated normal distribution .",
    "these approaches can be categorized according to whether they are suitable for one - sided truncated normal distributions ( @xcite , @xcite , @xcite , @xcite , @xcite ) or for two - sided truncated normal distributions ( @xcite , @xcite , @xcite , @xcite , @xcite , @xcite ) , or according to whether they consider the bivariate , @xmath16 , ( @xcite , @xcite , @xcite , @xcite ) or the multivariate case ( @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite ) .",
    "in addition , some of these methods put additional restrictions to the distribution paramaters ( e.g. @xcite , @xcite , @xcite require that @xmath17 ) . most of these methods either perform direct integration ( e.g. @xcite ) or they utilize the moment generating function tool ( e.g. @xcite ) .    an alternative strategy to deal with this problem has been followed in @xcite .",
    "more specifically , in @xcite , a markov chain monte carlo ( mcmc ) iterative scheme has been developed . according to this , at each iteration , @xmath18 succesive samplings",
    "take place , one from each one - dimensional conditionals of the truncated normal distribution .",
    "the mean of the @xmath19-th such distribution is the mean of @xmath20 conditioned on the @xmath21 . after performing several iterations , the estimation of the mean of the truncated normal results by performing an averaging over the produced samples .",
    "convergence issues of this scheme to the mean of the truncated normal distribution are a subject of the markov chain theory .",
    "a relative work that accelerates the method in @xcite is exhibited in @xcite .",
    "the work presented in this paper for approximating the mean of a multivariate truncated normal distribution has been inspired from that of @xcite .",
    "specifically , instead of selecting a sample from each one of the above one - dimensional distributions , we select its mean .",
    "thus , the proposed scheme departs from the statistical framework adopted in @xcite and moves to the deterministic one .",
    "this work is an extension of a relative scheme used in @xcite in the framework of spectral unmixing in hyperspectral images .",
    "in addition , a convergence proof of the proposed scheme is given when certain conditions are fulfilled .",
    "the quality of the approximation of the mean offered by the proposed method is assessed via the case where @xmath2 is the exponential correlation matrix .",
    "experimental results show that the new scheme converges significantly faster than the mcmc approach of @xcite .",
    "the paper is organized as follows .",
    "section 2 contains some necessary definitions and a brief description of the work in @xcite . in section 3 ,",
    "the newly proposed method is described and in section 4 conditions are given under which it is proved to converge . in section 5 , the proposed method is applied to the case where @xmath2 is the exponential correlation matrix . in section 6",
    "simulation results are provided and a relevant discussion is presented .",
    "finally , section 7 concludes the paper .",
    "let us consider the @xmath18-dimensional normal distribution @xmath22 where the @xmath18-dimensional vector @xmath1 is its mean and the @xmath23 matrix @xmath2 is its covariance matrix .",
    "let @xmath5 be a subset of @xmath6 with positive lebesgue measure .",
    "we denote by @xmath24 the truncated normal distribution which results from the truncation of @xmath25 in @xmath5 . speaking in mathematical terms @xmath26",
    "note that @xmath24 is _ proportional _ to @xmath27 , where @xmath28 , if @xmath29 and @xmath30 , otherwise .    in the scheme discussed in @xcite , a markov chain monte carlo ( mcmc ) method",
    "is proposed , to compute the mean of single or doubly truncated ( per coordinate ) normal distribution @xmath24 , where @xmath31 \\times [ a_2,b_2 ] \\times \\ldots \\times [ a_n , b_n]$ ] .",
    "the method relies on the sampling of the @xmath18 one - dimensional conditionals of the truncated normal distribution .",
    "more specifically , letting @xmath32 @xmath33 $ ] and @xmath34 denoting the expectation and the variance of @xmath20 conditioned on the rest coordinates , respectively follows the ( non - truncated ) @xmath19-th conditional of @xmath25 . ] , and @xmath35}(\\mu_i^*,$ ] @xmath36 denoting the ( one dimensional ) truncated normal distribution which results from the truncation of a normal distribution with mean @xmath37 and variance @xmath34 in @xmath38 $ ] , the iterative sampling scheme proposed in @xcite can be written as @xmath39}(e[x_1|x_2^{(t-1)},\\ldots , x_n^{(t-1)}],\\sigma_1^{*2 } ) \\\\ 2 . & x_2^{(t ) } \\sim { \\cal n}_{[a_2,b_2]}(e[x_2|x_1^{(t)},x_3^{(t-1)}\\ldots , x_n^{(t-1)}],\\sigma_2^{*2 } ) \\\\ \\vdots &   \\\\ n. & x_n^{(t ) } \\sim { \\cal n}_{[a_n , b_n]}(e[x_n|x_1^{(t)},x_2^{(t)}\\ldots , x_{n-1}^{(t)}],\\sigma_n^{*2 } ) \\end{array}\\ ] ] where @xmath40 denotes the sampling action and @xmath41 denotes the current iteration . after performing several ,",
    "say @xmath42 , iterations ( and after discarding the first few , say @xmath43 , ones ) the mean of each coordinate is estimated as @xmath44=\\frac{1}{k - k ' } \\sum_{t = k'}^k x_i^{(t ) } , \\ \\",
    "i=1,\\ldots , n\\ ] ]    the quantities @xmath37 and @xmath45 of each one of the above one dimensional conditionals are expressed in terms of the parameters @xmath1 and @xmath2 of the non - truncated multidimensional normal distribution as follows @xmath46    @xmath47    with @xmath48 being the @xmath49 matrix that results from @xmath2 after removing its @xmath19-th column and its @xmath19-th row , @xmath50 being the @xmath19-th column of @xmath2 excluding its @xmath19-th element and @xmath51 , @xmath52 being the ( @xmath53-dimensional ) vectors that result from @xmath4 and @xmath1 , respectively , after removing their @xmath19-th coordinates , @xmath20 and @xmath54 , respectively . note that @xmath37 depends on all @xmath55 s _ except _ @xmath20 .",
    "in the sequel , we focus on the case where @xmath5 is a set of the form @xmath56 \\times [ a_2,b_2 ] \\times \\ldots \\times [ a_n , b_n]$ ] , where for each interval @xmath38 $ ] it is either , ( i ) @xmath57 and @xmath58 or ( ii ) @xmath59 and @xmath60 .",
    "this means that , along each dimension , the truncation is one - sided and more specifically , case ( i ) corresponds to right truncation while case ( ii ) corresponds to left truncation .",
    "the proposed model for estimating the mean of @xmath61 @xmath62 , is of iterative nature and , at each iteration , it requires the computation of the ( one - dimensional ) @xmath63 function .",
    "this model has a close conceptual affinity with the one ( briefly ) presented in the previous section ( @xcite ) .",
    "more specifically , instead of utilizing the samples produced by the ( one dimensional ) distributions @xmath35}(e[x_i|x_1^{(t)},x_2^{(t ) } \\ldots , x_{i-1}^{(t ) } , x_{i+1}^{(t-1)},\\ldots,$ ] @xmath64,\\sigma_i^{*2})$ ] , we use the corresponding mean values ( here denoted by @xmath65 @xmath66^t$ ] ) .    as it is well known ( see e.g. @xcite ) , the mean @xmath67 of a truncated one dimensional normal distribution @xmath68 } ( \\mu^*,\\sigma^{*2})$ ] , which has resulted from the ( non - truncated ) normal distribution with mean @xmath69 and variance @xmath70 , is expressed as @xmath71 where @xmath72 , @xmath73 and @xmath74 .    however , since in the present paper we consider the cases where either ( i ) @xmath59 and @xmath60 or ( ii ) @xmath57 and @xmath58 , let us see now how eq .",
    "( [ truncmean1d ] ) becomes for each of these cases .",
    "\\(i ) @xmath75 , @xmath76 . in this case",
    "it is @xmath77 and as a consequence , @xmath78 , @xmath79 .",
    "thus , taking also into account the definitions of @xmath80 and @xmath81 and the fact that @xmath82 , eq .",
    "( [ truncmean1d ] ) gives ,",
    "@xmath83    \\(ii ) @xmath84 , @xmath85 . in this case",
    "it is @xmath86 and , as a consequence , @xmath87 , @xmath88 .",
    "working as in case ( i ) , eq .",
    "( [ truncmean1d ] ) gives @xmath89    eqs .",
    "( [ case - i ] ) and ( [ case - ii ] ) can be expressed compactly via the following single equation @xmath90 \\sigma^*\\ ] ] where @xmath91 ( @xmath92 ) is an indicator function which equals to @xmath93 if @xmath94 ( @xmath95 ) and @xmath30 otherwise and @xmath96    let us now return to the multidimensional case . since from the ( truncated ) conditional one - dimensional normals we no longer perform sampling but , instead , we consider their means , eq . ( [ trunc - mean ] ) is altered to @xmath97 where @xmath98 results from the current estimate of the ( @xmath18 - dimensional ) mean vector @xmath65 of the truncated normal distribution , after removing its @xmath19-th coordinate .",
    "putting now all the previous ingredients together ( that is , utilizing eqs .",
    "( [ cases - i - ii ] ) , ( [ trunc - mean2 ] ) and ( [ trunc - var ] ) ) we obtain the following iterative scheme    @xmath99 \\sigma_1^ * \\\\ 2 . & w_2^{(t ) } = \\mu_2^{*(t ) } + \\sqrt{\\frac{2}{\\pi } } \\left [ f(\\frac{\\mu_2^{*(t)}-a_2}{\\sqrt{2 } \\sigma_2^ * } ) i_{a_2 \\in { \\cal r } } -   f(\\frac{b_2-\\mu_2^{*(t)}}{\\sqrt{2 } \\sigma_2^ * } ) i_{b_2 \\in { \\cal r } } \\right ] \\sigma_2^ * \\\\ \\vdots &   \\\\ n. & w_n^{(t ) } = \\mu_n^{*(t ) } + \\sqrt{\\frac{2}{\\pi } } \\left [ f(\\frac{\\mu_n^{*(t)}-a_n}{\\sqrt{2 } \\sigma_n^ * } ) i_{a_n \\in { \\cal r } } -   f(\\frac{b_n-\\mu_n^{*(t)}}{\\sqrt{2 } \\sigma_n^ * } ) i_{b_n \\in { \\cal r } } \\right ] \\sigma_n^ * \\end{array}\\ ] ]    with @xmath100 being computed via eq .",
    "( [ trunc - mean2 ] ) , where @xmath101 ( the only parameter in ( [ trunc - mean2 ] ) that varies through iterations ) is defined as @xmath102^t$ ] , that is , the most recent information about @xmath103 s is utilized .",
    "more formally , we can say that the above scheme performs sequential updating and ( following the terminology used in @xcite ) it is a _ gauss seidel _ updating scheme .",
    "it is reminded that , due to the type of truncation considered here ( only left truncation or only right truncation per coordinate ) , the bracketed expression in each equation of ( [ new ] ) contains _ only one _ non identically equal to zero term . in the sequel , we consider separately the cases corresponding to @xmath10 and @xmath13 , i.e. ,    @xmath104    and @xmath105 where @xmath106 and @xmath107 @xmath9 , respectively , and @xmath108 is defined as in eq .",
    "( [ f ] ) .",
    "in this section we provide sufficient conditions under which the proposed scheme is proved to converge . before we proceed , we give some propositions and remind some concepts that will be proved useful in the sequel .    _",
    "proposition 1 : _ assume that @xmath2 is a symmetric positive definite @xmath23 matrix , @xmath50 is the @xmath19-th column of @xmath2 , after removing its @xmath19-th element and @xmath48 results from @xmath2 after removing its @xmath19-th row and its @xmath19-th column .",
    "also , let @xmath109 be the @xmath110 element of @xmath111 and @xmath112 be the @xmath53-dimensional vector resulting from the @xmath19-th row of @xmath111 after ( i ) removing its @xmath19-th element , @xmath109 , and ( ii ) multiplying the remaining elements by @xmath113 .",
    "then , it holds    \\(i ) @xmath114 and    \\(ii ) @xmath115 .",
    "the proof of this proposition is straightforward from the inversion lemma for block partitioned matrices ( @xcite , p. 53 ) and the use of permutation matrices , in order to define the schur complement @xmath116 for each row @xmath19 of @xmath2 .",
    "_ proposition 2 : _ it is @xmath117 where @xmath118 denotes the derivative of @xmath119 , which is defined in eq .",
    "( [ f ] ) .",
    "the proof of proposition 2 is given in the appendix .",
    "_ definition 1 : _ a mapping @xmath120 , where @xmath121 , is called _ contraction _ if for some norm @xmath122 there exists some constant @xmath123 ( called _ modulus _ ) such that @xmath124 is called _ contracting iteration_.    _ proposition 3 ( @xcite , pp .",
    "182 - 183 ) : _ suppose that @xmath125 is a contraction with modulus @xmath123 and that @xmath126 is a closed subset of @xmath6 .",
    "then    \\(a ) the mapping @xmath127 has a unique fixed point @xmath128 . is called _ fixed point _ of a mapping @xmath127 if it is @xmath129 . ]",
    "\\(b ) for every initial vector @xmath130 , the sequence @xmath131 , generated by @xmath132 converges to @xmath133 geometrically .",
    "in particular , @xmath134    let us define the mappings @xmath135 , @xmath9 as @xmath136 \\sigma_i^ * \\label{t - i}\\end{aligned}\\ ] ] where @xmath137 and @xmath138    and the mapping @xmath127 as @xmath139    let us define next the mapping @xmath140 as @xmath141    performing the sequential updating as described by eq .",
    "( [ new ] ) ( one at a time and in increasing order ) is equivalent to applying the mapping @xmath142 , defined as @xmath143 where @xmath144 denotes function composition . following the terminology given in @xcite ,",
    "@xmath145 is called _ the gauss seidel mapping based on the mapping @xmath127 _ and the iteration @xmath146 is called _ the gauss seidel algorithm based on mapping @xmath127_.    a direct consequence of ( * ? ? ?",
    "1.4 , pp.186 ) is the following proposition : _ proposition 4 : _ if @xmath125 is a contraction with respect to the @xmath147 norm , then the gauss - seidel mapping @xmath145 is also a contraction ( with respect to the @xmath147 norm ) , with the same modulus as @xmath127 . in particular ,",
    "if @xmath126 is closed , the sequence of the vectors generated by the gauss - seidel algorithm based on the mapping @xmath127 converges to the unique fixed point of @xmath127 geometrically . having given all the necessary definitions and results",
    ", we will proceed by proving that ( a ) for each mapping @xmath148 it holds @xmath149 , @xmath150 , where @xmath151 , @xmath152 are the @xmath153 and @xmath147 norms , respectively , ( b ) if @xmath111 is diagonally dominant then @xmath127 is a contraction and ( c ) provided that @xmath127 is a contraction , the algorithm @xmath146 converges geometrically to the unique fixed point of @xmath127 .",
    "we remind here that the @xmath53-dimensional vector @xmath154 results from the @xmath19-th row of @xmath111 , exluding its @xmath19-th element @xmath109 and dividing each element by the negative of @xmath109 .",
    "_ proposition 5 : _ for the mappings @xmath148 , @xmath9 , it holds @xmath155    _ proof : _ ( a ) we consider first the case where @xmath10 .",
    "let us consider the vectors @xmath156 . since @xmath1 is constant , utilizing eq .",
    "( [ mu - i ] ) it follows that @xmath157 also , it is @xmath158    taking the difference @xmath159 we have @xmath160    since @xmath161 is continuous in @xmath162 , the mean value theorem guarantees that there exists @xmath163 @xmath164 $ ] such that @xmath165 substituting eq .",
    "( [ mvt ] ) to ( [ diff - t ] ) we get @xmath166 substituting ( [ diff - mu - i ] ) and ( [ diff - a - i ] ) into ( [ diff - t1 ] ) and after some manipulations it follows that @xmath167    taking absolute values in eq .",
    "( [ diff - t2 ] ) and applying hlder s inequality @xmath168 , for @xmath169 and @xmath170 , it follows that @xmath171 ( from proposition 2 ) , and the ( trivial ) fact that @xmath172 , it follows that latexmath:[\\[\\label{norm - t }    thus , the claim has been proved .",
    "\\(b ) we consider now the case where @xmath13 .",
    "working similarly to the previous case , the difference @xmath174 is @xmath175 while the difference @xmath159 is expressed as    @xmath176    utilizing the mean value theorem we have that there exists @xmath177 @xmath178 $ ] such that @xmath179 substituting eq .",
    "( [ mvt1 ] ) to ( [ diff - t - b ] ) we get    @xmath180    substituting eqs .",
    "( [ diff - mu - i ] ) and ( [ diff - b - i ] ) into ( [ diff - t1-b ] ) , we obtain @xmath181 from this point on , the proof is exactly the same with that of ( a ) .",
    "_ proposition 6 : _ the mapping @xmath127 is a contraction in @xmath6 , with respect to the @xmath147 norm , provided that @xmath111 is diagonally dominant . _ proof : _ let @xmath182 .",
    "taking into account proposition 5 , it easily follows that @xmath183 @xmath184    now , ( a ) taking into account that the @xmath53-dimensional vector @xmath154 results from the @xmath19-th row of @xmath111 , exluding its @xmath19-th element @xmath109 and dividing each element by the negative of @xmath109 and ( b ) recalling that @xmath185 is the @xmath19-th row of @xmath111 excluding its @xmath19-th element @xmath109 , it is    @xmath186 provided that @xmath111 is diagonally dominant , it is @xmath187 or @xmath188 which proves the claim .",
    "_ theorem 1 : _ the algorithm @xmath146 converges geometrically to the unique fixed point of @xmath127 , provided that @xmath111 is diagonally dominant .",
    "_ proof : _ the proof is a direct consequence of the propositions 3 , 4 and 6 exposed before , applied for @xmath189 .",
    "an issue that naturally arises with the proposed method is how accurate the estimate of the mean is . since it is very difficult to give a theoretical analysis of this issue , mainly due to the highly complex nature of the propsoed iterative scheme ( see eq .",
    "( [ new ] ) ) , we will try to gain some insight for this subject via experimentation . to this end",
    ", we set @xmath2 equal to the _ exponential correlation _ matrix , which is frequently met in various fields of applications , e.g. , in signal processing applications .",
    "its general form is @xmath190 , \\ \\ ( 0 \\leq \\rho < 1)\\ ] ] it is easy to verify that the inverse of @xmath191 is expressed as @xmath192\\ ] ] also , it is straightforward to see that @xmath193 is diagonally dominant for all values of @xmath194 . thus , it is a suitable candidate for our case .",
    "in addition , it is `` controlled '' by just a single parameter ( @xmath195 ) , which facilitates the extraction of conclusions .",
    "note that for @xmath196 , @xmath193 becomes the identity matrix , while as @xmath195 increases towards @xmath93 the diagonal dominancy of @xmath193 decreases ( while its condition number increases ) . for @xmath195 close to @xmath93 , @xmath193 is alomost singular .    in the sequel",
    ", we consider the case of a zero mean normal distribution with covariance matrix as in ( [ exp - corr-1 ] ) , which is truncated in the region @xmath197^n$ ] , that is the truncation point is the same along all dimensions . without loss of generality , this choice has been deliberately selected , in order to keep our experimental framework controlled by just two parameters , @xmath195 and @xmath198 .",
    "performing simulations for various combinations of the values of @xmath195 and @xmath198 ( and for various dimensions @xmath18 ) we can gain some insight on the accuracy of the approximation of the mean provided by the proposed method . in the sequel , we use as benchmark the estimate of the mean provided by the ( widely accepted as reliable ) mcmc method ( @xcite ) .",
    "figure 1 , shows a three - dimensional graph of the difference ( assessed by its euclidean norm divided by @xmath18 ) between the estimates of the truncated mean obtained by the mcmc and the proposed methods , against @xmath195 and @xmath198 .",
    "it is worth noting that for smaller values of @xmath195 ( less than @xmath199 ) , the difference remains low ( less than @xmath200 ) , independently of the value of the cutting point @xmath198 . on the other hand , for larger values of @xmath195 ( above @xmath199 ) ,",
    "the difference increases .",
    "more specifically , it increases more for values of @xmath198 between ( approximately ) @xmath201 and @xmath202 .    in figure 2 , the shaded regions in the @xmath203 space correspond to low difference ( less than @xmath200 per dimension ) , for @xmath204 .",
    "it can be deduced that the behaviour of the proposed method is only slightly affected influenced by the dimensionality of the feature space .",
    "as a general conclusion , one can argue that the `` more diagonally dominant '' the @xmath193 is ( i.e. , the smaller the @xmath195 is ) , the more accurate the estimate of the mean provided by the proposed method is . from another perspective , the more @xmath191 `` approaches diagonality '' ( again , as @xmath195 becomes smaller ) ,",
    "the more accurate the obtained estimates are .",
    "the latter is also supported by the fact that in the extreme case of a diagonal covariance matrix , one has to solve @xmath18 independent one - dimensional problems , for which an analytic formula exists . in this case , it is easy to verify that the proposed method terminates after a single iteration ( see also comments in the next section ) .",
    "after having gained some insight on the capability of the proposed method to approximate the mean of a multivariate truncated normal distribution in the previous section , we proceed in this section with experiments where now the involved covariance matrices have no specific structure . as in the previous section , the mcmc method is used as benchmark .    _",
    "1st experiment : _ the purpose of this experiment is to compare the estimates of the mean of a truncated normal distribution obtained by the proposed and the mcmc methods , for dimensions @xmath18 varying from @xmath205 to @xmath206 . to this end",
    ", for each dimension @xmath18 , @xmath207 different truncated normal distributions ( defined by the means and the covariance matrices of the corresponding untruncated normals , as well as their truncation points ) have randomly been generated , such that , the corresponding inverse covariance matrix is diagonally dominant . for the @xmath19-th such distribution , @xmath208 , both the proposed and the mcmc methods have been applied . letting @xmath209 and @xmath210 denote the respective resulting estimates ,",
    "the mean difference per coordinate between the two estimates is computed , i.e. , @xmath211 and , averaging over @xmath19 , the quantity @xmath212 is obtained . in figure 3 , @xmath213 is plotted versus @xmath18 . from this figure , it can be concluded that the proposed scheme gives estimates that are very close to those given by the mcmc .",
    "thus , the fixed point of the proposed scheme ( when the diagonal dominance condition is fulfilled ) can be considered as a reliable estimate of the mean of the truncated normal distribution .",
    "next , in order to show the rapid convergence of the proposed scheme against the mcmc method , we focus on a single example ( however , the resulting conclusions are generally applicable ) . more specifically , table 1 shows the values of the @xmath153 norm of the difference @xmath214 divided by @xmath18 , i.e. the quantity @xmath215 , as the number of iterations evolves , for the @xmath216-dimensional left truncated normal distribution defined by @xmath217 @xmath218^t$ ] , @xmath219=,@xmath219 while its truncation point is @xmath220 @xmath221^t$ ] .",
    "it is clear that the proposed method converges rapidly to its estimate , while mcmc exhibits rather slow converge .",
    "this is the strong point of the proposed method compared to the mcmc ( keeping , however , in mind the requirement for diagonal dominance of the inverse covariance matrix , for the proposed method ) .",
    ".the evolution of @xmath222 for the mcmc and the proposed methods , for the first few iterations . [ cols=\">,>,>\",options=\"header \" , ]     in the sequel we try to get an indication about the performance of the proposed method when the diagonal dominance requirement does not hold .",
    "the following two experiments are in this direction .    _",
    "2nd experiment : _ we consider the three - dimensional case where @xmath223^t$ ] and @xmath219=,@xmath219 while the truncation point is @xmath224^t$ ] note that , although in this case the inverse of the covariance matrix , which is @xmath219 ^ -1=,@xmath219 is not diagonally dominant , the diagonal dominance condition is slightly violated ( only in the third row ) .",
    "the estimate of the mean of the truncated normal distribution obtained by the proposed method is @xmath225^t$ ] , which is quite close to the estimate obtained by the mcmc ( the mean absolute difference per coordinate is @xmath226 ) .",
    "it is worth noting that in this case , although eq .",
    "( [ norm - t ] ) is not satisfied ( @xmath227 , since @xmath111 is not diagonally dominant ) , the quantities @xmath228 , @xmath9 , are less than @xmath93 for all pairs of points @xmath229 is through @xmath230 . ] .",
    "thus , the ( tighter ) bound of eq .",
    "( [ norm - t1 ] ) is satisfied for all the pairs @xmath229 , which guarantees the rapid convergence of the method .",
    "however , note that , since @xmath228 depends on the data ( through @xmath230 ) , it can not be used as an upper bound in the proof of the contraction .    _",
    "3rd experiment : _ we consider now the case where @xmath231^t$ ] and    @xmath219=,@xmath219 while the truncation point is @xmath232^t$ ] . the matrix @xmath219 ^ -1=,@xmath219 is non - diagonally dominant and , in addition , the diagonal dominance condition is strongly violated ( in the last two rows ) .",
    "we run the proposed method for several different initial conditions .",
    "it is noted that , in contrast to the 2nd experiment , the quantity @xmath233 is now greater than @xmath93 for all pairs of @xmath229 .",
    "the algorithm in all cases converges to the vector @xmath234^t$ ] .",
    "however , in this case , the mean absolute difference per coordinate between this estimate and the one resulting from mcmc is @xmath235 , which is significantly greater than that in the previous experiment .    _",
    "4th experiment : _ in order to exhibit the scalability properties of the proposed method with respect to the dimensionality , an additional experiment has been conducted for @xmath236 .",
    "the mean , and the covariance matrix of the corresponding normal distribution , as well as the truncation points have been selected as in the 1st experiment ( note that the inverse of the covariance matrix is diagonally dominant ) .",
    "the algorithm gives its response in less than one minute , a time that is substantially smaller than that required from the mcmc method .",
    "analysing the previous results we may draw the following conclusions :    * provided that the inverse of the covariance matrix of the untrucated normal distribution is diagonally dominant , the proposed method gives very accurate estimates of the mean of the truncated normal distribution , using as benchmark the estimates of the mcmc method . *",
    "the proposed method converges much faster ( in very few iterations ) compared to the mcmc method . *",
    "even if the diagonal dominance constraint is slightly violated , the algorithm seems to converge to an accurate estimate of the mean of the truncated distribution , as the 2nd experiment indicates .",
    "* when the diagonal dominance condition is strongly violated , the algorithm still ( seems to ) converge to a vector .",
    "however , this vector is a ( much ) less accurate estimation of the mean of the truncated normal ( see the third experiment ) .",
    "* in the special case , where @xmath2 ( and , as a consequence @xmath111 ) is diagonal , the matrix @xmath237 is ( obviously ) equal to zero .",
    "thus , both eqs .",
    "( [ trunc - mean ] ) and ( [ trunc - mean2 ] ) implies that @xmath238 and eq .",
    "( [ trunc - var ] ) imply that @xmath239 . in other words , in this case ,",
    "both the proposed and the mcmc methods solve @xmath18 independent one - dimensional problems , with the deterministic method giving its estimate in a single iteration , since in this case , it reduces to ( [ case - i ] ) or ( [ case - ii ] ) .",
    "an additional observation is that the estimates provided by the two methods in this case are almost identical . letting intuition enter into the scene and generalizing a bit",
    ", one could claim that as @xmath2 approaches `` diagonality '' , the estimates of the mean of both methods are expected to be even closer to each other .",
    "this observation , may be an explanation of the decreasing trend observed in figure 1 , since , as the dimension increases , @xmath2 moves closer to `` diagonality '' , due to the diagonal dominance condition .",
    "* finally , the proposed method scales well with the dimensionality of the problem .",
    "in this paper , a new iterative algorithmic scheme is proposed for the approximation of the mean value of a one - sided truncated multivariate normal distribution .",
    "the algorithm converges in very few iterations and , as a consequence , it is much faster than the mcmc based algorithm proposed in @xcite .",
    "in addition , the algorithm is an extension of the one used in @xcite .",
    "the quality of the approximation of the mean is assessed through the case where the exponential correlation matrix is used as covariance matrix .",
    "the proof of convergence of the proposed scheme is provided for the case where @xmath111 is diagonally dominant . however , experimental results indicate that , even if this condition is softly violated , the method still provides estimates of the mean of the truncated normal , however , less accurate .",
    "finally , the method exhibits good scalablity properties with respect to the dimensinality .",
    "_ proof of proposition 2 : _                    in order to prove ( [ q3 ] )",
    "it suffices to show that @xmath263 , or , @xmath264 since in this case the sign of the second degree polynomial will be the opposite of that of the coefficient of @xmath265 ( i.e. , @xmath266 ) .",
    "we proceed again be considering separately the cases @xmath245 and @xmath254 .",
    "\\(i ) let @xmath267 .",
    "we set @xmath251 . in this case",
    "( [ prove ] ) becomes @xmath268 where @xmath269 .",
    "taking into account ( [ ineq ] ) , the right hand side inequality of ( [ prove1 ] ) holds . in order to prove the left hand inequlity of ( [ prove1 ] ) ( again taking into account ( [ ineq ] ) ) , it suffices to prove that      \\(ii ) let @xmath245 .",
    "the left hand side inequality of ( [ prove ] ) holds trivially , since in this case @xmath273 .",
    "we focus now on the right hand side inequality of ( [ prove ] ) . taking into account that ( a ) @xmath274 , for @xmath275 ( see e.g. @xcite ) and ( b ) @xmath82 , it is @xmath276 combining the right hand side inequality of ( [ ineq ] ) with ( [ q4 ] )",
    ", the right hand side inequality of ( [ prove ] ) holds if @xmath277 or @xmath278 or @xmath279 since @xmath280 , for @xmath281 ( from the taylor series expansion ) , the previous inequality holds if    @xmath282 or @xmath283 since @xmath284 , the previous inequality holds if @xmath285 the discriminant of the above second degree polynomial of @xmath286 is @xmath287 .",
    "thus , the above second degree polynomial is always positive since the coefficient of the term @xmath288 is positive . in other words , ( [ q5 ] ) holds .",
    "therefore , the right hand side inequality of ( [ prove ] ) also holds .",
    "thus , for @xmath245 , it is also @xmath263 and therefore @xmath272 . as a consequence ( [ q3 ] ) also holds",
    ". q.e.d .",
    "m.chiani , d.dardari , m.k .",
    "simon , `` new exponential bounds and approximations for the computation of error probability in fading channels '' , _ ieee transactions on wireless communications _ , 2(4 ) , 840 - 845 ( 2003 ) .",
    "r. nabben , r.s .",
    "varga , `` a linear algebra proof that the inverse of strictly ultrametric matrix is a strictly diagonally dominant stieltjes matrix '' , _ siam journal of matrix anal .",
    "_ , 15 , 107 - 113 ( 1994 ) .",
    "k. themelis , a. rontogiannis , k. koutroumbas , `` a novel hierarchical bayesian approach for sparse semisupervised hyperspectral unmixing '' , ieee transactions on signal processing , 60(2 ) , 585 - 599 ( 2012 ) ."
  ],
  "abstract_text": [
    "<S> a non trivial problem that arises in several applications is the estimation of the mean of a truncated normal distribution . in this paper , an iterative deterministic scheme for approximating this mean </S>",
    "<S> is proposed , motivated by an iterative markov chain monte carlo ( mcmc ) scheme that addresses the same problem . </S>",
    "<S> conditions are provided under which it is proved that the scheme converges to a unique fixed point . </S>",
    "<S> the quality of the approximation obtained by the proposed scheme is assessed through the case where the exponential correlation matrix is used as covariance matrix of the initial ( non truncated ) normal distribution . finally , the theoretical results are also supported by computer simulations , which show the rapid convergence of the method to a solution vector that ( under certain conditions ) is very close to the mean of the truncated normal distribution under study .    _ </S>",
    "<S> keywords : _ truncated normal distribution , contraction mapping , diagonally dominant matrix , mcmc methods , exponential correlation matrix </S>"
  ]
}