{
  "article_text": [
    "in 1867 , j.c . maxwell created a thought experiment to demonstrate a possible violation of the second law of thermodynamics : a thermally isolated container with a gas is divided into two parts and a fictitious demon opens or closes a door between the two parts depending on the velocity of approaching particles , creating an increase of the temperature in one of the compartments  @xcite .",
    "smoluchowski was the first to provide an explanation why maxwell s demon does not work .",
    "to this end , he modeled the demon mechanically by a trapdoor combined with a gentle spring .",
    "the trapdoor acts as a valve in such a way that fast particles coming from one side can open the door while slow ones can not , leading to a pressure difference .",
    "however , taking the full dynamics of the apparatus into account , smoluchowski could demonstrate that the energy of the spring system itself equilibrates at such a high energy that it opens and closes essentially randomly , leading to the same pressure difference as if the trapdoor was always open .    in 1928 , l. szilrd refined the concept of maxwell s demon , suggesting what is known today as the _ szilrd engine _",
    "starting point is a box that contains only one particle ( see fig .  [",
    "fig : szilard ] ) .",
    "if a wall is inserted in the middle , the particle will be in one of the two parts .",
    "expanding the volume isothermally to its original size by moving the shutter into the empty half of the box one can extract the work @xmath0 .",
    "however , this requires to know in which compartment the particle actually is , demonstrating that the possession of information can be converted into physical work .",
    "thus , in order to keep a szilrd engine running , a closed loop of measurement and feedback is needed .",
    "very recently , such a feedback scheme could be realized experimentally for the first time  @xcite .",
    "since the feedback mechanism in the szilrd engine was perceived as information processing at zero cost , it seemed to produce usable work from nothing , violating the second law .",
    "however , as shown by landauer  @xcite in 1961 , any irreversible logical operation requires to apply a well - defined minimum of work . in case of the szilrd engine ,",
    "the demon has to store the information about the particle position in a single bit .",
    "according to landauer s principle , resetting this bit requires to convert a work of @xmath1 into heat .",
    "as shown by bennett  @xcite , who realized measurement and feedback as reversible processes , this extra work for resetting restores the second law .",
    "recently maxwell s demon attracted renewed attention as it was shown that a system in a feedback loop obeys an integral fluctuation theorem ( ift ) of the form @xmath2 implying @xmath3 , where @xmath4 is the extracted work during feedback , @xmath5 is the inverse temperature , and @xmath6 is the gained mutual information between system and demon during the measurement  @xcite .",
    "this fluctuation theorem implies that in each cycle of the engine the extracted work is limited by the gained mutual information ,  a highly plausible result that nicely demonstrates the equivalence of thermodynamic work and information .",
    "subsequently this remarkable result was made more specific in various ways .",
    "for example , it was shown that one can construct feedback schemes that satisfy the inequality sharply  @xcite . moreover",
    ", the ift was generalized to schemes with finite - time relaxation  @xcite and continuous feedback schemes  @xcite .",
    "very recently , the generalized ift could also be confirmed experimentally  @xcite .",
    "the arising problem with these generalized jarzynski equalities is that the tightness of the bound seems to depend on the specific feedback scheme such as discrete and continuous feedback as well as memory tape models .",
    "this led barato _",
    "_ to look for a unifying master ift  @xcite .",
    "to achieve this , they duplicated the configuration space , modeling bit flips of the memory by transitions between the two replicas .",
    "doing so they followed smoluchowski s original idea of modeling the whole feedback loop as a physical device , defined as a stochastic markov process .    in this paper , we follow these lines of thought , being interested in the _ total _ cost of operating an information engine as described above . instead of duplicating the configuration space , we devise physically realizable stochastic processes of the joint system not only for relaxation , but also for the measurement process .",
    "we show that the generalized ift for the relaxation of system is always accompanied by an _ opposite _",
    "ift during the measurement carried out by the demon , restoring the second law for the joint system .",
    "this implies that a certain minimal amount of work has to be done on the memory , in accordance with landauer s principle .",
    "the calculation of the total cost enables us to derive the efficiency of the information engine as a function of its cycle time .",
    "we also discuss the optimal cycle time and corresponding efficiency , at which the extracted power is maximized .",
    "in what follows we consider a system with two different energy levels separated by @xmath7 .",
    "the system is coupled to a heat bath with inverse temperature @xmath5 .",
    "the controller ( demon ) is implemented as a 1-bit memory .",
    "we devise a discrete feedback scheme evolving in three steps ( see fig .  [",
    "fig : engine ] ) .",
    "first the system relaxes thermally without external influence .",
    "then the actual energy level , denoted by @xmath8 and @xmath9 , is copied to the memory of the controller . finally , if the memory bit is @xmath9 , the controller induces a flip of the system @xmath10 , extracting the energy @xmath7 ,",
    "otherwise it does nothing .    .",
    ", width=321 ]    following barato and seifert  @xcite , we are aiming to model these steps by stochastic markov processes , including both the system and the memory .",
    "this defines a four - dimensional configuration space in which each step can be represented by a simple @xmath11 matrix . in the following ,",
    "we discuss each of the three steps in detail : +    * 1 .",
    "relaxation * +    during relaxation , the system flips randomly according to the rules @xmath12 for convenience , we express these rates in terms of @xmath13 with @xmath14 . after infinite time , the system eventually reaches an equilibrium state with the stationary probability distribution @xmath15 .",
    "this means that the energy difference between the two levels is given by @xmath16 let us now describe the relaxation process in the composite configuration space of system and memory . throughout this paper",
    ", we will use a canonical configuration basis ordered by @xmath17 since the memory is inactive during relaxation , the time evolution operator @xmath18 for relaxation represented in this basis reads @xmath19 after the relaxation time @xmath20 , the corresponding transition matrix is given by @xmath21 in the infinite - time relaxation limit ( @xmath22 ) , this transition matrix reduces to @xmath23    * 2 .",
    "measurement * +    a perfect measurement would faithfully copy the system state to the memory , i.e. , if @xmath24 denotes the previous memory state and @xmath25 the actual system state , it would simply copy @xmath26 .",
    "however , it is well - known that such a _ perfect _ measurement is irreversible , leading to a diverging entropy production  @xcite",
    ". therefore , one usually considers _ imperfect _ measurements @xmath27 with a small error probability @xmath28 . using the basis ( [ basis ] )",
    "this corresponds to the transition matrix @xmath29 remarkably , this imperfect measurement process can be implemented by a stochastic markov process as well , since @xmath30 .",
    "the corresponding time evolution operator reads @xmath31 with a rate @xmath32 and it is easy to show that the transition matrix @xmath33 in eq .",
    "( [ tmi ] ) is retrieved in the limit of infinite measurement time : @xmath34 thus , we succeeded to implement the second step as a stochastic markov process as well .",
    "if the memory is considered as being in contact with some heat bath of inverse temperature @xmath5 during the stochastic measurement process , the time evolution defined above implies that the incorrectly measured state @xmath35 has a higher energy than the correctly measured state @xmath36 , and that the corresponding energy difference between the two composite states is given by @xmath37    * 3 .",
    "feedback * +    the purpose of the feedback is to use the information stored in the memory in order to extract energy from the system .",
    "if the preceding measurement was faithful , this would mean to perform the transitions @xmath38 these transitions alone would be again irreversible , causing an infinite entropy production .",
    "however , if we add symmetric transitions in the ( unlikely ) case of erroneous measurements , namely , @xmath39 we obtain the feedback transition matrix @xmath40 thus the feedback process is carried out in such a way that the system state is flipped ( @xmath41 ) for @xmath42 while it remains unchanged ( @xmath43 ) for @xmath44 .",
    "it is assumed that the feedback transition occurs instantaneously so that the total time @xmath45 of a complete cycle @xmath46 is given by @xmath47 .    since @xmath48 ,",
    "the feedback is fully reversible , hence it does not produce entropy in the environment . moreover , it is easy to see that it simply exchanges the second and the fourth component of a vector , and therefore it does not change the joint entropy of system and memory .",
    "however , as will be shown below , it generally changes the entropy of the subsystems .",
    "due to its reversible nature , the feedback as defined above can not be implemented as a stochastic markov process . however , we would like to point out that it is even possible to implement the feedback physically so that the entire chain of steps is represented cleanly as a sequence of stochastic processes .",
    "this can be done by replacing two subsequent cycles @xmath49 equivalently by @xmath50 .",
    "since @xmath51 and @xmath52 satisfy the stochasticity condition ( @xmath53 , @xmath54 ) , the whole sequence of steps can be implemented by stochastic processes , providing a safe ground for the calculation of entropy production , mutual information , work , and heat .",
    "having verified that this description is fully equivalent , we nevertheless keep the explicit feedback for simplicity in the original form .    since the rates @xmath55 and @xmath32 simply rescale @xmath20 and @xmath56 , we will set @xmath57 throughout the paper .",
    "thus , apart from @xmath20 and @xmath56 , the model is controlled by only two parameters , namely , the relaxation parameter @xmath58 and the error probability @xmath59 .",
    "transition matrices @xmath60,@xmath61 , and @xmath62 . after many cycles",
    ", the probability distributions between the steps will become stationary.,width=321 ]    if the information engine runs repeatedly through many cycles , the probability distributions between the three steps will become stationary . the corresponding stationary probability distributions @xmath63 , @xmath64 , and @xmath65 are represented as four - component vectors @xmath66 are determined by the equations @xmath67 with @xmath68 and @xmath69 ( see fig .",
    "[ fig : cycle ] ) .",
    "the reduced stationary probability vectors of the system @xmath70 and the memory @xmath71 are given , respectively , as @xmath72 .",
    "similarly , both the relaxation and feedback do not affect the memory , hence @xmath73 and @xmath74 . as a result , in a stationary situation",
    ", the information acquired during the measurement is statistically the same in each cycle , implying that @xmath75 .    as a simple example , first consider the case of infinite - time relaxation and measurement ( @xmath76 ) , where the matrices @xmath60,@xmath61 , and @xmath62 are given by eqs .",
    "( [ tri ] ) , ( [ tmi ] ) , and  ( [ tfi ] ) . in this case",
    "the normalized stationary probability vectors turn out to be given by @xmath77",
    "* 1 . shannon entropy * +    given the stationary probability distributions @xmath78 , it is straightforward to compute the entropies of the system  @xmath70 , the memory @xmath71 , and the joint system @xmath79 between the steps in a stationary cycle , using the definition of the shannon entropy @xmath80 where the sum runs over the vector components . because of the aforementioned coincidence of various probability vectors we have @xmath81 and @xmath82 .",
    "furthermore , the reversibility of the feedback process guarantees that @xmath83 .    for @xmath76 , the expressions for the shannon entropies reduce to @xmath84 h_{0,1,2}^{(m ) } & = &   h(\\epsilon\\bar q+\\bar \\epsilon q)+h(\\epsilon q + \\bar \\epsilon \\bar q ) \\ , , \\nonumber\\\\ h_{0,2}^{(sm ) } & = & h(q)+h(\\bar q)+h(\\epsilon)+h(\\bar \\epsilon ) \\\\",
    "h_1^{(sm ) } & = & h(q)+h(\\bar q)+h(\\epsilon\\bar q+\\bar \\epsilon q)+h(\\epsilon q + \\bar \\epsilon \\bar q)\\,,\\nonumber\\end{aligned}\\ ] ] where we used the notation @xmath85 .    during the relaxation process , where the system tries to restore the equilibrium distribution from the overpopulated ground state after energy extraction ,",
    "the system entropy @xmath86 is expected to increase , provided that the error probability @xmath59 is sufficiently small ( @xmath87 ) .",
    "the same applies to the composite entropy @xmath88 .    to summarize , the entropy changes during relaxation ( r ) , measurement  ( m ) , and feedback ( f ) are given by @xmath89    * 2 .",
    "mutual information * +    with these expressions , it is straightforward to compute the mutual information @xmath90 which is a measure for the correlation between system and memory .",
    "this correlation is expected to build up during the measurement and then to decrease during feedback and relaxation , implying the inequalities @xmath91 it is interesting to note that the change of the composite entropy is purely given by amount of mutual information acquired during the measurement , i.e. @xmath92 for @xmath76 , we have @xmath93 note that the result @xmath94 is true only if the relaxation time is infinite , which obviously destroys all correlations between system and memory . +    *",
    "3 . entropy production * +    let us now turn to the entropy production . according to schnakenberg  @xcite , whenever the system or the memory jumps spontaneously from the configuration @xmath95 to another configuration @xmath96 ,",
    "the amount of entropy @xmath97 is generated in the environment .",
    "here , @xmath98 denotes the transition rate at time @xmath99 .",
    "therefore , the mean entropy production rate is given by @xmath100 in an arbitrary nonequilibrium system , one would have to solve the master equation , plug the solution into the equation above , and integrate the resulting expression over a certain window of time .",
    "however , in the present case this is not necessary since the model is so simple that the rates happen to obey _ detailed balance _ , defined as @xmath101 in the stationary equilibrium state . in this case , it is therefore straightforward to rewrite the equation given above as @xmath102 allowing us to compute the average entropy production in each step directly without integration by means of @xmath103 here @xmath104 and @xmath105 denote the initial and the final probabilities for a finite time span while @xmath106 is the stationary probability distribution that would emerge after infinite long time .",
    "for example , during relaxation , @xmath106 can be obtained by taking @xmath107 in the expression for @xmath108 in eq .",
    "( [ p1stat ] ) .",
    "similarly , during measurement , @xmath109 with @xmath110 given in eq .",
    "( [ p2stat ] ) .    with the above formula",
    ", we obtain the following expressions for the entropy production in each process :    ) during relaxation ( lower curves ) and measurement ( upper curves ) for various values of @xmath59 as a function of @xmath58 . if the measurement is accurate enough ( @xmath111 ) , the entropy production during relaxation becomes negative.,width=321 ]    @xmath112    note that these expressions hold for any finite @xmath20 and @xmath56 .",
    "the last result is obvious since the feedback with @xmath113 is a reversible operation .    for @xmath76 , by inserting eqs .",
    "( [ p0stat])-([p2stat ] ) we get explicit expressions @xmath114 which are plotted for various error probabilities in fig .",
    "[ fig : entropyproduction ] .",
    "as one can see , for @xmath115 the entropy production during relaxation @xmath116 is negative , meaning that the engine imports entropy ( heat ) from the environment rather than producing it .",
    "obviously , this is the regime of interest where we would like to operate our information engine .",
    "however , as can be seen , the negative entropy production during relaxation is always overcompensated by a positive one during the measurement , which is consistent with the second law of thermodynamics .",
    "notice that the entropy production during measurement is always positive since @xmath117 .",
    "by virtue of clausius law @xmath118 , the produced entropy can be translated directly into an amount of heat . in most studies",
    ", it is usually assumed that the temperatures of the system and the memory are identical .",
    "however , for the sake of generality , let us allow the temperatures to be different , assigning @xmath119 during relaxation and @xmath120 during measurement , as sketched in fig .",
    "[ fig : totalscheme ] .",
    "thus the respective heat contributions averaged over all possible stochastic trajectories are given by @xmath121 here we use the sign convention that heat flowing away from the engine into the environment has a positive sign , i.e. we expect @xmath122 to be negative and @xmath123 to be positive .    in order to maintain stationarity of the system after one engine cycle ,",
    "the average work @xmath124 extracted during feedback should exactly balance the average heat @xmath125 during relaxation , i.e. @xmath126 consequently , the measurement process does not change the energy of the system .",
    "this requires an additional influx of energy in the form of extra work @xmath127 into the memory which is necessary to compensate the loss of heat @xmath128 flowing away to the environment during the measurement process : @xmath129 the average _ net _ work performed by the machine , defined as the difference of extracted and supplied work , is therefore given by @xmath130 note that the net work can change its sign depending on the choice of the parameters @xmath58 , @xmath59 , @xmath20 , and @xmath56 .",
    "let us first compute the extracted work . according to sect .",
    "@xmath124 is given by @xmath131 , where @xmath132 , see  ( [ deltae ] ) .",
    "using @xmath133 ( no system change during measurement ) together with the feedback identities @xmath134 and @xmath135 ( flip @xmath136 only when @xmath42 ) , it is easy to show explicitly that @xmath137 the additional work @xmath127 supplied during the measurement process can be interpreted as the energy needed to operate the measurement device .",
    "technically this contribution comes from the fact that the energy levels of the joint system are different during relaxation and measurement so that extra energy is needed to move them around . for example",
    ", this could be done by applying an external potential in order to make the energy level of the erroneous composite state @xmath35 higher than that of the correctly measured state @xmath36 by the amount of @xmath138 . when the external potential is turned on just before the measurement , the average energy of the composite of system and memory increases by @xmath139 and",
    "similarly it looses the energy @xmath140 when the potential is turned off at the end of the measurement : @xmath141 comparing the difference @xmath142 with eq .",
    "( [ henv ] ) one can see immediately that @xmath143 in the limit @xmath76 , the average work contributions read @xmath144",
    "the total entropy production of the whole setup during relaxation ( r ) and measurement ( m ) is given by @xmath145 according to eq .",
    "( [ dhi ] ) , the entropy differences in the joint system are given solely in terms of the mutual information difference @xmath146 while the entropy differences in the environment are given in terms of the transferred heat by @xmath147 .",
    "using eqs .",
    "( [ wex1 ] ) and ( [ wex2 ] ) we arrive at @xmath148 for the total system including the environment the second law of thermodynamics should be satisfied for each process , i.e. , @xmath149 or equivalently @xmath150 most existing studies are only interested in the first inequality during relaxation , while the other one during measurement is ignored .",
    "the purpose of this work is to point out that there is a _ second _ inequality for the measurement process as well , and that the two inequalities are complementary with respect to each other .",
    "more specifically , if @xmath151 , the extracted work is bounded from above by the mutual information while the work required to operate the memory is bounded from below by the same threshold . this means that the setup can not be used to gain work out of nothing , @xmath152 , as expected by the second law .",
    "however , if the two reservoir temperatures were different @xmath153 , the extracted work could be larger than the supplied one , in which case the system operates like a conventional heat engine ( see fig .  [",
    "fig : ift_infinite ] ) .",
    ", supplied work @xmath154 , and the mutual information @xmath155 are displayed as functions of @xmath58 for a constant error probability @xmath156 . here",
    "we choose @xmath157 and 5 with @xmath158 .",
    "note that @xmath159 is independent of @xmath160 .",
    "@xmath161 is positive at @xmath162 as expected from eq .",
    "( [ ww_inf ] ) . when the temperature of the measurement reservoir is sufficiently lower than that of the relaxation reservoir , for example @xmath163 , the extracted work @xmath161 can be larger than the supplied work @xmath154 , as indicated by the arrow .",
    ", width=321 ]    it is almost trivial to construct the integral fluctuation theorems ( ift s ) through the standard approach of stochastic thermodynamics  @xcite by considering the heat along all possible trajectories in the composite configurational state space . with an appropriate definition of the shannon entropy for a given trajectory  @xcite",
    ", one can easily get the fluctuation theorems for the total entropy production for each process as @xmath164 the second laws in eq .",
    "( [ thermod_lawsh ] ) are simple consequences of the ift s with @xmath165 .",
    "it is rather tricky to find the ift s in terms of work and mutual information , because it requires an equilibrium state as an initial condition .",
    "this is the case only when @xmath20 becomes infinite so that the system is in equilibrium at the start of the measurement as well as at the beginning of the feedback .",
    "however , note that the bounds for works in eq .",
    "( [ thermod_laws ] ) are valid even if @xmath20 is finite .",
    "in practice , an engine is only useful if the cycle time @xmath45 is finite .",
    "thus , it is obviously of interest to derive all physical quantities as function of the cycle time .",
    "this allows one to find the optimum for maximal power generation , as will be discussed in the next section .",
    "it is straightforward to obtain the transition matrices for relaxation and measurement for finite time spans @xmath20 and @xmath56 : @xmath166 @xmath167 where @xmath168 note that the transition probability from @xmath35 to @xmath36 during measurement , @xmath169 , is smaller than @xmath170 , which means that the measurement for finite @xmath56 is less accurate than in the limit of infinite time .",
    ", supplied work @xmath127 , and the mutual information @xmath171 and @xmath172 are shown as functions of @xmath173 . here , we choose @xmath174 , @xmath175 , @xmath156 , @xmath158 , and @xmath176 .",
    ", width=321 ]    solving eq .",
    "( [ ss ] ) , one can find explicit but complicated expressions for all stationary distributions such as @xmath63 , @xmath64 , and @xmath65 for finite @xmath20 and @xmath56 , which are not shown here explicitly .",
    "the heat dissipation during the finite - time relaxation and measurement can be obtained from eq .",
    "( [ henv ] ) while the extracted work and the supplied work are given by eqs .",
    "( [ ww1 ] ) and ( [ ww2 ] ) .    using the relation @xmath177",
    "we find that @xmath178 here @xmath179 is explicitly given by @xmath180 with @xmath181 . in a similar manner",
    ", we obtain @xmath161 as the function of @xmath182 : @xmath183 in the limit of @xmath184 ( @xmath185 ) , we consistently recover eq .",
    "( [ ww_inf ] ) .",
    "note that the finite - time works in  ( [ eq : finite_ws ] ) and ( [ eq : finite_wex ] ) decrease monotonously with @xmath186 and @xmath187 ( see fig .",
    "[ fig : ift_finite ] ) .",
    "moreover , since the correlation between system and memory builds up continuously during the measurement process , it is obvious that @xmath172 decreases with @xmath187 , remaining positive by definition .",
    "the positivity of @xmath172 guarantees that @xmath188 is also positive . on the other hand",
    ", @xmath189 can be negative for short - time measurement and relaxation , as its upper bound @xmath172 approaches zero for @xmath190 .    the monotonous dependence shown in fig .",
    "[ fig : ift_finite ] suggests that @xmath189 becomes maximal for maximal measurement accuracy ( @xmath191 ) and full relaxation ( @xmath22 ) in order to redistribute and pump the overpopulated ground state @xmath192 back to the energetically excited state @xmath193 .",
    "therefore , both limit @xmath194 have to be carried out simultaneously . to establish this combined limit conveniently ,",
    "let us from now on set @xmath195 meaning that @xmath196 . with this convention",
    "we expect @xmath189 to be maximal in the limit of infinite cycle time ( @xmath197 ) .",
    "moreover , as @xmath45 decreases , we expect @xmath189 to decrease and eventually to become negative .    if @xmath198 , the system operates like a conventional heat engine . for infinite @xmath45 the net work @xmath199",
    "is maximal , but the power ( net work per unit time ) vanishes . for finite but sufficiently large @xmath45 and",
    "properly chosen parameters the system still produces a positive net work , hence the power is positive .",
    "however , as can be seen in fig .",
    "[ fig : ift_finite ] , the curves for @xmath161 and @xmath127 cross each other at some finite cycle time @xmath200 . at this point",
    "we do no longer obtain any net work from the engine , hence the power vanishes again .",
    "consequently , there will be a particular cycle time in between , at which the power of the engine is maximal . in the next section",
    ", we will discuss this aspect in more detail .",
    "flows from the heat reservoir at @xmath201 ( high temperature ) into the engine which produces the work gain @xmath202 .",
    "the remaining heat @xmath203 is transferred to another reservoir at @xmath160 ( low temperature ) .",
    ", width=207 ]    let us now assume that the information engine operates in a regime where the net work is positive . in this case the whole setup can be interpreted as a conventional heat engine , as sketched in fig .",
    "[ fig : totalscheme2 ] . as @xmath204 , the upper reservoir for the relaxation process plays a role of a high - temperature heat source , while the lower reservoir in contact with the memory device acts as a heat sink .",
    "the _ efficiency _ of this heat engine in a single cycle is defined in the usual way as @xmath205 using eqs .",
    "( [ eq : finite_ws ] ) and ( [ eq : finite_wex ] ) the efficiency can be rewritten as @xmath206 where @xmath207 \\bar{\\mathcal r } \\ln \\left ( \\bar q /q \\right ) } .\\ ] ] note that the relaxation and measurement processes are not quasi - static so that even in the limit @xmath208 the engine never reaches the carnot efficiency @xmath209 .",
    "instead , we find that the efficiency is limited by a different upper bound @xmath210 which can be computed as follows . according to the monotonicity arguments discussed in the preceding section , @xmath211",
    "is expected to become maximal in the limit @xmath197 .",
    "this suggests that @xmath212 where @xmath213 fig .",
    "[ fig : eff_infinite ] shows @xmath214 as function of @xmath58 for several values of  @xmath59 .",
    "it is obvious that @xmath214 is positive for @xmath215 with a unimodal shape due to the similar behavior of @xmath161 demonstrated in fig .",
    "[ fig : ift_infinite ] . as @xmath216 , @xmath214 approaches zero except for @xmath217 . in the limit of both @xmath218 and @xmath219",
    ", @xmath220 approaches a constant bounded from below by @xmath221 .",
    "consequently , in order to obtain a positive work gain , the difference of temperatures should be at least @xmath222 for the infinite - time process .",
    "in short , we find that the efficiency of the information engine is bounded by @xmath223     according to eq .",
    "( [ eq : lambda_inf ] ) as function of @xmath58 for several values of @xmath59 . in the infinity - time limit ,",
    "@xmath161 and @xmath214 are positive only for @xmath224 .",
    "note that , as @xmath59 goes to zero , @xmath220 approaches to 2 , the minimum , at @xmath218 .",
    ", width=321 ]    in conventional heat engines operating with a finite cycle time , thermodynamic processes are no longer quasi - static , leading to an efficiency below the carnot bound @xmath225 . in our case , we also find that @xmath211 becomes maximal in the limit @xmath208 .",
    "however , in contrast to the carnot limit , where the entropy production vanishes , the entropy production per cycle in our model @xmath226 becomes _ also maximal _ at @xmath227 .",
    "this is one of the crucial features of our information engine which is totally distinct from conventional heat engines",
    ". nevertheless , the entropy production _ rate _",
    "( per unit time ) decreases with increasing @xmath45 and finally vanishes at @xmath228 .",
    "hence , one may also say that the maximum efficiency is found at the minimum entropy production rate .",
    "similarly , the average power gain , @xmath229 in fact vanishes for @xmath197 because @xmath199 remains finite in this limit . for a realistic engine , we usually want to optimize the power gain , trading off the efficiency against the cycle time .",
    "as expected , @xmath230 is maximized at some _ finite _ time , @xmath231 between @xmath232 and @xmath233 as shown in fig .",
    "[ fig : eff_finite ]  ( a ) .",
    "the efficiency of heat engines at maximal power has been studied previously in refs .",
    "@xcite . especially , for the curzon - ahlborn ( ca ) endoreversible model  @xcite , it is well - known that the efficiency @xmath234 at the optimal power is given by @xmath235 . in fig .",
    "[ fig : eff_maxp ] , we plot the efficiency at optimal power @xmath236 according to eq .",
    "( [ eq : eff2 ] ) as a function of @xmath210 instead of @xmath225 .",
    "it turns out that the functional behavior of @xmath237 is completely different from @xmath234 .     and power gain @xmath238 as functions of @xmath239 .",
    "( b ) @xmath240 and @xmath241 as functions of @xmath210 . in ( b ) , the dotted line @xmath242 is obtained from eq .",
    "( [ eq : tau_s ] ) and the dashed line corresponds to eq .",
    "( [ eq : tau_op ] ) .",
    "we choose @xmath174 , @xmath175 , @xmath156 , and @xmath158 for both ( a ) and ( b ) .",
    "we set @xmath176 for ( a ) and vary @xmath160 for ( b ) . ,",
    "title=\"fig:\",width=321 ]   and power gain @xmath238 as functions of @xmath239 .",
    "( b ) @xmath240 and @xmath241 as functions of @xmath210 . in ( b ) , the dotted line @xmath242 is obtained from eq .",
    "( [ eq : tau_s ] ) and the dashed line corresponds to eq .",
    "( [ eq : tau_op ] ) .",
    "we choose @xmath174 , @xmath175 , @xmath156 , and @xmath158 for both ( a ) and ( b ) .",
    "we set @xmath176 for ( a ) and vary @xmath160 for ( b ) . ,",
    "title=\"fig:\",width=332 ]    in more general situations , it has been found that the efficiency at the maximum power obeys a universal form , @xmath243 for small @xmath225 , when the engine and heat baths are strongly coupled  @xcite .",
    "our information engine exhibits a completely distinct behavior even for small @xmath210 . as seen in fig .",
    "[ fig : eff_maxp ] , @xmath237 is more or less the same as @xmath244 in this regime .    in order to investigate this unusual behavior in more detail ,",
    "we now examine @xmath237 analytically for small @xmath210 . as the stall time @xmath232 and",
    "thus @xmath231 ( @xmath245 ) becomes large , a small @xmath186 expansion may be valid for small @xmath210 .",
    "expanding @xmath211 in eq .",
    "( [ eq : eff2 ] ) up to the linear order of @xmath246 , we get @xmath247 where we have used @xmath248 with @xmath249 we also checked that the linear coefficient inside the parentheses in eq .",
    "( [ eq : expand1 ] ) is always numerically positive in the interval @xmath250 .",
    "as the stall time @xmath232 is defined by @xmath251 , eq .",
    "( [ eq : expand1 ] ) immediately gives us @xmath252 where the coefficient @xmath253 is given by @xmath254 it turns out that the scaling behavior in eq .",
    "( [ eq : tau_s ] ) extends quite well to finite @xmath210 , as can be seen in fig .",
    "[ fig : eff_finite ] ( b ) .     at the optimal power for various @xmath59 and @xmath58 . here",
    "we choose @xmath255 and @xmath174 for simplicity.,width=321 ]    the optimal time @xmath231 should satisfy @xmath256 which is rewritten as @xmath257 where @xmath258 . as @xmath259 , @xmath260 should be also small .",
    "this allows us to expand the above equation for small @xmath260 , yielding @xmath261 where @xmath262 and @xmath263 are the same as in eq .",
    "( [ eq : e01 ] ) , and @xmath264 is given by @xmath265 plugging the above expression for @xmath264 into eq .",
    "( [ eq : dpdt_r ] ) , we get @xmath266 which yields @xmath267 ^ 2 } \\ ,",
    "\\label{eq : tau_op}\\ ] ] where @xmath268 \\ .\\ ] ] finally , inserting eq .",
    "( [ eq : tau_op ] ) into eq .",
    "( [ eq : expand1 ] ) , it is straightforward to find an approximate expression for @xmath237 : @xmath269 } \\right ) \\eta_{max } .\\ ] ] therefore , in the limit of @xmath270 , one can see @xmath271 , not @xmath272 , and that the next correction is logarithmic and therefore quite slow .",
    "this calculation confirms that the linear irreversible thermodynamics slightly out of equilibrium in  @xcite should _ not _ be applicable in our case , simply because our processes are far from equilibrium .",
    "indeed , in our case , the entropy production is maximal in the limit of @xmath270 . in future studies ,",
    "the validity of @xmath271 should be addressed in the context of universality for general information engines showing the maximum efficiency at the same point where the entropy production is maximal .",
    "in this work , we have studied a simple example of an information engine which can be realized physically in terms of stochastic markov processes . in agreement with previous studies ,",
    "we find that the information feedback allows one to extract work in a situation where this would be thermodynamically impossible without feedback .",
    "moreover , we confirm that total entropy production during relaxation obeys a fluctuation theorem , implying that the extracted work is bounded _ from above _ by the mutual information gain between memory and system .    providing a physical realization of the memory and the feedback loop , we have shown that the measurement process exhibits similar properties which are opposite in character . in particular ,",
    "the entropy production during measurement is found to obey a fluctuation theorem as well .",
    "this implies that the measurement process itself costs energy , and that this additional energy supply is bounded _ from below _ by the same mutual information gain .",
    "putting these pieces together , it is no surprise that the total setup consisting of system and memory satisfies the conventional second law of thermodynamics .",
    "thus , we have shown that the thermodynamic second law , which is required to hold for the entire system during any finite process , leads to a duality in the properties of system and memory in this kind of information engines .    for simplicity ,",
    "we have presented most of our analytic results in the limit of infinite measurement- and relaxation time .",
    "however , the extension to finite times is straightforward . at the end of the paper ,",
    "we have explicitly described some numerical results for finite - time measurement and relaxation . as in conventional heat engines the efficiency of the information engine",
    "is maximized when the cycle time becomes infinite .",
    "however , in contrast to conventional heat engines , the entropy production is also maximal in this limit . on the other hand",
    ", we have demonstrated that the power gain acquires its maximum at a finite cycle time .",
    "we have also discussed the relation between the maximal efficiency and the efficiency at the operating point of maximal power .",
    "the striking differences between our model and conventional reversible heat engines can be traced back to the fact that our setup operates under non - equilibrium conditions .",
    "it would be interesting to investigate to what extent our observations can be explained in a universal framework .    *",
    "acknowledgments * this research was supported by the nrf grant no.2013r1a6a3a03028463 ( j.u . ) , 2013r1a1a2011079 ( c.k . ) , and 2013r1a1a2a10009722 ( h.p . ) .",
    "we thank the galileo galilei institute for theoretical physics for the hospitality and the infn for partial support during the completion of this work ."
  ],
  "abstract_text": [
    "<S> we study a two - level system controlled in a discrete feedback loop , modeling both the system and the controller in terms of stochastic markov processes . we find that the extracted work , which is known to be bounded from above by the mutual information acquired during measurement , has to be compensated by an additional energy supply during the measurement process itself , which is bounded by the same mutual information from below . </S>",
    "<S> our results confirm that the total cost of operating an information engine is in full agreement with the conventional second law of thermodynamics . </S>",
    "<S> we also consider the efficiency of the information engine as function of the cycle time and discuss the operating condition for maximal power generation . </S>",
    "<S> moreover , we find that the entropy production of our information engine is maximal for maximal efficiency , in sharp contrast to conventional reversible heat engines . </S>"
  ]
}