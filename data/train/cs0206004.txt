{
  "article_text": [
    "the frequent itemset mining problem @xcite is by now well known .",
    "we are given a set of items @xmath0 and a database @xmath1 of subsets of @xmath0 , together with a unique identifier .",
    "the elements of @xmath1 are called transactions .",
    "an _ itemset _",
    "@xmath2 is some set of items ; its _ support _ in @xmath1 , denoted by @xmath3 , is defined as the number of transactions in @xmath1 that contain all items of @xmath4 ; and an itemset is called @xmath5-_frequent _ in @xmath1 if its support in @xmath1 exceeds @xmath5 . @xmath1 and @xmath5 are omitted when they are clear from the context .",
    "the goal is now , given a minimal support threshold and a database , to find all frequent itemsets .",
    "the search space of this problem , all subsets of @xmath0 , is clearly huge . instead of generating and counting the supports of all these itemsets at once ,",
    "several solutions have been proposed to perform a more directed search through all patterns .",
    "however , this directed search enforces several scans through the database , which brings up another great cost , because these databases tend to be very large , and hence they do not fit into main memory .",
    "the standard apriori algorithm @xcite for solving this problem is based on the _ monotonicity property _ :",
    "all supersets of an infrequent itemset must be infrequent .",
    "hence , if an itemset is infrequent , then all of its supersets can be _ pruned _ from the search - space .",
    "an itemset is thus considered potentially frequent , also called a _ candidate _",
    "itemset , only if all its subsets are already known to be frequent . in every step of the algorithm , all candidate itemsets are generated and their supports are then counted by performing a complete scan of the transaction database .",
    "this is repeated until no new candidate itemsets can be generated .",
    "recent studies on frequent itemset mining algorithms resulted in significant performance improvements . in the early days",
    ", the size of the database and the generation of a reasonable amount of frequent itemsets were considered the most costly aspects of frequent itemset mining , and most energy went into minimizing the number of scans through the database . however ,",
    "if the minimal support threshold is set too low , or the data is highly correlated , the number of frequent itemsets itself can be prohibitively large . to overcome this problem , recently several proposals have been made to construct a concise representation of the frequent itemsets , instead of mining all frequent itemsets  @xcite .",
    "[ [ our - contributions ] ] our contributions + + + + + + + + + + + + + + + + +    the main goal of this paper is to present several new methods to identify redundancies in the set of all frequent itemsets and to exploit these redundancies , resulting in a concise representation of all frequent itemsets and significant performance improvements of a mining operation .    1 .",
    "we present a complete set of _ deduction rules _ to derive _ tight _ intervals on the support of candidate itemsets .",
    "we show how the deduction rules can be used to construct a _ minimal representation _ of all frequent itemsets , consisting of all frequent itemsets of which the exact support can not be derived , and present an algorithm that efficiently does so .",
    "3 .   also based on these deduction rules , we present an efficient method to find the exact support of all frequent itemsets , that are not in this concise representation , _ without scanning the database_. 4 .",
    "we present _ connections _ between our proposal and recent proposals for concise representations , such as _ free sets _",
    "@xcite , _ disjunction - free sets _  @xcite , and _ closed sets _  @xcite .",
    "we also show that known tricks to improve performance of frequent itemset mining algorithms , such as used in maxminer  @xcite and pascal  @xcite , can be described in our framework .",
    "we present several _ experiments _ on real - life datasets that show the effectiveness of the deduction rules .",
    "the outline of the paper is as follows . in section 2",
    "we introduce the deduction rules .",
    "section 3 describes how we can use the rules to reduce the set of frequent itemsets . in section 4",
    "we give an algorithm to efficiently find this reduced set , and in section 5 we evaluate the algorithm empirically .",
    "related work is discussed in depth in section 6 .",
    "in all that follows , @xmath0 is the set of all items and @xmath1 is the transaction database .    we will now describe sound and complete rules for deducing tight bounds on the support of an itemset @xmath2 , if the supports of all its subsets are given . in order to do this",
    ", we will not consider itemsets that are no subset of @xmath4 , and we can assume that all items in @xmath1 are elements of @xmath4 .",
    "indeed , `` projecting away '' the other items in a transaction database does not change the supports of the subsets of @xmath4 .    *",
    "( @xmath4-projection ) * let @xmath2 be an itemset .",
    "* the _ @xmath4-projection of a transaction @xmath6 _ , denoted @xmath7 , is defined as @xmath8 * the _ @xmath4-projection of a transaction database @xmath9 _ , denoted @xmath10 , consist of all @xmath4-projected transactions from @xmath1 .",
    "let @xmath11 be itemsets , such that @xmath12 .",
    "for every transaction database @xmath1 , the following holds : @xmath13    before we introduce the deduction rules , we introduce fractions and covers .    * ( @xmath4-fraction ) * let @xmath11 be itemsets , such that @xmath14 , the _ @xmath4-fraction _ of @xmath15 , denoted by @xmath16 equals the number of transactions in @xmath15 that exactly consist of the set @xmath4 .    if @xmath1 is clear from the context , we will write @xmath17 , and if @xmath18 , we will write @xmath19 .",
    "the support of an itemset @xmath4 is then @xmath20    * ( cover ) * let @xmath2 be an itemset .",
    "the _ cover _ of @xmath4 in @xmath1 , denoted by @xmath21 , consists of all transactions in @xmath1 that contain @xmath4 .",
    "again , we will write @xmath22 if @xmath1 is clear from the context .",
    "let @xmath23 be itemsets , and @xmath24 .",
    "notice that @xmath25 , and that @xmath26 . from the well - known _ inclusion - exclusion principle _",
    "@xcite we learn @xmath27 and since @xmath28 , we obtain @xmath29 from now on , we will denote the sum on the right - hand side of this last equation by @xmath30 .",
    "since @xmath17 is always positive , we obtain the following theorem .    for all itemsets @xmath31 , @xmath30 is a lower ( upper ) bound on @xmath32 if @xmath33 is even ( odd ) .",
    "the difference @xmath34 is given by @xmath17 . [ the : rules ]    we will refer to the rule involving @xmath35 as @xmath36 and omit @xmath37 when clear from the context .",
    "if for each subset @xmath38 , the support @xmath39 is given , then the rules @xmath40 allow for calculating lower and upper bounds on the support of @xmath37 .",
    "let @xmath41 denote the greatest lower bound we can derive with these rules , and @xmath42 the smallest upper bound we can derive . since the rules are sound , the support of @xmath37 must be in the interval @xmath43 $ ] . in @xcite , we show also that these bounds on the support of @xmath37 are _ tight _ ; i.e. , for every smaller interval @xmath44\\subset [ l , u]$ ] , we can find a database @xmath45 such that for each subset @xmath4 of @xmath37 , @xmath46 , but the support of @xmath37 is not within @xmath44 $ ] .    for all itemsets @xmath31 , the rules @xmath47 are sound and complete for deducing bounds on the support of @xmath37 based on the supports of all subsets of @xmath37 .",
    "the proof of the completeness relies on the fact that for all @xmath48 , we have @xmath49 we can consider the linear program consisting of all these equalities , together with the conditions @xmath50 for all fractions @xmath19 .",
    "the existence of a database @xmath45 that satisfies the given supports is equivalent to the existence of a solution to this linear program in the @xmath19 s and @xmath51 . from this equivalence ,",
    "tightness of the bounds can be proved . for the details of the proof",
    "we refer to @xcite .",
    "@xmath52    consider the following transaction database .",
    "@xmath53 figure  [ fig : boundsabcd ] gives the rules to determine tight bounds on the support of @xmath54 .",
    "using these deduction rules , we derive the following bounds on @xmath55 _ without counting in the database_.    lower bound:=@xmath56 ( rule @xmath57 ) + upper bound : @xmath58 ( rule @xmath59 )    therefore , we can conclude , without having to rescan the database , that the support of @xmath54 in @xmath1 is exactly @xmath60 , while a standard monotonicity check would yield an upper bound of @xmath61 .",
    "based on the deduction rules , it is possible to generate a summary of the set of frequent itemsets .",
    "indeed , suppose that the deduction rules allow for deducing the support of a frequent itemset @xmath4 _ exactly _ , based on the supports of its subsets .",
    "then there is no need to explicitly count the support of @xmath4 requiring a complete database scan ; if we need the support of @xmath4 , we can always simply derive it using the deduction rules .",
    "such a set @xmath4 , of which we can perfectly derive the support , will be called a _ derivable itemset _ ( di ) , all other itemsets are called _ non - derivable itemsets _ ( ndis ) .",
    "we will show in this section that the set of frequent ndis allows for computing the supports of all other frequent itemsets , and as such , forms a _ concise representation _ @xcite of the frequent itemsets . to prove this result",
    ", we first need to show that when a set @xmath4 is non - derivable , then also all its subsets are non - derivable . for each set @xmath4 , let @xmath62 ( @xmath63 ) denote the lower ( upper ) bound we can derive using the deduction rules .    * ( monotonicity ) * let @xmath64 be an itemset , and @xmath65 an item",
    ". then @xmath66 .",
    "in particular , if @xmath4 is a di , then also @xmath67 is a di .",
    "[ lem : lemma1 ]    the proof is based on the fact that @xmath68 . from theorem  [ the : rules ] we know that @xmath69 is the difference between the bound calculated by @xmath70 and the real support of @xmath4 .",
    "let now @xmath37 be such that the rule @xmath70 calculates the bound that is closest to the support of @xmath4 .",
    "then , the width of the interval @xmath71 $ ] is at least @xmath72 .",
    "furthermore , @xmath73 and @xmath74 are a lower and an upper bound on the support of @xmath67 ( if @xmath75 is odd , then @xmath76 is even and vice versa ) , and these bounds on @xmath67 differ respectively @xmath77 and @xmath78 from the real support of @xmath67 .",
    "when we combine all these observations , we get : @xmath79 .",
    "this lemma gives us the following valuable insights .",
    "[ cor : cor1 ] the width of the intervals exponentially shrinks with the size of the itemsets .    this remarkable fact is a strong indication that the number of large ndis will be very small .",
    "this reasoning will be supported by the results of the experiments .",
    "[ cor : cor2 ] if @xmath4 is a ndi , but it turns out that @xmath70 equals the support of @xmath4 , then all supersets @xmath67 of @xmath4 will be a di , with rules @xmath73 and @xmath80 .",
    "we will use this observation to avoid checking all possible rules for @xmath67 .",
    "this avoidance can be done in the following way : whenever we calculate bounds on the support of an itemset @xmath4 , we remember the lower and upper bound @xmath81 . if @xmath4 is a ndi ; i.e. , @xmath82 , then we will have to count its support . after we counted the support , the tests @xmath83 and @xmath84 are performed . if one of these two equalities obtains , we know that all supersets of @xmath4 are derivable , without having to calculate the bounds .    if we know that @xmath4 is a di , and that rule @xmath70 gives the exact support of @xmath4 , then @xmath74 gives the exact support for @xmath67 .",
    "[ cor : cor3 ] suppose that we want to build the entire set of frequent itemsets starting from the concise representation .",
    "we can then use this observation to improve the performance of deducing all supports .",
    "suppose we need to deduce the support of a set @xmath4 , and of a superset @xmath37 of @xmath4 ; instead of trying all rules to find the exact support for @xmath37 , we know in advance , because we already evaluated @xmath4 , which rule to choose .",
    "hence , for any itemset which is known to be a di , we only have to compute a single deduction rule to know its exact support .    from lemma  [ lem : lemma1 ]",
    ", we easily obtain the following theorem , saying that the set of ndis is a concise representation .",
    "we omit the proof due to space limitations .",
    "for every database @xmath1 , and every support threshold @xmath5 , let @xmath85 be the following set : @xmath86 @xmath85 is a concise representation for the frequent itemsets , and for each itemset @xmath37 not in @xmath85 , we can decide whether @xmath37 is frequent , and if @xmath37 is frequent , we can exactly derive its support from the information in @xmath85 .",
    "based on the results in the previous section , we propose a level - wise algorithm to find all frequent ndis .",
    "since derivability is monotone , we can prune an itemset if it is derivable .",
    "this gives the ndi - algorithm as shown below .",
    "the correctness of the algorithm follows from the results in lemma  [ lem : lemma1 ] .",
    "= = = = = = = = + ndi(@xmath87,@xmath5 ) + @xmath88 ; @xmath89 ; @xmath90 + * for all * @xmath4 in @xmath91 * do * @xmath92 + * while * @xmath93 not empty * do * + count the supports of all candidates in @xmath93 in one pass over @xmath1 ; + @xmath94 .",
    "+ @xmath95 + @xmath96 + * for all * @xmath97 * do * + * if * @xmath98 and @xmath99 * then * + @xmath100 + @xmath101 + @xmath102 + * for all * @xmath103 * do * + compute bounds @xmath43 $ ] on support of @xmath37 ; + * if * @xmath104 * then * @xmath105 + @xmath106 + * end while * + * return * ndi    since evaluating all rules can be very cumbersome , in the experiments we show what the effect is of only using a couple of rules .",
    "we will say that we use rules _ up to depth @xmath107 _ if we only evaluate the rules @xmath108 for @xmath109 .",
    "the experiments show that in most cases , the gain of evaluating rules up to depth @xmath107 instead of up to depth @xmath110 typically quickly decreases if @xmath107 increases .",
    "therefore , we can conclude that in practice most pruning is done by the rules of limited depth .",
    "for our experiments , we implemented an optimized version of the apriori algorithm and the ndi algorithm described in the previous section .",
    "we performed our experiments on several real - life datasets with different characteristics , among which a dataset obtained from a belgian retail market , which is a sparse dataset of @xmath111 transaction over @xmath112 items .",
    "the second dataset was the bms - webview-1 dataset donated by z. zheng et al .",
    "@xcite , containing @xmath113 transactions over @xmath114 items .",
    "the third dataset is the dense census - dataset as available in the uci kdd repository  @xcite , which we transformed into a transaction database by creating a different item for every attribute - value pair , resulting in @xmath115 transactions over @xmath116 items .",
    "the results on all these datasets were very similar and we will therefore only describe the results for the latter dataset .    figure  [ fig : widths ] shows the average width of the intervals computed for all candidate itemsets of size @xmath107 .",
    "naturally , the interval - width of the singleton candidate itemsets is @xmath115 , and is not shown in the figure . in the second pass of the ndi - algorithm , all candidate itemsets of size @xmath61 are generated and their intervals deduced . as can be seen , the average interval size of most candidate itemsets of size 2 is @xmath117 . from then on ,",
    "the interval sizes decrease exponentially as was predicted by corollary  [ cor : cor1 ] .",
    "figure  [ fig : crsupp ] shows the size of the concise representation of all ndis compared to the total number of frequent patterns as generated by apriori , for varying minimal support thresholds .",
    "if this threshold was set to @xmath118 , there exist @xmath119 frequent patterns of which only @xmath120 are non - derivable .",
    "again this shows the theoretical results obtained in the previous sections .    in the last experiment",
    ", we compared the strength of evaluating the deduction rules up to a certain depth , and the time needed to generate all ndis w.r.t .",
    "the given depth .",
    "figure  [ fig : crsize ] shows the results . on the x - axis , we show the depth up to which rules are evaluated .",
    "we denoted the standard apriori monotonicity check by @xmath121 , although it is actually equivalent to the rules of depth @xmath60 .",
    "the reason for this is that we also used the other optimizations described in section  3 .",
    "more specifically , if the lower or upper bound of an itemset equals its actual support , we can prune its supersets , which is denoted as depth @xmath60 in this figure .",
    "the left y - axis shows the number of ndis w.r.t . the given depth and",
    "is represented by the line ` concise representation ' .",
    "the line ` ndi ' shows the time needed to generate these ndis .",
    "the time is shown on the right y - axis .",
    "the ` ndi+di ' line shows the time needed to generate all ndis plus the time needed to derive all dis , resulting in all frequent patterns . as can be seen , the size of the concise representation drops quickly only using the rules of depth @xmath60 and @xmath61 . from there on ,",
    "higher depths result in a slight decrease of the number of ndis . from depth",
    "@xmath122 on , this size stays the same , which is not that remarkable since the number of ndis of these sizes is also small .",
    "the time needed to generate these sets is best if the rules are only evaluated up to depth @xmath61 .",
    "still , the running time is almost always better than the time needed to generate all frequent itemsets ( depth 0 ) , and is hardly higher for higher depths . for higher depths , the needed time increases , which is due to the number of rules that need to be evaluated .",
    "also note that the total time required for generating all ndis and deriving all dis is also better than generating all frequent patterns at once , at depth @xmath60,@xmath61,and @xmath123 .",
    "this is due to the fact that the ndi algorithm has to perform less scans through the transaction database . for",
    "larger databases this would also happen for the other depths , since the derivation of all dis requires no scan through the database at all .",
    "in the literature , there exist already a number of concise representations for frequent itemsets .",
    "the most important ones are _ closed itemsets _ , _ free itemsets _ , and _ disjunction - free itemsets_. we compare the different concise representations with the ndi - representation .",
    "an itemset @xmath4 is called _ free _ if it has no subset with the same support .",
    "we will denote the set of all frequent free itemsets with @xmath124 . in @xcite ,",
    "the authors show that freeness is anti - monotone ; the subset of a free set must also be free .",
    "@xmath124 itself is not a concise representation for the frequent sets , unless if the set @xmath125 is added  @xcite .",
    "we call the concise representation consisting of these two sets @xmath126 .",
    "notice that free sets @xcite and generators  @xcite are the same .",
    "_ disjunction - free _ sets are essentially an extension of free sets .",
    "a set @xmath4 is called disjunction - free if there does not exist two items @xmath127 in @xmath4 such that @xmath128 .",
    "this rule is in fact our rule @xmath129 .",
    "notice that free sets are a special case of this case , namely when @xmath130 .",
    "we will denote the set of frequent disjunction - free sets by @xmath131 .",
    "again , disjunction - freeness is anti - monotone , and @xmath131 is not a concise representation of the set of frequent itemsets , unless we add the border of @xmath131 .",
    "we call the concise representation containing these two sets @xmath132 .",
    "another type of concise representation that received a lot of attention in the literature @xcite are the closed itemsets .",
    "they can be introduced as follows : the _ closure _ of an itemset @xmath4 is the largest superset of @xmath4 such that its support equals the support of @xmath4 .",
    "this superset is unique and is denoted by @xmath133 .",
    "an itemset is called _ closed _ if it equals its closure .",
    "we will denote the set of all frequent closed itemsets by @xmath134 . in @xcite ,",
    "the authors show that @xmath134 is a concise representation for the frequent itemsets .          1 .",
    "we first show that @xmath135 .",
    "* let @xmath136 be a closed set .",
    "let @xmath4 be a smallest subsets of @xmath136 such that @xmath137 .",
    "suppose @xmath4 is not a free set .",
    "then there exist @xmath138 such that @xmath139 .",
    "this rule however implies that @xmath140 .",
    "this is in contradiction with the minimality of @xmath4 . *",
    "trivial , since @xmath141 is idempotent .",
    "+ this equality implies that @xmath142 is always a surjective function from @xmath143 to @xmath144 , and therefore , @xmath145 .",
    "2 .   suppose @xmath4 is not in @xmath132 . if @xmath4 is not frequent , then the result is trivially satisfied .",
    "otherwise , this means that @xmath4 is not a frequent free set , and that there is at least one subset @xmath37 of @xmath4 that is also not a frequent free set ( otherwise @xmath4 would be in the border of @xmath131 . )",
    "therefore , there exist @xmath146 such that @xmath147 .",
    "we now conclude , using lemma  [ lem : lemma1 ] , that @xmath4 is a derivable itemset , and thus not in ndi .",
    "other possible inclusions between the described concise representations do not satisfy , i.e. , for some datasets and support thresholds we have @xmath148 , while other datasets and support thresholds have @xmath149 .",
    "we omit the proof of this due to space limitations . we should however mention that even though @xmath131 is always a superset of ndi , in the experiments the gain of evaluating the extra rules is often small . in many cases",
    "the reduction of @xmath132 , which corresponds to evaluating rules up to depth 2 in our framework , is almost as big as the reduction using the whole set of rules .",
    "since our rules are complete , this shows that additional gain is in many cases unlikely .        in maxminer ,",
    "_ bayardo _ uses the following rule to derive a lower bound on the support of an itemset : @xmath150 with @xmath151 , @xmath138 , and @xmath152 .",
    "this derivation corresponds to repeated application of rules @xmath129 .      in their pascal - algorithm , _ bastide et al . _",
    "use counting inference to avoid counting the support of all candidates .",
    "the rule they are using to avoid counting is based on our rule @xmath153 . in fact",
    "the pascal - algorithm corresponds to our algorithm when we only check rules up to depth 1 , and do not prune derivable sets . instead of counting the derivable sets",
    ", we use the derived support . here the same remark as with the @xmath132-representation applies ; although pascal does not use all rules , in many cases the performance comes very close to evaluating all rules , showing that for these databases pascal is nearly optimal .",
    "j.  pei , j.  han , and r.  mao .",
    "closet : an efficient algorithm for mining frequent closed itemsets . in w.",
    "chen , j.f .",
    "naughton , and p.a .",
    "bernstein , editors , _ acm sigmod workshop on research issues in data mining and knowledge discovery _ , dallas , tx , 2000 ."
  ],
  "abstract_text": [
    "<S> recent studies on frequent itemset mining algorithms resulted in significant performance improvements . however , </S>",
    "<S> if the minimal support threshold is set too low , or the data is highly correlated , the number of frequent itemsets itself can be prohibitively large . to overcome this problem , recently several proposals have been made to construct a concise representation of the frequent itemsets , instead of mining all frequent itemsets . </S>",
    "<S> the main goal of this paper is to identify redundancies in the set of all frequent itemsets and to exploit these redundancies in order to reduce the result of a mining operation . </S>",
    "<S> we present deduction rules to derive tight bounds on the support of candidate itemsets . </S>",
    "<S> we show how the deduction rules allow for constructing a minimal representation for all frequent itemsets . </S>",
    "<S> we also present connections between our proposal and recent proposals for concise representations and we give the results of experiments on real - life datasets that show the effectiveness of the deduction rules . </S>",
    "<S> in fact , the experiments even show that in many cases , first mining the concise representation , and then creating the frequent itemsets from this representation outperforms existing frequent set mining algorithms . </S>"
  ]
}