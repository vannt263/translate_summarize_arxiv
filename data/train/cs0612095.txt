{
  "article_text": [
    "in machine learning pure applications of mdl are rare , partially because of the difficulties one encounters trying to define an adequate model code and data - to - model code , and partially because of the operational difficulties that are poorly understood .",
    "we analyze aspects of both the power and the perils of mdl precisely and formally .",
    "let us first resurrect a familiar problem from our childhood to illustrate some of the issues involved .",
    "the process of solving a jigsaw puzzle involves an _ incremental reduction of entropy _ , and this serves to illustrate the analogous features of the learning problems which are the main issues of this work .",
    "initially , when the pieces come out of the box they have a completely random ordering . gradually we combine pieces , thus reducing the entropy and increasing the order until the puzzle is solved . in this last stage",
    "we have found a maximal ordering .",
    "suppose that alice and bob both start to solve two versions of the same puzzle , but that they follow different strategies . initially , alice sorts all pieces according to color , and bob starts by sorting the pieces according to shape .",
    "( for the sake of argument we assume that the puzzle has no recognizable edge pieces . ) the crucial insight , shared by experienced puzzle aficionados , is that alice s strategy is efficient whereas bob s strategy is not and is in fact even worse than a random strategy .",
    "alice s strategy is efficient , since the probability that pieces with about the same color match is much greater than the unconditional probability of a match . on the other hand",
    "the information about the shape of the pieces can only be used in a relatively late stage of the puzzle process . bob s effort in the beginning",
    "is a waste of time , because he must reorder the pieces before he can proceed to solve the puzzle .",
    "this example shows that if the solution of a problem depends on finding a _ maximal _ reduction of entropy this does not mean that _ every _ reduction of entropy brings us closer to the solution .",
    "consequently reduction of entropy is not in all cases a good strategy .      above",
    "we use `` entropy '' in the often used , but inaccurate , sense of `` measure of unorderedness of an individual arrangement . ''",
    "however , entropy is a measure of uncertainty associated with a random variable , here a set of arrangements each of which has a certain probability of occurring .",
    "the entropy of every individual arrangement is by definition zero . to circumvent this problem , often the notion of `` empirical entropy '' is used , where certain features like letter frequencies of the individual object are analyzed , and the entropy is taken with respect to the set of all objects having the same features .",
    "the result obviously depends on the choice of what features to use : no features gives maximal entropy and all features ( determining the individual object uniquely ) gives entropy zero again . unless one has knowledge of the characteristics of a definite random variable producing the object as a typical outcome , this procedure gives arbitrary and presumably meaningless , results .",
    "this conundrum arises since classical information theory deals with random variables and the communication of information .",
    "it does not deal with the information ( and the complexity thereof ) in an individual object independent of an existing ( or nonexisting ) random variable producing it . to capture the latter notion",
    "precisely one has to use `` kolmogorov complexity '' instead of `` entropy , '' and we will do so in our treatment . for now , the `` kolmogorov complexity '' of a file is the number of bits in the ultimately compressed version of the file from which the original can still be losslessly extracted by a fixed general purpose decompression program .      transferring the jigsaw puzzling insights to the general case of learning algorithms using the minimum description length principle ( mdl ) , @xcite",
    ", we observe that although it may be true that the maximal compression yields the best solution , it may still not be true that every incremental compression brings us closer to the solution .",
    "moreover , in the case of many mdl problems there is a complicating issue in the fact that the maximal compression can not be computed .",
    "more formally , in constrained model selection the model is taken from a given model class .",
    "using two - part mdl codes for the given data , we assume that the shortest two - part code for the data , consisting of the model code and the data - to - model code , yields the best model for the data . to obtain the shortest code , a natural way is to approximate it by a process of finding ever shorter candidate two - part codes .",
    "since we start with a finite two - part code , and with every new candidate two - part code we decrease the code length , eventually we must achieve the shortest two - part code ( assuming that we search through all two - part codes for the data ) .",
    "unfortunately , there are two problems : ( i ) the computation to find the next shorter two - part code may be very long , and we may not know how long ; and ( ii ) we may not know when we have reached the shortest two - part code : with each candidate two - part code there is the possibility that further computation may yield yet a shorter one .",
    "but because of item ( i ) we can not a priori bound the length of that computation .",
    "there is also the possibility that the algorithm will never yield the shortest two - part code because it considers only part of the search space or gets trapped in a nonoptimal two - part code .",
    "we show that for some mdl algorithms the sequence of ever shorter two - part codes for the data converges in a finite number of steps to the best model . however , for every mdl algorithm the intermediate models may not convergence monotonically in goodness .",
    "in fact , in the sequence of candidate two - part codes converging to a ( globally or locally ) shortest , it is possible that the models involved oscillate from being good to bad .",
    "convergence is only monotone if the model - code parts in the successive two - part codes are always the shortest ( most compressed ) codes for the models involved . but this property can not be guaranteed by any effective method .",
    "it is very difficult , if not impossible , to formalize the goodness of fit of an individual model for individual data in the classic statistics setting , which is probabilistic .",
    "therefore , it is impossible to express the practically important issue above in those terms .",
    "fortunately , new developments in the theory of kolmogorov complexity @xcite make it possible to rigorously analyze the questions involved , possibly involving noncomputable quantities .",
    "but it is better to have a definite statement in a theory than having no definite statement at all .",
    "moreover , for certain algorithms ( like algorithm optimal mdl in theorem  [ alg.mdl ] ) we can guarantee that they satisfy the conditions required , even though these are possibly noncomputable . in section  [ sect.dm ]",
    "we review the necessary notions from @xcite , both in order that the paper is self - contained and the definitions and notations are extended from the previously used singleton data to multiple data samples .",
    "theorem  [ theo.recoding ] shows that the use of mdl will be approximately invariant under recoding of the data .",
    "the next two sections contain the main results : definition  [ def.mdlalg ] defines the notion of an mdl algorithm .",
    "theorem  [ alg.mdl ] shows that there exists such an mdl algorithm that in the ( finite ) limit results in an optimal model .",
    "the next statements are about mdl algorithms in general , also the ones that do not necessarily result in an optimal mdl code .",
    "theorem  [ theo.approxim ] states a sufficient condition for improvement of the randomness deficiency ( goodness of fit ) of two consecutive length - decreasing mdl codes .",
    "this extends lemma v.2 of the @xcite ( which assumes all programs are shortest ) and corrects the proof concerned .",
    "the theory is applied and illustrated in section  [ sect.single ] : theorem  [ theo.fluctuate ] shows by example that a minor violation of the sufficiency condition in theorem  [ theo.approxim ] can result in worsening the randomness deficiency ( goodness of fit ) of two consecutive length - decreasing mdl codes .",
    "the special case of learning dfas from positive examples is treated in section  [ sect.multi ] .",
    "the main result shows , for a concrete and computable mdl code , that a decrease in the length of the two - part mdl code does not imply a better model fit ( see section  [ sect.lmdl ] ) unless there is a sufficiently large decrease as that required in theorem  [ theo.approxim ] ( see remark  [ rem.smc ] ) .",
    "let @xmath0 , where @xmath1 denotes the natural numbers and we identify @xmath1 and @xmath2 according to the correspondence @xmath3 here @xmath4 denotes the _ empty word_. the _ length _ @xmath5 of @xmath6 is the number of bits in the binary string @xmath6 , not to be confused with the _ cardinality _ @xmath7 of a finite set @xmath8 . for example , @xmath9 and @xmath10 , while @xmath11 and @xmath12 . below we will use the natural numbers and the binary strings interchangeably .",
    "definitions , notations , and facts we use about prefix codes , self - delimiting codes , and kolmogorov complexity , can be found in @xcite and are briefly reviewed in appendix  [ sect.prel ] .",
    "the emphasis is on binary sequences only for convenience ; observations in any alphabet can be encoded in binary in a way that is theory neutral .",
    "therefore , we consider only data @xmath6 in @xmath2 . in a typical statistical inference situation",
    "we are given a subset of @xmath2 , the data sample , and are required to infer a model for the data sample .",
    "instead of @xmath2 we will consider @xmath13 for some fixed but arbitrarily large @xmath14 .    a _",
    "data sample _ @xmath15 is a subset of @xmath16 . for technical convenience",
    "we want a model @xmath17 for @xmath15 to contain information about the cardinality of @xmath15 .",
    "a _ model _ @xmath17 has the form @xmath18 , where @xmath19 and @xmath20 .",
    "we can think of @xmath21 as the @xmath21th binary string in @xmath13 .",
    "denote the cardinalities by lower case letters : @xmath22 if @xmath15 is a data sample and _ @xmath17 is a model for @xmath15 _ then @xmath23 , @xmath24 , and we write @xmath25 or @xmath26 .",
    "denote the _ complexity of a finite set _ @xmath8 by @xmath27the length ( number of bits ) of the shortest binary program @xmath28 from which the reference universal prefix machine @xmath29 computes a lexicographic listing of the elements of @xmath30 and then halts .",
    "that is , if @xmath31 , the elements given in lexicographic order , then @xmath32 . the shortest program @xmath28 , or , if there is more than one such shortest program , then the first one that halts in a standard dovetailed running of all programs , is denoted by @xmath33 .",
    "the _ conditional complexity _",
    "@xmath34 of @xmath26 is the length ( number of bits ) of the shortest binary program @xmath28 from which the reference universal prefix machine @xmath29 from input @xmath17 ( given as a list of elements ) outputs @xmath15 as a lexicographically ordered list of elements and halts .",
    "we have @xmath35 the upper bound follows by considering a self - delimiting code of @xmath15 given @xmath17 ( including the number @xmath36 of elements in @xmath15 ) , consisting of a @xmath37 bit long index of @xmath15 in the lexicographic ordering of the number of ways to choose @xmath36 elements from @xmath38 .",
    "this code is called the _ data - to - model code_. its length quantifies the maximal `` typicality , '' or `` randomness , '' any data sample @xmath15 of @xmath36 elements can have with respect to model @xmath17 with @xmath25 .",
    "the lack of typicality of @xmath15 with respect to @xmath17 is measured by the amount by which @xmath34 falls short of the length of the data - to - model code .",
    "the _ randomness deficiency _ of @xmath26 is defined by @xmath39 for @xmath26 , and @xmath40 otherwise .",
    "the randomness deficiency can be a little smaller than 0 , but not more than a constant . if the randomness deficiency is not much greater than 0 , then there are no simple special properties that single @xmath15 out from the majority of data samples of cardinality @xmath36 to be drawn from @xmath38 .",
    "this is not just terminology : if @xmath41 is small enough , then @xmath15 satisfies _ all _ properties of low kolmogorov complexity that hold for the majority of subsets of cardinality @xmath36 of @xmath42 . to be precise : a _ property _",
    "@xmath43 represented by @xmath17 is a subset of @xmath42 , and we say that @xmath15 satisfies property @xmath43 if @xmath15 is a subset of @xmath43 .",
    "let @xmath44 be natural numbers , and let @xmath45 , @xmath24 , @xmath46 , and let @xmath47 be a simple function of the natural numbers to the real numbers , that is , @xmath48 is a constant , for example , @xmath47 is @xmath49 or @xmath50 .",
    "\\(i ) if @xmath43 is a property satisfied by all @xmath26 with @xmath51 , then @xmath43 holds for a fraction of at least @xmath52 of the subsets of @xmath53 .",
    "\\(ii ) let @xmath43 be a property that holds for a fraction of at least @xmath54 of the subsets of @xmath38 .",
    "there is a constant @xmath55 , such that @xmath43 holds for every @xmath26 with @xmath56 .",
    "\\(i ) by assumption , all data samples @xmath26 with @xmath57 satisfy @xmath43 .",
    "there are only @xmath58 programs of length smaller than @xmath59 , so there are at most that many @xmath26 that do not satisfy .",
    "there are @xmath60 sets @xmath15 that satisfy @xmath26 , and hence a fraction of at least @xmath52 of them satisfy .",
    "\\(ii ) suppose @xmath43 does not hold for a data sample @xmath26 and the randomness deficiency satisfies @xmath61 .",
    "then we can reconstruct @xmath15 from a description of @xmath17 , and @xmath15 s index @xmath62 in an effective enumeration of all subsets of @xmath17 of cardinality @xmath36 for which @xmath43 does nt hold .",
    "there are at most @xmath63 such data samples by assumption , and therefore there are constants @xmath64 such that @xmath65 hence , by the assumption on the randomness deficiency of @xmath15 , we find @xmath66 , which contradicts the necessary nonnegativity of @xmath67 if we choose @xmath68 .",
    "the _ minimal randomness deficiency _ function of the data sample @xmath15 is defined by @xmath69 where we set @xmath70 .",
    "the smaller @xmath71 is , the more @xmath15 can be considered as a _",
    "typical _ data sample from @xmath17 .",
    "this means that a set @xmath17 for which @xmath15 incurs minimal randomness deficiency , in the model class of contemplated sets of given maximal kolmogorov complexity , is a `` best fitting '' model for @xmath15 in that model class  a most likely explanation , and @xmath72 can be viewed as a _ constrained best fit estimator_.      the length of the minimal two - part code for @xmath15 with model @xmath25 consist of the model cost @xmath73 plus the length of the index of @xmath15 in the enumeration of choices of @xmath36 elements out of @xmath74 ( @xmath75 and @xmath38 ) .",
    "consider the model class of @xmath17 s of given maximal kolmogorov complexity @xmath76 .",
    "the _ mdl _ function or _ constrained mdl estimator _ is @xmath77 where @xmath78 is the total length of two - part code of @xmath15 with help of the model @xmath17 .",
    "this function @xmath79 is the celebrated optimal two - part mdl code length as a function of @xmath76 , with the model class restricted to models of code length at most @xmath76 .",
    "the functions @xmath80 and @xmath81 are examples of kolmogorov s _ structure functions _ , @xcite .",
    "indeed , consider the following _ two - part code _ for @xmath26 : the first part is a shortest self - delimiting program @xmath28 for @xmath17 and the second part is @xmath37 bit long index of @xmath15 in the lexicographic ordering of all choices of @xmath36 elements from @xmath17 . since @xmath17 determines @xmath82",
    "this code is self - delimiting and we obtain the two - part code , where the constant @xmath83 is the length of an additional program that reconstructs @xmath15 from its two - part code .",
    "trivially , @xmath84 .",
    "for those @xmath76 s that have @xmath85 , the associated model @xmath25 in at most @xmath76 bits ( witness for @xmath86 ) is called a _ sufficient statistic _ for @xmath15 .",
    "if @xmath17 is a sufficient statistic for @xmath15 , then the randomness deficiency of @xmath15 in @xmath17 is @xmath83 , that is , @xmath15 is a typical data sample for @xmath17 , and @xmath17 is a model of best fit for @xmath15 .    if @xmath17 is a sufficient statistic for @xmath15 , then @xmath87 .",
    "the left - hand side of the latter equation is a two - part description of @xmath15 using the model @xmath25 and as data - to - model code the index of @xmath15 in the enumeration of the number of choices of @xmath36 elements from @xmath17 in @xmath88 bits .",
    "this left - hand side equals the right - hand side which is the shortest one - part code of @xmath15 in @xmath89 bits .",
    "therefore , @xmath90 the first and second inequalities are straightforward , the third inequality states that given @xmath25 we can describe @xmath15 in a self - delimiting manner in @xmath91 bits , and the final equality follows by the sufficiency property .",
    "this sequence of ( in)equalities implies that @xmath92 .",
    "note that the data sample @xmath15 can have randomness deficiency about 0 , and hence be a typical element for models @xmath17 , while @xmath17 is not a sufficient statistic .",
    "a sufficient statistic @xmath17 for @xmath15 has the additional property , apart from being a model of best fit , that @xmath93 and therefore by in appendix  [ sect.prel ] we have @xmath94 : the sufficient statistic @xmath17 is a model of best fit that is almost completely determined by @xmath95 , a shortest program for @xmath15 .",
    "the sufficient statistic associated with @xmath86 with the least @xmath76 is called the _ minimal sufficient statistic_.    reference @xcite and this paper analyze a canonical setting where the models are finite sets .",
    "we can generalize the treatment to the case where the models are the computable probability mass functions .",
    "the computability requirement does not seem very restrictive .",
    "we cover most , if not all , probability mass functions ever considered , provided they have computable parameters . in the case of multiple data we consider probability mass functions @xmath43 that map subsets @xmath96 into @xmath97",
    "$ ] such that @xmath98 .",
    "for every @xmath99 , we define @xmath100 . for data",
    "@xmath15 with @xmath101 we obtain @xmath102 and @xmath103 is a computable probability mass function with @xmath104@xmath105 .",
    "the general model class of computable probability mass functions is equivalent to the finite set model class , up to an additive logarithmic @xmath106 term .",
    "this result for multiple data generalizes the corresponding result for singleton data in @xcite . since the other results in @xcite such as and those in appendix  [ sect.formal ] , generalized to multiple data ,",
    "hold only up to the same additive logarithmic term anyway , they carry over to the probability models .",
    "the generality of the results are at the same time a restriction . in classical statistics",
    "one is commonly interested in model classes that are partially poorer and partially richer than the ones we consider .",
    "for example , the class of bernoulli processes , or @xmath107-state markov chains , is poorer than the class of computable probability mass functions of moderate maximal kolmogorov complexity @xmath76 , in that the latter class may contain functions that require far more complex computations than the rigid syntax of the classical classes allows .",
    "indeed , the class of computable probability mass functions of even moderate complexity allows implementation of a function mimicking a universal turing machine computation . on the other hand ,",
    "even the simple bernoulli process can be equipped with a noncomputable real bias in @xmath108 , and hence the generated probability mass function over @xmath14 trials is not a computable function .",
    "this incomparability of the algorithmic model classes studied here and the traditional statistical model classes , means that the current results can not be directly transplanted to the traditional setting .",
    "they should be regarded as pristine truths that hold in a platonic world that can be used as guideline to develop analogues in model classes that are of more traditional concern , as in @xcite .",
    "the first parameter we are interested in is the _ simplicity _ @xmath73 of the model @xmath17 explaining the data sample @xmath15 ( @xmath26 ) .",
    "the second parameter is _ how typical _ the data is with respect to @xmath17 , expressed by the randomness deficiency @xmath109 .",
    "the third parameter is how _ short the two part code _ @xmath110 of the data sample @xmath15 using theory @xmath17 with @xmath26 is .",
    "the second part consists of the full - length index , ignoring saving in code length using possible nontypicality of @xmath15 in @xmath17 ( such as being the first @xmath36 elements in the enumeration of @xmath38 ) .",
    "these parameters induce a partial order on the contemplated set of models .",
    "we write @xmath111 , if @xmath112 scores equal or less than @xmath113 in all three parameters . if this is the case , then we may say that @xmath112 is at least as good as @xmath113 as an explanation for @xmath15 ( although the converse need not necessarily hold , in the sense that it is possible that @xmath112 is at least as good a model for @xmath15 as @xmath113 without scoring better than @xmath113 in all three parameters simultaneously ) .",
    "the algorithmic statistical properties of a data sample @xmath15 are fully represented by the set @xmath114 of all triples @xmath115 with @xmath25 , together with a component wise order relation on the elements of those triples . the complete characterization of this set follows from the results in @xcite , provided",
    "we generalize the singleton case treated there to the multiple data case required here .    in that reference",
    "it is shown that if we minimize the length of a two - part code for an individual data sample , the two - part code consisting of a model description and a data - to - model code over the _ class of all computable models _ of at most a given complexity , then the following is the case . with _",
    "certainty _ and not only with high probability as in the classical case this process selects an individual model that in a rigorous sense is ( almost ) the best explanation for the individual data sample that occurs among the contemplated models .",
    "( in modern versions of mdl , @xcite , one selects the model that minimizes just the data - to - model code length ( ignoring the model code length ) , or minimax and mixture mdls .",
    "these are not treated here . )",
    "these results are exposed in the proof and analysis of the equality : @xmath116 which holds within negligible additive @xmath117 terms , in argument and value .",
    "we give the precise statement in in appendix  [ sect.formal ] .",
    "[ rem.witness ] every model ( set ) @xmath17 that witnesses the value @xmath86 , also witnesses the value @xmath118 ( but not vice versa ) .",
    "the functions @xmath81 and @xmath80 can assume all possible shapes over their full domain of definition ( up to additive logarithmic precision in both argument and value ) .",
    "we summarize these matters in appendix  [ sect.formal ] .",
    "how difficult is it to compute the functions @xmath119 , and the minimal sufficient statistic ? to express the properties appropriately we require the notion of functions that are not computable , but can be approximated monotonically by a computable function .",
    "[ def.semi ] a function @xmath120 is _ upper semicomputable _ if there is a turing machine @xmath121 computing a total function @xmath122 such that @xmath123 and @xmath124 .",
    "this means that @xmath125 can be computably approximated from above . if @xmath126 is upper semicomputable , then @xmath125 is lower semicomputable . a function",
    "is called _ semicomputable _ if it is either upper semicomputable or lower semicomputable .",
    "if @xmath125 is both upper semicomputable and lower semicomputable , then we call @xmath125 _ computable _ ( or recursive if the domain is integer or rational ) .    to put matters in perspective : even if a function is computable , the most feasible type identified above",
    ", this does nt mean much in practice .",
    "functions like @xmath127 of which the computation terminates in computation time of @xmath128 ( say measured in flops ) , are among the easily computable ones .",
    "but for @xmath129 , even a computer performing an unrealistic teraflop per second , requires @xmath130 seconds .",
    "this is more than @xmath131 years .",
    "it is out of the question to perform such computations .",
    "thus , the fact that a function or problem solution is computable gives no insight in how _",
    "feasible _ it is .",
    "but there are worse functions and problems possible : for example , the ones that are semicomputable but not computable . or worse yet , functions that are not even semicomputable .",
    "semicomputability gives no knowledge of convergence guarantees : even though the limit value is monotonically approximated , at no stage in the process do we know how close we are to the limit value . in section  [ ex.mdl ] , the indirect method of algorithm optimal",
    "mdl shows that the function @xmath81 ( the mdl - estimator ) can be monotonically approximated in the upper semicomputable sense .",
    "but in @xcite it was shown for singleton data samples , and therefore _ a fortiori _ for multiple data samples @xmath15 , the fitness function @xmath80 ( the direct method of remark  [ rem.direct ] ) can not be monotonically approximated in that sense , nor in the lower semicomputable sense , in both cases not even up to any relevant precision .",
    "let us formulate this a little more precisely :    the functions @xmath132 have a finite domain for a given @xmath15 and hence can be given as a table  so formally speaking they are computable .",
    "but this evades the issue : there is no algorithm that computes these functions for given @xmath15 and @xmath76 . considering them as two - argument functions it was shown ( and the claimed precision quantified ) :    * the function @xmath86 is upper semicomputable but not computable up to any reasonable precision .",
    "* there is no algorithm that given @xmath95 and @xmath76 finds @xmath86 .",
    "* the function @xmath72 is not upper nor lower semicomputable , not even to any reasonable precision . to put @xmath72 s computability properties in perspective ,",
    "clearly we can compute it given an oracle for the halting problem .",
    "+ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ the _ halting problem _ is the problem whether an arbitrary turing machine started on an initially all-0 tape will eventually terminate or compute forever .",
    "this problem was shown to be undecidable by a.m. turing in 1937 , see for example @xcite .",
    "an oracle for the halting problem will , when asked , tell whether a given turing machine computation will or will not terminate .",
    "such a device is assumed in order to determine theoretical degrees of ( non)computability , and is deemed not to exist . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ + but using such an oracle gives us power beyond effective ( semi)computability and therefore brings us outside the concerns of this paper .",
    "* there is no algorithm that given @xmath15 and @xmath89 finds a minimal sufficient statistic for @xmath15 up to any reasonable precision .      in what sense are the functions invariant under recoding of the data ? if the functions @xmath80 and @xmath81 give us the stochastic properties of the data @xmath15 , then we would not expect those properties to change under recoding of the data into another format . for convenience ,",
    "let us look at a singleton example .",
    "suppose we recode @xmath133 by a shortest program @xmath134 for it .",
    "since @xmath134 is incompressible it is a typical element of the set of all strings of length @xmath135 , and hence @xmath136 drops to the kolmogorov complexity @xmath137 already for some @xmath138 , so almost immediately ( and it stays within logarithmic distance of that line henceforth ) .",
    "that is , @xmath139 up to logarithmic additive terms in argument and value , irrespective of the ( possibly quite different ) shape of @xmath140",
    ". since the kolmogorov complexity function @xmath141 is not recursive , @xcite , the recoding function @xmath142 is also not recursive .",
    "moreover , while @xmath125 is one - to - one and total it is not onto .",
    "but it is the partiality of the inverse function ( not all strings are shortest programs ) that causes the collapse of the structure function .",
    "if one restricts the finite sets containing @xmath134 to be subsets of @xmath143 , then the resulting function @xmath144 is within a logarithmic strip around @xmath140 .",
    "the coding function @xmath125 is upper semicomputable and deterministic .",
    "( one can consider other codes , using more powerful computability assumptions or probabilistic codes , but that is outside the scope of this paper . ) however , the structure function is invariant under `` proper '' recoding of the data .    [ theo.recoding ] let @xmath125 be a recursive permutation of the set of finite binary strings in @xmath16 ( one - to - one , total , and onto ) , and extend @xmath125 to subsets @xmath145 .",
    "then , @xmath146 is `` close '' to @xmath81 in the sense that the graph of @xmath146 is situated within a strip of width @xmath147 around the graph of @xmath81 .",
    "let @xmath25 be a witness of @xmath86 .",
    "then , @xmath148 satisfies @xmath149 and @xmath150 .",
    "hence , @xmath151 . let @xmath152 be a witness of @xmath153 .",
    "then , @xmath154 satisfies @xmath155 and @xmath156 . hence , @xmath157 ( since @xmath158 ) .",
    "given @xmath159 , the data to explain , and the model class consisting of all models @xmath17 for @xmath15 that have complexity @xmath73 at most @xmath76 .",
    "this @xmath76 is the maximum complexity of an explanation we allow . as usual , we denote @xmath160 ( possibly indexed like @xmath161 ) and @xmath162 .",
    "we search for programs @xmath28 of length at most @xmath76 that print a finite set @xmath163 .",
    "such pairs @xmath164 are possible explanations .",
    "the _ best explanation _ is defined to be the @xmath164 for which @xmath165 is minimal , that is , @xmath166 .",
    "since the function @xmath72 is not computable , there is no algorithm that halts with the best explanation . to overcome this problem",
    "we minimize the randomness deficiency by minimizing the mdl code length , justified by , and thus maximize the fitness of the model for this data sample .",
    "since holds only up to a small error we should more properly say `` almost minimize the randomness deficiency '' and `` almost maximize the fitness of the model . ''",
    "[ def.mdlalg ] an algorithm @xmath30 is an _ mdl algorithm _ if the following holds .",
    "let @xmath15 be a data sample consisting of @xmath36 separated words of length @xmath14 in @xmath167 bits",
    ". given inputs @xmath15 and @xmath76 ( @xmath168 ) , algorithm @xmath30 written as @xmath169 produces a finite sequence of pairs @xmath170 , such that every @xmath171 is a binary program of length at most @xmath76 that prints a finite set @xmath172 with @xmath173 and @xmath174 for every @xmath175 .",
    "it follows that @xmath176 for all @xmath175 .",
    "note that an mdl algorithm may consider only a proper subset of all binary programs of length at most @xmath76 .",
    "in particular , the final @xmath177 may be greater than the optimal mdl code of length @xmath178 .",
    "this happens when a program @xmath28 printing @xmath17 with @xmath179 and @xmath180 is not in the subset of binary programs considered by the algorithm , or the algorithm gets trapped in a suboptimal solution .",
    "the next theorem gives an mdl algorithm that always finds the optimal mdl code and , moreover , the model concerned is shown to be an approximately best fitting model for dat @xmath15 .",
    "[ alg.mdl ] there exists an mdl algorithm which given @xmath15 and @xmath76 satisfies @xmath181 , such that @xmath182 .",
    "we exhibit such an mdl algorithm :    * algorithm optimal mdl ( @xmath183 ) *    * step 1 .",
    "* let @xmath15 be the data sample .",
    "run all binary programs @xmath184 of length at most @xmath185 in lexicographic length - increasing order in a dovetailed style .",
    "the computation proceeds by stages @xmath186 and in each stage @xmath62 the overall computation executes step @xmath187 of the particular subcomputation of @xmath188 , for every @xmath107 such that @xmath189 .",
    "* at every computation step @xmath190 , consider all pairs @xmath164 such that program @xmath28 has printed the set @xmath25 by time @xmath190 .",
    "we assume that there is a first elementary computation step @xmath191 such that there is such a pair .",
    "best explanation _",
    "@xmath192 at computation step @xmath193 be a pair that minimizes the sum @xmath194 among all the pairs @xmath164 .",
    "* step 3 . *",
    "we only change the best explanation @xmath195 of computation step @xmath196 to @xmath197 at computation step @xmath190 , if @xmath174 .    in this mdl algorithm the best explanation @xmath192 changes from time to time due to the appearance of a strictly better explanation .",
    "since no pair @xmath164 can be elected as best explanation twice , and there are only finitely many pairs , from some moment onward the explanation @xmath192 which is declared best does not change anymore .",
    "therefore the limit @xmath198 exists .",
    "the model @xmath199 is a witness set of @xmath200 .",
    "the lemma follows by ( [ eq.eq ] ) and remark  [ rem.witness ] .",
    "thus , if we continue to approximate the two - part mdl code contemplating every relevant model , then we will eventually reach the optimal two - part code whose associated model is approximately the best explanation .",
    "that is the good news .",
    "the bad news is that we do not know when we have reached this optimal solution .",
    "the functions @xmath201 and @xmath81 , and their witness sets , can not be computed within any reasonable accuracy , section  [ sect.comp ] .",
    "hence , there does not exist a criterion we could use to terminate the approximation somewhere close to the optimum .    in the practice of the real - world mdl , in the process of finding the optimal two - part mdl code , or indeed a suboptimal two - part mdl code",
    ", we often have to be satisfied with running times @xmath190 that are much less than the time to stabilization of the best explanation . for such small @xmath190 ,",
    "the model @xmath172 has a weak guarantee of goodness , since we know that @xmath202 because @xmath203 and therefore @xmath204 ( ignoring additive constants ) .",
    "that is , the randomness deficiency of @xmath15 in @xmath172 plus @xmath89 is less than the known value @xmath205 .",
    "theorem  [ alg.mdl ] implies that algorithm mdl gives not only _ some _ guarantee of goodness during the approximation process ( see section  [ sect.comp ] ) , but also that , in the limit , that guarantee approaches the value of its lower bound , that is , @xmath206 .",
    "thus , in the limit , algorithm optimal mdl will yield an explanation that is only a little worse than the best explanation .",
    "[ rem.direct ] * ( direct method ) * use the same dovetailing process as in algorithm optimal mdl , with the following addition . at every elementary computation step @xmath190 ,",
    "select a @xmath164 for which @xmath207 is minimal among all programs @xmath28 that up to this time have printed a set @xmath25 . here",
    "@xmath208 is the approximation of @xmath209 from above defined by @xmath210 the reference universal prefix machine @xmath29 outputs @xmath15 on input @xmath211 in at most @xmath190 steps@xmath105 .",
    "hence , @xmath207 is an approximation from below to @xmath212 .",
    "let @xmath213 denote the best explanation after @xmath190 steps .",
    "we only change the best explanation at computation step @xmath190 , if @xmath214 .",
    "this time the same explanation can be chosen as the best one twice .",
    "however , from some time @xmath190 onward , the best explanation @xmath213 does not change anymore . in the approximation process",
    ", the model @xmath172 has no guarantee of goodness at all : since @xmath72 is not semicomputable up to any significant precision , section  [ sect.comp ] , we can not know a significant upper bound neither for @xmath215 , nor for @xmath216 . hence",
    ", we must prefer the indirect method of algorithm optimal mdl , approximating a witness set for @xmath86 , instead of the direct one of approximating a witness set for @xmath72 .",
    "in practice we often must terminate an mdl algorithm as in definition  [ def.mdlalg ] prematurely .",
    "a natural assumption is that the longer we approximate the optimal two - part mdl code the better the resulting model explains the data .",
    "thus , it is tempting to simply assume that in the approximation every next shorter two - part mdl code also yields a better model .",
    "however , this is not true . to give an example that shows where things go wrong it is easiest to first give the conditions under which premature search termination is all right .",
    "suppose we replace the currently best explanation @xmath217 in an mdl algorithm with explanation @xmath218 only if @xmath219 is not just less than @xmath220 , but less by more than the excess of @xmath221 over @xmath222 .",
    "then , it turns out that every time we change the explanation we improve its goodness .",
    "[ theo.approxim ] let @xmath15 be a data sample with @xmath101 ( @xmath223 ) .",
    "let @xmath224 and @xmath218 be sequential ( not necessary consecutive ) candidate best explanations . produced by an mdl algorithm @xmath225 . if @xmath226 then @xmath227    for every pair of sets @xmath228 we have @xmath229 with @xmath230 and @xmath231 the first inequality uses the trivial @xmath232 and the nontrivial @xmath233 which follows by , and the second inequality uses the general property that @xmath234 . by the assumption in the theorem , @xmath235 since by assumption the difference in mdl codes @xmath236",
    ", it suffices to show that @xmath237 to prove the theorem .",
    "note that @xmath217 and @xmath218 are in this order sequential candidate best explanations in the algorithm , and every candidate best explanation may appear only once .",
    "hence , to identify @xmath217 we only need to know the mdl algorithm @xmath30 , the maximal complexity @xmath76 of the contemplated models , the data sample @xmath15 , the candidate explanation @xmath218 , and the number @xmath62 of candidate best explanations in between @xmath217 and @xmath218 . to identify @xmath238 from @xmath112 we only require @xmath222 bits .",
    "the program @xmath239 can be found from @xmath240 and the length @xmath241 , as the first program computing @xmath240 of length @xmath242 in the process of running the algorithm @xmath225 . since @xmath30 is an mdl algorithm we have @xmath243 , and @xmath244 .",
    "therefore , @xmath245 where @xmath246 is the number of bits we need to encode the description of the mdl algorithm , the descriptions of the constituent parts self - delimitingly , and the description of a program to reconstruct @xmath238 from @xmath112 .",
    "since @xmath247 , we find @xmath248 where the last inequality follows from @xmath249 and @xmath36 being an integer .",
    "we need an mdl algorithm in order to restrict the sequence of possible candidate models examined to at most @xmath250 with @xmath251 rather than all of the @xmath252 possible models @xmath17 satisfying @xmath25 .    in the sequence @xmath253 of candidate best explanations produced by an mdl algorithm , @xmath254 is actually better than @xmath197 ( @xmath255 ) , if the improvement in the two - part mdl code - length is the given logarithmic term in excess of the unknown , and in general noncomputable @xmath256 . on the one hand ,",
    "if @xmath257 , and @xmath258 then @xmath259 is a better explanation for data sample @xmath15 than @xmath172 , in the sense that @xmath260 on the other hand , if @xmath261 is large , then @xmath259 may be a much worse explanation than @xmath172 .",
    "then , it is possible that we improve the two - part mdl code - length by giving a worse model @xmath259 using , however , a @xmath262 such that @xmath263 while @xmath264 .",
    "assume that we want to infer a language , given a single positive example ( element of the language ) .",
    "the positive example is @xmath265 with @xmath266 , @xmath267 for @xmath268 .",
    "we restrict the question to inferring a language consisting of a set of elements of the same length as the positive example , that is , we infer a subset of @xmath16 .",
    "we can view this as inferring the slice @xmath269 of the ( possibly infinite ) target language @xmath270 consisting of all words of length @xmath14 in the target language .",
    "we identify the singleton data sample @xmath15 with its constituent data string @xmath6 . for the models we always have @xmath271 with @xmath19 . for simplicity",
    "we delete the cardinality indicator @xmath272 since it is always 1 and write @xmath273 .",
    "every @xmath274 can be represented by its characteristic sequence @xmath275 with @xmath276 if the @xmath21th element of @xmath16 is in @xmath17 , and 0 otherwise .",
    "conversely , every string of @xmath277 bits is the characteristic sequence of a subset of @xmath16 .",
    "most of these subsets are `` random '' in the sense that they can not be represented concisely : their characteristic sequence is incompressible .",
    "now choose some integer @xmath47 .",
    "simple counting tells us that there are only @xmath278 binary strings of length @xmath279 .",
    "thus , the number of possible binary programs of length @xmath279 is at most @xmath278 .",
    "this in turn implies ( since every program describes at best one such set ) that the number of subsets @xmath274 with @xmath280 is at most @xmath278 .",
    "therefore , the number of subsets @xmath274 with @xmath281 is greater than @xmath282 now if @xmath73 is significantly greater than @xmath137 , then it is impossible to learn @xmath17 from @xmath6 .",
    "this follows already from the fact that @xmath283 by ( note that @xmath284 ) .",
    "that is , we need more than @xmath285 extra bits of dedicated information to deduce @xmath17 from @xmath6",
    ". almost all sets in @xmath16 have so high complexity that no effective procedure can infer this set from a single example .",
    "this holds in particular for every ( even moderately ) random set .",
    "thus , to infer such a subset @xmath274 , given a sample datum @xmath286 , using the mdl principle is clearly out of the question .",
    "the datum @xmath6 can be literally described in @xmath14 bits by the trivial mdl code @xmath287 with @xmath6 literal at self - delimiting model cost at most @xmath288 bits and data - to - model cost @xmath289 .",
    "it can be concluded that the only sets @xmath17 that can possibly be inferred from @xmath6 ( using mdl or any other effective deterministic procedure ) are those that have @xmath290 .",
    "such sets are extremely rare : only an at most @xmath291 fraction of all subsets of @xmath16 has that small prefix complexity .",
    "this negligible fraction of possibly learnable sets shows that such sets are very nonrandom ; they are simple in the sense that their characteristic sequences have great regularity ( otherwise the kolmogorov complexity could not be this small ) .",
    "but this is all right : we do not want to learn random , meaningless , languages , but only languages that have meaning .",
    "`` meaning '' is necessarily expressed in terms of regularity .",
    "even if we can learn the target model by an mdl algorithm in the limit , by selecting a sequence of models that decrease the mdl code with each next model , it can still be the case that a later model in this sequence is a worse model than a preceding one .",
    "theorem  [ theo.approxim ] showed conditions that prevent this from happening .",
    "we now show that if those conditions are not satisfied , it can indeed happen .",
    "[ theo.fluctuate ] there is a datum @xmath6 ( @xmath292 ) with explanations @xmath192 and @xmath254 such that @xmath293 but @xmath294 .",
    "that is , @xmath259 is much worse fitting than @xmath172 .",
    "there is an mdl algorithm @xmath295 generating @xmath192 and @xmath254 as best explanations with @xmath296 .",
    "note that the condition of theorem  [ theo.approxim ] is different from the first inequality in theorem  [ theo.fluctuate ] since the former required an extra @xmath297 term in the right - hand side .",
    "fix datum @xmath6 of length @xmath14 which can be divided in @xmath298 with @xmath299 of equal length ( say @xmath14 is a multiple of 3 ) with @xmath300 , @xmath301 , @xmath302 , and @xmath303 ( with the last four equalities holding up to additive @xmath304 terms ) .",
    "additionally , take @xmath14 sufficiently large so that @xmath305 .",
    "define @xmath306 and an mdl algorithm @xmath307 that examines the sequence of models @xmath308 , with @xmath309 .",
    "the algorithm starts with candidate model @xmath310 and switches from the current candidate to candidate @xmath311 , @xmath312 , if that model gives a shorter mdl code than the current candidate .",
    "now @xmath313 and @xmath314 , so the mdl code length @xmath315 .",
    "our mdl algorithm uses a compressor that does not compress @xmath316 all the way to length @xmath317 , but codes @xmath316 self - delimitingly at @xmath318 bits , that is , it compresses @xmath316 by 10% .",
    "thus , the mdl code length is @xmath319 for every contemplated model @xmath311 ( @xmath309 ) .",
    "the next equalities hold again up to @xmath304 additive terms .    *",
    "the mdl code length of the initial candidate model @xmath310 is @xmath14 .",
    "the randomness deficiency @xmath320 .",
    "the last equality holds since clearly @xmath321 .",
    "* for the contemplated model @xmath322 we obtain the following .",
    "the mdl code length for model @xmath322 is @xmath323 .",
    "the randomness deficiency @xmath324 . * for the contemplated model @xmath325 we obtain the following .",
    "the mdl code length is @xmath326 .",
    "the randomness deficiency is @xmath327 .",
    "thus , our mdl algorithm initializes with candidate model @xmath310 , then switches to candidate @xmath322 since this model decreases the mdl code length by @xmath328 .",
    "indeed , @xmath322 is a much better model than @xmath310 , since it decreases the randomness deficiency by a whopping @xmath329 .",
    "subsequently , however , the mdl process switches to candidate model @xmath325 since it decreases the mdl code length greatly again , by @xmath328 . but",
    "@xmath325 is a much worse model than the previous candidate @xmath322 , since it increases the randomness deficiency again greatly by @xmath330 .    by theorem  [ theo.approxim ] we know that if in the process of mdl estimation by a sequence of significantly decreasing mdl codes a candidate model is represented by its shortest program , then the following candidate model which improves the mdl code is actually a model of at least as good fit as the preceding one . thus ,",
    "if in the example used in the proof above we encode the models at shortest code length , we obtain mdl code lengths @xmath14 for @xmath310 , @xmath331 for @xmath322 , and @xmath332 for @xmath325 .",
    "hence the mdl estimator using shortest model code length changes candidate model @xmath310 for @xmath322 , improving the mdl code length by @xmath329 and the randomness deficiency by @xmath329 . however , and correctly , it does not change candidate model @xmath322 for @xmath325 , since that would increase the mdl code length by @xmath330 .",
    "it so prevents , correctly , to increase the randomness deficiency by @xmath330 .",
    "thus , by the cited theorem , the oscillating randomness deficiency in the mdl estimation process in the proof above can only arise in cases where the consecutive candidate models are not coded at minimum cost while the corresponding two - part mdl code lengths are decreasing .",
    "assume that we want to infer a language , given a set of positive examples ( elements of the language ) @xmath15 .",
    "for convenience we restrict the question to inferring a language @xmath333 with @xmath19 .",
    "we can view this as inferring the slice @xmath269 ( corresponding to @xmath42 ) of the target language @xmath270 consisting of all words of length @xmath14 in the target language .",
    "since @xmath15 consists of a subset of positive examples of @xmath42 we have @xmath26 . to infer a language @xmath17 from a set of positive examples @xmath26 is , of course , a much more natural situation than to infer a language from a singleton @xmath6 as in the previous section .",
    "note that the complexity @xmath137 of a singleton @xmath6 of length @xmath14 can not exceed @xmath334 , while the complexity of a language of which @xmath6 is an element can rise to @xmath335 . in the multiple data sample setting @xmath89 can rise to @xmath335 , just as @xmath73 can .",
    "that is , the description of @xmath14 takes @xmath304 bits and the description of the characteristic sequence of a subset of @xmath16 may take @xmath277 bits , everything self - delimitingly .",
    "so contrary to the singleton datum case , in principle models @xmath17 of every possible model complexity can be inferred depending on the data @xmath15 at hand .",
    "an obvious example is @xmath336 .",
    "note that the cardinality of @xmath15 plays a role here , since the complexity @xmath337 with equality for certain @xmath15 .",
    "a traditional and well - studied problem in this setting is to infer a grammar from a language example .",
    "the field of grammar induction studies among other things a class of algorithms that aims at constructing a grammar by means of incremental compression of the data set represented by the digraph of a deterministic finite automaton ( dfa ) accepting the data set .",
    "this digraph can be seen as a model for the data set .",
    "every word in the data set is represented as a path in the digraph with the symbols either on the edges or on the nodes .",
    "the learning process takes the form of a guided incremental compression of the data set by means of merging or clustering of the nodes in the graph .",
    "none of these algorithms explicitly makes an estimate of the data - to - model code .",
    "instead they use heuristics to guide the model reduction . after a certain number of computational steps a proposal for a grammar",
    "can be constructed from the current state of the compressed graph .",
    "examples of such algorithms are sp @xcite , emile @xcite , adios @xcite , and a number of dfa induction algorithms , such as `` evidence driven state merging '' ( edsm ) , @xcite .",
    "related compression - based theories and applications appear in @xcite .",
    "our results ( above and below ) do not imply that compression algorithms improving the mdl code of dfas can never work on real life data sets .",
    "there is considerable empirical evidence that there are situations in which they do work . in those cases specific properties of a restricted class of languages or data sets must be involved .",
    "our results are applicable to the common digraph simplification techniques used in grammar inference .",
    "the results hold equally for algorithms that use just positive examples , just negative examples , or both , using any technique ( not just digraph simplification ) .",
    "a dfa @xmath338 , where @xmath8 is a finite set of _ input symbols _",
    ", @xmath339 is a finite set of _ states _ , @xmath340 is the _ transition function _ , @xmath341 is the _ initial state _ , and @xmath342 is a set of _ final states_.    the dfa @xmath30 is started in the initial state @xmath343 . if it is in state @xmath344 and receives input symbol @xmath345 it changes its state to @xmath346 .",
    "if the machine after zero or more input symbols , say @xmath347 , is driven to a state @xmath348 then it is said to _",
    "accept _ the word @xmath349 , otherwise it _ rejects _ the word @xmath350 .",
    "the _ language accepted _ by @xmath30 is @xmath351 is accepted by @xmath352 .",
    "we denote @xmath353 .",
    "we can effectively enumerate the dfas as @xmath354 in lexicographic length - increasing order .",
    "this enumeration we call the _ standard enumeration_.    the first thing we need to do is to show that all laws that hold for finite - set models also hold for dfa models , so all theorems , lemmas , and remarks above , both positive and negative , apply . to do so , we show that for every data sample @xmath145 and a contemplated finite set model for it , there is an almost equivalent dfa .",
    "[ prop.1 ] let @xmath162 , @xmath38 and @xmath75 .",
    "for every @xmath355 there is a dfa @xmath30 with @xmath356 such that @xmath357 ( which implies @xmath358 ) , and @xmath359 .",
    "since @xmath42 is a finite set of binary strings , there is a dfa that accepts it , by elementary formal language theory .",
    "define dfa @xmath30 such that @xmath30 is the first dfa in the standard enumeration for which @xmath356 .",
    "( note that we can infer @xmath14 from both @xmath17 and @xmath42 . ) hence , @xmath360 and @xmath361 .",
    "trivially , @xmath362 and @xmath363 , since @xmath30 may have information about @xmath15 beyond @xmath42 .",
    "this implies @xmath364 , so that @xmath365 .",
    "lemma  [ prop.2 ] is the converse of lemma  [ prop.1 ] : for every data sample @xmath15 and a contemplated dfa model for it , there is a finite set model for @xmath15 that has no worse complexity , randomness deficiency , and worst - case data - to - model code for @xmath15 , up to additive logarithmic precision .",
    "[ prop.2 ] use the terminology of lemma  [ prop.1 ] . for every @xmath366",
    ", there is a model @xmath25 such that @xmath367 , @xmath368 ( which implies @xmath369 ) , and @xmath370",
    ".    choose @xmath371 .",
    "then , @xmath372 and both @xmath368 and @xmath369 . since also @xmath364 , since @xmath30 may have information about @xmath15 beyond @xmath17 , we have @xmath373 .      to analyze the mdl estimation for dfas , given a data sample , we first fix details of the code .",
    "for the model code , the coding of the dfa , we encode as follows .",
    "let @xmath374 with @xmath375 , @xmath376 , and @xmath377 . by renaming of the states",
    "we can always take care that @xmath342 are the last @xmath125 states of @xmath339 .",
    "there are @xmath378 different possibilities for @xmath190 , @xmath379 possibilities for @xmath343 , and @xmath379 possibilities for @xmath125 .",
    "altogether , for every choice of @xmath380 there are @xmath381 distinct dfas , some of which may accept the same languages .    *",
    "small model cost but difficult to decode : * we can enumerate the dfas by setting @xmath382 and for every @xmath21 consider all partitions @xmath383 to two positive integer summands , and for every particular choice of @xmath380 considering every choice of final states , transition function , and initial state . this way we obtain a standard enumeration @xmath384 of all dfas , and ,",
    "given the index @xmath62 of a dfa @xmath385 we can retrieve the particular dfa concerned , and for every @xmath14 we can find @xmath386 .",
    "* larger model cost but easy to decode : * we encode a dfa @xmath30 with @xmath379 states and @xmath387 symbols self - delimitingly by    * the encoding of the number of symbols @xmath387 in self - delimiting format in @xmath388 bits ; * the encoding of the number of states @xmath379 in self - delimiting format in @xmath389 bits ; * the encoding of the set of final states @xmath390 by indicating that all states numbered @xmath391 are final states , by just giving @xmath392 in @xmath393 bits ; * the encoding of the initial state @xmath343 by giving its index in the states @xmath394 , in @xmath393 bits ; and * the encoding of the transition function @xmath190 in lexicographic order of @xmath395 in @xmath393 bits per transition , which takes @xmath396 bits altogether .",
    "altogether , this encodes @xmath30 in a self - delimiting format in @xmath397 bits .",
    "thus , we reckon the model cost of a @xmath398-dfa as @xmath399 bits . this cost has the advantage that it is easy to decode and that @xmath400 is an easy function of @xmath380 .",
    "we will assume this model cost .",
    "* data - to - model cost : * given a dfa model @xmath30 , the word length @xmath14 in @xmath401 bits which we simplify to @xmath402 bits , and the size @xmath36 of the data sample @xmath145 , we can describe @xmath15 by its index @xmath62 in the set of @xmath36 choices out of @xmath403 items , that is , up to rounding upwards , @xmath404 bits .",
    "for @xmath405 this can be estimated by @xmath406 , where @xmath407 ( @xmath408 ) is shannon s entropy function . for @xmath409 or @xmath410 we set the data - to - model cost to @xmath411 , for @xmath412 we set it to @xmath413 ( ignoring the possible saving of a @xmath414 term ) , and for @xmath415 we set it to the cost of @xmath416 .",
    "this reasoning brings us to the following mdl cost of a data sample @xmath15 for dfa model @xmath30 :    the _ mdl code length _ of a data sample @xmath15 of @xmath36 strings of length @xmath14 , given @xmath36 , for a dfa model @xmath30 such that @xmath417 denoting @xmath418 , is given by @xmath419 if @xmath36 is not given we write @xmath420 .",
    "given data sample @xmath15 and dfa @xmath30 with @xmath421 , we can estimate the randomness deficiency .",
    "again , use @xmath422 and @xmath162 . by",
    ", the randomness deficiency is @xmath423 then , substituting the estimate for @xmath404 from the previous section , up to logarithmic additive terms , @xmath424 thus , by finding a computable upper bound for @xmath425 , we can obtain a computable lower bound on the randomness deficiency @xmath426 that expresses the fitness of a dfa model @xmath30 with respect to data sample @xmath15 .",
    "the task of finding the smallest dfa consistent with a set of positive examples is trivial .",
    "this is the universal dfa accepting every example ( all of @xmath16 ) .",
    "clearly , such a universal dfa will in many cases have a poor generalization error and randomness deficiency .",
    "as we have seen , optimal randomness deficiency implies an optimal fitting model to the data sample .",
    "it is to be expected that the best fitting model gives the best generalization error in the case that the future data are as typical to this model as the data sample is .",
    "we show that the randomness deficiency behaves independently of the mdl code , in the sense that the randomness deficiency can either grow or shrink with a reduction of the length of the mdl code .",
    "we show this by example .",
    "let the set @xmath15 be a sample set consisting of 50% of all binary strings of length @xmath14 with an even number of 1 s .",
    "note , that the number of strings with an even number of 1 s equals the number of strings with an odd number of 1 s , so @xmath427 .",
    "initialize with a dfa @xmath30 such that @xmath428 .",
    "we can obtain @xmath15 directly from @xmath429 , so we have @xmath430 , and since @xmath410 ( @xmath418 ) we have @xmath431 , so that altogether @xmath432 , while @xmath433 , since @xmath434 . (",
    "the first equality follows since we can obtain @xmath36 from @xmath14 .",
    "we obtain a negative constant randomness deficiency which we take to be as good as 0 randomness deficiency .",
    "all arguments hold up to an @xmath83 additive term anyway . ) without loss of generality we can assume that the mdl algorithm involved works by splitting or merging nodes of the digraphs of the produced sequence of candidate dfas .",
    "but the argument works for every mdl algorithm , whatever technique it uses .",
    "_ initialize : _ assume that we start our mdl estimation with the trivial dfa @xmath435 that literally encodes all @xmath36 elements of @xmath15 as a binary directed tree with @xmath379 nodes .",
    "then , @xmath436 , which yields @xmath437 the last approximate equality holds since @xmath410 , and hence @xmath431 and @xmath438 .",
    "since the randomness deficiency @xmath439 , it follows that @xmath435 is a best fitting model for @xmath15 .",
    "indeed , it represents all conceivable properties of @xmath15 since it literally encodes @xmath15 .",
    "however , @xmath435 does not achieve the optimal mdl code .",
    "_ better mdl estimation : _ in a later mdl estimation we improve the mdl code by inferring the parity dfa @xmath440 with two states ( @xmath441 ) that checks the parity of 1 s in a sequence .",
    "then , @xmath442 we now consider two different instantiations of @xmath15 , denoted as @xmath443 and @xmath444 .",
    "the first one is regular data , and the second one is random data .    * case 1 , regular data : * suppose @xmath445 consisting of the lexicographic first 50% of all @xmath14-bit strings with an even number of occurrences of 1 s .",
    "then @xmath446 and @xmath447 in this case , even though dfa @xmath440 has a much better mdl code than dfa @xmath435 it has nonetheless a much worse fit since its randomness deficiency is far greater .    *",
    "case 2 , random data : * suppose @xmath15 is equal to @xmath444 , where @xmath444 is a random subset consisting of 50% of the @xmath14-bit strings with even number of occurrences of 1 s .",
    "then , @xmath448 , and @xmath449 in this case , dfa @xmath440 has a much better mdl code than dfa @xmath435 , and it has equally good fit since both randomness deficiencies are about 0 .    we conclude that improved mdl estimation of dfas for multiple data samples does nt necessarily result in better models , but can do so nonetheless .",
    "[ rem.smc ] by theorem  [ theo.approxim ] we know that if , in the process of mdl estimation by a sequence of significantly decreasing mdl codes , a candidate dfa is represented by its shortest program , then the following candidate dfa which improves the mdl estimation is actually a model of at least as good fit as the preceding one .",
    "let us look at an example : suppose we start with dfa @xmath450 that accepts all strings in @xmath2 .",
    "in this case we have @xmath451 and @xmath452 here @xmath453 , since @xmath454 .",
    "suppose the subsequent candidate dfa is the parity machine @xmath440 .",
    "then , @xmath455 since @xmath446 .",
    "since @xmath456 , we have @xmath457 , and @xmath458 .",
    "therefore , the improved mdl cost from model @xmath450 to model @xmath440 is accompanied by an improved model fitness since the randomness deficiency decreases as well .",
    "this is forced by theorem  [ theo.approxim ] , since both dfa @xmath440 and dfa @xmath450 have @xmath459 .",
    "that is , the dfas are represented and penalized according to their shortest programs ( a fortiori of length @xmath83 ) and therefore improved mdl estimation increases the fitness of the successive dfa models significantly .",
    "a binary string @xmath460 is a _ proper prefix _ of a binary string @xmath6 if we can write @xmath461 for @xmath462 .",
    "a set @xmath463 is _ prefix - free _ if for every pair of distinct elements in the set neither is a proper prefix of the other .",
    "a prefix - free set is also called a _ prefix code _ and its elements are called _",
    "code words_. as an example of a prefix code , encode the source word @xmath464 by the code word @xmath465 this prefix - free code is called _ self - delimiting _ , because there is fixed computer program associated with this code that can determine where the code word @xmath466 ends by reading it from left to right without backing up . this way a composite code message can be parsed in its constituent code words in one",
    "pass , by the computer program .",
    "since we use the natural numbers and the binary strings interchangeably , the notation @xmath467 where @xmath6 is ostensibly an integer means the length in bits of the self - delimiting code of the @xmath6th binary string . on the other hand ,",
    "the notation @xmath468 where @xmath6 is ostensibly a binary string means the self - delimiting code of the length @xmath5 of the binary string @xmath6 .",
    "using this code we define the standard self - delimiting code for @xmath6 to be @xmath469 .",
    "it is easy to check that @xmath470 and @xmath471 .",
    "let @xmath472 denote a standard invertible effective one - to - one code from @xmath473 to a subset of @xmath1 .",
    "for example , we can set @xmath474 or @xmath475 .",
    "we can iterate this process to define @xmath476 , and so on .      for precise definitions , notation , and results",
    "see the textbook @xcite . informally ,",
    "the kolmogorov complexity , or algorithmic entropy , @xmath137 of a string @xmath6 is the length ( number of bits ) of a shortest binary program ( string ) to compute @xmath6 on a fixed reference universal computer ( such as a particular universal turing machine ) .",
    "intuitively , @xmath137 represents the minimal amount of information required to generate @xmath6 by any effective process .",
    "the conditional kolmogorov complexity @xmath477 of @xmath6 relative to @xmath460 is defined similarly as the length of a shortest program to compute @xmath6 , if @xmath460 is furnished as an auxiliary input to the computation . for technical reasons we use a variant of complexity , so - called prefix complexity , which is associated with turing machines for which the set of programs resulting in a halting computation is prefix free .",
    "we realize prefix complexity by considering a special type of turing machine with a one - way input tape , a separate work tape , and a one - way output tape .",
    "such turing machines are called _ prefix _ turing machines . if a machine @xmath121 halts with output @xmath6 after having scanned all of @xmath28 on the input tape , but not further , then @xmath478 and we call @xmath28 a _ program _ for @xmath121 .",
    "it is easy to see that @xmath479 is a _",
    "prefix code_.    let @xmath480 be a standard enumeration of all prefix turing machines with a binary input tape , for example the lexicographic length - increasing ordered syntactic prefix turing machine descriptions , and let @xmath481 be the enumeration of corresponding functions that are computed by the respective turing machines ( @xmath482 computes @xmath483 ) .",
    "these functions are the _ partial recursive _ functions or _",
    "computable _ functions ( of effectively prefix - free encoded arguments ) .",
    "the prefix ( kolmogorov ) complexity of @xmath6 is the length of the shortest binary program from which @xmath6 is computed . for the development of the theory",
    "we require the turing machines to use _ auxiliary _ ( also called _ conditional _ ) information , by equipping the machine with a special read - only auxiliary tape containing this information at the outset .",
    "one of the main achievements of the theory of computation is that the enumeration @xmath484 contains a machine , say @xmath485 , that is computationally universal in that it can simulate the computation of every machine in the enumeration when provided with its index : @xmath486 for all @xmath487 .",
    "we fix one such machine and designate it as the _ reference universal prefix turing machine_.    [ def.kolmk ] using this universal machine we define the _ prefix ( kolmogorov ) complexity _ @xmath488 the _ conditional version _ of the prefix kolmogorov complexity of @xmath6 given @xmath460 ( as auxiliary information ) . the unconditional version is set to @xmath489 .",
    "in this paper we use the prefix complexity variant of kolmogorov complexity only for convenience ; the plain kolmogorov complexity without the prefix property would do just as well . the functions @xmath490 and @xmath491 , though defined in terms of a particular machine model , are machine - independent up to an additive constant and acquire an asymptotically universal and absolute character through church s thesis , that is , from the ability of universal machines to simulate one another and execute any effective process .",
    "the kolmogorov complexity of an individual object was introduced by kolmogorov @xcite as an absolute and objective quantification of the amount of information in it .",
    "the information theory of shannon @xcite , on the other hand , deals with _ average _ information _ to communicate _ objects produced by a _ random source_. since the former theory is much more precise , it is surprising that analogues of theorems in information theory hold for kolmogorov complexity , be it in somewhat weaker form .",
    "an example is the remarkable _ symmetry of information _ property .",
    "let @xmath134 denote the shortest prefix - free program for a finite string @xmath6 , or , if there are more than one of these , then @xmath134 is the first one halting in a fixed standard enumeration of all halting programs .",
    "it follows that @xmath141 .",
    "denote @xmath492 .",
    "then , @xmath493      it is customary in this area to use `` additive constant @xmath55 '' or equivalently `` additive @xmath83 term '' to mean a constant , accounting for the length of a fixed binary program , independent from every variable or parameter in the expression in which it occurs .",
    "we summarize a selection of the results in @xcite .",
    "there , the data sample @xmath15 is a singleton set @xmath494 .",
    "the results extend to the multiple data sample case in the straightforward way .",
    "\\(i ) the mdl code length @xmath79 with @xmath145 and @xmath162 can assume essentially every possible relevant shape @xmath495 as a function of the maximal model complexity @xmath76 that is allowed up to an additive @xmath496 term in argument and value .",
    "( actually , we can take this term as @xmath497 , but since this is cumbersome we use the larger @xmath496 term .",
    "the difference becomes large for @xmath498 . )",
    "these @xmath499 s are all integer - valued nonincreasing functions such that @xmath499 is defined on @xmath500 $ ] where @xmath501 , such that @xmath502 and @xmath503 .",
    "this is theorem iv.4 in @xcite for singleton data @xmath6 .",
    "there , @xmath140 is contained in a strip of width @xmath304 around @xmath499 . for multiple data @xmath15 ( @xmath101 ) a similar theorem holds up to an @xmath496 additive term in both argument and value , that is , the strip around @xmath499 in which @xmath81 is situated now has width @xmath504 .",
    "( the strip idea is made precise in below for , another result . ) as a consequence , so - called `` nonstochastic '' data @xmath15 for which @xmath86 stabilizes on @xmath89 only for large @xmath76 are common .",
    "\\(ii ) a model achieving the mdl code length @xmath79 , essentially achieves the best possible fit @xmath118 .",
    "this is theorem iv.8 in @xcite for singleton data and in this paper for multiple data .",
    "the precise form is : @xmath505 with @xmath506 and @xmath507 .",
    "\\(iii ) as a consequence of ( i ) and ( ii ) , the best - fit function @xmath80 can assume essentially every possible relevant shape as a function of the contemplated maximally allowed model complexity @xmath76 .    from the proof of item ( ii ) , we see that , given the data sample @xmath15 , for every finite set @xmath508 , of complexity at most @xmath509 and minimizing @xmath510 , we have @xmath511 .",
    "ignoring @xmath496 terms , at every complexity level @xmath76 , every best model at this level witnessing @xmath86 is also a best one with respect to typicality .",
    "this explains why it is worthwhile to find shortest two - part descriptions @xmath86 for the given data sample @xmath15 : this is the single known way to find an @xmath508 with respect to which @xmath15 is as typical as possible at model complexity level @xmath76 .",
    "note that the set @xmath512 is not enumerable so we are not able to generate such @xmath17 s directly , @xcite .",
    "the converse is not true : not every model ( a finite set ) witnessing @xmath72 also witnesses @xmath86 . for example , let @xmath265 with @xmath6 a string of length @xmath14 with @xmath513 .",
    "let @xmath514 ( we ignore the @xmath272 set giving the data sample cardinality since @xmath15 is a singleton set ) , where @xmath460 is a string of length @xmath515 such that @xmath516 and let @xmath517 .",
    "then both @xmath518 witness @xmath519 but @xmath520 while @xmath521 .",
    "9    p. adriaans and m. vervoort , the emile 4.1 grammar induction toolbox , pp .",
    "293295 in : _ proc .",
    "colloq . grammatical inference _ ,",
    "notes comput .",
    "2484 , springer , 2002 .",
    "barron , j. rissanen , and b. yu , the minimum description length principle in coding and modeling , _ ieee trans .",
    "inform . theory _ , it-44:6(1998 ) , 27432760 .",
    "n. chater and p.m.b .",
    "vitanyi ,  ideal learning of natural language : positive results about learning from positive evidence , _ journal of mathematical psychology _ , 51:3(2007 ) , 135 - 163 .    p.d .",
    "grnwald , _ the minimum description length principle _ , mit press , 2007 .",
    "kolmogorov , three approaches to the quantitative definition of information , _ problems inform .",
    "transmission _ 1:1 ( 1965 ) 17 .",
    "complexity of algorithms and objective definition of randomness .",
    "a talk at moscow math .",
    "meeting 4/16/1974 .",
    "an abstract available in _ uspekhi mat .",
    "nauk _ 29:4(1974),155 ; english translation in @xcite .",
    "lang , b.a .",
    "pearlmutter , and r.a .",
    "price , results of the abbadingo one dfa learning competition and a new evidence - driven state merging algorithm , pp .",
    "112 in : _ proc .",
    "colloq . grammatical inference _ ,",
    "notes comput .",
    "1433 , springer , 1998 .",
    "m. li , x. chen , x. li , b. ma , and p. m. b. vitnyi , the similarity metric , _ ieee trans .",
    "inform . theory _ , 50:12(2004 ) , 32503264 .",
    "m. li and p.m.b .",
    "vitnyi , _ an introduction to kolmogorov complexity and its applications _ , third edition , springer - verlag , new york , ( 2008 )    j.j .",
    "rissanen , a universal prior for integers and estimation by minimum description length , _ annals of statistics _ , 11:2(1983 ) , 416431 .",
    "rissanen , _ information and complexity in statistical modeling _ , springer - verlag , new york , 2007 .",
    "shannon . the mathematical theory of communication .",
    ", 27:379423 , 623656 , 1948 .",
    "shen , the concept of @xmath522-stochasticity in the kolmogorov sense , and its properties , _",
    "soviet math .",
    "_ , 28:1(1983 ) , 295299 .",
    "z. solan , d. horn , e. ruppin , and s. edelman , unsupervised learning of natural languages , _ proc .",
    "academy sci .",
    "_ , 102:33(2005 ) , 1162911634 .",
    "vereshchagin and p.m.b .",
    "vitnyi , kolmogorov s structure functions and model selection , _ ieee trans .",
    "inform . theory _",
    ", 50:12(2004 ) , 32653290 .",
    "wolff , computing as compression : an overview of the sp theory and system , _ new generation comput .",
    "_ , 13:2(1995 ) , 187214 .",
    "wolff , information compression by multiple alignment , unification and search as a unifying principle in computing and cognition , _ artificial intelligence review _",
    ", 19:3(2003 ) , 193230 .    , ruder boskovic institute , zagreb , croatia , 2003 ; _ 7th eur",
    "principles pract .",
    "discov . databases _ , lect .",
    "notes comput .",
    "2838 , springer , 2003 .",
    "pieter adriaans received his ph.d . from the university of amsterdam ( 1992 ) .",
    "he and his business partner , dolf zantinge , founded the software developer syllogic b.v . in 1989 , and sold the company to perot systems corporation in 1997 .",
    "adriaans is professor of computer science at the university of amsterdam since 1997 .",
    "he serves as editor of the handbook of philosophy of information , a project of elseviers science publishers , and is a member of the icgi ( international conference on grammar induction ) steering committee .",
    "he is adviser of robosail systems , a company that manufactures and sells self - learning autopilots , as well as senior research adviser for perot systems corporation .",
    "he has worked on learning , grammar induction , philosophy of information , and information and art .",
    "he holds several patents on adaptive systems management and on a method for automatic composition of music using grammar induction techniques .",
    "adriaans acted as project leader for various large international research and development projects : amongst others , the development of distributed database management software in cooperation with ibm and prognostic and health management for the joint strike fighter .",
    "he wrote papers and books on topics related to both computer science and philosophy , including a book on systems analysis and books on client / server and distributed databases as well as data mining .",
    "he composes and plays rock music and is an avid painter . in 2006",
    "he had an overview exhibition showing the harvest of forty years of painting .",
    "vitnyi received his ph.d . from the free university of amsterdam ( 1978 ) .",
    "he is a fellow at the national research institute for mathematics and computer science ( cwi ) in the netherlands , and professor of computer science at the university of amsterdam .",
    "he serves on the editorial boards of distributed computing ( until 2003 ) , information processing letters , theory of computing systems , parallel processing letters , international journal of foundations of computer science , journal of computer and systems sciences ( guest editor ) , and elsewhere .",
    "he has worked on cellular automata , computational complexity , distributed and parallel computing , machine learning and prediction , physics of computation , kolmogorov complexity , information theory , and quantum computing , publishing about 200 research papers and some books .",
    "he received a knighthood in 2007 . together with ming li",
    "they pioneered applications of kolmogorov complexity and co - authored `` an introduction to kolmogorov complexity and its applications , '' springer - verlag , new york , 1993 ( second edition 1997 , third edition 2008 ) , parts of which have been translated into chinese , russian , and japanese ."
  ],
  "abstract_text": [
    "<S> approximation of the optimal two - part mdl code for given data , through successive monotonically length - decreasing two - part mdl codes , has the following properties : ( i ) computation of each step may take arbitrarily long ; ( ii ) we may not know when we reach the optimum , or whether we will reach the optimum at all ; ( iii ) the sequence of models generated may not monotonically improve the goodness of fit ; but ( iv ) the model associated with the optimum has ( almost ) the best goodness of fit . to express the practically interesting goodness of fit of individual models for individual data sets we have to rely on kolmogorov complexity .    </S>",
    "<S> _ index terms_ minimum description length , model selection , mdl code , approximation , model fitness , kolmogorov complexity , structure functions , examples </S>"
  ]
}