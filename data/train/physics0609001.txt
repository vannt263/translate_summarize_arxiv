{
  "article_text": [
    "markov chain monte carlo ( mcmc ) methods are important tools in parametric modeling  @xcite where the goal is to determine a posterior distribution of parameters given a particular dataset .",
    "since these algorithms tend to be computationally intensive , the challenge is to produce algorithms that have better convergence rates and are therefore more efficient  @xcite .",
    "of particular concern are situations where there is a large range of scales associated with the target density , which we find are widespread in models from many different fields  @xcite .    in this manuscript",
    "we quantify the convergence of the mcmc method by the second largest eigenvalue in absolute value for the associated operator in @xmath0 .",
    "this is not the only numerical quantity that can be used to describe the convergence properties .",
    "other authors quantify convergence with different metrics : computing the constant of geometric convergence with respect to the total variation norm  @xcite , monitoring sample averages  @xcite , evaluating mixing of parallel chains  @xcite or looking at the integrated autocorrelation time of functions of the sample  @xcite .",
    "the connection between the second eigenvalue and total variation norm is discussed in  @xcite . to connect the second eigenvalue estimates to metrics based on autocorrelation",
    ", we would argue informally that the second eigenvalue determines the autocorrelation time of the slowest mixing function of the sample and as such represents a `` worst '' case for the length of time you would need to run the chain to reduce the variance of sample averages to a predefined level .",
    "there are a number of techniques to either determine exactly or bound the second eigenvalue or the constant of geometric convergence for mcmc algorithms on discrete state spaces  @xcite , but the methods for finding quantitative bounds for continuous state spaces require a more technical formulation . where work has been done in that area , upper bounds on the convergence rate",
    "can be derived using purely analytical  @xcite or semi - analytical techniques  @xcite , but may not always be very useful for selecting parameters optimally .",
    "therefore , in this work , we show that a conceptually straightforward variational method can provide convergence rate estimates for continuous state space applications .",
    "in contrast to earlier closely related work  @xcite , we move away from mathematical formalities , focussing from the start on specific examples and step through the calculations that provide the second eigenvalue bounds . in  @xcite ,",
    "rules of thumb are provided for determining the optimal acceptance rate and step lengths for both the random walk metropolis - hastings and the metropolis adjusted langevin algorithm , in the asymptotic limit of infinite dimensions where it can be proved that those methods are approximated by diffusion processes .",
    "the rules are widely used as they are independent of the specific form of the target density , appear from numerical simulations to be appropriate far from the infinite dimensional asymptotic limit and are easily implemented .",
    "in contrast , the approach proposed here is to establish convergence properties for particular mcmc algorithms based on their performance on simple target distributions without the need to set up a diffusion approximation in an infinite dimensional limit .",
    "poor performance or lack of convergence on these simple distributions then indicates that further application with more complex target densities will also suffer from convergence problems .",
    "conversely , an identification of a range of parameters which provide good convergence properties for simple target distributions may be used as a starting point for further applications .",
    "even though we provide only lower bounds on the second eigenvalue we show these bounds can be remarkably tight due to careful choice of test functions , and computing the approximate convergence rate as a function of algorithm parameters allows us to optimally tune those parameters .",
    "we have been able to obtain explicit formulas for one dimensional example problems but the method may be more generally applicable , when applied in an approximate way , as we demonstrate for a multidimensional problem .",
    "typically , one wishes to obtain a sample @xmath1 from a probability distribution @xmath2 which is sometimes called the _ target _ distribution .",
    "an mcmc algorithm works by creating a markov chain that has @xmath2 as its unique stationary distribution , i.e. after many steps of the chain any initial distribution converges to @xmath2 . a sufficient condition to establish @xmath2 as the stationary distribution is that the chain be ergodic and that the _ transition density _ , @xmath3 , of the chain",
    "satisfy detailed balance : @xmath4 given a _ proposal density _",
    "@xmath5 for generating moves , one way to construct the required transition density  @xcite is to define @xmath6 where @xmath7 is the _ acceptance probability _ of the step @xmath8 . obtaining the sample from the stationary distribution",
    "then involves letting the chain run past the transient ( _ burn - in _ ) time and recording iterates from the late time trajectory at time intervals exceeding the correlation time .",
    "how long it takes to reach the stationary distribution determines the efficiency of the algorithm and for a given target distribution , clearly it depends on the choice of the proposal density .",
    "we can write down the one - step evolution of a probability density @xmath9 as a linear operator : @xmath10 where @xmath11 , @xmath12 , @xmath13 is the dimension of the state space and all integrals are from @xmath14 to @xmath15 here and elsewhere in this manuscript .",
    "the second form makes it explicit that @xmath16 is the stationary distribution by the detailed balance relation .",
    "now , if the linear operator has a discrete set of eigenfunctions and eigenvalues , it holds that the asymptotic convergence rate is determined by the second largest eigenvalue in absolute value ( the largest being one )  @xcite .",
    "we will write this eigenvalue as @xmath17 , and will refer to it as the _ second eigenvalue _ meaning the second largest in _ absolute _ value . geometric convergence of the chain is ensured when @xmath18  @xcite , and then the discrepancy between the density at the @xmath19 iterate of the chain and the target density decreases as @xmath20 for large @xmath21 .",
    "many previous authors have taken this second eigenvalue approach , in both the finite and continuous state space settings  @xcite , as it provides a useful quantifier for the convergence rate .",
    "ideally we would like algorithm parameters to be adjusted such that @xmath17 is as as small as possible .",
    "the variational calculation allows us to obtain an estimate for @xmath17 , but before we can do this we need to convert our operator into a self - adjoint form which ensures that the eigenfunctions are orthogonal .",
    "this is easily accomplished by a standard technique  @xcite of modifying the transition density by @xmath22 and our self - adjoint operator is then given by @xmath23 where the `` diagonal '' part of the old operator ( multiplying @xmath24 ) need not be transformed using @xmath25 .",
    "it is easy to show that defined as above , @xmath26 is self - adjoint using the standard inner product in @xmath0 with respect to lebesgue measure .",
    "note that if @xmath27 is an eigenfunction of the operator @xmath26 , then @xmath28 is an eigenfunction of the original operator @xmath29 with the _ same _ eigenvalue .",
    "we consider two mcmc algorithms which essentially differ only in the choice of proposal density and acceptance probability that is used in selecting steps .",
    "we will refer to the _ random walk metropolis - hastings _ ( rwmh ) algorithm as that which uses a symmetric proposal density to determine the next move ; for example , a gaussian centered at the current point : + @xmath30 where @xmath31 is an inverse covariance matrix that needs to be chosen appropriately for the given problem ( _ importance sampling _ ) .",
    "in other words , the proposed move from @xmath32 to @xmath33 is given by @xmath34 where @xmath35 is a normal random variable , mean @xmath36 and covariance @xmath37 .",
    "thus the update on the current state has no deterministic component .",
    "we will see that when the target density is not spherically symmetric , a naive implementation of the metropolis - hastings algorithm where the step scales are all chosen to be equal leads to very poor performance of the algorithm . as would be expected the convergence deteriorates as a function of the ratio of the true scales of the target density to the scale chosen for the proposal density .",
    "one variant used to accelerate the standard algorithm is a _ smart _ monte carlo method  @xcite that uses the gradient of the negative of the log target density at every step , @xmath38 to give @xmath39 and @xmath40 can be considered either as a constant scaling of the gradient part of the step or , if it is the hessian of @xmath41 , as producing a newton - like optimization step  @xcite .",
    "the move to @xmath33 is generated as @xmath42 , so now we have a _ random _ component @xmath35 and a _ deterministic _ component @xmath43 .",
    "viewed like this , moves can be considered to be steps in an optimization algorithm ( moving to maximize the probability of the target density ) with random noise added .",
    "we will see that with an optimal choice of @xmath40 and for gaussian target densities , the smart monte carlo method can converge in one step to the stationary distribution .",
    "we will also see that for a one dimensional non - gaussian distribution it actually fails to converge , independent of the values of the scaling parameters .",
    "once we have the self - adjoint operator for the chain , @xmath26 from eqn . [ soperator ]",
    ", and we know the eigenfunction with eigenvalue @xmath44 , @xmath45 , we can look for a candidate second eigenfunction in the function space orthogonal to the first eigenfunction where the inner product is defined by @xmath46 . given a family of normalized candidate functions in this space , @xmath47 , with variational parameter @xmath48 , the variational principle  @xcite states @xmath49 and depending on how accurately our family of candidate functions captures the true second eigenfunction , this can give quite a close approximation to the second dominant eigenvalue . in the problems we examine in the following sections the target densities have an even symmetry which makes it straightforward to select a variational trial function : any function with odd symmetry will naturally lie in the orthogonal space . for more complicated problems with known symmetries this general principle may be useful in selecting variational families for the purposes of algorithm comparison .",
    "another approach to constructing the variational family is shown in the section on multidimensional target densities : choose the test function as a linear combination of two functions , one with the properties that are required ( i.e. slow convergence to the target distribution ) and then the additional term is used merely to preserve orthogonality .    this variational method for providing a lower bound to the second eigenvalue of the mcmc algorithm was foreshadowed by a similar approach of lawler and sokal  @xcite .",
    "these authors considered the flow of probability out of a subset a of the state space ; in our language , their test functions were confined to the family @xmath50 , where @xmath51 is the indicator function on the set @xmath52 . by allowing for more general test functions",
    ", we establish not only rigorous but also relatively tight bounds on convergence rates , providing guidance for parameter optimization and algorithm comparisons .",
    "writing out explicitly for @xmath26 in @xmath53 we have @xmath54 as we will see in the following section , the lower bound in eqn .",
    "[ varbound ] can be arbitrarily close to @xmath55 and therefore equality holds . in these situations",
    "we can also show that the chain does not converge geometrically , based on the total variation norm definition of geometric convergence  @xcite . however , whether the type of convergence changes or not , we still refer to the magnitude of the second eigenvalue estimate in determining efficiency of the algorithm .",
    "the rationale is that the second eigenvalue determines the longest possible autocorrelation time of a function of the mcmc sample ; the worst case autocorrelation time will be of the order @xmath56 which could be extremely long .",
    "we will also see that there can be eigenvalues in the spectrum that are close to @xmath57 which determine the asymptotic convergence rate , i.e. @xmath58 where @xmath59 .",
    "interestingly , for this situation there is oscillatory behavior of the markov chain .",
    "consider the simplest case of a one dimensional gaussian target density @xmath60 with variance @xmath61 . under the rwmh algorithm ,",
    "the proposal density is @xmath62 the issue is to determine @xmath63 optimally ; a first guess would be that @xmath64 is the best choice .",
    "the rationale behind this is that since the target and proposal densities have the same form , if they also have the same scales , then the convergence rate might be expected to be optimal .",
    "we will see that this is not actually correct .    to begin , define a variational function @xmath65 , orthogonal to the target density and normalized such that @xmath66 .",
    "we can motivate this choice by recognizing that any initial distribution that is asymmetric will most likely have a component of this test function , and a convergence rate estimate based on it roughly corresponds to how fast probability `` equilibrates '' between the tails .",
    "more commonly , variational calculations will use linear combinations of many basis functions with the coefficients as variational parameters .",
    "we find here that including higher order terms in the test function is unnecessary as we obtain tight enough bounds just retaining the lowest order term .",
    "we proceed by evaluating eqn .",
    "[ scalarprod ] noting that because of the form of the acceptance probability , eqn .",
    "[ acceptanceratio ] , there are two functional forms for the kernels @xmath3 and @xmath25 depending on the sign of @xmath67 , i.e. whether the `` energy '' change , @xmath68 , is positive or negative .",
    "it is then convenient to use the coordinate transformation @xmath69 or @xmath70 where @xmath71 and @xmath72 to evaluate the integrals .",
    "an explicit expression for @xmath53 can be obtained for this case of a gaussian target density .",
    "next , we use a numerical optimization method to maximize the bound defined by eqn .",
    "[ varbound ] with respect to @xmath48 .",
    "the result of this analysis is shown in fig . [ fig : optk3_oldmc ] along with an empirically determined convergence rate for comparison .",
    "( to obtain the rate empirically , we run the mcmc algorithm for many iterates on a random initial distribution and observe the pointwise differences from the distribution of the @xmath19 iterate and the target distribution for large @xmath21 .",
    "these differences are either fit using hermite polynomial functions or by looking for the multiplicative factor which describes the geometric decay of the @xmath19 difference from one iterate to the next . )    = 3.0 in    the variational bound tightly matches the empirical obtained eigenvalue estimates in this case , and an optimum step size @xmath63 can be ascertained .",
    "clearly our @xmath73 initial guess for the best scaling is far from optimal .",
    "it is also worth comparing the optimal step scale with those obtained from different methods . in  @xcite ,",
    "a derivation of the optimal step size and acceptance rate is proposed based on minimizing the integrated autocorrelation time of an arbitrary function of the chain s states in stationarity . by approximating the chain as an infinite dimensional diffusion process",
    ", formulas are derived for the optimal scaling of steps . for our one dimensional gaussian target density , the proposal density s optimal variance",
    "is suggested to be @xmath74 which is surprisingly close to the estimate we have obtained using the variational method from fig .",
    "[ fig : optk3_oldmc ] , @xmath75 . however , given the infinite dimensional limit in which the former approximation is made , and the different convergence criterion based on autocorrelation time rather than second eigenvalue , the agreement may be merely coincidental .    moving to the one dimensional smart monte carlo , we have a gaussian proposal density of the form : @xmath76 where @xmath77 is the variance of the random part of the step and @xmath78 is the scale of the deterministic part .",
    "( letting @xmath79 we recover the rwmh algorithm of eqn .",
    "[ 1dqxy ] . )",
    "taking @xmath80 corresponds to performing a newton step at every iterate of the algorithm .",
    "thus , since the log of the target density is purely quadratic , the current point will always be returned to the extremum at @xmath36 by the deterministic component of the smart monte carlo step and the random component will give a combined move drawn from @xmath81 , which has the form of an _ independence sampler _  @xcite .",
    "if we then also choose @xmath64 , we see immediately that we are generating moves from the target distribution from the beginning , i.e. we have convergence in one step starting from _ any _ initial distribution .",
    "in real problems , however , @xmath41 will not be quadratic .",
    "we may obtain an estimate for @xmath63 and @xmath82 by considering its quadratic approximation or curvature but in many cases those estimates will have to be adjusted . if the curvature is very small ( or in multidimensional problems if the quadratic approximations are close to singular ) , the parameters will have to be increased to provide a step size control to prevent wildly unconstrained moves ( analogous to the application of a trust region in optimization methods  @xcite ) . if the curvature is large but we believe that the target density is multimodal , we need to decrease the parameters to allow larger steps to escape the local extrema .",
    "therefore we examine in the following the dependence of the convergence rate as we vary both of the parameters @xmath63 and @xmath82 .    the acceptance probability eqn .",
    "[ acceptanceratio ] has two functional forms separated by a boundary in the @xmath83 plane given by @xmath84 in particular , the acceptance probability is @xmath85 now we have a complication over the rwmh method because depending on the sign of the coefficient function @xmath86 in eqn .",
    "[ boundary ] , we find that either @xmath87 on @xmath88 , @xmath89 on @xmath90 or vice versa . this is shown in fig . [",
    "fig : xy_regions ] .    as before , for a given value of @xmath82 and @xmath63 , we need to break up the double integrals of the scalar product @xmath53 , eqn .",
    "[ scalarprod ] , into the appropriate regions .",
    "our choice of variational function is the same as before ( since the target density is the same ) and we again can get an explicit ( but complicated ) expression for eqn .",
    "[ scalarprod ] which we maximize with respect to @xmath48 .",
    "the results of this analysis are shown in fig .",
    "[ fig : quadevals ] ( a ) , where we fix @xmath91 and vary @xmath82 , @xmath63 .",
    "we have confirmed that these lower bounds are quite accurate as shown in fig .",
    "[ fig : quadevals ] ( b ) .",
    "the remarkable feature of these results is that even for this simple gaussian problem , the selection of step scale parameters @xmath82 , @xmath63 is critical to achieve convergence .",
    "as already mentioned , there is a trivial choice of optimum with @xmath92 that gives one step convergence from any initial distribution ( and therefore @xmath93 ) .",
    "however , if we change parameters infinitesimally such that @xmath94 ( @xmath95 ) we go through a discontinuous transition where we see no convergence from _ any _ initial distribution .",
    "this can be understood by recognizing that after one step we will have a proposal density ( before accept / reject ) @xmath96 which has a factor @xmath97 less probability in its tails than the target density .",
    "suppose there is an initial distribution or point mass concentrated at @xmath98 , @xmath99 .",
    "the proposed step of the smart monte carlo algorithm , starting at @xmath32 , will revisit @xmath32 too infrequently by a factor @xmath100 .",
    "thus detailed balance will force the transition @xmath101 to be accepted with a probability of only @xmath100 , and thus the initial distribution will take an arbitrarily long time to converge to the target density .    more formally , we can compute the probability of rejection , @xmath102 , when @xmath103 are as above and we find , @xmath104 where @xmath105 and @xmath106 is the cumulative normal @xmath107 distribution function .",
    "we note that @xmath108 by continuity of @xmath109 , and then we use proposition @xmath110 from  @xcite to conclude that the markov chain is no longer geometrically convergent for these values of @xmath82 and @xmath63 .",
    "in fact this is only one of the two disconnected regions where no convergence is observed in fig .",
    "[ fig : quadevals ] .",
    "the largest of the two ( with @xmath111 ) is defined exactly by the equation @xmath112 ( compare fig .  [",
    "fig : xy_regions](c ) with fig .",
    "[ fig : quadevals ] ( a ) ) . in this region",
    "the bound on the second eigenvalue approaches @xmath55 as the variational parameter , @xmath113 .",
    "this corresponds to a perturbation on the target density of @xmath114 for the unsymmetrized mcmc operator @xmath29 . in other words",
    ", we have a test distribution that has exponentially more probability in its tails than the target density . for initial states",
    "@xmath32 arbitrarily far away from the origin , the acceptance probability @xmath115 in the region @xmath90 is arbitrarily small . to see this , note that eqn .",
    "[ acceptancesmc ] is an exponentially decaying function of @xmath116 in this region , and given the form of the proposal density eqn .",
    "[ 1dqxysmart ] , we see that the expected value of @xmath116 is arbitrarily large and negative . thus states far out will never be `` allowed back '' and the fat tails of @xmath45 will never shrink back down those of @xmath2 .",
    "furthermore , moves @xmath117 where @xmath88 are always accepted ( because @xmath89 on @xmath118 ) which simultaneously prevents convergence .",
    "the situation is analogous to that described for @xmath119 and @xmath120 , except now there is a cutoff both on the deterministic step and the random step .",
    "a typical example of this is shown in fig .",
    "[ fig : noconquad ] .",
    "once we cross to the @xmath121 region , moves @xmath117 where @xmath90 are always accepted by eqn .",
    "[ acceptancesmc ] ( fig .",
    "[ fig : xy_regions ] ( a ) ) .",
    "therefore excess probability in the tails is allowed to flow back into the central part of the distribution and the convergence is not blocked .",
    "= 3.0 in    in the second region where no convergence is observed , ( @xmath122 in fig .",
    "[ fig : quadevals ] ) , we have a situation where the deterministic step alone ( taking @xmath123 ) leads to the proposed moves being generated by an unstable mapping , from the @xmath124 to @xmath125 iterate : @xmath126 where @xmath127 . the trial variational function for this situation",
    "also maximizes the bound as @xmath128 , again implying that the tails are not decaying to the stationary distribution .",
    "the reason is that , even when @xmath129 , we have a situation in which the _ expected _ or mean position of a state @xmath32 after one step is @xmath33 where @xmath88 .",
    "thus excessive probability in the tails can not be shifted inward to match the target density .",
    "the lack of convergence in this region was already noted for the metropolis adjusted langevin algorithm ( mala ) , a special case of the smc algorithm where @xmath130 .",
    "as shown in  @xcite , if @xmath131 is bounded , a sufficient condition for mala to fail to be geometrically ergodic is @xmath132 where @xmath133 is the single stepsize control parameter for that algorithm .",
    "the equivalence to the smc method is established by setting @xmath134 .",
    "thus , for the gaussian target density @xmath131 , the condition is @xmath135 .",
    "referring to the solid white line in fig .",
    "[ fig : quadevals](a ) , the non - convergent parameter regime for mala lies along the line segment @xmath136 with @xmath135 which matches exactly with the boundary we have determined using the variational method .",
    "the @xmath137 `` trough '' is a special case where we have oscillatory behavior .",
    "that is , the second eigenvalue is negative but greater than @xmath57 and in fact convergence does occur .",
    "interestingly setting @xmath138 means that @xmath139 and the acceptance probability of eqn .",
    "[ acceptancesmc ] looks again like that of the rwmh algorithm , but the convergence is actually faster . in a sense , given that the deterministic part of the step moves @xmath140 and the target distribution is symmetric , the oscillatory behavior allows the chain to sample the distribution twice as fast .      in scientific or statistical applications where mcmc is used , the log of the target density will ordinarily have higher order terms beyond the quadratic order we studied in the previous section .",
    "for example , in a bayesian inference problem the posterior distribution will rarely have a simple gaussian form . both finding the maximum _ a posteriori _",
    "parameter estimates and sampling from the posterior are made more difficult in the presence of these higher order terms .",
    "therefore , we wish to extend the previous example by studying a target distribution of the form @xmath141 . here",
    ", the log of the target density is quartic and the proposal density ( gaussian ) no longer has the same form as the target density .",
    "we would like to understand the performance of the monte carlo algorithms in this circumstance .",
    "( the test distribution is taken to be @xmath142 , i.e. in the orthogonal space to the stationary distribution ) .",
    "the goal is to estimate the optimal value of @xmath63 , as before .",
    "we can argue approximately that the step scale should be such that @xmath143 for a typical move @xmath32 , i.e. the change in energy is about @xmath55 and the acceptance probability is therefore @xmath144 .",
    "this gives a typical value for @xmath145 .",
    "since the proposal density is gaussian with variance @xmath77 , we therefore would naively predict @xmath146 . applying the variational method , we were unable to find a closed form solution to eqn .",
    "[ scalarprod ] so we had to resort to numerical integrals in determining the bound in eqn .",
    "[ varbound ] .",
    "the results are shown in fig .",
    "[ fig : oldmcconquart ] for the rwmh method ; it suggests an optimal choice for the step size parameter , @xmath63 , which is an improvement over our initial guess of @xmath147 ( when @xmath148 ) . using the formulas for the optimal step scale from  @xcite",
    "coincidentally yields about @xmath149 , also a little off from our variational estimate .",
    "= 3.0 in    turning to the smart monte carlo algorithm , if we wish to make the deterministic part of the proposed move a newton step using the hessian of @xmath41 at @xmath150 we are left with a singular hessian and an infinite deterministic step , reinforcing the need for the step length control parameter , @xmath82 .",
    "surprisingly , we find that , _ independent _ of the value of @xmath82 and @xmath63 , ( @xmath151 fixed at @xmath55 ) , the scalar product @xmath152 as @xmath113 .",
    "thus there are no choices of scaling parameters which will lead to convergence .",
    "this is borne out by numerical simulation , see fig .",
    "[ fig : noconquart ] for the changes in an initial density under many iterates of the algorithm with an arbitrary choice for @xmath133 , @xmath82 .",
    "= 3.0 in    the failure of the smart monte carlo method for the quartic problem is clearly due to non - convergence of the tails of the distribution , and can be seen by analyzing the integrals defining the operator , eqn .",
    "[ scalarprod ] , and noting that they all tend to zero as the variational parameter tends to zero , independent of the choice for @xmath151 , @xmath82 and @xmath63 .    as a partial check on this result",
    ", we again apply the condition derived in  @xcite for the mala algorithm , which states that geometric convergence is not possible when @xmath153 where @xmath133 is the step scale parameter .",
    "now , for the quartic density @xmath131 , the quantity on the left of the inequality @xmath154 , so _ no _ value of @xmath133 can give geometric convergence .",
    "mala is a special case of the smc algorithm , but we have shown here that the latter also has convergence problems indicated by @xmath155 , for _ all _ values of its scaling parameters , @xmath82 and @xmath63 .",
    "the gaussian and quartic problems are representative examples of target densities on which we have tested the smart monte carlo method .",
    "as we have seen there are severe convergence problems on these distributions .",
    "we would expect that for real applications , where the log of the target density would contain components of these and higher order nonlinearities , similar convergence difficulties for the smart monte carlo method would occur . it may well be that in applications where the method is extensively used ( e.g.  @xcite ) the convergence criteria are less precise than ours .",
    "for example , it may be acceptable to merely monitor the variance of some function of the state space variables and conclude that convergence has been achieved when it ceases to change appreciably , or as in  @xcite , define efficiency by the integrated autocorrelation time .",
    "for multidimensional problems , it is quite common to find a large range of scales associated with the target density  @xcite .",
    "that is , the curvature of the probability density along some directions in the parameter space is much larger than in other directions .",
    "clearly , if an mcmc method is not designed to take these different scales into account through importance sampling , the algorithm will perform very poorly .",
    "if the curvature is very high in a particular direction and we try to take a moderately sized step , it will almost certainly be rejected but if we take small steps in directions that are essentially flat the mcmc algorithm will be very slow to equilibrate .",
    "we would like to show explicitly here what happens to the convergence rate when the scale of the problem has been underestimated or overestimated .    the variational calculations for the one dimensional examples of the previous section either yielded explicit formulas or gave integrals that were relatively fast to compute numerically .",
    "however as we go to multiple dimensions neither of these features are present , in general . typically the integrals describing @xmath53 will _ not _ factor into one dimensional integrals . for gaussian target densities the full space is broken into regions analogous to those in fig .",
    "[ fig : xy_regions ] , described by an equation like @xmath156 where @xmath52 is a symmetric @xmath13 by @xmath13 matrix which is not necessarily positive definite . for the rwmh algorithm applied to a multivariate gaussian target density with inverse covariance matrix @xmath157 , we have @xmath158 , and therefore all the dimensions are coupled through the energy change , @xmath159 .",
    "we would still like to be able to get a lower bound on @xmath17 , and to this end note that _ any _ test function orthogonal to the target density will work in eqn .",
    "[ varbound ] ; the bound does not explicitly require a variational parameter , however without it the estimate will be less accurate .",
    "it is still necessary to make choices for the test functions that are both tractable in computing @xmath160 and are `` difficult '' functions for the given algorithm to converge from , i.e. have a significant component along the true second eigenfunction .    as an example , take the multivariate gaussian distribution of the form @xmath161 with @xmath162 , and consider using the mh algorithm with importance sampling , i.e. @xmath163 where again @xmath31 is the inverse covariance matrix / step size control term and to simplify we assume that both @xmath157 and @xmath31 are diagonal . without any analysis we might guess that the optimum choice for @xmath31 is @xmath157 .",
    "first we construct a test function that will provide a useful bound when the proposed steps are too large for the natural scale of the problem . for simplicity , consider putting a delta function distribution at the origin .",
    "if we take large steps the acceptance probability should be low and there will be a large overlap between the initial state and the final state . in the limit that the proposed steps have infinite length , the initial state will not be changed at all and the bound on",
    "the second eigenvalue in absolute value will approach one . to do this",
    "more carefully we define a test function which is a gaussian whose variance will ultimately be taken to zero to represent the delta function .",
    "however , we also need to add another term to ensure the test function is orthogonal to the target density , in order to apply the variational bound .",
    "therefore , for the unsymmetric operator we write the test function as : @xmath164 where @xmath165 is the probability density for a multivariate gaussian with covariance matrix @xmath166 and @xmath52 and @xmath167 are constants . for the symmetrized operator",
    "the trial function is transformed to @xmath168 .",
    "@xmath52 and @xmath167 are constrained to satisfy the orthogonality relation @xmath169 and a normalization @xmath170 .",
    "these lead to the conditions @xmath171 then it can be seen that @xmath172 where we have used the orthogonality condition , the fact that @xmath165 integrates to @xmath55 and that @xmath26 is self - adjoint .",
    "writing out the operator @xmath26 explicitly we get @xmath173 the last term on the right hand side is @xmath174 , making use of the normalization condition , so we are left with @xmath175 since we are ultimately taking a limit as @xmath176 ( @xmath177 a delta function ) we can make approximations to these integrals as follows : @xmath178 and @xmath179 finally by taking @xmath176 we have the expression @xmath180 as already mentioned , for the multidimensional problem we expect different functional forms for the kernels @xmath25 and @xmath3 depending on the initial and final state @xmath83 and this is what makes decoupling the integrals difficult .",
    "however for this choice of test function the equation for the boundary ( with @xmath181 ) is given by @xmath182 and since @xmath157 is positive semidefinite we always stay on one side of the boundary ( the energy never decreases from the initial distribution placed at @xmath150 ) .",
    "then @xmath183 where @xmath184 and @xmath185 are the diagonal elements of the diagonal matrices @xmath31 and @xmath157 , respectively . with no importance sampling we would have @xmath186 where @xmath151 would be chosen to make sufficiently large steps to enable it to sample @xmath2 . a rough argument as follows can give some insight into the form of eqn .",
    "[ bigstepbound ] : @xmath187 is a measure of the scale in the @xmath188 coordinate direction of the proposal density , @xmath189 is the scale in the @xmath188 coordinate direction of the target density .",
    "suppose that @xmath190 for each @xmath191 , that is the scales of the proposal density are too large in all directions .",
    "then the ratio of the mean volume of moves generated by @xmath192 to the volume occupied by @xmath193 is exactly @xmath194 .",
    "intuitively , this ratio is proportional to the acceptance probability , and in the regime @xmath190 the acceptance probability determines the convergence properties .",
    "we want to use eqn .",
    "[ bigstepbound ] to show how choosing step sizes _ too large _ even in one direction will result in a very inefficient algorithm .",
    "suppose that for all but one of the directions we make @xmath195 , @xmath196 which would be roughly the correct scaling in those directions .",
    "then the bound on the second eigenvalue is @xmath197 from this we can see that as we go to larger and larger step sizes relative to the scale in the last direction ( @xmath198 ) , the bound on @xmath17 increases to @xmath55 . conversely we can argue that if one of the directions of the target density has a scale that is considerably smaller than the step scales being used in the proposal density , we will get very few acceptances and the convergence rate will be close to @xmath36 .",
    "hence we see explicitly the need for importance sampling to accelerate convergence .",
    "we would also like to address what happens in the other limit as the step size becomes excessively small compared to the natural scale of the problem .",
    "( in fact eqn .",
    "[ bigstepbound ] gives a lower bound of zero in that case which is not surprising as it is based essentially on the term in the operator equation which gives the probability of _ staying _ at the current state .",
    "if we take infinitesimally small steps , the acceptance probability will be one and we will never stay at the current state ) .",
    "when the step scales are infinitesimally small we expect intuitively that the bound on the second eigenvalue will also approach one ; even though the acceptance ratio is close to one , very small steps will never be able to `` explore '' the target distribution sufficiently . to compute this limit",
    ", we propose a test function which has components of the target density in all directions except the last , where it has an antisymmetric form to make sure it is orthogonal to the target density .",
    "with respect to the symmetrized operator @xmath26 this means @xmath199 here @xmath200 is the one dimensional gaussian density which is the @xmath188 factor in a diagonalized multivariate gaussian density .",
    "( recall that since @xmath2 is an eigenfunction of @xmath29 , then @xmath45 is an eigenfunction of @xmath26 . )",
    "we still have the problem of decoupling the @xmath13-dimensional multivariate problem into @xmath13 one dimensional problems . to manage this we use a device to re - express the operator equation , eqn .",
    "[ scalarprod ] , explicitly in terms of the change @xmath201 .",
    "( i.e. @xmath202 ) , which we denote by @xmath203 .",
    "that is @xmath204 then we use the integral representation of the delta function @xmath205 , factor @xmath206 , and rearrange the order of integration to give : @xmath207 where @xmath208 contains the integration over the now decoupled @xmath83 coordinates : @xmath209 note that the complex integral with respect to @xmath210 has a branch point at the roots of @xmath211 which lie on the imaginary axis at @xmath212 and @xmath213 .",
    "it simplifies the analysis to consider the situation @xmath214 for @xmath215 and assume that @xmath216 is even .",
    "this way , the roots of @xmath217 , @xmath218 and @xmath219 , are @xmath220 order poles and not branch points , also on the imaginary axis .",
    "if we now also assume that @xmath221 , then we can take a contour as shown in fig .",
    "[ fig : contour ] when @xmath222 and a similar one in the lower imaginary plane when @xmath223 .",
    "= 2.0 in    thus eqn .",
    "[ before_contour_integral ] is reduced to a residue term and a real integral which needs to be evaluated numerically .",
    "the result is plotted for @xmath224 in fig .",
    "[ fig : multi_varn11 ] along with the bound that came from eqn .",
    "[ largestepbad ] .",
    "thus we see the trade off between taking large steps that potentially can explore the space quickly but have a higher chance of being rejected and taking small steps which will have a high acceptance probability but will be unable to sample the space quickly .",
    "= 3.0 in    as we saw when doing the full variational calculation for the one dimensional problems , the best step scale to use is not what we may have guessed ; the natural choice @xmath225 here does not appear to minimize the second eigenvalue .",
    "we believe this kind of `` approximate '' variational approach may be a useful way to deal with problems which are difficult to analyze otherwise .",
    "by applying a variational method , it is possible to obtain an accurate ( lower bound ) estimate for the second eigenvalue of an mcmc operator and thus bound the asymptotic convergence rate of the chain to the target distribution .",
    "given such an estimate we can optimally tune the parameters in the proposal distribution to improve the performance of the algorithm .",
    "the procedure has a role to play between the various numerical algorithms that perform convergence diagnostics before the full simulations are run , to allow the user to manually tune parameters , and the adaptive schemes  @xcite that require no preliminary exploration .",
    "the simulations we performed to confirm our variational bounds in the case of a one dimensional target density and varying one step scale parameter , fig .  [ fig : optk3_oldmc ] and fig .",
    "[ fig : quadevals ]  ( b ) , would be infeasible to do as we move to higher dimensions and as we vary additional algorithm parameters .",
    "it is in those situations that the variational method can more efficiently identify regions of optimality .    in addition",
    ", the variational method allows us to discover weaknesses in variants of the random walk metropolis - hastings algorithm which on the surface appear to be reasonable prescriptions for sampling the target density .",
    "this is most dramatically seen in the smart monte carlo method discussed above which apparently has serious flaws for even the simplest of one dimensional target densities .",
    "although the smart mc method has been widely used in molecular dynamics applications  @xcite the scales are often chosen by physical considerations ( for example , to not exceed significantly the step sizes needed to accurately describe the dynamical evolution of the system ) and furthermore , the diagnostics of convergence are not as rigorous as ours ; typically a physical quantity is monitored till it appears to reach an equilibrium value , the rare events which correspond to the tails of the target distribution are possibly of lesser importance in those studies .",
    "therefore the convergence problems we have discussed here specifically in relation to the smart monte carlo method , to our knowledge , have not been previously examined .",
    "presumably the convergence problems can be corrected by a more careful discretization of the underlying diffusion equations , as was shown for the related langevin - type methods  @xcite .",
    "it would be interesting to apply the same technique to the more broadly used gradient based hybrid mc algorithms  @xcite and other non - adaptive accelerated methods ( e.g. parallel tempering  @xcite ) where the alternative techniques for determining convergence via diffusion approximations may be harder to apply .",
    "more generally , the variational analysis could be a useful tool in making comparisons between the convergence properties of the latest mcmc algorithms without extensive numerical simulation .",
    "the authors wish to thank cyrus umrigar for discussions and the usda - ars plant pathogen systems biology group at cornell university for computing resources .",
    "crm acknowledges support from usda - ars project 1907 - 21000 - 017 - 05 .",
    "we also acknowledge support from nsf dmr 0705167 ."
  ],
  "abstract_text": [
    "<S> we demonstrate the use of a variational method to determine a quantitative lower bound on the rate of convergence of markov chain monte carlo ( mcmc ) algorithms as a function of the target density and proposal density . </S>",
    "<S> the bound relies on approximating the second largest eigenvalue in the spectrum of the mcmc operator using a variational principle and the approach is applicable to problems with continuous state spaces . </S>",
    "<S> we apply the method to one dimensional examples with gaussian and quartic target densities , and we contrast the performance of the random walk metropolis - hastings ( rwmh ) algorithm with a `` smart '' variant that incorporates gradient information into the trial moves , a generalization of the metropolis adjusted langevin algorithm ( mala ) . we find that the variational method agrees quite closely with numerical simulations . </S>",
    "<S> we also see that the smart mcmc algorithm often fails to converge geometrically in the tails of the target density except in the simplest case we examine , and even then care must be taken to choose the appropriate scaling of the deterministic and random parts of the proposed moves . again </S>",
    "<S> , this calls into question the utility of smart mcmc in more complex problems . </S>",
    "<S> finally , we apply the same method to approximate the rate of convergence in multidimensional gaussian problems with and without importance sampling . </S>",
    "<S> there we demonstrate the necessity of importance sampling for target densities which depend on variables with a wide range of scales . </S>"
  ]
}