{
  "article_text": [
    "there is a vast number of applications benefiting from the nice properties of the kalman filter ( kf ) . among these properties ,",
    "the possibility of pre - computation of gains @xcite is of much relevance for applications .",
    "however , in some cases pre - computation is not possible or viable due to missing _ a priori _ relevant information .",
    "this is the case when using the kf to estimate the state of markov jump linear systems ( mjls ) , since the parameters are not known prior to the current time instant @xmath0 , in fact they depend on the markov chain current state @xmath1 .",
    "then , to use kf for mjls one needs to do either online computation of the gains or offline pre - computation of a number of sample path dependent gains , a figure that grows exponentially with time .",
    "this drawback of kf for mjls is one of the main motivations behind the emergence of other filters for this class of systems , see e.g. @xcite . among them , the one that is closer to the kf in terms of structure and performance is the linear minimum mean square estimator ( lmmse ) first introduced for mjls in @xcite .",
    "indeed , as we shall see later , the lmmse computation relies on coupled riccati equations that are quite similar to the ones arising in kalman filtering .",
    "another similarity is that both are optimal in the mean square error sense , although under different constraints .",
    "the main dissimilarity with kf lies in the fact that , instead of having path dependent gains , the lmmse has sets of @xmath2 gains , @xmath2 being the cardinality of the markov state space ( which we assume finite in this paper ) , that are precomputed based on the system matrices and the initial distribution of @xmath3 . during application , after observing @xmath1 one picks the corresponding gain from the precomputed sets of gains . in this way , one obtains the best estimate for the system state @xmath4 among all estimators that are _ linear _ and _ markovian _ , these being precisely the constraints we mentioned before .    however , there is currently no intermediary solution between the kf and the lmmse in literature . in this paper",
    "we provide a `` lattice '' of filters bridging the kf to the lmmse by relaxing the markovianity constraint and allowing clustered information of the markov chain to be considered when designing the filter gains . by clustered information ,",
    "we mean that we have a partition of the state space of the markov chain into several classes called clusters , and we observe the trajectory of classes the chain belongs to along time .",
    "the clusters may be chosen as they are considered as a design parameter , establishing a trade - off between complexity and performance that can be explored in practical systems aiming at the best feasible performance . at one extreme when only one cluster is taken into account",
    "our filter is equivalent to the lmmse , and at another extreme with @xmath2 clusters we retrieve the kalman filter ; intermediary number of clusters leads to filters with variable performances and computational burden .",
    "reasonably enough , the higher is the number of clusters the smaller is the attained estimation error and higher is the number of gains to compute .",
    "we start with a simple , precise formulation of the optimal estimation problem in section [ sec - problem ] , with the estimator in the classical form of luenberger observers .",
    "we then proceed in section [ sec - proof ] to a constructive proof that evaluates the estimation error and uses the completion of squares method to obtain the optimal gains .",
    "some remarks on how the proposed class of estimators includes both the kf and lmmse , and on the number of gains and riccati - like equations to be precomputed , are presented .",
    "some variants of the studied optimization problem and how to extend optimality to general estimators are briefly discussed in section [ sec - properties ] .",
    "we have also included a numerical example in section [ sec - example ] comparing the computational burden in terms of cpu time and the estimation error computed both via monte carlo simulation and via the proposed formula .",
    "the example makes clear that the performance is strongly dependent on the number of clusters and how the markov states are distributed in the clusters .",
    "consider the mjls @xmath5 with initial condition @xmath6 where @xmath7 is the normal distribution with mean @xmath8 and covariance matrix @xmath9 .",
    "the variable @xmath1 denotes the state of a markov chain with finite state space @xmath10 and initial distribution @xmath11 $ ] .",
    "the noise sequence @xmath12 is independent from @xmath13 and the markov chain @xmath3 , @xmath14=0 $ ] and @xmath15 $ ] is the identity matrix for all @xmath0 .",
    "we assume that @xmath16 and @xmath17 , @xmath18 .",
    "we shall consider luenberger observers in the form . ]",
    "@xmath19 where matrix @xmath20 is referred to as the filter gain , and the initial estimate is given by @xmath21 .",
    "this produces an estimation error @xmath22 satisfying @xmath23 and @xmath24 .",
    "as for the markov chain , we consider a partition @xmath25 for its state space , and employ the variable @xmath26 to indicate the partition being visited at time @xmath0 , that is , @xmath27 we assume that the observations of @xmath28 up to time @xmath0 are available to calculate the filter gains . we also assume that the jump variable @xmath3 at time @xmath0 is available , however in the clustered information filter we do not take into account its past values , that is , @xmath29 are not taken into account when calculating the gain ( to avoid an excessive number of branches , as explained earlier ) . moreover , the gain should not depend on future information , as it has to be implemented at every time instant @xmath0 .",
    "therefore , we impose that the filter matrices at time instant @xmath0 are in the form @xmath30 for measurable functions @xmath31 .",
    "gains satisfying this constraint are referred to as _",
    "feasible gains_. we are interested in obtaining the minimum mean square state estimation at a given time @xmath32 , leading to the optimization problem @xmath33 where we write @xmath34 for ease of notation .",
    "we refer to the filter satisfying as the _ clustered information lmmse _ , or clmmse for short .",
    "consider the mjls in and the filter in with an arbitrary sequence of feasible gains @xmath35 for each @xmath36 , @xmath37 and @xmath38 , @xmath39 , we define @xmath40 the variable @xmath41 plays an important role in the derivation of the formula for the optimal filter because the optimal gains @xmath42 are such that @xmath43 where @xmath44 is the solution of a riccati - like equation , as we shall see in the next theorem . the physical interpretation for @xmath41 is that , when divided by @xmath45 it gives the conditional error covariance matrix , e.g. it coincides with the filtering riccati equation of the kalman filter when @xmath46 ( as many clusters as markov states ) .",
    "+    we illustrate the notation introduced in on a simple example .",
    "consider the mjls with @xmath47 , @xmath48 , @xmath49 , @xmath50 , the clusters @xmath51 , @xmath52 , initial distribution @xmath53 $ ] and probability matrix @xmath54 consider trivial gains @xmath55 , so that @xmath56 and @xmath57 .",
    "moreover , the estimation error and the markov state are independent because all modes are identical , then @xmath58 for instance , for @xmath59 , @xmath60 and @xmath61 we have @xmath62 similarly , @xmath63 , @xmath64 , @xmath65 , @xmath66 and @xmath67 .",
    "note from that summing @xmath41 in the indexes corresponding to @xmath68 and @xmath69 we obtain @xmath70 , e.g. from the above we have @xmath71 .",
    "the optimal gain sequence can be ( pre-)computed based on the following sets of matrices .",
    "let @xmath72 for each @xmath73 . for each @xmath74",
    ", let @xmath39 and compute for each @xmath37 and @xmath38 , @xmath75 ,    \\end{aligned }    &   \\text{otherwise , }   \\end{cases}\\ ] ] where we denote @xmath76    [ theo - optimal - solution ] given the realization of @xmath1 , @xmath77 , and the corresponding cluster observations @xmath78 , the gains @xmath79 of the lmmse can be computed for each @xmath77 as @xmath80 where @xmath44 is given in",
    ". moreover , the conditional second moment of the estimation error is given , for each @xmath77 , by @xmath81 and it is optimal in the sense that , for any gain sequence @xmath82 , @xmath83    * proof * we start showing that and are true for the gains prescribed in .",
    "we proceed by induction in @xmath0 . for the time instant @xmath84 we have that the initial estimate is given by @xmath21 , yielding @xmath24 ( irrespectively of the filter gains ) , hence @xmath85 by the induction hypothesis we assume that and are valid for some @xmath86 . in order to complete the induction ,",
    "we now consider the time instant @xmath87 . for a filter with an arbitrary sequence of easible gains , denoted by @xmath82 , and a given realization @xmath88 we have @xmath89 note that the above quantity turns out to be zero whenever @xmath90 , irrespectively of @xmath91 , which makes and trivially true for @xmath87 in this case . now , in case @xmath92 we write @xmath93    { \\mathbbm{1}_{\\{\\rho(0)=\\ell_0,\\ldots,\\rho(k)=\\ell_{k},\\theta(k+1)=i \\ } } } \\big ) \\\\ & =    e\\big (   \\sum_{j=1}^n [ ( a_{\\theta(k)}-m_k l_{\\theta(k ) } ) { \\widetilde}x_k                     { \\widetilde}x_k ' ( a_{\\theta(k)}-m_k l_{\\theta(k ) } ) '    \\\\ & \\qquad   + ( g_{\\theta(k)}-m_k h_{\\theta(k ) } ) w_k w_k ' ( g_{\\theta(k)}-m_k h_{\\theta(k ) } ) ' ]    { \\mathbbm{1}_{\\{\\rho(0)=\\ell_0,\\ldots,\\rho(k)=\\ell_{k},\\theta(k+1)=i,\\theta(k)=j \\ } } } \\big ) \\\\ & =    e\\big (   \\sum_{j\\in s_{\\ell_k } } [ ( a_j - m_k l_j ) { \\widetilde}x_k                     { \\widetilde}x_k ' ( a_j - m_k l_j ) '    \\\\ & \\qquad   + ( g_j - m_k h_j ) w_k w_k ' ( g_j - m_k h_j ) ' ]    { \\mathbbm{1}_{\\{\\rho(0)=\\ell_0,\\ldots,\\rho(k-1)=\\ell_{k-1},\\theta(k+1)=i,\\theta(k)=j \\ } } } \\big ) \\end{aligned}\\ ] ] where the last equality comes from the fact that @xmath94 whenever @xmath95 is not in the cluster @xmath96 , and @xmath97 otherwise . resuming the above calculation and writing @xmath98 for ease of notation",
    ", we have : @xmath99 .",
    "\\end{aligned}\\ ] ] from basic properties of the markov chain we have that @xmath100 and @xmath101 are conditionally independent given @xmath1 , for any @xmath102 .",
    "moreover , from , and it can be shown that @xmath103 and @xmath101 are conditionally independent given @xmath1 , hence we may eliminate @xmath104 from the first conditional expectation in , leading to    @xmath105 \\\\ & = \\sum_{j\\in { \\widetilde}s } p_{ji }     \\big [ ( a_{j}-m_k l_{j } )          x_{\\ell_0,\\ldots,\\ell_{k-1},j , k}(m )   ( a_{j}-m_k l_{j } ) '   \\\\ & \\qquad \\qquad   + p_{\\ell_0,\\ldots,\\ell_{k-1},j , k } ( g_{j}-m_k h_{j } ) ( g_{j}-m_k h_{j } ) ' \\big ] , \\end{aligned}\\ ] ]    where we denote @xmath106 .",
    "we now turn our attention to the optimality of @xmath91 .",
    "consider a feasible gain sequence in the form @xmath107 where @xmath20 is the variable to be determined ; since @xmath108 is a function of @xmath109 only , we can use the induction hypothesis to write @xmath110 eq .",
    "allows to write @xmath111 , irrespectively of @xmath20 , and using we evaluate @xmath112 also , by plugging into , @xmath113 , \\end{aligned}\\ ] ] and , by completing squares and denoting @xmath114 for brevity , we obtain @xmath115 , \\end{aligned}\\ ] ] thus making clear that the minimal @xmath41 is attained by setting @xmath116 whenever @xmath117 , confirming the second equation in ; the inverse always exists because we have assumed @xmath17 .",
    "if @xmath95 is such that @xmath118 then the gain @xmath20 is immaterial for the error covariance , indeed we see from that such gain is not accounted for , so that one can pick @xmath55 , confirming the first equation in .",
    "chosing the gain as above we get the gain sequence @xmath119 and @xmath120 so that produces @xmath121 which confirms for the time instant @xmath87 . substituting @xmath122 in",
    "we get after some manipulations @xmath123= y_{\\ell_0,\\ldots,\\ell_{k},i , k+1 } , \\end{aligned}\\ ] ] which confirms for @xmath87 , thus completing the induction .",
    "it remains only to show the optimality of @xmath124 in terms of .",
    "this follows directly from , in fact , @xmath125 where all sums are in the indexes @xmath126 and we denote by @xmath127 the estimation error associated with the gain @xmath124 . @xmath128",
    "for each @xmath129 , we compute @xmath130 matrices on the left hand side of , hence we have ( up to ) this number of recursive riccati equations to solve .",
    "we also have the computation and storage of an equal number of gains .",
    "then , to obtain the state estimate at time @xmath131 , we have to store a total of @xmath132 gains when @xmath133 , and @xmath134 gains otherwise . regarding the number of matrix inverses",
    ", one may invert each @xmath44 given by and store it at time step @xmath0 for the forthcoming iterates , hence we have a total of ( up to ) @xmath135 inverses .",
    "note from that , given a realization of the markov chain @xmath1 , @xmath36 , the time instant @xmath131 involved in the problem formulation affects only the cardinality of the optimal gain sequence @xmath124 .",
    "more precisely , if @xmath137 is the gain sequence attaining , and , if we replace @xmath131 with @xmath138 in and obtain the new optimal gain sequence @xmath139 ( considering the same markov chain realization ) , then we have that @xmath140 , @xmath141 .",
    "this is consistent with the sense of optimality in , and is in perfect harmony with the theory of both kalman filter and the standard lmmse . as a consequence ,",
    "the provided clustered information lmmse is also a solution for the multiobjective problem @xmath142 or for any linear combination of mean square errors writen in the form @xmath143      it is simple to see that we retrieve the standard lmmse when we consider only one partition @xmath144 .",
    "in fact , in this setup we have @xmath145 and one can check by inspection that and the lmmse riccati equation ( * ? ? ?",
    "* eq . xx ) are identical . as for the kalman filter , if we set @xmath146 , @xmath73 , then @xmath147 if @xmath148 and is reduced to @xmath149 .",
    "\\end{aligned}\\ ] ] from theorem [ theo - optimal - solution ] we have @xmath150 so that , writing @xmath151 , substituting in the above equation for @xmath44 and manipulating ( cancelling the @xmath152s and @xmath153s ) yields @xmath154 ,    \\end{aligned}\\ ] ] which is the usual riccati difference equation appearing in kalman filters .",
    "this means that @xmath155 is equal to the kalman covariance matrix multiplied by the probability that the markov chain visits @xmath156 . in the gain formula , this probability is cancelled , yielding that the kalman gain coincide with @xmath157 .",
    "concluding , we have the kalman filter and the markovian lmmse in opposite `` extremes '' of the clmmse , and a lattice of estimators between them , depending on how the markov states are arranged in clusters .      consider linear estimators of the general form @xmath158 where matrices @xmath159 and @xmath160 , @xmath36 , are the optimization variables replacing @xmath20 in the problem ; consider also that @xmath161 and @xmath162 , where @xmath163 are measurable functions .",
    "it can be demonstrated that the optimal estimate satisfies @xmath164 which is produced by setting @xmath165 and @xmath166 where @xmath157 is as in , thus retrieving the luenberger observer form and the solution given in theorem [ theo - optimal - solution ] .",
    "this is not surprising since the `` innovation form '' @xmath167 for some @xmath20 is necessary for some basic properties of an observer to be fulfilled , e.g. for @xmath168 to remain zero a.s . for @xmath74 in cases when @xmath169 a.s .",
    "and there is no additive noise in the state ( @xmath170 , @xmath171 ) .",
    "we have applied the clmmse to the system given in @xcite . the system data is reproduced below for ease of reference .",
    "@xmath172 , a_2=\\left[\\begin{array}{ll}0 -0.2673\\\\0.81 1.134\\end{array}\\right ] , a_3=\\left[\\begin{array}{ll}0 -0.81\\\\0.81 0.972\\end{array}\\right ] , a_4=\\left[\\begin{array}{ll}0 -0.1863\\\\0.81 0.891\\end{array}\\right],\\ ] ] @xmath173 , l_i=\\left[\\begin{array}{ll}1&0\\end{array}\\right ] , h_i=1 , i=1,\\ldots,4,\\ ] ] @xmath174.\\ ] ] every possible cluster configuration has been taken into account , and the aim was to estimate the state at time instant @xmath175 .",
    "the mean square error was calculated by means of ( [ eq - def - p ] ) and ( [ eq - computing - p ] ) , which lead to @xmath176 ; results have been confirmed by monte carlo simulation .",
    "figure  [ fig1 ] shows the obtained results in groups according to the number of clusters @xmath177 .",
    "as expected , the standard lmmse ( with @xmath178 ) presented the largest estimation error and the kalman filter ( @xmath179 ) features the smallest one .",
    "the performance of other filters with `` intermediary configurations '' ( @xmath180 ) is similar to the lmmse regarding the error , hence they are not much appealing in view of their higher complexity .",
    "note that in this particular example , the modes are similar to each other ( only two parameters of @xmath181 change ) .     versus number of clusters @xmath177 for every cluster configuration for the system with data given in ( [ data1]).,width=377 ]    now let us introduce a more relevant change in one mode by replacing @xmath182 , @xmath183 , @xmath184 and @xmath185 with @xmath186 respectively .",
    "the results are displayed in figure  [ fig2 ] .",
    "we now can clearly distinguish two groups of filters , one with average errors around 106 and a second one around 104 . in this setup",
    ", there is a tendency for better performance when @xmath187 is isolated from other states , e.g. with @xmath188 and @xmath189 we have @xmath190 , while @xmath191 and @xmath192 lead to @xmath193 .",
    "moreover , considering that the ( hard to implement ) kalman filter yields @xmath194 we see that the filter with configuration @xmath188 and @xmath189 is quite competitive in this scenario .     versus number of clusters @xmath177 for every cluster configuration for the system with data given in ( [ toto]).,width=377 ]",
    "we have explored the markov state information structure in mjls leading to new filters whose estimation error and complexity lie in the between the standard kalman filter and the lmmse , and establish a trade - off between performance and computational burden .",
    "this allows one to explore the computational resources at hands more deeply , seeking for the best possible estimates .",
    "as illustrated in the example , the new filters can provide competitive alternatives to the existing ones .",
    "we note that , for a given plant , the computational burden depends only on @xmath177 , see the formulas given in section  [ rem - number - matrices ] .",
    "then , a relevant question is how to select the markov states to form each cluster so that the estimation error is minimized , which will be considered in future research ."
  ],
  "abstract_text": [
    "<S> new linear minimum mean square estimators are introduced in this paper by considering a cluster information structure in the filter design . </S>",
    "<S> the set of filters constructed in this way can be ordered in a lattice according to the refines of clusters of the markov chain , including the linear markovian estimator at one end ( with only one cluster ) and the kalman filter at the other hand ( with as many clusters as markov states ) . </S>",
    "<S> the higher is the number of clusters , the heavier are pre - compuations and smaller is the estimation error , so that the cluster cardinality allows for a trade - off between performance and computational burden . in this paper </S>",
    "<S> we propose the estimator , give the formulas for pre - computation of gains , present some properties , and give an illustrative numerical example . </S>"
  ]
}