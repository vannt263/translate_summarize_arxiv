{
  "article_text": [
    "since the early days of statistical learning theory understanding of the generalization abilities of empirical risk minimization has been a central question . in 1968 , vapnik and chervonenkis @xcite introduced the combinatorial property of classes of classifiers which we now call the _ vc dimension _ , which plays a crucial role not only in statistics but in many other areas of mathematics . by",
    "now it is strongly believed that the vc - dimension fully characterizes the properties of the empirical risk minimization algorithm .",
    "for example , when no restrictions are made on the distributions one can prove that the probability of error of the minimizer of empirical risk is close to the probability of error of the best classifier in the class , up to a term of order @xmath5 , with probability at least @xmath6 , where @xmath7 is the vc dimension of the class and @xmath1 is the sample size .",
    "one can also prove a minimax lower bound ( valid for any learning procedure ) matching up to absolute constants .",
    "but the fact that vc dimension alone describes the complexity term appears to be true only in the agnostic case , when no assumptions are made on the labelling mechanism .",
    "it was noticed several times in the literature , that when considering bounded noise , vc dimension alone is not a right complexity measure of erm @xcite . until now this phenomenon was discussed only for a small amount of specific classes . in this paper",
    "we present this yet unknown combinatorial complexity measure for general vc classes .    in the last twenty years",
    "many efforts were made to understand the conditions that imply fast @xmath8 convergence rates , instead of slow @xmath9 rates . by",
    "now these conditions are well understood ; we refer for example to van erven et al .",
    "@xcite for an extensive survey and related results . at the beginning of the 2000s , so - called _ localized _ complexities ( bartlett et al .",
    "@xcite , koltchinskii @xcite ) were introduced to statistical learning and became popular techniques for proving @xmath8 rates in different scenarios .",
    "but in addition to better rates , localization means that _ only a small vicinity of the best classifier _ really affects the learning complexity .",
    "almost fifty years after the introduction of vc theory this phenomenon is still not fully understood and studied .",
    "specifically , we lack tight error bounds based on localization and expressed in terms of intuitively - simple and calculable combinatorial properties of the class .",
    "existing approaches based on localization ( mainly , via _ local rademacher complexities _ ) are typically difficult to calculate directly , and the simpler relaxations of these bounds in the literature use localization merely to gain improvements due to the _ noise conditions _ , but fail to maintain the important improvements due to the local _ structure of the function class _ ( i.e. , localization of the complexity term in the bound ) .",
    "moreover , in classification literature there are no known general minimax lower bounds in terms of localized processes .",
    "there does exist one line of results which simultaneously give fast convergence rates and perform direct localization of a class of classifiers , to arrive at simple generalization bounds .",
    "specifically , massart and ndlec @xcite proved that under massart s bounded noise condition , generalization of order @xmath10 is possible , where @xmath11 is a margin parameter responsible for the noise level . to derive this bound , massart and ndlec",
    "use a localized analysis to obtain improved rates under these noise conditions .",
    "however , the bound does not reflect this localization in the _ complexity term _ itself : in this case , the factor @xmath12 .",
    "gin and koltchinskii @xcite refined this bound , establishing generalization of order @xmath13 for empirical risk minimization , where @xmath14 is a distribution - dependent quantity they refer to as _ alexander s capacity function _",
    "( from the work of alexander in the 80s @xcite ) .",
    "very recently , hanneke and yang @xcite introduced a novel combinatorial parameter @xmath15 , called the _ star number _ , which gives perfectly - tight distribution - free control on @xmath16 , and generally can not be upper bounded in terms of the vc dimension .",
    "thus ( as noted by hanneke @xcite ) , in terms of distribution - free guarantees on the generalization of empirical risk minimization , the implication of gin and koltchinskii s result is a bound @xmath17 .",
    "however , it was noted @xcite , that this bound is sometimes suboptimal . in this paper",
    "we will give a new argument showing potential gaps of this bound .",
    "the aim of this paper is to perform a tight distribution - free localization for vc classes under bounded noise by introducing a new distribution - free complexity measure , thus resolving the existing gap between upper and lower bounds .",
    "the complexity measure is a localized empirical entropy measure : essentially , a fixed point of the local empirical entropy . most of the results will be proved in expectation and in deviation .",
    "although results in expectation can usually be derived by integrating the results in deviation , we will directly prove results in expectation in the main part of the paper .",
    "proofs of standard technical propositions and some results in deviation will be moved to the appendix .",
    "this paper is organized as follows :    * in section @xmath18 we introduce the notation , definitions and previous results . * in section @xmath19 we introduce and further develop the machinery , based on the combination of shifted empirical processes @xcite and offset rademacher complexities @xcite .",
    "we also obtain a new upper bound on the error rate of empirical risk minimization in the realizable case , involving the star number and the growth function , which refines a recent result of hanneke @xcite in some cases ; this bound is a strict improvement over the distribution - free bound implied by the result of gin and koltchinskii in the realizable case . *",
    "section @xmath20 is devoted to an upper bound in terms of fixed point of global metric entropy .",
    "although it gives a fast convergence rate @xmath8 , it involves only a global information about the class .",
    "thus , this bound is suboptimal in some interesting cases , as are the other bounds in the literature based solely on global complexities for the class .",
    "we include the proof nevertheless , as it cleanly illustrates certain aspects of our approach ; for simplicity , we only present this result in the realizable case . *",
    "section @xmath21 contains our main results . in this section",
    "we introduce the local empirical entropy and prove that fixed points of local empirical entropy control the complexity of erm under bounded noise . *",
    "section @xmath22 is devoted to a novel lower bound in terms of fixed points of local empirical entropy under mild regularity assumptions . *",
    "section @xmath23 contains examples of values of fixed points for some standard classes . *",
    "section @xmath24 is devoted to discussions and some related general results .",
    "specifically , we prove that bounds based on our complexity measure are always not worse than the bounds based on local rademacher complexities .",
    "we define the _ instance space _ @xmath25 and the _ label space _ @xmath26 .",
    "we assume that the set @xmath27 is equipped with some @xmath28-algebra and a probability measure @xmath29 on measurable subsets is defined .",
    "we also assume that we are given a set of classifiers @xmath3 ; these are measurable functions with respect to the introduced @xmath28-algebra , mapping @xmath25 to @xmath30",
    ". we may always decompose @xmath31 .",
    "the risk of a classifier @xmath32 is its probability of error , denoted @xmath33 .",
    "it is known that among all functions the _ bayes classifier _ @xmath34 , where @xmath35 $ ] , minimizes the risk @xcite .",
    "symbol @xmath36 will denote minimum of two real numbers , @xmath37 will denote maximum of two real numbers and @xmath38 $ ] will denote an indicator of the event @xmath39 . for any subset @xmath40 define the _ region of disagreement _ as @xmath41 .",
    "we will also consider abstract real - valued functional classes , which will usually be denoted by @xmath42 .",
    "we will slightly abuse the notation and by @xmath43 always mean truncated logarithm : @xmath44 .",
    "the notation @xmath45 or @xmath46 will mean that for some universal constant @xmath47 it holds that @xmath48 for all @xmath49 .",
    "similarly , we introduce @xmath50 to be equivalent to @xmath51 .    a _ learner _ observes @xmath52 , an i.i.d . training sample from an unknown distribution @xmath29 . also denote @xmath53 .",
    "by @xmath54 we will denote expectation with respect to the empirical measure ( empirical mean ) induced by these samples .",
    "_ empirical risk minimization _ ( erm ) refers to any learning algorithm with the following property : given a training sample , it outputs a classifier @xmath55 that minimizes @xmath56 $ ] among all @xmath57 .",
    "depending on context we will usually refer to @xmath55 as an empirical risk minimizer and use the same abbreviation . at times",
    "we also refer to a _ ghost sample _ , which is another @xmath1 i.i.d .",
    "@xmath29-distributed samples , independent of the training sample , and we denote by @xmath58 the empirical mean with respect to the ghost sample .",
    "we say a set @xmath59 is shattered by @xmath3 if there are @xmath60 distinct classifications of @xmath61 realized by classifiers in @xmath3 .",
    "the _ vc dimension _ of @xmath3 is the largest integer @xmath7 such that there exists a set @xmath62 shattered by @xmath3 @xcite .",
    "we define the _ growth function _ @xmath63 as the maximum possible number of different classifications of a set of @xmath1 points realized by classifiers in @xmath3 ( maximized over the choice of the @xmath1 points ) . throughout the paper @xmath1",
    "will always denote the size of the training sample , @xmath7 will denote the vc dimension , and @xmath55 will denote the output of any erm algorithm . to focus on nontrivial scenarios , we will always suppose @xmath64 .",
    "@xmath65 is said to satisfy massart s bounded noise condition if @xmath66 and for some @xmath67 $ ] it holds @xmath68 with probability @xmath69 .",
    "this constant @xmath11 is referred to as the _ margin parameter_.    for any @xmath3 , the set of all corresponding distributions satisfying massart s bounded noise condition will be denoted by @xmath70 .",
    "the case @xmath71 corresponds to the so - called _ realizable case _",
    ", where @xmath72 almost surely , and @xmath73 corresponds to a well - specified _ agnostic _ case .",
    "the following result is classic @xcite .",
    "let @xmath3 be a class with vc - dimension @xmath7 .",
    "for any empirical risk minimizer @xmath55 over @xmath1 samples , for any @xmath74 , with probability at least @xmath6 , @xmath75 moreover , the following lower bound exists for an output @xmath76 of _ any _ algorithm based on @xmath1 samples : there exists @xmath74 such that , with probability greater than @xmath77 , @xmath78 thus we know that the vc - dimension is the right complexity measure for empirical risk minimization , and indeed for optimal learning , when no restrictions are made on the probability distribution .",
    "interestingly , this is not generally the case when @xmath79 . in this paper",
    ", we find this yet unknown essentially correct complexity measure , when @xmath11 is bounded away from @xmath80 and @xmath69 . but",
    "first , we review a refinement to the above bound for the case @xmath79 , due to gin and koltchinskii @xcite .",
    "specifically , consider the following definition .",
    "[ def : alexander ] for @xmath81 fix a set @xmath82 . for @xmath83 $ ]",
    "define @xmath84    this quantity ( essentially , instead taking @xmath85 directly .",
    "however , the results were proven under a very restrictive monotonicity assumption .",
    "taking the supremum allows one to dispense with such assumptions .",
    "] ) was introduced to the empirical processes literature by alexander @xcite , and is referred to as _",
    "alexander s capacity _ by gin and koltchinskii @xcite .",
    "the same quantity appeared independently in the literature on active learning , where it is referred to as the _ disagreement coefficient _ @xcite .",
    "@xmath86 is a distribution - dependent measure of the diversity of ways in which classifiers in a relatively small vicinity of @xmath87 can disagree with @xmath87 .",
    "gin and koltchinskii @xcite gave the following upper bound .",
    "let @xmath3 be a class of vc dimension @xmath7 , and @xmath55 the classifier produced by an erm based on @xmath1 training samples .",
    "for any probability measure @xmath88 , with probability at least @xmath89 , @xmath90 this bound is the best simple , easily calculable upper bound known so far for erm in the case of binary classification under massart s bounded noise condition .",
    "the proof of this bound is based on the analysis of the localized rademacher processes .",
    "so we may also consider this result as the best relaxation of the local rademacher analysis .",
    "recently , hanneke and yang @xcite introduced a distribution - free complexity measure , called the _ star number _ , which perfectly captures the worst case value for alexander s capacity .",
    "it is defined as follows .",
    "the star number @xmath15 is the largest integer such that there exist distinct @xmath91 and @xmath92 such that , for all @xmath93 , @xmath94 .    just like alexander s capacity ,",
    "the star number measures how diverse the disagreements with @xmath95 can be , in a small vicinity of @xmath95 . in terms of the one - inclusion graph studied by haussler , littlestone , and warmuth @xcite",
    ", the star number may be described as the maximum possible degree in the data - induced one - inclusion graph .",
    "it is easy to see that , for any class of vc dimension @xmath7 , it always holds that @xmath96 , but the difference may be as large as inifinte .",
    "we refer to @xcite for examples and further discussions related to the star number .",
    "one of the most interesting results about this value is its connection with the worst case of alexander s capacity .",
    "the paper of hanneke and yang contains the following equality @xmath97 as noted by hanneke @xcite , an immediate corollary of this and is that , for any @xmath98 , with probability at least @xmath89 , @xmath99 in particular , in the realizable case ( when @xmath71 ) , with probability at least @xmath89 , @xmath100 since @xmath15 controls alexander s capacity with equality , there is no room for any kind of improvement using the bound of gin and koltchinskii if we consider distribution - free upper bounds .",
    "however , the above bound for the realizable case has recently been refined by hanneke @xcite in the realizable case , establishing that for any @xmath101 , with probability at least @xmath89 , @xmath102 even this slight improvement indicates the suboptimality of the bound . in this paper",
    "we will further refine this bound and discuss in details the following fact : the pair @xmath103 alone is not a right complexity measure for the vc classes when @xmath11 is bounded away from zero .",
    "given a function class @xmath42 mapping @xmath104 to @xmath105 , one may consider the supremum of the empirical process : @xmath106 this quantity plays an important role in statistical learning theory . since the pioneering paper of vapnik and chervonenkis @xcite , the analysis of learning algorithms is usually performed by the tight uniform control over the process @xmath107 for a special class of functions .",
    "the behaviour of the supremum of this empirical process is tightly connected with the supremum of the so - called _ rademacher process _ : @xmath108 where @xmath109 denotes @xmath110 , @xmath111 are independent rademacher variables taking values @xmath112 with equal probabilities , and @xmath113 denoted the expectation over the @xmath114 random variables ( conditioning on the @xmath115 variables ) .",
    "this approach , however , usually leads to suboptimal upper and lower bounds that are not capturing both improved learning rates due to the noise conditions and the localization of the complexity term .",
    "we will instead consider different quantities , so - called _ shifted empirical _",
    "processes , introduced by lecu and mitchell @xcite . given @xmath116",
    ", we consider @xmath117 the second important quantity is an expected supremum of the _ offset rademacher process _ , introduced recently by liang , rakhlin , and sridharan @xcite : @xmath118 the last quantity was introduced for the analysis of a specific aggregation procedure under the square loss and so far has not been related to a shifted process . in this paper , we will investigate some new properties of these processes and show how they may be applied in the classification framework .",
    "the following short lemma and appears in a more general form in @xcite ( lemma @xmath21 ) .",
    "[ expectmax ] let @xmath119 be a finite set of binary vectors of cardinality @xmath120 .",
    "then for any @xmath116 , @xmath121    compare this result with an upper bound for rademacher averages @xcite where the best rate is of order @xmath122 .",
    "the next simple lemma is a new symmetrization lemma for the shifted process in expectation .",
    "[ symmetrization ] let @xmath42 be a functional class and @xmath123 an absolute constant .",
    "then @xmath124    proof technique is inspired by the proof of theorem @xmath19 in @xcite .",
    "using standard symmetrization trick and jensen s inequality we have @xmath125    interestingly , by setting @xmath126 we immediately obtain the standard symmetrization inequality .",
    "the next lemma , which provides a novel symmetrization tool for the shifted processes in deviation requires the following definition .",
    "this result is motivated by existing classic symmetrization results @xcite , but the proof technique is adapted for our shifted case .",
    "we say that a functional class @xmath42 is a @xmath127-_bernstein class _ if for any @xmath128 we have @xmath129 .",
    "the parameter @xmath130 is called the _",
    "bernstein parametr _ and @xmath131 the _",
    "bernstein constant_.    [ symmetrizationdev ] let @xmath42 be a @xmath132-bernstein class , such that for all @xmath128 we have @xmath133 and @xmath134 .",
    "fix constants @xmath135 , such that @xmath136 and @xmath137 .",
    "then if @xmath138 @xmath139    given a random sample let @xmath140 be the function achieving the supremum .",
    "@xmath141{\\mathbbm{1}}[(p - ( 1 + c_{2})p'_{n})\\tilde{g } < t/2 ] \\le { \\mathbbm{1}}[((1 + c_{2})p'_n - ( 1 + c_{1})p_n)\\tilde{g } > t/2].\\ ] ] taking expectation with respect to the ghost sample we have @xmath141p'[(p - ( 1 + c_{2})p'_{n})\\tilde{g } < t/2 ] \\le p'[((1 + c_{2})p'_n - ( 1 + c_{1})p_n)\\tilde{g } > t/2].\\ ] ]",
    "we further have @xmath142 =   p'\\bigl[(p - p'_{n})\\tilde{g } \\ge \\frac{t/2 + c_{2}p\\tilde{g}}{1 + c_{2}}\\bigr].\\ ] ] using the bernstein bound @xcite we have @xmath143 \\le \\exp\\left(-\\frac{n}{2}\\left(\\frac{t/2 + c_{2}p\\tilde{g}}{1 + c_2}\\right)^{2}/\\left(p\\tilde{g}^2 + \\frac{1}{3}\\frac{t/2 + c_{2}p\\tilde{g}}{1 + c_2}\\right)\\right).\\ ] ] then using bernstein condition we have @xmath144 a simple analysis shows that if @xmath145 , then @xmath146 finally , we have that if @xmath147 , then @xmath148 \\ge \\frac{1}{2}$ ] . taking an expectation with respect to the initial sample finishes the proof .    let @xmath15 be the star number of a class of binary classifiers @xmath3 .",
    "hanneke @xcite recently proved that in this case @xmath149 where @xmath150 = 0\\}$ ] is the _ version space_. that work also established a similar result holding with high probability : with probability at least @xmath6 , @xmath151 this result means that if the star number is bounded , then in the realizable case the expected measure of disagreement of the version space has order @xmath152 , where @xmath1 is the size of the learning sample .",
    "a reader familiar with the work of haussler , littlestone , and warmuth @xcite may remember that the performance of some learning algorithms can be controlled by the maximum possible out - degree in a corresponding orientation of the data - induced _ one - inclusion graph _ , and that there exists such an orientation with maximum out - degree at most the vc dimension .",
    "the above result has relations to this , except that instead of the out - degree of an oriented one - inclusion graph , the measure of the region of disagreement is controlled by the largest possible value of the ( undirected ) degree of the data - induced one - inclusion graph .",
    "since both the erm and the optimal classifier are contained in @xmath153 in the realizable case , one consequence of the above results is that , when @xmath154 , erm achieves the optimal order @xmath155 in its error rate .",
    "even more interesting , @xcite used the bound in a more subtle way to show that erm in the realizable case obtains expected error rate of order @xmath156 , and with probability at least @xmath89 has error rate bounded as in : i.e. , of order @xmath157 . via a more sophisticated variant of this argument , we obtain the following theorem , which is one of the novel contributions of this work .",
    "it offers interesting general refinements over which we discuss below .",
    "its proof is included in the appendix .",
    "[ startheorem ] let @xmath15 be the star number of a class of binary classifiers @xmath3 . in the realizable case",
    ", any erm @xmath55 has @xmath158 moreover , with probability at least @xmath6 , @xmath159    we may prove ( due to vapnik and chervonenkis s bound on the growth function @xcite ) that this inequality is an alternative way of recovering the upper bound @xmath160 proven by @xcite for erm , which is itself a refinement of the distribution - free bound implied by gin and koltchinskii s bound in the realizable case .",
    "[ twoclassesexample ] theorem [ startheorem ] yields simple examples showing the gaps in the distribution - free bound in the realizable case . specifically , suppose @xmath161 , define class @xmath162 as the classifiers on this @xmath25 with at most @xmath7 points classified @xmath69 , and class @xmath163 as the classifiers having at most @xmath164 points classified @xmath69 among @xmath165 and at most one point classified @xmath69 among @xmath166 . for both @xmath167 and @xmath168 ,",
    "the vc dimension is @xmath7 and the star number is @xmath15 .",
    "however , for @xmath167 theorem  [ startheorem ] gives a bound of order @xmath160 , but for @xmath163 it gives a smaller bound of order @xmath169 . in both cases ,",
    "these are known to be tight characterizations of erm in the realizable case @xcite .",
    "it should be noted , however , that one can also construct examples where theorem  [ startheorem ] is itself not tight .",
    "the main aim of this section is to give a simple bound in terms of a fixed point of global packings .",
    "we will further significantly improve this result in the next section , and therefore for simplicity here we will consider only the realizable case .",
    "we note that a similar result may be derived from classic results on ratio type empirical processes ( see section @xmath170 of @xcite ) .",
    "we include the details of our proof here anyway , as it also serves to illustrate certain aspects of our approach in simplified form .",
    "given a set of @xmath1 points we define for any two @xmath171 where @xmath172 .",
    "we further introduce @xmath173 where @xmath174 denotes the size of a maximal @xmath175-packing of @xmath176 under @xmath177 distance ( for the given @xmath178 points ) and @xmath179 is a set of projections of @xmath3 on @xmath180 .    in many statistical frameworks optimal rates",
    "are usually obtained when one carefully balances the radius and the logarithm of a packing number with respect to the same radius ( for example , yang and barron @xcite ) .",
    "it will be shown that in our bounds it is natural to choose @xmath181 such that @xmath182 for some @xmath183 $ ] .",
    "so we define @xmath184 the value @xmath185 will be referred to as a _",
    "fixed point of empirical entropy_. when @xmath3 is clear from the context , we simply write @xmath186 instead of @xmath185 . note that @xmath187 is a well - defined strictly positive - valued quantity , since we are using the truncated logarithm .",
    "[ coveringbound ] fix any function class @xmath3 ; denote its vc dimension @xmath7 . if @xmath188 ( realizable case ) , then for any erm @xmath55 , @xmath189 moreover with probability at least @xmath6 , @xmath190 and @xmath191    to prove this proposition we need a technical lemma , which may be considered as a modification of lemma @xmath22 in @xcite .",
    "[ expectmaxnew ] let @xmath42 be a set of functions taking binary values , and let @xmath183 $ ] be a constant .",
    "let @xmath192 be independent rademacher random variables .",
    "then @xmath193    given @xmath194 , let @xmath195 denote the set of binary vectors corresponding to the values of functions in @xmath42 .",
    "as above , for a fixed @xmath181 and fixed minimal @xmath181-covering subset @xmath196 , for each @xmath197 , @xmath198 will denote the closest vector to @xmath199 in @xmath200 .",
    "first we follow the decomposition proposed by liang , rakhlin , and sridharan @xcite : @xmath201 since @xmath198 is within hamming distance @xmath181 of @xmath199 , we know @xmath202 , and therefore the second summand in the above expression is at most @xmath203 .",
    "the third summand is upper bounded by @xmath204 by lemma [ expectmax ] and the first term is upper bounded by @xmath181 by the @xmath181-cover property of the @xmath198 vectors .",
    "then we use the standard relation that the size a of minimal covering is less than or equal to the size of a maximal packing @xcite to conclude that @xmath205 by choosing @xmath206 we have @xmath207    first we introduce a _ loss class _",
    "\\text{for}\\ f \\in { \\mathcal{f}}\\}$ ] .",
    "let @xmath55 be any erm and @xmath209 be a corresponding function in the loss class @xmath210 .",
    "we obviously have @xmath211 and @xmath212 . then for any @xmath116 @xmath213 by lemma [ symmetrization ] we have @xmath214 applying the lemma [ expectmaxnew ] and fixing @xmath215 we finish the proof of the bound on the expectation .",
    "[ threshold ] consider the class of threshold classifiers , that is @xmath216 - 1 : t \\in \\mathbb{r}\\}$ ] .",
    "using the definition of the star number it easy to see that it is equal to @xmath18 in this case and theorem [ startheorem ] gives an optimal @xmath8 upper bound for erm . at the same time the worst case packing numbers @xmath217 are of order @xmath218 .",
    "a simple analysis of the fixed point gives us @xmath219 and thus proposition [ coveringbound ] will give us suboptimal @xmath220 distribution free upper bound .",
    "although we captured that the rate is faster than @xmath9 , our analysis of the complexity term is suboptimal .",
    "the next section discusses a correction for this , which also yields optimal rates under moderate bounded noise in general .",
    "this section presents our main result . toward this end",
    ", we introduce a new complexity measure : the _ worst - case local empirical packing numbers_. given a set of @xmath1 points we fix some @xmath57 and construct a hamming ball of the radius @xmath181 .",
    "so , @xmath221 and define @xmath222 where once again @xmath174 denotes the size of a maximal @xmath175-packing of @xmath176 under @xmath177 distance ( for the given @xmath178 points ) . fix any @xmath223 $ ] and define @xmath224 when @xmath3 is clear from the context , we simply write @xmath225 instead of @xmath226 . the quantity",
    "@xmath227 defines the _ fixed point of a local empirical entropy_. we note that , because @xmath228 in this work , when @xmath229 the set on the right in this definition is finite and nonempty , so that @xmath227 is a well - defined strictly - positive integer .",
    "indeed , for any @xmath230 $ ] , the value @xmath231 satisfies @xmath232 , so that ( because @xmath233 is the truncated logarithm ) this @xmath181 is contained in the set ; in particular , this implies @xmath234 always .",
    "the next theorem is the main upper bound of this paper .",
    "[ mainupperbound ] fix any function class @xmath3 ; denote its vc dimension @xmath7 and star number @xmath15 . fix any",
    "@xmath235 $ ] .",
    "if @xmath88 , then for any erm @xmath55 , @xmath236 also , with probability at least @xmath89 , @xmath237 moreover @xmath238 where @xmath7 is the vc dimension of @xmath3 and @xmath15 is its star number .",
    "our complexity term is not worse than the distribution - free upper bound implied by the bound of gin and koltchinskii when @xmath11 is bounded from @xmath80 by a constant . in the last section we will discuss potential suboptimality when @xmath11 is small , due to the term @xmath239 in .",
    "another interesting property is that the bounds and involve neither the vc dimension nor the star number explicitly . at the same time",
    "one can control the complexity term with both of them from below and above .",
    "for any given @xmath57 , denote @xmath240 - { \\mathbbm{1}}[f^{*}(x)\\neq y]$ ] .",
    "consider the _ excess loss class _ @xmath241 , and the previously introduced class @xmath208\\ \\text{for}\\ f \\in { \\mathcal{f}}\\}$ ] , which may be interpreted as an excess loss class in the realizable case .",
    "the following properties are well known .    1 .   for any @xmath242 it holds @xmath243 = \\frac{1}{2}|f(x ) - f^{*}(x)| = \\frac{1}{4}(f(x ) - f^{*}(x))^2 $ ] .",
    "2 .   for any @xmath242 it holds @xmath244 .",
    "3 .   for any @xmath88",
    "the class @xmath245 is a @xmath246-bernstein class @xcite and @xmath247 @xcite .",
    "[ contraction ] let @xmath248 be an excess loss class associated with a given class @xmath3 , and fix any @xmath67 $ ] .",
    "for any @xmath183 $ ] and any distribution @xmath88 we have @xmath249 where @xmath250 are random variables conditionally independent given @xmath194 , with @xmath251 , and with @xmath252 = 0 $ ] and @xmath253 \\le \\exp(\\frac{\\lambda^{2}}{2})$ ] for all @xmath254 .",
    "first we notice that any @xmath255 may be defined by some @xmath57 .",
    "@xmath256 now consider the term @xmath257 . denoting @xmath258 ( an @xmath259-dependent random variable ) , we know that @xmath260 almost surely . furthermore , the event that @xmath261 has conditional probability ( given @xmath259 ) equal @xmath262 , and on this event we have @xmath263 .",
    "similarly , the event that @xmath264 occurs with conditional probability ( given @xmath259 ) equal @xmath265 , and on this event we have @xmath266 .",
    "thus , defining @xmath267 - { \\mathbbm{1}}[f^{*}(x_i ) = y_i]$ ] , these @xmath268 random variables are conditionally independent given @xmath194 , with @xmath269 = 0 $ ] .",
    "in particular , if @xmath270 for all @xmath271 , these are rademacher random variables , while if @xmath272 these random variables are equal to @xmath80 with probability @xmath69 . now note that , by the above reasoning about these events , @xmath273 therefore , denoting @xmath274 ( which are also conditionally independent over @xmath271 given @xmath275 ) and using the fact that @xmath276 almost surely , we have @xmath277 finally , because @xmath111 and @xmath278 both have zero conditional mean , so does @xmath279 , and since we also have @xmath280 , hoeffding s lemma ( @xcite lemma 8.1 ) implies @xmath281 \\le \\exp(25\\lambda^{2}/2)$ ]",
    ". the lemma easily follows , taking @xmath282 .    [ localization ]",
    "let @xmath42 be a set of functions taking binary values , containing the zero function , and let @xmath283 $ ] be a constant .",
    "let @xmath284 be any random variables conditionally independent given @xmath194 , with @xmath251 , and with @xmath252 = 0 $ ] and @xmath285 \\leq \\exp(\\frac{\\lambda^2}{2})$ ] for all @xmath254 .",
    "then @xmath286    this proof is deferred to the appendix .",
    "let @xmath55 be an erm and @xmath209 be a corresponding function in the excess loss class @xmath248 .",
    "we obviously have @xmath287 and @xmath288 .",
    "then for any @xmath116 , @xmath289 now using the symmetrization lemma ( lemma [ symmetrization ] ) we have @xmath290 applying the contraction lemma ( lemma [ contraction ] ) @xmath291 we are ready to apply the localization lemma ( lemma [ localization ] ) .",
    "the conditions on the @xmath292 variables required for lemma  [ localization ] are supplied by lemma  [ contraction ] , and all functions in @xmath293 take only binary values .",
    "thus , for a fixed @xmath294 , @xmath295 the proof of the deviation bound is analogous , and is presented in the appendix .",
    "the claimed bounds on @xmath296 are established in proposition  [ fixedcontrol ] below .",
    "the following proposition finishes the proof of theorem [ mainupperbound ] .",
    "[ fixedcontrol ]",
    "let @xmath7 be the vc - dimension and @xmath15 be the star number of @xmath3 .",
    "for any @xmath297 $ ] , it holds @xmath298    the first part of the proof closely follows the proof of theorem @xmath299 in @xcite , with slight modifications , to arrive at an upper bound on @xmath300 .",
    "the suprema in the definition of local empirical entropy are achieved at some set @xmath180 , some function @xmath57 , and some @xmath301 $ ] .",
    "letting @xmath302 , denote by @xmath303 the maximal @xmath304-packing ( under @xmath177 ) of @xmath305 , so that @xmath306 . also introduce a uniform probability measure @xmath307 on @xmath308 and fix @xmath309 .",
    "let @xmath310 be @xmath311 independent @xmath307-distributed random variables , and let @xmath39 denote the event that , for all @xmath312 with @xmath313 , there exists an @xmath314 such that @xmath315 .",
    "for a given pair of distinct functions @xmath316 , they disagree on some @xmath259 with probability @xmath317 using a union bound and summing over all possible unordered pairs @xmath318 will give us that @xmath319 . on the event @xmath39 ,",
    "functions in @xmath320 realize distinct classifications of @xmath310 . for any @xmath321 all classifiers in @xmath320 agree .",
    "thus , @xmath322 is bounded by the number of different classifications @xmath323 realized by classifiers in @xmath3 . by the multiplicative chernoff bound , on an event @xmath131 with @xmath324 we have @xmath325 using the definition of @xmath326 ( definition [ def : alexander ] ) we have @xmath327 with probability at least @xmath328 , @xmath329 using the union bound , we have that with probability greater than zero there exists a sequence of at most @xmath330 elements , such that all functions in @xmath303 classify this sequence distinctly . by the vapnik and chervonenkis lemma",
    ", we therefore have that @xmath331 using corollary @xmath332 from @xcite we have @xmath333 using @xmath334 ( theorem @xmath335 in @xcite ) we finally have @xmath336 now we upper bound @xmath337 , knowing that @xmath338 we obviously have @xmath339 . for @xmath340 we have @xmath341 , but @xmath342 if @xmath343 .",
    "finally , we have @xmath344 now we prove the lower bound . from established above , we know that @xmath345 is , up to an absolute constant , a distribution - free upper bound for @xmath346 , holding for all erm learners @xmath55",
    ". then any lower bound on @xmath347 holding for any erm learner is also a lower bound for @xmath345 . in particular , it is known @xcite that for any learning procedure @xmath76 , if @xmath348 , then @xmath349 , while if @xmath350 then @xmath351 .",
    "furthermore , in the particular case of erm , @xcite proves that any upper bound on @xmath352 holding for all erm learners @xmath55 must have size , up to an absolute constant , at least @xmath353 .",
    "together , these lower bounds imply @xmath354 .",
    "in this section we prove that under massart s bounded noise condition , fixed points of the local empirical entropy appear in minimax lower bounds .",
    "results are in expectation and generally use classic lower bound techniques from the literature @xcite , previously used only for specific classes .",
    "we will need the following definition , which will be motivated below .",
    "[ pseudoconvex ] fix a class of classifiers @xmath3 .",
    "assume that there exists a positive constant @xmath355 such that for any @xmath120 the supremum with respect to the radius in @xmath356 is achieved at some @xmath357 .",
    "this class will be referred to as @xmath294-_pseudoconvex_.    [ newlowerbound ] let @xmath76 be the output of any learning algorithm . fix any @xmath358-pseudoconvex class @xmath3 and any @xmath11 satisfying @xmath359 .",
    "then there exists a @xmath88 such that @xmath360    conditions involving the constant @xmath358 can be relaxed in different ways",
    ". it will be clear from our proof that we may remove the pseudoconvexity assumptions by redefining the local empirical entropy by removing the supremum with respect to the radius .",
    "alternatively one can remove the supremum by introducing certain monotonicity assumptions .",
    "we note that such assumptions were used implicitly in previous papers @xcite . in both relaxations",
    "our lower bound holds with @xmath361 .",
    "moreover , the bound is valid for an arbitrary class @xmath3 as we may always consider @xmath362 instead of @xmath358 , which is a minimal natural number satisfying @xmath363 .",
    "finally , we note that these monotonicity problems do not appear for convex classes , as noted by mendelson in @xcite .",
    "this is our motivation for the name of the condition in definition [ pseudoconvex ] : local entropy of the class has almost the same monotonicity properties as in the convex case . in the next section",
    "we will present examples of natural pseudoconvex classes .",
    "the next lemma is given in @xcite ( corollary @xmath364 ) .",
    "[ birgeslemma ] let @xmath365 be a finite family of distributions defined on the same measurable space and @xmath366 be a family of disjoint events .",
    "then @xmath367    first we consider the value @xmath368 .",
    "recall that the definition of this value considers suprema over @xmath57 and over @xmath120-element subsets of @xmath369 . without loss of generality",
    "we assume that these suprema are achieved at some classifier @xmath370 , some @xmath371 $ ] and at some particular set @xmath372 .",
    "let @xmath373 define the number of copies of @xmath374 in @xmath375 .",
    "we define @xmath376 .",
    "if all elements are distinct this measure is just a uniform measure on @xmath375 .",
    "we introduce a natural parametrization : any classifier is represented by an @xmath120-dimensional binary vector and two vectors ( for classifiers @xmath377 ) disagree only on a set corresponding to @xmath378 .",
    "the set of binary vectors corresponding to classifiers in @xmath3 will be denoted by @xmath379 . for a given binary vector @xmath380 define @xmath381 , where @xmath382 .",
    "let @xmath383 denote the classifier @xmath76 produced by the learning algorithm when @xmath384 is the data distribution , and let @xmath385 denote the binary vector corresponding to @xmath383 ; thus , @xmath385 is a random vector , which depends on the parameter @xmath380 only through the @xmath1 data points having distribution @xmath384 .",
    "it is known @xcite that @xmath386 | \\tilde{f } ) \\ge hp((x , y ) : \\tilde{f}(x ) \\neq f^{*}(x))$ ] , when @xmath88 .",
    "furthermore , when @xmath387 is the data distribution , we have @xmath388 .",
    "thus , we have @xmath389    let @xmath390 be the binary vector in @xmath379 corresponding to the classifier @xmath391 defined above , and fix a maximal subset @xmath392 satisfying the properties that for any @xmath393 we have @xmath394 and for any two @xmath395 we have @xmath396 .",
    "next , define @xmath397 as the minimizer of @xmath398 among @xmath399 .",
    "in particular , if @xmath400 , we have @xmath401 , so that @xmath402 .",
    "therefore , @xmath403    recalling that @xmath397 is a deterministic function of @xmath76 , which itself is a function of the @xmath1 data points , we may define disjoint subsets @xmath404 of @xmath405 , for @xmath400 , where @xmath404 corresponds to the collection of data sets that would yield @xmath406 .",
    "now , from markov s inequality and the fact that the vectors in @xmath399 are @xmath407-separated , we have @xmath408 .",
    "thus we have that @xmath409 we are interested in using lemma  [ birgeslemma ] to upper - bound @xmath410 . toward this end ,",
    "note that for any @xmath395 , standard calculations show that @xmath411 because for @xmath412 we have @xmath413 , it holds that @xmath414 .",
    "furthermore , for any @xmath415 we have @xmath416 .",
    "therefore , @xmath417 thus , by lemma [ birgeslemma ] , @xmath418 noting that @xmath419 , choosing @xmath420 yields @xmath421 so that the right hand side of is @xmath422 .",
    "altogether , we have that for @xmath423 , @xmath424 the term @xmath425 for @xmath426 is a part of the classic lower bound of @xcite and .",
    "the following observation is an important consequence of our analysis .",
    "[ coroloptimal ] consider a @xmath358-pseudoconvex class @xmath3 .",
    "let @xmath427 . then if the margin parameter @xmath11 is such that @xmath428 , then for any vc class @xmath3 the erm upper bound and the lower bound match up to the constant factors ( also appearing possibly in the argument of the fixed point ) , which may depend only on @xmath429 , @xmath430 and @xmath358 .",
    "it is known @xcite that erm is minimax optimal up to constant factors if @xmath431 .",
    "interestingly , our corollary is certainly not valid for @xmath432 .",
    "the optimal bound in the realizable case is of order @xmath433 @xcite , but erm can not generally have this convergence rate in the realizable case @xcite .",
    "this fact is perfectly reflected in our lower bound . when @xmath11 is close to @xmath69 the term @xmath434 disappears and we have a classic @xmath435 lower bound from @xcite .",
    "in this section we provide two examples of exact estimation of fixed points of local empirical entropies .",
    "first we consider threshold classifiers , introduced in example  [ threshold ] .",
    "for this particular class , @xmath436 and @xmath437 . from theorem [ mainupperbound ]",
    "we have @xmath438 , and explicit calculation for this special class reveals @xmath439 .",
    "in particular , in the realizable case @xmath440 .",
    "another example will be a class of linear separators in @xmath441 for @xmath442 .",
    "this class is known to have vc dimension @xmath443 .",
    "it is easy to verify that for this particular class @xmath444 @xcite .",
    "[ linearsep ] for the set @xmath3 of linear separators in @xmath445 , if @xmath446 , then for any @xmath426 @xmath447 in particular , @xmath448 .",
    "the upper bound follows directly from the theorem [ mainupperbound ] .",
    "at first we select a special set of points @xmath449 .",
    "it is known ( theorem @xmath450 in @xcite ) that in @xmath445 there exists a so called _ cyclic polytope _ with @xmath1 vertices , such that it has exactly @xmath451   @xmath452-dimensional faces for any @xmath453 .",
    "we choose @xmath454 , such that @xmath455 is a vertex of the cyclic polytope .",
    "we fix any linear separator @xmath456 such that all @xmath457 are in the same half - space with respect to this linear separator . without loss of generality",
    "we may assume that @xmath458 .",
    "in this notation using the property of cyclic polytopes we see that @xmath3 contains all classifiers with at most @xmath459 ones .",
    "we denote this set by @xmath460 .",
    "analysis of this particular set by massart and ndlec ( theorem @xmath21 in @xcite ) gives a @xmath461 lower bound for @xmath462 provided that @xmath426 . from theorem [ mainupperbound ] we know that this lower bound is also a lower bound for @xmath463 .",
    "thus @xmath464 .",
    "simultaneously , we have @xmath465 .",
    "so , it is enough to lower bound @xmath466 , which may be derived as a lower bound for erm in the realizable case .",
    "it is known ( theorem @xmath22 in @xcite , or theorem @xmath21 in @xcite ) that for this particular class @xmath460 in the realizable case there exists erm such that with probability at least @xmath328 we have @xmath467 .",
    "this implies that @xmath468 and thus @xmath469 .",
    "summerizing , we have @xmath470 .",
    "we finish the proof by noticing that @xmath471 .",
    "we note that the lower bound [ lowerbound ] may be applied for both classes .",
    "local entropies are well known in statistics since the early work of le cam @xcite . since",
    "then local metric entropies have appeared in minimax lower bounds @xcite and in the necessary and sufficient conditions for consistency of erm estimator in nonparametric regression @xcite .",
    "simultaneously , the upper bounds are usually given in terms of global entropies .",
    "interestingly , it is sometimes possible to recover optimal rates by considering only global packings @xcite .",
    "generally , empirical covering numbers of classes in statistics have two types of behaviour .",
    "there are _ parametric _ and _ vc - type _ classes where the logarithm of covering numbers scales as @xmath472 and expressive _ nonparametric classes _ where it scales as @xmath473 for some @xmath474 .",
    "it was proven in @xcite that for these expressive nonparametric classes local and global entropies are of the same order .",
    "thus for such classes localization of class does not give any significant improvement and minimax rates are usually obtained using only global entropies @xcite .",
    "the case of parametric and especially vc - type classes is more delicate and this paper is a first attempt to analyze the last tightly under bounded noise .",
    ", optimal rates were obtained in @xcite ] our results and examples show that localization of the class is usually needed for vc classes , but definitely not always .",
    "some parametric classes have the features of nonparametric classes : their local entropies are of the same order as their global entropies , and for them bounds in terms of global entropies are essentially optimal .",
    "it is not difficult to show that , in the proof of proposition [ linearsep ] , we gave an example of such a vc class @xmath460 .",
    "not surprisingly , massart and ndlec @xcite named this class _",
    "rich_. this class appears in almost all class - specific lower bounds @xcite , which are matched by global upper bounds .",
    "in contrast , there are still many interesting classes , for example , threshold classifiers , which are out of the scope of upper bounds based on global entropy",
    ".    we should note that a distribution - dependent local entropy has already appeared in the upper bounds in the classification literature under the name of the _ doubling dimension_. given a class of classifiers @xmath3 and a probability distribution @xmath475 , define the doubling dimension by @xmath476 where @xmath477 and @xmath478 is the @xmath175-covering number of @xmath42 with respect to the pseudo - metric @xmath479 .",
    "it was proved by bshouty , li , and long @xcite that in the realizable case , for any @xmath480 , if @xmath481 then with probability at least @xmath6 , for any erm @xmath55 we have @xmath482 . here",
    "it is easy to show that when considering the distribution - free setting , this bound is weaker than ours at least because it contains a square root of an extra logarithmic factor .",
    "the following simple inequality compares distribution - free doubling dimension and the local empirical entropy . for any @xmath484 ,",
    "@xmath485 to prove this inequality one may consider the uniform probability measure @xmath475 on the @xmath1 points maximizing the local packing number on the left hand side , in which case the pseudo - metric @xmath479 is merely @xmath486 times the hamming distance of the projections to these @xmath1 points .",
    "the constant @xmath18 appears simply due to the fact that empirical local entropies involve packing numbers while the doubling dimension involves covering numbers .",
    "bshouty , li , and long @xcite also study a non - erm distribution - dependent learning algorithm in the realizable case , and obtain an error rate guarantee essentially bounded by a fixed point @xmath487 , with probability at least @xmath89 . in light of",
    ", we see that in the worst case over distributions this is essentially no better than our theorem  [ mainupperbound ] ( with @xmath488 ) , which holds for the much - simpler learning algorithm erm .",
    "we also note that questions similar to ours have been considered recently by mendelson @xcite and by lecu and mendelson @xcite .",
    "both papers introduce distribution dependent fixed points of local entropies and show that in the convex regression setup for subgaussian classes they give optimal upper and lower bounds .",
    "however , the direct comparison with their results is problematic due to the fact that in the vc case we do not have convexity assumptions : they are replaced by noise assumptions and specifically used by our approach . moreover , since in the realizable case erm is not minimax optimal , it can be easily seen from our results that there may not exist a lower bound in terms of fixed points of the local empirical entropy in this case .",
    "we have compared our bound with some of the best known relaxations of the bounds based on local rademacher processes . however , the title of our paper demands also a direct comparison with the bounds based _ solely _ on local rademacher complexities . for this , we need the following result .",
    "let @xmath489 be a finite set such that for any @xmath490 if @xmath491 then @xmath492 for some @xmath493 and for any @xmath197 it holds @xmath494 for some @xmath495 .",
    "then @xmath496    for simplicity we will consider only the realizable case , and distribution - free setting .",
    "however we note that similar arguments will also work under bounded noise and general distributions @xmath475 .",
    "fix a sample @xmath454 . applying corollary @xmath497 from @xcite we have @xmath498 where @xmath499 is a fixed point of the local empirical rademacher complexity ,",
    "that is a solution of the following equality @xmath500 where @xmath501 denotes the _ star - hull _ of a class @xmath42 : that is , the class of functions @xmath502 , where @xmath128 and @xmath503 $ ] .",
    "since @xmath504 is star - shaped , it can be simply proven ( see appropriate discussions in @xcite ) that local empirical entropies are not increasing in its radius . using this fact together with it can be shown @xmath505 . from this",
    "it easily follows that @xmath506 . thus our bounds are not generally worse than the bounds based _ solely _ on local rademacher complexities .",
    "conceptually we are looking for fixed points of the right hand side of , while rademacher analysis works directly with the fixed points of the suprema of localized processes .",
    "there are still interesting questions and possible directions that are out of the scope of this paper :    1 .",
    "we are focusing on a distribution - free analysis . at the same time by just leaving the expectations with respect to the learning sample",
    "we may simply obtain a distribution - dependent version of theorem [ mainupperbound ] . recently",
    ", balcan and long @xcite have proven that for some special distributions @xmath475 , the class of homogenous linear separators admits faster rates of convergence of erm , compared to worst - case distributions .",
    "it may be interesting to generalize our results using distribution - dependent fixed points of the local empirical entropy ( based on random data , rather than worst - case data ) , and specifically to determine whether this yields rates as fast as @xcite under similar conditions on @xmath475 .",
    "2 .   our approach",
    "here makes use of shifted processes and offset rademacher processes , in place of explicit diameter - localization arguments such as used by @xcite .",
    "it seems a natural direction to develop a more general theory of this use to understand the limitations of the approach .",
    "for example , so far our analysis is specific to the well - specified case when @xmath66",
    ". it would be interesting to generalize our results to more general noise conditions and a miss - specified case .",
    "it is also interesting to refine our bounds in situations when @xmath11 is close to zero : i.e. , when the noise levels are high .",
    "it is known @xcite that when @xmath350 the control of rademacher processes based on the _ dudley integral _ @xcite give minimax optimal @xmath507 convergence rate .",
    "moreover , it is known that bounds based on just one covering are suboptimal in this case .",
    "if we fix @xmath508 , then the bound of gin and koltchinskii ( also based on the dudley integral ) will give us an optimal @xmath507 rate in expectation .",
    "simultaneously , we know that their bound is suboptimal when @xmath11 is close to @xmath69 .",
    "due to an extra term @xmath239 in our bound can guarantee only a suboptimal @xmath509 rate when @xmath508 , but we know that for many other values of @xmath11 our bound is significantly better .",
    "nonetheless , we believe that there is a transition , continuous in @xmath11 , from the dudley integral regime when @xmath350 to the regime when the local empirical entropy provides the optimal characterization of the rates obtained by erm .",
    "we have already discussed that erm may be suboptimal in the realizable case .",
    "thus , when considering minimax optimality there is a third regime , when we have almost no noise .",
    "however , since erm is such a natural and frequently - used method , it remains an interesting question to precisely characterize its risk .",
    "recall that the case when @xmath11 is bounded away from @xmath80 and @xmath69 is partially covered by our corollary [ coroloptimal ] .",
    "we hypothesize that in the realizable case ( and even in a more general regime when @xmath11 is close to @xmath69 ) our bound also characterizes the _ best possible _ bound on the risk of the worst - case choice of empirical risk minimizer @xmath55 , up to an absolute constant factor .",
    "it follows directly from our discussions that our hypothesis it true for the classes presented in section [ examples ] .",
    "partial analysis of the complexity of erm has recently been performed by hanneke @xcite .",
    "specifically , he finds that the correct characterization of the risk of erm is somewhere between the upper bounds , and a lower bound @xmath510 holding with probability greater than @xmath77 for a worst - case choice of @xmath98 ( and worst - case choice of erm ) .",
    "we know that in the realizable case , for the class presented in example [ twoclassesexample ] , the bound is matched . at the same time , for the class of linear separators presented in section [ examples ] , this lower bound is not tight .",
    "this , in particular , leads to the obvious conclusion that @xmath7 and @xmath15 are also not sufficient to fully characterize the risk of erm , even in the realizable case .",
    "00    _ k. s. alexander_. rates of growth and sample moduli for weighted empirical processes indexed by sets . probability theory and related fields , 75:379423 , 1987 . _ m. anthony , p. l. bartlett_. neural network learning : theoretical foundations . cambridge university press , 1999 .",
    "_ p. auer , r. ortner_. a new pac bound for intersection - closed concept classes .",
    "machine learning , 66(2 - 3 ) : 151163 , 2007 .",
    "balcan , p. m. long_. active and passive learning of linear separators under log - concave distributions . in proceedings of the 26th conference on learning theory , 2013 .",
    "_ p. l. bartlett , o. bousquet , s. mendelson_. local rademacher complexities .",
    "the annals of statistics , 33(4):14971537 , 08 , 2005 . _ p. l. bartlett , s. mendelson_. empirical minimization .",
    "probability theory related fields , 135(3):311334 , 2006 .",
    "_ s. boucheron , o. bousquet , g. lugosi_. theory of classification : a survey of recent advances .",
    "esaim : probability and statistics , 9:323375 , 2005 .",
    "_ s. boucheron , g. lugosi , p. massart_. concentration inequalities : a nonasymptotic theory of independence .",
    "cambridge , 2013 .",
    "_ n. h. bshouty , y. li , p. m. long_. using the doubling dimension to analyze the generalization of learning algorithms .",
    "journal of computer and system sciences , 2009 .",
    "_ l. devroye , g. lugosi_. combinatorial methods in density estimation .",
    "springer , new york , 2001 .",
    "_ l. devroye , l. gyrfi , g. lugosi_. a probabilistic theory of pattern recognition , volume 31 of applications of mathematics .",
    "verlag , new york , 1996 .",
    "dudley_. empirical processes . in ecole",
    "de probabilit de st .",
    "flour 1982 .",
    "lecture notes in mathematics 1097 , springer verlag , new york , 1984 .",
    "_ a. ehrenfeucht , d. haussler , m. kearns , l. valiant_. a general lower bound on the number of examples needed for learning . _ information and computation _ , 82(3):247261 , 1989 .",
    "_ h. edelsbrunner_. algorithms in combinatorial geometry .",
    "springer , berlin .",
    "_ e. gin , v. koltchinskii_. concentration inequalities and asymptotic results for ratio type empirical processes .",
    "the annals of probability , 34(3):11431216 , 2006 . _",
    "s. hanneke_. a bound on the label complexity of agnostic active learning . in proceedings of the 24th annual international conference on machine learning , 2007 .",
    "_ s. hanneke_. theory of disagreement - based active learning . foundations and trends in machine learning , 7 ( 2 - 3 ) : 131 - 309 , 2014 . _ s. hanneke , l. yang_. minimax analysis of active learning .",
    "journal of machine learning research , 16 ( 12 ) : 34873602 , 2015 . _ s. hanneke_. refined error bounds for several learning algorithms .",
    "http://arxiv.org/abs/1512.07146 , 2015 .",
    "_ s. hanneke_. the optimal sample complexity of pac learning .",
    "journal of machine learning research , 17 ( 38 ) : 1 - 15 , 2016 .",
    "_ d. haussler , n. littlestone , m. warmuth_. predicting @xmath511-functions on randomly drawn points .",
    "information and computation , 115:248292 , 1994 . _",
    "d. haussler_. sphere packing numbers for subsets of the boolean n - cube with bounded vapnik - chervonenkis dimension .",
    "theory ser .",
    "a , 69(2):217232 , 1995 . _ v. koltchinskii_. local rademacher complexities and oracle inequalities in risk minimization .",
    "annals of statistics , 34(6):25932656 , 2006 . _ v. koltchinskii_. oracle inequalities in empirical risk minimization and sparse recovery problems .",
    "flour lecture notes , 2011 . _ l. m. le cam_. convergence of estimates under dimensionality restrictions",
    "statist . 1 , 3853 , 1973 . _ g. lecu_. interplay between concentration , complexity and geometry in learning theory with applications to high dimensional data analysis .",
    "habilitation thesis , universit paris - est , 2011 .",
    "_ g. lecu , c. mitchell_. oracle inequalities for cross - validation type procedures . electronic journal of statistics , 6 , 18031837 , 2012 .",
    "_ g. lecu , s. mendelson_. learning subgaussian classes : upper and minimax bounds .",
    " http://arxiv.org/abs/1305.4825 , 2013 .",
    "_ t. liang , a. rakhlin , k. sridharan_. learning with square loss : localization through offset rademacher complexity .",
    "proceedings of the 28th conference on learning theory , 2015 . _",
    "p. massart_. concentration inequalties and model selection .",
    "ecole det de probabilits , saint flour .",
    "springer , new york , 2003 .",
    "_ p. massart , e. ndlec_. risk bounds for statistical learning .",
    "annals of statistics , 2006 . _ s. mendelson_. ` local ' vs. ` global ' parameters  breaking the gaussian complexity barrier .",
    "http://arxiv.org/abs/1504.02191 , 2015 .",
    "_ m. raginsky , a. rakhlin_. lower bounds for passive and active learning .",
    "advances in neural information processing systems 24 , nips , 2011 .",
    "_ a. rakhlin , k. sridharan , a. b. tsybakov_. empirical entropy , minimax regret and minimax risk .",
    "bernoulli , 2015  ( forthcoming ) .",
    "_ h. simon_. an almost optimal pac - algorithm .",
    "proceedings of the 28th conference on learning theory , pp . 15521563 , 2015 .",
    "_ m. talagrand_. sharper bounds for gaussian and empirical processes .",
    "the annals of probability , 22(1 ) : 2876 , 1994 . _ m. talagrand_. upper and lower bounds for stochastic processes .",
    "springer , berlin , vol .",
    "60 , 2014 .",
    "_ v. vapnik , a. chervonenkis_. on the uniform convergence of relative frequencies of events to their probabilities .",
    "ussr acad .",
    "181(4 ) , 781783 .",
    "english tranlation : soviet math . dokl . 9 , 915918 , 1968 . _ s. van de geer , m. wegkamp_. consistency for the least squares estimator in nonparametric regressionannals of statistics , vol .",
    "24 , no . 6 , 25132523 , 1996 .",
    "_ t. van erven , p. grnwald , n. mehta , m. reid , r. williamson_. fast rates in statistical and online learning .",
    "journal of machine learning research , 16 : 17931861 , 2015 . _",
    "m. vidyasagar_. learning and generalization with applications to neural networks .",
    "springer - verlag , 2nd edition , 2003 . _ m. wegkamp_. model selection in nonparametric regression .",
    "annals of statistics , vol .",
    "1 , 252273 , 2003 . _",
    "y. yang , a. barron_. information - theoretic determination of minimax rates of convergence .",
    "annals of statistics , 27 , 15641599 , 1999 .",
    "let @xmath512 have binomial distribution with parameters @xmath513 . then for any @xmath514 @xmath515 \\le \\exp\\left(-\\eta^{2}pn/3\\right ) , p\\left[z < ( 1 - \\eta){\\mathbb{e}}z\\right ] \\le \\exp\\left(-\\eta^{2}pn/2\\right)\\ ] ] and for particular @xmath516 @xmath517 \\le \\exp\\left(-pn/8\\right ) , p\\left[z > 3{\\mathbb{e}}z/2\\right ] \\le \\exp\\left(-pn/8\\right).\\ ] ]      let @xmath518 be a disagreement set of the version space of first @xmath519 instances of the learning sample .",
    "the random error set will be denoted by @xmath520 .",
    "using symmetrization lemma [ symmetrization ] and lemma [ expectmax ] we have for any @xmath116 @xmath521 we fix @xmath215 and prove that for any distribution @xmath522 .",
    "now we use @xmath523 let @xmath524 . conditionally on the first @xmath519 instances @xmath525 has binomial distribution .",
    "expectations with respect to the first and the last parts of the sample will be denoted respectively by @xmath526 and @xmath527 .",
    "conditionally on @xmath528 we introduce two events @xmath529 using multiplicative chernoff bounds we have @xmath530 and @xmath531 .",
    "denote @xmath532 . then @xmath533p(\\overline{a } )",
    "+ { \\mathbb{e}}'\\left[p(e_{1}|{\\text{dis}}_{0})\\big|a\\right]p(a).\\ ] ] for the first term we have @xmath534p(\\overline{a } ) \\le { \\mathbb{e}}'\\left[p(e_{1}|{\\text{dis}}_{0})\\big|\\overline{a}\\right ] \\le   \\frac{16\\log\\left(\\mathcal{s}_{\\mathcal{f}}\\left(\\frac{3np({\\text{dis}}_{0})}{4}\\right)\\right)}{np({\\text{dis}}_{0})}.\\ ] ] for the second term multiplied by @xmath535 we have @xmath536p({\\text{dis}}_{0})p(a )   & \\le 2{\\mathbb{e}}'p({\\text{dis}}_{0})\\exp\\left(-\\frac{np({\\text{dis}}_{0})}{16}\\right ) \\\\",
    "= 2p({\\text{dis}}_{0})\\exp\\left(-\\frac{np({\\text{dis}}_{0})}{16}\\right ) & \\le 2\\exp\\left(-\\frac{n}{16}\\right ) \\le \\frac{12}{n}.\\end{aligned}\\ ] ] combining previous results we have @xmath537 it easy to see that for all natural @xmath538 @xmath539 we have          the proof in deviation is based on the symmetrization lemma [ symmetrizationdev ] . at first",
    "we notice that @xmath210 is a @xmath542-bernstein class . thus fixing any @xmath543 , such that @xmath544 and @xmath545 we have for any @xmath546 @xmath547 where @xmath548 and @xmath549 . introducing the rademacher averages we have that @xmath550 has the same distribution as @xmath551 .",
    "we may represent the last quantity as a sum of two random variables @xmath552 both summands have the same distribution .",
    "we consider @xmath553 for a fixed @xmath412 .",
    "using the same decomposition as in lemma [ expectmaxnew ] and chernoff bound @xcite we have for a fixed @xmath554 @xmath555 where , as in lemma [ expectmaxnew ] , the operator @xmath556 denotes the nearest element in the @xmath181-covering . by denoting @xmath557 and @xmath558",
    "we have @xmath559 setting @xmath560 we have @xmath561 we set @xmath562 and choose @xmath563 and @xmath564 .",
    "then with probability at least @xmath6 , @xmath565 we finish the proof by setting @xmath566 .",
    "the upper bound easily follows from the general bound on packing numbers for vc classes @xcite .",
    "once again , given @xmath194 , let @xmath195 denote the set of binary vectors corresponding to the values of functions in @xmath42 .",
    "as above , for a fixed @xmath181 and fixed minimal @xmath181-covering subset @xmath196 , for each @xmath197 , @xmath198 will denote the closest vector to @xmath199 in @xmath200 .",
    "we will denote by @xmath567 the conditional expectation over the @xmath292 variables , given @xmath194 .",
    "note that @xmath568 the first term is @xmath569 by the @xmath181-cover property and the fact that @xmath251 .",
    "furthermore , as in the proof of lemma  [ expectmaxnew ] , the second term is at most @xmath570 .",
    "now we analyze the last term carefully .",
    "first we use the standard peeling argument .",
    "given a set @xmath571 of binary vectors we define @xmath572 = \\{w \\in w|a \\le \\rho_{h}(w , 0 ) < b\\}$ ] .",
    "@xmath573}\\left(\\sum\\limits_{i = 1}^{n}\\xi_{i}v_{i}- \\frac{c}{4}v_{i}\\right ) + \\frac{1}{n}\\sum\\limits_{k = 1}^{\\infty}{\\mathbb{e}}_{\\xi}\\max\\limits_{\\mathcal{n}_{\\gamma}\\left[2^{k}\\gamma / c , 2^{k + 1}\\gamma / c\\right]}\\left(\\sum\\limits_{i = 1}^{n}\\xi_{i}v_{i}- \\frac{c}{4}v_{i}\\right)_{+}\\end{aligned}\\ ] ] the first term is upper bounded by @xmath574 by lemma  [ expectmax ] and by noting that @xmath575| \\leq \\mathcal{m}_{1}(\\mathcal{b}_{h}(0 , ( 2\\gamma)/c,\\{x_1,\\ldots , x_n\\ } ) , ( 2\\gamma)/2 ) \\leq \\mathcal{m}^{\\text{loc}}_{1}(v,\\gamma , n , c)$ ] .",
    "now we upper - bound the second term .",
    "we start with an arbitrary summand . for @xmath576 ,",
    "we have    @xmath577}\\left(\\sum\\limits_{i = 1}^{n}\\xi_{i}v_{i}- \\frac{c}{4}v_{i}\\right ) \\\\ & \\le \\frac{1}{\\lambda}\\ln{\\mathbb{e}}_{\\xi}\\max\\limits_{v \\in \\{0\\ } \\cup\\mathcal{n}_{\\gamma}\\left[2^{k}\\gamma / c , 2^{k + 1}\\gamma / c\\right]}\\exp\\left\\{\\sum\\limits_{i = 1}^{n}\\lambda\\xi_{i}v_{i } - \\frac{\\lambda c}{4 } v_{i}\\right\\ } \\\\ & \\le \\frac{1}{\\lambda}\\ln\\left(\\sum\\limits_{v \\in \\mathcal{n}_{\\gamma}\\left[2^{k}\\gamma / c , 2^{k + 1}\\gamma / c\\right]}{\\mathbb{e}}_{\\xi}\\exp\\left\\{\\sum\\limits_{i = 1}^{n}\\lambda\\xi_{i}v_{i } - \\frac{\\lambda c}{4 } v_{i}\\right\\ }   + 1\\right ) \\\\ & \\le \\frac{1}{\\lambda}\\ln\\left(\\left|\\mathcal{n}_{\\gamma}\\left[2^{k}\\gamma / c , 2^{k + 1}\\gamma / c\\right]\\right|\\exp\\left\\{2^{k-2}\\gamma(4\\lambda^2 - \\lambda c)/c\\right\\ }   + 1\\right ) \\\\ &",
    "\\le \\frac{1}{\\lambda}\\ln\\left(\\left|\\mathcal{n}_{\\gamma}\\left[0 , 2^{k + 1}\\gamma / c\\right]\\right|\\exp\\left\\{2^{k-2}\\gamma(4\\lambda^2 - \\lambda c)/c\\right\\ }   + 1\\right ) \\\\ & \\le \\frac{1}{\\lambda}\\ln\\left(\\left(\\mathcal{m}^{\\text{loc}}_{1}({\\mathcal{g } } , 2\\gamma , n , c)\\right)^{2^{k + 1}}\\exp\\left\\{2^{k-2}\\gamma(4\\lambda^2 - \\lambda c)/c\\right\\ }   + 1\\right).\\end{aligned}\\ ] ]    here we used that any minimal covering is also a packing , and @xmath578\\right| \\le \\left|\\mathcal{m}^{\\text{loc}}_{1}({\\mathcal{g } } , 2\\gamma , n , c)\\right|^{2^{k + 1}}$ ] , where @xmath579 is a @xmath181-packing .",
    "we fix @xmath580 for some @xmath581 .",
    "observe that local entropy is nonincreasing and @xmath582 .",
    "thus , @xmath583 then we have @xmath584 we set @xmath585 and have @xmath586 where @xmath587 is an absolute constant .",
    "here we used that @xmath413 for @xmath412 and @xmath588 .",
    "finally , we have @xmath589      we will provide a detailed outline of the proof .",
    "this proof technically repeats the arguments from our previous results .",
    "the constants will be denoted by @xmath590 for @xmath591 .",
    "the idea is to combine the technique we previously used for theorem [ mainupperbound ] in expectation with the symmetrization lemma ( lemma [ symmetrizationdev ] ) .",
    "once again , let @xmath55 be any erm and @xmath209 be a corresponding function in the excess loss class @xmath248 .",
    "we have @xmath592 and @xmath288 .",
    "then for any @xmath116 @xmath593 now due to the fact that @xmath248 is a @xmath246-bernstein class we have , using lemma [ symmetrizationdev ] , @xmath594 provided that @xmath595 , @xmath596 and @xmath597 .",
    "now we use the same argument as in the proof of proposition [ coveringbound ] .",
    "specifically , to control the deviation of the value @xmath598 it is enough to control the deviation of @xmath599 now we use the argument from lemma [ contraction ] . to control",
    "it is enough to control the deviation of the term @xmath600 .",
    "we fix @xmath484 and use the decomposition as in the beginning of the proof of lemma [ localization ] .",
    "now the problem is reduced to the analysis a @xmath181-covering @xmath601 the concentration of the last term is given by a combination of chernoff bound ( as in proposition [ coveringbound ] ) and an upper bound for the exponential moment of @xmath602 from the proof of lemma [ localization ] ."
  ],
  "abstract_text": [
    "<S> in statistical learning the excess risk of empirical risk minimization ( erm ) is controlled by @xmath0 , where @xmath1 is a size of a learning sample , @xmath2 is a complexity term associated with a given class @xmath3 and @xmath4 $ ] interpolates between slow and fast learning rates . in this paper </S>",
    "<S> we introduce an alternative localization approach for binary classification that leads to a novel complexity measure : fixed points of the local empirical entropy . </S>",
    "<S> we show that this complexity measure gives a tight control over @xmath2 in the upper bounds under bounded noise . </S>",
    "<S> our results are accompanied by a minimax lower bound that involves the same quantity . in particular , </S>",
    "<S> we practically answer the question of optimality of erm under bounded noise for general vc classes .    </S>",
    "<S> statistical learning , pac learning , local metric entropy , local rademacher process , shifted empirical process , offset rademacher process , erm , alexander s capacity , disagreement coefficient , massart s noise condition </S>"
  ]
}