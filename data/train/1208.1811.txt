{
  "article_text": [
    "we are deeply grateful to wojciech czaja for his generous help on this article . research presented in this paper",
    "was supported in part by laboratory of telecommunication sciences .",
    "we gratefully acknowledge this support .    1 a. barrlund , perturbation bounds for the ldlh and lu decompositions . bit ,",
    "358 - 363 , 1991 .",
    "e. bura , r. pfeiffer , on the distribution of the left singular vectors of a random matrix and its applications .",
    "statistics and probability letters , vol .",
    "78 , pp . 2275 - 2280 , 2008 .",
    "d. chandler , w. m. kahan , the rotation of eigenvectors by a perturbation .",
    "siam j. numer .",
    ", vol . 7 , pp . 1 - 46 , 1970 . w. czaja , r. wang , mpsk classification by pca . in preparation",
    ". f. dopico , a note on sin@xmath225 theorems for singular subspace variations . bit ,",
    "395 - 403 , 2000 . c. huang , a. polydoros , likelihood methods for mpsk modulation classification .",
    "ieee transaction on communications , vol .",
    "43 , no . 2 - 4 ,",
    "1493 - 1504 , 1995 .",
    "s. s. soliman , s .- z .",
    "hsue , signal classification using statistical moments .",
    "ieee trrans . commun .",
    "908 - 916 , 1992 .",
    "stein , r. shakarchi , real analysis , measure theory , integration , & hilbert spaces .",
    "3 , p.91 , 2005 .",
    "a. swami , b. m. sadler , hierachical digital modulation classification using cumulants .",
    "ieee trans .",
    "3 , pp 416 - 429 , 2000",
    ". s. j. szarek , condition numbers of random matrices .",
    "j. complexity , vol . 7 , no .",
    "131 - 149 , 1991 .",
    "v. vu , singular vectors under random perturbation .",
    "random struct .",
    "algorithms , vol .",
    "526 - 538 , 2011 .",
    "p. a. wedin , perturbation bounds in connection with singular value decomposition .",
    "bit numer .",
    "99 - 111 , 1972 .",
    "h.weyl , das asymptotische verteilungsgesetz der eigenwerte linearer partieller differentialgleichungen ( mit einer anwendung auf die theorie der hohlraumstrahlung ) .",
    "441 - 479 , 1912 ."
  ],
  "abstract_text": [
    "<S> we perform a non - asymptotic analysis on the singular vector distribution under gaussian noise . in particular , we provide sufficient conditions of a matrix for its first few singular vectors to have near normal distribution . </S>",
    "<S> our result can be used to facilitate the error analysis in pca .     </S>",
    "<S> startsectionsection1@ -3.5ex plus -1ex minus -.2ex 2.3ex plus.2ex * * [ introduction]introduction the singular value decomposition ( svd ) lies in the heart of various dimension reduction ( dr ) techniques , such as pca , le , lle etc . in real world problems , noise from unknown sourses may largely change the dimension reduction result by changing either the principal directions , or the data distribution along those directions . aside from some unusual cases that involve nonlinear bias in the measurements , and because of the central limit theorem </S>",
    "<S> , it is always assumed that the noise vector in the raw data obeys the i.i.d . </S>",
    "<S> gaussian distribution , which simplifies both data processing and error analysis . </S>",
    "<S> however , when doing dimension reductions , the gaussian distribution is not preserved . </S>",
    "<S> this is because the low dimensional data is contained in the first few singular vectors of some predefined kernel , and elements of any singular vector are bounded by one , which prevent them from being gaussian variables . in @xcite , it is pointed out in a statistical and asymptotic way that for a fixed data matrix , as the noise level goes to 0 , the distribution of singular vectors would eventually become very close to a multivariate normal distribution . in this paper , we go further in this direction and provide non - asymptotic bound on the distance between the perturbed singular vectors and a multi - variate normal random variable . in applications , our result could be used in two ways :    * to predict for a fixed data matrix and noise level , whether or not can one reliably ( within a given confidence interval ) assume that the noise after dimension reduction is gaussian . </S>",
    "<S> * for a given noise level , to determine how many samples are necessary for the noise in the embedded space to be close ( e.g. , the distance is less than some given threshold ) to gaussian .    </S>",
    "<S> we state the mathematical description of our problem as follows . </S>",
    "<S> +   + * problem * : _ suppose @xmath0 is an @xmath1 noiseless data matrix , and @xmath2 is the observed noisy data . </S>",
    "<S> their associated svds are : @xmath3 and @xmath4 where the entries of @xmath5 are i.i.d . @xmath6 and @xmath7 is an absolute constant . due to the randomness of @xmath5 , </S>",
    "<S> all the quantities in ( 2 ) are random variables whose measures are induced by those of @xmath5 . under this setting </S>",
    "<S> , what is the distribution of @xmath8 ? _ + in this paper , without loss of generality , we focus only on the low rank and square @xmath0s ( i.e. , @xmath9 ) . </S>",
    "<S> all our techniques can be carried through in high rank and rectangular cases .    </S>",
    "<S> the perturbation problem of singular vectors has brought great attention in the field of statistics and numerical analysis . </S>",
    "<S> davis and kahan @xcite studied the deterministic perturbation on hermitain matrices and provides an upper bound for the rotation of singular vector subspaces caused by the perturbation . </S>",
    "<S> they showed that the span of a group of singular vectors as a subspace is changed by an amount which is propotional to the noise power and the reciprocal of eigenvalue gap . </S>",
    "<S> ( the change is charaterized in canonical angle between subspaces , denoted by @xmath10 ) . </S>",
    "<S> wedin @xcite extended this result to non - hermitain cases . </S>",
    "<S> dopico @xcite proved that the left and right singular vectors is perturbed towards the same direction . </S>",
    "<S> van vu @xcite considered random perturbations ( i.i.d . </S>",
    "<S> bernoulli distribution ) and provided a tighter upper bound for the canonical angles @xmath10 , which hold with large probability . </S>",
    "<S> the necessity of using the canonical angles to characterize the change is well illustrated in the following example in @xcite . consider the following two matrices , @xmath11 both matrices are diagonalizable , the eigenvectors of @xmath0 are @xmath12 and @xmath13 , while those of @xmath14 are @xmath15 and @xmath16 , for any @xmath17 . </S>",
    "<S> hence , the difference of these two bases does not go to zero with @xmath17 . </S>",
    "<S> +   + beside the canonical angle , some authors used another quantity : @xmath18 to describe the changes of singular subspaces . </S>",
    "<S> the latter has been proved to be an upper bound of the former . </S>",
    "<S> + in this paper , we use the pointwise matrix norm @xmath19 to bound the difference between two matrices . </S>",
    "<S> our technique is valid and simpler for the frobenius norm case so we omit it . </S>",
    "<S> however , if one tries to use the bound we provide on pointwise norm to derive a bound on frobenius norm using the equivalence of norms , he will get a looser bound than directly running over our technique on frobenius norm , and vice versa .    </S>",
    "<S> the rest of the paper is organized as follows . in section 2 </S>",
    "<S> , we introduce notations and some existing theorems to be used . in section 3 , we state several lemmas and our main theorem . section 4 </S>",
    "<S> contains the proof of the main theorem . in section 5 </S>",
    "<S> , we apply our result to an audio signal classification problem .     </S>",
    "<S> startsectionsection1@ -3.5ex plus -1ex minus -.2ex 2.3ex plus.2ex * * [ previous results]previous results throughout this paper , we consider only square data matrices with nontrivial dimensionality @xmath20 and rank @xmath21 . </S>",
    "<S> the variables @xmath0 , @xmath14 , @xmath22 , @xmath23 , @xmath24 , @xmath25 , @xmath26 , @xmath27 , @xmath28 , remain the same as defined in ( 1 ) and ( 2 ) . </S>",
    "<S> we assume @xmath9 . </S>",
    "<S> in addition , we assume that all the diagonal elements of the @xmath29 matrix @xmath30 are bounded away from zero . </S>",
    "<S> + for a given matrix @xmath31 , we use @xmath32 and @xmath33 to denote the qr decomposition of @xmath31 . </S>",
    "<S> the set of singular values of @xmath31 is denoted by @xmath34 ; the @xmath35th largest one is denoted by @xmath36 ; and the minimum singular value is by @xmath37 . </S>",
    "<S> we use @xmath5 to denote the normalized gaussian matrix whose entries are @xmath38 . for any matrix , @xmath39 denotes the frobenius norm , and @xmath40 the spectral norm . </S>",
    "<S> in addition , we use @xmath19 to denote the component - wise maximum norm of a matrix , i.e. , @xmath41 . </S>",
    "<S> the following theorem is due to dopico @xcite , who provided the worst - case bound on the frobenius norm of the deviation of singular vector subspace under perturbation . </S>",
    "<S> he proved that this bound is proportional to the frobenius norm of the perturbation as well as the reciprocal of the eigenvalue gap .    </S>",
    "<S> let @xmath0 and @xmath14 and their svds be as defined in ( 1 ) and ( 2 ) . </S>",
    "<S> define @xmath42 if @xmath43 , then @xmath44 where @xmath45 , @xmath46 . </S>",
    "<S> moreover , the left hand side of ( 3 ) is minimized for @xmath47 where @xmath48 is any svd of @xmath49 , and in this case , the equality can be attained .    in the proof of our main theorem ( theorem 4 ) </S>",
    "<S> , we will frequently encounter the spectral norm of gaussian matrices , which is known to be bounded in the following theorem .    </S>",
    "<S> let w be an @xmath50 matrix with i.i.d . </S>",
    "<S> normal entries with mean 0 and variance @xmath51 . </S>",
    "<S> then , its largest and smallest singular values obey : @xmath52    in order to estimate the gap of eigenvalues in ( 3 ) , we will make use of the following result .    </S>",
    "<S> ( @xcite ) let @xmath31 be an @xmath50 matrix and @xmath53 be a perturbation of @xmath31 . </S>",
    "<S> let @xmath54 and @xmath55 be the @xmath56th largest eigenvalue of @xmath31 and @xmath53 , respectively . </S>",
    "<S> then , for @xmath57 , @xmath58     startsectionsection1@ -3.5ex plus -1ex minus -.2ex 2.3ex plus.2ex * * [ main theorem]main theorem we now ready to state our main theorem .    </S>",
    "<S> let @xmath0 and @xmath14 and their svds be defined as in ( 1 ) and ( 2 ) . </S>",
    "<S> assume @xmath59 are small enough such that the following defined quantities satisfies @xmath60 and @xmath61 ( see below ) . </S>",
    "<S> then with probability ( with respect to the random gaussian noise w ) exceeding @xmath62 , there exists an unitary @xmath63 matrix @xmath64 , such that for @xmath65 , we have @xmath66 where @xmath67 is a gaussian matrix defined by @xmath68 , and where @xmath69 are defined later in formulas ( 19 ) , ( 20 ) , ( 25 ) , ( 29 ) , ( 17 ) , and where @xmath70 , @xmath71 , and @xmath72 .    </S>",
    "<S> note that the order of @xmath73 in terms of @xmath17 and @xmath74 are : @xmath75 , @xmath76 , @xmath77 , @xmath78 and @xmath79 .    </S>",
    "<S> if the eigenvalues of @xmath0 are all different , then ( 4 ) becomes @xmath80 where @xmath81 .    </S>",
    "<S> * remark 1 * : we observe that the perturbation @xmath8 can be approximated by a gaussian term @xmath82 only when it is the leading term in the error . </S>",
    "<S> the average magnitude of this term has asymptotic order of @xmath83 as @xmath17 and @xmath84 going to 0 , while the order of the leading terms on the right hand side is either @xmath85 or @xmath86 . </S>",
    "<S> therefore , to ensure the gaussian term is higher in order , we need the following condition on the @xmath87 pair : @xmath88    before going into the proof , we first establish three useful lemmas . </S>",
    "<S> the first one is an elementary observation in linear algebra , and the last one is a direct application of theorem 2 , so we omit their proofs .    if @xmath89 , then @xmath90    let @xmath91 be the distribution of the product of two independent @xmath6 normal random variables . </S>",
    "<S> let @xmath92 , @xmath93 , ... , @xmath94 be i.i.d . </S>",
    "<S> random variables drawn from @xmath91 . if @xmath20 , we have @xmath95 where @xmath96 .    </S>",
    "<S> if @xmath97 , one can verify that for all @xmath98 , @xmath99 we apply markov s inequality ( see eg . </S>",
    "<S> @xcite ) to have : @xmath100 letting @xmath101 in the above formula , we get @xmath102    let @xmath103 be an @xmath1 gaussian matrix whose elements are i.i.d . </S>",
    "<S> @xmath104 . </S>",
    "<S> let @xmath103 be written as @xmath105 with @xmath106 a @xmath29 matrix . then , with probability exceeding @xmath107 , @xmath108     startsectionsection1@ -3.5ex plus -1ex minus -.2ex 2.3ex plus.2ex * * [ proof of the main theorem]proof of the main theorem    the idea of the proof is the following : our goal is to charaterize the perturbed singular vector space @xmath109 , which is a left orthogonal matrix and satisfies @xmath110 , meaning that it , together with @xmath111 , diagonalizes @xmath14 . </S>",
    "<S> thus , if we can construct two other orthogonal matrices which also diagonalize @xmath14 with small error , then , by theorem 1 , they are expected to be good approximations to @xmath109 and @xmath111 . </S>",
    "<S> +   + we start the proof similarly to that in @xcite . </S>",
    "<S> define the random matrix @xmath112 as follows , @xmath113 where @xmath114 and @xmath115 due to the invariant property of gaussian matrix , @xmath112 has the same distribution as @xmath5 . </S>",
    "<S> the asymtotic order of @xmath17 in the error term @xmath116 is not satisfactory , we thus want to further diagonalize @xmath112 by using the following two matrices , @xmath117 with @xmath118 , and @xmath119 with @xmath120 </S>",
    "<S> . note that in doing so , we start to differ from @xcite by defining @xmath121 and @xmath122 explicitly . </S>",
    "<S> the second order terms in @xmath121 and @xmath122 are crucial for obtaining ( 4 ) . </S>",
    "<S> + keeping in mind that @xmath121 and @xmath122 are not yet unitary , we multiply the left hand side of ( 6 ) by @xmath123 on the left , and by @xmath122 on the right , to obtain , @xmath124 where the matrix @xmath125 includes all the terms whose order on @xmath17 is greater than or equal to 3 , so @xmath125 is @xmath126 . </S>",
    "<S> compare ( 7 ) with ( 6 ) , we see the above operation has changed the order of error term from @xmath127 to @xmath126 . </S>",
    "<S> if we denote the eigenvector subspaces corresponding to the two diagonal blocks by @xmath128 , @xmath129 , then they have the following expressions , @xmath130 recall that @xmath131 and @xmath132 denote the qr decomposition of a matrix @xmath31 . </S>",
    "<S> now , we want to orthogonalize @xmath133 and @xmath134 . for that purpose , for both sides of ( 7 ) , we multiply them by @xmath135 on the left , and @xmath136 on the right . this way we obtain : @xmath137 where @xmath138 with a little abuse of notation , we continue to use @xmath125 to denote the error term in ( 10 ) , though it was already changed by the left and right multiplication . </S>",
    "<S> we denote by @xmath139 the svd of the @xmath140 , with @xmath28 . in this case , </S>",
    "<S> ( 10 ) becomes @xmath141 on the other hand , the left hand side of ( 11 ) also satisfies @xmath142 combining ( 11 ) with ( 12 ) , we obtain : @xmath143 moving everything but @xmath0 on the left hand side to the right by multiplying the inverse of each matrix and utilizing the fact that @xmath144 and @xmath145 are orthogonal to each other , we derive : @xmath146 ( 13 ) combined with theorem 1 imply that @xmath109 and @xmath147 are the first @xmath35 left singular vectors of two very similar matrices ( different by the err term ) and that they are close . </S>",
    "<S> keeping this useful result in mind , we first turn to look at the big picture .    </S>",
    "<S> our final goal is to approximate by a gaussian variable the difference between @xmath148 and @xmath109 up to a roation : @xmath149 ( we willl define the unitary matrix m explicitly later ) , which can be decomposed as : @xmath150 we insert ( 8) into ( 14 ) to get @xmath151 here , the gaussian term is the second to last term on the right , so we want to prove all other terms are small . </S>",
    "<S> for that purpose , we move the gaussian term to the left and take the component - wise matrix norm on both sides , to have : @xmath152 observe that the left hand side of ( 15 ) is exactly what we want to bound in this theorem . the rest of the proof is divided into three parts to bound each term @xmath153 , @xmath154 , @xmath155 on the right hand side . </S>",
    "<S> +      from ( 2 ) and ( 13 ) , we obtain that @xmath156 and @xmath157 are the left eigenspaces of @xmath14 and @xmath158 , respectively . </S>",
    "<S> thus , we want to use theorem 1 to bound @xmath153 . for this purpose , we fisrt calculate the key parameter @xmath159 in that theorem . </S>",
    "<S> +   + recall @xmath2 . by theorem 2 and theorem 3 </S>",
    "<S> , the @xmath56th largest sigular value of @xmath0 and that of @xmath14 obey , with probability over @xmath160 , that @xmath161 equation ( 16 ) implies the following lower bound on @xmath159 : @xmath162 whenever @xmath61 , we can apply theorem 1 to the two svds in ( 2 ) and ( 13 ) , to obtain @xmath163 the matrix @xmath125 , defined in ( 7 ) and modified in ( 10 ) and ( 13 ) , is essentially a sum of several products of gaussian matrices . </S>",
    "<S> applying lemma 1 on @xmath125 and utilizing lemma 3 , we obtain the following bound : @xmath164 holds with probability over @xmath107 whenever @xmath17 is small enough such that @xmath165 . here , @xmath166 and @xmath167 where @xmath70 , @xmath71 , and @xmath72 . </S>",
    "<S> + now , the right hand side of ( 18 ) has the following bound : @xmath168 we are now ready to define the rotation @xmath64 which first appears in ( 14 ) . comparing ( 18 ) with the first term of ( 14 ) , we have @xmath169 where the explicit form of @xmath170 is given in theorem 1 </S>",
    "<S> . + we plug ( 21 ) into ( 18 ) , to obtain : @xmath171      we start with breaking @xmath154 into two parts : @xmath172 we estimate iv and v separately . </S>",
    "<S> @xmath173 observe that the entries of @xmath174 are i.i.d . </S>",
    "<S> @xmath6 and are independent of those in @xmath175 </S>",
    "<S> . therefore we can apply lemma 2 to each entry @xmath176 and those of @xmath177 to get , with probability at least @xmath178 , with @xmath179 , @xmath180 for @xmath181 , we first observe the following random upper bound , @xmath182 using ( 24 ) , the union bound , as well as the following inequality , @xmath183 we can estimate the probability that @xmath181 exceeds the value @xmath184 , @xmath185 where @xmath186 is the first element of @xmath187 . therefore , with probability exceeding @xmath188 , we have @xmath189 we combine the estimates of @xmath190 and @xmath181 to get , with probability greater than @xmath191 , @xmath192      the following calculation shows how far away is @xmath133 from unitary . </S>",
    "<S> @xmath193 hence , @xmath194 following a procedure similar to the one we used to obtain the bound in ( 21 ) , we derive the distance between this covariance matrix and the identity matrix in frobenius norm : @xmath195 here , @xmath196 was defined in ( 19 ) . </S>",
    "<S> when @xmath17 is small enough such that @xmath197 , theorem 2.2 in @xcite ] shows that the distance @xmath198 can be bounded by a function of @xmath199 : @xmath200 thus , @xmath201 to estimate ( 27 ) , we first note that from ( 25 ) and the assumption that @xmath202 , we obtain @xmath203 therefore , @xmath204 we insert ( 26 ) and ( 28 ) into ( 27 ) , to arrive at the bound : @xmath205 furthermore , from ( 8) and lemma 3 , it is straightforward to estimate : @xmath206 we combine the above two inequalities to get : @xmath207 we now aggregate the estimates of @xmath153 , @xmath154 and @xmath155 to get ( 4 ) and ( 5 ) .     </S>",
    "<S> startsectionsection1@ -3.5ex plus -1ex minus -.2ex 2.3ex plus.2ex * * [ application]application we use the m - psk ( phase shift keying ) modulation classification problem as an example to show how our result is used to make a sampling strategy and a new classification method </S>",
    "<S> .    in the so - called adaptive modulation system , the modulation type is varying with time . when the condition of channels ( such as fading or interference ) changes , the transmitter seeks to find the modulation type which best adapts to the new environment . </S>",
    "<S> meanwhile , if the receiver is not informed of these changes , a modulation classification procedure needs to be carried out as soon as the signal is received . here , we consider the classification problem for the widely used mpsk type of modulations which conveys the data by changing the phase of a carrier wave . here </S>",
    "<S> @xmath64 stands for the number of available phases and usually takes value 2 , 4 , 8 , 16 or 32 . </S>",
    "<S> an mpsk signal @xmath208 has the following mathematical representation : @xmath209 where @xmath103 is called the _ symbol period / duration _ ; @xmath210 is a function supported on @xmath211 $ ] and is called _ baseband waveform _ ; the frequency @xmath212 is called _ carrier frequency _ , and @xmath213 is the _ carrier phase_. all these parameters , whether assumed known or not , are fixed for the duration of the signal . </S>",
    "<S> @xmath214 denotes an additive white gaussian noise with two sided power spectral density @xmath215 . </S>",
    "<S> the information is encoded in the phase parameter @xmath216 . </S>",
    "<S> @xmath64 denotes the possible choice of phase shifts . </S>",
    "<S> if @xmath217 , then each symbol period can encode @xmath218 binary bits . </S>",
    "<S> the most popular cases are @xmath219 and @xmath220 , where the modulation is called 2-psk and 4-psk or more often bpsk and qpsk , respectively . </S>",
    "<S> the classification among m - psk modulations is defined to be the process of detecting the right value of @xmath64 from a received noisy signal @xmath208 .    among the literature about mpsk classifications </S>",
    "<S> ( see e.g. @xcite , </S>",
    "<S> @xcite , @xcite ) , there exist many different types of model settings , ranging from the fully blind classifications , i.e. , where all the parameters in ( 30 ) are unknown , to the constrained classfication problems where @xmath64 is the only unknown parameter . for illustration purposes , </S>",
    "<S> we consider a simple , yet nontrivial , partially blind model assuming that @xmath103 and @xmath221 are known , @xmath222}$ ] , @xmath223 for some unknown integer @xmath35 , @xmath213 is unknown , and @xmath224 is chosen unifromly from @xmath225 .    </S>",
    "<S> suppose we digitize the data in the following way . </S>",
    "<S> we take @xmath226 uniform samples from @xmath67 periods of @xmath208 and store them in an @xmath227 matrix @xmath0 , where @xmath228 denote the @xmath229th sample of the @xmath74th period , which has the expression : @xmath230 where @xmath5 is a guassian noise matrix with i.i.d . </S>",
    "<S> entries .    when noise is absent ( i.e. , @xmath231 ) , each column of @xmath0 is a function of @xmath224 . since @xmath224 has at most @xmath64 choices , the columns of @xmath0 also have only @xmath64 patterns . </S>",
    "<S> if we deem each column as an @xmath170 dimensional data point , then it merely is a high dimensional embedding of a zero dimensional parameter space . in other words , </S>",
    "<S> the @xmath170 dimensional graph of @xmath0 consists only of @xmath64 points . when noise is added , these @xmath64 points becomes @xmath64 clusters . </S>",
    "<S> hence , the classification problem of finding the correct @xmath64 is reduced to a clustering problem of finding the total number of clusters .    </S>",
    "<S> the complexity of all well known clustering methods , such as k - means and mean shift grows exponentially with dimension . </S>",
    "<S> therefore , for large data sets , a preprocessing step of dimension reduction is necessary . </S>",
    "<S> the dimension reduction procedure has another two advantages over other well known methods ( e.g. , @xcite , @xcite , @xcite ) :    * no carrier removal procedure is needed , relaxing the digitization rate to below the nyquist rate of the carrier frequency . * </S>",
    "<S> classification and detection are completed simultaneously .    for our problem setting , </S>",
    "<S> pca is the most suitable technique for the following reasons :    * the signal has gaussian noise and pca is just a linear - gaussian latent variable model . </S>",
    "<S> * this is a linear model when we deem @xmath232 as parameters of @xmath0 ( we will provide more discussion on this observation later ) . in telecommunications , </S>",
    "<S> the 2-d graph of all evaluations of @xmath232 with @xmath233 is formally called the constellation diagram of the mpsk modulation ( see figure 1 ) , and can be used as an identifier of the modulation type .    therefore , the idea of our method is using pca to map the high dimensional data to the two - dimensional constellation diagram on which clustering algorithm is applied to find the true cluster quantity .    </S>",
    "<S> now we provide more detail to legistate the linear model observation .        by definition , the matrix @xmath0 has the following decomposition : @xmath234 where for @xmath235 , @xmath236 , and @xmath237 , @xmath238 it is immediate to verify that when @xmath239 , we have @xmath240 , and @xmath181 is nearly orthogonal since we have assumed that @xmath224 are choosen unifromly at random from the parameter space ( rigorous calculation of the distance between @xmath181 and an orthogonal matrix can be found in [ 8 ] ) . </S>",
    "<S> this decomposition clearly shows how the noiseless part of @xmath0 linearly depends on @xmath232 . </S>",
    "<S> when noise is absent , pca does the job of seperating @xmath241 from @xmath242 , where the latter is just a scaled version of the constellation . with noise </S>",
    "<S> is added , as explained earlier , each individual point in the graph turns into a cluster ( figure 2 ) .        </S>",
    "<S> even though it is quite obvious to a human observer how many clusters there are in figure 2 without any prior knowledge , the exisiting clustering algorithms are surprisingly inferior by requiring either the number of clusters or the cluster radius as input . when the number of clusters is unknown as in our setting , many previous work suggest to do brute force on all the possible numbers ( which are 2 , 4 , 8 , 16 , 32 , maybe also 64 , 128 ) , and compare the clustering results in some ad hoc way to decide which is more likely . a more pleasant way to do this is by finding the cluster radius , which is exactly the place to use our results in this paper .    before applying our results , we first normalizate the singular values of @xmath0 by setting @xmath243 , so that the singular values of @xmath14 do not change with the matrix size , @xmath244 in hardware implementations , a larger value of @xmath170 is usually more difficult to realize than that of @xmath67 , because @xmath170 corresponds to the sampling rate and @xmath67 is the sample duration . hence , we assume @xmath245 . a generalized version of theorem 4 for rectangular matrix can be similarly built , by padding zeros to form an @xmath246 matrix . since now </S>",
    "<S> the factor @xmath247 in ( 34 ) is a normalized gaussian matrix , the other factor , @xmath248 , denotes the energy of noise , corresponding to @xmath17 in theorem 4 . as @xmath249 , @xmath250 . </S>",
    "<S> theorem 4 implies that for a given matrix , and a given confidence level , there exists a threshold @xmath251 such that , as long as @xmath252 , we can assume the first two eigenvectors of @xmath14 have multivariate normal distribution . in general , @xmath251 is a function of both @xmath170 and @xmath67 , which makes the inquality @xmath253 difficult to solve . </S>",
    "<S> fortunately , in our model , where @xmath254 is uniformly bounded for all @xmath67 , it can be verified from ( 5 ) that @xmath251 is universal for all sizes of @xmath0 . thus solving the simple inequality @xmath255 gives us the feasible region of @xmath256 .    with any feasible @xmath170 </S>",
    "<S> , we can apply the mean shift clustering method with gaussian kernel . by some elementary calculations </S>",
    "<S> , one can derive that the @xmath257 percentile of the gaussian noise on each data point in the embedded space equals @xmath258 .. we set this number to be the radius .    </S>",
    "<S> since the rank of @xmath0 for bpsk signals is one , then trying to invert the second singular value in theorem 4 makes the problem near singular and cause large error in the second dimension as shown in figure 2 . </S>",
    "<S> fortunately , this case is easy to be recogonized by simply examing whether the second singular value of @xmath0 is much smaller than the first one .    in our first experiment </S>",
    "<S> , we generate a qpsk signal with carrier frequence of 1ghz , symbol rate 10mhz , and damped by awgn with @xmath259 . </S>",
    "<S> we use the sampling rate 21 samples per symbol ( much lower than the nyquist rate of the carrier frequence , and satisfies @xmath260 ) and sample 200 symbols . in figure 3 , the result of pca is plotted , together with a circle whose radius is the @xmath257 percentile predicted by the theorem , and whose centers are those found by the meanshift algorithm . </S>",
    "<S> we can see that the prediction of radius is quite accurate .        in our second experiment </S>",
    "<S> , we let the snr decrease and examine the performance of the above algorithm . </S>",
    "<S> an classification is deemed as successful only when the number of clusters returned by the mean shift algorithm @xmath261 strictly equal to the true type @xmath64 . </S>",
    "<S> the result is plotted in figure 4 . </S>",
    "<S> as expected , when noise grows , the eigenvector distribution deviates from the gaussian distribution and the predicted radius becomes too small for the algorithm to find the correct @xmath64 .         </S>",
    "<S> startsectionsection1@ -3.5ex plus -1ex minus -.2ex 2.3ex plus.2ex * * [ conclusion]conclusion in this paper , we provided a condition under which the perturbation of the principal eigenvectors of a matrix under gaussian noise has a near - gaussian distribution . </S>",
    "<S> the condition is nonasymptotic and is useful in application . </S>",
    "<S> we provided an simple example of audio signal classification problem to illustrate how our theorem can be used to make sampling strategy and to form new classification technique . </S>",
    "<S> more details about this new classification scheme is discussed in @xcite . </S>"
  ]
}