{
  "article_text": [
    "we call a standard gaussian random matrix just _ gaussian _ and apply gaussian multipliers to    * numerically stabilize gaussian and block gaussian elimination using no pivoting , orthogonalization or symmetrization * approximate the leading singular spaces of an ill - conditioned matrix , associated with its largest singular values and * approximate this matrix by a low - rank matrix .",
    "ample empirical evidence shows efficiency of all these applications @xcite , @xcite , @xcite , but formal proofs are only known in the case of approximation under the assumption of small oversampling , that is , a minor increase of the multiplier size @xcite .",
    "we provide formal support for randomized stabilization of gaussian and block gaussian elimination and for randomized approximation without oversampling .",
    "our test results are in good accordance with our analysis and also show similar power of random circulant multipliers or their leading ( that is , northeastern ) toeplitz blocks for the elimination and approximation , respectively .",
    "simple extension of the known estimates enables formal support for this empirical observation in the case of low - rank approximation , but not in the case of elimination .",
    "moreover , we prove that with a high probability gaussian circulant multipliers can not fix numerical instability of the elimination algorithms for a specific narrow class of inputs .",
    "the issue remains open for the elimination preprocessed with other random structured multipliers .    in the next subsections we specify further the computational tasks and our results .",
    "gaussian elimination , applied numerically , with rounding errors , can fail even in the case of a nonsingular well - conditioned input matrix unless this matrix is also positive definite , diagonally dominant , or totally positive . for example",
    "( see part ( i ) of theorem [ thcgenp ] ) , numerical gaussian elimination fails when it is applied to the matrices of discrete fourier transform of large sizes , even though they are unitary up to scaling . in practice",
    "the user avoids the problems by applying gaussian elimination with partial pivoting , that is , appropriate row interchange , which has some limited formal and ample empirical support .",
    "our alternative is gaussian elimination with no pivoting .",
    "hereafter we use the acronyms _ gepp _ and _",
    "genp_.    in section [ ssgnp ] we prove that genp is safe numerically with a probability close to 1 when the input matrix is nonsingular , well - conditioned and preprocessed with a gaussian multiplier .    at the first glance",
    "our preprocessing may seem to be more costly than partial pivoting .",
    "the latter only involves order of @xmath0 comparisons for an @xmath1 input matrix @xmath2 , versus @xmath3 arithmetic operations for its multiplication by a gaussian multiplier .",
    "generation of @xmath0 independent gaussian entries of the @xmath1 multiplier is an additional charge , even though one can discount partly it because this preprocessing stage is independent of the input matrix .",
    "more careful comparison , however , shows that partial pivoting takes quite a heavy toll .",
    "it interrupts the stream of arithmetic operations with foreign operations of comparison , involves book - keeping , compromises data locality , and increases communication overhead and data dependence .",
    "choosing between gepp and genp with randomized preprocessing the user may also consider various other factors , some of them dynamic in time .",
    "for example , here is a relevant citation from @xcite :  the traditional metric for the efficiency of a numerical algorithm has been the number of arithmetic operations it performs .",
    "technological trends have long been reducing the time to perform an arithmetic operation , so it is no longer the bottleneck in many algorithms ; rather , communication , or moving data , is the bottleneck \" .",
    "our test results with gaussian input matrices are in good accordance with our formal estimates ( see figures [ fig : fig1][fig : fig5 ] and tables [ tab61][tab65a ] ) . in our tests",
    "the output accuracy of genp with preprocessing was a little lower than in the case of the customary gepp , as can be expected , but a single step of iterative refinement , performed at a dominated computational cost , has always fixed this discrepancy ( see figures [ fig : fig2 ] and [ fig : fig3 ] and table [ tab63 ] ) .",
    "the tests show similar results when we applied gaussian circulant rather than general gaussian multipliers , and in this case the cost of our preprocessing decreases dramatically ( see more on that in section [ srsm ] ) .",
    "finally , all our study of genp , including formal probabilistic support of its numerical performance , is immediately extended to _",
    "block gaussian elimination _",
    "( see section [ sbgegenpn ] ) , whereas pivoting can not amend this valuable algorithm , and so without our preprocessing , it is numerically unsafe to use it , unless an input matrix is nonsingular , well - conditioned , and positive definite , diagonally dominant , or totally positive .",
    "random multipliers are known to be highly efficient for low - rank approximations of an @xmath4 matrix @xmath2 having a small numerical rank @xmath5 . as the basic step ,",
    "one computes the product @xmath6 where @xmath7 is a random @xmath8 multiplier , for @xmath9 and a positive _ oversampling integer _",
    "the resulting randomized algorithms have been studied extensively , both formally and experimentally @xcite , @xcite .",
    "they are numerically stable , run at a low computational cost , allow low - cost improvement of the output accuracy by means of the power method , and have important applications to matrix computations , data mining , statistics , pdes and integral equations .    by extending our analysis of preprocessed genp , we prove that even for an @xmath11 gaussian multiplier , that is , _ without oversampling _ , the algorithms output rank-@xmath5 approximations of the input matrix with a probability close to 1",
    ". then again our test results are in good accordance with our formal estimates ( see figures [ fig : fig6 ] and [ fig : fig7 ] and tables [ tab67][tab614 ] ) .",
    "the decrease of @xmath10 to 0 should be theoretically interesting , although it has only minor practical promise , limited to the cases where the numerical rank @xmath5 is small and the user knows it .",
    "indeed , according to ( * ? ? ?",
    "* section 4.2 ) ,  it is adequate to choose @xmath12 or @xmath13 \" .",
    "the srft @xmath8 multipliers @xmath7 ( srft is the acronym for subsample random fourier transform ) involve only @xmath14 random parameters versus @xmath15 parameters of gaussian multipliers and accelerate the computation of the product @xmath6 by a factor of @xmath16 versus @xmath8 gaussian multipliers @xmath7 . it has been proved that they are expected to support rank-@xmath5 approximation assuming oversampling integers @xmath17 of order @xmath18 , and empirically this has been observed for reasonably small constants @xmath10 , usually being not more than 20 ( * ? ? ?",
    "* section 11 ) , @xcite .",
    "we readily extend the latter results to the case where @xmath8 products of random @xmath1 circulant and random @xmath8 permutation matrices are used as multipliers instead of srft matrices ( see remark [ remsrtft ] ) , and we observe such empirical behavior also in the cases when we preprocess genp by applying random circulant multipliers ( see figures [ fig : fig2 ] , [ fig : fig3 ] , [ fig : fig6 ] and [ fig : fig7 ] and tables [ tab64 ] , [ tab65 ] , [ tab611 ] , and [ tab614 ] ) . in the latter case",
    "we only need @xmath14 random parameters versus @xmath0 for a gaussian multiplier and accelerate gaussian preprocessing for genp by a factor of @xmath19 .",
    "this acceleration factor grows to @xmath20 if an input matrix is toeplitz or toeplitz - like and if we can apply the mba celebrated algorithm , which is just recursive block gaussian elimination adjusted to a toeplitz - like input ( * ? ? ?",
    "* chapter 5 ) and which is superfast , that is , runs in nearly linear arithmetic time .",
    "numerical stability problems , however , are well known for this algorithm @xcite , and fixing them was the central subject of the highly recognized papers @xcite and @xcite . pivoting could not be applied here because it destroys toeplitz structure , thus increasing the solution cost to cubic .",
    "so the authors first reduced the task to the case of cauchy - like inputs by specializing the techniques of the transformation of matrix structures from @xcite ( also see @xcite ) and then applied a fast cauchy - like variant of gepp using quadratic arithmetic time .",
    "stabilization of the mba superfast algorithm by means of random circulant multipliers would mean randomized acceleration by order of magnitude versus @xcite and @xcite because circulant multipliers preserve toeplitz - like structure of an input matrix and thus keep the mba algorithm superfast .",
    "empirically , gaussian circulant multipliers do stabilize genp numerically , but no formal support is known for this observed behavior .",
    "moreover we even prove that the application of genp fails for a specific narrow class of matrices , and with probability near 1 their preprocessing with gaussian circulant multipliers does not help ( see remark [ remsrtft ] ) .",
    "we know of no such hard inputs for some variations of using gaussian circulant multipliers for genp and mba , such as using products of random circulant and skew - circulant multipliers and using random circulant pre - multipliers and post - multipliers simultaneously .",
    "both variations preserve toeplitz - like input structure and allow superfast performance of the mba algorithm , but we have no formal support for the efficiency of such multipliers .",
    "preconditioning of linear systems of equations is a classical subject @xcite , @xcite , @xcite . for early work on randomized multiplicative preprocessing as a means of countering degeneracy of matrices",
    "see section 2.13  regularization of a matrix via preconditioning with randomization \" in @xcite and the bibliography therein . on the most recent advance in this direction",
    "see @xcite .",
    "on the early specialization of such techniques to gaussian elimination see @xcite .",
    "randomized multiplicative preconditioning for numerical stabilization of genp was proposed in ( * ? ? ?",
    "* section 12.2 ) and @xcite , although only weaker theorems on the formal support of this approach were stated and their proofs were omitted .",
    "the paper @xcite and the bibliography therein cover the heuristic application of prbms ( that is , partial random butterfly multipliers ) , providing some empirical support for genp with preprocessing . on low - rank approximation",
    "we refer the reader to the surveys @xcite and @xcite , which were the springboard for our study in section [ sapsr ] .",
    "we cite these and other related works throughout the paper and refer the reader to ( * ? ? ?",
    "* section 11 ) for further bibliography .",
    "the estimates of our corollary [ cor1 ] are close to the ones of ( * ? ? ?",
    "* theorem 3.8 ) , which were the basis for our algorithms in @xcite , @xcite , and @xcite . unlike the latter papers , however , we state these basic estimates in a simpler form , refine them by following @xcite rather than @xcite , and include their detailed proofs . on the related",
    "subject of estimating the norms and condition numbers of gaussian matrices and random structured matrices see @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , and @xcite . for a natural extension of our present work",
    ", one can combine randomized matrix multiplication with randomized augmentation and additive preprocessing of @xcite , @xcite , and @xcite .      in the next section we recall some definitions and basic results . in section [ sbgegenpn ]",
    "we show that genp and block gaussian elimination are numerically safe for a matrix whose all leading blocks are nonsingular and well conditioned . in section [ smrc ]",
    "we estimate the impact of preprocessing with general nonrandom multipliers on these properties of the leading blocks . in sections",
    "[ sapsr1 ] and [ srnd ] we extend our analysis from section [ smrc ] in order to cover the impact of gaussian and random structured multipliers , respectively . in section [ sapsr ]",
    "we recall an algorithm from @xcite for low - rank approximation and prove that this randomized algorithm is expected to work even with no oversampling . in section [ sexp ]",
    "we cover numerical tests ( the contribution of the last two authors ) .",
    "section [ sconcl ] contains a brief summary . in appendix [ sssm ] we recall the known probabilistic estimates for the error norms of randomized low - rank approximations . in appendix [ srsnrm ] we estimate the probability that a random matrix has full rank under the uniform probability distribution . in appendix [ sosvdi0 ]",
    "we estimate the perturbation errors of matrix inversion . in appendix",
    "[ stab ] we display tables with our test results , which are more detailed than the data given by the plots in section [ sexp ]",
    ". some readers may be only interested in the part of our paper on genp .",
    "they can skip sections [ sapsr ] and [ stails ] .",
    "except for using unitary circulant matrices in sections [ srnd ] and [ stails ] , we assume computations in the field @xmath21 of real numbers , but the extension to the case of the complex field @xmath22 is quite straightforward . hereafter",
    " flop \" stands for  arithmetic operation \" ,  i.i.d .",
    "\" stands for  independent identically distributed \" , and  gaussian matrix \" stands for  standard gaussian random matrix \" ( cf .",
    "definition [ defrndm ] ) .",
    "the concepts  large \" ,  small \" ,  near \" ,  closely approximate \" ,  ill - conditioned \" and  well - conditioned \" are quantified in the context . by saying  expect \" and  likely \" we mean  with probability @xmath23 or close to @xmath23 \" .",
    "( we only use the concept of the expected value in theorem [ thszcd ] , corollary [ cocd ] , and appendix [ sssm ] . )",
    "next we recall and extend some customary definitions of matrix computations @xcite , @xcite .",
    "@xmath24 is the class of real @xmath4 matrices @xmath25 .",
    "@xmath26 is a @xmath27 block diagonal matrix with the diagonal blocks @xmath28 .",
    "@xmath29 , @xmath30 , and @xmath31 denote a @xmath32 block matrix with the blocks @xmath28 . in both cases the blocks",
    "@xmath33 can be rectangular .",
    "@xmath34 is the @xmath1 identity matrix .",
    "@xmath35 is the @xmath36 matrix filled with zeros .",
    "we write @xmath37 and @xmath38 if the matrix size is defined by context .",
    "@xmath39 is the transpose of a matrix @xmath2 .",
    "@xmath40 denotes its leading , that is , northwestern @xmath36 block submatrix , and we also write @xmath41 .",
    "@xmath42 is the spectral norm of a matrix @xmath2 .",
    "@xmath43 is its frobenius norm .",
    "a real matrix @xmath44 is _ orthogonal _ if @xmath45 or @xmath46 .",
    "@xmath47 for an @xmath4 matrix @xmath2 of rank @xmath14 denotes a unique pair of orthogonal @xmath4 and upper triangular @xmath1 matrices such that @xmath48 and all diagonal entries of the matrix @xmath49 are positive ( * ? ? ?",
    "* theorem 5.2.3 ) .",
    "@xmath50 denotes the moore ",
    "penrose pseudo - inverse of an @xmath4 matrix @xmath2 , and @xmath51 denotes its svd where @xmath52 , @xmath53 , @xmath54 , and @xmath55 is the @xmath56th largest singular value of @xmath2",
    ". if a matrix @xmath2 has full column rank @xmath57 , then @xmath58 stands for @xmath59 , @xmath60 for @xmath61 , and @xmath62 for @xmath63 where @xmath64 can denote a scalar , a matrix , or a pair of such objects , e.g. , @xmath65 stands for @xmath66 .",
    "@xmath67 is the condition number of an @xmath4 matrix @xmath2 of a rank @xmath57 .",
    "such matrix is _ ill - conditioned _ if the ratio @xmath68 is large . if the ratio is reasonably bounded , then the matrix is _ well - conditioned_. an @xmath4 matrix @xmath2 has a _ numerical rank _ @xmath69 if the ratios @xmath70 are small for @xmath71 but not for @xmath72 .",
    "the following concepts cover all rectangular matrices , but we need them just in the case of square matrices , whose sets of leading blocks include the matrices themselves .",
    "a matrix is _ strongly nonsingular _",
    "if all its leading blocks are nonsingular .",
    "such a matrix is _ strongly well - conditioned _ if all its leading blocks are well - conditioned .",
    "we recall further relevant definitions and basic results of matrix computations in the beginning of section [ sapsr ] and in the appendix .",
    "for a nonsingular @xmath73 block matrix @xmath74 of size @xmath1 with nonsingular @xmath27 _ pivot block _ @xmath75 , define @xmath76 , the _ schur complement _ of @xmath77 in @xmath2 , and the block factorizations ,    @xmath78    and @xmath79    we verify readily that @xmath80 is the @xmath81 trailing ( that is , southeastern ) block of the inverse matrix @xmath82 , and so the schur complement @xmath83 is nonsingular since the matrix @xmath2 is nonsingular .",
    "factorization ( [ eqgenpin ] ) reduces the inversion of the matrix @xmath2 to the inversion of the leading block @xmath84 and its schur complement @xmath83 , and we can recursively reduce the task to the case of the leading blocks and schur complements of decreasing sizes as long as the leading blocks are nonsingular . after sufficiently many recursive steps of this process of block gaussian elimination ,",
    "we only need to invert matrices of small sizes , and then we can stop the process and apply a selected black box inversion algorithm .    in @xmath85",
    "recursive steps all pivot blocks and all other matrices involved into the resulting factorization turn into scalars , all matrix multiplications and inversions turn into scalar multiplications and divisions , and we arrive at a _ complete recursive factorization _ of the matrix @xmath2 .",
    "if @xmath86 at all recursive steps , then the complete recursive factorization ( [ eqgenpin ] ) defines genp and can be applied to computing the inverse @xmath82 or the solution @xmath87 to a linear system @xmath88 .",
    "actually , however , any complete recursive factorizations turns into genp up to the order in which we consider its steps .",
    "this follows because at most @xmath89 distinct schur complements @xmath90 for @xmath91 are involved in all recursive block factorization processes for @xmath1 matrices @xmath2 , and so we arrive at the same schur complement in a fixed position via genp and via any other recursive block factorization ( [ eqgenp ] ) .",
    "hence we can interpret factorization step ( [ eqgenp ] ) as the block elimination of the first @xmath92 columns of the matrix @xmath2 , which produces the matrix @xmath90 . if the dimensions @xmath93 and @xmath94 of the pivot blocks in two block elimination processes sum to the same integer @xmath92 , that is , if @xmath95 , then both processes produce the same schur complement @xmath90 .",
    "the following results extend this observation .",
    "[ thsch ] in the recursive block factorization process based on ( [ eqgenp ] ) , every diagonal block of every block diagonal factor is either a leading block of the input matrix @xmath2 or the schur complement @xmath96 for some integers @xmath97 and @xmath92 such that @xmath98 and @xmath99 .",
    "[ corec ] the recursive block factorization process based on equation ( [ eqgenp ] ) can be completed by involving no singular pivot blocks ( and in particular no pivot elements vanish ) if and only if the input matrix @xmath2 is strongly nonsingular .",
    "combine theorem [ thsch ] with the equation @xmath100 , implied by ( [ eqgenp ] ) .",
    "the following theorem bounds the norms of all pivot blocks and their inverses and hence bounds the condition numbers of the blocks , that is , precisely the quantities responsible for safe numerical performance of block gaussian elimination and genp .",
    "[ thnorms ] ( cf .",
    "* theorem 5.1 ) . )",
    "assume genp or block gaussian elimination applied to an @xmath1 matrix @xmath2 and write @xmath101 and @xmath102 , and so @xmath103 .",
    "then the absolute values of all pivot elements of genp and the norms of all pivot blocks of block gaussian elimination do not exceed @xmath104 , while the absolute values of the reciprocals of these elements and the norms of the inverses of the blocks do not exceed @xmath105 .",
    "observe that the inverse @xmath80 of the schur complement @xmath83 in ( [ eqgenp ] ) is the southeastern block of the inverse @xmath82 and obtain @xmath106 , @xmath107 , and @xmath108 .",
    "moreover @xmath109 , due to ( [ eqgenp ] ) . now the claimed bound follows from theorem [ thsch ] .",
    "[ reir ] by virtue of theorem [ thnorms ] the norms of the inverses of all pivot blocks involved into a complete ( and hence also into any incomplete ) recursive factorization of a strongly nonsingular matrix @xmath2 are at most @xmath105 .",
    "we have a reasonable upper bound on @xmath105 if the matrix @xmath2 is strongly well - conditioned as well . then in view of theorem [ thpert ]",
    "the inversion of all pivot blocks is numerically safe , and we say that _ genp is locally safe _ for the matrix @xmath2 .",
    "[ renrm ] in the recursive factorizations above only the factors of the leading blocks and the schur complements can contribute to the magnification of any input perturbation .",
    "namely at most @xmath85 such factors can contribute to the norm of each of the output triangular or block triangular factors @xmath110 and @xmath111 .",
    "this implies the moderately large worst case upper bound @xmath112 on their norms , which is overly pessimistic according to our tests .",
    "[ rerect ] our study in this and the next two sections can be extended readily to the cases of genp and block gaussian elimination applied to rectangular and possibly rank deficient matrices and to under- and over - determined and possibly rank deficient linear systems of equations .",
    "recursive factorization and elimination can be completed and are numerically safe when they are applied to any strongly nonsingular and strongly well - conditioned leading block of the input matrix , in particular to the input matrix itself if it is strongly nonsingular and strongly well - conditioned .",
    "_ preprocessing _ @xmath113 for a pair of nonsingular matrices @xmath114 and @xmath7 , one of which can be the identity matrix @xmath37 , reduces the inversion of a matrix @xmath2 to the inversion of a the product @xmath115 , and similarly for the solution of a linear system of equations .",
    "[ faprepr ] assume three nonsingular matrices @xmath114 , @xmath2 , and @xmath7 and a vector @xmath116",
    ". then @xmath117 , @xmath118 , @xmath119 . moreover , if @xmath120 , then @xmath121 , @xmath122 , and @xmath123 , @xmath124 .",
    "remark [ reir ] motivates the choice of the multipliers @xmath114 and @xmath7 for which the matrix @xmath115 is strongly nonsingular and strongly well - conditioned .",
    "this is likely to occur already if one of the multipliers @xmath114 and @xmath7 is the identity matrix and another one is a gaussian random matrix .",
    "the studies of pre - multiplication by @xmath114 and post - multiplication by @xmath7 are similar , and so we only prove the latter claim in the case of post - multiplication .",
    "we complete our proof in section [ ssgnp ] .",
    "it involves the norms of the inverses of the matrices @xmath125 for @xmath126 , which we estimate in this section assuming nonrandom multipliers @xmath7 .",
    "we begin with two simple lemmas .",
    "[ lepr2 ] if @xmath83 and @xmath127 are square orthogonal matrices , then @xmath128 for all @xmath56 .",
    "[ lepr1 ] suppose @xmath129 , @xmath130 , and @xmath131 . then @xmath132 if also @xmath133 , then @xmath134    we also need the following basic results ( cf .",
    "* corollary 8.6.3 ) ) .    [ thcndsb ]",
    "if @xmath135 is a submatrix of a matrix @xmath2 , then @xmath136 for all @xmath56 .",
    "[ thintpr ] suppose @xmath137 , @xmath138 , @xmath139 , @xmath140 and @xmath141 . then @xmath142 .",
    "the following theorem will enable us to estimate the norm @xmath143 .    [ 1 ] suppose @xmath144 , @xmath145 , @xmath146 , @xmath147 is svd ( cf .",
    "( [ eqsvd ] ) ) , and @xmath148 .",
    "then @xmath149    note that @xmath150 , and so @xmath151 for all @xmath56 by virtue of lemma [ lepr2 ] , because @xmath152 is a square orthogonal matrix .",
    "moreover it follows from theorem [ thcndsb ] that @xmath153 for all @xmath154 .",
    "combine this bound with the latter equations and apply lemma [ lepr1 ] .",
    "[ cor1 ] keep the assumptions of theorem [ 1 ] .",
    "then    \\(i ) @xmath155 ,    \\(ii ) @xmath156 if @xmath157 .",
    "substitute @xmath158 and @xmath159 into bound ( [ eqah ] ) , recall ( [ eqnrm+ ] ) , and obtain part ( i ) .",
    "if @xmath160 , then apply ( [ eqnrm+ ] ) to obtain that @xmath161 and @xmath162 .",
    "substitute these equations into part ( i ) and obtain part ( ii ) .",
    "let us extend the estimates of theorem [ 1 ] to the leading blocks of a matrix product .",
    "[ cogh ] keep the assumptions of theorem [ 1 ] and also suppose that the matrices @xmath163 and @xmath164 have full rank @xmath92 for a positive integer @xmath165 .",
    "then latexmath:[\\[||(ah)_{k , k}^+||\\le ||\\widehat h_{n , k}^+||~||a_{k , n}^+||\\le     note that @xmath125 and that the matrix @xmath167 has full rank .",
    "apply corollary [ cor1 ] for @xmath2 and @xmath7 replaced by @xmath167 and @xmath168 , respectively , and obtain that @xmath169 .",
    "combine ( [ eqnrm+ ] ) and theorem [ thintpr ] and deduce that @xmath170 .",
    "combine the two latter inequalities to complete the proof of part ( i ) .",
    "similarly prove part ( ii ) .",
    "fact [ faprepr ] , corollary [ corec ] and theorem [ thnorms ] together imply the following result .",
    "[ colocsf ] suppose that @xmath144 , @xmath145 , @xmath171 , and the matrices @xmath163 are strongly nonsingular and strongly well - conditioned for @xmath126 .",
    "then genp and block gaussian elimination are locally safe for the matrix product @xmath6 ( see remark [ reir ] on the concept  locally safe \" ) .",
    "in section [ srrm ] we recall the norm and condition estimates for gaussian matrices and deduce that these matrices are strongly nonsingular with probability 1 and are expected to be strongly well - conditioned . in section [ ssgnp ]",
    "we prove that the pair @xmath172 for a nonsingular and well conditioned matrix @xmath2 and a gaussian matrix @xmath7 is expected to satisfy the assumptions of corollary [ colocsf ] , implying that the application of genp and block gaussian elimination to the product @xmath6 is numerically safe .",
    "[ remu0 ] the above results do not hold if the mean greatly exceeds standard deviation of the i.i.d . entries of a multipliers @xmath7 .",
    "its power for achieving numerically safe genp and block gaussian elimination is usually lost in this case .",
    "indeed assume a mean @xmath173 and a standard deviation @xmath174 such that @xmath175 ( already @xmath176 is suficient ) .",
    "in this case the matrix @xmath7 is expected to be closely approximated by the rank-1 matrix @xmath177 where @xmath178 .",
    "[ defrndm ] a matrix is said to be _ standard gaussian random _",
    "( hereafter we say just _ gaussian _ ) if it is filled with i.i.d .",
    "gaussian random variables having mean @xmath179 and variance @xmath23 .    [ thdgr1 ] a gaussian matrix @xmath180 is strongly nonsingular with probability 1 .",
    "assume that the @xmath181 leading submatrix @xmath182 of a @xmath36 gaussian matrix @xmath180 is singular for some positive integer @xmath183 , that is , @xmath184 . since @xmath185 is a polynomial in the entries of the gaussian matrix @xmath182 , such matrices form an algebraic variety of a lower dimension in the linear space @xmath186 .",
    "( @xmath187 is an algebraic variety of a dimension @xmath188 in the space @xmath189 if it is defined by @xmath190 polynomial equations and can not be defined by fewer equations . ) clearly , lebesgue ( uniform ) and gaussian measures of such a variety equal 0 , being absolutely continuous with respect to one another .",
    "hence these measures of the union of @xmath97 such matrices are also 0 .",
    "[ thdgr ] assume a nonsingular @xmath1 matrix @xmath2 and an @xmath191 gaussian matrix @xmath168 .",
    "then the product @xmath192 is nonsingular with probability 1 .",
    "@xmath193 is a polynomials in the entries of the gaussian matrix @xmath168 .",
    "such a polynomial vanishes with probability 0 unless it vanishes identically in @xmath168 , but the matrix @xmath194 is positive definite , and so @xmath195 for @xmath196 .",
    "[ defnu ] @xmath197 denotes the random variables @xmath198 for a gaussian @xmath4 matrix @xmath180 and all @xmath56 , while @xmath199 , @xmath200 , @xmath201 , and @xmath202 denote the random variables @xmath203 , @xmath204 , @xmath205 , and @xmath206 , respectively .",
    "note that @xmath207 , @xmath208 , @xmath209 , and @xmath210 .",
    "[ thsignorm ] ( cf .",
    "* theorem ii.7 ) . )",
    "suppose @xmath211 , @xmath212 , and @xmath213 . then @xmath214 and @xmath215 .",
    "[ thsiguna ] ( cf .",
    "* proof of lemma 4.1 ) . )",
    "suppose @xmath216 , and @xmath217 and write @xmath218 and @xmath219",
    ". then @xmath220 .    the following condition estimates from ( * ?",
    "* theorem 4.5 ) are quite tight for large values @xmath221 , but for @xmath222 even tighter estimates ( although more involved ) can be found in @xcite .",
    "( see @xcite and @xcite on the early study . )",
    "[ thmsiguna ] if @xmath216 , then @xmath223 for @xmath224 , while @xmath225 with probability 1 .",
    "[ cogfrwc ] a gaussian matrix is expected to be strongly well - conditioned .",
    "the main result of this section is corollary [ cogmforalg ] , which supports application of genp and block gaussian elimination to the product @xmath6 of a nonsingular matrix @xmath2 and a gaussian matrix @xmath7 .",
    "we need the following simple basic lemma .",
    "[ lepr3 ] suppose @xmath7 is a gaussian matrix , @xmath83 and @xmath127 are orthogonal matrices , @xmath226 , @xmath227 , and @xmath228 for some @xmath92 , @xmath229 , and @xmath14",
    ". then @xmath230 and @xmath231 are gaussian matrices .",
    "[ cor10 ] suppose @xmath2 is a nonsingular @xmath1 matrix , @xmath7 is an @xmath191 gaussian matrix , for @xmath232 , and @xmath233 and @xmath234 are the random values of definition [ defnu ] .",
    "then    \\(i ) the matrix @xmath235 is nonsingular with probability @xmath23 ,    \\(ii ) @xmath236 , and    \\(iii ) @xmath237 .",
    "part ( i ) restates theorem [ thdgr ] . part ( ii ) follows because @xmath125 , @xmath168 is a gaussian matrix , and @xmath238 . part ( iii ) follows from corollary [ cogh ] because @xmath164 is a gaussian matrix by virtue of lemma [ lepr3 ] .",
    "[ cogmforalg ] suppose that the @xmath1 matrix @xmath2 is nonsingular and well - conditioned .",
    "then the choice of gaussian multiplier @xmath7 is expected to satisfy the assumptions of corollary [ colocsf ] .",
    "recall that @xmath125 and hence @xmath239 . then combine theorems [ thdgr ] , [ thsignorm ] , and [ thsiguna ] and corollary [ cor10 ] .",
    "this subsection involves complex matrices .",
    "a complex matrix @xmath240 is unitary if @xmath241 or @xmath242 where @xmath243 denotes its hermitian transpose , so that @xmath244 for a real matrix @xmath240 .",
    "hereafter @xmath245 denotes an @xmath14th primitive root of unity , @xmath246 is the matrix of the discrete fourier transform at @xmath14 points ( we use the acronym _ dft _ ) , and @xmath247 .",
    "an @xmath1 circulant matrix @xmath248 is defined by its first column @xmath249 .",
    "[ exc1 ] _ generation of random real circulant matrices .",
    "_ generate the vector @xmath250 of @xmath14 i.i.d .",
    "random real variables in the range @xmath251 $ ] under the uniform probability distribution on this range .",
    "define an @xmath1 circulant matrix @xmath252 with the first column @xmath250 .",
    "the following theorem links the matrices @xmath253 and @xmath254 to the class of circulant matrices .",
    "[ thcpw ] ( cf .",
    "let @xmath252 denote a circulant @xmath1 matrix defined by its first column @xmath250 and write @xmath255 . then @xmath256 .",
    "furthermore @xmath257 if the matrix @xmath252 is nonsingular .    by using fft",
    ", one can multiply the matrices @xmath253 and @xmath258 by a vector by using @xmath259 flops for any @xmath14 ( cf .",
    ", e.g. , @xcite ) , and theorem [ thcpw ] extends this complexity bound to multiplication of an @xmath1 circulant matrix and its inverses by a vector .",
    "we need @xmath3 flops in order to compute the product @xmath6 of the pair of @xmath1 matrices @xmath2 and @xmath7 .",
    "if , however , @xmath7 is a circulant matrix , then we can compute @xmath6 by using order of @xmath260 flops . for a toeplitz - like matrix @xmath2 defined by its displacement generator of bounded length @xmath261",
    ", we use @xmath262 flops in order to compute a displacement generator of length @xmath261 for the matrix @xmath6 .",
    "( see @xcite for the definition of displacement generators . ) in the case of toeplitz matrices we have @xmath263 and use @xmath264 flops .",
    "this motivates using gaussian circulant multipliers @xmath7 , that is , circulant matrices @xmath7",
    "whose first column vector is gaussian .",
    "it has been proved in @xcite that such matrices are expected to be well - conditioned , which is required for any multiplicative preconditioner .",
    "we can define a unitary circulant matrix by its first column vector @xmath265 for any set of real values @xmath266 .",
    "[ exunc ] _ generation of random unitary circulant matrices .",
    "_    \\(i ) generate a vector @xmath267 where @xmath268 ( and so @xmath269 for all @xmath270 ) and where @xmath271 are @xmath14 independent random real variables , e.g. , gaussian variables or the variables uniformly distributed in the range @xmath272 .",
    "\\(ii ) compute the vector @xmath273 , where @xmath253 denotes the @xmath1 dft matrix .",
    "output the unitary circulant matrix @xmath252 defined by its first column @xmath250 .",
    "our proof that gaussian multipliers enforce strong nonsingularity of a nonsingular matrix with probability 1 ( see theorem [ thdgr ] ) has been non - trivially extended in @xcite to the case of gaussian circulant multipliers . furthermore strong nonsingularity holds with probability close to 1 if we fill the first column of a multiplier @xmath114 or @xmath7 with i.i.d .",
    "random variables defined under the uniform probability distribution over a sufficiently large finite set ( see appendix [ srsnrm ] and @xcite ) .    in our tests with random input matrices ,",
    "gaussian circulant and general gaussian multipliers have shown the same power of supporting numerically safe genp ( see section [ sexgeneral ] ) , but we can not extend our basic lemma [ lepr3 ] and our corollary [ cogmforalg ] to the case of circulant matrices . moreover our theorem [ thcgenp ] and remark [ remext ] below show that , for a specific narrow class of input matrices @xmath2 , genp with these multipliers is expected to fail numerically .",
    "[ thcgenp ] assume a large integer @xmath14 and the @xmath1 dft matrix @xmath253 , which is unitary up to scaling by @xmath274 .",
    "\\(i ) then application of genp to this matrix fails numerically and    \\(ii ) a gaussian circulant @xmath1 multiplier @xmath275 with gaussian diagonal matrix @xmath276 ( having i.i.d .",
    "gaussian diagonal entries @xmath277 ) is not expected to fix this problem .",
    "\\(i ) subtract the first row of the block @xmath278 of the matrix @xmath253 and the resulting vector from its second row . obtain the vector @xmath279 with the norm @xmath280 .",
    "assume that @xmath14 is large and then observe that @xmath281 and that the variable @xmath282 is expected to be small , implying that @xmath283 because @xmath284 for large @xmath14 .",
    "\\(ii ) note that @xmath285 .",
    "the gaussian variable @xmath286 vanishes with probability 0 , and so we can assume that @xmath287 .",
    "multiply the first row of the block @xmath288 of the matrix @xmath289 by @xmath290 and subtract the resulting vector from the second row .",
    "obtain the vector @xmath291 with the norm @xmath292 assume that @xmath14 is large and then observe that @xmath293 and that the variable @xmath294 is expected to be small .",
    "hence @xmath295 is expected to equal 1 because @xmath296 , @xmath284 and the random variable @xmath297 is not expected to be close to 0 .",
    "[ remext ] the same argument shows that gaussian circulant multipliers @xmath252 are not expected to support genp for a bit larger class of matrices , e.g. , for @xmath298 where @xmath299 , @xmath300 , and @xmath301 and @xmath302 are two positive constants and the input size @xmath1 is large as well as where the matrix @xmath240 is strongly diagonally dominant .",
    "the reader is challenged to find out whether genp with a gaussian circulant preprocessor is expected to fail numerically for other classes of input matrices , in particular for any subclass of the classes of toeplitz or toeplitz - like matrices ( cf .",
    "@xcite and @xcite on these classes ) .",
    "another challenge is to choose a distinct random structured preprocessor for which the above problem is avoided .",
    "e.g. , consider the product @xmath303 where @xmath97 is a small integer exceeding 1 , @xmath304 are circulant matrices and @xmath305 are skew - circulant ( see the definition in @xcite ) . toward the same goal we can apply simultaneously random structured pre- and post - multipliers @xmath114 and @xmath7 , defined by some i.i.d .",
    "random parameters , or the pairs of prmb multipliers of @xcite . in the case of toeplitz or toeplitz - like input matrices",
    "@xmath2 , the multiplications @xmath306 and @xmath6 are much less costly if the multipliers @xmath114 and @xmath7 are circulant matrices , skew - circulant matrices , or the products of such matrices .",
    "suppose we seek a rank-@xmath5 approximation of a matrix @xmath307 that has a small numerical rank @xmath5 .",
    "one can solve this problem by computing svd of the matrix @xmath2 or , at a lower cost , by computing its rank - revealing factorization @xcite , @xcite , @xcite , but using random matrix multipliers instead has some benefits @xcite . in this section",
    "we study the latter randomized approach . in its first subsection",
    "we recall some relevant definitions and auxiliary results .",
    "truncate the square orthogonal matrices @xmath152 and @xmath308 and the square diagonal matrix @xmath309 of the svd of ( [ eqsvd ] ) , write @xmath310 , @xmath311 , and @xmath312 , and obtain _ thin svd _ @xmath313 now for every integer @xmath5 in the range @xmath314 , write @xmath315 and partition the matrices @xmath316 and @xmath317 into block columns , @xmath318 , and @xmath319 where @xmath320 , @xmath321 , and @xmath322 . then partition the thin svd as follows , @xmath323 and call the above decomposition the @xmath5-_truncation of thin svd _ ( [ eqthn ] ) . note that @xmath324 is an empty matrix and recall that @xmath325 denote the ranges ( that is , the column spans ) of the matrices @xmath326 and @xmath327 , respectively . if @xmath328 , then @xmath329 and @xmath330 are the left and right _ leading singular spaces _ , respectively , associated with the @xmath5 largest singular values of the matrix @xmath2",
    "the left singular spaces of a matrix @xmath2 are the right singular spaces of its transpose @xmath39 and vice versa .",
    "all matrix bases for the singular spaces @xmath329 and @xmath330 are given by the matrices @xmath331 and @xmath332 , respectively , for nonsingular @xmath333 matrices @xmath334 and @xmath335 .",
    "the bases are orthogonal if the matrices @xmath334 and @xmath335 are orthogonal .",
    "assume an @xmath4 matrix @xmath2 having a small numerical rank @xmath5 and a gaussian @xmath11 matrix @xmath7 .",
    "then according to ( * ? ? ?",
    "* theorem 4.1 ) , the column span of the matrices @xmath6 and @xmath336 is likely to approximate the leading singular space @xmath329 of the matrix @xmath2 , and if it does , then it follows that the rank-@xmath5 matrix @xmath337 approximates the matrix @xmath2 .",
    "in this subsection we recall the algorithm supporting this theorem , where temporarily we assume nonrandom multipliers @xmath7 . in the next subsections",
    "we keep it nonrandom and estimate the output approximation errors of the algorithm assuming no oversampling , suggested in @xcite .",
    "then we extend our study to the case where @xmath7 is a gaussian , and in section [ sapgm ] cover the results in the case of random structured multipliers .",
    "[ algbasmp ] * low - rank approximation of a matrix . *",
    "remarks [ reaccr ] and [ relss ] . )    input : : :    a matrix @xmath338 , its numerical rank    @xmath5 , and two integers @xmath339 and    @xmath340 .",
    "output : : :    an orthogonal matrix @xmath341 such    that the matrix @xmath342 has rank    at most @xmath261 and approximates the matrix @xmath2 .",
    "initialization : : :    @xmath343 generate an @xmath8 matrix    @xmath7 .",
    "computations : : :    @xmath343    +    1 .",
    "compute an @xmath8 orthogonal matrix    @xmath344 , sharing its range with the matrix    @xmath6 .    2 .",
    "compute and output the matrix @xmath345 and    stop .    this basic algorithm from @xcite",
    "uses @xmath346 flops overall .      in corollaries [ coerrb ] and [ coover ] of this subsection",
    "we estimate the error norms for the approximations computed by algorithm [ algbasmp ] whose oversampling parameter @xmath10 is set to 0 , namely for the approximation of an orthogonal basis for the leading singular space @xmath329 ( by column set of the matrix @xmath44 of the algorithm ) and for a rank-@xmath5 approximation of the matrix @xmath2 .",
    "we first recall the following results .",
    "[ thsng ] ( cf .",
    "( [ eqopr ] ) . )",
    "suppose @xmath2 is an @xmath4 matrix , @xmath347 is its svd , @xmath5 is an integer , @xmath348 , and @xmath349 is an orthogonal matrix basis for the space @xmath329",
    ". then @xmath350 .",
    "[ thover ] assume two matrices @xmath139 and @xmath131 and define the two matrices @xmath351 and @xmath352 of ( [ eqldtr ] ) .",
    "then @xmath353 where @xmath354 , @xmath355 .",
    "furthermore the columns of the matrix @xmath356 span the space @xmath329 if @xmath357 .",
    "these results together imply that the columns of the matrix @xmath336 form an approximate orthogonal basis of the linear space @xmath358 , and next we estimate the error norms of this approximations .",
    "[ thuvwz ] keep the assumptions of theorem [ thover ] .",
    "then    \\(i ) @xmath359 .",
    "\\(ii ) furthermore if the matrix @xmath360 is nonsingular , then @xmath361 .",
    "recall that @xmath362 then note that @xmath363 ) .",
    "combine this bound with lemma [ lepr1 ] and obtain that @xmath364 , which is not greater than @xmath365 by virtue of bound ( [ eqfrobun ] ) .",
    "this proves part ( i ) .    part ( ii ) follows because @xmath366 if the matrix @xmath360 is nonsingular and because @xmath367 and @xmath368 .",
    "combine theorems [ thpert1 ] , [ thover ] , and [ thuvwz ] to obtain the following estimates .",
    "[ coerrb ] keep the assumptions of theorem [ thover ] , let the matrix @xmath360 be nonsingular and write @xmath369 @xmath370 then @xmath371    next combine corollary [ copert ] with theorem [ thsng ] and employ the orthogonal projection @xmath372 ( cf .",
    "( [ eqopr ] ) ) to extend the latter estimate to bound the error norm of low - rank approximation of a matrix @xmath2 by means of algorithm [ algbasmp ] .",
    "[ coover ] keep the assumptions of corollary [ coerrb ] and write @xmath373 .",
    "then @xmath374    note that @xmath375 for any @xmath376 matrix @xmath240 .",
    "write @xmath377 , apply theorem [ thsng ] and obtain @xmath378",
    ". corollaries [ copert ] and [ coerrb ] together imply that @xmath379 .",
    "combine the above relationships .",
    "[ reaccr ] write @xmath380 and recall that @xmath381 for all positive integers @xmath270 and @xmath56 . therefore one can apply the power transforms @xmath382 for @xmath383 to increase the ratio @xmath384 , which shows the gap between the two singular values .",
    "consequently the bound @xmath385 on the error norm of the approximation of an orthogonal basis of the leading singular space @xmath329 by @xmath386 is expected to decrease as @xmath270 increases ( cf .",
    "* equation ( 4.5 ) ) ) .",
    "we use the matrix @xmath387 in algorithm [ algbasmp ] , but suppose we replace it with the matrices @xmath388 for small positive integer @xmath270 , or even for @xmath389 , which would amount just to symmetrization . then we would obtain low - rank approximation with the optimum error @xmath390 up to the terms of higher order in @xmath391 as long as the value @xmath392 is reasonably bounded from above .",
    "the power transform @xmath393 requires to increase by a factor of @xmath394 the number of matrix - by - vector multiplications involved , but for small positive integers @xmath270 , the additional computational cost is still dominated by the costs of computing the svd and rank - revealing factorizations .",
    "[ relss ] let us summarize our analysis .",
    "suppose that the ratio @xmath384 is large and that the matrix product @xmath395 has full rank @xmath5 and is well - conditioned .",
    "now set to 0 the oversampling integer parameter @xmath10 of algorithm [ algbasmp ] .",
    "then , by virtue of theorem [ thuvwz ] and corollaries [ coerrb ] and [ coover ] , the algorithm outputs a close approximation @xmath336 to an orthogonal bases for the leading singular space @xmath329 of the input matrix @xmath2 and a rank-@xmath5 approximation to this matrix .",
    "up to the terms of higher order , the error norm of the latter approximation is within a factor of @xmath396 from the optimal bound @xmath390 . by applying the above power transform of the input matrix @xmath2 at a low computational cost",
    ", we can decrease the error norm even below the value @xmath390 .      in this subsection",
    "we extend the results of the previous one to support the choice of gaussian multiplier @xmath7 in algorithm [ algbasmp ] , whose  actual outcome is very close to the typical outcome because of the measure concentration effect \" @xcite .    [ thrrk ] suppose @xmath139 , @xmath397 is its svd of ( [ eqsvd ] ) , @xmath398 is a gaussian matrix , and @xmath399 .",
    "\\(i ) then the matrix @xmath360 is gaussian .",
    "\\(ii ) assume the values @xmath385 and @xmath400 of corollaries [ coerrb ] and [ coover ] and the values @xmath401 and @xmath402 of definition [ defnu ]",
    ". then @xmath403 .",
    "@xmath404 is a gaussian matrix by virtue of lemma [ lepr3 ] .",
    "therefore so is its square submatrix @xmath360 as well .",
    "this proves part ( i ) , which implies part ( ii ) .",
    "[ coalg1 ] a gaussian multiplier @xmath7 is expected to support safe numerical application of algorithm [ algbasmp ] even where the oversampling integer parameter @xmath10 is set to 0 .",
    "combine theorems [ thdgr1 ] and [ thrrk ] with corollary [ cogfrwc ] .",
    "multiplication of an @xmath1 matrix @xmath2 by a gaussian matrix @xmath7 at stage 1 of algorithm [ algbasmp ] requires @xmath405 flops , but we can save a factor of @xmath406 flops by applying structured random multipliers @xmath7 . in particular we can use subsampled random fourier transforms ( srfts ) of ( * ? ? ?",
    "* equation ( 4.6 ) ) , subsampled random hadamard transforms ( srhts ) of @xcite , the chains of givens rotations ( cgrs ) of ( * ? ? ?",
    "* remark 4.5.1 ) , and the leading toeplitz submatrices @xmath407 and @xmath408 of random circulant @xmath1 matrices @xmath252 .",
    "we need just @xmath14 random parameters to define a gaussian circulant @xmath1 matrix @xmath252 and its leading toeplitz blocks @xmath407 and @xmath408 , and similarly for the other listed classes of structured matrices .",
    "[ exsrft ] for two fixed integers @xmath261 and @xmath14 , @xmath409 , srft @xmath8 matrices are the matrices of the form @xmath410 . here",
    "@xmath411 is a random @xmath1 diagonal matrix whose diagonal entries are i.i.d .",
    "variables uniformly distributed on the unit circle @xmath412 , @xmath253 is the dft matrix , and @xmath49 is a random @xmath8 permutation matrix defined by random choice of @xmath261 columns under the uniform probability distribution on the set of the @xmath14 columns of the identity matrix @xmath34 ( cf .",
    "* equation ( 4.6 ) and section 11 ) ) .",
    "theorem [ thcpw ] implies the following fact .",
    "[ cocpw ] assume an @xmath8 srft matrix @xmath83 .",
    "then @xmath413 is an @xmath8 submatrix of a unitary circulant @xmath1 matrix .    according to the extensive tests by many researchers , various random structured @xmath8 multipliers ( such as srft , srht , cgr and chr matrices )",
    "support low - rank approximation already where the oversampling parameter @xmath17 is a reasonable constant ( see @xcite and @xcite ) .",
    "in particular srft with oversampling by 20 is adequate in almost all applications of low - rank approximations @xcite .",
    "likewise , in our extensive tests covered in section [ stails ] , toeplitz multipliers defined as the @xmath11 leading blocks of @xmath1 random circulant matrices consistently supported low - rank approximation without oversampling as efficiently as gaussian multipliers .",
    "as in the case of our randomized support for genp and block gaussian elimination , formal analysis of the impact of random structured multipliers is complicated because we can not use lemma [ lepr3 ] .",
    "nevertheless , by allowing substantial oversampling , one can still prove that srft multipliers are expected to support low - rank approximation of a matrix having a small numerical rank .",
    "[ thsrtft ] error bounds for low - rank approximation with srft ( cf .",
    "* theorem 11.2 ) ) . fix four integers @xmath261 , @xmath229 , @xmath14 , and @xmath5 such that @xmath414 ^ 2\\log ( r)\\le l\\le n$ ] .",
    "assume an @xmath4 matrix @xmath2 with singular values @xmath415 , an @xmath8 srft matrix @xmath83 of example [ exsrft ] , and @xmath416 .",
    "then with a probability @xmath417 it holds that @xmath418    [ remsrtft ] clearly the theorem still holds if we replace the matrix @xmath83 by the matrix @xmath419 for a unitary matrix @xmath420 . in this case @xmath421 for the matrix @xmath49 of example [ exsrft ] and the circulant matrix @xmath275 ( cf .",
    "theorem [ thcpw ] ) . by virtue of theorem [ thsrtft ]",
    "we can expect that algorithm [ algbasmp ] would produce a rank-@xmath5 approximation if we choose a multiplier @xmath7 being an srft @xmath8 matrix or the @xmath8 submatrix @xmath422 of @xmath1 random unitary circulant matrix @xmath252 made up of its @xmath261 randomly selected columns where the selection is defined by the matrix @xmath423 of example [ exsrft ] and where @xmath261 is an integer of order @xmath424 . recall that multiplication of an @xmath1 toeplitz matrix by an @xmath8 matrix @xmath425 involves @xmath426 flops @xcite , versus @xmath427 in the straightforward algorithm .",
    "we performed numerical experiments with random general , circulant and toeplitz matrices by using matlab in the graduate center of the city university of new york on a dell computer with a intel core 2 2.50 ghz processor and 4 g memory running windows 7 .",
    "in particular we generated gaussian matrices by using the standard normal distribution function randn of matlab , and we use the matlab function rand for generating numbers in the range @xmath428 $ ] under the uniform probability distribution function for example [ exc1 ] .",
    "we display our estimates obtained in terms of the spectral matrix norm but our tests showed similar results where we used the frobenius norm instead .",
    "we applied both genp and the preprocessed genp to @xmath1 dft matrices @xmath429 and to the matrices @xmath2 generate as follows .",
    "we fixed @xmath430 and @xmath431 for @xmath432 , and first , by following ( * ? ? ?",
    "* section 28.3 ) , generated a @xmath27 matrix @xmath433 where we chose @xmath434 with @xmath435 for @xmath436 and @xmath437 for @xmath438 and where @xmath111 and @xmath187 were @xmath27 random orthonormal matrices , computed as the @xmath27 factors @xmath439 in the qr factorization of @xmath27 random matrices @xmath334 .",
    "then we generated gaussian toeplitz matrices @xmath84 , @xmath252 and @xmath411 such that @xmath440 and defined the @xmath1 matrix @xmath441 for every dimension @xmath14 , @xmath442 we run 1000 numerical tests where we solved the linear system @xmath120 with gaussian vector @xmath116 and output the maximum , minimum and average relative residual norms @xmath443 as well as the standard deviation . figure [ fig : fig1 ] and table [ tab61 ] show the norms of @xmath82 .",
    "they ranged from @xmath444 to @xmath445 in our tests .",
    "at first we describe the results of our tests for the latter class of matrices @xmath2 . as we expected gepp",
    "has always output accurate solutions to the linear systems @xmath88 in our tests ( see table [ tab62 ] ) .",
    "genp , however , was expected to fail for these systems , because the @xmath446 leading principal block @xmath447 of the matrix @xmath2 was singular , having nullity @xmath448 .",
    "indeed this caused poor performance of genp in our tests , which have consistently output corrupted solutions , with relative residual norms ranging from @xmath449 to @xmath450 .    in view of corollary [ cogmforalg ]",
    "we expected to fix this deficiency by means of multiplication by gaussian matrices , and indeed in all our tests we observed residual norms below @xmath451 , and they decreased below @xmath452 in a single step of iterative refinement ( see table [ tab63 ] ) .",
    "furthermore the tests showed the same power of preconditioning where we used the circulant multipliers of examples [ exc1 ] and [ exunc ] ( see tables [ tab64 ] and [ tab65 ] ) .",
    "as can be expected , the output accuracy of genp with preprocessing has deteriorated a little versus gepp in our tests .",
    "the output residual norms , however , were small enough to support application of the inexpensive iterative refinement .",
    "already its single step decreased the average relative residual norm below @xmath453 for @xmath454 in all our tests with gaussian multipliers and to about @xmath455 for @xmath454 in all our tests with circulant multipliers of examples [ exc1 ] and [ exunc ] .",
    "see further details in figures [ fig : fig2 ] and [ fig : fig3 ] and tables [ tab63][tab65 ] .",
    "this indicates that genp with preprocessing followed by even a single step of iterative refinement is backward stable , similarly to the celebrated result of @xcite .",
    "we also applied similar tests to the @xmath1 dft matrix @xmath429 .",
    "the results were in very good accordance with our study in section [ srnd ] .",
    "of course in this case the solution of a linear system @xmath120 can be computed immediately as @xmath456 , but we were not seeking the solution , but were trying to compare the performance of genp with and without preprocessing . in these tests the norm @xmath457 was fixed at @xmath274 .",
    "gepp produced the solution within the relative residual norm between @xmath458 and @xmath459 , but genp failed on the inputs @xmath253 both when we used no preprocessing and used preprocessing with random circulant multipliers of examples [ exc1 ] and [ exunc ] . in these cases the relative residual norms of the output approximations ranged between @xmath460 and @xmath461 .",
    "in contrast genp applied to the inputs preprocessed with gaussian multipliers produced quite reasonable approximations to the solution .",
    "already after a single step of iterative refinement , they have at least matched the level of gepp .",
    "table [ tab65a ] displays these norms in some detail .",
    ", scaledwidth=80.0% ]              we approximated the @xmath5-dimensional leading singular spaces of @xmath1 matrices @xmath2 that have numerical rank @xmath5 , and we also approximated these matrices with matrices of rank @xmath5 . for @xmath462 and @xmath463",
    "we generated @xmath1 random orthogonal matrices @xmath83 and @xmath127 and diagonal matrices @xmath464 such that @xmath465 , @xmath466 ( cf .",
    "* section 28.3 ) ) .",
    "then we computed the input matrices @xmath467 , for which @xmath468 and @xmath469 .",
    "furthermore we generated @xmath11 random matrices @xmath7 and computed the matrices @xmath470 , @xmath471 , @xmath326 , @xmath472 , @xmath473 , and @xmath474 .",
    "figures [ fig : fig4][fig : fig7 ] and tables [ tab66][tab614 ] display the resulting data on the residual norms @xmath475 and @xmath476 , obtained in 1000 runs of our tests for every pair of @xmath14 and @xmath5 . in these figures and tables",
    "@xmath477 denotes the residual norms of the approximations of the matrix bases for the leading singular spaces @xmath329 , and @xmath478 denotes the residual norms of the approximations of the matrix @xmath2 by the rank-@xmath5 matrix @xmath474 .",
    "figures [ fig : fig4 ] and [ fig : fig5 ] and tables [ tab66][tab613 ] show the norm @xmath477 .",
    "the last column of each of the tables displays the ratio of the observed values @xmath477 and its upper bound @xmath479 estimated up to the higher order terms ( cf .",
    "corollary [ coerrb ] ) . in our tests",
    "we had @xmath480 and @xmath481 .",
    "table [ tab66 ] covers the case where we generated gaussian multipliers @xmath7 .",
    "tables [ tab610 ] and [ tab613 ] cover the cases where we generated random @xmath1 circulant matrices of examples [ exc1 ] and [ exunc ] , respectively , and applied their @xmath11 toeplitz leading blocks as multipliers @xmath7 .",
    "figures [ fig : fig6 ] and [ fig : fig7 ] and tables [ tab67][tab614 ] show similar results of our tests for the observed residual norms @xmath478 and their ratios with their upper bounds @xmath482 , estimated up to the higher order terms ( cf .",
    "corollary [ coover ] ) .",
    "tables [ tab69][tab616 ] show some auxiliary information .",
    "namely , table [ tab69 ] displays the data on the ratios @xmath483 , where @xmath484 denotes the @xmath333 leading submatrix of the matrix @xmath7 .",
    "tables [ tab616 ] and [ tab617 ] display the average condition numbers of gaussian @xmath1 matrices and circulant @xmath1 matrices @xmath252 of example [ exc1 ] , respectively .",
    "the test results are in quite good accordance with our theoretical study of gaussian multipliers and suggest that the power of random circulant and toeplitz multipliers is similar to the power of gaussian multipliers , as in the case of various random structured multipliers of @xcite and @xcite .",
    "using different random multipliers , case r=8,scaledwidth=80.0% ]     using different random multipliers , case r=32,scaledwidth=80.0% ]     using different random multipliers , case r=8,scaledwidth=80.0% ]     using different random multipliers , case r=32,scaledwidth=80.0% ]",
    "it is known that a standard gaussian random matrix ( we refer to it as gaussian for short ) has full rank with probability 1 and is well - conditioned with a probability close to 1 .",
    "these properties motivated our application of gaussian multipliers to advancing matrix computations .",
    "in particular we preprocessed well - conditioned nonsingular input matrices by using gaussian multipliers to support genp ( that is , gaussian elimination with no pivoting ) and block gaussian elimination .",
    "these algorithms can readily fail in practical numerical computations without preprocessing , but we proved that we can avoid these problems with a probability close to 1 if we preprocess the input matrix by pre- or post - multiplying it by a gaussian matrix .",
    "our tests were in good accordance with that formal result , that is , we generated matrices that were hard for genp , but the problems were consistently avoided when we preprocessed the inputs with gaussian multipliers . in that case a single loop of iterative refinement was always sufficient to match the output accuracy of the customary algorithm of gepp , indicating that genp with preprocessing followed by even a single step of iterative refinement is backward stable , similarly to the celebrated result of @xcite .    in our tests",
    "we observed similar results even where we applied gaussian circulant ( rather than gaussian ) multipliers . under this choice",
    "we generated only @xmath14 random parameters for an @xmath1 input , and the multiplication stage was accelerated by a factor of @xmath19 .",
    "the acceleration factor increases to @xmath20 when the input matrix has the structure of toeplitz type , but we could support numerical stabilization of genp with gaussian circulant multipliers only empirically . moreover , we proved that with a high probability gaussian circulant multipliers can not fix numerical instability of the elimination algorithms for a specific narrow class of inputs ( see theorem [ thcgenp ] and remark [ remext ] ) .",
    "this should motivate the search for alternative randomized structured multipliers that would be expected to stabilize numerical performance of genp and block gaussian elimination for any input or , say , for any toeplitz and toeplitz - like input matrix . among the candidate multipliers",
    ", one can consider the products of random circulant and skew - circulant matrices , possibly used as both pre- and post - multipliers .",
    "suppose that indeed they are expected to stabilize block gaussian elimination numerically",
    ". then their support would be valuable for numerical application of the mba celebrated algorithm , because it is superfast for toeplitz and toeplitz - like input matrices and hence for their products with circulant and skew - circulant matrices ( cf .",
    ", e.g. , @xcite , ( * ? ? ?",
    "* chapter 5 ) , and @xcite ) .",
    "we have extended our analysis to the problem of rank-@xmath5 approximation of an @xmath4 matrix @xmath2 having a numerical rank @xmath5 . with a probability close to @xmath23",
    "the column set of the matrix @xmath6 , for an @xmath485 gaussian matrix @xmath7 and a small positive oversampling integer parameter @xmath10 , approximates a basis for the left leading singular space @xmath486 associated with the @xmath5 largest singular values of an @xmath4 matrix @xmath2 .",
    "having such an approximate basis available , one can readily approximate the matrix @xmath2 by a matrix of rank @xmath5 .",
    "this is an efficient , well developed algorithm ( see @xcite ) , but we proved that this algorithm is expected to produce a reasonable rank-@xmath5 approximation with gaussian multipliers already for @xmath487 , that is , even without customary oversampling , recommended in @xcite .",
    "then again in our tests the latter techniques were efficient even where instead of gaussian multipliers we applied random toeplitz multipliers , defined as the maximal leading submatrices of random circulant matrices .",
    "this has accelerated the multiplication stage and has limited randomization to @xmath14 parameters for an @xmath1 input .",
    "formal proof of the power of random structured srft multipliers with substantial oversampling is known for low - rank approximation ( * ? ? ?",
    "* section 11 ) , and we immediately extended it to the case when the products of random unitary circulant multipliers and random rectangular permutation matrices were applied instead of the srft matrices ( see section [ srnd ] ) .    a natural research challenge is the combination of our randomized multiplicative preprocessing with randomized augmentation and additive preprocessing , studied in @xcite , @xcite , @xcite , @xcite , and @xcite .",
    "* acknowledgements : * our research has been supported by nsf grant ccf1116736 . and psc cuny awards 645120042 and 657920043 .",
    "we are also grateful to the reviewer for thoughtful helpful comments .",
    "hspace0.5 in    o. axelsson , _ iterative solution methods _ , cambridge univ . press ,",
    "england , 1994 .",
    "j. r. bunch , stability of methods for solving toeplitz systems of equations , _",
    "siam j. sci .",
    "_ , * 6**(2 ) * , 349364 , 1985 .",
    "m. benzi , preconditioning techniques for large linear systems : a survey , _ j. of computational physics _ , * 182 * , 418477 , 2002 .",
    "d. becker , m. baboulin , j. dongarra , reducing the amount of pivoting in symmetric indefinite systems , _ proceedings of the 9th international conference on parallel processing and applied mathematics , ppam 2011 , lecture notes in computer science _ , * 7203 * , 133142 , springer - verlag ( 2012 ) .",
    "also _ inria _ research report 7621 ( 05/2011 ) , _ university of tennessee _ technical report icl - ut-11 - 06 .",
    "g. ballard , e. carson , j. demmel , m. hoemmen , n. knight , o. schwartz , communication lower bounds and optimal algorithms for numerical linear algebra .",
    "_ acta numerica _ , * 23 * , 1155 , 2014 .",
    "d. bini , v. y. pan , _ polynomial and matrix computations , volume 1 : fundamental algorithms _ ,",
    "birkhuser , boston , 1994 .",
    "z. chen , j. j. dongarra , condition numbers of gaussian random matrices , _ siam .",
    "j. on matrix analysis and applications _ , * 27 * , 603620 , 2005",
    ".    s. chandrasekaran , m. gu , x. sun , j. xia , j. zhu , a superfast algorithm for toeplitz systems of linear equations , _ siam .",
    "j. on matrix analysis and applications _ , * 29 * , * 4 * , 12471266 , 2007",
    ".    r. e. cline , r. j. plemmons , and g. worm , generalized inverses of certain toeplitz matrices , _ linear algebra and its applications , _ * 8 * , 2533 , 1974 .",
    "j. demmel , the probability that a numerical analysis problem is difficult , _ math . of computation",
    "_ , * 50 * , 449480 , 1988 .",
    "r. a. demillo , r. j. lipton , a probabilistic remark on algebraic program testing , _ information processing letters _ , * 7 * , * 4 * , 193195 , 1978 .",
    "k. r. davidson , s. j. szarek , local operator theory , random matrices , and banach spaces , in _",
    "handbook on the geometry of banach spaces _",
    "( w. b. johnson and j. lindenstrauss editors ) , pages 317368 , north holland , amsterdam , 2001 .",
    "a. edelman , eigenvalues and condition numbers of random matrices , _",
    "siam j. on matrix analysis and applications _ ,",
    "* 9 * , * 4 * , 543560 , 1988 .",
    "a. edelman , b. d. sutton , tails of condition number distributions , _ siam j. on matrix analysis and applications _ ,",
    "* 27 * , * 2 * , 547560 , 2005 .",
    "a. greenbaum , _ iterative methods for solving linear systems _ , siam , philadelphia , 1997 .",
    "m. gu , stable and efficient algorithms for structured systems of linear equations , _",
    "siam j. on matrix analysis and applications _ , * 19 * , 279306 , 1998 .",
    "m. gu , s. c. eisenstat , efficient algorithms for computing a strong rank - revealing qr factorization , _ siam journal on scientific computing _ , * 17 * , 848869 , 1996 .",
    "i. gohberg , t. kailath , v. olshevsky , fast gaussian elimination with partial pivoting for matrices with displacement structure , _ mathematics of computation _ , * 64 * , 15571576 , 1995 .",
    "g. h. golub , c. f. van loan , _ matrix computations _ , johns hopkins university press , baltimore , maryland , 2013 ( fourth addition ) .",
    "n. j. higham , _ accuracy and stability in numerical analysis _ , siam , philadelphia , 2002 ( second edition ) .",
    "n. halko , p. g. martinsson , j. a. tropp , finding structure with randomness : probabilistic algorithms for constructing approximate matrix decompositions , _ siam review _ , * 53 ,  2 * , 217288 , 2011 .",
    "y. p. hong , c.t .",
    "pan , the rank revealing qr decomposition and svd , _ math . of computation",
    "_ , * 58 * , 213232 , 1992",
    ".    m. w. mahoney , randomized algorithms for matrices and data , _ foundations and trends in machine learning _",
    ", now publishers , * 3 ,  2 * , 2011 .",
    "( abridged version in : advances in machine learning and data mining for astronomy , edited by m. j. way , et al .",
    "647 - 672 , 2012 . )",
    "p. g. martinsson , v. rokhlin , m. tygert , a fast algorithm for the inversion of toeplitz matrices , _ comput .",
    "_ , * 50 * , 741752 , 2005 .",
    "martinsson , v. rokhlin , m. tygert , a randomized algorithm for the decomposition of matrices , _ applied and computational harmonic analysis _ , * 30 * , 4768 , 2011 .",
    "v. y. pan , on computations with dense structured matrices , _ math . of computation",
    "_ , * 55 ,  191 * , 179190 , 1990 . also in _ proc . intern .",
    "symposium on symbolic and algebraic computation ( issac89 ) _ , 3442 , acm press , new york , 1989 .",
    "pan , on the existence and computation of rank - revealing lu factorization , _ linear algebra and its applications _ , * 316 * , 199222 , 2000 .    v.",
    "y. pan , _ structured matrices and polynomials : unified superfast algorithms _ , birkhuser / springer , boston / new york , 2001",
    ".    v. y. pan , transformations of matrix structures work again , _ linear algebra and its applications _ , 465 , 132 , 2015 .",
    "available at arxiv:1311.3729[math.na ]    v. y. pan , fast approximation algorithms for computations with cauchy matrices , polynomials and rational functions , _ proc . of the ninth international computer science symposium in russia ( csr2014 ) _ , ( e. a. hirsch et al .",
    ", editors ) , moscow , russia , june 2014 , _ lecture notes in computer science ( lncs ) _ , * 8476 * , pp . 287 - 300 springer international publishing , switzerland , 2014 .",
    "also tech .",
    "report tr 2014005 , _ phd program in comp .",
    "_ , _ graduate center , cuny _ , 2014 .",
    "v. y. pan , d. grady , b. murphy , g. qian , r. e. rosholt , a. ruslanov , schur aggregation for linear systems and determinants , _ theoretical computer science _ , _ special issue on symbolic  numerical algorithms _",
    "( d. a. bini , v. y. pan , and j. verschelde editors ) , * 409 * , * 2 * , 255268 , 2008 .",
    "v. y. pan , d. ivolgin , b. murphy , r. e. rosholt , y. tang , x. yan , additive preconditioning for matrix computations , _ linear algebra and its applications _ , * 432 * , 10701089 , 2010 .",
    "d. s. parker , b. pierce , the randomizing fft : an alternative to pivoting in gaussian elimination , tech .",
    "report csd 950037 , _ computer science dept .",
    "california at los angeles _ , 1995 .",
    "v. y. pan , g. qian , randomized preprocessing of homogeneous linear systems of equations , _ linear algebra and its applications _ , * 432 * , 32723318 , 2010 .",
    "v. y. pan , g. qian , solving linear systems of equations with randomization , augmentation and aggregation , _ linear algebra and its applications _ , * 437 * , 28511876 , 2012 .",
    "v. y. pan , g. qian , x. yan , supporting genp with random multipliers , tech .",
    "report tr 2013016 , _ phd program in comp .",
    "_ , _ graduate center , cuny _ , 2013 ,    v. y. pan , g. qian , a. zheng , randomized preconditioning of the mba algorithm , in _ proc . international symp . on symbolic and algebraic computation ( issac2011 ) _ , san jose , california , june 2011 ( edited by anton leykin ) , 281288 , acm press , new york , 2011",
    ".    v. y. pan , g. qian , a. zheng , randomized preprocessing versus pivoting , _ linear algebra and its applications _ , * 438 ,  4 * , 18831899 , 2013 .    v. y. pan , g. qian , a. zheng , randomized matrix computations , tech .",
    "report tr 2012009 , _ phd program in comp .",
    "_ , _ graduate center , cuny _ , 2012 .    and    v. y. pan , g. qian , l. zhao , randomized augmentation and additive preprocessing , preprint , 2014 .    v. y. pan , g. qian , a. zheng , z. chen , matrix computations and polynomial root - finding with preprocessing , _ linear algebra and its applications _ , 2014 , doi : 10.1016/j.laa.2014.06.027 .",
    "also arxiv:1311.3730[math.na ] .",
    "v. y. pan , j. svadlenka , l. zhao , estimating the norms of circulant and toeplitz random matrices and their inverses , _ linear algebra and its applications _ , 2014 ( in press ) , doi : 10.1016/j.laa.2014.06.027 .",
    "also arxiv:1311.3730[math.na ]    v. y. pan , l. zhao , a single random triangular toeplitz multiplier ensures strong nonsingularity , preprint , 2014 .",
    "r. d. skeel , iterative refinement im[plies numerical stability for gaussian elimimation , _ math . of computation",
    "_ , * 35 ,  151 * , 817832 , 1980 .",
    "j. t. schwartz , fast probabilistic algorithms for verification of polynomial identities , _ journal of acm _ ,",
    "* 27 * , * 4 * , 701717 , 1980 .",
    "s. j. szarek , condition numbers of random matrices , _ journal of complexity _ , * 7 ,  2 * , 131149 , 1991 .",
    "sun , on perturbation bounds for qr factorization , _ linear algebra and its applications _ , * 215 * , 95111 , 1995 .",
    "g. w. stewart , _ matrix algorithms , vol i : basic decompositions _ , siam , philadelphia , 1998 .",
    "g. w. stewart , _ matrix algorithms , vol ii : eigensystems _ , siam , philadelphia , 2003",
    ".    a. sankar , d. spielman , s .- h .",
    "teng , smoothed analysis of the condition numbers and growth factors of matrices , _",
    "siam j. on matrix analysis _ , * 28 * , * 2 * , 446476 , 2006 .    j. a. tropp , improved analysis of the subsampled randomized hadamard transform , _ adv .",
    "data anal .",
    "_ , * 3 ,  12 * , special issue , `` sparse representation of data and images , '' 115 - 126 , 2011 .",
    "r. e. zippel , probabilistic algorithms for sparse polynomials , _ proceedings of eurosam79 , lecture notes in computer science _ , * 72 * , 216226 , springer , berlin , 1979 .    * *",
    "let us reproduce some known bounds for the expected values of the norms and condition numbers of random matrices .",
    "[ thszcd ] ( i ) @xmath488 , ( ii ) @xmath489 for @xmath216 .",
    "see @xcite for part ( i ) and ( * ? ? ?",
    "* theorem 6.1 ) for part ( ii ) .",
    "the bounds of part ( i ) of the theorem are quite tight ( cf . theorem [ thsignorm ] ) .",
    "the bounds of part ( ii ) imply the following more specific estimates .",
    "[ cocd ] @xmath490 , @xmath491 for @xmath492 and @xmath493 .",
    "the paper @xcite proposed using algorithm [ algbasmp ] with the positive oversampling integer parameter @xmath10 ( see ( * ? ? ?",
    "* algorithm 4.1 and theorems 10.1 and 10.6 ) ) .",
    "this choice relied on the following bounds of ( * ? ? ?",
    "* theorems 10.5 and 10.6 ) on the expected value @xmath494 of the output error norm of the algorithm for @xmath495 , @xmath496 @xmath497 here is a simplified variant of the latter estimate from ( * ? ? ?",
    "* equation ( 1.8 ) ) , @xmath498    quite typically the values @xmath499 for @xmath71 are not known , but one can adapt the parameter @xmath261 by using a posteriori error estimation",
    ". one can simplify this estimation by recalling from ( * ? ? ?",
    "* equation ( 4.3 ) ) that @xmath500 .",
    "here @xmath501 is the @xmath56th column of @xmath11 gaussian matrix , that is , @xmath502 are @xmath5 independent gaussian vectors of length @xmath14 , and @xmath5 is an integer parameter ( see our remark [ reaccr ] on improving this approximation ) .",
    "here is an alternative simplified expression from ( * ? ? ?",
    "* equation ( 1.9 ) ) , @xmath503 under some mild assumptions on the positive oversampling integer @xmath10 .",
    "the above bounds show that low - rank approximations of high quality can be obtained by using a reasonably small oversampling integer parameter @xmath10 , say @xmath504 , but they do not apply where @xmath505 .",
    "our analysis of the basic algorithms relies on corollary [ cor10 ] and provides some reasonable formal support even where @xmath487 .",
    "_ uniform random sampling _ of elements from a finite set @xmath506 is their selection from this set at random , independently of each other and under the uniform probability distribution on the set @xmath506 .",
    "the total degree of a multivariate monomial is the sum of its degrees in all its variables .",
    "the total degree of a polynomial is the maximal total degree of its monomials .",
    "[ ledl ] @xcite , @xcite , @xcite . for a set @xmath506 of a cardinality @xmath507 in any fixed ring",
    "let a polynomial in @xmath229 variables have a total degree @xmath508 and let it not vanish identically on the set @xmath509 .",
    "then the polynomial vanishes in at most @xmath510 points of this set .",
    "[ thdl ] under the assumptions of lemma [ ledl ] let the values of the variables of the polynomial be randomly and uniformly sampled from a finite set @xmath506",
    ". then the polynomial vanishes with a probability at most @xmath511 .",
    "[ codlstr ] let the entries of a general or toeplitz @xmath4 matrix have been randomly and uniformly sampled from a finite set @xmath506 of cardinality @xmath507 ( in any fixed ring ) .",
    "let @xmath512 . then ( a )",
    "every @xmath27 submatrix @xmath240 for @xmath513 is nonsingular with a probability at least @xmath514 and ( b ) is strongly nonsingular with a probability at least @xmath515 .    clearly",
    "the claims of the corollary hold for generic matrices .",
    "now note that the singularity of a @xmath27 matrix means that its determinant vanishes , but the determinant is a polynomial of total degree @xmath92 in the entries .",
    "therefore theorem [ thdl ] implies parts ( a ) and consequently ( b ) .",
    "part ( c ) follows because a fixed entry of the inverse vanishes if and only if the respective entry of the adjoint vanishes , but up to the sign the latter entry is the determinant of a @xmath516 submatrix of the input matrix @xmath240 , and so it is a polynomial of degree @xmath517 in its entries .",
    "[ thpert ] ( * ? ? ?",
    "* corollary 1.4.19 ) . assume a pair of square matrices @xmath2 ( nonsingular ) and @xmath518 such that @xmath519",
    ". then @xmath520 and @xmath521 .",
    "[ thpert1 ] ( * ? ? ? * theorem 5.1 ) .",
    "assume a pair of @xmath4 matrices @xmath2 and @xmath522 , and let the norm @xmath523 be small",
    ". then @xmath524 .",
    "@xmath525 denotes the _ orthogonal projector _ on the range of a matrix @xmath2 having full column rank ,    @xmath526    [ copert ] suppose @xmath4 matrices @xmath2 and @xmath522 have full rank .",
    "then    @xmath527    clearly @xmath528 @xmath529 consequently    @xmath530 and @xmath531 and obtain that @xmath532 .",
    "substitute the bound of theorem [ thpert1 ] .",
    ".the norms @xmath533 of the input matrices @xmath2 [ cols=\"^,^,^,^,^,^ \" , ]"
  ],
  "abstract_text": [
    "<S> we study two applications of standard gaussian random multipliers . at </S>",
    "<S> first we prove that with a probability close to 1 such a multiplier is expected to numerically stabilize gaussian elimination with no pivoting as well as block gaussian elimination . </S>",
    "<S> then , by extending our analysis , we prove that such a multiplier is also expected to support low - rank approximation of a matrix without customary oversampling . </S>",
    "<S> our test results are in good accordance with this formal study . </S>",
    "<S> the results remain similar when we replace gaussian multipliers with random circulant or toeplitz multipliers , which involve fewer random parameters and enable faster multiplication . </S>",
    "<S> we formally support the observed efficiency of random structured multipliers applied to approximation , but not to elimination . </S>",
    "<S> moreover , we prove that with a probability close to 1 gaussian random circulant multipliers do not fix numerical instability of the elimination algorithms for a specific narrow class of well - conditioned inputs . </S>",
    "<S> we know of no such hard input classes for various alternative choices of random structured multipliers , but for none of such multipliers we have a formal proof of its efficiency for numerical gaussian elimination .    </S>",
    "<S> [ [ math .- subject - classification ] ] * 2000 math . </S>",
    "<S> subject classification : * + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    15a52 , 15a12 , 15a06 , 65f22 , 65f05    [ [ key - words ] ] * key words : * + + + + + + + + + + + +    random matrices , random multipliers , gaussian elimination , pivoting , block gaussian elimination , low - rank approximation , srft matrices , random circulant matrices </S>"
  ]
}