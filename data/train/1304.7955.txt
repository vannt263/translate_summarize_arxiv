{
  "article_text": [
    "many mechanical and biological systems are controlled by signals which contain noise .",
    "this poses a problem .",
    "the noise apparently corrupts the control signal , thereby preventing precise control . however , precise control can be realised , despite the occurrence of noise , as has been demonstrated experimentally in biological systems . for example , in neural - motor control , as reported in @xcite , the movement error is believed to be mainly due to inaccuracies of the neural - sensor system , and not associated with the neural - motor system .",
    "the minimum - variance principle proposed in @xcite has greatly influenced the theoretical study of biological computation . assuming the magnitude of the noise in a system depends strongly on the magnitude of the signal , the conclusion of @xcite is that a biological system is controlled by minimising the execution error .",
    "a key feature of the control signal in a biological system is that biological computation often only takes on a finite number of values .",
    "for example , ` bursting ' neuronal pulses in the neural - motor system control seem very likely to have only three states , namely inactive , excited , and inhibited .",
    "this kind of signal ( neuronal pulses ) can be abstracted as a dynamic trajectory which is zero for most of the time , but intermittently takes a very large value . generally , this kind of signal looks like a train of irregularly spaced dirac - delta functions . in this work",
    "we shall theoretically investigate the way signals in realistic biological systems are associated with precise control performance .",
    "we shall use bursting neuronal pulse trains as a prototypical example of this phenomenon .    in a biological system",
    ", noise is believed to be inevitable and essential ; it is a part of a biological signal and , for example , the magnitude of the noise typically depends strongly on the magnitude of the signal @xcite .",
    "one characteristic of the noise in a system is the dispersion index , @xmath0 , which describes the statistical regularity of the control signal .",
    "when the variance in the control signal is proportional to the @xmath1-th power of the mean control signal , the dispersion index of the control noise is said to be @xmath0 .",
    "it was shown in @xcite and elsewhere ( e.g. , @xcite ) that an optimal solution of analytic form can be found when the stochastic control signal is supra - poisson , i.e. , when @xmath2 . however , the resulting control is not precise and a non - zero execution error arises . in recent papers ,",
    "a novel approach was proposed to find the optimal solution for control of a neural membrane @xcite , and a model of saccadic eye movement @xcite .",
    "it was shown that if the noise of the control signal is more regular than poisson process ( i.e. , if it is sub - poisson , with @xmath3 ) , then the execution error can be shown to reduce towards zero @xcite .",
    "this work employed the theory of young measures young , young1 , and involved a very specific sort of solution ( a ` relaxed optimal parameterized measure solution ' ) .",
    "we note that _ many _ biological signals are more regular than a poisson process : e.g. , within in - vivo experiments , it has often been observed that neuronal pulse signals are sub - poisson in character ( @xmath3 ) @xcite .",
    "however , in @xcite , only a one - dimensional linear model was studied in detail .",
    "thus the results and methods can not be applied to the control of general dynamical systems .",
    "the work of @xcite however , leads to a much harder problem : the general mathematical link between the regularity of the signal s noise and the control performance that can be achieved .    in the present work we establish some general mathematical principles linking the regularity of the noise in a control signal with the precision of the resulting control performance , for _ general _ nonlinear dynamical systems of high dimension .",
    "we establish a general theoretical framework that yields _ precise control _ from a _ noisy controller _ using modern mathematical tools .",
    "the control signal is formulated as a gaussian ( random ) process with a signal - dependent variance .",
    "our results show that if the control signal is more regular than a poisson process ( i.e. , if @xmath3 ) , then the control optimisation problem naturally involves solutions with a specific singular character ( parameterized measure optimal solutions ) , which can achieve precise control performance .",
    "in other words , we show how to achieve results where the variance in control performance can be made arbitrarily small .",
    "this is in clear contrast to the situation where the control signals are poisson or more random than poisson ( @xmath2 ) , where the optimal control signal is an ordinary function , not a parameterized measure , and the variance in control performance does not approach zero .",
    "the new results can be applied to a large class of control problems in nonlinear dynamical systems of high dimension .",
    "we shall illustrate the new sort of solutions with an example of neural - motor control , given by the control of straight - trajectory arm movements , where neural pulses act as the control signals .",
    "we show how pulse trains may be realised in nature which lead towards the optimisation of control performance .",
    "to establish a theoretical approach to the problem of noisy control , we shall consider the following general system @xmath4where : @xmath5 is time ( @xmath6 ) , @xmath7^{\\top } $ ] is a column vector of ` coordinates ' describing the state of the system to be controlled ( a @xmath8-superscript denotes transpose ) and @xmath9^{\\top } $ ] , is a column vector of the signals used to control the @xmath10 system .",
    "the dynamical behaviour of the @xmath10 system , in the absence of a control signal , is determined by @xmath11 and @xmath12 , where @xmath11 consists of @xmath13 functions : @xmath14^{\\top } $ ] and @xmath12 is an @xmath15 ` gain matrix ' with elements @xmath16 .",
    "the system ( [ general ] ) is a generalisation of the dynamical systems studied in the literature harris , harris1,feng2,ross .",
    "as stated above , the control signal , @xmath17 , contains noise .",
    "we follow harris s work @xcite on signal - dependent noise theory by modelling the components of the control signal as@xmath18where @xmath19 is the mean control signal at time @xmath5 of the @xmath20th component of @xmath17 and all noise ( randomness ) is contained in @xmath21 .",
    "in particular , we take the @xmath22 to be an independent gaussian white noises obeying @xmath23 = 0 $ ] , @xmath24 = \\sigma _ { i}(t)\\sigma _ { j}(t^{\\prime } ) \\delta ( t - t^{\\prime } ) \\delta _ { ij}$ ] where @xmath25 $ ] denotes expectation , @xmath26 is dirac - delta function , and @xmath27 is kronecker delta .",
    "the quantities @xmath28 , which play the role of standard deviations of the @xmath22 , are taken to explicitly depend on the mean magnitudes of the control signals : @xmath29where @xmath30 is a positive constant and @xmath0 is the dispersion index of the control process ( described above ) .",
    "thus , we can formulate the dynamical system , eq .",
    "( [ general ] ) , as a system of it diffusion equations : @xmath31where : ( i ) @xmath32^{\\top } $ ] contains @xmath33 independent standard wiener processes ; ( ii ) the quantity @xmath34 denotes the column vector @xmath35^{\\top } $ ] , the @xmath20th component of which has the form @xmath36 ; ( iii ) the quantity @xmath37 is the matrix , the @xmath38th element of which is given by @xmath39 where @xmath40 and @xmath41 .",
    "we make the assumption that the range of each @xmath42 is bounded : @xmath43 with @xmath44 a positive constant .",
    "let @xmath45^{m}$ ] be the region where the control signal takes values , with @xmath46 denoting the @xmath33-order cartesian product .",
    "let @xmath47 be state space of @xmath10 . in this paper",
    "we assume it be bounded .",
    "let us now introduce the function @xmath48^{\\top } $ ] , which represents the objective that is to be controlled and optimised .",
    "for example , for a linear output we can take @xmath49 for some @xmath50 matrix @xmath51 ; in the case that we control the magnitude of @xmath10 , we can take @xmath52 ; we may even allow dependence on time , for example if the output decays exponentially with time , we can take @xmath53 for some constant @xmath54 .",
    "the aim of the control problem we consider here is : ( i ) to ensure the expected trajectory of the objective @xmath55 reaches a specified target at a given time , @xmath56 , and ( ii ) to minimise the _ execution error _ accumulated by the system , during the time , @xmath57 , that the system is required to spend at the ` target ' @xcite . in the present context ,",
    "we take the motion to start at time @xmath58 and subject to the initial condition @xmath59 .",
    "the target has coordinates @xmath60 and we need to choose the controller , @xmath17 , so that for the time interval @xmath61 the expected state of the objective @xmath62 of the system satisfies @xmath63=z(t)$ ] .",
    "the _ accumulated execution error",
    "_ is @xmath64 and we require this to be minimised .",
    "statistical properties of @xmath65 can be written in terms of @xmath66 , the probability density of the state of the system ( [ ito ] ) at time @xmath5 , which satisfies the fokker - plank equation@xmath67 \\label{fpe}\\ ] ] with @xmath68}{\\partial x_{i } } \\\\ & + \\frac{1}{2}\\sum_{i , j=1}^{n}\\frac{\\partial^{2}\\{\\sum_{k=1}^{m}\\kappa _ { k}^{2}b_{ik}(x , t)b_{jk}(x , t)|\\lambda_{k}(t)|^{2\\alpha}p(x , t)\\}}{\\partial x_{i}\\partial x_{j}}.\\end{aligned}\\ ] ]    three important quantities are the following :    * the _ accumulated _ _ execution error _ : @xmath69 ; * the _ expectation condition _ on @xmath65 : @xmath70 , for all @xmath5 in the interval @xmath71 ; * the _ dynamical equation _ of @xmath66 described as ( [ fpe ] ) .",
    "to illustrate the idea of the solutions we introduce here , namely young measure optimal solutions , we provide a simple example .",
    "consider the situation where @xmath10 and @xmath72 are one - dimensional functions , while @xmath73 , @xmath74 , @xmath75 , @xmath76 and @xmath77 .",
    "thus ( general ) becomes@xmath78this has the solution @xmath79 .",
    "thus , its expectation is @xmath80 and its variance is @xmath81 .",
    "the solution of the optimisation problem is the minimum of the following functional : @xmath82=\\int_{t}^{t+r}\\int_{0}^{t}\\exp ( 2p(t - s))q^{2}|\\lambda ( s)|^{2\\alpha } dsdt   \\nonumber \\\\ & & + \\int_{t}^{t+r}\\left\\ { \\gamma ( t)[x_{0}\\exp ( pt)+\\int_{0}^{t}\\exp ( p(t - s))q\\lambda ( s)ds - z_{0}]\\right\\ } dt   \\nonumber \\\\ & = & \\int_{t}^{t+r}\\{[g(t)|\\lambda ( t)|^{2\\alpha } -f(t)\\lambda ( t)]+\\mu ( t)\\}dt \\label{eg1}\\end{aligned}\\]]with @xmath83 & t\\leq t \\\\ \\frac{q^{2}}{2p}\\bigg[\\exp ( 2p(t+r - t))-1\\bigg ] & t+r\\geq t > t\\end{array}\\right .   \\\\",
    "f(t ) & = & \\left\\ { \\begin{array}{ll } -\\int_{t}^{t+r}q\\gamma ( s)\\exp ( p(s - t))ds & t\\leq t \\\\ -\\int_{t}^{t+r}q\\gamma ( s)\\exp ( p(s - t))ds & t+r\\geq t > t\\end{array}\\right .   \\\\ \\mu ( t ) & = & \\gamma ( t)[x_{0}\\exp ( pt)-z_{0}],\\end{aligned}\\]]for some integrable function @xmath84 , which serves as a lagrange auxiliary multiplier .    in the general case , we minimise ( a ) using ( b ) and ( c ) as constraints via the introduction of appropriate @xmath10 and @xmath5 dependent lagrange multipliers .",
    "this leads to a functional of the mean control signal , @xmath85 $ ] , with the form @xmath86=\\int_{t}^{t+r}h(t,\\lambda ( t))dt$ ] ( see below and appendix a ) .",
    "let us use @xmath87^{\\top } $ ] to denote the value of @xmath88 at a given time of @xmath5 , i.e. , @xmath89 ; @xmath90 will serve as a variable of the young measure ( see below ) .",
    "we find @xmath91+z(t ) \\label{h}\\]]where @xmath92 , @xmath93 and @xmath60 are functions with respect to @xmath5 but are independent of the variable @xmath94 .    the abstract hamiltonian minimum ( maximum ) principle ( ahmp ) @xcite provides a necessary condition for the optimal solution of minimising ( a ) with ( b ) and ( c ) , which is composed of the points in the domain of definition of @xmath95 , namely , @xmath96 , that minimize the function @xmath97 in ( [ h ] ) , at each time",
    ", @xmath5 , which is named _ hamiltonian integrand_. this principle tells us that the optimal solution should pick values of the minimum of @xmath97 with respect to @xmath94 , for each @xmath5 .    if the control signal is supra - poisson or poisson , namely the dispersion index @xmath2 , for each @xmath98 $ ] , the hamiltonian integrand @xmath97 is convex ( or semi - convex ) with respect to @xmath90 and so has a unique minimum point with respect to each @xmath99 .",
    "so , the optimal solution is a deterministic _ function _ of time : for each @xmath100 , @xmath101 can be regarded as picking value at the minimum point of @xmath97 for @xmath102 .",
    "when @xmath103 , namely when the control signal is sub - poisson , it follows that @xmath104 is no longer a convex function",
    "[ al ] show the possible minimum points of the term @xmath105 with @xmath106 and @xmath107 . from the assumption that the range of each @xmath42 is bounded , namely @xmath108 , it then directly follows , from the form of @xmath104 , that the value of @xmath42 which optimises @xmath104 is not unique ; there are _ three _ possible minimum values : @xmath109 , @xmath110 , and @xmath44 , as shown in table [ maximum points ] .",
    "so , no explicit function @xmath111 exists which is the optimal solution of the optimisation problem ( a)-(c )",
    ". however , an infinimum of ( [ h ] ) does exist .",
    "proceeding intuitively , we first make an arbitrary choice of one of the three optimal values for @xmath99 ( namely one of @xmath109 , @xmath110 , and @xmath44 ) and then _ average _ over all possible choices at each time . with @xmath112 the probability density of @xmath99 at time @xmath5 ,",
    "the average is carried out using the distribution ( probability density functional ) @xmath113\\propto{\\prod\\nolimits_{t , i}}\\eta_{t , i}(\\xi)$ ] which represents independent choices of the control signal at each time .",
    "thus , for example , the functional @xmath114 $ ] becomes _ functionally averaged _ over @xmath115 according to @xmath116d[\\lambda]\\right ) dt$ ] .",
    "the optimisation problem has thus shifted from determining a function ( as required when @xmath117 ) to determining a _ probability density functional _ , @xmath113 $ ] .",
    "this intuitively motivated procedure is confirmed by optimisation theory- and this leads us to young measure theory .",
    "let us spell it out in a mathematical way .",
    "young measure theory young , young1 provides a solution to an optimization problem where a solution , which was a function , becomes a linear functional of a parameterized measure . by way of explanation , a function , @xmath88 , yields a single value for each @xmath5 , but a parameterized measure @xmath118 yields a _ set of values _ on which a measure ( i.e. , a weighting ) @xmath119 is defined for each @xmath5 .",
    "a _ functional _ with respect to a parameterized measure can be treated in a similar way to a solution that is an explicit function , by averaging over the set of values of the parameterized measure at each @xmath5 . in detail",
    ", a functional of the form @xmath86=\\int_{0}^{t}h(t,\\lambda ( t))dt$ ] , of an explicit function , @xmath88 , can have its definition extended to a parameterized measure @xmath119 , namely @xmath120=\\int_{0}^{t}\\int_{\\omega } h(t,\\xi ) \\eta _ { t}(d\\xi ) dt$ ] . in this sense",
    ", an explicit function can be regarded as a special solution that is a ` parameterized concentrated measure ' ( i.e. , involving a dirac - delta function ) in that we can write @xmath121=\\int_{0}^{t}\\int_{\\omega } h(t,\\xi ) \\delta ( \\xi -\\lambda ( t))d\\xi dt$ ] .",
    "thus , we can make the equivalence between the explicit function @xmath88 and a parameterized concentrated measure @xmath122 and then replace this concentrated measure , when appropriate , by a young measure .",
    "technically , a young measure is a class of parameterized measures that are relatively weak*-compact such that the lebesgue function space can be regarded as its dense subset in the way mentioned above .",
    "thus , by enlarging the solution space from the function space to the ( larger ) young measure space , we can find a solution in the larger space and the minimum value of the optimisation problem , in the young measure space , coincides with the infinimum in the lebesgue function space .",
    "for any function @xmath123 , we denote a symbol @xmath124 as the inner product of @xmath123 over the parameterized measure @xmath125 , by averaging @xmath123 with respect to @xmath90 via @xmath126 .",
    "that is we define @xmath127 to represent @xmath128 . in this way we can rewrite the optimisation problem ( a)-(c ) as : @xmath129(x , t),~on~[0,t]\\times \\xi , ~p(x,0)=p_{0}(x ) , \\\\ &",
    "x\\in \\xi,~t\\in[0,t+r ] \\\\ & \\int_{\\xi } \\phi ( x , t)p(x , t)dx = z(t),~on~[t , t+r],~\\eta \\in \\mathcal{y}.\\end{array}\\right .",
    "\\label{rop}\\]]here , @xmath130 denotes the young measure space , which is defined on the state space @xmath131 with @xmath132 $ ] , while @xmath133 denotes a shorthand for the young measure associated with control ; @xmath134 is defined as @xmath135\\circ p=\\mathcal{\\int } _ { \\omega}\\mathcal{l}(t , x,\\xi ) \\circ p(x ,",
    "t)\\eta _ { t}(d\\xi ) \\\\ & = & \\int_{\\omega } \\bigg\\{-\\sum_{i=1}^{n}\\frac{\\partial \\lbrack a_{i}(x , t,\\xi ) p(x , t)]}{\\partial x_{i } } \\\\ & & + \\frac{1}{2}\\sum_{i , j=1}^{n}\\frac{\\partial ^{2}\\{[b(x , t,\\xi ) b^{\\top } ( x , t,\\xi ) ] _ { ij}p(x , t)\\}}{\\partial x_{i}\\partial x_{j}}\\bigg\\}\\eta _ { t}(d\\xi )",
    "\\\\ & = & \\int_{\\omega}\\bigg\\{-\\sum_{i=1}^{n}\\frac{\\partial\\lbrack a_{i}(x , t)+\\sum_{j=1}^{m}b_{ij}(x , t)\\xi_{j})p(x , t)]}{\\partial x_{i } } \\\\ & & + \\frac{1}{2}\\sum_{i , j=1}^{n}\\frac{\\partial^{2}\\{\\sum_{k=1}^{m}\\kappa _ { k}^{2}b_{ik}(x , t)b_{jk}(x , t)|\\xi_{k}|^{2\\alpha}p(x , t)\\}}{\\partial x_{i}\\partial x_{j}}\\bigg\\}\\eta_{t}(d\\xi).\\end{aligned}\\]]so , we can study the relaxation problem ( [ rop ] ) instead of the original one , ( a)-(c ) .",
    "we assume that the constraints in ( [ rop ] ) admit a nonempty set of @xmath136 , which guarantees that the problem ( [ rop ] ) has a solution .",
    "we also assume the existence and uniqueness of the cauchy problem of the fokker - plank equation ( [ fpe ] ) .    the abstract hamiltonian minimum ( maximum ) principle ( theorem 4.1.17 roub ) also provides a similar necessary condition for the young measure solution of ( [ rop ] ) , if it admits a solution , that is composed of the points in @xmath96 which minimise the integrand of the underlying ` abstract hamiltonian ' . by employing variational calculus with respect to the young measure",
    ", we can derive the form ( [ h ] ) , for the hamiltonian integrand .",
    "see appendix a for details .    via this principle",
    ", the problem conceptively reduces to finding the minimum points of @xmath104 . from table",
    "[ maximum points ] , for a sufficiently large @xmath137 , it can be seen that , if @xmath3 , then the minimum points for each @xmath5 with @xmath106 may be two points @xmath138 or @xmath139 . hence , in the case of @xmath3 , the optimal solution of ( [ rop ] ) is a measure on @xmath140 or @xmath139 .",
    "this implies that the optimal solution of ( [ rop ] ) should have the following form @xmath141 , where @xmath142 stands for the cartesian product , and each @xmath143 we adopt is a measure on @xmath144 : @xmath145\\delta _ { 0}(\\cdot ) \\label{functional density}\\]]where @xmath146 and @xmath147 are non - negative weight functions .",
    "the optimisation problem corresponds to the determination of the @xmath148 and @xmath147 .",
    "averaging with respect to @xmath149 corresponds to the optimal control signal when the noise is sub - poisson ( @xmath3 ) .",
    "this assignment of a probability density for the solution at each time is known in the mathematical literature as a young measure @xcite . for all @xmath20 and @xmath5 ,",
    "the weight functions satisfy : ( i ) @xmath150 and ( ii ) @xmath151 ( owing to the properties mentioned above that @xmath152 can not simultaneously have both @xmath44 and @xmath109 as optimal ) .",
    "consider the simple one - dimensional system ( [ linear ] ) .",
    "we shall provide the explicit form of the optimal control signal @xmath17 as a young measure . taking expectation for both sides in ( [ linear ] ) , we have @xmath153since we only minimise the variance in @xmath154 $ ] for some @xmath155 and @xmath156 , the control signal @xmath17 for @xmath157 is picked so that the expectation of @xmath65 can reach @xmath158 at the time @xmath159 .",
    "after some simple calculations , we find a deterministic @xmath88 as follows : @xmath160\\]]such that @xmath161",
    ". then we pick @xmath162 for @xmath163 $ ] such that @xmath164 for all @xmath165 $ ] . hence ,",
    "@xmath166 for all @xmath167 $ ] . in the interval",
    "@xmath154 $ ] , as discussed above , for a sufficiently large @xmath44 , the optimal solution of @xmath88 should be a young measure that picks values in @xmath168 . to sum up",
    ", we can construct the optimal @xmath88 as follows : @xmath169 & t\\in \\lbrack t , t+r]~\\mathrm{if}~-pz_{0}/q>0~\\mathrm{or } \\\\",
    "\\delta _ { -m_{y}}(\\cdot ) \\frac{pz_{0}}{q~m_{y}}+\\delta _ { 0}(\\cdot ) [ 1-\\frac{pz_{0}}{q~m_{y } } ] & t\\in \\lbrack t ,",
    "t+r]~\\mathrm{if}~pz_{0}/q\\geq 0.\\end{array}\\right.\\]]it can be seen that in @xmath170 , @xmath119 is in fact a deterministic function as the same as @xmath88 .",
    "we now illustrate the control performance when the noise is sub - poisson . for the general nonlinear system ( [ general ] ) , we can not obtain an explicit expression for the probability density functional @xmath113 $ ] , eq .",
    "( functional density ) , or the value of the variance ( execution error ) .",
    "however , we can adopt a _ non optimal _ probability density functional which illustrates the property of the exact system , that the execution error becomes arbitrarily small when the bound of the control signal , @xmath44 , becomes arbitrarily large . in the simple case ( [ linear ] ) , we note that if there is a @xmath171 , such that @xmath172 , then the variance becomes , expressed by young measure @xmath173 , @xmath174 which converges to zero as @xmath175 , due to @xmath176 .",
    "that is , the minimised execution error can be arbitrarily small if the bound of the control signal , @xmath44 , goes sufficiently large .",
    "in fact , this phenomenon holds for general cases .",
    "the non optimal probability density functional is motivated by assuming that there is a deterministic control signal @xmath177 which controls the dynamical system @xmath178 which is the original system ( [ general ] ) , _ with the noise removed_. the deterministic control signal @xmath177 causes @xmath179 to precisely achieve the target trajectory @xmath180 for @xmath181 .",
    "then , we add the noise with the signal - dependent variance : @xmath182 with some @xmath3 , which leads a stochastic differential equation , @xmath183 .",
    "the non optimal probability density that is appropriate for time @xmath5 , namely @xmath184 , is constructed to have a mean over the control values @xmath144 , which equals @xmath177 .",
    "this probability density is @xmath185where @xmath186 and , by definition , @xmath187 .",
    "we establish in appendix b that the expectation condition ( ( b ) above ) holds asymptotically when @xmath188 which shows that the non optimal probability density functional is appropriately ` close ' to the optimal functional . the accumulated execution error associated with the non optimal functional",
    "is estimated as @xmath189dt}=o\\bigg(\\frac{1}{m_{y}^{1/2-\\alpha } } \\bigg )   \\label{error_est}\\]]and , in this way , optimal performance of control , with sub - poisson noise , can be seen to become precise as @xmath44 is made large . by contrast , if @xmath2 , the accumulated execution error is always greater than some positive constant .    to gain an intuitive understanding of why the effects of noise are eliminated for @xmath176",
    "we discretise the time @xmath5 into small bins of identical size @xmath190 . using the ` noiseless control ' @xmath191",
    ", we divide the time bin @xmath192 $ ] into two complementary intervals : @xmath193 $ ] and @xmath194 $ ] , and assign @xmath195 for the first interval and @xmath196 for the second .",
    "when @xmath197 the effect of the control signal @xmath198 on the system approaches that of @xmath191 , although @xmath198 and @xmath191 are quite different .",
    "the variance of the noise in the first interval is @xmath199 and is @xmath110 in the second .",
    "hence , the overall noise effect of the bin is @xmath200 .",
    "remarkably , this tends to zero as @xmath175 _ if _ @xmath201 ( i.e. , for sub - poisson noise ) .",
    "the discretisation presented may be regarded as a formal stochastic realisation of the probability density functional ( young measure ) adopted .",
    "the interpretation above can be verified in a rigorous mathematical way .",
    "see appendix b for details .",
    "let us now consider an application of this work : the control of straight - trajectory arm movement , which has been widely studied win , sim , tan , ike and applied to robotic control .",
    "the dynamics of such structures are often formalised in terms of coordinate transformations .",
    "nonlinearity arises from the geometry of the joints .",
    "the change in spatial location of the hand that results from bending the elbow depends not only on the amplitude of the elbow movement , but also on the state of the shoulder joint .    for simplicity ,",
    "we ignore gravity and viscous forces , and only consider the movement of a hand on a horizontal plane in the absence of friction .",
    "let @xmath202 denote the angle between the upper arm and horizontal direction , and @xmath203 be the angle between the forearm and upper arm ( fig .",
    "[ arms ] ) .",
    "the relation between the position of hand @xmath204 $ ] and the angles @xmath205 $ ] is @xmath206,\\end{aligned}\\]]where @xmath207 are moments of inertia with respect to the center of mass , for the upper arm and forearm . when moving a hand between two points , a human maneuvers their arm so as to make the hand move in roughly a straight line between the end points .",
    "we use this to motivate the model by applying geostatics theory @xcite .",
    "this implies that the arm satisfies an euler - lagrange equation , which can be described as the following nonlinear two - dimensional system of differential equations:@xmath208 + c(\\theta _ { 1},\\theta _ { 2},\\dot{\\theta}_{1},\\dot{\\theta}_{2})\\left [ \\begin{array}{c } \\dot{\\theta}_{1 } \\\\ \\dot{\\theta}_{2}\\end{array}\\right ] = \\gamma _ { 0}\\left [ \\begin{array}{c } q_{1 } \\\\",
    "q_{2}\\end{array}\\right ] ,   \\nonumber \\\\ & & \\theta _ { 1}(0)=-\\frac{\\pi } { 2},~\\theta _ { 2}(0)=\\frac{\\pi } { 2},~\\dot{\\theta}_{1}(0)=\\dot{\\theta}_{2}(0)=0 .",
    "\\label{am}\\end{aligned}\\]]in these equations @xmath209 , \\\\ c & = & k\\sin \\theta _ { 2}\\left [ \\begin{array}{ll } \\dot{\\theta}_{2 } & \\dot{\\theta}_{1}+\\dot{\\theta}_{2 } \\\\",
    "\\dot{\\theta}_{1 } & 0\\end{array}\\right ] , ~q_{i}=\\lambda _ { i}(t)+\\kappa _ { 0}|\\lambda _ { i}(t)|^{\\alpha } \\frac{dw_{i}}{dt},\\end{aligned}\\]]where @xmath210 , @xmath211 , and @xmath212 are , respectively the mass , length , and moment of inertia with respect to the center of mass for the @xmath20th part of the system and @xmath213 ( @xmath214 ) denotes the upper arm ( forearm ) , @xmath215 are the lengths of the upper- and fore - arms , and @xmath216 is the scale parameter of the force .",
    "additionally , @xmath217 , while @xmath218 are the _ means _ of two torques @xmath219 , which are motor commands to the joints .",
    "the torques are accompanied by signal - dependent noises .",
    "all other quantities are fixed parameters .",
    "see win for the full details of the model .",
    "the values of the parameters we pick here are listed in table [ parameters ] .    for this example , we shall aim to control the hand such that it starts at @xmath58 , with the initial condition of ( [ am ] ) , reaches the target at coordinates @xmath220 $ ] at time @xmath159 , and then stays at this target for a time interval of @xmath57 .",
    "we use the minimum variance principle to determine the optimal task , which is more advantageous than other optimisation criteria to control a robot arm @xcite .",
    "let @xmath221 $ ] be the cartesian coordinates of the hand that follow from the angles @xmath222 $ ] .",
    "the minimum variance principle determines @xmath223dt$ ] , subject to the constraint that @xmath224=[h_{1},h_{2 } ] $ ] for @xmath225 , with @xmath226 . despite not being in possession of an explicit analytic solution",
    ", we can conclude that if @xmath227 , the optimisation problem results from the unique minimum to the hamiltonian integrand and hence yields @xmath228 and @xmath229 which are _ ordinary functions_. however , if @xmath176 , the optimal solution of the optimisation problem follows from a probability density functional analogous to eq .",
    "( [ functional density ] ) ( i.e. , a young measure over @xmath230 ) .",
    "thus , we can relax the optimisation problem via young measure as follows :",
    "@xmath231 and @xmath232dt \\\\ \\mathrm{subject~to } & \\mathbf{e}[x_{1}(t),x_{2}(t)]=[h_{1},h_{2}],~t\\in[t , t+r ] \\\\ & \\xi_{i}\\in[-m_{y},m_{y}].\\end{array}\\right .   \\label{ramop}\\end{aligned}\\ ] ]    we used euler s method to conduct numerical computations , with a time step of @xmath233 msec in ( [ am ] ) .",
    "this yields a dynamic programming problem ( see methods ) .",
    "[ mean ] shows the means of the optimal control signals @xmath234 with @xmath235 and @xmath236 : @xmath237according to the form of the optimal young measure , the optimal solution should be @xmath238ds & \\bar{\\lambda}_{i}(t)>0 \\\\ \\bigg[\\frac{|\\bar{\\lambda}_{i}(t)|}{m_{y}}\\delta_{-m_{y } } ( s)+(1-\\frac{|\\bar{\\lambda}_{i}(t)|}{m_{y}})\\delta_{0 } ( s)\\bigg]ds & \\bar{\\lambda}_{i}(t)<0 \\\\ \\delta_{0 } ( s)ds & \\mathrm{otherwise}.\\end{array}\\right.\\ ] ]    it can be shown ( derivation not given in this work ) that in the absence of the noise term , the arm can be accurately controlled to reach a given target for any @xmath155 . in this case ,",
    "[ desire ] shows the dynamics of the angles , their velocities , and accelerators , in the controlled system , removed noise .",
    "see , in comparison , the dynamical system with noise , whose dynamics of the angles , velocities , and accelerations are illustrated in fig .",
    "[ fact ] , and its dynamics are exactly the same as those in the case with noise removed .",
    "however , the acceleration dynamics of a noisy dynamic system appear discontinuous since the control signals , that have noises and are added to the right - hand sides of the mechanical equations , are discontinuous ( noisy ) in a numerical realisation",
    ". however , according to the theory of stochastic differential equations @xcite , ( [ am ] ) has continuous solution .",
    "hence , these discontinuous acceleration dynamics lead very smooth dynamics of velocities and angles , as shown in fig .",
    "[ fact ] .    figs .",
    "[ figs1 ] ( a ) and ( b ) illustrate that the probability density functional , for this problem , contains optimal control signals that are similar to neural pulses . despite the optimal solution not being an ordinary function when @xmath3 , the trajectories of the angles @xmath202 and @xmath203 of the arm appear quite smooth , as shown in fig .",
    "[ fact ] ( a ) , and the target is reached very precisely if the value of @xmath44 is large . by comparison , when @xmath239 the outcome has a standard deviation between @xmath240 to @xmath241 cm , which may lead to a failure to reach the target .",
    "a direct comparison between the execution error of the cases @xmath242 and @xmath243 is shown in the supplementary movies ( supplementary videos ` video s1 ' and ` video s2 ' ) of arm movements of both cases .",
    "our conclusion is that a young measure optimal solution , in the case of sub - poisson control signals , can realize a precise control performance even in the presence of noise .",
    "however , poisson or supra - poisson control signals can not realise a precise control performance , despite the existence of an explicit optimal solution in this case .",
    "thus @xmath3 significantly reduces execution error compared with @xmath2 .    with different @xmath56 ( the starting time of reaching the target ) and @xmath57 ( the duration of reaching the target ) , under sub - poisson noise , i.e. , @xmath244 , the system can be precisely controlled by optimal young measure signals with a sufficiently large @xmath44 . since the target in the reachable region of the arm",
    ", it implies that the original differential system of ( [ am ] ) with the noise removed can be controlled for any @xmath155 and @xmath156 @xcite .",
    "according to the discussion in appendix b ( theorem thm2 ) , the execution error can be arbitrarily small when @xmath44 is sufficiently large .",
    "however , for a smaller @xmath56 , i.e. , the more rapid the control is , the larger means of the control signals will be .",
    "as for the duration @xmath57 , by picking the control signals as fixed values ( zeros in this example ) such that the velocities keep zeros , the arm will stay at the target for arbitrarily long or short . similarly , with a large @xmath44 , the error ( variance ) of staying at the target can be very small .",
    "to illustrate these arguments , we take @xmath245 ( msec ) and @xmath246 ( msec ) for example ( all other parameters are the same as above ) .",
    "[ meanv1 ] shows that the means of the optimal young measure control signals before reaching the target have larger amplitudes than those when @xmath247 ( msec ) and fig .",
    "factv1 shows that the arm can be precisely controlled to reach and stay at the target .",
    "the movement error depends strongly on the value of the dispersion index , @xmath248 , and the bound of the control signal , @xmath44 .",
    "[ figs2 ] indicates a quantitative difference in the execution error between the two cases @xmath176 and @xmath249 , if @xmath248 is close to ( but less than ) @xmath250",
    ". the execution error can be appreciable unless a large @xmath44 is used .",
    "for example if @xmath251 , as in fig . [ figs2 ] , the square root of the execution error is approximately @xmath252 cm when @xmath236 .",
    "from ( [ error_est ] ) , the error decreases as @xmath44 increases , behaving approximately as a power - law , as illustrated in the inner plot of fig .",
    "the logarithm of the square root of the execution error is found to depend approximately linearly on the logarithm of @xmath44 when @xmath253 , with a slope close to @xmath254 , in good agreement with the theoretical estimate ( [ error_est ] ) .",
    "we note that in a biological context , a set of neuronal pulse trains can achieve precise control in the presence of noise .",
    "this could be a natural way to approximately implement the probability density functional when @xmath176 .",
    "all other parameters are the same as above ( @xmath253 ) .",
    "the firing rates are illustrated in fig .",
    "[ figs1 ] ( a ) and ( b ) and broadly coincide with the probability density functional we have discussed . in particular , at each time @xmath5 , the probability @xmath255 can be approximated by the fraction of the neurons that are firing , with the mean firing rates equal the means of the control signals ( see methods ) .",
    "the approximations of the components of the noisy control signals are shown in figs .",
    "[ figs3 ] ( a ) and ( b ) respectively .",
    "fig.[figs3 ] ( c ) and ( d ) illustrate such an implementation of the optimal solution by neuronal pulse trains . using the pulse trains as control signals , we can realise precise movement control .",
    "we enclose two videos ` movieup.avi ' and moviedown.avi to demonstrate the efficiency of the control by pulse trains with two different targets .",
    "as they show , the targets are precisely accessed by the arm .",
    "we point out that the larger the ensemble is , the more precise the control performance will be , because a large number of the neurons in an ensemble can theoretically lead to a large @xmath44 as we mentioned above , which results in an improvement of the approximation of a young measure and decreases the execution error as stated in ( [ error_est ] ) .",
    "we note that these kinds of patterns of pulse trains have been widely reported in experiments , for example , the synchronous neural bursting reported in @xcite .",
    "this may provide a mathematical rationale for the nervous system to adopt pulse - like signals to realise motor control .",
    "in this paper , we have provided a general mathematical framework for controlling a class of stochastic dynamical systems with random control signals whose noisy variance can be regarded as a function of the signal magnitude . if the dispersion index , @xmath248 , is @xmath256 , which is the case when the control signal is sub - poisson , an optimal solution of explicit function does not exist but has to be replaced by a young measure solution .",
    "this parameterized measure can lead a precise control performance , where the controlling error can become arbitrarily small .",
    "we have illustrated this theoretical result via a widely - studied problem of arm movement control .    in the control problem of biological and robotic systems ,",
    "large control signals are needed for rapid movement control @xcite .",
    "when noise occurs , this will cause imprecision in the control performance . as pointed out in @xcite ,",
    "a trade - off should be considered when conducting rapid control with noises . in this paper",
    ", we still use a `` large '' control signal but with different contexts . with sub - poisson noises",
    ", we proved that a sufficiently large @xmath44 , i.e. , a sufficiently large region of the control signal values , can lead precise control performance . hence , a large region of control signal values plays a crucial role in realising precise control in noisy environments , for both `` slow '' and `` rapid '' movement control . in numerical examples , the larger @xmath44 we pick , the smaller control error will be , as shown in the inset plot of fig .",
    "[ figs2 ] as well as ( error_est ) ( theorem [ thm2 ] in appendix b ) .",
    "implementation of the young measure approach in biological control appears to be a natural way to achieve precise execution error in the presence of sub - poisson noise . in particular , in the neural - motor control example illustrated above , the optimal solution in the case of @xmath3 is quite interesting .",
    "assume we have an ensemble of neurons which fire pulses synchronously within a sequence of non overlapping time windows , as depicted in figs .",
    "[ figs1 ] c and d. we see that the firing neurons yield control signals which are very close , in form , to the type of young measure solution",
    ". this conclusion may provide a mathematical rationale for the nervous system why to adopt pulse - like trains to realise motor control .",
    "additionally , we point out that , our approach may have significant ramifications in other fields , including robot motor control and sparse functional estimation , which are issues of our future research .",
    "* numerical methods for the optimisation solution .",
    "* we used euler s method to conduct numerical computations , with a time step of @xmath257 msec in ( [ am ] ) with @xmath176 .",
    "this yields a dynamic programming problem .",
    "first , we divide the time domain @xmath258 $ ] into small time bins with a small size @xmath190 .",
    "then , we regard the process @xmath255 in each time bin @xmath259 $ ] as a static measure variable . thus , the solution reduces to finding two series of nonnegative parameters @xmath260 and @xmath261 with @xmath262 and @xmath263 such that @xmath264 the approximate solution of the optimisation problem requires nonnegative @xmath260 and @xmath261 that minimise the final movement errors .",
    "we thus have a dynamic programming problem .",
    "we should point out that in the literature , a similar method was proposed to solve the optimisation problem in a discrete system with control signals taking only two values ike , aih , aih1 .",
    "thus , the dynamical system ( [ general ] ) becomes the following difference equations via the euler method : @xmath265m_{y}\\bigg\\ } \\\\ & & + \\sum_{j=1}^{m}b_{ij}(x(k),t_{k})\\kappa _ { j}\\sqrt{\\delta t}(\\mu _ { j , k}+\\nu _ { j , k})m_{y}^{\\alpha } \\nu _ { j},~k=0,1,2,\\cdots , \\end{aligned}\\]]where @xmath266 , @xmath41 , are independent standard gaussian random variables .",
    "we can derive difference equations for the expectations and variances of @xmath267 , by ignoring the higher order terms with respect to @xmath190 : @xmath268 \\\\ & & + \\sum_{j=1}^{m}\\mathbf{e}[b_{ij}(x(k),t_{k})][\\mu _ { j , k}-\\nu _ { j , k}]m_{y}\\bigg\\ } \\\\ & & \\mathrm{cov}(x_{i}(k+1),x_{i^{\\prime } } ( k+1))=\\mathrm{cov}(x_{i}(k),x_{i^{\\prime } } ( k))+\\delta t\\mathrm{cov}\\bigg(x_{i^{\\prime } } ( k),\\big\\{a_{i}(x(k),t_{k } ) \\\\ & & + \\sum_{j=1}^{m}b_{ij}(x(k),t_{k})[\\mu _ { j , k}-\\nu _ { j , k}]m_{y}\\big\\}\\bigg ) \\\\ & & + \\delta t\\mathrm{cov}\\bigg(x_{i}(k),\\big\\{a_{i^{\\prime } } ( x(k),t_{k})+\\sum_{j=1}^{m}b_{i^{\\prime } j}(x(k),t_{k})[\\mu _ { j , k}-\\nu _ { j , k}]m_{y}\\big\\}\\bigg ) \\\\ & & + \\delta t\\sum_{j=1}^{m}\\mathrm{cov}(b_{i^{\\prime } j}(x(k),t_{k}),b_{ij}(x(k),t_{k}))(\\mu _ { j , k}+\\nu _ { j , k})m_{y}^{2\\alpha } .\\end{aligned}\\]]thus , eq . ( [ rop ] ) becomes the following discrete optimization problem : @xmath269with @xmath267 a gaussian random vector with expectation @xmath270 and covariance matrix @xmath271 .",
    "* neuronal pulse trains approximating young measure solution .",
    "* at each time @xmath272 , the measure @xmath273 can be approximated by the fraction of the neuron ensemble that are firing . in detail , assuming that the means of the optimal control signals are @xmath274 , @xmath275 , and there are one ensemble of excitatory neurons and another ensemble of inhibitory neurons .",
    "a fraction of the neurons fire so that the mean firing rates satisfy : @xmath276-\\mathbf{e}[r_{i}^{inh}(t)]\\bigg\\},\\ ] ] where @xmath277 and @xmath278 are the firing rates of the excitatory and inhibitory neurons respectively , and @xmath279 is a scalar factor . in occurrence of sub - poisson noise ,",
    "the noisy control signals @xmath280 are approximated by @xmath281.\\ ] ] both ensembles of neurons are imposed with baseline activities , which bound the minimum firing rates away from zeros , given the spontaneous activities of neurons when no explicit signal is transferred .",
    "a numerical approach involves discretise time @xmath5 into small bins of identical size @xmath190 .",
    "the firing rates can be easily estimated by averaging the population activities in a time bin .",
    "we have used @xmath282 neurons to control the system , with two ensembles of neurons with equivalent numbers that approximate the first and second components of control signal , respectively . each neuron ensemble",
    "have @xmath283 neurons with @xmath284 excitatory and @xmath284 inhibitory neurons .",
    "let : @xmath285 be the time - varying p.d.f . @xmath66 that is second - order continuous - differentiable with respect to @xmath10 and @xmath5 that is embedded in the sobolev function space @xmath286",
    "; @xmath287 be the function space of @xmath288 , regarding as a function with respect to @xmath10 with a fixed @xmath100 ; @xmath289 be the function space of @xmath290 , regarding as a function with respect to @xmath5 with a given @xmath291 .",
    "the spaces @xmath292 can be regarded being embedded in @xmath285 .",
    "in addition , let : @xmath293 be the function space where the image @xmath294 $ ] is embedded ; @xmath295 be the space of linear operator @xmath296 , denoted above ; @xmath297 be the space composed of bounded linear operator from linear space @xmath298 to @xmath299 ; and @xmath300 be the dual space of the linear space @xmath298 : @xmath301 .",
    "furthermore , let @xmath302 be the tangent space of young measure space @xmath130 : @xmath303 . for simplicity , we do not specify the spaces and just provide the formalistic algebras , and then the following is similar to chapter 4.3 in @xcite with appropriate modifications .",
    "define @xmath304 thus , ( [ rop ] ) can be rewritten as : @xmath305 the gteaux differentials of these maps with respect to @xmath66 , denoted by @xmath306 , are : @xmath307dx dt \\\\ ( \\nabla_{p}{\\pi})\\circ(\\hat{p}-p)&=&\\bigg(\\frac{\\partial(\\hat{p}-p)}{\\partial t } -(\\mathcal{l}\\cdot\\eta)\\circ(\\hat{p}-p),\\hat{p}(x,0)-p(x,0)\\bigg ) \\\\ ( \\nabla_{p}j)\\circ(\\hat{p}-p)&=&\\int_{\\xi}\\phi(x , t)[\\hat{p}(x , t)-p(x , t)]dx \\\\\\end{aligned}\\ ] ] for two time - varying p.d.f",
    ". @xmath308 . here ,",
    "@xmath309 , @xmath310 , @xmath311 . and",
    ", the differentials of these maps with respect to the young measure @xmath312 are : @xmath313 for two young measures @xmath314 . here , @xmath315 , @xmath316 , and @xmath317 .    then , we are in the position to derive the result of ( [ h ] ) by the following theorem , as a consequence from theorem 4.1.17 in @xcite .    [ thm1 ] @xmath318 , @xmath319 and @xmath320 as defined in ( [ sym ] ) .",
    "assume that : ( 1 ) .",
    "the trajectory of @xmath65 in ( [ ito ] ) is bounded almost surely ; ( 2 ) . @xmath11 , @xmath12 and @xmath321 are @xmath322 with respect to @xmath323 .",
    "let @xmath324 be the optimal solution of ( [ rop ] ) .",
    "then , there are some @xmath325 , @xmath326^{\\top}$ ] with @xmath327 , and @xmath328 , such that @xmath329 and the abstract maximum principle @xmath330 .",
    "\\label{amp}\\ ] ] holds with `` abstract hamiltonian '' : @xmath331_{ij}\\frac{\\partial ^{2}\\mu _",
    "{ 1}}{\\partial x_{i}\\partial x_{j}}\\bigg\\}dx .",
    "\\label{h1}\\end{aligned}\\ ] ]    under the conditions in this theorem , we can conclude that the fokker - planck equation has a unique solution @xmath332 that is continuously dependent of @xmath312 from theory of stochastic differential equation @xcite ; @xmath333 is frchet differentiable at @xmath334 because @xmath65 is assumed to be almost surely bounded ; @xmath335 and @xmath336 ( in fact @xmath337 ) is gteaux equi - differentiable around @xmath338 because of @xmath339 with @xmath323 bounded @xcite ; the partial differential @xmath340 is weak - continuous with respect to @xmath312 because it is linearly dependent of @xmath312 .",
    "in addition , from the existence and uniqueness of the fokker - planck equation , @xmath341 has a bounded inverse .",
    "this implies that the following__adjoint equation _ _ @xmath342has a solution for @xmath343 , denoted by @xmath344 .",
    "let @xmath345 $ ] , which should be solution of the following equation @xmath346,~x\\in \\xi , \\end{array}\\right.\\]]with the dual operator @xmath347 of @xmath296 ( the operator in the back - forward kolmogorov equation ) , still dependent of @xmath323 and the value of @xmath88 ( namely @xmath90 in young measure ) : @xmath348_{ij}\\frac{\\partial ^{2}q}{\\partial x_{i}x_{j}}.\\]]we pick @xmath349 with @xmath350 and @xmath351 .",
    "so , @xmath352 should satisfy equation ( [ cond2 ] ) .",
    "in fact , @xmath352 can be regarded as functions ( or generalized functions ) with respect to @xmath323 .",
    "thus , the conditions of lemma 1.3.16 in @xcite can be verified , which implies that the gradients of the maps @xmath353 and @xmath336 with respect to @xmath312 , by regarding @xmath343 from @xmath354 , as follows : @xmath355    from the abstract hamilton minimum principle ( theorem 4.1.17 in @xcite ) , applied to each solution of ( [ rop ] ) , denoted by @xmath356 , there exists a nonzero function @xmath357 such that @xmath358is an abstract hamiltonian , with respect to @xmath359 . with the definitions of @xmath352 , ( [ cond1 ] )",
    "becomes @xmath360owing to @xmath361 .    by specifying @xmath344 with @xmath352",
    ", we have @xmath362\\cdot \\tilde{\\eta}\\rangle = -\\langle p^{\\ast } , [ \\mathcal{l}^{\\ast } \\circ \\mu _ { 1}]\\cdot \\tilde{\\eta}\\rangle \\\\ & = & -\\int_{0}^{t+r}\\int_{\\xi } \\int_{\\omega } p^{\\ast } \\bigg\\{\\sum_{i=1}^{n}a_{i}(x , t,\\xi ) \\frac{\\partial \\mu _ { 1}}{\\partial x_{i } } \\\\ & & + \\frac{1}{2}\\sum_{i , j=1}^{n}[b(x , t,\\xi ) b(x , t,\\xi ) ^{\\top } ] _ { ij}\\frac{\\partial ^{2}\\mu _ { 1}}{\\partial x_{i}\\partial x_{j}}\\bigg\\}\\tilde{\\eta}_{t}(d\\xi ) dxdt,\\end{aligned}\\]]where @xmath363 stands for the time - varying density corresponding to the optimal young measure solution @xmath356 . from this",
    ", letting @xmath364 , we have the `` abstract hamiltonian '' in the form of ( [ h1 ] ) as the hamiltonian integrand of @xmath365 .",
    "the hamiltonian abstract minimum ( maximum ) principle indicates the optimal young measure @xmath366 is only concentrated at the minimum points of @xmath104 with respect to @xmath94 for each @xmath5 , namely .",
    "that is , ( amp ) holds .",
    "this completes the proof .    from this theorem , since the variances depend on the magnitude of the signal as described in ( [ sigma i ] ) , removing the terms without @xmath90 , it is equivalent to look at the minima of @xmath367 in the form of @xmath368instead of @xmath104 , where @xmath369this gives formula ( [ h ] ) .      the control performance inequality ( [ error_est ] ) can be derived from the following theorem .",
    "[ thm2 ] let @xmath370 be the solution of equation ( [ ds ] ) and @xmath371 .",
    "assume that there are a positive measurable function @xmath372 and a positive constant @xmath373 such that @xmath374 , @xmath375 and@xmath376 hold for all @xmath377 and @xmath6 .",
    "then , for any non - random initial value , namely , @xmath378 , with the non - optimal young measure ( [ functional density1 ] ) , we have    1 .   @xmath379 ; 2 .   @xmath380    as @xmath381 .    comparing the differential equation of @xmath10 , i.e. ( [ ito ] ) , and that of @xmath370 , ( [ ds ] ) , we have @xmath382dt+b(x , t,\\lambda(t))dw_{t}$ ] . and , replacing @xmath136 with the young measure @xmath383 , in the form of ( [ functional density1 ] ) , from the conditions in this theorem , we have @xmath384dt\\bigg\\}^{2 } \\\\ & + \\mathbf{e}\\bigg\\{\\int_{0}^{\\tau}\\vert b(x , t,\\lambda)\\vert^{2}\\cdot \\hat{\\eta}_{t}dt\\bigg\\ } \\\\ & \\leq\\mathbf{e}\\bigg\\{\\int_{0}^{\\tau}\\kappa(t)\\vert x(t)-\\hat{x}(t)\\vert dt\\bigg\\}^{2}+\\sum_{k=1}^{m}\\int_{0}^{\\tau}\\kappa(t)|\\lambda_{k}|^{2\\alpha } \\cdot\\hat{\\eta}_{k , t}dt \\\\ & \\leq\\int_{0}^{\\tau}\\kappa^{2}(s)ds\\int_{0}^{\\tau}\\mathbf{e}\\vert x(t)-\\hat { x}(t)\\vert^{2}dt+\\sum_{k=1}^{m}\\int_{0}^{\\tau}\\frac{m^{2\\alpha}_{y}}{m_{y}}\\kappa(t)|\\hat{u}_{k}(t)|dt,\\end{aligned}\\ ] ] for any @xmath385 . by using grnwall s inequality",
    ", we have @xmath386 \\frac{1}{m^{1 - 2\\alpha}_{y}}\\sum_{k=1}^{m}\\kappa(t)|\\hat{u}_{k}(t)|dt . \\label{errorest}\\end{aligned}\\ ] ] noting that for @xmath371 , @xmath387 implies that @xmath388 as @xmath44goes to infinity .",
    "this proves the second item in this theorem .",
    "in addition , @xmath389 also approaches zero as @xmath44 goes to infinity .",
    "this proves the first item of the theorem .",
    "this completes the proof .",
    "hence , as @xmath44 goes to infinity , the non optimal solution ( functional density1 ) can asymptotically satisfy the constraint and the error variance goes to zero as @xmath44 goes to infinity .",
    "therefore , the performance error of the real optimal solution of the optimisation problem ( [ rop ] ) approaches zero as @xmath175 in the case of @xmath176 .",
    "furthermore , we can conclude from ( [ errorest ] ) that the execution error , measured by the standard deviation , can be approximated as ( [ error_est ] ) .",
    "99 osborne lc , lisberger sg , bialek w 2005 _ nature _ * 437 * 412416 .                                                       in @xmath390 with respect to the variable @xmath391 with @xmath392 , @xmath393 and @xmath394 for @xmath395 ( blue curves ) and @xmath396 ( red curves ) : ( a ) . the plots of @xmath397 with respect to @xmath391 for @xmath398 ( blue ) and @xmath399 ( red ) and their mimimum points ; ( b ) the inner plot of @xmath400 for @xmath401 $ ] to show that the @xmath402 does be a minimum point for @xmath399 ( red ) ; ( c ) .",
    "the plots of the derivatives of @xmath400 with respect to @xmath391 for @xmath398 ( blue ) and @xmath399 ( red ) . ]    , @xmath403 , and @xmath404 ) , where @xmath405 is fixed and others not , and two arms ( upper arm @xmath406 and the forearm @xmath407 ) .",
    "button @xmath404 is to reach some given target ( red cross ) by moving front- and back - arms . ]",
    "( blue solid ) and @xmath408 ( green solid ) in the straight - trajectory arm movement example with @xmath409 and @xmath236 .",
    "the blue and red dash vertical lines stand for the start and end time points of the duration of reaching the target respectively . ]    .",
    "target is set by @xmath410 and @xmath411 but _ without noise _ : the dynamics of the angles ( a ) , the angle velocities ( b ) and accelerations ( c ) ( the blue solid curves for those of @xmath412 and the green solid curves for @xmath413 ) .",
    "the blue and red dash vertical lines stand for the start and end time points of the duration of reaching the target . ]    , and @xmath399 , @xmath236 : the dynamics of the angles ( a ) , the angle velocities ( b ) and accelerations ( c ) ( the blue solid curves for those of @xmath414 and the red solid for @xmath415 ) .",
    "the blue and red dash vertical lines stand for the start and end time points of the duration of reaching the target . ]",
    "( blue solid ) and @xmath416 ( green solid ) in the straight - trajectory arm movement example with @xmath245 ( sec ) and @xmath246 ( msec ) .",
    "the blue and red dash vertical lines stand for the start and end time points of the duration of reaching the target . ]     and @xmath246 ( msec ) : the dynamics of the angles ( a ) , the angle velocities ( b ) and accelerations ( c ) ( the blue solid curves for those of @xmath414 and the red solid curves for @xmath415 ) .",
    "the blue and red dash vertical lines stand for the start and end time points of the duration of reaching the target . ]"
  ],
  "abstract_text": [
    "<S> how can precise control be realised in intrinsically noisy systems ? here , we develop a general theoretical framework that provides a way to achieve precise control in signal - dependent noisy environments . </S>",
    "<S> when the control signal has poisson or supra - poisson noise , precise control is not possible . if , however , the control signal has sub - poisson noise , then precise control is possible . for this case , </S>",
    "<S> the precise control solution is not a function , but a rapidly varying random process that must be averaged with respect to a governing probability density functional . </S>",
    "<S> our theoretical approach is applied to the control of straight - trajectory arm movement . </S>",
    "<S> sub - poisson noise in the control signal is shown to be capable of leading to precise control . </S>",
    "<S> intriguingly , the control signal for this system has a natural counterpart , namely the bursting pulses of neurons trains of dirac - delta functions in biological systems to achieve precise control performance . </S>"
  ]
}