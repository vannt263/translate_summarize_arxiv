{
  "article_text": [
    "recently , deep neural networks ( dnns ) have been successfully applied to solve complicated problems in pattern recognition , computer vision , and speech recognition , cf .  @xcite . despite its great success ,",
    "designing and training a deep neural network is still among the greatest challenges in the field , cf .",
    "@xcite . in this work ,",
    "we focus on the study of the feedforward neural networks ( fnns ) .",
    "one major reason for the difficulty in designing and training",
    "an fnn is that their performance is highly dependent on various factors in a very complicated way .",
    "besides the determinative factor being the architecture of an fnn @xcite , the work in @xcite demonstrates the impact of different activation functions to the performance of an fnn .",
    "moreover , the choice of error functions is also shown to be influential , cf .",
    "@xcite .    even with a well designed fnn architecture , training the fnn efficiently is as challenging as the design of the network .",
    "the most popular fnn training algorithm is the well known backpropagation ( bp ) algorithm , cf .  @xcite .",
    "although the bp algorithm shares a great convenience of being very simple , early works argue that problems with the bp algorithm is essentially its nature of being a gradient descent algorithm , cf .  @xcite .",
    "since an overall cost function for training an fnn is often in a large scale and highly non - convex , the bp algorithm suffers from two major drawbacks , namely , ( i ) existence of undesired local minima , and ( ii ) slow convergence speed . one major instrument in such a study is via an error / loss surface analysis , cf .",
    "however , even for a very simple problem , such as the classic xor problem , the error surface analysis is very complicated and the results are still hard to conclude .",
    "specifically , works in @xcite demonstrate such a difficulty of characterising the error surface of an fnn . on the other hand",
    ", although the bp algorithm is suspected to be sensitive to initialisations , e.g.  @xcite , recent results reported in @xcite suggest that modern fnn learning algorithms can overcome the problem of local optima quite conveniently .",
    "such an observation could be explained by the pioneering works in @xcite , which developed some conditions on the structure of fnns to eliminate undesired local minima .",
    "unfortunately , these local minima free conditions seem to be still limited for an application in analysing dnns .    in order to deal with slow convergence speed of the bp algorithm ,",
    "various modified bp algorithms have been developed , such as momentum based bp algorithm @xcite , conjugate gradient algorithm @xcite , and bfgs algorithm @xcite .",
    "although newton s method is always of great interest for training an fnn , a complete implementation of newton s fnn algorithm is often computationally prohibitive .",
    "instead , certain approximations of the hessian matrix have been already proposed in the community to deal with the issue , such as a diagonal approximation structure @xcite , or a block diagonal approximation structure @xcite . however , without a true evaluation of the hessian , performance of these heuristic approximations is hardly convincing .",
    "although existing works @xcite have characterised the hessian matrix by applying the partial derivatives , unfortunately these results fail to provide further information of the hessian matrix , due to the limitations of partial derivatives . therefore , in this work , we propose to employ the techniques from multivariable differential calculus to analyse fnns from a perspective of smooth optimisation .",
    "the paper is organised as follows . in section  [ sec:02 ] , by revisiting the classic bp algorithm , we introduce the concepts and techniques from multivariable differential calculus that are crucial in our analysis and development . in section  [ sec:03 ] ,",
    "we characterise the critical point conditions of the general fnn learning cost function , and derive several conditions or design principles for eliminating local minima . with a further analysis of the hessian matrix of the cost function at the global minima , we characterise the isolatedness properties of the global minima in section  [ sec:04 ] .",
    "an approximate newton s algorithm is developed by leveraging a simple structure of the true hessian at the global minima . in section  [ sec:05 ] , we study the classic xor problem .",
    "some classic results on xor networks are reviewed in the developed framework of this paper .",
    "section  [ sec:06 ] studies a challenging benchmark of the four - region classification problem with numerical experiments .",
    "finally , conclusions and outlooks are given in section  [ sec:07 ] .",
    "by revisiting the classic bp algorithm in this section , we aim to introduce the mathematical notations and concepts used in our development and analysis .",
    "we refer to @xcite for further readings on the technique .",
    "let us denote by @xmath0 the number of layers in an fnn structure , and by @xmath1 the number of processing units in the @xmath2-th layer with @xmath3 .",
    "specifically , by letting @xmath4 , we refer to the input layer .",
    "let @xmath5 be an elementary activation function , which is often constructed to be non - constant , continuous , and monotonically increasing .",
    "popular choices of activation function include the sigmoid function or @xmath6 function .",
    "we denote by @xmath7 and @xmath8 the first derivative and second derivative of the activation function .    for the @xmath9-th unit in an fnn architecture , referring to the @xmath10-th unit in the @xmath2-th layer , we can define each unit mapping as @xmath11 where @xmath12 denotes the output from the @xmath13-th layer , @xmath14 is the weight vector associated with the @xmath9-th unit , and @xmath15 is a scalar bias associated with each unit . for the sake of convenience in technical representation of our analysis , we drop the scalar bias @xmath16 in our further development . straightforwardly , we can define the @xmath2-th layer evaluation mapping by stacking all unit mappings in the layer as @xmath17^{\\top}\\!\\!\\ ! ,      \\end{split}\\ ] ] with @xmath18 \\in   \\mathbb{r}^{n_{l-1 } \\times n_{l}}$ ] being the @xmath2-th weight matrix .",
    "specifically , let us denote by @xmath19 the input , then we define @xmath20 iteratively .",
    "by composing all the layer - wise mappings together , we end up with the overall network mapping as @xmath21 where @xmath22 . for a specific learning task ,",
    "one often deploys a suitable error function @xmath23 . in this work ,",
    "we only consider the supervised learning setting with a dataset with @xmath24 samples , denoted by @xmath25 . finally ,",
    "for a given input @xmath26 , we define the overall _ fnn learning cost function _ as @xmath27 it is trivial to see that , if the error function @xmath28 is differentiable in @xmath29 , then the cost function @xmath30 is differentiable in both fnn weights @xmath31 and the input data @xmath32 . for the supervised learning setting , i.e. , given dataset @xmath25 , the overall fnn learning cost function can be formed as @xmath33 for the convenience of presentations in the rest of the paper",
    ", we will drop the summation , only focus on the analysis of sampled cost function @xmath30 .    in order to develop a gradient descent algorithm to minimise the cost function @xmath30 .",
    "we need to define the derivative of all processing units , i.e. , for a given @xmath9-th unit , we have @xmath34 and similarly , for the @xmath2-th layer @xmath35^{\\top}\\!\\!\\!.      \\end{split}\\ ] ] for simplicity , we denote @xmath36 .    due to its layer - wise structure of the fnn , by fixing a parameter @xmath37",
    ", we can define a truncated learning cost function , referred to as the @xmath2-th _ tail error function _ , as @xmath38 by applying the chain rule of multivariable derivative , we compute the first derivation of @xmath30 with respect to @xmath37 in direction @xmath39 as @xmath40 where @xmath41 and @xmath42 refer to the derivative of @xmath43 with respect to the first and second argument .",
    "explicitly , the first derivative of @xmath43 , i.e. , @xmath44 @xmath45 in direction @xmath46 , is computed as @xmath47 where the operator @xmath48 puts a vector into a diagonal matrix form , and the derivative of @xmath43 with respect to the second parameter in direction @xmath49 as @xmath50 let us denote @xmath51 for all @xmath3 . then , it is straightforward to derive the gradient of @xmath30 with respect the @xmath2-th weight matrix @xmath52 as @xmath53 which is a rank - one matrix update . by exploring the layer - wise structure of the fnn ,",
    "the corresponding vector @xmath54 , referred to as the _ @xmath2-th error vector _ , can be computed iteratively backwards from the output layer @xmath0 , i.e. @xmath55 with @xmath56 . with such a backward mechanism in computing the gradient @xmath57 , we recover the classic bp algorithm , as presented in algorithm  [ algo:01 ] .",
    "such a strategy can be easily adapted to both the batch learning and the online learning setting . in",
    "what follows , we will focus on an analysis in the batch learning setting .",
    "with the gradient computed explicitly as in , the critical points of the fnn learning cost @xmath30 are characterised by simply setting it to zero , namely , @xmath58 . more explicitly , for a given sample @xmath59",
    ", we have the following equation system in @xmath22 as @xmath60 obviously , it is hardly possible to completely characterise all critical points of @xmath30 .",
    "however , the major goal for training an fnn is to minimise the error function @xmath61 . in other words , it is more sensible to characterise the gradient of the error function @xmath28 with respect to the output of the fnn , i.e. , @xmath62 , instead of the gradient of the overall fnn learning cost @xmath63 . in order to guarantee uniqueness of optimal solutions in minimising the error function @xmath61 ,",
    "it is reasonable to ensure that the function @xmath28 is strictly convex .",
    "with such a construction , training an fnn has no problems in ending at local minima of the error function .",
    "[ prin : error ] the error function @xmath23 needs to be strictly convex .",
    "nows , we rewrite the critical point conditions of the overall learning cost @xmath30 in eq .  , in terms of @xmath64 .",
    "similar to the iterative construction of the error vector @xmath54 as in eq .",
    ", we construct a sequence of matrices as , for all @xmath65 , @xmath66 with @xmath67 . for a given @xmath68-th sample @xmath59 , the equation system as in eq .",
    "can be rewritten as @xmath69          \\nabla_{e}(\\phi_{l}^{(i ) } ) = 0,\\ ] ] where @xmath70 denotes the kronecker product of matrices .",
    "let us denote the total number of variables in an fnn by @xmath71 then , by collecting all equation systems from all the samples @xmath72 , we end up with the following gigantic equation system in @xmath73 as @xmath74      } _ { = : \\mathbf{p } \\in \\mathbb{r}^{n_{net } \\times ( t \\cdot n_{l } ) } }      \\ !",
    "\\left[\\!\\!\\ ! \\begin{array}{c }",
    "\\nabla_{e}(\\phi_{l}^{(1 ) } ) \\\\      \\vdots \\\\",
    "\\nabla_{e}(\\phi_{l}^{(t ) } )      \\end{array}\\!\\!\\!\\right ] \\!= 0.\\ ] ] obviously , the trivial solution of @xmath75 for all @xmath72 is the only solution , if and only if the rank of matrix @xmath76 is equal to @xmath77 .",
    "thus , we just proved the following theorem .",
    "let the error function @xmath23 be strictly convex .",
    "then the fnn learning cost function @xmath78 is free of local minima , if and only if , @xmath79    given the number of rows of @xmath76 being @xmath80 , we conclude our second principle of fnn design as follows .",
    "[ prin : number ] the total number of variables in an fnn , i.e. , @xmath80 , needs to be greater than or equal to @xmath77 .",
    "now , let us have a closer look at the matrix @xmath76 to identify potential strategies to ensure the rank condition to be satisfied as much as possible . by collecting the entries of @xmath81 s and @xmath82 s",
    ", we construct the following two partitioned matrices @xmath83\\ ] ] and @xmath84.\\ ] ] the two matrices certainly share the same partition structure by construction .",
    "then the matrix @xmath76 is simply the tracy - singh product of @xmath85 and @xmath86 as defined in eq .",
    ", i.e. , the pairwise kronecker product for each pair of partitions in @xmath85 and @xmath86 , denoted by @xmath87 by the properties of the tracy - singh product @xcite , it follows directly @xmath88 since the rank of @xmath86 is determined by the data and the fnn architecture , from a design perspective , it is therefore more sensible to guarantee the rank of @xmath85 . recall the construction of partitions @xmath81 as in eq .",
    ", i.e. , products of @xmath89 s and @xmath37 s , it is thus reasonable to make these matrix terms as of full rank as possible .",
    "we can then conclude the following two principles of designing an fnn .",
    "[ prin : weight ] weight matrix @xmath90 needs to be of full rank for all @xmath3 .",
    "[ prin : act ] the derivative of activation function @xmath91 needs to be nonzero for all @xmath92 , i.e. , @xmath93 .",
    "it is trivial to verify that most of popular activation functions , such as sigmoid , @xmath6 , logistic , softplus , and softsign , are monotonically increasing , hence satisfy principle  [ prin : act ] .    in the rest of this section",
    ", we consider an fnn architecture with only one hidden layer , i.e. , @xmath94 , and derive a classic result in the literature .",
    "we prove the following well known , but still arguable , results as shown in @xcite .",
    "[ prop:01 ] given an fnn architecture with only one hidden layer , including a dummy unit .",
    "assume that the training dataset consists of @xmath24 unique samples .",
    "then an fnn learning cost function is free of local minima , if the following four conditions are fulfilled :    1 .",
    "the error function @xmath28 is strictly convex ; + 2 .",
    "there are @xmath95 units in the hidden layer ; + 3 .   with any weight matrix @xmath96 in the hidden layer having no identical columns , @xmath24 unique samples produce a basis in the output space of the hidden layer ; + 4",
    ".   the derivatives of activation functions employed in the output layer are nonzero .    without loss of generality",
    ", we can bring the dummy hidden unit to be real , which represents the associated scaler bias and receives a constant input of value @xmath97 from the input layer , i.e. , @xmath98 .",
    "then the critical point conditions at the output layer @xmath99 reads as @xmath100          } _ { : = p_{2 } \\in \\mathbb{r}^{(t \\cdot n_{2 } ) \\times ( t \\cdot n_{2 } ) } }          \\left[\\!\\!\\ ! \\begin{array}{c }          \\nabla_{e}(\\phi_{2}^{(1 ) } ) \\\\          \\vdots \\\\",
    "\\nabla_{e}(\\phi_{2}^{(t ) } )          \\end{array}\\!\\!\\!\\right ] \\!= 0,\\ ] ] with @xmath101 equations ,",
    "i.e. , @xmath102 is a square matrix .",
    "it is straightforward to conclude that condition ( 4 ) ensures @xmath103\\big ) = n_{2}.\\ ] ] with condition ( 3 ) , the matrix @xmath102 is of full rank .",
    "hence , the result follows .",
    "certainly , fnn architectures used in real applications often consist of more hidden layers .",
    "nevertheless , if we fix the @xmath2-th hidden layer with @xmath104 , then locally the previous @xmath13-th layer can be treated as the input layer , while the following @xmath105-th layer can be considered as the output layer .",
    "the learning cost function in the output @xmath106 is nothing but the @xmath105-th tail error function as defined in eq .  .",
    "the results in preposition  [ prop:01 ] can be used for choosing the number of units in the hidden layers .    for a given @xmath2-th hidden layer with @xmath104",
    ", we assume that there are @xmath107 unique patterns after forwarding thought the previous @xmath108 layers",
    ". then we need at least @xmath109 units in this layer to ensure a potential exemption of local minima .",
    "this design principle is very heuristic , since knowledge of the number of patterns in the hidden layers are generally unknown .",
    "when pooling is used in a feedforward convolutive neural network , it might be interesting to see the role of this principle .",
    "we treat this aspect as one potential research direction in the future .",
    "the information from the hessian matrix is critically important for designing efficient numerical algorithms .",
    "specifically , definiteness of the hessian matrix is the indicator to the isolatedness of the critical points , which will affect significantly the convergence speed of the algorithms .",
    "the hessian form of the fnn learning cost function @xmath30 is a bilinear operator @xmath110 , computed by the second derivative of @xmath30 .",
    "its computation is rather straightforward but very tedious . for the sake of readability",
    ", we only present one component of the second derivative with respect to two specific indices @xmath10 and @xmath2 , i.e. , @xmath111 where @xmath112 is the diagonal matrix with the second derivative of the activation functions on the diagonal .",
    "let @xmath113 be the desired output of the fnn , reached by an global minima @xmath114 . then all gradients @xmath115 of the error function are equal to zero . as a sequel , the second summand in the last equation in eq .",
    "therefore , the hessian @xmath116 evaluated at @xmath114 is given as @xmath117 where @xmath118 is the hessian form of the error function @xmath28 . for a given sample @xmath59 ,",
    "we construct the following two identically partitioned matrices , i.e. , @xmath119          \\mathsf{h}_{e}(\\phi_{l}^{*(i ) } )          \\left[\\!\\!\\ ! \\begin{array}{c }",
    "\\psi_{l}^{(i ) } \\\\      \\vdots \\\\",
    "\\psi_{1}^{(i ) }       \\end{array}\\!\\!\\!\\right]^{\\top}\\ ] ] and @xmath120          \\left[\\!\\!\\ ! \\begin{array}{c }",
    "\\phi_{l-1}^{*(i ) } \\\\      \\vdots \\\\",
    "\\phi_{0}^{*(i ) }       \\end{array}\\!\\!\\!\\right]^{\\top}.\\ ] ] then , the hessian of @xmath30 at a global minima @xmath114 can be represented in a matrix form as @xmath121 if the activation functions used at the output layer follow principle  [ prin : act ] , then the rank of matrix @xmath122 as defined in eq .",
    "is determined by the rank of the hessian @xmath123 .",
    "if the error function @xmath28 is chosen according to principle  [ prin : error ] , then it needs to be further assume have a non - degenerate hessian , i.e. , @xmath28 being a morse function , cf .",
    "@xcite consequently , we have @xmath124 hence , we introduce the next design principle on the choice of error function , in addition to principle  [ prin : error ]",
    ".    1.a[strong choice of error function ] [ prin : error2 ] the error function @xmath23 needs to be strictly convex and morse , i.e. , the hessian @xmath125 is non - degenerate for all @xmath126 .",
    "since both matrices @xmath127 and @xmath122 are positive semi - definite , the hessian matrix @xmath128 is simply a sum of @xmath24 rank-@xmath129 positive semi - definite matrices .",
    "we can conclude the following result .    given an fnn architecture satisfying principle  [ prin : error ] , [ prin : number ] , [ prin : act ] , [ prin : error2 ] , and a training dataset consisting of @xmath24 unique samples .",
    "if a global minimum @xmath114 of the fnn learning cost is reachable , then the rank of the hessian matrix of @xmath30 is bounded from above by @xmath130    it is important to notice that the hessian @xmath128 is neither diagonal nor block diagonal , which demotivates the existing approximate strategies of the hessian in @xcite . with our explicit characterisation of the hessian at global minima , we propose to approximate the hessian of @xmath30 at arbitrary point @xmath31 with the structure as shown in eq .  .",
    "the corresponding approximate newton s algorithm is presented in algorithm  [ algo : newtonbp ] .",
    "it is important to notice that , in step  6 , a scaled identity matrix with @xmath131 is added to the approximate hessian in order to ensure the existence of the inverse .",
    "moreover , the computation of the newton update in step  6 can be efficiently carried out by implementing a block cholesky decomposition .",
    "the xor problem is a classic and most well known demonstrative scenario for fnn training .",
    "two specific network structures , i.e. , the @xmath132-@xmath132-@xmath97 xor network and the @xmath132-@xmath133-@xmath97 xor network , have been extensively studied in the literature . in this section",
    ", we apply our developed analysis to investigate some classic results on the xor networks .",
    "we confine ourselves to the network structure with only one hidden layer , while the structure of the input and output layers are fixed .",
    "namely , we have @xmath94 , @xmath134 , and @xmath135 .",
    "all processing units are chosen to be the sigmoid function .",
    "we define the input patterns as @xmath136       = \\left [      \\begin{matrix }          0 & 0 & 1 & 1 \\\\          0 & 1 & 0 & 1      \\end{matrix }      \\right ] \\in \\mathbb{r}^{2 \\times 4},\\ ] ] and correspondingly , the output @xmath137 = [ 0 , 1 , 1 , 0 ] \\in \\mathbb{r}^{1 \\times 4}.\\ ] ] there are @xmath138 unique samples",
    ". then the fnn learning cost function is defined as @xmath139 where @xmath140 with @xmath141 and @xmath142 .",
    "firstly , let us have a look at the @xmath132-@xmath132-@xmath97 xor network , i.e. , @xmath143 .",
    "we then investigate the rank of the following matrix @xmath144      \\psi_{1}^{(1 ) } \\otimes \\phi_{0}^{(1 ) } &      \\!\\!\\ldots\\!\\ ! &      \\psi_{1}^{(4 ) } \\otimes \\phi_{0}^{(4 ) }      \\end{array}\\!\\!\\!\\right].\\ ] ] by considering the biases associated with each unit , the total number of fnn variables is @xmath145 since @xmath135 , it is easily to see that @xmath146 we can construct two matrices @xmath147      \\psi_{1}^{(i ) }       \\end{array}\\!\\!\\!\\right ]          \\mathsf{h}_{e}(\\phi_{2}^{*(i ) } )          \\left[\\!\\!\\ ! \\begin{array}{c }",
    "\\psi_{2}^{(i ) } \\\\[1 mm ]      \\psi_{1}^{(i ) }       \\end{array}\\!\\!\\!\\right]^{\\top}\\ ] ] and @xmath148      \\phi_{0}^{*(i ) }       \\end{array}\\!\\!\\!\\right ]          \\left[\\!\\!\\ ! \\begin{array}{c }",
    "\\phi_{1}^{*(i ) } \\\\[1 mm ]      \\phi_{0}^{*(i ) }       \\end{array}\\!\\!\\!\\right]^{\\top}.\\ ] ] since the hessian @xmath149 is a scalar , it is trivial to see that the hessian of @xmath150 is a sum of four rank - one matrices , i.e. , @xmath151 , hence degenerate .",
    "we then conclude the classic results as in @xcite .",
    "let a @xmath132-@xmath132-@xmath97 xor network architecture satisfy all four principles  [ prin : error]-[prin : act ] .",
    "then the network has no local minima and global minima have a degenerate hessian .",
    "similar analysis on the @xmath132-@xmath133-@xmath97 xor network leads to the following results .    let a @xmath132-@xmath133-@xmath97 xor network architecture satisfy three principles  [ prin : error ] , [ prin : number ] , [ prin : act ] .",
    "then the network has no local minima and global minima have a degenerate hessian .",
    "it is clear then that any xor network with an architecture @xmath132-@xmath152-@xmath97 for @xmath153 has only degenerate global minima .",
    "in our numerical experiments , we investigate performance of our proposed approximate newton s algorithm on the four regions classification benchmark , as originally proposed in @xcite .        in @xmath154 around the origin , we have a square area @xmath155 , and three concentric circles with their radiuses being @xmath97 , @xmath132 , and @xmath133 .",
    "four regions / classes are interlocked , nonconvex , as shown in figure  [ fig:1a ] .",
    "we draw randomly @xmath156 samples in the box for training , and specify the corresponding output to be the @xmath68-th basis vector in @xmath157 .",
    "we deploy an fnn architecture with two hidden layers , i.e. , @xmath158 . in both hidden layer",
    ", there are @xmath159 units each .",
    "hence , we have @xmath134 , @xmath160 , and @xmath161 .",
    "all activation functions are chosen to be sigmoid . finally , the standard least squares error is used as the error function , which is morse and strictly convex .",
    "our experiments indicates that the approximate newton s algorithm is capable of handling vanishing gradient , and reach much lower value in the error function .",
    "more specifically , the convergence speed of the proposed approximate newton s fnn algorithm is much faster than the classic bp algorithm , shown in figure  [ fig : converge ] .    by random sampling",
    ", the learned fnn produces quite accurate results as shown in figure  [ fig:1c ] , expect the points on close to the boundary . in order to copy with such an issue , we construct a framed dataset , consisting @xmath162 samples on the two sides along the boundary with the other @xmath163 random samples shown in figure  [ fig:1b ] .",
    "we test the same test sample with the fnn learned from the framed dataset , leading to a much more accurate classification shown in figure  [ fig:1d ] .",
    "in this work , we developed a smooth optimisation perspective on the challenge of designing and training an fnn architecture in the supervised learning setting . by characterising the critical point conditions of the overall learning cost function , we derive some mild conditions to ensure an fnn to be free of local minima .",
    "classic results on this matter in both xor scenario and general setting are also reviewed .",
    "our analysis also identifies the hessian structure of the cost function at the global minima , which leads to an approximate newton s fnn algorithm .",
    "x.  glorot and y.  bengio , `` understanding the difficulty of training deep feedforward neural networks , '' in _ proceedings of the thirteenth international conference on artificial intelligence and statistics ( aistats-10 ) _ , vol .  9 , 2010 , pp . 249256 .",
    "s.  sun , w.  chen , l.  wang , x.  liu , and t .- y .",
    "liu , `` on the depth of deep neural networks : a theoretical view , '' in _ proceedings of the @xmath164 aaai conference on artificial intelligence _",
    ", 2016 , pp . 20662072 .",
    "t.  falas and a.  g. stafylopatis , `` the impact of the error function selection in neural network - based classifiers , '' in _ proceedings of the international joint conference on neural networks ( ijcnn ) _ , vol .  3 , 1999 , pp .",
    "17991804 .",
    "r.  s. sutton , `` two problems with backpropagation and other steepest - descent learning procedures for networks , '' in _ proceedings of the @xmath165-th annual conference of the cognitive science society _ , 1986 , pp .",
    "823831 .",
    "i.  g. sprinkhuizen - kuyper and e.  j.  w. boers , `` the local minima of the error surface of the 2 - 2 - 1 xor network , '' _ annals of mathematics and artificial intelligence _ , vol .  25 , no .  1 ,",
    "pp . 107136 , 1999 .",
    "a.  choromanska , m.  henaff , m.  mathieu , g.  b. arous , and y.  lecun , `` the loss surfaces of multilayer networks , '' in _ proceedings of the @xmath166 international conference on artificial intelligence and statistics ( aistats ) _ , 2015 , pp .",
    "192204 .",
    "i.  j. goodfellow , o.  vinyals , and a.  m. saxe , `` qualitatively characterizing neural network optimization problems , '' published at the @xmath167 international conference on learning representations ( iclr ) .",
    "arxiv:1412.6544 . , 2015 ."
  ],
  "abstract_text": [
    "<S> despite the recent great success of deep neural networks in various applications , designing and training a deep neural network is still among the greatest challenges in the field . in this work </S>",
    "<S> , we present a smooth optimisation perspective on designing and training multilayer feedforward neural networks ( fnns ) in the supervised learning setting . by characterising the critical point conditions of an fnn based optimisation problem </S>",
    "<S> , we identify the conditions to eliminate local optima of the corresponding cost function . moreover , </S>",
    "<S> by studying the hessian structure of the cost function at the global minima , we develop an approximate newton fnn algorithm , which is capable of alleviating the vanishing gradient problem . </S>",
    "<S> finally , our results are numerically verified on two classic benchmarks , i.e. , the xor problem and the four region classification problem .    </S>",
    "<S> designing and training feedforward neural networks : a smooth optimisation perspective + hao  shen + e - mail : hao.shen@tum.de + department of electrical and computer engineering + technische universitt mnchen , germany +    * * + forward neural networks ( fnns ) , smooth optimisation , critical point analysis , hessian matrix , approximate newton s method . </S>"
  ]
}