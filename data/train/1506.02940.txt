{
  "article_text": [
    "univariate models have been widely used for short - run forecast ( see , e.g. , ( * ? ? ?",
    "* examples of chapter 2 ) . in",
    "what follows , we recall some of these techniques , focusing ourselves particularly on the analysis of autoregressive ( ar ) processes , moving average ( ma ) processes , and a combination of both types , the so - called arma processes ; for further details , see , for example , @xcite and references therein .",
    "the observation on the time - series variable @xmath0 made at date @xmath1 is denoted by  @xmath2 , whereas @xmath3 indicates the total number of observations .",
    "moreover , we denote the @xmath4th lag of a time series @xmath5 by @xmath6 ( the value of the variable @xmath0 @xmath7 periods ago ) ; similarly , @xmath8 denotes the value of @xmath0 @xmath7 periods to the future , where , for any fixed @xmath9 is such that @xmath10 and @xmath11 .",
    "the @xmath4th autocovariance of a series @xmath2 is the covariance between @xmath2 and its @xmath7th lag , that  is , @xmath12 , whereas the @xmath4th autocorrelation coefficient is the correlation between @xmath2 and @xmath6 , that s is , @xmath13 .",
    "when the  average and variance of a variable are unknown , we can estimate them by taking a random sample of @xmath14 observations . in a simple random sample ,",
    "@xmath14 objects are drawn at random from a population , and each object is equally likely to be drawn .",
    "the value of the random variable @xmath0 for the @xmath15th randomly drawn object is denoted @xmath16 . because each object is equally likely to be drawn and",
    "the distribution of @xmath16 is the same for all @xmath15 , the random variables @xmath17 are independent and identically distributed ( i.i.d . ) . given a variable @xmath0 , we denote by @xmath18 its sample average with respect to the @xmath14 observations @xmath17 , that s is , @xmath19 , whereas we define the related sample variance by @xmath20 the @xmath4th autocovariances , resp .",
    "autocorrelations , can be estimated by the @xmath4th sample autocovariances , resp .",
    "autocorrelations , as follows : @xmath21 , resp .",
    "@xmath22 , where @xmath23 denotes the sample average of @xmath24 computed over the observations @xmath25 . concerning forecast based on regression models that relates a time series variable to its past values , for completeness",
    ", we shall start with the first - order autoregressive process , namely the ar(1 ) model , which uses @xmath26 to forecast @xmath2 .",
    "a systematic way to forecast is to estimate an ordinary least squares ( ols ) regression .",
    "the ols estimator chooses the regression coefficients so that the estimated regression line is as close as possible to the observed data , where the closeness is measured by the sum of the squared mistakes made in predicting @xmath2 given @xmath26 .",
    "hence , the ar(1 ) model for the series @xmath2 is given by @xmath27 where @xmath28 and @xmath29 are the regression coefficients .",
    "in this case , the intercept @xmath28 is the value of the regression line when @xmath30 , the slope @xmath29 represents the change in @xmath2 associated with a unit change in @xmath26 , and @xmath31 denotes the error term whose nature will be later clarified . let us assume that the value @xmath32 of the time series @xmath2 at initial time @xmath33 is given ; then @xmath34 , so that iterating relation up to order @xmath35 , we get @xmath36 hence , taking @xmath37 with @xmath38 , we obtain @xmath39 a time series @xmath2 is called _ stationary _ if its probability distribution does not change over time , that is , if the joint distribution of @xmath40 does not depend on @xmath41 ; otherwise , @xmath2 is said to be _",
    "nonstationary_. in , the process @xmath2 consists of both time - dependent deterministic and stochastic parts , and , thus , it can not be stationary .    formally , the process with stochastic initial conditions results from if and only if @xmath42 .",
    "it follows that if @xmath43 is bounded , then , as @xmath44 , we have @xmath45 see , for example , ( * ? ? ?",
    "equation can be rewritten by means of the lag operator , which acts as follows : @xmath46 , so that eq .   becomes @xmath47 .",
    "assuming that @xmath48 = 0 $ ] for all @xmath1 , we have @xmath49 & = e \\biggl [ \\frac{\\beta_0 } { 1 - \\beta_1 } + \\sum _ { j=0}^{\\infty } \\beta_1^j u_{t - j } \\biggr ] = \\frac{\\beta_0 } { 1 - \\beta_1 } + \\sum _ { j=0}^{\\infty } \\beta_1^j e [ u_{t - j } ] = \\frac{\\beta_0 } { 1 - \\beta_1 } = \\mu,\\\\   v[y_t ] & = e \\biggl [ \\biggl ( y_t - \\frac{\\beta_0 } { 1 - \\beta_1 } \\biggr)^2 \\biggr ] = e \\biggl [ \\biggl ( \\sum _ { j=0}^{\\infty } \\beta_1^j u_{t - j } \\biggr)^2 \\biggr]\\\\ & = e\\bigl[\\bigl(u_t+\\beta_1 u_{t-1}+ \\beta_1 ^ 2 u_{t-2 } + \\cdots\\bigr)^2 \\bigr]\\\\ & = e\\bigl[u_t^2+\\beta_1 ^ 2 u_{t-1}^2+\\beta_1 ^ 4 u_{t-2}^2 + \\cdots+ 2 \\beta_1 u_t u_{t-1 } + 2 \\beta_1 ^ 2 u_t u_{t-2 } + \\cdots\\bigr]\\\\ & = \\sigma^2 \\bigl(1 + \\beta_1 ^ 2 + \\beta_1 ^ 4 + \\cdots\\bigr)=\\frac{\\sigma^2}{1-\\beta _ 1 ^ 2 } , \\end{aligned}\\ ] ] where we have used that @xmath50 = 0 $ ] for @xmath51 and @xmath52 . hence , both the mean and variance are constants , and thus the covariances are given by @xmath53 & = e \\biggl [ \\biggl ( y_t - \\frac{\\beta_0 } { 1 - \\beta _ 1 } \\biggr ) \\biggl ( y_{t-1}- \\frac{\\beta_0 } { 1 - \\beta_1 } \\biggr ) \\biggr]\\\\ & = e\\big[\\bigl(u_t+\\beta_1 u_{t-1}+\\cdots+ \\beta_1^\\tau u_{t-\\tau } + \\cdots\\bigr)\\\\ & \\quad\\xch{\\times } { } \\bigl(u_{t-\\tau}+\\beta_1 u_{t-\\tau-1}+ \\beta_1 ^ 2 u_{t-\\tau-2 } + \\cdots\\bigr)\\big]\\\\ & = e\\bigl[\\bigl(u_t+\\beta_1 u_{t-1}+\\cdots+ \\beta_1^{\\tau-1 } u_{t-\\tau-1}\\\\ & \\quad + \\beta_1^\\tau\\bigl(u_{t-\\tau}+ \\beta_1 u_{t-\\tau-1}+\\beta_1 ^ 2 u_{t-\\tau-2 } + \\cdots\\bigr)\\bigr)\\\\ & \\quad\\xch{\\times } { } \\bigl(u_{t-\\tau}+\\beta_1 u_{t-\\tau-1}+ \\beta_1 ^ 2 u_{t-\\tau-2 } + \\cdots\\bigr)\\bigr]\\\\ & = \\beta_1^\\tau e\\bigl[\\bigl(u_{t-\\tau}+ \\beta_1 u_{t-\\tau-1}+\\beta_1 ^ 2 u_{t-\\tau -2 } + \\cdots\\bigr)^2\\bigr]=\\beta_1^\\tau v[y_{t-\\tau}]\\\\ & = \\beta_1^\\tau\\frac{\\sigma^2}{1-\\beta_1 ^ 2 } = : \\gamma(\\tau ) . \\end{split } \\ ] ] the previous ar(1 ) can be generalized by considering arbitrary but finite order . in particular , an ar(@xmath54 )",
    "process can be described by the equation @xmath55 where @xmath56 are constants , whereas @xmath31 is the error term represented by a  random variable with zero mean and variance @xmath57 . using the lag operator , we can rewrite eq .   as @xmath58 .",
    "in such a framework , it is standard to assume that the following four properties hold ( see , e.g. , ( * ? ? ? * chap .",
    "14.4 ) ) :    * @xmath31 has conditional mean zero , given all the regressors , that is,@xmath59 , which implies that the best forecast of @xmath2 is given by the @xmath60 regression .",
    "* @xmath16 has a stationary distribution , and @xmath16 , @xmath61 are assumed to become independent as @xmath7 gets large .",
    "if the time - series variables are nonstationary , then the forecast can be biased and inefficient , or conventional ols - based statistical inferences can be misleading . *",
    "all the variables have nonzero finite fourth moments .",
    "* there is no perfect multicollinearity , namely it is not true that , given a  certain regressor , it is a perfect linear function of the variables .      in this section ,",
    "we show how the previously introduced class of models can be used to predict the future behavior of a certain quantity of interest .",
    "if @xmath2 follows the @xmath60 model and @xmath62 are unknown , then the forecast of @xmath63 is given by @xmath64 .",
    "forecasts must be based on estimates of the coefficients @xmath65 by using the ols estimators based on historical data .",
    "let @xmath66 denote the forecast of @xmath63 based on @xmath67 : @xmath68 then such a forecast refers to some data beyond the data set used to estimate the regression , so that the data on the actual value of the forecasted dependent variable are not in the sample used to estimate the regression .",
    "forecasts and forecast error pertain to `` out - of - sample '' observations .",
    "the forecast error is the mistake made by the forecast ; this is the difference between the value of @xmath63 that actually occurred and its forecasted value @xmath69 .",
    "the root mean squared forecast error rmsfe is a measure of the size of the forecast error @xmath70}$ ] , and it is characterized by two sources of error : the error arising because future values of @xmath71 are unknown and the error in estimating the coefficients @xmath65 .",
    "if the first source of error is much larger than the second , the rmsfe is approximately @xmath72 , the standard deviation of the error @xmath31 , which is estimated by the standard error of regression ( ser ) .",
    "one useful application used in time - series forecasting is to test whether the lags of one regressor have useful predictive content .",
    "the claim that a variable has no predictive content corresponds to the null hypothesis that the coefficients on all lags of that variable are zero .",
    "such a hypothesis can be checked by the so - called granger causality test ( gct ) , a type of f - statistic approach used to test joint hypothesis about regression coefficients . in particular , the gct method tests the hypothesis that the coefficients of all the values of the variable in @xmath73 , namely the coefficients of @xmath74 , are zero , and hence this null hypothesis implies that such regressors have no predictive content for @xmath2 .",
    "let us recall relevant statistical methods used to optimally choose the number of lags in an autoregression model ; in particular , we focus our attention on the _ bayes _ method ( bic ) and on the _ akaike _ method ( aic ) ; for more details , see , for example , ( * ? ? ?",
    "the bic method is specified by @xmath75 where @xmath76 is the _ sum of squared residuals _ of the estimated @xmath60 .",
    "the @xmath77 estimator of @xmath54 is the value that minimizes @xmath78 among all the possible choices . in the first term of eq .",
    ", the sum of squared residuals necessarily decreases when adding a lag .",
    "in contrast , the second term is the number of estimated regression coefficients times the factor @xmath79 , so this term increases when adding a lag .",
    "this implies that the @xmath77 trades off these two aspects .",
    "the aic approach is defined by @xmath80 and hence the main difference between the @xmath81 and @xmath77 is that the term @xmath82 in the @xmath77 is replaced by @xmath83 in the @xmath81 , so the second term in the @xmath81 is smaller .",
    "but the second term in the @xmath81 is not large enough to assure choosing the correct length , so this estimator of @xmath54 is not consistent .",
    "we recall that an estimator is consistent if , as the size of the sample increases , its probability distribution concentrates at the value of the parameter to be estimated .",
    "so , the bic estimator @xmath84 of the lag length in an autoregression is correct in large samples , that is , @xmath85 .",
    "this is not true for the alc estimator , which can overestimate @xmath54 even in large samples ; for the proof , see , for example , ( * ? ? ?",
    "* appendix 14.5 ) .      a further relevant topic in econometric analysis",
    "is constituted by nonstationarities that are due to trends and breaks .",
    "a trend is a persistent long - term movement of a variable over time .",
    "a time - series variable fluctuates around its trend .",
    "there are two types of trends , deterministic and stochastic .",
    "a _ deterministic trend _ is a nonrandom function of time .",
    "in contrast , a stochastic trend is characterized by a random behavior over time .",
    "our treatment of trends in economic time series focuses on stochastic trend .",
    "one of the simplest models of time series with stochastic trend is the one - dimensional _ random walk _ defined by the relation @xmath86 , where @xmath31 is the error term represented by a normally distributed random variable with zero mean and variance @xmath57 . in this case ,",
    "the best forecast of tomorrow s value is its value today .",
    "a extension of the latter is the _ random walk with drift _ defined by @xmath87 , where the best forecast is the value of the series today plus the drift @xmath28 .",
    "a  random walk is nonstationary because the variance of a random walk increases over time , so the distribution of @xmath2 changes over time .",
    "in fact , since @xmath31 is uncorrelated with @xmath26 , we have @xmath88 with @xmath89 if and only if @xmath90 . the random walk is a particular case of an @xmath91 model with @xmath92 .",
    "if @xmath93 and @xmath31 is stationary , then @xmath2 is stationary .",
    "the condition for the stationarity of an @xmath60 model is that the roots of @xmath94 are greater than one in absolute value .",
    "if an @xmath60 has a root equal to one , then we say that the series has a _ unit root _ and a _",
    "stochastic trend_. stochastic trends usually bring many issues , for example , the autoregressive coefficients are biased toward zero . because @xmath2 is nonstationary , the assumptions for time - series regression do not hold , and we can not rely on estimators and test statistics having their usual large - sample normal distributions ;",
    "see , for example , ( * ? ? ?",
    "* chap .",
    "in fact , the ols estimator of the autoregressive coefficient @xmath95 is consistent , but it has a nonnormal distribution ; then the asymptotic distribution of @xmath96 is shifted toward zero . another problem caused by stochastic trend is the nonnormal distribution of the t - statistic , which means that conventional confidence intervals are not valid and hypothesis tests can not be conducted as usual .",
    "the t - statistic is an important example of a test statistic , namely of a statistic used to perform a  hypothesis test .",
    "a statistical hypothesis test can make two types of mistakes : a _ type i error _ , in which the null hypothesis is rejected when , in fact , it is true , and a _ type ii error _ , in which the null hypothesis is not rejected when , in fact , it is false .",
    "the prespecified rejection probability of a statistical hypothesis test when the null hypothesis is true , that is , the prespecified probability of a type i error , is called the _ significance level _ of the test .",
    "the _ critical value _ of the test statistic is the value of the statistic for which the test just rejects the null hypothesis at the given significance level .",
    "p - value _ is the probability of obtaining a test statistic , by random sampling variation , at least as adverse to the null hypothesis value as is the statistic actually observed , assuming that the null hypothesis is correct .",
    "equivalently , the @xmath54-value is the smallest significance level at which you can reject the null hypothesis .",
    "the value of the t - statistic is @xmath97 and is well approximated by the standard normal distribution when @xmath14 is large because of the central limit theorem ( see , e.g. , ( * ? ? ?",
    "* chap.4.3 ) ) .",
    "moreover , stochastic trends can lead two time series to appear related when they are not , a problem called _ spurious regression _ ( see , e.g. , ( * ? ? ?",
    "2 ) for examples ) . for the @xmath91 model ,",
    "the most commonly used test to determine stochastic trends , is the dickey  fuller test ( see , e.g. , ( * ? ? ?",
    "3 ) for details . for this test , we first subtract @xmath26 from both sides of the equation @xmath98",
    ". then we assume that the following hypothesis test holds : @xmath99 with @xmath100 . for an @xmath101 model ,",
    "it is standard to use the augmented dickey  fuller test ( adf ) , which tests the null hypothesis @xmath102 against the one - side alternative @xmath103 in the regression @xmath104 under the null hypothesis .",
    "let us note that since @xmath2 has a stochastic trend , it follows that , under the alternative hypothesis , @xmath2 is stationary .",
    "the adf statistic is the ols t - statistic testing @xmath105 . if , instead , the alternative hypothesis is that @xmath2 is stationary around a deterministic linear time trend , then this trend @xmath1 must be added as an additional regressor . in this case , the dickey  fuller regression becomes @xmath106 and we test for @xmath107 .",
    "the adf statistic does not have a normal distribution , and hence different critical values have to be used .",
    "a second type of nonstationarity arises when the regression function changes over the course of the sample . in economics",
    ", this can occur for a variety of reasons , such as changes in economic policy , changes in the structure of the economy , or an invention that changes a specific industry .",
    "these breaks can not be neglected by the regression model .",
    "a problem caused by breaks is that the ols regression estimates over the full sample will estimate a relationship that holds `` on average , '' in the sense that the estimate combines two different periods , and this leads to poor forecast .",
    "there are two types of testing for breaks : testing for a break at a known date and for a break at an unknown break date .",
    "we consider the first option for an @xmath60 model .",
    "let @xmath108 denote the hypothesized break date , and let @xmath109 be the binary variable such that @xmath110 if @xmath111 and @xmath112 if @xmath113 .",
    "then the regression including the binary break indicator and all interaction terms reads as follows : @xmath114 + \\gamma_2 \\bigl[d_t ( \\tau ) \\times y_{t-2 } \\bigr ] + \\cdots+ \\gamma_p \\bigl[d_t ( \\tau ) \\times y_{t - p } \\bigr ] + u_t \\end{split } \\ ] ] under the null hypothesis of no breaks , @xmath115 . under the alternative hypothesis",
    "that there is a break , the regression function is different before and after the break date @xmath108 , and we can use the f - statistic performing the so - called the chow test ( see , e.g. , ( * ? ? ? * chap .   5.3.3 ) ) .",
    "if we suspect a break between two dates @xmath116 and  @xmath117 , the chow test can be modified to test for breaks at all possible dates @xmath108 between @xmath116 and @xmath117 , then using the largest of the resulting f - statistics to test for a break at an unknown date .",
    "the latter technique is called the _",
    "quandt likelihood ratio statistic _ ( qlr ) ( see , e.g. , ( * ? ? ? * chap .",
    "because the qlr statistic is the largest of many f - statistics , its distribution is not the same as that of an individual f - statistic ; also , the critical values for the qlr statistic must be obtained from a special distribution .",
    "in the following , we consider finite - order moving - average ( ma ) processes ( see , e.g. , ( * ? ? ?",
    "* chap .",
    "the _ moving - average process of order @xmath118 _ , ma(@xmath118 ) , is defined by @xmath119 ; equivalently , by using the lag operator we get @xmath120 .",
    "every finite ma(@xmath118 ) process is stationary , and we have    * @xmath121 = \\alpha_0 , $ ] * @xmath122 = e[(y_t - \\alpha_0)^2 ] = ( 1 + \\alpha_1 ^ 2 + \\alpha_2 ^ 2 + \\cdots+ \\alpha_q^2 ) \\sigma^2 , $ ] * @xmath123 \\",
    "= e[(y_t - \\alpha_0)(y_{t+\\tau } - \\alpha _ 0 ) ] $ ] + @xmath124 + @xmath125 + @xmath126 .",
    "$ ]    combining both an autoregressive ( ar ) term of order @xmath54 and a moving - average ( ma ) term of order @xmath118 , we can define the process denoted as arma(@xmath127 ) and represented by @xmath128 again , exploiting the lag operator , we can write @xmath129",
    "in what follows , we focus our study on the so - called vector autoregression ( var ) econometric model , also using some remarks on the relation between the univariate time series models described in the first part , and the set of simultaneous equations systems of traditional econometrics characterizing the var approach ( see , e.g. , ( * ? ? ? * chap .",
    "2 ) ) .",
    "we have so far considered forecasting a single variable . however , it is often necessary to allow for a multidimensional statistical analysis if we want to forecast more than one - parameter dynamics .",
    "this section introduces a model for forecasting multiple variables , namely the vector autoregression ( var ) model , in which lagged values of two or more variables are used to forecast their future values .",
    "we start with the autoregressive representation in a var model of order @xmath54 , denoted by var(@xmath54 ) , where each component depends on its own lagged values up to @xmath54 periods and on the lagged values of all other variables up to order @xmath54 .",
    "it follows that the main idea behind the var model is to know how new information , appearing at a certain time point and concerning one of the observed variables , is processed in the system and which impact it has over time not only for this particular variable but also for the other system parameters .",
    "hence , a var(@xmath54 ) model is a set of @xmath130 time - series regressions @xmath131 ) in which the regressors are lagged values of all @xmath130 series and the number of lags equals  @xmath54 for each equation . in the case of two time series variables ,",
    "say , @xmath2 and @xmath132 , the var(@xmath54 ) consists of two equations of the form @xmath133 where the @xmath134s and the @xmath135s are unknown coefficients , and @xmath136 and @xmath137 are error terms represented by normally distributed random variables with zero mean and variance @xmath138 .",
    "the var assumptions are the same as those for the time - series regression defining ar models and applied to each equation ; moreover , the coefficients of each var are estimated by means of the ols approach . the reduced form of a vector autoregression of order@xmath54 is defined as , where @xmath139 , are quadratic matrices , @xmath140 represents the @xmath130-dimensional vector of residuals at time  @xmath1 , and @xmath141 is the vector of constant terms . system   can be rewritten compactly as @xmath142 , where @xmath143 , @xmath144 = 0 , \\",
    "e[u_t u_t ' ] = \\sigma_{uu}$ ] , and @xmath145 = 0 $ ] for @xmath146 . such a  system is stable if and only if all included variables are stationary , that is , if all roots of the characteristic equation of the lag polynomial are outside the unit circle , namely @xmath147 for @xmath148 ( for details , see , e.g. , ( * ? ? ?",
    "we use this condition because we saw in section  [ trends ] that the condition for the stationarity of an @xmath60 model is that the roots of @xmath94 are greater than one in absolute value .",
    "if an @xmath60 has a root equal to one , we say that the series has a _ unit root _ and",
    "a _ stochastic trend_. moreover , the previous system can be rewritten by exploiting the ma representation as follows : @xmath149 with @xmath150 the autocovariance matrices are defined as @xmath151 $ ] ; without loss of generality , we set @xmath107 and , therefore , @xmath152 , whence we obtain @xmath153 & = a_1 e\\bigl[z_{t-1 } z_{t-\\tau } ' \\bigr ] + a_2 e\\bigl[z_{t-2 } z_{t-\\tau } ' \\bigr ] \\\\ & \\quad + \\cdots+ a_p e\\bigl[z_{t - p } z_{t-\\tau } ' \\bigr ] + e\\bigl[u_t z_{t-\\tau}'\\bigr ] \\quad \\end{split } \\ ] ] and , for @xmath154 , @xmath155 since the autocovariance matrix entries link a variable with both its delays and the remaining model variables , we have that if the autocovariance between @xmath156 and @xmath0 is positive , then @xmath156 tends to move accordingly with @xmath0 and vice  versa , whereas if @xmath156 and @xmath0 are independent , their autocovariance obviously equals zero .",
    "an appropriate method for the lag length selection of var is fundamental to determine properties of var and related estimates .",
    "there are two main approaches used for selecting or testing lag length in var models .",
    "the first consists of rules of thumb based on the periodicity of the data and past experience , and the second is based on formal information criteria .",
    "var models typically include enough lags to capture the full cycle of the data ; for monthly data , this means that there is a minimum of 12 lags , but we will also expect that there is some seasonality that is carried over from year to year , so often lag lengths of 1315 months are used ( see , e.g. , ( * ? ? ?",
    "* chap.2.5 ) ) . for quarterly data ,",
    "it is standard to use six lags .",
    "this captures the cyclical components in the year and any residual seasonal components in most cases .",
    "usually , we decide to choose the number of lags not exceeding @xmath157 , where @xmath130 is the number of endogenous variables , @xmath54 is the lag length , and @xmath158 is the total number of observations .",
    "we use this limitation because the estimate of all these coefficients increases the amount of forecast estimation errors , which can result in a  deterioration of the accuracy of the forecast itself . the lag length in var",
    "can be formally determined using information criteria ; let @xmath159 be the estimate of the covariance matrix with the @xmath160 element @xmath161 , where @xmath162 is the ols residual from the @xmath7th equation .",
    "the bic for the @xmath130th equation in a var model is @xmath163 + k(kp + 1 ) \\frac{\\ln t}{t},\\ ] ] whereas the aic is computed using eq .  , modified by replacing the term @xmath164 by  @xmath83 . among a set of candidate values of @xmath54 , the estimated lag length",
    "@xmath84 is the value of @xmath54 that minimizes bic(@xmath54 ) .",
    "iterated multivariate forecasts are computed using a var in much the same way as univariate forecasts are computed using an autoregression .",
    "the main new feature of a multivariate forecast is that the forecast of one variable depends on the forecast of all variables in the var . to compute multiperiod var forecasts @xmath165 periods ahead",
    ", it is necessary to compute forecast of all variables for all intervening periods between @xmath158 and @xmath166 .",
    "then the following scheme applies : compute the one - period - ahead forecast of all the variables in the var , then use those forecasts to compute the two - period - ahead forecasts , and repeat the previous stops until the desired forecast horizon . for example , the two - period - ahead forecast of @xmath167 based on the two - variable var(@xmath54 ) in eq .",
    "is @xmath168 where the coefficients in are the ols estimates of the var coefficients .",
    "an important question in multiple time series is to assign the value of individual variables to explain the remaining ones in the considered system of equations .",
    "an example is the value of a variable @xmath2 for predicting another variable @xmath132 in a dynamic system of equations or understanding if the variable  @xmath2 is informative about future values of @xmath132 .",
    "the answer is based on the determination of the so - called granger causality parameter for a time - series model ( for details , see , e.g. , ( * ? ? ? * chap .",
    "2.5.4 ) ) . to define the concept precisely , consider the bivariate var model for two variables @xmath169 as in eq .",
    "using this system of equations , granger causality states that , for linear models , @xmath132 granger causes @xmath2 if the behavior of past @xmath2 can better predict the behavior of  @xmath132 than the past @xmath132 alone .",
    "for the model in system , if @xmath132 granger causes @xmath2 , then the coefficients for the past values of @xmath132 in the @xmath2 equation are nonzero , that  is , @xmath170 for @xmath171 .",
    "similarly , if @xmath2 granger causes @xmath132 in the @xmath132 equation , then the coefficients for the past values of @xmath2 are nonzero , that is , @xmath172 for @xmath173 . the formal testing for granger causality is then done by using an f test for the joint hypothesis that the possible causal variable does not cause the other variable .",
    "we can specify the null hypothesis for the granger causality test as follows .",
    "@xmath174 & h_1{:}\\ \\boldsymbol{\\mathrm{granger \\ causality } } \\",
    "x_t \\mbox { does    predict } y_t \\mbox { if } \\\\ & \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\gamma_{11 } \\neq0 , \\gamma _ { 12 } \\neq0 , \\ldots , \\mbox { or }   \\gamma_{1p } \\neq0 , \\end{split } \\ ] ] whereas the f test implementation is based on two models .",
    "model 1 : :    ( unrestricted )    +    @xmath175 model 2 : :    ( restricted )    +    @xmath176    in the first model , we have @xmath177 , so the variable @xmath132 compares in the equation of @xmath2 , namely the values of @xmath132 are useful to predict  @xmath2 . instead , in the second model , @xmath178 , so @xmath132 does not granger cause @xmath2 .",
    "the test statistic has an @xmath179 distribution with@xmath180 degrees of freedom : @xmath181",
    "if this @xmath179 statistic is greater than the critical value for a chosen level of significance , we reject the null hypothesis that @xmath132 has no effect on @xmath2 and conclude that @xmath132 granger causes @xmath2 .      in section [ trends",
    "] , we introduced the model of random walk with drift as follows : @xmath182 if @xmath2 follows eq .",
    ", then it has an autoregressive root that equals 1 .",
    "if we consider a random walk for the first difference of the trend , then we obtain @xmath183 hence , if @xmath2 follows eq .",
    ", then @xmath184 follows a random walk , and accordingly is stationary ; this is the second difference of @xmath2 and is denoted @xmath185 .",
    "a series that has a random walk trend is said to be integrated of order one , or i(1 ) ; a  series that has a trend of the form is said to be integrated of order two , or i(2 ) ; and a series that has no stochastic trend and is stationary is said to be integrated of order zero , or i(0 ) .",
    "the order of integration in the i(1 ) and i(2 ) terminology is the number of times that the series needs to be differenced for it to be stationary .",
    "if @xmath2 is i(2 ) , then @xmath184 is i(1 ) , so @xmath186 has an autoregressive root that equals 1 . if , however , @xmath2 is i(1 )",
    ", then @xmath184 is stationary .",
    "thus , the null hypothesis that @xmath2 is i(2 ) can be tested against the alternative hypothesis that @xmath2 is i(1 ) by testing whether @xmath184 has a unit autoregressive root .",
    "sometimes , two or more series have the same stochastic trend in common . in this special case ,",
    "referred to as cointegration , regression analysis can reveal long - run relationships among time series variables .",
    "one could think that a  linear combination of two processes i(1 ) is a process i(1 ) .",
    "however , this is not always true .",
    "two or more series that have a common stochastic trend are said to be _",
    "suppose that @xmath132 and @xmath2 are integrated of order one .",
    "if , for some coefficient @xmath187 , @xmath188 is integrated of order zero , then @xmath132 and @xmath2 are said to be _ cointegrated _ , and the coefficient @xmath187 is called the _ cointegrating coefficient_. if @xmath132 and @xmath2 are cointegrated , then they have a common stochastic trend that can be eliminated by computing the difference @xmath189 , which eliminates this common stochastic trend .",
    "there are three ways to decide whether two variables can be plausibly modeled exploiting the cointegration approach , namely , by expert knowledge and economic theory , by a qualitative ( graphical ) analysis of the series checking for common stochastic trend , and by performing statistical tests for cointegration .",
    "in particular , there is a cointegration test when @xmath187 is unknown .",
    "initially , the cointegrating coefficient @xmath187 is estimated by ols estimation of the regression=1 @xmath190 and then we use the dickey  fuller test ( see section [ trends ] ) to test for a unit root in @xmath191 ; this procedure is called the engle ",
    "granger augmented dickey  fuller test for cointegration ( eg - adf test ) ; for details ,",
    "see , for example , ( * ? ? ?",
    "the concepts covered so far can be extended to the case of more than two variables , for example , three variables , each of which is i(1 ) , are said to be cointegrated if @xmath192 is stationary .",
    "the dickey ",
    "fuller needs the use of different critical values ( see table  [ eg ] ) , where the appropriate line depends on the number of regressors used in the first step of estimating the ols cointegrating regression .",
    ".critical values for the eg - adf statistic [ cols=\"^,^,^,^\",options=\"header \" , ]     a different estimator of the cointegrating coefficient is the dynamic ols ( dols ) estimator , which is based on the equation @xmath193 in particular , from eq .",
    "we notice that dols includes past , present , and future values of the changes in @xmath132 .",
    "the dols estimator of @xmath187 is the ols estimator of @xmath187 in eq .  .",
    "the dols estimator is efficient , and statistical inferences about @xmath187 and @xmath141s in eq .",
    "are valid .",
    "if we have cointegration in more than two variables , for example , three variable @xmath194 , each of which is i(1 ) , then they are cointegrated with cointegrating coefficients @xmath195 and @xmath196 if @xmath197 is stationary .",
    "the eg - adf procedure to test for a single cointegrating relationship among multiple variables is the same as for the case of two variables , except that the regression in eq .",
    "is modified so that both @xmath198 and @xmath199 are regressors .",
    "the dols estimator of a single cointegrating relationship among multiple @xmath156s involves the level of each @xmath156 along with lags of the first difference of each @xmath156 .",
    "in this first part of our ambitious project to use multivariate statistical techniques to study critic econometric data of one of the most influential economy in italy , namely the verona import  export time series , we have focused ourselves on a self - contained introduction to techniques of estimating ols - type regressions , analysis of the correlations obtained between the different variables and various types of _ information criteria _ to check for the goodness of fit .",
    "a particular relevance has been devoted to the application of tests able to enlightening various types of nonstationarity for the considered time series , for example , the _ augmented dickey  fuller test _",
    "( adf ) and the _ quandt likelihood ratio statistic _ ( qlr ) .",
    "moreover , we have also exploited both the _ granger causality _ test and the _ engle  granger augmented dickey  fuller _",
    "test for cointegration ( eg - adf ) in order to analyze if and how these variables are related to each other and to have a measure on how much a variable gives information on the other one .",
    "such approaches constitute the core of the second part of our project , namely the aforementioned verona case study .",
    "the author would like to acknowledge the excellent support that dr .",
    "chiara segala gave him .",
    "her help has been fundamental to develop the whole project , particularly , for the realization of the applied sections , which constitute the core of the whole work ."
  ],
  "abstract_text": [
    "<S> this work is the first part of a project dealing with an in - depth study of effective techniques used in econometrics in order to make accurate forecasts in the concrete framework of one of the major economies of the most productive italian area , namely the province of verona . in particular , we develop an approach mainly based on vector autoregressions , where lagged values of two or more variables are considered , granger causality , and the stochastic trend approach useful to work with the cointegration phenomenon . </S>",
    "<S> latter techniques constitute the core of the present paper , whereas in the second part of the project , we present how these approaches can be applied to economic data at our disposal in order to obtain concrete analysis of import  export behavior for the considered productive area of verona .    </S>",
    "<S> ./style / arxiv - vmsta.cfg    econometrics time series , autoregressive models , granger causality , cointegration , stochastic nonstationarity , aic and bic criteria , trends and breaks    introduction the analysis of time series data constitutes a key ingredient in econometric studies . </S>",
    "<S> last years have been characterized by an increasing interest toward the study of econometric time series . </S>",
    "<S> although various types of regression analysis and related forecast methods are rather old , the worldwide financial crisis experienced by markets starting from last months of 2007 , and which is not yet finished , has put more attention on the subject . moreover , analysis and forecast problems have become of great momentum even for medium and small enterprizes since their economic sustainability is strictly related to the propensity of a bank to give credits at reasonable conditions .    </S>",
    "<S> in particular , great efforts have been made to read economic data not as monads , but rather as constituting pieces of a whole . </S>",
    "<S> namely , new techniques have been developed to study interconnections and dependencies between different factors characterizing the economic history of a certain market , a given firm , a specified industrial area , and so on . from this point of view , methods such as the vector autoregression , the cointegration approach , and the copula techniques have been benefitted by new research impulses .    </S>",
    "<S> a challenging problem is then to apply such instruments in concrete situations and the problem becomes even harder if we take into account the economies are hardly hit by the aforementioned crisis . </S>",
    "<S> a particularly important case study is constituted by a close analysis of import  export time series . </S>",
    "<S> in fact , such an information , spanning from countries to small firms , has the characteristic to provide highly interesting hints for people , for example , politicians or ceos , to depict future economic scenarios and related investment plans for the markets in which they are involved .    exploiting precious economic data that the commerce chamber of veronaprovince has put at our disposal , we successfully applied some of the relevant approaches already cited to find dependencies between economic factor characterizing the province economy and then to make effective forecasts , very close to the real behavior of studied markets .    for completeness </S>",
    "<S> , we have split our project into two parts , namely the present one , which aims at giving a self - contained introduction to the statistical techniques of interest , and the second one , where the verona import  export case study have been treated in detail .    in what follows , </S>",
    "<S> we first recall univariate time series models , paying particular attention to the ar model , which relates a time series to its past values . </S>",
    "<S> we will explain how to make predictions , by using these models , how to choose the delays , for example , using the akaike and bayesian information crtiteria ( aic , resp . </S>",
    "<S> bic ) , and how to behave in the presence of trends or structural breaks </S>",
    "<S> . then we move to the vector autoregression ( var ) model , in which lagged values of two or more variables are used to forecast future values of these variables . </S>",
    "<S> moreover , we present the granger causality , and , in the last part , we return to the topic of stochastic trend introducing the phenomenon of cointegration . </S>"
  ]
}