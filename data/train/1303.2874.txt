{
  "article_text": [
    "generalized linear mixed models ( glmms ) have become a popular and very useful class of statistical models .",
    "see , for example , @xcite , mcculloch , searle and neuhaus ( @xcite ) for some wide - ranging accounts of glmms with theory and applications . in the earlier years after glmm was introduced , one of the biggest challenges in inference about these models was computation of the maximum likelihood estimators ( mles ) .",
    "as is well known , the likelihood function under a glmm typically involves integrals that can not be computed analytically .",
    "the computational difficulty was highlighted by the infamous salamander mating data , first introduced by mccullagh and nelder [ ( @xcite ) , section 14.5 ] .",
    "a mixed logistic model , which is a special case of glmm , was proposed for the salamander data that involved crossed random effects for the female and male animals . however , due to the fact that the random effects are crossed , the likelihood function involves a high - dimensional integral that not only does not have an analytic expression , but is also difficult to evaluate numerically [ e.g. , @xcite , section 4.4.3 ] . for years , the salamander data has been a driving force for the computational developments in glmm .",
    "virtually every numerical procedure that was proposed used this data as a `` gold standard '' to evaluate , or demonstrate , the procedure .",
    "see , for example , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , and @xcite .      to illustrate the numerical difficulty as well as a theoretical challenge , which is the main objective of the current paper ,",
    "let us begin with an example .",
    "[ ex1 ] a mixed logistic model was proposed by @xcite for the salamander data , and has since been used [ e.g. , @xcite ] .",
    "some alternative models , but only in terms of reparametrizations , have been considered [ e.g. , @xcite ] .",
    "@xcite noted that some of these models have ignored the fact that a group of salamanders were used in both the summer experiment and one of the fall experiments ; in other words , there were replicates for some of the pairs of female and male animals .",
    "nevertheless , all of these models are special cases of the following , more general setting .",
    "suppose that , given the random effects @xmath0 , where @xmath1 is a subset of @xmath2 , binary responses @xmath3 , @xmath4 , @xmath5 are conditionally independent such that , with @xmath6 , we have @xmath7 , where @xmath8 , @xmath9 is an known vector of covariates , @xmath10 is a unknown vector of parameters , and @xmath11 denote all the random effects @xmath12 and @xmath13 that are involved . here @xmath14 is the number of replicates for the @xmath15 cell . without loss of generality , assume that @xmath1 is a irreducible subset of @xmath16 in that @xmath17 are the smallest positive integers such that @xmath18 .",
    "furthermore , suppose that the random effects @xmath12 s and @xmath13 s are independent with @xmath19 and @xmath20 , where @xmath21 are unknown variances .",
    "one may think of the random effects @xmath12 and @xmath13 as corresponding to the female and male animals , as in the salamander problem .",
    "in fact , for the salamander data , @xmath22 for half of the pairs @xmath15 , and @xmath23 for the rest of the pairs . it can be shown [ e.g. , @xcite , page  126 ; also see section  [ sec4 ] in the sequel ] that the log - likelihood function for estimating @xmath24 involves an integral of dimension @xmath25 , which , in particular , increases with the sample size , and the integral can not be further simplified .",
    "the fact that the random effects are crossed , as in example  [ ex1 ] , presents not only a computational challenge but also a theoretical one , that is , to prove that the mle is consistent in such a model . in contrast , the situation is very different if the glmm has clustered , rather than crossed , random effects .",
    "for example , consider the following .",
    "[ ex2 ] suppose that , given the random effects @xmath26 , binary responses @xmath27 are conditionally independent such that , with @xmath28 , we have @xmath29 , where @xmath30 is a vector of known covariates , @xmath10 a vector of unknown coefficients , and @xmath31 .",
    "furthermore , suppose that the @xmath12 s are independent with @xmath32 , where @xmath33 is unknown .",
    "it is easy to show that the log - likelihood function for estimating @xmath34 only involves one - dimensional integrals .",
    "not only that , a major theoretical advantage of this case is that the log - likelihood can be expressed as a sum of independent random variables .",
    "in fact , this is a main characteristic of glmms with clustered random effects .",
    "therefore , limit theorems for sums of independent random variables [ e.g. , @xcite , chapter 6 ] can be utilized to obtain asymptotic properties of the mle .    generally speaking , the classical approach to proving consistency of the mle [ e.g. , @xcite , chapter 6 ; @xcite ] relies on asymptotic theory for sum of random variables , independent or not .",
    "however , one can not express the log - likelihood in example  [ ex1 ] as a sum of random variables with manageable properties .",
    "for this reason , it is very difficult to tackle asymptotic behavior of the mle in the salamander problem , or any glmm with crossed random effects , assuming that the numbers of random effects in all of the crossed factors increase .",
    "in fact , the problem is difficult to solve even for the simplest case , as stated in the open problem below .    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ open problem [ _ e.g. , jiang _ ( @xcite ) , _ page 541 _ ] : _ suppose that @xmath35 , an unknown parameter , @xmath23 for all @xmath36 , @xmath37 , and @xmath38 are known , say , @xmath39 in example  [ ex1 ] .",
    "thus , @xmath40 is the only unknown parameter .",
    "suppose that @xmath41 .",
    "is the mle of @xmath40 consistent ? _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    it was claimed [ @xcite , pages  541 , 550 ] that even for this seemingly trivial case , the answer was not known but expected to be anything but trivial .",
    "the problem regarding consistency of the mle in glmms with crossed random effects began to draw attention in early  1997 .",
    "it remained unsolved over the past 15 years , and was twice cited as an open problem in the literature , first in jiang [ ( @xcite ) , page  173 ] and later in jiang [ ( @xcite ) , page  541 ] .",
    "the latter also provided the following supporting evidence for a positive answer [ @xcite , page  550 ] .",
    "let @xmath42 .",
    "consider a subset of the data , @xmath43 .",
    "note that the subset is a sequence of i.i.d .",
    "random variables .",
    "it follows , by the standard arguments , that the mle of @xmath40 based on the subset , denoted by @xmath44 , is consistent .",
    "let @xmath45 denote the mle of @xmath40 based on the full data , @xmath46 .",
    "the point is that even the mle based on a subset of the data , @xmath44 , is consistent ; and if one has more data ( information ) , one is expected to do better .",
    "therefore , @xmath45 has to be consistent as well .      in section  [ sec2 ] ,",
    "we give a positive answer to the open problem as well as the proof .",
    "surprisingly , the proof is fairly short , thanks to a new , nonstandard technique that we introduce , known as the _ subset argument_. using this argument , we are able to establish both cramr ( @xcite ) and @xcite types of consistency results for the mle .",
    "it is fascinating that a 15-year - old problem can be solved in such a simple way",
    ". the new technique may be useful well beyond solving the open problem  for proving consistency of the mle in cases of dependent observations .",
    "we consider some applications of the subset argument in section  [ sec3 ] regarding consistency of the mle in a general glmm .",
    "an example is used in section  [ sec4 ] to further illustrate the new technique .",
    "remark and discussion on a number of theoretical and practical issues are offered in section  [ sec5 ] .",
    "throughout this section , we focus on the open problem stated in section [ sec1 ] . let @xmath40 denote the true parameter .",
    "[ th1 ] there is , with probability tending to one , a root to the likelihood equation , @xmath45 , such that @xmath47 .",
    "the idea was actually hinted in jiang [ ( @xcite ) , page  550 ] as `` evidence '' that supports a positive answer ( see the last paragraph of section  [ sec1.2 ] of the current paper ) .",
    "basically , the idea suggests that , perhaps , one could use the fact that the mle based on the subset data is consistent to argue that the mle based on the full data is also consistent .",
    "the question is how to execute the idea .",
    "recall that , in the original proof of wald [ ( @xcite ) ; also see @xcite ] , the focus was on the likelihood ratio @xmath48 , and showing that the ratio converges to zero outside any ( small ) neighborhood of @xmath49 , the true parameter vector .",
    "can we execute the subset idea in terms of the likelihood ratio ? this leads to consideration of the relationship between the likelihood ratio under the full data and that under the subset data .",
    "it is in this context that the following _ subset inequality _ ( [ eq2 ] ) is derived ( see section  [ sec5.1 ] for further discussion ) , which is the key to the proof .",
    "let @xmath50}$ ] denote the ( row ) vector of @xmath51 , and @xmath52}$ ] the ( row ) vector of the rest of the @xmath53 .",
    "let @xmath54},y_{[2]})$ ] denote the probability mass function ( p.m.f . ) of @xmath55},y_{[2]})$ ] , @xmath54})$ ] the p.m.f . of @xmath50}$ ] , @xmath56}|y_{[1]})=\\frac{p_{\\mu}(y_{[1 ] } , y_{[2]})}{p_{\\mu}(y_{[1]})}\\ ] ] the conditional p.m.f . of @xmath52}$",
    "] given @xmath50}$ ] , and @xmath57 the probability distribution , respectively , when @xmath40 is the true parameter . for any @xmath58 , we have @xmath59},y_{[2 ] } ) \\leq p_{\\mu+\\varepsilon}(y_{[1 ] } , y_{[2]})|y_{[1]}\\bigr \\}&=&\\mathrm { p}_{\\mu } \\biggl\\{\\frac{p_{\\mu + \\varepsilon}(y_{[1]},y_{[2]})}{p_{\\mu}(y_{[1]},y_{[2]})}\\geq1\\big\\vert y_{[1 ] } \\biggr\\ } \\nonumber \\\\ & \\leq&\\mathrm { e } \\biggl\\{\\frac{p_{\\mu+\\varepsilon}(y_{[1 ] } , y_{[2]})}{p_{\\mu}(y_{[1]},y_{[2]})}\\big\\vert y_{[1 ] } \\biggr\\ } \\nonumber\\\\ & = & \\sum_{y_{[2]}}\\frac{p_{\\mu+\\varepsilon}(y_{[1]},y_{[2]})}{p_{\\mu}(y_{[1 ] } , y_{[2]})}p_{\\mu}(y_{[2]}|y_{[1 ] } ) \\\\ & = & \\sum_{y_{[2]}}\\frac{p_{\\mu+\\varepsilon}(y_{[1]},y_{[2]})}{p_{\\mu}(y_{[1 ] } ) } \\nonumber \\\\ & = & \\frac{p_{\\mu+\\varepsilon}(y_{[1]})}{p_{\\mu}(y_{[1]})},\\nonumber\\end{aligned}\\ ] ] using ( [ eq1 ] ) .",
    "a more general form of ( [ eq2 ] ) is given in section  [ sec5.1 ] .",
    "on the other hand , by the standard asymptotic arguments [ e.g. , @xcite , page  9 ] , it can be shown that the likelihood ratio @xmath60})/p_{\\mu}(y_{[1]})$ ] converges to zero in probability , as @xmath61 .",
    "here we use the fact that the components of @xmath50}$ ] , @xmath62 are independent bernoulli random variables .",
    "it follows that , for any @xmath63 , there is @xmath64 such that , with probability @xmath65 , we have @xmath66},y_{[2]})\\leq p_{\\mu+\\varepsilon}(y_{[1 ] } , y_{[2]})|y_{[1]}\\}\\leq\\gamma^{m\\wedge n}$ ] for some @xmath67 , if @xmath68 .",
    "the argument shows that @xmath69 , hence converges to @xmath70 in probability .",
    "it follows , by the dominated convergence theorem , that @xmath71},y_{[2]})\\leq p_{\\mu + \\varepsilon}(y_{[1]},y_{[2]})\\}\\rightarrow0 $ ] .",
    "similarly , we have @xmath72},y_{[2]})\\leq p_{\\mu-\\varepsilon}(y_{[1 ] } , y_{[2]})\\}\\rightarrow0 $ ] . the rest of the proof follows by the standard arguments [ e.g. , @xcite , pages  910 ] .",
    "the result of theorem  [ th1 ] is usually referred to as cramr - type consistency [ cramr ( @xcite ) ] , which states that a root to the likelihood equation is consistent .",
    "however , it does not always imply that the mle , which by definition is the ( global ) maximizer of the likelihood function , is consistent .",
    "a stronger result is called wald - type consistency [ @xcite ; also see @xcite ] , which states that the mle is consistent .",
    "note that the limiting process in theorem  [ th1 ] is @xmath41 , or , equivalently , @xmath61 ( see section  [ sec5.4 ] for discussion ) . with a slightly more restrictive limiting process , the wald - consistency can actually be established , as follows .",
    "[ th2 ] if @xmath73 as @xmath74 , then the mle of @xmath40 is consistent .",
    "define @xmath75 , where @xmath76 and @xmath77 .",
    "write @xmath78 . for any integer @xmath79 ,",
    "divide the interval @xmath80 by @xmath81 , @xmath82 , where @xmath83 $ ] and @xmath84 .",
    "it is easy to show that @xmath85},y_{[2]})|\\leq mn$ ] uniformly for all @xmath40 .",
    "thus , for any @xmath86 , there is @xmath87 , such that @xmath88},y_{[2]})-\\log p_{\\lambda_{k , j } } ( y_{[1]},y_{[2]})=\\{(\\partial/\\partial\\mu)\\log p_{\\mu}(y_{[1]},y_{[2 ] } )    where @xmath89 lies between @xmath90 and @xmath91 .",
    "it follows that @xmath92},y_{[2]})}{p_{\\mu}(y_{[1 ] } , y_{[2]})}\\leq e^{\\delta(m\\wedge n)}\\max_{1\\leq j\\leq j } \\frac{p_{\\lambda_{k , j}}(y_{[1]},y_{[2]})}{p_{\\mu}(y_{[1]},y_{[2]})}.\\ ] ] therefore , by the subset argument [ see ( [ eq2 ] ) ] , we have @xmath93},y_{[2]})}{p_{\\mu}(y_{[1]},y_{[2]})}>1\\big\\vert y_{[1 ] } \\biggr\\ } \\nonumber \\\\ & & \\qquad\\leq\\sum_{j=1}^{j}\\mathrm { p}_{\\mu } \\biggl\\{\\frac{p_{\\lambda_{k , j } } ( y_{[1]},y_{[2]})}{p_{\\mu}(y_{[1]},y_{[2]})}>e^{-\\delta(m\\wedge n)}\\big\\vert y_{[1 ] } \\biggr\\ } \\\\ & & \\qquad \\leq e^{\\delta(m\\wedge n)}\\sum_{j=1}^{j } \\frac{p_{\\lambda_{k , j}}(y_{[1]})}{p_{\\mu}(y_{[1]})}.\\nonumber\\end{aligned}\\ ] ]    on the other hand , we have @xmath94 ; and , similarly , @xmath95 .",
    "let @xmath96 with @xmath97 . if @xmath98 , then , for any @xmath99 , write @xmath100 .",
    "we have , on @xmath101 , @xmath102})}{p_{\\mu}(y_{[1]})}&= & \\biggl\\ { \\biggl(\\frac { p_{1}}{p_{0 } } \\biggr)^{p_{0}+\\delta } \\biggl(\\frac{1-p_{1}}{1-p_{0 } } \\biggr)^{1-p_{0}-\\delta } \\biggr\\}^{m\\wedge n } \\\\ & \\leq&\\bigl\\{a_{\\delta}^{-1}(1-p_{1})^{1-p_{0}-\\delta } \\bigr\\}^{m\\wedge n } \\\\ & \\leq&\\bigl[a_{\\delta}^{-1}\\exp\\bigl\\{(1-\\lambda_{k , j } ) ( 1-p_{0}-\\delta)\\bigr\\ } \\bigr]^{m\\wedge n } \\\\ & \\leq&\\exp \\bigl[\\bigl\\{1-p_{0}-\\delta-\\log a_{\\delta}-(1-p_{0}- \\delta)k\\bigr\\ } ( m\\wedge n ) \\bigr],\\end{aligned}\\ ] ] where @xmath103 .",
    "it follows , by ( [ eq3 ] ) , that @xmath104},y_{[2]})}{p_{\\mu}(y_{[1]},y_{[2]})}>1\\big\\vert y_{[1 ] } \\biggr\\ } \\\\ & & \\qquad \\leq\\frac{mn}{\\delta(m\\wedge n)}\\exp\\bigl[\\bigl\\{1-p_{0}-\\log a_{\\delta}-(1-p_{0 } -\\delta)k\\bigr\\}(m\\wedge n)\\bigr]\\end{aligned}\\ ] ] on @xmath101 , or , equivalently , that @xmath105},y_{[2]})}{p_{\\mu}(y_{[1]},y_{[2]})}>1 , |\\delta|\\leq\\delta\\big\\vert y_{[1 ] } \\biggr\\ } \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & & \\qquad \\leq\\frac{mn}{\\delta(m\\wedge n)}\\exp\\bigl[\\bigl\\{1-p_{0}-\\log a_{\\delta}-(1-p_{0 } -\\delta)k\\bigr\\}(m\\wedge n ) \\bigr]1_{{\\cal a}_{\\delta}}.\\end{aligned}\\ ] ] note that @xmath106})$ ] . by taking expectations on both sides of ( [ eq4 ] )",
    ", it follows that the unconditional probability corresponding to the left side is bounded by the right side without @xmath107 , for @xmath108 therefore , we have @xmath109 } , y_{[2]})}{p_{\\mu}(y_{[1]},y_{[2]})}>1",
    "\\mbox { for some } k\\geq k , |\\delta|\\leq\\delta \\biggr\\ } \\nonumber \\\\ & & \\qquad\\leq\\sum_{k = k}^{\\infty}\\mathrm { p}_{\\mu } \\biggl\\{\\sup_{\\lambda\\in [ k , k+1)}\\frac { p_{\\lambda}(y_{[1]},y_{[2]})}{p_{\\mu}(y_{[1]},y_{[2]})}>1,|\\delta|\\leq \\delta \\biggr\\ } \\nonumber \\\\ & & \\qquad\\leq\\frac{mn}{\\delta(m\\wedge n)}\\exp\\bigl\\{(1-p_{0}-\\log a_{\\delta } ) ( m \\wedge",
    "n)\\bigr\\}\\sum_{k = k}^{\\infty}e^{-(1-p_{0}-\\delta)(m\\wedge n)k } \\nonumber\\\\ & & \\qquad=\\frac{mn}{\\delta(m\\wedge n)}\\exp\\bigl\\{(1-p_{0}-\\log a_{\\delta } ) ( m \\wedge n)\\bigr\\}\\frac{e^{-(1-p_{0}-\\delta)(m\\wedge n)k}}{1-e^{-(1-p_{0}-\\delta ) ( m\\wedge n)}}\\\\ & & \\qquad= \\bigl\\{1-e^{-(1-p_{0}-\\delta)(m\\wedge n ) } \\bigr\\}^{-1}\\nonumber\\\\ & & \\qquad\\quad{}\\times\\exp\\bigl [ -(m\\wedge n)\\bigl \\{(1-p_{0}-\\delta)k-1+p_{0}+\\log a_{\\delta}\\nonumber\\\\ & & \\hspace*{80pt}\\qquad\\quad{}-(m\\wedge n)^{-1}\\log(m\\vee n)+(m\\wedge n)^{-1}\\log\\delta\\bigr\\}\\bigr].\\nonumber\\end{aligned}\\ ] ] thus , if we choose @xmath110 such that @xmath111 , then , for large @xmath112 , the probability on the left side of ( [ eq5 ] ) is bounded by @xmath113 . on the other hand , we have @xmath114 , as @xmath61 .",
    "thus , we have @xmath115},y_{[2]})}{p_{\\mu}(y_{[1 ] } , y_{[2]})}>1 \\mbox { for some } \\lambda \\geq k \\biggr\\ } \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & & \\qquad\\leq 2e^{-(m\\wedge n)/2}+\\mathrm { p}\\bigl({\\cal a}_{\\delta } ^{c } \\bigr)\\longrightarrow0\\end{aligned}\\ ] ] as @xmath61 .",
    "similarly , the left side of ( [ eq6 ] ) , with the words `` @xmath116 '' replaced by `` @xmath117 , '' goes to zero , as @xmath61 , if @xmath110 is chosen sufficiently large .    on the other hand ,",
    "again by the subset argument , it can be shown ( see the supplementary material [ @xcite ] ) that for any @xmath58 and @xmath118 } \\frac{p_{\\lambda}(y_{[1]},y_{[2]})}{p_{\\mu}(y_{[1]},y_{[2]})}>1 \\biggr\\ } & \\longrightarrow&0\\end{aligned}\\ ] ] as @xmath41 .",
    "the consistency of the mle then follows by combining ( [ eq7 ] ) with the previously proved results .",
    "we consider a few more applications of the subset argument , introduced in the previous section .",
    "all applications are regarding a general glmm , whose definition is given below for the sake of completeness [ see , e.g. , @xcite for further details ] .",
    "\\(i ) suppose that , given a vector @xmath119 of random effects , responses @xmath120 are conditionally independent with conditional density function , with respect to a @xmath121-finite measure @xmath122 , given by the exponential family @xmath123 $ ] , where @xmath124 is a dispersion parameter ( which in some cases is known ) , and @xmath125 are known , continuously differentiable functions with respect to @xmath126 and @xmath124 .",
    "the natural parameter of the conditional exponential family , @xmath126 , is therefore associated with the conditional mean , @xmath127 , according to the properties of the exponential family [ e.g. , @xcite , section 2.2.2 ] .",
    "( ii ) furthermore , suppose that @xmath128 satisfies @xmath129 , where @xmath130 are known vectors , @xmath10 is a vector of unknown parameters , and @xmath131 is a link function .",
    "( iii ) finally , assume that @xmath132 , where the covariance matrix @xmath133 may depend on a vector @xmath134 of dispersion parameters .",
    "it is typically possible to find a subset of the data that are independent , in some way , under a general glmm .",
    "for example , under the so - called anova glmm [ e.g. , @xcite ] , a subset of independent data can always be found . here",
    "an anova glmm satisfies @xmath135 , where @xmath136 , @xmath137_{1\\leq i\\leq n}$ ] , @xmath138 , @xmath139 , are known matrices , @xmath140 are vectors of independent random effects , and @xmath141 are independent .",
    "examples  [ ex1 ] and  [ ex2 ] are special cases of the anova glmm . note that in both examples the responses are indexed by @xmath15 , instead of @xmath142 , but this difference is trivial .",
    "nevertheless , the `` trick '' is to select a subset , or more than one subsets if necessary , with the following desirable properties : ( i ) the subset(s ) can be divided into independent clusters with the number(s ) of clusters increasing with the sample size ; and ( ii ) the combination of the subset(s ) jointly identify all the unknown parameters .",
    "more specifically , let @xmath143 be the @xmath144th subset of the data , @xmath145 , where @xmath146 is a fixed positive integer .",
    "suppose that , for each @xmath144 , there is a partition , @xmath147 .",
    "let @xmath148_{i\\in s_{a , j}}$ ] , and @xmath149 be the probability density function ( p.d.f . ) of @xmath150 , with respect to the measure @xmath122 ( or the product measure induced by @xmath122 if @xmath150 is multivariate ) , when @xmath151 is the true parameter vector .",
    "let @xmath152 denote the parameter space , and @xmath49 the true parameter vector .",
    "then , ( i ) and ( ii ) can be formally stated as follows :    @xmath153 are independent with @xmath154 as @xmath155 ;    for every @xmath156 , we have @xmath157 < 0.\\ ] ]    note that ( a2 ) controls the average kullback  leibler information [ @xcite ] ; thus , the inequality always holds if @xmath158 is replaced by @xmath159 .",
    "let us first consider a simpler case by assuming that @xmath152 is finite . although the assumption may seem restrictive , it is not totally unrealistic .",
    "for example , any computer system only allows a finite number of digits .",
    "this means that the parameter space that is practically stored in a computer system is finite . using the subset argument",
    ", it is fairly straightforward to prove the following ( see the supplementary material [ @xcite ] ) .",
    "[ th3 ] under assumptions and , if , in addition ,    for every @xmath156 , we have @xmath160\\longrightarrow 0 , \\qquad 1\\leq a\\leq b,\\ ] ] then @xmath161 , as @xmath162 , where @xmath163 is the mle of @xmath151 .",
    "we now consider the case that @xmath152 is a convex subspace of @xmath164 , the @xmath165-dimensional euclidean space , in the sense that @xmath166 implies @xmath167 for every @xmath168 . in this case , we need to strengthen assumptions ( a2 ) , ( a3 ) to the following :    @xmath169 , the interior of @xmath152 , and there is @xmath170 [ same as in ( b3 ) below ] such that , for every @xmath58 , we have @xmath171<0.\\end{aligned}\\ ] ]    there are positive constant sequences @xmath172 such that @xmath173 with @xmath174 , where @xmath175 is the p.d.f . of @xmath176 given that @xmath177 is the true parameter vector , @xmath178 with @xmath179 ; and ( for the same @xmath180 ) @xmath181 \\longrightarrow0,\\qquad   1\\leq a\\leq b.\\end{aligned}\\ ] ]    [ th4 ] under assumptions and , there is , with probability @xmath182 , a root to the likelihood equation , @xmath163 , such that @xmath183 , as @xmath162 .    aside from the use of the subset argument , the lines of the proof are similar to , for example , the standard arguments of lehmann and casella [ ( @xcite ) , the beginning part of the proof of theorem 5.1 ] , although some details are more similar to @xcite .",
    "we outline the key steps below and refer the details to the supplementary material [ @xcite ] .",
    "once again , the innovative part is the consideration of the conditional probability given the subset data and , most importantly , the subset inequality ( [ eq15 ] ) in the sequel .",
    "for any @xmath58 , assume , without loss of generality , that @xmath184 and @xmath185 .",
    "essentially , all we need to show is that , as @xmath162 , @xmath186 where @xmath187 is the boundary of @xmath188 , which consists of @xmath189 such that @xmath190 for some @xmath191 .",
    "define @xmath192 , \\qquad 1\\leq a\\leq b,\\ ] ] and @xmath193 .",
    "then , @xmath194 , where @xmath195 .",
    "then , we have @xmath196    for a fixed @xmath197 , let @xmath198 be a small , positive number to be determined latter , and @xmath199 + 1 $ ] . for any @xmath200 , where @xmath201 , select a point @xmath202 from the subset",
    "@xmath203 , if the latter is not empty ; otherwise , do not select .",
    "let @xmath204 denote the collection of all such points .",
    "also let @xmath205 denote the left side of  ( [ eq9 ] ) .",
    "it can be shown that @xmath206 \\\\[-8pt ] \\nonumber & & \\qquad\\leq\\mathrm { p}_{\\theta_{0 } } \\biggl\\{\\exp \\biggl(\\frac{2d\\varepsilon b}{k } \\biggr ) > 2 \\biggr\\}+\\mathrm { p}_{\\theta_{0 } } \\bigl\\{p_{\\theta_{0}}(y)\\leq2 \\max_{\\theta \\in d}p_{\\theta}(y ) \\bigr\\}.\\end{aligned}\\ ] ]",
    "we now apply the subset argument .",
    "let @xmath50}$ ] denote the combined vector of @xmath153 , and @xmath52}$ ] the vector of the rest of @xmath120 . then ,",
    "similar to the argument of ( [ eq2 ] ) , we have , for any @xmath207 , @xmath208 } \\bigr\\ } \\leq 2\\frac{p_{\\theta}(y_{[1]})}{p_{\\theta_{0}}(y_{[1]})}.\\ ] ] using this result , it can be shown that @xmath209}\\ } = o_\\mathrm { p}(1)$ ] . from here , ( [ eq12 ] ) can be established .",
    "again , theorem  [ th4 ] is a cramr - consistency result . on the other hand",
    ", wald - consistency can be established under additional assumptions that control the behavior of the likelihood function in a neighborhood of infinity .",
    "for example , the following result may be viewed as an extension of theorem  [ th2 ] .",
    "the proof is given in the supplementary material [ @xcite ] . once again , the subset argument plays a critical role in the proof . for simplicity , we focus on the case of discrete responses , which is typical for glmms .",
    "in addition , we assume the following . for any @xmath210 , define @xmath211 and write , in short , @xmath212 for @xmath108    there are sequences of constants , @xmath213 , and random variables , @xmath214 , where @xmath215 do not depend on @xmath79 , such that @xmath216 and @xmath217    there is a subset of independent data vectors , @xmath218 [ not necessarily among those in ( a1 ) ] so that : ( i ) @xmath219 is bounded , @xmath220 being the p.m.f . of @xmath221 under @xmath151 ; ( ii ) there is a sequence of positive constants , @xmath222 , with @xmath223 , and a subset @xmath224 of possible values of @xmath221 , such that for every @xmath225 and @xmath226 , there is @xmath227 satisfying @xmath228 ; ( iii ) @xmath229 for some constant @xmath230 ; and ( iv ) @xmath231 , and @xmath232 for some @xmath233 and @xmath234 , where @xmath235 .",
    "it is easy to verify that the new assumptions ( c1 ) , ( c2 ) are satisfied in the case of theorem  [ th2 ] for the open problem ( see the supplementary material [ @xcite ] ) .",
    "another example is considered in the next section .",
    "[ th5 ] suppose that holds ; hold for any fixed @xmath236 ( instead of some @xmath236 ) , and with the @xmath237 in ( [ eq11 ] ) replaced by @xmath238 .",
    "in addition , suppose that , hold .",
    "then , the mle of @xmath49 is consistent .",
    "let us consider a special case of example  [ ex1 ] with @xmath35 , but @xmath33 and @xmath239 unknown .",
    "we change the notation slightly , namely , @xmath240 instead of @xmath3 .",
    "suppose that @xmath241 such that @xmath242 , @xmath243 ( as in the case of the salamander data ) .",
    "we use two subsets to jointly identify all the unknown parameters .",
    "the first subset is similar to that used in the proofs of theorems  [ th1 ] and  [ th2 ] , namely , @xmath244 .",
    "let @xmath245 be the total number of such @xmath246 s , and assume that @xmath247 , as @xmath74 .",
    "then , the subset satisfies ( a1 ) .",
    "let @xmath248 .",
    "it can be shown that the sequence @xmath249 is a sequence of i.i.d .",
    "random vectors with the probability distribution , under @xmath151 , given by @xmath250,\\ ] ] where @xmath251 , with @xmath252 , and @xmath253 . by the strict concavity of the logarithm",
    ", we have @xmath254 < 0\\ ] ] unless @xmath255 is a.s .",
    "@xmath256 a constant , which must be one because both @xmath257 and @xmath258 are probability distributions .",
    "it is easy to show that the probability distribution of ( [ eq16 ] ) is completely determined by the function @xmath259_{r=1,2}$ ] , where @xmath260 with @xmath261 , @xmath262 , and @xmath263 . in other words , @xmath264 for all values of @xmath265 if and only if @xmath266 .",
    "@xcite showed that the function @xmath267 is injective [ also see @xcite , page  221 ] .",
    "thus , ( [ eq17 ] ) holds unless @xmath268 and @xmath269 .",
    "it remains to deal with a @xmath151 that satisfies @xmath268 , @xmath270 , but @xmath271 .",
    "for such a @xmath151 , we use the second subset , defined as @xmath272 such that @xmath273 and @xmath274 .",
    "let @xmath275 be the total number of all such @xmath142 s , and assume that @xmath276 as @xmath277 .",
    "it is easy to see that ( a1 ) is , again , satisfied for the new subset .",
    "note that any @xmath151 satisfying @xmath268 and @xmath269 is completely determined by the parameter @xmath278 .",
    "furthermore , the new subset is a sequence of i.i.d .",
    "random vectors with the probability distribution , under such a @xmath151 , given by @xmath279,\\ ] ] where @xmath280 has the bivariate normal distribution with @xmath281 and @xmath282 .",
    "similar to ( [ eq17 ] ) , we have @xmath283 < 0\\ ] ] unless @xmath284 for all values of @xmath285 .",
    "consider ( [ eq18 ] ) with @xmath286 and let @xmath287 denote the probability distribution of @xmath280 with the correlation coefficient @xmath288 . by fubini s theorem , it can be shown that @xmath289 hereafter , we refer the detailed derivations to the supplementary material [ @xcite ] . by slepian s inequality [ e.g. , @xcite , pages  157158 ] , the integrand on the right side of ( [ eq20 ] ) is strictly increasing with @xmath288 , hence so is the integral .",
    "thus , if @xmath290 , at least we have @xmath291 , hence ( [ eq19 ] ) holds .    in summary , for any @xmath292 , we must have either ( [ eq17 ] ) or ( [ eq19 ] ) hold . therefore , by continuity , assumption ( b2 ) holds , provided that true variances , @xmath293 are positive .",
    "note that , in the current case , the expectations involved in ( b2 ) do not depend on either @xmath294 or @xmath295 , the total sample size .    to verify ( b3 ) , it can be shown that @xmath296 .",
    "furthermore , we have @xmath297 in a neighborhood of @xmath49 , @xmath298 .",
    "therefore , ( [ eq9 ] ) holds with @xmath299 .    as for ( [ eq10 ] ) , it is easy to show that the partial derivatives involved are uniformly bounded for @xmath300 .",
    "thus , ( [ eq10 ] ) holds for any @xmath180 such that @xmath301 , @xmath302 .",
    "furthermore , the left side of ( [ eq11 ] ) is bounded by @xmath303 for some constant @xmath304 , @xmath302 ( note that @xmath305 in this case ) .",
    "thus , for example , we may choose @xmath306 , @xmath307 , to ensure that @xmath179 , @xmath302 , and ( [ eq11 ] ) holds .    in conclusion , all the assumptions of theorem  [ th4 ]",
    "hold provided that @xmath308 , @xmath309 , and @xmath310 .",
    "similarly , the conditions of theorem  [ th5 ] can be verified .",
    "essentially , what is new is to check assumptions ( c1 ) and ( c2 ) .",
    "see the supplementary material [ @xcite ] .",
    "in proving a number of results , we have demonstrated the usefulness of the subset argument . in principle , the method allows one to argue consistency of the mle in any situation of dependent data , not necessarily under a glmm , provided that one can identify some suitable subset(s ) of the data whose asymptotic properties are easier to handle , such as collections of independent random vectors .",
    "the connection between the full data and subset data is made by the subset inequality , which , in a more general form , is a consequence of the martingale property of the likelihood - ratio [ e.g. , @xcite , pages  244246 ] : suppose that @xmath311 is a subvector of a random vector @xmath312 .",
    "let @xmath313 and @xmath314 denote the p.d.f.s of @xmath312 and @xmath311 , respectively , with respect to a @xmath121-finite measure @xmath122 , under the parameter vector @xmath151 . for simplicity ,",
    "suppose that @xmath315 are positive a.e .",
    "@xmath122 , and @xmath316 is a positive , measurable function .",
    "then , for any @xmath151 , we have @xmath317 where @xmath318 denotes the probability distribution corresponding to @xmath258 .      on the other hand ,",
    "the subset argument merely provides a method of proof for the consistency of the full - data mle  it by no means suggests the subset - data mle as a replacement for the full - data mle .",
    "in fact , there is an information loss if such a replacement takes place . to quantify the information loss , assume the regularity conditions for exchanging the order of differentiation and integration .",
    "then , the fisher information matrix based on the full data can be expressed as @xmath319-\\mathrm { e}_{\\theta } \\biggl\\ { \\frac { 1}{p_{\\theta}(y ) } \\frac{\\partial^{2}}{\\partial\\theta\\,\\partial\\theta'}p_{\\theta}(y ) \\biggr\\ } \\\\ & = & i_{\\mathrm { f},1}(\\theta)-i_{\\mathrm { f},2}(\\theta).\\end{aligned}\\ ] ] similarly , the information matrix based on the subset data can be expressed as @xmath320 , where @xmath321 is @xmath322 with @xmath323 replaced by @xmath50}$ ] , @xmath324 [ @xmath325})$ ] denotes the p.d.f .",
    "( or p.m.f . ) of @xmath50}$ ] ] . by conditioning on @xmath50}$ ]",
    ", it can be shown that @xmath326 , while @xmath327 .",
    "it follows that @xmath328 for all @xmath151 . here",
    "the inequality means that the difference between the left side and right side is a nonnegative definite matrix .",
    "( [ eq21 ] ) suggests that the information contained in the full data is no less than that contained in the subset data , which , of course , is what one would expect .",
    "furthermore , the information loss is given by @xmath329 } \\biggr\\ } \\biggr],\\ ] ] where @xmath330})$ ] denotes the conditional covariance matrix given @xmath50}$ ] under @xmath151 .",
    "the derivations of ( [ eq21 ] ) and ( [ eq22 ] ) are deferred to the supplementary material [ @xcite ] .",
    "it is seen from ( [ eq22 ] ) that the information loss is determined by how much ( additional ) variation there is in the score function , @xmath331 , given the subset data @xmath50}$ ] . in particular ,",
    "if @xmath50}=y$ ] , then the score function is a constant vector given @xmath50}$ ] ( and @xmath151 ) ; hence @xmath332}\\}=0 $ ] , thus , there is no information loss . in general",
    ", of course , the subset data @xmath50}$ ] is not chosen as @xmath323 ; therefore , there will be some loss of information .",
    "nevertheless , the information contained in the subset data is usually sufficient for identifying at least some of the parameters .",
    "note that consistency is a relatively weak asymptotic property in the sense that various estimators , including those based on the subset data and , for example , the method of moments estimator of @xcite , are consistent , even though they may not be asymptotically efficient .",
    "essentially , for the consistency property to hold , one needs that , in spite of the potential information loss , the remaining information that the estimator is able to utilize grows with the sample size .",
    "for example , in the open problem ( sections  [ sec1 ] and  [ sec2 ] ) , the information contained in @xmath333 grows at the rate of @xmath112 , which is sufficient for identifying @xmath40 ; in the example of section  [ sec4 ] , the information contained in @xmath265 grows in the order of @xmath245 , which is sufficient for identifying @xmath40 and @xmath252 , while the information contained in @xmath285 grows at the rate of @xmath275 , which is sufficient for identifying @xmath334 .",
    "the identification of the `` right '' subset in a given problem is usually suggested by the nature of the parametrization . as mentioned",
    "( see the third paragraph of section  [ sec3 ] ) , a subset @xmath50}$ ] of independent data can always be found under the anova glmm ( e.g. , starting with the first observation , @xmath335 , one finds the next observation such that it involves different random effects from those related to @xmath335 , and so on ) .",
    "if the @xmath50}$ ] is such that @xmath336 , where @xmath337 is as in ( [ eq21 ] ) and @xmath338 denotes the smallest eigenvalue , the subset @xmath50}$ ] is sufficient for identifying all the components of @xmath151 ; otherwise , more than one subsets are needed in order to identify all the parameters , as is shown in section  [ sec4 ] .",
    "the subset argument offers a powerful tool for establishing consistency of the mle in glmm with crossed random effects .",
    "note that the idea has not followed the traditional path of attempting to develop a ( computational ) procedure to approximate the mle .",
    "in fact , this might explain why the computational advances over the past two decades [ see , e.g. , @xcite , section 4.1 for an overview ] had not led to a major theoretical breakthrough for the mle in glmm in terms of asymptotic properties .",
    "note that the mle based on the subset data is a consistent estimator of the true parameter , and in that sense it is an approximation to the mle based on the full data ( two consistent estimators of the same parameter approximate each other ) .",
    "however , there is an information loss , as discussed in the previous subsection [ see ( [ eq22 ] ) ] , so one definitely wants to do better computationally .",
    "one computational method that has been developed for computing the mle in glmms , including those with crossed random effects , is monte carlo em algorithm [ e.g. , mccullogh ( @xcite ) , @xcite ] . here",
    ", however , we would like to discuss another , more recent , computational advance , known as _ data cloning _ [ dc ; @xcite , @xcite ] .",
    "the dc uses the bayesian computational approach for frequentist purposes",
    ". let @xmath339 denote the prior density function of @xmath151 .",
    "then , one has the posterior , @xmath340 where @xmath341 is the integral of the numerator with respect to @xmath151 , which does not depend on @xmath151 .",
    "there are computational tools using the markov chain monte carlo for posterior simulation that generate random variables from the posterior without having to compute the numerator or denominator of ( [ eq23 ] ) [ e.g. , gilks , richardson and spiegelhalter ( @xcite ) ; @xcite ] .",
    "thus , we can assume that one can generate random variables from the posterior .",
    "if the observations @xmath323 were repeated independently from @xmath110 different individuals such that all of these individuals result in exactly the same data , @xmath323 , denoted by @xmath342 , then the posterior based on @xmath343 is given by @xmath344 @xcite , @xcite showed that , as @xmath110 increases , the right side of ( [ eq24 ] ) converges to a multivariate normal distribution whose mean vector is equal to the mle , @xmath163 , and whose covariance matrix is approximately equal to @xmath345 . therefore , for large @xmath110 , one can approximate the mle by the sample mean vector of , say , @xmath346 generated from the posterior distribution ( [ eq24 ] ) . denoted the sample mean by @xmath347 , and call it the dc mle .",
    "furthermore , @xmath348 [ see ( [ eq21 ] ) , ( [ eq22 ] ) ] can be approximated by @xmath110 times the sample covariance matrix of @xmath346 .",
    "@xcite successfully applied the dc method to obtain the mle for the salamander - mating data .",
    "note that the dc mle is an approximate , rather than exact , mle , in the sense that , as @xmath349 , the difference between @xmath347 and the exact mle vanishes .",
    "because we have established consistency of the exact mle , it follows that the dc mle is a consistent estimator as long as the number @xmath110 increase with the sample size .",
    "more precisely , it is shown in the supplementary material [ @xcite ] that , for every @xmath350 , there is @xmath351 such that for every @xmath352 and @xmath353 , there is @xmath354 such that @xmath355 , if @xmath356 , where @xmath49 is the true parameter vector .",
    "note that , as far as consistency is concerned , one does not need that @xmath205 goes to infinity .",
    "this makes sense because , as @xmath349 , the posterior ( [ eq24 ] ) is becoming degenerate [ the asymptotic covariance matrix is @xmath357 ; thus , one does not need a large @xmath205 to `` average out '' the variation in @xmath358 .",
    "thus , from an asymptotic point of view , the result of the current paper provides a justification for the dc method .",
    "more importantly , because @xmath359 are up to one s choice , one can make sure that they are large enough so that there is virtually no information loss , as was concerned earlier . in this regard",
    ", a reasonably large @xmath205 would reduce the sampling variation and therefore improve the dc approximation , and make the computation more efficient .",
    "see @xcite for discussion on how to choose @xmath205 and @xmath110 from practical points of view .    as for the prior @xmath339 , @xcite",
    "only suggests that it be chosen according to computational convenience and be proper ( to avoid improper posterior ) . following the subset idea , an obvious choice for the prior would be the multivariate normal distribution with mean vector @xmath360 , the subset - data mle , and covariance matrix @xmath361 [ defined above ( [ eq21 ] ) ] .",
    "note that @xmath337 is much easier to evaluate than @xmath362 .",
    "this would make the procedure more similar to the empirical bayes than the hierarchical one .",
    "nevertheless , the dc only uses the bayesian computational tool , as mentioned .      in some applications of glmm ,",
    "the estimation of the random effects are of interest .",
    "there have also been developments in semiparametric glm and nonparametric anova . in those cases ,",
    "the random effects are treated the same way as the fixed effects . as a result ,",
    "the proof of the consistency results in those cases usually impose constraints on the ratio of the number of effects and number of observations falling in each cluster [ e.g. , @xcite , and wang , tsai and qu ( @xcite ) ] . a major difference exists , however , between the case of clustered data ( e.g. , example  [ ex2 ] ) and that with crossed random effects ( e.g. , example  [ ex1 ] ) in that , in the latter case , the data can not be divided into independent groups ( with the number of groups increasing with the sample size ) . furthermore , the necessary constraints are very different depending on the interest of estimation .",
    "consider , for example , a very simple case of linear mixed model , @xmath363 , @xmath364 , where the @xmath12 s and @xmath13 s are random effects , and @xmath365 s are errors .",
    "assume , for simplicity , that all the random effects and errors are i.i.d .",
    "@xmath366 , so that @xmath40 is the only unknown parameter .",
    "suppose that @xmath367 , while @xmath368 is fixed , say , @xmath369 . in this case , @xmath370 is a consistent estimator of the cluster mean , @xmath371 .",
    "on the other hand , the mle of @xmath40 , which is also @xmath372 , is inconsistent ( because it converges in probability to @xmath373 , which is not equal to @xmath40 with probability one ) .",
    "note that here the ratio of the number of effects and number of observations in the cluster is @xmath374 .",
    "apparently , this is sufficient for consistently estimating the mixed effect @xmath373 , but not the fixed effect @xmath40 .",
    "one might suspect that the case @xmath369 is somewhat extreme , as @xmath40 and @xmath375 are `` inseparable '' ; but it does not matter . in fact , for any @xmath376 , as long as it is fixed , the mle of @xmath40 is @xmath377 , which converges in probability to @xmath378 as @xmath367 , and @xmath379 with probability one .",
    "thus , the only way that the mle of @xmath40 can be consistent is to have both @xmath368 and @xmath380 go to @xmath381 .",
    "the example also helps to explain why it is necessary to consider the limiting process @xmath61 , instead of something else , in the open problem .",
    "the result of theorem  [ th1 ] shows that @xmath382 is also sufficient for the consistency of the mle .",
    "in fact , from the proof of theorem  [ th1 ] it follows that , for large @xmath17 , we have with probability tending to one that the conditional probability that @xmath383 given @xmath50}$ ] is bounded by @xmath384 for some constant @xmath67 . the corresponding upper bound under theorem  [ th3 ] is @xmath385 for some constant @xmath386 , where @xmath387 is the number of independent vectors in the subset @xmath50}$ ] , and a similar result holds under theorem  [ th4 ] with the upper bound being @xmath388 $ ] .",
    "the assumption of theorem  [ th3 ] , namely , ( a1 ) , makes sure that @xmath389 as the sample size increases ; the assumptions of theorem  [ th4 ] , namely , ( a1 ) and  ( b3 ) , make sure that , in addition , the @xmath390 in the above vanishes as @xmath391 .",
    "although estimation of the random effects is not an objective of this paper , in some cases this is of interest .",
    "for example , one may consider estimating the conditional mean of @xmath392 given @xmath12 in the open problem ( which may correspond to the conditional probability of successful mating with the @xmath142th female in the salamander problem ) .",
    "as mentioned , the data are not clustered in this case ; in other words , all the data are in the same cluster , so the ratio of the number of effects over the number of observations is @xmath393 , which goes to zero as @xmath61 .",
    "it is easy to show that @xmath394 is a consistent estimator of @xmath395 , where @xmath76 and the ( unconditional ) expectation is with respect to @xmath396 , @xmath397 .",
    "similarly , @xmath398 is a consistent estimator of @xmath399 , where the ( unconditional ) expectation is with respect to @xmath400 , @xmath401 .",
    "the author wishes to thank all the researchers who have had the opportunity , and interest , to discuss with the author about the open problem over the past 15 years , especially those who have spent time thinking about a solution .",
    "the author is grateful for the constructive comments from an associate editor and two referees that have led to major improvements of the results and presentation ."
  ],
  "abstract_text": [
    "<S> we give answer to an open problem regarding consistency of the maximum likelihood estimators ( mles ) in generalized linear mixed models ( glmms ) involving crossed random effects . </S>",
    "<S> the solution to the open problem introduces an interesting , nonstandard approach to proving consistency of the mles in cases of dependent observations . using the new technique , </S>",
    "<S> we extend the results to mles under a general glmm . </S>",
    "<S> an example is used to further illustrate the technique . </S>"
  ]
}