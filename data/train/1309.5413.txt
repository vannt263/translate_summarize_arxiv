{
  "article_text": [
    "say @xmath12 are independent , identically distributed ( iid ) bernoulli random variables with mean @xmath1 .",
    "write @xmath13 to denote @xmath14 and @xmath15 .",
    "the purpose of this work is to present a new algorithm for estimating @xmath1 with @xmath2 so that the relative error @xmath16 has a known distribution that does not depend on the value of @xmath1 . in other words , with this algorithm it is possible to compute @xmath17 exactly for any @xmath18 , without needing any kind of approximation or limiting behavior .",
    "many randomized approximation algorithms operate by reducing the problem to finding @xmath1 for an iid stream of bernoulli random variables . for example , approximating the permanent of a matrix with positive entries  @xcite , the number of solutions to a disjunctive normal form expression  @xcite , the volume of a convex body  @xcite , and counting the lattice points inside a polytope can all be put into this framework . in general , anywhere an acceptance rejection method is used to build an approximation algorithm , our method applies .    for these types of algorithms , the cost is usually dominated by the number of @xmath19 draws that are needed , and so the focus is on minimizing the expected number of such draws needed .",
    "suppose @xmath20 is a function of @xmath21 and auxiliary randomness ( represented by @xmath22)$ ] ) that outputs @xmath2 .",
    "let @xmath23 be a stopping time with respect to the natural filtration so that the value of @xmath2 only depends on @xmath24 and @xmath25 . then call @xmath23 the _ running time _ of the algorithm .",
    "the simplest algorithm for estimating @xmath1 just fixes @xmath26 , and sets @xmath27 in this case @xmath28 has a binomial distribution with parameters @xmath29 and @xmath1 .",
    "the standard deviation of @xmath28 is @xmath30 therefore , to get an estimate which is close to @xmath1 in the sense of having small relative error , @xmath29 should be of the form @xmath31 ( for some constant @xmath32 ) so that the standard deviation is @xmath33 and so roughly proportional to @xmath1 . from the central limit theorem , roughly @xmath34",
    "samples are necessary to get @xmath35 $ ] for @xmath36 .",
    "( see section  [ sec : lowerbound ] for a more detailed form of this argument . )",
    "but @xmath1 is unknown at the beginning of the algorithm !",
    "dagum , karp , luby and ross  @xcite dealt with this circularity problem with their stopping rule algorithm . in this context of @xmath19 random variables ,",
    "their algorithm can be written as follows .",
    "fix @xmath37 with @xmath36 and @xmath38 .",
    "let @xmath23 be the smallest integer such that @xmath39 .",
    "then @xmath40    call this method .",
    "they showed the following result for their estimate ( stopping rule theorem of  @xcite ) . @xmath41 and @xmath42 \\leq [ 1 + ( 1 + \\epsilon)4(e - 2)\\ln(2/\\delta)\\epsilon^{-2}]/p$ ] .",
    "they also showed that any such @xmath37 approximation algorithm that applies to all @xmath43 $ ] ( lemma 7.5 of  @xcite ) must satisfy @xmath44 \\geq ( 4e^2)^{-1}(1 - \\delta)(1 - \\epsilon)^2   ( 1 - p)\\epsilon^{-2}\\ln(\\delta^{-1}(2 - \\delta)).\\ ] ]    the factor of @xmath45 in the running time of is somewhat artificial . as mentioned earlier , a heuristic central limit theorem argument ( see section  [ sec : lowerbound ] ) indicates that the correct factor in the running time should be 2 ( this is the same 2 in the denominator of the exponential in the standard normal density ) .",
    "our algorithm is similar to , but with a continuous modification that yields several desirable benefits .",
    "the estimate @xmath46 is a fixed integer divided by a negative binomial random variable . in the algorithm proposed here , the estimate is a fixed integer divided by a gamma random variable . since gamma random variables are scalable , the relative error of the estimate does not depend on the value of @xmath1 .",
    "this allows a much tighter analysis of the error , since the value of @xmath1 is no longer an issue .",
    "in particular , the algorithm attains ( to first order ) the @xmath47 running time that is likely the best possible .",
    "the new algorithm is called the gamma bernoulli approximation scheme ( ) .",
    "[ thm : upperbound ] the method of section  [ sec : algorithm ] , for any integer @xmath48 , outputs an estimate @xmath2 using @xmath23 samples where @xmath42 = k / p$ ] , @xmath49 = p$ ] , and the density of @xmath50 is @xmath51 suppose @xmath52 , @xmath53 , and @xmath54 then @xmath55 .",
    "the lower bound of  @xcite for random variables in @xmath11 $ ] can be improved for @xmath56 random variables .",
    "the following theorem is proved in section  [ sec : lowerbound ] .",
    "for @xmath57 and @xmath53 any algorithm that returns @xmath2 for @xmath43 $ ] satisfying @xmath58 must have @xmath44 \\geq { ( 1/5)\\epsilon^{-2}(1 + 2\\epsilon)(1 - \\delta)\\ln((2-\\delta)\\delta^{-1})p^{-1}}.\\ ] ]    as @xmath4 and @xmath5 go to 0 , the ratio between the upper and lower bounds converges to 10 for these results . from central limit theorem considerations ,",
    "it is likely that the upper bound constant of 2 is the correct one ( see  section  [ sec : lowerbound ] ) .",
    "the algorithm is based upon properties of a one dimensional poisson point process .",
    "write @xmath59 for the exponential distribution with rate @xmath60 and mean @xmath61 .",
    "so @xmath62 has density @xmath63 . here",
    "@xmath64 denotes the indicator function that evaluates to 1 if the expression is true and is 0 otherwise .",
    "let @xmath65 be iid @xmath59 random variables .",
    "set @xmath66 .",
    "then @xmath67 is a _ one dimensional poisson point process of rate @xmath60 .",
    "_    the sum of exponential random variables is well known to be a gamma distributed random variable .",
    "( it is also called the erlang distribution . ) for all @xmath68 , the distribution of @xmath69 is gamma with shape and rate parameters @xmath68 and @xmath60 .",
    "the density of this random variable is @xmath70^{-1 } \\lambda^i t^{i - 1 } \\exp(-t \\lambda ) { { \\bf 1}}(t \\geq 0),\\ ] ] and write @xmath71 .",
    "the key property used by the algorithm is _ thinning _ where each point in @xmath72 is retained independently with probability @xmath1 .",
    "the result is a new poisson point process @xmath73 which has rate @xmath74 .",
    "( see for instance  @xcite . )",
    "the intuition is as follows . for a poisson point process of rate @xmath60 , the chance that a point in @xmath72 lies in an interval",
    "@xmath75 $ ] is approximately @xmath76 , while the chance that a point in @xmath73 lies in interval @xmath77 = \\lambda p h$ ] since points are only retained with probability @xmath1 .",
    "hence the new rate is @xmath74 .    for completeness the next lemma verifies this fact directly by establishing that the distribution of the minimum point in @xmath73 is @xmath78 .",
    "[ lem : thinning ] let @xmath79 so for @xmath80 , @xmath81 .",
    "let @xmath82 where @xmath83 then @xmath84    @xmath85 has moment generating function @xmath86 = pe^t/(1 - ( 1 - p)e^t)$ ] when @xmath87 .",
    "the moment generating function of @xmath88 is @xmath89 = \\lambda(\\lambda - t)^{-1}$ ] when @xmath90 .",
    "the moment generating function of @xmath91 is the composition @xmath92 when @xmath93 , and so @xmath94 .",
    "another useful fact is that exponential distributions ( and so gamma distributions ) scale easily .",
    "[ lem : scaling ] let @xmath95 .",
    "then for @xmath96 , @xmath97 .",
    "the moment generating function of @xmath98 is @xmath99^a$ ] for @xmath100 , so that of @xmath101 is @xmath102 = m_x(\\beta t ) = [ b/(b - \\beta t)]^a   = [ \\beta^{-1 } b/(\\beta^{-1}b - t)]^a\\ ] ] exactly the moment generating function of a @xmath103 .    together",
    "these results give the approach .",
    "rl + 1 ) & @xmath104 , @xmath105 .",
    "+ 2 ) & repeat + 3 ) & @xmath106 , @xmath107 + 4 ) & @xmath108 , @xmath109 + 5 ) & until @xmath110 + 6 ) & @xmath111 +    the output @xmath2 of satisfies @xmath112 and @xmath49 = p$ ] .",
    "the number of @xmath19 calls @xmath23 in the algorithm satisfies @xmath42 = k / p$ ] .",
    "the relative error @xmath113 has density @xmath114    from lemma  [ lem : thinning ] , the distribution of @xmath115 is @xmath116 . from lemma  [ lem :",
    "scaling ] , the distribution of @xmath117 is @xmath118 . hence @xmath49 = { \\mathbb{e}}[p / x]$ ] where @xmath119",
    ". now @xmath120 & = \\int_0^\\infty \\frac{1}{s}\\frac{(k-1)^k}{(k-1 ) ! }    s^{k - 1}\\exp(-(k-1)s ) \\ ds \\\\   & = \\frac{(k-1)^k}{(k-1 ) ! }",
    "\\int_0^\\infty s^{k - 2}\\exp(-(k-1)s ) \\ ds \\\\   & = \\frac{(k-1)^k}{(k-1 ) ! } \\cdot \\frac{(k-2)!}{(k-1)^{k-1 } }   = \\frac{k-1}{k-1 } = 1,\\end{aligned}\\ ] ] so @xmath49 = { \\mathbb{e}}[p / x ] = p$ ] .",
    "since @xmath23 , the number of @xmath19 drawn by the algorithm , is the sum of @xmath29 geometric random variables ( each with mean @xmath121 ) , @xmath23 has mean @xmath122 .",
    "the density of @xmath123 follows from the fact that @xmath124 has a @xmath118 distribution .",
    "note that for given @xmath29 this probability can be computed exactly in @xmath29 steps using the incomplete gamma function .",
    "hence for a given error bound and accuracy requirement , it is possible to exactly find the minimum @xmath29 using less work than flipping @xmath122 coins .    to show theorem  [ thm : upperbound ] , bounds on the tail of a gamma random variable are needed .",
    "chernoff bounds  @xcite are the simplest way to bound the tails .",
    "let @xmath0 be iid random variables with finite mean and finite moment generating function for @xmath125 $ ] , where @xmath18 .",
    "let @xmath126 , and @xmath127 / \\exp(t \\gamma { \\mathbb{e}}[x]).$ ] then @xmath128 ) & \\leq h(\\gamma )   \\ \\ \\ , \\text { for all } t \\in [ 0,b ] \\text { and } \\gamma \\geq 1.\\\\ { \\mathbb{p}}(x \\leq \\gamma{\\mathbb{e}}[x ] ) & \\leq        h(\\gamma )   \\ \\ \\ , \\text { for all } t \\in [ a,0 ] \\text { and } \\gamma \\leq 1.\\end{aligned}\\ ] ]    for @xmath129 , let @xmath130",
    ". then @xmath128 ) & \\leq g(\\gamma)^k \\ \\ \\",
    "\\text { for all } \\gamma \\geq 1 \\\\   { \\mathbb{p}}(x \\leq \\gamma { \\mathbb{e}}[x ] ) & \\leq g(\\gamma)^k \\ \\ \\     \\text { for all } \\gamma \\leq 1.\\end{aligned}\\ ] ]    for @xmath119 , @xmath131 = k/(k-1)$ ] and the moment generating function is @xmath132 = ( 1 - t/(k-1))^{-k}$ ] when @xmath133 .",
    "letting @xmath134 , that makes @xmath135 from the chernoff bound @xmath136 letting @xmath137 minimizes the right hand side , making it @xmath138^k.\\ ] ]    for @xmath139 $ ] , @xmath140    let @xmath141 , then the goal is to show ( after taking the natural logarithm of both sides ) @xmath142 the taylor series expansion gives @xmath143 when @xmath144 , all the terms on the right hand side are negative , so truncating gives the result .",
    "when @xmath145 $ ] , the terms alternate and are decreasing in absolute value , so truncation again gives the desired result .",
    "for @xmath146 , when @xmath147 @xmath148 .    let @xmath119 . then @xmath149 , so @xmath150 ) + { \\mathbb{p}}(x < \\gamma_2 { \\mathbb{e}}[x ] ) , \\\\   & \\leq [ \\gamma_1/\\exp(\\gamma_1 - 1 ) ] + [ \\gamma_2/\\exp(\\gamma_2 - 1)].\\end{aligned}\\ ] ] where @xmath151(1 - \\epsilon)^{-1}$ ] and @xmath152(1 + \\epsilon)^{-1}$ ] .",
    "note @xmath153 is an increasing function when @xmath154 , and a decreasing function when @xmath155 . for @xmath156 , @xmath157    hence @xmath158 + [ \\gamma'_2/\\exp(\\gamma'_2 - 1)]\\ ] ] where @xmath159 this means @xmath160    using the bound from the previous lemma @xmath161    now turn to @xmath162 : @xmath163 by clearing the denominator it is possible ( if tedious ) to verify that @xmath164 hence @xmath165 combining the tail bounds for @xmath162 and @xmath166 gives the result .",
    "the new algorithm intentionally introduces random smoothing to make the estimate easier to analyze . for a fixed number of flips , a sufficient statistic for the mean of a bernoulli random variable",
    "is the number of times the coin came up heads . call this number @xmath167 .    for @xmath29 flips of the coin",
    ", @xmath167 will be a binomial random variable with parameters @xmath29 and @xmath1 .",
    "then @xmath168 is the unbiased estimate of @xmath1 . by the central limit theorem ,",
    "@xmath28 will be approximately normally distributed with mean @xmath1 and standard deviation @xmath169 .",
    "therefore ( for small @xmath1 ) , @xmath170 will be approximately normal with mean 1 and standard deviation @xmath171 .",
    "let @xmath172 denote such a normal .",
    "then well known bounds on the tails of the normal distribution give @xmath173    therefore , to get @xmath174 requires about @xmath175 samples . a bound on the lower tail",
    "may be found in a similar fashion . since",
    "only about this many samples are required by the algorithm of section  [ sec : algorithm ] , the constant of 2 in front is most likely the best possible .    to actually prove a lower bound , follow the approach of  @xcite that uses wald s sequential probability ratio test .",
    "consider the problem of testing hypothesis @xmath176 versus @xmath177 , where @xmath178 .",
    "suppose there is an approximating scheme that approximates @xmath1 within a factor of @xmath179 with chance at least @xmath180 for all @xmath181 $ ] using @xmath23 flips of the coin .",
    "then take the estimate @xmath2 and accept @xmath182 ( reject @xmath182 ) if @xmath183 and accept @xmath184 ( reject @xmath184 ) if @xmath185 .",
    "then let @xmath186 be the chance that @xmath182 is rejected even though it is true , and @xmath187 be the chance that @xmath184 is accepted even though it is false . from the properties of the approximation scheme , @xmath186 and @xmath187 are both at most @xmath188 .",
    "wald presented the sequential probability ratio test for testing @xmath182 versus @xmath184 , and showed that it minimized the expected number of coin flips among all tests with the type i and ii error probabilities @xmath186 and @xmath187  @xcite .",
    "this result was formulated as corollary 7.2 in  @xcite .",
    "[ fct : wald ] if @xmath23 is the stopping time of any test of @xmath182 versus @xmath184 with error probabilities @xmath186 and @xmath187 such that @xmath189 , then @xmath190 \\geq -(1 - \\delta)\\omega_0^{-1}\\ln((2-\\delta)\\delta^{-1}).\\ ] ] where @xmath191 $ ] with @xmath192 , @xmath193 and @xmath194 .",
    "this gives the following lemma for @xmath19 random variables .",
    "fix @xmath57 and @xmath53 .",
    "let @xmath23 be the stopping time of any @xmath195 approximation scheme that applies to @xmath13 for all @xmath196 $ ] .",
    "then @xmath44 \\geq { ( 1/5)\\epsilon^{-2}(1 + 2\\epsilon)(1 - \\delta)\\ln((2-\\delta)\\delta^{-1})p^{-1}}.\\ ] ]    as noted above , using the approximation scheme with @xmath4 and @xmath197 to test if @xmath198 or @xmath199 gives @xmath200 and @xmath201 . here",
    "\\\\   & = p_0 \\ln\\left[\\frac{p_1(1 - p_1)^{1/p_0 - 1}}{p_0(1 - p_0)^{1/p_0 - 1}}\\right].\\end{aligned}\\ ] ]    consider a function of the form @xmath203 where @xmath204 is a constant .",
    "then @xmath205 for @xmath206 , and @xmath207 , which gives @xmath208 hence for all @xmath209 , @xmath210 is strictly increasing in @xmath211 .",
    "setting @xmath212 gives @xmath213 , so @xmath214 for @xmath215 .",
    "using @xmath216 and @xmath214 in fact  [ fct : wald ] gives @xmath217 & \\geq -\\omega_0^{-1}(1 - \\delta)\\ln((2-\\delta)\\delta^{-1}).\\end{aligned}\\ ] ]    since @xmath218 is alternating and decreasing in magnitude for @xmath206 : @xmath219 also , since @xmath220 @xmath221 \\\\ & \\geq \\frac{2\\epsilon + \\epsilon^2}{(1 + \\epsilon)^2 } -     \\frac{1}{2 } \\cdot \\left[\\frac{2\\epsilon + \\epsilon^2}{(1 + \\epsilon)^2 }     \\right]^2     \\cdot \\frac{p_0}{1 - p_0}.\\end{aligned}\\ ] ]    for @xmath222 , @xmath223 and the last factor of the second term can be removed .",
    "putting the bounds on the terms of @xmath224 together , @xmath225 \\\\",
    "& = p_0 \\frac{-5\\epsilon^2(1 + 2\\epsilon + ( 3/2)\\epsilon^2 + ( 2/5)\\epsilon^3 ) }    { ( 1 + \\epsilon)^4 } \\\\   & \\geq - p_0 5\\epsilon^2 / ( 1 + 2\\epsilon).\\end{aligned}\\ ] ] the last inequality follows from the fact that for @xmath57 , @xmath226",
    "a well known trick allows extension of the algorithm to @xmath11 $ ] random variables with mean @xmath227 , rather than just bernoulli s .",
    "let @xmath228 be a @xmath11 $ ] random variable with mean @xmath227 .",
    "then for @xmath22)$ ] , @xmath229 .    for @xmath22)$ ] and @xmath230 $ ] , @xmath231.\\ ] ]    therefore the algorithm of section  [ sec : algorithm ] can be applied to any @xmath11 $ ] random variable at the cost of one uniform on @xmath11 $ ] per draw of the random variable .",
    "a new algorithm for estimating the mean of @xmath11 $ ] variables is given with the remarkable property that the relative error in the estimate has a distribution independent of the quantity to be estimated .",
    "the estimate is unbiased . to obtain an estimate which has absolute relative error @xmath4 with probability at least @xmath232 requires at most @xmath233 samples .",
    "the factor of 2 is an improvement over the factor of @xmath234 in  @xcite .",
    "informal central limit theorem arguments indicate that this factor of 2 in the running time is the best possible .",
    "the provable lower bound on the constant is improved from the @xmath235 of  @xcite to @xmath236 for @xmath56 random variables ."
  ],
  "abstract_text": [
    "<S> say @xmath0 are independent identically distributed bernoulli random variables with mean @xmath1 . </S>",
    "<S> this paper builds a new estimate @xmath2 of @xmath1 that has the property that the relative error , @xmath3 , of the estimate does not depend in any way on the value of @xmath1 . </S>",
    "<S> this allows the construction of exact confidence intervals for @xmath1 of any desired level without needing any sort of limit or approximation . </S>",
    "<S> in addition , @xmath2 is unbiased . for @xmath4 and @xmath5 in @xmath6 , to obtain an estimate where @xmath7 , the new algorithm takes on average at most @xmath8 samples . </S>",
    "<S> it is also shown that any such algorithm that applies whenever @xmath9 requires at least @xmath10 samples . </S>",
    "<S> the same algorithm can also be applied to estimate the mean of any random variable that falls in @xmath11 $ ] . </S>"
  ]
}