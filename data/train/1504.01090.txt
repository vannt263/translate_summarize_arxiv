{
  "article_text": [
    "it is an inherent property of sensor networks that there are several observations of the desired source due to the availability of several sensors .",
    "if the observations at different nodes are correlated , it makes sense to make use of the correlation among the measurements to reduce the communication costs .",
    "this is particularly important for wireless networks where the resources , i.e. power and bandwidth are limited . in networks with a centralized topology , as shown in fig .",
    "[ topology ] ( a ) , the measurements from all the nodes are to be sent directly to a fusion center . in this case , one could think of the data that is already available at the fusion center ( sent by other nodes ) as side information .",
    "for an ad hoc topology with communication among the nodes , as shown in fig .",
    "[ topology ] ( b ) , the observation at the receiving node could be treated as side information . in either case , due to the correlation between the side information at the receiver and the data to be sent",
    ", the required transmission rate could be reduced using distributed source coding @xcite .",
    "the block diagram of the resulting source coding problem with side information is shown in fig . [ fig ] .",
    "the vectors @xmath0 , @xmath1 , and @xmath2 are the desired source , the observation at the encoder , and the side information available to the decoder , respectively . as shown in fig .",
    "[ fig ] , an optimal estimation @xmath3 of @xmath0 is obtained from the received data and the side information at the decoder .",
    "this setup is commonly referred to as the wyner - ziv problem after the celebrated work by wyner and ziv @xcite .",
    "note that this setup is less general than those shown in fig .",
    "1 , where there may be several sources . however , we focus on this setup for simplicity .            in this work ,",
    "we consider i.i.d .",
    "vector sources . note that in practice , the sources are correlated in time . by considering i.i.d .",
    "vector sources , we allow for correlation within each vector .",
    "this partially models the memory in the sources . to make full use of the memory in the signals",
    ", one has to either avoid the i.i.d .",
    "assumption , or let the dimension of the vector sources tend to infinity . for our derivations , we use the i.i.d .",
    "assumption and finite dimensions , and therefore accept some level of suboptimality .",
    "the wyner - ziv problem first appeared as an attempt to generalize the slepian and wolf s results in @xcite for lossless coding of two digital sources to the case of lossy source coding .",
    "the main result in @xcite was that , it is possible to encode two digital sources losslessly and separately with a sum - rate that asymptotically achieves the joint entropy of the two sources given that the decoding is performed jointly . in @xcite ,",
    "wyner and ziv considered a similar problem for lossy source coding , where one source is to be encoded and sent to the decoder and the other one is available only to the decoder and serves as the side information .",
    "the results were later extended to continuous sources in @xcite .",
    "it was shown that in general , this setup incurs a loss compared to the case where the side information is available at the encoder , but if the sources are gaussian , no loss occurs .    since in our setup",
    "the observation @xmath1 is in general not the same as the desired source @xmath0 , we refer to it as the remote wyner - ziv problem . in this paper",
    ", we consider the remote wyner - ziv problem with a covariance matrix distortion constraint .",
    "we derive a closed - form formula for the rate - distortion function ( rdf ) , and then study the special cases and applications of this result to source coding problems in networks . in order to formally define the problem , we introduce the distortion constraints which are of interest for this work in section [ define - distortions ] , but before that , we need to define our notation .",
    "we denote matrices and vectors by boldface uppercase and lowercase letters , respectively .",
    "we consider zero - mean stationary gaussian sources , which generate independent vectors @xmath4 , @xmath5 , and @xmath6 , as shown in fig . [ fig ] .",
    "the ordered set of vectors @xmath7 is denoted by @xmath8 for compactness .",
    "we denote the expectation operation by @xmath9 $ ] .",
    "the covariance and cross - covariance matrices are denoted by the symbol @xmath10 followed by an appropriate subscript . as an example , for random vectors @xmath0 , @xmath1 and @xmath2 , the conditional cross - covariance of @xmath0 and @xmath1 given @xmath2 and the conditional covariance of @xmath0 given @xmath1 and @xmath2 are denoted by @xmath11 and @xmath12 , respectively , and are defined as :    @xmath13 , \\right . \\\\",
    "{ { \\bf{\\sigma } } _ { { \\bf{x}}|{{\\bf{y}}}{{\\bf{z } } } } } & \\buildrel \\delta \\over = e \\left [ ( { \\bf{x } } - { \\boldsymbol{\\mu}}_{{\\bf{x}}|{\\bf{y}},{\\bf{z}}})({\\bf{x } } - { \\boldsymbol{\\mu}}_{{\\bf{x}}|{\\bf{y}},{\\bf{z}}})^t \\left| { \\bf{y}},{\\bf{z } } \\right ] , \\right.\\end{aligned}\\ ] ]    where the superscript @xmath14 denotes the transpose of a matrix , and @xmath15 denotes the mean . for instance , @xmath16 $ ] is the conditional mean of @xmath0 given @xmath1 and @xmath2 .",
    "markov chains are denoted by two - headed arrows as in @xmath17 , which means that if @xmath1 is given , then @xmath0 and @xmath18 are independent . the information theoretic operations of differential entropy and mutual information are denoted by @xmath19 and @xmath20 , respectively .",
    "the trace operation is denoted by @xmath21 .",
    "the set of symmetric positive - definite matrices is denoted by @xmath22 .",
    "the statement @xmath23 ( @xmath24 ) means that @xmath25 is positive semidefinite ( definite ) . the @xmath26 identity matrix is denoted by @xmath27 .",
    "we use @xmath28 to show an @xmath26 diagonal matrix having the elements @xmath29 on its main diagonal .",
    "we also make use of the following notations for briefness :    @xmath30      consider the remote wyner - ziv problem where the decoder makes an estimation of the desired source @xmath4 using the received message @xmath31 and the side information @xmath6 .",
    "for the special case where the sources are scalar , the well - known mean - squared error distortion constraint is in the form of @xmath32 , where @xmath33 is a given target distortion , and @xmath34 is the variance of the reconstruction error . with this distortion constraint ,",
    "the scalar gaussian remote wyner - ziv problem was treated in @xcite@xcite , and the rate - distortion function was derived in a rather simple form .    for the vector gaussian case , there are several options to define a quadratic distortion constraint .",
    "the most obvious one is the mean - squared error distortion constraint , which is the generalization of the above - mentioned distortion constraint for the case of vector sources .",
    "it is defined as follows :    @xmath35    where @xmath33 is a given target distortion .",
    "another constraint , which is related to a relay network problem as discussed in section [ mutual_info ] , is a mutual information constraint defined as :    @xmath36    where @xmath37 is a given constant .",
    "for both the mean - squared error distortion and mutual information constraints , the vector gaussian problem was solved in @xcite , and turned out to have a parametric form , resembling the familiar water - filling solutions for some other gaussian rate - distortion problems .",
    "the last constraint we discuss is the covariance matrix distortion constraint , which is defined as :    @xmath38    where @xmath39 is a given positive - definite target distortion matrix . for the vector gaussian wyner - ziv problem with the covariance matrix distortion constraint",
    ", the rate - distortion function can be derived using standard techniques for a limited set of distortion matrices ( e.g. @xmath40 ) @xcite .",
    "however , the general case does not appear to be manageable to solve using standard techniques , and no closed - form statement is available for the general rdf in the literature . in this paper",
    ", we treat this problem and study some of its applications .",
    "we elaborate on some aspects of this problem and our motivation in the next subsection .      during the recent years",
    ", there has been a shift from the traditional mean - squared error distortion constraint to covariance matrix distortions in the area of multiterminal source coding @xcite@xcite .    in @xcite , a multiple description scenario",
    "is considered where @xmath41 encoders send their descriptions of a vector gaussian source to @xmath41 individual and one joint decoders .",
    "each decoder makes an estimation of the source within a given covariance matrix distortion constraint .",
    "the sum - rate of the multiple descriptions problem is specified in @xcite under certain assumptions on the distortion matrices .",
    "the gaussian one - helper problem with the covariance matrix distortion constraint was studied in @xcite .",
    "here in addition to a main encoder which observes the direct source and sends a message to the decoder with rate @xmath42 , there is a helper with noisy observations , which sends a message with rate @xmath43 to the decoder .",
    "the decoder estimates the source using the received messages .",
    "the rate regions for this problem for the scalar and general helpers were derived in @xcite and @xcite , respectively .",
    "in addition to the above works , there are also several results which bound the rate - distortion regions of source coding problems under the covariance matrix distortion constraint @xcite@xcite . in the so - called ceo problem @xcite , a number of _ agents _",
    "encode and send their observations of a source with certain rates to a _",
    "ceo_. the ceo makes an estimation of the source using the received messages from the agents .",
    "the rate region for the vector gaussian ceo problem under the covariance matrix distortion constraint was outer - bounded in @xcite .",
    "finally , in @xcite , a set of @xmath41 noisy observations of @xmath44 correlated scalar gaussian sources are encoded separately and sent with the rates @xmath45 to a joint decoder .",
    "the decoder makes an estimation of the sources using the received messages .",
    "the rate - distortion regions for this problem are inner- and outer - bounded for three different distortion constraints including the covariance matrix distortions .",
    "although the vector gaussian remote wyner - ziv problem has a rather simple setup , we could not find any closed - form solution for the general problem under the covariance matrix distortion constraint in the literature .",
    "neither does such a closed - form solution follow as a special case of the above - mentioned results .",
    "in general , with the covariance matrix distortion constraint , the matrix form of the target distortion gives rise to new issues compared to the scalar target distortions , which make the problem harder to solve .",
    "for the wyner - ziv problem , the covariance matrix of the unknown part of the source at the decoder is @xmath46 . with a scalar target distortion @xmath47",
    ", the following two cases might arise :    * @xmath48 , for which the required rate is zero .",
    "* @xmath49 , which means that some nonzero rate has to be spent . in this case , with an optimal coding scheme , the variance of the reconstruction error at the decoder would be the same as the target distortion .    for a matrix target distortion @xmath50 on the other hand",
    ", it might happen that none of the two cases @xmath51 and @xmath52 hold .",
    "in general , there are two sets of directions in @xmath53 . in one set of directions ,",
    "the distortion constraint is already satisfied at the decoder using the side information @xmath54 . in the other set of directions ,",
    "the distortion constraint can not be satisfied without some help from the encoder , making the minimum required rate a nonzero value .",
    "in such cases , the covariance matrix of the reconstruction error at the decoder is not guaranteed to be equal to @xmath50 .",
    "the argument for considering the covariance matrix distortion constraints despite the above - mentioned issues is the generality of the results . for the remote wyner - ziv problem , the resulting rate - distortion function",
    "@xmath55 would be a mapping from @xmath56 to @xmath57 .",
    "for a given problem with a constraint on @xmath39 , there is in general a subset of @xmath58 for which the constraint is satisfied .",
    "we can then choose one member of this subset which minimizes the rate .",
    "as an example ( among other examples that will be studied in this paper ) , we will show later on , that the ubiquitous case of the rdf with the mean - squared error distortion is simply equivalent to @xmath55 at a particular distortion matrix @xmath59 in @xmath58 .",
    "this particular @xmath59 is the one that minimizes the rate @xmath60 subject to a mean - squared error constraint on @xmath50 .",
    "in addition to generality , the covariance matrix distortion constraint has other advantages .",
    "defining the distortion with respect to the covariance matrix of the reconstruction error allows for formulating new problems .",
    "in many sensor network setups , there are constraints on the sum - rate or weighted sum - rate of the network . as we will show in section [ applications ]",
    ", this can be formulated as a constraint on the determinant of the distortion matrices .",
    "one can then optimize a function ( such as snr ) over the network subject to this constraint in order to optimally allocate the rates and distortions to the sensors .",
    "section [ wz ] is dedicated to the vector gaussian remote wyner - ziv problem under covariance matrix distortion constraints .",
    "we start with defining a minimum for a pair of symmetric positive - definite matrices based on the joint diagonalization of the matrices .",
    "we then derive some properties for this notion of minimum of two matrices , based on which we find an explicit formula for @xmath55 .",
    "this minimum of two matrices seems to be natural to our source coding problem , and in addition to the rdf itself , it appears also in the coding schemes and the reconstruction error at the decoder . to derive the rdf , we first lower - bound @xmath55 using the properties of the minimum of two matrices and information - theoretic arguments .",
    "next we upper - bound it by suggesting a linear coding scheme .",
    "the lower and upper bounds coincide , thus yielding the rdf .    in section [ applications ] , we present examples of applications and special cases of the results obtained in section [ wz ] .",
    "we consider three applications .",
    "first , we consider a similar source coding problem under the mean - squared error distortion constraint , and will show that the resulting rdf is a special case of @xmath55 for a certain choice of the distortion matrix @xmath50 .",
    "next , we consider a relay network scenario where in addition to the data transmitted to the center from a main transmitter , there is a relay which would like to help the main transmitter by sending its own observation to the center with a certain rate . the problem could be formulated as a source coding problem giving a rate - information function @xcite .",
    "we will show that similar to the previous case , the resulting rate - information function will be given by @xmath55 for a certain choice of the distortion matrix @xmath50 .",
    "we then show how one could implement this scenario using the results from section [ wz ] .    finally , as the third and most elaborated example of the applications , we consider a sensor network with a centralized topology .",
    "the observation from each sensor is encoded and transmitted to the fusion center with a certain rate .",
    "we assume that the the weighted sum - rate of the network is limited to a given amount . at the fusion center",
    "one would like to fuse the received data in a manner that the output experiences no linear distortion . we will show that under the given weighted sum - rate constraint , one could design and allocate the distortion matrices to the sensor nodes in order to maximize the output signal to noise ratio at the fusion center .",
    "we conclude the paper in section [ conclusion ] by discussing the main results and possibilities for future work .",
    "in this section , we solve the source coding problem in networks which was simplified to the block diagram in fig .",
    "[ fig ] . later in section [ applications ] , we will show how we can use the results for optimizing certain functions in networks .",
    "we start with a formal definition of the source coding problem that was introduced in section [ intro ] .",
    "assume that @xmath61 is a sequence of i.i.d .",
    "zero - mean random vectors such that @xmath62 , @xmath63 , and @xmath64 are jointly gaussian for @xmath65 .",
    "the encoder observes @xmath66 , and using an encoding function :    @xmath67    sends a message to the decoder .",
    "the decoder observes @xmath68 and receives the message from the encoder , based on which makes an estimation of @xmath8 using a decoding function :    @xmath69    the rate - distortion pair @xmath70 is achievable if there exists a block length @xmath71 and the encoding and decoding functions @xmath72 and @xmath73 , such that for @xmath74 :    @xmath75.\\end{aligned}\\ ] ]    the rate - distortion region for the above problem is the closure of the set of achievable rate - distortion pairs @xmath70 .",
    "let @xmath76 be the rate - distortion region for the above problem .",
    "the rdf @xmath60 is then defined as :    @xmath77    notice that @xmath78 is the covariance matrix of the reconstruction error at the decoder when in addition to the side information @xmath2 , the uncoded observation @xmath1 of the encoder is also available at the decoder .",
    "since this is not achievable with finite rates , we must have @xmath79 , otherwise the rate would become infinite .",
    "we thus assume that it holds .",
    "suppose that the auxiliary random variable @xmath18 satisfies the markov chain @xmath80 .",
    "from the results in @xcite , it follows that the operational rdf defined above is equal to the following information rdf :    @xmath81    we write @xmath0 in terms of its linear estimation from @xmath1 and @xmath2 and the estimation error @xmath82 as follows :    @xmath83    where @xmath82 is independent of @xmath1 and @xmath2 , and @xmath84 .",
    "see appendix [ appendix2 ] for a quick derivation of the matrices @xmath85 and @xmath86 .    defining @xmath87 as :    @xmath88    and using ( [ xyz ] ) and the markov chain in ( [ mut - info ] ) , one can write :    @xmath89    we solve problem ( [ mut - info ] ) in the rest of this section . to do so , we first introduce a notion of minimum for two positive - definite matrices in the next subsection .",
    "based on the joint diagonalization of two matrices , we define a minimum for two positive - definite matrices and derive some important properties of this definition .",
    "these properties will be crucial in the derivation of the rdf in the next two subsections .",
    "although any joint diagonalization would work for this matter , we focus on a particular case which is defined in the sequel .",
    "consider the ordered pair @xmath90 of two @xmath26 symmetric positive - definite matrices .",
    "denote the eigenvalue decomposition of @xmath91 by :    @xmath92    where @xmath93 and @xmath94 .",
    "consider the principal square - root @xmath95 of @xmath91 and denote the eigenvalue decomposition of @xmath96 by :    @xmath97    such that @xmath98 and @xmath99 .",
    "we then define the joint diagonalizer of @xmath90 as follows .",
    "[ defn_joint_diag ] the joint diagonalizer @xmath100 of the ordered pair @xmath90 is defined as :    @xmath101    one can verify that :    @xmath102    where the diagonal matrix @xmath103 is defined as @xmath104 .",
    "this variant of joint diagonalization is based on another form of diagonalization found in ( * ? ? ?",
    "* theorem 8.3.1 ) . based on the above joint diagonalization",
    ", we define the minimum of @xmath90 as follows :    the minimum of the pair @xmath90 of symmetric positive - definite matrices with the joint diagonalizer @xmath100 is defined as :    @xmath105    some properties of the above definitions are summarized in the following lemmas .",
    "[ lem_properties ] for the pair @xmath90 of symmetric positive - definite matrices with the joint diagonalizer @xmath100 the following properties hold :    1 .   @xmath106 .",
    "2 .   @xmath107 .",
    "@xmath108 , and @xmath109 .",
    "4 .   @xmath110 if and only if @xmath111 , and @xmath112 if and only if @xmath113 .    the proof follows immediately from the definitions and is thus left out .",
    "the proof of the following lemma follows from the results in @xcite .",
    "[ lem_properties2 ] consider the following optimization problem :    @xmath114    where @xmath115 .",
    "we then have @xmath116 .",
    "the following theorem is the main result of this section .",
    "[ th1 ] the rate - distortion function for problem ( [ mut - info ] ) for @xmath117 is given by :    @xmath118    we prove the theorem by deriving coinciding upper and lower bounds on the rdf in sections [ lower ] and [ upper ] , respectively .",
    "let @xmath119 be the joint diagonalizer of @xmath120 as given in definition [ defn_joint_diag ] , such that : is of full rank . the rank - deficient case can be converted to an equivalent problem with full - rank matrices using an appropriate transformation .",
    "see e.g. appendix a in @xcite .",
    "]    @xmath121    define the function @xmath122 as :    @xmath123    the following lemma is then the main result of this subsection .",
    "[ lem1 ] the rdf @xmath60 defined in ( [ mut - info ] ) is lower - bounded as @xmath124 .",
    "see appendix [ appendix_lowerbound ] .      in this subsection , we upper bound @xmath55 with the same function @xmath122 which appeared in the lower bound in the previous subsection .",
    "this thus in combination with the lower - bound , gives the rdf in a closed form .",
    "it is important to note though that the knowledge of the rdf does not necessarily specify how one could encode the observations to achieve @xmath55 .",
    "the upper bound derived in this subsection is constructive , in the sense that it is based on an achievable coding scheme .",
    "such a scheme should suggest a way to transform @xmath1 into another variable @xmath18 , such that for a given distortion @xmath39 :    * the required rate for delivering @xmath18 to the decoder is no higher than @xmath55 .",
    "* the decoder could make an estimate @xmath125 of @xmath0 using @xmath18 and @xmath2 , such that the distortion constraint ( [ covariance_distortion_constraint ] ) is satisfied .",
    "the role of @xmath18 here is to model the quantization effect on @xmath1 ( or a transformation of @xmath1 ) .",
    "it is thus typically composed of a linear transformation of @xmath1 and an additive noise term that represents the quantization noise .",
    "we suggest such a scheme in the sequel .",
    "suppose that @xmath119 , @xmath126 , and @xmath127 are defined as in ( [ joint1 ] ) and ( [ joint2 ] ) .",
    "suppose also that @xmath128 is the orthogonal matrix of the eigenvectors of @xmath129 giving the eigenmatrix @xmath130 .",
    "the result of this subsection is then given by the following lemma .",
    "[ lem3 ] the following gaussian test channel achieves @xmath122 given by ( [ explicit ] ) :    @xmath131    where @xmath85 is defined in ( [ xyz ] ) , and the the coding noise @xmath132 is independent of @xmath1 with the covariance matrix given by :    @xmath133    moreover , the covariance matrix of the reconstruction error at the optimal decoder is given by :    @xmath134    see appendix [ appendix_upperbound ] .",
    "note that the coding scheme given by ( [ scheme ] ) involves two steps :    * linear transformation of the observation @xmath1 into @xmath135 , * quantization of the resulting sequence such that the quantization noise @xmath136 becomes gaussian and independent of @xmath1 with the covariance matrix given by ( [ cov_noise ] ) .",
    "the second step can be performed using a high - dimensional dithered vector quantizer .",
    "the dithered quantizer guarantees that @xmath136 will be independent of @xmath1 , and the high dimension allows for the gaussianity of @xmath136 @xcite .",
    "finally , notice that an immediate result of the above lemma is that @xmath137 , which establishes the desired upper bound on the rdf .",
    "_ remark : _ the first and second terms at the right - hand side of ( [ rec_err_formula ] ) are related to the error due to the remoteness of the source and the coding artifacts , respectively .",
    "this separability resembles the wolf and ziv s result in @xcite , where it is shown that for a point - to - point remote joint source - channel coding problem with mean - squared error distortions , the end - to - end distortion can be decomposed as the sum of two terms .",
    "one term is due to the remoteness of the source and the other one is due to coding .",
    "note however , that it was shown in @xcite , that this result is not optimal for the multiterminal case .",
    "for the vector gaussian remote wyner - ziv problem with a scalar mean - squared error constraint , the rate - distortion function is given by @xcite :    @xmath138    it was shown in @xcite that @xmath139 is given by :    @xmath140    where @xmath141 are the eigenvalues of @xmath129 , and @xmath142 is a constant ( water level ) satisfying the following constraint :    @xmath143    we will show that this is equivalent to a special case of @xmath55 for a particular choice of the distortion matrix @xmath50 .",
    "denote the eigenvalue decomposition of @xmath144 by @xmath145 , and define :    @xmath146    where @xmath142 is defined by ( [ water ] ) .",
    "it is clear that @xmath147 and is thus a valid distortion matrix . substituting @xmath148 in ( [ finalrdf ] ) yields :    @xmath149      consider a relay network where a main transmitter uses @xmath150 antennas to transmit the gaussian signal @xmath0 to the end receiver which has its own noisy observation @xmath2 of @xmath0 using @xmath151 antennas .",
    "there is also a relay that makes the noisy observation @xmath1 using its @xmath152 antennas and transmits to the end receiver with a given maximum rate .",
    "it is desired to find the minimum rate at which the system can reliably transmit .",
    "assuming that the statistics and channel state information for @xmath0 are known to the relay and receiver , it was shown in @xcite that this problem is equivalent to the following optimization problem :    @xmath153    for a given @xmath37 , and the resulting rate - mutual information function @xmath154 for @xmath155 is given by :    @xmath156 } ^ { -1 } \\right ) } , \\ ] ]    where @xmath157 , @xmath158 are the eigenvalues of @xmath159 , are defined as the nonzero eigenvalues of @xmath160 .",
    "note however that for any matrix @xmath161 the zero - excluded multispectrum is the same for @xmath162 and @xmath163 .",
    "one can then see that @xmath157 are also the eigenvalues of @xmath164 . using ( [ yprime ] ) and ( [ concov1 ] ) , this simplifies to @xmath159 . ] and @xmath165 satisfies the following :    @xmath166    see @xcite and the references therein for more details on the problem setup and formulation .",
    "we will first show that @xmath167 is a special case of @xmath60 for a particular choice of the distortion matrix @xmath39 , and then using the results of section [ wz ] , we will show how to design a coding scheme that achieves @xmath154 .",
    "suppose that the eigenvalue decomposition of @xmath159 is given by :    @xmath168    define :    @xmath169    where @xmath170 is given by ( [ mut - inf - water ] ) .",
    "from the facts that @xmath171 and @xmath172 , and using ( [ new_eig ] ) and ( [ opt_d_inf ] ) , it follows that @xmath173 , or equivalently @xmath174 . therefore , @xmath148 is a valid distortion matrix .",
    "substituting @xmath148 in ( [ finalrdf ] ) , noting that @xmath175 , and using property 4 in lemma [ lem_properties ] , we have :    @xmath176    where ( [ ub_opt2 ] ) follows from some straightforward manipulation of ( [ rate - rate ] ) and comparing the result with ( [ ub_opt3 ] ) .",
    "recall that for any valid distortion matrix @xmath39 , the coding scheme ( [ scheme ] ) achieves @xmath60 . since @xmath148 given by ( [ opt_d_inf ] ) is a valid distortion matrix , to implement a coding scheme that yields @xmath177 , one could set @xmath178 and use ( [ scheme ] ) .      a simple diagram of a sensor array as well as a centralized network of wireless sensors is shown in fig . [ array_network ] . in both cases",
    ", there is a central processing unit which makes an estimation of the desired source @xmath179 using the received signals . for the sensor array ,",
    "the signals which are fed to the processing unit are impaired versions of the desired source affected by the environment .",
    "one can then design the processing unit so as to minimize the noise at the output .",
    "for the sensor network , the sensor signals which are already impaired by the environment effects , have to be transmitted to the center with limited rates via wireless links .",
    "this incurs extra noise terms which are due to digital transmission ( coding noise ) .",
    "although this extra noise term degrades the performance , its impact can be alleviated by the fact that the coding noise can be controlled by the designer .",
    "in other words , in addition to the processing unit , one can make use of the distortion matrices as design parameters in order to minimize the output noise at the processing center .",
    "let us elaborate on this idea in the sequel .",
    "consider an array of @xmath71 sensors , where sensor @xmath180 observes a noisy version @xmath181 of a gaussian source @xmath182 given by :    @xmath183    where @xmath184 is the time index , and @xmath185 is the additive noise which is assumed to be gaussian and independent of the source @xmath182 and the noise terms at the other nodes . define @xmath186 as @xmath187^t$ ] ( and similarly for @xmath188 and @xmath189 ) .",
    "one could then rewrite ( [ scalar_samples ] ) in the vector form as :    @xmath190    where we drop the dependency on @xmath184 for simplicity of notation .",
    "the sources @xmath191 are related to a desired source @xmath179 as :    @xmath192    where @xmath193 are @xmath26 matrices .",
    "defining @xmath194 as @xmath195 $ ] , we rewrite ( [ predictablity ] ) as :    @xmath196    where @xmath197^t$ ] ( and we also define @xmath198 and @xmath199 similarly ) . at the processing unit , as shown in fig .",
    "[ array_network ] ( a ) , the output @xmath2 is produced using the following filtering operation :    @xmath200    where @xmath201 $ ] .",
    "the objective is to maximize the output snr of the filter @xmath202 while enforcing only additive distortion to the reconstruction ( no linear distortion ) .",
    "a typical example of this scenario with no linear distortion is in microphone arrays , where the linear distortion would cause undesired artefacts in the resulting audio or speech signal @xcite .",
    "another example of applications is in networked control systems , where the feedback data is transmitted via a communication network . in certain cases , because of the implementation constraints the source coding scheme used to compress the feedback information should incur no linear distortion @xcite .    to formulate the problem , notice that the output error of the filter @xmath202 is given by :    @xmath203    where @xmath204 is",
    "the linear distortion of the signal and @xmath205 is the additive distortion . to minimize the additive distortion while suppressing the linear distortion",
    ", one could choose @xmath206 , where    @xmath207 ) \\;\\;\\;\\ ; s.t . \\;\\;\\;\\ ; { \\bf{hw}}^t = { \\bf{i}}.\\ ] ]    there is a closed form solution for ( [ hst ] ) given by @xcite :    @xmath208    which gives the following snr at the output :    @xmath209    now suppose that instead of @xmath71 array sensors , we have a network of @xmath71 wireless sensors as shown in fig .",
    "[ array_network ] ( b ) .",
    "node @xmath210 encodes its observation @xmath211 and transmits it with the rate @xmath212 for a target distortion @xmath213 .",
    "this rdf is equivalent to ( [ finalrdf ] ) for the special case where @xmath214 , @xmath215 and @xmath216 .",
    "the result is given by :    @xmath217    we further assume that @xmath218 . from property 4 in lemma [ lem_properties ] it then follows that :    @xmath219    note that we have assumed independent decoding of the messages received from the sensors .",
    "this is in general suboptimal .",
    "an optimal strategy requires a joint decoding similar to the ceo problem @xcite , which would be difficult to solve analytically and also to implement .    from the definition of the distortion constraint ( [ covariance_distortion_constraint ] ) and the gaussianity of the achievable coding scheme ( see also the proof of lemma [ lem3 ] )",
    ", it follows that the reconstruction @xmath220 at the decoder satisfies @xmath221 , with @xmath222 being independent of @xmath220 and @xmath223 .",
    "equivalently , one could write :    @xmath224    where @xmath225 and @xmath211 are independent , and :    @xmath226    we apply the invertible map @xmath227 to @xmath228 to obtain the following equivalent ( but preferred ) version of the reconstruction :    @xmath229    with :    @xmath230    combining ( [ vector_samples ] ) and ( [ yhat1 ] ) we have :    @xmath231    where @xmath232 . recall that the objective is to maximize the snr at the output of the processing unit using the received noisy signals @xmath233 , while incurring no linear distortion to the output signal . comparing ( [ input1 ] ) to ( [ vector_samples ] ) , it is clear that this problem is similar to the case of the sensor array .",
    "one can thus apply the same filter as ( [ best - filter ] ) to @xmath234 to maximize the output snr . to do so ,",
    "we rewrite ( [ input1 ] ) as :    @xmath235    where @xmath236^t$ ] , and @xmath0 and @xmath237 are defined similarly .",
    "the output snr similar to ( [ snr ] ) is then given by :    @xmath238^{-1 } \\right\\}}. \\end{aligned}\\ ] ]    notice that due to the wireless link between the sensors and the center and the limited transmission rate , in addition to the additive noise @xmath189 in the measurements , there is also a reconstruction error @xmath239 due to quantization and coding . therefore",
    ", the covariance matrix of the noise term @xmath237 in ( [ input2 ] ) depends on the set of distortions as emphasized by the notation in ( [ snr2 ] ) . while the quantization error is inevitable , one could shape the covariance matrix @xmath240 in ( [ snr2 ] ) by manipulating the distortions @xmath241 in order to maximize the output snr .    to be more specific ,",
    "suppose that it is required that the weighted sum - rate of the network @xmath242 does not exceed a given amount @xmath243 for a given set of weights @xmath244 .",
    "the problem can then be formulated as follows :    @xmath245    the weights @xmath244 could be chosen for example to equalize the power consumption at the sensors , or to give the network sum - rate by being set equal to each other , or to satisfy any other criterion specified by the user . without loss of generality ,",
    "we assume that :    @xmath246    it is worth mentioning that this problem includes the allocation of the rates to the nodes , but is not limited to it , since rate allocation is equivalent to the specification of the determinant of the distortion matrices @xmath213 according to ( [ rdf_enhance2 ] ) , while in ( [ min ] ) the whole matrices @xmath241 must be chosen optimally to maximize the output snr .",
    "this possibility is a result of having the target distortions in the matrix form .",
    "problem ( [ min ] ) is equivalent to the following problem :    @xmath247^{-1 } \\right\\ } }   \\nonumber   \\\\ & \\;\\;\\;\\ ; s.t . \\;\\;\\;\\ ;   \\sum_{i=1}^n \\alpha_i r({\\bf{d}}_i ) = r.\\end{aligned}\\ ] ]    substituting ( [ rdf_enhance2 ] ) in the constraint in ( [ min1 ] ) , noting that @xmath248 , and using ( [ yhat2 ] ) , we rewrite ( [ min1 ] ) as :    @xmath249}^{-1 } { \\bf{w}}_i^t \\ ! \\right)^{-1 } \\ ! \\right\\ } } \\nonumber \\\\ &    \\;\\;\\;\\ ; s.t . \\;\\;\\;\\ ;   \\prod_{i=1}^n \\left| { \\bf{d}}_i \\right| ^{\\alpha_i } = \\beta,\\end{aligned}\\ ] ]    where @xmath250 is defined as :    @xmath251    for simplicity , we assume that @xmath252 in ( [ predictablity ] ) are invertible . notice that even if the relationship between @xmath188 and @xmath179 is not invertible , one could find an invertible matrix @xmath252 which minimizes for example the mean - squared error between @xmath188 and @xmath253",
    ". in this case , ( [ predictablity ] ) would be an approximation ( see @xcite for more details ) .",
    "however , the approximation error could be added to the additive noise @xmath189 in ( [ vector_samples ] ) .",
    "one could thus assume that @xmath254 always holds with an invertible @xmath252 .",
    "we define @xmath255 and @xmath256 as :    @xmath257    the following set of equations then follow from the kkt conditions for problem ( [ min2 ] ) ( see appendix [ appendix_kkt ] for the derivation ) :    @xmath258    in general , it is not easy to solve ( [ kkt_result1])([kkt_result3 ] ) analytically to find the unknowns @xmath85 , @xmath142 , and @xmath259 . instead , one could proceed by using numerical methods . in the rest of this subsection , we consider special cases of this problem where further analysis is possible and leads to interesting results .",
    "we assume that ( [ kkt_result1 ] ) could be approximated as :    @xmath260    from ( [ kkt1 ] ) and ( [ kkt2 ] ) , this means that the distortion matrices @xmath213 in ( [ kkt1 ] ) have small eigenvalues , or equivalently , @xmath261 has large eigenvalues .",
    "this holds , if the rates @xmath212 are high enough .",
    "we will show later , that in fact in order for the results to hold , the rates need not be very high , and depending on the setup , even for relatively low rates the results will be valid .",
    "the high - rate assumption can also be interpreted as :    @xmath262    we will use this form later to find the optimal value of the parameter @xmath142 .",
    "combining ( [ kkt_result2 ] ) and ( [ kkt_result1_aprx ] ) , using ( [ weights ] ) , and defining :    @xmath263    yields :    @xmath264    from ( [ a_eqn ] ) , it is easy to see that @xmath85 and @xmath265 share the same eigenvectors .",
    "denote the eigenvalue decomposition of @xmath265 by :    @xmath266    one could then write @xmath85 as :    @xmath267    to find @xmath268 , we substitute ( [ a_eig ] ) and ( [ s_eig ] ) in ( [ a_eqn ] ) to obtain :    @xmath269    which gives the following solution :    @xmath270    note that the only unknown parameter in ( [ a_eqn3 ] ) is @xmath142 . to find @xmath142 , we use ( [ other - form ] ) and ( [ kkt1 ] ) to rewrite ( [ kkt_result3 ] ) as :    @xmath271    defining @xmath170 as :    @xmath272    substituting ( [ kkt_result1_aprx ] ) in ( [ kkt3_aprx ] ) , and simplifying the result yields :    @xmath273    finally , substituting",
    "( [ a_eqn3 ] ) in ( [ lambda1 ] ) leads to the following equation for @xmath142 :    @xmath274    to summarize the results , given the parameters @xmath252 , @xmath275 , @xmath276 for @xmath277 , one should take the following steps to find the optimal allocation of the distortions @xmath213 to the nodes :    * calculate @xmath265 using ( [ s ] ) , and find @xmath278 and @xmath279 from ( [ s_eig ] ) .",
    "* calculate @xmath170 from ( [ gamma ] ) and then find @xmath142 by solving ( [ lambda2 ] ) .",
    "* use ( [ a_eqn3 ] ) and ( [ a_eig ] ) to find @xmath85 . * calculate @xmath255 using ( [ kkt_result1_aprx ] ) . *",
    "calculate @xmath213 using ( [ kkt1 ] ) .",
    "note that the whole process depends on finding a solution for ( [ lambda2 ] ) .",
    "define @xmath280 as :    @xmath281    we then prove the following proposition :    [ high_rate_proposition ] if @xmath282 , there is a unique solution to ( [ lambda2 ] ) , otherwise ( [ lambda2 ] ) has no solution",
    ".    see appendix [ appendix_uniqueness_highrate ] .",
    "we perform a few simulations to further study the behaviour of the output snr as a function of the distortion matrices @xmath213 .",
    "we assume that there are two nodes @xmath283 , and @xmath284 .",
    "we consider the following structure for the covariance matrix of both signal and noise :    @xmath285    where @xmath286 is the @xmath287th element of @xmath10 .",
    "for the signal @xmath179 , we choose @xmath288 and @xmath289 . for the noise terms",
    ", we use @xmath290 and @xmath291 as control parameters . the dimension and the weighted sum - rate",
    "are kept fixed on @xmath292 and @xmath293 , respectively , unless otherwise stated .",
    "we check the optimality of the results and the gain that can be achieved .",
    "we first specify the parameters @xmath294 , and the covariance matrix of the noise terms .",
    "we find the distortion matrices @xmath295 suggested by the procedure described above and their corresponding snr .",
    "we then compare the resulting snr to the snr given by other choices of the distortion matrices . assuming that the eigenvalue decomposition of @xmath296 is given by @xmath297 , we use the following formula to generate other distortion matrices :    @xmath298    where @xmath299 and @xmath300 are weighting parameters , and @xmath301 is a diagonal matrix whose diagonal elements are drawn from a uniform distribution in the interval @xmath302 $ ] , with @xmath303 being the largest element of @xmath304 . by choosing @xmath299 close to @xmath305 and @xmath300 close to @xmath306 , the resulting distortion matrices @xmath213 will be slightly perturbed versions of @xmath296 . on the other hand , by choosing @xmath299 close to @xmath306 and @xmath300 close to @xmath305",
    ", the resulting @xmath213 will be random .",
    "using ( [ random_distortion ] ) , we generate @xmath41 valid distortion pairs @xmath307 using the following algorithm :    * initialization * : set @xmath308 .",
    "* iterations * : while @xmath309 perform the following steps :    1 .",
    "generate @xmath310 using ( [ random_distortion ] ) 2 .",
    "if the result is not a valid distortion matrix , go back to 1 , otherwise generate @xmath311 using ( [ random_distortion ] ) .",
    "3 .   normalize @xmath311 such that the constraint in ( [ min ] ) holds .",
    "if the result is not a valid distortion matrix , go back to 2 .",
    "calculate the snr given by @xmath307 .",
    "increase @xmath312 by 1 .",
    "we set @xmath313 for the rest of this subsection . to study the behavior of the output snr around @xmath314 , we set @xmath315 and @xmath316 . figure [ local_max ]",
    "illustrates the results for different choices of the parameters . as seen from fig .",
    "[ local_max ] , the theoretical results from the high - rate approximation indeed give the local maximum for the output snr .            to verify that @xmath314 is the global maximizer of the output snr and to see the gap between the maximum snr and the one resulting from a random allocation of the distortion matrices , we set @xmath317 and @xmath318 .",
    "the results are illustrated in fig .",
    "[ global_max ] .",
    "as seen from the plots , the gap between the optimal snr and the one resulting from a random allocation could in some cases be as large as 5 db .    to show the scalability in sense of the number of sensors , we repeat the simulation in fig . [ global_max ] ( a ) with 4 sensors .",
    "two sensors have similar parameters with sensor 1 in fig . [ global_max ] ( a ) , and",
    "the other two sensors have similar parameters with sensor 2 in the same figure .",
    "the weights are set as @xmath319 .",
    "the result is shown in fig . [",
    "4nodes ] , which shows that for 4 sensors , as expected , the output snr is higher compared to 2 sensors ( fig .",
    "[ global_max ] ( a ) ) due to the higher number of the available noisy signals .",
    "moreover , comparison of the figures shows that the gap between the optimal performance and the one resulting from the random allocation of the distortion matrices is similar for the 4 sensors and 2 sensors scenarios .",
    "are chosen randomly .",
    "the blue line indicates the output snr at @xmath320 , which gives the optimal solution .",
    "the choice of parameters is similar to the simulation in fig .",
    "[ global_max ] ( a ) . ]",
    "next we study the validity of the high - rate assumption .",
    "notice that even with low rates , the procedure described to find the optimal distortion allocation might give a solution .",
    "the problem , however is that if the rates are not high enough , the approximation in ( [ kkt_result1_aprx ] ) will not hold , and the resulting distortion matrices might violate the sum - rate constraint in ( [ min ] ) . the approximation error is negligible if @xmath321 . the difference between @xmath322 and @xmath243 as a function of @xmath243",
    "is plotted in fig .",
    "[ accuracy ] for @xmath323 , and @xmath324 and @xmath325 for the covariance matrices of the noise terms .",
    "note that for a weighted sum - rate @xmath243 that is as low as @xmath326 nats per block ( equivalent to @xmath327 nats per sample or @xmath328 bits per sample ) the high - rate assumption still holds with a negligible error .",
    "and the resulting weighted sum - rate due to the high - rate approximation as a function of @xmath243 ]      we assume that @xmath330 , which means that all the sources are scalar .",
    "we further assume that there are only two nodes . while the high - rate case studied in the previous subsection",
    "is of interest for practical applications , the scalar case with two nodes may be of less practical relevance .",
    "however , it leads to interesting analytical results that makes it worth studying .",
    "we denote the scalar version of @xmath213 and @xmath252 by @xmath331 and @xmath332 , respectively .",
    "the variance of random variables is denoted by @xmath333 followed by a subscript .",
    "we assume that we are interested in the sum - rate ( so @xmath334 ) and @xmath335 to further simplify the problem for analytical derivations .",
    "applying these assumptions to ( [ kkt1])([kkt_result3 ] ) , and defining @xmath336 and @xmath337 as :    @xmath338    one can show that the target distortions @xmath339 and @xmath340 are given by :    @xmath341    based on this , and defining the following parameters :    @xmath342 ^ 2}{(\\sigma_{n_1 } - \\sigma_1)(\\sigma_{n_2 } - \\sigma_2)},\\\\ \\label{r2 } & r_{min } = \\frac{1}{4 } \\log \\frac{\\left[\\min(\\sigma_1,\\sigma_2)\\right]^2}{(\\sigma_{n_1 } - \\sigma_1)(\\sigma_{n_2 } - \\sigma_2)},\\end{aligned}\\ ] ]    we prove the following proposition .",
    "[ prop ] the stationary point @xmath343 given by ( [ d1])-([d2 ] ) is the unique maximizer of the output snr , if @xmath344 , and is the unique minimizer , if @xmath345 .",
    "see appendix [ appendix_scalar ]     and @xmath346 for the scalar case with two nodes for three different values of the network sum - rate , ( a ) @xmath347 , ( b ) @xmath348 , and ( c ) @xmath349 .",
    "the circles show the stationary points suggested by the kkt conditions . ]    to observe the behaviour of the output snr as a function of the distortions , consider the following example .",
    "let @xmath350 , @xmath351 , @xmath352 , @xmath353 , @xmath354 , and @xmath355 . from ( [ r1 ] ) and ( [ r2 ] ) we have @xmath356 and @xmath357 .",
    "we make use of the weighted sum - rate constraint @xmath358 to write @xmath359 as a function of @xmath360 or @xmath346 only ( denoted by @xmath361 and @xmath362 , respectively ) .",
    "we plot @xmath361 and @xmath362 for three different values @xmath363 of the weighted sum - rate @xmath243 .",
    "the result is shown in fig .",
    "[ plots ] . as seen in fig .",
    "[ plots ] ( a ) , for @xmath364 the output snr is minimized at @xmath365 , which is not desired .",
    "the maximum snr should then be on the boundary , which means that the whole rate should be given to one of the nodes .",
    "it can be seen in the figure that the highest snr is achieved for the smallest @xmath346 , suggesting that the whole rate should be allocated to node 2 .",
    "this is in agreement with intuition , since node 2 is less noisy than node 1 . for @xmath348",
    ", one could see in fig .",
    "[ plots ] ( b ) that @xmath365 is not a feasible point .",
    "similar to the previous case , the optimal solution should again be on the boundary .",
    "finally , for @xmath366 , @xmath365 maximizes the output snr .",
    "note that the above example suggests that the distortion allocation has a water - filling form .",
    "the critical rate @xmath367 acts as the water level . if @xmath243 is above this level , the rate is split between the nodes based on their noise levels .",
    "if @xmath243 is below the water level , the noisier node is omitted , and the whole rate is given to the other node .",
    "we considered a source coding problem in a networked setup under covariance matrix distortion constraints .",
    "we modelled the problem as a vector gaussian remote wyner - ziv problem and solved the problem by deriving an explicit formula for the rate - distortion function and designing coding schemes that asymptotically achieve the rate - distortion function .",
    "we then studied some applications of the results .",
    "in particular , we showed that the rate - distortion function for the equivalent wyner - ziv problem with mean - squared error distortions and the rate - information function modelling a relay network source coding problem are special cases of our results .",
    "finally , we considered a centralized sensor network with a weighted sum - rate constraint where each node transmits its observation with a certain rate and distortion , and the received data is fused at the center to maximize the output snr without enforcing linear distortions . for this problem we bridged between noise reduction and source coding by showing that the distortion matrices and the rates at the individual nodes could be designed to maximize the output snr at the center .",
    "we considered special cases such as the high - rate case or the case of scalar sources in order to obtain analytical results .",
    "further work could be other possible special cases of the noise reduction problem where analytical results can be obtained .    [ [ appendix2 ] ]    from the fact that @xmath82 is uncorrelated with @xmath1 and @xmath2 and :    @xmath368    it follows that :    @xmath369    where @xmath370 and @xmath371 are defined as :    @xmath372",
    "we start from the following chain of inequalities :    @xmath373    where ( [ lb2 ] ) is because the knowledge of the side information @xmath2 at the encoder can not increase the rate , ( [ lb3 ] ) is because from ( [ xyz ] ) , when @xmath2 is given , @xmath87 is a sufficient statistic of @xmath1 for the estimation of @xmath0 , ( [ lb4 ] ) follows from ( [ yprime ] ) and the data processing inequality , ( [ lb5 ] ) is because choosing @xmath374 to be gaussian maximizes the differential entropy , and we have added the constraint @xmath375 , since any valid @xmath376 must satisfy this condition , ( [ lb6 ] ) follows from ( [ concov1])([concov2 ] ) , and @xmath377 is defined as :    @xmath378    from lemma [ lem_properties2 ] it follows that @xmath379 . substituting this and ( [ concov1 ] ) in ( [ lb7 ] ) yields :",
    "@xmath380      based on the results in @xcite , it is enough to show that for @xmath381 defined in ( [ scheme ] ) , we have @xmath382 , and the covariance matrix @xmath383 of the reconstruction error satisfies @xmath384 .",
    "we start with the following chain of equalities :    @xmath385    where ( [ ub3 ] ) follows from ( [ scheme ] ) and ( [ yprime ] ) , and ( [ ub4 ] ) follows from ( [ concov1 ] ) . rewriting @xmath386 as :    @xmath387    and substituting ( [ ub6 ] ) and ( [ cov_noise ] ) in ( [ ub4 ] ) and simplifying the result",
    ", we get :    @xmath388    next , we derive the covariance matrix of the reconstruction error .",
    "similar to ( [ xyz ] ) , we can write @xmath389 as :    @xmath390    where @xmath391 is independent of @xmath381 and @xmath2 . form ( [ xuz ] ) it follows that @xmath392 , or :    @xmath393    from ( [ scheme ] ) , ( [ yprime ] ) , and ( [ concov1 ] ) we have :    @xmath394    also note that :    @xmath395    where ( [ weird3 ] ) , ( [ weird4 ] ) , ( [ weird5 ] ) and ( [ weird6 ] ) follow from ( [ scheme ] ) , ( [ xyz ] ) , ( [ yprime ] ) and ( [ concov1 ] ) , respectively . the covariance matrix of the reconstruction error can then be written as :    @xmath396    where ( [ err1 ] ) and ( [ err2 ] ) follow from ( [ xuz ] ) , ( [ err3 ] ) is the result of substituting ( [ weird1 ] ) in ( [ err2 ] ) , ( [ err4 ] ) follows from ( [ weird2 ] ) and ( [ weird6 ] ) , ( [ err6 ] ) follows from ( [ joint1 ] ) , and ( [ err7 ] ) follows from ( [ ub6 ] ) and ( [ cov_noise ] ) .    finally , rewriting ( [ err9 ] ) as :    @xmath397    and using property 3 in lemma [ lem_properties ] , it is clear that @xmath384 , as desired .",
    "this completes the proof .",
    "we start with defining @xmath85 as in ( [ kkt_result2 ] ) . writing @xmath213 in terms of @xmath255 using ( [ kkt1 ] ) , and substituting in the constraint in ( [ min2 ] ) yields ( [ kkt_result3 ] ) .",
    "we derive ( [ kkt_result1 ] ) to complete the proof . applying the matrix inversion lemma to @xmath398}^{-1}$ ] in ( [ min2 ] ) , and using the definitions ( [ kkt1 ] ) and ( [ kkt_result2 ] ) , we rewrite the optimization problem as follows :    @xmath399    for a given @xmath400",
    ", we differentiate the lagrangian of ( [ min3 ] ) with respect to @xmath255 .",
    "to do so , we first apply a logarithm function to the constraint in ( [ min3 ] ) , and then write the lagrangian form of the problem as :    @xmath401    where @xmath402 and @xmath403 are defined as :    @xmath404    note that from ( [ kkt_result2 ] ) it follows that @xmath402 does not depend on @xmath255 . using ( [ kkt2 ] ) , we rewrite @xmath403 as :    @xmath405    substituting ( [ fj2 ] ) in ( [ lagrange ] ) , differentiating with respect to @xmath255 while taking into account that @xmath255 is symmetric @xcite , and setting the derivative equal to @xmath306 yields :    @xmath406 \\nonumber \\\\ \\label{deriv_hadamard } & - \\lambda \\alpha_i \\left [ { \\bf{z}}_i^{-1 } + \\left ( { \\bf{c}}_i - { \\bf{z}}_i \\right)^{-1 } \\right ] \\circ { \\bf{i}}_n = { \\bf{0}},\\end{aligned}\\ ] ]    where @xmath407 denotes the hadamard product . from ( [ deriv_hadamard ] )",
    "it follows that :    @xmath408 = { \\bf{0}}.\\ ] ]    replacing @xmath409 by @xmath85 in ( [ deriv ] ) and simplifying the result yields ( [ kkt_result1 ] ) .",
    "let us define :      using basic calculus one can show that @xmath411 is a monotonically increasing and strictly concave function of @xmath142 , which varies from @xmath306 to @xmath412 when @xmath142 goes from @xmath306 to @xmath413 .",
    "it then follows that the function @xmath414 is monotonically increasing with the range @xmath415 .",
    "one could then see that @xmath416 has one root , if and only if :                          using the constraint in ( [ scalar2 ] ) , we then write @xmath424 in terms of @xmath425 and substitute it in the cost function to obtain an unconstrained function of @xmath425 .",
    "we call this function @xmath426 . differentiating @xmath426 with respect to @xmath425 yields :    @xmath427 \\left [ ( \\sqrt{\\beta ' } \\!-\\ ! \\sigma_2)d'_1 \\!-\\ ! ( \\beta ' \\!-\\ ! \\sigma_1 \\sigma_2 ) \\right]}{\\left ( d'_1 \\right)^2 \\left ( \\sigma_2 d'_1 \\!-\\ !",
    "\\sigma_1 \\sigma_2 \\!+\\ ! \\beta ' \\right)^2 } , \\end{aligned}\\ ] ]    which is zero at @xmath428 ( equivalent to @xmath429 using ( [ change_var ] ) ) .",
    "the sign of the derivative around this point determines whether there is an extremum or not .",
    "the denominator and the first term in the numerator in ( [ diff ] ) are positive .",
    "we thus study only the second term in the numerator .",
    "we replace @xmath425 by @xmath430 .",
    "the result is @xmath431 , which implies that if @xmath432 , the derivative is positive for @xmath433 and negative for @xmath434 .",
    "this means that if @xmath432 , the minimum is at @xmath435 , or equivalently @xmath429 is the global maximizer of the snr .",
    "similarly , one could write @xmath425 in terms of @xmath424 and substitute in the cost function to obtain @xmath436 . considering the sign of",
    "the derivative around @xmath437 leads to the conclusion that if @xmath438 , the snr is maximized at @xmath437 . combining the two conditions , the following should hold in order to have the snr maximized at @xmath439 :                          s. c. draper and g. w. wornell , _ side information aware coding strategies for estimation under communication constraints _ , ieee journal on selected areas in communications , vol .",
    "6 , pp . 1 - 11 , aug .",
    "2004 .        c. tian and j. chen , _ remote vector gaussian source coding with decoder side information under mutual information and distortion constraints _ , ieee transactions on information theory , vol .",
    "10 , pp.4676 - 4680 , oct .",
    "a. zahedi , j. stergaard , s. h. jensen , p. naylor , and s. bech , _ distributed remote vector gaussian source coding with covariance distortion constraints _ , ieee international symposium on information theory ( isit ) , honolulu , hi , usa , jul .",
    "2014 .",
    "a. lapidoth and i .- h .",
    "wang , _ communicating remote gaussian sources over gaussian multiple access channels _ , ieee international symposium on information theory ( isit ) , saint petersburg , russia , aug . 2011 .",
    "e. i. silva , m. s. derpich , and j. stergaard , _ an achievable data - rate region subject to a stationary performance constraint for lti plants _ , ieee transactions on automatic control , vol .",
    "56 , no . 8 , pp .",
    "1968 - 1973 , aug . 2011 .",
    "adel zahedi received the m.sc .",
    "degree from iran university of science and technology , iran in 2011 , and the ph.d .",
    "degree from aalborg university , denmark in 2016 .",
    "he is currently a postdoctoral researcher at aalborg university .",
    "jan stergaard ( s98m99sm11 ) received the m.sc.e.e .",
    "degree from aalborg university , aalborg , denmark , in 1999 and the ph.d .",
    "degree ( _ cum laude _ ) from delft university of technology , delft , the netherlands , in 2007 .    from 1999 to 2002 , he worked as an r&d engineer at eti a / s , aalborg , and from 2002 to 2003 , he was an r&d engineer with eti inc . , va , usa . between september 2007 and june 2008",
    ", he was a postdoctoral researcher at the university of newcastle , nsw , australia . from june 2008 to march 2011 , he was a postdoctoral researcher / assistant professor at aalborg university , and , since 2011 , has been an associate professor with the same university .",
    "he has been a visiting researcher at tel aviv university , tel aviv , israel , and at the universidad tcnica federico santa mara , valparaso , chile .",
    "dr .  stergaard received a danish independent research council s young researcher s award , a best ph.d .",
    "thesis award by the european association for signal processing ( eurasip ) , and fellowships from the danish independent research council and the villum foundation s young investigator programme .",
    "he is an associate editor of the eurasip journal on _ advances in signal processing",
    "_    sren holdt jensen ( s87-m88-sm00 ) received the m.sc .",
    "degree in electrical engineering from aalborg university , aalborg , denmark , in 1988 , and the ph.d .",
    "degree in signal processing from the technical university of denmark , lyngby , denmark , in 1995 . before joining the department of electronic systems of aalborg university ,",
    "he was with the telecommunications laboratory of telecom denmark , ltd , copenhagen , denmark ; the electronics institute of the technical university of denmark ; the scientific computing group of danish computing center for research and education ( unic ) , lyngby ; the electrical engineering department of katholieke universiteit leuven , leuven , belgium ; and the center for personkommunikation ( cpk ) of aalborg university .",
    "he is full professor and heading a research team working in the area of numerical algorithms , optimization , and signal processing for speech and audio processing , image and video processing , multimedia technologies , and digital communications .",
    "jensen was an associate editor for the ieee transactions on signal processing , elsevier signal processing and eurasip journal on advances in signal processing , and is currently associate editor for the ieee / acm transactions on audio , speech and language processing .",
    "he is a recipient of an european community marie curie fellowship , former chairman of the ieee denmark section and the ieee denmark section s signal processing chapter .",
    "he is member of the danish academy of technical sciences and was in january 2011 appointed as member of the danish council for independent research ",
    "technology and production sciences by the danish minister for science , technology and innovation .",
    "patrick naylor ( m89 , sm07 ) received his beng degree in electronic and electrical engineering from the university of sheffield , u.k .",
    ", in 1986 and the phd .",
    "degree from imperial college , london , u.k . , in 1990 . since 1990",
    "he has been a member of academic staff in the department of electrical and electronic engineering at imperial college london .",
    "his research interests are in the areas of speech , audio and acoustic signal processing .",
    "he has worked in particular on adaptive signal processing for dereverberation , blind multichannel system identification and equalization , acoustic echo control , speech quality estimation and classification , single and multi - channel speech enhancement and speech production modelling with particular focus on the analysis of the voice source signal .",
    "in addition to his academic research , he enjoys several fruitful links with industry in the uk , usa and in mainland europe .",
    "he is the chair of the ieee signal processing society technical committee on audio and acoustic signal processing , a director of the european association for signal processing ( eurasip ) and formerly an associate editor of ieee signal processing letters and ieee transactions on audio speech and language processing .",
    "sren bech received a m.sc . and a ph.d . from the department of acoustic technology ( at ) of the technical university of denmark .",
    "from 1982 to 1992 he was a research fellow at at studying perception and evaluation of reproduced sound in small rooms . in 1992",
    "he joined bang & olufsen where he is currently head of research . in 2011",
    "he was appointed professor in audio perception at aalborg university .",
    "his research interest includes human perception of reproduced sound in small and medium sized rooms .",
    "experimental procedures and statistical analysis of data from sensory analysis of audio and video quality .",
    "general perception of sound in small rooms is also a major research interest ."
  ],
  "abstract_text": [
    "<S> we consider a source coding problem with a network scenario in mind , and formulate it as a remote vector gaussian wyner - ziv problem under covariance matrix distortions . </S>",
    "<S> we define a notion of minimum for two positive - definite matrices based on which we derive an explicit formula for the rate - distortion function ( rdf ) . </S>",
    "<S> we then study the special cases and applications of this result . </S>",
    "<S> we show that two well - studied source coding problems , i.e. remote vector gaussian wyner - ziv problems with mean - squared error and mutual information constraints are in fact special cases of our results . finally , we apply our results to a joint source coding and denoising problem . </S>",
    "<S> we consider a network with a centralized topology and a given weighted sum - rate constraint , where the received signals at the center are to be fused to maximize the output snr while enforcing no linear distortion . </S>",
    "<S> we show that one can design the distortion matrices at the nodes in order to maximize the output snr at the fusion center . </S>",
    "<S> we thereby bridge between denoising and source coding within this setup .    </S>",
    "<S> source coding , sensor networks , covariance matrix distortions , rate - distortion functions , noise reduction </S>"
  ]
}