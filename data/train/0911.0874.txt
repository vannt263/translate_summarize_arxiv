{
  "article_text": [
    "two - player zero - sum games play a fundamental role in game theory because their analysis is straightforward . the min - max theorem of von neumann @xcite",
    "establishes a unique value of the game .",
    "figure [ figure simple game ] shows the payoff matrix for a basic game .",
    "since neither of the pure strategies for player a dominates the other , a mixed strategy is optimal .",
    "player a will choose action 0 with probability 1/4 and action 1 with probability 3/4 . by doing this , the expected payoff for player a is 3/4 no matter which strategy player b chooses .    [ ] [ ] [ 0.9]@xmath0 [ ] [ ] [ 0.9]@xmath1 [ ] [ ] [ 0.9]@xmath1 [ ] [ ] [ 0.9]@xmath2 [ ] [ ] [ 0.9]0 [ ] [ ] [ 0.9]1 [ ] [ ] [ 0.9]0 [ ] [ ] [ 0.9]1 [ ] [ ] [ 1]player b [ ] [ ] [ 1]player a [ ] [ ] [ 0.9]@xmath3     in a bayesian game , the payoff matrix , which we represent by @xmath4 over the domain of pure strategies @xmath5 and @xmath6 , is a random quantity .",
    "we can model this as a game with a random state @xmath7 that determines the payoffs , referred to in the literature as the `` type . ''",
    "the value of such a game depends on the information known to the players . if the neither player knows @xmath7 ( aside from its distribution ) , then the value of the game can be derived from the expected value of the payoff matrix .",
    "additionally , if both players know @xmath7 then the value can be derived from the payoff matrix associated with every instance of @xmath7 and averaged .",
    "more interesting cases occur when only one player knows @xmath7 or when both players have incomplete information about @xmath7 .",
    "consider a situation where different functions of the state @xmath7 are known to both of the players of the game .",
    "these functions can be represented by a partition over the support of @xmath7 known as the information structure , as illustrated in figure [ figure information structure ] .",
    "it has been established that games of this form can be solved by expanding the space of pure strategies to include strategies that depend on the available information .",
    "the min - max theorem still holds .",
    "samples of inquiries into the value of information can be found in the following publications : @xcite , @xcite , @xcite , @xcite . while information can only increase a players optimal score in a game , not all information structures are equal , even if they contain the same quantity of information . for a given resolution ,",
    "what is the optimal information structure ?",
    "[ ] [ ] [ 0.8]distribution of state @xmath7 [ ] [ ] [ 0.8]quantization into bins     in this work we consider a repeated game setting where the state of the game @xmath7 is i.i.d . and",
    "known non - causally to a helper who is assisting one of the players of the game .",
    "the helper can communicate with a rate limit of @xmath8 bits per iteration of the game .",
    "the opposing player may or may not know @xmath7 but is certainly aware of the conspiracy against him . even the protocol for communication is known ( or learned ) by the opposing player .",
    "we establish information theoretic lower - bounds to the optimal performance .",
    "we show through examples that the intuition provided by rate - distortion theory can be misleading in this setting .",
    "we illustrate in figure [ figure erasure game ] a two - player game with a binary , equally distributed state .",
    "player a has three pure strategies , @xmath1 , @xmath9 , and @xmath2 .",
    "player b only has two strategies , @xmath1 , and @xmath2 .",
    "the matrix values represent the payoff player a receives for any given state @xmath7 and pair of actions @xmath10 and @xmath11 ( pure strategies ) . as",
    "this is a zero - sum game , player b receives negative of the payoff of player a. we concern ourselves with average payoff , averaged with respect to probabilities if mixed strategies are involved .",
    "[ ] [ ] [ 0.8]3 [ ] [ ] [ 0.8]0 [ ] [ ] [ 0.8]0 [ ] [ ] [ 0.8]1 [ ] [ ] [ 0.8]@xmath12 [ ] [ ] [ 0.8]@xmath12 [ ] [ ] [ 0.8]0 [ ] [ ] [ 0.8]1 [ ] [ ] [ 0.8]0 [ ] [ ] [ 0.8]e [ ] [ ] [ 0.8]1 [ ] [ ] [ 0.8]player b [ ] [ ] [ 0.8]player a [ ] [ ] [ 0.8 ] [ ] [ ] [ 0.8]@xmath13 [ ] [ ] [ 0.8]@xmath14   or @xmath15 with probability 1 . therefore , if player a does not know the state , @xmath15 is the only choice.,title=\"fig : \" ]    notice that player a will at all costs avoid choosing a pure strategy of @xmath16 when the state is @xmath13 , or vice versa , hence the label `` erasure game . ''",
    "the same consequences would result from a finite but greatly negative payoff in place of @xmath12 .",
    "] so if player a is ignorant of the state @xmath7 , he has no option but to choose @xmath9 with probability one .",
    "we can therefore deduce the value of the game for two of the four information structures in table [ table erasure game value ] .",
    "if player b does not know the state of the game , then on average a will get a payoff of 1/2 .",
    "if player b does know the state @xmath7 then he will choose the strategy @xmath17 , resulting in a payoff of zero .",
    "[ table erasure game value ]    .value of erasure game under varying information structures [ cols=\"<,>\",options=\"header \" , ]     on the other hand , if player a knows the state @xmath7 while player b does not , then equilibrium will occur with player a choosing @xmath18 and scoring @xmath19 on average .",
    "finally , we can consider the case where they both know the state . as we observed in the game of figure",
    "[ figure simple game ] , player a will choose the mixed strategy consisting of @xmath18 with probability 1/4 and @xmath15 with probability 3/4 .",
    "player b will choose @xmath17 with probability 1/4 and @xmath20 with probability 3/4 .",
    "the resulting average payoff is 3/4 .    in the `` erasure game ''",
    "the state is binary , so these four information structures mentioned exhaust all deterministic information structures .",
    "no intriguing optimization problem presents itself in only one iteration of the game .",
    "for example , we ca nt ask , `` what is the best information structure for player a that has cardinality two ? ''",
    "there is only one choice of information structure .",
    "fortunately , a more graceful spectrum of information structures is available with vector quantization .",
    "consider a repeated - game setting of the erasure game where a helper observes the state of the game and communicates to player a over a rate - limited channel .",
    "both players know all past actions and states .",
    "additionally , the helper , and possibly player b , observe the state completely and non - causally .",
    "the helper and player a select a block length @xmath21 and arrange a protocol by which the helper will send @xmath22 bits describing the state to player a. player a will then play the game for @xmath21 iterations .",
    "player b has full knowledge of the protocol but does not actually see the message .",
    "we ask for the maximum average payoff that can be achieved for a give rate @xmath8 . in other words ,",
    "what is the max - min value of this game as a function of @xmath8 ?",
    "we begin with a simple case . as table [ table erasure game value ]",
    "shows , if player a and player b both know the state @xmath7 , the value of the game is 3/4 .",
    "but what if player a only learns of the state through communication from a helper , while player b observes the state directly ? what rate of communication is needed for player a to still achieve an average payoff of 3/4 ?",
    "recall that the uniquely optimal strategy for player a is to choose @xmath18 with probability 1/4 and @xmath15 with probability 3/4 .",
    "we can use rate - distortion theory as inspiration for answering this question .",
    "rate - distortion theory prescribes a formula for finding the minimum description rate needed to reconstruct a sequence of observations with limited average distortion .",
    "the procedure is to find the correlation of the source and reconstruction that satisfies the distortion constraint and results in the lowest mutual information . by describing a random source at a rate greater than the mutual information , it is possible to assure with high probability that the reproduction will have the desired correlation with the source as measured by first order statistics .    in the case of the erasure game where player b knows the state @xmath7 ,",
    "suppose we decide to encode the state for player a using a rate - distortion - like code and an erasure test - channel such that the action sequence results in @xmath23 roughly 1/4 of the time and @xmath24 roughly 3/4 of the time .",
    "the rate required is @xmath25 bits per iteration .",
    "unfortunately , this will not result in a good strategy for player a. the encoding schemes that arise in rate - distortion theory are deterministic .",
    "the action sequence @xmath26 is a deterministic function of the state sequence @xmath27 .",
    "since player b observes the state sequence , he will deduce the actions of player a and anticipate them every time .",
    "player a is effectively not playing a mixed strategy .",
    "the resulting payoff will be 0 .",
    "the insufficiency of rate - distortion - like codes is a concern even when player b does not know the state . after watching the actions of player a for roughly @xmath28 iterations",
    "it will be possible to deduce the entire action sequence .",
    "to a clever opponent who does not know the state , the actions will appear random and appropriately distributed for the beginning @xmath29 portion of the block , for large enough @xmath21 ( related to results from @xcite ) , and later in the block , when @xmath30 , the opponent will be able to decode the sequence with high probability and anticipate every action .",
    "the bottom line is that rate - distortion - like codes place no emphasis on producing random actions .",
    "the work in @xcite prescribes an encoding scheme for generating correlated random variables . the minimum description rate for the state sequence @xmath27 needed to produce a sequence of actions @xmath26 with a distribution that is arbitrary close in total variation to the desired mixed strategy is wyner s common information @xmath31 , defined in @xcite .",
    "the resulting encoding scheme for producing this `` strong coordination '' @xcite between the state @xmath7 and the action @xmath10 uses randomized encoding and randomized decoding .",
    "figure [ figure encoding diagram ] illustrates that the encoder uses the message to specify the index of a sequence @xmath32 from a predefined codebook , just as in rate - distortion - like codes , but here the sequences do not represent reconstruction sequences . after the decoder identifies the sequence @xmath32 he produces the actions @xmath26 randomly as the output of a memoryless channel from @xmath33 to @xmath10 . in this way",
    "the codebook is separated from the action sequence @xmath26 , allowing more randomness to be injected into the actions .",
    "[ ] [ ] [ 0.8]@xmath34 [ ] [ ] [ 0.8]@xmath35 [ ] [ ] [ 0.8]@xmath36 [ ] [ ] [ .6]x [ ] [ ] [ .4]x [ ] [ ] [ .4]x [ ] [ ] [ .4]x [ ] [ ] [ .4 ]   to allow an action sequence @xmath36 to be random even to an observer of the state .",
    "an auxiliary variable @xmath33 is chosen which separates @xmath7 and @xmath10 into a markov chain , and a codebook of @xmath35 sequences is agreed upon .",
    "the state sequence @xmath34 is a random realization .",
    "the encoder observes @xmath34 and randomly chooses among jointly typical @xmath35 sequences from the codebook .",
    "after sending the index of the @xmath35 sequence to the decoder , an action sequence is generated randomly conditioned on @xmath35 . if the codebook is populated densely enough , the actions @xmath36 will be memoryless and appropriately correlated with the state sequence @xmath34 .",
    "the required density of the codebook is a topic visited in @xcite , @xcite , @xcite , and @xcite.,title=\"fig:\",scaledwidth=35.0% ]    the common information for a binary erasure channel can be found in @xcite .",
    "thus , the minimum description rate needed for a helper to describe the state @xmath7 to player a in order to achieve the optimal average payoff in the erasure game when player b knows the state is @xmath37 , where @xmath38 is the binary entropy function .",
    "just as the information structure in a bayesian game defines the set of pure strategies for each player , the description rate of the state that a helper is allowed to use also defines the set of block strategies allowed , which may have structure and memory from one game iteration to the next .",
    "we know from implications in @xcite that the set of achievable stationary strategies is the set of conditional distribution @xmath39 that yield @xmath40",
    ". however , stationary strategies are not the only strategies worth considering .",
    "a degenerate bayesian game is represented by the payoff matrices in figure [ figure degenerate game ] . in this game player",
    "b has only one pure strategy .",
    "player a is not really playing against an opponent but is simply maximizing the average value of a function of the state @xmath7 and action @xmath10 .",
    "in fact , the payoffs in this game are simply the negative hamming distortion between @xmath7 and @xmath10 ; therefore , the rate - value tradeoff for this game is a canonical example from rate - distortion theory .",
    "a payoff @xmath41 is achievable at rate @xmath8 if @xmath42 for @xmath43 , where @xmath38 is the binary entropy function .    [ ] [ ] [ 0.8]0 [ ] [ ] [ 0.8]-1 [ ] [ ] [ 0.8]0 [ ] [ ] [ 0.8]0 [ ] [ ] [ 0.8]1 [ ] [ ] [ 0.8]player b [ ] [ ] [ 0.8]player a [ ] [ ] [ 0.8 ] [ ] [ ] [ 0.8]@xmath13 [ ] [ ] [ 0.8]@xmath14 .",
    "the rate - value tradeoff of this degenerate game can be cast as a rate - distortion problem .",
    "randomized actions for player a are unnecessary.,title=\"fig : \" ]    in this degenerate bayesian game the optimal strategy has structure and predictability .",
    "there is no adversary to compete with , so producing a random sequence of actions is unnecessary .",
    "these degenerate games simplify to rate - distortion problems and poignantly highlight situations where fully generating correlated random variables is not necessary .",
    "the state of a bayesian game should be encoded by a helper in such a way that allows a random mixed strategy to be correlated with the state , even if that strategy is not entirely unpredictable for the duration of the communication block .",
    "one way to achieve this is to follow the encoding procedure used to generate correlated random variables , as in figure [ figure encoding diagram ] , but use a smaller codebook .",
    "specifically , a codebook is constructed by first choosing a conditional distribution @xmath44 and generating @xmath45 sequences @xmath46 independently for each @xmath47 and i.i.d . according to @xmath48 .",
    "the encoder chooses randomly ( according to the appropriate distribution prescribed in @xcite ) from all sequences @xmath46 that are jointly typical with the state and sends the index @xmath49 to the decoder .",
    "the decoder then constructs the sequence @xmath46 from the index @xmath49 and synthesizes a memoryless channel according to @xmath50 to produce an action sequence @xmath26 . if the rate @xmath51 then this would produce a memoryless strategy , meaning that the opponent could not use observations of the past actions and states to infer about future actions",
    "however , by letting @xmath52 we also allow for situations where memoryless strategies are not of the essence .    in order for the encoder to find at least one jointly typical @xmath46 sequence in the codebook with high probability , a rate requirement of @xmath53 is necessary . beyond that , any excess rate will serve to randomize the actions for a portion of the encoding block . at first",
    "the actions will appear random and correlated with the state .",
    "after observing a designated fraction of the block , the opponent will be able to deduce the index of the message using a channel decoder , revealing the future of the @xmath46 sequence , from which the future actions will be randomly generated .",
    "the transition from player b knowing nothing about the next action @xmath10 to the point where player b can decode @xmath46 with high probability becomes sharper as the block length @xmath21 grows .",
    "let @xmath54 be the transition threshold such that for the first @xmath55 iterations where @xmath56 the actions are random according to the mixed strategy @xmath39 and for the final @xmath55 iterations where @xmath57 the sequence @xmath46 is known with high probability , in the limit as @xmath21 is large .",
    "then for the case where player b does not know the state sequence , @xmath58 .",
    "for the case where player b knows the state sequence , @xmath59 .",
    "let us designate some notation to represent the score player a achieves in a game under different settings .",
    "let @xmath60 represent the minimum average payoff player a receives by playing a strategy @xmath39 when player b does not know the state @xmath7 .",
    "when player b does know the state , we use a superscript to indicate this and represent the minimum average payoff for player a by @xmath61 .",
    "additionally , when an auxiliary random variable @xmath33 is involved in constructing the action @xmath10 such that @xmath44 and player b knows @xmath33 , we represent the minimum average payoff for player b as @xmath62 or @xmath63 , depending on whether or not player b also knows the state .",
    "each of these values can be computed from the payoff matrix @xmath64 : @xmath65    [ theorem main ] if player b does not know the state sequence , an average payoff @xmath41 for player a is achievable with a state - information description rate of @xmath8 if @xmath66 if player b knows the state sequence , an average payoff @xmath41 for player a is achievable with a state - information description rate of @xmath8 if @xmath67    the lower bound of theorem [ theorem main ] accommodates settings where full randomization is needed , such as optimal play in the erasure game when player b knows the state , and it also accommodates efficient communication for degenerate games where the payoff does not change when the opponent learns about the action @xmath10 .",
    "it is not clear , however , whether or not this gives the whole tradeoff for sub - optimal play in games where the communication limits do not allow for ideal randomization .",
    "for example , what is the value of the erasure game when player b knows the state sequence and @xmath68 , where @xmath38 is the binary entropy function ?    if the bound in theorem [ theorem main ] is not tight , a couple of ideas come to mind for improvement .",
    "one is to use an encoding method that is not stationary but moves from one strategy to another as the opponent learns . in game settings ,",
    "time - sharing must be analyzed carefully .",
    "the performance depends on whether the time - sharing is interleaved or not .",
    "a related idea is to use a layered encoding scheme , so that the opponent learns the message a little bit at a time .",
    "a possible layered approach follows .    the helper who observes the state can send a message in layers to player a by selecting two auxiliary random variables @xmath69 and @xmath70 and an action @xmath10 such that @xmath71 form a markov chain .",
    "a codebook of @xmath72 sequences of size @xmath73 is generated from the i.i.d .",
    "distribution , and for each of these sequences a codebook of @xmath74 sequences of size @xmath75 is generated conditioned on @xmath72 .",
    "we let @xmath76 be small .",
    "the encoder first finds a @xmath72 sequence that is appropriately correlated with the state sequence @xmath34 and then chooses randomly from the @xmath74 sequences in the conditional codebook that are appropriately correlated with both @xmath72 and @xmath34 .",
    "the decoder constructs @xmath72 and @xmath74 from the message and synthesizes a memoryless channel to generate the action sequence @xmath36 from @xmath72 and @xmath74 .    using this encoding scheme ,",
    "the block will be divided into three sections partitioned by @xmath77 and @xmath78 , where the knowledge of player b transitions sharply at these thresholds in the limit as @xmath21 becomes large .",
    "for the first @xmath55 iterations where @xmath79 the actions are random according to the mixed strategy @xmath39 .",
    "for the middle @xmath55 iterations where @xmath80 the opponent knows @xmath69 , which is correlated with @xmath10 , and for the final @xmath55 iterations where @xmath81 the opponent knows both @xmath69 and @xmath70 . in the setting",
    "where player b does not know the state sequence , @xmath82 and @xmath83 as long as @xmath84 . in the setting where player b knows the state sequence , @xmath85 and @xmath86 as long as @xmath87 .",
    "then this layered encoding scheme does not provide any benefit over theorem [ theorem main ] . and with either assumption about player b , if @xmath88 , the encoding scheme can be modified to increase @xmath77 . ]",
    "we omit the explicit lower bound that is obtained from this encoding scheme .",
    "it is a straightforward derivation and provides little additional insight .",
    "however , if a layered scheme proves to provide improvement over theorem [ theorem main ] , then the idea of adding additional layers with additional auxiliary variables comes to mind .",
    "each additional variable would introduce a new phase of performance in the game .",
    "this quickly becomes cumbersome .",
    "perhaps instead there is a smooth way of adjusting the strategy as the game proceeds and the opponent infers more about the communication .",
    "we have considered partial state information in a bayesian game from an optimization perspective .",
    "if a limited amount of state information is passed by a helper to one of the players in a two player zero - sum repeated bayesian game , how much can it increase the value of the game ?",
    "the description of the state can be used to correlate the actions in the game with the state . in some settings ,",
    "mutual information is the description rate needed to adequately correlate behavior .",
    "but in the adversarial setting of games , a rate - distortion - like compression of the state information at a rate equal to the mutual information results in behavior that is predictable by the opponent .",
    "on the other hand , wyner s common information has been shown to be the description rate needed to fully correlate actions in a completely unpredictable way . however , this can be more than necessary .",
    "we introduce a communication scheme that performs efficiently ( theorem [ theorem main ] ) in the two extreme cases , where memoryless randomization is essential and where it is irrelevant . although the communication is based on i.i.d .",
    "codebooks , the performance in the game changes dramatically mid - way through the communication block as the opponent infers the compressed message",
    ". the non - stationary performance of a stationary encoding scheme introduces new challenges in the quest for efficient compression .",
    "1 j. von neumann .",
    "`` zur theorie der gesellschaftsspiele . ''",
    "_ mathematische annalen _ , 100 ( 1928 ) : 295 - 320 .",
    "o. gossner .",
    "`` ability and knowledge . '' to appear in _ games and economic behavior_. i. gilboa and e. lehrer .",
    "`` the value of information - an axiomatic approach . '' _ journal of mathematical economics _ , 20 ( 1991 ) : 443 - 459 .",
    "e. shmaya .",
    "`` the value of information structures in zero - sum games with lack of information on one side . '' _ internation journal of game theory _ , 34 ( 2006 ) : 155 - 165 .",
    "e. lehrer and d. rosenberg .",
    "`` what restrictions do bayesian games impose on the value of information ? '' _ journal of mathematical economics _ , 42 ( 2006 ) : 343 - 357 .",
    "`` communication requirements for generating correlated random variables . ''",
    "isit 2008 , toronto .",
    "`` the common information of two dependent random variables . '' _ ieee trans . on info .",
    "it-21 , no .",
    "2 , march 1975 .",
    "p. cuff , h. permuter , and t. cover .",
    "`` coordination capacity . ''",
    "submitted to _",
    "ieee trans . on info .",
    "theory _ , 2009 .",
    "t. han and s. verd , `` approximation theory of output statistics , '' _ ieee trans .",
    "on info . theory _ , vol .",
    "3 , may 1993 ."
  ],
  "abstract_text": [
    "<S> two - player zero - sum repeated games are well understood . computing the value of such a game is straightforward . additionally , </S>",
    "<S> if the payoffs are dependent on a random state of the game known to one , both , or neither of the players , the resulting value of the game has been analyzed under the framework of bayesian games . </S>",
    "<S> this investigation considers the optimal performance in a game when a helper is transmitting state information to one of the players .    </S>",
    "<S> encoding information for an adversarial setting ( game ) requires a different result than rate - distortion theory provides . </S>",
    "<S> game theory has accentuated the importance of randomization ( mixed strategy ) , which does not find a significant role in most communication modems and source coding codecs . </S>",
    "<S> higher rates of communication , used in the right way , allow the message to include the necessary random component useful in games . </S>"
  ]
}