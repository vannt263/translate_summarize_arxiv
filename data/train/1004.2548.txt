{
  "article_text": [
    "the distribution - free chain ladder model ( dfcl ) of mack @xcite is a popular model for stochastic claims reserving . in this paper",
    "we use a time series formulation of the dfcl model which allows for bootstrapping the claims reserves .",
    "an important aspect of this model is that it can provide a justification for the classical deterministic chain ladder ( cl ) algorithm which originally was not founded on an underlying stochastic model .",
    "moreover , it allows for the study of prediction uncertainties .",
    "note that there are different stochastic models that lead to the cl reserves ( see for example wthrich - merz @xcite , section 3.2 ) . in the present paper",
    "we use the dfcl formulation to reproduce the cl reserves .",
    "the paper presents a novel methodology for estimating a bayesian dfcl model utilising a framework of approximate bayesian computation ( abc ) in a non - standard manner .",
    "a methodology utilising markov chain monte carlo ( mcmc ) , abc and a bayesian bootstrap procedure is developed in a distribution - free setting .",
    "the abc framework is required because we work in a distribution - free setting in which we make no parametric assumptions about the form of the likelihood .",
    "effectively , the abc methodology allows us to overcome the fact that we can not evaluate the likelihood point - wise in the dfcl model .",
    "typically , abc methodology circumvents likelihood evaluations by simulation from the likelihood .",
    "however , in this case simulation from the likelihood model is not directly available because no parametric assumption is made .",
    "we combine abc methodology with bootstrap to overcome this additional complexity that the dfcl model presents in the abc framework .",
    "then , by using an mcmc numerical sampling algorithm combined with the novel version of abc that has the embedded bootstrap procedure , we are able to obtain samples from the intractable posterior distribution of the dfcl model parameters .",
    "this allows us to utilise this methodology to obtain the bayesian posterior distribution of the dfcl model parameters empirically .",
    "then we demonstrate two approaches in which we can utilise the posterior samples for the dfcl model parameters to obtain the bayesian predictive distribution of the claims .",
    "the first approach involves using each posterior sample to numerically estimate the full predictive claims distribution given the observed claims .",
    "the alternative approach involves using the posterior samples for the dfcl model parameters to form bayesian point estimators .",
    "then , conditional on these point estimators , we can obtain the bayesian conditional predictive distribution for the claims .",
    "the second approach will be relevant for comparisons with the classical and credibility approaches .",
    "the first approach has the benefit that it integrates out of the bayesian predictive claims distribution the parameter uncertainty associated with estimation of the dfcl chain ladder parameters .",
    "the paper then analyses the parameter estimates in the dfcl model , the associated claims reserves and the mean square errors of prediction ( msep ) from both the frequentist perspective and a contrasting bayesian view .",
    "in doing so we analyse cl point estimators for parameters of the dfcl model , the resulting estimated reserves and the associated msep from the classical perspective .",
    "these include non - parametric bootstrap estimated prediction errors which can be obtained via one of two possible bootstrap procedures , conditional or unconditional . in this paper",
    "we consider the process of conditional back propagation ; see @xcite for in - depth discussion .",
    "these classical frequentist estimators are then compared to bayesian point estimators .",
    "the bayesian estimates considered are the maximum _ a posteriori _ ( map ) and the minimum mean square error ( mmse ) estimators . for comparison with the classical frequentist reserve estimates",
    ", we also obtain the associated bayesian estimated reserves conditional upon the bayesian point estimators .",
    "in addition , since in the bayesian setting we obtain samples from the posterior for the parameters we use these along with the msep obtained by the estimated bayesian point estimators to obtain associated posterior predictive intervals to be compared with the classical bootstrap procedures .",
    "we then robustify the prediction of reserves by rao - blackwellization , that is , we integrate out the influence of the unknown variance parameters in the dfcl model .",
    "having done this , we analyse the resultant msep .",
    "this is again only achievable since in the bayesian setting we obtain samples from the joint posterior for the cl factors and the variances .    to summarize our contribution",
    ", the novelty within this paper involves the development and comparison of a new estimation methodology to work with the bayesian cl model for the dfcl model which makes no parametric assumptions on the form of the likelihood function ; see also gisler - wthrich @xcite .",
    "this is unlike the works of yao @xcite and peters et al .",
    "@xcite that assume explicit distributions in order to construct the posterior distributions in the bayesian context .",
    "instead we demonstrate how to work directly with the intractable likelihood functions and the resulting intractable posterior distribution , using novel abc methodology . in this regard",
    "we demonstrate that we do not need to make any parametric assumptions to perform posterior inference , avoiding potentially poor model assumptions made , as for example in the paper of yao @xcite .",
    "* outline of this paper . * the paper begins with a presentation of the claims reserving problem and then presents the model we shall consider .",
    "this is followed by the description of the classical cl algorithm and the construction of a bayesian model that can be used to estimate the parameters of the model .",
    "the bayesian model is constructed in a distribution - free setting .",
    "this is followed by a discussion on classical versus bayesian parameter estimators along with a bootstrap based procedure for the estimation of the parameter uncertainty in the classical setting .",
    "the next section presents the methodology of abc coupled with a novel bootstrap based sampling procedure which will allow us to work directly with the distribution - free bayesian model .",
    "we then illustrate the developed algorithm on a synthetic data set and the real data set , comparing performance to the classical results and those obtained via credibility theory .",
    "we briefly outline the claims development triangle structure we utilise in the formulation of our models .",
    "assume there is a run - off triangle containing claims development data with the structure given in table 1 .",
    "[ table : tab.1 claims development triangle ]    .claims development triangles .",
    "[ cols=\"^ , > , > , > , > , > , > , > , > , > , > \" , ]     assume that @xmath0 are cumulative claims with indices @xmath1 and @xmath2 , where @xmath3 denotes the accident year and @xmath4 denotes the development year ( cumulative claims can refer to payments , claims incurred , etc ) .",
    "we make the simplifying assumption that the number of accident years is equal to the number of observed development periods , that is , @xmath5 . at time @xmath6",
    ", we have observations @xmath7and for claims reserving at time @xmath6 we need to predict the future claims @xmath8 moreover , we define the set @xmath9 for @xmath10 , that is , @xmath11 is the first column in table 1 .      in the classical ( deterministic ) chain ladder algorithm",
    "there is no underlying stochastic model .",
    "it is rather a recursive algorithm that is used to estimate the claims reserves and which has proved to give good practical results .",
    "it simply involves the following recursive steps to predict unobserved cumulative claims in @xmath12 .",
    "set @xmath13 and for @xmath14 @xmath15since this is a deterministic algorithm it does not allow for quantification of the uncertainty associated with the predicted reserves . to analyse the associated uncertainty",
    "there are several stochastic models that reproduce the cl reserves ; for example mack s distribution - free chain ladder model @xcite , the over - dispersed poisson model ( see england - verrall @xcite ) or the bayesian chain ladder model ( see gisler - wthrich @xcite ) .",
    "we use a time series formulation of the bayesian chain ladder model in order to use bootstrap methods and bayesian inference .",
    "we use an additive time series version of the bayes chain ladder model ( model assumptions 3.1 in gisler - wthrich @xcite ) .",
    "[ model ass ]     1 .",
    "we define the cl factors by @xmath16 and the standard deviation parameters by @xmath17 .",
    "we assume independence between all these parameters , i.e. the prior density of @xmath18 is given by @xmath19 where @xmath20 denotes the density of @xmath21 and @xmath22 denotes the density of @xmath23 .",
    "conditionally , given @xmath24 and @xmath25 , we have : * cumulative claims @xmath0 in different accident years @xmath3 are independent . *",
    "cumulative claims satisfy the following time series representation @xmath26where conditionally , given @xmath27 , we have that the residuals @xmath28 are i.i.d .  satisfying @xmath29 = 0~\\text { and } ~\\mathrm{var}\\left [ \\varepsilon _ { i , j}|{\\cal b}_0,\\bm{f } , \\bm{\\xi}\\right ] = 1 , \\label{residualassumptionsdfcl}\\ ] ] and @xmath30=1 $ ] for all @xmath31 .    * remark . *",
    "note that the assumptions on the residuals are slightly involved in order to guarantee that cumulative claims @xmath0 are positive @xmath32-a.s .",
    "[ corind ] under model assumptions [ model ass ] we have that conditionally , given @xmath33 , the random variables @xmath34 are independent .",
    "thus , we obtain the following posterior distribution for @xmath35 , given @xmath33 , @xmath36    this result follows from theorem 3.2 in gisler - wthrich @xcite ; from prior independence of the parameters ; and the fact that @xmath37 only depends on @xmath38 , @xmath39 and @xmath0 ( markov property ) .",
    "this has important implications for the abc sampling algorithm developed below .    in order to perform the bayesian analysis we make explicit assumptions on the prior distributions of @xmath35 .",
    "[ model ass2 ]    in addition to model assumptions [ model ass ] we assume that the prior model for all parameters @xmath40 is given by :    * @xmath41 , where @xmath42 is a gamma distribution with mean @xmath43 = \\alpha_j \\beta_j = \\widehat{f}_{j}^{(cl)}$ ] ( see ) and large variance to have diffuse priors . * the variances @xmath44 , where @xmath45 is an inverse gamma distribution with mean @xmath46 = b_j/(a_j-1)=\\widehat{\\sigma } _ { j}^{2(cl ) } $ ] ( see below ) and large variance .",
    "* remarks *    1 .",
    "the likelihood model is intractable , meaning that no density can be written down analytically in the dfcl model . in formulating the bayesian model we have only made distributional assumptions on the priors for the parameters @xmath35 but not on the observable cumulative claims @xmath0 .",
    "though we make distributional assumptions for the priors , the model is distribution - free because no distributional assumptions on the cumulative claims are made . as a result of only making assumptions on the priors , a standard bayesian analysis using analytic posterior distributions can not be performed .",
    "one way out of this dilemma would be to re - formulate the bayesian model by making distributional assumptions ( for example , this is done in yao @xcite ) but then the model is no longer distribution - free .",
    "another approach would be to use credibility methods ( see gisler - wthrich @xcite ) but this only gives statements for the first two moments . in the present",
    "set up we develop abc methods that allow for a full distributional answer for the posterior distributions without making explicit distributional assumptions for the cumulative claims @xmath0 .",
    "our priors are chosen as diffuse priors with large variances .",
    "this again highlights the differences between specification of the prior distributions and making distributional assumptions for the actual likelihood model , these are mutually exclusive ideas .",
    "we select the priors to ensure that we maintain several relevant aspects of the dfcl model .",
    "in particular , it is important to utilise priors that enforce the strict positivity of the parameters @xmath47 .",
    "we note here that the parametric bayesian model developed in yao @xcite failed in this aspect when it came to prior specification .",
    "therefore we develop an alternative prior structure that satisfies these required properties of the dfcl model .",
    "this section considers both classical and bayesian estimators for the chain ladder framework , including both the chain ladder factors and the variance parameters .      in the classical cl method ,",
    "the cl factors are estimated by @xmath48 given in .",
    "the variance parameters are estimated by @xmath49 see ( 3.4 ) in wthrich - merz @xcite .",
    "note that this estimator is only well - defined for @xmath50 .",
    "there is a vast literature and discussion on the estimation of tail parameters .",
    "we do not enter this discussion here but we simply choose the estimator given in mack @xcite for the last variance parameter which is defined by @xmath51      in a bayesian inference context one calculates the posterior distribution of the parameters , given @xmath33 .",
    "as in we denote this posterior by @xmath52 . since the mcmc - abc bootstrap procedure will allow us to obtain samples from the posterior distribution of the bayesian dfcl model presented , we can now consider estimating cl point estimators using these samples .",
    "there are two commonly used point estimators in bayesian analysis that correspond to the posterior mode ( map ) and the posterior mean ( mmse ) , respectively : @xmath53and @xmath54 ,   \\label{mmse_f } \\\\",
    "\\widehat{\\sigma } _ { j}^ { ( mmse ) } & = & \\int ~\\sigma _ { j}~\\pi \\left ( \\sigma _ { j}|\\mathcal{d}_{i}\\right ) d\\sigma _ { j}=e\\left[\\left .",
    "\\xi_j\\right|\\mathcal{d}_i \\right ] .",
    "\\label{mmse_var}\\end{aligned}\\ ] ] in the case in which @xmath55 is not independent of @xmath56 , the map estimators obtained through joint maximization are optimal .",
    "however , in practice one often works with marginal estimators for simplicity . additionally , note that for diffuse priors we find ( see corollary 5.1 in gisler - wthrich @xcite )",
    "@xmath57 hence , using corollary [ corind ] , we obtain the approximation @xmath58 & = & e\\left[\\left .",
    "c_{i , j}\\right|\\mathcal{d}_i,\\bm{f } , \\bm{\\xi } \\right]\\right|\\mathcal{d}_i \\right]=c_{i , i - i}~e\\left[\\left .",
    "\\prod_{j = i - i}^{j-1}f_j\\right|\\mathcal{d}_i \\right ] \\nonumber\\\\ \\label{bayespredictor } & = & c_{i , i - i}~ \\prod_{j = i - i}^{j-1}e\\left[\\left .",
    "f_j\\right|\\mathcal{d}_i \\right ] ~=~c_{i , i - i}~ \\prod_{j = i - i}^{j-1}\\widehat{f}_{j}^ { ( mmse ) } \\\\\\nonumber & \\approx & c_{i , i - i}~ \\prod_{j = i - i}^{j-1}\\widehat{f}_{j}^{(cl ) } ~=~\\widehat{c}_{i , j},\\end{aligned}\\ ] ] where on the last line we have an equality if the diffusivity of the priors @xmath20 tends to infinity .",
    "this is exactly the argument why the bayesian cl model can be used to justify the cl predictors ; see gisler - wthrich @xcite .",
    "in addition , the posterior samples for the dfcl model parameters , obtained via the mcmc - abc bootstrap procedure , will allow us to obtain the predictive distribution of the claims in two ways . the first is the full predictive distribution of the claims obtained after integrating out the posterior uncertainty associated with the bayesian dfcl model parameters to empirically estimate @xmath59 in practice , this numerical procedure involves taking each posterior sample for the dfcl model parameters and obtaining an estimate of the predicted claims .",
    "the second approach involves using one of the bayesian point estimators for the parameters such as the mmse to obtain @xmath60 .",
    "alternatively , one may consider a rao - blackwellised version of the bayesian predictive distribution of claims involving @xmath61 having numerically integrated out the bayesian posterior uncertainty associated with the dfcl variance parameters .",
    "such methods are typically known as empirical bayesian approaches .",
    "these results can then be applied to estimate any risk measures .",
    "for example , if we fix a security level 95% we can calculate the var on that level , which is defined by @xmath62\\bigg| \\mathcal{d}_i\\bigg ) = \\min \\left\\ { x;~ p \\bigg [ c_{i , j}-e\\left[c_{i , j}|\\mathcal{d}_i\\right ] > x \\bigg| \\mathcal{d}_i \\bigg ] \\le 0.05 \\right\\}.\\ ] ]",
    "assume that we have calculated the bayesian predictor or the cl predictor given in .",
    "then we would like to determine the prediction uncertainty , that is , we would like to study the deviation of @xmath63 around its predictor .",
    "if one is only interested in second moments , the so - called conditional mean square error of prediction ( msep ) , one can often estimate the error terms analytically .",
    "however , other uncertainty measures like value - at - risk ( var ) can only be determined numerically ; see ( [ eqns ] ) .",
    "a popular numerical method is the bootstrap method .",
    "the bootstrap technique was developed by efron @xcite and extended by efron - tibshirani @xcite and davison - hinkley @xcite . in the actuarial literature",
    "the development of bootstrap procedures includes the work of taylor @xcite , taylor - mcguire @xcite , @xcite , england - verrall @xcite , @xcite and pinheiro et al .",
    "@xcite .",
    "this procedure allows one to obtain information regarding an aggregated distribution given a single realisation of the data .",
    "to apply the bootstrap procedure one introduces a minimal amount of model structure such that resampling observations can be achieved using observed samples of the data .    in this section",
    "we present a bootstrap algorithm in the classical frequentist approach .",
    "that is , we assume that the cl factors @xmath64 and the standard deviation parameters @xmath65 given in model assumptions [ model ass ] are unknown constants .",
    "the bootstrap then generates synthetic data denoted by @xmath66 that allow for the study of the fluctuations of @xmath67 and @xmath68 ( for details see section 7.4 in wthrich - merz @xcite ) . in the presented text",
    "we restrict ourselves to the conditional resampling approach presented in section 7.4.2 of wthrich - merz @xcite .      1 .",
    "calculate estimated residuals @xmath69 for @xmath70 , @xmath71 , conditional on the estimators @xmath72 and @xmath73 and the observed data @xmath74 : @xmath75 2 .",
    "these residuals @xmath76 give the empirical bootstrap distribution @xmath77 .",
    "3 .   sample i.i.d .",
    "residuals @xmath78 for @xmath70 , @xmath71 .",
    "4 .   generate bootstrap observations ( conditional resampling ) @xmath79which defines @xmath80 .",
    "note that for the unconditional version of bootstrap we should generate @xmath81 . for a discussion on this approach , see section 7.4.1 of @xcite .",
    "5 .   calculate bootstrapped cl parameters @xmath82and @xmath83 by @xmath84 6 .",
    "repeat steps 3 - 5 and obtain empirical distributions from the bootstrap samples @xmath85 , @xmath82 and @xmath83 .",
    "these are then used to quantify the parameter estimation uncertainty .",
    "this non - parametric classical bootstrap method can be seen as a frequentist approach .",
    "this means that we do not express our parameter uncertainty by the choice of an appropriate prior distribution .",
    "we rather use a point estimator for the unknown parameters and then study the possible fluctuations of this point estimator .",
    "the main difficulty now is that the non - parametric bootstrap method , as described above , underestimates the `` true '' uncertainty .",
    "this comes from the fact that the estimated residuals @xmath86 , in general , have variance smaller than 1 ( see formula ( 7.23 ) in wthrich - merz @xcite ) .",
    "this means that our estimated residuals are not appropriately scaled .",
    "therefore , frequentists use several different scalings to correct this fact ( see formula ( 7.24 ) in wthrich - merz @xcite or england - verrall @xcite ) . here",
    ", we use a different approach by introducing the novel bayesian bootstrap method embedded within an mcmc - abc algorithm to obtain empirically the posterior distribution of the bayesian dfcl model , described below . having obtained this , we can then calculate all required bayesian parameter estimates , capital reserve estimates and associated risk measures such as var . before presenting the methodology for this novel mcmc - abc algorithm",
    "we will finalize this section with the decompositions of the msep under frequentist , bayesian and credibility approaches .",
    "let us for the time - being concentrate on the conditional msep given by @xmath87\\\\ \\nonumber & = & \\text{var}\\left(\\left.c_{i , j}\\right| \\mathcal{d}_i\\right ) + \\left(e\\left[\\left.c_{i , j } \\right| \\mathcal{d}_i\\right]-\\widehat{c}_{i , j}\\right)^2.\\end{aligned}\\ ] ] the first term is known as the conditional process variance and the second term as the parameter estimation uncertainty . in the frequentist approach ( i.e.  for given deterministic @xmath64 and @xmath65 ) these terms",
    "can be calculated as @xmath88 \\bigr)^2~ \\sum_{j = i - i}^{j-1}~ \\frac{\\sigma^2_j / f^2_{j}}{e\\left.\\left[c_{i , j}\\right|c_{i , i - i}\\right ] } \\stackrel{def.}{=}c_{i , i - i } \\gamma_{i - i},\\ ] ] and @xmath89-\\widehat{c}_{i , j}\\right)^2 = c^2_{i , i - i}\\left(\\prod_{j={i - i}}^{j-1}f_j -\\prod_{j={i - i}}^{j-1}\\widehat{f}^{(cl)}_j\\right)^2 \\stackrel{def.}{=}c_{i , i - i}^2 \\delta_{i - i};\\ ] ] see wthrich - merz @xcite , section 3.2 .",
    "the process variance is estimated by replacing the parameters by its estimators , @xmath90 the parameter estimation error is more involved and there we need the bootstrap algorithm .",
    "assume that the bootstrap method gives @xmath91 bootstrap samples @xmath92 .",
    "then the parameter estimation error is estimated by the sample variance of the product of the bootstrap observation chain ladder parameter estimates @xmath92 , which gives the estimator @xmath93 .      in the bayesian setup , ( i.e.  choosing prior distributions for the unknown parameters @xmath94 and @xmath95 ) we obtain a natural decomposition of the conditional msep : @xmath96\\right ) & = & \\text{var}\\left(\\left.c_{i , j}\\right| \\mathcal{d}_i\\right)\\\\ \\nonumber & = & e\\left[\\left.\\text{var}\\left(\\left.c_{i , j } \\right| \\mathcal{d}_i , \\bm{f},\\bm{\\xi}\\right)\\right| \\mathcal{d}_i\\right ] + \\text{var}\\left(\\left.e\\left[\\left.c_{i , j } \\right| \\mathcal{d}_i,\\bm{f},\\bm{\\xi}\\right]\\right| \\mathcal{d}_i\\right).\\end{aligned}\\ ] ] the average process variance is given by ( see wthrich - merz @xcite , lemma 3.6 ) @xmath97 = c_{i , i - i } \\sum_{j = i - i}^{j-1 } e\\left[\\left",
    ". \\prod_{m = i - i}^{j-1}f_m ~\\xi_j^2 \\prod_{n = j+1}^{j-1 } f_n^2 \\right|\\mathcal{d}_i\\right]\\\\ & & \\quad = c_{i , i - i } \\sum_{j = i - i}^{j-1 }   \\prod_{m = i - i}^{j-1}e\\left[\\left.f_m   \\right|\\mathcal{d}_i\\right ] e\\left[\\left.\\xi_j^2 \\right|\\mathcal{d}_i\\right ] \\prod_{n = j+1}^{j-1 } e\\left[\\left.f_n^2 \\right|\\mathcal{d}_i\\right]\\stackrel{def.}{=}c_{i , i - i } \\widehat{\\gamma}_{i - i}^{bayes } , \\nonumber\\end{aligned}\\ ] ] where we have used posterior independence ( [ posteriorindependence ] ) .",
    "the parameter estimation error is given by @xmath98\\right| \\mathcal{d}_i\\right ) = c_{i , i - i}^2 \\text { var}\\left ( \\left.\\prod_{j = i - i}^{j-1 } f_j \\right| \\mathcal{d}_i\\right)\\stackrel{def.}{=}c_{i , i - i}^2 \\widehat{\\delta}_{i - i}^{bayes},\\ ] ] where we have used .",
    "using ( [ posteriorindependence ] ) , we obtain for the last term @xmath99 -\\prod_{j = i - i}^{j-1}e\\left [ \\left .",
    "\\mathcal{d}_i\\right]^2 \\right].\\ ] ] in order to calculate these two terms given in and , we need to calculate the posterior distribution of @xmath35 , given @xmath33 .",
    "since we do not have a full distributional model , we can not write down the likelihood function , which would allow for analytical solutions or markov chain monte carlo ( mcmc ) simulations . therefore we introduce the abc framework which allows for distribution - free simulations using appropriate bootstrap samples and a distance metric .",
    "this will be discussed in section [ abcsection ] .      as mentioned previously",
    ", we can also consider the credibility estimates given in gisler - wthrich @xcite .",
    "as long as we are only interested in the second moments ( i.e.  conditional msep ) we can also use credibility estimators , which are minimum variance estimators that are linear in the observations . for diffuse priors we obtain the approximation given in corollary 7.2 of gisler - wthrich @xcite @xmath100\\right ) = c_{i , i - i}\\widehat{\\gamma } _ { i - i}^{cred}+c_{i , i - i}^{2}\\widehat{\\delta } _ { i - i}^{cred},\\]]where@xmath101 in the results section we compare the frequentist bootstrap approach , the credibility approach and the abc bootstrap approach that is described below ( see table 7 below ) .",
    "to estimate numerically the parameters , predicted claims and associated uncertainty measures such as the msep presented in the previous sections , the bayesian approach requires the ability to sample from the posterior distribution of the dfcl model parameters . obtaining samples @xmath102 which are realisations of a random vector distributed with a posterior distribution @xmath103 in the dfcl model is difficult since the likelihood is intractable .",
    "hence , standard numerical approaches such as markov chain monte carlo ( mcmc ) algorithms ( see gilks et al .",
    "@xcite ) can not be directly used since they all require explicit repeated evaluation of the likelihood function at each stage of the markov chain sampling algorithm .",
    "it is common to avoid this difficulty by making distributional assumptions for the form of the likelihood .",
    "this then violates the dfcl model assumption but allows for relatively standard sampling procedures to be applied . in this regard ,",
    "one possible approach involves making a specific gaussian assumption for the likelihood .",
    "one problem with this assumption , which is evident immediately , is that it precludes skewness in the model . here",
    ", we do not make any such assumptions and instead we work in a truly distribution - free model using abc to facilitate sampling from an intractable posterior distribution .",
    "there is an additional complexity in the dfcl model not typically encountered when working with abc methodology .",
    "typically , abc methodology is developed in the case in which the model likelihood can not be evaluated point - wise , but conditional on parameter values , synthetic data is easily simulated from the model ; see examples in peters - sisson  @xcite and peters et al .",
    "this is not the case in the bayesian dfcl model . under the dfcl model",
    "the likelihood is only expressed by moment conditions , hence we can not evaluate the likelihood point - wise and also the simulation from the likelihood can not be performed directly .",
    "this is why we introduce the novel concept of the bayesian bootstrap which is embedded within the abc methodological framework .",
    "hence , to sample from the posterior in our dfcl model we develop a novel formulation of the abc methodology based on the bootstrap and conditional back transformation procedure , similar to that discussed in section [ section : bootstrap dfcl ] .",
    "abc methods aim to sample from posterior distributions in the presence of computationally intractable likelihood functions . for an application in risk modelling of abc methodology , see peters - sisson @xcite . in this article",
    "we present a novel mcmc - abc algorithm . before presenting some details of the numerical mcmc procedure",
    ", we note that alternative numerical algorithms could be considered in the abc context .",
    "for example , a sequential monte carlo ( smc ) based algorithms which can improve simulation efficiency can be found in del moral et al .",
    "@xcite , sisson et al .",
    "@xcite , peters et al .",
    "@xcite,@xcite and marjoram et al .",
    "@xcite .      in this section",
    "we provide a brief description of abc methodology , which describes a suite of methods developed specifically for working with models in which the likelihood is computationally intractable . here",
    "we work with a bayesian model and consider the likelihood intractability to arise in the sense that we may not evaluate the likelihood point - wise .",
    "the abc method we consider here embeds an intractable target posterior distribution , in our case denoted by @xmath104 , into a general augmented model @xmath105 where @xmath106 is an auxiliary vector on the same space as @xmath74 . in this augmented bayesian model ,",
    "the weighting function @xmath107 weights the intractable posterior . in this paper",
    "we consider the hierarchical model assumption , where we work with @xmath108 ; see reeves and pettitt  @xcite .    the mechanism in the abc framework which allows one to avoid the evaluation of the intractable likelihood involves replacing this evaluation with data simulation from the likelihood .",
    "that is , given a realisation of the parameters of the model , a synthetic data set @xmath106 is generated and compared to the original data set .",
    "this is a key aspect of the novel methodology we develop in this paper , since we utilise a bootstrap procedure to perform this simulation in the dfcl model setting .",
    "then summary statistics @xmath109 derived from this data are compared to summary statistics of the observed data @xmath110 and a distance @xmath111 is calculated .",
    "finally , a weight is given to these parameters according to the weighting function @xmath112 , which may give greater weight when @xmath109 and @xmath110 are close ( i.e. where @xmath111 is small ) .",
    "for example , under the `` hard decision '' ( hd ) weighting given by @xmath113 a reward is given to summary statistics of the augmented auxiliary variables @xmath114 within an @xmath115-tolerance of the summary statistic of the actual observed data @xmath116 , as measured by distance metric @xmath117 .",
    "hence , in the abc context , an approximation to the intractable target posterior marginal distribution @xmath104 , for which we are interested in formulating an empirical estimate , is given by @xmath118 as briefly mentioned , obtaining samples from the abc posterior can be achieved using a number of numerical procedures , in this paper we consider an mcmc approach .",
    "the mcmc class of likelihood - free algorithm is justified on a joint space formulation , in which the stationary distribution of the markov chain is given by @xmath119 .",
    "the corresponding target distribution for the marginal distribution @xmath120 as @xmath121 , recovering the `` true '' ( intractable ) posterior , assuming that @xmath116 are sufficient statistics and that the weighting function converges to a point mass on @xmath116 as @xmath122 ; see peters - sisson  @xcite and references therein for detailed discussion .",
    "accordingly , the tolerance @xmath115 is typically set as low as possible for a given computational budget . in this paper we focus on the class of mcmc - based sampling algorithms .    the abc methodology is novel both in the statistics literature and in the actuarial literature .",
    "it is informative to clearly provide the justification for this approach both theoretically and numerically .",
    "the simplest understanding of abc is achieved by considering a rejection algorithm , therefore we provide a basic argument for how the abc methodology works in simple rejection sampling in appendix [ section : justification for abc ] .",
    "the actuarial dfcl model considered in this paper requires the more sophisticated mcmc - abc methodology described below .      for given observations",
    "@xmath74 we want to sample from @xmath123 with an intractable likelihood function .",
    "we assume that @xmath110 is either the data itself or a summary of the data such as a sufficient statistic for the model from which we assume data @xmath74 is a realisation .",
    "we assume that , given a set of parameters values @xmath124 , we can generate from the dfcl model ( via a conditional bootstrap procedure ) a synthetic data set denoted @xmath125 .",
    "we define a hard decision function @xmath126 for a given tolerance level @xmath127 and a distance metric @xmath128 , where @xmath129 is the indicator function which equals 1 if the event is true and 0 otherwise .",
    "as demonstrated in appendix a , we use the approximation , - , which gives us in the bayesian dfcl model setting , @xmath130 } { e\\left[g(\\mathcal{d}_{i}|\\mathcal{d}_{i}^{\\ast})\\right]}.\\ ] ] in the next step the numerator of is approximated using the empirical distribution : @xmath131 \\approx \\pi(\\bm{f},\\bm{\\sigma } ) \\frac{1}{l } \\sum_{l=1}^l g\\left ( \\mathcal{d}_{i}|\\mathcal{d}_{i}^{\\ast,(l)}(\\bm{f},\\bm{\\sigma})\\right),\\ ] ] where @xmath132 . finally , we need to consider the denominator @xmath133 $ ] . in general",
    "this has a non - trivial form that can not be calculated analytically .",
    "however , since we use an mcmc based method the denominators cancel in the accept - reject stage of the algorithm",
    ". therefore , the intractability of the denominator does not impede sampling from the posterior .",
    "thus we use @xmath134\\\\ & \\approx \\pi(\\bm{f } ) \\pi(\\bm{\\sigma } ) \\frac{1}{l } \\sum_{l=1}^l g\\left(\\mathcal{d}_{i}|\\mathcal{d}_{i}^{\\ast,(l)}(\\bm{f},\\bm{\\sigma})\\right ) \\end{aligned}\\ ] ] in order to obtain samples from @xmath135 .",
    "almost universally , @xmath136 is adopted to reduce computation but on the other hand this will slow down the rate of convergence to the stationary distribution .",
    "note that sometimes one also uses softer decision functions for @xmath137 .",
    "the role of the distance measure @xmath138 is evaluated by peters et al .",
    "we further extend this analysis to the class of models considered in this paper .",
    "we analyse several choices for the distance measure @xmath138 such as mahlanobis distance , scaled euclidean distance and the manhattan `` city block '' distance .",
    "fan et al .",
    "@xcite demonstrate that it is not efficient to utilise the standard euclidean distance , especially when summary statistics considered are on different scales .",
    "additionally , using an mcmc - abc algorithm , it is important to assess convergence diagnostics . particularly when using mcmc - abc where serial correlation in the markov chain samples can be significant if the sampler is not designed carefully .",
    "we assess autocorrelation of the simulated markov chain , the geweke @xcite time series statistic and the gelman - rubin @xcite r - statistic convergence diagnostic in an abc setting .    *",
    "concluding : * we apply three different techniques in order to treat the intractable likelihood :    1 .",
    "abc is used to get a handle on the likelihood and therefore the intractable posterior .",
    "2 .   as a result of using abc",
    "we need to be able to generate synthetic data samples from the dfcl model given realisations of the parameters .",
    "these data samples come from the bootstrap algorithm .",
    "we use a well understood mcmc based sampling algorithm that does not require calculation of the non - analytic normalizing constants for the target distribution @xmath135 .",
    "the reason for this is that in the acceptance probability of the mcmc algorithm , the normalizing constant for the target posterior appears both in the numerator and denominator , resulting in cancellation .",
    "the specific details of the mcmc algorithm and abc choices are provided in the appendix b.",
    "to test the accuracy of the methodology , first we use synthetic data generated with known parameter values .",
    "the tuning of the proposal distribution in this study is done for the simplest `` base '' distance metric , the weighted euclidean distance . to study the effect of the distance metric in a comparative fashion we shall keep the proposal distribution unchanged .",
    "the first example we present has a claims triangle of size @xmath139 .",
    "in this example we fix the true model parameters , denoted by @xmath140 and @xmath141 and given in table [ tab2 ] , used to generate the synthetic data set .      to generate the synthetic observations for @xmath74 , we generate randomly the first column ( i.e. @xmath11 ) .",
    "then conditional on this realisation of @xmath11 we make use of the model given in ( [ model ass ] ) to generate the remaining columns of @xmath74 , ensuring the model assumptions are satisfied .",
    "this requires setting @xmath142 sufficiently large ( for appropriate choices of @xmath143 and @xmath144 ) and then sampling i.i.d .",
    "realisations of @xmath145 $ ] used to obtain @xmath33 ; see the observations in table [ tab2 ] .",
    "we perform a sensitivity analysis , studying the impact of the distance metric on the mixing of the markov chain in the case of joint estimation of the chain ladder factors and the variance parameters .",
    "the pre - tuned coefficient of variation of the gamma proposal distribution for each parameter of the posterior was performed using the following settings ; @xmath146 , @xmath147 , @xmath148 and initial values @xmath149 for all @xmath150 .",
    "additionally , the prior parameters for the chain ladder factors @xmath21 were set as @xmath151 and the parameters for the variance parameters @xmath152 were set as @xmath153 .",
    "after tuning the proposal distributions during burn - in and rounding the shape parameters , we found that @xmath154 for all @xmath155 produced average acceptance probabilities for each parameter between 0.3 and 0.5 .",
    "this is a range typically used in practice when designing mcmc sampling algorithms .",
    "then , keeping the proposal distribution constant and using a common data set @xmath74 , we ran three versions of the mcmc - abc algorithm for 200,000 samples corresponding to :    1 .   scaled euclidean distance and joint estimation of posterior for @xmath156 ; 2 .",
    "mahlanobis distance ( modified ) and joint estimation of posterior for @xmath156 ; and 3 .",
    "manhattan `` city block '' distance and joint estimation of posterior for @xmath156 .",
    "we estimate the three convergence diagnostics given in appendix b. the results of this analysis are presented as a function of markov chain iteration @xmath157 post burn - in of 50,000 samples .",
    "* autocorrelation function * : figure [ fig1 ] shows the estimated autocorrelation functions for the markov chains of the random variables @xmath158 and @xmath159 .",
    "we analyze the marginal parameters to get a reasonable estimate of the mixing behavior of the mcmc - abc algorithm .",
    "the results demonstrate the degree of serial correlation in the markov chains generated for these parameters as a function of lag time @xmath160 .",
    "the higher the decay rate in the tail of the estimated acf as a function of @xmath160 , the better the mixing of the mcmc algorithm . due to the independence properties of this model",
    "there is little difference between results obtained for scaled euclidean and mahlanobis distances . as shown in appendix c ,",
    "the estimate of the covariance matrix is diagonal on all but the right lower @xmath161 block .",
    "hence , we recommend using the simple scaled euclidean distance metric as it provided the best trade - off between simplicity and mixing performance .",
    "* geweke time series diagnostic * : figure [ fig2 ] shows results for the geweke time series diagnostic .",
    "again , we present the results for the random variables @xmath158 and @xmath159 .",
    "note , we used the posterior mean as the sample function and a set of increasing values for @xmath162 from @xmath163 increasing in steps of 5,000 samples to @xmath91 . in each case",
    "we split the chain in each `` window '' given by @xmath164 and @xmath165 according to recommendations from geweke et al .",
    "we then calculate the convergence diagnostic @xmath166 which is the difference between these two means divided by the asymptotic standard error of their difference . as the chain length increases @xmath167 , the sampling distribution of @xmath168 if the chain has converged .",
    "hence values of @xmath166 in the tails of a standard normal distribution suggest that the chain was not fully converged early on ( i.e. during the 1st window ) .",
    "hence , we plot @xmath166 scores versus increasing @xmath162 and monitor if they lie within a @xmath169 confidence interval @xmath170 $ ] .",
    "the results in figure [ fig2 ] clearly demonstrate the convergence properties of the distance functions differ .",
    "again this is more material in the markov chain for the variance parameter when compared to the markov chain results for the chain ladder factor .",
    "the main point we note is that again one would advise against use of the `` city block '' distance metric . *",
    "gelman and rubin r statistic * : figure [ fig3 ] presents the gelman and rubin convergence diagnostic . to calculate this we ran 20 chains in parallel , each of length 10,000 samples and for each chain we discarded 250 samples as burn - in .",
    "we then estimated the @xmath171 statistic as a function of simulation time post burn - in .",
    "figure [ fig3 ] shows the convergence rate of the @xmath171 statistic to 1 for each distance metric on increasing blocks of 200 samples . using this summary statistic , all three distance metrics are very similar in terms of convergence rate of the r statistic to 1 .",
    "overall , these three convergence diagnostics demonstrate that the simple scaled euclidean distance metric is the superior choice .",
    "secondly , we see appropriate convergence of the markov chains under three convergence diagnostics which tests different aspects of the mixing of the markov chains , giving confidence in the performance of the mcmc - abc algorithm for this model .      in this section we present results for the scaled euclidean distance metric , with a markov chain of length 200,000 samples discarding the first 50,000 samples as burn - in .",
    "table [ tab3 ] shows the cl parameter estimates for the dfcl model and the associated parameter estimation error .",
    "we define the following quantities :    * @xmath172 , @xmath173 , @xmath174 and @xmath175|\\sigma_{0:j-1}$ ] denote respectively the maximum a - posteriori , minimum mean square error , posterior standard deviation of the conditional distribution of chain ladder factor @xmath21 and the posterior coverage probability estimates at @xmath176 of the conditional distribution of chain ladder factor @xmath21 . each of these estimates is conditional on knowledge of the true @xmath177 .",
    "* @xmath178 , @xmath179 , @xmath180 and @xmath175 $ ] denote the same quantities for the unconditional distribution after joint estimation of @xmath181 and @xmath182 .",
    "* @xmath183 $ ] and @xmath184 $ ] denote the average acceptance probabilities of the markov chain .",
    "* @xmath185 , @xmath186 , @xmath187 and @xmath175 $ ] denote the same quantities for the chain ladder variances as those defined above for chain ladder factors .",
    "note , the estimates for @xmath178 and @xmath188 were obtained marginally . for the frequentist approach",
    "we obtain the standard error in the estimates by using 1,000 bootstrap realisations of @xmath189 to obtain @xmath190 .",
    "we use these bootstrap samples to calculate the standard deviation in the estimates of the parameters in the classical frequentist cl approach , given in brackets @xmath191 next to their corresponding estimators .",
    "the standard errors in the bayesian parameter estimates are obtained by blocking the markov chain into 100 blocks of length 1,500 samples and estimating the posterior quantities on each block .",
    "in this example we consider estimation using real claims reserving data from wthrich - merz @xcite , see table 3 .",
    "this yearly loss data is turned into annual cumulative claims and divided by 10,000 for the analysis in this example .",
    "we use the analysis from the previous study to justify use of the joint mcmc - abc simulation algorithm with a scaled euclidean distance metric .",
    "we pre - tuned the coefficient of variation of the gamma proposal distribution for each parameter of the posterior .",
    "this was performed using the following settings : @xmath146 , @xmath192 , @xmath193 and initial values @xmath149 for all @xmath194 .",
    "here we make a strict requirement of the tolerance level to ensure we have accurate results from our abc approximation .",
    "additionally , the prior parameters for the chain ladder factors @xmath21 were set as @xmath195 and the parameters for the variance @xmath152 priors were set as @xmath196 .",
    "the code for this problem was written in matlab and it took approximately 10 min to simulate 200,000 samples from the mcmc - abc algorithm on intel xeon 3.4ghz processor with 2 gb ram .",
    "after tuning the proposal distributions during burn - in we obtained rounded shape parameters + @xmath197 $ ] provided average acceptance probabilities between 0.3 and 0.5 .    * estimates of @xmath143 and @xmath198 * + figures [ fig4 ] presents box - whisker plots of estimates of the distributions of the parameters @xmath181 and @xmath182 obtained from the mcmc - abc algorithm , post burn - in .",
    "figure [ fig5 ] shows the bayesian mcmc - abc empirical distributions of the ultimate claims , @xmath63 for @xmath199 . in table",
    "[ tab5 ] we present the predicted cumulative claims for each year along with the estimates for the chain ladder factors and chain ladder variances under both the classical approach and the bayesian model .",
    "we see that with this fairly vague prior specified , we do indeed obtain convergence of the mcmc - abc based bayesian estimates @xmath200 to the classical estimates @xmath201 .",
    "* dependence on tolerance @xmath115 * + figure [ fig6 ] presents a study of the histogram estimate of the marginal posterior distribution for chain ladder factor @xmath202 .",
    "the plot was obtained by sampling from the full posterior @xmath203 for each specified tolerance value , @xmath204 .",
    "then the samples for the particular chain ladder parameter in each plot are turned into a smoothed histogram estimate for each @xmath204 and plotted .",
    "the results of this analysis demonstrated that when @xmath115 is large , in this model greater than around @xmath205 , the likelihood is not having an influence on the abc posterior distribution .",
    "hence , under an mcmc - abc algorithm , this results in acceptance probabilities for the chain being artificially high , resulting in estimates of the posterior which reflect the prior distribution used ( in this case a vague prior ) . as @xmath204 is reduced , we notice that the changes in the estimate of the posterior distribution also reduces .",
    "the aim of this study is to demonstrate that once @xmath204 reaches a small enough level , the effect of reducing it further is minimal on the posterior distribution .",
    "we see that changing @xmath204 from @xmath206 to @xmath207 has not had a material impact on the posterior mean or variance , the change is less than @xmath208 . as a result , reducing @xmath204 past this point can not be justified relative to the significant increase in computational effort required to achieve such a further reduction in @xmath204 .",
    "ultimately , we would like an algorithm which could work well for any @xmath204 , the smaller the better .",
    "however , we note that with a decreasing @xmath204 in the sampler we present in this paper , one must take additional care to ensure the markov chain is still mixing and not `` stuck '' in a particular state , as is observed to be the case in all mcmc - abc algorithms . to avoid this acknowledged difficulty with mcmc - abc , one should run much longer mcmc chains or alternatively use of more sophisticated sampling algorithms such as smc samplers prc - abc based algorithms ; see sisson et al .",
    "@xcite .",
    "the conclusion of these findings is that a value of @xmath193 , which was used for the analysis of the data in this paper , is suitable numerically and computationally .",
    "+   +   +   +   + * var and msep . * + in table [ tab6 ] we present the predictive var at @xmath169 and @xmath209 levels for the ultimate predicted claims , obtained from the mcmc - abc algorithm .",
    "these are easily obtained under the bayesian setting , using the mcmc - abc posterior samples to explicitly obtain samples from the full predictive distribution of the cumulative claims after integrating out the parameter uncertainty numerically .",
    "in addition to this , we present the analysis of the msep under the bootstrap frequentist procedure and the bayesian mcmc - abc and credibility estimates for the total predicted cumulative claims for each accident year @xmath3 . we also present results for the sum of the total cumulative claims for each accident year , and the associated parameter uncertainty and process variance ( see section [ section : bootstrap dfcl ] for details ) .",
    "we can make the following conclusions from these results :    1 .",
    "the estimates of process variance for each @xmath63 demonstrate that the frequentist bootstrap and the credibility estimates are very close for all accident years @xmath3 .",
    "the bayesian results compare favorably with the credibility results .",
    "2 .   the results for the parameter estimation error for the predicted cumulative claims @xmath63 demonstrate for small @xmath3 that the bayesian approach results in a smaller estimation error compared to the frequentist approach . for large @xmath3",
    ", the bayesian approach produces larger estimation error relative to the credibility approach .",
    "the total results for the process variance for @xmath210 demonstrate that the frequentist and credibility results are very close . additionally , bayesian total results are largest followed by credibility and then frequentist estimates which is in agreement with theoretical bounds . 4 .   the total results for the parameter estimation error for @xmath210 demonstrate that frequentist unconditional bootstrap procedure results in the lowest total error .",
    "the bayesian approach and credibility total parameter errors are close .",
    "additionally , we note that the results in table 7.1 of wthrich - merz @xcite , for the total parameter estimation error under an unconditional frequentist bootstrap with unscaled residuals is also very close to the total obtained under the frequentist approach .",
    "this paper has presented a distribution - free claims reserving model under a bayesian paradigm .",
    "a novel advanced mcmc - abc algorithm was developed to obtain estimates from the resulting intractable posterior distribution of the chain ladder factors and chain ladder variances .",
    "we assessed several aspects of this algorithm , including the properties of the convergence of the mcmc algorithm as a function of the distance metric approximation in the abc component .",
    "the methodologies performance was demonstrated on a synthetic data set generated from known parameters .",
    "next , it was applied to a real claims reserving data set .",
    "the results we obtained for predicted cumulative ultimate claims were compared to those obtained via classical chain ladder methods and via credibility theory .",
    "this clearly demonstrated that the algorithm is working accurately and provides us not only with the ability to obtain point estimates for the first and second moments of the ultimate cumulative claims , but also with an accurate empirical approximation of the entire distribution of the ultimate claims .",
    "this is valuable for many reasons , including prediction of reserves which are not based on centrality measures such as the tail based var results we present .",
    "* * * * the fist author thanks eth fim and eth risk lab for their generous financial assistance whilst completing aspects of this work at eth .",
    "the first author also thanks the department of mathematics and statistics at the university of nsw for support through an australian postgraduate award and to csiro for support through a postgraduate research top up scholarship .",
    "finally , this material was based upon work partially supported by the national science foundation under grant dms-0635449 to the statistical and applied mathematical sciences institute , north carolina , usa . any opinions , findings , and conclusions or recommendations expressed in this material are those of the author(s ) and do not necessarily reflect the views of the national science foundation    9    davison , a.c . ,",
    "hinkley , d.v .",
    "( 1997 ) . _ bootstrap methods and their application_. cambridge university press , cambridge .",
    "del moral , p. , doucet , a. , jasra , a. ( 2006 ) .",
    "sequential monte carlo samplers .",
    "_ journal of the royal statistical society series .",
    "@xmath211(3 ) , 411 - 436 .    efron , b. ( 1979 ) .",
    "bootstrap methods : another look at the jackknife . _",
    "annals of statistics_.  @xmath212(1 ) , 1 - 26 .",
    "efron , b. , tibshirani , r.j .",
    "( 1993 ) . _",
    "an introduction to the bootstrap_. chapman & hall , ny .    england , p.d . ,",
    "verrall , r.j .",
    "analytic and bootstrap estimates of prediction errors in claims reserving . _",
    "insurance : mathematics and economics_. @xmath213(3 ) , 281 - 293 .",
    "england , p.d . ,",
    "verrall , r.j .",
    "stochastic claims reserving in general insurance .",
    "_ british actuarial journal_. @xmath214(3 ) , 443 - 518 .",
    "england , p.d . ,",
    "verrall , r.j .",
    "predictive distributions of outstanding liabilities in general insurance . _",
    "annals of actuarial science_. @xmath215(2 ) , 221 - 270 .",
    "fan , y. , sisson , s.a . , peters , g.w .",
    "improved efficiency in approximate bayesian computation .",
    "_ technical report , statistics department , university of new south wales_.    gelman , a. , rubin , d.b .",
    "inference from iterative simulation using multiple sequences .",
    "_ statistical science _",
    "@xmath216 , 457 - 472 .",
    "geweke , j.f .",
    "( 1991 ) . _",
    "evaluating the accuracy of sampling - based approaches to the calculation of posterior moments_. in j.m .",
    "bernardo , j.o .",
    "berger , a.p .",
    "dawid and a.f.m .",
    "smith ( eds . ) bayesian statistics , 4 , oxford univeristy press , oxford .",
    "gilks , w.r .",
    ", richardson , s. , spiegelhalter , d.j .",
    "_ markov chain monte carlo in practice_. chapman & hall , london .",
    "gisler , a. , wthrich , m.v .",
    "credibility for the chain ladder reserving method .",
    "_ astin bulletin _",
    "@xmath217(2 ) , 483 - 526 .",
    "gramacy , r.b . ,",
    "samworth , r.j . , king , r. ( 2008 ) .",
    "importance tempering .",
    "_ preprint , arxiv : 0707.4242v5 [ stat.co]_.    mack , t. ( 1993 ) . distribution - free calculation of the standard error of chain ladder reserve estimates . _ astin bulletin _ @xmath218 , 213 - 225 .",
    "marjoram , p. , molitor , j. , plagnol , v. , tavare , s. ( 2003 ) .",
    "markov chain monte carlo without likelihoods .",
    "_ proceedings of the national academy of science usa _",
    "@xmath219 , 15324 - 15328 .",
    "peters , g.w . , sisson , s.a .",
    "bayesian inference , monte carlo sampling and operational risk .",
    "_ journal of operational risk _",
    "@xmath215(3 ) , 27 - 50 .",
    "peters , g.w . ,",
    "fan , y. , sisson , s.a .",
    "( 2008 ) . on sequential monte carlo , partial rejection control and approximate bayesian computation .",
    "_ preprint , statistics department , university of new south wales_.    peters , g.w . , sisson , s.a . , fan , y. ( 2008 ) . design efficiency for `` likelihood free '' sequential monte carlo samplers .",
    "_ preprint , statistics department , university of new south wales .",
    "_    pinheiro , p.j.r .",
    ", andrade e silva , j.m .",
    "and de lourdes centeno , m. ( 2003 ) .",
    "bootstrap methodology in claim reserving . _",
    "journal of risk insurance _",
    "@xmath220(4 ) , 701 - 714 .",
    "roberts , g.o . , gelman , a. , gilks , w.r .",
    "weak convergence and optimal scaling of random walk metropolis algorithm . _",
    "annals of applied probability _",
    "@xmath212 , 110 - 120 .",
    "peters , g.w . , shevchenko , p. , wthrich , m.v . , ( 2008 ) .",
    "model uncertainty in claims reserving within tweedie s compound poisson models .",
    "_ astin bulletin _ * 39*,1 - 33 .",
    "peters , g.w . , nevat , i. , sisson , s.a . , fan , y. , yuan , j. ( 2009 ) .",
    "bayesian symbol detection in wireless relay networks .",
    "accepted to appear in _ ieee transactions on signal processing_.    proakis , j.g . ,",
    "manolakis , d.g . , ( 1996 ) .",
    "_ digital signal processing_. upper saddle river , n.j .",
    "prentice hall .",
    "reeves , r.w . , pettitt , a.n .",
    "( 2005 ) . a theoretical framework for approximate bayesian computation . presented at the international workshop for statistical modelling , sydney .",
    "sisson , s.a . ,",
    "fan , y. , tanaka , m. ( 2007 ) .",
    "sequential monte carlo without likelihoods .",
    "_ proceedings of the national academy of science usa _",
    "@xmath221 , 1760 - 1765 .",
    "sisson , s.a . , peters , g.w . , fan , y. , briers , m. , ( 2008 ) .",
    "likelihood free samplers .",
    "_ preprint , statistics department , university of new south wales .",
    "_    taylor , g.c . ( 1987 ) .",
    "regresion models in claims analysis i : theory .",
    "lxxiv , 354 - 383 .",
    "taylor , g.c . ,",
    "mcguire , g. ( 2005 ) .",
    "synchronous bootstrapping of seemingly unrelated regressions .",
    "conference paper , _",
    "36th astin colloquium , 2005 _ zurich , switzerland .",
    "taylor , g.c . ,",
    "mcguire , g. ( 2007 ) . a synchronous bootstrap to account for dependencies between lines of business in the estimation of loss reserve prediction error . _ north american actuarial journal _",
    "@xmath214(1 ) , 37 - 44 .",
    "wthrich , m.v .",
    ", merz , m. ( 2008 ) .",
    "_ stochastic claims reserving methods in insurance_. wiley finance .",
    "yao , j. ( 2008 ) .",
    "bayesian approach for prediction error in chain - ladder claims reserving .",
    "_ conference paper presented at the astin colloqium _",
    ", manchester uk .",
    "the abc algorithm is typically justified in the simple rejection sampling framework .",
    "this then extends in a straightforward manner to other sampling frameworks such as the mcmc algorithm we utilise in this paper .",
    "we denote the posterior density from which we wish to draw samples by @xmath222 with @xmath223 , where @xmath224 denotes support of the posterior distribution and @xmath225 is the support for @xmath226 .",
    "the abc method aims to draw from this posterior density @xmath227 without the requirement of evaluating the computationally expensive or in our setting intractable likelihood @xmath228 .",
    "the cost of avoiding this calculation is that we obtain an `` approximation ''",
    ".       * 1st case .",
    "* we assume that the support @xmath225 is discrete .",
    "given an observation @xmath229 , we would like to sample from @xmath230 .",
    "then the original rejection sampling algorithm reads as follows :    * rejection sampling abc *    1 .",
    "sample @xmath231 from prior @xmath232 ; 2 .",
    "simulate synthetic data set of auxiliary variables @xmath233 ; 3 .",
    "abc rejection condition : if @xmath234 then accept sample @xmath231 , else reject sample and return to step 1 .",
    "then the chosen @xmath231 is distributed from @xmath235 .",
    "this follows from a simple rejection argument , denote @xmath236 if @xmath231 was chosen . then , the joint density of @xmath237 conditional on @xmath238 is given by @xmath239 this implies that @xmath240 henceforth , this algorithm generates samples @xmath241 , for @xmath242",
    ".       * 2nd case .",
    "* for more general supports @xmath225 one replaces the strict equality @xmath243 with a tolerance @xmath127 and a measure of discrepancy or a distance metric @xmath244 . in this case",
    "the posterior distribution is given by @xmath245 .",
    "implementing this algorithm in a rejection sampling framework gives the following :    * rejection sampling abc *    1 .",
    "sample @xmath231 from prior @xmath232 ; 2 .",
    "simulate synthetic data set of auxiliary variables @xmath233 ; 3 .",
    "abc rejection condition 2 : if @xmath246 then accept sample @xmath231 , else reject sample and return to step 1 .    in this case the joint density of @xmath237 , conditional on @xmath247 , is given by @xmath248 note that for appropriate choices of the distance metric @xmath117 and assuming the necessary continuity properties for the densities we obtain that @xmath249 this concept was taken further with the intention of improving the simulation efficiency by reducing the number of rejected samples . to achieve this",
    ", sufficient statistics were used to replace the comparison between the auxiliary variables ( `` synthetic data '' ) @xmath250 and the observations @xmath226 . denoting the sufficient statistics by @xmath251 and @xmath252 ,",
    "allows one to decompose the likelihood under the fisher - neyman factorization theorem into @xmath253 for appropriate functions @xmath254 and @xmath255 . in the abc context presented above",
    ", the consequence of this decomposition is that when @xmath256 the obtained samples are from the posterior density @xmath257 similar to .",
    "in general , summary statistics will be used when sufficient statistics are not attainable .",
    "we develop an mcmc - abc algorithm which has an adaptive proposal mechanism and annealing of the tolerance during burn - in of the markov chain .",
    "having reached the final tolerance post annealing , denoted @xmath259 , we utilise the remaining burn - in samples to tune the proposal distribution to ensure an acceptance probability between the range of 0.3 and 0.5 is achieved .",
    "the optimal acceptance probability when posterior parameters are i.i.d .",
    "gaussian was proven to be at 0.234 ; see roberts et al .  @xcite .",
    "though our problem does not match the required conditions for this proof , it provides a practical guide . to achieve this",
    ", we tune the coefficient of variation of the proposal , in our case it is the shape parameter of the gamma proposal distribution .",
    "we impose an additional constraint that the minimum shape parameter value is set at @xmath260 for @xmath261 .    * mcmc - abc algorithm using bootstrap samples .",
    "*    1 .   for @xmath262 initialize the parameter vector randomly , this gives @xmath263 .",
    "initialize the proposal shape parameters @xmath264 for all @xmath155 .",
    "2 .   for @xmath265 1 .    set @xmath266 .",
    "2 .    for @xmath267 1 .",
    "sample proposal @xmath268 from a @xmath269-distribution .",
    "we denote the gamma proposal density by @xmath270 .",
    "this gives proposed parameter vector @xmath271 .",
    "2 .    conditional on @xmath272 , generate synthetic bootstrap data set @xmath273 using the bootstrap procedure detailed in section [ section : bootstrap dfcl ] where we replace the cl parameter estimates @xmath274 by the parameters @xmath275 .",
    "3 .    evaluate summary statistics @xmath276 and @xmath277 and corresponding decision function @xmath278 as described in section 5 .",
    "4 .    accept proposal with abc acceptance probability @xmath279 that is , simulate @xmath280 and set @xmath281 if @xmath282 .",
    "5 .   if @xmath283 and @xmath284 then check to see if tuning of the proposal is required .",
    "define the average acceptance probability over the last 100 iterations of updates for parameter @xmath3 by @xmath285 and consider the adaption : @xmath286 then set the proposal shape parameter as @xmath287 .",
    "the mcmc - abc algorithm presented can be enhanced by utilising an idea of gramacy et al .",
    "@xcite in an abc setting .",
    "this involves a combination of tempering the tolerance @xmath288 and importance sampling corrections .",
    "we start with the choices of the abc components .",
    "* generation of a synthetic data set * : [ synthetic data ] note that in this setting not only is the likelihood intractable but also the generation of a synthetic data set @xmath289 given the current parameter values @xmath290 is not straightforward .",
    "the synthetic data set @xmath289 is generated using the bootstrap procedure described in section [ section : bootstrap dfcl ] .",
    "note that both the bootstrap residual @xmath86 and the bootstrap samples @xmath291 are functions of the parameter choices ; see section 4.1 . therefore we generate for given @xmath64 and @xmath65 the bootstrap residuals @xmath292 and the bootstrap samples @xmath293 according to the non - parametric bootstrap ( see section 4.1 ) where we replace the cl parameter estimates @xmath274 by the parameters @xmath294 .",
    "* summary statistics * : we introduce summary statistics to replace sufficient statistics when they are not attainable for a given model . then , in order to define the decision function @xmath255 , we introduce summary statistics ; see appendix [ section : justification for abc ] . for the observed data @xmath33 we define the vector @xmath295 where @xmath296 denotes the number of residuals @xmath86 . for given @xmath294 ,",
    "we generate the bootstrap sample @xmath297 as described above .",
    "the corresponding residuals @xmath298 should also be close to the standardized observations .",
    "therefore , we define its empirical mean and standard deviation by @xmath299^{1/2}.\\end{aligned}\\ ] ] hence , the summary statistics for the synthetic data is given by @xmath300    * distance metrics * :    * _ mahlanobis distance and scaled euclidean distance _ + here",
    "we draw on the analysis of sisson et al .",
    "@xcite that proposes the use of the mahlanobis distance metric given by@xmath301 ^{\\top } ~\\sigma _ { \\mathcal{d}_{i}}^{-1}~ \\left [ s\\left ( \\mathcal{d}_{i};0,1\\right ) -s \\left ( \\mathcal{d}_{i } ^{\\ast } ; \\mu^\\ast , s^\\ast\\right ) \\right ] , \\end{aligned}\\]]where the covariance matrix @xmath302 is an appropriate scaling described in appendix [ section : mahlanobis distance metric ] .",
    "the scaled euclidean distance is obtained when we only consider the diagonal elements of the covariance matrix @xmath303 .",
    "+ note , the covariance matrix @xmath303 provides a weighting on each element of the vector of summary statistics to ensure they are scaled appropriately according to their influence on the abc approximation .",
    "there are many other such weighting schemes one could conceive .",
    "manhattan `` city block '' distance _",
    "+ we consider the @xmath304-distance given by @xmath305    * decision function * : we work with a hard decision function given by @xmath306    * tolerance schedule * : we use the sequence @xmath307note , the use of an mcmc - abc algorithm can result in `` sticking '' of the chain for extended periods .",
    "therefore , one should carefully monitor convergence diagnostics of the resulting markov chain for a given tolerance schedule .",
    "there is a trade - off between the length of the markov chain required for samples approximately from the stationary distribution and the bias introduced by non zero tolerance . in this paper",
    "we set @xmath308 via preliminary analysis of the markov chain sampler mixing rates for a transition kernel with coefficient of variation set to one .",
    "we note that in general , practitioners will have a required precision in posterior estimates that can be directly used to determine , for a given computational budget , a suitable tolerance @xmath308 .",
    "* convergence diagnostics : * we stress that when using an mcmc - abc algorithm , it is crucial to carefully monitor the convergence diagnostics of the markov chain .",
    "this is more important in the abc context than in the general mcmc context due to the possibility of extended rejections where the markov chain can stick in a given state for long periods .",
    "this can be combatted in several ways which will be discussed once the algorithm is presented .",
    "the convergence diagnostics we consider are evaluated only on samples post annealing of the tolerance threshold and after an initial burn - in period once tolerance of @xmath308 is reached .",
    "if the total chain has length @xmath91 , the initial burn - in stage will correspond to the first @xmath309 samples and we define @xmath310 .",
    "we denote by @xmath311 the markov chain of the @xmath3-th parameter after burn - in .",
    "the diagnostics we consider are given by :    _ autocorrelation .",
    "_ this convergence diagnostic will monitor serial correlation in the markov chain .",
    "for given markov chain samples for the @xmath3-th parameter @xmath311 , we define the biased autocorrelation estimate at lag @xmath160 by @xmath312[\\theta_i^{(t+\\tau)}-\\widehat{\\mu}\\left(\\theta_i\\right)],\\ ] ] where @xmath313 and @xmath314 are the estimated mean and standard deviation of @xmath315 .     _ geweke @xcite time series diagnostic . _ for parameter",
    "@xmath315 it is calculated as follows :    1 .",
    "split the markov chain samples into two sequences , @xmath164 and @xmath165 , such that @xmath316 , and with ratios @xmath317 and @xmath318 fixed such that @xmath319 for all @xmath162 .",
    "2 .   evaluate @xmath320 and @xmath321 corresponding to the sample means on each sub sequence .",
    "3 .    evaluate consistent spectral density estimates for each sub sequence , at frequency 0 , denoted @xmath322 and @xmath323 .",
    "the spectral density estimator considered in this paper is the classical non - parametric periodogram or power spectral density estimator .",
    "we use welch s method with a hanning window ; for details see appendix [ section : power spectral density estimate ] .",
    "4 .   evaluate convergence diagnostic given by @xmath324 + according to the central limit theorem , as @xmath167 one has that @xmath325 if the sequence @xmath311 is stationary .",
    "_ gelman - rubin @xcite r - statistic diagnostic .",
    "_ this approach to convergence analysis requires that one runs multiple parallel independent markov chains each starting at randomly selected initial starting points ( we run five chains ) . for comparison purposes we split the total computational budget of @xmath162 into @xmath326 .",
    "the convergence diagnostic for parameter @xmath315 is calculated using the following steps :    1 .",
    "generate five independent markov chain sequences , producing the chains for parameter @xmath315 denoted @xmath327 for @xmath328 .",
    "2 .   calculate the sample means @xmath329 for each sequence and the overall mean @xmath330 .",
    "3 .   calculate the variance of the sequence means @xmath331 4 .",
    "calculate the within - sequence variances @xmath332 for each sequence .",
    "calculate the average within - sequence variance , @xmath333 .",
    "6 .   estimate the target posterior variance for parameter @xmath315 by the weighted linear combination @xmath334 .",
    "this estimate is unbiased for samples which are from the stationary distribution . in the case",
    "in which not all sub chains have reached stationarity , this overestimates the posterior variance for a finite @xmath162 but asymptotically , @xmath335 , it converges to the posterior variance .",
    "improve on the gaussian estimate of the target posterior given by + @xmath336 by accounting for sampling variability in the estimates of the posterior mean and variance .",
    "this can be achieved by making a student - t approximation with location @xmath330 , scale @xmath337 and degrees of freedom @xmath338 , each given respectively by : @xmath339 and @xmath340 , where the variance is estimated as + @xmath341 note , the covariance terms are estimated empirically using the within sequence estimates of the mean and variance obtained for each sequence .",
    "calculate the convergence diagnostic @xmath342 , where as @xmath167 one can prove that @xmath343 .",
    "this convergence diagnostic monitors the scale factor by which the current distribution for @xmath315 may be reduced if simulations are continued for @xmath344 .",
    "in the mahlanobis distance metric , estimation of the scaling weights is given by the covariance @xmath345 , where @xmath346 and @xmath347 are the sample mean and standard deviation of @xmath296 i.i.d .",
    "residuals @xmath348 ( see also - ) .",
    "next we outline the estimation of @xmath349 by a matrix @xmath350 .    * starting with the elements @xmath351 with @xmath352 , we obtain from the conditional resampling bootstrap * * @xmath353 if @xmath354 or @xmath355 * * @xmath356 * considering the elements @xmath357 and also @xmath358 of the covariance matrix @xmath359 , for simplicity we set @xmath360 . * considering elements @xmath361 , we assess now @xmath362 either analytically or numerically by simulation of appropriate i.i.d .  residuals .",
    "+ * parametric approximation * * * in approximating @xmath363 and @xmath364 we assume i.i.d . samples @xmath365 . * * using the assumptions we know that : + @xmath366 , + @xmath367   = \\frac{1}{(n-1)^2}[2n(1+\\frac{5}{n^2})]$ ] , + @xmath368 $ ] . * *   under these assumptions : + 1 .",
    "if the distribution of @xmath348 is skewed then it is more appropriate to do a numerical approximation with the observed residuals from the bootstrap algorithm .",
    "the precision @xmath369 from the mcmc - abc algorithm should depend on the size of the claims triangle , that is , the number of residuals @xmath296 .",
    "this is calculated via a modified technique using welch s method ; see proakis - manolakis @xcite , 910 - 913 .",
    "this involves performing the following steps :    * split each sequence @xmath164 and @xmath165 into @xmath370 non - overlapping blocks of length @xmath371 . *",
    "apply a hanning window function @xmath372 to the samples of the markov chain in each block .",
    "* take the discrete fourier transform ( dft ) of each windowed block given by @xmath373 .",
    "* estimate the spectral density ( sd ) as @xmath374 ."
  ],
  "abstract_text": [
    "<S> the intention of this paper is to estimate a bayesian distribution - free chain ladder ( dfcl ) model using approximate bayesian computation ( abc ) methodology . </S>",
    "<S> we demonstrate how to estimate quantities of interest in claims reserving and compare the estimates to those obtained from classical and credibility approaches . in this context , a novel numerical procedure utilising markov chain monte carlo ( mcmc ) , abc and a bayesian bootstrap procedure was developed in a truly distribution - free setting . </S>",
    "<S> the abc methodology arises because we work in a distribution - free setting in which we make no parametric assumptions , meaning we can not evaluate the likelihood point - wise or in this case simulate directly from the likelihood model . </S>",
    "<S> the use of a bootstrap procedure allows us to generate samples from the intractable likelihood without the requirement of distributional assumptions , this is crucial to the abc framework . </S>",
    "<S> the developed methodology is used to obtain the empirical distribution of the dfcl model parameters and the predictive distribution of the outstanding loss liabilities conditional on the observed claims </S>",
    "<S> . we then estimate predictive bayesian capital estimates , the value at risk ( var ) and the mean square error of prediction ( msep ) . </S>",
    "<S> the latter is compared with the classical bootstrap and credibility methods .    </S>",
    "<S> claims reserving , distribution - free chain ladder , mean square error of prediction , bayesian chain ladder , approximate bayesian computation , markov chain monte carlo , annealing , bootstrap </S>"
  ]
}