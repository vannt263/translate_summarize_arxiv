{
  "article_text": [
    "this workshop was initiated by the hera experiments in order to get a common vision on the issues of data persistency and long term analysis . in this context , it is interesting to look at how past experiments dealt with these issues .",
    "one example are the lep experiments which stopped data taking at the end of the year 2000 .",
    "the experiments have their data stored in the castor tape archiving system at cern .",
    "an agreement with the cern it division exists so that these tapes will be copied onto new tapes whenever there is a migration to new media .",
    "however , two experiments saw a few tapes being lost recently , so the idea came up to maintain a additional copy of the lep data at lhc tier 1 centres .",
    "the storage capacity needed for this copy ( the entire lep data set is estimated to be 100 tb including monte carlo ( mc ) , raw and reconstructed data ) would be negligible compared to the volume of the lhc data and mc productions to come .",
    "clearly , data is useless without the associated software to read and analyse it .",
    "the experiments stored their software on afs at cern .",
    "this includes reconstruction code as well as simulation code .",
    "analysis codes are also stored there to various extents .",
    "the point in time when the simulation and recostruction codes are considered to be ` frozen ' is a good occasion to collect the components of this part of the overall analysis chain , package it and make it run on a system independent of the central software installation .",
    "this guarantees to some degree that no dependencies such as detector conditions databases etc .",
    "were forgotten .",
    "however , it does not guarantee that this code will run on recent computing platforms .",
    "delphi for example has a software cd project which includes all delphi software in source form .",
    "it runs independently from afs and includes everything to run the detector simulation and reprocess data , however not the physics generators .",
    "aleph went further and also conserved the computing environment .",
    "they distributed a ` mini - system ' to each participating institute consisting of a laptop and an external disk containing the data and mc samples .",
    "it runs the linux distribution used at the end of lep as well the necessary aleph software to access it .",
    "opal attempts to keep its analysis environment alive by adapting the software to changes in the computing platform such as new os versions and new staging software .",
    "an example of a necessary migration to a different technology is the file catalogue database .",
    "most of the experiments used fatmen  @xcite for this purpose .",
    "as the central fatmen server was decommissioned years ago , the database content had to be moved to another system , e.g.  text files ( at the cost of some functionality ) .",
    "all experiments wrote most of their reconstruction and simulation code in fortran and used zebra  @xcite ( except aleph which used bos  @xcite ) for reading and writing event data to files . while fortran compilers probably will stay around for quite some time , the central support for cernlib  @xcite ( which zebra is a part of ) has unexpectedly ended and with the upcoming transition to version  5 of scientific linux cern , migration and validation of cernlib on the new standard platform will be left to the experiments . even though a subset of the cernlib functions is now part of root  @xcite , it is not straightforward to interface them with the existing experiments code as this would imply modifications the experiments software and the danger of side effects of unknown size .",
    "there are also examples of parts of the software which stopped working : the event display programs of opal and delphi do not run on recent platforms .",
    "this is because they relied on commercial libraries and there is no personpower available to re - write the complete event display at this stage .",
    "a means against the ` ageing ' of software is to move to new languages and libraries along with the operating system and hardware moves .",
    "l3 wrote c++ code which was interfaced with the existing fortran reconstruction code to read the tracks and clusters data from zebra files and write them out as root files . because root is actively supported today ( as opposed to zebra )",
    ", this improves the long term accessibility of the data .",
    "aleph and delphi developed similar c++ frameworks .",
    "all four experiments investigated quaero  @xcite as a means of preserving event - by - event data in the form of four - vectors at the level of reconstructed jets , leptons and photons .",
    "the idea was to allow future physics models beyond the standard model to be tested quickly on preserved data .",
    "quaero automatically determines the variables built from four - vector information which are most sensitive to such a model .",
    "performing such a model - independent analysis turned out to be difficult as the agreement between mc and data usually was only checked at some analysis - specific level of preselection . in addition , tracks and clusters in the event are often interpreted depending on what is optimal for a given analysis .",
    "quaero was therefore not a viable solution for data preservation .",
    "while it was straightforward to organise the physical storage of the data , funding for tapes and their migration being the only factor , there is no canonical way to ensure that the knowledge present in people s minds is written down in any form of documentation .",
    "the range of level of documentation goes from a complete user s guide to using the collaboration - wide software to one line descriptions of variables in analysis group specific ntuples .    in most cases",
    "the last link in the analysis chain , namely user - specific analysis code and plotting macros , resided in user s directories and never made it into a centrally managed repository .",
    "these directories are deleted by the hosting labs some time after a user has left the lab , a standard case for highly mobile young students who often were the responsibles of the analyses , and thus potentially important information is destroyed .",
    "information related to the limited precision of the detector simulation ( e.g.  which event variables are affected by shifts between mc and data and should be used with particular care ) was probably never systematically collected and written down . in some cases , additional smearing of variables in mc was applied in order to properly describe the data for a given analysis . however , such corrections sometimes stayed in the user s analysis code and were not propagated back into the collaboration s code .",
    "to get from the reconstructed data to the final publication requires often the equivalent of several person - years .",
    "figure  [ fig : analysis - flow ] shows a typical analysis flow of a lep experiment from the recording of the data to the final publication . for complete preservation",
    ", one could think of preserving all the intermediate steps ( shown in the following table ) required to get to the publication :    [ cols=\"^,<\",options=\"header \" , ]         of course , experiments achieved step a ) even though data are threatened by moving to newer computing platforms . in most cases , steps",
    "f ) , g ) , h ) and did not happen .",
    "step i ) was preserved for some publications , e.g.  @xcite .",
    "a handful of two - dimensional histograms were published as tables of number of background and data events as well as signal efficiencies .",
    "these values have been re - used in a search for new physics  @xcite .",
    "another example is  @xcite .",
    "this 140 page paper contains 66 pages with 77 tables . while with the former paper",
    "it is still possible to read the data from the paper and type it in by hand to use it for further interpretation , it would be a tedious and error - prone task to do this for the latter .",
    "fortunately , the durham hepdata reaction database  @xcite team extracted the tables from the electronic publication and made them available in a machine - readable format  @xcite .",
    "most experiments made step j ) which is implemented by publishers or arxiv .",
    "going from one abstraction layer of data to the next involves running computer code . in order to properly use such code",
    "one must rely on accompanying documentation .",
    "the reconstruction and detector simulation code is usually documented and thus re - usable because there is a large number of users and most users are _ not _ developers .",
    "this is completely different for analysis - specific ( ` personal ' ) code : in most cases the user and the developer are the same person and thus no effort is spent on documentation .",
    "all four experiments participated in lep wide working groups ( wg ) .",
    "the participation usually implied the exchange of some kind of data at a very high abstraction level ( but in more detail than shown in the experiments individual publications ) .",
    "it also forced the involved persons to write down documentation on how to use this data .    in some cases , mc files with four - vectors in a common format",
    "were produced ( e.g.  lep @xmath1 wg  @xcite ) or histograms with data , signal efficiencies and expected background ( lep susy wg  @xcite and lep higgs wg  @xcite ) were exchanged . for the lep higgs wg group ,",
    "some features of the events observed at highest signal / background were exchanged  @xcite .",
    "aleph currently has one ongoing analysis , potentially a few more ; opal has 4 - 5 papers in the pipeline , l3 has still about five ongoing analyses while delphi has ten papers to be published ( for five of them analysis is still ongoing , i.e.   analysis jobs running ) .",
    "thus even years after the collaborations have disbanded , physics results are still being produced .    during the active life of the collaboration a paper needed to be approved by the editorial board prior to publication .",
    "aleph has a re - use policy which deviates from this standard procedure  @xcite . under this agreement",
    ", any member of the aleph collaboration can now publish an analysis using the archived data without having to go through the full approval process .",
    "even people outside aleph can use aleph data for publications if the author list includes at least one former aleph member .",
    "some conditions must be met e.g.   that the analysis does not attempt to reproduce certain results such as precision measurements .",
    "about five publications were already published under this scheme including a re - fit of @xmath2 to predictions at next - to - next - to - leading order qcd  @xcite which only became available several years after the end of data taking  @xcite .",
    "this analysis could therefore not have been performed during the lifetime of the collaboration .",
    "opal has a policy similar to the one of aleph ( i.e.  outsiders can join ) but still requires approval by the long - term editorial board  @xcite .",
    "delphi foresees to have a similar policy  @xcite once the collaboration is considered to have disbanded .",
    "should we observe signs of physics beyond the standard model in the future , one wants to test the sensitivty of the lep data to such physics models or even confirm or exclude them . the first step (",
    "as suggested in  @xcite ) will be to calculate how such new physics modifies observables measured with great precision at lep and compare to the published values . for more in - depth studies",
    ", it will be necessary to run an event generator for the physics processes in question and very likely this requires the experiments software to run on today s computing platforms .",
    "the long term storage of files and associated software was well organised at the end of data taking .",
    "however , due to the shift from fortran to c++ programming , support for the software libraries essential for accessing and analysing data is fading out . thus efforts will be needed to assure access to the very valuable lep data in the future .",
    "further work is required to preserve ( write down ) the knowledge inside the former members minds .    in case",
    "there is a wish or the need to re - use of lep data ( e.g.  in light of new theoretical calculations or signs of new physics at running or future experiments ) , there are still some collaboration members around to do this .",
    "aleph and opal ( and most likely delphi in the future as well ) even allow the use of their data by ` outsiders ' under the guidance of a former collaboration member ."
  ],
  "abstract_text": [
    "<S> the four lep experiments aleph , delphi , l3 and opal successfully recorded @xmath0 collision data during the years 1989 to 2000 . </S>",
    "<S> as part of the ordinary evolution in high energy physics , these experiments can not be repeated and their data is therefore unique . </S>",
    "<S> this article briefly reviews the data preservation efforts undertaken by the four experiments beyond the end of data taking . </S>",
    "<S> the current status of the preserved data and associated tools is summarised . </S>"
  ]
}