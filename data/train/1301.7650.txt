{
  "article_text": [
    "the multi - level monte carlo method proposed in @xcite approximates the expectation of some functional applied to some stochastic processes like e.  g.  solutions of stochastic differential equations ( sdes ) at a lower computational complexity than classical monte carlo simulation , see also @xcite .",
    "multi - level monte carlo approximation is applied in many fields like mathematical finance @xcite , for sdes driven by a lvy process @xcite or by fractional brownian motion @xcite .",
    "the main idea of this article is to reduce the computational costs further by applying the multi - level monte carlo method as a variance reduction technique for some higher order weak approximation method . as a result",
    ", the computational effort can be significantly reduced while the optimal order of convergence for the root mean - square error is preserved .",
    "+   + the outline of this paper is as follows .",
    "we give a brief introduction to the main ideas and results of the multi - level monte carlo method in section  [ section2:mlmc - simulation - original ] .",
    "based on these results , in section  [ sec3:improved - mlmc - estimator ] we present as the main result a modified multi - level monte carlo algorithm that allows to reduce the computational costs significantly .",
    "depending on the relationship between the orders of variance reduction and of the growth of the costs , there exists a reduction of the computational costs by a factor depending on the weak order of the underlying numerical method . as an example , the modified multi - level monte carlo algorithm is applied to the problem of weak approximation for stochastic differential equations driven by brownian motion in section  [ sec4:numerical - examples - sdes ] .",
    "let @xmath3 be a probability space with some filtration @xmath4 and let @xmath5 denote a stochastic process on the interval @xmath6 $ ] adapted to the filtration . in the following , we are interested in the approximation of @xmath7 for some functional @xmath8 where @xmath9 denotes a suitable class of functionals that are of interest .",
    "further , let an equidistant discretization @xmath10 with @xmath11 of the time interval @xmath12 with step size @xmath13 be given .",
    "then , we consider a probability space @xmath14 with some filtration @xmath15 and we denote by @xmath16 a discrete time approximation of @xmath17 on the grid @xmath18 , adapted to @xmath19 . here , the probability spaces @xmath3 and @xmath20 may be but do not have to be equal and we assume that @xmath21 approximates @xmath17 in the weak sense with some order @xmath22 , i.e.@xmath23 for all @xmath8 .",
    "+   + in order to approximate the expectation of @xmath24 we apply the multi - level monte carlo estimator introduced in @xcite . for some fixed @xmath25 with @xmath26 and some @xmath27",
    "we define the step sizes @xmath28 and let @xmath29 denote the discrete time approximation process on the grid @xmath30 based on step size @xmath31 for @xmath32 . then , the multi - level monte carlo estimator is defined by @xmath33 for some @xmath34 using the estimators @xmath35 and @xmath36 for @xmath37 . then , we get @xmath38 here",
    ", we have to point out , that both approximations @xmath39 and @xmath40 are simulated simultaneously based on the same realisation of the underlying driving random process whereas @xmath41 and @xmath42 are independent realisations for @xmath43 .",
    "+   + now , there are two sources of errors for the approximation . on the one hand",
    ", we have a systematical error due to the discrete time approximation @xmath44 based on step size @xmath13 which is given by the bias of the method . on the other hand",
    ", there is a statistical error from the estimator for the expectation of @xmath45 by the monte carlo simulation .",
    "therefore , we consider the root mean - square error @xmath46 of the multi - level monte carlo method in the following . in order to rate the performance of an approximation method",
    ", we will analyse the root mean - square error of the method compared to the computational costs . therefore ,",
    "we denote by @xmath47 the computational costs of the approximation method @xmath21 . in order to determine @xmath47 ,",
    "one may use a cost model where e.g.  each operation or evaluation of some function is charged with the price of one unit , i.e.  one counts the number of needed mathematical operations or function evaluations .",
    "further , each random number that has to be generated to compute @xmath21 may also be charged with the price of one unit .",
    "+   + then , it is well known that the optimal order of convergence in the case of the classical monte carlo estimator @xmath48 is given by @xmath49 where @xmath2 is the weak order of convergence of the approximations @xmath21 , see duffie and glynn  @xcite .",
    "thus , higher order weak approximation methods result in a higher order of convergence with respect to the root mean - square error .",
    "clearly , the best root mean - square order of convergence that can be achieved is at most @xmath50 .",
    "however , the order bound @xmath50 can not be reached by any weak order @xmath2 approximation method in the case of the classical monte carlo simulation .",
    "therefore , in order to attain the optimal order of convergence for the root mean - square error we apply the multi - level monte carlo estimator ( [ mlmc - estimator - giles ] ) . the following theorem due to giles  @xcite",
    "is presented in a slightly generalized version suitable for our considerations .",
    "[ main - theorem - giles ] for some @xmath34 , let @xmath51 denote the approximation process on the grid @xmath30 with respect to step size @xmath52 for each @xmath53 , respectively .",
    "suppose that there exist some constants @xmath54 and @xmath55 and @xmath56 such that for the bias    1 .",
    "@xmath57    and for the variances    1 .",
    "@xmath58 , 2 .",
    "@xmath59 for @xmath60 , 3 .",
    "@xmath61 .",
    "further , assume that there exist constants @xmath62 and @xmath63 such that for the computational costs    1 .",
    "@xmath64 , 2 .",
    "@xmath65 for @xmath60 , 3 .",
    "@xmath66 .",
    "then , for some arbitrarily prescribed error bound @xmath67 there exist values @xmath68 and @xmath69 for @xmath53 , such that the root mean - square error of the multi - level monte carlo estimator @xmath70 has the bound @xmath71 with computational costs bounded by @xmath72 for some positive constant @xmath73 .",
    "the calculations for the proof follow the lines of the original proof due to giles @xcite . considering the mean square - error @xmath74 we make use of the weight @xmath750,1[\\,$ ] and claim that @xmath76 then , we can calculate @xmath68 from the bias and we have to solve the minimization problem @xmath77 under the constraint that @xmath78 . as a result of this",
    ", we obtain the following values for @xmath68 and @xmath69 : +   + let @xmath79 and @xmath80 , @xmath81 for @xmath60 and @xmath82 for some @xmath830,1[\\,$ ] with :    * in case of @xmath84 and @xmath85 or in case of @xmath86 and @xmath87 : @xmath88 * in case of @xmath89 and @xmath85 : @xmath90",
    "the order of convergence of the multi - level monte carlo estimator @xmath70 given in ( [ mlmc - estimator - giles ] ) is optimal in the given framework .",
    "however , the computational costs can be reduced if a modified estimator is applied .",
    "as yet , the estimator @xmath70 is based on some weak order @xmath1 approximations @xmath51 for @xmath53 on each level .",
    "now , let us apply some cheap low order weak approximation @xmath51 on levels @xmath91 combined with some probably expansive high order weak approximation @xmath92 on the finest level @xmath68 .",
    "the idea is , that the approximations @xmath51 contribute a variance reduction while the approximation @xmath92 results in a small bias of the multi - level monte carlo estimator , thus reducing the number of levels needed to attain a prescribed accuracy .",
    "+   + let @xmath21 be an order @xmath1 weak approximation method and let @xmath93 be an order @xmath2 weak approximation method applied on the finest level .",
    "further , let @xmath94 with @xmath95 denote the number of levels in order to indicate the dependence on the weak order @xmath2 .",
    "then , we define the modified multi - level monte carlo estimator by @xmath96 with the estimators @xmath97 for @xmath98 based on the order @xmath1 weak approximations @xmath51 as defined in section  [ section2:mlmc - simulation - original ] , however now applying the modified estimator @xmath99 which combines the weak order @xmath1 approximations @xmath100 with the weak order @xmath2 approximations @xmath101 . clearly , all conditions of theorem  [ main - theorem - giles ] have to be fulfilled for @xmath102 replaced by @xmath92 .",
    "then , in the case of @xmath103 , the improved multi - level monte carlo estimator @xmath104 features significantly reduced computational costs compared to the originally proposed estimator @xmath105 .    [ main - prop - improvement ]",
    "let conditions 1)7 ) of theorem  [ main - theorem - giles ] be fulfilled and suppose that there exist constants @xmath106 and @xmath107 such that for the computational costs    1 .",
    "@xmath108 , 2 .",
    "@xmath109 for @xmath110 , 3 .",
    "@xmath111    with some @xmath112 such that @xmath113 and @xmath114 .",
    "then , the multi - level monte carlo estimator @xmath104 based on a weak order @xmath115 approximation scheme on levels @xmath116 and some weak order @xmath103 approximation scheme on level @xmath117 has reduced computational costs :    a.   in case of @xmath84 and @xmath118 , there exists some @xmath119 such that for all @xmath1200,\\varepsilon_0]$ ] holds @xmath121 provided that @xmath122 , @xmath123 and @xmath124 . in case of @xmath84 and @xmath125 then ( [ main - prop - improvement - aussage1 ] ) holds if in addition @xmath126 and @xmath127 .",
    "further , for @xmath128 it holds @xmath129 if @xmath115 and @xmath130 .",
    "+ b.   in case of @xmath89 and @xmath131 and if @xmath132 , @xmath122 , we get @xmath133 and @xmath134 if @xmath115 and @xmath123 .",
    "+ c.   in case of @xmath86 and @xmath135 we obtain @xmath136 if @xmath137 .",
    "if the parameter @xmath750,1[\\,$ ] is chosen as @xmath138 then the computational costs @xmath139 are asymptotically minimal . in general , if @xmath86 or if @xmath140 then it holds that @xmath141 for @xmath142 .",
    "* proof . *",
    "assume that @xmath143 .",
    "let @xmath144 , @xmath145 , @xmath146 and @xmath147 .",
    "then , the computational costs for @xmath104 are @xmath148 with @xmath149 and @xmath69 for @xmath150 given in .",
    "without loss of generality , suppose that @xmath151 for @xmath43 and that @xmath152 with @xmath153 in the case of @xmath154 .",
    "in the following , we make use of the two estimates @xmath155 let @xmath156 .",
    "then , we obtain the lower bound @xmath157 \\label{proof - main - prop - lower - bound - ieq - alg } \\ ] ] where @xmath158 , @xmath159 , @xmath160 , @xmath161 and @xmath162 for @xmath163 . +   + next , we calculate for the case of @xmath156 the upper bound @xmath164 \\notag \\\\          & + t \\sum_{i=0}^k \\left ( \\hat{c}_{3,0}^{(i ) } t^{\\delta_i-\\gamma }      + \\hat{c}_3^{(i ) } \\frac{(m^{-1 } t)^{\\delta_i-\\gamma } - h_{l_p}^{\\delta_i-\\gamma } }      { 1-m^{\\gamma-\\delta_i } } + \\hat{c}_{3,l_p}^{(i ) } h_{l_p}^{\\delta_i-\\gamma_{l_p } } \\right )      \\label{proof - main - prop - upper - bound - ieq - alg } \\ ] ] with @xmath165 for @xmath166 .",
    "+   + in case of @xmath84 and @xmath167 , we prove that there exists some @xmath119 such that for all @xmath1200,\\varepsilon_0]$ ] follows @xmath168 . from the lower bound ( [ proof - main - prop - lower - bound - ieq - alg ] ) for @xmath169 and the upper bound ( [ proof - main - prop - upper - bound - ieq - alg ] ) for @xmath170 we get the estimate @xmath171 in the following , we make us of @xmath172 and @xmath173 , i.e.  we have @xmath174 and @xmath175 as @xmath176 .",
    "+   + multiplying both sides of ( [ proof - main - prop - difference - ieq - alg ] ) with @xmath177 and taking into account the assumptions @xmath178 and @xmath179 results in @xmath180      h_{l_p}^{-\\frac{\\min \\{\\beta-\\gamma,\\beta_{l_p}-\\gamma_{l_p}\\}}{2}}\\ , .",
    "\\label{proof - main - prop - difference2-ieq - alg } \\ ] ] as a result of ( [ proof - main - prop - difference2-ieq - alg ] ) follows that in the case of @xmath118 there exists some @xmath119 such that @xmath181 for all @xmath1200,\\varepsilon_0]$ ] . in the case of @xmath125",
    "there exists some @xmath119 such that ( [ proof - main - prop - result - case-1 ] ) holds for all @xmath1200,\\varepsilon_0]$ ] if @xmath182 and @xmath183 .",
    "finally , @xmath184 follows from ( [ proof - main - prop - upper - bound - ieq - alg ] ) . +   +",
    "in case of @xmath86 and @xmath185 , we have to compare the dominating terms as @xmath176 .",
    "therefore , we get from the lower bound that @xmath186 and from the upper bound @xmath187 making use of these two estimates ( [ proof - main - prop - lower - bound - ieq - simp ] ) and ( [ proof - main - prop - upper - bound - ieq - simp ] ) , this results in the estimate ( [ main - prop - improvement - aussage3 ] ) where @xmath140 because we require that @xmath188 .",
    "+   + in general , it follows that @xmath189 from the upper bound ( [ proof - main - prop - upper - bound - ieq - alg ] ) for @xmath86 and any @xmath190 , @xmath191 .",
    "further , there is an asymptotically optimal choice for the parameter @xmath750,1[\\,$ ] such that the computational costs are asymptotically minimal . calculating a lower bound for @xmath170 and taking into account the upper bound ( [ proof - main - prop - upper - bound - ieq - simp ] )",
    ", we get @xmath192 with some constant @xmath193 independent of @xmath194 and @xmath195 .",
    "now , we have to find some @xmath1960,1[\\,$ ] such that @xmath1970,1 [ \\ , } c \\varepsilon^{-2-\\frac{\\gamma-\\beta}{p } }      \\frac{q^{\\frac{\\beta-\\gamma}{2p}}}{1-q}\\ ] ] for all @xmath198 .",
    "solving this minimization problem leads to @xmath199 which is asymptotically the optimal choice for @xmath830,1[\\,$ ] in case of @xmath86 .",
    "+   + in case of @xmath89 , we get the following lower bound @xmath200 where @xmath201 , @xmath159 , @xmath160 , @xmath161 and @xmath162 for @xmath163 . +   + next , we calculate for @xmath89 the upper bound @xmath202 \\notag \\\\      & + t \\sum_{i=0}^k \\left ( \\hat{c}_{3,0}^{(i ) } t^{\\delta_i-\\gamma }      + \\hat{c}_3^{(i ) } \\frac{(m^{-1 } t)^{\\delta_i-\\gamma } - h_{l_p}^{\\delta_i-\\gamma}}{1-m^{\\gamma-\\delta_i } }      + \\hat{c}_{3,l_p}^{(i ) } h_{l_p}^{\\delta_i-\\gamma_{l_p } } \\right )      \\label{proof - main - prop - upper - bound - eq - alg } \\ ] ] where we applied the relation ( [ l-1-upper - bound ] ) .",
    "+   + suppose that @xmath131 and @xmath203 .",
    "then , we get from the upper bound ( [ proof - main - prop - upper - bound - eq - alg ] ) that @xmath204 .",
    "further , comparing the lower and the upper bounds ( [ proof - main - prop - lower - bound - eq - alg ] ) and ( [ proof - main - prop - upper - bound - eq - alg ] ) , we asymptotically obtain that @xmath205 which proves statement ( [ main - prop - improvement - aussage2 ] ) . @xmath206",
    "especially , if @xmath207 and @xmath208 , then it follows in case of @xmath86 and @xmath185 that @xmath209 thus , if @xmath210 it follows directly that @xmath211",
    "for illustration of the improvement that can be realized with the proposed modified multi - level monte carlo estimator , we consider the problem of weak approximation for stochastic differential equations ( sdes ) @xmath212 with initial value @xmath213 driven by @xmath214-dimensional brownian motion .",
    "+   + in the following , we compare for several numerical examples the root mean - square errors versus the computational costs for the multi - level monte carlo estimator @xmath70 proposed in @xcite and described in section  [ section2:mlmc - simulation - original ] with the proposed modified multi - level monte carlo estimator @xmath104 described in section  [ sec3:improved - mlmc - estimator ] . as a measure for the computational costs , we count the number of evaluations of the drift and diffusion functions taking into account the dimension @xmath215 of the solution process as well as the dimension @xmath214 of the driving brownian motion . +   + in the following , we consider on each level @xmath53 an equidistant discretization @xmath216 of @xmath217 $ ] with step size @xmath218 .",
    "further , we denote by @xmath219 the approximation at time @xmath220 . in case of the multi - level monte carlo estimator @xmath70 we apply on each level @xmath53 the euler - maruyama scheme on the grid @xmath30 given by @xmath221 and @xmath222 where @xmath223 and @xmath224 for @xmath225 .",
    "the euler - maruyama scheme converges with order @xmath50 in the mean - square sense and with order @xmath226 in the weak sense to the solution of the considered sde at time @xmath227 @xcite .",
    "+   + on the other hand , for the modified multi - level monte carlo estimator @xmath104 the euler - maruyama scheme is applied on levels @xmath116 whereas on level @xmath117 a second order weak stochastic runge - kutta ( srk ) scheme ri6 proposed in @xcite is applied . the srk scheme ri6 on level @xmath117 is defined on the grid @xmath228 by @xmath229 , @xmath230 where @xmath231 and @xmath232 for @xmath233 with stages @xmath234 where @xmath235 and @xmath236 based on independent random variables @xmath237 with @xmath238 .",
    "thus , we have @xmath226 and @xmath239 for the modified multi - level monte carlo estimator @xmath104 in the following .",
    "further , for both schemes the variance decays with the same order as the computational costs increase , i.  e.@xmath240 .",
    "then , the optimal order of convergence attained by the multi - level monte carlo method is @xmath241 due to theorem  [ main - theorem - giles ] . for the presented simulations , we denote by mlmc em the numerical results for @xmath70 based on the euler - maruyama scheme only and by mlmc srk the results for @xmath104 based on the combination of the euler - maruyama scheme and the srk scheme ri6 .",
    "+   + as a first example , we consider the scalar linear sde with @xmath242 given by @xmath243 using the parameters @xmath244 and @xmath245 .",
    "we choose @xmath246 and apply the functionals @xmath247 and @xmath248 , see figure  [ bild - test1 ] .",
    "the presented simulations are calculated using the prescribed error bounds @xmath249 for @xmath250 . in figure  [ bild - test1 ]",
    "we can see the significantly reduced computational effort for the estimator @xmath251 ( mlmc srk ) compared to the estimator @xmath70 ( mlmc em ) in case of a linear and a nonlinear functional .",
    "( left ) and @xmath248 ( right).,title=\"fig:\",width=207 ]   ( left ) and @xmath248 ( right).,title=\"fig:\",width=207 ]    the second example is a nonlinear scalar sde with @xmath242 given by @xmath252 we apply the functional @xmath253 .",
    "then , the approximated expectation is given by @xmath254 . here , the results presented in figure  [ bild - test2 ] ( left ) are calculated for @xmath255 applying the prescribed error bounds @xmath249 for @xmath256 . here",
    ", the improved estimator @xmath251 performs much better than @xmath70 also for nonlinear functionals and a nonlinear sde .",
    "finally , we consider a nonlinear multi - dimensional sde with a @xmath257 dimensional solution process driven by an @xmath258 dimensional brownian motion with non - commutative noise : @xmath259 with initial condition @xmath260 .",
    "then , the approximated first moment of the solution is given by @xmath261 for @xmath262 .",
    "the simulation results calculated at @xmath246 for the error bounds @xmath249 for @xmath256 are presented in figure  [ bild - test2 ] ( right ) .",
    "again , in the multi - dimensional non - commutative noise case the proposed estimator @xmath251 needs significantly less computational effort compared to the estimator @xmath70 which reveals the theoretical results in proposition  [ main - prop - improvement ] .",
    "in this paper we proposed a modification of the multi - level monte carlo method introduced by m.  giles which combines approximation methods of different orders of weak convergence .",
    "this modified multi - level monte carlo method attains the same mean square order of convergence like the originally proposed method that is in some sense optimal .",
    "however , the newly proposed multi - level monte carlo estimator can attain significantly reduced computational costs . as an example , there is a reduction of costs by a factor @xmath0 for the problem of weak approximation for sdes driven by brownian motion in case of @xmath89 .",
    "this has been approved by some numerical examples for the case of @xmath239 and @xmath226 where four times less calculations are needed compared to the standard multi - level monte carlo estimator . here , we want to point out that there also exist higher order weak approximation schemes , e.  g.@xmath263 in case of sdes with additive noise @xcite , that may further improve the benefit of the modified multi - level monte carlo estimator .",
    "future research will consider the application of this approach to , e.g. , more general sdes like sdes driven by lvy processes @xcite or fractional brownian motion @xcite and to the numerical solution of spdes @xcite .",
    "further , the focus will be on numerical schemes that feature not only high orders of convergence by also minimized constants for the variance estimates ."
  ],
  "abstract_text": [
    "<S> the multi - level monte carlo method proposed by m.  giles  ( 2008 ) approximates the expectation of some functionals applied to a stochastic process with optimal order of convergence for the mean - square error . in this paper , a modified multi - level monte carlo estimator </S>",
    "<S> is proposed with significantly reduced computational costs . as the main result </S>",
    "<S> , it is proved that the modified estimator reduces the computational costs asymptotically by a factor @xmath0 if weak approximation methods of orders @xmath1 and @xmath2 are applied in case of computational costs growing with same order as variances decay .    and    </S>",
    "<S> multi - level monte carlo , monte carlo , variance reduction , weak approximation , stochastic differential equation + msc 2000 : 65c30 , 60h35 , 65c20 , 68u20 </S>"
  ]
}