{
  "article_text": [
    "bayesian models can be evaluated and compared in several ways .",
    "most simply , any model or set of models can be taken as an exhaustive set , in which case all inference is summarized by the posterior distribution .",
    "the fit of model to data can be assessed using posterior predictive checks ( rubin , 1984 ) , prior predictive checks ( when evaluating potential replications involving new parameter values ) , or , more generally , mixed checks for hierarchical models ( gelman , meng , and stern , 2006 ) .",
    "when several candidate models are available , they can be compared and averaged using bayes factors ( which is equivalent to embedding them in a larger discrete model ) or some more practical approximate procedure ( hoeting et al . , 1999 ) or continuous model expansion ( draper , 1999 ) .    in other settings ,",
    "however , we seek not to check models but to compare them and explore directions for improvement . even if all of the models being considered have mismatches with the data",
    ", it can be informative to evaluate their predictive accuracy , compare them , and consider where to go next .",
    "the challenge then is to estimate predictive model accuracy , correcting for the bias inherent in evaluating a model s predictions of the data that were used to fit it .    a natural way to estimate",
    "out - of - sample prediction error is cross - validation ( see vehtari and lampinen , 2002 , for a bayesian perspective ) , but researchers have always sought alternative measures , as cross - validation requires repeated model fits and can run into trouble with sparse data . for practical reasons alone , there remains a place for simple bias corrections such as aic ( akaike , 1973 ) , dic ( spiegelhalter et al . , 2002 , van der linde , 2005 ) , and , more recently , waic ( watanabe , 2010 ) , and all these can be viewed as approximations to different versions of cross - validation ( stone , 1977 ) .    at the present time , dic appears to be the predictive measure of choice in bayesian applications , in part because of its incorporation in the popular bugs package ( spiegelhalter et al . , 1994 , 2003 ) .",
    "various difficulties have been noted with dic ( see celeux et al .",
    ", 2006 , plummer , 2008 , and much of the discussion of spiegelhalter et al . , 2002 ) but there has been no consensus on an alternative .",
    "one difficulty is that all the proposed measures are attempting to perform what is , in general , an impossible task : to obtain an unbiased ( or approximately unbiased ) and accurate measure of out - of - sample prediction error that will be valid over a general class of models and that requires minimal computation beyond that needed to fit the model in the first place .",
    "when framed this way , it should be no surprise to learn that no such ideal method exists .",
    "but we fear that the lack of this panacea has impeded practical advances , in that applied users are left with a bewildering array of choices .",
    "the purpose of the present article is to explore aic , dic , and waic from a bayesian perspective in some simple examples .",
    "much has been written on all these methods in both theory and practice , and we do not attempt anything like a comprehensive review ( for that , see vehtari and ojanen , 2012 ) .",
    "our unique contribution here is to view all these methods from the standpoint of bayesian practice , with the goal of understanding certain tools that are used to understand models .",
    "we work with three simple ( but , it turns out , hardly trivial ) examples to develop our intuition about these measures in settings that we understand .",
    "we do not attempt to derive the measures from first principles ; rather , we rely on the existing literature where these methods have been developed and studied .    in some ways ,",
    "our paper is similar to the review article by gelfand and dey ( 1994 ) , except that they were focused on model choice whereas our goal is more immediately to estimate predictive accuracy for the goal of model comparison . as we shall discuss in the context of an example , given the choice between two particular models",
    ", we might prefer the one with higher expected predictive error ; nonetheless we see predictive accuracy as one of the criteria that can be used to evaluate , understand , and compare models .",
    "one way to evaluate a model is through the accuracy of its predictions .",
    "sometimes we care about this accuracy for its own sake , as when evaluating a forecast .",
    "in other settings , predictive accuracy is valued not for its own sake but rather for comparing different models .",
    "we begin by considering different ways of defining the accuracy or error of a model s predictions , then discuss methods for estimating predictive accuracy or error from data .",
    "consider data @xmath0 , modeled as independent given parameters @xmath1 ; thus @xmath2 . with regression",
    ", one would work with @xmath3 . in our notation here",
    "we suppress any dependence on @xmath4 .",
    "preferably , the measure of predictive accuracy is specifically tailored for the application at hand , and it measures as correctly as possible the benefit ( or cost ) of predicting future data with the model .",
    "often explicit benefit or cost information is not available and the predictive performance of a model is assessed by generic scoring functions and rules .",
    "measures of predictive accuracy for point prediction are called scoring functions .",
    "a good review of the most common scoring functions is presented by gneiting ( 2011 ) , who also discusses the desirable properties for scoring functions in prediction problems .",
    "we use the squared error as an example scoring function for point prediction , because the squared error and its derivatives seem to be the most common scoring functions in predictive literature ( gneiting , 2011 ) .",
    "measures of predictive accuracy for probabilistic prediction are called scoring rules .",
    "examples include the quadratic , logarithmic , and zero - one scores , whose properties are reviewed by gneiting and raftery ( 2007 ) .",
    "bernardo and smith ( 1994 ) argue that suitable scoring rules for prediction are proper and local : propriety of the scoring rule motivates the decision maker to report his or her beliefs honestly , and for local scoring rules predictions are judged only on the plausibility they assign to the event that was actually observed , not on predictions of other events .",
    "the logarithmic score is the unique ( up to an affine transformation ) local and proper scoring rule ( bernardo , 1979 ) , and appears to be the most commonly used scoring rule in model selection .",
    "[ [ mean - squared - error . ] ] mean squared error .",
    "+ + + + + + + + + + + + + + + + + + +    a model s fit to new data can be summarized numerically by mean squared error , @xmath5 , or a weighted version such as @xmath6 .",
    "these measures have the advantage of being easy to compute and , more importantly , to interpret , but the disadvantage of being less appropriate for models that are far from the normal distribution .    [",
    "[ log - predictive - density - or - log - likelihood . ] ] log predictive density or log - likelihood .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    a more general summary of predictive fit is the log predictive density , @xmath7 , which is proportional to the mean squared error if the model is normal with constant variance .",
    "the log predictive density is also sometimes called the log - likelihood .",
    "the log predictive density has an important role in statistical model comparison because of its connection to the kullback - leibler information measure ( see burnham and anderson , 2002 , and robert , 1996 ) . in the limit of large sample sizes , the model with the lowest kullback - leibler information  and thus , the highest expected log predictive density  will have the highest posterior probability .",
    "thus , it seems reasonable to use expected log predictive density as a measure of overall model fit .    given that we are working with the log predictive density",
    ", the question may arise : why not use the log posterior ?",
    "why only use the data model and not the prior density in this calculation ?",
    "the answer is that we are interested here in summarizing the fit of model _ to data _ , and for this purpose the prior is relevant in estimating the parameters but not in assessing a model s accuracy .",
    "we are not saying that the prior can not be used in assessing a model s fit to data ; rather we say that the prior density is not relevant in computing _ predictive _ accuracy .",
    "predictive accuracy is not the only concern when evaluating a model , and even within the bailiwick of predictive accuracy , the prior is relevant in that it affects inferences about @xmath1 and thus affects any calculations involving @xmath8 . in a sparse - data setting ,",
    "a poor choice of prior distribution can lead to weak inferences and poor predictions .",
    "under standard conditions , the posterior distribution , @xmath9 , approaches a normal distribution in the limit of increasing sample size ( see , e.g. , degroot , 1970 ) . in this asymptotic limit",
    ", the posterior is dominated by the likelihood  the prior contributes only one factor , while the likelihood contributes @xmath10 factors , one for each data point  and",
    "so the likelihood function also approaches the same normal distribution .    as sample size @xmath11",
    ", we can label the limiting posterior distribution as @xmath12 . in this limit",
    "the log predictive density is p(y|)=c(y)-(k(2 ) + |v_0/n| + ( -_0)^t(v_0/n)^-1(-_0 ) ) , where @xmath13 is a constant that only depends on the data @xmath14 and the model class but not on the parameters @xmath1 .",
    "the limiting multivariate normal distribution for @xmath1 induces a posterior distribution for the log predictive density that ends up being a constant ( equal to @xmath15 ) minus @xmath16 times a @xmath17 random variable , where @xmath18 is the dimension of @xmath1 , that is , the number of parameters in the model .",
    "the maximum of this distribution of the log predictive density is attained when @xmath1 equals the maximum likelihood estimate ( of course ) , and its posterior mean is at a value @xmath19 lower . for actual posterior distributions ,",
    "this asymptotic result is only an approximation , but it will be useful as a benchmark for interpreting the log predictive density as a measure of fit .    with singular models ( e.g. mixture models and overparameterized complex models more generally ) a set of different parameters can map to a single data model , the fisher information matrix is not positive definite , plug - in estimates are not representative of the posterior , and the distribution of the deviance does not converge to a @xmath20 distribution . the asymptotic behavior of such models can be analyzed using singular learning theory ( watanabe , 2009 , 2010 ) .",
    "the ideal measure of a model s fit would be its out - of - sample predictive performance for new data produced from the true data - generating process .",
    "we label @xmath21 as the true model , @xmath14 as the observed data ( thus , a single realization of the dataset @xmath14 from the distribution @xmath22 ) , and @xmath23 as future data or alternative datasets that could have been seen . the out - of - sample predictive fit for a new data point @xmath24 using logarithmic score is then , p_post(_i ) = _ post(p(_i| ) ) = p(_i|)p_post()d . in the above expression",
    ", @xmath25 is the predictive density for @xmath24 induced by the posterior distribution @xmath26 .",
    "we have introduced the notation @xmath27 here to represent the posterior distribution because our expressions will soon become more complicated and it will be convenient to avoid explicitly showing the conditioning of our inferences on the observed data @xmath14 .",
    "more generally , we use @xmath27 and @xmath28 to denote any probability or expectation that averages over the posterior distribution of @xmath1 .",
    "we must then take one further step .",
    "the future data @xmath24 are themselves unknown and thus we define the expected out - of - sample log predictive density , & = & + [ elppd ] & = & _ f ( p_post(_i ) ) = ( p_post(_i ) ) f(_i)d . in the machine learning literature",
    "this is often called the mean log predictive density . in any application",
    ", we would have some @xmath27 but we do not in general know the data distribution @xmath21 . a natural way to estimate the expected out - of - sample log predictive density would be to plug in an estimate for @xmath21 , but this will tend to imply too good a fit , as we discuss in section [ section.waic ] . for now we consider the estimation of predictive accuracy in a bayesian context .    to keep comparability with the given dataset , one can define a measure of predictive accuracy for the @xmath10 data points taken one at a time : & = & + [ elppd_2 ] & = & _ i=1^n _ f ( p_post(_i ) ) , which must be defined based on some agreed - upon division of the data @xmath14 into individual data points @xmath29 .",
    "the advantage of using a pointwise measure , rather than working with the joint posterior predictive distribution , @xmath30 is in the connection of the pointwise calculation to cross - validation , which allows some fairly general approaches to approximation of out - of - sample fit using available data .",
    "it is sometimes useful to consider predictive accuracy given a point estimate @xmath31 , thus , [ elpepd ]  e_f ( p(| ) ) . for models with independent data given parameters , there is no difference between joint or pointwise prediction given a point estimate , as @xmath32 .      in practice",
    "the parameter @xmath1 is not known , so we can not know the log predictive density @xmath7 .",
    "for the reasons discussed above we would like to work with the posterior distribution , @xmath33 , and summarize the predictive accuracy of the fitted model to data by @xmath34 to compute this predictive density in practice , we can evaluate the expectation using draws from @xmath26 , the usual posterior simulations , which we label @xmath35 , @xmath36 : & = & + & = & _ i=1^n ( _ s=1^s p(y_i|^s ) ) .",
    "[ butgut3 ] we typically assume that the number of simulation draws @xmath37 is large enough to fully capture the posterior distribution ; thus we shall refer to the theoretical value ( [ lppd ] ) and the computation ( [ butgut3 ] ) interchangeably as the log pointwise predictive density or lppd of the data .    as we shall discuss in section [ section.waic ] ,",
    "the lppd of observed data @xmath14 is an underestimate of the elppd for future data ( [ elppd_2 ] ) .",
    "hence the plan is to like to start with ( [ butgut3 ] ) and then apply some sort of bias correction to get a reasonable estimate of ( [ elppd_2 ] ) .      as is well known in hierarchical modeling ( see , e.g. , spiegelhalter et al . , 2002 , gelman et al . , 2003 ) , the line separating prior distribution from likelihood is somewhat arbitrary and is related to the question of what aspects of the data will be changed in hypothetical replications . in a hierarchical model with direct parameters @xmath38 and hyperparameters @xmath39 , factored as @xmath40 , we can imagine replicating new data in existing groups ( with the ` likelihood ' being proportional to @xmath41 ) or new data in new groups ( a new @xmath42 is drawn , and the ` likelihood ' is proportional to @xmath43 ) . in either case",
    "we can easily compute the posterior predictive density of the observed data @xmath14 :    * when predicting @xmath44 ( that is , new data from existing groups ) , we compute @xmath45 for each posterior simulation @xmath46 and then take the average , as in ( [ butgut3 ] ) . * when predicting @xmath47 ( that is , new data from a new group ) , we sample @xmath48 from @xmath49 to compute @xmath50 .",
    "similarly , in a mixture model , we can consider replications conditioning on the mixture indicators , or replications in which the mixture indicators are redrawn as well .",
    "similar choices arise even in the simplest experiments .",
    "for example , in the model @xmath51 , we have the option of assuming the sample size is fixed by design ( that is , leaving @xmath10 unmodeled ) or treating it as a random variable and allowing a new @xmath52 in a hypothetical replication .",
    "we are not bothered by the nonuniqueness of the predictive distribution .",
    "just as with posterior predictive checks ( rubin , 1984 ) , different distributions correspond to different potential uses of a posterior inference . given some particular data ,",
    "a model might predict new data accurately in some scenarios but not in others .",
    "vehtari and ojanen ( 2012 ) discus different prediction scenarios where the future explanatory variable @xmath53 is assumed to be random , unknown , fixed , shifted , deterministic , or constrained in some way . here",
    "we consider only scenarios with no @xmath4 , @xmath54 is equal to @xmath55 , or @xmath53 is equal to @xmath4 .",
    "variations of cross - validation and hold - out methods can be used for more complex scenarios .",
    "for example , for time series with unknown finite range dependencies , @xmath56-block cross - validation ( burman et al . ,",
    "1994 ) can be used .",
    "similar variations of information criteria have not been proposed .",
    "regular cross - validation and information criteria can be used for time series in case of stationary markov process and squared error or a scoring function or rule which is well approximated by a quadratic form ( akaike , 1973 , burman et al . , 1994 ) .",
    "challenges of evaluating structured models continue to arise in applied problems ( for example , jones and spiegelhalter , 2012 ) .",
    "for historical reasons , measures of predictive accuracy are referred to as _ information criteria _ and are typically defined based on the deviance ( the log predictive density of the data given a point estimate of the fitted model , multiplied by @xmath57 ; that is @xmath58 ) .",
    "a point estimate @xmath59 and posterior distribution @xmath26 are fit to the data @xmath14 , and out - of - sample predictions will typically be less accurate than implied by the within - sample predictive accuracy . to put it another way , the accuracy of a fitted model s predictions of future data will generally be lower , in expectation , than the accuracy of the same model s predictions for observed data  even if the family of models being fit happens to include the true data - generating process , and even if the parameters in the model happen to be sampled exactly from the specified prior distribution .",
    "we are interested in prediction accuracy for two reasons : first , to measure the performance of a model that we are using ; second , to compare models .",
    "our goal in model comparison is not necessarily to pick the model with lowest estimated prediction error or even to average over candidate models  as discussed in gelman et al .",
    "( 2003 ) , we prefer continuous model expansion to discrete model choice or averaging  but at least to put different models on a common scale .",
    "even models with completely different parameterizations can be used to predict the same measurements .",
    "when different models have the same number of parameters estimated in the same way , one might simply compare their best - fit log predictive densities directly , but when comparing models of differing size or differing effective size ( for example , comparing logistic regressions fit using uniform , spline , or gaussian process priors ) , it is important to make some adjustment for the natural ability of a larger model to fit data better , even if only by chance .",
    "several methods are available to estimate the expected predictive accuracy without waiting for out - of - sample data .",
    "we can not compute formulas such as ( [ elppd ] ) directly because we do not know the true distribution , @xmath21 . instead we can consider various approximations .",
    "we know of no approximation that works in general , but predictive accuracy is important enough that it is still worth trying .",
    "we list several reasonable - seeming approximations here .",
    "each of these methods has flaws , which tells us that any predictive accuracy measure that we compute will be only approximate .    *",
    "_ within - sample predictive accuracy .",
    "_ a naive estimate of the expected log predictive density for _ new _ data is the log predictive density for _ existing _ data . as discussed above , we would like to work with the bayesian pointwise formula , that is , lppd as computed using the simulation ( [ butgut3 ] ) .",
    "this summary is quick and easy to understand but is in general an overestimate of ( [ elppd_2 ] ) because it is evaluated on the data from which the model was fit . * _ adjusted within - sample predictive accuracy . _ given that lppd is a biased estimate of elppd , the next logical step is to correct that bias .",
    "formulas such as aic , dic , and waic ( all discussed below ) give approximately unbiased estimates of elppd by starting with something like lppd and then subtracting a correction for the number of parameters , or the effective number of parameters , being fit",
    ". these adjustments can give reasonable answers in many cases but have the general problem of being correct at best only in expectation , not necessarily in any given case .",
    "* _ cross - validation .",
    "_ one can attempt to capture out - of - sample prediction error by fitting the model to training data and then evaluating this predictive accuracy on a holdout set .",
    "cross - validation avoids the problem of overfitting but remains tied to the data at hand and thus can be correct at best only in expectation .",
    "in addition , cross - validation can be computationally expensive : to get a stable estimate typically requires many data partitions and fits . at the extreme , leave - one - out cross - validation ( loo - cv ) requires @xmath10 fits except when some computational shortcut can be used to approximate the computations .      in much of the statistical literature on predictive accuracy , inference for @xmath1",
    "is summarized not by a posterior distribution @xmath27 but by a point estimate @xmath59 , typically the maximum likelihood estimate .",
    "out - of - sample predictive accuracy is then defined not by the expected log _ posterior _ predictive density ( [ elppd ] ) but by @xmath60 defined in ( [ elpepd ] ) , where both @xmath14 and @xmath23 are random .",
    "there is no direct way to calculate ( [ elpepd ] ) ; instead the standard approach is to use the log posterior density of the observed data @xmath14 given a point estimate @xmath59 and correct for bias due to overfitting .",
    "let @xmath18 be the number of parameters estimated in the model .",
    "the simplest bias correction is based on the asymptotic normal posterior distribution . in this limit ( or in the special case of a normal linear model with known variance and uniform prior distribution ) , subtracting @xmath18 from the log predictive density given the maximum likelihood estimate is a correction for how much the fitting of @xmath18 parameters will increase predictive accuracy , by chance alone : [ aic ] _",
    "aic= p(y|_mle ) - k. as defined by akaike ( 1973 ) , aic is the above multiplied by @xmath57 ; thus @xmath61 .",
    "it makes sense to adjust the deviance for fitted parameters , but once we go beyond linear models with flat priors , we can not simply add @xmath18 .",
    "informative prior distributions and hierarchical structures tend to reduce the amount of overfitting , compared to what would happen under simple least squares or maximum likelihood estimation .    for models with informative priors or hierarchical structure ,",
    "the effective number of parameters strongly depends on the variance of the group - level parameters .",
    "we shall illustrate in section [ normalic ] with the univariate normal model and in section [ modelcomparison ] with a classic example of educational testing experiments in 8 schools . under the hierarchical model in that example",
    ", we would expect the effective number of parameters to be somewhere between 8 ( one for each school ) and 1 ( for the average of the school effects ) .",
    "there are extensions of aic which have an adjustment related to the effective number of parameters ( see vehtari and ojanen , 2012 , section 5.5 , and references therein ) but these are seldom used due to stability problems and computational difficulties , issues that have motivated the construction of the more sophisticated measures discussed below .",
    "dic ( spiegelhalter et al . , 2002 ) is a somewhat bayesian version of aic that takes formula ( [ aic ] ) and makes two changes , replacing the maximum likelihood estimate @xmath59 with the posterior mean @xmath62 and replacing @xmath18 with a data - based bias correction .",
    "the new measure of predictive accuracy is , [ dic ] _",
    "dic= p(y|_bayes ) - p_dic , where @xmath63 is the effective number of parameters , defined as , [ dic.2 ] p_dic = 2(p(y|_bayes)- _ post(p(y| ) ) ) , where the expectation in the second term is an average of @xmath1 over its posterior distribution . expression ( [ dic.2 ] ) is calculated using simulations @xmath35 , @xmath36 as , [ pd1 ] p_dic= 2(p(y|_bayes)- _",
    "s=1^s p(y|^s ) ) .",
    "the posterior mean of @xmath1 will produce the maximum log predictive density when it happens to be same as the mode , and negative @xmath63 can be produced if posterior mean is far from the mode .    an alternative version of dic uses a slightly different definition of effective number of parameters : [ pd2 ] p_dicalt = 2_post(p(y| ) ) .",
    "both @xmath63 and @xmath64 give the correct answer in the limit of fixed model and large @xmath10 and can be derived from the asymptotic @xmath20 distribution ( shifted and scaled by a factor of @xmath65 ) of the log predictive density . for linear models with uniform prior distributions , both these measures of effective sample size reduce to @xmath18 .",
    "of these two measures , @xmath63 is more numerically stable but @xmath64 has the advantage of always being positive . compared to previous proposals for estimating the effective number of parameters , easier and more stable monte carlo approximation of dic made it quickly popular .",
    "the actual quantity called dic is defined in terms of the deviance rather than the log predictive density ; thus , @xmath66      waic ( introduced by watanabe , 2010 , who calls it the widely applicable information criterion ) is a more fully bayesian approach for estimating the out - of - sample expectation ( [ elppd_2 ] ) , starting with the computed log pointwise posterior predictive density ( [ butgut3 ] ) and then adding a correction for effective number of parameters to adjust for overfitting .",
    "two adjustments have been proposed in the literature .",
    "both are based on pointwise calculations and can be viewed as approximations to cross - validation , based on derivations not shown here .",
    "the first approach is a difference , similar to that used to construct @xmath63 : p_waic 1 = 2_i=1^n ( ( _ post p(y_i|))- _ post(p(y_i| ) ) ) , which can be computed from simulations by replacing the expectations by averages over the @xmath37 posterior draws @xmath35 : p_waic 1 = 2_i=1^n((_s=1^s p(y_i|^s ) ) - _ s=1^s p(y_i|^s ) ) .    the other measure uses the variance of individual terms in the log predictive density summed over the @xmath10 data points : [ vlpd1.5 ] p_waic 2 = _",
    "i=1^n _ post ( p(y_i| ) ) .",
    "this expression looks similar to ( [ pd2 ] ) , the formula for @xmath64 ( although without the factor of 2 ) , but is more stable because it computes the variance separately for each data point and then sums ; the summing yields stability .    to calculate ( [ vlpd1.5 ] ) , we compute the posterior variance of the log predictive density for each data point @xmath29 , that is , @xmath67 , where @xmath68 represents the sample variance , @xmath69 . summing over all the data points @xmath29 gives the effective number of parameters : [ vlpd2 ] p_waic 2 = _",
    "i=1^n v_s=1^s ( p(y_i|^s ) ) .",
    "we can then use either @xmath70 or @xmath71 as a bias correction : [ waicformula ] _",
    "waic= - p_waic .    in the present article",
    ", we evaluate both @xmath72 and @xmath73 . for practical use ,",
    "we recommend @xmath73 because its series expansion has closer resemblance to the series expansion for loo - cv and also in practice seems to give results closer to loo - cv .    as with aic and dic ,",
    "we define waic as @xmath57 times the expression ( [ waicformula ] ) so as to be on the deviance scale . in watanabe s original definition ,",
    "waic is the negative of the average log pointwise predictive density ( assuming the prediction of a single new data point ) and thus is divided by @xmath10 and does not have the factor @xmath74 ; here we scale it so as to be comparable with aic , dic , and other measures of deviance .    for a normal linear model with large sample size , known variance , and uniform prior distribution on the coefficients , @xmath75 and @xmath76",
    "are approximately equal to the number of parameters in the model .",
    "more generally , the adjustment can be thought of as an approximation to the number of ` unconstrained ' parameters in the model , where a parameter counts as 1 if it is estimated with no constraints or prior information , 0 if it is fully constrained or if all the information about the parameter comes from the prior distribution , or an intermediate value if both the data and prior distributions are informative .",
    "compared to aic and dic , waic has the desirable property of averaging over the posterior distribution rather than conditioning on a point estimate .",
    "this is especially relevant in a predictive context , as waic is evaluating the predictions that are actually being used for new data in a bayesian context .",
    "aic and dic estimate the performance of the plug - in predictive density , but bayesian users of these measures would still use the posterior predictive density for predictions .",
    "other information criteria are based on fisher s asymptotic theory assuming a regular model for which the likelihood or the posterior converges to a single point , and where maximum likelihood and other plug - in estimates are asymptotically equivalent .",
    "waic works also with singular models and thus is particularly helpful for models with hierarchical and mixture structures in which the number of parameters increases with sample size and where point estimates often do not make sense .",
    "for all these reasons , we find waic more appealing than aic and dic .",
    "the purpose of the present article is to gain understanding of these different approaches by applying them in some simple examples .",
    "a cost of using waic is that it relies on a partition of the data into @xmath10 pieces , which is not so easy to do in some structured - data settings such as time series , spatial , and network data .",
    "aic and dic do not make this partition explicitly , but derivations of aic and dic assume that residuals are independent given the point estimate @xmath59 : conditioning on a point estimate @xmath59 eliminates posterior dependence at the cost of not fully capturing posterior uncertainty .",
    "ando and tsay ( 2010 ) have proposed an information criterion for the joint prediction , but its bias correction has the same computational difficulties as many other extensions of aic and it can not be compared to cross - validation , since it is not possible to leave @xmath10 data points out in the cross - validation approach .",
    "it makes sense that @xmath63 and @xmath77 depend not just on the structure of the model but on the particular data that happen to be observed . for a simple example , consider the model @xmath78 , with @xmath10 large and @xmath79 .",
    "that is , @xmath1 is constrained to be positive but otherwise has a noninformative uniform prior distribution .",
    "how many parameters are being estimated in this model ? if the measurement @xmath14 is close to zero , then the effective number of parameters @xmath80 is approximately @xmath16 , since roughly half the information in the posterior distribution is coming from the data and half from the prior constraint of positivity .",
    "however , if @xmath14 is positive and large , then the constraint is essentially irrelevant , and the effective number of parameters is approximately 1 .",
    "this example illustrates that , even with a fixed model and fixed true parameters , it can make sense for the effective number of parameters to depend on data .",
    "there is also something called the bayesian information criterion ( a misleading name , we believe ) that adjusts for the number of fitted parameters with a penalty that increases with the sample size , @xmath10 ( schwartz , 1978 ) .",
    "the formula is @xmath81 , which for large datasets gives a larger penalty per parameter compared to aic and thus favors simpler models .",
    "watanabe ( 2013 ) has also proposed a widely applicable bayesian information criterion ( wbic ) which works also in singular and unrealizable cases .",
    "bic and its variants differ from the other information criteria considered here in being motivated not by an estimation of predictive fit but by the goal of approximating the marginal probability density of the data , @xmath82 , under the model , which can be used to estimate relative posterior probabilities in a setting of discrete model comparison . for reasons described in gelman and shalizi ( 2012 ) , we do not typically find it useful to think about the posterior probabilities of models but we recognize that others find bic and similar measures helpful for both theoretical and applied reason . for the present article , we merely point out that bic has a different goal than the other measures we have discussed .",
    "it is completely possible for a complicated model to predict well and have a low aic , dic , and waic , but , because of the penalty function , to have a relatively high ( that is , poor ) bic . given that bic is not intended to predict out - of - sample model performance but rather is designed for other purposes , we do not consider it further here .      in bayesian cross - validation",
    ", the data are repeatedly partitioned into a training set @xmath83 and a holdout set @xmath84 , and then the model is fit to @xmath83 ( thus yielding a posterior distribution @xmath85 ) , with this fit evaluated using an estimate of the log predictive density of the holdout data , @xmath86 .",
    "assuming the posterior distribution @xmath87 is summarized by @xmath37 simulation draws @xmath35 , we calculate the log predictive density as @xmath88 .    for simplicity",
    ", we will restrict our attention here to leave - one - out cross - validation ( loo - cv ) , the special case with @xmath10 partitions in which each holdout set represents a single data point . performing the analysis for each of the @xmath10 data points ( or perhaps a random subset for efficient computation if @xmath10 is large )",
    "yields @xmath10 different inferences @xmath89 , each summarized by @xmath37 posterior simulations , @xmath90 .",
    "the bayesian loo - cv estimate of out - of - sample predictive fit is [ xformula1 ] [ xformula2 ] _ loo - cv = _",
    "i=1^n p_post(-i)(y_i ) , _ i=1^n ( _ s=1^sp(y_i|^is ) ) .",
    "each prediction is conditioned on @xmath91 data points , which causes underestimation of the predictive fit . for large @xmath10",
    "the difference is negligible , but for small @xmath10 ( or when using @xmath18-fold cross - validation ) we can use a first order bias correction @xmath92 by estimating how much better predictions would be obtained if conditioning on @xmath10 data points ( burman , 1989 ) : b = -_-i , where _ -i",
    "i=1^n_j=1^n p_post(-i)(y_j ) , _",
    "i=1^n_j=1^n ( _ s=1^s p(y_j|^is ) ) .",
    "the bias - corrected bayesian loo - cv is then _ cloo - cv = _ loo - cv + b. the bias correction @xmath92 is rarely used as it is usually small , but we include it for completeness .    to make comparisons to other methods ,",
    "we compute an estimate of the effective number of parameters as [ ploocv ] p_loo - cv = - _ loo - cv or , using bias - corrected loo - cv , p_cloo - cv & = & - _ cloo + & = & _ -i - _ loo .",
    "cross - validation is like waic in that it requires data to be divided into disjoint , ideally conditionally independent , pieces .",
    "this represents a limitation of the approach when applied to structured models .",
    "in addition , cross - validation can be computationally expensive except in settings where shortcuts are available to approximate the distributions @xmath89 without having to re - fit the model each time . for the examples in this article such shortcuts are available , but we used the brute force approach for clarity . if no shortcuts are available , common approach is to use @xmath18-fold cross - validation where data is partitioned in @xmath18 sets . with moderate value of @xmath18 , for example 10 ,",
    "computation time is reasonable in most applications .    under some conditions ,",
    "different information criteria have been shown to be asymptotically equal to leave - one - out cross - validation ( as @xmath93 , the bias correction can be ignored in the proofs ) .",
    "aic has been shown to be asymptotically equal to loo - cv as computed using the maximum likelihood estimate ( stone , 1997 ) .",
    "dic is a variation of the regularized information criteria which have been shown to be asymptotically equal to loo - cv using plug - in predictive densities ( shibata , 1989 ) .",
    "bayesian cross - validation works also with singular models , and bayesian loo - cv has been proven to asymptotically equal to waic ( watanabe , 2010 ) . for finite @xmath10",
    "there is a difference , as loo - cv conditions the posterior predictive densities on @xmath91 data points .",
    "these differences can be apparent for small @xmath10 or in hierarchical models , as we discus in our examples .",
    "other differences arise in regression or hierarchical models .",
    "loo - cv assumes the prediction task @xmath94 while waic estimates @xmath95 , so waic is making predictions only at @xmath4-locations already observed ( or in subgroups indexed by @xmath96 ) .",
    "this can make a noticeable difference in flexible regression models such as gaussian processes or hierarchical models where prediction given @xmath96 may depend only weakly on all other data points @xmath97 .",
    "we illustrate with a simple hierarchical model in section [ modelcomparison ] .",
    "the cross - validation estimates are similar to the jackknife ( efron and tibshirani , 1993 ) . even though we are working with the posterior distribution ,",
    "our goal is to estimate an expectation averaging over @xmath98 in its true , unknown distribution , @xmath21 ; thus , we are studying the frequency properties of a bayesian procedure .",
    "all the different measures discussed above are based on adjusting the log predictive density of the observed data by subtracting an approximate bias correction .",
    "the measures differ both in their starting points and in their adjustments .",
    "aic starts with the log predictive density of the data conditional on the maximum likelihood estimate @xmath59 , dic conditions on the posterior mean @xmath99 , and waic starts with the log predictive density , averaging over @xmath33 .",
    "of these three approaches , only waic is fully bayesian and so it is our preference when using a bias correction formula .",
    "cross - validation can be applied to any starting point , but it is also based on the log pointwise predictive density .",
    "in order to better understand the different information criteria , we begin by evaluating them in the context of the simplest continuous model .",
    "consider data @xmath100 with noninformative prior distribution , @xmath101 .",
    "[ [ aic . ] ] aic .",
    "+ + + +    the maximum likelihood estimate is @xmath102 , and the probability density of the data given that estimate is p(y|_mle)=-(2 ) -_i=1^n(y_i-|y)^2 = - ( 2 ) -(n-1)s^2_y,[py.1 ] where @xmath103 is the sample variance of the data .",
    "only one parameter is being estimated , so _",
    "aic = p(y|_mle ) - k = -(2 ) -(n-1)s^2_y - 1 .",
    "[ py.1.5 ]    [ [ dic . ] ] dic .",
    "+ + + +    the two pieces of dic are @xmath104 and the effective number of parameters @xmath105 $ ] . in this example with a flat prior density , @xmath106 and so @xmath104 is given by ( [ py.1 ] ) . to compute the second term in @xmath63 , we start with [ py.2 ] p(y|)=-(2 ) - , and then compute the expectation of ( [ py.2 ] ) , averaging over @xmath1 in its posterior distribution , which in this case is simply @xmath107 .",
    "the relevant calculation is @xmath108 , and then the expectation of ( [ py.2 ] ) becomes , [ py.3 ] _",
    "post(p(y|))=-(2 ) - . subtracting ( [ py.3 ] ) from ( [ py.1 ] ) and multiplying by 2 yields @xmath63 , which is exactly 1 , as all the other terms cancel .",
    "so , in this case , dic and aic are the same .",
    "[ [ waic . ] ] waic .",
    "+ + + + +    in this example , waic can be easily determined analytically as well .",
    "the first step is to write the predictive density for each data point , @xmath109 . in this case ,",
    "@xmath110 and @xmath111 , and so we see that @xmath112 . summing the terms for the @xmath10 data points , we get , _ i=1^n p_post(y_i ) & = & -(2)-(1+)-_i=1^n(y_i-|y)^2 + [ py.4 ] & = & -(2)-(1+)-s^2_y .",
    "next we determine the two forms of effective number of parameters .",
    "to evaluate @xmath113 $ ] , the first term inside the parentheses is simply ( [ py.4 ] ) , and the second term is @xmath114 twice the difference is then , [ py.4.5 ] p_waic 1 = s^2_y + 1-n(1 + ) .    to evaluate @xmath115 , for each data point @xmath29 , we start with @xmath116 , and compute the variance of this expression , averaging over the posterior distribution , @xmath117 . after the dust settles , we get [ py.5 ] p_waic 2=s^2_y + , and @xmath118 , combining ( [ py.4 ] ) and ( [ py.4.5 ] ) or ( [ py.5 ] ) .    for this example , waic differs from aic and dic in two ways .",
    "first , we are evaluating the pointwise predictive density averaging each term @xmath119 over the entire posterior distribution rather than conditional on a point estimate , hence the differences between ( [ py.1 ] ) and ( [ py.4 ] ) .",
    "second , the effective number of parameters in waic is not quite 1 . in the limit of large @xmath10 , we can replace @xmath103 by its expected value of 1 , yielding @xmath120 .",
    "waic is not so intuitive for small @xmath10 .",
    "for example , with @xmath121 , the effective number of parameters @xmath77 is only 0.31 ( for @xmath70 ) or 0.5 ( for @xmath71 ) . as we shall see shortly , it turns out that the value 0.5 is correct , as this is all the adjustment that is needed to fix the bias in waic for this sample size in this example .    [ [ cross - validation . ] ] cross - validation .",
    "+ + + + + + + + + + + + + + + + +    in this example , the leave - one - out posterior predictive densities are p_post(-i)(y_i ) = ( y_i||y_-i , 1 + . ) , where @xmath122 is @xmath123 .",
    "the sum of the log leave - one - out posterior predictive densities is _",
    "i=1^n p_post(-i)(y_i ) = -(2)-(1+)-_i=1^n(y_i-|y_-i)^2 .    for the bias correction and the effective number of parameters we need also _",
    "i=1^n_j=1^n p_post(-i)(y_j ) = -(2)-(1+)-_i=1^n_j=1^n(y_j-|y_-i)^2 .",
    "[ [ in - expectation . ] ] in expectation .",
    "+ + + + + + + + + + + + + + +    as can be seen above , aic , dic , waic , and @xmath124 all are random variables , in that their values depend on the data @xmath14 , even if the model is known .",
    "we now consider each of these in comparison to their target , the log predictive density for a new data set @xmath23 . in these evaluations",
    "we are taking expectations over both the observed data @xmath14 and the future data @xmath23 .",
    "our comparison point is the expected log pointwise predictive density ( [ elppd_2 ] ) for new data : = _ i=1^n ( p_post(_i ) ) = -(2)-(1+)-_i=1^n((_i-|y)^2 ) .",
    "this last term can be decomposed and evaluated : ( _ i=1^n(_i-|y)^2 ) & = & ( n-1)(s^2_y ) + n((-|y)^2 ) + & = & n+1 , [ tildesum ] and thus the the expected log pointwise predictive density is = -(2)-(1+)- .",
    "[ py.6 ]    we also need the expected value of the log pointwise predictive density for existing data , which can be obtained by plugging @xmath125 into ( [ py.4 ] ) : [ py.6.5 ] ( ) = -(2)-(1+)- . despite what the notation might seem to imply ,",
    "elppd is _ not _ the same as @xmath126 ; the former is the expected log pointwise predictive density for future data @xmath23 , while the latter is this density evaluated at the observed data @xmath14 .",
    "the correct ` effective number of parameters ' ( or bias correction ) is the difference between @xmath126 and @xmath127 , that is , from ( [ py.6 ] ) and ( [ py.6.5 ] ) , [ true.p ] ( ) - = - = , which is always less than 1 : with @xmath121 it is 0.5 and in the limit of large @xmath10 it goes to 1 .    the target of aic and dic is the performance of the plug - in predictive density .",
    "thus for comparison we also calculate = = -(2 ) - . inserting ( [ tildesum ] ) into that last term yields the expected out - of - sample log density given the point estimate as ( p(|(y ) ) ) = ( 2 ) - - .",
    "[ [ in - expectation - aic - and - dic . ] ] in expectation : aic and dic .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + +    the expectation of aic from ( [ py.1.5 ] ) is , ( _ aic ) = -(2 ) - ( n-1)(s^2_y ) - 1 = -(2 ) - - , and also for dic , which in this simple noninformative normal example is the same as aic .",
    "thus aic and dic unbiasedly estimate the log predictive density given point estimate for new data for this example .",
    "we can subtract the above expected estimate from its target , expression ( [ py.6 ] ) , to obtain : -(_aic ) = -(1 + ) + = + o(n^-3 ) = + o(n^-2 ) , in this simple example , the estimated effective number of parameters differs from the appropriate expectation ( [ true.p ] ) , but combining two wrongs makes a right , and aic / dic performs decently .    [ [ in - expectation - waic . ] ] in expectation : waic .",
    "+ + + + + + + + + + + + + + + + + + + + +    we can obtain the expected values of the two versions of @xmath77 , by taking expectations of ( [ py.4.5 ] ) and ( [ py.5 ] ) , to yield , ( p_waic 1)=+1-n(1+)= + o(n^-3 ) and ( p_waic 2)=1-=. for large @xmath10 , the limits work out ; the difference ( [ true.p ] ) and both versions of @xmath77 all approach 1 , which is appropriate for this example of a single parameter with noninformative prior distribution . at the other extreme of @xmath128 , the difference ( [ true.p ] ) and @xmath129 take on the value @xmath16 , while @xmath130 is slightly off with a value of 0.31 .",
    "asymptotic errors are -(_waic 1 ) & = & + o(n^-3)=",
    "+ o(n^-2 ) + -(_waic",
    "2 ) & = & - = - + o(n^-2 ) .",
    "[ [ in - expectation - cross - validation . ] ] in expectation : cross - validation .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the expectation over @xmath14 is , _ loo - cv = -(2)-(1+)-_i=1^n((y_i-|y_-i)^2 ) .",
    "terms @xmath131 are not completely independent as @xmath122 are overlapping , but it does not affect the expectation . using ( [ tildesum ] ) to evaluate the sum in the last term we get _ i=1^n((y_i-|y_-i)^2)=n+ and ( _ loo - cv ) = -(2)-(1+)- . subtracting this from elppd yields , for @xmath132 , -(_loo - cv ) = -(1+)+(1",
    "+ ) = -(1- ) = + o(n^-3 ) .",
    "this difference comes from the fact the cross - validation conditions on @xmath91 data points .    for the bias correction",
    "we need ( _ -i)=-(2)-(1 + ) - + 1 - . the bias - corrected loo - cv is ( _ cloo - cv ) = -(2)-(1+)-+ . subtracting this from elppd yields , -(_cloo - cv ) = - , showing much improved accuracy from the bias correction .",
    "the effective number of parameters from leave - one - out cross - validation is , for @xmath132 , p_loo - cv&=&()-(_loo - cv ) + & = & -(1+)+(1 + ) - + + & = & -(1- ) + + & = & + o(n^-3 ) and , from the - bias corrected version : p_cloo - cv=()-(_cloo - cv ) = .",
    "the above calculations get more interesting when we add prior information so that , in effect , less than a full parameter is estimated from any finite data set .",
    "we consider data @xmath100 with a normal prior distribution , @xmath133 . to simplify the algebraic expressions we shall write @xmath134 , the prior precision and equivalent number of data points in the prior .",
    "the posterior distribution is then @xmath135 and the posterior predictive distribution for a data point @xmath29 is @xmath136 .",
    "[ [ aic.-1 ] ] aic .",
    "+ + + +    adding a prior distribution does not affect the maximum likelihood estimate , so @xmath137 is unchanged from ( [ py.1 ] ) , and aic is the same as before .",
    "[ [ dic.-1 ] ] dic .",
    "+ + + +    the posterior mean is @xmath138 , and so and the first term in dic is p(y|_bayes)&=&- ( 2 ) -(n-1)s^2_y - n(|y-_bayes)^2 + & = & - ( 2 ) -(n-1)s^2_y - n()^2(|y-)^2.[py.7 ] next we evaluate ( [ dic.2 ] ) for this example ; after working through the algebra , we get , [ py.8 ] p_dic= , which makes sense : the flat prior corresponds to @xmath139 , so that @xmath140 ; at the other extreme , large values of @xmath141 correspond to prior distributions that are much more informative than the data , and @xmath142 .    [ [ waic.-1 ] ] waic .",
    "+ + + + +    going through the algebra , the log pointwise predictive density of the data is & = & _ i=1^n p_post(y_i ) + & = & -(2 ) - ( 1+)-s^2_y - ( |y-)^2 .",
    "we can also work out p_waic 1 & = & s^2_y + ( |y-)^2 + - n(1 + ) + p_waic 2 & = & s^2_y + ( |y-)^2 + .",
    "we can understand these formulas by applying them to special cases :    * a flat prior distribution ( @xmath139 ) yields @xmath143 and @xmath144 , same as ( [ py.4.5 ] ) and ( [ py.5 ] ) . in expectation , these are @xmath145 and @xmath146 , as before . * a prior distribution equally informative as data ( @xmath147 ) yields @xmath148 and @xmath149 . in expectation ,",
    "averaging over the prior distribution and the data model , these both look like @xmath150 , approximately halving the effective number of parameters , on average , compared to the noninformative prior .",
    "* a completely informative prior distribution ( @xmath151 ) yields @xmath152 , which makes sense , as the data provide no information .",
    "[ [ cross - validation.-1 ] ] cross - validation .",
    "+ + + + + + + + + + + + + + + + +    for loo - cv , we need p_post(-i)(y_i)=(y_i|,1 + . ) and _",
    "i=1^n p_post(-i)(y_i)&= & -(2)-(1 + ) + & & -_i=1^n(y_i-)^2 .",
    "the expectation of the sum in the the last term is ( using the marginal expectation , @xmath153 ) , ( y_i-)^2 = n + , and the expectation of leave - one - out cross - validation is ( _ cloo - cv ) = -(2)-(1+)- ( ) . if @xmath154 this is same as with uniform prior and it increases with increasing @xmath141 .      next consider the balanced model with data @xmath155 , and prior distribution @xmath156 .",
    "if the hyperparameters are known , the inference reduces to the non - hierarchical setting described above : the @xmath157 parameters have independent normal posterior distributions , each based on @xmath157 data points . aic and dic",
    "are unchanged except that the log predictive probabilities and effective numbers of parameters are summed over the groups .",
    "the results become more stable ( because we are averaging @xmath103 for @xmath157 groups ) but the algebra is no different .    for waic and cross - validation ,",
    "though , there is a possible change , depending on how the data are counted .",
    "if each of the @xmath158 observations is counted as a separate data point , the results again reduce to @xmath157 copies of what we had before .",
    "but another option is to count each _ group _ as a separate data point , in which case the log pointwise predictive density ( [ butgut3 ] ) changes , as does @xmath77 in ( [ vlpd1.5 ] ) or ( [ vlpd2 ] ) , as all of these now are average over @xmath157 larger terms corresponding to the @xmath157 vectors @xmath159 , rather than @xmath158 little terms corresponding to the individual @xmath160 s .    in a full hierarchical model with hyperparameters unknown ,",
    "dic and waic both change in recognition of this new source of posterior uncertainty ( while aic is not clearly defined in such cases ) .",
    "we illustrate in section [ modelcomparison ] , evaluating these information criteria for a hierarchical model with _",
    "unknown _ hyperparameters , as fit to the data from the 8-schools study .",
    "we illustrate the ideas using a simple linear prediction problem .",
    "figure [ hibbs1 ] shows a quick summary of economic conditions and presidential elections over the past several decades .",
    "it is based on the ` bread and peace ' model created by political scientist douglas hibbs ( see hibbs , 2008 , for a recent review ) to forecast elections based solely on economic growth ( with corrections for wartime , notably adlai stevenson s exceptionally poor performance in 1952 and hubert humphrey s loss in 1968 , years when democrats were presiding over unpopular wars ) .",
    "better forecasts are possible using additional information such as incumbency and opinion polls , but what is impressive here is that this simple model does pretty well all by itself .    for simplicity",
    ", we predict @xmath14 ( vote share ) solely from @xmath4 ( economic performance ) , using a linear regression , @xmath161 , with a noninformative prior distribution , @xmath162 , so that the posterior distribution is normal - inverse-@xmath20 .",
    "fit to all 15 data points in figure [ hibbs1 ] , the posterior mode @xmath163 is @xmath164 .",
    "although these data form a time series , we are treating them here as a simple regression problem .",
    "in particular , when considering leave - one - out cross - validation , we do not limit ourselves to predicting from the past ; rather , we consider the elections as 15 independent data points .",
    "[ [ posterior - distribution - of - the - observed - log - predictive - density - pytheta . ] ] posterior distribution of the observed log predictive density , @xmath8 . + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in our regression example , the log predictive probability density of the data is @xmath165 , with an uncertainty induced by the posterior distribution , @xmath166 .",
    "the posterior distribution @xmath167 is normal - inverse-@xmath20 . to get a sense of uncertainty in the log predictive density @xmath168",
    ", we compute it for each of @xmath169 posterior simulation draws of @xmath1 .",
    "figure [ llsim ] shows the resulting distribution , which looks roughly like a @xmath170 ( no surprise since three parameters are being estimated  two coefficients and a variance  and the sample size of 15 is large enough that we would expect the asymptotic normal approximation to the posterior distribution to be pretty good ) , scaled by a factor of @xmath65 and shifted so that its upper limit corresponds to the maximum likelihood estimate ( with log predictive density of @xmath171 , as noted earlier ) .",
    "the mean of the posterior distribution of the log predictive density is @xmath172 , and the difference between the mean and the maximum is 1.7 , which is close to the value of @xmath173 that would be predicted from asymptotic theory , given that 3 parameters are being estimated .",
    "figure [ llsim ] is reminiscent of the direct likelihood methods of dempster ( 1974 ) and aitkin ( 2010 ) .",
    "our approach is different , however , in being fully bayesian , with the apparent correspondence appearing here only because we happen to be using a flat prior distribution .",
    "a change in @xmath174 would result in a different posterior distribution for @xmath8 and thus a different figure [ llsim ] , a different expected value , and so forth .",
    "[ [ log - predictive - density - of - the - observed - data . ] ] log predictive density of the observed data .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    for this simple linear model , the posterior predictive distribution of any data point has an analytic form ( @xmath175 with @xmath176 degrees of freedom ) , but it is easy enough to use the more general simulation - based computational formula ( [ butgut3 ] ) . calculated either way , the log pointwise predictive density is @xmath177 . unsurprisingly ,",
    "this number is slightly lower than the predictive density evaluated at the maximum likelihood estimate : averaging over uncertainty in the parameters yields a slightly lower probability for the observed data .",
    "[ [ aic.-2 ] ] aic .",
    "+ + + +    fit to all 15 data points , the mle @xmath178 is @xmath164 . since",
    "3 parameters are estimated , the value of @xmath179 is @xmath180 and @xmath181    [ [ dic.-2 ] ] dic .",
    "+ + + +    the relevant formula is @xmath182 .",
    "the second of these terms is invariant to reparameterization ; we calculate it as @xmath183 based on a large number @xmath37 of simulation draws .    the first term is not invariant . with respect to the prior @xmath184 ,",
    "the posterior means of @xmath185 and @xmath92 are @xmath186 and @xmath187 , the same as the maximum likelihood estimate .",
    "the posterior means of @xmath188 , @xmath189 , and @xmath190 are @xmath191 , @xmath192 , and @xmath193 .",
    "parameterizing using @xmath188 , we get @xmath194 which gives @xmath195 , @xmath196 , and @xmath197 .    [ [ waic.-2 ] ] waic .",
    "+ + + + +    the log pointwise predictive probability of the observed data under the fitted model is @xmath198 the effective number of parameters can be calculated as @xmath199 or @xmath200 then @xmath201 , and @xmath202 , so @xmath203 is 86.2 or 87.2 .",
    "[ [ leave - one - out - cross - validation . ] ] leave - one - out cross - validation .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we fit the model 15 times , leaving out a different data point each time . for each fit of the model , we sample @xmath37 times from the posterior distribution of the parameters and compute the log predictive density .",
    "the cross - validated pointwise predictive accuracy is @xmath204 which equals @xmath205 .",
    "multiplying by @xmath57 to be on the same scale as aic and the others , we get 87.6 .",
    "the effective number of parameters from cross - validation , from ( [ ploocv ] ) , is @xmath206 .",
    "given that this model includes two linear coefficients and a variance parameter , these all look reasonable as an effective number of parameters .",
    "we next explore bayesian predictive error models in the context of a classic example from rubin ( 1981 ) of an educational testing experiment , measuring the effects of a test preparation program performed in eight different high schools in new jersey .",
    "a separate randomized experiment was conducted in each school , and the administrators of each school implemented the program in their own way .",
    "the results , based on a separate regression analyses performed in each school , are displayed in table [ tab5.2 ] .",
    "three modes of inference were proposed for these data :    * _ no pooling _ : separate estimates for each of the eight schools , reflecting that the experiments were performed independently and so each school s observed value is an unbiased estimate of its own treatment effect .",
    "this model has eight parameters : an estimate for each school . *",
    "_ complete pooling _ : a combined estimate averaging the data from all schools into a single number , reflecting that the eight schools were actually quite similar ( as were the eight different treatments ) , and also reflecting that the variation among the eight estimates ( the left column of numbers in table [ tab5.2 ] ) is no larger than would be expected by chance alone given the standard errors ( the rightmost column in the table ) .",
    "this model has only one , shared , parameter . * _ hierarchical model : _ a bayesian meta - analysis , partially pooling the eight estimates toward a common mean .",
    "this model has eight parameters but they are constrained through their hierarchical distribution and are not estimated independently ; thus the effective number of parameters should be some number less than 8 .",
    "rubin ( 1981 ) used this small example to demonstrate the feasibility and benefits of a full bayesian analysis , averaging over all parameters and hyperparameters in the model .",
    "here we shall take the bayesian model as given .",
    "we throw this example at the predictive error measures because it is a much - studied and well - understood example , hence a good way to develop some intuition about the behavior of aic , dic , waic and cross - validation in a hierarchical setting .",
    "the hierarchical model is @xmath207 , where @xmath159 and @xmath208 are the estimate and standard error for the treatment effect in school @xmath209 , and the hyperparameters @xmath210 determine the population distribution of the effects in the schools .",
    "we assume a uniform hyperprior density , @xmath211 , and the resulting posterior distribution for the group - level scale parameter @xmath212 is displayed in figure [ fig5.5 ] .",
    "the posterior mass is concentrated near 0 , implying that the evidence is that there is little variation in the true treatment effects across the @xmath213 schools .",
    "table[8schools.deviance ] [ deviance.table ] illustrates the use of predictive log densities and information criteria to compare the three models  no pooling , complete pooling , and hierarchical  fitted to the sat coaching data .",
    "we only have data at the group level , so we necessarily define our data points and cross - validation based on the 8 schools , not the individual students .    for this model ,",
    "the log predictive density is simply @xmath214 we shall go down the rows of table [ deviance.table ] to understand how the different information criteria work for each of these three models , then we discuss how these measures can be used to compare the models .",
    "[ [ aic.-3 ] ] aic .",
    "+ + + +    the log predictive density is higher  that is , a better fit  for the no pooling model .",
    "this makes sense : with no pooling , the maximum likelihood estimate is right at the data , whereas with complete pooling there is only one number to fit all 8 schools .",
    "however , the ranking of the models changes after adjusting for the fitted parameters ( 8 for no pooling , 1 for complete pooling ) , and the expected log predictive density is estimated to be the best ( that is , aic is lowest ) for complete pooling .",
    "the last column of the table is blank for aic , as this procedure is defined based on maximum likelihood estimation which is meaningless for the hierarchical model .",
    "[ [ dic.-3 ] ] dic .",
    "+ + + +    for the no - pooling and complete - pooling models with their flat priors , dic gives results identical to aic ( except for possible simulation variability , which we have essentially eliminated here by using a large number of posterior simulation draws ) .",
    "dic for the hierarchical model gives something in between : a direct fit to data ( lpd ) that is better than complete pooling but not as good as the ( overfit ) no pooling , and an effective number of parameters of 2.8 , closer to 1 than to 8 , which makes sense given that the estimated school effects are pooled almost all the way back to their common mean .",
    "adding in the correction for fitting , complete pooling wins , which makes sense given that in this case the data are consistent with zero between - group variance .",
    "[ [ waic.-3 ] ] waic .",
    "+ + + + +    this fully bayesian measure gives results similar to dic .",
    "the fit to observed data is slightly worse for each model ( that is , the numbers for lppd are slightly more negative than the corresponding values for lpd , higher up in the table ) , accounting for the fact that the posterior predictive density has a wider distribution and thus has lower density values at the mode , compared to the predictive density conditional on the point estimate .",
    "however , the correction for effective number of parameters is lower ( for no pooling and the hierarchical model , @xmath77 is about half of @xmath63 ) , consistent with the theoretical behavior of waic when there is only a single data point per parameter , while for complete pooling , @xmath77 is only a bit less than 1 , roughly consistent with what we would expect from a sample size of 8) . for all three models here ,",
    "@xmath77 is much less than @xmath63 , with this difference arising from the fact that the lppd in waic is already accounting for much of the uncertainty arising from parameter estimation .",
    "[ [ cross - validation.-2 ] ] cross - validation .",
    "+ + + + + + + + + + + + + + + + +    for this example it is impossible to cross - validate the no - pooling model as it would require the impossible task of obtaining a prediction from a held - out school given the other seven .",
    "this illustrates on main difference to information criteria , which assume new prediction for these same schools and thus work also in no - pooling model . for complete pooling and for the hierarchical model",
    ", we can perform leave - one - out cross - validation directly . in this model the local prediction of cross - validation is based only on the information coming from the other schools , while the local prediction in waic is based on the local observation as well as the information coming from the other schools . in both cases the prediction is for unknown future data , but the amount of information used is different and thus predictive performance estimates differ more when the hierarchical prior becomes more vague ( with the difference going to infinity as the hierarchical prior becomes uninformative , to yield the no - pooling model ) .",
    "this example shows that it is important to consider which prediction task we are interested in and that it is not clear what @xmath10 means in asymptotic results that feature terms such as @xmath215 .",
    "[ [ comparing - the - three - models . ] ] comparing the three models .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + +    for this particular dataset , complete pooling wins the expected out - of - sample prediction competition .",
    "typically it is best to estimate the hierarchical variance but , in this case , @xmath216 is the best fit to the data , and this is reflected in the center column in table [ deviance.table ] , where the expected log predictive densities are higher than for no pooling or complete pooling .    that said , we still prefer the hierarchical model here , because we do not believe that @xmath212 is truly zero . for example",
    ", the estimated effect in school a is 28 ( with a standard error of 15 ) and the estimate in school c is @xmath217 ( with a standard error of 16 ) .",
    "this difference is not statistically significant and , indeed , the data are consistent with there being zero variation of effects between schools ; nonetheless we would feel uncomfortable , for example , stating that the posterior probability is 0.5 that the effect in school c is larger than the effect in school a , given that data that show school a looking better .",
    "it might , however , be preferable to use a more informative prior distribution on @xmath212 , given that very large values are both substantively implausible and also contribute to some of the predictive uncertainty under this model .",
    "in general , predictive accuracy measures are useful in parallel with posterior predictive checks to see if there are important patterns in the data that are not captured by each model . as with predictive checking ,",
    "the log score can be computed in different ways for a hierarchical model depending on whether the parameters @xmath1 and replications @xmath218 correspond to estimates and replications of new data from the existing groups ( as we have performed the calculations in the above example ) or new groups ( additional schools from the @xmath219 distribution in the above example ) .",
    "there are generally many options in setting up a model for any applied problem .",
    "our usual approach is to start with a simple model that uses only some of the available information  for example , not using some possible predictors in a regression , fitting a normal model to discrete data , or ignoring evidence of unequal variances and fitting a simple equal - variance model .",
    "once we have successfully fitted a simple model , we can check its fit to data and then alter or expand it as appropriate .",
    "there are two typical scenarios in which models are compared .",
    "first , when a model is expanded , it is natural to compare the smaller to the larger model and assess what has been gained by expanding the model ( or , conversely , if a model is simplified , to assess what was lost ) .",
    "this generalizes into the problem of comparing a set of nested models and judging how much complexity is necessary to fit the data .    in comparing nested models , the larger model typically has the advantage of making more sense and fitting the data better but the disadvantage of being more difficult to understand and compute .",
    "the key questions of model comparison are typically : ( 1 ) is the improvement in fit large enough to justify the additional difficulty in fitting , and ( 2 ) is the prior distribution on the additional parameters reasonable ?    the second scenario of model comparison is between two or more nonnested models  neither model generalizes the other .",
    "one might compare regressions that use different sets of predictors to fit the same data , for example , modeling political behavior using information based on past voting results or on demographics . in these settings , we are typically not interested in _ choosing _ one of the models  it would be better , both in substantive and predictive terms , to construct a larger model that includes both as special cases , including both sets of predictors and also potential interactions in a larger regression , possibly with an informative prior distribution if needed to control the estimation of all the extra parameters . however , it can be useful to _ compare _ the fit of the different models , to see how either set of predictors performs when considered alone .    in any case , when evaluating models in this way , it is important to adjust for overfitting , especially when comparing models that vary greatly in their complexity , hence the value of the methods discussed in this article .",
    "when comparing models in their predictive accuracy , two issues arise , which might be called statistical and practical significance .",
    "lack of statistical significance arises from uncertainty in the estimates of comparative out - of - sample prediction accuracy and is ultimately associated with variation in individual prediction errors which manifests itself in averages for any finite dataset .",
    "some asymptotic theory suggests that the sampling variance of any estimate of average prediction error will be of order 1 , so that , roughly speaking , differences of less than 1 could typically be attributed to chance , but according to plummer ( 1996 ) , this asymptotic result does not necessarily hold for nonnested models .",
    "a practical estimate of related sampling uncertainty can be obtained by analyzing the variation in the expected log predictive densities @xmath220 using parametric or nonparametric approaches ( vehtari and lampinen , 2002 ) .",
    "practical significance depends on the purposes to which a model will be used .",
    "sometimes it may be possible to use an application - specific scoring function that is so familiar for subject - matter experts that they can interpret the practical significance of differences .",
    "for example , epidemiologists are used to looking at differences in area under receiver operating characteristic curve ( auc ) for classification and survival models . in settings without such conventional measures , it is not always clear how to interpret the magnitude of a difference in log predictive probability when comparing two models . is a difference of 2 important ? 10 ?",
    "one way to understand such differences is to calibrate based on simpler models ( mcculloch , 1989 ) .",
    "for example , consider two models for a survey of @xmath10 voters in an american election , with one model being completely empty ( predicting @xmath221 for each voter to support either party ) and the other correctly assigning probabilities of 0.4 and 0.6 ( one way or another ) to the voters .",
    "setting aside uncertainties involved in fitting , the expected log predictive probability is @xmath222 per respondent for the first model and @xmath223 per respondent for the second model .",
    "the expected improvement in log predictive probability from fitting the better model is then @xmath224 .",
    "so , for @xmath225 , this comes to an improvement of 20 , but for @xmath226 the predictive improvement is only 2 .",
    "this would seem to accord with intuition : going from 50/50 to 60/40 is a clear win in a large sample , but in a smaller predictive dataset the modeling benefit would be hard to see amid the noise .    in our studies of public opinion and epidemiology",
    ", we have seen cases where a model that is larger and better ( in the sense of giving more reasonable predictions ) does not appear dominant in the predictive comparisons .",
    "this can happen because the improvements are small on an absolute scale ( for example , changing the predicted average response among a particular category of the population from 55% yes to 60% yes ) and concentrated in only a few subsets of the population ( those for which there is enough data so that a more complicated model yields noticeably different predictions ) .",
    "average out - of - sample prediction error can be a useful measure but it does not tell the whole story of model fit .",
    "cross - validation and information criteria make a correction for using the data twice ( in constructing the posterior and in model assessment ) and obtain asymptotically unbiased estimates of predictive performance for a given model .",
    "however , when these methods are used for model selection , the predictive performance estimate of _ the selected model _ is biased due to the selection process ( see references in vehtari and ojanen , 2012 ) .    if the number of compared models is small , the bias is small , but if the number of candidate models is very large ( for example , the number of models grows exponentially as the number of observations @xmath10 grows , or the number of covariates @xmath227 in covariate selection ) a model selection procedure can strongly overfit the data .",
    "it is possible to estimate the selection - induced bias and obtain unbiased estimates , for example by using another level of cross - validation . this does not , however , prevent the model selection procedure from possibly overfitting to the observations and consequently selecting models with suboptimal predictive performance .",
    "this is one reason we view cross - validation and information criteria as an approach for understanding fitted models rather than for choosing among them .",
    "the current state of the art of measurement of predictive model fit remains unsatisfying .",
    "formulas such as aic , dic , and waic fail in various examples : aic does not work in settings with strong prior information , dic gives nonsensical results when the posterior distribution is not well summarized by its mean , and waic relies on a data partition that would cause difficulties with structured models such as for spatial or network data .",
    "cross - validation is appealing but can be computationally expensive and also is not always well defined in dependent data settings . for these reasons ,",
    "bayesian statisticians do not always use predictive error comparisons in applied work , but we recognize that there are times when it can be useful to compare highly dissimilar models , and , for that purpose , predictive comparisons can make sense . in addition , measures of effective numbers of parameters are appealing tools for understanding statistical procedures , especially when considering models such as splines and gaussian processes that have complicated dependence structures and thus no obvious formulas to summarize model complexity .    thus we see the value of the methods described here , for all their flaws . right now",
    "our preferred choice is cross - validation , with waic as a fast and computationally - convenient alternative .",
    "waic is fully bayesian ( using the posterior distribution rather than a point estimate ) , gives reasonable results in the examples we have considered here , and has a more - or - less explicit connection to cross - validation , as can be seen its formulation based on pointwise predictive density ( watanabe , 2010 , vehtari and ojanen , 2012 ) .",
    "a useful goal of future research would be a bridge between waic and cross - validation with much of the speed of the former and robustness of the latter .",
    "-        akaike , h. ( 1973 ) . information theory and an extension of the maximum likelihood principle . in",
    "_ proceedings of the second international symposium on information theory _ , ed .",
    "b. n. petrov and f. csaki , 267281 .",
    "budapest : akademiai kiado . reprinted in _ breakthroughs in statistics _",
    "s. kotz , 610624 .",
    "new york : springer ( 1992 ) .",
    "dempster , a .p .",
    "( 1974 ) . the direct use of likelihood for significance testing .",
    "proceedings of conference on foundational questions in statistical inference , department of theoretical statistics : university of aarhus , 335352 .",
    "jones , h. e. and spiegelhalter , d. j. ( 2012 ) . improved probabilistic prediction of healthcare performance indicators using bidirectional smoothing models .",
    "_ journal of the royal statistical society a _ * 175 * , 729747 .",
    "spiegelhalter , d. , thomas , a. , best , n. , gilks , w. , and lunn , d. ( 1994 , 2003 ) . bugs : bayesian inference using gibbs sampling .",
    "mrc biostatistics unit , cambridge , england ."
  ],
  "abstract_text": [
    "<S> we review the akaike , deviance , and watanabe - akaike information criteria from a bayesian perspective , where the goal is to estimate expected out - of - sample - prediction error using a bias - corrected adjustment of within - sample error . </S>",
    "<S> we focus on the choices involved in setting up these measures , and we compare them in three simple examples , one theoretical and two applied . the contribution of this review is to put all these information criteria into a bayesian predictive context and to better understand , through small examples , how these methods can apply in practice .    </S>",
    "<S> keywords : aic , dic , waic , cross - validation , prediction , bayes </S>"
  ]
}