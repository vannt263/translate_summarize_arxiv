{
  "article_text": [
    "high - dimensional data is increasingly common in modern statistical analysis , where the number of variables is on the order of thousands and beyond . in an international competition on the analysis of breast cancer ,",
    "the raw data has @xmath0 bins for predictors ( hand , 2008 ) . at the center for cancer research , the proteomic data for ovarian cancer has @xmath1 predictors and it is free for anyone to download ( http://home.ccr.cancer.gov/ncifdaproteomics/ovariancd_postqaqc.zip ) . in a bankruptcy application , foster and stine ( 2004 , _ jasa _ ) used @xmath2 predictors in their model .",
    "efron ( 2008 , 2010 ) mentioned @xmath3 for microarray gene expression data , @xmath4 for imaging processing , and @xmath5 for snp analysis . in a different area of application ,",
    "a _ new york times _ article ( october 30 , 2005 ) reported that google utilizes millions of variables about its users and advertisers in its predictive modeling to deliver the message to which each user is most likely to respond . furthermore , in the field of astrostatistical applications , efron ( 2008 ) mentioned @xmath6 , which would dwarf other data and is probably qualified to be called mother of high - dimensional data .    in certain areas of _ predictive modeling _ with high - dimensional data , statistical methods and machine - learning tools",
    "appear to be doing very well , but in other applications where _ causal inferences _ are involved , the results are often less than satisfactory .",
    "shmueli ( 2010 , _ statistical science _ ) asked the question ,  to explain or to predict ? \" , and emphasized that  the type of uncertainty associated with explanation is of a different nature than that associated with prediction \" ( a la helmer and rescher , 1959 ) .",
    "here explanatory modeling refers to the statistical analysis of cause - and - effect .",
    "the focus of our study is an extension of this effort .    for this purpose",
    ", we found that the field of gene identification provides an excellent framework to discuss a number of issues related to the limitations and potential misuses of causal inference from high - dimensional data .",
    "the datasets in our study are taken from the field of microarray gene identification .",
    "this technology is a powerful tool for measuring the relative expression level of thousands of genes in a single experiment . in particular",
    ", every cell in an organism expresses its own set of genes .",
    "skin cells express different genes than bone cells , and colon cancer cells express different genes than normal colon cells .",
    "therefore , one way to determine what genes cause a particular trait or disease is to compare the genes expressed in one cell type to those expressed in the other cell type .",
    "microarrays allow this kind of comparative study to take place on a very large scale : the expression level of thousands of genes can be compared across cell types . in this study ,",
    "our goal is to identify those genes that are differentially expressed in cancer , as these differentially expressed genes may actually be the cause of cancer formation and progression . in order to do this ,",
    "the hundreds to hundreds of thousands of data points collected in microarray experiments need to be analyzed using sound and robust statistical methods .    given the massive size of the datasets , and the number of statistical techniques available for analysis , the field has attracted an enormous amount of attention from researchers around the globe .",
    "a survey of the literature reveals that there is a huge variety of techniques for selecting genes whose aberrant expression correlates with a particular tissue type or disease state .",
    "these techniques can be roughly grouped into the following two categories :    * _ multiple hypothesis testing _ that includes @xmath7-tests , the bonferroni correction , false discovery rates , empirical bayes , sidak method , @xmath8-values , mid @xmath9-values , platform @xmath9-value , @xmath10-test , two - step non - parametric statistical analysis , regularized @xmath7-test , hierarchical lognormal - normal model , etc . for references , see for instance leek and storey ( 2011 ) , efron ( 2011 , 2010 , 2008 ) , bar et al .",
    "( 2010 ) , storey ( 2010 ) , ferreira1 and zwinderman ( 2006 ) , dudoit , shaffer and boldrick ( 2003 ) , sierra and echeverria ( 2003 ) , guyon and elisseeff ( 2003 ) , benjamini and hochberg ( 1995 ) , etc . * _ statistical models and machine - learning methods _ that includes logistic regression , anova , support vector machines , neural networks , random forests , @xmath11-nearest neighbors , diagonal linear discriminant analysis , nave bayes , nearest centroid , rough set , emerging pattern , a genetic - algorithm - based fisher s discriminant analysis , mahalanobis decorrelation , latent class analysis , laplace approximated em microarray analysis , pathway analysis , neighborhood mutual information , fuzzy mutual information , and numerous other variations . for references , see for instance huang et al .",
    "( 2011 ) , zuber and strimmer ( 2011 ) , wang and simon ( 2011 ) , bar et al .",
    "( 2010 ) , hu et al .",
    "( 2010 ) , mongan et al .",
    "( 2010 ) , cordell ( 2009 ) , lee et al .",
    "( 2008 ) , dean and raftery ( 2008 ) , ma and huang ( 2007 ) , guyon and elisseeff ( 2003 ) , etc .",
    "in addition , there are countless references within each of the above categories .",
    "furthermore , each technique in the above lists can have endless variations .",
    "for instance ,    * in their paper , ",
    "should we abandon the @xmath7-test in the analysis of gene expression microarray data , \" jeanmougin et al .",
    "( 2010 ) considered eight different tests representative of various modeling strategies in gene expression data : anova ( homoscedastic ) , welch s @xmath7-test ( heteroscedastic ) , rvm ( homoscedastic ) , limma ( homoscedastic and based on a bayesian framework ) and smvar ( heteroscedastic and based on structural model ) , plus two non - parametric approaches with the wilcoxon s test and the sam test . *",
    "regression - based methods would include sliced inverse regression , correlated component regression , lasso regression , the elastic net , non - negative garrote method , etc . among these methods , in the area of lasso regression , there are nine different methods in a matlab toolbox by liu et al .",
    "( 2009 ) , and for each method one can define the penalty functions in multiple ways to generate new models ( http://www.public.asu.edu/~jye02/software/slep/ ) .",
    "it is conceivable that one may find countless other variations in the literature .",
    "* in the area of support vector machines ( svm ) , there are at least 25 different kernels + ( http://crsouza.blogspot.com/2010/03/kernel-functions-for-machine-learning.html ) .",
    "+ with some modifications and hybridizations , it would be straightforward to generate hundreds of additional kernels without knowing which one is best suited to analyze the data at hand . * the well - known cart method , classification and regression tree , as laid out in breiman , friedman , olshen , and stone ( 1983 ) has endless variations in the machine - leaning literature including gini index , chi - square criterion , entropy , genetic - algorithm tree ( cha and tappert , 2009 ) , and neural - network tree ( sas , 2003 ) .",
    "the techniques can easily fill up a huge volume to greet a biologist or to send him / her down the wrong path when it comes to analyzing a high - dimensional dataset .",
    "* neural network models have even more variations than cart : multilayer perceptrons ( mlps ) , radial basis function ( rbf ) networks , and many other forms of network architectures .",
    "sas , a software package , provides nine different kinds of architectures , fourteen different kinds of error functions , eight different kinds of combination functions , and twelve different kinds of activation functions .",
    "there are a lot more in scholarly publications ( see e.g. , a 2009 book by pereira and rao titled , _ data mining using neural networks : a guide for statisticians _ ) .",
    "a google search of  neural network architecture \" ( with quotations marks ) rendered 3,130,000 results , with more coming every day .",
    "the situation reminds us the famous example from 1972 where 10,465 techniques were constructed in the estimation of a statistical quantity called the location parameter ( stigler , 2010 , a la andrews et al . , 1972 ) .",
    "for the modern - day gene hunt , the number of techniques available is equally endless .",
    "a natural question is : how reliable are these statistical tests and modeling techniques ?",
    "specifically , one may ask whether the models are stable , whether they are consistent , and whether it is true that  the increased level of algorithmic complexity does not always translate to improved biological understanding \" ( mongan et al . , 2010 ) .",
    "along the same line of inquiry , wang and simon ( 2011 , p. 22",
    ", table 5 ) found that many tools achieved high prediction accuracies , yet did so using different important genes for the same disease . in an earlier study ,",
    "efron ( 2008 , p. 7 ) pointed out that    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ the prostate data has e(fdr ) = 0.68 , indicating low power [ here e(fdr ) = the expected value of false discovery rate ] .",
    "if the whole study were rerun , we could expect a different list of 50 likely nonnull genes , barely overlapping with the first list .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    in short , the scientific literature focused on the identification of relevant genes from microarray data is vast and not necessarily reliable . consequently , the main objective of this article is to evaluate some widely - used statistical tests and machine - learning approaches in the analysis of microarray data . specifically , we look at two microarray datasets ( detailed below ) and we generate sets of simulated microarray data for which the genes that contribute to the diseased state are known .",
    "we employed several well - known statistical methods to identify the differentially expressed ( important ) genes and classify the datasets .",
    "based on the results for the experimental and simulated datasets , we make recommendations on the statistical methods we find to be the most reliable .",
    "our study was motivated by two datasets that are widely known in the field of cancer research :    * colon cancer from alon et al . , ( 1999 ) .",
    "the data consists of 2,000 genes and 62 patients , 40 who have colon cancer , and 22 who do not . * prostate cancer from singh et al .",
    "the data consists of 6,033 genes and 102 patients , 52 who have cancer and 50 who are healthy .    in section 2 ,",
    "we review previous results from the colon cancer data and then present new results on other models that outperform the original results in terms of accuracy and the number of genes selected .",
    "furthermore , we discuss the merits and potential pitfalls of the top models we tested on both the colon and prostate cancer data , cautioning that even the results from these top models may be deceiving under certain conditions . in section 3 ,",
    "the top models undergo further scrutiny when we test their performance in a variety of scenarios using simulated data .",
    "the results of analyzing the simulated data further strengthen our belief that several popular models may mislead investigators analyzing microarray data . in section 4 ,",
    "the relationship between sample size , number of genes and statistical reliability is explored in depth .",
    "our analysis suggests that gradient boosting is a significantly better tool than the others explored , and that the sample size used in some microarray experiments is not sufficiently large for most statistical methods to accurately and consistently select the most important genes to classify the data .",
    "in this section , we review previous statistical analyses of the colon cancer dataset .",
    "we then compare the performances of our models with these previously - published results .",
    "much to our delight , some of our models achieved 100% accuracy in classifying the data in multiple runs with different random splits of the data into training and validation purposes .",
    "in addition , our models achieved this accuracy using fewer genes than most previously - published analyses .",
    "however , our further investigations indicate that the models are not reliable , as will be seen in the subsequent discussions .",
    "table 1 ( see next page ) presents a brief summary of the previous analysis of the colon cancer dataset .",
    "we note that the models attempt to classify a sample as cancerous or normal using anywhere from 5 to 2,000 genes , and the error rates range from 11.3% to 34% .",
    "llll    ' '' ''    height 1pt @a xhline    & * variable * & * # of genes * & * prediction * + & * selection * & * selected * & * error * +    ' '' ''    height 1pt @a xhline    blind bet ( no model ) & - - & 2000 & 33% +    alon et al .",
    "( 1999 ) & & + _ proc .",
    "sci _ & clustering & 500 & n / a +    weston et al .",
    "( 2001 ) & & + _ adv neural informat _ & svm & 15 & 11.4% +    guyon et al .",
    "( 2002 ) & & + _ machine learning _ & svm & 8 & 34% +    weston et al .",
    "( 2003 ) & & + _ j. machine learning _ & kernel methods & 20 & 13.7% +    su et al .",
    "( 2002 ) & & + _ bioinformatics _ & @xmath7-tests , svm & 100 & n / a",
    "+    do et al .",
    "( 2005 ) & & + _ j. royal stat soc .",
    "_ & fdr & 1938 & n / a +    ma et al .",
    "( 2007 ) & & + _ bmc bioinformatics _ & lasso & 19 & 12.9% +    lee et al .",
    "( 2008 ) & & + _ j. biopharmaceut .",
    "stat _ & svm ( 1-norm ) & 8 & 11.3%@xmath12 +    lee et al .",
    "( 2008 ) & & + _ j. biopharmaceut .",
    "stat _ & svm ( iffs ) & 5 & 11.3%@xmath12 +    bar et al .",
    "( 2010 ) & & + _ statistical science _ & laplace em & 61 & n / a +    ' '' ''    height 1pt @a xhline    the first statistical method we used to analyze the colon cancer microarray data was that of partial least squares ( pls ) with leave - one - out cross - validation . here",
    "leave - one - out means that , given @xmath13 observations , the model was trained using @xmath14 data points , and the model was used to predict whether the remaining data is representative of cancer or no cancer .",
    "the model was run @xmath13 times by altering which @xmath14 of the @xmath13 data points were used for training , and which dataset was being classified as cancerous or normal based on the trained model .",
    "quite impressively , when pls selected the nine most important genes , the leave - one - out prediction error was 0% ( table 2 , see next page ) .",
    "this indicates that the model could always correctly classify the one microarray dataset that was not used for training purposes .    before running the pls models to get the data shown in table 2",
    ", the original 2,000 genes in the dataset were prescreened by an @xmath15-square variable selection procedure which selected only 25 genes from the larger pool .",
    "the @xmath15-square procedure is one of many prescreening techniques available for reducing the dimensionality of a dataset before searching for the most important genes ( guyon and elisseeff , 2003 ) .",
    "this prescreening procedure is essential , as pls does not perform reliably using thousands of predictors at a time .",
    "the genes that were selected from the @xmath15-square variable selection procedure are then used in the subsequent runs of the partial least squares model .",
    "the default pls with 25 genes achieved 0% error rate , meaning it always correctly classified whether the remaining data set represented cancer or no cancer .",
    "we then used a stepwise elimination process to cut the low - ranking genes from the model .",
    "we found that the 16 lowest - ranking genes of the 25 prescreened genes could be eliminated from the model without impacting predictions : that is , using only the 9 genes whose expression varies the most between cancer and no cancer , the error rate of pls is 0% .",
    "however , if we cut down to the top 8 genes , the error rate went up slightly to 3.2% .    lcc    ' '' ''    height 1pt @a xhline    & * # of genes * & * leave - one - out * + & * selected * & * prediction error * +    ' '' ''    height 1pt @a xhline    pls-1 & 9 & 0% + pls-2 & 8 & 3.2% +    ' '' ''    height 1pt @a xhline    the next statistical methods we utilized to analyze the colon cancer data are that of logistic regression and neutral networks ( nn ) , both of which require variable prescreening similar to pls .",
    "our regression and neural network analyses are based on random splits of the data into a training set ( 75% of the data ) and validation set ( 25% of the data ) .",
    "the results of analyzing the colon cancer data using these statistical methods are displayed in table 3 .",
    "lcrr    ' '' ''    height 1pt @a xhline    & * # of * & * training * & * validation * + & * genes * & * error rate * & * error rate * +    ' '' ''    height 1pt @a xhline    regression-1 & 13 & 0% & 0% + regression-2 & 12 & 0% & 5.9% + pls-3 & 13 & 0% & 0% + pls-4 & 12 & 0% & 5.9% + neural network-1 & 19 & 0% & 0% + neural network-2 & 18 & 4.3% & 13.3% +    ' '' ''    height 1pt @a xhline    from tables 2 and 3 , the best model appears to be pls-1 with 9 genes and 0% leave - one - out error rate .",
    "while the error rate is the same as achieved for other statistical methods , we say pls-1 appears to be the best model as it required the fewest number of genes to classify the data with a 0% error rate . in order to facilitate comparison with regression and neural networks , partial least squares",
    "was also conducted by placing 75% of the data in the training set , and 25% in the validation set .",
    "pls-3 is the analysis when 13 genes were selected via this split of the data , and pls-4 is the analysis when 12 genes were selected via this split of the data .",
    "notice that when compared to regression and neural networks , pls with 13 genes does just as well as regression with 13 genes and neural networks with 19 genes .",
    "notice , however , that when the data is split up in this manner , pls requires more than 9 genes to achieve a 0% error rate .",
    "while the low error rates we have obtained are desirable , this does not demonstrate that the statistical methods are consistently classifying the data .",
    "for instance , it is plausible that each method uses a very different set of genes to achieve the low classification error rates .",
    "in order to explore the between - model consistency , we looked at the top genes selected by partial least squares , regression and the neural network ( table 4 ) . in each case , an @xmath15-square prescreening method was applied to reduce the number of genes input into the statistical methods from 2,000 to 38 .",
    "fortunately , the three models appear to be very consistent : seven genes are selected by all three statistical methods , and three genes are selected by at least two statistical methods . every gene selected by pls-1",
    "was also selected by one or both of the other statistical methods .",
    "llll    ' '' ''    height 1pt @a xhline    & * regression * & * pls-1 * & * neural network * + & accuracy = 100% & accuracy = 100% & accuracy = 100% +    ' '' ''    height 1pt @a xhline    1 & gene-1025 & & + 2 & & gene-1466@xmath16 & + 3 & & & gene-1421 + 4 & & gene-1482@xmath16 & gene-1702 + 5 & gene-1466@xmath16 & & + 6 & gene-1482@xmath16 & & gene-258 + 7 & & & + 8 & & & gene-1475 + 9 & gene-1895@xmath17 & & gene-1895@xmath17 + 10 & & & + 11 & & & gene-1914 + 12 & gene-580 & & + 13 & gene-662 & & gene-1889 + 14 & & & gene-945 + 15 & & & +    ' '' ''    height 1pt @a xhline    having checked for consistency among the statistical methods , we next sought to ensure that the way the data was being partitioned does not bias the results . to undertake this analysis , we used five different random seeds to split the data into the training set and the validation set .",
    "we found that independent of which datasets were placed in the training set and which were placed in the validation set , the number of genes required to achieve a 0% validation error rate were rather consistent for each statistical method , as shown in table 5 .",
    "lccc    ' '' ''    height 1pt @a xhline    * seed * & * schwarz * & * training * & * validation * + & * bayesian criterion * & * error rate * & * error rate * +    ' '' ''    height 1pt @a xhline    1 & 178.146 & 0% & 0% + 2 & 178.159 & 0% & 0% + 3 & 178.156 & 0% & 0% + 4 & 178.178 & 0% & 0% + 5 & 178.148 & 0% & 0% +    ' '' ''",
    "height 1pt @a xhline      taken at face value , our results appear very encouraging in the sense that three different models , pls , regression and neural network , achieved 0% prediction error .",
    "table 4 also showed that the three models select seven common genes and are very consistent .",
    "but subsequent analysis in sections 2.2 , 2.3 , 3.2 , 3.3 will cast doubt on conclusions drawn from these models .",
    "first of all , from table 6 , we observe that the estimates of the logistic regression model are relatively small while the corresponding standard errors of the estimates are quite large . as a result , the corresponding wald chi - squares are also very small and the @xmath9-values are not significant .",
    "thus , any conclusions drawn from the model are dubious and may not generalize well for new data .",
    "lcccc    ' '' ''    height 1pt @a xhline    * gene * & * estimate * & * standard error * & * wald chi - square * & * * pr**@xmath18*chisq * +    ' '' ''    height 1pt @a xhline    1025 & 0.00608 & 0.0475 & 0.02 & 0.898 + 1231 & 0.1188 & 0.4193 & 0.08 & 0.777 + 1351 & 0.0203 & 0.1179 & 0.03 & 0.8631 + 1367 & -0.06 & 0.2638 & 0.05 & 0.8202 + 1466 & -0.0349 & 0.4738 & 0.01 & 0.9413 + 1482 & 0.0258 & 0.3734 & 0 & 0.9449 + 1644 & 0.0523 & 0.29 & 0.03 & 0.857 + 1769 & -0.1642 & 0.5339 & 0.09 & 0.7584 + 1895 & -0.00606 & 0.0785 & 0.01 & 0.9385 + 249 & 0.00391 & 0.0111 & 0.12 & 0.7249 + 419 & 0.04 & 0.1262 & 0.1 & 0.751 + 580 & -0.0267 & 0.1666 & 0.03 & 0.8726 + 662 & -0.0103 & 0.0406 & 0.06 & 0.799 +    ' '' ''    height 1pt @a xhline    the problem is related to  complete separation \" in binary regression where the maximum likelihood function does not exist and the iterations do not converge . as a result , the model ,",
    "albeit with 0% error rates in multiple runs on different seeds , may not hold up to future observations .",
    "a discussion of this phenomenon can be found at http://www.ats.ucla.edu/stat/sas/library/logistic.pdf .",
    "another reference on the convergence of the maximum likelihood estimate is in stokes ( 2004 ) .",
    "potential ways to fix this problem for logistic regression have been proposed ; see , e.g. , firth ( 1993 ) , heinze and schemper ( 2002 ) , and park and hastie ( 2008 ) .",
    "next , we considered the parameter estimates of pls as shown in table 7 .    lcc    ' '' ''    height 1pt @a xhline    * gene * & * standardized parameter * & * rejected by parameter * + & * estimate * & * estimate ? * +    ' '' ''    height 1pt @a xhline    1769 & -0.69336 & no + 1466 & -0.31386 & no + 1367 & -0.29207 & no + 1482 & 0.1661 & no + 419 & 0.30232 &",
    "no + 1351 & 0.3035 & no + 1644 & 0.37666 & no + 249 & 0.45373 & no + 1231 & 0.50063 & no +    ' '' ''    height 1pt @a xhline    the traditional cutoff @xmath19-value for statistical significance in the standardized parameter estimates are values outside @xmath201.96 . however , in table 7 , all of the genes selected fail to cross this significance threshold . in practice , users often do not know whether the parameter estimators are normally distributed and cutoff values of 0.1 or 0.2 are often used for the standardized estimates . in table 7 , the cutoff is 0.1 and it renders a model with 100% prediction accuracy .",
    "we now turn our attention to the reliability of the neural network predictions . to do this , we will limit our model to a structure with only _ one hidden unit _ to facilitate the comparison of parameter estimates .",
    "table 8 below shows some of the top predictors selected by the neural network .    lc    ' '' ''    height 1pt @a xhline    * gene * & * weight * + & * ( ranked by absolute value ) * +    ' '' ''    height 1pt @a xhline    1769 & -2.866915264 + 1231 & 2.634994782 + 1421 & -2.243876759 + 1702 & -1.999461292 + 1351 & 1.722926448 + 258 & -1.650588414",
    "+ 1644 & 1.530640588 + 1475 & 1.401072018 + 1895 & -1.107656933 + 419 & 1.002626559 +    ' '' ''    height 1pt @a xhline    note that in table 8 , the selection of the top genes is rather subjective and the cutoff is arbitrary . in practice",
    ", one can try backward elimination , forward inclusion , and the stepwise procedure to select the genes .",
    "however , unlike pls and regression , the literature contains no reliable ways to calculate the standard error of the weight and hence there is no way to judge whether the estimates of the weights would behave like those in table 7 ( pls ) and table 6 ( regression ) .",
    "the same can be said for svm ( support vector machines ) and other methods in table 1 .    in conclusion",
    ", a regression model or pls can achieve 100% accuracy but the parameter estimates are not reliable . in the current literature ,",
    "the problem of  complete separation \" of logistic regression is well - known , but there is no such analysis for pls , neural networks , support vector machines , or other models . in the next section ,",
    "we provide further proof that models with high accuracy can be very misleading .",
    "we will now shift our focus from the colon cancer dataset to the benchmark prostate cancer data that has 102 patients ( 52 cancer , 50 normal ) and 6,033 genes .",
    "the data was collected and analyzed by a team of 15 scientists from a dozen institutions including harvard medical school , whitehead institute / massachusetts institute of technology , and bristol - myers squibb inc .",
    "princeton .",
    "as one would imagine , it is very expensive to conduct a microarray experiment of this magnitude , and it would be desirable to have more cost - effective alternatives . as a result , we frame up a scenario as follows : a biologist who has a limited budget collected only 10% of the samples as compared to the benchmark dataset ( i.e. , there are only 10 patients in the sample ) .",
    "the biologist pre - screened the 6,033 genes by a statistical variable selection technique , then ran the pls model with leave - one - out cross - validation , and found that the model can classify the validation datasets as cancer or normal with 100% prediction accuracy .",
    "in addition , the biologist used regression to double check the pls results and also obtained 100% prediction accuracy with the same genes as the pls model : gene1149 , gene4201 , and gene4780 . finally , the biologist double - checked the statistical results by examining the posterior probabilities as shown in table 9 .",
    "lcc    ' '' ''    height 1pt @a xhline    * patient * & * mean posterior probability * & * cancer or normal ? * +    ' '' ''    height 1pt @a xhline    1 & 1 & cancer + 2 & 1 & cancer + 3 & 0.998 & cancer + 4 & 0.995 & cancer + 5 & 0.979 & cancer + 6 & 0.011 & normal + 7 & 0.007 & normal + 8 & 0.005 & normal + 9 & 0 & normal + 10 & 0 & normal +    ' '' ''    height 1pt @a xhline    the posterior probabilities indicate that the model did extremely well classifying the data .",
    "this would represent an excellent finding for the biologist , especially considering that regression and pls use different methodologies : regression is based on the maximum likelihood estimation of the parameters of the following equation :    @xmath21    while pls is based on the extraction of latent variables from the covariance matrices of    @xmath22    since the two methodologies are vastly different , the results appear to have reinforced each other in a significant manner .",
    "the scientist also noticed that pls has been widely used in analytic chemistry ( see , e.g. , wold et al . , 2001 ) and other fields ( see , e.g. , vinzi et al . , 2010 )",
    ", so the results are very encouraging and the 10% sample has the potential to cut research costs by 90% . if the results hold water , it would be great news for all researchers in this field of study .",
    "but now the problem : three other imaginary scientists did the same experiment with a different 10% of the sample .",
    "the situation is depicted in the process flow shown in figure 1 .",
    "now the miracle ( see table 10 below ) : the four scientists all achieved 100% prediction accuracy but the genes they selected are vastly different .    lclcc    ' '' ''    height 1pt @a xhline    * method * & * prediction * & * genes selected * & * sample * & * seed * + & * accuracy * & & * size * & +    ' '' ''    height 1pt @a xhline    pls - e1 & 100% & 1149 , 4201 , 4780 & 10 & 12345 + pls - e2 & 100% & 38 , 476 , 5585 & 10 & 23451 + pls - e3 & 100% & 1352 , 1751 , 3560 & 10 & 34512 + pls - e4 & 100% & 38 , 1871 & 10 & 45123 + efron & n / a & 610 , 1720 , 332 , 364 , & 102 & n / a + ( 2010 ) & & 914 , 3940 , 4546 , 1068 , 579 , 4331 & & +    ' '' ''    height",
    "1pt @a xhline    table 10 also includes the 10 genes that were selected by efron ( 2010 ) which have very little in common with the rest four sets of the genes . in summary ,",
    "four scientists set out to collect data and use pls to find the most important genes . in two of the four cases ,",
    "the biologists even confirmed their pls predictions using regression .",
    "each of their models has a 100% prediction accuracy , but the genes they picked are vastly different .",
    "which set of genes would you believe ?",
    "we conclude that the 100% prediction accuracy actually misled our imaginary scientists to believe that a sample size of only 10 patients is sufficient to analyze the prostate cancer dataset .",
    "in this section , we will create our own data to simulate microarray data .",
    "comparable to the colon cancer dataset , the simulated data will contain the expression level of 2,000  genes \" for 62 simulated  patients \" . in the colon cancer dataset ,",
    "more than 15 of the genes are correlated with a correlation coefficient @xmath23 ( see the scatterplot of gene493 and gene249 in figure 2 ) .",
    "0.0000 . ]    as a result , we added correlations to the three genes x1 , x2 , x3 in our simulation data . from this point forward , xi represents the numerical gene expression level of gene xi .",
    "the other 1,997 gene expression levels are generated from a uniform distribution .",
    "the corresponding formulae for generating correlated x1 , x2 and x3 are as follows .",
    "assume @xmath24 , @xmath25 , and @xmath26 are independent and identically distributed random variables .",
    "let    @xmath27    @xmath28    using b=0.8 and c=0.75 gives the following correlations between x1 , x2 and x3 :    @xmath29    @xmath30    @xmath31    the correlations of the three predictors in the simulation data are shown in figure 3 .        to facilitate the simulations of 5-gene and 10-gene interactions , we re - scaled the ranges of the variables .",
    "the mean , standard deviation , maximum , and minimum values of x1 , x2 , x3 are listed in table 11 .",
    "lclcc    ' '' ''    height 1pt @a xhline    gene & mean & standard deviation & minimum & maximum + x1 & 43.9 & 26.4 & 0.4 & 97.2 + x2 & 166.3 & 67.2 & 23.5 & 283 + x3 & 278.3 & 92.4 & 35.2 & 481.3 +    ' '' ''    height 1pt @a xhline    we will use this controlled dataset to represent the gene expression level of 62 patients . then , we will define functions that classify the 62 patients as cancerous or normal .",
    "this will allow us to examine how the various statistical methods perform at classifying the simulated data as diseased or normal .",
    "the advantage of this approach is that we already know how each dataset should be classified , and we also know which gene expression levels are responsible for that classification of the disease .",
    "there is precedent for using simulated data to rigorously examine the reliability of a statistical model .",
    "for instance , park and hastie ( 2008 ) investigated gene - gene and gene - environment interactions using three discrete epistatic models and a heterogeneity model of two interacting genes .",
    "each of the two genes is assumed to have a dominant allele ( form ) and a recessive allele , and the models captured different potential modes of interaction between the two genes .",
    "noisy data was generated from each of these models , and statistical methods along with multifactor dimensionality reduction were used to train and classify the simulated datasets .",
    "what park and hastie did was model the interaction between the two genes of interest in their study ( 2008 ) .",
    "however , in the case of microarray data , there are potentially thousands of interacting genes .",
    "this greatly complicates the analysis of microarray data , as the scale makes it nearly impossible to model the gene - gene interactions . to put this in perspective , cordell ( 2009 , _ nature reviews genetics _ ) wrote an extensive review on detecting gene - gene interactions that underlie human disease .",
    "the review discussed different methods for deciphering all two - locus interactions and the associated computational costs of each method .",
    "the article concluded  an exhaustive search of all three - way , four - way or higher - level interactions seems impractical in a genome - wide setting . \"",
    "this point was driven further home in a recent article by van steen entitled  travelling the world of gene - gene interactions \" ( 2011 , _ briefings in bioinformatics _ ) . given this reality",
    ", we can not expect to build models that will accurately capture the interaction between all genes that give rise to cancer .",
    "therefore , we can not build a discrete , allele - based model comparable to that of park and hastie for our purposes .",
    "instead , we will have to generate expression - level datasets that are comparable to datasets generated from microarray experiments .",
    "we then need mathematical equations that can classify the dataset as diseased or normal based on the expression level of a hand - selected set of genes .      in order to classify our simulated datasets , we have designed equations that take the dataset as input , and output whether the dataset represents a diseased state or a normal state .",
    "we start with a disease in which only a single gene is responsible for the disease , and we build up from there adding more contributing genes , and more complex ( nonlinear ) interactions between the genes .",
    "our simulated dataset consists of 2,000 genes / predictors ( x1-x2000 ) for 62  patients \" and eight initial equations that will be used to classify the simulated patients as diseased or normal . in the first three disease equations",
    ", each gene linearly contributes to the disease state , and there are no gene - gene interactions .    *",
    "* disease1 * : disease or normal is determined solely by the expression level of x1 : + @xmath32 + where 0 represents a normal dataset and 1 represents a diseased dataset .",
    "this is similar to a number of single gene diseases , including hemophilia a ( x - linked recessive disease determined by f8 gene ) , cystic fibrosis ( autosomal recessive disease determined by cftr gene ) , sickle - cell anemia ( autosomal recessive disease determined by hbb gene ) and huntington s disease ( autosomal dominant disease determined by htt gene ) to name a few ( chial , 2008 ) . * * disease2 * : disease or normal is determined by a linear combination of x1 , x2 : + @xmath33 + this is similar to the familial breast cancer , which is attributed to two genes : brca1 and brca2 ( ritchie et al . , 2001 ) .",
    "note that familial breast cancer is rare ( about 5% of the female population ) .",
    "for the non - familial breast cancer , the genetic structure is a lot more complicated .",
    "* * disease3 * : disease or normal is determined by a linear combination of x1 , x2 , x3 : + @xmath34 + both colon cancer and prostate cancer involve more than three genes ( see , e.g. , table 1 and table 10 ) .",
    "our analysis will start with three genes and gradually move upward .",
    "the cutoff value in each function was designed so that the distribution of the disease is _ relatively balanced _ as shown in the colon cancer and prostate cancer data .",
    "for instance , the disease 1 function @xmath35 used a cutoff of 53.1 to keep the number of cancer and normal patients from being heavily skewed in either direction .",
    "figure 4 displays the histograms of diseased and normal cases using disease1 and disease2 . in the remaining diseases , the cutoffs",
    "were similarly chosen so that the distribution of disease is relatively balanced . instead of presenting the exact cutoff value in the other diseases , we will simply use @xmath36 to represent the cutoff chosen for disease _",
    "i_.        in each of disease1 through disease3 ,",
    "only linear combinations of different genes are considered , so no gene - gene interactions take place , and each gene contributes independently to the disease state .",
    "disease4 through disease8 each account for _ nonlinear _ contributions of genes , and gene - gene interactions .    *",
    "* disease4 * is determined by a nonlinear combination of three genes ( x1 , x2 and x3 ) with the nonlinear term in x1 : + @xmath37 + this represents a scenario in which x1 is a stronger determinant of the disease than x2 or x3 . *",
    "* disease5 * is a modification of disease4 where x1 , x2 and x3 are centered around their means ( @xmath38 is the mean of gene xi ) : + @xmath39 * * disease6 * is similar to disease4 , except there is a nonlinear term for both the expression level of x1 and x2 : + @xmath40 * * disease7 * is determined by a nonlinear combination of three genes : + @xmath41 + in this case , all gene interaction occurs in a pair - wise fashion .",
    "the effect of each gene individually is not considered . *",
    "* diseas8 * is determined by a nonlinear combination of three genes : + @xmath42 + in this case , all three genes interact and mutations in a single gene , or even two genes , have no independent effect on the disease state .      in their ground - breaking paper in _ cancer cell _",
    ", singh et al . (",
    "2002 ) used @xmath43-nearest neighbor for binary classification and obtained 90% accuracy in the prediction of prostate cancer .",
    "furthermore , they maintained that ( p. 206 ) :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ the successful prediction of patient outcome will ultimately lead to improved decision making regarding current therapeutic options and the rational selection of patients at high risk for relapse for clinical trials testing adjuvant therapeutics .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    we agree with this statement . but",
    "a cautionary note is that certain predictive models may produce 100% accuracy yet pick only _ irrelevant genes _ ; that is , genes that are not implicated in the disease state ( see results of disease5 below ) .",
    "we summarize our findings in tables 12 and 13 .",
    "lllll    ' '' ''    height 1pt @a xhline    & * disease1 * & * disease2 * & * disease3 * & * disease4 * +    ' '' ''    height 1pt @a xhline    true genes & x1 & x1 , x2 & x1 , x2 , x3 & x1 , x2 , x3 + interaction & linear & linear & linear & nonlinear + type & & & & + & & & & +    ' '' ''    height 1pt @a xhline     +   +    ' '' ''    height 1pt @a xhline    decision tree & & & x1 , x2 & + & ( 0% ) & ( 1.6% ) & ( 4.8% ) & ( 0% ) + boosting & & , @xmath44 & , @xmath44 & + & ( 0% ) & ( 0% ) & ( 0% ) & ( 0% ) + pls & , x666 & & x1 , x3 , x1009 & , @xmath44 + & ( 0% ) & ( 3.3% ) & ( 3.2% ) & ( 0% ) + nn & , @xmath44 & , x1025 & x1 , x3 , @xmath44 & , @xmath44 + & ( 0% ) & ( 0% ) & ( 5.6% ) & ( 11.8% ) + reg - stepwise & none & x2 & x1 , x540 & none + & & ( 6.3% ) & ( 27.8% ) & + regression default & & , @xmath44 & x3 , @xmath44 & + & ( 0% ) & ( 0% ) & ( 5.9% ) & ( 0% ) +    ' '' ''    height 1pt @a xhline    for the linear diseases 1 and 2 , each model with the exception of stepwise regression did an excellent job classifying the data . the statistical methods each identified the genes that contribute to the disease state , and classify the data with at least 96% accuracy . while some of the statistical methods identified genes that were not implicated in the disease state , these false discoveries are much less worrisome to biologists than false non - discoveries . for disease 3 , only gradient boosting selected all of the important genes .",
    "disease 4 is the first nonlinear disease we examined . in the formula to compute disease4 ,",
    "the expression level of x1 is squared , meaning it represents the _ most important gene _",
    ", with x2 and x3 being less influential .",
    "this may be the reason that three models ( decision tree , gradient boosting , and regression ) picked up only x1 and subsequently had a 0% error rate . while identifying all contributing genes is most desirable , identifying the major contributing gene is most important from a biological perspective , so these three methods can adequately handle disease4 .",
    "lllll    ' '' ''    height 1pt @a xhline    & * disease5 * & * disease6 * & * disease7 * & * disease8 * +    ' '' ''    height 1pt @a xhline    true genes & x1 , x2 , x3 & x1 , x2 , x3 & x1 , x2 , x3 & x1 , x2 , x3 + interaction & nonlinear & no interaction & nonlinear & nonlinear + type & & & 2-way interaction & 3-way interaction + & & & & +    ' '' ''    height 1pt @a xhline     +   +    ' '' ''    height 1pt @a xhline    decision & & x2 & x2 , @xmath44 & x1 , x2 + tree & ( 0% ) & ( 3.2% ) & ( 3.2% ) & ( 4.8% ) +    boosting & , @xmath44 & & x3 , x10 & x1 , x2 , @xmath44 + & ( 11.8% ) & ( 0% ) & ( 11.8% ) & ( 11.8% ) +    pls & & none & x2 , x3 , @xmath44 & x1 , x2 + & & & ( 0% ) & ( 9.7% ) + nn & & @xmath44 & x2 , x3 , @xmath44 & x1 , x2 , @xmath44 + & & ( 44.4% ) & ( 0% ) & ( 0% ) + reg- & irrelevant genes & x2 & x2 , x616 & x1 , x2 + stepwise & ( 29.4% ) & ( 11.1% ) & ( 23.5% ) & ( 5.9% ) + reg - default & & @xmath44 & x2 , @xmath44 & x1 , x2 + & & ( 44.4% ) & ( 3.2% ) & ( 4.8% ) + lasso & & & x2 , x3 , & x1 , x2 , + & & ( 0% ) & @xmath44 ( 0% ) & @xmath44 ( 0% ) +    ' '' ''    height 1pt @a xhline    turning our attention to nonlinear diseases _ with _ gene interactions , we begin to notice the statistical methods have difficultly identifying all contributing genes ( table 13 ) . while there are several statistical methods for disease6 through disease8 that can identify two of the three contributing genes , there is not a single statistical model that correctly identifies all three relevant genes for disease7 or disease8 . in subsequent analysis",
    ", we will show that with an increase of sample size from @xmath45 to 102 , boosting can do very well with 3-gene interactions . for higher - order interactions ( 5 genes or 10 genes )",
    ", we will show that larger samples are needed .",
    "disease5 ( first column of table 13 ) gave another surprising result .",
    "here pls , lasso , and default regression achieved 0% error rates , but the genes they picked are purely irrelevant .",
    "this raises a red flag : how can the statistical methods achieve 0% error rates when the genes that determined the disease state are not even being considered ?",
    "note that the results of lasso is compatible with the findings in a forthcoming _ statistical science _",
    "article ( huang et al . , p. 14 - 15 ) : lasso  selects 17 genes out of 30 and 435 markers out of 532 , failing to shed light on the most important genetic markers , \" http://www.imstat.org/sts/future_papers.html .",
    "here we used the least angle regression with 5-fold cross - validation to run lasso .",
    "the results are not very encouraging .    fortunately , both decision tree and gradient boosting were able to correctly identify x1 as the major contributor of disease5 , though they were unable to get the minor contributors .",
    "in fact , in two clean cuts , decision tree faithfully picks x1 with 100% prediction accuracy using leave - one - out cross - validation ( figure 5 ) .",
    "this stands in stark comparison to other popular models like regression and neural network , which select all irrelevant genes .",
    "this is an indication that the tree family methods ( decision tree and boosting ) may be better at detecting the underlying structure of the gene interactions .",
    "a related note is that disease5 is not representative of any known disease in the sense that there is no biological basis to support the centering of the variables around their means .",
    "the centering created the following histograms for cancer ( right panel , figure 6 ) and normal patients ( left panel , figure 6 ) :        in figure 6 , the histogram on the right has a big gap in the middle and is unlikely to happen for cancer or any biological diseases ; instead , it is a  statistical disease \" which causes three popular models ( neural network , pls , and regression ) to select irrelevant variables with 100% classification accuracy .",
    "we believe there are many other statistical diseases like the one presented here . with time and effort",
    ", we may found more examples to expose the weaknesses of statistical models that would ultimately strengthen the science of statistical disciplines .",
    "a final detail about disease1 through disease8 is that we generated our own data so that the numbers of cancer and non - cancer patients are relatively balanced . in reality , this is not the case . in some scenarios",
    ", we tried to create 1,000 patients , where 97% are normal and 3% have cancer .",
    "the data is lopsided and none of the models we tried were able to handle it .",
    "consequently , we used an _ oversampling _ technique to select all cancer patients and an equal number of normal patients .",
    "the oversampling process did work well , and gave results that are similar to what we have in this section .",
    "in section 3 , the simulation datasets were motivated by the colon cancer benchmark data which has 62 patients and 2,000 genes . at this sample size",
    ", there were several statistical methods that worked well when a linear combination of genes caused the disease .",
    "however , the models did not work well with nonlinear interactions of three genes . in this section",
    ", we study the effect of both sample size and the number of contributing genes on the reliability and accuracy of several statistical models .",
    "recall that in section 3.2 , when there are only three important genes implicated in the simulated disease , certain models did well with linear equations ( disease1 through disease4 ) but not so for the nonlinear equations with gene - interactions ( @xmath13 = 62 ) . in this section",
    "we increase the sample size from @xmath13 = 62 to @xmath13= 102 ( with @xmath13 = 102 being motivated by the prostate cancer dataset ) , and consider disease6 through disease8 as defined in section 3 . with this moderate increase of sample size , gradient boosting",
    "_ picks all important genes _ and its prediction accuracies range from 93% to 100% as shown in table 14 below . in table 14 , we also include a semi - saturated nonlinear three gene model :    @xmath46    lcccc    ' '' ''    height 1pt @a xhline    & * disease5 * & * disease6 * & * disease7 * & * disease8 * +    ' '' ''    height 1pt @a xhline    true & x1 , x2 , x3 & x1 , x2 , x3 & x1 , x2 , x3 & x1 , x2 , x3 + genes & no interaction & 2nd order & 3rd order & 3rd order + & & interaction & interaction & interaction + & & & & semi - saturated +    ' '' ''    height 1pt @a xhline     +   +    ' '' ''    height 1pt @a xhline    decision & & x2 , x3 & x2 & x2 + tree & ( 7.1% ) & ( 2.5% ) & ( 2.9% ) & ( 3% ) + gradient & & & & + boosting & ( 5.9% ) & ( 6.7% ) & ( 6.7% ) & ( 6.7% ) +    ' '' ''    height 1pt @a xhline    recall that gradient boosting did not perform well on disease7 and disease8 when @xmath45 patients ( table 13 ) .",
    "table 14 above shows that if @xmath13 = 102 patients , then the model picks all the important genes with high prediction accuracy , and it even gets the relevant minor contributing genes ( see disease6 in table 14 ) .      in this section",
    "we go further to include 5 important genes in the simulation study :    @xmath47    table 15 below summarizes the results with gradient boosting and the benjamini - hochberg fdr procedure .",
    "we found that the results from adjusted @xmath9-values by fdr ( benjamini and hochberg , 1995 ) and adaptive fdr ( benjamini et al . , 2006 ) are compatible to one another .",
    "lll    ' '' ''    height 1pt @a xhline    * method * & @xmath48 * patients * & @xmath49 * patients * +    ' '' ''    height 1pt @a xhline    boosting-1 & x1-x4 & x1-x5 + ( 6,033 genes ) & ( missed x5 ) & accuracy = 86.3% + boosting-2 & x1-x5 & x1-x5 + ( 2,000 genes ) & accuracy = 79% & accuracy = 86.3% +    ' '' ''    height 1pt @a xhline    fdr-1 & x1-x4 & x1-x5 + ( 6,300 genes ) & ( missed x5 ) & + 2 other genes + fdr-2 & x1-x4 & x1-x5 + ( 2,000 genes ) & ( missed x5 ) & + 6 other genes + fdr-3 & x1-x5 & x1-x5 + ( 100 genes ) & + 1 other gene & +    ' '' ''    height 1pt @a xhline    table 15 shows that if @xmath50 , then both gradient boosting and the benjamini - hochberg fdr procedure would be able to pick up all important genes .",
    "note that microarray experiments are very expensive ( although the price is decreasing in recent years ) , and a large sample like @xmath50 may be beyond the reach of many scientists .",
    "consequently a procedure that can handle small sample is highly desired .    turning to the case when",
    "@xmath51 patients , both gradient boosting and fdr missed one important gene , which is _ not acceptable _ to biologists - once an important gene is lost , then it can not be recovered . nevertheless ,",
    "if the number of the genes is cut from 6,033 to 2,000 using a pre - screening method , then boosting would succeed , but this is not the case with fdr . the problem with fdr persisted when we cut the number of genes to 500 ( not shown in table 16 ) .",
    "but if the number of genes is cut to 100 , then fdr would succeed .",
    "gradient boosting is well - known for being able to model nonlinear phenomena ( friedman , 2001 , _ annals of statistics _ ) , but if there are too many genes in the model ( e.g. , 6,033 genes , too much noise ) and @xmath13 is relatively small ( e.g. , 102 patients ) , then the model would fail . for this reason , in table 15 , we use gradient boosting to rank the predictors , cut the bottom ones and then re - fit the model",
    ". this procedure may raise eyebrows if we do it with fdr ( where @xmath9-values are involved ) and one may argue that the repeated adjustments of @xmath9-values would violate the validity of statistical inference . in our opinion , both gradient boosting and fdr are _ exploratory _ tools in the gene selection , and hence the issue of statistical inference does not really matter .",
    "we now extend our simulation to include 10 important genes :",
    "@xmath53    the results are shown in table 16 below .",
    "lllll    ' '' ''    height 1pt @a xhline    & @xmath48 & @xmath49 & @xmath54 & @xmath55 +    ' '' ''    height 1pt @a xhline    boosting-1 & 1 gene & 6 genes & 8 genes & 9 genes + ( 6,033 genes ) & @xmath56 & @xmath56 & @xmath56 & @xmath56 +    boosting-2 & 4 genes & 9 genes & & + ( 500 genes ) & @xmath56 & @xmath56 & fdiscovery = 98% & fdiscovery = 98% + & & & accuracy = 83% & accuracy = 87% +    boosting-3 & 9 genes & & & + ( 100 genes ) & @xmath56 & fdiscovery = 90% & fdiscovery = 90% & fdiscovery = 90% + & & accuracy = 80% & accuracy = 86% & accuracy = 88% +    boosting-4 & 9 genes & & & + ( 20 genes ) & @xmath56 & fdiscovery = 20% & fdiscovery = 20% & fdiscovery = 20% + & & accuracy = 87% & accuracy = 91% & accuracy = 88% +    boosting-5 & 9 genes & & & + ( 12 genes ) & @xmath56 & fdiscovery = 17% & fdiscovery = 17% & fdiscovery = 17% + & & accuracy = 88% & accuracy = 92.2% & accuracy = 94.5% +    ' '' ''    height 1pt @a xhline    fdr & 1 gene & 5 genes & 8 genes & 7 genes + ( 6,033 genes ) & + x1379 & & + x1502 & + fdr & 4 genes & 5 genes & 7 genes & 8 genes + ( 2,000 genes ) & & & & + fdr & 4 genes & 8 genes & 9 genes & + ( 500 genes ) & + x37 & + x382 & & + fdr & 4 genes & 8 genes & & + ( 100 genes ) & + x59 & + x59 & & + fdr & 5 genes & & & + ( 20 genes ) & & & & + x20 +    ' '' ''    height 1pt @a xhline    pls - m1 & 9 genes & 9 genes & 9 genes & + ( 40 genes ) & & & & fdiscovery = 0% + & & & & accuracy = 89% +    pls - m2 & 9 genes & & & 9 genes + ( 20 genes ) & & fdiscovery = 0% & fdiscovery = 0% & + & & accuracy = 89% & accuracy = 89% & +    ' '' ''    height 1pt @a xhline    the data in table 16 allows us to readily see the effects of sample size on gene selection , accuracy and the false discovery rates of various statistical methods .",
    "when looking at ten interacting genes , which can reasonably be expected in cancer , a sample size of @xmath51 is too small for fdr , gradient boosting and pls , as all important genes are not identified .",
    "these false non - discoveries are costly to biologists , as once a gene is screened out of a pool , there is no chance of identifying that gene as an essential component of a disease .",
    "increasing the sample size to @xmath50 starts to paint a different picture .",
    "all three methods perform better at this sample size .",
    "however , in each case , a prescreening method is required to cut down the number of genes used in the model . in the case of fdr and pls ,",
    "the prescreened pool of genes must be of size 20 in order to identify all the relevant genes . when the sample size is @xmath57 patients , gradient boosting performs consistently well on prescreened pools of size 100 genes or smaller .",
    "as we increase the sample size , we find that fdr s performance improves , with all ten relevant genes for disease11 being selected from larger prescreened pools .",
    "pls s performance also improves with sample size , but does not pick up all 10 genes when @xmath13 = 408 patients and there are 20 genes in the model .",
    "gradient boosting is a consistent performer at larger sample sizes , provided a prescreening procedure is applied to cut down the pool of 6,033 genes .",
    "however , gradient boosting is well - known for being an algorithm for _ greedy _ function approximation ( friedman , 2001 ) . as a result ,",
    "the false discovery rate is relatively high .",
    "for instance , in table 16 , boosting-3 with @xmath50 has an extremely high false discovery rate of 90% . while it is desirable that boosting identified all 10 relevant genes , this 90% false discovery rate means the boosting algorithm is technically selecting all 100 prescreened genes .",
    "this seems to suggest that boosting is a weak model .",
    "however figure 7 shows that the variable importance scores of the top relevant genes are much higher than the majority of the other genes , with the exception of exactly one false discovery in the 11 most important genes .",
    "therefore , while boosting is ranking all genes as important , it is ranking the top ten genes ( the ten relevant genes that determine the disease state ) as significantly more important than essentially all of the other genes .",
    "this chart can thus be used to allow a biologist to cut down the number of genes without resulting in the emergence of false non - discoveries .",
    "this is precisely how we cut down the size of the prescreened pool of genes in boosting-4 and boosting-5 of table 16 .",
    "the genes implicated in the disease are shown in dark blue and those not implicated in the disease are shown in light green . in spite of the high fdiscovery rate of 90% , the eleven genes with the highest score contain all ten genes that cause the disease . ]      in the literature , the gene - gene interactions have been modeled using taylor polynomials ( see , e.g. , park and hastie , 2008 ; assimes et al . , 2008 ; cordell , 2009 ) .",
    "our results showed that when gene interactions are described by taylor polynomials , gradient boosting is a reliable method when coupled with prescreening and a sufficiently large sample size . in this section",
    ", we explore a number of non - taylor interactions with _ multiple thresholds _ and determine if the statistical methods have the same level of success .",
    "the diseases explored are :    @xmath58    @xmath59    @xmath60    for disease101 , the thresholds were chosen so that x1 and x2 are more important than x3 .",
    "for disease102 , the three genes ( x1 , x2 , x3 ) have equal weights . for disease103 , x1x2 and x3 have equal weights .",
    "table 17 below shows the results of fdr , gradient boosting , and pls models for these three diseases .    llll    ' '' ''    height 1pt @a xhline    & * disease101 * & * disease102 * & * disease103 * + & x1",
    "@xmath61 x2 @xmath62 x3 & x1 , x2 , x3 have & x1*x2 and x3 have + & & equal weights & equal weights +    ' '' ''    height 1pt @a xhline    fdr & , x3 & x1 , x3 & x2 , x3 + ( 100 genes ) & & & +    ' '' ''    height 1pt @a xhline    boosting-1 & , @xmath44 & , @xmath44 & , @xmath44 + ( 100 genes ) & fdiscovery = 78% & fdiscovery = 60% & fdiscovery = 96% + & accuracy = 87% & accuracy = 93% & accuracy = 100% +    boosting-2 & , @xmath44 & , @xmath44 & , @xmath44 + ( 20 genes ) & fdiscovery = 90% & fdiscovery = 70% & fdiscovery = 85% + & accuracy = 87% & accuracy = 87% & accuracy = 93% +    boosting-3 & , @xmath44 & , @xmath44 & , @xmath44 + ( 10 genes ) & fdiscovery = 80% & fdiscovery = 70% & fdiscovery = 70% + & accuracy = 87% & accuracy = 93% & accuracy = 93% +    boosting-4 & , x3 , @xmath44 & , @xmath44 & , @xmath44 + ( 5 genes ) & fdiscovery = 40% & fdiscovery = 40% & fdiscovery = 40% + & accuracy = 87% & accuracy = 93% & accuracy = 93% +    boosting-5 & , x3 & & + ( 3 genes ) & fdiscovery = 0% & fdiscovery = 0% & fdiscovery = 0% + & accuracy = 87% & accuracy = 93% & accuracy = 93% +    ' '' ''    height 1pt @a xhline    pls & , x3 & x1 , x2 & x2 , x3 + ( 100 genes ) & fdiscovery = 0% & & + & accuracy = 90% & & +    ' '' ''    height 1pt @a xhline    from table 17 , we can see that for non - taylor gene interactions , pls and fdr do not produce reliable results at a sample size of @xmath51 .",
    "however , gradient boosting continues to prove to be a better tool , as it can well identify the relevant genes and classify the data at a sample size of 102 when the gene interactions are non - taylor .",
    "in table 1 ( colon cancer data ) , we presented certain classification results from the svm community .",
    "collectively , the results in table 1 indicate that there is room for improvement . in this section",
    ", we will discuss our evaluation of the svm technology .    note that given a set of data that is completely separated by a specific threshold such as disease1 through disease11 in our simulations , _ theoretically _ it is possible to construct two convex hulls to separate the data ( figure 8) ,        where a convex hull is defined as    @xmath63    given the 2 convex hulls , one can define _ support vectors _ to find optimal separation of the data as shown in figure 9 .        for nonlinear problems such as disease10 and disease 11 ,",
    "the goal of a support vector machine is to transform the data from the low - dimensional space to a high - dimensional space and then use a hyper - plane to separate the data ( see , e.g. , hastie , tibshirani , friedman , 2011 ) .",
    "when there are 6,033 genes , the prediction space is not really  low dimensional , \" but a forward - selection process starting with a single gene is feasible .",
    "furthermore , if the biological interactions of certain genes are roughly known , then that piece of knowledge may guide the statistician to pick a kernel for optimal separation .    in our simulation study",
    ", disease10 uses 5 important genes to create the target disease ; consequently we would expect the svm technology to achieve 100% prediction accuracy when all 5 important genes are fed into the model .",
    "table 18 below shows that this is not the case for the four different kernels we tried .",
    "lcccc    ' '' ''    height 1pt @a xhline     +   +    ' '' ''    height 1pt @a xhline    * kernel * & @xmath48 & @xmath49 & @xmath54 & @xmath55 +    ' '' ''    height 1pt @a xhline    linear & 89% & 94% & 93% & 90% + polynomial & 85% & 92% & 96% & 90% + rbf & 89% & 95% & 97% & 94% + sigmoid & 89% & 90% &",
    "93% & 89% +    ' '' ''    height 1pt @a xhline    table 19 shows the svm prediction accuracy for disease11 with 10 important genes .",
    "the results are equally disheartening .    lcccc    ' '' ''    height 1pt @a xhline     +   +    ' '' ''    height 1pt @a xhline",
    "* kernel * & @xmath48 & @xmath49 & @xmath54 & @xmath55 +    ' '' ''    height 1pt @a xhline    linear & 77% & 90% & 88% & 82% + polynomial & 81% & 86% & 86% & 84% + rbf & 73% & 94% & 90% & 86% + sigmoid & 73% & 92% & 87% & 83% +    ' '' ''    height 1pt @a xhline    table 19 also shows that the increase of sample size from @xmath50 to @xmath64 or 408 only confounds the model and the result is a decrease of classification accuracy .    in the svm literature",
    ", there exists a great variety of kernel functions that can be used to transform nonlinear to linear data",
    ". a list of 25 kernels can be found at ( http://crsouza.blogspot.com/2010/03/kernel-functions-for-machine-learning.html ) .",
    "the kernels include laplacian kernel , anova kernel , spline kernel , bessel kernel , cauchy kernel , chi - square kernel , histogram intersection kernel , generalized @xmath7-student kernel , bayesian kernel , wavelet kernel , etc . with modifications and hybridizations ,",
    "one may be able to generate hundreds or thousands of other kernels as partially shown in table 1 of this paper",
    ".    it would be interesting to know whether any of new kernels could actually separate the data in disease1 through disease11 , disease101 through disease103 , and possibly in other cases where gene interactions are nonlinear or non - taylor .",
    "we do not see an easy way to meet this challenge .",
    "but the effort should be worthwhile and may generate new insights into this important field of statistical learning .",
    "the same may be true for thousands of new techniques in the field of variable selection .",
    "in modern statistics , the literature on variable selection is vast , complicated , and chaotic .",
    "a systematic approach to evaluate these new tools can be a huge challenge , but it may be able to provide scientists certain guidance on how to select tools in the important task of variable selection .",
    "in a 2008 lecture , stanley young of niss ( the national institute of statistical sciences ) maintained that    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ empirical evidence is that 80 - 90% of the claims made by epidemiologists are false ; these claims do not replicate when retested under rigorous conditions .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    young s conclusion was based on the following : findings from top medical research journals , a survey of journal editors from diverse fields of science , and a finding that of 20 claims coming from observational studies , only one replicated when tested in nih funded randomized clinical trials .    in a follow - up article , young and karr ( 2011 ) further examined 52 claims from _ journal of the american medical association , the new england journal of medicine , journal of the national cancer institute _ , and _ archives of internal medicine_. the conclusion is that  any claim coming from an observational study is most likely to be wrong . \"",
    "the motto , of course , has exceptions even in observational studies .",
    "for example , in astronomy we can not manipulate any of the relevant quantities , yet predictions in astronomy are often more accurate than those produced by double - blind randomized controlled experiments .    in the field of gene identification ,",
    "data from microarray experiments and from similar settings are in the category of observational studies , although they are called  experiments \" in the broad scientific community .",
    "the use of statistical analysis to determine which gene ( or set of genes ) is the cause of a specific disease is often confounded by the following factors :    1 .",
    "we can not flip a coin and then assign a patient to have cancer or no cancer . consequently the @xmath7-tests , @xmath9-values , bonferrani adjustment , and benjamini - hochberg fdr all lose their footing in the statistical analysis of cause - and - effect .",
    "another problem with @xmath9-values is that  statistical significance \" is not the same as  practical significance \" and the @xmath9-values can be very misleading in the gene selection process .",
    "furthermore , our work has shown that the @xmath7-test is not efficient in the detection of non - taylor interactions .",
    "the situation is worse with the regression model or other machine - learning tools such as neural networks and support vector machines , even if the investigators use various randomization techniques on the laboratory animals .",
    "freedman ( 2008 , _ statistical science _ ) pointed out that  randomization does not justify logistic regression . \" in addition , he questioned the scientific ground of using logit versus probit or other types of the link functions in binary regression . furthermore , in binary regression , often there are _ multiple _ variables on the right - hand side of the equation , and we simply can not expect biologists to randomize or manipulate the variables in the experiment .",
    "the same can be said for other models such as partial least squares , support vector classifier , and gradient boosting .",
    "another problem with the statistical models and the adjusted @xmath9-values is that often a single gene is responsible for the onset of a specific disease .",
    "but when that gene is altered , it may change the expression level of dozens of other downstream genes .",
    "imagine a gene that is solely responsible for causing a specific disease ; in its active state , the gene releases proteins and then the expression of 10 or 20 other genes is affected in the process .",
    "now how do we expect the shuffling of statistical methods to identify the primary gene and not pick up all the secondary genes instead ?",
    "fortunately , in the area of gene identification , new techniques have been developed that would confirm to the motto of rubin and holland .",
    "as one example , biologists can create knockout organisms in which the function of a particular gene is shut down . using these knockouts , a biologist is able to infer the impact of the shut - down gene by studying how organisms with the gene differ from organisms without the gene .",
    "for instance , fong and colleagues developed knockout mice for the fragile histidine triad ( fhit ) gene .",
    "they discovered that mice without this gene are more susceptible to carcinogen - induced tumor formation than those mice that express the gene ( fong et al . , 2000 ) .",
    "while experiments like this are incredibly powerful , this approach is not a feasible way to identify unknown / undetermined genes that cause a particular disease - there are simply too many genes in the genome for a biologist to knock them all out and observe their effect .",
    "statistical analysis of microarray data reduces this large pool of genes to a reasonably - sized pool that biologists could then examine using more rigorous experimental approaches .",
    "therefore , statistical analysis of microarray data can be viewed as an important first step in identifying genes that potentially cause a particular disease .    in this study",
    ", we found that the technique of stochastic gradient boosting ( freeman , 2001 ) was able to identify all important genes from a pool of 6,033 with the sample size of @xmath51 .",
    "we did this with simulation datasets that involve 5-gene interactions , 10-gene interactions and non - taylor 3-gene interactions .",
    "the simulations have been crafted to match the real situation as closely as possible ( section 3 ) - the equations are deterministic but they also mimic the colon cancer data and the prostate cancer data ( efron , 2010 , 2008 ) : random elements , correlations among the genes , etc . in all cases ,",
    "the gradient boosting was able to pick the contributing genes .    in comparison , the following techniques missed important genes in various scenarios : bonferrani adjustment , benjamini - hochberg fdr , logistic regression , partial least squares , lasso ( least angle regression ) , neural network , decision tree , and support vector machine .",
    "this is problematic and points to the crucial distinction between _ false discovery _ and _ false non - discovery _ in the statistical gene search , where false discovery leads to the selection of irrelevant genes and false non - discovery misses out important genes that can not be recovered in the subsequent analysis . from the biological view point , false non - discovery is _ not acceptable _ for the very reason that if an important gene is lost in the statistical exploration , then it will mislead subsequent research efforts .",
    "in addition , our investigation shows that commonly used measures in binary classifications can be very misleading in gene identification : error rate , false positive , false negative , and other measures that are derived from these values ( sensitivity , specificity , roc curves , the area under the roc curve , @xmath10-measures , precision , recall , etc . ) .",
    "the most troubling is that some commonly used models would produce 100% accuracy measures and select different sets of genes .",
    "they simply can not stand the scrutiny of parameter estimates and model stability .",
    "currently there are thousands of tools for variable selection , with new ones showing up at an exponential rate .",
    "the growth of this field will provide us new techniques to tackle many hard problems with high - dimensional data .",
    "nevertheless , the growth also creates a problem for scientists who are facing thousands of variables and thousands of tools to select the relevant variables . in most cases ,",
    "nobody knows which variable is causing what and existing subject knowledge often conflicts with each other . in many cases , the search process is like trying to find a black cat in a dark house .    in our investigation , we compared the results from real - world data and from simulation studies .",
    "the use of simulation is a standard practice in statistics , even college students know that ulam and von newmann did it in the manhattan project some 70 years ago .",
    "but in the fields of gene search and variable selection , the literature is very shy on this technology .",
    "here you play god , create the genes of your liking , investigate the sample size needed , compare the tools , and finally select the top variables for the specific phenomenon under the study . in our case , we found that certain widely used models ( neural network , pls , logistic regression , and lasso ) would render 100% prediction accuracy with genes that are _ not responsible _ for our simulated diseases . on the other hand , with moderate sample size",
    ", gradient boosting will be shown to be a superior model for gene selection , though we suspect there are more tools that are appropriate for gene search .",
    "we believe a platform would be beneficial in helping to select the top tools before we try to select the top variables .",
    "alon , u. , barkai , n. , notterman , d. a. , gish , k. , ybarra , s. , mack , d. , and levine , a. j. ( 1999 ) .",
    "broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays .",
    "sci . _ * 96 * 6745-6750 .",
    "assimes t. l. , knowles , j. w. , basu , a. , iribarren , c. , southwick , a. , et al .",
    "susceptibility locus for clinical and subclinical coronary artery disease at chromosome 9p21 in the multi - ethnic advance study",
    ". _ hum mol genet . _ * 17 * 23202328 .",
    "bar , h. , booth , j. , schifano , e. , and wells , m. t. ( 2010 ) .",
    "laplace approximated em microarray , analysis : an empirical bayes approach for comparative microarray experiments .",
    "_ statistical science_. * 25 * 388-407 .",
    "black , m. a. and doerge , r. w. ( 2002 ) .",
    "calculation of the minimum number of replicate spots required for detection of significant gene expression fold change in microarray experiments .",
    "_ bioinformatics_. * 18 * 1609-1616 .",
    "fong , l. y. y. , fidanza , v. , zanesi , n. , lock , l. f. , siracusa , l. d. , mancini , r. , siprashvili , z. , ottey , m. , martin , s. e. , druck , t. , mccue , p. a. , croce , c. m. , and huebner , k. ( 2000 ) .",
    "torre - like syndrome in fhit - deficient mice . _ proc .",
    "acad . sci_. * 97 * 4742-4747 .",
    "hu , q. , pan , w. , an , s. , ma , p. , and wei , j. ( 2010 ) .",
    "an efficient gene selection technique for cancer recognition based on neighborhood mutual information .",
    "j. machine learning & cyber_. * 1 * 6374 .",
    "jeanmougin m , de reynies a , marisa l , paccard c , nuel g , et al . ( 2010 ) .",
    "should we abandon the t - test in the analysis of gene expression microarray data : a comparison of variance modeling strategies .",
    "_ plos one_. * 5 * e12336 .",
    "lettre , g , palmer , c. d. , young , t. , ejebe , k. g. , allayee , h , et al .",
    "genome - wide association study of coronary heart disease and its risk factors in 8,090 african americans : the nhlbi care project .",
    "_ plos genet_. * 7 * e1001300 .          magidson , j. ( 2010 ) .",
    "correlated component regression : a prediction / classification methodology for possibly many features ,  _ proceedings of the 2010 joint statistical meeting_. ( http://statisticalinnovations.com/technicalsupport/ccr.amstat.pdf ) .",
    "mongan , m.a . ,",
    "dunn ii , r.t .",
    "et al . ( 2010 ) .",
    "a novel statistical algorithm for gene expression analysis helps differentiate pregnane x receptor - dependent and independent mechanisms of toxicity .",
    "_ plos one_. * 5 * e15595 .              ritchie , m. d. , hahn , l. w. , roodi , n. , bailey , r. , dupont , w. d. , parl , f. f. and moore , j. h. ( 2001 ) .",
    "multifactor - dimensionality reduction reveals high - order interactions among estrogen - metabolism genes in sporadic breast cancer . _",
    "j. human genet_. * 69 * 138147 .",
    "vinzi , v. e. , chin , w. w. , henseler , j. , wang , h. ( 2010 ) , eds . ,",
    "handbook of partial least squares , springer .",
    "wang , l. , liu , x. , liang , h. and carroll , r. j. ( 2011 ) .",
    "estimation and variable selection for generalized additive partial linear models .",
    "_ the annals of statistics_. * 39 * 1827-1851 ."
  ],
  "abstract_text": [
    "<S> over the past decades , statisticians and machine - learning researchers have developed literally thousands of new tools for the reduction of high - dimensional data in order to identify the variables most responsible for a particular trait . </S>",
    "<S> these tools have applications in a plethora of settings , including data analysis in the fields of business , education , forensics , and biology ( such as microarray , proteomics , brain imaging ) , to name a few .    </S>",
    "<S> in the present work , we focus our investigation on the limitations and potential misuses of certain tools in the analysis of the benchmark colon cancer data ( 2,000 variables ; alon et al . , 1999 ) and the prostate cancer data ( 6,033 variables ; efron , 2010 , 2008 ) . </S>",
    "<S> our analysis demonstrates that models that produce 100% accuracy measures often select different sets of genes and can not stand the scrutiny of parameter estimates and model stability .    </S>",
    "<S> furthermore , we created a host of simulation datasets and  artificial diseases \" to evaluate the reliability of commonly used statistical and data mining tools . </S>",
    "<S> we found that certain widely used models can classify the data with 100% accuracy without using any of the variables responsible for the disease . with moderate sample size and suitable pre - screening </S>",
    "<S> , stochastic gradient boosting will be shown to be a superior model for gene selection and variable screening from high - dimensional datasets . </S>"
  ]
}