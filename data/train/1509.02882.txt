{
  "article_text": [
    "consider the differential equation parameterized by @xmath6 and governing @xmath7 : @xmath8 the differential equation is assumed to be uniformly hyperbolic ( details in section [ sec : uniformhyperbolicity ] ) .",
    "we are also given a @xmath9 cost function @xmath10 and assume that the system is _ ergodic _",
    ", i.e. , the infinite time average : @xmath11 depends on @xmath2 but does not depend on the initial condition @xmath12 .",
    "the differentiability of @xmath13 with respect to @xmath2 has been proven by ruelle @xcite .",
    "obtaining an estimation of @xmath4 is crucial in many computational and engineering problems . indeed , many applications involve simulations of nonlinear dynamical systems that exhibit a chaotic behavior .",
    "for instance , chaos can be encountered in the following fields : climate and weather prediction @xcite , turbulent combustion simulation @xcite , nuclear reactor physics @xcite , plasma dynamics in fusion @xcite and multi - body problems @xcite .",
    "the quantities of interest are often time averages or expected values of some cost function @xmath14 . estimating the derivative of @xmath3 is particularly valuable in : +    * * numerical optimization*. the derivative of @xmath3 with respect to a design parameter @xmath2 is used by gradient - based algorithms in order to efficiently optimize the design parameters in high dimensional design spaces ( see @xcite ) . * * uncertainty quantification*. the derivative of @xmath3 with respect to a parameter @xmath2 gives a useful information for assessing the error and uncertainty in the computed @xmath3 ( see @xcite ) .",
    "for example , we could obtain a useful information about the impact of mankind on the climate by computing the derivative of the long time averaged global mean temperature to the amount of anthropogenic emissions ( @xcite shows how sensitivity analysis is used in climate studies ) . in the simulation of a turbulent airflow over an aircraft , estimating",
    "the derivative of the long time averaged drag to a shape design parameter is of extreme importance for engineers allowing them to improve their design @xcite .",
    "it has been shown that in many of these practical examples , the quantities of interest exhibit ergodic properties , popularly known as _ chaotic hypothesis _",
    "@xcite , @xcite .",
    "+ when it comes to computing @xmath4 , conventional methods based on linearizing the initial value problem ( [ difeq ] ) become ill - conditioned when the system is chaotic .",
    "they compute derivatives that are orders of magnitude too large and the error grows exponentially larger as the simulation runs longer @xcite,@xcite .",
    "this failure is due to the so - called _ butterfly effect _ and the explanation has been published by lea et al .",
    "+ some algorithms have been developed to overcome this failure .",
    "lea et al .",
    "proposed the ensemble adjoint method which applies the adjoint method to many random trajectories , then averages the computed derivatives @xcite , @xcite .",
    "however , the algorithm is computationally expensive even for small dynamical system such as lorenz s one .",
    "based on the fluctuation dissipation theorem , abramov and majda provided an algorithm that successfully computes the desired derivative @xcite .",
    "nonetheless , this algorithm assumes the dynamical system to have an equilibrium distribution similar to the gaussian distribution , an assumption often violated in very dissipative systems .",
    "recent work by cooper and haynes has alliviated this limitation by introducing a nonparametric method to estimate the equilibrium distribution @xcite .",
    "more methods have been developed to compute @xmath4 , in particular the _ least squares shadowing ( lss ) _ algorithm which computes it by solving a constrained least squares problem @xcite .",
    "the big advantage of this method is its simplicity since the least squares problem can easily be formulated and efficiently solved as a linear system .",
    "compared to the previously presented methods , lss is less sensitive to the dimension of the dynamical system and does nt require any explicit knowledge about its steady - state distribution in phase space .",
    "+ this paper provides a theoretical foundation for lss by proving that it gives a useful estimation of @xmath4 when the dynamical system is a uniformly hyperbolic flow . compared to the discrete case ( uniformly hyperbolic map ) for which we already have a proof of convergence @xcite ,",
    "the continuous case is more difficult to deal with due to the apparition of the _ neutral subspace _ ( details in section [ sec : uniformhyperbolicity ] ) .",
    "however , it is very important to treat the continuous case since most applications and real - life problems require a continuous description of the physics and involve differential equations .",
    "+ in the next section , the mathematical formulation of convergence is introduced as well as theorem lss which will be proved in the following sections .",
    "section [ sec : uniformhyperbolicity ] presents the concept of uniform hyperbolicity for the readers who are not familiar with the subject .",
    "section [ sec4 ] points out the new behaviour and properties that come with continuous dynamical systems ( in opposition to discrete maps ) .",
    "section [ sec5 ] defines the shadowing direction and proves its existence as well as uniqueness .",
    "section [ sec6 ] shows that the derivative of @xmath3 can be computed using the shadowing direction and bounds the upper error .",
    "section [ sec7 ] then demonstrates that the least squares problem gives a good approximation of the shadowing direction .",
    "finally , section [ sec8 ] uses all the previous results and concludes the proof of theorem lss by showing that the estimation error vanishes as the least squares problem increases in size .",
    "in order to obtain an algorithm of practical relevance , a discrete version of the above problem should be formulated .",
    "first , we replace the differential equation ( [ difeq ] ) parametrized by @xmath2 by a family of operators : let @xmath15 be the family of maps parametrized by @xmath6 such that it is a `` discretization '' of the differential equation using a uniform time stepsize of @xmath16 . in other words ,",
    "if @xmath17 is a trajectory satisfying the initial differential equation ( [ difeq ] ) for a particular @xmath18 , we have : @xmath19 in this case , @xmath20 corresponds to a perfect numerical integration scheme with a stepsize of @xmath16 .",
    "we ask for the differential equation to be `` smooth '' enough so that the maps @xmath21 are @xmath22 .",
    "+ then , assuming that all trajectories @xmath23 belong to a compact set @xmath24 and that @xmath2 also lies in a compact set @xmath25 , we can approximate @xmath26 with a riemann sum and have the following bound on the integration error :    @xmath27}h j(\\varphi_s(u(0),ih),s)\\big)\\big| \\leq h \\sup_{s\\in s}\\big(\\|dj(\\cdot , s)\\|^\\infty_\\lambda\\big)\\sup_{s \\in s}\\big(\\|f(\\cdot , s)\\|^\\infty_\\lambda\\big)\\ ] ]    where @xmath28 and @xmath29 , @xmath30 are the infinite norms of @xmath31 and the derivative of @xmath32 with respect to the first variable on the compact set @xmath24 respectively . since the bound does nt depend on @xmath33 , we finally have :    @xmath34}h j(\\varphi_s(u(0),ih),s)\\big| = o(h)\\ ] ]    to simplify the expressions , we introduce the new notation @xmath35 or even @xmath36 depending on which parameter is fixed and when there is no ambiguity . + for a sequence @xmath37\\}$ ] satisfying @xmath38 , the _",
    "least squares shadowing _ method attempts to compute the derivative @xmath39 via    under ergodicity and hyperbolicity assumptions , @xmath40}\\big[(dj(u_i , s))v_i^{\\{h , t\\}}+\\partial_sj(u_i , s)+\\big(\\eta_i^{\\{h , t\\}}(j(u_i , s)-\\langle j\\rangle(s))\\big)\\big]\\\\ & = \\lim_{t \\to \\infty}\\lim_{h \\to 0 } \\frac{h}{t}\\sum_{i=1}^{[\\frac{t}{h}]}\\big[(dj(u_i , s))v_i^{\\{h , t\\}}+\\partial_sj(u_i , s)+\\big(\\eta_i^{\\{h , t\\}}(j(u_i , s)-\\langle j\\rangle(s))\\big)\\big]\\end{aligned}\\ ] ] where @xmath41 , @xmath42 $ ] is the solution to the constrained least squares problem : @xmath43 } ( \\|v_i^{\\{h , t\\}}\\|^2+\\alpha ( \\eta_i^{\\{h , t\\}})^2 ) \\\\ & \\textrm{s.t . }",
    "\\quad v_{i+1}^{\\{h , t\\}}=(d\\varphi_s(u_i , h))v_i^{\\{h , t\\}}+\\partial_s\\varphi_s(u_i , h)+h\\eta_i^{\\{h , t\\}}\\partial_h\\varphi_s(u_i , h ) , \\end{split}\\ ] ] where @xmath44 is any positive constant and @xmath45 is the euclidean norm in @xmath46 . +    here the linearized operators are defined as :    @xmath47    @xmath48,@xmath49,@xmath50,@xmath51 and @xmath52 are a @xmath53 vector , a scalar , an @xmath54 matrix , an @xmath55 vector and an @xmath55 vector , respectively , representing the partial derivatives . +",
    "let us now proceed to the presentation of the uniform hyperbolicity property : we say that the dynamical system ( [ difeq ] ) has a compact , global , uniformly hyperbolic attractor @xmath56 at @xmath2 if for all @xmath57 the map @xmath58 satisfies : +    1 .   for all @xmath59 , @xmath60 where @xmath61 is the euclidean distance in @xmath46 .",
    "2 .   there is a @xmath62 and @xmath63 , such that for all @xmath64 , there is a splitting of @xmath46 representing the space of perturbations around @xmath65 : @xmath66 where the subspaces are : * @xmath67 is the _ unstable subspace _ at @xmath65 , * @xmath68 is the _ stable subspace _ at @xmath65 .",
    "* @xmath69 is the _",
    "neutral subspace _ at @xmath65 .",
    "+ @xmath70,@xmath71 and @xmath72 are all continuous with respect to @xmath65 . +    if @xmath73 with @xmath74 , @xmath75 , @xmath76 and @xmath77 , the continuity of the three subspaces and the compacity of @xmath24 implies that : @xmath78 this is because if @xmath79 , then by the continuity of @xmath71 , @xmath70 , @xmath72 and the compactness of @xmath80 , there must be a @xmath81 such that @xmath82 and @xmath83 which contradicts assumption ( [ space_decomp ] ) .",
    "thus :    @xmath84    the _ stable _ , _ unstable _ and _ neutral subspaces _ are also _ invariant _ under the differential of the map @xmath20 , i.e. , if @xmath38 and @xmath85 , then    @xmath86    because of their relative simplicity , studies of uniformly hyperbolic dynamical systems ( also known as `` ideal chaos '' ) have provided a lot of insight into the properties of chaotic dynamical systems @xcite .",
    "although most real - life dynamical systems are not uniformly hyperbolic , they can be classified as _ quasi - hyperbolic _ : results obtained on hyperbolic systems can often be generalized to them @xcite .",
    "this proof covers the convergence os lss for uniform hyperbolic flows , nevertheless , numerical results have shown that the algorithm also works for non - ideal chaos @xcite . +",
    "one should bear in mind that the dynamical system is continuous which means that a solution @xmath87 to equation ( [ difeq ] ) forms a continuous trajectory in phase space .",
    "thus , the sequence @xmath88 is no more than a sequence of sample points on the continuous trajectory , the time stepsize between two consecutive points being @xmath16 .",
    "the _ neutral subspace _ @xmath89 which is unidimensional , is constituted by the line tangent to the continuous trajectory at the sampling point @xmath90 .",
    "+ consequently , a perturbation in the direction of the _ neutral subspace _ around the point @xmath90 can be interpreted as a time shift .",
    "this means that for @xmath91 and @xmath92 infinitesimal , if @xmath93 : @xmath94 and @xmath95 which implies : @xmath96 i.e. @xmath97 since @xmath98 for all @xmath91 , then ( [ neu ] ) implies that a small perturbation in the direction of the _ neutral subspace _ remains bounded under the action of forward or backward iterations . on the contrary , a small perturbation in the _ stable _ or _ unstable subspace _",
    "gets amplified exponentially under the action of backward or forward iterations respectively .",
    "+ the second point to be discussed in this section is the fact that the discretization of a continuous trajectory does nt have to respect a uniform stepsizing .",
    "a better way to discretize a trajectory would be the following one : @xmath99 is a sampling of a continuous trajectory if @xmath100 where @xmath101 . in order to have : @xmath102}\\frac{\\tau_i j(u_i , s)}{\\sum_j\\tau_j^{s}}\\big| = o(h)\\ ] ] we only need @xmath103 as @xmath104 which actually happens if , for instance , the time dilatation factors @xmath105 are bounded ( @xmath106 means @xmath107}$ ] in the above expression ) . from now on , a discretization of a trajectory is a sequence of couples @xmath108 .",
    "in this section , we will prove a variant of the shadowing lemma for the purpose of defining the shadowing direction and prove its existence and uniqueness .",
    "+ the hyperbolic structure ensures the structural stability of the attractor @xmath24 under perturbation in @xmath2 @xcite , @xcite . without loss of generality , we will assume that @xmath109 and choose a sequence @xmath110 such that : @xmath111 in this case , the superscript in @xmath112 is the value of the parameter @xmath2 . @xmath16 and @xmath113 are fixed so they do not appear in the notation .",
    "+    [ theorem1 ] if the system is uniformly hyperbolic and @xmath114 continuously differentiable with respect to @xmath2 and @xmath65 , then for all sequence @xmath115 satisfying @xmath116 , there is a @xmath117 such that for all @xmath118 there is a sequence @xmath119 satisfying @xmath120 , @xmath121 and @xmath122 for all @xmath123 . furthermore , @xmath124 and @xmath125 are @xmath126-uniformly continuously differentiable to @xmath2 .",
    "+    the @xmath126-uniform continuous differentiability of @xmath124 means that for all @xmath127 and @xmath128 there is a @xmath129 such that if @xmath130 then @xmath131 and @xmath132 for all @xmath126 .",
    "+ to prepare for the proof , let @xmath133 be the space of bounded sequences in @xmath46 and @xmath134 the hyperplane of @xmath46 defined by @xmath135 .",
    "we introduce @xmath136 as the space of bounded sequences @xmath137 such that @xmath138 for all @xmath139 ( @xmath140 has no components in the _ neutral subspace _ ) . in other words : @xmath141 finally , by considering the space @xmath142 of bounded sequences of @xmath143 , we denote @xmath144 the product of @xmath136 by @xmath142 : @xmath145 we then introduce the notation @xmath146 where @xmath147 , @xmath148 and define the norm : @xmath149 as defined above , the space @xmath144 is a banach space .",
    "we can now define the map @xmath150 : @xmath151 as : @xmath152 for a given @xmath2 , @xmath153 if and only if @xmath154 samples , with timesteps @xmath155 , a continuous trajectory satisfying ( [ difeq ] ) . we use the implicit function theorem to complete the proof , which requires f to be differentiable with respect to @xmath156 and its derivative to be non - singular at @xmath157 , @xmath158 and @xmath109 .",
    "+    under the conditions of theorem [ theorem1 ] , f has frchet derivative at all @xmath159 and @xmath118 : @xmath160 where @xmath161    we have : @xmath162 in the @xmath144 norm thanks to the uniform continuity of @xmath163 and @xmath164 on the compact set @xmath24 .",
    "now , we only need to prove that the linear map we obtained is bounded .",
    "since @xmath163 and @xmath164 are continuous they are uniformly bounded in the compact set @xmath24 .",
    "thus , the norm of the linear map is less than @xmath165 .",
    "+    under conditions of theorem [ theorem1 ] , the frchet derivative of @xmath150 at @xmath166 and @xmath109 is a bijection .",
    "the frchet derivative of @xmath150 at @xmath166 and @xmath109 in the direction @xmath167 is : @xmath168 to prove its bijectivity , we only need to show that for any @xmath169 there is a unique @xmath170 such that @xmath171 + in this case , we can find an analytical expression for the pre - image of @xmath172 .",
    "let @xmath167 be defined as : @xmath173 we can verify that @xmath174 for all @xmath126 .",
    "and @xmath175 + we still have to ensure that @xmath167 belongs to @xmath144 .",
    "based on ( [ invariance ] ) , we notice that the @xmath176 we ve just defined belongs to @xmath135 .",
    "then , thanks to ( [ space_decomp ] ) , we can write @xmath177 where @xmath178 , @xmath179 and @xmath180 . since @xmath71 , @xmath70 and @xmath72 are continuous with respect to @xmath65 and @xmath24 is compact : @xmath181 where @xmath182 . + consequently , for all @xmath126 : @xmath183 because @xmath184 and @xmath185 are invariant under @xmath186 ( property ( [ invariance ] ) ) .",
    "thus , @xmath176 is uniformly bounded and @xmath187 . in the same way",
    ", we show that for all @xmath126 : @xmath188 where @xmath189 . consequently , @xmath190 is uniformly bounded which leads to @xmath191 and @xmath161 .",
    "+ because of linearity , uniqueness of @xmath192 such that @xmath171 only needs to be proved for @xmath193 . since @xmath194",
    ", @xmath195 is equivalent to @xmath196 .",
    "thanks to property ( [ invariance ] ) , by splitting @xmath197 and knowing that @xmath198 , we have : @xmath199 where the two parentheses are in @xmath200 and @xmath201 respectively . again knowing that @xmath194",
    ", both parentheses should be equal to zero .",
    "this is true for all @xmath126 , so we obtain : @xmath202 for all @xmath203 .",
    "based on the properties of uniform hyperbolicity , @xmath204 and @xmath205 .",
    "if for some @xmath206 we have @xmath207 , then : @xmath208 which means that @xmath209 is unbounded ( @xmath210 ) .",
    "similarly , if @xmath211 for some @xmath126 then : @xmath212 and @xmath213 is also unbounded .",
    "consequently , for @xmath214 to be bounded we must have @xmath215 for all @xmath126 .",
    "+ on the other hand , showing that @xmath216 is trivial : @xmath217 since @xmath218 then @xmath219 .",
    "this is true for all @xmath126 , which means that @xmath220 .",
    "this proves the uniqueness of @xmath167 for @xmath221 .",
    "+    ( _ of theorem [ theorem1 ] _ ) since @xmath222 , @xmath223 is a zero point of @xmath150 at @xmath109 .",
    "based on this information and on the two previous lemmas , the implicit function theorem states that there exist @xmath117 such that for all @xmath118 there is a unique @xmath224 satisfying @xmath225 and @xmath226 .",
    "furthermore , this @xmath224 is continuously differentiable to @xmath2 , i.e. , @xmath227 is continuous with respect to @xmath2 in the @xmath144 norm . by the definition of derivatives ( in @xmath228 , @xmath229 .",
    "continuity of @xmath230 in @xmath144 then implies that @xmath231 and @xmath232 are @xmath126-uniformly countinuous with respect to @xmath2 . by defining",
    ": @xmath233 we finally obtain the results of theorem ( [ theorem1 ] ) .",
    "+ if we return to the expanded notation of @xmath124 , this theorem states that for a discretization @xmath234 of a continuous trajectory satisfying ( [ difeq ] ) for @xmath109 , there is a series @xmath235 also being a discretization of a continuous trajectory at nearby values of @xmath2 .",
    "in addition , @xmath235 _ shadows _ @xmath234 meaning that @xmath235 is close to @xmath234 when @xmath2 is close to 0 . also , @xmath236 exists and is @xmath126-uniformly bounded .",
    "+ the _ shadowing direction _",
    "@xmath237 is defined as the uniformly bounded series : @xmath238 in addition , we can find 2 constants @xmath239 and @xmath240 independant of @xmath16 such that for all @xmath126 : @xmath241 please refer to appendix [ appendix 1 ] to see how these constants are built .",
    "in this section , we prove an easier version of theorem lss in which we replace the solution @xmath242\\big\\}$ ] to the constrained least squares problem ( [ constraint ] ) by the shadowing direction we found earlier @xmath243\\big\\}$ ] which can be written @xmath244\\big\\}$ ] if @xmath16 is to be displayed explicitly .",
    "+    if uniform hyperbolicity holds and @xmath20 is continuously differentiable for all @xmath16 , then for all constinuously differentiable function @xmath245 whose infinite time average : @xmath246 is independant of the initial state @xmath247 , let @xmath244\\big\\}$ ] be the sequence of shadowing direction , then : @xmath248}\\big[(dj(u_i,0))v_i^{\\{h,\\infty\\}}+(\\partial_sj(u_i,0))+\\big(\\eta_i^{\\{h,\\infty\\}}(j(u_i,0)-\\langle j\\rangle(0))\\big)\\big]\\\\ & = \\lim_{t \\to \\infty}\\lim_{h \\to 0}\\frac{h}{t}\\sum_{i=1}^{[\\frac{t}{h}]}\\big[(dj(u_i,0))v_i^{\\{h,\\infty\\}}+(\\partial_sj(u_i,0))+\\big(\\eta_i^{\\{h,\\infty\\}}(j(u_i,0)-\\langle j\\rangle(0))\\big)\\big]\\end{aligned}\\ ] ]    the proof is essentially an exchange of limits through uniform convergence .",
    "since @xmath249 is independant of @xmath247 , we set @xmath250 as defined in the previous section and we know that @xmath251 . in order to simplify the notations , @xmath252 and @xmath253",
    "will be expressed as @xmath124 and @xmath125 in what follows .",
    "the integral could be discretized into an infinite sommation that involves the time dilatation factors : @xmath254}\\frac{\\tau_i^{s}j(u^{s}_i , s)}{\\sum_j\\tau_j^{s}}\\\\ & = \\lim_{t\\to + \\infty}\\lim_{h \\rightarrow 0}\\sum_{i=1}^{[\\frac{t}{h}]}\\frac{\\tau_i^{s}j(u^{s}_i , s)}{\\sum_j\\tau_j^{s}}\\end{aligned}\\ ] ] the two limits can be permuted at will but we will only keep one notation for the remaining of the proof .",
    "we can write : @xmath255}\\frac{h\\tau_i^sj(u_i^s , s)}{h\\sum_j\\tau_j^s}-\\frac{hj(u_i^0,0)}{t}\\bigg)\\\\ & = \\lim_{s\\rightarrow 0 } \\lim_{h \\rightarrow 0}\\lim_{t \\rightarrow + \\infty}\\bigg ( \\frac{1}{s}\\sum_{i=1}^{[\\frac{t}{h}]}\\frac{h\\tau_i^sj(u_i^s , s)}{h\\sum_j\\tau_j^s}-\\frac{hj(u_i^0,0)}{h\\sum_j\\tau_j^s}+\\frac{hj(u_i^0,0)}{h\\sum_j\\tau_j^s}-\\frac{hj(u_i^0,0)}{t}\\bigg)\\\\ & = \\lim_{s\\rightarrow 0}\\lim_{h \\rightarrow 0}\\lim_{t \\rightarrow + \\infty}\\bigg ( \\frac{1}{s}\\sum_{i=1}^{[\\frac{t}{h}]}\\frac{h(j(u_i^s , s)-j(u_i^0,0)+(\\tau_i^s-1)j(u_i^s , s))}{h\\sum_j\\tau_j^s}\\\\ & -\\frac{(h\\sum_j(\\tau_j^s-1))\\times hj(u_i^0,0)}{t(h\\sum_j\\tau_j^s)}\\bigg)\\\\ & = \\lim_{s\\rightarrow 0}\\lim_{h \\rightarrow 0}\\lim_{t \\rightarrow + \\infty}\\bigg(\\frac{1}{s}\\sum_{i=1}^{[\\frac{t}{h}]}\\frac{h(j(u_i^s , s)-j(u_i^0,0)+(\\tau_i^s-1)j(u_i^s , s))}{t+h\\sum_j(\\tau_j^s-1)}\\\\ & -\\frac{(h\\sum_j(\\tau_j^s-1))\\times hj(u_i^0,0)}{t(t+h\\sum_j(\\tau_j^s-1))}\\bigg)\\\\ & = \\lim_{s\\rightarrow 0}\\lim_{h \\rightarrow 0}\\lim_{t \\rightarrow + \\infty}\\bigg ( \\frac{h}{t}\\sum_{i=1}^{[\\frac{t}{h}]}\\frac{(j(u_i^s , s)-j(u_i^0,0))}{s}+o(s)\\bigg)\\\\ & + \\lim_{s\\rightarrow 0}\\lim_{h \\rightarrow 0}\\lim_{t \\rightarrow + \\infty}\\bigg(\\frac{h(\\tau_i^s-1)}{ts}(j(u_i^s , s)-\\frac{1}{t}\\sum_jhj(u_i^0,0))+o(s)\\bigg)\\end{aligned}\\ ] ]    let us eliminate @xmath256 in the first term .",
    "we define : @xmath257 then , thanks to the mean value theorem , for all @xmath126 there exist an @xmath258 $ ] such that : @xmath259 consequently : @xmath260}\\frac{(j(u_i^s , s)-j(u_i^0,0))}{s}\\bigg)=\\lim_{s\\rightarrow 0}\\lim_{h \\rightarrow 0}\\lim_{t \\rightarrow + \\infty } \\bigg ( \\frac{h}{t}\\sum_{i=1}^{[\\frac{t}{h}]}\\gamma_i^{\\xi_i(s)}\\bigg)\\\\\\ ] ] we can choose a neighborhood of @xmath261 that contains @xmath262 for all @xmath126 ( for @xmath2 sufficiently small ) and in which both @xmath263 and @xmath264 are uniformly continuous . since the @xmath265 are @xmath126-uniformly continuous and bounded , for all @xmath266 there exists @xmath117 such that for all @xmath267 : @xmath268 thus , for all @xmath118 , @xmath269 for all @xmath126 , therefore for all @xmath16 , @xmath33 ( @xmath270 ) : @xmath271}\\gamma_i^{\\xi_i(s)}-\\frac{h}{t}\\sum_{i=1}^{[\\frac{t}{h}]}\\gamma_i^{0}\\bigg\\|\\leq \\frac{h}{t}\\sum_{i=1}^{[\\frac{t}{h}]}\\|\\gamma_i^{\\xi_i(s)}-\\gamma_i^0\\|<\\epsilon\\ ] ] hence , @xmath272}\\gamma_i^{\\xi_i(s)}\\bigg)-\\lim_{h \\rightarrow 0}\\lim_{t \\rightarrow + \\infty } \\bigg ( \\frac{h}{t}\\sum_{i=1}^{[\\frac{t}{h}]}\\gamma_i^{0}\\bigg)\\bigg\\|\\leq\\epsilon\\ ] ] finally , @xmath273}\\gamma_i^{\\xi_i(s)}\\bigg)= \\lim_{h \\rightarrow 0}\\lim_{t \\rightarrow + \\infty}\\bigg ( \\frac{h}{t}\\sum_{i=1}^{[\\frac{t}{h}]}\\gamma_i^{0}\\bigg)\\ ] ] which grants us the desired result for the first term via the definition of @xmath274 .",
    "+ for the second term , @xmath14 is continuously differentiable thus continuous and the @xmath275 are @xmath126-uniformly continuously differentiable and bounded .",
    "based on that , for @xmath2 sufficiently small , we can find a compact neighborhood of @xmath261 that contains @xmath276 for all @xmath277 and in which @xmath278 will be uniformly continuous .",
    "consequently , the sequence @xmath279 which can be writen @xmath280 converges uniformly to @xmath281 when @xmath2 goes to @xmath5 .",
    "because the term @xmath282 does not depend on @xmath2 at all , we finally have : @xmath283 which conludes the proof .",
    "the main task of this section is to provide a bound for : @xmath284 for @xmath285 where the @xmath286 , @xmath287 are the solution to the least squares problem : @xmath288 } ( \\|v_i^{\\{h , t\\}}\\|^2+\\alpha ( \\eta_i^{\\{h , t\\}})^2 ) \\quad\\\\ \\label{least squares 2 } & \\textrm{s.t . }",
    "\\quad v_{i+1}^{\\{h , t\\}}=(d\\varphi_s(u_i , h))v_i^{\\{h , t\\}}+(\\partial_s\\varphi_s(u_i , h))+h\\eta_i^{\\{h , t\\}}\\partial_h\\varphi_s(u_i , h),\\end{aligned}\\ ] ] the shadowing lemma guarantees the existence of a shadowing trajectory , but provides no clear way to compute @xmath289 , @xmath290 .",
    "this section suggests that the solution to the least squares problem gives a useful approximation of the shadowing trajectory allowing us to compute@xmath4 . without loss of generality",
    ", we consider that @xmath109 in ( [ least squares 2 ] ) . by definition ,",
    "the shadowing trajectory satisfies : @xmath291 after taking the derivative to @xmath2 on both sides for @xmath109 , we obtain : @xmath292 thus , the shadowing direction satisfies the constraint in equation ( [ least squares ] ) and :    @xmath293 } ( \\|v_i^{\\{h , t\\}}\\|^2+\\alpha ( \\eta_i^{\\{h , t\\}})^2 ) \\leq \\sum_{i=1}^{[\\frac{t}{h } ] } ( \\|v_i^{\\{h,\\infty\\}}\\|^2+\\alpha ( \\eta_i^{\\{h,\\infty\\}})^2 ) \\leq \\frac{t}{h}(||\\mathbf{v}^{\\{\\infty\\}}||^2_{\\mathrm{b } } + \\alpha||\\boldsymbol\\eta^{\\{\\infty\\}}||^2)\\end{aligned}\\ ] ]    consequently : @xmath294 since @xmath295 : @xmath296 we can get a similar bound for @xmath297 :    @xmath298    concerning @xmath299 and @xmath300 , combining the constraint equation ( [ least squares 2 ] ) as well as ( [ infinity ] ) we obtain : @xmath301    thus , since @xmath302 , @xmath303 and based on the following definition of the _ unstable _ and _ stable subspaces _ : @xmath304    we obtain : @xmath305}^{\\{h , t\\}+}\\|\\\\ \\max_i ||e_i^{\\{h , t\\}-}||\\leq \\max(1,c)\\|e_{1}^{\\{h , t\\}-}\\|\\end{aligned}\\ ] ]    in addition to that , @xmath306 which means that @xmath307 where @xmath308 is the identity operator in @xmath46 . for @xmath77 , @xmath309 and @xmath310 where @xmath311 and @xmath312 are compact sets containing @xmath313 and @xmath109 , we can find a positive constant @xmath314 such that : @xmath315 since @xmath21 is @xmath22 .",
    "@xmath316 belongs to a space of finite dimension so all norms are equivalent and any choice of norm @xmath317 is valid . for a sufficienty small @xmath16 such that @xmath318 , we get : @xmath319 for all @xmath320 . consequently : @xmath321}\\|e_i^{\\{h , t\\}-}\\|^2 \\geq \\sum_{i=1}^{[\\frac{t}{h}]}(1-hk)^{2(i-1 ) } \\|e_1^{\\{h , t\\}-}\\|^2\\\\ \\geq \\|e_1^{\\{h , t\\}-}\\|^2 \\frac{1-(1-hk)^{2[\\frac{t}{h}]}}{h(2k - hk^2)}\\end{aligned}\\ ] ] which means that : @xmath322}}\\sum_{i=1}^{[\\frac{t}{h}]}\\|e_i^{\\{h , t\\}-}\\|^2\\\\ & \\leq \\frac{h(2k - hk^2)}{1-e^{-2tk}}\\sum_{i=1}^{[\\frac{t}{h}]}\\|e_i^{\\{h , t\\}-}\\|^2 \\leq",
    "\\frac{2hk}{1-e^{-2tk}}\\sum_{i=1}^{[\\frac{t}{h}]}\\|e_i^{\\{h , t\\}-}\\|^2\\end{aligned}\\ ] ] for @xmath33 sufficiently big , @xmath323 which means that : @xmath324}\\|e_i^{\\{h , t\\}-}\\|^2\\end{aligned}\\ ] ] since @xmath325 then : @xmath326 leading to : @xmath327}\\big(\\|v_i^{\\{h , t\\}}\\|^2+\\|v_i^{\\{h,\\infty\\}}\\|^2\\big)\\\\ & \\leq \\frac{8hk}{\\gamma } \\times \\frac{t}{h}(2||\\mathbf{v}^{\\{\\infty\\}}||^2_{\\mathrm{b } } + \\alpha||\\boldsymbol\\eta^{\\{\\infty\\}}||^2)\\end{aligned}\\ ] ] finally : @xmath328 where @xmath329 is a constant that does nt depend on @xmath16 nor on @xmath33 . in the same way we obtain : @xmath330    even though the bounds we found for @xmath287 , @xmath331 , @xmath332 and @xmath333 may depend on @xmath33 and/or @xmath16 , we will see in the next section that they are strong enough to prove the convergence of the algorithm .",
    "furthermore , experimental simulations have shown that @xmath334 and @xmath287 does nt necessarily grow when @xmath335 , @xmath336 and stay bounded @xcite .",
    "in this section , we use the results obtained previously to prove our initial theorem :    for a @xmath22 map @xmath21 and a @xmath9 cost function @xmath14 , the following limit exists and is equal to : @xmath248}\\big[(dj(u_i , s))v_i^{\\{h , t\\}}+(\\partial_sj(u_i , s))+\\big(\\eta_i^{\\{h , t\\}}(j(u_i , s)-\\langle j\\rangle(s))\\big)\\big]\\\\ & = \\lim_{t \\to \\infty}\\lim_{h \\to 0 } \\frac{h}{t}\\sum_{i=1}^{[\\frac{t}{h}]}\\big[(dj(u_i , s))v_i^{\\{h , t\\}}+(\\partial_sj(u_i , s))+\\big(\\eta_i^{\\{h , t\\}}(j(u_i , s)-\\langle j\\rangle(s))\\big)\\big]\\end{aligned}\\ ] ]    because @xmath14 is @xmath9 and @xmath24 is compact , @xmath337 is uniformly bounded , i.e. , there exists a constant @xmath338 such that @xmath339 for all @xmath126 .",
    "let @xmath334 be defined as in the previous section , then : @xmath340}\\big[(dj(u_i , s))v_i^{\\{h , t\\}}+(\\partial_sj(u_i , s))+\\big(\\eta_i^{\\{h , t\\}}(j(u_i , s)-\\langle j \\rangle(s))\\big)\\big]\\\\ & -\\frac{h}{t}\\sum_{i=1}^{[\\frac{t}{h}]}\\big[(dj(u_i , s))v_i^{\\{h,\\infty\\}}+(\\partial_sj(u_i , s))+\\big(\\eta_i^{\\{h,\\infty\\}}(j(u_i , s)-\\langle j\\rangle(s))\\big)\\big]\\bigg|\\\\ & = \\bigg| \\frac{h}{t}\\sum_{i=1}^{[\\frac{t}{h}]}\\big[(dj(u_i , s))e_i^{\\{h , t\\}}+\\big(\\epsilon_i^{\\{h , t\\}}(j(u_i , s)-\\langle j\\rangle(s))\\big)\\big]\\bigg|\\\\ & = \\bigg| \\frac{h}{t}\\sum_{i=1}^{[\\frac{t}{h}]}\\big[(dj(u_i , s))(e_i^{\\{h , t\\}+}+e_i^{\\{h , t\\}-}+e_i^{\\{h , t\\}0})+\\big(\\epsilon_i^{\\{h , t\\}}(j(u_i , s)-\\langle j\\rangle(s))\\big)\\big]\\bigg|\\\\ & < \\bigg| \\frac{h}{t}\\sum_{i=1}^{[\\frac{t}{h}]}\\big[(dj(u_i , s))(e_i^{\\{h , t\\}+}+e_i^{\\{h , t\\}-})\\bigg|+\\bigg| \\frac{h}{t}\\sum_{i=1}^{[\\frac{t}{h}]}\\big[(dj(u_i , s))e_i^{\\{h , t\\}0}+\\big(\\epsilon_i^{\\{h , t\\}}(j(u_i , s)-\\langle j\\rangle(s))\\big)\\big]\\bigg|\\end{aligned}\\ ] ]    for the first term :    @xmath341}\\big[(dj(u_i , s))(e_i^{\\{h , t\\}+}+e_i^{\\{h , t\\}-})\\bigg| < & \\frac{h}{t}\\sum_{i=1}^{[\\frac{t}{h}]}\\|(dj(u_i , s))\\|\\|e_i^{\\{h , t\\}+}\\|\\\\ & + \\frac{h}{t}\\sum_{i=1}^{[\\frac{t}{h}]}\\|(dj(u_i , s))\\|\\|e_i^{\\{h , t\\}-}\\|\\end{aligned}\\ ] ]    @xmath342}c\\lambda^{h(\\frac{t}{h}-i)}\\|e_{[\\frac{t}{h}]}^{\\{h , t\\}+}\\|+\\sum_{i=1}^{[\\frac{t}{h}]}c\\lambda^{h(i-1)}\\|e_0^{\\{h , t\\}-}\\|\\big)\\\\ & \\leq \\frac{h}{t } \\frac { 2ac}{(1-\\lambda^h)}\\times e\\sqrt{t } \\\\ & \\leq \\frac{1}{\\sqrt{t}}\\times\\frac{2ace}{\\log(\\frac{e}{\\lambda})}\\end{aligned}\\ ] ]    which goes to @xmath5 when @xmath33 increases",
    ". thus , we notice that , in the stable and unstable subspaces , the difference @xmath334 between the shadowing trajectory @xmath343 and its approximation @xmath286 decreases extremely fast so that the whole term @xmath344}\\big[(dj(u_i , s))(e_i^{\\{h , t\\}+}+e_i^{\\{h , t\\}-})\\big|$ ] tends to @xmath5 .",
    "+ the situation is more complicated for the second term since there is no reason for @xmath331 and @xmath345 to decrease when @xmath346 increases .",
    "the cancellation of the second term is the result of the mutual cancellation of the elements in the sommation as we shall see .",
    "based on the sampling points @xmath235 for the continuous shadowing trajectory found in section [ sec5 ] , we consider the new set of sampling points @xmath347 which satisfy the following relation : @xmath348 for all @xmath126 .",
    "we can notice that the new sampling points describe the same continuous trajectory as the old set of values .",
    "assuming that @xmath345 and @xmath334 are bounded , we have for a sufficiently small @xmath2 :    @xmath349}\\frac{h(\\tau_i^{s}+s\\epsilon_i^{\\{h , t\\}})j(u^{'s}_i , s)}{h\\sum_j(\\tau_j^{s}+s\\epsilon_j^{\\{h , t\\}})}\\end{aligned}\\ ] ]     + we would obtain by following the same operations we did in section [ sec6 ] ( but upside down this time ) : @xmath350}\\big[(dj(u_i , s))e_i^{\\{h , t\\}0}+\\big(\\epsilon_i^{\\{h , t\\}}(j(u_i , s)-\\langle j\\rangle(s))\\big)\\big]\\bigg)\\\\ & = \\lim_{h \\rightarrow 0}\\lim_{t \\rightarrow + \\infty}\\lim_{s\\rightarrow 0}\\bigg ( \\frac{h}{t}\\sum_{i=0}^{[\\frac{t}{h}]}\\frac{(j(u_i^{'s},s)-j(u_i^s , s))}{s}\\bigg)\\\\ & + \\lim_{h \\rightarrow 0}\\lim_{t \\rightarrow + \\infty}\\lim_{s\\rightarrow 0}\\bigg(\\frac{h\\epsilon_i^{\\{h , t\\}}}{t}\\big(j(u_i^{'s},s)-\\frac{1}{t}\\sum_jhj(u_i^s , s)\\big)\\bigg)\\\\ & = \\lim_{h \\rightarrow 0 } \\lim_{t \\rightarrow + \\infty}\\lim_{s\\rightarrow 0}\\bigg ( \\frac{1}{s}\\sum_{i=0}^{[\\frac{t}{h}]}\\frac{h(\\tau_i^s+s\\epsilon_i^{\\{h , t\\}})j(u_i^{'s},s)}{h\\sum_j(\\tau_j^s+s\\epsilon_j^{\\{h , t\\}})}-\\frac{h\\tau_i^sj(u_i^s , s)}{h\\sum_j\\tau_j^s}\\bigg)+o(s)\\\\ & = \\lim_{s\\rightarrow 0}\\lim_{h \\rightarrow 0 } \\lim_{t \\rightarrow + \\infty}\\bigg ( \\frac{1}{s}\\sum_{i=0}^{[\\frac{t}{h}]}\\frac{h(\\tau_i^s+s\\epsilon_i^{\\{h , t\\}})j(u_i^{'s},s)}{h\\sum_j(\\tau_j^s+s\\epsilon_j^{\\{h , t\\}})}-\\frac{h\\tau_i^s j(u_i^s , s)}{h\\sum_j\\tau_j^s}\\bigg)+o(s)\\\\ & = \\lim_{s\\rightarrow 0}\\frac{\\langle j \\rangle(s)-\\langle j \\rangle ( s)}{s}+o(s)=0\\end{aligned}\\ ] ]    however , this is not necessarily true since @xmath331 and @xmath345 may grow as @xmath346 increases . permuting the limits",
    "is much more delicate but is still possible .",
    "please refer to appendix [ appendix2 ] for further details about how to permute the limits .",
    "the idea is to use the relation : @xmath321 } ( \\|v_i^{\\{h , t\\}}\\|^2+\\alpha ( \\eta_i^{\\{h , t\\}})^2 ) \\leq \\sum_{i=1}^{[\\frac{t}{h } ] } ( \\|v_i^{\\{h,\\infty\\}}\\|^2+\\alpha ( \\eta_i^{\\{h,\\infty\\}})^2 ) \\leq \\frac{t}{h}(||\\mathbf{v}^{\\{\\infty\\}}||^2_{\\mathrm{b } } + \\alpha||\\eta^{\\{\\infty\\}}||^2)\\end{aligned}\\ ] ] to show that most @xmath331 , @xmath345 remain bounded and that the contribution of the unbounded terms fades out .",
    "+ in conclusion : @xmath351}\\big[(dj(u_i , s))v_i^{\\{h , t\\}}+(\\partial_sj(u_i , s))+\\big(\\eta_i^{\\{h , t\\}}(j(u_i , s)-\\langle j \\rangle(s))\\big)\\big ] \\bigg|\\\\ & \\leq \\bigg| \\frac{d\\langle j \\rangle } { ds } - \\frac{h}{t}\\sum_{i=1}^{[\\frac{t}{h}]}\\big[(dj(u_i , s))v_i^{\\{h,\\infty\\}}+(\\partial_sj(u_i , s))+\\big(\\eta_i^{\\{h,\\infty\\}}(j(u_i , s)-\\langle j\\rangle(s))\\big)\\big ] \\bigg|\\\\ & + \\bigg| \\frac{h}{t}\\sum_{i=1}^{[\\frac{t}{h}]}\\big[(dj(u_i , s))v_i^{\\{h , t\\}}+(\\partial_sj(u_i , s))+\\big(\\eta_i^{\\{h , t\\}}(j(u_i , s)-\\langle",
    "j\\rangle(s))\\big)\\big]\\\\ & -\\frac{h}{t}\\sum_{i=1}^{[\\frac{t}{h}]}\\big[(dj(u_i , s))v_i^{\\{h,\\infty\\}}+(\\partial_sj(u_i , s))+\\big(\\eta_i^{\\{h,\\infty\\}}(j(u_i , s)-\\langle j\\rangle(s))\\big)\\big]\\bigg|\\\\\\end{aligned}\\ ] ] and we have showed that both terms go to @xmath5 as @xmath336 and @xmath335 . this concludes the proof .     +",
    "as we have shown through this paper , lss gives us a good estimation for @xmath4 when the dynamical system is uniformly hyperbolic . after running a simulation for a given @xmath2 , an arbitrary initial condition @xmath247 and an uniform time steptize of @xmath16",
    ", we obtain a sequence of reference sampling points ] @xmath352 . if we had access to the _ shadowing direction _ , we would easily compute : @xmath353}\\big[(dj(u_i^s , s))v_i^{\\{h,\\infty\\}}+(\\partial_sj(u_i^s , s))+\\big(\\eta_i^{\\{h,\\infty\\}}(j(u_i^s , s)-\\langle j\\rangle(s))\\big)\\big]\\end{aligned}\\ ] ] however , in real - life problems we usually do not have access to the _ stable _ and _ unstable subspaces _ around each @xmath124 prohibiting the usage of the closed form expression of @xmath343 and @xmath354 .",
    "thus , we have no other choice than computing an approximation of the _ shadowing direction_. this approximation is given by the solution to the least squares problem : @xmath355 } ( \\|v_i^{\\{h , t\\}}\\|^2+\\alpha ( \\eta_i^{\\{h , t\\}})^2 ) \\\\ & \\textrm{s.t . }",
    "\\quad v_{i+1}^{\\{h , t\\}}=(d\\varphi_s(u_i^s , h))v_i^{\\{h , t\\}}+\\partial_s\\varphi_s(u_i^s , h)+h\\eta_i^{\\{h , t\\}}\\partial_h\\varphi_s(u_i^s , h ) , \\end{split}\\ ] ] after solving this quadratic optimization problem , we estimate @xmath4 using expression ( [ concl ] ) again where the @xmath237 are replaced by @xmath356 .",
    "as we have seen previously , this estimation converges to the real value of @xmath4 when the time stepsize @xmath16 is refined and the integration lapse @xmath33 increases .",
    "in our case , we have an explicit expression for @xmath237 : @xmath357 as we did previously : @xmath358 so : @xmath359 where @xmath311 is a compact set of @xmath360 ( for example @xmath361 $ ] ) and @xmath362 .",
    "we ve also used the taylor expansion of @xmath363 for @xmath335 and assumed that @xmath364 is well - defined and continuous on the compact set @xmath365 . + following similar steps ,",
    "we obtain : @xmath366      let @xmath367 be an arbitrary bound and we will assume that @xmath368 for simplicity reasons ( without loss of generality ) . if @xmath369 is the number of elements that are bigger or equal to @xmath370 , we have : @xmath371 with the equality being verified in the worst case scenario where all the unbounded terms are equal to @xmath370 , all the bounded terms are equal to @xmath5 and @xmath372 is exactly equal to @xmath373 .",
    "then , let us compute the contribution of the terms that are unbounded . for that purpose",
    ", we introduce an indicator function @xmath374 that is equal to @xmath5 when both @xmath331 and @xmath345 are below @xmath370 and equal to @xmath375 when at least one of them is bigger ( or equal ) than @xmath370 .",
    "we have : @xmath350}\\delta(i)\\big[(dj(u_i , s))e_i^{\\{h , t\\}0}+\\big(\\epsilon_i^{\\{h , t\\}}(j(u_i , s)-\\langle j\\rangle(s))\\big)\\big]\\bigg)\\\\ & \\leq\\lim_{h\\to 0}\\lim_{t \\to \\infty}\\bigg ( \\frac{h}{t}\\sum_{i=1}^{[\\frac{t}{h}]}\\delta(i)\\big[\\|dj\\|_\\infty^\\lambda \\|e_i^{\\{h , t\\}0}\\|+\\big(2\\|\\epsilon_i^{\\{h , t\\}}\\|\\|j\\|^\\lambda_\\infty)\\big)\\big]\\bigg)\\\\ & \\leq\\lim_{h\\to 0}\\lim_{t \\to \\infty}\\bigg ( \\frac{h}{t}\\max\\big(\\|dj\\|_\\infty^\\lambda,2\\|j\\|^\\lambda_\\infty\\big ) \\sum_{i=1}^{[\\frac{t}{h}]}\\delta(i)\\big(\\|e_i^{\\{h , t\\}0}\\|+\\|\\epsilon_i^{\\{h , t\\}}\\|\\big)\\bigg)\\\\ & \\leq\\lim_{h\\to",
    "0}\\lim_{t \\to \\infty}\\bigg ( \\frac{h}{t}\\max\\big(\\|dj\\|_\\infty^\\lambda,2\\|j\\|^\\lambda_\\infty\\big ) \\times n_e c\\bigg)\\\\ & \\leq\\lim_{h\\to 0}\\lim_{t \\to \\infty}\\bigg ( \\frac{h}{t}\\max\\big(\\|dj\\|_\\infty^\\lambda,2\\|j\\|^\\lambda_\\infty\\big ) \\times \\frac{t(||\\mathbf{v}^{\\{\\infty\\}}||^2_{\\mathrm{b } } + ||\\eta^{\\{\\infty\\}}||^2)}{hc^2}c\\bigg)\\\\ & \\leq \\frac{\\max\\big(\\|dj\\|_\\infty^\\lambda,2\\|j\\|^\\lambda_\\infty\\big)(||\\mathbf{v}^{\\{\\infty\\}}||^2_{\\mathrm{b } } + ||\\eta^{\\{\\infty\\}}||^2)}{c}\\end{aligned}\\ ] ] we have used the fact that @xmath376}\\delta(i)\\big(\\|e_i^{\\{h , t\\}0}\\|+\\|\\epsilon_i^{\\{h , t\\}}\\|\\big)$ ] is maximized when we have the maximum number of unbounded elements and when all of them have the same value .",
    "the worst case scenario is again the one where we have @xmath369 unbounded elements all equal to @xmath370 .",
    "since @xmath370 is arbitrarly big , the contribution of the unbounded terms is as small as we want .",
    "then , the limits can permuted in the expression : @xmath350}(1-\\delta(i))\\big[(dj(u_i , s))e_i^{\\{h , t\\}0}+\\big(\\epsilon_i^{\\{h , t\\}}(j(u_i , s)-\\langle j \\rangle(s))\\big)\\big]\\bigg)\\\\ & = \\lim_{h \\rightarrow 0}\\lim_{t \\rightarrow + \\infty}\\lim_{s\\rightarrow 0}\\bigg((1-\\delta(i ) ) \\frac{h}{t}\\sum_{i=0}^{[\\frac{t}{h}]}\\frac{(j(u_i^{'s},s)-j(u_i^s , s))}{s}\\bigg)\\\\ & + \\lim_{h \\rightarrow 0}\\lim_{t \\rightarrow + \\infty}\\lim_{s\\rightarrow 0}\\bigg((1-\\delta(i))\\frac{h\\epsilon_i^{\\{h , t\\}}}{t}(j(u_i^{'s},s)-\\frac{1}{t}\\sum_jhj(u_i^s , s))\\bigg)\\\\ & = \\lim_{h \\rightarrow 0 } \\lim_{t \\rightarrow + \\infty}\\lim_{s\\rightarrow 0}\\bigg ( ( 1-\\delta(i))\\frac{1}{s}\\sum_{i=0}^{[\\frac{t}{h}]}\\frac{h(\\tau_i^s+s\\epsilon_i^{\\{h , t\\}})j(u_i^{'s},s)}{h\\sum_j(\\tau_j^s+s\\epsilon_j^{\\{h , t\\}})}-\\frac{h\\tau_i^sj(u_i^s , s)}{h\\sum_j\\tau_j^s}\\bigg)+o(s)\\\\ & = \\lim_{s\\rightarrow 0}\\lim_{h \\rightarrow 0 } \\lim_{t \\rightarrow + \\infty}\\bigg((1-\\delta(i ) ) \\frac{1}{s}\\sum_{i=0}^{[\\frac{t}{h}]}\\frac{h(\\tau_i^s+s\\epsilon_i^{\\{h , t\\}})j(u_i^{'s},s)}{h\\sum_j(\\tau_j^s+s\\epsilon_j^{\\{h , t\\}})}-\\frac{h\\tau_i^sj(u_i^s , s)}{h\\sum_j\\tau_j^s}\\bigg)+o(s)\\\\ & = \\lim_{s\\rightarrow 0 } \\frac{\\langle j\\rangle(s)-\\langle j \\rangle ( s)}{s}+o(s)=0\\end{aligned}\\ ] ] the last equality comes from property ( [ riemann sum ergodic ] ) we had on riemann sums ."
  ],
  "abstract_text": [
    "<S> for a parameterized hyperbolic system @xmath0 the derivative of the ergodic average @xmath1 to the parameter @xmath2 can be computed via the least squares shadowing algorithm ( lss ) . </S>",
    "<S> we assume that the sytem is ergodic which means that @xmath3 depends only on @xmath2 ( not on the initial condition of the hyperbolic system ) . </S>",
    "<S> after discretizing this continuous system using a fixed timestep , the algorithm solves a constrained least squares problem and , from the solution to this problem , computes the desired derivative @xmath4 . </S>",
    "<S> the purpose of this paper is to prove that the value given by the lss algorithm approaches the exact derivative when the discretization timestep goes to @xmath5 and the timespan used to formulate the least squares problem grows to infinity .    </S>",
    "<S> sensitivity analysis , dynamical systems , chaos , uniform hyperbolicity , ergodicity , least squares shadowing    34a34 , 34d30 , 37a99 , 37d20 , 37d45 , 37n99 , 46n99 , 65p99 </S>"
  ]
}