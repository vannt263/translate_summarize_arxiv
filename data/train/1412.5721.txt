{
  "article_text": [
    "one of the most basic and well - studied optimization models in unsupervised machine learning is @xmath0-means clustering . in this problem",
    "we are given the set @xmath7 of @xmath8 points ( or vectors ) in euclidian space .",
    "the goal is to partition @xmath7 into @xmath0 sets called clusters @xmath9 and choose one cluster center @xmath10 for each cluster @xmath11 to minimize @xmath12 in the standard offline setting , the set of input points is known in advance and the data access model is unrestricted .",
    "even so , obtaining provably good solutions to this problem is difficult .",
    "see section  [ related ] .    in the streaming model",
    "the algorithm must consume the data in one pass and is allowed to keep only a small ( typically constant or poly - logarithmic in @xmath8 ) amount of information .",
    "nevertheless , it must output its final decisions when the stream has ended . for example , the location of the centers for @xmath0-means .",
    "this severely restricted data access model requires new algorithmic ideas .",
    "see section  [ related ] for prior art .",
    "notice that , in the streaming model , the assignment of individual points to clusters may become available only in hindsight .",
    "in contrast , the online model of computation does not allow to postpone clustering decisions . in this setting ,",
    "an a priori unknown number of points arrive one by one in an arbitrary order .",
    "when a new point arrives the algorithm must either put it in one of the existing clusters or open a new cluster ( consisting of a single point ) .",
    "note that this problem is conceptually non trivial even if one could afford unbounded computational power at every iteration .",
    "this is because the quality of current choices depend on the unknown ( yet unseen ) remainder of the stream .    in this paper",
    ", we consider the very restricted setting in the intersection of these two models .",
    "we require the algorithm outputs a single cluster identifier for each point online while using space and time at most poly - logarithmic in the length of the stream .",
    "this setting is harder than the streaming model . on the one hand ,",
    "any space efficient online algorithm is trivially convertible to a streaming algorithm .",
    "one could trivially keep sufficient statistics for each cluster such that the centers of mass could be computed at the end of the stream .",
    "the computational and space overhead are independent of the length of the stream . on the other hand ,",
    "the online problem resists approximation even in one dimension and @xmath13 .",
    "consider the stream where @xmath14 and @xmath15 ( acting as one dimensional vectors ) .",
    "any online clustering algorithm must assign them to different clusters . otherwise , the algorithm cost is @xmath16 and the optimal is cost is trivially @xmath17 .",
    "if the the algorithm assigns @xmath18 and @xmath19 to different clusters , the third point might be @xmath20 for some @xmath21 . at this point ,",
    "the algorithm is forced to assign @xmath22 to one of the existing clusters incurring cost of @xmath23 which is arbitrarily larger than the optimal solution of cost @xmath16 .",
    "this example also proves that any online algorithm with a bounded approximation factor ( such as ours ) must create strictly more than @xmath0 clusters .    in this work",
    "we provide algorithms for both online @xmath0-means and _ semi - online _ @xmath0-means . in the _ semi - online _ model",
    "we assume having a lower bound , @xmath24 , for the total optimal cost of @xmath0-means , @xmath6 , as well as an estimate for @xmath8 , the length of the stream .",
    "algorithm  [ algsemionline ] creates at most @xmath25 clusters in expectation and has an expected objective value of @xmath26 . from a practical viewpoint ,",
    "it is reasonable to assume having rough estimates for @xmath24 and @xmath8 .",
    "since the dependence on both estimates is logarithmic , the performance of the _ semi - online _ algorithm will degrade significantly only if our estimates are wrong by many orders of magnitude . in the fully online model",
    "we do not assume any prior knowledge .",
    "algorithm  [ algfullyonline ] operates in that setting and opens a comparable number of clusters to algorithm  [ algsemionline ] .",
    "but , its approximation factor guarantee degrades by a @xmath27-factor .      in the context of machine learning ,",
    "the results of @xmath0-means were shown to provide powerful unsupervised features @xcite on par , sometimes , with neural nets for example .",
    "this is often referred to as ( unsupervised ) feature learning .",
    "intuitively , if the clustering captures most of the variability in the data , assigning a single label to an entire cluster should be pretty accurate .",
    "it is not surprising therefore that cluster labels are powerful features for classification . in the case of online machine learning",
    ", these cluster labels must also be assigned online .",
    "the importance of such an online @xmath0-means model was already recognized in machine learning community @xcite .    for information retrieval",
    ", @xcite investigated the incremental @xmath0-centers problem .",
    "they argue that clustering algorithms , in practice , are often required to be online .",
    "we observe the same at yahoo .",
    "for example , when suggesting news stories to users , we want to avoid suggesting those that are close variants of those they already read . or , conversely , we want to suggest stories which are a part of a story - line the user is following . in either scenario ,",
    "when yahoo receives a news item , it must immediately decide what cluster it belongs to and act accordingly .      in the offline setting where the set of all points is known in advance",
    ", lloyd s algorithm @xcite provides popular heuristics .",
    "it is so popular that practitioners often simply refer to it as @xmath0-means .",
    "yet , only recently some theoretical guaranties were proven for its performance on  well clusterable \" inputs @xcite .",
    "the @xmath0-means++ @xcite algorithm provides an expected @xmath28 approximation or an efficient seeding algorithm .",
    "a well known theoretical algorithm is due to kanungo et al .",
    "it gives a constant approximation ratio and is based on local search ideas popular in the related area of design and analysis of algorithms for facility location problems , e.g. , @xcite .",
    "recently , @xcite improved the analysis of @xcite and gave an adaptive sampling based algorithm with constant factor approximation to the optimal cost . in an effort to make adaptive sampling techniques more scalable",
    ", @xcite introduced @xmath0-means@xmath29 which reduces the number of passes needed over the data and enables improved parallelization .",
    "the streaming model was considered by @xcite and @xcite and later by @xcite .",
    "they build upon adaptive sampling ideas from @xcite and branch - and - bound techniques from @xcite .",
    "the first ( to our knowledge ) result in online clustering dates back the @xmath0-centers result of @xcite . for @xmath0-means an expectation maximization ( em )",
    "approach was investigated by @xcite .",
    "their focus was on online em as a whole but their techniques include online clustering .",
    "they offer very encouraging results , especially in the context of machine learning . to the best of our understanding",
    ", however , their techniques do not extend to arbitrary input sequences .",
    "in contrast , the result of @xcite provides provable results for the online setting in the presence of base-@xmath0-means algorithm as experts .",
    "a closely related family of optimization problems is known as facility location problems .",
    "two standard variants are the uncapacitated facility location problem ( or the simple plant location problem in the operations research jargon ) and the k - median problem .",
    "these problems are well - studied both from computational and theoretic viewpoints ( a book @xcite and a survey @xcite provide the background on some of the aspects in this area ) .",
    "meyerson @xcite suggested a simple and elegant algorithm for the online uncapacitated facility location with competitive ratio of @xmath30 .",
    "fotakis @xcite suggested a primal - dual algorithm with better performance guarantee of @xmath31 .",
    "anagnostopoulos et al .",
    "@xcite considered a different set of algorithms based on hierarchical partitioning of the space and obtained similar competitive ratios .",
    "the survey @xcite summarizes the results in this area . as a remark",
    ", @xcite already considered connections between facility location problems and clustering .",
    "interestingly , their algorithm is often referred to as  the doubling algorithm \" since the cluster diameters double as the algorithm receives more points . in our work",
    "the facility location cost is doubled which is technically different but intuitively related .",
    "we begin with presenting the _ semi - online _ algorithm .",
    "it assumes knowing the number of vectors @xmath8 and some lower bound @xmath24 for the value of the optimal solution .",
    "these assumptions make the algorithm slightly simpler and the result slightly tighter . nevertheless , the _ semi - online _ online already faces most of the challenges faced by the _ fully online _ version .",
    "in fact , proving the correctness of the online algorithm ( section [ secfullyonline ] ) would require only minor adjustments to the proofs in this section .",
    "the algorithm uses ideas from the online facility location algorithm of meyerson @xcite .",
    "the intuition is as follows ; think about @xmath0-means and a facility location problem where the service costs are squared euclidean distances . for the facility cost , start with @xmath32 which is known to be too low . by doing that the algorithm is  encouraged \" to open many facilities ( centers ) which keeps the service costs low . if the algorithm detect that too many facilities were opened",
    ", it can conclude that the current facility cost is too low .",
    "it therefore doubles the facility cost of opening future facilities ( centers ) .",
    "it is easy to see that the facility cost can not be doubled many times without making opening new clusters prohibitively expensive . in algorithm [ algfullyonline ]",
    "we denote the distance of a point @xmath33 to a set @xmath34 as @xmath35 . as a convention , if @xmath36 then @xmath37 for any @xmath33 .",
    "@xmath7 , @xmath0 , @xmath24 , @xmath8 @xmath38 @xmath39 ; @xmath40 ; @xmath41 @xmath42 @xmath43 @xmath44 ; @xmath45 ; @xmath46 @xmath47    consider some optimal solution consisting of clusters @xmath48 with cluster centers @xmath49 .",
    "let @xmath50 be the cost of the @xmath51-th cluster in the optimal solution and @xmath52 be the value of the optimal solution .",
    "let @xmath53 be the average squared distance to the cluster center from a vectors in the @xmath51-th optimal cluster .",
    "@xmath54 we define a partition of the cluster @xmath55 into subsets that we call _ rings _ : @xmath56 and for @xmath57 @xmath58\\right\\ } \\ .\\ ] ]    note that we consider only values of @xmath59 since @xmath60 for @xmath61 .",
    "to verify assume the contrary and compute @xmath53 .",
    "[ thmnumberofclustersemionline ] let @xmath34 be the set of clusters defined by algorithm  [ algsemionline ] .",
    "then @xmath62=o\\left(k \\log _ { } n \\log _ { } \\frac{w^*}{w^*}\\right ) \\ .\\ ] ]    consider the phase @xmath63 of the algorithm where , for the first time @xmath64 the initial facility cost @xmath32 is doubled at every phase during each of which the algorithm creates @xmath65 clusters . the total number of clusters opened before phase @xmath63 is trivially upper bounded by @xmath66 . which is , in turn , @xmath67 by the choice of @xmath32 .    bounding the number of centers opened during and after phase @xmath63 is more complicated .",
    "denote by @xmath68 the set of points in the ring @xmath69 that our algorithm encounters during phase @xmath70 .",
    "the expected number of clusters initiated by vectors in the ring @xmath69 during phases @xmath71 is at most @xmath72 this is because once we open the first cluster with a center at some @xmath73 the probability of opening a cluster for each subsequent vector @xmath74 is upper bounded by @xmath75 by the ( squared ) triangle inequality for @xmath76 .",
    "therefore the expected number of clusters chosen from @xmath77 during and after phase @xmath63 is at most @xmath78 summing up over all @xmath79 using @xmath80 we obtain that the expected number of centers chosen during phases @xmath81 is at most @xmath82 substituting @xmath83 completes the proof of the theorem .    before we estimate the expected cost of clusters opened by our online algorithm we prove the following technical lemma .",
    "[ random ] we are given a sequence @xmath84 of @xmath8 independent experiments .",
    "each experiment succeeds with probability @xmath85 where @xmath86 and @xmath87 for @xmath88 .",
    "let @xmath89 be the ( random ) number of sequential unsuccessful experiments , then : @xmath90\\le b.\\ ] ]    let @xmath91 be the maximal index for which @xmath92 for all @xmath93 .",
    "@xmath94 & = & \\sum_{i=1}^{n'}a_i \\pr[t \\ge i ] \\\\ & \\le & \\sum_{i=1}^{n'}a_i\\prod_{j=1}^i\\left(1-\\frac{a_j}{b}\\right)\\\\ & \\le & b \\sum_{i=1}^{n'}\\frac{a_i}{b}\\prod_{j=1}^{i-1}\\left(1-\\frac{a_j}{b}\\right)\\\\ & \\le & b.\\end{aligned}\\ ] ] the last inequality uses @xmath95 for @xmath96 .    [ thmapproxsemionline ]",
    "let @xmath97 be the cost of the online assignments of algorithm [ algsemionline ] and @xmath6 the optimal @xmath0-means clustering cost .",
    "then @xmath98=o(w^ * ) \\ .\\ ] ]    consider the service cost of vectors in each ring @xmath69 in two separate stages . before a vector from the ring",
    "is chosen to start a new cluster and after . before a center from @xmath69",
    "is chosen each vector @xmath99 is chosen with probability @xmath100 . here",
    ", @xmath34 is the set of centers already chosen by the algorithm before encountering @xmath33 .",
    "if @xmath33 is not chosen the algorithm incurs a cost of @xmath101 . by lemma [ random ]",
    "the expected sum of these costs is bounded by @xmath102 . summing over all the rings we get a contribution of @xmath103 .    after a vector @xmath99",
    "is chosen to start a new cluster , the service cost of each additional vector @xmath104 is at most @xmath105 . summing up over all vectors and rings , this stage contributes are most @xmath106 to the cost of our solution .",
    "all in all , the expected online @xmath0-means cost is bounded by @xmath98 = o(f_r k\\log n   + w^ * ) \\ .\\ ] ]    we now turn to estimating @xmath107 $ ] .",
    "consider the first phase @xmath108 of the algorithm such that @xmath109 by equation  [ centersafterphase ] the expected number of clusters opened during and after phase @xmath108 is at most @xmath110 . by markov s inequality",
    "the probability of opening more than @xmath111 clusters is at most @xmath112 .",
    "therefore , with probability at least @xmath113 the algorithm will conclude while at phase @xmath108 .",
    "let @xmath114 be the probability that our algorithm terminates before round @xmath108 .",
    "since the probability of concluding the execution at each of the rounds after @xmath108 is at least @xmath113 we derive an upper bound @xmath115&\\le & p f_{r''-1 } + ( 1-p)\\sum_{r = r''}^{+\\infty}f_r\\cdot \\frac59 \\cdot \\left(\\frac49\\right)^{r - r''}\\\\ & < & f_{r '' } + f_{r''}\\cdot \\frac59 \\sum_{i=0}^{+\\infty}2^i\\cdot \\left(\\frac49\\right)^i = o(f_{r '' } ) \\ ] ] combining @xmath107 = o(f_{r''})$ ] with our choice of @xmath116 and our previous observation that @xmath117 = o(f_r k\\log n   + w^*)$ ] completes the proof .",
    "algorithm [ algfullyonline ] is fully online yet it defers from algorithm [ algsemionline ] in only a few aspects .",
    "first , since @xmath8 is unknown , the initial facility cost and the doubling condition can not depend on it .",
    "second , it must generate its own lower bound @xmath24 based on a short prefix of points in the stream .",
    "note that @xmath24 is trivially smaller that @xmath6 .",
    "any clustering of @xmath118 points must put at least two points in one cluster , incurring a cost of @xmath119 .",
    "@xmath7 , @xmath0 @xmath120 the first @xmath118 distinct vectors in @xmath7 ; and @xmath121 ( for each of these * yield * itself as its center ) @xmath122 @xmath39 ; @xmath40 ; @xmath123 @xmath124 @xmath42 @xmath43 @xmath44 ; @xmath45 ; @xmath46 @xmath47    [ thmnumberofclusterfullyonline ] let @xmath34 be the set of clusters defined by algorithm  [ algfullyonline ] .",
    "then @xmath62=o\\left(k \\log _ { } n \\log _ { } \\frac{w^*}{w^*}\\right ) = o\\left(k \\log _ { } n \\log _ { } \\gamma n \\right ) \\ .\\ ] ]    here @xmath125 is the dataset `` aspect ratio '' .    intuitively , for the same lower bound @xmath24 algorithm  [ algfullyonline ] should create fewer centers than algorithm  [ algsemionline ] since its initial facility cost is higher and it is doubled more frequently .",
    "this intuition is made concrete by retracing the proof of theorem  [ thmnumberofclusterfullyonline ] to show @xmath62 = o\\left(k \\log _ { } n \\log _ { } \\frac{w^*}{w^*}\\right ) \\ .\\ ] ]    to get a handle on the value of @xmath126 , observe that @xmath127 . combining this with the definition of @xmath128 we get @xmath129 .",
    "let @xmath97 be the cost of the online assignments of algorithm [ algfullyonline ] and @xmath6 the optimal @xmath0-means clustering cost .",
    "then @xmath98=o(w^ * \\log",
    "n ) \\ .\\ ] ]    we start by following the argument of the proof of theorem  [ thmapproxsemionline ] verbatim .",
    "we arrive at the conclusion that @xmath98 = o(f_r k\\log n   + w^*)\\ ] ] where @xmath102 is the final facility cost of the algorithm and @xmath130 is its last phase . showing that @xmath107 = o(w^*/k)$ ] will therefore complete the proof .",
    "consider any phase @xmath131 of the algorithm where @xmath108 is the smallest index such that @xmath132 let @xmath133 be the number of points from the input the algorithm went through by the end of phase @xmath70 .",
    "let @xmath134 be the number of clusters opened during phase @xmath70 and @xmath135 the number those who are _ not _ the first in their ring .",
    "@xmath136 the term @xmath137 is an upper bound on the number of rings at the end of stage @xmath70 .",
    "we pessimistically count at most one ( the first ) cluster from each such ring . following the argument in the proof of theorem [ thmnumberofclustersemionline ] that lead us to equation  ( [ centersafterphase ] )",
    "we conclude @xmath138 \\le 12w^*/f_r$ ] .",
    "algorithm  [ algfullyonline ] only advances to the next phase if @xmath139 which requires @xmath140 . by markov s inequality and the fact that @xmath138 \\le 12w^*/f_r \\le k/3 $ ] the probability of reaching the next phase is at most @xmath141 .",
    "we now estimate @xmath107 $ ] .",
    "let @xmath114 be the probability that our algorithm finishes before round @xmath108 .",
    "we have @xmath115&\\le & p f_{r''-1 } + ( 1-p)\\sum_{r = r''}^{+\\infty}f_r\\cdot \\frac56 \\cdot \\left(\\frac16\\right)^{r - r''}\\\\ & \\le&f_{r '' } + f_{r''}\\cdot \\frac56\\sum_{i=0}^{+\\infty}2^i\\cdot \\left(\\frac16\\right)^i = o(f_{r''})\\\\ \\ ] ] since @xmath142 the proof is complete .",
    "while experimenting with the algorithm , we discovered that some @xmath143 factors were , in fact , too pessimistic in practice .",
    "we also had to make some pragmatic decisions about , for example , how to set the initial facility cost .",
    "as another practical adjustment we introduce the notion of @xmath144 and @xmath145 . the value of @xmath144 is the number of clusters we would like the algorithm to output while @xmath145 is the actual number of clusters generated .",
    "internally , the algorithm operates with a value of @xmath146 .",
    "this is a heuristic ( entirely ad - hoc ) conversion that compensates for the @xmath145 being larger than @xmath0 by design .",
    "@xmath144 @xmath146 @xmath120 the first @xmath147 vectors in @xmath7 ( for each of these * yield * itself as its center ) @xmath148 half the sum of the @xmath149 smallest squared distances of points in @xmath34 to their closest neighbor . @xmath39 ; @xmath40 ; @xmath150 @xmath42 @xmath43 @xmath44 ; @xmath45 ; @xmath151",
    "@xmath47 @xmath152      to evaluate our algorithm we executed it on @xmath153 different datasets .",
    "all the datasets that we used are conveniently aggregated on the libsvm website @xcite and on the uci dataset collection @xcite .",
    "some basic information about each dataset is given in table  [ table1 ] .",
    ".the table gives some basic information about the datasets we experimented with .",
    "the column under @xmath154 gives the number of non zero entries in the entire dataset , @xmath8 the number of vectors and @xmath155 their dimension .",
    "much more information is provided on libsvm website @xcite and in the uci dataset collection @xcite . [ cols=\"^,^,^,^\",options=\"header \" , ]      one of the artifacts of applying our online @xmath0-means algorithm is that the number of clusters is not exactly known a priory . but as we see in figure  [ fig1 ] , the number of resulting clusters is rather predictable and controllable .",
    "figure  [ fig1 ] gives the ratio between the number of clusters output by the algorithm , @xmath145 , and the specified target @xmath144 .",
    "the results reported are mean values of @xmath156 runs for every parameter setting .",
    "the observed standard deviation of @xmath145 is typically in the range @xmath157 $ ] and never exceeded @xmath158 in any experiment .",
    "figure  [ fig1 ] clearly shows that the ratio @xmath159 is roughly constant and close @xmath160 .",
    "interestingly , the main differentiator is the choice of dataset .     on the @xmath161-axis as a function of @xmath144 on the @xmath162-axis .",
    "the value @xmath144 is given to the algorithm as input and @xmath145 is the resulting cardinality of the center set @xmath34 .",
    "we clearly see that this ratio is roughly constant and close @xmath163 .",
    "interestingly , the main differentiator is the dataset itself and not the value of @xmath144 . ]      throughout this section , we measure the online @xmath0-means clustering cost with respect to different baselines .",
    "we report averages of at least 3 different independent executions for every parameter setting . in figure  [ fig2 ]",
    "the reader can see the online @xmath0-means clustering cost for the set of centers chosen online by our algorithm for different values of @xmath144 and different datasets . for normalization",
    ", each cost is divided by @xmath164 , the sum of squares of all vector norms in the dataset ( akin to the theoretical @xmath0-means cost of having one center at the origin ) .",
    "note that some datasets are inherently unclusterable . even using many cluster centers ,",
    "the @xmath0-means objective does not decrease substantially .",
    "nevertheless , as expected , the @xmath0-means cost obtained by the online algorithm , @xmath165 , decreases as a function of @xmath144 .",
    "-means clustering cost ( @xmath165 ) as a function of @xmath144 for the different datasets . for normalization , each cost is divided by @xmath164 , the sum of squares of all vector norms in the dataset ( akin to the @xmath0-cost of once center in the origin ) . ]",
    "the monotonicity of @xmath165 with respect to @xmath144 is unsurprising . in figure  [ fig3 ]",
    "we plot the ratio @xmath166 as a function of @xmath144 . here , @xmath167 is the sum of squared distances of input points to @xmath145 input points chosen uniformly at random ( as centers ) .",
    "note that in each experiment the number of clusters used by the random solution and online @xmath0-means is identical , namely , @xmath145 .",
    "figure  [ fig3 ] illustrates something surprising .",
    "the ratio between the costs remains relatively fixed per dataset and almost independent to @xmath144 .",
    "put differently , even when the @xmath0-means cost is significantly lower than picking @xmath0 random centers , they improve in similar rates as @xmath0 grows .",
    "-axis , the value of @xmath165 divided by @xmath167 .",
    "the latter is the cost of choosing , uniformly at random , as many cluster centers ( from the data ) as the online algorithm did .",
    "a surprising observation is that this ratio is almost constant for each dataset and almost independent of @xmath144 ( on the @xmath162-axis ) . ]",
    "the next experiment compares online @xmath0-means to @xmath0-means++ . for every value of @xmath144 we ran online @xmath0-means to obtain both @xmath165 and @xmath145 .",
    "then , we invoke @xmath0-means++ using @xmath145 clusters and computed its cost , @xmath168 .",
    "this experiment was repeated @xmath156 times for each dataset and each value of @xmath144 .",
    "the mean results are reported in figure  [ fig4 ] .",
    "unsurprisingly , @xmath0-means++ is usually better in terms of cost .",
    "but , the reader should keep in mind that @xmath0-means++ is an _ offline _ algorithm that requires @xmath0 passes over the data compared with the online computational model of our algorithm .",
    "-axis we plot @xmath169 as a function of @xmath144 on the @xmath162-axis .",
    "the values of @xmath165 is the cost of running algorithm  [ algexperimental ] with parameter @xmath144 .",
    "the value of @xmath168 is the cost of running @xmath0-means++ with @xmath145 clusters , @xmath145 is the number of clusters online @xmath0-means actually used .",
    "we see that , except for the datasets _ adult _ and _ shuttle.binary_ , @xmath0-means++ and online @xmath0-means are comparable . for _ adult _ and _ shuttle.binary _ online @xmath0-means is worse by a small constant factor .",
    "note ( figure  [ fig3 ] ) that both _",
    "adult _ and _ shuttle.binary _ are datasets for which online @xmath0 means is dramatically better than random . ]",
    "-axis @xmath170 both using @xmath145 clusters .",
    "the value of @xmath145 is obtained by running online @xmath0-means with input @xmath144 on the @xmath162-axis .",
    "the @xmath161-axis depicts @xmath166 .",
    "note that the performance of @xmath0-means++ and online @xmath0-means are very similar almost everywhere .",
    "the advantage of @xmath0-means++ ( see figure  [ fig4 ] ) occurs when the clustering cost is a minuscule fraction of random clustering . ]",
    "we would like to thank anna choromanska and sergei vassilvitskii for very helpful suggestions and to dean foster for helping us with the proof of the lemma [ random ] .",
    "ankit aggarwal , amit deshpande , and ravi kannan .",
    "adaptive sampling for k - means clustering . in _ approximation , randomization , and combinatorial optimization .",
    "algorithms and techniques , 12th international workshop , approx 2009 , and 13th international workshop , random 2009 , berkeley , ca , usa , august 21 - 23 , 2009 . proceedings _ , pages 1528 , 2009 .",
    "nir ailon , ragesh jaiswal , and claire monteleoni .",
    "streaming k - means approximation . in yoshua bengio , dale schuurmans , john  d. lafferty , christopher k.  i. williams , and aron culotta , editors , _ nips _ , pages 1018 .",
    "curran associates , inc . , 2009 .",
    "moses charikar , chandra chekuri , toms feder , and rajeev motwani . incremental clustering and dynamic information retrieval . in _ proceedings of the twenty - ninth annual acm symposium on theory of computing _ , stoc 97 , pages 626635 , new york , ny , usa , 1997 .",
    "anna choromanska and claire monteleoni .",
    "online clustering with experts . in _ proceedings of the fifteenth international conference on artificial intelligence and statistics , aistats 2012 , la palma , canary islands ,",
    "april 21 - 23 , 2012 _ , pages 227235 , 2012 .",
    "adam coates , andrew  y. ng , and honglak lee .",
    "an analysis of single - layer networks in unsupervised feature learning . in geoffrey",
    "j. gordon , david  b. dunson , and miroslav dudk , editors , _ aistats _ , volume  15 of _ jmlr proceedings _ , pages 215223 .",
    "jmlr.org , 2011 .",
    "tapas kanungo , david  m. mount , nathan  s. netanyahu , christine  d. piatko , ruth silverman , and angela  y. wu .",
    "a local search approximation algorithm for k - means clustering . in _",
    "symposium on computational geometry _",
    ", pages 1018 , 2002 .",
    "percy liang and dan klein .",
    "online em for unsupervised models . in _",
    "human language technologies : conference of the north american chapter of the association of computational linguistics , proceedings , may 31 - june 5 , 2009 , boulder , colorado , usa _ , pages 611619 , 2009 ."
  ],
  "abstract_text": [
    "<S> this paper shows that one can be competitive with the @xmath0-means objective while operating online . in this model , the algorithm receives vectors @xmath1 one by one in an arbitrary order . for each vector @xmath2 the algorithm outputs a cluster identifier before receiving @xmath3 . </S>",
    "<S> our online algorithm generates @xmath4 clusters whose @xmath0-means cost is @xmath5 where @xmath6 is the optimal @xmath0-means cost using @xmath0 clusters . </S>",
    "<S> suppresses poly - logarithmic factors . </S>",
    "<S> ] we also show that , experimentally , it is not much worse than @xmath0-means++ while operating in a strictly more constrained computational model . </S>"
  ]
}