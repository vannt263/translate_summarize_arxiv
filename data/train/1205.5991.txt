{
  "article_text": [
    "let @xmath0 denote the number of partitions of @xmath3 , or the number of ways that @xmath3 can be written as a sum of positive integers without regard to the order of the terms ( a000041 in @xcite ) .",
    "the classical way to compute @xmath0 uses the generating function representation of @xmath0 combined with euler s pentagonal number theorem @xmath4 from which one can construct the recursive relation @xmath5    equation provides a simple and reasonably efficient way to compute the list of values @xmath6 .",
    "alternatively , applying fft - based power series inversion to the right - hand side of gives an asymptotically faster , essentially optimal algorithm for the same set of values .",
    "an attractive feature of euler s method , in both the recursive and fft incarnations , is that the values can be computed more efficiently modulo a small prime number .",
    "this is useful for investigating partition function congruences , such as in a recent large - scale computation of @xmath0 modulo small primes for @xmath3 up to @xmath7 @xcite .    while efficient for computing @xmath0 for all @xmath3 up to some limit , euler s formula is impractical for evaluating @xmath0 for an isolated , large @xmath3 .",
    "one of the most astonishing number - theoretical discoveries of the 20th century is the hardy - ramanujan - rademacher formula , first given as an asymptotic expansion by hardy and ramanujan in 1917 @xcite and subsequently refined to an exact representation by rademacher in 1936 @xcite , which provides a direct and computationally efficient expression for the single value @xmath0 .    simplified to a first - order estimate , the hardy - ramanujan - rademacher formula states that @xmath8 from which one gathers that @xmath0 is a number with roughly @xmath9 decimal digits .",
    "the full version can be stated as @xmath10\\right)\\end{aligned}\\ ] ] where @xmath11 is the dedekind sum @xmath12 and where the remainder satisfies @xmath13 with @xmath14    it is easily shown that @xmath15 for every positive @xmath16 .",
    "rademacher s bound therefore implies that @xmath17 terms in suffice to compute @xmath0 exactly by forcing @xmath18 and rounding to the nearest integer .",
    "for example , we can take @xmath19 when @xmath20 .",
    "in fact , it was pointed out by odlyzko @xcite that the hardy - ramanujan - rademacher formula `` gives an algorithm for calculating @xmath0 that is close to optimal , since the number of bit operations is not much larger than the number of bits of @xmath0 '' . in other words",
    ", the time complexity should not be much higher than the trivial lower bound @xmath21 derived from just for writing down the result .",
    "odlyzko s claim warrants some elaboration , since the hardy - ramanujan - rademacher formula ostensibly is a triply nested sum containing @xmath22 inner terms .",
    "the computational utility of the hardy - ramanujan - rademacher formula was , of course , realized long before the availability of electronic computers .",
    "for instance , lehmer @xcite used it to verify ramanujan s conjectures @xmath23 and @xmath24 .",
    "implementations are now available in numerous mathematical software systems , including pari / gp @xcite , mathematica @xcite and sage @xcite .",
    "however , apart from odlyzko s remark , we find few algorithmic accounts of the hardy - ramanujan - rademacher formula in the literature , nor any investigation into the optimality of the available implementations .",
    "the present paper describes a new c implementation of the hardy - ramanujan - rademacher formula .",
    "the code is freely available as a component of the fast library for number theory ( flint ) @xcite , released under the terms of the gnu general public license .",
    "we show that the complexity for computing @xmath0 indeed can be bounded by @xmath1 , and observe that our implementation comes close to being optimal in practice , improving on the speed of previously published software by more than two orders of magnitude .",
    "we benchmark the code by computing some extremely large isolated values of @xmath0 .",
    "we also investigate efficiency compared to power series methods for evaluation of multiple values , and finally apply our implementation to the problem of computing congruences for @xmath0 .",
    "a naive implementation of formulas  requires @xmath22 integer operations to evaluate dedekind sums , and @xmath25 numerical evaluations of complex exponentials ( or cosines , since the imaginary parts ultimately cancel out ) . in the following section ,",
    "we describe how the number of integer operations and cosine evaluations can be reduced , for the moment ignoring numerical evaluation .",
    "a first improvement , used for instance in the sage implementation , is to recognize that dedekind sums can be evaluated in @xmath26 steps using a gcd - style algorithm , as described by apostol @xcite , or with knuth s fraction - free algorithm @xcite which avoids the overhead of rational arithmetic .",
    "this reduces the total number of integer operations to @xmath27 , which is a dramatic improvement but still leaves the cost of computing @xmath0 quadratic in the size of the final result .    fortunately , the @xmath28 sums have additional structure as discussed in @xcite , allowing the computational complexity to be reduced .",
    "since numerous implementers of the hardy - ramanujan - rademacher formula until now appear to have overlooked these results , it seems appropriate that we reproduce the main formulas and assess the computational issues in more detail .",
    "we describe two concrete algorithms : one simple , and one asymptotically fast , the latter being implemented in flint .      using properties of the dedekind eta function",
    ", one can derive the formula ( which whiteman @xcite attributes to selberg ) @xmath29 in which the summation ranges over @xmath30 and only @xmath31 terms are nonzero . with a simple brute force search for solutions of the quadratic equation",
    ", this representation provides a way to compute @xmath28 that is both simpler and more efficient than the usual definition .",
    "although a brute force search requires @xmath32 loop iterations , the successive quadratic terms can be generated without multiplications or divisions using two coupled linear recurrences .",
    "this only costs a few processor cycles per loop iteration , which is a substantial improvement over computing dedekind sums , and means that the cost up to fairly large @xmath33 effectively will be dominated by evaluating @xmath31 cosines , adding up to @xmath34 function evaluations for computing @xmath0 .",
    "integers @xmath35 @xmath36 , where @xmath28 is defined as in @xmath33 @xmath37 @xmath38 @xmath39 @xmath40 * if * @xmath41 * then * @xmath42 @xmath43 * if * @xmath44 * then * @xmath45 @xmath46    a basic implementation of is given as algorithm [ alg : simplesum ] . here",
    "the variable @xmath47 runs over the successive values of @xmath48 , and @xmath49 runs over the differences between consecutive @xmath47 .",
    "various improvements are possible : a modification of the equation allows cutting the loop range in half when @xmath33 is odd , and the number of cosine evaluations can be reduced by counting the multiplicities of unique angles after reduction to @xmath50 , evaluating a weighted sum @xmath51 at the end  possibly using trigonometric addition theorems to exploit the fact that the differences @xmath52 between successive angles tend to repeat for many different @xmath53 .      from selberg s formula , a still more efficient but considerably more complicated multiplicative decomposition of @xmath28 can be obtained .",
    "the advantage of this representation is that it only contains @xmath26 cosine factors , bringing the total number of cosine evaluations for @xmath0 down to @xmath54 .",
    "it also reveals exactly when @xmath55 ( which is about half the time ) .",
    "we stress that these results are not new ; the formulas are given in full detail and with proofs in @xcite .",
    "first consider the case when @xmath33 is a power of a prime .",
    "clearly @xmath56 and @xmath57 .",
    "otherwise let @xmath58 and @xmath59 . then , using the notation @xmath60 for jacobi symbols to avoid confusion with fractions , we have @xmath61 where @xmath62 , @xmath63 and @xmath64 respectively are any solutions of @xmath65 provided , when @xmath66 , that such an @xmath64 exists and that @xmath67 . if , on the other hand , @xmath66 and either of these two conditions do not hold , we have @xmath68    if @xmath33 is not a prime power , assume that @xmath69 where @xmath70 .",
    "then we can factor @xmath28 as @xmath71 , where @xmath72 are any solutions of the following equations .",
    "if @xmath73 , then @xmath74 if @xmath75 , then @xmath76 and if @xmath77 is odd or divisible by 8 , then @xmath78 where @xmath79 , @xmath80 , @xmath81 .",
    "here @xmath82 denotes an operation done on integers , rather than a modular division . all other solving steps in  amount to computing greatest common divisors , carrying out modular ring operations , finding modular inverses , and computing modular square roots .",
    "repeated application of these formulas results in algorithm [ alg : fastsum ] , where we omit the detailed arithmetic for brevity .",
    "integers @xmath83 , @xmath84 @xmath36 , where @xmath28 is defined as in compute the prime factorization @xmath85 @xmath86 @xmath87 compute @xmath72 by solving the respective case of ",
    "@xmath88 @xmath89 @xmath90 @xmath91      a precise complexity analysis of algorithm [ alg : fastsum ] should take into account the cost of integer arithmetic .",
    "multiplication , division , computation of modular inverses , greatest common divisors and jacobi symbols of integers bounded in absolute value by @xmath32 can all be performed with bit complexity @xmath92 .    at first sight",
    ", integer factorization might seem to pose a problem .",
    "we can , however , factor all indices @xmath33 summed over in in @xmath93 bit operations .",
    "for example , using the sieve of eratosthenes , we can precompute a list of length @xmath9 where entry @xmath33 is the largest prime dividing @xmath33 .",
    "a fixed index @xmath33 is a product of at most @xmath26 prime powers with exponents bounded by @xmath26 . for each prime power",
    ", we need @xmath94 operations with roughly the cost of multiplication , and @xmath94 square roots , which are the most expensive operations .    to compute square roots modulo @xmath95",
    ", we can use the tonelli - shanks algorithm @xcite or cipolla s algorithm @xcite modulo @xmath96 followed by hensel lifting up to @xmath95 . assuming that we know a quadratic nonresidue modulo @xmath96 , the tonelli - shanks algorithm requires @xmath97 multiplications in the worst case and @xmath98 multiplications on average , while cipolla s algorithm requires @xmath98 multiplications in the worst case @xcite .",
    "this puts the bit complexity of factoring a single exponential sum @xmath28 at @xmath99 , and gives us the following result :    [ thm : modular ] assume that we know a quadratic nonresidue modulo @xmath96 for all primes @xmath96 up to @xmath9 .",
    "then we can factor all the @xmath28 required for evaluating @xmath0 using @xmath100 bit operations .",
    "the assumption in theorem [ thm : modular ] can be satisfied with a precomputation that does not affect the complexity . if @xmath101 denotes the least quadratic nonresidue modulo the @xmath33th prime number , it is a theorem of erds @xcite that as @xmath102 , @xmath103 given the primes up to @xmath104 , we can therefore build a table of nonresidues by testing no more than @xmath105 candidates .",
    "since @xmath106 and a quadratic residue test takes @xmath107 time , the total precomputation time is @xmath108 .    in practice ,",
    "it is sufficient to generate nonresidues on the fly since @xmath94 candidates need to be tested on average , but we can only prove an @xmath109 bound for factoring an isolated @xmath28 by assuming the extended riemann hypothesis which gives @xmath110 @xcite .      as a matter of practical efficiency , the modular arithmetic should be done with as little overhead as possible .",
    "flint provides optimized routines for arithmetic with moduli smaller than 32 or 64 bits ( depending on the hardware word size ) which are used throughout ; including , among other things , a binary - style gcd algorithm , division and remainder using precomputed inverses , and supplementary code for operations on two - limb ( 64 or 128 bit ) integers .",
    "we note that since @xmath111 , we can always reduce @xmath3 modulo @xmath33 , and perform all modular arithmetic with moduli up to some small multiple of @xmath33 . in principle , the implementation of the modular arithmetic in flint thus allows calculating @xmath0 up to approximately @xmath112 on a 64-bit system , which roughly equals the limit on @xmath3 imposed by the availability of addressable memory to store @xmath0 .    at present",
    ", our implementation of algorithm [ alg : fastsum ] simply calls the flint routine for integer factorization repeatedly rather than sieving over the indices .",
    "although convenient , this technically results in a higher total complexity than @xmath1 .",
    "however , the code for factoring single - word integers , which uses various optimizations for small factors and hart s `` one line factor '' variant of lehman s method to find large factors @xcite , is fast enough that integer factorization only accounts for a small fraction of the running time for any feasible @xmath3 .",
    "if needed , full sieving could easily be added in the future .",
    "likewise , the square root function in flint uses the tonelli - shanks algorithm and generates a nonresidue modulo @xmath96 on each call .",
    "this is suboptimal in theory but efficient enough in practice .",
    "we now turn to the problem of numerically evaluating  using arbitrary - precision arithmetic , given access to algorithm [ alg : fastsum ] for symbolically decomposing the @xmath28 sums . although bounds the truncation error in the hardy - ramanujan - rademacher series , we must also account for the effects of having to work with finite - precision approximations of the terms .",
    "we assume the use of variable - precision binary floating - point arithmetic ( a simpler but less efficient alternative , avoiding the need for detailed manual error bounds , would be to use arbitrary - precision interval arithmetic ) .",
    "basic notions about floating - point arithmetic and error analysis can be found in @xcite .    if the precision is @xmath49 bits , we let @xmath113 denote the unit roundoff .",
    "we use the symbol @xmath114 to signify a floating - point approximation of an exact quantity @xmath115 , having some relative error @xmath116 when @xmath117 .",
    "if @xmath114 is obtained by rounding @xmath115 to the nearest representable floating - point number ( at most 0.5 ulp error ) at precision @xmath49 , we have @xmath118 . except where otherwise noted , we assume correct rounding to nearest .",
    "a simple strategy for computing @xmath0 is as follows . for a given @xmath3",
    ", we first determine an @xmath119 such that @xmath120 , for example using a linear search . a tight upper bound for @xmath121",
    "can be computed easily using low - precision arithmetic .",
    "we then approximate the @xmath33th term @xmath122 using a working precision high enough to guarantee    @xmath123    and perform the outer summation such that the absolute error of each addition is bounded by @xmath124 .",
    "this clearly guarantees @xmath125 , allowing us to determine the correct value of @xmath0 by rounding to the nearest integer .",
    "we might , alternatively , carry out the additions exactly and save one bit of precision for the terms .",
    "in what follows , we derive a simple but essentially asymptotically tight expression for a working precision , varying with @xmath33 , sufficiently high for to hold . using algorithm  [ alg : fastsum ] ,",
    "we write the term to be evaluated in terms of exact integer parameters @xmath126 as @xmath127    let @xmath128 , @xmath129 and let @xmath49 be a precision in bits with @xmath130 .",
    "suppose that @xmath131 and @xmath132 can be evaluated on @xmath133 with relative error at most @xmath134 for floating - point input , and suppose that @xmath135 can be approximated with relative error at most @xmath136 .",
    "then we can evaluate @xmath137 with relative error less than @xmath138 .",
    "[ thm : cos_error ]    we first reduce @xmath96 and @xmath139 with exact integer operations so that @xmath140 , giving an angle in the interval @xmath141",
    ". then we approximate @xmath142 using three roundings , giving @xmath143 where @xmath144 .",
    "the assumption @xmath145 gives @xmath146 and therefore also @xmath147 .",
    "next , we evaluate @xmath148 where @xmath149 or @xmath150 depending on the argument reduction . by taylor s theorem",
    ", we have @xmath151 where @xmath152 for some @xmath153 between @xmath115 and @xmath114 , giving @xmath154 . finally , rounding results in @xmath155 where @xmath156 .",
    "the inequality @xmath157 gives @xmath158 .    to obtain a simple error bound for @xmath159 where @xmath160",
    ", we make the somewhat crude restriction that @xmath161 .",
    "we also assume @xmath162 and @xmath163 , which are not restrictions : if @xmath119 is chosen optimally using rademacher s remainder bound , the maximum @xmath33 decreases and the minimum @xmath115 increases with larger @xmath3 . in particular , @xmath161 is sufficient with rademacher s bound ( or any tighter bound for the remainder ) .",
    "we assume that @xmath164 is precomputed ; of course , this only needs to be done once during the calculation of @xmath0 , at a precision a few bits higher than that of the @xmath165 term .",
    "suppose @xmath161 and let @xmath49 be a precision in bits such that @xmath166",
    ". let @xmath160 where @xmath164 is defined as in and where @xmath33 is constrained such that @xmath162 and @xmath163 .",
    "assume that @xmath167 has been precomputed with @xmath168 and that @xmath169 and @xmath170 can be evaluated with relative error at most @xmath134 for floating - point input .",
    "then we can evaluate @xmath159 with relative error at most @xmath171 .",
    "[ thm : u_error ]    we first compute @xmath172 where @xmath173 .",
    "next , we compute @xmath174 where we have to bound the error @xmath175 propagated in the composition as well as the rounding error @xmath176 in the evaluation of @xmath177 . using the inequality @xmath178 , we have @xmath179    evaluating @xmath177 using the obvious sequence of operations results in @xmath180 where @xmath181 and @xmath182 where @xmath183 .",
    "this expression is maximized by setting @xmath114 as small as possible and taking @xmath184 , which gives    @xmath185    expanding using and gives @xmath186 .",
    "finally , we obtain @xmath187 by a direct application of the assumptions .",
    "put together , assuming floating - point implementations of standard transcendental functions with at most 1  ulp error ( implying a relative error of at most @xmath188 ) , correctly rounded arithmetic and the constant @xmath135 , we have :    let @xmath161 .",
    "for to hold , it is sufficient to evaluate using a precision of @xmath189 bits .",
    "[ thm : prec ]    we can satisfy the assumptions of lemmas [ thm : cos_error ] and [ thm : u_error ] . in particular , @xmath190 . the top - level arithmetic operations in , including the square roots , amount to a maximum of @xmath191 roundings .",
    "lemmas [ thm : cos_error ] and [ thm : u_error ] and elementary inequalities give the relative error bound @xmath192 the result follows by taking logarithms in .",
    "to make theorem [ thm : prec ] effective , we can use @xmath193 and bound @xmath194 using with @xmath195 and @xmath196 , giving @xmath197 naturally , for @xmath198 , the same precision bound can be verified to be sufficient through direct computation .",
    "we can even reduce overhead for small @xmath3 by using a tighter precision , say @xmath199 , up to some limit small enough to be tested exhaustively ( perhaps much larger than 2000 ) . the requirement that @xmath200 always holds in practice if we set a minimum precision ; for @xmath3 feasible on present hardware , it is sufficient to never drop below ieee double ( @xmath201-bit ) precision .",
    "we assume that @xmath49-bit floating - point numbers can be multiplied in time @xmath202 .",
    "it is well known ( see @xcite ) that the elementary functions exp , log , sin etc . can be evaluated in time @xmath203 using methods based on the arithmetic - geometric mean ( agm ) .",
    "a popular alternative is binary splitting , which typically has cost @xmath204 but tends to be faster than the agm in practice .    to evaluate @xmath0 using the hardy - ramanujan - rademacher formula",
    ", we must add @xmath17 terms each of which can be written as a product of @xmath26 factors . according to and the error analysis in the previous section",
    ", the @xmath33th term needs to be evaluated to a precision of @xmath205 bits .",
    "using any combination of @xmath206 algorithms for elementary functions , the complexity of the numerical operations is @xmath207 which is nearly optimal in the size of the output .",
    "combined with the cost of the factoring stage , the complexity for the computation of @xmath0 as a whole is therefore , when properly implemented , softly optimal at @xmath1 . from with the best known complexity bound for elementary functions ,",
    "we obtain :    the value @xmath0 can be computed in time @xmath208 .",
    "a subtle but crucial detail in this analysis is that the additions in the main sum must be implemented in such a way that they have cost @xmath209 rather than @xmath17 , since the latter would result in an @xmath25 total complexity .",
    "if the additions are performed in - place in memory , we can perform summations the natural way and rely on carry propagation terminating in an expected @xmath94 steps , but many implementations of arbitrary - precision floating - point arithmetic do not provide this optimization .",
    "one way to solve this problem is to add the terms in reverse order , using a precision that matches the magnitude of the partial sums . or ,",
    "if we add the terms in forward order , we can amortize the cost by keeping separate summation variables for the partial sums of terms not exceeding @xmath210 bits .",
    "flint uses the mpir library , derived from gmp , for arbitrary - precision arithmetic , and the mpfr library on top of mpir for asymptotically fast arbitrary - precision floating - point numbers and correctly rounded transcendental functions @xcite .",
    "thanks to the strong correctness guarantees of mpfr , it is relatively straightforward to write a provably correct implementation of the partition function using theorem [ thm : prec ] .",
    "although the default functions provided by mpfr are quite fast , order - of - magnitude speedups were found possible with custom routines for parts of the numerical evaluation .",
    "an unfortunate consequence is that our implementation currently relies on routines that , although heuristically sound , have not yet been proved correct , and perhaps are more likely to contain implementation bugs than the well - tested standard functions in mpfr .",
    "all such heuristic parts of the code are , however , well isolated , and we expect that they can be replaced with rigorous versions with equivalent or better performance in the future .      inspired by the sage implementation , which was written by jonathan bober , our implementation switches to hardware ( ieee double ) floating - point arithmetic to evaluate when the precision bound falls below 53 bits .",
    "this speeds up evaluation of the `` long tail '' of terms with very small magnitude .    using hardware arithmetic entails some risk .",
    "although the ieee floating - point standard implemented on all modern hardware guarantees 0.5 ulp error for arithmetic operations , accuracy may be lost , for example , if the compiler generates long - double instructions which trigger double rounding , or if the rounding mode of the processor has been changed .",
    "we need to be particularly concerned about the accuracy of transcendental functions .",
    "the hardware transcendental functions on the intel pentium processor and its descendants guarantee an error of at most 1  ulp when rounding to nearest @xcite , as do the software routines in the portable and widely used fdlibm library @xcite .",
    "nevertheless , some systems may be equipped with poorer implementations .",
    "fortunately , the bound and theorem [ thm : prec ] are lax enough in practice that errors up to a few ulp can be tolerated , and we expect any reasonably implemented double - precision transcendental functions to be adequate .",
    "most importantly , range reducing the arguments of trigonometric functions to @xmath133 avoids catastrophic error for large arguments which is a misfeature of some implementations .",
    "mpfr implements the exponential and hyperbolic functions using binary splitting at high precision , which is asymptotically fast up to logarithmic factors .",
    "we can , however , improve performance by not computing the hyperbolic functions in @xmath159 from scratch when @xmath33 is small .",
    "instead , we precompute @xmath211 with the initial precision of @xmath164 , and then compute @xmath212 from @xmath213 ; that is , by @xmath33th root extractions which have cost @xmath214 . using the builtin mpfr functions ,",
    "root extraction was found experimentally to be faster than evaluating the exponential function up to approximately @xmath215 over a large range of precisions .",
    "for extremely large @xmath3 , we also speed up computation of the constant @xmath164 by using binary splitting to compute @xmath135 ( adapting code written by h. xue @xcite ) instead of the default function in mpfr , which uses arithmetic - geometric mean iteration .",
    "as has been pointed out previously @xcite , binary splitting is more than four times faster for computing @xmath135 in practice , despite theoretically having a @xmath216 factor worse complexity . when evaluating @xmath0 for multiple values of @xmath3 , the value of @xmath135 should of course be cached , which mpfr does automatically .",
    "the mpfr cosine and sine functions implement binary splitting , with similar asymptotics as the exponential function . at high precision , our implementation switches to custom code for evaluating @xmath217 when @xmath139 is not too large , taking advantage of the fact that @xmath218 is an algebraic number .",
    "our strategy consists of generating a polynomial @xmath219 such that @xmath220 and solving this equation using newton iteration , starting from a double precision approximation of the desired root . using a precision that doubles with each step of the newton iteration , the complexity is @xmath221 .",
    "the numbers @xmath137 are computed from scratch as needed : caching values with small @xmath96 and @xmath139 was found to provide a negligible speedup while needlessly increasing memory consumption and code complexity .",
    "our implementation uses the minimal polynomial @xmath222 of @xmath223 , which has degree @xmath224 for @xmath225 @xcite .",
    "more precisely , we use the scaled polynomial @xmath226 $ ] . this polynomial is looked up from a precomputed table when @xmath3 is small , and otherwise is generated using a balanced product tree , starting from floating - point approximations of the conjugate roots . as a side remark ,",
    "this turned out to be around a thousand times faster than computing the minimal polynomial with the standard commands in either sage or mathematica .",
    "we sketch the procedure for high - precision evaluation of @xmath227 as algorithm  [ alg : minpoly ] , omitting various special cases and implementation details ( for example , our implementation performs the polynomial multiplications over @xmath228 $ ] by embedding the approximate coefficients as fixed - point numbers ) .",
    "coprime integers @xmath96 and @xmath139 with @xmath229 , and a precision @xmath49 an approximation of @xmath137 accurate to @xmath49 bits @xmath230 @xmath231 @xmath232 @xmath233 @xmath234 \\phi + \\frac{1}{2}\\right\\rfloor x^k$ ] compute precisions @xmath235",
    "@xmath236 @xmath237 @xmath115    we do not attempt to prove that the internal precision management of algorithm  [ alg : minpoly ] is correct .",
    "however , the polynomial generation can easily be tested up to an allowed bound for @xmath139 , and the function can be tested to be correct for all pairs @xmath238 at some fixed , high precision @xmath49 .",
    "we may then argue heuristically that the well - behavedness of the object function in the root - finding stage combined with the highly conservative padding of the precision by several bits per iteration suffices to ensure full accuracy at the end of each step in the final loop , given an arbitrary @xmath49 .    a different way to generate @xmath222 using chebyshev polynomials",
    "is described in @xcite .",
    "one can also use the squarefree part of an offset chebyshev polynomial @xmath239 directly , although this is somewhat less efficient than the minimal polynomial .    alternatively ,",
    "since @xmath240 is the real part of a root of unity , the polynomial @xmath241 could be used .",
    "the use of complex arithmetic adds overhead , but the method would be faster for large @xmath139 since @xmath242 can be computed in time @xmath243 using repeated squaring .",
    "we also note that the secant method could be used instead of the standard newton iteration in algorithm [ alg : minpoly ] .",
    "this increases the number of iterations , but removes the derivative evaluation , possibly providing some speedup .    in our implementation , algorithm [ alg :",
    "minpoly ] was found to be faster than the mpfr trigonometric functions for @xmath244 roughly when the precision exceeds @xmath245 bits .",
    "this estimate includes the cost of generating the minimal polynomial on the fly .",
    "algorithm [ alg : rademain ] outlines the main routine in flint with only minor simplifications .",
    "to avoid possible corner cases in the convergence of the hrr sum , and to avoid unnecessary overhead , values with @xmath246 ( exactly corresponding to @xmath247 ) are looked up from a table .",
    "we only use @xmath33 , @xmath3 , @xmath119 in theorem [ thm : prec ] in order to make the precision decrease uniformly , allowing amortized summation to be implemented in a simple way .",
    "@xmath248 @xmath0 determine @xmath119 and initial precision @xmath249 using theorem [ thm : prec ] @xmath250",
    "@xmath251 @xmath252 write term @xmath33 as by calling algorithm [ alg : fastsum ] determine term precision @xmath253 for @xmath194 using theorem [ thm : prec ] @xmath254 @xmath255 @xmath256 @xmath257 @xmath258 @xmath259",
    "@xmath260    since our implementation presently relies on some numerical heuristics ( and in any case , considering the intricacy of the algorithm ) , care has been taken to test it extensively .",
    "all @xmath261 have been checked explicitly , and a large number of isolated @xmath262 have been compared against known congruences and values computed with sage and mathematica .    as a strong robustness check ,",
    "we observe experimentally that the numerical error in the final sum decreases with larger @xmath3 .",
    "for example , the error is consistently smaller than @xmath263 for @xmath264 and smaller than @xmath265 for @xmath266 .",
    "this phenomenon reflects the fact that overshoots the actual magnitude of the terms with large @xmath33 , combined with the fact that rounding errors average out pseudorandomly rather than approaching worst - case bounds .",
    "table [ tab : timings ] and figure [ fig : plot ] compare performance of mathematica 7 , sage 4.7 and flint on a laptop with a pentium t4400 2.2 ghz cpu and 3 gib of ram , running 64-bit linux . to the author s knowledge , mathematica and sage contain the fastest previously available partition function implementations by far .",
    "the flint code was run with mpir version 2.4.0 and mpfr version 3.0.1 .",
    "since sage 4.7 uses an older version of mpir and mathematica is based on an older version of gmp , differences in performance of the underlying arithmetic slightly skew the comparison , but probably not by more than a factor two .",
    "the limited memory of the aforementioned laptop restricted the range of feasible @xmath3 to approximately @xmath267 . using a system with an amd opteron 6174 processor and 256 gib ram allowed calculating @xmath268 , @xmath269 and @xmath2 as well .",
    "the last computation took just less than 100 hours and used more than 150 gib of memory , producing a result with over 11 billion bits .",
    "some large values of @xmath0 are listed in table [ tab : values ] .",
    ".timings for computing @xmath0 in mathematica 7 , sage 4.7 and flint up to @xmath270 on the same system , as well as flint timings for @xmath271 to @xmath272 ( * ) done on different ( slightly faster ) hardware .",
    "calculations running less than one second were repeated , allowing benefits from data caching .",
    "the rightmost column shows the amount of time in the flint implementation spent computing the first term . [ cols=\"^,^,^,^,^\",options=\"header \" , ]",
    "two obvious improvements to our implementation would be to develop a rigorous , and perhaps faster , version of algorithm [ alg : minpoly ] for computing @xmath137 to high precision , and to develop fast multithreaded implementations of transcendental functions to allow computing @xmath0 for much larger @xmath3 .",
    "curiously , a particularly simple agm - type iteration is known for @xmath273 ( see @xcite ) , and it is tempting to speculate whether a similar algorithm can be constructed for @xmath274 , allowing faster evaluation of the first term .    some performance could also be gained with faster low - precision transcendental functions ( up to a few thousand bits ) and by using a better bound than for the truncation error .",
    "the algorithms described in this paper can be adapted to evaluation of other hrr - type series , such as the number of partitions into distinct parts    @xmath275    using asymptotically fast methods for numerical evaluation of hypergeometric functions , it should be possible to retain quasi - optimality .",
    "finally , it remains an open problem whether there is a fast way to compute the isolated value @xmath0 using purely algebraic methods .",
    "we mention the interesting recent work by bruinier and ono @xcite , which perhaps could lead to such an algorithm .",
    "the author thanks silviu radu for suggesting the application of extending weaver s table of congruences , and for explaining weaver s algorithm in detail . the author also thanks the anonymous referee for various suggestions , and jonathan bober for pointing out that erds theorem about quadratic nonresidues gives a rigorous complexity bound without assuming the extended riemann hypothesis .    finally , william hart gave valuable feedback on various issues , and generously provided access to the computer hardware used for the large - scale computations reported in this paper .",
    "the hardware was funded by hart s epsrc grant ep / g004870/1 ( in algebraic number theory ) and hosted at the university of warwick .",
    "a. fousse , g. hanrot , v. lefvre , p. plissier and p. zimmermann , _ mpfr : a multiple - precision binary floating - point library with correct rounding _ , acm transactions on mathematical software , vol .",
    "33 , issue 2 ( 2007 ) , http://www.mpfr.org/    a. odlyzko , _ asymptotic enumeration methods _ , in _ handbook of combinatorics _ vol . 2 , r. graham , m. grtschel , l. lovsz ( editors ) , elsevier , the netherlands ( 1995 ) pp . 10631229 ."
  ],
  "abstract_text": [
    "<S> we describe how the hardy - ramanujan - rademacher formula can be implemented to allow the partition function @xmath0 to be computed with softly optimal complexity @xmath1 and very little overhead . </S>",
    "<S> a new implementation based on these techniques achieves speedups in excess of a factor 500 over previously published software and has been used by the author to calculate @xmath2 , an exponent twice as large as in previously reported computations .    </S>",
    "<S> we also investigate performance for multi - evaluation of @xmath0 , where our implementation of the hardy - ramanujan - rademacher formula becomes superior to power series methods on far denser sets of indices than previous implementations . as an application , we determine over 22 billion new congruences for the partition function , extending weaver s tabulation of 76,065 congruences . </S>"
  ]
}