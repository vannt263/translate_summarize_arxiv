{
  "article_text": [
    "there was clearly a time , perhaps not too far in the recent past , when bayesian methods were considered `` beyond the pail '' by frequentist statisticians .",
    "but bayesian methods have been resurgent in recent years , to the extent that few statisticians have no interest in them , even if they do not buy the complete philosophical package .    in this article",
    "i summarize my perspective on the role of bayesian methods in statistics , borrowing from a more extensive discussion in little ( @xcite ) .",
    "i then provide a brief overview of bayesian inference for missing data problems , both modeling and ignoring the missing data mechanism , and multiple imputation ( mi ) , an important practical tool for dealing with missing data that has a bayesian etiology . finally , i give some examples of bayesian missing - data methods which i believe frequentists could profitably add to their analytical toolkit .",
    "bayesian methods are particularly useful for handling missing data problems in statistics .",
    "incomplete data problems are readily amenable to likelihood - based methods , since they do not require a rectangular data matrix",
    ". maximum likelihood ( ml ) is an important approach , but the loglikelihoods corresponding to missing data problems are typically more complex than likelihoods for complete data , deviate more from the quadratic approximations that underlie asymptotic inferences , and are subject to under - identification or weak identification of parameters . consequently , ml requires iterative calculations , information matrix - based standard errors are often difficult to compute in high - dimensional problems , and asymptotic ml inferences can have serious deficiencies , particularly with small fragmentary samples .",
    "in contrast , draws from the bayesian posterior distribution can often be computed using direct simulation or markov chain monte carlo techniques , and these provide estimates of standard errors without the need to compute and invert the information matrix .",
    "the inferences based on the posterior distribution often have better frequentist properties than asymptotic inferences based on ml .",
    "furthermore , multiple imputations can be generated as a byproduct of bayesian calculations , and provide a  practically useful and flexible tool for solving missing data problems .",
    "these points are further developed in the material that follows .",
    "the statistics world is still largely divided into frequentists , who base inference for an unknown parameter @xmath0 on hypothesis tests or confidence intervals from the distribution of statistics in repeated sampling , and bayesians , who formulate a model for the data and prior distribution for unknown parameters , and base inferences for unknowns on posterior distributions .",
    "bayesians are also `` subjective , '' as when proper priors are elicited , and `` objective , '' as when conventional `` reference priors '' are adopted .",
    "both these facets of the bayesian paradigm have useful roles , depending on context .",
    "asymptotic maximum likelihood inference can be seen as a form of large sample bayes , with the interval for @xmath0 being interpreted as a posterior credibility interval rather than a confidence interval .",
    "i believe both systems of statistical inference have strengths and weaknesses and , hence , the best course is to seek a compromise that combines them in a way that capitalizes on their strengths .",
    "the bayesian paradigm is best suited for making statistical inference under an assumed model . indeed , under full probability modeling",
    ", with prior distributions assigned to parameters , the bayes theorem is indeed a  theorem  the path determined by probability theory to inference about unknowns given the data , whether the targets are parameters or predictions of unobserved quantities .",
    "the frequentist approach , on the other hand , has various well - known limitations regarding inference under an assumed model .",
    "first , it is not prescriptive : frequentist theory seems more like a set of concepts for assessing properties of inference procedures , rather than an inferential system _ per se_. under an agreed model , and assuming large samples , there is a relatively prescriptive path to inference based on maximum likelihood ( ml ) estimates and their large - sample distribution .",
    "however , other frequentist approaches are entertained in practice , like generalized estimating equations , based on robustness or other considerations . also , there is no prescriptive frequentist approach to small sample problems .",
    "indeed , for many problems , such as the behrens  fisher problem of comparing two means of normal distributions with different unknown variances , no procedure exists that has exact repeated - sampling properties , such as exact nominal confidence coverage for all values of the unknown parameter .",
    "bayesian methods provide exact frequentist coverage for some complete - data problems  this occurs , in particular , in problems where the bayesian and frequentist inferences are the same , as in @xmath1 inference for normal multiple regression with a uniform prior on the regression coefficients and log variance . for more complex problems , including problems with missing data ,",
    "bayesian methods do not generally provide exact frequentist coverage , but they often improve on ml by providing better small - sample inferences , perhaps because bayesian model shrinkage moderates inferences based on extreme parameters estimates . as just one example , consider the adjustment of estimates for categorical data motivated by bayesian ideas ( agresti , @xcite ) .",
    "the bayesian approach is prescriptive in the sense that , once a model and prior distribution are specified , there is a clear path to inferences based on the posterior distribution , or optimal estimates for a given choice of loss function .",
    "there is no prescription for choosing the model and prior distribution  that is what makes applied statistics interesting  but certain `` reference '' prior distributions for complete - data problems can be expected to yield good frequentist properties when applied to missing data problems ; see , for example , little ( @xcite ) .",
    "frequentist inference violates the likelihood principle , and is ambiguous about whether to condition on ancillary or approximately ancillary statistics when performing repeated sampling calculations .",
    "little ( @xcite ) provides more discussion , with examples .",
    "an attractive feature of bayesian methods in complete - data or missing - data problems is the treatment of nuisance parameters .",
    "bayesian inference integrates over these parameters , rather than fixing them at their ml estimates .",
    "this tends to yield inferences with improved frequentist properties , since the uncertainty about these parameters is taken into account .",
    "for example , for complete or incomplete data problems , restricted ml , which integrates over location parameters , is generally viewed as superior to ml , which maximizes them .    if we were handed the model on a plate and told to do inference for unknowns , then bayesian statistics is the clear winner .",
    "the problem , of course , is that we never know the true model .",
    "bayesian inference requires and relies on a high degree of model specification ( efron , @xcite)full specification of a  likelihood and prior .",
    "developing a good model is challenging , particularly in complex problems .",
    "furthermore , all models are wrong , and bad models lead to bad answers : under the frequentist paradigm , the search for procedures with good frequentist properties provides some degree of protection against model misspecification , but there seems no such built - in protection under a strict bayesian paradigm where frequentist calculations are not entertained .",
    "good principles for picking models are essential , and here i feel frequentist methods have an important role .",
    "we want models that yield inferences with good frequentist properties , such as 95% credibility intervals that cover the unknown parameter 95% of the time if the procedure was applied to repeated samples .",
    "the bayesian has some tools for model development and checking , like bayes factors and model averaging , but bayesian hypothesis testing has well known problems , and , in my view , frequentist approaches are essential when it comes to model development and assessment .    to summarize , bayesian statistics is strong for inference under an assumed model , but relatively weak for the development and assessment of models .",
    "frequentist statistics provides useful tools for model development and assessment , but has weaknesses for inference under an assumed model .",
    "if this summary is accepted , then the natural compromise is to use frequentist methods for model development and assessment , and bayesian methods for inference under a model .",
    "this capitalizes on the strengths of both paradigms , and is the essence of the approach known as calibrated bayes ( cb ) .",
    "many statisticians have advanced cb ideas ( e.g. , peers , @xcite ; welch , @xcite ; dawid , @xcite ) , but i was particularly influenced by seminal papers by box ( @xcite ) and rubin ( @xcite ) .",
    "box ( @xcite ) wrote ,    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ `` i believe that@xmath2 sampling theory is needed for exploration and ultimate criticism of the entertained model in the light of the current data , while bayes theory is needed for estimation of parameters conditional on adequacy of the model . ''",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    he based his implementation of this idea on the factorization : latexmath:[\\[p(y,\\theta |\\mathrm{model } ) = p(y|\\mathrm{model})p(\\theta    posterior distribution of the parameter @xmath0 given data  @xmath4 and model , and is the basis for inference , and the first term on the right side is the marginal distribution of the data @xmath4 under the model , and is used to assess the validity of the model , with the aid of frequentist considerations .",
    "specifically , discrepancy functions of the observed data @xmath5 are assessed from the perspective of realizations from their marginal distribution @xmath6 .",
    "a questionable feature of this `` prior predictive checking '' is that checks are sensitive to the choice of prior distribution even when this choice has limited impact on the posterior inference ; in particular , it leads to problems with assessment of models involving noninformative priors .",
    "rubin ( @xcite ) wrote ,    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ `` the applied statistician should be bayesian in principle and calibrated to the real world in practice  appropriate frequency calculations help to define such a tie@xmath2 frequency calculations are useful for making bayesian statements scientific , scientific in the sense of capable of being shown wrong by empirical test ; here the technique is the calibration of bayesian probabilities to the frequencies of actual events . ''",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    rubin ( @xcite ) advocated model checking based on a different distribution , namely , @xmath7 , the predictive distribution of future data @xmath8 and parameter values @xmath9 given the model and observed data @xmath4 .",
    "this leads to posterior predictive checks ( rubin , @xcite ; gelman , meng and stern , @xcite ) , which extend frequentist checking methods by not limiting attention to checking statistics that have a known distribution under the model .",
    "these checks involve an amalgam of bayesian and frequentist ideas , but are clearly frequentist in spirit in that they concern embedding the observed data within a sequence of unobserved data sets that could have been generated under the model , and seeing whether the observed data are `` reasonable . ''",
    "philosophy aside , perhaps the main reason why bayesian methods have flourished in recent years is the development of powerful computational tools ,  like the gibbs sampler and other markov chain monte carlo ( mcmc ) methods .",
    "these , together with gains in computing power , have made it computationally feasible to carry out the high - dimensional integrations required .",
    "an important early breakthrough in mcmc methods actually occurred for a missing data problem , as i discuss in example  [ ex2 ] below .",
    "even if frequentists are completely against bayes , they can apply these bayesian computational tools with weak prior distributions , and interpret results as approximations to ml , with similar asymptotic properties .",
    "i divide the development of missing data methods in statistics into four eras :      early missing data methods involved complete - case analysis , that is , simply discarding data with any values missing , or simple imputation methods , which filled in missing values with best estimates and analyzed the filled - in data .",
    "the latter approach was developed quite extensively in the case of analysis of variance with missing outcomes , which were imputed to maintain a balanced design and hence an easily inverted design matrix ( see little and rubin , @xcite , chapter  2 ) .",
    "these ingenious methods are now mainly of historical interest , since inverting the design matrix corresponding to the unbalanced data is not a big problem given advances in modern computing .",
    "ml methods were developed for some simple missing data problems , notably bivariate normal data with missing values on one variable , which anderson ( @xcite ) solved noniteratively by factoring the likelihood ( see example  [ ex1 ] below ) .",
    "ml for complex problems was iterative and generally too hard given limits of computation , although progress was made for contingency tables ( hartley , @xcite ) and normal models ( hartley and hocking , @xcite ) .",
    "ml methods became popular and feasible in the mid-1970s with the development of the expectation  maximization ( em ) algorithm .",
    "em builds a link with complete - data ml and is simple to program in several important multivariate models , including the multivariate normal model with a general pattern of missing values . the term em was coined in the famous paper by dempster , laird and rubin ( @xcite ) , which established some key properties of the method , including the fact that the likelihood does not decrease at each iteration .",
    "the em algorithm had been previously discovered several times for particular models ( e.g. , , @xcite ; hartley , @xcite ; baum et al . , @xcite ) , and had been formulated in some generality by orchard and woodbury ( @xcite ) and sundberg ( @xcite ) . the simplicity and versatility of em motivated extensions of em to handle more difficult problems , and applications to a  variety of complete - data models for categorical and continuous data , as reviewed in little and rubin ( @xcite ) , mclachlan and krishnan ( @xcite ) and meng and van dyk ( @xcite ) . for generalizations of the em idea ,",
    "see lange ( @xcite ) .",
    "another important development in this era was the formulation of models for the missing data mechanism , and associated sufficient conditions for when the missing data mechanism can be ignored for frequentist and bayesian inference ( rubin , @xcite ) .",
    "the transition from ml to bayesian methods in the missing data setting was initiated by tanner and wong ( @xcite ) , who described data augmentation to generate the posterior distribution of the parameters of the multivariate normal model with missing data .",
    "data augmentation is closely related to the gibbs sampler , as discussed below .",
    "another important development was rubin s ( @xcite ) proposal to handle missing data in public use data sets by multiple imputation ( mi ) , motivated by bayesian ideas . in its infancy , this proposal seemed very exotic and computationally impractical  not any more ! mcmc facilitates bayesian multiple imputation , and is now implemented in publicly - available software for the convenience of users of both bayesian and frequentist persuasions .",
    "model - based missing data methods are potentially vulnerable to model misspecification , although they tend to outperform nave methods even when the model is misspecified ( e.g. , little , @xcite ) .",
    "modern interest in limiting effects of model misspecification by adopting robust procedures has extended to missing data problems , notably with `` doubly robust '' procedures based on augmented inverse - probability weighted estimating equations ( robins , rotnitsky and zhao , @xcite ) . from a more directly model - based bayesian perspective , robustness takes the form of developing models that make relatively weak structural assumptions .",
    "a  method based on one such model , penalized spline of propensity prediction ( pspp , little and an , @xcite ) , is discussed in example  [ ex4 ] below .",
    "another aspect of robustness concerns has been more interest in model checks for standard missing data models ( gelman et al . , @xcite ) .",
    "i now sketch the likelihood and bayesian theory for the analysis of data with missing values that underlies the methods in sections  [ sec32 ] and  [ sec33 ] .",
    "i then describe the transition from sections  [ sec32 ] to  [ sec34 ] for the case of multivariate normal models , and elaborations for non - normal data .",
    "likelihoods can be defined for nonrectangular data , so likelihood methods apply directly to missing - data problems : @xmath10 given the likelihood function , standard approaches are to maximize it , leading to ml , with associated large sample standard errors based on the information , the sandwich estimator or the bootstrap ; or to add a prior distribution and compute the posterior distribution of the parameters",
    ". draws from the predictive distribution of the missing values can also be created as a basis for mi .    as described in little and rubin ( @xcite , chapter  6 ) ,",
    "let @xmath11 represent a data matrix with  @xmath12 rows ( cases ) and @xmath13 columns ( variables ) , and define the missing - data indicator matrix @xmath14 , with entries @xmath15 if @xmath16 is observed , @xmath17 if @xmath16 is missing . also , write @xmath18 , where @xmath19 represents the observed components of @xmath4 and @xmath20 the missing components . a full parametric model factors the distribution of ( @xmath21 ) into a distribution @xmath22 for @xmath4 indexed by unknown parameters @xmath0 , and a distribution @xmath23 for @xmath24 given  @xmath4 indexed by unknown parameter @xmath25 .",
    "( this is called a selection model factorization ; the alternative factorization into the marginal distribution of @xmath24 and the conditional distribution of @xmath4 given @xmath24 is called a pattern - mixture model . )",
    "if @xmath4 was fully observed , the posterior distribution of the parameters would be @xmath26 where @xmath27 is the prior distribution of the parameters , @xmath28 is the complete - data likelihood and const . is a normalizing constant . with incomplete data ,",
    "the full posterior distribution becomes @xmath29\\\\[-8pt ] & & \\quad\\propto\\pi(\\theta,\\psi ) \\times l(\\theta,\\psi |y_{\\mathrm{obs}},m),\\nonumber\\end{aligned}\\ ] ] where @xmath30 is the observed likelihood , obtained by integrating the missing values out of the complete - data likelihood : @xmath31 a simpler posterior distribution of @xmath0 ignores the missing data mechanism , and is based on the likelihood given the observed data @xmath19 : @xmath32 which does not involve the model for the distribution of @xmath24 .    statistical analysis based on ( [ eq2 ] ) is considerably easier than analysis based on ( [ eq1 ] ) , since ( a ) the model for the missing - data mechanism is hard to specify , and has a strong influence on inferences ; ( b ) the integration over the missing data is often easier for equation  ( [ eq2 ] ) than for equation  ( [ eq1 ] ) ; and ( c ) the full model is often under - identified or poorly identified ; identification is in some ways less of an issue in bayesian inference , but results rest strongly on the choice of prior distribution .",
    "thus , ignoring the missing - data mechanism is useful if it is justified .",
    "sufficient conditions for ignoring the missing - data mechanism and basing inference on  ( [ eq2 ] ) ( rubin , @xcite ; little and rubin , @xcite , chapter  6 ) are as follows :    1 .   missing at random ( mar ) : @xmath33 for all @xmath20 , 2 .   a - priori independence : @xmath34 .    of these , mar is the key condition in practice , and it implies that missingness can depend on values in the data set that are observed , but not on values that are missing .",
    "the main challenges in developing posterior distributions based on ( [ eq1 ] ) or ( [ eq2 ] ) are the choice of model and computational issues , since the likelihood based on the data with missing values is typically much more complex than the complete - data likelihood . in the ml world , the expectation  maximization ( em ) algorithm creates a tie between the complicated observed data likelihood and the simpler complete - data likelihood , facilitating this computational task .",
    "specifically , suppose the missing - data mechanism is ignorable , and let @xmath35 be the current estimate of the parameter @xmath0 .",
    "the e - step of em finds the expected complete - data loglikelihood if @xmath0 equaled @xmath35 : @xmath36 the m - step of em determines @xmath37 by maximizing this expected complete - data loglikelihood : @xmath38    in the bayesian world , the analog of em is data augmentation , a variant of the gibbs sampler .",
    "( an even closer analog to the gibbs sampler is the expectation conditional maximization algorithm , a variant of em . )",
    "the key idea is to iterate between draws of the missing values and draws of the parameters ; draws of missing values replace expected values of functions of the missing values in the e - step of em , and draws of the parameters replace maximization over the parameters in the m - step of em .",
    "specifically , suppose the missing data mechanism is ignorable , and let @xmath39 be current draws of the missing data and parameters at iteration @xmath40 here and elsewhere a superscript @xmath41 is used to denote a draw from a distribution .",
    "then at iteration @xmath42 , the analog of the e - step ( [ eq3 ] ) of em is to draw new values of the missing data : @xmath43 ) is to draw a new set of parameters from the completed - data posterior distribution : @xmath44 as @xmath1 tends to infinity , this sequence converges to a draw @xmath45 from the joint posterior distribution of @xmath20 and @xmath0 .",
    "those familiar with mcmc methods will recognize this as an application of the gibbs sampler to the pair of variables @xmath20 and @xmath0 .",
    "the utility lies in the fact that ( [ eq5 ] ) is often facilitated since the distribution conditions on the parameters , and ( [ eq6 ] ) is a complete - data problem since it conditions on the imputations derived from ( [ eq5 ] ) . for more discussion of bayesian computations for missing data ,",
    "see tan and tian ( @xcite ) .",
    "draws @xmath46 of the missing data from equation ( [ eq5 ] ) at convergence can be used to create @xmath47 multiply - imputed data sets .",
    "bayesian mi combining rules can then be used for inferences that propagate imputation uncertainty .",
    "i outline the theory when the missing data mechanism is ignorable , although it readily extends to the case of nonignorable nonresponse .",
    "the idea is to relate the observed - data posterior distribution ( [ eq1 ] ) to the `` complete - data '' posterior distribution that would have been obtained if we had observed the missing data @xmath20 , namely , @xmath48 equations ( [ eq2 ] ) and ( [ eq7 ] ) can be related by standard probability theory as @xmath49\\\\[-8pt ] & & \\quad= \\int p(\\theta|y_{\\mathrm{obs}},y_{\\mathrm{mis}})p(y_{\\mathrm{mis}}|y_{\\mathrm{obs}})\\,dy_{\\mathrm{mis}}.\\nonumber\\end{aligned}\\ ] ] equation ( [ eq8 ] ) implies that the posterior distribution latexmath:[$p_{\\mathrm{ign}}(\\theta    values , @xmath46 , from their posterior distribution , @xmath51 , imputing the drawn values to complete the data set , and then drawing @xmath0 from its `` completed - data '' posterior distribution , @xmath52 when the posterior mean and variance are adequate summaries of the posterior distribution , ( [ eq9 ] ) can be effectively replaced by @xmath53\\nonumber \\\\[-8pt]\\\\[-8pt ] & & { } + \\operatorname{var } [ e(\\theta |y_{\\mathrm{obs}},y_{\\mathrm{mis}})|y_{\\mathrm{obs } }   ] .\\hspace*{-20pt}\\nonumber\\end{aligned}\\ ] ] approximating ( [ eq10 ] ) and ( [ eq11 ] ) using draws of @xmath20 yields @xmath54 where latexmath:[$\\hat{\\theta } ^{(d ) } = e(\\theta    of  @xmath0 from the @xmath41th completed data set , and @xmath56 say , where latexmath:[$\\bar{v } = d^ { - 1}\\sum_{d = 1}^{d } \\operatorname{var}(\\theta    complete - data posterior covariance matrix of @xmath0 calculated for the @xmath41th data set @xmath58 is the between - imputation covariance matrix , and @xmath59 is a  correction to improve the approximation for small  @xmath60 .",
    "the quantity @xmath61 in ( [ eq13 ] ) estimates the contribution to the variance from imputation uncertainty , missed ( i.e. , set to zero ) by single imputation methods .",
    "equations ( [ eq12 ] ) and ( [ eq13 ] ) are basic mi combining rules .",
    "refinements that replace the normal reference distribution for scalar @xmath0 by a student @xmath1 distribution are given in rubin and schenker ( @xcite ) , with further small - sample @xmath1 refinements in barnard and rubin ( @xcite ) ; extensions to hypothesis testing are described in rubin ( @xcite ) or little and rubin ( @xcite , chapter  10 ) . besides incorporating imputation uncertainty ,",
    "another benefit of multiple imputation is that the averaging over data sets in ( [ eq12 ] ) results in more efficient point estimates than does single random imputation .",
    "often mi is not much more difficult than doing a single imputation  the additional computing from repeating an analysis @xmath60 times is not a major burden and methods for combining inferences are straightforward .",
    "most of the work is in generating good predictive distributions for the missing values .    from a frequentist perspective",
    ", bayesian mi for a  parametric model has similar large - sample properties to ml , and it can be simpler computationally . another attractive feature of mi is that the imputation model can differ from the analysis model by including variables not included in final analysis .",
    "some examples follow :    mi was originally proposed for public use files , where the imputer often has variables available for imputation , like geography , that are not available to the analyst because of confidentiality constraints .",
    "such variables can be included in the imputation model , but will not be available for analysis .",
    "in other settings , auxiliary variables that are not suitable for inclusion in the final model , such as side - effect data for drugs in a clinical trial , may be useful predictors in an imputation model .    for public use files ,",
    "users perform analyses with different subsets of variables .",
    "different ml analyses involving a variable with missing values imply different imputation models , to the extent that they involve different sets of variables .",
    "a more coherent approach is to multiply impute missing variables using a common model , and then apply mi methods to each of the analyses involving subsets of variables .",
    "this allows variables not in the subset to help predict the missing values .",
    "mi combining rules can also be applied when the complete - data inference is not bayesian ( for example , nonparametric tests or design - based survey inference ) .",
    "the assumptions contained in the imputation model are then confined to the imputations , and with small amounts of missing data , simple imputation models may suffice .",
    "sections  [ sec:4 ] and  [ sec:5 ] sketched the basic theory of bayesian inference for missing data and the related method of mi .",
    "we now provide some examples of important models where these methods can be put to practical use .",
    "we focus mainly on continuous variables , although methods for categorical variables , and mixtures of continuous and categorical variables , are also available ( little and rubin , @xcite ) .",
    "[ ex1 ] i mentioned in section  [ sec31 ] the factored likelihood method of anderson ( @xcite ) .",
    "consider bivariate normal data on two variables @xmath62 where @xmath63 is observed for all @xmath12 observations , and  @xmath64 is for @xmath65 observations , that is , has missing values .",
    "assume the missing - data mechanism is ignorable .",
    "the factored likelihood is obtained by transforming the joint normal distribution of @xmath62 into the marginal distribution of @xmath63 , normal with mean @xmath66 and variance @xmath67 , and the conditional distribution of @xmath64 given @xmath63 , normal with mean @xmath68 and variance @xmath69 .",
    "the likelihood then factorizes into the normal likelihood for @xmath70 based on the @xmath12 cases with @xmath63 observed , and the normal likelihood for @xmath71 based on the @xmath72 cases with both @xmath63 and @xmath64 observed .",
    "the ml estimates are immediate : the sample mean and sample variance of @xmath63 ( with denominator @xmath12 ) based on all @xmath12 observations for @xmath73 , and the least squares estimates of the regression of @xmath64 on @xmath63 ( with no degrees of freedom correction for @xmath74 ) based on the  @xmath72 complete cases for @xmath75 .",
    "ml estimates of other parameters , such as the mean @xmath76 of @xmath64 , are obtained by expressing them as functions of @xmath77 and then substituting ml estimates of those parameters . in particular",
    ", this leads to the well - known regression estimate of @xmath76 : @xmath78 which is easily seen to be obtained when missing values of @xmath64 are imputed as predictions from the regression of @xmath64 on @xmath63 , with regression coefficients estimated on the complete cases .    a corresponding bayesian analysis is obtained by adding conjugate prior distributions for @xmath73 and @xmath75 , and computing draws from the posterior distributions of these parameters .",
    "the posterior distribution of @xmath73 is based on standard bayesian methods applied to the sample of @xmath12 complete observations on  @xmath63inverse - chi - squared for  @xmath67 , normal for @xmath66 given @xmath67 , and student s @xmath1 for @xmath66 .",
    "the posterior distribution of  @xmath75 is based on standard bayesian methods for the regression of @xmath64 on @xmath63 applied to the  @xmath72 complete observations on @xmath63 and @xmath64inverse chi - squared for  @xmath69 , normal for @xmath79 given @xmath69 , and multivariate  @xmath1 for @xmath80 .",
    "draws @xmath81 from these posterior distributions are simple to compute ( see little and rubin , @xcite , chapter  7 , for details )",
    ". draws from the posterior distribution of other parameters are then created in the same way as ml estimates , by expressing the parameters as functions of @xmath77 and then substituting draws .",
    "for example , a  draw from the posterior distribution of @xmath76 is @xmath82 a formula that mirrors the ml formula ( [ eq14 ] ) .",
    "this bayesian approach is asymptotically equivalent to ml , but it has several useful features .",
    "first , prior knowledge about the parameters can be incorporated in the prior distribution if this is available ; if not , noninformative reference prior distributions can be applied .",
    "second , the posterior distributions do a better job of capturing uncertainty in small samples ; for example , the draws ( [ eq15 ] ) incorporate @xmath1-like corrections , which are not provided by standard asymptotic ml calculations .",
    "third , the draws yield immediate estimates of uncertainty , such as the posterior variance , and 95% credibility intervals .",
    "the factored likelihood approach does not yield conveniently simple formulas for large sample variances based on the information matrix .",
    "these are easily approximated by draws ( [ eq15 ] ) , and are actually superior ( in a frequentist sense ) to asymptotic variances since they reflect the uncertainty better .",
    "computational advantages in simulating drawsfrom the posterior distribution are modest in the current bivariate normal example , since there are not many parameters .",
    "these benefits are more substantial in larger problems where the factored likelihood trick can be applied .",
    "suppose that there are @xmath83 variables @xmath84 such that ( a )  the data have a  monotone pattern , such that @xmath85 is always observed when @xmath86 is observed , for @xmath87 and ( b )  the conditional distribution of @xmath88 has a distribution ( not necessarily normal ) with unknown parameters @xmath89 , for @xmath90 and ( c ) the parameters @xmath91 are distinct and have independent prior distributions",
    ". draws from the posterior distribution of @xmath92 can then be obtaining from a sequence of complete - data posterior distributions , with the posterior distribution of  @xmath89 based on the subset of data with @xmath93 observed ( little and rubin , @xcite , chapter  7 ) .",
    "this elegant scheme forms the basis for mi in the case of a monotone pattern . in particular ,",
    "sas proc mi yields multiple imputations for normal models , where the regressions of @xmath85 on @xmath94 are not required to be linear and additive , as would be the case if the joint distribution was multivariate normal .    when the data are monotone but the parameters of the sequence of conditional distributions are not a - priori independent , or when the pattern is not monotone , these simple factored likelihood methods no longer apply , and draws from the posterior distribution need an iterative scheme .",
    "markov chain monte carlo methods then come to the rescue , as in the next example .",
    "[ ex2 ] suppose observations @xmath95 are assumed to be randomly sampled from a multivariate normal distribution , that is , @xmath96 the normal distribution with mean @xmath97 and covariance matrix @xmath98 .",
    "there are missing values , and let  @xmath99 , @xmath100 denote respectively the set of observed and missing values for observation @xmath101 .",
    "given current draw @xmath102 of the parameters , missing values ( [ eq5 ] ) are drawn as latexmath:[\\[\\label{eq16 } y_{\\mathrm{mis,}i}^{(d , t + 1 ) } \\sim p\\bigl ( y_{\\mathrm{mis,}i }    which is the multivariate normal distribution of the missing variables given the observed variables in @xmath101 , with parameters that are functionsof  @xmath104 , readily computed using the sweep operator ( little and rubin , @xcite , section  7.4 ) .",
    "new parameters ( [ eq6 ] ) are drawn from the posterior distribution given the filled - in data , which is a standard bayesian problem , namely , @xmath105\\\\[-8pt ] & & \\hspace*{11pt}{}y_{\\mathrm{obs } } , y_{\\mathrm{mis}}^{(d , t + 1)}\\bigr),\\nonumber\\end{aligned}\\ ] ] where ( [ eq17 ] ) is a draw from an inverse wishart distribution , and ( [ eq18 ] ) is a draw from a multivariate normal distribution .",
    "details of these steps were originally described in tanner and wong ( @xcite ) as part of their data augmentation algorithm , and they form the basis for the multiple imputation algorithm in sas proc mi , originally developed by schafer(schafer , @xcite ) . steps ( [ eq16])([eq18 ] ) are closely related to the em algorithm for ml estimation , except that they lead to draws from the posterior distribution .",
    "when feasible as here , it is recommended to first program em , and correct programming errors by checking that the likelihood increases with each iteration , and then convert the em algorithm into the gibbs algorithm , essentially by replacing the conditional means of the missing values in the e - step by draws ( [ eq16 ] ) , and the complete - data ml parameters in the m - step by draws ( [ eq17 ] ) and ( [ eq18 ] ) .",
    "mi based on this model is available in a variety of software , including sas proc mi .",
    "a frequentist statistician might compute ml estimates and associated standard errors based on the matrix .",
    "however , the gibbs algorithm outlined above is simpler than computing information - matrix based standard errors , which are not an immediate output from em .",
    "so a frequentist can use the draws from gibbs algorithm to compute tests and confidence intervals for the parameters , exploiting the asymptotic equivalence of bayes and frequentist inferences ( little and rubin , @xcite , chapter  6 ) . as in the previous example",
    ", the bayesian approach improves some aspects of small sample inference by including @xmath1-like corrections reflecting uncertainty in the variance parameters .",
    "example  [ ex2 ] allows missing data to be multiply imputed for a general pattern of missing values , rather than the monotone pattern in example  [ ex1 ] .",
    "a limitation is that it assumes a multivariate normal distribution for the set of variables with missing values ( normality can be relaxed for variables that are completely observed ) .",
    "this is a relatively strong parametric assumption  in particular , it assumes that the regressions of missing variables on observed variables are normal , linear and additive , which is not very appealing when a missing variable is binary or regressions involve interactions , for example .",
    "one approach to this problem is to modify the model to allow for mixtures of continuous and categorical variables .",
    "the general location model of olkin and tate ( @xcite ) provides a useful starting point ( little and rubin , @xcite , chapter  14 ) .",
    "this is useful , but the need to formulate a tractable joint distribution for the variables is restrictive . a more flexible approach is to mi for a sequence of conditional regression models for each missing variable , given all the other variables .",
    "this sequential regression multivariate imputation ( srmi ) method is only approximately bayes , but what it loses in theoretical coherence it gains in practical flexibility .",
    "it is the topic of the next example .",
    "[ ex3 ] suppose we have a general pattern of missing values , and @xmath84 are the set of variables with missing values , and @xmath106 represents a set of fully observed variables .",
    "srmi ( raghunathan et al . , @xcite ; van buuren et al . ,",
    "@xcite ) specifies models for the distribution of each variable @xmath107 given all the other variables @xmath108 and @xmath106 , indexed by parameters @xmath109 , with density @xmath110 , and a noninformative prior distribution for @xmath109 .",
    "missing values of @xmath107 at iteration @xmath111 are imputed according to the following scheme : let @xmath112 be the observed or imputed value of @xmath107 at iteration @xmath1 , and let @xmath113 denote the filled - in data set with imputations of @xmath114 from iteration @xmath111 and imputations of @xmath115 from iteration @xmath1 . for @xmath116 ,",
    "create new imputations of the missing values of @xmath107 as follows :    @xmath117 , the posterior distribution of given data @xmath118 ;    @xmath119 , if @xmath120 is missing , @xmath121 .",
    "this algorithm is repeated for @xmath122 until the imputations are stable ; typically , more than one chain is run to facilitate assessment of convergence ( gelman and rubin , @xcite ) .",
    "the algorithm is then repeated @xmath60 times to create @xmath60 multiply - imputed data sets , and inferences are based on standard mi combining rules .",
    "the positive feature of srmi is that it reduces the multivariate missing data problems into a set of univariate problems for each variable given all the other variables , allowing flexibility in the choice of model for each incomplete variable ; that is , nonlinear and interaction terms are allowed in the regressions , and the error distribution can be chosen to match the nature of the outcome ",
    "logistic for a  binary variable , and so on .",
    "the drawback is that the regression models for each variable given the others does not generally correspond to a coherent model for the joint distribution of @xmath123 given @xmath106 .",
    "thus , the mi s are not draws from a well - defined posterior distribution .",
    "this does not seem to be a  major problem in practice , and srmi is a flexible and practical tool for handling a variety of missing data problems .",
    "software is available ( raghunathan , solenberger and van hoewyk , @xcite ; mice , @xcite ) .",
    "the regression models in srmi are parametric , and potentially vulnerable to model misspecification .",
    "as noted in section  [ sec34 ] , one recent interest in missing data research has been the development of robust methods that do not involve strong parametric assumptions .",
    "my last example concerns so - called `` doubly - robust methods '' for missing data .",
    "[ ex4 ] for simplicity , we consider the case where missingness is confined to a  single variable @xmath4 .",
    "let @xmath124 be a vector of variables for observation @xmath101 , with @xmath95 observed for @xmath125 and missing for @xmath126 , and @xmath127 observed for @xmath121 .",
    "we assume that the probability that @xmath95 is missing depends on @xmath128 but not @xmath95 , so the missing data mechanism is mar .",
    "we consider estimation and inference for the mean of @xmath129 .",
    "let @xmath130 denote the missing - data indicator for @xmath95 , with @xmath131 when @xmath95 is missing and @xmath132 when @xmath95 is observed .",
    "a number of robust methods involve the propensity to be observed , estimated by a logistic or probit regression of @xmath24 on @xmath133 ( rosenbaum and rubin , @xcite ; little , @xcite ) .",
    "in particular , propensity weighting computes the mean of the complete cases , weighted by the inverse of the estimated probability that @xmath4 is observed .",
    "propensity weighting can yield estimates with large variances , and more efficient estimates are obtained by predicting the missing values of @xmath4 based on a model , with robustness supplied by a calibration term that weights the residuals from the complete cases ( robins , rotnitzky and zhao , @xcite ; rotnitzky , robins and scharfstein , @xcite ; bang and robins , @xcite ; scharfstein , rotnitsky and robins , @xcite ; kang and schafer , @xcite ) . in this context , an estimator is doubly robust ( dr ) if it is consistent if either ( a ) the prediction model relating @xmath4 to @xmath133 is correctly specified or ( b ) the model for the propensity to respond is correctly specified . in my last example , we describe a bayesian missing data method , called penalized spline of propensity prediction ( pspp ) , that has a  dr property .",
    "define the logit of the propensity score for @xmath95 to be observed as @xmath134 where @xmath25 denotes unknown parameters .",
    "the pspp method is based on the balancing property of the propensity score , which means the missingness of  @xmath95 depends only on @xmath135 only through the propensity score , under the mar assumption ( rosenbaum and rubin , @xcite ) .",
    "given this property , the mean of @xmath4 can be written as @xmath136 + e\\bigl[m_{i } \\times e\\bigl(y_{i}|p_{i}^{*}(\\psi)\\bigr)\\bigr].\\ ] ] thus , the missing data can be imputed conditioning on the propensity score .",
    "this leads to the penalized spline of propensity prediction method ( pspp ) ( little and an , @xcite ; zhang and little , @xcite ) .",
    "imputations in this method are predictions from the following model : @xmath137 where @xmath138 denotes the normal distribution with mean @xmath139 and constant variance @xmath140 .",
    "the first component of the mean function , @xmath141 , is a spline function of the propensity score @xmath142 .",
    "the second component @xmath143 is a parametric function , which includes any covariates other than @xmath144 that predict @xmath95 .",
    "one of the predictors , here @xmath145 , is omitted from the @xmath146-function to avoid multicollinearity .",
    "a variety of spline functions can be chosen ; we choose a penalized spline ( eilers and marx , @xcite ; ruppert , wand and carroll , @xcite ) of the form @xmath147\\\\[-8pt ] & & { } + \\sum_{k = 1}^{k } \\beta_{k + 1 } \\bigl ( p_{i}^{*}(\\psi ) - \\kappa_{k }   \\bigr ) _",
    "{ + } , \\hspace*{-30pt}\\nonumber\\end{aligned}\\ ] ] where 1 , @xmath148 is the truncated linear basis ; @xmath149 are selected fixed knots and @xmath13 is the total number of knots , and @xmath150 are random effects , assumed normal with mean 0 and variance @xmath151 .",
    "this model can be fitted by ml using a number of existing software packages , such as proc mixed in sas ( sas , @xcite ; ngo and wand , @xcite ; littell et al . , @xcite ) and @xmath152 in s - plus ( pinheiro and bates , @xcite ) .",
    "the first step of fitting a pspp model estimates the propensity score , for example , by a logistic regression model or probit model of @xmath24 on @xmath153 in the second step , the regression of @xmath4 on @xmath154 is fit as a spline model with the other covariates included in the model parametrically in the @xmath146-function .",
    "when @xmath4 is a continuous variable we choose a normal distribution with constant variance . for other types of data , extensions of the pspp",
    "can be formulated by using the generalized linear models with different link functions .",
    "the average of the observed and imputed values of @xmath4 has a dr property , meaning that the predicted mean of @xmath4 is consistent if either ( a ) the mean of @xmath95 given @xmath155 in model ( [ eq3 ] ) is correctly specified , or ( b1 ) the propensity @xmath142 is correctly specified , and ( b2 ) @xmath156.the robustness feature derives from the fact that the regression function @xmath146 does not have to be correctly specified , and the spline part of the regression function involves a weak parametric assumption , practically similar to the dr property mentioned above ( little and an , @xcite ; zhang and little , @xcite )",
    ".    how does bayes play into these methods ?",
    "the missing values of @xmath95 can be multiply - imputed under this model , but note that these imputations involve a substantial number of unknown parameters , namely , the regression coefficients and variances @xmath157 of the spline model , the regression coefficients  @xmath158 in the parametric component @xmath146 , the residual variance  @xmath140 , and the nuisance parameters @xmath25 in the propensity function .",
    "uncertainty in these parameters is readily propagated under the bayesian paradigm by drawing them from their posterior distributions , which is reasonably straightforward using a gibbs sampler .",
    "i began this article by summarizing some arguments in favor of the cb approach to statistical inference , which to my mind incorporates the best features of both bayesian and frequentist statistics .",
    "in short , inferences should be bayesian , but model development and checking requires careful attention to frequentist properties of the resulting bayesian inference .",
    "this cb `` roadmap '' is not a complete solution , since the interplay between model development and model inference , involving questions such as what range of model uncertainty should be included as part of formal statistical inference ( draper , @xcite ) , remains illusive and hard to pin down .",
    "however , i find the cb approach a very satisfying basis for approaching practical statistical inference .    in the remainder of the article i argued that the cb approach is particularly attractive for dealing with problems of missing data . in a sense ,",
    "all of inferential statistics is a missing data problem , since it involves making inferences about something that is unknown and hence `` missing '' ; in that broad sense , i am merely restating the previous paragraph .",
    "however , if missing data is considered more restrictively as referring to situations where the data matrix is incomplete , or partial information is available on some variables , then the bayesian paradigm is conceptually straightforward , since likelihoods do not require a fully - recorded rectangular data set .",
    "simply put , bayesian statistics involves generating predictive distributions of unknowns given the data . applied to missing data , it requires a predictive distribution for the missing data given the observed data .",
    "multiple imputations are simply draws from this predictive distribution , and can be used for other analyses if a good model is chosen for the predictive distribution .    in sections",
    "[ sec:4 ] and  [ sec:5 ] i outlined the basic theory underlying bayes inference and mi with missing data , emphasizing the role of the mar assumption .",
    "an important extension of this theory is to problems of coarsened data , where some values in the data set are rounded , grouped or censored ( heitjan and rubin , @xcite ; heitjan , @xcite ) .",
    "this theory has connections with the concept of noninformative censoring that underlies many methods of survival analysis with censored data .",
    "mi can be applied in these settings ( hsu et al . , @xcite ) , and is particularly useful for conditioning imputations on auxiliary variables not included in the primary analysis .    in section  [ sec:6 ]",
    "i illustrated bayesian approaches to missing data , mainly for normal models , in view of their practical importance and historical interest .",
    "i emphasize that bayesian methods are also useful for non - normal missing data problems .",
    "the srmi methods are not restricted to normal models , and bayes and/or mi can be applied to categorical data models and mixtures of categorical and continuous variables ( little and rubin , @xcite , chapters 13 and 14 ) , and generalized linear models with missing covariates ( ibrahim et al . , @xcite ) .",
    "bayesian hierarchical models are also natural for longitudinal data ( gilks et al . , @xcite ; ibrahim and molenberghs , @xcite ) and small area estimation ( ghosh and rao , @xcite ) , with or without missing data .    in the examples i focused on mar models , but bayesian approaches to nmar models",
    "are also very appealing . a key problem",
    "when data are not missing at random is lack of identifiability of the model , and bayesian methods provide a formal solution to the problem by allowing the formulation of prior distributions for unidentified parameters that reflect the uncertainty ( rubin , @xcite ; daniels and hogan , @xcite ) .",
    "resulting inferences are arguably superior to frequentist methods based on sensitivity analysis , where unidentified parameters are varied over a range . for a recent illustration of these ideas ,",
    "see long , little and lin ( @xcite ) , who apply bayesian missing data methods to handle noncompliance in clinical trials .",
    "i have focused here more on the bayesian part of cb , given the emphasis of the workshop that motivated this article on bayesian tools .",
    "concerning the calibrated part , good frequentist properties require realistic models for the predictive distribution of the missing values , and this requires attention to checking the fit of the model to the observed data , and sensitivity analyses to assess the impact of departures from mar .",
    "gelman et al .",
    "( @xcite ) and abayomi , gelman and levy ( @xcite ) provide useful methods for model checking for multiple imputation , and more work in this area would be useful .",
    "useful comments from two referees on an earlier draft are greatly appreciated , and helped to improve the article .",
    "baum , l. e. , petrie , t. , soules , g. and weiss , n. ( 1970 ) . a  maximization technique occurring in the statistical analysis of probabilistic functions of markov chains .",
    "statist . _",
    "* 41 * 164171 .",
    "littell , r.  c. , milliken , g.  a. , stroup , w.  w. and wolnger , r.  d. ( 1996 ) . _",
    "sas system for mixed models_. sas institute inc . ,",
    "cary , nc .",
    "little , r.  j. ( 1979 ) .",
    "maximum likelihood inference for multiple regression with missing values : a simulation study .",
    "_ j.  roy .",
    "b _ * 41 * 7687 .                              orchard , t. and woodbury , m. a. ( 1972 ) .",
    "a missing information principle : theory and applications . in _ proc .",
    "6th berkeley symposium on mathematical statistics and probabality _ * 1 * 697715 .",
    "california press , berkeley , ca .",
    "raghunathan , t. e. , lepkowski , j. m. , van hoewyk , j. and solenberger , p. ( 2001 ) . a multivariate technique for multiply imputing missing values using a sequence of regression models .",
    "_ survey methodology _ * 27 * 8595 .",
    "raghunathan , t.  e. , solenberger , p.  w. and van hoewyk , j. ( 2009 ) .",
    "iveware : imputation and variance estimation software .",
    "available at http://www.isr.umich.edu/src/smp/ive/[http://www.isr.umich.edu/ ] http://www.isr.umich.edu/src/smp/ive/[src/smp/ive/ ] .",
    "zhang , g. and little , r.  j. ( 2009 ) .",
    "extensions of the penalized spline propensity prediction method of imputation .",
    "_ biometrics _ * 65 * 911918 .",
    "http://dx.doi.org/10.1111/j.1541-0420.2008.01155[doi : 10.1111/ ] http://dx.doi.org/10.1111/j.1541-0420.2008.01155[j.1541-0420.2008.01155 ] ."
  ],
  "abstract_text": [
    "<S> it is argued that the calibrated bayesian ( cb ) approach to statistical inference capitalizes on the strength of bayesian and frequentist to statistical inference . in the cb approach , </S>",
    "<S> inferences under a particular model are bayesian , but frequentist methods are useful for model development and model checking . in this article </S>",
    "<S> the cb approach is outlined . </S>",
    "<S> bayesian methods for missing data are then reviewed from a cb perspective . </S>",
    "<S> the basic theory of the bayesian approach , and the closely related technique of multiple imputation , is described . </S>",
    "<S> then applications of the bayesian approach to normal models are described , both for monotone and nonmonotone missing data patterns . </S>",
    "<S> sequential regression multivariate imputation and penalized spline of propensity models are presented as two useful approaches for relaxing distributional assumptions .    . </S>"
  ]
}