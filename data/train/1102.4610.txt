{
  "article_text": [
    "[ s : one ]    the importance of accounting for statistical errors is well established in astronomical analysis : a measurement is of little value without an estimate of its credible range .",
    "various strategies have been developed to compute uncertainties resulting from the convolution of photon count data with _",
    "instrument calibration products _ such as effective area curves , energy redistribution matrices , and point spread functions .",
    "a major component of these analyses is good knowledge of the instrument characteristics , described by the instrument calibration data . without the transformation from measurement signals to physically interesting units afforded by the instrument calibration",
    ", the observational results can not be understood in a meaningful way . however , even though it is well known that the measurements of the instrument s properties ( e.g. , quantum efficiency of a ccd detector , point spread function of a telescope , etc . ) have associated measurement uncertainties , the calibration of instruments is often taken on faith , with only nominal estimates used in data analysis , even when it is recognized that these uncertainties can cause large systematic errors in the inferred model parameters . in many subfields",
    "( exceptions include : e.g. gravitational wave astrophysics , virgo collaboration 2010 , ligo collaboration 2010 and references therein ; cmb analyses , mather et al .  1999 ,",
    "rosset et al .  2010 , jarosik et al .",
    "2011 , and references therein ; and extra - solar planet / planetary disk work , e.g. butler et al .",
    "1996 , maness et al .",
    "2011 , and references therein ) , instrument calibration uncertainty is often ignored entirely , or in some cases , it is assumed that the calibration error is uniform across an energy band or an image area .",
    "this can lead to erroneous interpretation of the data .",
    "calibration products are derived by comparing data from well - defined sources obtained in strictly controlled conditions with predictions , either in the lab or using a particularly well - understood astrophysical source .",
    "parametrized models are fit to these data to derive best - fit parameters that are then used to derive the relevant calibration products .",
    "the errors on these best - fit values carry information on how accurately the calibration is known and could be used to account for calibration uncertainty in model fitting .",
    "unfortunately , however , the errors on the fitted values are routinely discarded . even beyond the errors in these fitted values , calibration products are subject to uncertainty stemming from differences between the idealized calibration experiments and the myriad of complex settings in which the products are used . suspected systematic uncertainty can not be fully understood until suitable data are acquired or cross - instrument comparisons are made ( david et al .",
    "prospectively , this source of uncertainty is difficult to quantify but is encompassed to a certain extent in the experience of the calibration scientists .",
    "different mechanisms have been proposed to quantify this type of uncertainty , ranging from adopting ad hoc distributions such as truncated gaussian ( drake et al .",
    "2006 ) to uniform deviations over a specified range .",
    "as long as it can be characterized even loosely , statistical theory provides a mechanism by which this information can be included to better estimate the errors in the final analysis .",
    "users and instrument builders agree that incorporating calibration uncertainty is important ( see davis 2001 ; drake et al .",
    "2006 ; grimm et al .",
    "for example , drake et al .  (",
    "2006 ) demonstrated that error bars on spectral model parameters are underestimated by as much as a factor of 5 ( see their figure  5 ) for high counts data when calibration uncertainty is ignored ( @xmath0 counts for typical ccd resolution spectra ) .",
    "such underestimations can lead to incorrect interpretations of the analysis results . despite this , calibration uncertainties are rarely incorporated because only a few ad hoc techniques exist and no robust principled method is available . in short , there is no common language or standard procedure to account for calibration uncertainty .    historically , at the international congress of radiology and electricity held in brussels in september 1910 , mme .",
    "curie was asked to prepare the first standard based on high energy photon emission ( x-/@xmath1-ray ) : 21.99 milligrams of pure radium chloride in a sealed glass tube , equivalent to 1.67x10@xmath2 curies of radioactive radium ( e.g. , brown 1997 pg  9ff and references therein ) .",
    "the problem then became : how to measure other samples , in reference to this standard ?",
    "although the sample preparation was done by very accurate chemistry techniques , the tricky part was designing and building the instrument to quantify the high - energy photon emission . at the next international committee meeting ( 1912 ,",
    "paris ) calibrating the standard was done by specialized electroscopes balancing the ` ionization current ' from two sources .",
    "this instrument was deemed to have an uncertainty of one part in 400 ( rutherford and chadwick 1911 ) .",
    "the original paper also describes a method for calibrating the detector .",
    "although these measurements were quite carefully done , and complex for their time , the result was a single value ( the intensity ) and had a single number quantifying its error ( @xmath3 ; rutherford and chadwick 1911 ) . in this case , the effect of this original unavoidable measurement error on one s final measurement of a source intensity ( in curies ) is straightforward to propagate , such as by the delta - method .",
    "nowadays , meetings about absolute standards and measuring instruments are much more complex , incorporating multiple kinds of measurements for a single standard ( e.g. codata ; mohr , taylor , and newell 2008 ) . as well , in the general literature",
    ", one finds increasingly complex methods dealing with e.g. multivariate data and calibration ( sundberg 1999 , osbourne 1991 ) , and even methods for ` traceability ' back to known standards ( cox and harris 2006 ) .",
    "these approaches formulate their complexities in terms of cross - correlations of parameters .",
    "this methodology has also been successfully used in modern astrophysics , such as in combining optical observations of supernovae for cosmological purposes ( e.g. kim and miquel 2006 ) .",
    "initially , j.  drake and other co - authors did try formulating the dependencies and anticorrelations of the final calibration product uncertainties in terms of correlation coefficients .",
    "however , after considerable exploration , they found this approach unable to capture the complexities of spacecraft calibration , especially at high energies .",
    "first , each part of a modern instrument such as the chandra observatory is measured at multiple energies and multiple positions , as well as calibrating the whole system on the ground .",
    "second , interestingly , the instrument is modeled by a complex physics - based computer code .",
    "the original calibration measurements are not used directly , but are benchmarks for the physical systems modeled therein .",
    "high energy astrophysics brings a third difficulty : the previous papers assumed a gauss - normal distribution for the calibration - product uncertainties ; this certainly does not hold for most real instruments in the high energy regime . hence , expanding beyond drake et al .",
    "( 2006 ) , in this paper , we describe how to ` short - circuit ' tracing back to the original calibration uncertainties by using the entire instrument - modeling code as part of statistical computing techniques .",
    "we see this in the context of the movement towards `` uncertainty quantification '' ( uq ) of large computer codes ( see , e.g. , christie et al .",
    "2005 ) .",
    "until recently , the best available general strategy in high - energy astrophysics was to compute the root - mean - square of the measurement errors and the calibration errors and then to fit the source model using the resulting error sum ( see bevington and robinson 1992 ) .",
    "unfortunately , the use and interpretation of the standard deviation relies on gaussian errors , that the calibration errors are uncorrelated , and that the uncertainty on the calibration products can be uniquely translated to an uncertainty in each bin in data space .",
    "none of these assumptions are warranted .",
    "furthermore , this method , equivalent to artificially inflating the statistical uncertainty on the data , will lead to biased fits , error bars without proper coverage , and incorrect estimates of goodness of fit .",
    "individual groups have also tried various instrument - specific methods .",
    "these range from bootstrapping ( simpson and mayer - hasselwander 1986 ) to raising and lowering response `` wings '' by hand ( forrest 1988 , forrest vestrand and mcconnell 1997 ) , and in one case , analytical marginalization over a particular kind of instrumental uncertainty ( bridle et al .",
    "2002 ) . in general and in important cross - instrument comparisons , however , all but the crudest methods ( e.g. , multiplying each instrument s total effective area by a fitted `` uncertainty factor '' as in hanlon et al .",
    "1995 , schmelz et al .",
    "2009 ) are very difficult to handle .",
    "methods for handling systematic errors exist in other fields such as particle physics ( heinrich and lyons 2007 and references therein ) and observational cosmology ( bridle et al .",
    "2002 ) . in their review of systematic errors , heinrich and lyons ( 2007 ) advocate parameterizing the systematics into statistical models and marginalizing over the nuisance parameters of the systematics .",
    "they described various statistical strategies to incorporate systematic errors which range from simple brute force @xmath4 fitting to fully bayesian hierarchical modeling .",
    "unfortunately these analytical methods rely on gaussian model assumption that are inappropriate for high energy astrophysics and are also highly case specific .",
    "accounting for calibration uncertainty is further complicated by complex and large scale correlation in the calibration products .",
    "the value of the calibration product at one point can depend strongly on far away values and even data collected using a different instrument .",
    "for example , the _ chandra _  low energy transmission grating spectrometer ( letgs ) + high resolution camera - spectroscopic readout ( hrc - s ) effective area is calibrated using the power - law source pks 2155 - 304 . because the high - order contributions to the spectrum can not be disentangled",
    ", the index of the power - law depends strongly on an analysis of the same source with data obtained contemporaneously with the high energy transmission grating spectrometer ( hetgs ) + acis - s .",
    "thus , changes in the hetgs+acis - s effective area will affect the longer - wavelength letgs+hrc - s effective area .",
    "the complex correlations can result in a diverse set of plausible effective area curves .",
    "the choice among these curves can strongly affect the final best fit in day - to - day analyses .",
    "the nominally better strategy of folding the calibration uncertainty through to the final statistical errors on fitted model parameters is unfortunately unfeasible : the complex correlations make it difficult to quantify the affect on the final analysis of uncertainty in the calibration product .",
    "drake et al .  ( 2006 ) proposed a strategy that accounts for these correlations by generating synthetic datasets from a nominal effective area and then fitting a model separately using each of a number of instance of a simulated effective area and then estimating the effect of the calibration error via the variance in the resulting fitted model parameters .",
    "this procedure can be implemented using standard software packages such as _ xspec _ ( arnaud 1996 ) and _ sherpa _  ( freeman et al .",
    "2001 , refsdal et al .",
    "2009 ) and demonstrates the importance of including calibration errors in data analysis . however , in practice there are some difficulties in implementing it with real data where the true parameters are not known _",
    "a priori_. the ad hoc nature of the bootstrapping - type procedure means its statistical properties are not well understood , requiring the sampling distributions to be calibrated on a case - by - case basis .",
    "that is , the procedure requires verification whenever different models are considered or different parts of the parameter space are explored .",
    "the large number of fits required also imposes a heavy computational cost .",
    "most importantly , it requires numerous simulated calibration products that must be supplied to end users either directly through a comprehensive database or through instrument specific software for generating them . in general , both these strategies impose a heavy burden on calibration or analysis software maintainers .    the primary objective of this article is to propose well - defined and general methods to incorporate complex calibration uncertainty into spectral analysis in a manner that can be replicated in general practice without precise calibration expertise .",
    "although we develop a general framework for incorporating calibration uncertainty , we limit our detailed discussion to accounting for uncertainty in the effective area for _ chandra_/acis - s in spectral analysis . we propose a bayesian framework , where knowledge of calibration uncertainties is quantified through a prior probability . in this way ,",
    "information quantified by calibration scientists can be incorporated into a coherent statistical analysis .",
    "operationally , this involves fitting a highly - structured statistical model that does not assume the calibration products are known fixed quantities , but rather incorporates their uncertainty through a prior distribution .",
    "we describe two statistical strategies below for incorporating this uncertainty into the final fit .",
    "multiple imputation fits the model several times using standard fitting routines , but with a different value of the calibration product used in each fit .",
    "alternatively , using an iterative markov chain monte carlo ( mcmc ) sampler allows us to incorporate calibration uncertainty directly into the fitting routine by updating the calibration products at each iteration . in either case",
    ", we advocate updating the calibration products based solely on information provided by calibration scientists and not on the data being analyzed ( i.e. , not updating products given the data being analyzed ; see also discussion about computational feasibility in  [ sec : disc : fullbayes ] ) .",
    "this strategy leads to simplified computation and reliance on the expertise of the calibration scientists rather than on the idiosyncratic features of the data .",
    "we adopt the strategy of drake et al .",
    "( 2006 ) to quantify calibration uncertainty using an ensemble of simulated calibration products , that we call the _ calibration sample_. we use principal component analysis ( pca ) to simplify this representation .",
    "a glossary of the terms and symbols that we use is given in table  [ tab : glossary ] .",
    "in  [ s : cs ] we describe the calibration sample and illustrate the importance of properly accounting for calibration uncertainty in spectral analysis .",
    "our basic methodology is outlined in ",
    "[ s : meth ] , where we describe how the calibration sampler can be used to generate the replicates necessary for multiple imputation or can be incorporated into an mcmc fitting algorithm .",
    "we also show how pca can provide a concise summary of the complex correlations of the calibration uncertainty .",
    "specific algorithms and strategies for implementing this general framework for spectral analysis appear in  [ s : alg ] .",
    "our proposed methods are illustrated with a simulation study and an analysis of 15 radio loud quasars ( siemiginowska et al .",
    "2008 ) in  [ s : ex ] . in ",
    "[ sec : disc ] we discuss future directions and a general framework for handling calibration uncertainties from astrophysical observations with similar form as our yx - ray examples .",
    "we summarize the work in  [ sec : summ ] .",
    "to coherently and conveniently incorporate calibration uncertainty into spectral fitting , we follow the suggestion of drake et al .",
    "( 2006 ) to represent it using a randomly generated set of calibration products that we call the _ calibration sample_. in this section we begin by describing this calibration sample , and how it can be used to represent the inherent systematic uncertainty .",
    "the methods that we discuss in this and the following sections are quite general and in principle can be applied to account for systematic uncertainty in any calibration product . for clarity ,",
    "we illustrate their application to instrument effective areas .",
    "we begin with a simple model of telescope response that assumes position and time invariance . in particular , suppose the response of a detector to an incident photon spectrum @xmath5 , @xmath6 where @xmath7 represents the detector channel at which a photon of energy @xmath8 is recorded , @xmath9 represents the parameters of the source model , and @xmath10 , @xmath11 , and @xmath12 are the effective area , point spread function , and energy redistribution matrix of the detector , respectively .",
    "we aim to develop methods to estimate @xmath9 and compute error bars that properly account for uncertainty in @xmath10 .",
    "of course @xmath11 and @xmath12 are also subject to uncertainty and in  [ sec : disc : gen ] we discuss extensions of the methods described here to handle more general sources of calibration uncertainty .    as an illustration , we consider observations obtained using the spectroscopic array of the _ chandra _  axaf ccd imaging spectrometer detector ( acis - s ) .",
    "according to drake et al .",
    "( 2006 ) , it is possible to generate a calibration sample of effective area curves for this instrument by explicitly including uncertainties in each of its subsystems ( uv / ion shield transmittance , ccd quantum efficiency , and the telescope mirror reflectivity ) .",
    "the result is a set of simulations of the effective area curves .",
    "these encompass the range of its uncertainty , with more of the simulated curves similar to its most likely value , and fewer curves that represent possible but less likely values . in principle , some may be more likely than others , in which case weights that indicate the relative likelihood are required . in this article",
    ", we assume that all of the simulations in the set are equally likely , that is the simulations are representative of calibration uncertainty .",
    "the set of @xmath13 simulations is the _ calibration sample _ and denoted @xmath14 , where @xmath15 is one of the simulated effective area curves .",
    "the complicated structure in the uncertainty for the true effective area is illustrated in figure  [ fig : arf ] using the calibration sample of size @xmath16 generated by drake et al .",
    "a selection of six of the @xmath15 from @xmath17 are plotted as colored dashed lines and compared with the default effective area , @xmath18 that is plotted as a solid black line .",
    "the second panel plots the differences , @xmath19 for the same selection .",
    "the light gray area represents the full range of @xmath17 and the dark gray area represents intervals that contain 68.3% of the @xmath15 at each energy .",
    "the complexity of the uncertainty of @xmath10 is evident .",
    "we use the calibration sample illustrated in figure  [ fig : arf ] as the representative example throughout this article .",
    "we discuss here the effect of the uncertainty represented by the calibration sample on fitted spectral parameters and their error bars .",
    "we employ simulated spectra representing a broad range in parameter values . in particular",
    ", we simulated four data sets of an absorbed power - law source with three parameters ( power - law index @xmath20 , absorption column density @xmath21 , and normalization ) using the fakeit routine in xspecv12 .",
    "the data sets were all simulated without background contamination using the xspec model wabs*powerlaw , nominal default effective area @xmath18 from the calibration sample of drake et al .",
    "( 2006 ) , and a default rmf for acis - s .",
    "the power law parameter ( @xmath20 ) , column density ( @xmath21 ) , and nominal counts for the four simulations ( see also table  [ t : sim ] ) were    simulation  1 : : :    @xmath22 , @xmath23 ,    and @xmath24 counts ; simulation  2 : : :    @xmath25 , @xmath26 ,    and @xmath24 counts ; simulation  3 : : :    @xmath22 , @xmath23 ,    and @xmath27 counts ; and simulation  4 : : :    @xmath25 , @xmath26 ,    and @xmath27 counts    respectively .    to illustrate the effect of calibration uncertainty , we selected the 15 curves in @xmath28 with the largest maximum values and the 15 curves with the smallest maximum values . in some sense",
    ", these are the 30 most extreme effective area curves in @xmath29 .",
    "they are plotted as @xmath30 in the first panel of figure  [ fig : arf_shift ] , along with a horizontal line at zero that represents the default ( @xmath31 ) .",
    "we used the bayesian method of van dyk et al .",
    "( 2001 ) to fit simulation  1 and simulation  2 each 31 times , using each of the 31 curves of @xmath15 plotted in figure  [ fig : arf_shift ] .",
    "the resulting marginal and joint posterior distributions for @xmath20 and @xmath21 appear in rows 2 - 4 of figure  [ fig : arf_shift ] ; the contours plotted in the third row correspond to a posterior probability of 95% for each fit .",
    "were constructed by peeling ( green 1980 ) the original monte carlo sample .",
    "this involves removing the most extreme sampled values which are defined as the vertices of the smallest convex set containing the sample ( i.e. , the convex hull ) .",
    "this is repeated until only 95% of the sample remains .",
    "the final hull is plotted as the contour .",
    "this is a reasonable approximation because the posterior distributions appear roughly convex . ]",
    "the figure clearly shows that the effect of calibration uncertainty swamps the ordinary statistical error . the scientist who assumes that the true effective area is known to be @xmath18",
    "may dramatically underestimate the error bars , and may miss the correct region entirely .    as a second illustration we fit simulation  1 and simulation  3 each 31 times , using the same @xmath15 as in figure  [ fig : arf_shift ] and with @xmath18 , again using the method of van dyk et al .",
    "the resulting posterior distributions of @xmath20 and @xmath21 are plotted in figure  [ fig : e4e5_shifts ] .",
    "comparing the two columns of the figure , the relative contribution of calibration uncertainty to the total error bars appears to grow with counts .",
    "for this reason , accounting for calibration uncertainty is especially important with rich high - count spectra .",
    "in fact , in our simulations there appears to be a limiting value where the statistical errors are negligible and the total error bars are due entirely to calibration uncertainty .",
    "the total error bars do not go below this limiting value regardless of how many counts are observed .",
    "we must emphasize , however , that we are assuming that the observed counts are uninformative as to which of the calibration products in the calibration sample are more or less likely .",
    "if we were not to make this assumption , however , and if a data set were so large that we were able to exclude a large portion of the calibration sample as inconsistent with the data , the remaining calibration uncertainty would be reduced and its effect would be mitigated . in this case , the default effective area and effective area curves similar to the default could potentially be found inconsistent with the data and thus the fitted model parameters could be different from what we would get if we simply relied on the default curve . in this article , however , we assume that either the data set is not large enough to be informative for the calibration products or that we do not wish to base instrumental calibration on the idiosyncrasies of a particular data set . both figures  [ fig : arf_shift ] and [ fig :",
    "e4e5_shifts ] suggest that while the fitted values depend on the choice of @xmath32 , the statistical errors for the parameters given any fixed @xmath33 are more - or - less constant .",
    "the systematic errors due to calibration uncertainty shift the fitted value but do not effect its variance . of course , in practice we do not know @xmath10 and must marginalize over it , so the total error bars are larger than any of the errors bars that are computed given a particular fixed @xmath10 . how to coherently compute error bars that account for calibration uncertainty",
    "is our next topic .",
    "in this section , we outline how the calibration sample can be used in principled statistical analyses and describe how the complex calibration sample can be summarized in a concise and complete manner using pca .        in a standard astronomical data analysis problem ,",
    "as represented by equation  [ eq : sim_arf ] , it is assumed that @xmath34 and that @xmath9 is estimated using @xmath35 , where @xmath36 is the observed counts and @xmath37 is an objective function used for probabilistic estimation and calculation of error bars .",
    "typical choices of @xmath37 are the bayesian posterior distribution , the likelihood function , the cash statistic , or a @xmath4 statistic .",
    "we use the notation @xmath35 because we generally take a bayesian perspective , with @xmath38 representing a probability distribution and the notation `` @xmath39 '' referring to conditioning , e.g. , @xmath40 is to be read as `` the probability of @xmath41 _ given _ that @xmath42 is true . ''",
    "when @xmath10 is unknown , it becomes a nuisance parameter in the model , and the appropriate objective function becomes @xmath43 .",
    "using bayesian notation , @xmath44 where the primary source of information for @xmath10 is not the observation counts , @xmath36 , but the large datasets and physical calcuations used by calibration scientists , and which we denote here by @xmath45 . generally speaking",
    ", we expect the information for @xmath9 to come from @xmath36 rather than @xmath45 , at least given @xmath10 and we expect the information for @xmath10 to come from @xmath45 rather than @xmath36 .",
    "this can be expressed mathematically by two conditional independence assumptions :    1 .",
    "@xmath46 , and 2 .",
    "@xmath47 .",
    "we make these conditional independence assumptions , and implicitly condition on @xmath45 throughout this article . in this case",
    ", we can rewrite the above equation as @xmath48 which effectively replaces the posterior distribution @xmath49 with the prior distribution @xmath50 .",
    "finally , we can focus attention on @xmath9 by marginalizing out @xmath10 , @xmath51 that is , the objective function is simply the average of the objective functions used in the standard analysis , but with @xmath18 replaced by each of the @xmath52 .",
    "thus , the marginalization in equation  [ eq : marginal ] does not necessarily involve estimating @xmath49 nor specifying a parametric prior or posterior distributions for @xmath10 .",
    "when this marginalization is properly computed , systematic errors from calibration uncertainty are rigorously combined with statistical errors without need for gaussian quadrature .    of course , when @xmath13 is large as in the calibration sample of drake et al .",
    "( 2006 ) , evaluating and optimizing equation  [ eq : newobjective ] would be a computationally expensive task . in this section",
    "we outline two strategies that aim to significantly simplify the necessary computation .",
    "the first is a general purpose but approximate strategy that can be used with any standard model fitting technique and the second is a simple adaptation that can be employed when monte carlo is used in bayesian model fitting .",
    "details and illustrations of both methods appear in  [ s : alg ] .",
    "the first strategy takes advantage of a well - established statistical technique known as _ multiple imputation _ that is designed to handle missing data ( rubin 1987 , schafer 1997 ) .",
    "multiple imputation relies on the availability of a number of monte carlo replications of the missing data .",
    "the replications are called the _ imputations _ and are designed to represent the statistical uncertainty regarding the unobserved values of the missing data .",
    "although the calibration products are not missing data _ per se _ , the calibration sample provides exactly what is needed for us to apply the method of multiple imputation : a monte carlo sample that represents the uncertainty in an unobserved quantity .    with the calibration sample in hand ,",
    "it is straightforward to apply multiple imputation .",
    "a subset of @xmath17 of size @xmath53 is randomly selected and called the multiple imputations or the _ multiple imputation sample_. the standard data analysis method is then applied @xmath54 times , once with each of the @xmath54 imputations of the calibration products .",
    "this produces @xmath54 sets of parameter estimates along with their estimated variance - covariance matrices .",
    "] , which we denote @xmath55 and @xmath56 , respectively , for @xmath57 . in the simplest form of the method of multiple imputation",
    ", we assume that each @xmath55 follows a multivariate normal distribution with mean @xmath9 .",
    "the final fitted values and error bars are computed using a set of simple moment calculations known as the _ multiple imputation combining rules _",
    "( e.g. , harel and zhou 2005 ) .",
    "the parameter estimate is computed simply as the average of the individual fitted values , @xmath58 to compute the error bars , we must combine two sources of uncertainty : the statistical uncertainty that would arise even if the calibration product were known with certainty and the systematic uncertainty stemming from uncertainty in the calibration product .",
    "each of the @xmath54 standard analyses is computed as if the calibration product were known and therefore each @xmath56 is an estimate of the statistical uncertainty .",
    "our estimate of the statistical uncertainty is simply the average of these individual estimates , @xmath59 the systematic uncertainty , on the other hand , is estimated by looking at how changing the calibration product in each of the @xmath54 analyses affects the fitted parameter .",
    "thus , the systematic uncertainty is estimated as the variance of the fitted values , @xmath60 finally the two components of variance are combined for the total uncertainty , @xmath61 where the @xmath62 term accounts for small number @xmath63 of imputations .",
    "if @xmath63 is small relative to the dimension of @xmath9 , @xmath64 will be unstable , and more sophisticated estimates should be used ( e.g. , li et al .  1991 ) .",
    "here we focus on univariate summaries and error bars which depend only on one element of @xmath65 and the corresponding diagonal element of @xmath64 .",
    "when computing the error bars for one of the univariate fitted parameters in @xmath65 , say component @xmath66 of @xmath65 , it is generally recommended that the number of sigma used be inflated to adjust for the typically small value of @xmath54 .",
    "that is , rather than using one- and two - sigma for 68.3% and 95.4% intervals as is appropriate for the normal distribution , a @xmath67  distribution should be used , requiring a larger number of sigma to obtain 68.3% and 95.4% intervals . in the univariate case ,",
    "the _ degrees of freedom _ of the @xmath67 distribution determine the degree of inflation and can be estimated by @xmath68 where @xmath69 and @xmath70 are the @xmath66th diagonal terms of @xmath71 and @xmath72 .",
    "the method of multiple imputation is based on a number of assumptions .",
    "first , it is designed to give approximate error bars on @xmath9 that include the effects of the imputed quantity , but if a full posterior distribution on @xmath9 is desired , then a more detailed bayesian calculation must be performed ( see below )",
    ". it will provide an approximately valid answer in general when the imputation model is compatible with the estimation procedure , i.e. , when @xmath65 is the posterior mode from essentially the same distribution as is used for the imputation ( meng 1994 ) .",
    "furthermore , the computed standard deviations @xmath73 can be identified with 68% credible intervals only when the posterior distributions are multi - variate normal .",
    "additionally , when @xmath54 is small , the coverage must be adjusted using the @xmath67-distribution ( equation  [ eq : mi - df ] ) .",
    "multiple imputation offers a simple general strategy for accounting for calibration uncertainty using standard analysis methods .",
    "because this method is only approximate , however , our preferred solution is a monte carlo method that is robust , reliable , and fast . in principle",
    ", monte carlo methods can handle any level of complexity present in both the astrophysical models and in the calibration uncertainty .",
    "monte carlo can be used to construct powerful methods that are able to explore interesting regions in high - dimensional parameter spaces and , for instance , determine best - fit values of model parameters along with their error bars . in this context",
    ", it is used as a fitting engine , similar to levenberg - marquardt , powell , simplex , and other minimization algorithms .",
    "one of its main advantages is that it is highly flexible and can be applied to a wide variety of problems .",
    "a single run is sufficient to describe the variations in the model parameters that arise due to both statistical and systematic errors , which therefore leads to reduced computational costs .. ] consider a monte carlo sample obtained by sampling the model parameters @xmath9 given the data , @xmath36 , and the calibration product , @xmath74 , @xmath75 where @xmath76 is the iteration number and @xmath77 are the values of the parameters at iteration @xmath76 .",
    "the set of parameter values thus obtained is used to estimate the best - fit values and the error bars .",
    "when calibration uncertainty is included , we can no longer condition on @xmath18 as a known value of the calibration product .",
    "instead we add a new step that updates @xmath10 according to the calibration uncertainties .",
    "in particular , @xmath77 is updated using the same iterative algorithm as above , with an additional step at each iteration that updates @xmath10 .",
    "suppose at iteration @xmath76 , @xmath78 is the realization of the calibration product .",
    "then the new algorithm consists of the following two steps : @xmath79    under the conditional independence assumptions of section  [ s : meth : stat : marg ] , we can simplify this sampler by replacing @xmath49 with @xmath50 in the first step : @xmath80 this independence assumption gives us the freedom not to estimate the posterior distribution @xmath49 and simplifies the structure of the algorithm .",
    "it effectively separates the complex problem of model fitting in the presence of calibration uncertainties into two simpler problems : ( i ) fitting a model with known calibration and ( ii ) the quantification of calibration uncertainties independent of the current data @xmath36 .",
    "the methods that we propose so far require storage of a large number of replicates of @xmath81 .",
    "since calibration products can be observation specific this requires a massive increase in the size of calibration databases .",
    "this concern is magnified when we consider uncertainties in the energy redistribution matrix , @xmath12 , and point spread function , @xmath11 , and combining multiple observations , each with their own calibration products .",
    "although in principle this could be addressed by developing software that generates the calibration sample on the fly , we propose a more realistic and immediate solution that involves statistical compression of @xmath17 .",
    "compression of this sort takes advantage of the fact that many of the replicates in @xmath17 differ very little from each other and in principle we can reduce the sample s dimensionality from thousands to only a few with little loss of information .",
    "here we describe how principal component analysis ( pca ) can accomplish this for the _ chandra_/acis - s calibration sample generated by drake et al .",
    "( 2006 ) and illustrated in figure  [ fig : arf ] .",
    "pca is a commonly applied linear technique for dimensionality reduction and data compression ( jolliffe 2002 , anderson 2003 , ramsay and silverman 2005 , bishop 2007 ) .",
    "mathematically , pca is defined as an orthogonal linear transformation of a set of variables such that the first transformed variable defines the linear function of the data with the greatest variance , the second transformed variable define the linear function _ orthogonal to the first _ with the greatest variance , and so on .",
    "pca aims to describe variability and is generally computed on data with mean zero . in practice ,",
    "the mean of the data is subtracted off before the pca and added back after the analysis .",
    "computation of the orthogonal linear transformation is accomplished with the singular value decomposition of a data matrix with each variable having mean zero .",
    "this generates a set of eigenvectors that correspond to the orthogonal transformed variables , along with their eigenvalues that indicate the proportion of the variance correlated with each eigenvector .",
    "the eigenvectors with the largest values are known as the _ principal components_. by selecting a small number of the largest principal components , pca allows us to effectively summarize the variability of a large data set with a handful of orthogonal eigenvectors and their corresponding eigenvalues .",
    "our aim is to effectively compress @xmath17 using pca . using the singular vector decomposition of a matrix with rows equal to the @xmath82 with @xmath83",
    ", we compute the eigenvectors @xmath84 and corresponding eigenvalues @xmath85 , ordered such that @xmath86 the fraction of the variance of @xmath17 in the direction of @xmath87 is @xmath88 in practice , this gives us the option of using a smaller number of components , @xmath89 in the reconstruction , that is sufficient to account for a certain fraction of the total variance . a large amount of compression can be achieved because very few components are needed to compute the effective area to high precision . for example , in the case of acis effective areas , 8 - 10 components ( out of 1000 ) can account for 95% of the variance , and @xmath90 components can account for 99% of the variance .",
    "note that this approximation is valid only when considered over the full energy range ; small localized variations in @xmath17 that contribute little to the total variance , even if they may play a significant role in specific analyses ( the depth of the c - edge , for example ) may not be accounted for .    with the pca representation of @xmath17 in hand ,",
    "we wish to generate replicates of @xmath10 that mimic @xmath17 . in doing so ,",
    "however , we must account for the fact that calibration products typically vary from observation to observation to reflect deterioration of the telescope over time and other factors that vary among the observations .",
    "however , even though the magnitudes of the calibration products may change , the underlying uncertainties are less variant and are comparable across different regions of the detector at different times . we thus suppose that the differences among the calibration samples can be represented by simply changing the default calibration product , at least in many cases . that is , we assume that the distribution in the calibration samples differ only in their ( loosely defined ) average and that differences in their variances can be ignored . under this assumption , we can easily generate calibration replicates based on the first @xmath91 principal components as @xmath92 where @xmath93 is the observation - specific effective area that would currently be created by users , @xmath18 is the nominal default effective area from calibration , @xmath94 , @xmath95 , and @xmath96 are independent standard normal random variables .",
    "in addition to the first @xmath91 principal components , this representation aims to improve the replicates by including the residual sum of the remaining @xmath97 components .",
    "equation  [ eq : pca0 ] shows how we account for @xmath93 .",
    "if @xmath93 were equal to @xmath18 , equation  [ eq : pca0 ] would reduce to the standard pca representation . to account for the observation - specific effective area ,",
    "we add the offset @xmath98 . equation  [ eq : pca ] rearranges the terms to express @xmath99 as the sum of calibration quantities that we propose to provide in place of @xmath17 .",
    "in particular , using equation  [ eq : pca ] , we can generate any number of monte carlo replicates from @xmath17 , using only @xmath100 , @xmath93 , @xmath101 , and @xmath102 . in this way we need only provide instrument - specific and not observation - specific values of @xmath101 , and @xmath102 .",
    "figure  [ fig : pca ] illustrates the use of pca compression on the calibration sample generated by drake et al .",
    "( 2006 ) and illustrated in figure  [ fig : arf ] .",
    "we generated 1000 replicate effective areas using equation  [ eq : pca ] with @xmath103 and @xmath104 .",
    "the dashed and dotted lines in the upper left panel respectively superimpose the full range and 68.3% intervals of these replicates on the corresponding intervals for the origi nal calibration sample , plotted in light and dark grey . in this case , using @xmath104 captures 96% of the variation in @xmath17 , as computed with equation  [ eq : pcafrac ] .",
    "the remaining three panels give cross sections at 1.0 , 1.5 , and 2.5 kev .",
    "the distributions of the 1000 replicates generated using equation  [ eq : pca ] appears as solid lines , and those of the original calibration sample as a gray regions .",
    "the figure shows that pca replicates generated with @xmath104 are quite similar to the original calibration sample .",
    "although the pca representation can not be perfect ( e.g. , it does not fully represent uncertainty overall or in certain energy regions ) it is much better than not accounting for uncertainty at all .",
    "in this section we describe specific algorithms that incorporate calibration uncertainty into standard data analysis routines . in  [ s : alg : mi ] we show how multiple imputation can be used with popular scripted languages like _ heasarc_/xspec and _ ciao/_sherpa _ _  for spectral fitting , and in ",
    "[ s : alg : mc ] we describe some minor changes that can be made to sophisticated markov chain monte carlo samplers to include the calibration sample . in both sections",
    "we begin with cumbersome but precise algorithms and then show how approximations can be made to simplify the implementation .",
    "our recommended algorithms appear in  [ s : alg : mi : pca ] and  [ s : alg : prag : pca ] . in  [ s : ex ] we demonstrate that these approximations have a negligible effect on the final fitted values and error bars .",
    "multiple imputation is an easy to implement method that relies heavily on standard fitting routines .",
    "an algorithm for accounting for calibration uncertainty using multiple imputation can is described by :    step  1 : : :    for @xmath57 , repeat the following :    +    step  1a : ; ;      randomly sample @xmath105 from @xmath29 .",
    "step  1b : ; ;      fit the spectral model ( e.g. , using _ sherpa _ ) in the usual way , but      with effective area set to @xmath105    step  1c : ; ;      record the fitted values of the parameters as      @xmath55    step  1d : ; ;      compute the variance - covariance matrix of the fitted values and      record the matrix as @xmath56 .",
    "( in      _ sherpa _",
    "this can be done using the covariance function . )",
    "step  2 : : :    use equation  [ eq : mi - est ] to compute the fitted value ,    @xmath65 of @xmath9 .",
    "step  3 : : :    use equations  [ eq : mi - win][eq : mi - total ] to compute the    variance - covariance matrix , @xmath106 ,    of @xmath65 .",
    "the square root of the diagonal terms of    @xmath106 are the error bars of    individual parameters . step  4 : : :    use equation  [ eq : mi - df ] to compute the degrees of freedom for each    component of @xmath65 which are used to properly    calibrate the error bars computed in step  3 .",
    "asymptotically , @xmath107 error  bars correspond to equal - tail @xmath108 intervals under the gaussian distribution .",
    "when the number of imputations is small , @xmath109 error  bars should be used instead , where @xmath110 , a number @xmath111 , can be looked up in any standard @xmath67-distribution table using `` df '' equal to the degrees of freedom computed in step  4 , see ",
    "[ s : ex : mi ] for an illustration .",
    "if the correlations among the fitted parameters are not needed , the error bars of the individual fitted parameters can be computed one at a time using equations  [ eq : mi - win][eq : mi - total ] with @xmath55 and @xmath56 replaced by the fitted value of the individual parameter and the square of its error bars , both computed using @xmath105 .      using the pca approximation results in a simple change to the algorithm in  [ s : alg : mi : pca ] : step  1a",
    "is replaced by ( see equation  [ eq : pca ] ) :    step  1a : : :    set    @xmath112 ,    where @xmath113 are independent standard    normal random variables .",
    "the choice between this algorithm and the one described in section  [ s : alg : mi : full ] should be determined by the availability of a sample of size @xmath63 from a ( in which case the algorithm in section  [ s : alg : mi : full ] should be used ) or of the pca summaries of a required for the algorithm in this section .      in  [",
    "s : meth : stat : mc ] we considered simple monte carlo methods that simulate directly from the posterior distribution , @xmath114 . more generally , markov chain monte carlo ( mcmc ) methods can be used to fit much more complicated models .",
    "( good introductory references to mcmc can be found in gelman 2003 and gregory 2005 . )",
    "a markov chain is an ordered sequence of parameter values such that any particular value in the sequence depends on the history of the sequence only through its immediate predecessor . in this way",
    "mcmc samplers produce dependent draws from @xmath115 by simulating @xmath77 from a distribution that depends on the previous value of @xmath9 in the markov chain , @xmath116 .",
    "that is , @xmath117 is designed to be simple to sample from , while the full @xmath35 may be quite complex .",
    "the price of this , however , is that the @xmath77 may not be statistically independent of the @xmath118 ; and in fact may have appreciable correlation with @xmath119 ( that is , an autocorrelation of length @xmath120 ) .",
    "the distribution @xmath117 is derived using methods such as the metropolis - hastings algorithm and/or the gibbs sampler that ensures that the resulting markov chain converges properly to @xmath35 .",
    "van dyk et al .  ( 2001 ) show how gibbs sampling can be used to derive @xmath117 in high - energy spectral analysis .",
    "their method has recently been generalized in a _ sherpa",
    "_  module called pyblocxs ( bayesian low count x - ray spectral analysis in python , to be released ) that relies more heavily on metropolis - hastings than on gibbs sampling and can accommodate a larger class of spectral models .",
    "] in this section we show how pyblocxs can be modified to account for calibration uncertainty .",
    "for clarity we use the notation @xmath121 to indicate a single iteration of pyblocxs run with the effective area set to @xmath10 .",
    "in  [ s : meth : stat : mc ] we describe how a monte carlo sampler can be constructed to account for calibration uncertainly under the assumption that the observed counts carry little information as to the choice of effective area curve . in particular , we must iteratively update @xmath78 and @xmath77 by sampling them as described in equations  [ eq : mc1 ] and [ eq : mc2 ] .",
    "sampling @xmath78 from @xmath50 can be accomplished by simply selecting an effective area curve at random from @xmath17 .",
    "updating @xmath9 is more complicated , however , because we are using mcmc . we can not directly sample @xmath77 from @xmath122 as stipulated by equation  [ eq : mc2 ] .",
    "the pyblocxs update of @xmath77 depends on the previous iterate , @xmath118 .",
    "thus , we must iterate step  2 of the fully bayesian sampler several times before it converges and delivers an uncorrelated draw from @xmath122 . in this way , we iterate step  2 in the following sampler until the dependence on @xmath118 is negligible . to simplify notation",
    ", we display iteration @xmath123 rather than iteration @xmath76 ; notice that after @xmath124 repetitions , step  2 returns @xmath125 . in practice we expect a relatively small value of @xmath124 ( @xmath126 or fewer ) will be sufficient , see ",
    "[ s : ex : pbayes ] .",
    "the mcmc step for a given @xmath76 is as follows :    step  1 : : :    sample @xmath127 .",
    "step  2 : : :    for @xmath128 , : :    sample    @xmath129 .    once the mcmc sampler run is completed , the ` best - fit ' and confidence bounds for each parameter are typically determined from the mean and widths of the histograms constructed from the traces of @xmath130 ; or mean and widths of the contours ( for multiple parameters ) , as in figures  [ fig : arf_shift ] and [ fig : comp_e5_1to4 ] ; see park et al .  ( 2008 ) for discussion .      using the pca approximation results in a simple change to the algorithm in  [ s : alg : prag ] : step  1 is replaced by    step  1 : : :    set    @xmath131 ,    where @xmath132 are independent standard    normal random variables .",
    "because of the advantages in storage that this method confers , and the negligible effect that the approximation has on the result ( see  [ s : ex : comp ] ) , this is our recommended method when using mcmc to account for calibration uncertainty with data sets with ordinary counts .",
    "in this section we investigate optimal values of the tuning parameters needed by the algorithms and compare the performance of the algorithms with simulated and with real data . throughout , we use the absorbed power law simulations described in table  [ t : sim ] to illustrate our methods and algorithms .",
    "the eight simulations represent a @xmath133 design with the three factors being ( 1 ) data simulated with @xmath18 and with an extreme effective area curve from @xmath17 , ( 2 ) @xmath24 and @xmath27 nominal counts , and ( 3 ) two power law spectral models .",
    "these simulations include the four described in  [ s : cs : ex ] .",
    "we investigate the number of imputations required in multiple imputation studies in ",
    "[ s : ex : mi ] , and the number of subiterations required in mcmc runs in  [ s : ex : pbayes ] .",
    "we compare the results from the different algorithms ( multiple imputation with sampling and with pca , and pyblocxs with sampling and pca ) in detail in ",
    "[ s : ex : comp ] , and apply them to a set of quasar spectra in ",
    "[ s : ex : quasar ] .",
    "when using multiple imputation , we must decide how many imputations are required to adequately represent the variability in @xmath29 .",
    "although in the social sciences as few as 3 - 10 imputations are sometimes recommended ( e.g. , schafer 1997 ) , larger numbers more accurately represent uncertainty . to investigate this",
    "we fit spectra from simulation  1 and simulation  2 using _",
    ", with different values of @xmath54 , the number of imputations . for each value of @xmath54",
    "we generate @xmath54 effective area curves , @xmath99 , using equation  [ eq : pca ] , fit the simulated spectrum @xmath54 times , once with each @xmath99 , derive the @xmath134 error bars , and combine the @xmath54 fits using the multiple imputation combining rules in equations  [ eq : mi - est][eq : mi - total ] .",
    "this gives us a single total error bar for each parameter .",
    "we repeat this process 200 times for each value of @xmath54 to investigate the variability of the computed error bar for each value of @xmath54 .",
    "the result appears in the first two rows of figure  [ fig : mi ] . for small values of @xmath54 the error bars are often too small or too large . with @xmath54",
    "larger than about 20 , however , the multiple imputation error bars are quite accurate . even with @xmath135 , however , the error bars computed with multiple imputation are more representative of the actual uncertainty than when we fix the effective area at @xmath18 , which is represented by @xmath136 in figure  [ fig : mi ] . generally speaking ,",
    "@xmath137 is usually adequate , but @xmath137 to @xmath138 is better if computational time is not an issue . note that the size of the calibration sample @xmath29 is generally much larger than this , and it is therefore a fair sample to use in the bayesian sampling techniques described in  [ s : alg : mc ] .    when @xmath54 is relatively small",
    ", the computed @xmath107 error bars may severely underestimate the uncertainty , and must be corrected for the degrees of freedom in the imputations ( see equation  [ eq : mi - df ] ) . to illustrate this ,",
    "we compute the nominal coverage of the standard @xmath139 interval for each of the mi analyses described in the previous paragraph . when @xmath54 is large , such intervals",
    "are expected to contain the true parameter value 68.3% of the time , the probability that a gaussian random variable is within one standard deviation of its mean . with small @xmath54",
    ", however , the coverage decreases because of the extra uncertainty in the error bars .",
    "the bottom two rows of figure  [ fig : mi ] illustrate the importance of adjusting for the degrees of freedom , especially when using relatively small values of @xmath63 .",
    "the plots give the range of nominal coverage rates for one @xmath73 error bars . for large",
    "@xmath54 the coverage approaches @xmath140 , but for small @xmath54 it can be as low as 50 - 60% .",
    "this can be corrected by computing the degrees of freedom using equation  [ eq : mi - df ] and using @xmath109 instead of @xmath141 , as described in ",
    "[ s : alg : mi : full ] .      as noted in  [ s : alg : prag ] , in order to obtain a sample from the @xmath142 as in equation  [ eq : mc2 ] we must iterate pyblocxs @xmath124 times to eliminate the dependence of @xmath118 . to investigate how large @xmath124 must be ,",
    "we run pyblocxs on the spectra from simulations  1 and simulation  5 of table  [ t : sim ] , which were generated using the `` default '' and an `` extreme '' effective area curve .",
    "since simulation  5 was generated using the `` extreme '' effective area curve , it is the `` extreme '' curve that is actually `` correct '' and the `` default '' curve that is `` extreme '' . when running pyblocxs with the `` default '' effective area curve , we initiated the chain at the posterior mean of the parameters given the `` extreme '' curve , and vis versa .",
    "this ensures that we are using a relatively extreme starting value and will not underestimate how large @xmath124 must be to generate an essentially independent draw . the resulting autocorrelation and time series plots for @xmath20",
    "appear in figure  [ fig : acf ] .",
    "the autocorrelation plots report the correlation of @xmath77 and @xmath143 for each value of @xmath124 plotted on the horizontal axis .",
    "the plots show that for @xmath144 the autocorrelations are essentially zero for both spectra , and we can consider @xmath77 and @xmath145 to be essentially independent .",
    "similarly , the time series plots show that there is no effect of the starting value past the tenth iteration .",
    "similar plots for @xmath21 and the normalization parameter ( not included ) are essentially identical .",
    "thus , in all subsequent computations we set @xmath146 in the pragmatic bayesian samplers .",
    "generally speaking , the user should construct autocorrelation plots to determine how large @xmath124 must be in a particular setting .",
    "when we iterate step  2 in the pragmatic bayesian method , we are more concerned with the mixing of the chain once it has reached its stationary distribution , rather than convergence of the chain to its stationary distribution .",
    "this is because convergence to the stationary distribution will be assessed using the final chain of @xmath147 in the regular way , i.e. , using multiple chains ( gelman & rubin 1992 , van dyk et al .",
    "2001 ) . even after the stationary distribution has been reached , we need to obtain a value of @xmath148 in step  2 that is essentially independent of the previous draw , given @xmath149 .",
    "thus , we focus on the autocorrelation of the chain @xmath147 for fixed @xmath10 .",
    "this said , if the posterior of @xmath9 is highly dependent on @xmath10 and @xmath150 and @xmath151 are extreme within the calibration sample , that the conditional posterior distribution of @xmath9 given @xmath150 and @xmath151 may be be quite different and we may need to allow @xmath9 to converge to its new conditional posterior distribution .",
    "the time series plots in figure  6 investigate this possibility when extreme values of @xmath10 are used .",
    "luckily , the effect of these extreme starting values still burns off in just a few iterations , as is evident in figure  [ fig : acf ] .",
    "we discuss two classes of algorithms in ",
    "[ s : alg ] to account for calibration uncertainty in spectral analysis : multiple imputation , and a pragmatic bayesian mcmc sampler . for each , we consider two methods of exploring the calibration product sample space : first by directly sampling from the set of effective areas @xmath29 , and second by simulating an effective area from a compressed principal component representation . here",
    ", we evaluate the effectiveness of each of the four resulting algorithms , and show that they all produce comparable results , and are a significant improvement over not including the calibration uncertainty in the analysis .",
    "we fit each of the eight simulated data sets described in table  [ t : sim ] using each of the four algorithms .",
    "the first four simulations are identical to those described in ",
    "[ s : cs : ex ] .",
    "analyses carried out using multiple imputation all used @xmath137 imputations . for analyses using the pca approximation to @xmath29",
    ", we used @xmath152 .",
    "for pragmatic bayesian methods , we used @xmath146 inner iterations . figure  [ fig : comp_e5_1to4 ] gives the resulting estimated marginal posterior distributions for @xmath20 for each of the eight simulations and each of the four fitting algorithms along with the results when the effective area is fixed at @xmath18 .",
    "parameter traces ( also known as time series ) are also shown for all the simulations for the two mcmc algorithms ( see  [ s : alg : mc ] ) .",
    "although the fitted values differ somewhat ( see simulations 1 , 2 , 3 , and 6 ) among the four algorithms that account for calibration uncertainty , the differences are very small relative to the errors and overall the four methods are in strong agreement .",
    "when we do not account for calibration uncertainly , however , the error bars can be much smaller and in some cases the nominal 68% intervals do not cover the true value of the parameter ( see simulations 1 , 2 , 5 , and 6 , corresponding to larger nominal counts ) . when we do account for calibration uncertainty , only in simulation  6",
    "did the 68% intervals not contain the true value , and in this case the 95% ( not depicted ) do contain the true value .",
    "results for @xmath153 are similar but omitted from figure  [ fig : comp_e5_1to4 ] to save space .",
    "an advantage of using mcmc is that it maps out the posterior distribution ( under the conditional independence assumptions of section  [ s : meth : stat : marg ] ) rather than making a gaussian approximation to the posterior distribution .",
    "notice the non - gaussian features in the posterior distributions plotted for simulations 1 , 3 , 5 , and 7 ( corresponding to the harder spectral model ) .",
    "here we illustrate our methods with a realistic case , using x - ray spectra available for a small sample of radio loud quasars observed with the _ chandra _ x - ray observatory in 2002 ( siemiginowska et al .",
    "we performed the standard data analysis including source extraction and calibration with ciao software ( _ chandra _ interactive analysis of observations ) .",
    "the x - ray emission in radio loud quasars originates in a close vicinity of a supermassive black hole and could be due to an accretion disk or a relativistic jet .",
    "it is well described by a compton scattering process and the x - ray spectrum can be modeled by an absorbed power law : @xmath154 where @xmath155 is the absorption cross - section , and the three model parameters are : the normalization at 1  kev , @xmath156 ; the photon index of the power law , @xmath20 ; and the absorption column , @xmath21 .    the number of counts in the x - ray spectra varied between 8 and 5500 . after excluding two datasets ( obsid 3099 which had 8 counts , and obsid",
    "836 which is better described by a thermal spectrum ) , we reanalyzed the remaining 15 sources to include calibration uncertainty . in fitting each source",
    ", we included a background spectrum extracted from the same observation over a large annulus surrounding the source region .",
    "we adopted a complex background model ( a combination of a polynomial and 4 gaussians ) that was first fit to the blank - sky data provided by the _",
    "chandra _ x - ray center to fix its shape .",
    "while fitting the models to the source and background spectra , we only allow for the normalization of the background model to be free .",
    "this is an appropriate approach for very small background counts in the chandra spectra of point sources .",
    "we used this background model for all spectra ( except for two  obsids 3101 and 3106  that had short 5  ksec exposure times and small number of counts @xmath157 , for which the background was ignored ) . the original analysis ( siemiginowska et al .",
    "2008 ) did not take into account calibration errors , and as we show below the statistical errors are significantly smaller than the calibration errors for sources with a large number of counts .",
    "we fit each spectrum accounting for uncertainty in the effective area in two ways :    1 .   with the multiple imputation method in  [ s : alg : mi : pca ] using sherpa for the individual fits , and 2 .   with the pragmatic bayesian algorithm in  [ s : alg : prag : pca ] using pyblocxs for mcmc sampling .    both of these fits use the pca approximation using 14 observation - specific default effective area curves , @xmath158 in equation  [ eq : pca ] with @xmath152 .",
    "we use @xmath137 multiple imputations and @xmath146 subiterations in the pragmatic bayesian sampler . to illustrate the effect of accounting for calibration uncertainty , we compared the first fit with the sherpa fit that fixes the effective area at @xmath158 and each of the second and third fits with the pyblocxs fit that also fixes the effective area at @xmath158 .",
    "the results appear in figure  [ fig : quasar ] which compares the error bars computed with ( @xmath159 ) and without ( @xmath160 ) accounting for calibration uncertainty .",
    "the left panel uses _ sherpa _  and computes the total error using multiple imputation , and the right panel uses pyblocxs and computes the total error using the pragmatic bayesian method .",
    "the plots demonstrate the importance of properly accounting for calibration uncertainty in high - counts , high - quality observations .",
    "the systematic error becomes prominent with high counts because the statistical error is small , and @xmath159 deviates from @xmath160 , asymptotically approaching a value of @xmath161 .",
    "this asymptotic value represents the limiting accuracy of any observation carried out with this instrument , regardless of source strength or exposure duration . for the absorbed power law model applied here ,",
    "the systematic uncertainty on @xmath20 becomes comparable to the statistical error for spectra with counts @xmath162 , with the largest correction seen in obsid  866 , which had @xmath163 counts .",
    "in the previous sections , we have worked through a specific example ( chandra effective area ) in some detail .",
    "now , in this section , we present two more complete generalizations .",
    "the first is the case ignored previously , when the data have something interesting to say about the calibration uncertainties . in the second , we explain how to generalize the algorithms we worked through earlier to the full range of instrument responses , including energy redistribution matrices and point spread functions .      to avoid the assumption that the observed counts carry little information as to the choice of effective area curve , we can employ a fully bayesian approach that bases inference on the full posterior distribution @xmath164 . to do this via mcmc",
    ", we must construct a markov chain with stationary distribution @xmath164 , which can be accomplished by iterating a two - step gibbs sampler , for @xmath165 .    * a fully bayesian sampler *",
    "step  1 : : :    sample @xmath166 .",
    "step  2 : : :    sample    @xmath167 .",
    "notice that unlike in the pragmatic bayesian approach in ",
    "[ s : alg : mc ] , step  1 of this sampler requires @xmath10 to be updated given the current data . unfortunately , sampling @xmath168 is computationally quite challenging .",
    "the difficulty arrises because the fitted value of @xmath9 can depend strongly on @xmath10 .",
    "that is , calibration uncertainty can have a large effect on the fitted model , see drake et al .",
    "( 2006 ) and  [ s : cs : ex ] . from a statistical point of view",
    ", this means that given @xmath36 , @xmath9 and @xmath10 can be highly dependent and @xmath168 can depend strongly on @xmath77 .",
    "thus a large proportion of the replicates in @xmath29 may have negligible probability under @xmath169 and it can be difficult to find those that have appreciable probability without doing an exhaustive search .",
    "the computational challenges of a fully bayesian approach are part of the motivation behind our recommendation of the pragmatic bayesian method . despite the computational challenges ,",
    "there is good reason to pursue a fully bayesian sampler .",
    "insofar as the data are informative as to which replicates in @xmath29 are more  or less  likely , the dependence between @xmath9 and @xmath10 can help us to eliminate possible values of @xmath9 along with replicates in @xmath29 , thereby reducing the total error bars for @xmath9 .",
    "work to tackle the computational challenges of the fully bayesian approach is ongoing .",
    "in general , the response of a detector to incident photons arriving at time @xmath67 can be written as @xmath170 where @xmath171 and @xmath7 are the measured photon location and energy ( or the detector channel ) , while @xmath172 and @xmath8 are the true photon sky location and energy ; the source physical model @xmath173 describes the energy spectrum , morphology ( point , extended , diffuse , etc . ) , and variability with parameters @xmath9 ; and @xmath174 are the expected counts in detector channel space .",
    "calibration is carried out using well known instances of @xmath173 to determine the quantities @xmath175 it is important to note that all of the quantities in equation  [ eq : resp ] have uncertainties associated with them .",
    "our goal is providing a fast , reliable , and robust strategy to incorporate the jittering patterns in all of the calibration products and to draw proper inference , best fits and error bars , reflecting calibration uncertainty .    in principle , using a calibration sample to represent uncertainty and the statistical methods for incorporating the calibration sample described in ",
    "[ s : meth ] and  [ s : alg ] can be applied directly to calibration uncertainty for any of the calibration products .",
    "the use of pca , however , to summarize the calibration sample may not be robust enough for higher dimensional and more complex calibration products .",
    "more sophisticated image analysis techniques or hierarchically applied pca may be more appropriate .",
    "our basic strategy , however , of providing instrument - specific summaries of the variability in the calibration uncertainty and observation - specific measures of the mean ( or default ) calibration product , is quite general .",
    "thus , in this section , we focus on the generalization of equation  [ eq : pca0 ] and begin by rephrasing the equation as @xmath176 here the mean is the mean of the calibration sample , the offset is the shift that we impose on the center of distribution of the calibration uncertainty to account for observation - specific differences , the explained variability is the portion of the variability that summarize in parametric and/or systematic way ( e.g. , using pca ) , and the residual variability is the portion of the variability left unexplained by the systematic summary .",
    "these four terms correspond to the four terms in equation  [ eq : pca0 ] .",
    "the formulation in equation  [ eq : gencompgen ] removes the necessity of depending solely on pca to summarize variance in the calibration sample , and allows us to use a variety of methods to generate the simulated calibration products .",
    "for example , we can even include such loosely stated measures of uncertainty as `` the effective area is uncertain by x% at wavelength y '' .",
    "this formulation is not limited to describing effective areas alone , but can also be used to encompass the calibration uncertainty in response matrices and point spread functions",
    ". the precise method by which the variance terms are generated may vary widely , but in all foreseeable cases they can be described as in equation  [ eq : gencompgen ] , with an offset term and a random variance component added to the mean calibration product , and with an optional residual component . the calibration sample simulated in this way",
    "form an informative prior @xmath177 that could be used like @xmath50 in equation  [ eq : mc1 ] .",
    "some potential methods of describing the variance terms are :    1 .",
    "when a large calibration sample is available , the random component is simply the full set of calibration products in the sample .",
    "when using a monte carlo for model fitting , as in  [ s : meth : stat].3 , a random index is chosen at each iteration and the calibration product corresponding to that index is used for that iteration .",
    "this process preserves the weights of the initial calibration sample . in this scenario",
    "the residual component is identically zero . 2 .",
    "if the calibration uncertainty is characterized by a multiplicative polynomial term in the source model , the explained variance component in equation  [ eq : gencompgen ] can be obtained by sampling the parameters of the polynomial , from a gaussian distribution , using their best - fit values and the estimated errors .",
    "these simulated calibration products can then be used to modify the nominal products inside each iteration .",
    "thus , the offset and residual terms are zero , and only the polynomial parameter best - fit values and errors need to be stored .",
    "3 .   if a calibration product is newly identified , it may be systematically off by a fixed but unknown amount over a small passband , and users can specify their own estimate of calibration uncertainty as a randomized additive constant term over the relevant range .",
    "this is essentially equivalent to using a correction with a first - order polynomial .",
    "the stored quantities are the average offset , the bounds over which the offset can range , and a pointer specifying whether to generate uniform or gaussian deviates over that range .",
    "we have developed a method to handle in a practical way the effect of uncertainties in instrument response on astrophysical modeling , with specific application to _ chandra_/acis instrument effective area .",
    "our goal has been to obtain realistic error bars on astrophysical source model parameters that include both statistical and systematic errors . for this purpose",
    ", we have developed a general and comprehensive strategy to describe and store calibration uncertainty and to incorporate them into data analysis .",
    "starting from the full , precise , but cumbersome objective - function of the parameters , data , and instrument uncertainties , we adopt a bayesian posterior - probability framework and simplify it in a few key places to make the problem tractable .",
    "this work holds practical promise for a generalized treatment of instrumental uncertainties in not just spectra but also imaging , or any kind of higher - dimensional analyses ; and not just x - rays , but across wavelengths and even to particle detectors .",
    "our scheme treats the possible variations in calibration as an informative prior distribution while estimating the posterior probability distributions of the source model parameters .",
    "thus , the effects of calibration uncertainty is automatically included in the result of a single fit .",
    "this is different from a usual sensitivity study in that we provide an actual uncertainty estimate .",
    "our analysis shows that systematic error contribution in high counts spectra is more significant than when there are few counts ; therefore , including calibration uncertainty in a spectral fitting strategy is highly recommended for high quality data .",
    "we adopt the calibration uncertainty variations , in particular the effective area variations for the _ chandra_/acis - s detector , described by drake et al .",
    "( 2006 ) , as an exemplar case . using the effective area sample @xmath17 simulated by them , we    1 .",
    "show that variations in effective areas lead to large variations in fitted parameter values ; 2 .",
    "demonstrate that systematic errors are relatively more important for high counts , when statistical errors are small ; 3 .   describe how the calibration sample can be effectively compressed and summarized by a small number of components from a principal components analysis ; 4 .",
    "outline two separate algorithms with which to incorporate systematic uncertainties within spectral analysis : 1 .",
    "an approximate , but quick method based on the multiple imputation combining rule that carries out spectral fits for different instances of the effective area and merges the mean of the variances with the variance of the means ; and 2 .   a pragmatic bayesian method that incorporates sampling of the effective areas as from a prior distribution within an mcmc iteration scheme .",
    "detail two methods of sampling @xmath99 : directly from the calibration sample @xmath17 , and via a pca decomposition 6 .",
    "show that @xmath178 representative samples of @xmath99 are needed to obtain relatively reliable estimates of uncertainty ; 7 .",
    "apply the method to a real dataset of a sample of quasars and show that known systematic uncertainties require that , e.g. , the power - law index @xmath20 can not be determined with an accuracy better than @xmath179 ; and 8 .",
    "discuss future directions of our work , both in relaxing the constraint of not allowing the calibration sample @xmath17 to be affected by the data , and in generalizing the technique to other sources of calibration uncertainty .",
    "this work was supported by nasa aisrp grant nng06gf17 g ( hl , ac , vlk ) , and cxc nasa contract nas8 - 39073 ( vlk , as , jjd , pr ) , nsf grants dms 04 - 06085 and dms 09 - 07522 ( dvd , ac , sm , tp ) , and nsf grants dms-0405953 and dms-0907185 ( xlm ) .",
    "we acknowledge useful discussions with herman marshall , alex blocker , jonathan mcdowell , and arnold rots .",
    "aguirre , et al .",
    ", 2011 , apjs , 192 , 4 anderson , t.w . , 2003 ,",
    "_ an introduction to multivariate statistical analysis _ , 3@xmath180 ed . , john wiley & sons , ny arnaud , k.  a. , 1996 , astronomical data analysis software and systems v , 101 , 17 bevington , p.r . , and robinson , d.k . , 1992 ,",
    "_ data reduction and error analysis for the physical sciences _ , mcgraw - hill , 2@xmath181 ed .",
    "bishop , c. , 2007 , _ pattern recognition and machine learning _",
    ", 1@xmath182 ed . ,",
    "springer , ny bridle , s.l . , et al . , 2002 ,",
    "mnras , 335(4 ) , 1193 brown , a. , 1997 , _ the neutron and the bomb : a biography of sir james chadwick _ , oxford university press butler , r. p. , marcy , g. w. , williams , e. , mccarthy , c. , dosanjh , p. , & vogt , s. s. 1996 , pasp , 108 , 500 casella , g. , and berger , r.l . , 2001 ,",
    "_ statistical inference _",
    ", 2@xmath181 ed .",
    ", duxbury press , ca christie , m.a . , et al .",
    ", 2005 , _ los alamos science _ 29 , 6 conley , et al .",
    ", 2011 , _ apjs _ , 192 , 1 , 1 cox , m.g . , and harris , p.m. , 2006 , meas .",
    "technol . , 17 , 533 david , l. , et al . , 2007 , chandra calibration workshop , # 2007.23 davis , j.e . , 2001 , apj , 548 , 1010 drake , j.j . , et al . , 2006 , ferraty , f. , and vieu , p. , 2006",
    ", _ nonparametric functional data analysis : theory and practice _ , springer , 1@xmath182 ed .",
    ", ny forrest , d.j .",
    "et al . , 1997 , technical report , new hampshire univ .",
    "durham , nh forrest , d.j . , 1988 , baas , 20 , p. 740 freeman , p. , et al .",
    ", 2001 , proc .",
    "spie , 4477 , 76 gelman , a. , carlin , j.b . ,",
    "stern , h.s . , and rubin , d.b .",
    "bayesian data analysis _",
    ", second edition , chapman & hall / crc texts in statistical science gelman , a. , and rubin , d.b .",
    ", 1992 , statistical sci . , 7 , 457 green , p.j . , 1980 , _ interpreting multivariate data _ , 3 - 19 .",
    "chinchester : wiley .",
    "p241 gregory , p.c . , 2005 ,",
    "_ bayesian logical data analysis for the physical sciences _ , in _ x - ray astronomy handbook _ , cambridge university press grimm , h .- j . , et al . , 2009 ,",
    "hanlon , l.o .",
    ", et al . , 1995 , ap&ss , 231 , 157 harel , o. , and zhou , x.  h.  a. , 2005 , statistics in medicine , 26 , 3057 heinrich , j. , and lyons , l. , 2007 , ann .",
    "nucl .  part .",
    ", 57 , 145 heydorn , k. , and anglov , t. , 2002 , accred .  qual .",
    "7 , 153 jarosik , n. , et al . , 2011 , _",
    "apjs _ , 192 , 14 jolliffe , i. , 2002 , _ principal component analysis _",
    ", 2@xmath181 ed . , springer , ny kashyap , v.l . , et al . , 2008 , proc .",
    "spie , 7016 , 7016p.1 kim , a.g .",
    ", and miquel , r. , 2006 , astroparticle physics , 24 , 45 li , k .- h . , et al . , 1991 , statistica sinica , 1 , 65 ligo collaboration 2010 , _ nuclear instruments and methods in physics research a _ , 624 , 223 mandel , k.s . , wood - vasey , w.m . ,",
    "friedman , a.s . , and kirshner , r.p . , 2009 , _",
    "apj _ , 704 , 629 maness , et al .",
    ", 2011 , _ apj _ , 707 , 1098 marshall , h. , 2006 , iachec , lake arrowhead , ca mather , j.c . ,",
    "fixsen , d.j . ,",
    "shafer , r.a . ,",
    "mosier , c. , and wilkinson , d.t .",
    ", 1999 , apj , 512 , 511 meng , x .-",
    "l . , 1994 , statistical science , 9 , 538 mohr , p.j . ,",
    "taylor , b.n . , and newell , d.b . , 2008 , j. phys",
    "chem . ref .",
    "data , 37 , 3 , 1187 osbourne 1991 , international statistical review , 59 , 3 , 309 park , t. , et al . , 2008 , apj , 688 , 807 ramsay , j. , and silverman , b.w . , 2005 , _ functional data analysis _ , springer , 2@xmath181 ed . , ny refsdal , b. et al .",
    "2009 , proc .  of the 8th python in science conference , ( scipy 2009 ) , g. varoquaux , s. van der walt , j. millman ( eds . ) , pp .",
    "51 - 57 ( 2009 ) rosset , c. , et al .",
    "2010 , a&a , 520 , 13 rubin , d.b .",
    "1987 , _ multiple imputation for nonresponse in surveys _ , j.wiley & sons , ny rutherford , e.s . , and chadwick , j. , 1911 , proc .",
    "london , 24 , 141 schafer , j.  l. , 1997 , _ analysis of incomplete multivariate data _ , chapman & hall , new york schmelz , j.t .",
    "2009 , apj , 704 , 863 siemiginowska , a. , et al . , 2008 ,",
    "apj , 684 , 811 simpson , g. , and mayer - hasselwander , h. , 1986 , a&a , 162 , 340 sundberg , rolf , 1999 , scandinavian journal of statistics , 26 , 161 taris , et al . , 2011 , a&a 526 , a25 thoms , maraston , & johansson , 2010 , accepted for publication in mnras van dyk , d. , et al . , 2001 ,",
    "apj , 548 , 224 virgo collaboration 2011 , classical and quantum gravity , 28 , 2 , 5005     is plotted as a solid black curve .",
    "the bottom panel is constructed in the same manner , but using @xmath30 , in order to magnify the structure in @xmath17 .",
    "the curves in @xmath17 form a complex tangle that appears to defy any systematic pattern .",
    "as we shall see , we can use principle component analysis to form a concise summary of @xmath29.,width=624 ]     with the largest maximum in blue and the 15 curves with the smallest maximum in red , each with @xmath18 subtracted off .",
    "the solid black horizontal line at zero represents @xmath18 .",
    "the two columns in the six lower panels correspond to simulations  1 and 2 , respectively and plot the posterior distributions of @xmath20 and @xmath21 using each of the 31 effective area curves in the first panel .",
    "the rows of the bottom six panels correspond to the posterior distribution of @xmath20 , the 95.4% contour of the joint posterior distribution , and the posterior distribution of @xmath21 .",
    "the colors of the plotted posterior distributions indicate the effective area curve that was used to generate the distribution .",
    "the solid vertical black lines in the the second and fourth rows indicate the values of the parameters used with @xmath18 to generate simulations  1 and 2 .",
    "the effect of the choice of effective area curves on the posterior distributions is striking.,title=\"fig:\",width=480 ]   with the largest maximum in blue and the 15 curves with the smallest maximum in red , each with @xmath18 subtracted off .",
    "the solid black horizontal line at zero represents @xmath18 .",
    "the two columns in the six lower panels correspond to simulations  1 and 2 , respectively and plot the posterior distributions of @xmath20 and @xmath21 using each of the 31 effective area curves in the first panel .",
    "the rows of the bottom six panels correspond to the posterior distribution of @xmath20 , the 95.4% contour of the joint posterior distribution , and the posterior distribution of @xmath21 .",
    "the colors of the plotted posterior distributions indicate the effective area curve that was used to generate the distribution .",
    "the solid vertical black lines in the the second and fourth rows indicate the values of the parameters used with @xmath18 to generate simulations  1 and 2 .",
    "the effect of the choice of effective area curves on the posterior distributions is striking.,title=\"fig:\",height=480 ]     ( row  1 ) and @xmath21 ( row  2 ) when fitting simulation  3 ( column  1 with @xmath27 counts ) and simulation  1 ( column  2 with @xmath24 counts ) .",
    "the replicates in each panel correspond to 30 effective area curves randomly selected from @xmath29 .",
    "the posterior distributions plotted with solid lines were constructed using @xmath18 .",
    "the statistical errors are smaller with the larger data set so that calibration errors are relatively more important . , title=\"fig:\",width=312 ] ( row  1 ) and @xmath21 ( row  2 ) when fitting simulation  3 ( column  1 with @xmath27 counts ) and simulation  1 ( column  2 with @xmath24 counts ) .",
    "the replicates in each panel correspond to 30 effective area curves randomly selected from @xmath29 .",
    "the posterior distributions plotted with solid lines were constructed using @xmath18 .",
    "the statistical errors are smaller with the larger data set so that calibration errors are relatively more important .",
    ", title=\"fig:\",width=312 ]     and give intervals for each energy bin that contain 100% and 68.3% of the calibration sample .",
    "the dashed and dotted lines outline intervals for each energy bin containing 100% and 68.3% of 1000 pca replicates of the effective area , sampled using equation  [ eq : pca0 ] .",
    "the correspondence between the calibration sample and the pca sample is quite good , especially for the 68.3% intervals .",
    "the solid horizontal line is @xmath18 and dotted line near it is the almost identical @xmath183 .",
    "the other three panels give histograms of the calibration sample ( grey ) and the pca sample ( solid line ) in each of three energy bins , represented by @xmath184 signs in the first panel.,title=\"fig:\",width=624 ]   and give intervals for each energy bin that contain 100% and 68.3% of the calibration sample .",
    "the dashed and dotted lines outline intervals for each energy bin containing 100% and 68.3% of 1000 pca replicates of the effective area , sampled using equation  [ eq : pca0 ] .",
    "the correspondence between the calibration sample and the pca sample is quite good , especially for the 68.3% intervals .",
    "the solid horizontal line is @xmath18 and dotted line near it is the almost identical @xmath183 .",
    "the other three panels give histograms of the calibration sample ( grey ) and the pca sample ( solid line ) in each of three energy bins , represented by @xmath184 signs in the first panel.,title=\"fig:\",width=624 ]    .",
    "we show the result of varying @xmath54 on fits carried out for spectra from simulation  1 ( left column ) and simulation  2 ( right column ) . for each @xmath185",
    ", we generate @xmath66 effective area curves @xmath186 using equation  [ eq : pca ] , and carry out separate fits for each using _ sherpa _ , and combine the the results of the fits using the multiple imputation combining rules ( equations  [ eq : mi - est][eq : mi - total ] ) .",
    "this gives us one value for the combined ( statistical and systematic ) error bar .",
    "we repeat this process 200 times for each @xmath66 to investigate the variability of the computed error bar .",
    "the average computed errors ( filled symbols ) are shown for the power - law index @xmath20 ( top row ) and the absorption column density @xmath21 ( second row ) as a function of @xmath66 along with the uncertainty on the errors due to sampling ( thin vertical bars ) .",
    "the total error is grossly underestimated for @xmath187 ( computed for only the default effective area ) , and the uncertainty on the error decreases for @xmath188 .",
    "typically , @xmath189 is sufficient to obtain a reasonably accurate estimate of the total error .",
    "we also show the coverage fraction for the derived error bars for @xmath20 ( third row from the top ) and @xmath21 ( bottom row ) .",
    "the coverage is small for small @xmath66 because the degrees of freedom is small ( see equation  [ eq : mi - df ] ) but asymptotically approaches gaussian coverage of @xmath190 for large @xmath66 .",
    ", height=480 ]     is shown for four cases , where a spectrum is simulated using one effective area curve and the fit is possibly carried out with another .",
    "this explores the dependence of the fitting methodology ( codified in the routine pyblocxs ) on misspecified calibration .",
    "the top row shows the acf for simulation  1 ( generated using `` default '' effective area curve ; see table  [ t : sim ] ) and the bottom row for simulation  5 ( generated using an `` extreme '' effective area curve ) .",
    "the diagonal plots show the acf when the `` correct '' effective curve is used to fit the spectrum , i.e. , the same curve as was used to generate it , and the cross - diagonal plots show the case when the fitting is carried out using a different effective area curve .",
    "the cases in the left column both use the `` default '' effective area to fit the simulated spectra , and the cases in the right column both use the `` extreme '' curve .",
    "the autocorrelation functions demonstrate that @xmath191 and @xmath192 are essentially uncorrelated regardless of whether the correct effective area curve was used in the fit or not .",
    "thus , we set @xmath146 in our pragmatic bayesian samplers . , width=624 ]    , shown for same cases as the autocorrelation cases shown before . while the autocorrelation determines the `` stickiness '' of the mcmc iterations , the time series demonstrates that choosing misspecified calibration files does not have any effect on the convergence of the solutions .",
    "the traces are shown in the same order as before , for all iterations @xmath76 .",
    "the inset shows the last 50 iterations , with @xmath191 denoted by filled circles , and consecutive iterations connected by thin straight lines .",
    "the necessity of using @xmath193 is apparent in the slow changes in the values of @xmath191 .",
    ", width=624 ]     as applied to the simulated spectra 1 - 4 in table  [ t : sim ] .",
    "these are spectra which are generated using the default effective area .",
    "the `` true '' value of the power - law index parameter that was used to generate the simulated spectra is shown as the vertical dashed line .",
    "for each simulation , posterior probability density functions of the power - law index parameter are computed using the pragmatic bayesian with pca ( black solid curve ;  [ s : alg : mc].4 ) , pragmatic bayesian with sampling from @xmath29 ( red dashed curve ;  [ s : alg : mc].3 ) , multiple imputation with pca ( green dotted curve ;  [ s : alg : mi].2 ) , multiple imputation with samples from @xmath29 ( brown dot - dashed curve ;  [ s : alg : mi].1 ) , and the combined posteriors from individual runs using the full sample @xmath29 ( purple dash - dotted curve ) .",
    "results for the column density parameter @xmath21 are similar .",
    "we use @xmath137 samples for multiple imputation .",
    "the density curves are obtained from smoothed histograms of mcmc traces from pyblocxs for the bayesian cases , and are gaussians with the appropriate mean and variance obtained via fitting with xspecv12 for the multiple imputation cases .",
    "also shown are the 68% equal - tail intervals as horizontal bars , with the most probable value of the photon index indicated with an ` x ' for each of these case , and additionally for the case where a fixed effective area was used to obtain only the statistical error .",
    "note that in all cases , fitting with the default effective area alone leads to an underestimate of the true uncertainty in the fitted parameter .",
    ", title=\"fig:\",width=307 ]   as applied to the simulated spectra 1 - 4 in table  [ t : sim ] .",
    "these are spectra which are generated using the default effective area .",
    "the `` true '' value of the power - law index parameter that was used to generate the simulated spectra is shown as the vertical dashed line .",
    "for each simulation , posterior probability density functions of the power - law index parameter are computed using the pragmatic bayesian with pca ( black solid curve ;  [ s : alg : mc].4 ) , pragmatic bayesian with sampling from @xmath29 ( red dashed curve ;  [ s : alg : mc].3 ) , multiple imputation with pca ( green dotted curve ;  [ s : alg : mi].2 ) , multiple imputation with samples from @xmath29 ( brown dot - dashed curve ;  [ s : alg : mi].1 ) , and the combined posteriors from individual runs using the full sample @xmath29 ( purple dash - dotted curve ) .",
    "results for the column density parameter @xmath21 are similar .",
    "we use @xmath137 samples for multiple imputation .",
    "the density curves are obtained from smoothed histograms of mcmc traces from pyblocxs for the bayesian cases , and are gaussians with the appropriate mean and variance obtained via fitting with xspecv12 for the multiple imputation cases .",
    "also shown are the 68% equal - tail intervals as horizontal bars , with the most probable value of the photon index indicated with an ` x ' for each of these case , and additionally for the case where a fixed effective area was used to obtain only the statistical error .",
    "note that in all cases , fitting with the default effective area alone leads to an underestimate of the true uncertainty in the fitted parameter .",
    ", title=\"fig:\",width=307 ] +   as applied to the simulated spectra 1 - 4 in table  [ t : sim ] .",
    "these are spectra which are generated using the default effective area .",
    "the `` true '' value of the power - law index parameter that was used to generate the simulated spectra is shown as the vertical dashed line .",
    "for each simulation , posterior probability density functions of the power - law index parameter are computed using the pragmatic bayesian with pca ( black solid curve ;  [ s : alg : mc].4 ) , pragmatic bayesian with sampling from @xmath29 ( red dashed curve ;  [ s : alg : mc].3 ) , multiple imputation with pca ( green dotted curve ;  [ s : alg : mi].2 ) , multiple imputation with samples from @xmath29 ( brown dot - dashed curve ;  [ s : alg : mi].1 ) , and the combined posteriors from individual runs using the full sample @xmath29 ( purple dash - dotted curve ) .",
    "results for the column density parameter @xmath21 are similar .",
    "we use @xmath137 samples for multiple imputation .",
    "the density curves are obtained from smoothed histograms of mcmc traces from pyblocxs for the bayesian cases , and are gaussians with the appropriate mean and variance obtained via fitting with xspecv12 for the multiple imputation cases .",
    "also shown are the 68% equal - tail intervals as horizontal bars , with the most probable value of the photon index indicated with an ` x ' for each of these case , and additionally for the case where a fixed effective area was used to obtain only the statistical error .",
    "note that in all cases , fitting with the default effective area alone leads to an underestimate of the true uncertainty in the fitted parameter .",
    ", title=\"fig:\",width=307 ]   as applied to the simulated spectra 1 - 4 in table  [ t : sim ] .",
    "these are spectra which are generated using the default effective area .",
    "the `` true '' value of the power - law index parameter that was used to generate the simulated spectra is shown as the vertical dashed line .",
    "for each simulation , posterior probability density functions of the power - law index parameter are computed using the pragmatic bayesian with pca ( black solid curve ;  [ s : alg : mc].4 ) , pragmatic bayesian with sampling from @xmath29 ( red dashed curve ;  [ s : alg : mc].3 ) , multiple imputation with pca ( green dotted curve ;  [ s : alg : mi].2 ) , multiple imputation with samples from @xmath29 ( brown dot - dashed curve ;  [ s : alg : mi].1 ) , and the combined posteriors from individual runs using the full sample @xmath29 ( purple dash - dotted curve ) .",
    "results for the column density parameter @xmath21 are similar .",
    "we use @xmath137 samples for multiple imputation .",
    "the density curves are obtained from smoothed histograms of mcmc traces from pyblocxs for the bayesian cases , and are gaussians with the appropriate mean and variance obtained via fitting with xspecv12 for the multiple imputation cases .",
    "also shown are the 68% equal - tail intervals as horizontal bars , with the most probable value of the photon index indicated with an ` x ' for each of these case , and additionally for the case where a fixed effective area was used to obtain only the statistical error .",
    "note that in all cases , fitting with the default effective area alone leads to an underestimate of the true uncertainty in the fitted parameter .",
    ", title=\"fig:\",width=307 ]    .",
    "these are spectra which are generated using an extreme instance of an effective area from @xmath29 .",
    "the fits when only one effective area is used are done with the default effective area .",
    "note that in many cases , not incorporating the calibration uncertainties results in intervals for the parameter which does not contain the true value .",
    ", title=\"fig:\",width=307 ] .",
    "these are spectra which are generated using an extreme instance of an effective area from @xmath29 .",
    "the fits when only one effective area is used are done with the default effective area .",
    "note that in many cases , not incorporating the calibration uncertainties results in intervals for the parameter which does not contain the true value .",
    ", title=\"fig:\",width=307 ] + .",
    "these are spectra which are generated using an extreme instance of an effective area from @xmath29 .",
    "the fits when only one effective area is used are done with the default effective area .",
    "note that in many cases , not incorporating the calibration uncertainties results in intervals for the parameter which does not contain the true value .",
    ", title=\"fig:\",width=307 ] .",
    "these are spectra which are generated using an extreme instance of an effective area from @xmath29 .",
    "the fits when only one effective area is used are done with the default effective area .",
    "note that in many cases , not incorporating the calibration uncertainties results in intervals for the parameter which does not contain the true value .",
    ", title=\"fig:\",width=307 ]     for each of the 8 simulations .",
    "all the simulations are shown on the same plot , rescaled ( to depict the fractional deviation from the mean , inflated by a factor of 3 ) and offset ( by an integer corresponding to the number assigned to the simulation ) for clarity .",
    "the traces for both the mcmc+pca ( pragmatic bayesian algorithm using pca to generate new effective areas ; solid black lines ) and mcmc+sample ( pragmatic bayesian algorithm with sampling from @xmath29 ; dotted red lines ) are shown , with the latter overlaid on the former . the last 50 iterations are shown zoomed out in the absissa for clarity , and shows each transformed @xmath191 as filled circles , connected by thin lines of the corresponding style and color . note that all iterations @xmath76 are shown , but in the calculations of the posterior probability distributions , only every @xmath194 iteration , where @xmath146 , is used ( see figure  [ fig : acf ] ) . , width=652 ]    ) are shown .",
    "the abscissae represent the statistical uncertainty @xmath160 as derived by adopting a fixed , nominal effective area , and fit with absorbed power - law models using _",
    "ciao_/sherpa ( stronger sources tend to have smaller error bars ) .",
    "they are compared with the total error , @xmath159 derived using ( a ) the multiple imputation combining rule (  [ s : alg : mi].2 ) with _",
    "ciao_/sherpa ( @xmath195 ) , and ( b ) the pragmatic bayesian method with pca (  [ s : alg : mc].4 ) , with pyblocxs .",
    "( similar results are obtained when using the pragmatic bayesian method for the full sample of effective areas . )",
    "the different symbols correspond to the analysis carried out for different observations .",
    "the dotted line represents equality , where the total error is identical to the statistical error .",
    "the systematic error can not be ignored when the statistical error is small , and represents the limiting accuracy of a measurement . ,",
    "title=\"fig:\",width=307 ] ) are shown .",
    "the abscissae represent the statistical uncertainty @xmath160 as derived by adopting a fixed , nominal effective area , and fit with absorbed power - law models using _",
    "ciao_/sherpa ( stronger sources tend to have smaller error bars ) .",
    "they are compared with the total error , @xmath159 derived using ( a ) the multiple imputation combining rule (  [ s : alg : mi].2 ) with _",
    "ciao_/sherpa ( @xmath195 ) , and ( b ) the pragmatic bayesian method with pca (  [ s : alg : mc].4 ) , with pyblocxs .",
    "( similar results are obtained when using the pragmatic bayesian method for the full sample of effective areas . )",
    "the different symbols correspond to the analysis carried out for different observations .",
    "the dotted line represents equality , where the total error is identical to the statistical error .",
    "the systematic error can not be ignored when the statistical error is small , and represents the limiting accuracy of a measurement .",
    ", title=\"fig:\",width=307 ]    c|l @xmath10 & effective area ( arf ) curve + @xmath196 & replicate @xmath10 generated from pca representation of the calibration sample + @xmath18 & the default effective area curve .",
    "+ @xmath158 & the observation specific effective area curve .",
    "+ @xmath15 & effective area curve @xmath197 in the calibration sample + @xmath17 & a set of effective areas , the calibration sample + @xmath198 & average offset of @xmath17 from @xmath18 + @xmath72 & the between imputation ( or systematic ) variance of @xmath65 .",
    "+ @xmath70 & diagonal element @xmath66 of @xmath72 + @xmath8 & energy of incident photon + @xmath7 & energy channel at which the detector registers the incident photon + @xmath199 & random variate generated from the standard normal distribution + @xmath200 & fractional variance of component @xmath197 in the pca representation + @xmath124 & number of inner iterations in pyblocxs , typically @xmath201 + @xmath91 & number of components used in pca analysis , here @xmath202 + @xmath203 & principal component number or index + @xmath204 & the superscript indicates the running index of random draws + @xmath117 & an mcmc kernel + @xmath205 & the mcmc kernel used in pyblocks + @xmath13 & number of replicate effective area curves in calibration sample + @xmath197 & replicate effective area number or index , or principal component number + @xmath66 & imputation number or index + @xmath54 & number of imputations + @xmath206 & response of a detector to incident photons , see equation  [ eq : sim_arf ] + @xmath37 & objective function ( posterior distribuiton , likelihood , or perhaps @xmath4 ) + @xmath11 & point spread function ( psf ) + @xmath12 & energy redistribution matrix ( rmf ) + @xmath207 & eigenvalue or pc coefficient of component @xmath197 in the pca representation + @xmath208 & astrophysical source model + @xmath64 & total variance of @xmath65 .",
    "+ @xmath87 & eigen- or feature - vector for component @xmath197 in the pca representation + @xmath71 & the within imputation ( or statistical ) variance of @xmath65 .",
    "+ @xmath69 & diagonal elements @xmath66 of @xmath71 + @xmath209 & true sky location of photons + @xmath210 & locations of incident photons as registered by detector + @xmath36 & data , typically used here as counts spectra in detector pi bins + @xmath45 & data and physical calculations used by calibration scientists + @xmath9 & model parameter of interest + @xmath65 & estimate of @xmath9 + @xmath55 & estimate of @xmath9 corresponding to imputed effective area @xmath66 + @xmath56 & estimates variance of @xmath55 + @xmath160 & @xmath211 , representing the statistical error on @xmath9 + @xmath159 & @xmath73 , representing the total error on @xmath9 + @xmath102 & a sum of the smaller components , j+1 to l in the pca representation +"
  ],
  "abstract_text": [
    "<S> while considerable advance has been made to account for statistical uncertainties in astronomical analyses , systematic instrumental uncertainties have been generally ignored . </S>",
    "<S> this can be crucial to a proper interpretation of analysis results because instrumental calibration uncertainty is a form of systematic uncertainty . ignoring it can underestimate error bars and introduce bias into the fitted values of model parameters . </S>",
    "<S> accounting for such uncertainties currently requires extensive case - specific simulations if using existing analysis packages . </S>",
    "<S> here we present general statistical methods that incorporate calibration uncertainties into spectral analysis of high - energy data . </S>",
    "<S> we first present a method based on multiple imputation that can be applied with any fitting method , but is necessarily approximate . </S>",
    "<S> we then describe a more exact bayesian approach that works in conjunction with a markov chain monte carlo based fitting . </S>",
    "<S> we explore methods for improving computational efficiency , and in particular detail a method of summarizing calibration uncertainties with a principal component analysis of samples of plausible calibration files . </S>",
    "<S> this method is implemented using recently codified _ </S>",
    "<S> chandra _  effective area uncertainties for low - resolution spectral analysis and is verified using both simulated and actual _ </S>",
    "<S> chandra _  data . </S>",
    "<S> our procedure for incorporating effective area uncertainty is easily generalized to other types of calibration uncertainties . </S>"
  ]
}