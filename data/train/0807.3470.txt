{
  "article_text": [
    "our aim is bayesian discriminative inference in the case where the model family @xmath3 is known to be incorrect . here",
    "@xmath4 is a data vector and @xmath5 its class , and the @xmath6 are parameters of the model family . by discriminative we mean predicting the conditional distribution @xmath7 .",
    "the bayesian approach of using the posterior of the generative model family @xmath3 has not been shown to be justified in this case , and it is known that it does not always generalize well to new data ( in case of point estimates , see for example @xcite ; in this paper we provide a toy example that illustrates the fact for posterior distributions ) .",
    "therefore alternative approaches such as bayesian regression are applied @xcite .",
    "it can be argued that the best solution is to improve the model family by incorporating more prior knowledge .",
    "this is not always possible or feasible , however , and simplified models are being generally used , often with good results . for example , it is often practical to use mixture models even if it is known a priori that the data can not be faithfully described by them ( see for example @xcite ) .",
    "there are good reasons for still applying bayesian - style techniques @xcite but the general problem of how to best do inference with incorrect model families is still open .    in practice ,",
    "the usual method for discriminative tasks is bayesian regression .",
    "it disregards all assumptions about the distribution of @xmath4 , and considers @xmath4 only as covariates of the model for @xmath5 .",
    "bayesian regression may give superior results in discriminative inference , but the omission of a generative model for @xmath4 ( although it may be readily available ) makes it difficult to handle missing values in the data . numerous heuristic methods for imputing missing values have been suggested , see for example @xcite , but no theoretical arguments of their optimality have been presented . here",
    "we assume that we are given a generative model family of the full data ( @xmath4 , @xmath5 ) , and therefore have a generative mechanism readily available for imputing missing values .    from the generative modeling perspective , bayesian regression ignores any information about @xmath5 supplied by the marginal distribution of @xmath4 .",
    "this is justified if ( i ) the covariates are explicitly chosen when designing the experimental setting and hence are not noisy , or ( ii ) there is a separate set of parameters for generating @xmath4 on the one hand and @xmath5 given @xmath4 on the other , and the sets are assumed to be independent in their prior distribution . in the latter case the posterior factors out into two parts , and the parameters used for generating @xmath4 are neither needed nor useful in the regression task .",
    "see for instance @xcite for more details .",
    "however , there has been no theoretical justification for bayesian regression in the more general setting where the independence does not hold .    for point estimates of generative models",
    "it is well known that maximizing the joint likelihood and the conditional likelihood give in general different results .",
    "maximum conditional likelihood gives asymptotically a better estimate of the conditional likelihood @xcite , and it can be optimized with expectation - maximization - type procedures @xcite . in this paper",
    "we extend that line of work to show that the two different approaches , joint and conditional modeling , result in different posterior distributions which are asymptotically equal only if the true model is within the model family .",
    "we give an axiomatic justification to the discriminative posterior , and demonstrate empirically that it works as expected .",
    "if there are no covariates , the discriminative posterior is the same as the standard posterior of joint density modeling , that is , ordinary bayesian inference .",
    "+ to our knowledge the extension from point estimates to a posterior distribution is new .",
    "we are aware of only one suggestion , the so - called supervised posterior @xcite , which also has empirical support in the sense of maximum a posteriori estimates @xcite .",
    "the posterior has , however , only been justified heuristically .    for the purpose of regression ,",
    "the discriminative posterior makes it possible to use more general model structures than standard bayesian regression ; in essence any generative model family can be used .",
    "in addition to giving a general justification to bayesian regression - type modeling , predictions given a generative model family @xmath3 should be better if the whole model is ( at least approximately ) correct .",
    "the additional benefit is that the use of the full generative model gives a principled way of handling missing values .",
    "the gained advantage , compared to using the standard non - discriminative posterior , is that the predictions should be more accurate assuming the model family is incorrect .    in this paper , we present the necessary background and definitions in section 2 .",
    "the discriminative posterior is derived briefly from a set of five axioms in section 3 ; the full proof is included as an appendix .",
    "there is a close resemblance to cox axioms , and standard bayesian inference can indeed be derived also from this set . however , the new axioms allow also inference in the case where the model manifold is known to be inadequate . in section 4",
    "we show that discriminative posterior can be extended in a standard manner @xcite to handle data missing at random . in section 5",
    "we present some experimental evidence that the discriminative posterior behaves as expected .",
    "in this paper , we prove the following two claims ; the claims follow from theorem  [ th : posterior ] , discriminative posterior , which is the main result of this paper .    given a discriminative model , a model @xmath8 for the conditional density , bayesian regression results in consistent conditional inference .    given a joint density model @xmath9 , discriminative posterior results in consistent conditional inference .    in accordance with @xcite , we call inference consistent if the utility is maximized with large data sets .",
    "this paper proves both of the above claims .",
    "notice that although the claim 1 is well known , it has not been proven , aside from the special case where the priors for the margin @xmath4 and @xmath10 are independent , as discussed in the introduction and in @xcite .      throughout the paper ,",
    "observations are denoted by @xmath11 , and assumed to be i.i.d .",
    "we use @xmath12 to denote the set of all possible models that could generate the observations .",
    "models that are applicable in practice are restricted to a lower dimensional manifold @xmath13 of models , @xmath14 .",
    "in other words , the subspace @xmath13 defines our _ model family _",
    ", in this work denoted by a distribution @xmath3 parameterized by @xmath15 .",
    "there exists a model in @xmath12 which describes the `` true '' model , which has actually generated the observations and is typically unknown . with slight abuse of notation",
    "we denote this model by @xmath16 , with the understanding that it may be outside our parametric model family .",
    "in fact , in practice no probabilistic model is perfectly true and is false to some extent , that is , the data has usually not been generated by a model in our model family , @xmath17 .",
    "the distribution induced in the model parameter space @xmath18 after observing the data @xmath19 is referred to as a _",
    "posterior_. by standard posterior we mean the posterior obtained from bayes formula using a full joint density model . in this paper",
    "we discuss the discriminative posterior , which is obtained from axioms 15 below .      in this subsection",
    ", we introduce the research problem by recapitulating the known difference between point estimates of joint and conditional likelihood .",
    "we present the point estimates in terms of kullback - leibler divergences , in a form that allows generalizing from point estimates to the posterior distribution in section [ sec : axiomatic ] .",
    "bayesian inference can be derived in a decision theoretic framework as maximization of the expected utility of the decision maker @xcite . in general",
    ", the choice of the utility function is subjective .",
    "however , several arguments for using log - probability as utility function can be made , see for example  @xcite .",
    "when inspecting a full generative model at the limit where the amount of data is infinite , the joint posterior distribution @xmath20 becomes a point solution , @xmath21 .",
    "an accurate approximation of the log - likelihood is produced by a utility function minimizing the approximation error @xmath22 between the point estimate @xmath23 and the true model @xmath24 as follows : @xmath25 if the true model is in the model family , that is , @xmath26 , equation ( [ eq : map ] ) can be minimized to zero and the resulting point estimate is effectively the map solution . if @xmath17 the resulting point estimate is the best estimate of the true joint distribution @xmath27 with respect to @xmath22",
    ".    however , the joint estimate may not be optimal if we are interested in approximating some other quantity than the likelihood .",
    "consider the problem of finding the best point estimate @xmath28 for the conditional distribution @xmath29 .",
    "the average kl - divergence between the true conditional distribution at @xmath24 and its estimate at @xmath6 is given by @xmath30 and the best point estimate with respect to @xmath31 is @xmath32 by equations ( [ eq : map ] ) and ( [ eq : cond ] ) we may write @xmath33 therefore the point estimates @xmath23 and @xmath28 are different in general . if the model that has generated the data does not belong to the model family , that is @xmath17 , then by equation ( [ eq : condwins ] )",
    "the joint estimate is generally worse than the conditional estimate in conditional inference , measured in terms of conditional likelihood .",
    "see also @xcite .",
    "a discriminative model does not make any assumptions on the distribution of the margin of @xmath4 .",
    "that is , it does not incorporate a generative model for @xmath4 , and can be interpreted to rather use the empirical distribution of @xmath34 as its margin @xcite .    a generative model , on the other hand , assumes a specific parametric form for the full distribution @xmath3 .",
    "a generative model can be learned either as a joint density model or in a discriminative manner .",
    "our point in this paper is that the selection corresponds to choosing the utility function ; this is actually our fifth axiom in section [ sec : axiomatic ] . in joint density modeling",
    "the utility is to model the full distribution @xmath35 as accurately as possible , which corresponds to computing the standard posterior incorporating the likelihood function . in discriminative learning",
    "the utility is to model the conditional distribution of @xmath7 , and the result is a discriminative posterior incorporating the conditional likelihood . a generative model optimized in discriminative manner is referred to as a discriminative joint density model in the following .    for generative models and in case of point estimates , if the model family is correct , a maximum likelihood ( ml ) solution is better for predicting @xmath36 than maximum conditional likelihood ( cml ) .",
    "they have the same maximum , but the asymptotic variance of cml estimate is higher @xcite .",
    "however , in case of incorrect models , a maximum conditional likelihood estimate is better than maximum likelihood @xcite . for an example illustrating that cml can be better than ml in predicting @xmath36 , see @xcite .",
    "since the discriminative joint density model has a more restricted model structure than bayesian regression , we expect it to perform better with small amounts of data .",
    "more formally , later in theorem [ th : posterior ] we move from point estimate of equation ( [ eq : condpoint ] ) to a discriminative posterior distribution @xmath37 over the model parameters @xmath6 in @xmath13 .",
    "since the posterior is normalized to unity , @xmath38 , the values of the posterior @xmath37 are generally smaller for larger model families ; the posterior is more diffuse .",
    "equation ( [ eq : cond ] ) can be generalized to the expectation of the approximation error , @xmath39 = -\\int{\\sum_{c}{p(\\x , c\\mid\\tilde\\theta)p_d(\\theta\\mid d)\\log{p(c\\mid\\x,\\theta)}}d\\x d\\theta}+{\\rm const . }",
    "\\label{eq : expk}\\ ] ] the expected approximation error is small when both the discriminative posterior distribution @xmath37 and the conditional likelihood @xmath40 are large at the same time . for small amounts of data ,",
    "if the model family is too large , the values of the posterior @xmath37 are small .",
    "the discriminative joint density model has a more restricted model family than that of the bayesian regression , and hence the values of the posterior are larger . if the model is approximately correct , the discriminative joint density model will have a smaller approximation error than the bayesian regression , that is , @xmath40 is large somewhere in @xmath13 .",
    "this is analogous to selecting the model family that maximizes the evidence ( in our case the expected conditional log - likelihood ) in bayesian inference ; choosing a model family that is too complex leads to small evidence ( see , e.g. , @xcite ) .",
    "the difference to the traditional bayesian inference is that we do not require that the true data generating distribution @xmath24 is contained in the parameter space @xmath13 under consideration .",
    "in this section we generalize the point estimate @xmath28 presented in section [ sec : point estimates ] to a _ discriminative posterior distribution _ over @xmath41 .",
    "[ th : posterior ] it follows from axioms 16 listed below that , given data @xmath42 , the discriminative posterior distribution @xmath43 is of the form @xmath44 the predictive distribution for new @xmath45 , obtained by integrating over this posterior , @xmath46 , is consistent for conditional inference .",
    "that is , @xmath47 is consistent for the utility of conditional likelihood .",
    "the discriminative posterior follows from requiring the following axioms to hold :    1 .",
    "the posterior @xmath37 can be represented by non - negative real numbers that satisfy @xmath38 .",
    "a model @xmath41 can be represented as a function @xmath48 that maps the observations @xmath11 to real numbers .",
    "the posterior , after observing a data set @xmath49 followed by an observation @xmath11 , is given by @xmath50 , where @xmath51 is a twice differentiable function in both of its parameters .",
    "exchangeability : the value of the posterior is independent of the ordering of the observations .",
    "that is , the posterior after two observations @xmath11 and @xmath52 is the same irrespective of their ordering : + @xmath53 .",
    "5 .   the posterior must agree with the utility .",
    "for @xmath16 , and @xmath54 , the following condition is satisfied : @xmath55 where @xmath56 is a very large data set sampled from @xmath27 .",
    "we further assume that the discriminative posteriors @xmath57 at @xmath58 and @xmath59 are equal only if the corresponding conditional kl - divergences @xmath31 are equal .",
    "the first axiom above is simply a requirement that the posterior is a probability distribution in the parameter space .",
    "the second axiom defines a model in general terms ; we define it as a mapping from event space into a real number .",
    "the third axiom makes smoothness assumptions on the posterior .",
    "the reason for the axiom is technical ; the smoothness is used in the proofs .",
    "cox @xcite makes similar assumptions , and our proof therefore holds in the same scope as cox s ; see @xcite .",
    "the fourth axiom requires exchangeability ; the shape of the posterior distribution should not depend on the order in which the observations are made .",
    "this deviates slightly from analogous earlier proofs for standard bayesian inference , which have rather used the requirement of associativity @xcite or included it as an additional constraint in modeling after presenting the axioms @xcite .    the fifth axiom states , in essence , that asymptotically ( at the limit of a large but finite data set ) the shape of the posterior is such that the posterior is always smaller if the `` distance '' @xmath60 is larger .",
    "if the opposite would be true , the integral over the discriminative posterior would give larger weight to solutions further away from the true model , leading to a larger error measured by @xmath60 .",
    "axioms 15 are sufficient to fix the discriminative posterior , up to a monotonic transformation . to fix the monotonic transformation we introduce the sixth axiom :    1 .   for fixed @xmath4",
    "the model reduces to the standard posterior . for the data set @xmath61 , the discriminative posterior @xmath62 matches the standard posterior + @xmath63 .",
    "we use @xmath64 to denote the posterior when no data is observed ( _ prior distribution _ ) .",
    "the proof is in the appendix . for clarity of presentation",
    ", we additionally sketch the proof in the following .",
    "it follows from axiom 4 that the function @xmath51 is of the form @xmath65    where @xmath66 is a monotonic function which we , by convention , fix to the identity function .",
    "the problem then reduces to finding a functional form for @xmath48 . utilizing both the equality part and the inequality part of axiom 5",
    ", the following proposition can be derived .",
    "it follows from axiom 5 that @xmath67    finally , the axiom 6 effectively states that we decide to follow the bayesian convention for a fixed @xmath4 , that is , to set @xmath68 .",
    "discriminative posterior gives a theoretically justified way of handling missing values in discriminative tasks .",
    "discriminative models can not readily handle components missing from the data vector , since the data is used only as covariates .",
    "however , standard methods of handling missing data with generative models @xcite can be applied with the discriminative posterior .    the additional assumption we need to make is a model for which data are missing .",
    "below we derive the formulas for the common case of data missing independently at random .",
    "extensions incorporating prior information of the process by which the data are missing are straightforward , although possibly not trivial .",
    "write the observations @xmath69 .",
    "assume that @xmath70 can be missing and denote a missing observation by @xmath71 .",
    "the task is still to predict @xmath5 by @xmath72 .",
    "since we are given a model for the joint distribution which is assumed to be approximately correct , it will be used to model the missing data .",
    "we denote this by @xmath73 , @xmath74 , with a prior @xmath75 , @xmath76 .",
    "we further denote the parameters of the missing data mechanism by @xmath77 .",
    "now , similar to joint density modeling , if the priors for the joint model , @xmath75 , and missing - data mechanism , @xmath78 , are independent , the missing data mechanism is ignorable @xcite . in other words , the posterior that takes the missing data into account can be written as @xmath79 where @xmath80 and we have used to @xmath81 and @xmath82 to denote the portions of data set with @xmath74 and @xmath71 , respectively .",
    "equation  ( [ eq : missing ] ) has been obtained by using @xmath83 to construct a model family in which the data is missing independently at random with probability @xmath77 , having a prior @xmath78 .",
    "that is , we define a model family that generates the missing data in addition to the non - missing data , @xmath84 where @xmath41 and @xmath85 denotes the support of @xmath70 .",
    "the parameter space @xmath13 is spanned by @xmath76 and @xmath77 .",
    "the equation  ( [ eq : missing ] ) follows directly by applying theorem  [ th : posterior ] to @xmath86 .",
    "when @xmath74 , and + @xmath87 . ]",
    "notice that the posterior for @xmath88 of  ( [ eq : missing ] ) is independent of @xmath77 .",
    "the division to @xmath70 and @xmath89 can be made separately for each data item .",
    "the items can have different numbers of missing components , each component @xmath90 having a probability @xmath91 of being missing .",
    "the discriminative posterior can be sampled with an ordinary metropolis - hastings algorithm where the standard posterior @xmath92 is simply replaced by the discriminative version @xmath93 , where @xmath94 .    in mcmc sampling of the discriminative posterior ,",
    "the normalization term of the conditional likelihood poses problems , since it involves a marginalization over the class variable @xmath5 and latent variables @xmath95 , that is , @xmath96 . in case of discrete latent variables , such as in mixture models",
    ", the marginalization reduces to simple summations and can be computed exactly and efficiently . if the model contains continuous latent variables the integral needs to be evaluated numerically .",
    "we first set up a toy example where the distance between the model family and the true model is varied .      in this experiment",
    "we compare the performance of logistic regression and a mixture model .",
    "logistic regression was chosen since many of the discriminative models can be seen as extensions of it , for example conditional random fields @xcite . in case of logistic regression , the following theorem exists :    @xcite + for a k - class classification problem with equal priors , the pairwise log - odds ratio of the class posteriors is affine if and only if the class conditional distributions belong to any fixed exponential family .    that is , the model family of logistic regression incorporates the model family of any generative exponential ( mixture ) model .",
    "a direct interpretation is that any generative exponential family mixture model defines a smaller subspace within the parametric space of the logistic regression model .",
    "since the model family of logistic regression is larger , it is asymptotically better , but if the number of data is small , generative models can be better due to the restricted parameter space ; see for example @xcite for a comparison of naive bayes and logistic regression .",
    "this happens if the particular model family is at least approximately correct .",
    "the true model generates ten - dimensional data @xmath4 from the two - component mixture @xmath97 where @xmath98 indexes the mixture component , and @xmath99 is a gaussian with mean @xmath100 and standard deviation @xmath101 .",
    "the data is labeled according to the generating mixture component ( i.e. , the `` class '' variable ) @xmath102 .",
    "two of the ten dimensions contain information about the class .",
    "the `` true '' parameters , used to generate the data , on these dimensions are @xmath103 , and @xmath104 the remaining eight dimensions are gaussian noise with @xmath105 for both components .    the `` incorrect '' generative model used for",
    "inference is a mixture of two gaussians where the variances are constrained to be @xmath106 . with increasing @xmath90",
    "the model family thus draws further away from the true model .",
    "we assume for both of the @xmath107 a gaussian prior @xmath108 having the same fixed hyperparameters . ]",
    "@xmath109 for all dimensions .",
    "data sets were generated from the true model with 10000 test data points and a varying number of training data points ( @xmath110 ) .",
    "both the discriminative and standard posteriors were sampled for the model .",
    "the goodness of the models was evaluated by perplexity of conditional likelihood on the test data set .",
    "standard and discriminative posteriors of the incorrect generative model are compared against bayesian logistic regression . a uniform prior for the parameters @xmath111 of the logistic regression model was assumed .",
    "as mentioned in subsection [ sec : logreg vs mm ] above , the gaussian naive bayes and logistic regression are connected ( see for example @xcite for exact mapping between the parameters ) .",
    "the parameter space of the logistic model incorporates the true model as well as the incorrect model family , and is therefore the optimal discriminative model for this toy data .",
    "however , since the parameter space of the logistic regression model is larger , the model is expected to need more data samples for good predictions .",
    "the models perform as expected ( figure [ fig : toycomparison ] ) .",
    "the model family of our incorrect model was chosen such that it contains useful prior knowledge about the distribution of @xmath4 .",
    "compared with logistic regression , the model family becomes more restricted which is beneficial for small learning data sets ( see figure  [ fig : toycomparison]a ) .",
    "compared to joint density sampling , discriminative sampling results in better predictions of the class @xmath5 when the model family is incorrect .",
    "the models were compared more quantitatively by repeating the sampling ten times for a fixed value of @xmath112 for each of the learning data set sizes ; in every repeat also a new data set was generated .",
    "the results in figure [ fig : toycomparison2 ] confirm the qualitative findings of figure [ fig : toycomparison ] .",
    "the posterior was sampled with metropolis - hastings algorithm with a gaussian jump kernel . in case of joint density and discriminative sampling",
    ", three chains were sampled with a burn - in of 500 iterations each , after which every fifth sample was collected .",
    "bayesian regression required more samples for convergence , so a burn - in of 5500 samples was used .",
    "convergence was estimated when carrying out experiments for figure [ fig : toycomparison2 ] ; the length of sampling chains was set such that the confidence intervals for each of the models was roughly the same .",
    "the total number of samples was 900 per chain .",
    "the width of jump kernel was chosen as a linear function of data such that the acceptance rate was between 0.20.4 @xcite as the amount of data was increased .",
    "selection was carried out by preliminary tests with different random seeds .",
    "[ [ missing - data . ] ] missing data .",
    "+ + + + + + + + + + + + +    the experiment with toy data is continued in a setting where 50% of the learning data are missing at random .",
    "mcmc sampling was carried out as described above .",
    "for the logistic regression model , a multiple imputation scheme is applied , as recommended in  @xcite . in order to have sampling conditions comparable to sampling from a generative model , each sampling chain used one imputed data set .",
    "imputation was carried out with the generative model ( representing our current best knowledge , differing from the true joint model , however ) .    as can be seen from figure  [ fig : toycomparison ] , discriminative",
    "sampling is better than joint density sampling when the model is incorrect .",
    "the performance of bayesian regression seems to be affected heavily by the incorrect generative model used to generate missing data .",
    "as can be seen from figure  [ fig : toycomparison2 ] , surprisingly the bayesian regression is even worse than standard posterior",
    ". the performance could be increased by imputing more than one data sets with the cost of additional computational complexity , however .    [ cols=\"^,^,^,^ \" , ]",
    "we have introduced a principled way of making conditional inference with a discriminative posterior . compared to standard joint posterior density , discriminative posterior results in better inference when the model family is incorrect , which is usually the case . compared to purely discriminative modeling , discriminative",
    "posterior is better in case of small data sets if the model family is at least approximately correct .",
    "additionally , we have introduced a justified method for incorporating missing data into discriminative modeling .",
    "joint density modeling , discriminative joint density modeling , and bayesian regression can be seen as making different assumptions on the margin model @xmath113 .",
    "joint density modeling assumes the model family to be correct , and hence also the model of @xmath4 margin to be correct",
    ". if this assumption holds , the discriminative posterior and joint density modeling will asymptotically give the same result .",
    "on the other hand , if the assumption does not hold , the discriminative joint density modeling will asymptotically give better or at least as good results .",
    "discriminative joint density modeling assumes that the margin model @xmath113 may be incorrect , but the conditional model @xmath40 , derived from the joint model that includes the model for the margin , is in itself at least approximately correct .",
    "then inference is best made with discriminative posterior as in this paper .",
    "finally , if the model family is completely incorrect  or if there is lots of data  a larger , discriminative model family and bayesian regression should be used .",
    "another approach to the same problem was suggested in @xcite , where the traditional generative view to discriminative modeling has been extended by complementing the conditional model for @xmath114 with a model for @xmath115 , to form the joint density model @xmath116 .",
    "that is , a larger model family is postulated with additional parameters @xmath88 for modeling the marginal @xmath4 .",
    "the conditional density model @xmath117 is derived by bayes rule from a formula for the joint density , @xmath118 , and the model for the marginal @xmath115 is obtained by marginalizing it .",
    "this conceptualization is very useful for semisupervised learning ; the dependency between @xmath6 and @xmath88 can be tuned by choosing a suitable prior , which allows balancing between discriminative and generative modeling . the optimum for semisupervised learning",
    "is found in between the two extremes .",
    "the approach of @xcite contains our discriminative posterior distribution as a special case in the limit where the priors are independent , that is , @xmath119 where the parameters @xmath6 and @xmath88 can be treated independently .",
    "also @xcite can be viewed as giving a theoretical justification for bayesian discriminative learning , based on generative modeling .",
    "the work introduces a method of extending a fully discriminative model into a generative model , making discriminative learning a special case of optimizing the likelihood ( that is , the case where priors separate ) .",
    "our work starts from different assumptions .",
    "we assume that the utility functions can be different , depending on the goal of the modeler .",
    "as we show in this paper , the requirement of our axiom 5 that the utility should agree with the posterior will eventually lead us to the proper form of the posterior .",
    "here we chose conditional inference as the utility , obtaining a discriminative posterior .",
    "if the utility had been joint modeling of @xmath11 , we would have obtained the standard posterior ( the case of no covariates ) .",
    "if the given model is incorrect the different utilities lead to different learning schemes . from this point of view , the approach of @xcite is principled only if the `` true '' model belongs to the postulated larger model family @xmath120 .    as a practical matter",
    ", efficient implementation of sampling from the discriminative posterior will require more work .",
    "the sampling schemes applied in this paper are simple and computationally intensive ; there exist several more advanced methods which should be tested .",
    "this work was supported in part by the pascal2 network of excellence of the european community .",
    "this publication only reflects the authors views .",
    "we use the notation @xmath121 and denote the set of all possible observations @xmath122 by @xmath123 .    for purposes of some parts of the proof , we assume that the set of possible observations @xmath123 is finite .",
    "this assumption is not excessively restrictive , since any well - behaving infinite set of observations and respective probabilistic models can be approximated with an arbitrary accuracy by discretizing @xmath123 to sufficiently many bins and converting the probabilistic models to the corresponding multinomial distributions over @xmath123 .",
    "* example : * assume the observations @xmath124 are real numbers in compact interval @xmath125 $ ] and they are modelled by a well - behaving probability density @xmath126 , that is , the set of possible observations @xmath123 is an infinite set .",
    "we can approximate the distribution @xmath127 by partitioning the interval @xmath125 $ ] into @xmath128 bins @xmath129 $ ] of width @xmath130 each , where @xmath131 , and assigning each bin a multinomial probability @xmath132 .",
    "one possible choice for the family of models @xmath126 would be the gaussian distributions parametrized by the mean and variance ; the parameter space of these gaussian distributions would span a 2-dimensional subspace @xmath13 in the @xmath133 dimensional parameter space @xmath12 of the multinomial distributions .    [ [ from - exchangeability - fhcbf - xp_dthetamid - cbf - x - fhcbf - xp_dthetamid - cbf - x - it - follows - that - the - posterior - is - homomorphic - to - multiplicativity ] ] from exchangeability @xmath134 it follows that the posterior is homomorphic to multiplicativity ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    1 .",
    "exchangeability : the value of the posterior is independent of the ordering of the observations .",
    "that is , the posterior after two observations @xmath135 and @xmath136 is the same irrespective of their ordering : @xmath137 .    for simplicity",
    "let us denote @xmath138 , @xmath139 , and @xmath140 .",
    "the exchangeability axiom thus reduces to the problem of finding a function @xmath51 such that @xmath141 where @xmath142 . by denoting @xmath143 and @xmath144 ,",
    "the equation becomes @xmath145 .",
    "we begin by assuming that function @xmath51 is differentiable in both its arguments ( in a similar manner to cox ) . differentiating with respect to @xmath95 , @xmath146 and @xmath147 in turn , and writing @xmath148 for @xmath149 and @xmath150 for @xmath151 , we obtain @xmath152 differentiating equation ( [ eq : diff z ] ) wrt .",
    "@xmath95 , @xmath147 and @xmath146 in turn , we get @xmath153 and differentiating equation ( [ eq : diff y ] ) wrt .",
    "@xmath147 we get @xmath154 by solving @xmath155 from equation ( [ eq : diff xz ] ) and @xmath156 from equation ( [ eq : diff yz ] ) and inserting into equation ( [ eq : diff yx ] ) , we get @xmath157 by inserting equation ( [ eq : intermediate1 ] ) into equation ( [ eq : diff zz ] ) , we get @xmath158 notice that by equation ( [ eq : diff z ] ) we can write @xmath159 .",
    "inserting this into equation ( [ eq : intermediate2 ] ) , and dividing by @xmath160 , the equation simplifies to @xmath161 the equation can be written also as @xmath162 since the left hand side depends on @xmath163 and right hand side depends on @xmath164 , it follows that both must be functions of only @xmath95 .",
    "furthermore , since the derivative of the logarithm of a function is a function of @xmath95 , the function itself must be of the form @xmath165 on the other hand , dividing equation ( [ eq : diff y ] ) by equation ( [ eq : diff x ] ) , we get @xmath166 by equation ( [ eq : intermediate3 ] ) , @xmath167 . inserting this , we get @xmath168 now",
    ", since left hand side depends only on @xmath169 and right hand side on @xmath170 , each must be a function of @xmath95 only , that is @xmath171 .",
    "furthermore , we note that we must have @xmath172 since the condition must be fulfilled for all @xmath173 , we must have @xmath174 for each @xmath146 as well . summing these",
    ", we get @xmath175 we can then write @xmath176 combining these to obtain the differential @xmath177 we get @xmath178 by denoting @xmath179 , we obtain @xmath180 the function @xmath181 can be incorporated into our model , that is @xmath182 . by inserting @xmath183",
    "we get the final form @xmath184 @xmath185    [ [ mapping - from - pcmid - bf - x - theta - to - hrtheta - is - monotonically - increasing ] ] mapping from @xmath186 to @xmath187 is monotonically increasing ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~            consider the points in the parameter space @xmath12 , where @xmath194 and @xmath195 for @xmath196 ( `` corner points '' ) . in these points the linear combinations vanish and the equivalent inequalities ( [ eq : ax4 ha ] ) and ( [ eq : ax4pa ] ) become @xmath197      from the equivalence ( [ eq : ax4hpnew ] ) ( and the symmetry of the models with respect to re - labeling the data items ) it follows that @xmath198 must be of the form @xmath199 where @xmath189 is a monotonically increasing function . @xmath185        note , that we can decompose @xmath31 as @xmath205 where @xmath206 and @xmath207 .",
    "consider any @xmath24 and the set of points @xmath6 that satisfy @xmath208 , where @xmath209 is some constant .",
    "from the fifth axiom ( the equality part ) it follows that there must exist a constant @xmath210 that defines the same set of points @xmath6 , defined by @xmath211 . from the inequality part of the same axiom it follows that @xmath212 is a monotonically increasing function .",
    "hence , @xmath213                  and @xmath222 so the last @xmath220 for each @xmath4 is @xmath223 where the sum only includes the independent variables @xmath224 for the fixed @xmath4 .",
    "this way we can make the dependency on each @xmath219 explicit in equation ( [ eq : easy ] ) :                                            axiom 6 is used to fix this constant to unity by requiring the discriminative posterior to obey the bayesian convention for a fixed @xmath4 , that is , the discriminative modeling should reduce to bayesian joint modeling when there is only one covariate .",
    "we require that for a fixed @xmath241 and a data set @xmath242 the discriminative posterior matches the joint posterior @xmath243 of a model @xmath244 , where @xmath245 , @xmath246 clearly , @xmath68 satisfies the axiom 6 , i.e. , @xmath247 equals @xmath62 for all @xmath41 .",
    "if the proposal would be false , the axiom 6 should be satisfied for some @xmath248 and for all @xmath6 and data sets .",
    "in particular , the result should hold for a data set having a single element , @xmath11 .",
    "the discriminative posterior would in this case read @xmath249 where @xmath250 and @xmath251 is a normalization factor , chosen so that the posterior satisfies @xmath252 .",
    "the joint posterior would , on the other hand , read @xmath253 these two posteriors should be equal for all @xmath6 : @xmath254 because the normalization factors @xmath255 and @xmath251 in equation ( [ eq : ax4ratio ] ) are constant in @xmath6 , also @xmath256 must be constant in @xmath6 .",
    "this is possible only if @xmath68 or @xmath40 is a trivial function ( constant in @xmath6 ) for all @xmath5 ."
  ],
  "abstract_text": [
    "<S> we study bayesian discriminative inference given a model family @xmath0 that is assumed to contain all our prior information but still known to be incorrect . </S>",
    "<S> this falls in between `` standard '' bayesian generative modeling and bayesian regression , where the margin @xmath1 is known to be uninformative about @xmath2 . </S>",
    "<S> we give an axiomatic proof that _ discriminative posterior _ is consistent for conditional inference ; using the discriminative posterior is standard practice in classical bayesian regression , but we show that it is theoretically justified for model families of joint densities as well . </S>",
    "<S> a practical benefit compared to bayesian regression is that the standard methods of handling missing values in generative modeling can be extended into discriminative inference , which is useful if the amount of data is small . </S>",
    "<S> compared to standard generative modeling , discriminative posterior results in better conditional inference if the model family is incorrect . </S>",
    "<S> if the model family contains also the true model , the discriminative posterior gives the same result as standard bayesian generative modeling . </S>",
    "<S> practical computation is done with markov chain monte carlo . </S>"
  ]
}