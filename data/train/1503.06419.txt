{
  "article_text": [
    "the benefits of cooperative work were explored by nature well before the advent of the human species as attested by the collective structures built by slime molds and social insects @xcite . in the human context",
    ", the socio - cognitive niche hypothesis purports that hominin evolution relies so heavily on social elements to the point of viewing a band of hunter ",
    "gatherers as a ` group - level predator ' @xcite . whether physical or social , those structures and organizations may be thought of as the organisms solutions to the problems that endanger their existence ( see , e.g. , @xcite ) and have motivated the introduction of the concept of social intelligence in the scientific milieu @xcite . in the computer science circle ,",
    "that concept prompted the proposal of optimization heuristics based on social interaction , such as the popular particle swarm optimization algorithm @xcite and the perhaps lesser - known adaptive culture heuristic @xcite .    despite the prevalence of the notion that cooperation can aid a group of agents to solve problems more efficiently than if those agents worked in isolation , and of the success of the social interaction heuristics in producing near optimal solutions to a variety of combinatorial optimization problems , we know little about the conditions that make cooperative work more efficient than independent work .",
    "in particular , we note that since cooperation ( or communication , in general ) reduces the diversity or heterogeneity of the group of agents , it may actually end up reducing the efficiency of group work @xcite .",
    "efficiency here means that the time to solve a problem scales superlinearly with the number of individuals or resources employed in the task .    in this contribution we study the performance of a group of cooperative agents following a first - principle research strategy to study cooperative problem solving set forth by huberman in the 1990s that consists of tackling easy tasks , endowing the agents with simple random trial - and - test search tools , and using plain protocols of collaboration @xcite .",
    "here the task is to find the global maxima of three realizations of the nk - fitness landscape characterized by different degrees of ruggedness ( i.e. , values of the parameter @xmath0 ) .",
    "we use a group of m agents which , in addition to the ability to perform individual trial - and - test searches , can imitate a model agent  the best performing agent at the trial  with a probability @xmath1 .",
    "hence our model exhibits the two critical ingredients of a collective brain according to bloom : imitative learning and a dynamic hierarchy among the agents @xcite .",
    "the model exhibits also the key feature of distributed cooperative problem solving systems , namely , the exchange of messages between agents informing each other on their partial success ( i.e. , their fitness at the current trial ) towards the completion of the goal @xcite .",
    "we find that the presence of local maxima in the fitness landscape introduces a complex trade - off between the computational cost to solve the task and the group size @xmath2 .",
    "in particular , for a fixed frequency of imitation @xmath1 , there is an optimal value of @xmath2 at which the computational cost is minimized .",
    "this finding leads to the conjecture that the efficacy of imitative learning could be a factor determinant of the group size of social animals @xcite .",
    "our study departs from the vast literature on cooperation that followed robert axelrod s 1984 seminal book the evolution of cooperation @xcite since in that game theoretical approach it is usually assumed a priori that mutual cooperation is the most rewarding strategy to the individuals . on the other hand , here we consider a problem solving scenario and a specific cooperation mechanism ( imitation ) aiming at determining in which conditions cooperation is more efficient than independent work .",
    "the rest of the paper is organized as follows . in section [ sec : nk ] we offer a brief overview of the nk model of rugged fitness landscapes . in section [ sec : imi ] we present a detailed account of the imitative learning search strategy and in section [ sec : res ] we study its performance on the task of finding the global maxima of three realizations of the nk landscape with @xmath3 and ruggedness parameters @xmath4 , @xmath5 and @xmath6 .",
    "the rather small size of the solution space ( @xmath7 binary strings of length @xmath8 ) allows the full exploration of the space of parameters and , in particular , the study of the regime where the group size is much greater than the solution space size .",
    "finally , section [ sec : conc ] is reserved to our concluding remarks .",
    "the nk model of rugged fitness landscapes introduced by kauffman @xcite offers the ideal framework to test the potential of imitative learning in solving optimization problems of varied degrees of difficulty , since the ruggedness of the landscape can be tuned by changing the two integer parameters  @xmath9 and @xmath0  of the model . roughly speaking",
    ", the parameter @xmath9 determines the size of the solution space whereas the value of @xmath10 influences the number of local maxima and minima on the landscape .",
    "the solution space consists of the @xmath11 distinct binary strings of length @xmath9 , @xmath12 with @xmath13 . to each string",
    "we associate a fitness value @xmath14 which is an average of the contributions from each component @xmath15 in the string , i.e. , @xmath16 where @xmath17 is the contribution of component @xmath15 to the fitness of string @xmath18 .",
    "it is assumed that @xmath17 depends on the state @xmath19 as well as on the states of the @xmath0 right neighbors of @xmath15 , i.e. , @xmath20 with the arithmetic in the subscripts done modulo @xmath9 .",
    "in addition , the functions @xmath21 are @xmath9 distinct real - valued functions on @xmath22 . as usual , we assign to each @xmath17 a uniformly distributed random number in the unit interval @xcite .",
    "hence @xmath23 has a unique global maximum .    for @xmath4",
    "there are no local maxima and the sole maximum of @xmath24 is easily located by picking for each component @xmath15 the state @xmath25 if @xmath26 or the state @xmath27 , otherwise . for @xmath28 , finding the global maximum of the nk model is a np - complete problem @xcite , which essentially means that the time required to solve the problem using any currently known deterministic algorithm increases exponentially fast as the size @xmath9 of the problem grows @xcite .",
    "the increase of the parameter @xmath0 from @xmath29 to @xmath30 decreases the correlation between the fitness of neighboring configurations ( i.e. , configurations that differ at a single component ) in the solution space and for @xmath31 those fitness values are uncorrelated so the nk model reduces to the random energy model @xcite .     and @xmath4 ( _ circles _ ) , @xmath32 ( _ triangles _ ) and @xmath33 ( _ inverted triangles _ )",
    ". the lines are guides to the eye.,scaledwidth=48.0% ]    to illustrate the effect of varying @xmath0 on the ruggedness of the fitness landscape , in fig .",
    "[ fig:1 ] we show the relative fitness of a string @xmath34 as function of its hamming distance @xmath35 to the global maximum for @xmath36 and different values of @xmath0 . here",
    "@xmath37 stands for the fitness of the global maximum . for each @xmath0 , the figure shows the results for a single realization of the fitness landscape and for a single trajectory in the solution space that begins at the maximum ( @xmath38 ) and changes the state components sequentially until all @xmath9 states are reversed ( @xmath39 ) .",
    "we note that the ruggedness of the landscapes ( essentially , the number of local maxima ) can vary wildly between landscapes characterized by the same values of @xmath9 and @xmath28 @xcite and so can the performance of any search heuristic based on the local correlations of the fitness landscape . hence in order to appreciate the role of the parameters that are relevant to imitative learning , namely , the group size @xmath2 and the imitation probability @xmath1 , for fixed @xmath9 and @xmath0 we consider a single realization of the nk fitness landscape .",
    "we assume a group or system composed of @xmath2 agents .",
    "each agent operates in an initial binary string drawn at random with equal probability for the digits @xmath29 and @xmath40 . in the typical situation",
    "that the size of the solution space is much greater than the group size @xmath41 we can consider those initial strings as distinct binary strings , but here we will consider the case that @xmath42 as well , so that many copies of a same string are likely to appear in this initial stage of the simulation .",
    "in addition , we assume that the agents operate in parallel .    at any trial @xmath43 , each agent can choose with a certain probability between two distinct processes to operate on the strings .",
    "the first process , which happens with probability @xmath44 , is the elementary or minimal move in the solution space , which consists of picking a string bit @xmath45 at random with equal probability and then flipping it .",
    "the repeated application of this operation is capable of generating all the @xmath11 binary strings starting from any arbitrary string . the second process , which happens with probability @xmath1 , is the imitation of a model string .",
    "we choose the model string as the highest fitness string in the group at trial @xmath43 . the string to be updated ( target string ) is compared with the model string and the different digits are singled out",
    ". then the agent selects at random one of the distinct bits and flips it so that the target string becomes now more similar to the model string .",
    "the parameter @xmath46 $ ] is the imitation probability and the case @xmath47 corresponds to the baseline situation in which the @xmath2 agents explore the solution space independently .",
    "the specific imitation procedure proposed here was inspired by the mechanism used to model the influence of an external media @xcite in the celebrated agent - based model proposed by axelrod to study the process of culture dissemination @xcite .",
    "it is important to note that in the case the target string is identical to the model string , and this situation is not uncommon since the imitation process reduces the diversity of the group , the agent executes the elementary move with probability one .",
    "this procedure is different from that used in @xcite , in which strings identical to the model string are not updated within the imitation process .",
    "however , both procedures yield qualitatively similar results , except in the regime where imitation is extremely frequent , i.e. , for @xmath48 . in particular , in the present implementation , a small group can find the global maximum in the case @xmath49 since the model string can explore the solution space through the elementary move whereas the other strings are simply followers .",
    "the collective search ends when one of the agents finds the global maximum and we denote by @xmath50 the number of trials made by the agent that found the solution .",
    "of course , @xmath50 stands also for the number of trials made by any one of the @xmath2 agents , since they operate in parallel and the search halts simultaneously for all agents .",
    "in other words , the trial number @xmath43 is updated , or more specifically , incremented by one unit , when the @xmath2 agents have executed one of the two processes on its associated string .",
    "we note that except for the case @xmath47 , the update of the @xmath2 agents is not strictly a parallel process since the model strings may change several times within a given trial .",
    "nonetheless , since in a single trial all agents are updated , the total number of agent updates at trial @xmath43 is given by the product @xmath51 .",
    "the efficiency of the search strategy is measured by the total number of agent updates necessary to find the global maximum ( i.e. , @xmath52 ) which can then be interpreted as the computational cost of the search .",
    "in addition , since we expect that the typical number of trials to success @xmath50 scales with the size of the solution space @xmath11 , we will present the results in terms of the rescaled computational cost , defined as @xmath53 .",
    "an interesting variant of our imitation rule is obtained by relaxing the condition that only the string with the highest fitness value can be imitated and allowing any string to pose as a model according to an imitation probability function proportional to the string relative fitness ( e.g. , a fermi function ) .",
    "such fitness dependent imitation probability functions are frequently used in the study of cooperation dilemmas @xcite and may be useful to prevent the string population being trapped in the local maxima . in this paper , however , we will consider only the noiseless or zero - temperature limit of those imitation functions , where only the string with the maximum relative fitness can be imitated .",
    "here we report the results of extensive monte carlo simulations of groups of agents that use imitative learning to search for the global maxima of three representative realizations of the nk fitness landscape . for fixed @xmath9 and @xmath0",
    "we generate a single realization of the fitness landscape and carry out @xmath54 searches in order to determine the dependence of the mean rescaled computational cost @xmath55 on the imitation probability @xmath1 and on the group size @xmath2 .",
    "in addition , for the baseline case in which the @xmath2 agents explore the landscape independently ( @xmath47 ) we derive an analytical expression for the computational cost .     as function of the binary string length @xmath9 that determines the size of the solution space @xmath11 .",
    "these results , which were obtained by the numerical calculation of the eigenvalues of the tridiagonal stochastic matrix @xmath56 , do not depend on the ruggedness of the landscape .",
    ", scaledwidth=48.0% ]      in this case there is no imitation ( @xmath47 ) and the ruggedness of the landscape has no effect on the performance of the search , which depends only on the size of the solution space , @xmath11 .",
    "because of this independence on the landscape we can derive exact results for the time needed for @xmath2 independent agents to find the global maximum . for this analysis",
    "we can assume that the global maximum is the string with the @xmath9 digit values equal to 1 , i.e. , @xmath57 , without loss of generality .",
    "let us consider first the case of a single agent that operates on a string with @xmath58 digits 1 . according to the elementary move",
    ", the probability that the resulting string has @xmath15 digits 1 is @xmath59 for @xmath60 , @xmath61 , and @xmath62 , where @xmath63 is the kronecker delta . since @xmath64 and @xmath65 for @xmath66 the matrix @xmath56 is a tridiagonal stochastic matrix .",
    "the associated stochastic process has a single absorbing state @xmath67 and @xmath68 yields the probability that the agent finds the solution before or at trial @xmath43 . for large @xmath43",
    "this quantity is given @xmath69 where @xmath70 is the second largest eigenvalue of @xmath56 .",
    "( the largest eigenvalue is 1 because @xmath56 is a stochastic matrix . )",
    "the probability that the agent finds the solution exactly at trial @xmath71 is @xmath72 , i.e. , @xmath73 which is a geometric distribution with success probability @xmath74 .",
    "the value of @xmath70 can be easily obtained numerically ( see , e.g. , @xcite ) provided @xmath9 is not too large since @xmath75 is on the order of the unity and so it becomes practically impossible to distinguish @xmath70 from @xmath40 for @xmath76 .",
    "the mean rescaled computational cost @xmath77 $ ] is shown in fig .",
    "[ new:2 ] for different string lengths @xmath9 .",
    "( recall that @xmath78 at this stage . ) in particular for @xmath3 we find @xmath79 that implies @xmath80 . for very large @xmath9 ,",
    "the probability of success becomes @xmath81 .",
    "we note that although eq .",
    "( [ geom1 ] ) is valid strictly for large @xmath50 only , the fact that @xmath50 is typically on the order of @xmath11 makes this geometric distribution an exceedingly good approximation to the correct distribution of absorbing times .",
    "now let us consider the case of @xmath82 agents searching independently for the global maximum .",
    "since the process halts when one of the agents finds the global maximum , the halting time is @xmath83 where @xmath84 and @xmath85 are independent random variables distributed by the geometric distribution ( [ geom1 ] ) .",
    "it is easy to show that the distribution of @xmath86 is also geometric with probability of success @xmath87 @xcite . in the case of @xmath88 agents the halting time @xmath89 and since",
    "both @xmath86 and @xmath84 are geometrically distributed , though with distinct success probabilities , we find that @xmath90 is also geometrically distributed with probability of success @xmath91 .",
    "the generalization of this reasoning to @xmath2 agents yields that the mean scaled cost is @xmath92}.\\ ] ] since @xmath93 is close to @xmath40 we can write @xmath94 so that @xmath95 $ ] for @xmath96 and @xmath97 for @xmath98 .",
    "we recall that for @xmath3 we have @xmath99 . as expected , eq .",
    "( [ cind ] ) matches the simulation data very well ( see fig .",
    "[ fig:2 ] ) .     as function of the group size @xmath2 for the imitation probability @xmath47 ( _ circles _ ) , @xmath100 ( _ triangles _ ) , @xmath101 ( _ inverted triangles _ ) , and @xmath49 ( _ squares _ ) .",
    "the solid curve is eq .",
    "( [ cind ] ) and the dashed line is the linear function @xmath102 .",
    "the parameters of the nk landscape are @xmath3 and @xmath4 .",
    "the landscape exhibits a single maximum .",
    ", scaledwidth=48.0% ]      the nk fitness landscape with @xmath4 exhibits a single global maximum and no local maxima .",
    "the results of the performance of the imitative search for a landscape with @xmath3 and @xmath4 are summarized in fig .",
    "[ fig:2 ] .",
    "as shown in the previous subsection , the mean computational cost for non - interacting agents ( @xmath47 ) is a constant @xmath103 provided the group size @xmath2 is not too large compared to @xmath99 , which is close to the size of the solution space , @xmath7 . in the case",
    "the group size @xmath2 is very large , the agents begin to duplicate each other s work leading to a linear increase of @xmath104 with increasing @xmath2 .",
    "more pointedly , in this regime we find @xmath102 ( see the straight line in fig .",
    "[ fig:2 ] ) .",
    "we stress that a constant computational cost means that the time @xmath50 the group requires to find the global maximum decreases with the inverse of the group size ( i.e. , the time to solve the problem decreases linearly with the number of agents ) , whereas a computational cost that grows linearly with the group size means that @xmath50 does not change as more agents are added to the group .    allowing the agents to imitate the best performer ( model ) at each trial",
    "leads to a rapid reduction of the computational cost provided the group size remains small .",
    "the best performance is achieved for @xmath105 and @xmath49 and corresponds to a fiftyfold decrease of the computational cost with respect to the independent search ( @xmath47 ) .",
    "the fact that the best performance is obtained when the imitation probability is maximum is due to the absence of local maxima in the landscape for @xmath4 .",
    "we recall that for @xmath49 only the model string , which is likely to be represented by several copies in the group , can perform the elementary move ; all other strings must imitate the model . as a result , the effective size of the search space is greatly reduced , i.e. , the strings are concentrated in the vicinity of the model string which can not accommodate many more than @xmath106 strings without duplication of work .",
    "this is the reason we observe the degradation of the performance when the group size increases beyond its optimal value .",
    "note that for @xmath4 the imitative learning search always performs better than the independent search .",
    "now we consider a somewhat more complex nk fitness landscape by setting @xmath3 and @xmath5 . in the particular realization of the landscape",
    "studied here there are 5 maxima in total , among which 4 are local maxima .",
    "the relative fitness of those maxima ( @xmath107 ) , as well as their hamming distances to the global maximum ( @xmath35 ) , is presented in table [ table : k2 ] .",
    ".local maxima for the studied realization of the nk fitness landscape with @xmath3 and @xmath5 [ cols=\"^,^,^ \" , ]     [ table : k2 ]    the results of the imitative learning search are summarized in fig .",
    "[ fig:3 ] where the mean rescaled computational cost is plotted against the group size for different values of the imitation probability .",
    "the performance of the independent search ( @xmath47 ) is identical to that shown in fig .",
    "[ fig:2 ] for @xmath4 , since that search strategy is not affected by the complexity of the landscape .",
    "the results for the cooperative search ( @xmath108 ) , however , reveal a trade - off between the group size @xmath2 and the imitation probability @xmath1 .",
    "in particular , for @xmath109 we observe a steep increase of the computational cost for intermediate values of @xmath2 .",
    "this happens because the group can be trapped near one of the local maxima . for large groups ( @xmath110 in this case )",
    ", chances are that some of the initial strings are close to the global maximum and end up attracting the rest of the group to its neighborhood , thus attenuating the effect of the local maxima .",
    "the robust finding is that for any @xmath108 there is an optimal group size , which depends on the value of @xmath1 , that minimizes the computational cost of the search .     as function of the group size @xmath2 for the imitation probability @xmath100 ( _ triangles _ ) , @xmath101 ( _ inverted triangles _ ) , @xmath111 ( _ squares _ ) and @xmath112 ( _ circles _ ) .",
    "the solid line is the linear function @xmath102 .",
    "the parameters of the nk landscape are @xmath3 and @xmath5 .",
    "the landscape exhibits 4 local maxima and a single global maximum.,scaledwidth=48.0% ]     agents and probability of imitation @xmath112 .",
    "the thin horizontal lines indicate the relative fitness of the 4 local maxima given in table [ table : k2 ] .",
    "the parameters of the nk landscape are @xmath3 and @xmath5.,scaledwidth=48.0% ]    a better understanding of the dynamics of the search is offered in fig .",
    "[ fig:4 ] which shows the relative fitness of the model string as function of the number of trials for @xmath88 and @xmath112 .",
    "the role of the local maxima as transitory attractors of the search is evident in this figure . a typical run with @xmath113 agents , which is approximately the location of the maximum of @xmath114 in fig .  [ fig:3 ] , yields essentially a flat line at the highest fitness local maximum ( maximum 4 in table [ table : k2 ] ) and a sudden jump to the global maximum .",
    "because there are many copies of the model string ( the mean hamming distance between the @xmath113 strings is less than 1 ) , the variants produced by the elementary move are attracted back to the local maximum .",
    "this is the reason why a small group can search the solution space much more efficiently than a large one in the case imitation is very frequent .",
    "the particular realization of the nk fitness landscape with @xmath3 and @xmath6 that we consider now has 53 maxima , including the global maximum , which poses a substantial challenge to any hill - climbing type of search strategy . as in the previous case , we observe in fig .",
    "[ fig:5 ] a trade - off between @xmath2 and @xmath1 , but now the results reveal how imitative learning may produce disastrous performances on rugged landscapes for certain ranges of those parameters . the strategy of following or imitating a model string can backfire if the fitness landscape exhibits high fitness local maxima that are distant from the global maximum .",
    "a large group may never escape from those maxima due to the attractive effect of the clones of the model string .",
    "this is what we observed in the case of @xmath101 for which we were unable to find the global maximum with groups of size @xmath115 .",
    "as function of the group size @xmath2 for the imitation probability @xmath116 ( _ circles _ ) , @xmath117 ( _ squares _ ) , @xmath100 ( _ triangles _ ) and @xmath101 ( _ inverted triangles _ ) .",
    "for @xmath101 we find @xmath118 for @xmath115 .",
    "the solid line is the linear function @xmath102 .",
    "the parameters of the nk landscape are @xmath3 and @xmath6 .",
    "the landscape exhibits 52 local maxima and a single global maximum.,scaledwidth=48.0% ]     as function of the probability of imitation @xmath1 for the group size @xmath105 ( solid line ) , @xmath113 ( dashed line ) and @xmath119 ( dotted line )",
    ". the parameters of the nk landscape are @xmath3 and @xmath6 .",
    "the landscape exhibits 52 local maxima and a single global maximum.,scaledwidth=48.0% ]    we note , however , that for a fixed group size @xmath2 it is always possible to tune the imitation probability @xmath1 so that the imitative learning strategy performs better than ( or , in a worst - case scenario , equal to ) the independent search .",
    "this point is illustrated in fig .",
    "[ fig:6 ] that shows the computational cost as function of @xmath1 . for any fixed value of @xmath2 , the computational cost exhibits a well - defined minimum that determines the value of the optimal imitation frequency .",
    "as hinted in the previous figures , this optimal value decreases with increasing group size .    in order to verify the robustness of our findings ,",
    "which were obtained for specific realizations of the nk fitness landscape , we have considered four random realizations of the landscape with @xmath3 and @xmath6 in addition to the realization studied above .",
    "the comparison between the mean computational costs to find the global maxima of the five realizations of the landscape is shown in fig .",
    "[ fig:8 ] for the imitation probability @xmath100 .",
    "the results are qualitatively the same , despite the wild fluctuations of @xmath114 in the regime where the search is trapped in the local maxima .",
    "it is reassuring to note that the initial decrease of the mean cost with increasing group size and the existence of an optimal group size that minimizes that cost , which are exhibited by all five realizations of the nk landscapes shown in fig .",
    "[ fig:8 ] , are robust properties of the imitative learning search .     for the probability of imitation @xmath100 as function of the group size @xmath2 for five realizations ( different symbols ) of the nk fitness landscape with @xmath3 and @xmath6 .",
    "the solid line is the linear function @xmath102 .",
    ", scaledwidth=48.0% ]",
    "in this paper we study quantitatively the problem solving performance of a group of agents capable to learn by imitation .",
    "the performance measure we consider is the computational cost to find the global maximum of three specific realizations of the nk fitness landscape .",
    "the computational cost is defined as the product of the number of agents in the group and the number of attempted trials till some agent finds the global maximum .",
    "our main conclusion , namely , that for a fixed probability of imitation @xmath1 there is a value of group size that minimizes the computational cost corroborates the findings of a similar study in which the task was to solve a particular cryptarithmetic problem @xcite . hence our conjecture that the efficacy of imitative learning could be a factor determinant of the group size of social animals ( see @xcite for a discussion on the standard selective pressures on group size in nature ) .",
    "we note that in the case the connectivity between agents is variable , i.e. , each agent interacts with @xmath120 distinct randomly picked agent ( here we have focused on the fully connected network @xmath121 only ) then there is an optimal connectivity value that minimizes the computational cost @xcite .",
    "it would be most interesting to understand how the network topology influences the performance of the group of imitative agents and how the optimal network topology correlates with the known animal social networks @xcite .",
    "the main aim of our contribution is to show that the existence of an optimal group size that maximizes performance for imitative learning @xcite is insensitive to the choice of the fitness landscape , so it is likely a robust property of populations that use imitation as a cooperative strategy .",
    "although we have focused on the effect of the parameter @xmath0 , which roughly determines the ruggedness of the nk landscape , the parameter @xmath9 ( the length of the strings ) also plays a relevant role on the search for the global maximum , besides the obvious role of fixing the size @xmath11 of the search space .",
    "( of course , since the typical time to find the global maximum scales with @xmath11 , even moderate values of @xmath9 , say @xmath122 , would render the simulations unfeasible . )",
    "the nontrivial role is that the value of @xmath9 seems to pose an upper bound to the optimal size of the group @xmath2 in the regime that imitation is very frequent ( see figs .",
    "[ fig:2 ] , [ fig:3 ] and [ fig:5 ] ) .",
    "this is so because in that regime the strings are concentrated in the close vicinity of the model string , which can not accommodate more than @xmath123 different strings .",
    "we do not purport to offer here any novel method to explore efficiently rugged landscapes , but the finding that for small group sizes imitative learning decreases considerably the computational cost of the search , even in a very rugged landscape ( see data for @xmath101 in fig .  [ fig:5 ] ) motivates us to address the question whether in such landscapes that strategy could achieve a better - than - random performance for all group sizes .",
    "this is achieved automatically for smooth landscapes ( see figs .",
    "[ fig:2 ] and [ fig:3 ] ) but not for rugged ones ( see fig .  [ fig:5 ] and @xcite ) .",
    "clearly , the way to accomplish this aim is to decrease the probability of imitation @xmath1 as the group size @xmath2 increases , following the location of the minima shown in fig .",
    "[ fig:6 ] .",
    "it is interesting to note that the finding that too frequent interactions between agents may harm the performance of the group ( see fig .",
    "[ fig:6 ] ) may offer a theoretical justification for henry ford s factory design in which the communication between workers was minimized in order to maintain the established efficiency and maximal productivity @xcite ."
  ],
  "abstract_text": [
    "<S> the idea that a group of cooperating agents can solve problems more efficiently than when those agents work independently is hardly controversial , despite our obliviousness of the conditions that make cooperation a successful problem solving strategy . </S>",
    "<S> here we investigate the performance of a group of agents in locating the global maxima of nk fitness landscapes with varying degrees of ruggedness . </S>",
    "<S> cooperation is taken into account through imitative learning and the broadcasting of messages informing on the fitness of each agent . </S>",
    "<S> we find a trade - off between the group size and the frequency of imitation : for rugged landscapes , too much imitation or too large a group yield a performance poorer than that of independent agents . by decreasing the diversity of the group , imitative learning may lead to duplication of work and hence to a decrease of its effective size . </S>",
    "<S> however , when the parameters are set to optimal values the cooperative group substantially outperforms the independent agents . </S>"
  ]
}