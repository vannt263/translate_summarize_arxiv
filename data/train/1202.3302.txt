{
  "article_text": [
    "computer - aided drug discovery ( cadd ) is an area of research that is concerned with the identification of chemical compounds that are likely to possess specific biological activity , that is , the ability to bind certain target biomolecules such as proteins .",
    "cadd approaches are employed in order to prioritize molecules in commercially available chemical libraries for experimental biological screening .",
    "the prioritization of molecules is critical since these libraries frequently contain many millions of molecules making experimental testing intractable .",
    "the process of using computational methods to filter out those compounds which are not expected to exhibit strong biological activity is called virtual screening .",
    "computational methods have been used extensively to assist in experimental drug discovery studies . in general",
    ", there are two major computational drug discovery approaches , ligand based and structure based .",
    "the former is used when the three - dimensional structure of the drug target is unknown but the information about a reasonably large number of organic molecules active against a specific set of targets is available . in this case",
    ", the available data can be studied using cheminfomatic approaches such as quantitative structure - activity relationship ( qsar ) modeling [ for a review of qsar methods see a. tropsha , in @xcite ] .",
    "in contrast , the structure - based methods rely on the knowledge of three - dimensional structure of the target protein , especially its active site ; this data can be obtained from experimental structure elucidation methods such as x - ray or nuclear magnetic resonance ( nmr ) or from modeling of protein three - dimensional structure .",
    "virtual screening is one of the most popular structure - based cadd approaches where , typically , three - dimensional protein structures are used to discover small molecules that fit into the active site ( a process referred to as docking ) and have high predicted binding affinity ( scoring ) .",
    "traditional docking protocols and scoring functions rely on explicitly defined three - dimensional coordinates and standard definitions of atom types of both receptors and ligands . albeit reasonably accurate in some cases , structure - based virtual screening approaches are for the most part computationally inefficient [ @xcite ] . as a result of computational inefficiency",
    "there is a limit to the number of compounds which can reasonably be screened by these methods .",
    "furthermore , recent extensive studies into the comparative accuracy of multiple available scoring functions suggest that accurate prediction of binding orientations and affinities of receptor  ligand pairs remains a formidable challenge [ @xcite ] . yet",
    "millions of compounds in available chemical databases and billions of compounds in synthetically feasible chemical libraries are available for virtual screening calling for the development of approaches that are both fast and accurate in their ability to identify a small number of viable and experimentally testable computational hits .",
    "recently , we introduced a novel structure - based cheminformatic workflow to search for complimentary ligands based on receptor information ( colibri ) [ @xcite ] .",
    "this novel computational drug discovery strategy combines the strengths of both structure - based and ligand - based approaches while attempting to surpass their individual shortcomings . in this approach",
    ", we extract the structure of the binding pocket from the protein and then represent both the receptor active site and its corresponding ligand in the same universal , multidimensional chemical descriptor space ( note that in principle , the descriptors used for receptors and ligands do not have to be the same , and we will be exploring the use of different descriptor types in future studies ) . we reasoned that mapping of both binding pockets and corresponding ligands onto the same multidimensional chemistry space would preserve the complementary relationships between binding sites and their respective ligands .",
    "thus , we expect that ligands binding to similar active sites are also similar . in cheminformatics applications ,",
    "the similarity is described quantitatively using one of the conventional metrics , such as manhattan or euclidean distance in multidimensional descriptor space .",
    "thus , the chief hypothesis in colibri is that the relative location of a novel binding site with respect to other binding sites in multidimensional chemistry space could be used to predict the location of the ligand(s ) complementary to this site in the ligand chemistry space .",
    "after generation of descriptors , the dataset is split into training and test sets and then variable selection is carried out to generate models optimizing this complementarity between the binding pocket and ligand spaces .",
    "these models are then applied to a binding pocket in a  protein of interest to generate a predicted virtual ligand point which is used as a query in chemical similarity searches to identify putative ligands of the receptor in available chemical databases . in this paper , we build upon the work of @xcite to develop a substantially more advanced and efficient version of colibri .",
    "the problem can be generally stated as follows : for a set of @xmath0 known protein ",
    "ligand pairs , with @xmath1 and @xmath2 descriptors , respectively , given a new protein we want to be able to predict what ligand(s ) will bind to it .",
    "two virtual drug screens will be used as a benchmark for testing the methods discussed and developed here :    a set of 800 chemically and functionally diverse protein  ligand pairs obtained from the protein data bank ( pdb ) database on experimentally measured binding affinity ( pdbbind ) [ @xcite ] .",
    "these compounds are described by a set of 150 chemical descriptors .",
    "these descriptors include information related to the electronic attributes , hydrophobicity and steric properties of the compounds . for a more detailed discussion on the different types of chemical descriptors , see @xcite ( @xcite )",
    ". we will refer to this data set as the 800 receptor  ligand pairs ( rlp800 ) data .",
    "results and further details on this and two additional data sets can be found in section [ secresults ] .",
    "the world drug index ( wdi ) [ @xcite ] database which contains approximately 54,000 drug candidates ( ligands ) .",
    "each compound in the wdi is described by the same set of 150 chemical descriptors as the rlp800 data .",
    "the accuracy of our prediction is based on how close , in euclidean distance , our prediction is to the actual ligand .",
    "this is then compared against the distances of all of the ligands in the space to the actual ligand .",
    "a standard measure of predictive accuracy used in the qsar literature [ @xcite , @xcite ] is based on ranking these distances , from smallest to largest .",
    "defining @xmath3 to be the rank of our prediction of test ligand @xmath4 , model performance is defined as the average rank over each of the new points we are trying to predict , @xmath5 .",
    "this criterion reflects the average size of the search space needed to find each compound . here",
    "@xmath6 denotes the number of new ( i.e. , test ) ligands we are predicting .    the @xmath7 effectiveness of the methods studied and developed here is illustrated in figure [ figikcca ] .",
    "figure [ figikcca](a ) is a histogram of the ranks , @xmath3 for our novel method which is a variant of canonical correlation analysis ( cca ) we call _ indefinite kernel cca _ ( ikcca ) ( section [ secikcca ] ) , on the rlp800 data .",
    "the previous state of the art for these data sets is @xmath8 ( the vertical line furthest to the right labelled oloff et al . ) which are larger by a factor of 5 to 10 as compared to cca ( section [ seccca ] ) and its improvements , kcca ( section [ seckcca ] ) and ikcca .",
    "as we were primarily interested in comparing our results against those of @xcite we did not look into other performance metrics other than mean rank .",
    "however , it would be interesting to pursue other , potentially more relevant measures of binding affinity such as kd , ki and ic50 as was done by @xcite , where cca is linked to these performance measures .    , resulting from prediction on the test data from the rlp800 dataset .",
    "performance on the wdi data . ]    while not discussed in this paper , an important unresolved issue in this cheminformatic - based approach to the prediction of protein  ligand binding is the selection of meaningful chemical descriptors .",
    "the type of chemical descriptors used can have a drastic effect on the predictive accuracy of an algorithm .",
    "one possible approach to addressing this issue would be to use a recently developed method called sparse cca ( scca ) , @xcite , @xcite , @xcite and @xcite .",
    "scca uses a lasso - like approach to identify sparse linear combinations of two sets of variables that are highly correlated with each other .",
    "an approach based on scca to the prediction of protein  ligand binding may prove to be quite useful in resolving some of the issues arising from chemical descriptor selection .    in section [ secresults ] we present results and details on the rlp800 data set as well as on two additional data sets .",
    "in sections [ seccca ] and [ seckcca ] we outline cca and kcca , respectively . in section [ secikcca ]",
    "we propose a new method , ikcca , which encompasses nonpositive semi - definite ( psd ) kernels ( i.e. , indefinite kernels ) , specifically we consider a class of kernels related to the normalized graph laplacian used in spectral clustering .",
    "finally , in section [ secprediction ] we show how prediction of a new ligand is done using cca ( and its variants ) .",
    "in addition to the real data results discussed in section [ secintroduction ] , we also tested our method on two additional data sets [ which we refer to as experimental settings ( es ) , the reason for which will become clearer in what follows ] .",
    "these data ( including the rlp800 data ) are subsets of a collection of 1300 complexes taken from pdbbind [ @xcite ] .",
    "these 1300 complexes are referred to as the _ refined set _ ( rs ) , a set of entries that meet a defined set of criteria regarding crystal structure quality .",
    "a representative subsample of 195 of the complexes is called the _ core set _ ( cs ) .",
    "this is a collection of complexes selected by clustering the rs into 65 groups using protein sequence similarity and retaining only 3 complexes from each cluster .",
    "the three experimental settings considered are denoted by es i [ this experimental setting was used in @xcite ] , es ii and es iii . in each of these experimental settings",
    "the rs and cs complexes are separated into training and testing sets in such a way as to test different aspects of our model .",
    "in es i the 637 training and 163 test complexes are randomly sampled from the rs .",
    "es i is meant to provide a general test of our models performance . in es",
    "ii the training ( 153 complexes ) and testing ( 36 complexes ) sets are sampled from the cs in such a way that the various protein families in the cs are well represented in both .",
    "this separation is meant to test the performance of our cca - based methods when the sample size is small .",
    "finally , in es iii the testing set ( 162 complexes ) is composed of proteins which are under represented in the training set ( 1006 complexes ) .",
    "this is meant to test our methods ability to correctly identify novel complexes .",
    "@lcce3.0cd3.1d3.1d2.2d2.1@ * setting * & & & & & & & & + es i & rs & 637 & 163 & 800 & 18.1 & 10 & 7.5 & 4.5 + & rs@xmath9wdi & & & 54121 & 310 & 67 & 56 & 30 + [ 3pt ] es ii & rs & 153 & 36 & 189 & & 8 & 13.75 & 3.5 + & rs@xmath9wdi & & & 53994 & & 275.1 & & 92.9 + [ 3pt ] es iii & rs & 1006 & 162 & 1168 & & 11.9 & 7.4 & 4.4 + & rs@xmath9wdi & & & 54120 & & 53 & 24.3 & 18.2 +    a note on how we use the training and testing sets : the tuning parameters for our model are selected , as discussed in section [ subsectuneparams ] , using only the training set . once tuning parameters have been selected , prediction on the testing set is then performed .",
    "this is meant to test the models performance on as - yet unobserved complexes .",
    "the results for each of these experimental settings is summarized in table  [ tableresults ] .",
    "the columns labeled `` train '' and `` test '' correspond to the size of the training / testing sets for each particular experimental setting .",
    "the column labeled `` embed '' corresponds to the total number of ligands against which our prediction is to be ranked .",
    "the remaining columns correspond to the method used and the average rank performance ( defined in section [ secintroduction ] ) of that method .",
    "the second row , second column in each cell labeled `` rs@xmath9wdi '' shows the results for each method on the reduced set plus the world drug index .",
    "these results are meant to more accurately mimic an actual drug screen by having a larger test set to search against . as the method used in @xcite failed to provide useful results for the es ii and es",
    "iii experimental settings , no results are reported here .",
    "generally speaking , in all cases ikcca , using the ngl kernel , outperformed the other methods .",
    "all the cca - based methods provide considerable improvement over the previous approach .    looking a bit closer at the results it is interesting to note that while all three cca - based methods performed worse on the es ii data , kcca had the largest drop in performance .",
    "this can be seen by comparing the average rank performance against the total number of ligands we are searching against .",
    "the decrease in performance in all cases more than likely has to do with the small size of the training set . in the case of kcca , its considerable decrease in performance ,",
    "we suspect , may have to do with not having a large enough training sample to reliably select the bandwidth parameter @xmath10 .",
    "for ikcca the adaptive nature of the local kernel is probably what allows it to perform well in the low sample size setting .",
    "cca [ @xcite ] naturally lends itself to the problem of predicting the binding between proteins and ligands .",
    "this can be understood for the following reasons : first , traditional methods of prediction , for example , regression , assume a direction of dependence between the variables to be predicted and the predictive variables .",
    "here we have a symmetric , not causal , type of relationship : the binding between a  protein and its ligand is inherently co - dependent .",
    "second , in addition to capturing the dependence structure we are looking to model , cca is well suited to the type of prediction we are interested in performing . to understand this ,",
    "consider the following ( see also section  [ sectoyex1 ] for a more detailed discussion ) .",
    "the objective of cca is to find directions in one space , and directions in a second space such that the correlation between the projections of these spaces onto their respective directions is maximized .",
    "these directions are commonly referred to as canonical vectors .",
    "let us assume that a  set of directions are found so that the corresponding projections of proteins and of ligands are strongly correlated .",
    "predicting a new ligand given a new protein would begin with projecting the new protein into canonical correlation space .",
    "then , assuming the same correlation structure holds for this new point , prediction of the new ligand would amount to interpolating its location in ligand space based on the location of the protein in protein space .",
    "this will be discussed in greater detail in section [ secprediction ] .",
    "next we provide a brief discussion on the details of cca and kcca .",
    "let @xmath11 and @xmath12 , @xmath13 denote a protein ",
    "ligand pair .",
    "the sample of pairs is collected in matrices @xmath14 and @xmath15 with @xmath16 and @xmath17 as the descriptors for a row .",
    "the objective of cca is to find the linear combinations of the columns of @xmath18 ( proteins ) , say @xmath19 and the linear combinations of the columns of  @xmath20 ( ligands ) , say @xmath21 such that the correlation , @xmath22 is maximized . without loss of generality",
    "assume that the matrices @xmath18 and @xmath20 have been mean centered .",
    "letting @xmath23 , @xmath24 and @xmath25 the cca optimization problem is @xmath26 subsequent directions are found by imposing the additional constraints@xmath27 for @xmath28 and@xmath29 , @xmath30 , @xmath31 .    in order to avoid issues arising from multicollinearity and singularity of the covariance matrices",
    "we impose a penalty [ @xcite ] on the directions  @xmath32 and @xmath33 so that the constraints in ( [ eqcca ] ) are modified to be @xmath34 where @xmath35 is a regularization parameter .",
    "the predictive accuracy of this approach was discussed in section [ secintroduction ] , with results summarized in figure [ figikcca ] . recall that the lines in these figures labeled cca correspond to the average predicted rank using cca , which improved upon @xcite shown by the lines labeled a such .      an appealing aspect of cca is its intuitive geometric interpretation [ @xcite and @xcite ] .",
    "a  geometric perspective lends itself to a better understanding of the general behavior of cca , and provides further evidence of its applicability to the protein  ligand matching problem .",
    "taking a closer look at the @xmath4th canonical correlation , @xmath36 , @xmath37 ( @xmath38 ) , in the optimization problem shown in ( [ eqcca ] ) , it can be seen that this quantity is in fact equal to the cosine of the angle between @xmath39 and @xmath40 ( @xmath41 and @xmath42 are commonly referred to as canonical variates ) . with this in mind maximizing the cosine ( i.e. , correlation ) can equivalently be thought of as minimizing the angle between @xmath41 and @xmath42 .",
    "furthermore , it can be shown that minimizing the angle is equivalent to minimizing the distance between pairs of canonical variates , @xmath43 subject to the constraints described in ( [ eqcca ] ) .",
    "note that viewed in this way , in canonical correlation space , this amounts to finding a system of coordinates such that the distance between coordinates is minimized .",
    "this is a sense in which cca is an appropriate approach to the protein  ligand matching problem .",
    "as will be seen in sections [ seckcca ] and [ secikcca ] , this geometric interpretation of cca extends naturally to kcca and ikcca .",
    "note that the regularized variant of cca does not have the same geometric interpretation , nonetheless viewing regularized cca in this manner still provides useful insight into its behavior .",
    "consider the protein  ligand problem as outlined above .",
    "for this toy example we set @xmath44 and @xmath45 .",
    "suppose the descriptors for this toy example are molecular weight ( mw ) and surface area ( sa ) of the molecule .",
    "recall that each row of  @xmath46 and each row of @xmath47 corresponds to an observation , a  protein or a ligand , respectively , and the columns correspond to the descriptors mw and sa .",
    "the pairs are identified by a unique label , corresponding to ids from the protein data bank ( pdb ) ( http://www.pdb.org[www.pdb.org ] ) .",
    "figure [ plotex1 ] shows the two toy data sets .     in the ligand space",
    "corresponds to a weighted average ( discussed in section [ secprediction ] ) of the cyan points and the purple point , that is , of the nearest neighbors of 11gs in the protein space . ]    from figure [ plotex1 ] it can be seen that the distribution of points in the two spaces are quite similar in the sense that the location of corresponding points in the two spaces are close .",
    "the points connected to 11gs ( red ) by dashed black lines are its three nearest neighbors .",
    "the cyan points are neighbors shared in both spaces and the blue and purple points are mismatched .",
    "two of three neighbors are shared in common ( in the euclidean sense ) .",
    "consider the case where the red point in ligand space is not observed and the task is to predict its value .",
    "using the weighted average ( see section [ secprediction ] for details on the derivation of the weights ) of the points in ligand space that correspond to the nearest neighbors of the point 11gs in the protein space ( points highlighted in cyan and purple in ligand space ) would yield a relatively poor prediction despite the strong apparent similarity between the two distributions of points .",
    "next suppose that instead of carrying out the prediction of a new ligand in the original data space we carry out our prediction in canonical correlation space .",
    "solving for @xmath32 and @xmath33 in ( [ eqcca ] ) , gives us the canonical vectors shown in figure [ ccaprojdir ] .",
    "what is important to notice is how the distribution of points along the first and second canonical directions in both protein and ligand space are quite similar .",
    "this is due to the property of alignment that arises naturally from maximizing the correlation .",
    "figure [ motivateprojplot ] shows the projections of the data onto the first two canonical vectors ( note that separate directions are found in protein and ligand space ) .",
    "we can see that with the slight modification in alignment that has resulted from the cca projections , the point 11gs now shares the same neighbors in both spaces .",
    "in particular note that now the predicted value in the projected ligand space is closer to the actual value ( again using the weighted average ) .     onto the first and second canonical vectors .",
    "in contrast to figure [ plotex1 ] , the point 11gs now shares the same neighbors in both spaces and the predicted value in green is much closer to the actual value . ]",
    "this example was deliberately chosen to illustrate the case where cca is effective .",
    "however , in most cases the relationship between points in different spaces may be far more complicated , as we now illustrate .",
    "we now consider an example where the relationship between spaces is more complex .",
    "suppose that we have the same general framework as in section [ sectoyex1 ] but rather than having both protein and ligand space characterized by mw and sa , we now have that the space of proteins has descriptors @xmath48 and @xmath49 and that the space of ligands has descriptors @xmath50 and @xmath51 , shown in figure [ datasimplekernel ] . as",
    "before the observation highlighted in red , 1a94 , corresponds to a new protein whose corresponding ligand we are trying to predict .",
    "the point highlighted in cyan is one of the 3-nearest neighbors of 1a94 in both spaces .",
    "those points highlighted in purple ( and blue ) are nearest neighbors in only the protein ( and ligand ) spaces , respectively .",
    "the point @xmath52 in the ligand space , highlighted in green is a weighted average of the nearest neighbors of the point 1a94 in protein space .",
    "using @xmath52 as a prediction of the new ligand would not provide a particularly accurate prediction",
    ".     in ligand space corresponds to a weighted average of the points 1a08 , 1a09 and 1a1b , that is , the nearest neighbors of the point 1a94 in protein space . ]    as before , we use cca to try and find a linear combination of the descriptors which best align the two spaces .",
    "figure [ ccsimplekernel ] is a plot of the projections onto the first and second canonical variates in protein and ligand space .",
    "the color scheme is the same as in figure [ datasimplekernel ] .",
    "as can be seen , standard cca does not seem to be able to find a good alignment between the two spaces , which is confirmed by the relatively low values of the canonical correlations , 0.79 and 0.54 , respectively , for the first and second directions .    in section [ seckcca ]",
    "we show how mappings into a kernel induced feature space can be used to improve prediction .",
    "this will lead to our discussion of kcca .",
    "returning to the example in section [ sectoyex2 ] , suppose it is believed that some type of functional relationship exists between the descriptors across spaces that is best characterized by looking at the second order polynomials of the descriptors within each space , that is , @xmath53 \\\\[-8pt ]    \\phi_y\\dvtx   ( d_y^1 , d_y^2 ) & \\rightarrow&((d_y^1)^2,(d_y^2)^2,d_y^1d_y^2 ) .",
    "\\nonumber\\end{aligned}\\ ] ] figure [ kernsimplekernelreceptor ] shows plots of proteins and ligands embedded into this three dimensional space .",
    "as can be seen there are now two neighbors shared in common between spaces ( colored in cyan ) .",
    "furthermore the prediction of the new observation , @xmath52 ( in green ) by a weighted average of its three nearest neighbors in feature space is , by comparison , much closer to the actual value than the corresponding prediction in object space .    .",
    "looking at the plots on the top and bottom ( corresponding to protein and ligand space , respectively ) , the overall correspondence between points in protein space and ligand space is much better than in the original ( object ) space .",
    "this improved mapping will allow cca to do a better job aligning the two spaces . ]    as before cca is used on this transformed data , now in feature space , to align the space of proteins and ligands .",
    "figure [ kernccsimplekernelreceptor ] shows a plot of the projected data .",
    "note that now both the new protein and its ligand ( highlighted in red ) share three neighbors and that the distribution of points within each of the spaces is quite similar .",
    "the quality of the alignment is further confirmed by looking at the canonical correlation values which are near 1 for each of the first two directions . since the value of the third canonical correlation is considerably smaller ( approximately 0.2 ) we only project onto the first two directions .",
    "it is worth noting that , as a result of overfitting , the kernel canonical correlation values can sometimes be artificially large due to strong correlation between features in kernel space .",
    "regularization methods for helping to control these effects in the kernel case will be discussed in section [ subseckcca ] .",
    "highlighted in green on the plot on the right is close to the actual value of 1a94 . ]    in general , finding explicit mappings such as those in ( [ simplefeatmap ] ) is impractical or simply not possible as in some cases this would require an infinite dimensional feature space . as we will see in the following section ,",
    "kernels allow us to avoid such issues .",
    "kcca [ bach and jordan(@xcite ) , @xcite , @xcite ] extends cca by finding directions of maximum correlation in a kernel induced feature space .",
    "let @xmath54 and @xmath55 be the feature space maps for proteins and ligands , respectively .",
    "the sample of pairs , now mapped into feature space , are collected in matrices @xmath56 and @xmath57 with @xmath58 and @xmath59 as their respective row elements .",
    "the objective , as before , is to find linear combinations , @xmath60 and @xmath61 such that the correlation , @xmath62 , @xmath63 , is maximized .",
    "note that because @xmath32 and @xmath33 lie in the span of @xmath56 and  @xmath57 , these can be re - expressed by the linear transformations @xmath64 and @xmath65 . letting @xmath66 and @xmath67 with @xmath68 and @xmath69 being the associated kernel functions for each space , respectively",
    ", the cca optimization problem in ( [ eqcca ] ) now becomes @xmath70 here the subscript @xmath71 in @xmath72 is included to emphasize the fact that the space of functions we are considering are in a rkhs .",
    "subsequent directions are found by including the additional constraints that @xmath73 for @xmath28 , and @xmath74 , @xmath75 .    in order to avoid trivial solutions ,",
    "we penalize the directions @xmath76 and @xmath77 modifying the constraints in ( [ eqkcca ] ) to be @xmath78 here @xmath79 is a regularization parameter .",
    "note that the geometric interpretation of ( unregularized ) kcca , provided that data have been centered in feature space , is the same as cca .",
    "the only difference lies in the fact that the space in which this geometry is observed is in feature space rather than object space .    in order for kcca to be understood as maximizing correlation in feature space centering must be performed in feature space .",
    "centering in feature space can be done as follows .",
    "let @xmath80 where @xmath81 is an @xmath82 matrix of ones , then @xmath83 we assume throughout that the kernel matrices are centered .        [ smileydata ]    the predictive accuracy of this approach was discussed in section [ secintroduction ] , with results summarized in figure [ figikcca ] .",
    "recall that the cyan line in figure [ figikcca ] corresponds to the average predicted rank using kcca which is an improvement over both @xcite and cca .",
    "we saw in section [ exfeatmap ] that kcca was able to overcome some of the obstacles encountered by standard cca . where kcca begins to encounter problems",
    "is when the distribution of points within a space is nonstandard and/or heterogeneous . to illustrate this",
    "consider the example shown in figure [ smileydata ] , as with the protein  ligand matching problem , there is a one - to - one correspondence between points in the two spaces .",
    ", each of the three clusters is in fact composed of two subclusters .",
    "likewise , each of the two clusters in the plots on the right are composed of three subclusters . ]",
    "is highlighted . ]",
    "the underlying structure between these spaces is illustrated in figure [ smileystructure ] .",
    "the top row of plots tells us about how the distribution of points on the right ( cluster space ) relates to the distribution of points on the left ( smiley face space ) .",
    "the bottom set of plots tells us about how the distribution of points on the left is related to distribution of points on the right .",
    "if we were to look at the two spaces as marginal distributions , there is a distinct impression of the three clusters in the left , and two in the right . the joint distribution , however , has six distinct groups .",
    "looking at the plots on the left in figure [ smileystructure ] , each of the three clusters is in fact composed of two subclusters .",
    "likewise , each of the two clusters in the plots on the right are composed of three subclusters .",
    "ideally , the projections onto the kcca directions would identify each of these six groups , shown in figure [ figsmiley6cols ] .",
    "using an rbf kernel with @xmath84 we look at the first five canonical directions .",
    "ideally , what we would see is a separation of each of the groups as well as a strong alignment between each of the spaces .",
    "what we find looking at figure [ smileyrbf ] , a  scatter plot matrix of the first five kernel canonical variates ( kcv ) , is that while the leading correlations are large ( 0.98 , 0.97 , 0.95 , 0.80 , 0.75 ) , we are not able to find the structure in the data we were looking for , that is , separating out the six groups ( with each of the colors corresponding to one of the six groups ) .",
    "note that only the projections in the smiley face space are shown since the cluster space projections look essentially the same .    .",
    "each of the colors in this plot corresponds to one of the six underlying subpopulation in the data ( see figure [ smileystructure ] for details ) . ]    in the context of the protein  ligand matching problem this type of situation presents a potential problem . suppose a new point , say in the space with the smiley face , is projected into kcca space .",
    "as can be seen in figure [ smileyrbf ] , there is a great deal of overlap between each of the six subgroups in the projected space .",
    "in particular note that each of the overlapped groups is composed of , respectively , the left eye , right eye and mouth .",
    "the reason this type of behavior presents a problem is that each of the eyes and the mouth are actually composed of two different subpopulations where each of the populations correspond to very different groups in the space with the two clusters .",
    "so while we may be able to accurately predict the location of a new point in kcca space the interpretation of its surrounding neighbors may not be so meaningful .",
    "a potential shortcoming of standard kcca , which was illustrated in the example presented in figure  [ smileydata ] , is that standard positive definite kernels can be limited in their ability to capture nonstandard heterogeneous behavior in the data .",
    "a more general class of kernels which is better suited to handle this type of behavior takes the form @xmath85 here @xmath86 denotes some neighborhood of the observation @xmath87 , such as a @xmath88 nearest neighborhood or a fixed radius @xmath89-neighborhood .",
    "kernels of this form restrict attention to the local structure of the data and allow for a flexible definition of similarity .",
    "our motivation for considering this class of kernels in the context of the protein  ligand matching problem is the following . in the rlp800 dataset",
    "there are approximately 150 important subgroups in the data .",
    "these subgroups correspond to unique proteins , or more specifically their binding pockets , which typically have three or four different conformations specific to a particular ligand .",
    "exploitation of this group structure in the data can help improve prediction .",
    "this can be accomplished by using a `` local kernel '' function that allows us to capture these groups more readily than , say , the rbf kernel .",
    "the intuition here follows from the example presented in section [ sectoyexnonstd ] where we saw that the type of groups that an rbf kernel will be able to find will be dictated by the choice of the bandwidth parameter @xmath10 .",
    "the local kernel overcomes this by adjusting locally to the data . by adjusting to the data",
    "locally it is better able to exploit this group structure.=1    in summary , given a new protein , its projection in this local kernel cca space will be more likely to fall into a group of similar proteins . then",
    ", as before , the goal is that the ligands associated with this group of proteins provide an accurate representation of the ligand we are trying to predict .",
    "this improved performance exploiting group structure in the data comes at some price .",
    "in particular , the problem encountered with this class of kernels is that they are frequently indefinite ( see the discussion following definition [ innprod ] ) . as a result of the indefiniteness ,",
    "many of the properties and optimality guarantees no longer hold .",
    "indefinite kernels have recently gained increased interest [ @xcite , @xcite , @xcite , @xcite ] where , rather than defining @xmath90 to be a function defined in a rkhs , @xmath90 is defined in a space characterized by an _ indefinite inner product _ called a  _ krein _ space . in section  [ secindefkern ]",
    "we provide an overview of some of the definitions and theoretical results about krein spaces [ following the discussion of @xcite ] .    before discussing ikcca",
    ", we will need to provide some definitions and theorems related to indefinite inner product spaces , that is , krein spaces [ more details can be found in @xcite ] .",
    "[ innprod ] let @xmath91 be a vector space on the scalar field .",
    "an inner product @xmath92 on @xmath91 is a bilinear form where for all @xmath93 , @xmath94 :    * @xmath95 ; * @xmath96 ; * @xmath97 for all @xmath98 implies @xmath99 .",
    "the importance of @xmath91 being a vector space on a _ scalar field _ is that it allows for a flexible definition of an inner product ( i.e. , the scalar in one of the dimensions could be complex or negative as we will see below ) .",
    "an inner product is said to be _ positive _ if for all @xmath100 , @xmath101 .",
    "it is called a  _ negative _ inner product , if for all @xmath102 , @xmath103 .",
    "an inner product is called indefinite if it is neither strictly positive nor strictly negative .",
    "[ rem1 ] to illustrate how indefinite inner products arise in the context of our problem , consider the following .",
    "suppose we have a symmetric kernel function @xmath90 , which is indefinite , the implication of this is that the resulting kernel matrix @xmath104 is indefinite and that it therefore contains positive _ and _ negative eigenvalues .",
    "let @xmath105 be the eigendecomposition of @xmath106 , where @xmath107 are the eigenvectors and @xmath108 is the diagonal matrix of eigenvalues starting with the @xmath109 positive eigenvalues , followed by the @xmath110 negative ones and the @xmath111 eigenvalues equal to 0 . to see how @xmath106 can be interpreted as a matrix composed of inner products in this indefinite inner product space consider the following representation of its eigendecomposition :",
    "@xmath112 let @xmath113 and @xmath114 be equal to the first @xmath115 columns of @xmath116 .",
    "define the @xmath4th row of @xmath114 to be equal to @xmath117 we then have a kernel matrix composed of elements @xmath118 \\\\[-8pt ] & = & \\langle\\phi_i , \\phi_j \\rangle_{\\mathcal{h}_{+ } } - \\langle\\phi _ i , \\phi_j \\rangle_{\\mathcal{h}_{- } } \\nonumber\\\\ & = & \\langle\\phi_i , \\phi_j \\rangle_{\\mathcal{k}}. \\nonumber\\end{aligned}\\ ] ] from ( [ eqik ] ) we can see that unlike psd kernels where @xmath119 for any , with indefinite kernels @xmath120 can take on any value , making optimization over such a quantity challenging .    despite this difference , many of the properties that hold for reproducing kernel hilbert spaces ( rkhs ) , such as ( and perhaps most importantly ) the reproducing property [ @xcite ] , also hold for these indefinite inner product spaces [ see @xcite for details ] . the key difference lies in the fact that rather than minimizing ( maximizing ) a regularized risk functional , as in the rkhs setting , the corresponding optimization problem becomes that of finding a stationary point of a similar risk functional .",
    "section [ secindefkern ] provided some insight into the challenges that arise from dealing with indefinite kernels .",
    "in particular , remark [ rem1 ] points to the fact that the solution that we find may not be globally , or even locally , optimal ( as it may be a saddle point ) .",
    "the form of the ikcca problem we present in this section is motivated by the discussion of the previous section and the works of @xcite and @xcite .",
    "in particular , the addition of a  stabilizing function on the indefinite inner product @xmath121 as discussed in @xcite led us to consider introducing a constraint on the behavior on the indefinite kernels matrix itself .    in the following ,",
    "let @xmath122 denote the frobenius norm .",
    "define @xmath123 to mean that the matrix @xmath124 is positive semi - definite and let @xmath125 be tuning parameters ( discussed in more detail later this section ) . here",
    "@xmath126 and @xmath127 are the ( potentially ) indefinite kernels and @xmath128 and @xmath129 will be the positive semi - definite approximations of these kernels . with this notation in mind , we now define the ikcca optimization problem : @xmath130 where @xmath131 and @xmath132 . note that this optimization problem and the kcca optimization problem are only equivalent when the kernel matrices @xmath126 and @xmath127 are positive semi - definite ( see the [ @xcite ] for details on the equivalency between the optimization problem in ( [ sccaoptim ] ) and ( [ eqkcca ] ) and a proof of theorem [ thmikccakoptim ] ) .",
    "[ thmikccacavvex ] letting @xmath133 , the optimization problem in @xmath134 is concave in @xmath135 and @xmath136 , @xmath137 and convex in @xmath128 and @xmath129 .    see the for a proof .",
    "let @xmath138 denote the positive part of the matrix @xmath139 , that is , @xmath140 , where @xmath141 and @xmath142 are @xmath4th eigenvalue ",
    "eigenvector pair of the matrix @xmath18 . with this in mind , we have the following theorem .",
    "[ thmikccakoptim ] letting @xmath133 , and given the optimization problem in @xmath134 the optimal values for @xmath128 and @xmath129 are given by @xmath143 \\\\[-8pt ]   \\mathbf{k}_y & = & ( \\mathbf{k}_y^0)_{+}. \\nonumber\\end{aligned}\\ ] ]    the proof of theorem [ thmikccakoptim ] makes use of the following lemma .",
    "let @xmath144 be a known , square , not necessarily positive - definite matrix , and @xmath145 a square , unknown matrix , then :    [ lemfroboptim ] the solution to the optimization problem @xmath146 is @xmath147    the proofs of theorem [ thmikccakoptim ] and lemma [ lemfroboptim ] can be found in the .",
    "points @xmath148 and @xmath149 are projected onto their first @xmath109 canonical directions as follows : first compute their kernelization , using the indefinite kernel functions @xmath150 and @xmath151 , @xmath152 then calculate @xmath153 where @xmath154 and @xmath155 .",
    "we now return to the example in section [ sectoyexnonstd ] using the kernel defined in ( [ localkernel ] ) with weights ( [ eqnglweightsrbf ] ) .",
    "note that this kernel is closely related to the normalized graph laplacian ( ngl ) kernel used in spectral clustering ; see @xcite for an overview of spectral clustering methods . from figure [ glsmiley ]",
    ", it can be seen that we are now able to capture the underlying structure of the data , identifying each of the six subpopulations : @xmath156 and @xmath157 here @xmath158 is the symmetric @xmath159-neighborhood of the point @xmath16 [ i.e. , if @xmath160 then @xmath161 and @xmath162 where @xmath163 is the @xmath159th neighbor of the point @xmath16 .    .",
    "this is a scatter plot matrix of the projections onto the first five ikcca variates ( ikcv ) using the kernel in ( [ localkernel ] ) with weights ( [ eqnglweightsrbf ] ) .",
    "unlike the projections shown in figure [ smileyrbf ] , here we are able to separate out the six groups . ]    looking at plots of the first four eigenvectors ( figures [ eigensmile ] and [ eigencluster ] ) in both the smiley face space and the cluster space , we can see how the behavior of the eigenvectors causes the segmentation of the data that we observe in figure [ glsmiley ] .",
    "first , we discuss how these figures are generated and then what it is they are telling us .",
    "generate an equally spaced dimensional grid spanning the range of values in each space .",
    "calculate the kernel representation and projection of each grid point into ikcca space .",
    "use the projected values to assign color intensities to each point in the grid of each space ( darker for negative values , lighter for positive values ) .",
    "plot the grid and for each point using the colors calculated from the previous step .",
    "the important thing to note in both of these figures is the distribution of positive and negative projected values and how these are driving the segmentation , which we observe in figure [ glsmiley ] .",
    "for example , in figure [ eigensmile ] the first canonical variate segments out one of the faces ( red ) from the other ( blue ) .     using the kernel in ( [ localkernel ] ) with weights ( [ eqnglweightsrbf ] ) .",
    "these plots allow us to visualize how the canonical vectors separate out each of the clusters . ]     using the kernel in ( [ localkernel ] ) with weights ( [ eqnglweightsrbf ] ) . ]",
    "[ secprediction ]      let us define the projected values of the observations in protein and ligand space onto their first @xmath109 canonical vectors as @xmath164 , @xmath165 , and @xmath166 , @xmath167 , @xmath168 .",
    "the predicted value of @xmath169 is calculated as follows [ using a modification of the lle algorithm of @xcite ] :    compute the @xmath159 neighbors of the data point @xmath170 ( the projected value of @xmath171 into canonical correlation space ) .",
    "define @xmath172 to be the @xmath159 nearest neighbors of the point @xmath87 .",
    "recall that cca finds directions which best align two spaces .",
    "thus , assuming that directions @xmath177 and @xmath178 , @xmath37 , have been found such that the correlation between spaces is strong , using the weights @xmath173 found in protein space should provide a reliable estimate of @xmath169 .",
    "values for the tuning parameters , @xmath179 ( the regularization parameter ) , @xmath180 ( the number of dimensions we are projecting into ) , @xmath181 ( the neighborhood for the lle - based prediction ) , @xmath10 ( for the rbf kernel ) and @xmath182 ( for the ngl kernel ) are found by searching over a suitable @xmath183 grid for each .",
    "the final set of parameters are selected based on which produces the lowest average rank ( discussed in section [ secintroduction ] ) ."
  ],
  "abstract_text": [
    "<S> drug discovery is the process of identifying compounds which have potentially meaningful biological activity . </S>",
    "<S> a major challenge that arises is that the number of compounds to search over can be quite large , sometimes numbering in the millions , making experimental testing intractable . </S>",
    "<S> for this reason computational methods are employed to filter out those compounds which do not exhibit strong biological activity . </S>",
    "<S> this filtering step , also called virtual screening reduces the search space , allowing for the remaining compounds to be experimentally tested .    in this paper </S>",
    "<S> we propose several novel approaches to the problem of virtual screening based on canonical correlation analysis ( cca ) and on a kernel - based extension . </S>",
    "<S> spectral learning ideas motivate our proposed new method called indefinite kernel cca ( ikcca ) . </S>",
    "<S> we show the strong performance of this approach both for a toy problem as well as using real world data with dramatic improvements in predictive accuracy of virtual screening over an existing methodology .    ,    ,    ,    . </S>"
  ]
}