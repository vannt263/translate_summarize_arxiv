{
  "article_text": [
    "acoustic source localization in ocean waveguides has been a focus of underwater acoustic research in recent decades .",
    "numerous methods have been developed to solve this problem , including the successful and widely used method of matched - field processing ( mfp).@xcite@xmath0@xcite despite the success of mfp , it is limited in some practical applications .",
    "one drawback is its sensitivity to the mismatch between model - generated replica fields and measurements .",
    "mfp gives reasonable predictions only if the ocean environment can be accurately modeled .",
    "unfortunately , this is difficult because the realistic ocean environment is complicated and unstable .",
    "an alternative approach to the source localization problem is to find features directly from data.@xcite@xmath0@xcite recently , machine learning@xcite@xmath0@xcite has obtained remarkable results when applied to areas such as speech recognition,@xcite image processing@xcite and natural language processing.@xcite among the earth sciences , it has also been used for applications in seismology.@xcite@xmath0@xcite however , the few publications on machine learning in ocean acoustics are based on the state - of - the - art in the early 1990s . steinberg _ et al . _",
    "( 1991 ) @xcite used neutral networks to determine the source depth and ozard _ et al . _",
    "( 1991 ) @xcite simulated the range and depth discrimination using artificial neural networks in matched field processing .",
    "( 1994 ) @xcite developed a method to estimate ocean - sediment density from sparse measurements using generalized radial basis function networks .",
    "more recently , nonlinear regression was applied to the source localization problem.@xcite since the theory , techniques and computational resources of machine learning have developed considerably over the last decade , the goal of this paper is to explore the feasibility of neural networks in locating ocean sources and lay the groundwork for the application of present day machine learning methods in ocean acoustics .",
    "supervised machine learning , in which acoustic observations ( training data ) are used to train the neural network , is investigated here .",
    "presently , there are numerous efficient open source machine learning libraries , including tensorflow,@xcite scikit - learn,@xcite theano,@xcite caffe,@xcite torch.@xcite in this paper we use tensorflow , a high - level neural network library , to design our fnn architecture and solve the classification problem .",
    "the paper is organized as follows .",
    "the input data preprocessing and source range mapping are discussed in secs .",
    "[ subsec:2:1 ] and [ subsec:2:2 ] .",
    "the theoretical basis of feed - forward neural networks is given in secs .",
    "[ subsec:2:3 ] and [ subsec:2:4 ] .",
    "simulations and experimental results in secs .",
    "[ sec:3 ] and [ sec:4 ] demonstrate the performance of the fnn . in sec .",
    "[ sec:5 ] , the effect of varying the fnn parameters is discussed .",
    "the conclusion is given in sec .",
    "[ sec:6 ] .",
    "the dynamics of the ocean and its boundary cause a stochastic relationship between the received phase and amplitude at the array and the source range . for the first test cases implemented here",
    "we assume a simple deterministic relationship between ship range and pressure . the pressure ",
    "range relationship is in general unknown but may be discovered using machine learning methods . here",
    "a feed - forward neural network ( fnn ) is used to accomplish this task .",
    "the received pressure is preprocessed and used as the input of the neural network ( sec .  [ subsec:2:1 ] ) .",
    "the output is a set of binary vectors which map into discrete source ranges ( sec .",
    "[ subsec:2:2 ] ) .",
    "the structure and training method of fnn are described in secs .",
    "[ subsec:2:3 ] and [ subsec:2:4 ] .      to make the processing independent of the complex source spectra",
    ", the received array pressure is transformed to a normalized sample covariance matrix before feeding it to the fnn . the complex pressure at frequency @xmath1 obtained by taking the dft of the input pressure data at @xmath2 sensors is denoted by @xmath3^t$ ] .",
    "the sound pressure is modeled as @xmath4 where @xmath5 is the noise , @xmath6 is the source term , and @xmath7 is the green s function . to reduce the effect of the source amplitude @xmath8 ,",
    "this complex pressure is normalized according to @xmath9    the normalized sample covariance matrices ( scms ) are averaged over @xmath10 snapshots to form the conjugate symmetric matrix    @xmath11    where @xmath12 denotes conjugate transpose operator .",
    "the product @xmath13 contains an @xmath14 term , which for large snr is dominant and thus reduces the effect of the source phase .",
    "preprocessing the data according to eqs .",
    "( [ press ] ) and ( [ scm ] ) ensures that the green s function is used for localization .",
    "only the real and imaginary parts of the complex valued entries of diagonal and upper triangular matrix in @xmath15 are used as input to fnn to save memory and improve calculation speed .",
    "these entries are vectorized to form the real - valued input @xmath16 of size @xmath17 to the neural network .      a set of source ranges is discretized into @xmath18 bins of width @xmath19 . for a given source range @xmath20 , the binary vector @xmath21 , which is the desired output of the neural network ,",
    "is defined such that @xmath22 where @xmath23 is the input sample index and @xmath24 with @xmath25 are the labeled source ranges .",
    "thus , each vector @xmath21 maps to a unique range @xmath24 ( fig .",
    "[ fig : fig1 ] ) .",
    "it is useful to think of @xmath26 as the probability that the source is observed at range @xmath24 when designing the fnn architecture .",
    "source localization as a classification problem .",
    "the ranges from @xmath27 to @xmath28 are mapped from binary vectors . ]",
    "the feed - forward neural network , also known as multi - layer perceptron , is constructed using a feed - forward directed acyclic architecture.@xcite in an fnn , see fig .",
    "[ fig : fig2](a ) , information is directed from the input neurons through the hidden neurons to the output neurons .",
    "therefore the outputs can be expressed as deterministic functions of the inputs.@xcite in this study , the fnn computes the probability @xmath29 that the source is at range @xmath24 , for @xmath25 , given input * x*.     ( a ) diagram of a feed - forward neural network and ( b ) sigmoid function .",
    ", title=\"fig : \" ]   ( a ) diagram of a feed - forward neural network and ( b ) sigmoid function . ,",
    "title=\"fig : \" ]    the basic neural network model@xcite is described as a series of functional transformations . here",
    ", three layers ( input layer @xmath30 , hidden layer @xmath31 and output layer @xmath32 ) are used to construct the fnn . the input layer @xmath30",
    "is comprised of @xmath33 input variables @xmath34^t$ ] . the @xmath35th linear combination of the input variables is given by @xmath36 where @xmath37 is the number of neurons in @xmath31 and the superscript indicates that the corresponding parameters are in the first layer of the network . the parameters @xmath38 and @xmath39 are weights and biases , respectively .",
    "the linear combinations of weights and biases @xmath40 are also called activations . in @xmath31 ,",
    "these activations are then transformed using an activation function @xmath41 , @xmath42 the activation function is nonlinear in general with the logistic sigmoid and the hyperbolic tangent function the most common .",
    "the logistic sigmoid ( see fig .  [",
    "fig : fig2](b ) ) is chosen here : @xmath43    similarly , for output layer @xmath32 , the output unit activations are expressed as linear combinations of @xmath44 @xmath45 where @xmath18 is the total number of output neurons . @xmath46 and",
    "@xmath47 represent weights and biases for the second layer .",
    "the output unit activations are transformed by an appropriate activation function , which is determined by the nature of the data and the assumed distribution of target variables.@xcite for standard regression problems , the activation function is the identity , @xmath48 . for multiple binary classification problems ,",
    "each output unit activation is transformed by a logistic sigmoid function .",
    "we treat source localization as a multi - class classification problem and apply the softmax function to the @xmath32 activations@xcite .",
    "the output of @xmath32 is then @xmath49 where @xmath29 satisfies @xmath50 and @xmath51 and represents the probability that the source is at range @xmath24 .",
    "the vector @xmath52 is the set of all weight and bias parameters .      before applying the fnn to unlabeled data , the weights and biases @xmath52 are determined by training the model on labeled data .",
    "the observation @xmath53 consists of the vectorized scm ( see sec .",
    "[ subsec:2:1 ] ) . the desired output of the neural network is the binary target vector @xmath21 , which is given for observed data ( see sec .",
    "[ subsec:2:2 ] ) .",
    "thus @xmath21 represents the true probability distribution of the source range for input @xmath53 .",
    "the kullback - leibler ( kl ) divergence is used to measure the dissimilarity between the fnn - estimated probability distribution @xmath54 and the desired probability distribution @xmath55@xcite @xmath56,\\ ] ] where @xmath26 is the @xmath57th entry of the vector @xmath21 and @xmath58 .",
    "minimizing the kl divergence is equivalent to minimizing @xmath59 since the desired output @xmath21 is independent of @xmath52 .",
    "@xmath60 is also known as the cross entropy error function .    for @xmath61 observation vectors ,",
    "the input matrix is formed as @xmath62 $ ] .",
    "the corresponding averaged cross entropy and solution for the weights and biases are @xmath63 and @xmath64.\\end{aligned}\\ ] ]    between the first and second layer there are @xmath65 weights and @xmath37 biases . between the second and third layer",
    "there are @xmath66 weights and @xmath18 biases .",
    "thus in total there are @xmath67 unknowns to solve .",
    "several optimization methods are provided in the tensorflow software . in this paper ,",
    "adam@xcite(adaptive moment estimation ) is used to solve the optimization problem presented in eq .",
    "( [ weights ] ) .",
    "the classification problem is implemented as follows :    \\1 .",
    "data preprocessing .",
    "the recorded pressure signals are fourier transformed and @xmath10 snapshots form the scm from which the neural network input @xmath68 is formed .",
    "division of preprocessed data into training data and test sets .",
    "for the training data , the known source ranges @xmath69 for each input @xmath53 are discretized and encoded using a binary vector @xmath21 .",
    "\\3 . training the fnn .",
    "@xmath70 $ ] are used as the training input and the corresponding binary range vectors @xmath71 $ ] as the desired output .",
    "the model weights and biases are chosen to minimize the cross - entropy ( sec .",
    "[ subsec:2:4 ] ) .",
    "prediction on unlabeled data .",
    "the weights trained in step 3 are used in the fnn to predict the source range for test data .",
    "the resulting binary vector output is mapped back to range , and the prediction error is reported by the mean absolute percentage error ( see sec .",
    "[ subsec:3:3 ] ) .",
    "acoustic data is simulated using kraken .",
    "@xcite the environmental parameters are chosen to simulate the noise09 experiment ( see fig .  [",
    "fig : fig3 ] ) .",
    "the source frequency used is 300 hz .",
    "the source depth is constant at 5 m in a 152 m pekeris waveguide , with sound speed 1500 m / s in the water column and a fluid halfspace bottom with sound speed 1600 m / s , density 1.6 @xmath72 , and attenuation coefficient 0.1 @xmath73 .",
    "the vertical array consists of 16 receivers spanning 130145 m depth with inter - sensor spacing 1 m.    a source traveling away from the receiver at 2 m / s is simulated by varying the range from 0.1 to 2.86 km at 2 m intervals .",
    "realizations with different snrs are generated by adding appropriate complex gaussian noise to the simulated received complex pressure signals .",
    "since the source moves in range and the source level is assumed constant with range , snr is defined at the most distant range bin    @xmath74    where @xmath75 is sound pressure signal received by the @xmath76th sensor at the longest source - receiver distance and @xmath77 represents the noise variance .    environment . ]",
    "the scm for a 16element vertical array is formed at each range point by averaging over @xmath78 successive snapshots ( 9 snapshots overlapped ) according to eq .",
    "( [ scm ] ) .",
    "the number of neurons in the input layer is therefore @xmath79 .",
    "the range sample interval is 2 m , with 1380 total range samples ( 1 s duration per snapshot ) .",
    "thus , a total of @xmath80 matrices constitute the sample set spanning the whole range 0.12.86 km .    for each snr , two realizations of noisy measurements",
    "are generated .",
    "one realization of size 1380 @xmath81 272 is used for the training set . for the test set ,",
    "the range sample interval is changed to 20 m , and a realization of size 138 @xmath81 272 is used as input .    in the test set ,",
    "@xmath82 output neurons represent ranges from 0.12.86 km incremented by 20 m. the number of neurons in the hidden layer is @xmath83 . to prevent overfitting , the `` keep dropout '' technique,@xcite with probability 0.5 ,",
    "the initial learning rate for the adam optimizer@xcite is 0.01 and the maximum number of iterations is 1000 .",
    "the total number of unknown weights and biases to solve for is @xmath84 ( see sec .",
    "[ subsec:2:4 ] ) .      to quantify the prediction performance of the classifier ,",
    "two measures are used .",
    "\\1 . the mean absolute percentage error ( mape ) over @xmath61 samples",
    "is defined as @xmath85 where @xmath86 and @xmath87 are the predicted range and the ground truth range , respectively .",
    "we define a 10% error interval with respect to the ground truth range @xmath88 , whose upper and lower bounds are @xmath89 and @xmath90 : @xmath91    the prediction performance is examined for four snrs ( @xmath9210 , @xmath925 , 0 , 5 db ) .",
    "figure [ fig : fig4 ] compares range predictions by fnn and 10% error bars with the true ranges on test data . for the four snrs tested ,",
    "the mape for the fnn predictions is 20.6 , 6.5 , 0.2 and 0.0% , respectively .",
    "the fnn predicted 100% of the source ranges within 10% when snr @xmath93 db .",
    "( color online ) range predictions by fnn on test data set with snr of ( a ) @xmath9210 , ( b ) @xmath925 , ( c ) 0 , and ( d ) 5 db . ]    as described in sec .",
    "[ sec:2 ] , the output @xmath94 of fnn represents the probability distribution over a discrete set of possible ranges . to demonstrate the evolution of the probability distribution as the fnn",
    "is trained , @xmath94 versus training steps is plotted in fig .",
    "[ fig : fig5 ] for the signal with snr 5 db at range 1.5 km .",
    "output probability for range 0.12.86 km ( the true range is 1.5 km ) after training steps ( 0 , 50 , 100 , 150 , 200 , 250 , 300 ) . ]    in fig .",
    "[ fig : fig6 ] , the convergence of the fnn algorithm is investigated .",
    "figure  [ fig : fig6 ] shows the convergence of the cross entropy eq .",
    "( [ cross_entropy ] ) with respect to the optimization step on training and test data .",
    "it shows that the fnn converges after about 300 steps at all snrs . for low snrs ( @xmath95 db ) ,",
    "the fnn classifier generates poor predictions on test data while performing well on training data , which indicates overfitting .",
    "( color online ) cross entropy eq .",
    "( [ cross_entropy ] ) versus optimization steps on training ( solid ) and test ( dashed ) data with snr of ( a ) @xmath9210 , ( b ) @xmath925 , ( c ) 0 , and ( d ) 5 db . ]",
    "overfitting can also be determined by plotting the cross entropy versus number of training samples ( fig .",
    "[ fig : fig7 ] ) , known as the learning curves . for low snrs , the test error is significantly greater than the training data error , confirming that the fnn model overfits the data in this scenario ( figs .",
    "[ fig : fig7](a ) and [ fig : fig7](b ) ) .",
    "one approach to reduce overfitting is to increase the training set size , but gathering additional data can be difficult or impossible due to experimental or computational limitations . for higher snrs ( e.g. , 0 and 5 db ) ,",
    "both test and training errors converge to low cross entropy , figs .",
    "[ fig : fig7](c ) and [ fig : fig7](d ) .",
    "( color online ) learning curves , i.e. cross entropy versus training set size on training ( solid ) and test ( dashed ) data with snr of ( a ) @xmath9210 , ( b ) @xmath925 , ( c ) 0 , and ( d ) 5 db . ]",
    "shipping noise data radiated by r / v new horizon during the noise09 experiment was used to demonstrate the performance of the fnn localization .",
    "the experiment geometry is shown in fig .",
    "[ fig : fig8 ] , with bottom - moored vertical linear arrays ( vlas ) indicated by triangles and the three ship tracks used for range estimation .",
    "the water depth along the ship tracks was 152 m. the hydrophone sampling rate was 25 khz .",
    "the data from vla2 , consisting of 16 hydrophones at 1 m spacing , are used for range estimation .",
    "the frequency spectra of shipping noise recorded on the top hydrophone during the three periods are shown in fig .",
    "[ fig : fig9 ] .",
    "the striations indicate that the source was moving",
    ". the snr decreases with increasing source - receiver distance .",
    "data from period jd031 01:4302:05 are used as the training set and jd031 01:0501:24 and jd035 13:4113:51 are used as the test sets ( test - data-1 and test - data-2 ) . for each set ,",
    "the @xmath96 scm at each range and frequency , averaged over 10 successive 1-s snapshots , is used as fnn input .",
    "there are 1380 samples in the training data set and 120 samples in each of the test data sets .",
    "the number of neurons in hidden layer is chosen as @xmath83 . the source - receiver range 0.13 km is divided into @xmath82 range points , i.e. 138 neurons in the output layer . as in the simulations in sec .",
    "[ subsec:3:2 ] , the keep probability for training dropout is 0.5 , the initial learning rate is 0.01 and the maximum iteration step is 1000 .",
    "the gps antenna is separated from the noise  generating propeller by a distance @xmath97 . to account for this difference we use the range between the propeller and vla2 as the ground truth range @xmath88:@xmath98 where @xmath99 represents the range between the gps antenna and vla2 . according to the r / v new horizon handbook , @xmath100 m. in the following ,",
    "the ranges have been corrected by eq .",
    "( [ correction ] ) .",
    "the shipping noise has a wide frequency band as seen from fig .",
    "[ fig : fig9 ] .",
    "thus in the following subsection , performance of the fnn with the single and multi - frequency input are compared .",
    "the performance of all test cases are summarized in table  [ table:1 ] .",
    "( color online ) spectra of shipping noise during periods ( a ) jd031 01:4302:05 , ( b ) jd031 01:0501:24 , and ( c ) jd035 13:4113:51 .",
    ", title=\"fig : \" ]   ( color online ) spectra of shipping noise during periods ( a ) jd031 01:4302:05 , ( b ) jd031 01:0501:24 , and ( c ) jd035 13:4113:51 .",
    ", title=\"fig : \" ]   ( color online ) spectra of shipping noise during periods ( a ) jd031 01:4302:05 , ( b ) jd031 01:0501:24 , and ( c ) jd035 13:4113:51 .",
    ", title=\"fig : \" ]      input scms are formed at 350 , 550 , 750 , and 950 hz . for each frequency",
    ", the corresponding scm is first used to train the fnn .",
    "then , the optimal weights and biases ( @xmath52 ) at each frequency are used to predict the source - receiver ranges in the test data .",
    "the estimated snrs at farthest range are 811 , 711 and 45 db for training data , test - data-1 and test - data-2 , respectively .",
    "the number of input neurons used is @xmath101 , with 52,746 total weights and biases to solve .",
    "the cross entropy converges in 300 steps at each frequency during the training ( fig .",
    "[ fig : fig10 ] ) .",
    "the fnn range prediction results are given in fig .",
    "[ fig : fig11 ] along with @xmath88 .",
    "the 10% error areas are also shown in fig .",
    "[ fig : fig11 ] for comparison . as in sec .",
    "[ sec:3 ] , eq .",
    "( [ mape ] ) is used to quantify the prediction performance .",
    "the mape statistics of the fnn for training data , test - data-1 and test - data-2 are given in part i of table  [ table:1 ] .",
    "the minimum error is 12.4% ( fig .",
    "[ fig : fig11](j ) ) at 550 hz on test - data-2 and the poorest prediction occurs at 350 hz for both test - data-1 and test - data-2 ( figs .",
    "[ fig : fig11](e ) and ( i ) ) . in general , the predictions are better at close ranges due to higher snr .",
    "( color online ) cross entropy versus optimization step on training data for fnn with single frequency inputs .",
    "( a ) 350 hz , ( b ) 550 hz , ( c ) 750 hz , and ( d ) 950 hz . ]",
    "multi  frequency scms are formed by concatenating multiple single ",
    "frequency scms to one input vector , thus increasing the number of input neurons to @xmath102 , where @xmath103 is the number of frequencies . as before ,",
    "there are @xmath83 neurons in the hidden layer and @xmath82 neurons in the output layer .",
    "we choose @xmath104 , resulting in 157,194 weights and biases to be determined .",
    "four frequency bands are investigated : 450550 , 560650 , 660750 , and 450900 hz .",
    "the range predictions are shown in fig .",
    "[ fig : fig12 ] .",
    "the source  receiver ranges ( @xmath88 ) with error bars of @xmath105 are provided for comparison .",
    "the detailed prediction statistics of mape are given in part ii of table  [ table:1 ] .",
    "the minimum prediction error is 8.8% on test - data-1 at frequency band 660750 hz , compared with 12.4% for single frequency scm input .",
    "thus multi - frequency scm inputs improves source range prediction .",
    "the number of output neurons , corresponding to the resolution of range steps , was varied to determine its effect on range estimation results . previously ( see sec .",
    "[ sec:4 ] ) @xmath82 output neurons were used , corresponding to a range resolution of 20 m. figure  [ fig : fig13 ] shows the results of fnn range estimation on test - data-1 for range increments from 2 to 200 m. as before , there are 1380 samples in the training data with 10 snapshots averaged for each sample .",
    "the fnn performs well for all tested range resolutions with the best prediction performance for the 50 m resolution .",
    "in addition , the neural network can be trained efficiently by tensorflow regardless of the number of output neurons . for each of the six cases",
    ", it takes less than one minute cpu time ( 2015 imac ) for the training .",
    "the number of snapshots averaged to create the scms may affect the fnn predictions .",
    "increasing the number of snapshots makes the input more robust to noise , but could introduce mismatch if the source is moving or the environment is evolving . increasing",
    "the number of snapshots from 1 to 20 improves performance , see fig .",
    "[ fig : fig14 ] .",
    "the number of neurons in the hidden layer also affects fnn prediction performance . in secs .",
    "[ sec:3 ] ,  [ sec:4 ] , [ subsec:5:1 ] and [ subsec:5:2 ] , the number of hidden neurons is set to @xmath83 .",
    "part iii of table  [ table:1 ] shows the mape of fnn with different numbers of hidden neurons .",
    "the results show that the fnn has the minimum error when the number of hidden neurons is chosen as 64 for test - data-1 ( 9.8% ) and 128 for test - data-2 ( 9.2% ) .",
    "the mape statistic of the fnn with two hidden layers is shown in part iv of table  [ table:1 ] .",
    "p2.8 cm < p2.2 cm < p2.2 cm < p2.2 cm < p2.2 cm < p2.2 cm < & frequency&no . of hidden & no . of hidden & + & ( hz ) & layers & neurons & training data ( % ) & test - data-1 ( % ) & test - data-2 ( % ) + & 350 & 1 & 128 & 0.0 & 65.1 & 58.2 + & 550&1 & 128 & 0.0 & 15.5 & 12.4 + & 750 & 1 & 128 & 0.0 & 16.7 & 15.0 + & 950&1 & 128 & 0.0 & 13.7 & 18.3 + & 450 , 490 , 520 , 550&1 & 128 & 0.1 & 13.7 & 9.2 + & 560 , 590 , 620 , 650 & 1 & 128 & 0.1 & 14.4 & 14.0 + & 660 , 690 , 720 , 750 & 1 & 128 & 0.0 & 8.8 & 10.7 + & 450 , 600 , 750 , 900 & 1 & 128 & 0.0 & 10.2 & 11.6 + & & 1 & 16 & 2.8 & 35.6 & 22.9 + & & 1 & 32 & 0.0 & 23.5 & 18.0 + & & 1 & 64 & 0.0 & 9.8 & 15.2 + & & 1 & 128 & 0.1 & 13.7 & 9.2 + & & 1 & 256 & 0.0 & 15.1 & 9.4 + & & 1 & 512 & 0.0 & 15.4 & 11.1 + & & 1 & 1024 & 0.2 & 10.3 & 9.6 + & & 1 & 2048 & 0.0 & 9.9 & 12.1 + & & 2 & 32 & 8.9 & 39.4 & 27.1 + & & 2 & 64 & 0.1 & 20.2 & 11.6 + & & 2 & 128 & 0.1 & 23.1 & 9.4 + & & 2 & 256 & 0.1 & 10.5 & 11.2 + & & 2 & 512 & 0.0 & 17.9 & 8.8 + & & 2 & 1024 & 0.2 & 17.6 & 10.9 + & & 2 & 2048 & 0.0 & 21.9 & 11.2 +      only one source is considered in this paper .",
    "the simultaneous multiple source location problem is a more challenging problem , especially for sources close to each other .",
    "solving this problem with fnn is a multiple binary classification problem and will require additional training data .",
    "although the fnn with one hidden layer works well for the data sets in this paper , it deserves more effort on more complicated machine learning algorithms , e.g. deep learning .",
    "the appropriate deep learning algorithms are expected to gain better performance , especially for the complicated and large - scale problem .",
    "this paper presents an approach for source localization in ocean waveguides within a machine learning framework .",
    "the localization is posed as a supervised learning classification problem and solved by a feed - forward neural network ( fnn ) with one hidden layer .",
    "taking advantage of the modern machine learning library tensorflow , the large scale neural network is trained efficiently .",
    "normalized scms are fed as input to the fnn and the binary range vectors are used as output .",
    "simulations show that fnn achieves a good prediction performance for signals with snr above 0 db even with deficient training samples .",
    "noise09 experimental data further demonstrates the validity of this approach .",
    "the results show that multi - frequency input generate more accurate predictions than single frequency .",
    "in addition , both simulation and experimental results indicate that the fnn model used converges rapidly , which demonstrates its efficiency and stability in this classification problem .",
    "the training and test data were from the same ship . in a realistic application ,",
    "data from multiple ships of opportunity can be used as training data by taking advantage of the automatic identification system ( ais ) , a gps system required on all cargo carriers .",
    "machine learning is an attractive method for locating ocean sources because of its capability to learn features from data , without requiring sound propagation modeling .",
    "this data - driven approach makes machine learning a good candidate for ocean acoustic applications , especially for unknown environments .",
    "h. schmidt , a. b. baggeroer , w. a. kuperman , and e. k. scheer , `` environmentally tolerant beamforming for high - resolution matched field processing : deterministic mismatch , '' j. acoust .",
    "88 * , 18511862 , ( 1990 ) .",
    "a. b. baggeroer , w. a. kuperman , and p. n. mikhalevsky , `` an overview of matched field methods in ocean acoustics , '' ieee j. ocean .",
    "eng . * 18 * , 401424 , ( 1993 ) .",
    "d. f. gingras and p. gerstoft , `` inversion for geometric and geoacoustic parameters in shallow water : experimental results , '' j. acoust .",
    "97 * , 35893598 , ( 1995 ) . z.",
    "h. michalopoulou and m. b. porter , `` matched - field processing for broad - band source localization , '' ieee j. ocean .",
    "* 21 * , 384392 , ( 1996 ) . c. f. mecklenbraker and p. gerstoft , `` objective functions for ocean acoustic inversion derived by likelihood methods , '' j. comput",
    "* 8 * , 259270 , ( 2000 ) .",
    "c. debever and w. a. kuperman , `` robust matched - field processing using a coherent broadband white noise constraint processor , '' j. acoust .",
    "122 * , 19791986 , ( 2007 ) .",
    "s. e. dosso and m. j. wilmut , `` bayesian multiple source localization in an uncertain environment , '' j. acoust .",
    "129 * , 35773589 , ( 2011 ) .",
    "s. e. dosso and m. j. wilmut , `` maximum - likelihood and other processors for incoherent and coherent matched - field localization , '' j. acoust .",
    "132 * , 22732285 , ( 2012 ) .",
    "s. e. dosso and m. j. wilmut , `` bayesian tracking of multiple acoustic sources in an uncertain ocean environment , '' j. acoust .",
    ". am . * 133 * , el274el280 , ( 2013 ) .",
    "l. t. fialkowski , m. d. collins , w. a. kuperman , j. s. perkins , l. j. kelly , a. larsson , j. a. fawcett and l. h. hall , `` matched - field processing using measured replica fields , '' j. acoust .",
    "107 * , 739746 , ( 2000 ) .",
    "g. hinton , l. deng , d. yu , g. e. dahl , a. mohamed , n. jaitly , a. senior , v. vanhoucke , p. nguyen , t. n. sainath and b. kingsbury ,  deep neural networks for acoustic modeling in speech recognition : the shared views of four research groups , \" ieee signal proc . mag . * 29 * , 8297 , ( 2012 ) .",
    "j. l. perry and d. r. baumgardt ,  lg depth estimation and ripple fire characterization using artificial neural networks , \" in _ artificial intelligence applications , proceedings . ,",
    "seventh ieee conference _ * 1 * , 231234 , ( 1991 ) .",
    "a. khler , m. ohrnberger and f. scherbaum ,  unsupervised feature selection and general pattern discovery using self - organizing maps for gaining insights into the nature of seismic wavefields , \" comput . geosci .",
    "* 35 * , 17571767 , ( 2009 ) .",
    "a. caiti and t. parisini , `` mapping ocean sediments by rbf networks , '' ieee j. ocean .",
    "* 19 * , 577582 , ( 1994 ) .",
    "r. lefort , g. real and a. drmeau , `` direct regressions for underwater acoustic source localization in fluctuating oceans , '' appl . acoust . *",
    "116 * , 303310 , ( 2017 ) .",
    "m. abadi , a. agarwal and p. barham _",
    "et al_. ,  tensorflow : large - scale machine learning on heterogeneous distributed systems , \" _ software available from tensorflow.org_ , ( 2015 ) .                      * fig .",
    "[ fig : fig1 ] . source localization as a classification problem .",
    "the ranges from @xmath27 to @xmath28 are mapped from binary vectors . *",
    "[ fig : fig2 ] . ( a ) diagram of a feed - forward neural network and ( b ) sigmoid function . * fig .",
    "[ fig : fig3 ] . environment . * fig .",
    "[ fig : fig4 ] .",
    "( color online ) range predictions by fnn on test data set with snr of ( a ) @xmath9210 , ( b ) @xmath925 , ( c ) 0 , and ( d ) 5 db . * fig .",
    "[ fig : fig5 ] . output probability for range 0.12.86 km ( the true range is 1.5 km ) after training steps ( 0 , 50 , 100 , 150 , 200 , 250 , 300 ) .",
    "[ fig : fig6 ] .",
    "( color online ) cross entropy eq .",
    "( [ cross_entropy ] ) versus optimization steps on training ( solid ) and test ( dashed ) data with snr of ( a ) @xmath9210 , ( b ) @xmath925 , ( c ) 0 , and ( d ) 5 db . * fig .",
    "[ fig : fig7 ] .",
    "( color online ) learning curves , i.e. cross entropy versus training set size on training ( solid ) and test ( dashed ) data with snr of ( a ) @xmath9210 , ( b ) @xmath925 , ( c ) 0 , and ( d ) 5 db . *",
    "[ fig : fig8 ] .",
    "( color online ) ship tracks for noise09 experiment during the periods ( a ) jd031 01:4302:05 ( training data , ship speed 2 m / s ) , ( b ) jd031 01:0501:24 ( test - data-1 , ship speed @xmath922 m / s ) , and ( c ) jd035 13:4113:51 ( test - data-2 , ship speed 4 m / s ) . * fig .  [",
    "fig : fig9 ] .",
    "( color online ) spectra of shipping noise during periods ( a ) jd031 01:4302:05 , ( b ) jd031 01:0501:24 , and ( c ) jd035 13:4113:51 . * fig .",
    "[ fig : fig10 ] .",
    "( color online ) cross entropy versus optimization step on training data for fnn with single frequency inputs .",
    "( a ) 350 hz , ( b ) 550 hz , ( c ) 750 hz , and ( d ) 950 hz . * fig .",
    "[ fig : fig11 ] .",
    "( color online ) range predictions on training data ( a , b , c , d , first row ) , test - data-1 ( e , f , g , h , second row ) and test - data-2 ( i , j , k , l , third row ) by fnn with single frequency scm inputs .",
    "( a)(e)(i ) 350 hz , ( b)(f)(j ) 550 hz , ( c)(g)(k ) 750 hz , ( d)(h)(l ) 950 hz .",
    "the time index increment is 10 s for training and test - data-1 , and 5 s for test - data-2 . *",
    "[ fig : fig12 ] .",
    "( color online ) range predictions on training data ( a , b , c , d , first row ) , test - data-1 ( e , f , g , h , second row ) and test - data-2 ( i , j , k , l , third row ) by fnn with multi - frequency inputs .",
    "( a)(e)(i ) 450 , 490 , 520 , 550 hz . ( b)(f)(j ) 560 , 590 , 620 , 650 hz .",
    "( c)(g)(k ) 660 , 690 , 720 , 750 hz .",
    "( d)(h)(l ) 450 , 600 , 750 , 900 hz . the time index increment is 10 s for training and test - data-1 , and 5 s for test - data-2 . *",
    "[ fig : fig13 ] .",
    "( color online ) range predictions on test - data-1with multi - frequency input ( 450 , 490 , 520 , 550 hz ) with different range resolutions , i.e. output neurons .",
    "( a ) 2 m , 1380 output neurons , ( b ) 4 m , 690 neurons , ( c ) 10 m , 276 neurons , ( d ) 50 m , 55 neurons , ( e ) 100 m , 27 neurons , ( f ) 200 m , 13 neurons .",
    "the time index increment is 10 s. * fig .",
    "[ fig : fig14 ] .",
    "( color online ) range predictions on test - data-1 with multi - frequency input ( 450 , 490 , 520 , 550 hz ) with averaging across ( a ) 1 , ( b ) 5 , and ( c ) 20 1-s snapshots .",
    "the time index increment is 10 s."
  ],
  "abstract_text": [
    "<S> source localization is solved as a classification problem by training a feed - forward neural network ( fnn ) on ocean acoustic data . </S>",
    "<S> the pressure received by a vertical linear array is preprocessed by constructing a normalized sample covariance matrix ( scm ) , which is used as input for the fnn . </S>",
    "<S> each neuron of the output layer represents a discrete source range . </S>",
    "<S> fnn is a data - driven method that learns features directly from observed acoustic data , unlike model - based localization methods such as matched - field processing that require accurate sound propagation modeling . </S>",
    "<S> the fnn achieves a good performance ( the mean absolute percentage error below 10% ) for predicting source ranges for vertical array data from the noise09 experiment . </S>",
    "<S> the effects of varying the parameters of the method , such as number of hidden neurons and layers , number of output neurons and number of snapshots in each input sample are discussed . </S>"
  ]
}