{
  "article_text": [
    "surveys for planetary transits monitor tens or hundreds of thousands of stars at high precision , with rapid time sampling , and over long periods .",
    "the resulting datasets can be used not only to discover and characterise transiting exoplanets , but also to study a wide range of phenomena of stellar origin , such as pulsations , rotation , and binarity .",
    "however , transit survey data are typically affected by a wide range of artefacts , many of which affect all the target stars simultaneously , although to varying degrees .    the kepler space mission , launched on march 6 , 2009 @xcite , exemplifies this opportunity and the associated challenges .",
    "the unprecedented precision and baseline of the @xmath0150000 kepler light curves , most of which are now in the public domain , opens up the possibility of studying the micro - variability of stars other than the sun for the first time , probing amplitudes ranging from a few parts per thousand to a few parts per million , and timescales of weeks to years . however , there are also artefacts in the raw data , particularly long term trends , which are prominent in the light curves of all but the highest variability stars .",
    "this is visible in figure  [ fig : eg_curves ] , which shows a set of example light curves from the first month of kepler observations , known as quarter 1 ( hereafter q1 ; see section  [ sec : results_q1 ] for more details ) .",
    "these common - mode instrumental effects are usually known as _",
    "systematics_. when the cause of the systematics is understood well enough , it is possible to formulate explicit models for their removal . in many other situations ,",
    "however , we have little knowledge regarding the nature or number of systematic trends in the data . in this paper",
    "we present the development of a new methodology , which we refer to as _ astrophysically robust correction _",
    "( arc ) , which addresses the issue of discovering and removing underlying systematics in astronomical data .",
    "we apply the method to data from the first quarter ( q1 ) of kepler data , and show that it efficiently removes the systematic artefacts in that dataset .",
    "@xcite used the resulting cleaned data to perform a preliminary statistical study of the variability of the kepler target stars on month timescales .",
    "the problem of trend identification and removal in ensembles of stellar light curves was first addressed in the context of ground - based transit surveys , such as ogle @xcite , hatnet @xcite , or superwasp @xcite .",
    "the sources of systematics in these surveys are atmospheric ( e.g.  differential airmass variations across the field - of - view , seeing and atmospheric transparency variations ) as well as instrumental ( e.g.  pointing jitter combined with inter- and intra - pixel sensitivity variations ) .",
    "the two methods , which are most widely used to identify and remove systematics in ground - based transit surveys are known as sysrem @xcite and tfa ( trend filtering algorithm , @xcite ) .",
    "sysrem attempts to explain each light curve as a linear superposition of all the other light curves , and adjusts the coefficients of this linear basis model so as to minimise the total squared variance of all the light curves .",
    "the resulting algorithm is very similar to principal component analysis ( pca , see e.g.@xcite ) , although it allows for individual weighting of each observation of each star .",
    "because of this , the trends are not strictly orthogonal , and must be identified and removed iteratively .",
    "the number of times this is done must then represent a trade - off between removing systematics and preserving real variability .",
    "tfa proceeds in a similar fashion , but the trends are identified in a limited subset of the light curves , which is selected by the user , so as to contain a representative sample of the systematics , but minimal intrinsic variability .    the images from which the photometry is extracted , or ancillary sensors , can also be used to measure meteorological and instrumental parameters , which are thought to affect the photometry , for example seeing , airmass , detector temperature , etc  from these , one may construct a linear basis with which to model the systematics , a procedure known as external parameter decorrelation , ( epd , @xcite ) .",
    "epd is usually applied as a preliminary step prior to running a search for additional trends using ( e.g. ) sysrem or tfa .",
    "although the epd method as described in @xcite is designed for large ensembles of light curves , the same principle , namely fitting a given light curve with a linear combination of external parameters , can also be applied in situations where a single star is being monitored , for example high - precision observations of planetary transits or eclipses .",
    "all three of the methods outlined above make a number of important assumptions :    * that the relationship between the trends and the light curves is linear ; * that the data used to identify the trends ( light curves or external parameter data ) is sufficient to describe them completely ; * that the majority of the stars are not significantly variable , so that systematics dominate the global sum of the squared residuals which is used as a figure of merit .",
    "the third assumption becomes problematic in the case of space - based transit surveys such as kepler and corot @xcite , which routinely reach sub - millimagnitude precision . as these missions are revealing , many , if not most , stars are intrinsically variable at that level .",
    "intrinsic variability can become an important , if not dominant , contribution to the global figure of merit one is trying to optimize , exacerbating the problem of preserving this variability whilst removing the systematics . as a result , in pca - like approaches , the first few principal components can easily be dominated by a small number of highly variable stars , which happen to account for a large fraction of the overall variance of the data .",
    "this problem is exacerbated by the fact that these methods are couched in the framework of maximum likelihood , and therefore are not robust against overfitting .",
    "@xcite partially address this problem , by identifying and removing from the training set any light curves , whose basis coefficients are deemed anomalous . in this paper",
    "we go further , introducing a criterion which explicitly expresses our desiderata that the trends should be constituted of many small contributions from many light curves , and using a fully bayesian model to avoid over - fitting ( see section  [ sec : arcintro ] ) .",
    "the pipeline used to process kepler data before releasing it to the public incorporates a ` pre - search data conditioning ' ( pdc ) stage .",
    "the pdc is designed to remove instrumental systematics so as to optimize the efficiency of the transit detection process .",
    "early implementations of the pdc @xcite followed a procedure similar to epd , but attempted to identify and remove signals of stellar origin in the frequency domain before applying the correction .",
    "this preserved astrophysical variability in some , but not all cases .",
    "additionally , in some cases the pdc was found to introduce high - frequency noise into the light curves .",
    "this effect has been noted in a previous publication @xcite in which the noise properties of short ( 1-min ) and long ( 30-min ) cadence data were compared for the same targets .",
    "it is due to the fact that the estimate of each systematic trend is necessarily noisy .",
    "these issues made the pdc - processed light curves unsuitable for variability studies , and motivated the development of the alternative method , which is the subject of this paper . in the mean time , kepler pipeline has also evolved . in more recent implementations of the pdc , known as pdc - map ( maximum a - posteriori , @xcite ) ,",
    "a subset of highly correlated and quiet stars is used to generate a cotrending basis vector set , as well as a set of priors over the coefficients of each basis vector , which is then used in the correction of all the light curves .",
    "this leads to a significant improvement in robustness , but we shall defer a detailed comparison to section  [ sec : results_q1 ] .",
    "our starting point in the development of a robust detection and correction methodology for the kepler data was not fundamentally different to those of previously published methods : like virtually all of them , we use a linear basis model and identify the trends from the light curves themselves , as is done in sysrem and tfa .",
    "however , our approach differs in the following important ways :    * we perform the linear basis regression using the _ variational bayes _ ( vb ) method of approximate bayesian inference to ensure that the procedure is robust to uncertainties in the data , while maintaining computational tractability and scalability to large datasets ; * we apply _ automatic relevance determination(ard ) _ priors to the coefficients of the basis vectors .",
    "these priors have zero mean and their individual widths are adjusted iteratively to maximise the model evidence .",
    "this ensures that trends are identified and removed only if there is significant evidence for them in the data ; * we formulate an explicit criterion , based on a measure of the _ entropy _ of each trend , to reflect our belief that the trends are systematic i.e.  that they are present , at some level , in the majority of light curves , and that the contribution of any single light curve to any given trend should be small ; * we incorporate a de - noising step after the identification of each trend , which uses the _ empirical mode decomposition _ ( emd ) algorithm , chosen to avoid any intrinsic bias towards trends on particular timescales .",
    "the entropy criterion is similar in spirit to the modified pca method of @xcite , but it actively selects trends which satisfy our desiderata , rather than merely rejecting problematic light curves .",
    "this paper is structured as follows .",
    "section [ sec : method ] gives an overview of the arc , focussing on aspects which are new and/or with which our intended readership may be unfamiliar , namely our trend selection and stopping criteria and the emd algorithm .",
    "section  [ sec : results ] shows some illustrative results , starting with a walk - through on a synthetic example dataset , and then moving on to the kepler q1 data . in section  [ sec :",
    "disc ] , we compare the performance of the pdc - map and the arc .",
    "finally , section  [ sec : conclusions ] summarises the advantages and limitations of the arc and discusses open questions on which future work will focus .",
    "detailed descriptions of the bayesian linear basis regression model and of the vb inference method are given for reference in appendices  [ sec : vblbm ] and [ sec : vb ] .",
    "the main challenge , when performing systematics removal with a linear basis model , is to identify the appropriate basis , i.e.  the systematic trends .",
    "we adopt an iterative approach to this problem , isolating the most dominant systematic trend first , removing it and then repeating the process . at each iteration , the dominant systematic is identified by constructing a set of ` candidate ' trends from the light curves themselves , ranking them according to the entropy criterion , selecting the top few candidates , combining them into a single time - series using pca , and denoising the resulting trend .",
    "we now proceed to describe each of these steps in detail .",
    "first , we introduce a few notation conventions .",
    "our data consists of @xmath1 observations each of the relative flux of @xmath2 stars . throughout this paper ,",
    "we use the subscript @xmath3 ,  , @xmath4 to denote observation number and @xmath5 ,  , @xmath6 to denote the star number .",
    "we denote the @xmath7 observation of star @xmath8 as @xmath9 , and the corresponding observation time @xmath10 . the light curve of star @xmath8 forms a column vector @xmath11 ,  , @xmath12{^{\\sf t}}$ ] ( where @xmath13 is the matrix transpose of @xmath14 ) .",
    "we consider each of the observed light curves , @xmath15 , to be composed of a linear combination of an underlying ` true ' stellar signal , @xmath16 , an unknown number , @xmath17 , of systematic trends , @xmath18 ( @xmath19 ,  , @xmath17 ) , and an observation noise process , @xmath20 : @xmath21 where the unknown factors @xmath22 represent the contribution of the @xmath23 systematic trend to the @xmath24 light curve .    defining the ensemble of all other light curves , @xmath25 , we seek to model as much of @xmath15 as possible using @xmath26 as a _",
    "basis_. this is achieved via a bayesian linear model with inference performed using variational bayes ( see appendices  [ sec : vblbm ] and [ sec : vb ] for details ) such that , @xmath27 where the @xmath28 are a set of weights .",
    "we repeat the process for each light curve in turn , resulting in a set of putative explanatory vectors @xmath29 . in the remainder of this section",
    ", we refer to the @xmath30 as candidate trends , to distinguish them from the ` adopted ' trends @xmath18 .",
    "our primary hypothesis is that systematic trends are present in the majority of the light curves .",
    "thus , if @xmath30 represents a linear combination of ` true ' systematic trends , it should be composed of many small contributions from many of the other light curves , rather than a few dominant contributions from a small number of light curves .",
    "more formally , the distribution of the weights @xmath31 should have a high shannon entropy @xcite : @xmath32 where we have defined @xmath33 by analogy with a normalised probability distribution .",
    "we therefore rank each candidate trend @xmath30 according to the entropy @xmath34 of the associated set of weights .",
    "the @xmath30 with the highest entropy are expected to be mutually similar , since they all represent a _",
    "systematic _ trend . on the other hand ,",
    "those with the lowest entropy correspond to cases where a particular light curve was found to be very similar to one or two others , but not to the rest .",
    "this is illustrated in the case of the kepler q1 data by figure [ fig : maxent_basis1 ] .",
    "we form a reduced basis @xmath35 by selecting the few candidate trends with the highest entropy .",
    "we normally use ten , but as the maximum entropy trends are mutually similar , the exact number ( within reason ) is not important .          if , as expected , the trends in the reduced basis @xmath35 are representative of a single , dominant systematic , then they should be mutually similar , as illustrated in figure  [ fig : maxent_basis1 ] . we can thus use pca to extract a single trend from the reduced basis ( the first principal component ) , and evaluate just how dominant this trend is . the first principal component of @xmath35 contains most of the information , and explains most of the overall variance of @xmath35 .    we start by decomposing @xmath35 into a pair of orthonormal matrices of singular components , @xmath36 , and @xmath37 , and a diagonal matrix @xmath38 of singular values , @xmath39 . the latter are the square roots of @xmath40 , the eigenvalues of @xmath41 .",
    "@xmath42 @xmath36 is also the matrix of eigenvectors of @xmath41 : @xmath43 in which @xmath44 is a diagonal matrix of eigenvalues , @xmath45 .",
    "spectral radius _ of each eigenvector , defined as @xmath46 is then a measure of the fraction of the overall variance of @xmath35 explained by that eigenvector . in this paper",
    "we consider , in particular , the spectral radius,@xmath47 , of the first eigenvector .    . ]    as discussed above , if the candidate trends which make up @xmath35 are truly systematic , they should also be _ self - similar _ , and the spectral radius of the first principal component should be large ( close to unity ) .",
    "if so , the first principal component contains pertinent information regarding the systematic trends in the data .",
    "for example , fig .",
    "[ fig : u_eg ] shows the first principal component of the matrix @xmath35 identified from all the kepler q1 light curves .",
    "its spectral radius is @xmath48 , so it is highly representative of the @xmath35 , and it would be reasonable to adopt it as one of the @xmath18 .",
    "however , as it is a linear combination of a finite number of noisy light curves , it may still contain significant amounts of noise . using it directly",
    "may therefore introduce noise into the light curves , particularly those of bright stars .",
    "thus , we introduce a final de - noising step in our trend discovery process",
    ".      _ empirical mode decomposition _",
    "( emd , @xcite ) decomposes a time - series into a series of _ intrinsic modes _",
    ", each of which admits a well - behaved hilbert transform , enabling the computation of an energy - frequency - time distribution , or hilbert spectrum .",
    "we chose to use emd for de - noising of the systematic trends , rather than any other method , because it does not focus on a particular frequency range , but it also does not rely on harmonicity or linearity .",
    "the core computation of emd lies in the successive detection of maxima and minima in the time - series .",
    "these turning points are thence used to define upper and lower envelopes to the signal by fitting a spline curve which passes through the turning - point locations .",
    "the mid - point of these envelopes is then used as a baseline which is removed from the signal .",
    "this procedure is iterated until the baseline function is flat ( to within a threshold ) .",
    "the resultant waveform at this point forms an intrinsic mode , which is then removed from the original time series .",
    "the entire process is then repeated , successively extracting intrinsic modes until the residual has no turning points .",
    "figure [ fig : emd_eg ] shows this process , which is referred to as _ sifting _ in the emd literature , applied to a simple synthetic time - series .",
    "the example signal consists of a sine curve superimposed on a quadratic , as shown in the blue trace in the top left sub - figure .",
    "the green curves are the envelope functions at each iteration and the red trace is the mid - point baseline , which is subtracted from the time series at each iteration .",
    "we see that , even after a small number of iterations , the sine component is highlighted and this indeed forms the first intrinsic mode of the data .",
    "we continue this process until relative changes in the baseline function are @xmath49 ( this results in some 20 iterations for the example shown here ) .",
    "figure [ fig : emd_eg_res ] shows both resultant intrinsic mode functions extracted from this data after iterating the sifting process until relative changes are below the threshold .    .",
    "]      starting from the original set of light curves , the trend identification and removal proceeds as follows :    1 .",
    "identify a set of candidate trends @xmath29 , each associated with entropy @xmath50 ; 2 .   construct a candidate basis @xmath35 from the 10 candidate trends with the highest entropy ; 3 .",
    "apply pca to @xmath35 and measure the spectral radius @xmath47 of its first principal component ; 4 .   if @xmath51 , stop . 5 .   otherwise , perform emd on the first principal component , and adopt the intrinsic mode with the largest variance as the next systematic trend @xmath52",
    "remove all the trends identified so far to produce a set of partially corrected light curves @xmath53 where the @xmath22 are once again found using vb with arc priors .",
    "return to step ( i ) , with the @xmath54 as inputs ;    the process continues until @xmath51 at step ( iv ) .",
    "the choice of stopping threshold @xmath55 is a matter of fine - tuning .",
    "typical values used in this work varied between 0.6 and 0.9 , and the appropriate threshold was found by trial and error ( i.e.by examining small subsets of the corrected light curves obtained using various values of @xmath55 ) .",
    "all the kepler results presented in this paper used @xmath56 .",
    "we note that one could also select this threshold based on theoretical arguments .",
    "for example , recalling that the @xmath57 s are normalised eigenvalues of the matrix @xmath58 ( eqs .",
    "[ eq : eig ] & [ eq : specr ] ) , we can make use of the fact that the set of eigenvalues of an arbitrary matrix are bounded by the weyl inequalities . for a large matrix ,",
    "this can be extended to formulate probability distributions over the expected values in the presence of noise @xcite .",
    "this could in principle be used to select @xmath55 in a probabilistic manner .",
    "however , ultimately , the threshold will remain subjective , whether it is couched in terms of @xmath59 , or a user - defined credibility limit in probability space . on balance",
    ", we felt that trial and error was the best means of ensuring that the choice of @xmath55 reflects the all - important domain knowledge of the user .",
    "the computational cost of the vb linear basis modelling done in step ( i ) scales as @xmath60 .",
    "it thus becomes prohibitive if @xmath2 is very large .",
    "when treating a large number of light curves , it is possible to decouple the trend identification and removal processes , using only a subset of the light curves for trend identification .",
    "this subset might be selected at random , or adjusted to contain light curves displaying evidence of systematics , but not strong intrinsic variability .",
    "the trend removal process scales more benignly as @xmath61 and so can be applied easily even to large sets of light curves .",
    "in this section we apply the arc method outlined above to two example datasets : first , a synthetic example for explanatory purposes , and the kepler q1 data , where we compare our results to the output of the existing pdc - map pipeline .",
    "in this subsection we present a simple synthetic example , to act as a ` walk - through ' guide to the methodology and as a proof of concept .",
    "we start with a set of 200 synthetic ` light curves ' formed from random phase , frequency and amplitude sine curves , plus additive gaussian noise . to these",
    "we add random amounts of two ` systematic trends ' : an exponential decay with long time constant , and a quadratic term .",
    "this constitutes our input dataset , examples of which may be seen in the top panel of figure [ fig : synth_eg ] .",
    "note that , without loss of generality , we normalise the curves such that each have unit variance and zero mean .",
    "% of the variance .",
    "bottom panel : the two basis trends discovered by the algorithm ( solid lines ) and the corresponding trends injected into the data ( dashed lines ) ; we note that the solid and dashed lines are almost undistinguishable as the recovered trends are very close to the originals . ]",
    "the steps of the method are illustrated in figure [ fig : synth_eg ] .",
    "the trends were identified in a representative subset of 50 ` light curves ' chosen at random , and the basis @xmath35 was constructed from the ten highest - entropy trends at each iteration .",
    "two systematic trends were identified before the stopping criterion was reached using @xmath62 .",
    "the resulting correction is illustrated on an example ` light curve ' ( not one of the subset in which the trends were identified ) in figure  [ fig : synth_eg_res ] .        in this synthetic example",
    "we may compare the original data , prior to addition of systematic trend artefacts , with the recovered detrended traces , as we have the luxury of knowing the ` ground truth ' .",
    "the resultant linear correlations between the true ` light curves ' and those recovered by the algorithm lie between 0.94 and 0.999 , with a median value of 0.98 , indicating a very high reconstruction accuracy ( we note that the median correlation between the original curves and the trend - corrupted versions is only 0.11 ) .",
    "we now apply the arc to the first month of kepler data , known as q1 .",
    "the arc - corrected q1 data were already used by @xcite to study the statistics of stellar variability , but here we present a detailed description of the results and a comparison with the more recently released pdc - map data .",
    "the pdc - map data we use in this paper were made public on 25 april 2012 as part of data release 14 , although the only significant difference between this and the earlier release of q1 used by @xcite is the replacement of the original pdc results with the output of the newer pdc - map pipeline .",
    "the data to which the arc was applied are essentially the same .",
    "the q1 data consist of 1639 observations of 156097 stars taken over 33.5 days in may ",
    "june 2009 .",
    "the data we consider has been pre - processed only to convert it from pixel - level data to light curves , as described in @xcite .",
    "the incident light is collected by 24 square modules , arranged to cover the focal plane of the telescope @xcite .",
    "each module consists of two rectangular charge - coupled device ( ccd ) detectors , each of which has two readout channels , resulting in a total of 84 channels and 94.6 million pixels .",
    "we first identified a single set of basis trends for all the light curves using a randomly selected subset of 200 light curves and using a threshold @xmath56 .",
    "these are shown in figure [ fig : global_basis ] .",
    "the first two have low - complexity , and are roughly linear and quadratic in nature , respectively .",
    "the third shows more complex dynamics , but is nonetheless present in a very large number of q1 light curves .",
    "it is caused by the on / off switching of the reaction wheel heaters @xcite .",
    "we note that , although the first two trends resemble low - order polynomials , they are _ not _ simply linear and quadratic components . furthermore , if we were to remove such low - order polynomials from each light curve individually , we would be failing to take into account the fact that the systematic trends are _ global _ rather than specific to each light curve .",
    "we then repeated the trend identification step , first on a module - by - module basis , then separately for each output channel .",
    "the basis trends identified in the latter case are shown in figure  [ fig:4x4trends ] .",
    "we compared the results from all three approaches , and ultimately settled on the output channel specific basis .",
    "the motivation for this is two fold .",
    "first , it is reasonable to expect that the systematics affecting the data collected on different detectors and read by different sets of electronics might differ , and visual examination of the corrected light curves appeared to bear this out , although the differences were very small .",
    "each of the q1 light curves was then corrected using the set of basis trends identified for the corresponding output channel .",
    "the full set of basis trends and corrected light curves is available online at www.physics.ox.ac.uk / users / aigrain / keplersys/.",
    "in this section , we assess the performance of the arc systematics correction on the q1 data , and compare it to the pdc - map results .",
    "figure [ fig : eg_results ] shows a selection of raw q1 light curves along with the pdc - map- and arc - corrected versions .",
    "these were selected at random on four different output channels in different parts of the detector , and are intended to represent a fairly typical subset of light curves , rather than a best- or worst - case scenario .",
    "the most striking outcome of the comparison is that the pdc - map and arc results are , to first order , very similar .",
    "both successfully preserve stellar variability in most cases , and remove most of the trends which one would deem to be systematic after visually inspecting many light curves .        however , there are some minor differences between the corrections applied by the pdc - map and the arc . in the top panel of figure  [ fig : eg_results ] ,",
    "the low - frequency component of the applied corrections differs slightly .",
    "it is very difficult to determine , on a dataset of this duration , which correction is more appropriate . in the bottom panel , the pdc - map attempts to correct for the effect of periodic pointing anomalies caused by the presence of an eclipsing binary ( eb ) among the guide stars used in q1",
    "this is a known effect , which was remedied after q1 ( by eliminating the offending object from the set of guide stars ) . as we shall see below , although the eb signature is present among the basis vectors identified by the pdc - map pipeline , it remains challenging to correct .",
    "the eb signature is not captured at all by the arc .",
    "it is visible in the candidate trends itendified for some ( not all ) of the output channels , before the emd denoising step , but it is never represented in the intrinsic mode with the largest variance , which is adopted as the ` smoothed ' basis trend .",
    "this is a limitation of our approach .",
    "more importantly , however , in this example the pdc - map also introduces significant amounts of high - frequency noise , which was not present in the original light curve .",
    "the emd de - noising step avoids this , at the cost of failing to correct for the eb trend .    to investigate the differences between the pdc - map and arc further , we computed the relative scatter of the corrected light curves on a range of timescales .",
    "we worked from the normalised light curves ( after dividing them by the raw median flux ) , smoothed them on the timescale of interest using a median filter of the corresponding width , and estimated the scatter @xmath63 as 1.48 times the median of the absolute deviations from unity .",
    "figure  [ fig : rms_comp ] shows the ratio @xmath64 ( where the subscript pdc refers to the pdc - map ) as a function of the median flux , for two representative output channels ( using all stars in each channel , namely 1290 and 2007 respectively ) , and for three different timescales : 30 minutes ( the sampling of kepler long - cadence observations ) , 6 hours ( most relevant for transit detection , and 6 days ( relevant , for example , to stellar rotation studies ) .    on all timescales ,",
    "the ratio is below 1 for the majority of stars , meaning that the scatter of the arc - corrected light curves is smaller than that of the pdc - map light curves .",
    "this effect is particularly noticeable for the fainter stars , where the scatters resulting from the two corrections often differ by a factor of 3 or more . at this stage",
    ", it would be tempting to interpret this as indicating that the pdc - map introduces some spurious signal into many of the light curves , which the arc does not , presumably thanks to the de - noising step .",
    "of course , we can not entirely rule out the alternative possiblity that the arc might be removing real astrophysical signal .",
    "however , it seems implausible that it would do systematically across a wide range of timescales , particularly because the ard priors used at the trend removal stage ensure that trends are only removed if there is significant evidence for them in the data .    in order to verify that arc is indeed removing only systematics , and that any alterations of the underlying ( astrophysical ) signals are minimal",
    ", we performed an injected signal test .",
    "we randomly selected 200 low - variability stars , displaying little other than white noise in their arc - processed light curves , from mod2.out1 .",
    "we added artificial sinusoidal signals ( similar to those described in section 3.1 ) to the _ raw _ light curves for these stars .",
    "we then used the arc to identify a basis from to a set of light curves including some with injected signals and some without .",
    "this basis was then used to detrend both sets of light curves , allowing us to recover an estimate of the injected signals by subtraction .",
    "we then compared this to the actual injected signal in each case : any discrepancies must have been introduced by the arc process . to quantify these discrepancies , we then evaluated , for each star , the variance of the residuals ( the difference between the estimated and injected signals ) , divided by the variance of the injected signal .",
    "this is a quantitative measure of any discrepancy introduced by the arc .",
    "we found that this discrepancy measure has a mean value of 1% and is never above 5% , confirming that the arc procedure is unlikely to remove real astrophysical variability .",
    "we also note that the number of cases , where the pdc - map and the arc give very different scatters , increases at longer timescales .",
    "there are a few cases where the arc scatter is significantly larger than the pdc - map value , and these are more numerous in the module 2 example than in the module 13 one .",
    "@xcite carried out an in - depth investigation of the noise properties of the pdc data .",
    "in particular , they showed that the majority of sun - like stars observed by kepler appear to display more variability on 6.5-hour timescales than the sun .",
    "this is an important result , because it has a potentially serious impact on kepler s ability to detect transits of habitable planets . to estimate the variability on transit timescale",
    ", @xcite used a quantity known as the 6.5-hour combined differential photometric precision ( cdpp ) .",
    "cdpp estimates computed by the kepler pipeline itself are now publicly available and distributed with the kepler data .",
    "our 6-hour @xmath63 estimates are not exactly identical to the cdpp values for the same dataset , because they are obtained in a slightly different fashion , but they do measure a similar quantity , namely the light curve scatter on typical transit timescales .",
    "although improving the detectability of transits was not our primary motive in developing the arc , the latter yields slightly lower scatter on transit timescales than the pdc , and hence the arc may prove useful for transit searches .",
    "we also computed ` average ' power spectra before and after correction , for the bright stars ( median raw flux @xmath65e@xmath66/s ) in the same two output channels ( giving 56 and 69 stars for the two output channels respectively ) .",
    "these are simply averages of the individual power spectra of each light curve .",
    "the results are shown in figure  [ fig : psd_comp ] .",
    "again , both systematics correction methods significantly reduce the light curve scatter on most timescales , and their behaviour is very similar at low frequencies ( except for the very lowest , but these are poorly constrained due to the limited duration of the dataset ) .",
    "however , the power spectra conclusively demonstrate the fact that the pdc - map introduces high - frequency noise into the light curves ( see also * ? ? ? * though this reports results using pdc rather than pdc - map ) .",
    "this effect is more significant in some modules than in others , but the differences with the arc become noticeable upwards of about 2 cycles per day .",
    "we note that , for mod2.out1 , the set of systematics inferred by the arc algorithm contains several high - frequency components .",
    "similar trends may well be present in mod13.out1 , but they are not significant in a large enough fraction of the light curves from that channel to be identified .",
    "this may explain the presence of excess power at high - frequencies in the average power spectrum of the arc - corrected light curves from that channel , compared to mod2.out1 .",
    "we ran a test where we arbitrarily fixed the number of trends identified to 5 ; this leads to the identification of the ` reaction - wheel heater ' trend in most channels , but with very low spectral radius , indicating that it is not significant in the majority of light curves .",
    "this highlights the capricious trade - off between over- and under - fitting : in this work we have attempted to find a balance using the spectral radius threshold , but other choices are possible .",
    "the power spectra shown in the bottom panel of figure  [ fig : psd_comp ] correspond to the central module of the kepler detector , where the only trends identified by the arc were long - term .",
    "there are clearly systematic effects at well - defined frequencies , which are not well captured by either the pdc - map or the arc .",
    "the signal at @xmath67 cycles / day and harmonics thereof correspond to the aforementioned eb guide star ; there are also a number of higher - frequency effects .",
    "the fact that neither the pdc - map nor the arc correct these effects well suggests that they may not obey one of the fundamental assumptions underlying both methods .",
    "for example , if the contribution of a given trend to a given light curve varies during the quarter , the linear basis model will not be able to capture it .",
    "there are also some interesting differences in the average power at a given frequency , and in the relative reduction in this power introduced by the systematics corrections , for the two output channels shown . however , it would probably be unwise to read too much into this , as these power spectra are based on relatively small numbers of light curves ( @xmath68 in each case ) .",
    "another point which we note as interesting , but refrain from discussing further , is that all the power spectra  before and after systematics correction of any kind ",
    "are remarkably well - described by power laws with index @xmath69 , as expected for an auto - regressive stochastic process , and frequently used to model the so - called ` solar background ' ( @xcite .",
    "see @xcite for a more detailed discussion of this aspect ) .",
    "there are a few larger differences between the arc and pdc - map corrections . in figure",
    "[ fig : arc_pdf_diff_eg ] we highlight representative , but rare , examples of large deviations .",
    "the top two examples show cases where the post - arc scatter is larger than the post - pdc scatter ( by a factor of @xmath70 on 30 min timescales , @xmath71 on 6 hr timescales and @xmath72 on 6 day timescales ) . in both cases",
    "the light curve contains a strong signature of an effect that the arc can not deal with , for example the binary guide star ( top row ) or strong discontinuities ( pixel sensitivity drop - outs , 2nd row ) .",
    "the bottom two examples show cases where the post - arc scatter is considerably smaller than the post - pdc scatter ( by a factor of @xmath71 on 30 min timescales , @xmath72 on 6 hr timescales and @xmath73 on 6 day timescales ) . in both cases",
    "this is because the range of basis vectors available in the pdc - map detrending algorithm does not explain the light curve as effectively as the arc .",
    "it is important to highlight two points regarding these deviations .",
    "firstly , the cases where post - arc scatter is significantly larger than the post - pdc - map scatter are much rarer than _",
    "vice versa_. secondly , the effects the arc method can not currently deal with are either not present in later quarters ( e.g. the eclipsing binary guide star ) or will be dealt with separately by utilising jump - correction software , developed primarily for quarter 2 analysis , which will be described in a later paper .",
    "we have presented a novel method to correct systematic trends present in large ensembles of light curves , while preserving ` true ' astrophyiscal variability , which we call the arc method . the arc is specifically designed for continuous , high - precision observations , where most stars display significant variability .",
    "it follows established practice in using linear basis models , but incorporates a number of new features , which are intended to ensure its robustness ( bayesian framework , use of shrinkage priors ) , and its computational efficiency ( approximate inference using variational bayes ) .",
    "a de - noising step is used to prevent the introduction of spurious random noise into the light curves .",
    "we demonstrated the performance of the method on a simple synthetic dataset , and on the data from the first month of kepler operations .",
    "the results were satisfactory in both cases . in the synthetic example , the systematic trends were correctly identified .",
    "the original light curves included the building blocks of realistic stellar signals , namely harmonic functions with random , variable phases and amplitudes , and were recovered robustly , with no evidence of systematic changes in amplitude or frequency . for the kepler data , the arc results are similar to those of the pdc - map component of the kepler pipeline for most targets , but with some important differences .",
    "in particular , we find that the pdc - map still tends to introduce high - frequency noise into the light curves ( just as noted by @xcite for the original pdc ) .",
    "importantly , this effect remains noticeable on the timescales typical of planetary transits . to address this problem ,",
    "the kepler team recently developed a new incarnation of the pdc , known as multi - scale map , or msmap for short @xcite , which uses a band - splitting approach to separate systematics on different timescales .",
    "this does reduce the injection of high - frequency noise into the light curves compared to the pdc - map .",
    "however , in msmap the treatment of long - term systematics ( on timescales @xmath74 days ) has reverted to the maximum likelihood approach of the original pdc pipeline .",
    "consequently there is no protection against overfitting , and msmap suppresses all variations on timescales longer than @xmath75 days  whether astrophysical or instrumental ( see fig .  1 in * ? ? ?",
    "therefore , although we have not had the opportunity to make a detailed comparison between the new msmap data and the arc , we expect the letter to be preferrable for variability studies .",
    "we did , however , identify certain types of systematics which are not well - corrected by either the pdc - map or arc .",
    "further work needs to be done on these : one area in particular which could be improved is the de - noising step , as the emd method used for that step in the present paper is not suitable for sharp systematic effects .",
    "another possibility , which we intend to investigate , is to treat the systematics as multiplicative , rather than additive , by working in log flux units .",
    "this which would account for gradual amplitude changes , which could occur due to the combined effect of pixel drift and aperture contamination ( if one of the stars in the aperture is more variable than the other ) .",
    "the impact of the residual systematic effects should be relatively limited , because they are confined to well - defined frequency ranges .",
    "overall , we therefore conclude that the arc - corrected light curves could become a very useful resource for both stellar astrophysics and exoplanet searches , and we have made the q1 data publicly available ( the code is also available on request ) .",
    "however , the full potential of the kepler data lies in its very long baseline .",
    "this paper focused on the first month of data only , whereas more than three years are now in the public domain .",
    "a number of additional difficulties arise when trying to process the additional quarters , and to stitch together multiple quarters .",
    "many of the quarters contain discontinuities , most of which are associated with monthly interruptions in the observations to transfer data back to earth .",
    "many of the discontinuities are followed by quasi - systematic trends which are present in most light curves , but with slightly different shapes .",
    "as the satellite rotates by @xmath76 about its viewing axis between each quarter , each star falls on a different detector , and this causes changes in both the median flux level and the nature and amplitude of the systematic trends .",
    "the arc is not designed to address these problems .",
    "we are in the process of developing a complementary set of ` fault - correction ' tools , to be applied on a light - curve by - light - curve basis before running the arc on each channel .",
    "the same methodology can then be used to stitch together data from different quarters .",
    "these developments will be covered in a forthcoming paper .    as a final cautionary note",
    ", we wish to stress that the arc is intended to provide a fast , automated and robust treatment of a large ensemble of light curves .",
    "it is not intended to replace the detailed examination of the pixel - level data @xcite , which remains the method of choice for the correction of systematic trends and artefacts in small numbers of light curves .",
    "we wish to thank the kepler science operations centre and pipeline teams , and in particular jon jenkins and jeoffrey van cleve , for taking the time to discuss our methodology with us and providing some constructive feedback .",
    "am and sa are supported by grants from the uk science and technology facilities council ( refs st / f006888/1 and st / g002266/2 ) .",
    "this paper includes data collected by the kepler mission .",
    "funding for the kepler mission is provided by the nasa science mission directorate .",
    "all of the data presented in this paper were obtained from the mikulski archive for space telescopes ( mast ) .",
    "stsci is operated by the association of universities for research in astronomy , inc .",
    ", under nasa contract nas5 - 26555 .",
    "support for mast for non - hst data is provided by the nasa office of space science via grant nnx09af08 g and by other grants and contracts .",
    "sro and sre would like to thank the uk epsrc for support under the orchid grant , ep / io11587/1 .",
    "99    baglin a. , auvergne m. , barge p. , deleuil m. , michel e. , corot exoplanet science team , 2009 , iaus , 253 , 71    bakos g.   . ,",
    "et al . , 2003 , pasp , 114 , 974    bakos g.   . , et al . , 2007 ,",
    "apj , 670 , 826    bishop c.  m. , 2006 , _ pattern recognition and machine learning _ , springer    borucki w.  j. , et al . , 2010 , sci , 327 , 977    everson r , roberts s , 2000 , ieee trans .",
    "48(7 ) , 2083 .",
    "fox c , roberts s , 2011 , artificial intelligence review , 38(2 ) , 85    gilliland r.  l. , et al .",
    ", 2011 , apjs , 197 , 6    harrison , t.  e. , coughlin , j.  l. , ule , n.  m. , l ' op ` ez - morales , m. , 2012 , aj , 143 , 4    harvey , j.  w. 1985 , in _ future missions in solar , heliospheric and space plasma physics _ , esa sp-235    hoaglin d.  c. , mostellar f. , tukey j.  w. , 1983 , _ understanding robust and exploratory data analysis _ , john wiley , new york    huang n.  e. , et al . , 1998 , rspsa , 454 , 903    jenkins j.  m. , et al . , 2010 ,",
    "apj , 713 , l87    jenkins , j.  m. , et al . , 2013 , kepler data characteristics handbook , ksci-19040 - 004    kinemuchi k. , barclay t. , fanelli m. , pepper j. , still m. , howell s.  b. , 2012 , pasp , 124 , 963    kovcs g. , bakos g. , noyes r.  w. , 2005 , mnras , 356 , 557    mcquillan a. , aigrain s. , roberts s. , 2012 , a&a , 539 , a137    murphy s.  j. , 2012 , mnras , 422 , 665    neal r. , 1998 , in _ neural networks and machine learning _ , springer - verlag , 97    penny w. , roberts s. , 2002 , in _ iee proceedings on vision , image and signal processing _ , 149(1 ) , 33    petigura e. and marcy g. , 2012 , pasp 124 , 1073 .    pollacco d.  l. , et al . , 2006 , pasp , 118 , 1407    shannon c.  e. , 1951 , the bell system technical journal , 30 , 50    smith j.  c. , et al . , 2012 ,",
    "pasp , 124 , 1000    stumpe m.  c. , et al . , 2012 , pasp , 124 , 985    tamuz o. , mazeh t. , zucker s. , 2005 , mnras , 356 , 1466    thompson s.  e. , j.  l. christiansen , j. jenkins , m.  r. haas , 2013",
    ", kepler data release notes 21 , ksci-19061 - 00l    udalski a. , et al . , 2002 , aca , 52 , 1    van cleve j.  e. & caldwell d.  a , 2009 , _ kepler instrument handbook _ ( ksci-19033 ) .    wall j.  v. , jenkins c.  r. , ellis r. , huchra j. , kahn s. , rieke g. , stetson p.  b. , 2003 , _ practical statistics for astronomers _ , cambridge university press",
    "in a general linear basis model , the observations of a dependent variable are modelled as a linear combination of basis functions of an independent variable , plus a noise term . for consistency with the notation used in the rest of the paper , we call the independent variable @xmath77 and the dependent variable @xmath78 , so that : @xmath79 where the @xmath80 s are known as the _ factor weights _ and the @xmath81 s are the _ basis functions_. the noise term , @xmath82 , is taken to be drawn from a normal ( gaussian ) distribution with zero mean and precision ( inverse variance ) @xmath83 .",
    "note that an offset , or _",
    "term , can easily be included in the model by including a basis function which is one everywhere .    defining the column vectors @xmath84 ,  , @xmath85{^{\\sf t}}$ ] and @xmath86 ,  , @xmath87{^{\\sf t}}$ ] , we can re - write equation  ( [ eq : lbm ] ) as : @xmath88 we then obtain a system of simultaneous linear equations describing the dataset as a whole : @xmath89 where @xmath90 is an @xmath91-element matrix with elements @xmath92 and @xmath93 ,  , @xmath94{^{\\sf t}}$ ] .    the _ likelihood _ , i.e.  the probability that our model gave rise to the observations , is then simply @xmath95 where @xmath96 is the probability of drawing a vector @xmath97 from a multi - variate gaussian distribution with mean vector @xmath98 and covariance matrix @xmath99 , and @xmath100 is the identity matrix .      the maximum likelihood ( ml ) solution for the weights is given by the standard pseudo - inverse equation , namely @xmath101 however , the ml approach is prone to over - fitting .",
    "some relief from the inherent problems associated with ml solutions may be obtained by introducing a prior over the weights and obtaining the _ maximum a posteriori _ ( map ) solution .",
    "this approach still relies on _ point estimates _ of the parameters of the models , and consequently fails to take into account some of the intrinsic uncertainties .",
    "a full bayesian solution is obtained by marginalising over the posterior distributions of the variables , which can be achieved , for example , using sample - based approaches such as markov - chain monte - carlo ( mcmc ) .",
    "these can be computationally intensive however , and they scale poorly to large numbers of parameters , and it is often problematic to establish whether convergence has been reached . in this paper , we advocate an alternative solution based on the _ variational bayes _ ( vb ) framework . in recent years",
    "vb has been extensively used in the machine learning literature as a method of choice for approximate bayesian inference , as it offers computational tractability even on very large data sets . a full tutorial on vb is given in @xcite .",
    "in what follows we describe the key features and concentrate on the derivations of update equations for the linear basis model at the core of this paper .      in a bayesian framework ,",
    "the priors over the parameters of the model must be specified . for each of the weights , a zero - mean gaussian prior with precision @xmath102 is adopted .",
    "the zero mean ensures that the weight associated with a given basis function is only non - zero if the data requires it to be .",
    "the gaussian is the least informative choice of distribution for a quantity that can be either positive or negative , and is therefore appropriate when little information is known about the weights a priori .",
    "it is also convenient to chose a form for the prior that is conjugate with the likelihood , because it makes the marginalization process analytic .",
    "the joint prior over all the weights is thus : @xmath103 the value of @xmath102 does not enter into the likelihood , but it does control the parameters , and it is therefore known as a hyper - parameter . in a fully bayesian treatment , rather than fixing @xmath102 to a specific value , we marginalise over it , under a so - called hyper - prior .",
    "it is convenient to select a hyper - prior for @xmath102 that is conjugate with the prior over the weights , because then the marginalisation can be done analytically .",
    "as the prior is a gaussian , and @xmath102 should always be positive ( because it represents an inverse variance ) , a suitable form for the hyper - prior is a gamma distribution with shape parameter @xmath104 and scale parameter @xmath105 : @xmath106 where @xmath107 is the gamma function .",
    "the noise precision , @xmath83 , is also a parameter of the model , for which we need to adopt a prior .",
    "again , a gamma distribution is a suitable choice , with hyperparameters @xmath108 : @xmath109    we concatenate all the parameters and hyperparameters of the model into the vector @xmath110{^{\\sf t}}$ ] . as the weights depend upon the scale @xmath102 but not the noise precision @xmath83 the joint prior distribution over @xmath111 factorises as : @xmath112    our objective in bayesian regression is to estimate the posterior distribution , which fully describes our knowledge regarding the parameters of the model , @xmath111 , given the data @xmath113 .",
    "this is related to the likelihood and prior by bayes s theorem : @xmath114 the denominator is the _ evidence _ or _",
    "marginal likelihood _ of the data under the model , and is given by @xmath115 more than a mere normalisation term , the evidence is a quantitative measure of the extent to which the data supports the model .",
    "whether we are interested in the model evidence or in the posterior distribution over individual parameters ( after marginalising over the others ) , the functional dependence of the likelihood and posterior distribution on the parameters are generally unknown .",
    "however , we can propose an analytical approximation for any one of them , which can if necessary be refined later .",
    "this gives rise to the class of _ approximate inference _ methods .    in _ variational",
    "_ inference , we introduce an analytically tractable distribution @xmath116 , which we use to approximate the posterior distribution @xmath117 .",
    "we use this to write the log evidence as the sum of two separate terms : @xmath118 \\ , \\mathrm{d}{\\mbox{\\boldmath $ \\theta$ } } \\\\    & = & \\int q({\\mbox{\\boldmath $ \\theta$}}| { \\mathbf{d } } ) \\ , \\log \\left [      \\frac{p ( { \\mathbf{d } } , { \\mbox{\\boldmath $ \\theta$}})}{p({\\mbox{\\boldmath $ \\theta$}}| { \\mathbf{d } } ) }      \\cdot \\frac{q({\\mbox{\\boldmath $ \\theta$}}| { \\mathbf{d } } ) } { q({\\mbox{\\boldmath $ \\theta$}}| { \\mathbf{d } } ) }    \\right ] \\ , \\mathrm{d}{\\mbox{\\boldmath $ \\theta$ } } \\\\    & = & \\int q({\\mbox{\\boldmath $ \\theta$}}| { \\mathbf{d } } ) \\log \\frac{p ( { \\mathbf{d } } , { \\mbox{\\boldmath $ \\theta$}})}{q({\\mbox{\\boldmath $ \\theta$}}| { \\mathbf{d } } ) } ~\\mathrm{d}{\\mbox{\\boldmath $ \\theta$ } } \\nonumber \\\\     & & \\hspace{1 cm } - \\int q({\\mbox{\\boldmath $ \\theta$}}| { \\mathbf{d } } ) \\log \\frac{p({\\mbox{\\boldmath $ \\theta$}}| { \\mathbf{d } } ) } { q({\\mbox{\\boldmath $ \\theta$}}| { \\mathbf{d } } ) } ~\\mathrm{d}{\\mbox{\\boldmath $ \\theta$ } } \\\\ \\label{eq : vb }   & \\equiv & \\mathcal{f}(p , q ) + { \\rm kl}(p , q ) . \\label{fundamental}\\end{aligned}\\ ] ] equation [ fundamental ] is the fundamental equation of the vb - framework . the first term on the right - hand side , @xmath119 , is known as the ( negative ) variational free energy .",
    "the second , @xmath120 is the kullback - leibler ( kl ) divergence between the approximate posterior @xmath121 and the true posterior @xmath122 .",
    "importantly , the kl divergence is always positive . @xmath119",
    "thus provides a strict _ lower bound _ on the log evidence .",
    "moreover , because the kl divergence is zero when the two densities are the same , @xmath119 will become equal to the log evidence when the approximating posterior is equal to the true posterior , i.e.  when @xmath123 .",
    "the aim of vb learning is therefore to maximise @xmath119 and so make the approximate posterior as close as possible to the true posterior .",
    "this requires the extremisation of an integral with respect to a functional , which is typically achieved using the _ calculus of variations_.    however , to obtain a _ practical _ inference algorithm , we restrict the range of proposal posterior distributions over which we perform the optimization .",
    "first , we restrict ourselves to distributions belonging to the exponential family .",
    "this ensures that extremisation over the _ function _ @xmath124 can be replaced exactly by extremisation with respect to the _ parameters _ @xmath111 . as this family includes all the commonly - used probability distributions , this constraint is not normally problematic .",
    "what we obtain is then a set of coupled update equations over the parameters which are cycled until a convergence criterion is met .",
    "this approach is a generalisation of the _ expectation - maximisation _ ( em ) algorithm , which is obtained as a special case of variational bayes in the limit of the @xmath125 distributions being replaced by delta functions at their maximum - likelihood values @xcite .",
    "furthermore , we also assume that the posterior distribution is _ separable _ , meaning that it can be written as a product of independent functions of different parameters ( or subsets of the parameters ) .",
    "this makes the resultant inference algorithm computationally very rapid , with little loss of information .",
    "we begin by writing this factorisation as : @xmath126 this makes the resultant inference algorithm computationally very rapid , with little loss of information .",
    "the aim is to find the distribution @xmath116 for which @xmath119 is largest , which is done by optimising each of the factors in turn .",
    "we start by substituting this separable expression into @xmath119 : @xmath127 \\mathrm{d}{\\mbox{\\boldmath $ \\theta$}}.\\ ] ] next , we write down separately the terms which depend on one of the parameters , @xmath128 : @xmath129 . \\nonumber\\end{aligned}\\ ] ] we then rearrange the above equation , so as to isolate the dependency on @xmath128 : @xmath130 the second factor in each of the last two terms in equation  ( [ eq:3parts ] ) is the integral of a probability distribution , and is thus equal to one . furthermore , the third term is independent of @xmath128 , so : @xmath131 where @xmath132 represents a term that is constant with respect to @xmath128 . the quantity in curly braces in the first term of equation  ( [ eq:2parts ] ) is the expectation of @xmath133 under the candidate posterior distribution for all the other parameters , @xmath134 .",
    "we wish to isolate the part of it which depends on @xmath128 . to this end",
    ", we define the quantity @xmath135_j \\ ,",
    "\\prod_{i \\neq j }      q(\\theta_i | { \\mathbf{d } } ) \\ , \\mathrm{d}{\\mbox{\\boldmath $ \\theta_{(i \\neq j)}$}},\\ ] ] where @xmath136_j$ ] contains all the terms in @xmath137 , which depend on @xmath128 .",
    "we can then write @xmath138 and thus equation  ( [ eq:2parts ] ) becomes : @xmath139 \\",
    ", \\mathrm{d } \\theta_j \\nonumber\\\\   & - &   \\int q(\\theta_j | { \\mathbf{d } } ) \\ , \\log q(\\theta_j | { \\mathbf{d } } ) \\ ,",
    "\\mathrm{d }    \\theta_j \\ , + { \\mbox{const}}(j ) .",
    "\\label{eq:1part}\\end{aligned}\\ ] ] the non - constant terms in equation [ eq:1part ] can readily be identified as the negative kl divergence between @xmath140 and @xmath141 $ ] .",
    "this divergence is minimised ( and is zero ) when the two distributions are the same . therefore ,",
    "the free energy @xmath119 is maximised with respect to @xmath128 simply by setting @xmath142 the additive constant in equation [ eq : up1 ] is set by normalising the distribution @xmath143 . thus ,",
    "if we take the exponential of both sides and normalize , we obtain the updated distribution for parameter @xmath128 as : @xmath144}{\\int \\exp \\left [ f^{\\rm{old}}(\\theta_j ) \\right ] \\mathrm{d}\\theta_j}.\\ ] ] as previously mentioned , we have chosen to use candidate distributions which belong to the exponential family , so that extremisation over the function @xmath116 can be replaced exactly by extremisation with respect to the parameters @xmath111 , and thus we can re - write equation [ eq : up2 ] @xmath145}{\\int \\exp \\left [ f(\\theta_j^{\\rm{old } } ) \\right ] \\mathrm{d}\\theta_j}.\\ ] ] we note that , as with the em algorithm , each iteration is _ guaranteed _ to improve the marginal likelihood of the data under the model .    in the following subsections ,",
    "we describe how the parameters of the linear basis model ( the weights @xmath146 , the weight precision @xmath102 , and the noise precision @xmath83 ) are inferred using variational bayes and detail the update equations associated with each of them .",
    "the marginal for the weights is @xmath147 \\ ,",
    "\\mathrm{d}\\alpha \\ , \\mathrm{d}\\beta   \\label{eq : f_w}\\ ] ] our update equations are hence of the form @xmath148\\ ] ]",
    "substituting for the terms in equation [ eq : f_w ] gives @xmath149 where @xmath150 and @xmath151 are the expectations of the weight and noise precisions under the proposal distributions over @xmath102 and @xmath83 ( see the next two sections for details of the form of these distributions ) .",
    "the weight posterior is therefore a normal distribution : @xmath152 , with @xmath153 thus , the posterior precision matrix , @xmath154 , takes the usual bayesian form of being the sum of the data precision , and the prior precision , @xmath155 . if @xmath156 , i.e. in the absence of prior on the weights , we recover the ml solution of equation  ( [ eq : ml ] )",
    ".      we let @xmath157 \\",
    ", \\mathrm{d } { \\mathbf{w } } \\ , \\mathrm{d}\\beta \\nonumber \\\\ & = & \\int q ( { \\mathbf{w } } | { \\mathbf{d } } ) \\log \\left[p ( { \\mathbf{w } } |\\alpha)\\ , p(\\alpha ) \\right ] \\mathrm{d } { \\mathbf{w } } \\label{eq : wprec}\\end{aligned}\\ ] ] as before , the negative free energy is maximised when @xmath158\\ ] ] by substituting for the terms in equation [ eq : wprec ] we find that the updated weight precision posterior density is a gamma distribution @xmath159 where the updated hyper - hyperparameters , @xmath160 and @xmath161 , are given by @xmath162 where @xmath163 and @xmath164 are the initial estimates for @xmath104 and @xmath105 , and @xmath17 is the number of basis functions . the updated value for @xmath102 , to be substituted into equations  ( [ eq : wupdates ] ) ,",
    "is then simply the mean of this distribution : @xmath165      again we start by writing the marginal @xmath166 \\ , \\mathrm{d } { \\mathbf{w } } \\ , \\mathrm{d}\\alpha \\nonumber \\\\    & = & \\int q ( { \\mathbf{w } } | { \\mathbf{d } } ) \\log [ p ( { \\mathbf{d } } | { \\mathbf{w } } , \\beta ) \\ , p(\\beta ) ] \\,\\mathrm{d } { \\mathbf{w } } .",
    "\\label{eq : noiseprec}\\end{aligned}\\ ] ] the negative free energy is then maximised when @xmath167\\ ] ] by substituting for the terms in equation [ eq : noiseprec ] we find , as with @xmath102 , that the posterior distribution over @xmath83 is of gamma form , @xmath168 .",
    "the update equations for @xmath169 , @xmath170 and @xmath151 are : @xmath171 where @xmath1 is the number of data points .      instead of using the isotropic gaussian of equation [ global ] , where the distribution over all the weights has a common scale ( defined by the single hyperparameter @xmath102 ) , we can split the weights into groups and allow different groups to have different scales in their distributions ; each weight @xmath172",
    "can indeed have its own scale hyperparameter .",
    "this approach is often referred to as _ automatic relevance determination _ ( ard ) @xcite , because by inspecting the inferred scales associated with the weights we can see which ( groups of ) weights are relevant to the problem at hand .",
    "the posteriors for the weights of any basis functions which are not helpful in explaining the data will evolve towards zero - mean distributions with vanishingly small variance .",
    "conversely , the weights of basis functions which are well supported by the data will entertain larger variances in their posterior distributions .",
    "this means we may operate with a rich basis set and allow the bayesian model to select only those basis functions that have explanatory power in the data .",
    "we can allow different weights to have different scales , but still take into account domain knowledge which  for example  may lead us to believe that certain parameters should have a similar posterior scale , by adopting _ structured priors _",
    "@xcite , of the form @xmath173 where the weights have been split into @xmath174 groups , with @xmath175 weights in the @xmath176 group , @xmath177 and @xmath178 is a diagonal matrix with ones in the row corresponding to the @xmath176 group , and zeros elsewhere .",
    "use of structured priors results in vb updates for the posterior weight covariance and weight precision as follows @xmath179 the other updates are exactly the same as for the global variance scale over the parameters .",
    "all the code for the arc was written in matlab .",
    "the basis discovery phase for all mod.out took just under an hour running on a quad - core 2.6ghz processor under a unix operating system .",
    "the subsequent trend removal over all mod.out took some five minutes and scales linearly with the number of light curves being detrended . without loss of generality",
    "we normalise each light curve to have zero mean and unit variance prior to analysis .",
    "we note that this transformation is reversible subsequent to processing . the variational bayes basis model , detailed in this appendix , is used in both the trend discovery and trend removal phases of the algorithm , differing only in the details of the structured priors , as described below .      in this phase of the algorithm",
    "we implement equation [ eq : linmod ] in which successive light curves are modelled as linear combinations of one another . in this phase",
    "we use a single global weight precision hyperparameter , @xmath102 , over which we place a gamma distribution with hyper - hyperparameters @xmath180 . the noise precision , @xmath83 is also gamma distributed with initial hyper - hyperparameters @xmath181 .      in this phase",
    "we wish to allow individual precisions associated with the weights of the model , as each weight is linked to a putative ( pre - discovered ) trend component .",
    "this allows shrinkage on trends that are not present in a light curve and avoids falsely removing any components for which there is not compelling evidence .",
    "we hence follow the procedure for structured priors as detailed in the previous section of the appendix . once more we set vague priors for each weight precision , @xmath182 , and for the noise precision @xmath83 , choosing @xmath183 ."
  ],
  "abstract_text": [
    "<S> space - based transit search missions such as kepler are collecting large numbers of stellar light curves of unprecedented photometric precision and time coverage . </S>",
    "<S> however , before this scientific goldmine can be exploited fully , the data must be cleaned of instrumental artefacts . </S>",
    "<S> we present a new method to correct common - mode systematics in large ensembles of very high precision light curves . </S>",
    "<S> it is based on a bayesian linear basis model and uses shrinkage priors for robustness , variational inference for speed , and a de - noising step based on empirical mode decomposition to prevent the introduction of spurious noise into the corrected light curves . after demonstrating the performance of our method on a synthetic dataset </S>",
    "<S> , we apply it to the first month of kepler data . </S>",
    "<S> we compare the results , which are publicly available , to the output of the kepler pipeline s pre - search data conditioning , and show that the two generally give similar results , but the light curves corrected using our approach have lower scatter , on average , on both long and short timescales . </S>",
    "<S> we finish by discussing some limitations of our method and outlining some avenues for further development . </S>",
    "<S> the trend - corrected data produced by our approach are publicly available .    </S>",
    "<S> [ firstpage ]    kepler space telescope , systematics correction , variational bayesian inference . </S>"
  ]
}