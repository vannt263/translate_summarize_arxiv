{
  "article_text": [
    "over the past decades , neural networks have been successfully applied for data modelling due to its universal approximation power for nonlinear maps @xcite , and learning capability from a collection of training samples @xcite . in practice , however , it is quite challenging to properly determine an appropriate architecture ( here refers to the number of hidden nodes ) of a neural network so that the resulting learner model can achieve sound performance for both learning and generalization . to resolve this problem ,",
    "one turns to develop constructive approaches for building neural networks , starting with a small sized network , followed by incrementally generating hidden nodes ( corresponding to a set of input weights and biases ) and output weights until a pre - defined termination criterion meets .",
    "being a fundamental of manifold applications , it is essential to ensure that the constructive neural network shares the universal approximation property . in @xcite ,",
    "barron proposed a greedy learning framework based on the work reported in @xcite , and established some significant results on the convergence rate . in @xcite , kwok and yeung presented a method to construct neural networks through optimizing some objective functions .",
    "theoretically , their proposed algorithm can generate universal approximators for any continuous nonlinear functions provided that the activation function meets some conditions .",
    "it has been aware that the process of iterative searching for an appropriate set of parameters ( input weights , biases and output weights ) is time - consuming and computationally intensive although the universal approximation property holds , but very difficult to be employed for dealing with large - scale data analytics .",
    "randomized approaches for large - scale computing are highly desirable due to their effectiveness and efficiency @xcite . in",
    "machine learning for data modelling , randomized algorithms have demonstrated great potential in developing fast learner models and learning algorithms with much less computational cost @xcite .",
    "readers are strongly recommended to refer to our survey paper @xcite for more details . from algorithm perspectives ,",
    "randomized learning techniques for neural networks received attention in later 80 s @xcite and further developed in early 90 s @xcite .",
    "a common and basic idea behind these randomized learning algorithms is a two - step training paradigm , that is , randomly assigning the input weights and biases of neural networks and evaluating the output weights by solving a linear equation system using the well - known least squares method and its regularized versions . from approximation theory viewpoint ,",
    "randomized radial basis function ( rbf ) networks proposed in @xcite and random vector functional - link ( rvfl ) networks proposed in @xcite can be regarded as random basis approximators ( rbas ) @xcite .",
    "thus , it is essential and interesting to look into the universal approximation capability of rbas in the sense of probability . in @xcite , igelnik and",
    "pao proved that a rvfl network with random parameters ( input weights and biases ) chosen from the uniform distribution defined over a range can be a universal approximator with probability one for continuous functions . in @xcite",
    ", husmeier revisited this significant result and showed that the universal approximation property of rvfl networks holds as well for symmetric interval setting of the random parameter scope if the function to be approximated meets lipschitz condition . in @xcite , ivan et al .",
    "empirically investigated the feasibility of rbas for data modelling and showed that a supervisory mechanism is necessary to make rvfl networks applicable .",
    "their experiments clearly indicate that rvfl networks fail to approximate a target function with a very high probability if the setting for random parameters is improper .",
    "this phenomenon was further studied with mathematical justifications in @xcite . from implementation considerations ,",
    "we need to know two key parameters involved in design of rvfl networks : the number of hidden nodes and the scope of random parameters . indeed , as one of the special features of this type of learner models , the first parameter associated with the modelling accuracy must be set largely .",
    "the second parameter is related to the approximation capability of the class of random basis functions .",
    "obviously , these settings play very important roles to successfully build randomized learner models for real world applications .",
    "intuitively , constructive or incremental rvfl ( irvfl ) networks may be a possible solution for problem solving .",
    "however , our recent work reported in @xcite reveals the infeasibility of irvfl networks , if they are incrementally built with random input weights and biases assigned in a fixed scope and its convergence rate satisfies some certain conditions .",
    "thus , further researches on supervisory mechanisms with adaptive scope setting in random assignment of the input weights and biases to ensure the universal approximation capability of irvfl networks are being expected .    to the development of randomized learning techniques for neural networks , our prime and original contribution from this paper is the way of assigning the random parameters with an inequality constraint and adaptively selecting the scope of the random parameters , ensuring the universal approximation property of the built randomized learner models .",
    "indeed , this work firstly touches the base of implementation of the random basis function approximation theory .",
    "it should be clarified that one can not view scns as a specific implementation of rvfl networks due to some remarkable distinctions in randomization of the learner model .",
    "three algorithmic implementations of scns , namely algorithm sc - i , sc - ii and sc - iii , are presented with the same supervisory mechanism for configuring the random parameters , but different methods for computing the output weights .",
    "concretely , sc - i employs a constructive scheme to evaluate the output weights only for the newly added hidden node and keep all of the previously obtained output weights unchanged ; sc - ii recalculates a part of the current output weights by solving a local least squares problem with a user specified shifting window size ; and sc - iii finds the output weights all together through solving a global least squares problem with the current learner model .",
    "our experimental results on a toy example for function approximation and real world regression tasks demonstrate remarkable improvements on modelling performance , compared with the results obtained by existing methods such as modified quickprop ( mq ) @xcite and irvfl @xcite .",
    "the remainder of this paper is organized as follows :",
    "section 2 briefly reviews constructive neural networks with a recall for the infeasibility of irvfl networks .",
    "section 3 details our proposed stochastic configuration networks with both theoretical analysis and algorithmic description .",
    "section 4 reports our simulation results , and section 5 concludes this paper with some remarks on further studies .",
    "the following notation is used throughout this paper .",
    "let @xmath0 be a set of real - valued functions , span@xmath1 denote a function space spanned by @xmath2 ; @xmath3 denote the space of all lebesgue measurable functions @xmath4:\\mathbb{r}^{d}\\rightarrow \\mathbb{r}^{m}$ ] defined on @xmath5 , with the @xmath6 norm defined as @xmath7 the inner product of @xmath8:\\mathbb{r}^{d}\\rightarrow \\mathbb{r}^{m}$ ] and @xmath9 is defined as @xmath10 in the special case that @xmath11 , for a real - valued function @xmath12 defined on @xmath5 , its @xmath6 norm becomes @xmath13 , while the inner product of @xmath14 and @xmath15 becomes @xmath16 .",
    "instead of training a learner model with a fixed architecture , the process of constructive neural networks starts with a small sized network then adds hidden nodes incrementally until an acceptable tolerance is achieved .",
    "this approach does not require any prior knowledge about the complexity of the network for a given task .",
    "this section briefly reviews some closely related work on constructive neural networks . some comments on these methods",
    "are also given .",
    "given a target function @xmath17 , suppose that we have already built a slfnn with @xmath18 hidden nodes , i.e. , @xmath19 ( @xmath20 , @xmath21 ) , and the current residual error , denoted as @xmath22 , does not reach an acceptable tolerance level .",
    "the construction process is concerned with how to incrementally add @xmath23 , @xmath24 ( @xmath25 and @xmath26 ) leading to @xmath27 until the residual error falls into an expected tolerance @xmath28 .      in @xcite ,",
    "barron proposed a greedy approximation framework based on jones lemma @xcite .",
    "the main result can be stated in the following theorem",
    ".    * theorem 1 ( barron @xcite ) . * given a hilbert space @xmath29 , suppose @xmath30 ( closure of the convex hull of the set g ) , and @xmath31 for each @xmath32 .",
    "let @xmath33 . for @xmath34 ,",
    "the sequence of approximations @xmath35 is deterministic and described as @xmath36 where @xmath37 and @xmath24 is chosen such that the following condition holds @xmath38 then , for every @xmath39 , @xmath40 , where @xmath41 .",
    "* remark 1 .",
    "* some optimization techniques are needed in order to find a suitable @xmath24 to meet the condition ( [ greedy_condition ] ) .",
    "in fact , the whole process aims to minimize @xmath42 at each iteration , which is functionally equivalent to choosing @xmath43 that maximizes @xmath44 .",
    "note that this constructive scheme is only applicable to the target function @xmath9 belonging to the closure of the convex hull of @xmath29 , that means the convergence to all @xmath6 functions can not be guaranteed under this framework .",
    "in addition , the new leaner model @xmath45 comes from a specific convex combination of the previous model @xmath46 and the newly added term @xmath24 , which results in a weak solution compared against one obtained through optimizing the left hand side of ( [ greedy_condition ] ) with respect to the coefficient @xmath47 .",
    "in @xcite , kwok and yeung proposed an incremental learning strategy for building slfnns and proved the universal approximation property . at each step , the input weights and biases ( @xmath48 and @xmath49 ) of the newly added node at the hidden layer are obtained by maximizing an objective function and the output weights are evaluated using the least squares method .",
    "this theoretical result can be stated as follows :    * theorem 2 ( kwok and yeung @xcite ) . * let @xmath2 be a set of basis function .",
    "assume that span(@xmath2 ) is dense in @xmath6 space and @xmath50 , @xmath51 for some @xmath52 .",
    "if @xmath24 is selected as to maximize @xmath53 , then @xmath54 .",
    "* remark 2 . * theoretically , the convergence property can be guaranteed provided that we can find @xmath24 to maximize @xmath53 . in practice",
    ", however , local minima occurs frequently when performing the gradient ascent algorithm .",
    "sometimes , during the course of incremental learning , the optimization process in constructing a new hidden node seems meaningless as the updating of hidden parameters is excessively slow , which means the reduction of residual error will be close to zero .",
    "in fact , this point was discussed in @xcite and a modified quickprop algorithm was suggested in @xcite in order to alleviate this problem . however , this weakness can not be overcome completely for some complex tasks . in essence , the modified quickprop algorithm that iteratively finds the appropriate hidden parameters ( @xmath48 and @xmath49 ) may still face some obstacles in the optimization process of the objective function when it is searching in a plateau of the error surface .",
    "once @xmath48 ( @xmath49 ) at some iterative step falls in a region that both the first and second derivatives of the objective function with respect to @xmath48 ( @xmath49 ) are nearly zero , the learning reaches a halt and may be terminated . in a nut shell ,",
    "the derivative - based optimization processes suffer from some inherent shortcomings and have lower probability to generate a universal learner eventually .",
    "the constructive methods mentioned above consist of two phases : selecting the hidden parameters ( the input weights and biases ) according to some certain criteria and evaluating the output weights after adding a new hidden node . although the universal approximation property can be guaranteed by incrementally adding the hidden nodes , the resulting slfnns usually need a quite few hidden nodes to achieve good learning performance . in practice , seeking for a basis function through maximizing @xmath53 is very time consuming .",
    "thus , for many real world applications , deterministic methods used in building constructive neural networks seem have no or less applicability . as one of pathways to generate neural networks with the universal approximation property ( corresponding to the perfect learning power ) ,",
    "randomized approaches have great potential to access a faster and feasible solution @xcite .      random vector functional - link ( rvfl ) networks @xcite can be regarded as a randomized version of slfnns , where the input weights and biases are randomly assigned and fixed during the training phase , and the output weights are analytically evaluated by the least squares method @xcite . in @xcite , igelnik and pao theoretically justified its universal approximation property based on monte - carlo method with the limit - integral representation of the target function .",
    "the main result can be restated as follows :    * theorem 3 ( igelnik and pao @xcite ) .",
    "* for any compact set @xmath55 , given @xmath56 ( i.e. , the set of all continuous functions defined over @xmath57 ) , and any activation function @xmath58 that satisfies @xmath59 or @xmath60 , there exist a sufficiently large @xmath61 , a set of @xmath62 and a probabilistic space @xmath63 , such that @xmath64 can approximate @xmath9 with arbitrary accuracy in the sense of probability one , if @xmath48 and @xmath49 are randomly assigned over @xmath63 and follow certain distribution .",
    "alternatively , this result can be expressed as @xmath65 where @xmath66 is the expectation operator with respect to the probabilistic space @xmath63 .    * remark 3 . * in @xcite",
    ", husmeier revisited the universal approximation property of rvfl networks with a symmetric interval setting for the random parameters .",
    "strictly speaking , such a property holds for only target functions satisfying lipschitz condition . from our experience",
    ", however , most of data modelling problems from real world applications meet lipschitz condition .",
    "thus , for simplicity , we adopt the symmetric interval setting for the random parameters in this paper .",
    "it should be aware that such a special setting does not limit the development of our framework at all , and it is just a matter of implementation indeed .    the theoretical result stated in theorem 3 is fundamental and significant for building randomized neural networks .",
    "similar to the case of making use of the slfnns in resolving real world problems , we need to develop effective algorithms to implement such a class of randomized predictive models .",
    "it is natural to think of an incremental implementation of rvfl ( irvfl ) networks , where the model is built incrementally with random assignment of the input weights and biases , and constructive evaluation of its output weights .",
    "although the construction process of irvfl networks seems to be computationally efficient , unfortunately the universal approximation property of the constructed learner can not be guaranteed . the following theorem 4 from our recent work @xcite have justified this point .",
    "* theorem 4 ( li and wang @xcite ) .",
    "* let span(@xmath2 ) be dense in @xmath6 space and @xmath50 , @xmath51 for some @xmath52 .",
    "suppose that @xmath24 is randomly generated and @xmath23 is given by @xmath67 for sufficiently large @xmath61 , if the followings hold : @xmath68 and @xmath69 then , the constructive neural network with random weights , @xmath45 , has no universal approximation capability , that is , @xmath70 theorem 4 reveals that irvfl networks may not share the universal approximation property , if the output weights are taken as ( [ th4_con1 ] ) and the residual error sequence meets conditions ( [ th4_con2 ] ) and ( [ th4_con3 ] ) .",
    "the consequence stated in theorem 4 still holds if the output weights are evaluated using the least squares method .",
    "we state this interesting result in the following theorem 5 .    *",
    "theorem 5 .",
    "* let span(@xmath2 ) be dense in @xmath6 space and @xmath50 , @xmath51 for some @xmath52 .",
    "suppose that @xmath24 is randomly generated and the output weights are calculated by solving the global least square problem , i.e. , @xmath71=\\arg \\min_{\\beta}\\|f-\\sum_{j=1}^{l}\\beta_jg_j\\|$ ] . for sufficiently large @xmath61 , if ( [ th4_con2 ] ) and ( [ th4_con3 ] ) hold for the residual error sequence @xmath72 ( corresponding to @xmath73 in theorem 4 ) , the constructive neural network with random hidden nodes has no universal approximation capability , that is , @xmath74 * proof .",
    "* simple computations can verify that the sequence @xmath75 is monotonically decreasing and converges .",
    "indeed , let @xmath76 , we have @xmath77 then , ( [ th5_con ] ) can be easily obtained by following the proof of theorem 4 .    clearly , the universal approximation property is conditional to irvfl networks whatever the output weights are evaluated .",
    "this happens due to multiple reasons ( e.g. the scope setting and/or the improper way to assign the random parameters ) that is hard to tell mathematically . in this paper",
    ", we propose a solution for constructing randomized learner models under a supervisory mechanism to ensure the universal approximation property .",
    "universal approximation property is fundamental to a learner model for data modelling .",
    "logically , one can not expect to build a neural network with good generalization but poor learning performance .",
    "therefore , it is essential to share the universal approximation property for scns .",
    "this section details our proposed scns , including proofs of the universal approximation property and algorithm descriptions .",
    "some remarks on algorithmic implementations are also given .",
    "given a target function @xmath78 , suppose that a scn with @xmath18 hidden nodes has already been constructed , that is , @xmath19 ( @xmath20 , @xmath21 ) , where @xmath79^\\mathrm{t}$ ] .",
    "denoted the current residual error by @xmath80 $ ] .",
    "if @xmath81 does not reach a pre - defined tolerance level , we need to generate a new random basis function @xmath24 ( @xmath25 and @xmath26 ) and evaluate the output weights @xmath23 so that the leading model @xmath27 will have an improved residual error . in this paper , we propose a method to randomly assign the input weights and biases with a supervisory mechanism ( inequality constraint ) , and provide with three ways to evaluate the output weights of scns with @xmath61 hidden nodes ( i.e. , after adding the new node ) .",
    "mathematically , we can prove that the resulting randomized learner models incrementally built based on our stochastic configuration idea are universal approximators .",
    "* theorem 6 . *",
    "suppose that span(@xmath2 ) is dense in @xmath6 space and @xmath50 , @xmath51 for some @xmath52 .",
    "given @xmath82 and a nonnegative real number sequence @xmath83 with @xmath84 and @xmath85 .",
    "for @xmath20 , denoted by @xmath86 if the random basis function @xmath24 is generated to satisfy the following inequalities : @xmath87 and the output weights are constructively evaluated by @xmath88 then , we have @xmath89 where @xmath90 , @xmath79^{\\mathrm{t}}$ ]",
    ".    * proof .",
    "* according to ( [ step2 ] ) , it is easy to verify that @xmath91 is monotonically decreasing . thus , the sequence @xmath73 is convergent as @xmath92 . from ( [ delta1 ] ) ,",
    "( [ step1 ] ) and ( [ step2 ] ) , we have @xmath93 therefore , the following inequality holds : @xmath94 note that @xmath95 , by using ( [ contract ] ) , we can easily show that @xmath96 which implies @xmath97 .",
    "this completes the proof of theorem 6 .",
    "+ theorem 6 provides a constructive scheme , i.e. , ( [ step1 ] ) and ( [ step2 ] ) , that can consequently lead to a universal approximator . unlike the strategy that maximizes some objective functions in @xcite , the supervisory mechanism ( [ step1 ] ) , which aims at finding appropriate @xmath25 and @xmath26 for a new hidden node , weakens the demanding condition as required to achieve the maximum value for @xmath98 or @xmath99 ( for the case @xmath11 ) . in fact , the existence of @xmath25 and @xmath26 satisfying ( [ step1 ] ) can be easily deduced because @xmath100 is a continuous function in the parameter space , and @xmath101 can be far less than @xmath102 once the @xmath103 is selected to approach 1 .",
    "overall , the supervisory mechanism described in ( [ step1 ] ) not only makes it possible to randomly assign the hidden parameters , which performs more flexibly and efficiently in generating a new hidden node , but also enforces the residual error to be zero along with the constructive process .",
    "* remark 4 . *",
    "our proposed supervisory mechanism in ( [ step1 ] ) indicates that random assignment of @xmath25 and @xmath26 should be constrained and data dependent .",
    "that is to say , the configuration of hidden parameters needs to be relevant to the given training samples , rather than totally rely on the distribution and/or scoping parameter setting of the random weights and biases .",
    "to the best of our knowledge , our attempt on the supervisory mechanism ( [ step1 ] ) is the first time in design of randomized learner models , and it fills the gaps between the scheme of solving a global nonlinear optimization problem ( that are usually quite demanding in both time and space since iterative solution seems inevitable ) and the strategy of freely random assignment of the hidden parameters without any constraint .",
    "it is worth mentioning that theorem 6 is still valid if the learning parameter @xmath103 is unfixed and set based on an increasing sequence approaching 1 ( always less than 1 ) . that makes sc algorithm",
    "be more flexible for the implementation of randomly searching @xmath25 and @xmath26 even when the residual error is smaller .",
    "it has been observed that finding out appropriate @xmath25 and @xmath26 for the newly added node becomes more challenging as the residual error becomes smaller . in this case",
    ", we need to set the value of @xmath103 to be extremely close to 1 .    it is obvious that @xmath104^{\\mathrm{t}}$ ] in theorem 6 is analytically evaluated by @xmath105 and remain fixed for further adding steps .",
    "this determination scheme , however , may cause very slow convergence rate for the constructive process .",
    "thus , we consider a recalculation scheme for the output weights , that is , once @xmath106 have been generated according to ( [ step1 ] ) , @xmath107 can be evaluated by minimizing the global residual error .",
    "the following theorem 7 gives a result on the universal approximation property if the least squares method is applied to update the output weights in a proceeding manner .",
    "let @xmath71=\\arg \\min_{\\beta}\\|f-\\sum_{j=1}^{l}\\beta_jg_j\\|$ ] , @xmath108 $ ] , and define intermediate values @xmath109 for @xmath110 and @xmath111 , where @xmath112^{\\mathrm{t}}$ ] and @xmath113 .",
    "* theorem 7 . *",
    "suppose that span(@xmath2 ) is dense in @xmath6 space and @xmath50 , @xmath51 for some @xmath52 .",
    "given @xmath82 and a nonnegative real number sequence @xmath83 with @xmath84 and @xmath85 .",
    "for @xmath20 , denoted by @xmath114 if the random basis function @xmath24 is generated to satisfy the following inequalities : @xmath115 and the output weights are evaluated by @xmath116=\\arg \\min_{\\beta}\\|f-\\sum_{j=1}^{l}\\beta_jg_j\\|.\\ ] ] then , we have @xmath117 where @xmath118 , @xmath119^{\\mathrm{t}}$ ] .    * proof . * it is easy to show that @xmath120 for @xmath20 , so @xmath121 is monotonically decreasing and convergent .",
    "hence , we have @xmath122 using the same arguments in the proof of theorem 6 , we can obtain @xmath123 , that completes the proof of theorem 7 .",
    "evaluation of the output weights in theorem 7 is straightforward with the use of moore - penrose generalized inverse @xcite .",
    "unfortunately , this method is infeasible for large - scale data analytics . to solve this problem , we suggest a trade - off solution with window shifting concept in the global least squares method ( i.e. , we only optimize a part of the output weights after the number of the hidden nodes exceeds a given window size ) . indeed ,",
    "this selective scheme for evaluating the output weights is meaningful and significant for dealing with large - scale data processing .",
    "similar to the proof of theorem 7 , the resulting scn shares the universal approximation property . here",
    ", we state this result in the following theorem 8 and omit the detailed proof .",
    "* theorem 8 . *",
    "suppose that span(@xmath2 ) is dense in @xmath6 space and @xmath50 , @xmath51 for some @xmath52 .",
    "given @xmath82 and a nonnegative real number sequence @xmath83 with @xmath84 and @xmath85 .",
    "for a given window size @xmath124 and @xmath20 , denoted by @xmath125 if the random basis function @xmath24 is generated to satisfy the following inequalities : @xmath126 and , as @xmath127 , the output weights are evaluated by @xmath128=\\arg \\min_{\\beta}\\|f-\\sum_{j=1}^{l}\\beta_jg_j\\|;\\ ] ] otherwise ( i.e.,@xmath129 ) , the output weights will be selectively evaluated ( i.e. , keep @xmath130 unchanged and renew the left @xmath131 ) by @xmath132=\\arg \\min_{\\beta_{l - k+1 } , \\ldots,\\beta_{l}}\\|f-\\sum_{j=1}^{l - k}\\beta_j^{*}g_j-\\sum_{j = l - k+1}^{l}\\beta_jg_j\\|.\\ ] ] then , we have @xmath117 where @xmath118 , @xmath119^{\\mathrm{t}}$ ] .",
    "this subsection details the proposed sc algorithms , i.e. , sc - i , sc - ii and sc - iii , which are associated with theorems 6 , 8 and 7 , respectively . in general , the main components of our proposed sc algorithms can be summarized as follows :    * * configuration of hidden parameters * : randomly assigning the input weights and biases to meet the constraint ( [ step1 ] ) ( ( [ step3 ] ) or ( [ step5 ] ) ) , then generating a new hidden node and adding it to the current learner model . * * evaluation of output weights * : constructively or selectively determining the output weights of the current learner model .",
    "[ sc1 ]    lll * algorithm sc - i * + given inputs @xmath133 , @xmath134 and outputs @xmath135 , @xmath136 ; + set maximum number of hidden nodes @xmath137 , expected error tolerance @xmath28 , maximum times + of random configuration @xmath138 ; choose a set of positive scalars @xmath139 ; + * 1 . *",
    "initialize @xmath140^\\mathrm{t}$ ] , @xmath82 , two empty sets @xmath141 and @xmath142 ; + * 2 . * * while * @xmath143 and @xmath144 , * do * +   + * 3.for * @xmath145 , * do * + * 4.for * @xmath146 , * do * + * * 5.**randomly assign @xmath147 and @xmath26 from @xmath148^d$ ] and @xmath148 $ ] , respectively ; + * * 6.**calculate @xmath149 , @xmath150 based on eq .",
    "( [ hiddennode ] ) and ( [ factor1 ] ) , and @xmath151 ; + * * 7.if**@xmath152 + * 8.save * @xmath25 and @xmath26 in @xmath142 , @xmath153 in @xmath141 , respectively ; + * 9.else * go back to * step 4 * + * 10.end if * + * 11.end for*(corresponds to * step 4 * ) + * * 12.if**@xmath142 is not empty + * * 13.**find @xmath154 , @xmath155 that maximize @xmath156 in @xmath141 , and set @xmath157 $ ] ; + * 14.break * ( go to * step 18 * ) ; + * 15.else * randomly take @xmath158 , renew @xmath159 , return to * step 4 * ; + * 16.end if * + * 17.end for * ( corresponds to * step 3 * ) +   + * * 18.**calculate @xmath160 , @xmath161 ; + * * 19.**@xmath162^{\\mathrm{t}}$ ] ; + * * 20.**@xmath163 ; + * * 21.**renew @xmath164 ; @xmath165 ; + * 22 . * * end while * + * 23 . * * return * @xmath166 , @xmath167 $ ] and @xmath168 $ ] .",
    "+    given a training dataset with inputs @xmath133",
    ", @xmath169^\\mathrm{t}\\in \\mathds{r}^{d}$ ] and its corresponding outputs @xmath135 , where @xmath170^\\mathrm{t}\\in \\mathds{r}^{m}$ ] , @xmath171 .",
    "denoted by @xmath172^\\mathrm{t}\\in \\mathds{r}^{n\\times m}$ ] as the corresponding residual error vector before adding the @xmath61-th new hidden node , where @xmath173\\in \\mathds{r}^n$ ] with @xmath161 .",
    "let @xmath174^\\mathrm{t},\\ ] ] which stands for the activation of the @xmath61-th hidden node for the input @xmath175 , @xmath176 .",
    "the ( current ) hidden layer output matrix can be expressed as @xmath177 $ ] .    in practice ,",
    "the target function is presented as a collection of input - output data pairs .",
    "so @xmath105 ( @xmath110 ) becomes @xmath178 for the sake of brevity , we introduce a set of variables @xmath179 and use them in algorithm descriptions ( pseudo codes ) : @xmath180 recall that for a given window size @xmath124 in sc - ii , as @xmath127 , the suboptimal solution @xmath181^{\\mathrm{t}}\\in \\mathds{r}^{l\\times m}$ ] can be computed using the standard least squares method , that is , @xmath182 where @xmath183 is the moore - penrose generalized inverse @xcite and @xmath184 represents the frobenius norm .",
    "as @xmath129 , a portion of the output weights can be evaluated by @xmath185 where @xmath186 consists of the most recent @xmath187 , @xmath188 is composed of the last @xmath124 columns of @xmath189 , i.e. , @xmath190 $ ] , and the left ( previous ) @xmath191 remain unchanged .",
    "it is easy to see that sc - ii and sc - iii become consistent once @xmath192 .",
    "[ sc2 ]    [ cols=\"<,^,<\",options=\"header \" , ]     table 4 and 5 report some results on robustness of the modelling performance with respect to the window size @xmath124 in sc - ii .",
    "it should be noted that we specified the number of hidden nodes being added in order to perform the robustness analysis .",
    "furthermore , for the purpose of examining the efficiency , we set the training error tolerance for each task ( the same as in table 3 ) and recorded the real time cost and number of hidden notes required for achieving that tolerance .",
    "all of the results reported in table 4 and 5 are averaged over 100 independent runs . for db 1 , seven cases ( @xmath193 ) are considered and the number of hidden nodes is set as 35 .",
    "it can been clearly seen that both training and test performance become better along with increasing the value of @xmath124 .",
    "importantly , small @xmath124 values may lead to inferior results , for example , the training and test rmses with @xmath194 , @xmath195 and @xmath196 are out of the tolerance level . on the other hand",
    ", the efficiency of sc - ii will become higher as the value of @xmath124 increases .",
    "apart from the scenarios of @xmath194 , @xmath195 and @xmath196 , the difference among the remaining cases can be ignored that the number of hidden nodes is about 20 while the whole time cost is around 0.26@xmath197 ( see table 4 ) . for db 2 , five cases ( @xmath198 )",
    "are examined while the number of hidden nodes is set as 25 .",
    "similarly , all of the results , except for @xmath194 and @xmath195 , are comparable . in particular , for @xmath199 , the time cost and number of hidden nodes required for achieving the error tolerance ( @xmath200 ) are around 0.2@xmath197 and 16 , respectively .",
    "figure 3 depicts the modelling performance ( for test dataset ) of irvfl , sc - i , sc - ii and sc - iii on db 1 , respectively .",
    "firstly , the importance of the scale factor @xmath201 in the activation function , which directly determines the range of random parameters , is examined by performing different settings .",
    "secondly , the infeasibility of irvfl is illustrated in comparison with the proposed sc algorithms , where effectiveness and benefits from the supervisory mechanism of scns can be clearly observed .",
    "the maximum number of the hidden nodes to be added is set as 100 for these four methods examined here . from figure 3(a ) , ( b ) , ( c ) , it is apparent that irvfl networks perform poorly .",
    "in fact , we attempted to add 20,000 hidden nodes to see the performance of irvfl networks with different values of @xmath201",
    ". unfortunately , the final result is very similar to that depicted in figure 3(c ) , which is aligned with our findings in @xcite . in comparison ,",
    "the test performance of sc - i shown in figure 3(d ) is much better than irvfl , that corresponds to the records in table 1 and the error changing curves in figure 1 , respectively .",
    "in addition , both sc - ii and sc - iii outperform sc - i as the recalculation of @xmath23 contributes a lot in constructing a universal approximator with a much more compact structure .",
    "this section briefly explains why the performance of our proposed sc algorithms are better than mq and irvfl algorithms .",
    "then , further remarks are provided to highlight some characteristics of scns .    although the universal approximation property can be ensured by maximizing an objective function ( see section 2 ) , the modified quickprop ( mq ) algorithm , aiming at iteratively finding the appropriate hidden parameters ( @xmath48 and @xmath49 ) for the new hidden node , may face some obstacles in proceeding the optimization of certain objective function .",
    "in essence , it is a gradient - ascent algorithm with consideration of second - order information , and may be problematic when it is searching in a plateau over the error surface .",
    "that is to say , the well - trained parameters ( @xmath48 and @xmath49 ) at certain iterative step fall into a region where both the first and second order of derivatives of the objective function are nearly zero , learning basically reaches a halt and can be terminated by a patience parameter for the purpose of time - saving . the overall speed for parameter updating might be quite slow",
    "this issue seems severe for regression tasks . for implementations",
    ", mq works less flexibly and seems very likely to fail in constructing a universal approximator , not to mention some inherent flaws including the selection of initial weights , learning rate , as well as a reasonable stopping criterion . besides , there will be much computational burden in dealing with large - scale problems .",
    "the unique feature of the proposed scns is the use of randomness in learner model design with a supervisory mechanisms .",
    "compared against modified quickprop algorithm , sc - ii and sc - iii are computationally manageable and easy to be implemented .",
    "in addition , the output weights @xmath23 in irvfl are analytically calculated by @xmath202 and then remain fixed in further phases .",
    "this constructive approach for computing the output weights , together with the freely random assignment of the hidden parameters , may cause very slow decreasing rate of the residual error sequence , and as a result the learner model may fail to approximate the target function .",
    "although this evaluation method for the output weights is also applied for sc - i , the whole construction process works successfully in building a universal approximator . in the end , some features of our proposed scns are summarized as follows :    * the selection of @xmath103 that to some extent is directly associated with the error decreasing speed is quite important . in our experiments ,",
    "its setting is unfixed and based on an increasing sequence stating from 0.9 and approaching 1 .",
    "that makes it possible for the implementation of randomly searching @xmath25 and @xmath26 . as the constructive process proceeds ,",
    "the current residual error becomes smaller which makes the configuration task on @xmath25 and @xmath26 be more challenging ( i.e. , need more searching attempts as @xmath61 becomes larger ) . from our experience , this difficulty can be effectively alleviated by setting @xmath103 extremely close to 1 .",
    "* in sc algorithms , we used the scaling sigmoidal function in the hidden nodes , of which @xmath201 may keep varying during the learning course .",
    "this implies that the scope setting for @xmath48 and @xmath49 should not be fixed . in our simulations , @xmath201 is automatically decided from a given set @xmath203 , in accompany with the setup of @xmath103 .",
    "this strategy makes it possible for the built learner to possess multi - scale random basis functions rather than rvfl networks ( as argued in @xcite ) or irvfl , and can inherently improve the probability of finding certain appropriate setting of the input weights and biases . * in implementation",
    ", @xmath138 controls the pool size of the random basis function candidates .",
    "it is related to both opportunity and efficiency , so we need to chose this parameter carefully with trade - off mind .",
    "this operation aims at finding the most appropriate pairs of @xmath48 and @xmath49 that returns the largest @xmath156 ( see algorithm sc - i ) . roughly speaking",
    ", this manipulation can be viewed as an alternative to find a ` suboptimal ' solution of the maximization problem discussed in @xcite . *",
    "based on the theoretical analysis given in section 3 , many types of activation functions can be used in scns , such as gaussian , sine , cosine and tanh function .",
    "figure 4 shows both the training and test performance of sc - i and irvfl with different choices of activation functions .",
    "it can be clearly seen that our sc - i algorithm is more efficient and effective than irvfl as the random assignment of @xmath48 and @xmath49 from [ -1,1 ] is unworkable and useless , no matter what kind of activation function is employed in the hidden layer . * as a compromise between sc - i and sc - iii , sc - ii provides more flexibility in dealing with large - scale data modelling tasks , by right of its distinctive window - based recalculation for the output weights . through optimizing a part of the output weights according to a given window size",
    ", sc - ii not only outperforms sc - i , because the evaluation manner of the output weights in sc - i is not based on any objectives and no further adjustments are provided throughout the construction process ; but also has some advantages over sc - iii in building universal approximators for coping with large - scale data analytics . in practice ,",
    "when the number of training samples is extremely large and @xmath129 , the computation burden for calculating @xmath204 in sc - ii is far less than calculating @xmath183 in sc - iii . besides",
    ", the robustness of sc - ii with regard to the window size is favourable once @xmath124 is assigned from a reasonable range , as shown in table 4 and 5 .",
    "it is meaningful and interesting to find out more characteristics about sc - ii in the future .",
    "our proposed framework in this paper provides with an alternative solution for building randomized learner models with supervisory mechanisms . as a powerful tool for fast data modelling",
    ", scns can be incrementally built by stochastically configuring the input weights and biases of each hidden node , and determining the output weights via constructive evaluation or solving an optimization problem for linear models .",
    "each scn model can be regarded as a specific implementation of our sc algorithms , which in theory ensure the convergence provided that newly generated nodes are kept adding to the model . from our hands - on experiences as demonstrated in the simulations , sc - iii exhibits the best performance in terms of the convergence rate , sc - i converges slowest but still outperforms irvfl although they evaluate the output weights in the same way .",
    "it should be pointed that sc - ii embedded with a given window size for updating the output weights makes a good trade - off between the efficiency and the scalability ( i.e. , for large - scale data analytics ) .    in this work ,",
    "we focus on slfnn architecture with a sigmoidal basis / activation function for the hidden nodes .",
    "some immediate extensions to rvfl architecture ( with direct links between the inputs and the outputs ) and echo state networks ( with recurrent feedback at the hidden layer ) can be done easily . as a technical contribution to the machine learning community",
    ", our proposed stochastic configuration idea in this paper is significant and applicable for data representation with deep learning @xcite .",
    "plenty of researches on scns and its applications can be explored in the future , and some of them have been done by our group . for instance , deep scns ( dscns ) and robust scns ( rscns ) have been developed for dealing with data representation and regression , and uncertain data analytics respectively , which will be reported in our series of publications .",
    "also , we are extending the present algorithms to ensemble learning , online learning and distributed learning . except for these studies mentioned above ,",
    "the following researches are being expected : robustness analyses of the modelling performance with respect to some key parameters involved in design of scns ; and a guideline for selecting the basis / activation function so that the modelling performance can be maximized .",
    "t. chen , h. chen , universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems , ieee transactions on neural networks 6(4 ) ( 1995 ) 911 - 917 .",
    "l. k. jones , a simple lemma on greedy approximation in hilbert space and convergence rates for projection pursuit regression and neural network training , the annals of statistics 20 ( 1 ) ( 1992 ) 608 - 613 .                            w. f. schmidt , m. a. kraaijveld , r. p. duin , feedforward neural networks with random weights , proc .",
    "of 11th iapr international conference on pattern recognition , vol .",
    "conference b : pattern recognition methodology and systems ( 1992 ) 1 - 4 ."
  ],
  "abstract_text": [
    "<S> this paper contributes to a development of randomized methods for neural networks . </S>",
    "<S> the proposed learner model is generated incrementally by stochastic configuration ( sc ) algorithms , termed as stochastic configuration networks ( scns ) . </S>",
    "<S> in contrast to the existing randomised learning algorithms for single layer feed - forward neural networks ( slfnns ) , we randomly assign the input weights and biases of the hidden nodes in the light of a supervisory mechanism , and the output weights are analytically evaluated in either constructive or selective manner . as fundamentals of scn - based data modelling techniques , we establish some theoretical results on the universal approximation property . three versions of sc algorithms are presented for regression problems ( applicable for classification problems as well ) in this work . </S>",
    "<S> simulation results concerning both function approximation and real world data regression indicate some remarkable merits of our proposed scns in terms of less human intervention on the network size setting , the scope adaptation of random parameters , fast learning and sound generalization . </S>"
  ]
}