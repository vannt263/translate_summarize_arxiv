{
  "article_text": [
    "suppose we are given the convolution of two signals , @xmath2 .",
    "when , under which conditions , and how can we reconstruct @xmath0 and @xmath1 from the knowledge of @xmath3 if both @xmath0 _ and _ @xmath1 are unknown ?",
    "this challenging problem , commonly referred to as _ blind deconvolution _",
    "problem , arises in many areas of science and technology , including astronomy , medical imaging , optics , and communications engineering , see e.g.  @xcite .",
    "indeed , the quest for finding a fast and reliable algorithm for blind deconvolution has confounded researchers for many decades .",
    "it is clear that without any additional assumptions , the blind deconvolution problem is ill - posed .",
    "one common and useful assumption is to stipulate that @xmath0 and @xmath1 belong to known subspaces  @xcite .",
    "this assumption is reasonable in various applications , provides flexibility and at the same time lends itself to mathematical rigor .",
    "we also adopt this subspace assumption in our algorithmic framework ( see section  [ s : prelim ] for details ) .",
    "but even with this assumption , blind deconvolution is a very difficult non - convex optimization problem that suffers from an overabundance of local minima , making its numerical solution rather challenging .    in this paper",
    ", we present a numerically efficient blind deconvolution algorithm that converges geometrically to the optimal solution .",
    "our regularized gradient descent algorithm comes with rigorous mathematical convergence guarantees .",
    "the number of measurements required for the algorithm to succeed is only slightly larger than the information theoretic minimum .",
    "moreover , our algorithm is also robust against noise . to the best of our knowledge ,",
    "the proposed algorithm is the first blind deconvolution algorithm that is numerically efficient , robust against noise , and comes with rigorous recovery guarantees under certain subspace conditions .",
    "since blind deconvolution problems are ubiquitous in science and engineering , it is not surprising that there is extensive literature on this topic .",
    "it is beyond the scope of this paper to review the existing literature ; instead we briefly discuss those results that are closest to our approach .",
    "we gladly acknowledge that those papers that are closest to ours , namely  @xcite , are also the ones that greatly influenced our research in this project .    in the inspiring article  @xcite , ahmed , recht , and romberg",
    "develop a convex optimization framework for blind deconvolution .",
    "the formal setup of our blind deconvolution problem follows essentially their setting . using the meanwhile well - known lifting trick",
    ",  @xcite transforms the blind deconvolution problem into the problem of recovering a rank - one matrix from an underdetermined system of linear equations . by replacing the rank condition by a nuclear norm condition",
    ", the computationally infeasible rank minimization problem turns into a convenient convex problem .",
    "the authors provide explicit conditions under which the resulting semidefinite program is guaranteed to have the same solution as the original problem .",
    "in fact , the number of required measurements is not too far from the theoretical minimum .",
    "the only drawback of this otherwise very appealing convex optimization approach is that the computational complexity of solving the semidefinite program is rather high for large - scale data and/or for applications where computation time is of the essence .",
    "overcoming this drawback was one of the main motivations for our paper .",
    "while  @xcite does suggest a fast matrix - factorization based algorithm to solve the semidefinite program , the convergence of this algorithm to the optimal solution is not established in that paper .",
    "the theoretical number of measurements required in  @xcite for the semidefinite program to succeed is essentially comparable to that for our non - convex algorithm to succeed .",
    "the advantage of the proposed non - convex algorithm is of course that it is dramatically faster .",
    "furthermore , numerical simulations indicate that the empirically observed number of measurements for our non - convex approach is actually smaller than for the convex approach .",
    "the philosophy underlying the method presented in our paper is strongly motivated by the non - convex optimization algorithm for phase retrieval proposed in  @xcite , see also  @xcite . in the pioneering paper  @xcite",
    "the authors use a two - step approach : ( i ) construct in a numerically efficient manner a good initial guess ; ( ii ) based on this initial guess , show that simple gradient descent will converge to the true solution .",
    "our paper follows a similar two - step scheme . at first glance",
    "one would assume that many of the proof techniques from  @xcite should carry over to the blind deconvolution problem .",
    "alas , we quickly found out that despite some general similarities between the two problems , phase retrieval and blind deconvolution are indeed surprisingly different . at the end",
    ", we mainly adopted some of the general `` proof principles '' from  @xcite ( for instance we also have a notion of local regularity condition - although it deviates significantly from the one in  @xcite ) , but the actual proofs are quite different .",
    "for instance , in  @xcite and  @xcite convergence of the gradient descent algorithm is shown by directly proving that the distance between the true solution and the iterates decreases .",
    "the key conditions ( a local regularity condition and a local smoothness condition ) are tailored to this aim . for the blind deconvolution problem we needed to go a different route .",
    "we first show that the objective function decreases during the iterations and then use a certain local restricted isometry property to transfer this decrease to the iterates to establish convergence to the solution .",
    "we also gladly acknowledge being influenced by the papers  @xcite by montanari and coauthors on matrix completion via non - convex methods . while the setup and analyzed problems are quite different from ours , their approach informed our strategy in various ways . in  @xcite , the authors propose an algorithm which comprises a two - step procedure .",
    "first , an initial guess is computed via a spectral method , and then a nonconvex problem is formulated and solved via an iterative method .",
    "the authors prove convergence to a low - rank solution , but do not establish a rate of convergence .",
    "as mentioned before , we also employ a two - step strategy .",
    "moreover , our approach to prove stability of the proposed algorithm draws from ideas in  @xcite .",
    "we also benefitted tremendously from  @xcite . in that paper , sun and",
    "luo devise a non - convex algorithm for low - rank matrix completion and provide theoretical guarantees for convergence to the correct solution .",
    "we got the idea of adding a penalty term to the objective function from  @xcite ( as well as from  @xcite ) . indeed , the particular structure of our penalty term closely resembles that in  @xcite .",
    "in addition , the introduction of the various neighborhood regions around the true solution , that are used to eventually characterize a `` basin of attraction '' , stems from  @xcite",
    ". these correspondences may not be too surprising , given the connections between low - rank matrix completion and blind deconvolution . yet , like also discussed in the previous paragraph , despite some obvious similarities between the two problems , it turned out that many steps and tools in our proofs differ significantly from those in  @xcite . also this should not come as a surprise , since the measurement matrices and setup differ significantly . moreover , unlike  @xcite , our paper also provides robustness guarantees for the case of noisy data .",
    "indeed , it seems plausible that some of our techniques to establish robustness against noise are applicable to the analysis of various recent matrix completion algorithms , such as e.g.  @xcite .",
    "we briefly discuss other interesting papers that are to some extent related to our work .",
    "@xcite proposes a projected gradient descent algorithm based on matrix factorizations and provide a convergence analysis to recover sparse signals from subsampled convolution .",
    "however , this projection step can be hard to implement , which does impact the efficiency and practical use of this method . as suggested in  @xcite",
    ", one can avoid this expensive projection step by resorting to a heuristic approximate projection , but then the global convergence is not fully guaranteed . on the other hand , the papers  @xcite consider identifiability issue of blind deconvolution problem with both @xmath0 and @xmath1 in random linear subspaces and achieve nearly optimal result of sampling complexity in terms of information theoretic limits .",
    "very recently ,  @xcite improved the result from  @xcite by using techniques from algebraic geometry .",
    "the past few years have also witnessed an increasing number of excellent works other than blind deconvolution but related to nonconvex optimization  @xcite .",
    "the paper  @xcite analyzes the problem of recovering a low - rank positive semidefinite matrix from linear measurements via a gradient descent algorithm .",
    "the authors assume that the measurement matrix fulfills the standard and convenient restricted isometry property , a condition that is not suitable for the blind deconvolution problem ( besides the fact that the positive semidefinite assumption is not satisfied in our setting ) . in  @xcite ,",
    "chen and wainwright study various the solution of low - rank estimation problems by projected gradient descent .",
    "the very recent paper  @xcite investigates matrix completion for rectangular matrices . by `` lifting '' , they convert the unknown matrix into a positive semidefinite one and apply matrix factorization combined with gradient descent to reconstruct the unknown entries of the matrix .",
    "@xcite considers an interesting blind calibration problem with a special type of measurement matrix via nonconvex optimization . besides some general similarities",
    ", there is little overlap of the aforementioned papers with our framework .",
    "finally , a convex optimization approach to blind deconvolution and self - calibration that extends the work of  @xcite can be found in  @xcite , while  @xcite also covers the joint blind deconvolution - blind demixing problem .",
    "this paper is organized as follows .",
    "we introduce some notation used throughout the paper in the remainder of this section .",
    "the model setup and problem formulation are presented in section  [ s : prelim ] .",
    "section  [ s : algo ] describes the proposed algorithm and our main theoretical result establishing the convergence of our algorithm .",
    "numerical simulations can be found in section  [ s : numerics ] .",
    "section  [ s : proof ] is devoted to the proof of the main theorem . since the proof is quite involved , we have split this section into several subsections .",
    "some auxiliary results are collected in the appendix .",
    "we introduce notation which will be used throughout the paper .",
    "matrices and vectors are denoted in boldface such as @xmath4 and @xmath5 .",
    "the individual entries of a matrix or a vector are denoted in normal font such as @xmath6 or @xmath7 for any matrix @xmath4 , @xmath8 denotes its nuclear norm , i.e. , the sum of its singular values ; @xmath9 denotes its operator norm , i.e. , the largest singular value , and @xmath10 denotes its the frobenius norm , i.e. , @xmath11 . for any vector @xmath5 , @xmath12 denotes its euclidean norm . for both matrices and vectors , @xmath13 and @xmath14 stand for the transpose of @xmath4 and @xmath5 respectively while @xmath15 and @xmath16 denote their complex conjugate transpose .",
    "we equip the matrix space @xmath17 with the inner product defined as @xmath18 a special case is the inner product of two vectors , i.e. , @xmath19 for a given vector @xmath20 , @xmath21 represents the diagonal matrix whose diagonal entries are given by the vector @xmath20 . for any @xmath22 , denote @xmath23 as @xmath24 is a constant which depends linearly on @xmath25 , but on no other parameters .",
    "we consider the blind deconvolution model @xmath26 where @xmath3 is given , but @xmath0 and @xmath1 are unknown . here ",
    "@xmath27 \" denotes circular convolution decays sufficiently fast  @xcite . ] .",
    "we will usually consider @xmath0 as the `` blurring function '' and @xmath1 as the signal of interest .",
    "it is clear that without any further assumption it is impossible to recover @xmath0 and @xmath1 from @xmath3 .",
    "we want to impose conditions on @xmath0 and @xmath1 that are realistic , flexible , and not tied to one particular application ( such as , say , image deblurring ) . at the same time",
    ", these conditions should be concrete enough to lend themselves to meaningful mathematical analysis .",
    "a natural setup that fits these demands is to assume that @xmath0 and @xmath1 belong to known linear subspaces .",
    "concerning the blurring function , it is reasonable in many applications to assume that @xmath0 is either compactly supported or that @xmath0 decays sufficiently fast so that it can be well approximated by a compactly supported function .",
    "therefore , we assume that @xmath28 satisfies @xmath29 where @xmath30 , i.e. , only the first @xmath31 entries of @xmath0 are nonzero and @xmath32 for all @xmath33 .",
    "concerning the signal of interest , we assume that @xmath1 belongs to a linear subspace spanned by the columns of a known matrix @xmath34 , i.e. , @xmath35 for some matrix @xmath34 of size @xmath36 . here",
    "we use @xmath37 instead of @xmath38 for the simplicity of notation later . for theoretical purposes",
    "we assume that @xmath34 is a gaussian random matrix .",
    "numerical simulations suggest that this assumption is clearly not necessary .",
    "for example , we observed excellent performance of the proposed algorithm also in cases when @xmath34 represents a wavelet subspace ( appropriate for images ) or when @xmath34 is a hadamard - type matrix ( appropriate for communications ) .",
    "we hope to address these other , more realistic , choices for @xmath34 in our future research .",
    "finally , we assume that @xmath39 as the additive white complex gaussian noise where @xmath40 and @xmath41 and @xmath42 are the true blurring function and the true signal of interest , respectively . in that way @xmath43 actually serves as a measure of snr ( signal to noise ratio ) . for our theoretical analysis as well as for numerical purposes , it is much more convenient to express   in the fourier domain , see also  @xcite . to that end , let @xmath44 be the @xmath45 unitary discrete fourier transform ( dft ) matrix and let the @xmath46 matrix @xmath47 be given by the first @xmath31 columns of @xmath44 ( then @xmath48 ) . by applying the scaled dft matrix @xmath49 to both sides of   we get @xmath50 which follows from the property of circular convolution and discrete fourier transform . here , @xmath51 where @xmath52 denotes pointwise product .",
    "by definition of @xmath0 in  , we have @xmath53 therefore ,   becomes , @xmath54 where @xmath55 ( we use @xmath56 instead of @xmath57 simply because it gives rise to a more convenient notation later , see e.g.  ) .",
    "note that if @xmath34 is a gaussian matrix , then so is @xmath58 .",
    "furthermore , @xmath59 is again a complex gaussian random vector .",
    "hence by replacing @xmath60 in   by @xmath3 , we arrive with a slight abuse of notation at @xmath61    for the remainder of the paper , instead of considering the original blind deconvolution problem  , we focus on its mathematically equivalent version  , where @xmath62 , @xmath63 , @xmath64 , @xmath65 and @xmath66 .",
    "as mentioned before , @xmath41 and @xmath42 are the ground truth .",
    "our goal is to recover @xmath41 and @xmath42 when @xmath47 , @xmath57 and @xmath3 are given .",
    "it is clear that if @xmath67 is a solution to the blind deconvolution problem .",
    "then so is @xmath68 for any @xmath69 .",
    "thus , all we can hope for in the absence of any further information , is to recover a solution from the equivalence class @xmath70 .",
    "hence , we can as well assume that @xmath71 .",
    "as already mentioned , we choose @xmath47 to be the  low - frequency \" discrete fourier matrix , i.e. , the first @xmath31 columns of an @xmath45 unitary dft ( discrete fourier transform ) matrix .",
    "moreover , we choose @xmath57 to be an @xmath72 complex gaussian random matrix , i.e. , @xmath73 where  @xmath74 \" is the imaginary unit .",
    "we define the matrix - valued linear operator @xmath75 via @xmath76 where @xmath77 denotes the @xmath78-th column of @xmath79 and @xmath80 is the @xmath78-th column of @xmath81 immediately , we have @xmath82 , @xmath83 and @xmath84 for all @xmath85 .",
    "this is essentially a smart and popular trick called  lifting \"  @xcite , which is able to convert a class of nonlinear models into linear models at the costs of increasing the dimension of the solution space .",
    "it seems natural and tempting to recover @xmath67 obeying   by solving the following optimization problem @xmath86 where @xmath87",
    "we also let @xmath88 which is a special case of @xmath89 when @xmath90 furthermore , we define @xmath91 , an important quantity throughout our discussion , via @xmath92 when there is no danger of ambiguity , we will often denote @xmath91 simply by @xmath93 . but",
    "let us remember that @xmath91 is always a function of @xmath94 and measures the relative approximation error of @xmath94 .",
    "obviously , minimizing   becomes a nonlinear least square problem , i.e. , one wants to find a pair of vectors @xmath95 or a rank-1 matrix @xmath96 which fits the measurement equation in   best .",
    "solving   is a challenging optimization problem since it is highly nonconvex and most of the available algorithms , such as alternating minimization and gradient descent , may suffer from getting easily trapped in some local minima .",
    "another possibility is to consider a convex relaxation of   at the cost of having to solve an expensive semidefinite program . in the next section",
    "we will describe how to avoid this dilemma and design an efficient gradient descent algorithm that , under reasonable conditions , will always converge to the true solution .",
    "0      however , in this particular problem , minimizing   might not be enough to guarantee recovery . before moving to further discussion",
    ", we define the incoherence parameter between @xmath97 and @xmath41 as @xmath98 and it is easy to see that @xmath99 . both lower and upper bounds are tight , i.e. , @xmath100 if @xmath41 is parallel to one of @xmath97 and @xmath101 if @xmath41 is a 1-sparse vector of length @xmath31",
    ".    one may naturally question if @xmath102 is really necessary here . now recall that in matrix completion  @xcite , we know the left and right singular vectors of  ground truth \" can not be  too aligned \" with those of the measurement matrices .",
    "the same philosophy works here as well .",
    "it has been shown numerically that the incoherence parameter is not only an important ingredient for establishing exact recovery via convex programming  @xcite but also it affects the performance of algorithm and sampling complexity . in other words , an important underlying assumption here is that @xmath47 and the ground truth @xmath41 are well  incoherent \" to each other and thus @xmath102 is relatively small .",
    "so more precisely , we are looking for a pair of @xmath67 via @xmath103 where @xmath104 is a tuning parameter which controls the incoherence .",
    "now one may want to ask a similar question , i.e. , for this nonconvex optimization problem , whether adding this incoherence constraint will enhance the performance or it is just artifact of proof ?",
    "the answer is that the incoherence constraint does improve empirical performance under some extreme cases .",
    "but it is obvious that @xmath105 is a nonconvex cone .",
    "therefore , this extra constraint creates new difficulties although it helps enforce the incoherence constraint .",
    "in this section we introduce our algorithm as well as our main theorems which establish convergence of the proposed algorithm to the true solution .",
    "as mentioned above , in a nutshell our algorithm consists of two parts : first we use a carefully chosen initial guess , and second we use a variation of gradient descent , starting at the initial guess to converge to the true solution",
    ". one of the most critical aspects is of course that we must avoid getting stuck in local minimum or saddle point .",
    "hence , we need to ensure that our iterates are inside some properly chosen _ basin of attraction _ of the true solution",
    ". the appropriate characterization of such a basin of attraction requires some diligence , a task that will occupy us in the next subsection",
    ". we will then proceed to introducing our algorithm and analyzing its convergence .",
    "the road toward designing a proper basin of attraction is basically paved by three observations , described below .",
    "these observations prompt us to introduce three neighborhoods ( inspired by  @xcite ) , whose intersection will form the desired basin of attraction of the solution .    *",
    "observation 1 - nonuniqueness of the solution : * as pointed out earlier , if @xmath95 is a solution to  , then so is @xmath106 for any @xmath69 .",
    "thus , without any prior information about @xmath107 and/or @xmath108 , it is clear that we can only recover the true solution up to such an unknown constant @xmath109 .",
    "fortunately , this suffices for most applications . from the viewpoint of numerical stability",
    "however , we do want to avoid , while @xmath110 remains bounded , that @xmath111 and @xmath112 ( or vice versa ) .",
    "to that end we introduce the following neighborhood : @xmath113 ( recall that @xmath114 . )    * observation 2 - incoherence : * our numerical simulations indicate that the number of measurements required for solving the blind deconvolution problem with the proposed algorithm does depend ( among others ) on how much @xmath41 is correlated with the rows of the matrix @xmath47  the smaller the correlation the better .",
    "a similar effect has been observed in blind deconvolution via convex programming  @xcite .",
    "we quantify this property by defining the _ incoherence _ between the rows of @xmath47 and @xmath41 via @xmath115 it is easy to see that @xmath99 and both lower and upper bounds are tight ; i.e. , @xmath100 if @xmath41 is parallel to one of @xmath97 and @xmath101 if @xmath41 is a 1-sparse vector of length @xmath31 .",
    "note that in our setup , we assume that @xmath57 is a random matrix and @xmath42 is fixed , thus with high probability , @xmath42 is already sufficiently incoherent with the rows of @xmath57 and thus we only need to worry about the incoherence between @xmath47 and @xmath41 .",
    "it should not come as a complete surprise that the incoherence between @xmath41 and the rows of @xmath47 is important .",
    "the reader may recall that in matrix completion  @xcite the left and right singular vectors of the solution can not be  too aligned \" with those of the measurement matrices .",
    "a similar philosophy seems to apply here .",
    "being able to control the incoherence of the solution is instrumental in deriving rigorous convergence guarantees of our algorithm .",
    "for that reason , we introduce the neighborhood @xmath116 where @xmath117 .",
    "* observation 3 - initial guess : * it is clear that due to the non - convexity of the objective function , we need a carefully chosen initial guess .",
    "we quantify the distance to the true solution via the following neighborhood @xmath118 where @xmath119 is a predetermined parameter in @xmath120 $ ] .",
    "it is evident that the true solution @xmath121 .",
    "note that @xmath122 implies @xmath123 and @xmath124 .",
    "therefore , for any element @xmath125 its incoherence can be well controlled by @xmath126      our approach consists of two parts : we first construct an initial guess that is inside the `` basin of attraction '' @xmath127 .",
    "we then apply a carefully regularized gradient descent algorithm that will ensure that all the iterates remain inside @xmath127 .    due to the difficulties of directly projecting onto @xmath128 (",
    "the neigbourhood @xmath129 is easier to manage ) we add instead a regularizer @xmath130 to the objective function @xmath89 to enforce that the iterates remain inside @xmath131 .",
    "while the idea of adding a penalty function to control incoherence is proposed in different forms to solve matrix completion problems , see e.g. ,  @xcite , our version is mainly inspired by  @xcite .",
    "hence , we aim to minimize the following regularized objective function to solve the blind deconvolution problem : @xmath132 where @xmath89 is defined in   and @xmath133 , the penalty function , is of the form @xmath134,\\ ] ] where @xmath135 and @xmath136 . here",
    "we assume @xmath137 and @xmath138 .",
    "the idea behind this , at first glance complicated , penalty function is quite simple . the first two terms in   enforce the projection of @xmath94 onto @xmath139 while the last term is related to @xmath140 ; it will be shown later that any @xmath141 gives @xmath142 if @xmath143 .",
    "since @xmath144 is a truncated quadratic function , it is obvious that @xmath145 and @xmath133 is a continuously differentiable function .",
    "those two properties play a crucial role in proving geometric convergence of our algorithm presented later .",
    "another important issue concerns the selection of parameters .",
    "we have three unknown parameters in @xmath133 , i.e. , @xmath146 , @xmath147 , and @xmath148 . here , @xmath147 can be obtained via algorithm  1 and @xmath143 is guaranteed by theorem  [ thm : init ] ; @xmath149 because @xmath150 and @xmath151 concentrates around @xmath152 . in practice ,",
    "@xmath153 which is the inverse of the snr , can often be well estimated .",
    "regarding @xmath148 , we require @xmath117 in order to make sure that @xmath154",
    ". it will depend on the specific application how well one can estimate @xmath155 .",
    "for instance , in wireless communications , a very common channel model for @xmath41 is to assume a _",
    "rayleigh fading model _",
    "@xcite , i.e. , @xmath156 . in that case",
    "it is easy to see that @xmath157      since we are dealing with complex variables , instead of using regular derivatives it is more convenient to utilize wirtinger derivatives   where @xmath158 and @xmath159 , the wirtinger derivatives are defined as @xmath160 and @xmath161 two important examples used here are @xmath162 and @xmath163 . ] , which has become increasingly popular since  @xcite .",
    "note that @xmath164 is a real - valued function and hence we only need to consider the derivative of @xmath164 with respect to @xmath165 and @xmath37 and the corresponding updates of @xmath166 and @xmath38 because a simple relation holds , i.e. , @xmath167 in particular , we denote @xmath168 and @xmath169 .",
    "we also introduce the adjoint operator of @xmath170 , given by @xmath171    both @xmath172 and @xmath173 can now be expressed as @xmath174 where each component yields @xmath175^*{\\boldsymbol{h}}= [ { \\mathcal{a}}^*({\\mathcal{a}}({\\boldsymbol{h}}{\\boldsymbol{x}}^ * - { \\boldsymbol{h}}_0{\\boldsymbol{x}}_0^ * ) - { \\boldsymbol{e}})]^*{\\boldsymbol{h } } , \\label{eq : wfx}\\\\ \\nabla g_{{\\boldsymbol{h } } } & = & \\frac{\\rho}{2d}\\left[g'_0\\left(\\frac{\\|{\\boldsymbol{h}}\\|^2}{2d}\\right ) { \\boldsymbol{h}}+ \\frac{l}{4\\mu^2 } \\sum_{l=1}^l g'_0\\left(\\frac{l|{\\boldsymbol{b}}_l^*{\\boldsymbol{h}}|^2}{8d\\mu^2}\\right ) { \\boldsymbol{b}}_l{\\boldsymbol{b}}_l^*{\\boldsymbol{h}}\\right ] , \\label{eq : wgh } \\\\ \\nabla g_{{\\boldsymbol{x } } } & = & \\frac{\\rho}{2d } g'_0\\left ( \\frac{\\|{\\boldsymbol{x}}\\|^2}{2d}\\right ) { \\boldsymbol{x}}. \\label{eq : wgx}\\end{aligned}\\ ] ]    our algorithm consists of two steps : initialization and gradient descent with constant stepsize .",
    "the initialization is achieved via a spectral method followed by projection .",
    "the idea behind spectral method is that @xmath176 and hence one can hope that the leading singular value and vectors of @xmath177 can be a good approximation of @xmath178 and @xmath67 respectively .",
    "the projection step ensures @xmath179 , which the spectral method alone might not guarantee . we will address the implementation and computational complexity issue in section  [ s : numerics ] .",
    "compute @xmath180 find the leading singular value , left and right singular vectors of @xmath177 , denoted by @xmath147 , @xmath181 and @xmath182 respectively .",
    "solve the following optimization problem : @xmath183 and @xmath184 output : @xmath185    obtain @xmath186 via algorithm  [ initial ] .",
    "@xmath187 @xmath188      our main finding is that with a diligently chosen initial guess @xmath186 , simply running gradient descent to minimize the regularized non - convex objective function @xmath189 will not only guarantee linear convergence of the sequence @xmath190 to the global minimum @xmath67 in the noiseless case , but also provide robust recovery in the presence of noise .",
    "the results are summarized in the following two theorems .",
    "[ thm : init ] the initialization obtained via algorithm  [ initial ] satisfies @xmath191 and @xmath192 holds with probability at least @xmath193 if the number of measurements satisfies @xmath194 here @xmath119 is any predetermined constant in @xmath120 $ ] , and @xmath195 is a constant only linearly depending on @xmath25 with @xmath196 .    the proof of theorem  [ thm : init ] is given in section  [ s : init ] .",
    "while the initial guess is carefully chosen , it is in general not of sufficient accuracy to already be used as good approximation to the true solution .",
    "the following theorem establishes that as long as the initial guess lies inside the basin of attraction of the true solution , regularized gradient descent will indeed converge to this solution ( or to a solution nearby in case of noisy data ) .",
    "[ thm : main ] consider the model in   with the ground truth @xmath67 , @xmath197 and the noise @xmath198 .",
    "assume that the initialization @xmath186 belongs to @xmath199 and that @xmath200 algorithm  [ agd ] will create a sequence @xmath201 which converges geometrically to @xmath67 in the sense that with probability at least @xmath202 , there holds @xmath203 and @xmath204 , @xmath205 , @xmath206 is the fixed stepsize and @xmath207 is the angle between @xmath208 and @xmath41 .",
    "here @xmath209 holds with probability @xmath210    * remarks : *    1 .   while the setup in   assumes that @xmath47 is a matrix consisting of the first @xmath31 columns of the dft matrix , this is actually not necessary for theorem  [ thm : main ] . as the proof will show , the only conditions on @xmath47 are that @xmath211 and that the norm of the @xmath78-th row of @xmath47 satisfies @xmath212 for some numerical constant @xmath213 .",
    "the minimum number of measurements required for our method to succeed is roughly comparable to that of the convex approach proposed in  @xcite ( up to log - factors ) .",
    "thus there is no price to be paid for trading a slow , convex - optimization based approach with a fast non - convex based approach . indeed , numerical experiments indicate that the non - convex approach even requires a smaller number of measurements compared to the convex approach , see section  [ s : numerics ] .",
    "the convergence rate of our algorithm is completely determined by @xmath214 . here ,",
    "the _ regularity constant _",
    "@xmath215 is specified in   and @xmath216 where @xmath217 .",
    "the attentive reader may have noted that @xmath218 depends essentially linearly on @xmath219 , which actually reflects a tradeoff between sampling complexity ( or statistical estimation quality ) and computation time .",
    "note that if @xmath220 gets larger , the number of constraints is also increasing and hence leads to a larger @xmath218 .",
    "however , this issue can be solved by choosing parameters smartly .",
    "theorem  [ thm : main ] tells us that @xmath221 should be roughly between @xmath155 and @xmath222 .",
    "therefore , by choosing @xmath223 and @xmath224 , @xmath218 is optimized and @xmath225 which is shown in details in section  [ s : smooth ] .",
    "relations   and   are basically equivalent to the following : @xmath226 which says that @xmath190 converges to an element of the equivalence class associated with the true solution @xmath227 ( up to a deviation governed by the amount of additive noise ) . 5 .",
    "the matrix @xmath228 , as a sum of @xmath220 rank-1 random matrices , has nice concentration of measure properties under the assumption of theorem  [ thm : main ] .",
    "asymptotically , @xmath229 converges to @xmath230 with rate @xmath231 , which will be justified in lemma  [ lem : ay - hx ] of section  [ s : init ] ( see also  @xcite ) . note that @xmath232 if one lets @xmath233 , then @xmath234 will converge almost surely to @xmath235 under the law of large numbers and the cross term @xmath236",
    "will converge to @xmath230 .",
    "in other words , asymptotically , @xmath237 for all fixed @xmath94 .",
    "this implies that if the number of measurements is large , then @xmath89 behaves  almost like \" @xmath238 , the noiseless version of @xmath89 .",
    "this provides the key insight into analyzing the robustness of our algorithm , which is reflected in the so - called  _ robustness condition _ \" in  .",
    "moreover , the asymptotic property of @xmath239 is also seen in our main result  .",
    "suppose @xmath220 is becoming larger and larger , the effect of noise diminishes and heuristically , we might just rewrite our result as @xmath240 , which is consistent with the result without noise .",
    "we present empirical evaluation of our proposed gradient descent algorithm ( algorithm  [ agd ] ) using simulated data as well as examples from blind deconvolution problems appearing in communications and in image processing .",
    "we first investigate how many measurements are necessary in order for an algorithm to reliably recover two signals from their convolution .",
    "we compare algorithm  [ agd ] , a gradient descent algorithm for the sum of the loss function @xmath241 and the regularization term @xmath130 , with the gradient descent algorithm only applied to @xmath241 and the nuclear norm minimization proposed in @xcite .",
    "these three tested algorithms are abbreviated as _ reggrad , grad _ and _ nnm _ respectively . to make fair comparisons ,",
    "both reggrad and grad are initialized with the normalized leading singular vectors of @xmath177 , which are computed by running the power method for @xmath242 iterations . though we do not further compute the projection of @xmath181 for reggrad as stated in the third step of algorithm  [ initial ] , we emphasize that the projection can be computed efficiently as it is a linear programming on @xmath31-dimensional vectors . a careful reader may notice that in addition to the computational cost for the loss function @xmath241 , reggrad also requires to evaluate @xmath130 and its gradient in each iteration .",
    "when @xmath243 consists of the first @xmath31 columns of a unitary dft matrix , we can evaluate @xmath244 and the gradient of @xmath245 using fft .",
    "thus the additional per iteration computational cost for the regularization term @xmath130 is only @xmath246 flops .",
    "the stepsizes in both reggrad and grad are selected adaptively in each iteration via backtracking . as suggested by the theory ,",
    "the choices for @xmath146 and @xmath148 in reggrad are @xmath247 and @xmath248 .",
    "we conduct tests on random gaussian signals @xmath62 and @xmath249 with @xmath250 .",
    "the matrix @xmath65 is the first @xmath31 columns of a unitary @xmath45 dft matrix , while @xmath66 is either a gaussian random matrix or a partial hadamard matrix with randomly selected @xmath251 columns and then multiplied by a random sign matrix from the left .",
    "when @xmath57 is a gaussian random matrix , @xmath220 takes @xmath252 equal spaced values from @xmath253 to @xmath254 .",
    "when @xmath57 is a partial hadamard matrix , we only test @xmath255 with @xmath256 being integers . for each given triple @xmath257",
    ", fifty random tests are conducted .",
    "we consider an algorithm to have successfully recovered @xmath227 if it returns a matrix @xmath258 which satisfies @xmath259 we present the probability of successful recovery plotted against the number of measurements in figure  [ fig_phase ] .",
    "it can be observed that reggrad and grad have similar performance , and both of them require a significantly smaller number of measurements than nnm to achieve successful recovery of high probability .",
    "theorem  [ thm : main ] indicates that the number of measurements @xmath220 required for algorithm  [ agd ] to achieve successful recovery scales linearly with @xmath155 .",
    "we conduct numerical experiments to investigate the dependence of @xmath220 on @xmath155 empirically .",
    "the tests are conducted with @xmath155 taking on @xmath260 values @xmath261 . for each fixed @xmath155 , we choose @xmath41 to be a vector whose first @xmath155 entries are @xmath262 and the others are @xmath230 so that its incoherence is equal to @xmath155 when @xmath47 is low frequency fourier matrix",
    ". then the tests are repeated for random gaussian matrices @xmath57 and random gaussian vectors @xmath42 .",
    "the empirical probability of successful recovery on the @xmath263 plane is presented in figure  [ fig_l_mu ] , which suggests that @xmath220 does scale linearly with @xmath155 .",
    "while reggrad and grad have similar performances in the simulation when @xmath41 is a random gaussian signal ( see figure  [ fig_phase ] ) , we investigate their performances on a fixed @xmath41 with a large incoherence .",
    "the tests are conducted for @xmath264 , @xmath265 being a random gaussian signal , @xmath66 being a random gaussian matrix , and @xmath65 being a low frequency fourier matrix .",
    "the signal @xmath41 with @xmath266 is formed in the same way as in section  [ sec : simulation , lvsincoherence ] ; that is , the first @xmath267 entries of @xmath41 are one and the other entries are zero .",
    "the number of measurements @xmath220 varies from @xmath268 to @xmath269 . for each @xmath220",
    ", @xmath267 random tests are conducted .",
    "figure  [ fig_phase_fixed_h ] shows the probability of successful recovery for reggrad and grad .",
    "it can be observed that the successful recovery probability of reggrad is at least @xmath270 larger than that of grad when @xmath271 .",
    "we explore the robustness of algorithm  [ agd ] when the measurements are contaminated by additive noise .",
    "the tests are conducted with @xmath272 , @xmath273 when @xmath57 is a random gaussian matrix , and @xmath274 when @xmath57 is a partial hadamard matrix .",
    "tests with additive noise have the measurement vector @xmath3 corrupted by the vector @xmath275 where @xmath276 is standard gaussian random vector , and @xmath277 takes nine different values from @xmath278 to @xmath262 .",
    "for each @xmath277 , fifty random tests are conducted .",
    "the average reconstruction error in db plotted against the signal to noise ratio ( snr ) is presented in fig .",
    "[ fig_stability ] .",
    "first the plots clearly show the desirable linear scaling between the noise levels and the relative reconstruction errors .",
    "moreover , as desired , the relative reconstruction error decreases linearly on a @xmath279-@xmath279 scale as the number of measurements @xmath220 increases .      in order to demonstrate the effectiveness of algorithm  [ agd ] for real world applications , we first test the algorithm on a blind deconvolution problem arising in communications .",
    "indeed , blind deconvolution problems and their efficient numerical solution are expected to play an increasingly important role in connection with the emerging internet - of - things  @xcite .",
    "assume we want to transmit a signal from one place to another over a so - called time - invariant multi - path communication channel , but the receiver has no information about the channel , except its _ delay spread _",
    "( i.e. , the support of the impulse response ) .",
    "in many communication settings it is reasonable to assume that the receiver has information about the signal encoding matrix  in other words , we know the subspace @xmath57 to which @xmath42 belongs to . translating this communications jargon into mathematical terminology",
    ", this simply means that we are dealing with a blind deconvolution problem of the form  .",
    "these encoding matrices ( or so - called spreading matrices ) are often chosen to have a convenient structure that lends itself to fast computations and minimal memory requirements .",
    "one such choice is to let @xmath280 , where the @xmath46 matrix @xmath281 consists of @xmath31 ( randomly or not randomly ) chosen columns of an @xmath282 hadamard matrix , premultiplied with an @xmath282 diagonal random sign matrix @xmath283 . instead of a partial hadamard matrix",
    "we could also use a partial fourier matrix ; the resulting setup would then closely resemble an ofdm transmission format , which is part of every modern wireless communication scheme .",
    "for the signal @xmath42 we choose a so - called qpsk scheme , i.e. , each entry of @xmath284 takes a value from @xmath285 with equal probability .",
    "the actual transmitted signal is then @xmath286 . for the channel @xmath41 ( the blurring function ) we choose a real - world channel , courtesy of intel corporation",
    "here , @xmath41 represents the impulse response of a multipath time - invariant channel , it consists of 123 time samples , hence @xmath287 .",
    "otherwise , the setup follows closely that in sec .",
    "[ sec : simulation , phase ] .",
    "for comparison we also include the case when @xmath57 is a random gaussian matrix .",
    "the plots of successful recovery probability are presented in figure  [ fig_phase_real ] , which shows that algorithm  [ agd ] can successfully recover the real channel @xmath41 with high probability if @xmath288 when @xmath57 is a random gaussian matrix and if @xmath289 when @xmath57 is a partial hadamard matrix .",
    "thus our theory seems a bit pessimistic .",
    "it is gratifying to see that very little additional measurements are required compared to the number of unknowns in order to recover the transmitted signal .",
    "next , we test algorithm  [ agd ] on an image deblurring problem , inspired by  @xcite .",
    "the observed image ( figure  [ mri_blurred ] ) is a convolution of a @xmath290 mri image ( figure  [ mri_original ] ) with a motion blurring kernel ( figure  [ kernel ] ) .",
    "since the mri image is approximately sparse in the haar wavelet basis , we can assume it belongs to a low dimensional subspace ; that is , @xmath291 , where @xmath292 with @xmath293 denotes the mri image reshaped into a vector , @xmath294 represents the wavelet subspace and @xmath295 is the vector of wavelet coefficients .",
    "the blurring kernel @xmath296 is supported on a low frequency region .",
    "therefore @xmath297 , where @xmath65 is a reshaped @xmath298d low frequency fourier matrix and @xmath299 is a short vector .",
    "figure  [ mri_initial_guess ] shows the initial guess for algorithm  [ agd ] in the image domain , which is obtained by running the power method for fifty iterations .",
    "while this initial guess is clearly not a good approximation to the true solution , it suffices as a starting point for gradient descent . in the first experiment",
    ", we take @xmath34 to be the wavelet subspace corresponding to the @xmath300 largest haar wavelet coefficients of the original mri image , and we also assume the locations of the @xmath301 nonzero entries of the kernel are known . figure  [ mri_recovered_both_known ]",
    "shows the reconstructed image in this ideal setting .",
    "it can be observed that the recovered image is visually indistinguishable from the ground truth mri image . in the second experiment",
    ", we test a more realistic setting , where both the support of the mri image is the wavelet domain and the support of the kernel are not known .",
    "we take the haar wavelet transform of the blurred image ( figure  [ mri_blurred ] ) and select @xmath34 to be the wavelet subspace corresponding to the @xmath302 largest wavelet coefficients .",
    "we do not assume the exact support of the kernel is known , but assume that its support is contained in a small box region . the reconstructed image in this setting is shown in figure  [ mri_recovered_both_unknown ] . despite not knowing the subspaces exactly",
    ", algorithm  [ agd ] is still able to return a reasonable reconstruction .    yet",
    ", this second experiment also demonstrates that there is clearly room for improvement in the case when the subspaces are unknown .",
    "one natural idea to improve upon the result depicted in figure  [ mri_recovered_both_unknown ] is to include an additional total - variation penalty in the reconstruction algorithm .",
    "we leave this line of work for future research .",
    "this section is devoted to the proof of theorems  [ thm : init ] and  [ thm : main ] . since proving theorem  [ thm : main ]",
    "is a bit more involved , we briefly describe the architecture of its proof . in subsection",
    "[ ss : conditions ] we state four key conditions : the _ local regularity condition _ will allow us to show that the objective function decreases ; the _ local restricted isometry property _ enables us to transfer the decrease in the objective function to a decrease of the error between the iterates and the true solution ; the _ local smoothness condition _ yields the actual rate of convergence , and finally , the _ robustness condition _ establishes robustness of the proposed algorithm against additive noise .",
    "armed with these conditions , we will show how the three regions defined in section  [ s : algo ] characterize the convergence neighborhood of the solution , i.e. , if the initial guess is inside this neighborhood , the sequence generated via gradient descent will always stay inside this neighborhood as well . in subsections  [",
    "s : lemmata][s : init ] we justify the aforementioned four conditions , show that they are valid under the assumptions stated in theorem  [ thm : main ] , and conclude with a proof of theorem  [ thm : init ] .",
    "[ cond : rip ] the following local restricted isometry property ( rip ) for @xmath303 holds uniformly for all @xmath304    @xmath305    condition  [ cond : rip ] states that @xmath303 almost preserves the @xmath306-distance between @xmath307 over a",
    " local \" region around the ground truth @xmath308 .",
    "the proof of condition  [ cond : rip ] is given in lemma  [ lem : rip ] .",
    "[ cond : ae ] for the noise @xmath309 , with high probability there holds , @xmath310 if @xmath311 .",
    "this condition follows directly from  .",
    "it is quite essential when we analyze the behavior of algorithm  [ agd ] under gaussian noise . with those two conditions above in hand , the lower and upper bounds of @xmath89",
    "are well approximated over @xmath127 by two quadratic functions of @xmath93 , where @xmath93 is defined in  .",
    "a similar approach towards noisy matrix completion problem can be found in  @xcite .",
    "for any @xmath312 , applying condition  [ cond : rip ] to   leads to @xmath313 and similarly @xmath314 where @xmath315 because @xmath316 and @xmath317 is a pair of dual norm and @xmath318 .",
    "moreover , with the condition  [ cond : ae ] ,   and   yield the followings : @xmath319 and @xmath320    the third condition is about the regularity condition of @xmath321 , which is the key to establishing linear convergence later . the proof will be given in lemma  [ lem : reg ] .",
    "[ cond : reg ] let @xmath321 be as defined in   and @xmath322 .",
    "then there exists a _",
    "regularity constant _",
    "@xmath323 such that @xmath324_+\\ ] ] for all @xmath325 where @xmath326 with @xmath327 .",
    "in particular , in the noiseless case , i.e. , @xmath328 , we have @xmath329    besides the three regions defined in   to  , we define another region @xmath330 via @xmath331 for proof technical purposes .",
    "@xmath330 is actually the sublevel set of the nonconvex function @xmath164 .",
    "finally we introduce the last condition called _ local smoothness condition _ and its corresponding quantity @xmath218 which characterizes the choice of stepsize @xmath206 and the rate of linear convergence .    [",
    "cond : smooth ] denote @xmath332 .",
    "there exists a constant @xmath333 such that @xmath334 for all @xmath335 , i.e. , the whole segment connecting @xmath5 and @xmath336 , which is parametrized by @xmath337 , belongs to the nonconvex set @xmath338    the upper bound of @xmath218 , which scales with @xmath339 , will be given in section  [ s : smooth ] .",
    "we will show later in lemma  [ lem : induction ] that the stepsize @xmath206 is chosen to be smaller than @xmath340 hence @xmath341    [ lem : betamu ] there holds @xmath342 ; under condition  [ cond : rip ] and  [ cond : ae ] , we have @xmath343 .",
    "if @xmath344 , by the definition of @xmath345 in  , at least one component in @xmath345 exceeds @xmath346 .",
    "we have @xmath347 where @xmath348 and @xmath349 this implies @xmath350 and hence @xmath342 .",
    "+ for any @xmath351 , we have @xmath312 now . by  ,",
    "@xmath352 therefore , @xmath353 and @xmath343 .",
    "this lemma implies that the intersection of @xmath330 and the boundary of @xmath129 is empty .",
    "one might believe this suggests that @xmath354 .",
    "this may not be true .",
    "a more reasonable interpretation is that @xmath330 consists of several disconnected regions due to the non - convexity of @xmath321 , and one or several of them are contained in @xmath129 .",
    "[ lem : line_section ] denote @xmath355 and @xmath356 .",
    "let @xmath357 .",
    "if @xmath358 and @xmath359 for all @xmath360 $ ] , we have @xmath361 .",
    "let us prove the claim by contradiction .",
    "if @xmath362 , since @xmath358 , there exists @xmath363 for some @xmath364 $ ] , such that @xmath365 .",
    "however , since @xmath366 , by , we have @xmath367 .",
    "this leads to a contradiction .",
    "lemma  [ lem : line_section ] tells us that if one line segment is completely inside @xmath330 with one end point in @xmath129 , then this whole line segment lies in @xmath368    [ lem : induction ] let the stepsize @xmath216 , @xmath369 and @xmath218 be the constant defined in  .",
    "then , as long as @xmath370 , we have @xmath371 and @xmath372    it suffices to prove . if @xmath373 , then @xmath374 , which implies directly",
    "so we only consider the case when @xmath375 .",
    "define the function @xmath376 then @xmath377 since @xmath378 is a real - valued function with complex variables ( see   for details ) . by the definition of derivatives",
    ", we know there exists @xmath379 , such that @xmath380 for all @xmath381 .",
    "now we will first prove that @xmath382 for all @xmath383 by contradiction .",
    "assume there exists some @xmath384 $ ] such that @xmath385 .",
    "then there exists @xmath386 , such that @xmath387 and @xmath380 for all @xmath388 , since @xmath378 is a continuous function .",
    "this implies @xmath389 since @xmath390 for @xmath391 by and the assumption @xmath392 , we have @xmath393 then , by using the modified descent lemma ( lemma  [ lem : dsl ] ) , @xmath394 where the final inequality is due to @xmath395 , @xmath396 and @xmath375 .",
    "this contradicts @xmath397 .",
    "therefore , there holds @xmath382 for all @xmath398 .",
    "similarly , we can prove @xmath399 which implies @xmath400 .",
    "again , by using lemma  [ lem : dsl ] we can prove @xmath401 where the final inequality is due to @xmath402 .",
    "we conclude this subsection by proving theorem  [ thm : main ] under the _ local regularity condition _ , the _ local rip condition _ , the _ robustness condition _ , and the _ local smoothness condition_. the next subsections are devoted to justifying these conditions and showing that they hold under the assumptions of theorem  [ thm : main ] .    [ * of theorem  [ thm : main ] * ] suppose that the initial guess @xmath403 , we have @xmath404 .",
    "this holds , because @xmath405 where @xmath406 , @xmath407 and @xmath408 therefore @xmath409 for all @xmath85 and @xmath410 since @xmath411 ,   combined with @xmath412 imply that @xmath413 and hence @xmath414 denote @xmath415 combining lemma  [ lem : induction ] by choosing @xmath216 with condition  [ cond : reg ] , we have @xmath416_+\\ ] ] with @xmath417 , @xmath327 and @xmath418 for all @xmath419 obviously , the inequality above implies @xmath420_+ , \\ ] ] and by monotonicity of @xmath421 , there holds @xmath422_+   \\leq ( 1 - \\eta\\omega )   \\left [   { \\widetilde{f}}({\\boldsymbol{z}}_t )   - c \\right]_+ .\\ ] ]    therefore , by induction , we have @xmath423_+ & \\leq & \\left(1 - \\eta\\omega\\right)^t \\left [ { \\widetilde{f}}({\\boldsymbol{z}}_0 ) - c\\right]_+ \\leq \\frac{1}{3 } ( 1 - \\eta\\omega)^{t }   { { \\varepsilon}}^2 d_0 ^ 2\\end{aligned}\\ ] ] where @xmath424 and hence @xmath425_+ \\leq    \\left [ \\frac{1}{3}{{\\varepsilon}}^2 d_0 ^ 2 - a\\|{\\mathcal{a}}^*({\\boldsymbol{e}})\\|^2 \\right]_+ \\leq \\frac{1}{3}{{\\varepsilon}}^2 d_0 ^ 2.$ ] now we can conclude that @xmath426_+$ ] converges to @xmath230 geometrically . note that over @xmath127 , @xmath427 where @xmath428 , @xmath429 is defined in   and @xmath430 .",
    "there holds @xmath431_+ \\leq \\frac{1}{3}(1 - \\eta\\omega)^t   { { \\varepsilon}}^2 d_0 ^ 2\\ ] ] and equivalently , @xmath432 solving the inequality above for @xmath433 , we have @xmath434 let @xmath435 , @xmath436 by   and triangle inequality , we immediately conclude that @xmath437    now we derive the upper bound for @xmath438 and @xmath439 due to symmetry , it suffices to consider @xmath438 .",
    "the bound follows from standard linear algebra arguments : @xmath440 where the second equality uses @xmath441      this subsection introduces several lemmata , especially lemma  [ lem : orth_decomp ] ,  [ lem : ripu ] and  [ lem : key ] , which are central for justifying conditions  [ cond : rip ] and  [ cond : reg ] .",
    "after that , we will prove the _ local rip condition _ in lemma  [ lem : rip ] based on those three lemmata .",
    "we start with defining a linear space @xmath442 , which contains @xmath308 , via @xmath443 its orthogonal complement is given by @xmath444 denote @xmath445 to be the projection operator from @xmath17 onto @xmath442 .    for any @xmath446 and @xmath447 , there are unique orthogonal decompositions @xmath448 where @xmath449 and @xmath450 .",
    "more precisely , @xmath451 and @xmath452 we thereby have the following matrix orthogonal decomposition @xmath453 where the first three components are in @xmath442 while @xmath454 .",
    "[ lem : orth_decomp ] recall that @xmath455 .",
    "if @xmath456 , we have the following useful bounds @xmath457 and @xmath458 moreover , if @xmath459 and @xmath460 , we have @xmath461 .",
    "this lemma is actually a simple version of singular value / vector perturbation .",
    "it says that if @xmath462 is of @xmath463 , then the individual vectors @xmath94 are also close to @xmath67 , with the error of order @xmath464    the equality implies that @xmath465 , so there holds @xmath466 . since @xmath467 ,",
    "by , we have @xmath468 this implies that @xmath469 on the other hand , @xmath470 the above two inequalities imply that @xmath471 , and similarly we have @xmath472 .",
    "the equality implies that @xmath473 and hence @xmath474",
    ". moreover , also implies @xmath475 which yields @xmath476 .",
    "+   + if @xmath459 and @xmath460 , there holds @xmath477 .",
    "then @xmath478 where @xmath117 .    in the following ,",
    "we introduce and prove a series of local and global properties of @xmath303 :    [ lem : a - upbd ] for @xmath303 defined in  , @xmath479 with probability at least @xmath210    [ lem : ripu ] let @xmath303 be the operator defined in  , then on an event @xmath480 with probability at least @xmath193 , @xmath303 restricted on @xmath442 is well - conditioned , i.e. , @xmath481 where @xmath445 is the projection operator from @xmath17 onto @xmath442 , provided @xmath482 .",
    "now we introduce a property of @xmath303 when restricted on rank - one matrices .",
    "[ lem : key ] on an event @xmath483 with probability at least @xmath484 , we have @xmath485 uniformly for any @xmath486 and @xmath487 , provided @xmath488 .    due to the homogeneity , without loss of generality",
    "we can assume @xmath489 .",
    "define @xmath490 it suffices to prove that @xmath491 uniformly for all @xmath492 with high probability , where @xmath493 is the unit sphere in @xmath494 for fixed @xmath492 , notice that @xmath495 is the sum of subexponential variables with expectation @xmath496 . for any generalized @xmath497 variable @xmath498 satisfies @xmath499 where",
    "@xmath500 are i.i.d . @xmath501",
    "random variables and @xmath502 . here",
    "we set @xmath503 , @xmath504 and @xmath505 .",
    "therefore , @xmath506 and we have @xmath507 applying   and setting @xmath508 there holds @xmath509 that is , @xmath510 with probability at least @xmath511 .",
    "we define @xmath512 and @xmath513 as @xmath514-nets of @xmath515 and @xmath516 , respectively . then , @xmath517 and @xmath518 follow from the covering numbers of the sphere ( lemma 5.2 in  @xcite ) .    by taking the union bounds over @xmath519 we have @xmath520 holds uniformly for all @xmath521 with probability at least @xmath522 our goal is to show that @xmath491 uniformly for all @xmath492 with the same probability . for any @xmath492",
    ", we can find its closest @xmath523 satisfying @xmath524 and @xmath525 .",
    "by , with probability at least @xmath193 , we have @xmath526 .",
    "then straightforward calculation gives @xmath527 where the first inequality is due to @xmath528 for any @xmath529 , and the second inequality is due to @xmath530 for any @xmath531 .",
    "similarly , @xmath532 therefore , if @xmath533 , there holds @xmath534 therefore , if @xmath535 with @xmath195 reasonably large and @xmath196 , we have @xmath536 and @xmath491 uniformly for all @xmath492 with probability at least @xmath537 .    finally , we introduce a local rip property of @xmath303 conditioned on the event @xmath538 , where @xmath480 and @xmath483 are defined in lemma  [ lem : ripu ] and lemma  [ lem : key ]    [ lem : rip ] over @xmath127 with @xmath138 and @xmath539 , the following rip type of property holds for @xmath303 : @xmath540 provided @xmath541 for some numerical constant @xmath213 and conditioned on @xmath542    let @xmath543 , and @xmath544 where @xmath545 by , we have @xmath546 and hence @xmath547 since @xmath548 , by , we have @xmath549 by , we have @xmath550 by , we have @xmath551 , @xmath552 , @xmath553 , and @xmath461 . by substituting all those estimations into",
    ", it ends up with @xmath554 where @xmath555 is a numerical constant and @xmath556 . combining and together with @xmath213 sufficiently large",
    ", numerical computation gives @xmath557 for all @xmath312 given @xmath558 .      in this subsection",
    ", we will prove condition  [ cond : reg ] . throughout this section , we assume @xmath480 and @xmath483 holds where @xmath480 and @xmath483 are mentioned in lemma  [ lem : ripu ] and lemma  [ lem : key ] . for all @xmath559 , consider @xmath560 and @xmath561 defined in and",
    "let @xmath562 where @xmath563 with @xmath564 .",
    "the particular form of @xmath565 serves primarily for proving the local regularity condition of @xmath133 , which will be evident in lemma  [ lem : regg ] .",
    "the following lemma gives bounds of @xmath566 and @xmath567 .",
    "+    [ lem : dxdh ] for all @xmath559 with @xmath539 , there holds @xmath568 , @xmath569 , and @xmath570 .",
    "moreover , if we assume @xmath571 additionally , we have @xmath572 .",
    "we first prove that @xmath568 , @xmath569 , and @xmath570 : + case 1 : @xmath573 and @xmath574 . in this case , we have @xmath575 first , notice that @xmath576 and @xmath577 . by , we have @xmath578 secondly , we estimate @xmath579 note that @xmath580 . by @xmath573",
    ", we have @xmath581 .",
    "by @xmath582 , we get @xmath583 . by",
    ", we have @xmath584 , so @xmath585 where @xmath586 moreover , by , we have @xmath587 .",
    "then we have @xmath588 finally , gives @xmath476 and @xmath589 .",
    "combining   and  , we have @xmath590 where @xmath591 .   + case 2 : @xmath592 and @xmath593 . in this case",
    ", we have @xmath594 by the symmetry of @xmath595 , we can prove @xmath596 , @xmath597 moreover , we can prove @xmath568 , @xmath598 and @xmath599 .",
    "+   + next , under the additional assumption @xmath571 , we now prove @xmath600 : + case 1 : @xmath573 and @xmath574 . by gives",
    "@xmath589 , which implies @xmath601 case 2 : @xmath592 and @xmath593 .",
    "notice that in this case we have @xmath596 , so @xmath602 .",
    "therefore @xmath603    [ lem : regf ] for any @xmath325 with @xmath539 , the following inequality holds uniformly : @xmath604 provided @xmath541 for some numerical constant @xmath213 .    in this section , define @xmath605 and @xmath606 as @xmath607 which gives @xmath608 notice that generally @xmath609 does not hold . recall that @xmath610^ * { \\boldsymbol{h}}.\\ ] ] define @xmath611 and we have @xmath612 . since @xmath613 where @xmath614 by the cauchy - schwarz inequality",
    ", @xmath615 has the lower bound @xmath616 in the following , we will give an upper bound for @xmath617 and a lower bound for @xmath618 .",
    "upper bound for @xmath617 : by and , we have @xmath619 for some numerical constant @xmath620 . then by @xmath621 and letting @xmath556 for a sufficiently large numerical constant @xmath213 , there holds @xmath622    lower bound for @xmath618 : by , we have @xmath623 and therefore @xmath624 if @xmath539 . since @xmath548 , by , there holds @xmath625 with the upper bound of @xmath626 in  , the lower bound of @xmath627 in , and  , we finally arrive at @xmath628    now let us give a lower bound for @xmath629 , @xmath630 where @xmath316 and @xmath317 are a pair of dual norms and @xmath631 if @xmath632 combining the estimation of @xmath615 and @xmath629 above leads to @xmath633 as we desired .",
    "[ lem : regg ] for any @xmath634 with @xmath539 and @xmath143 , the following inequality holds uniformly @xmath635 where @xmath636    recall that @xmath637 . using the wirtinger derivative of @xmath345 in   and",
    ", we have @xmath638 where @xmath639 and @xmath640 we will give lower bounds for @xmath641 , @xmath642 and @xmath643 for two cases .",
    "+    [ [ case-1 ] ] case 1 + + + + + +    @xmath573 and @xmath574 .",
    "[ [ lower - bound - of - h_1 ] ] lower bound of @xmath641 : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    notice that @xmath644 we have @xmath645 , which implies that @xmath646 .",
    "we claim that @xmath647 in fact , if @xmath648 , we get @xmath649 ; if @xmath650 , we get straightforwardly .",
    "[ [ lower - bound - of - h_2 ] ] lower bound of @xmath642 : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the assumption @xmath573 gives @xmath651 which implies that @xmath652    [ [ lower - bound - of - h_3 ] ] lower bound of @xmath643 : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    when @xmath653 , @xmath654 when @xmath655 , by , there holds @xmath589 .",
    "then by @xmath117 , we have @xmath656 where @xmath657",
    "this implies that @xmath658 so we always have @xmath659    [ [ case-2 ] ] case 2 : + + + + + + +    @xmath592 and @xmath593 .    [ [ lower - bound - of - h_1 - 1 ] ] lower bound of @xmath641 : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the assumption @xmath592 gives @xmath660 which implies that @xmath661    [ [ lower - bound - of - h_2 - 1 ] ] lower bound of @xmath642 : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    notice that @xmath662 we have @xmath663 , which implies that @xmath664 .",
    "we claim that @xmath665 in fact , if @xmath666 , we get @xmath667 ; if @xmath668 , we get straightforwardly .    [ [ lower - bound - of - h_3 - 1 ] ] lower bound of @xmath643 : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    when @xmath669 , @xmath670 when @xmath671 , by , there hold @xmath672 and @xmath589 , which implies that @xmath673 by @xmath117 and @xmath674 , similarly we have @xmath675 this implies that for @xmath85 , @xmath676 so we always have @xmath677 to sum up the two cases , we have @xmath678 where @xmath145 and it implies .    [ lem : reg ]",
    "let @xmath164 be as defined in  , then there exists a positive constant @xmath679 such that @xmath680_+\\ ] ] with @xmath681 and @xmath682 for all @xmath325 .",
    "here we set @xmath636    following from lemma  [ lem : regf ] and lemma  [ lem : regg ] , we have @xmath683 for @xmath684 or @xmath685 and @xmath686 where @xmath687 and @xmath137 . adding them together gives @xmath688 where both @xmath689 and @xmath690 are bounded by @xmath691 in lemma  [ lem : dxdh ] .",
    "note that @xmath692_+ } \\leq \\sqrt { 2\\sqrt{2 } \\|{\\mathcal{a}}^*({\\boldsymbol{e}})\\| \\delta d_0 } \\leq \\frac{\\sqrt{5}\\delta d_0}{4 } + \\frac{4}{\\sqrt{5}}\\|{\\mathcal{a}}^*({\\boldsymbol{e}})\\|.\\ ] ] dividing both sides of   by @xmath693 , we obtain @xmath694 the local rip condition implies @xmath695 and hence @xmath696 , where @xmath429 is defined in  . combining the equation above and  , @xmath697_+ }",
    "+ \\sqrt{g({\\boldsymbol{h } } , { \\boldsymbol{x}})}\\right ) \\\\ & & + \\frac{\\sqrt{5}\\delta d_0}{4 } - \\left ( \\frac{\\sqrt{5}\\delta d_0}{4 } + \\frac{4}{\\sqrt{5}}\\|{\\mathcal{a}}^*({\\boldsymbol{e}})\\|\\right)\\big ] - 2\\|{\\mathcal{a}}^*({\\boldsymbol{e}})\\| \\\\",
    "& \\geq & \\frac{1}{6\\sqrt{5 } } \\left [ \\sqrt { \\left[{\\widetilde{f}}({\\boldsymbol{h } } , { \\boldsymbol{x } } ) - \\|{\\boldsymbol{e}}\\|^2\\right]_+ } -   29\\|{\\mathcal{a}}^*({\\boldsymbol{e}})\\|\\right]\\end{aligned}\\ ] ] where @xmath698_+ + g({\\boldsymbol{h } } , { \\boldsymbol{x}})$ ] follows from definition and  .",
    "finally , we have @xmath699_+ } - 29 \\|{\\mathcal{a}}^*({\\boldsymbol{e}})\\|\\right]_+^2\\ ] ] for all @xmath700 for any nonnegative fixed real numbers @xmath701 and @xmath702 , we have @xmath703_+ + b \\geq \\sqrt{(x - a)_+}\\ ] ] and it implies @xmath704_+^2 + b^2 ) \\longrightarrow [ \\sqrt{(x - a)_+ } - b ] _ + ^2   \\geq \\frac{(x - a)_+}{2 } - b^2.\\ ] ]    therefore , by setting @xmath705 and @xmath706 , there holds @xmath707_+ \\\\ & \\geq & \\frac{d_0}{5000 } \\left [ { \\widetilde{f}}({\\boldsymbol{h } } , { \\boldsymbol{x } } ) - ( \\|{\\boldsymbol{e}}\\|^2 + 1700 \\|{\\mathcal{a}}^*({\\boldsymbol{e}})\\|^2 ) \\right]_+.\\end{aligned}\\ ] ]      for any @xmath708 and @xmath709 such that @xmath710 , there holds @xmath711 with @xmath712\\ ] ] where @xmath348 and @xmath713 holds with probability at least @xmath193 from lemma  [ lem : a - upbd ] .",
    "in particular , @xmath714 and @xmath715 follows from @xmath716 and  .",
    "therefore , @xmath218 can be simplified into @xmath717 by choosing @xmath718    by , we have @xmath719 .",
    "note that @xmath720 where @xmath721 and @xmath722,\\quad \\nabla g_{{\\boldsymbol{x } } } = \\frac{\\rho}{2d}g'_0\\left ( \\frac{\\|{\\boldsymbol{x}}\\|^2}{2d}\\right ) { \\boldsymbol{x}}.\\ ] ]    [ [ step-1 ] ] step 1 : + + + + + + +    we estimate the upper bound of @xmath723 .",
    "a straightforward calculation gives @xmath724 note that @xmath725 directly implies @xmath726 where @xmath727 moreover , @xmath728 implies @xmath729 combined with @xmath730 and @xmath731 , we have @xmath732    [ [ step-2 ] ] step 2 : + + + + + + +    we estimate the upper bound of @xmath733 . due to the symmetry between @xmath734 and @xmath735 , we have , @xmath736    [ [ step-3 ] ] step 3 : + + + + + + +    we estimate the upper bound of @xmath737 .",
    "notice that @xmath738 , which implies that for any @xmath739 , there holds @xmath740 is not differentiable at @xmath741 .",
    "therefore , by  , it is easy to show that @xmath742 where @xmath743 therefore , by @xmath725 , we have @xmath744    [ [ step-4 ] ] step 4 : + + + + + + +    we estimate the upper bound of @xmath745 .",
    "denote @xmath746   \\\\ & + \\frac{\\rho l}{8d\\mu^2 } \\sum_{l=1}^l \\left[g'_0\\left(\\frac{l|{\\boldsymbol{b}}_l^*({\\boldsymbol{h}}+ { \\boldsymbol{u}})|^2}{8d\\mu^2}\\right ) { \\boldsymbol{b}}_l^*({\\boldsymbol{h}}+ { \\boldsymbol{u } } ) - g'_0\\left(\\frac{l|{\\boldsymbol{b}}_l^*{\\boldsymbol{h}}|^2}{8d\\mu^2}\\right ) { \\boldsymbol{b}}_l^*{\\boldsymbol{h}}\\right]{\\boldsymbol{b}}_l \\\\ & : = { \\bm{j}}_1 + { \\bm{j}}_2.\\end{aligned}\\ ] ] similar to , we have @xmath747 now we control @xmath748 . since @xmath749 , we have @xmath750 and @xmath751 where both @xmath752 and @xmath753 are bounded by @xmath754 .",
    "let @xmath755 be @xmath756 applying   and   leads to @xmath757 since @xmath758 and @xmath759 , there holds @xmath760 this implies that @xmath761 in summary , by combining  , , , , and , we conclude that @xmath762 with @xmath763 , there holds @xmath764 \\|{\\boldsymbol{w}}\\|.\\ ] ]      this section is devoted to justifying the validity of the _ robustness condition _ and to proving theorem  [ thm : init ] , i.e. , establishing the fact that algorithm  [ initial ] constructs an initial guess @xmath765    [ lem : ay - hx ] for @xmath766 , there holds @xmath767 with probability at least @xmath193 if @xmath768 moreover , @xmath769 with probability at least @xmath193",
    "if @xmath770 in particular , we fix @xmath771 and then robustness condition  [ cond : ae ] holds , i.e. , @xmath772 .    in this proof , we can assume @xmath773 and @xmath774 , without loss of generality .",
    "first note that @xmath775 we will use the matrix bernstein inequality to show that @xmath776 by definition of @xmath303 and @xmath777 in   and  , @xmath778 = \\sum_{l=1}^l { \\mathcal{z}}_l,\\ ] ] where @xmath779 and @xmath780 . in order to apply bernstein inequality",
    ", we need to estimate both the exponential norm @xmath781 and the variance .",
    "@xmath782 for some constant @xmath213 . here ,   of lemma",
    "[ lem : multiple1 ] gives @xmath783 and @xmath784 follows from   where both @xmath785 and @xmath786 are sub - gaussian random variables .",
    "now we give an upper bound of @xmath787 .",
    "@xmath788 where @xmath789 follows from   and @xmath790 @xmath791 \\right \\| \\\\ & & + \\left\\| \\operatorname{e}\\sum_{l=1}^l e_l^2 \\|{\\boldsymbol{b}}_l\\|^2 { \\boldsymbol{a}}_l{\\boldsymbol{a}}_l^ * \\right\\| \\\\ & \\leq & c\\frac{k}{l } \\left\\| \\sum_{l=1}^l |{\\boldsymbol{b}}_l^*{\\boldsymbol{h}}_0|^2 { \\boldsymbol{i}}_n\\right\\| + c\\frac{\\sigma^2k}{l^2 } \\left\\| \\sum_{l=1}^l \\operatorname{e}({\\boldsymbol{a}}_l{\\boldsymbol{a}}_l^ * ) \\right\\| = c\\frac{(\\sigma^2 + 1 ) k}{l}\\end{aligned}\\ ] ] where we have used the fact that @xmath792 therefore , we now have the variance @xmath793 bounded by @xmath794 we apply bernstein inequality  , and obtain @xmath795 with probability at least @xmath193 if @xmath796    regarding the estimation of @xmath229 , the same calculations immediately give @xmath797 and @xmath798 \\right\\| , \\left\\| \\operatorname{e}\\left [   ( { \\mathcal{a}}^*({\\boldsymbol{e}}))^*{\\mathcal{a}}^*({\\boldsymbol{e}})\\right ] \\right\\| \\right\\ } \\leq \\frac{\\sigma^2\\max\\{k , n\\}}{l}.\\ ] ]    applying bernstein inequality   again , we get @xmath799 with probability at least @xmath193 if @xmath800    lemma  [ lem : ay - hx ] lays the foundation for the initialization procedure , which says that with enough measurements , the initialization guess via spectral method can be quite close to the ground truth . before moving to the proof of theorem  [ thm : init ] , we introduce a property about the projection onto a closed convex set .",
    "[ lem : kmc ] let @xmath801 be a closed nonempty convex set .",
    "there holds @xmath802 where @xmath803 is the projection of @xmath5 onto @xmath804 .",
    "this is a direct result from theorem 2.8 in  @xcite , which is also called _",
    "kolmogorov criterion_. now we present the proof of theorem  [ thm : init ] .    [ * of theorem  [ thm : init ] * ] without loss of generality , we again set @xmath773 and by definition , all @xmath805 @xmath42 , @xmath806 and @xmath807 are of unit norm .",
    "also we set @xmath808 by applying the triangle inequality to  , it is easy to see that @xmath809 which gives @xmath810 it is easier to get an upper bound for @xmath811 here , i.e. , @xmath812 which implies @xmath813 the estimation of @xmath814 involves lemma  [ lem : kmc ] . in our case , @xmath814 is the minimizer to the function @xmath815 over @xmath816 therefore , @xmath814 is actually the projection of @xmath817 onto @xmath804 .",
    "note that @xmath818 implies @xmath819 and hence @xmath820 moreover , @xmath814 yields    @xmath821    for all @xmath822 because the cross term is nonnegative due to lemma  [ lem : kmc ] .",
    "let @xmath823 and we get @xmath824    so far , we have already shown that @xmath825 and @xmath826 .",
    "now we will show that @xmath827    first note that @xmath828 for all @xmath829 , which follows from weyl s inequality  @xcite for singular values where @xmath830 denotes the @xmath831-th largest singular value of @xmath177 .",
    "hence there holds @xmath832    on the other hand , @xmath833 where the second equation follows from @xmath834 and @xmath835 .",
    "therefore , we have @xmath836 where @xmath837 . if we substitute @xmath838 by @xmath839 into  , @xmath840 where @xmath839 follows from @xmath841 . combining   and   leads to @xmath842",
    "now we are ready to estimate @xmath843 as follows , @xmath844 where @xmath845 , @xmath846 and @xmath847 follows from  .",
    "[ lem : dsl ] if @xmath848 is a continuously differentiable real - valued function with two complex variables @xmath5 and @xmath849 , ( for simplicity , we just denote @xmath848 by @xmath850 and keep in the mind that @xmath850 only assumes real values ) for @xmath851 .",
    "suppose that there exists a constant @xmath218 such that @xmath852 for all @xmath853 and @xmath854 such that @xmath855 and @xmath856 .",
    "then @xmath857 where @xmath858 is the complex conjugate of @xmath859 .",
    "the proof simply follows from proof of descent lemma ( proposition a.24 in  @xcite ) .",
    "however it is slightly different since we are dealing with complex variables .",
    "denote @xmath860 since @xmath848 is a continuously differentiable function , we apply the chain rule @xmath861 then by the fundamental theorem of calculus , @xmath862      the key concentration inequality we use throughout our paper comes from proposition 2 in  @xcite .",
    "[ berngaussian ] consider a finite sequence of @xmath863 of independent centered random matrices with dimension @xmath864 .",
    "assume that @xmath865 where the norm @xmath866 of a matrix is defined as @xmath867 \\leq 2 \\}.\\ ] ] and introduce the random matrix @xmath868 compute the variance parameter @xmath869 then for all @xmath870 , we have the tail bound on the operator norm of @xmath871 ,    @xmath872    with probability at least @xmath873 where @xmath620 is an absolute constant .    for convenience",
    "we also collect some results used throughout the proofs .",
    "[ lem:6jb ] let @xmath874 be a random variable which obeys @xmath875 , then @xmath876 which is proven in lemma 2.2.1 in  @xcite .",
    "moreover , it is easy to verify that for a scalar @xmath877 @xmath878    [ lem : multiple1 ] let @xmath879 , then @xmath880 and @xmath881 and @xmath882}= n{\\boldsymbol{i}}_n.\\ ] ] let @xmath883 be any deterministic vector , then the following properties hold    @xmath884    @xmath885}= \\|{\\boldsymbol{q}}\\|^2 { \\boldsymbol{i}}_n.\\ ] ]    let @xmath886 be a complex gaussian random vector in @xmath887 , independent of @xmath888 , then @xmath889",
    "s.  ling , t.  strohmer , and k.  wei acknowledge support from the nsf via grant dtra - dms 1322393",
    ".                                            j.  d. lee , b.  recht , n.  srebro , j.  tropp , and r.  r. salakhutdinov .",
    "practical large - scale optimization for max - norm regularization . in _ advances in neural information processing systems _ , pages 12971305 , 2010 .",
    "r.  vershynin .",
    "introduction to the non - asymptotic analysis of random matrices . in y.  c. eldar and g.  kutyniok , editors , _ compressed sensing : theory and applications _ , chapter  5 . cambridge university press , 2012 ."
  ],
  "abstract_text": [
    "<S> we study the question of reconstructing two signals @xmath0 and @xmath1 from their convolution @xmath2 . </S>",
    "<S> this problem , known as _ </S>",
    "<S> blind deconvolution _ , pervades many areas of science and technology , including astronomy , medical imaging , optics , and wireless communications . </S>",
    "<S> a key challenge of this intricate non - convex optimization problem is that it might exhibit many local minima . </S>",
    "<S> we present an efficient numerical algorithm that is guaranteed to recover the exact solution , when the number of measurements is ( up to log - factors ) slightly larger than the information - theoretical minimum , and under reasonable conditions on @xmath0 and @xmath1 . </S>",
    "<S> the proposed regularized gradient descent algorithm converges at a geometric rate and is provably robust in the presence of noise . to the best of our knowledge , </S>",
    "<S> our algorithm is the first blind deconvolution algorithm that is numerically efficient , robust against noise , and comes with rigorous recovery guarantees under certain subspace conditions . </S>",
    "<S> moreover , numerical experiments do not only provide empirical verification of our theory , but they also demonstrate that our method yields excellent performance even in situations beyond our theoretical framework . </S>"
  ]
}