{
  "article_text": [
    "particle filtering , as it applies here , is a powerful technique for ( recursive ) estimation and inference in nonlinear dynamical state - space models subject to stochastic influence . in theory ,",
    "the state of an underlying stochastic dynamical system can be recursively estimated by composing the posterior probability of the state conditioned on the random observations as they become available and using a given initial prior and the stochastic dynamical system model .",
    "this process is known as recursive bayesian filtering @xcite and it is generally intractable in practice @xcite .",
    "the particle filter is an approximation of the bayesian filter that employs random sampling to represent the posterior where such samples ( or particles ) are propagated in practice through ( sequential ) importance sampling @xcite that attempts to capture the dynamics of the underlying system as well as the likelihood model on the observations .",
    "a resampling step inserted into the recursion is also crucial to avoid sample degeneracy and control the variance over time . to date , the particle filter has been well studied and we point to the still relevant early work @xcite as well as the comprehensive coverage in @xcite for further background .    the convergence of the particle filter in time has been well considered @xcite . by convergence in time , we mean that one can show that the approximation error , with respect to an idealised bayesian filter , and due mostly to the random sampling , can be controlled through time @xcite .",
    "one can even show the filter approximation error due to the sampling approximation remains bounded uniformly in time @xcite .",
    "such results provide significant grounding for the application of particle filtering in numerous application domains @xcite .    despite substantial analysis justifying use of the particle filter in numerous applications of interest ,",
    "a limitation to date surrounds application of the particle filter in high - dimensional estimation and inference problems @xcite .",
    "the limiting factor is computational complexity .",
    "analysis in @xcite suggests that the particle filter approximation error is exponential in the dimension of the underlying ( measurement ) model while the same error is controlled by the number of samples according to something like the inverse square root .",
    "this relationship is clearly exposited in @xcite .",
    "the conclusion is simply that , an enormous number of particles ( exponential in the dimension ) must be maintained if one is to control the estimation error at a reasonable level when applying standard particle filter implementations in a high - dimensional dynamical estimation problem .",
    "an enormous number of particles means a heavy computational burden which is often so large it is prohibitive .",
    "the good news is that recent studies @xcite imply that high - dimensional particle filtering may be feasible in particular applications and/or if one is willing to accept a degree of systematic bias . in @xcite ,",
    "the particle filter is applied in a static setting where the objective is to sample from some high - dimensional target distribution . in this case , through a sequence of intermediate and simpler distributions , it is shown that the particle filter will converge to a sampled representation of the target distribution with a typical monte carlo error ( inverse in the number of particles ) given a complexity on the order of the dimension squared .",
    "although @xcite deals only , in essence , with a static problem of sampling from a fixed target distribution , the analysis introduces a novel way of thinking about high - dimensional particle filtering which may carry over to dynamic filtering problems .",
    "related work appears in @xcite .",
    "rebeschini and van handel in @xcite consider particle filtering in large - scale dynamic random fields .",
    "they introduce a simple blocked particle filter which localises the filter to the blocks in a partition of the random field . here",
    ", localisation means that the particle filtering prior distribution at each time is independently updated / corrected within each block through application of the observations locally conditioned on that block .",
    "the posterior over each block is independent across blocks and the posterior over the entire field is just the product of each blocked posterior . the real contribution of @xcite is a descriptive and technical analysis that shows the error introduced due to the localisation procedure can be readily controlled if the dynamics of the random field at each site are only locally dependent on those sites within close proximity .",
    "the standard sampling approximation error is shown to be exponential in only the size of the individual blocks .",
    "the number of samples / particles controls the sampling approximation error at the typical rate while the error due to the localisation process is a systematic bias that can only be controlled through an increase in the block size .",
    "since each block is updated independently , parallel implementation is readily applicable and the computational burden may be alleviated , albeit this remains to be seen in practice . while the results of @xcite are at the proof - of - concept stage , the idea is incredibly powerful and it provides the entire motivation for the study herein .",
    "it was just noted that the error due to the localisation , or blocking , procedure discussed in @xcite is systematic and controlled only by the block size . actually , this is an exaggeration and the stated result is significantly more promising . to analyse the effect of the blocking procedure ,",
    "the authors introduce a tool termed the decay of correlations property which captures a spatial notion of stability , i.e. which measures how quickly dependence between sites in the field decays as a function of the distance between those sites .",
    "they show that if a suitable decay of correlations exists , then the error at any site in the field introduced due to the blocking procedure alone is dependent primarily on that site s distance to the border of the block . in other words , the systematic bias introduced due to the blocking operator is small at those sites in the field far removed from the block borders , large on the borders and varies in between .",
    "the blocking bias is not spatially uniform .",
    "the error due to the sampling procedure inherent to all particle filters is still exponential in the block size and controlled by the number of particles .",
    "the sampling approximation error implies one should not seek excessively large block sizes . on the other hand ,",
    "small blocks imply the spatial inhomogeneity of the total error caused by the blocking bias is exasperated . ideally , one would like an algorithm that maintains a spatially homogeneous error within each block .",
    "one can then focus on considering , in detail , the block - size vs. error tradeoff .",
    "the authors in @xcite devote much discussion to the spatial inhomogeneity of the blocking bias and its significance .",
    "this work details a simple , yet relevant , algorithmic adjustment to the blocked particle filter introduced in @xcite .",
    "the idea is to spatially average the bias caused by the blocking procedure by considering an adaptive sequence of partitions over the random field instead of a single partition .",
    "this adaptive blocking procedure has the effect of spatially smoothing the error caused by any single application of the blocking / localisation operation . in a certain class of random fields",
    ", this smoothing effect leads to a completely spatially homogeneous error bound .",
    "that is , the bound on the total filtering error at any site in the field is completely independent of the location of that site . in more general random fields ,",
    "this spatial averaging effect can lead to a significant reduction in the spatial inhomogeneity of the particle filter error bounds when compared with @xcite .",
    "this smoothing effect is of practical relevance in those cases in which the blocks should be kept relatively small for computational reasons or in which the dynamics of the random field are less than trivially localised .",
    "the results presented here are largely at the same proof - of - concept level as those presented in the motivating paper @xcite .",
    "the stated results in both studies are of a quantitative nature that is far from optimal .",
    "nevertheless , the applicability of the particular algorithms is likely far less restricted than a strict reading of the results would suggest ; indeed this is seemingly also true for the celebrated time - uniform particle filter convergence results @xcite .",
    "it is with this applicability in mind that the algorithmic extensions considered herein are proposed .",
    "the extensions considered are conceptually simple , easy to implement , and do not generally add to the computational burden of the algorithm . beyond the important spatial smoothing effect of the proposed filter ,",
    "allowing for ( adapting ) multiple partitions of the field may also provide algorithmic robustness in those cases in which the random field is time - varying etc as the partitions can be adapted online , or it may allow one to adaptively focus computation on certain locations of interest for periods of time etc .",
    "other advantages of partition adaptation are envisioned .",
    "the paper is organised as follows . in section 2",
    "we introduce the model and problem setup . in section 3",
    "we introduce the bayesian filtering framework , the ( standard ) particle filtering algorithm and we introduce the adaptively blocked particle filter for high - dimensional estimation problems . in section 4 we state the main result , which consists of a time - average and spatially smoothed total error bound on the adaptively blocked particle filter approximation to the true bayesian nonlinear filter . in section 4",
    "we also note that the bound may well be completely spatially - uniform and we outline the strategy for proving this result . in section 5",
    "we explore in more detail the error bound and the spatial smoothing effect of adaptively sequencing through partitions in the blocked particle filter . in section 5",
    "we discuss , in more detail , the class of random fields and the sequence of partitions that may lead to complete spatial uniformity in the error bound .    to this point , the problem formulation , algorithm , convergence results , and the algorithmic / convergence discussions are given .",
    "a casual reader may stop at this point , and take for granted the convergence results and the ( increased ) spatial homogeneity of the filtering approximation error across the random field .",
    "indeed , the conceptual simplicity of the algorithm , and its spatial averaging property , may be sufficient to convince one of the general existence of such a convergence result ( given also the results and analysis in @xcite ) .",
    "subsequently , the details of this result , as they are far from optimal , may be of lesser significance .",
    "going forward , in section 6 we provide the technical analysis leading to the main result and following the proof strategy introduced earlier .",
    "the proofs required in this work largely overlap with those in @xcite and only the required changes are derived here , with reference made to the motivating paper as often as possible . indeed",
    ", we encourage all readers interested in high - dimensional particle filtering to study @xcite since a very descriptive and accessible coverage of this topic and the blocked particle filter is provided therein ( prior to the detailed technical analysis set out in @xcite and to which we point as often as possible to prove our case ) .",
    "for simplicity and to ease comparison we borrow the problem scenario directly from @xcite .    consider a polish state space @xmath0 with @xmath1-algebra @xmath2 and reference measure @xmath3 , and a polish state space @xmath4 with @xmath1-algebra @xmath5 and a reference measure @xmath6 .",
    "introduce on @xmath0 , a markov chain @xmath7 with a transition density @xmath8 with respect to @xmath3 .",
    "introduce on @xmath4 , a sequence @xmath9 that is conditionally independent given @xmath7 and has a transition density @xmath10 with respect to @xmath6 .",
    "we interpret @xmath7 as an underlying dynamical process that is observed through @xmath9 .",
    "the pair @xmath11 is also a markov chain .",
    "now suppose the state @xmath12 at each time @xmath13 is a random field @xmath14 indexed by a ( finite ) undirected graph @xmath15 where @xmath16 corresponds to the set of sites and @xmath17 corresponds to the set of edges that define the structure of the field .",
    "the dimension of the model is then at least as big as the cardinality of the vertex set @xmath16 and we assume this to be large .    to be more precise , the space @xmath0 and @xmath4 are of product form @xmath18 and @xmath19 respectively .",
    "the associated reference measures then given by @xmath20 and @xmath21 where @xmath22 and @xmath23 are reference measures on @xmath24 and @xmath25 respectively .",
    "the transition densities @xmath26 and @xmath27 are given by @xmath28 where @xmath29 and @xmath30 are defined with respect to @xmath22 and @xmath23 respectively .",
    "the model assumes the observations @xmath9 are completely local in the sense that @xmath31 depends only on @xmath32 or , in other words , the conditional distribution of @xmath33 given @xmath34 depends only on @xmath35 .    a distance @xmath36 , that counts hops along the shortest path between @xmath37 , is associated with @xmath15 .",
    "now for a fixed @xmath38 and for each vertex @xmath39 we define @xmath40 which specifies a neighbourhood of @xmath41 .",
    "we then assume the dynamics of @xmath7 are local in the sense that @xmath42 depends only on @xmath43 or in other words the conditional distribution of @xmath35 given @xmath44 depends only on @xmath45 .",
    "more precisely , the dynamics obey @xmath46 whenever @xmath47 where @xmath48 for @xmath49 .",
    "we refer to the motivating paper @xcite for further discussion on such models and the references therein for background of where such models appear in the literature . also , see @xcite for such modelling motivation .",
    "since the process @xmath7 is not directly observable , the filtering problem of interest is one of recursively estimating the unobserved state @xmath34 given the observation history @xmath50 .",
    "that is , the filtering problem is one of computing @xmath51\\ ] ] where @xmath52 is the probability measure under which @xmath11 is a markov chain with a transition probability @xmath53 that can be factored as @xmath54 for any @xmath55 and where the initial condition @xmath56 is an arbitrary probability measure @xmath57 on @xmath0 .",
    "notationally , @xmath58 .",
    "firstly , we outline the ideal nonlinear recursive bayes filter , followed by the standard bootstrap particle filter .",
    "we note briefly the computational problem involved in applying the bootstrap filter to high - dimensional estimation problems .",
    "we then outline our adaptively blocked particle filter and note its straightforward relationship to the algorithm of @xcite and discuss generally the motivation for this algorithm as it applies to filtering of high - dimensional systems .",
    "it is well known that through an application of bayes rule , the filter @xmath59 can be computed recursively via @xmath60 where , it is common for practical , as well as conceptual , reasons to define @xmath61 where @xmath62 is a prediction in which the filter estimate @xmath63 is propagated forward using the dynamics of the underlying process @xmath7 and @xmath64 is a so - called correction ( or update ) in which the predicted distribution is updated by conditioning it on the observation @xmath65 to obtain @xmath59 . graphically , @xmath66 the recursive structure of the filter allows estimation of the underlying dynamic process to be carried out ` on - line ' over a long time horizon and incorporating measurements as they become available .",
    "however , it is well known that to this point such a filter is impractical since at the level of arbitrary probability measures it must be considered of infinite dimension . in general , no exact finite dimensional nonlinear filter can be computed .",
    "one approximation to the nonlinear filter employs sampling and monte carlo approximation .",
    "let @xmath67 denote the number of samples ( or particles ) used in the approximation and define @xmath68 to be the sampling operator which computes a random measure @xmath69 with respect to some probability measure @xmath70 .",
    "this random measure is a discrete approximation of @xmath70 and converges to @xmath70 with @xmath71 at a typical rate of @xmath72 .",
    "the most common and arguably the simplest monte carlo approximation of nonlinear filtering is given by @xmath73 where @xmath74 now consists of three operations @xmath75 this recursion yields the bootstrap particle filtering algorithm @xcite .",
    "this algorithm is simple to implement and @xmath76 converges to the exact filter @xmath59 as @xmath77 . resampling and",
    "other operations are typically incorporated into the bootstrap particle filter to improve performance @xcite .",
    "if the ( standard ) bootstrap particle filter is applied to a system of dimension @xmath78 , then , typically @xcite , the approximation error is exponential in @xmath78 and inversely proportional to something like @xmath79 . if @xmath78 is large , then one needs a huge number of particles @xmath71 to achieve a desired error rate and this requires a heavy computational burden . in many applications , like target tracking @xcite , there are typically no computational barriers to achieving an acceptable error . in large - scale ,",
    "high - dimensional , estimation problems the particle filter is often computationally infeasible which motivates the study in @xcite and obviously in this work .    to this end",
    ", we introduce a partition @xmath80 of the vertex set @xmath16 into non - overlapping blocks @xmath81 now suppose there exists a finite number @xmath82 of partitions @xmath83 of this type .",
    "there exists a non - negative constant @xmath84 and a positive @xmath85 such that given a positive @xmath86 then for every node @xmath39 we have @xmath87 where @xmath88 and we write @xmath89 when @xmath90 for some @xmath91 in some @xmath80 .",
    "here , @xmath92 is the smallest average distance between any site and the borders @xmath93 of those blocks containing it , while @xmath94 captures a similar property in a more round about manner .",
    "we now define @xcite the blocking operator for some partition @xmath80 @xmath95 where for any measure @xmath70 on @xmath96 and @xmath49 we denote by @xmath97 the marginal of @xmath70 on @xmath98 .",
    "the random field described by the measure @xmath99 on @xmath0 is independent across blocks defined by the partition @xmath80 .",
    "the adaptively blocked particle filter adds a blocking operation into the bootstrap particle filter recursion @xmath73 where @xmath100 consists of four operations @xmath101 where @xmath102 is a partition switching signal . if @xmath103 then the adaptively blocked particle filter reduces to the blocked particle filter considered in @xcite . if @xmath104 then the adaptively blocked particle filter reduces to the bootstrap particle filter .",
    "the resulting algorithm is given in algorithm [ alg : filter ] .",
    "consider the partitions @xmath83 let @xmath105 resample i.i.d .",
    "@xmath106 , @xmath107 sample @xmath108 , @xmath107 , @xmath109 compute @xmath110 , @xmath107 , @xmath111 let @xmath112    the only difference between the adaptively blocked particle filter and the block particle filter of @xcite is the adaptive consideration of multiple field partitions during the execution of the algorithm .",
    "going forward , the notation @xmath76 will refer to the adaptively blocked particle filter of algorithm [ alg : filter ] .    at any time @xmath13 ,",
    "only measurements in block @xmath113 are used to update the filter in block @xmath91 .",
    "each block @xmath113 can therefore be updated in parallel .",
    "the complexity of updating each block with a given error is thus dependent only on the cardinality of that block and not on the dimension of the entire random field .",
    "if the additional error for the entire filter caused by the blocking approximation can be sufficiently controlled , it seems the curse of dimensionality as it applies to the particle filter in general can be alleviated .",
    "it is clear that at any given iteration the blocking operator decouples the distribution at the boundaries of the blocks . in the motivating paper @xcite , only a single partition is considered and it follows that the filtering approximation error will be larger at those vertices close to the boundary of each block than at those sites toward the centre of each block .",
    "the result is a spatially non - homogeneous filtering error and indeed the error bound derived in the motivating paper @xcite is spatially dependent .",
    "a comprehensive and insightful discussion on this problem is provided in @xcite . by adaptively applying different partitions",
    "one may ensure that , on average , each site of the random field is ( at least approximately ) the same distance from the borders of the blocks during some cycle .",
    "one would hope then that the error is , on average , spatially homogeneous and that one can achieve an error bound that is site independent .",
    "we note with this in mind the important special case of algorithm [ alg : filter ] in which @xmath114 or where the partitions are applied in a cyclical order .    we stress that beyond the possibility of spatial smoothing , allowing for multiple partitions of the field may also provide algorithmic robustness in those cases in which the random field is time - varying since the partitions can be adapted online , or it may allow one to adaptively focus computation on certain locations of interest for periods of time etc .",
    "the design of @xmath1 offers seemingly much flexibility and other advantages of partition adaptation are envisioned but the discussion here will focus on spatial averaging of the blocking bias .",
    "as in @xcite we define the following norm @xmath115^{1/2}\\ ] ] between two random measures @xmath70 and @xmath116 on @xmath0 where @xmath117 is the class of all measurable functions @xmath118 with @xmath119 whenever @xmath48 for @xmath49 .",
    "for example , one then finds @xmath120 which captures the typical monte carlo approximation error . the goal is to bound the error between the nominal ( ideal )",
    "bayesian filter @xmath59 and the adaptively blocked particle filter @xmath76 .",
    "recall that both the ideal filter and the adaptively blocked particle filter are defined recursively @xmath121 where @xmath61 and @xmath100 . from the triangle inequality we get @xmath122 for some @xmath123 where @xmath124 with @xmath125 is an",
    "ideal adaptively blocked filter ; i.e. considering the adaptive blocking operation as it applies to the ( non - sampled version of the ) ideal bayesian filter .    the expectation appearing in the definition of @xmath126 is taken only with respect to the random sampling @xmath68 ; see @xcite .",
    "hence , @xmath127^{1/2 } = \\sup_{f\\in\\mathfrak{x}^j : |f| \\leq 1 } |\\pi_n^\\mu(f)-\\tilde\\pi_n^\\mu(f)|\\ ] ] since no sampling occurs in @xmath59 or in @xmath128 and in this case @xmath126 defines a local version of the total variation @xcite which , as in @xcite , we sometimes denote by @xmath129 for @xmath123 .",
    "the first term in this decomposition quantifies the bias introduced by the blocking operation alone .",
    "the second term quantifies the error due to the variance of the random sampling",
    ". typical analysis on the convergence of the bootstrap particle filter deals only with a variance term ( since @xmath130 in that case ) .",
    "let @xmath131 and @xmath132 . also define @xmath133 , @xmath134 and @xmath135 .",
    "finally , let @xmath136 .",
    "[ thm : variancebound ] there exists a constant @xmath137 , depending only on @xmath138 and @xmath139 such that the following holds .",
    "suppose there exist @xmath140 and @xmath141 such that @xmath142 then for every @xmath143 , @xmath144 , and @xmath39 we have @xmath145 where @xmath146 depend only on @xmath147 , @xmath148 , @xmath149 , @xmath138 and @xmath139 .",
    "the variance depends on the dimension of the sampling and so is necessarily dependent on the size of the blocks .",
    "this is exactly what is expected @xcite of the variance in the sense that it recovers the behaviour of the standard bootstrap particle filter ( without blocking ) which is dependent on the size of the entire field .",
    "the blocking operation essentially reduces the large - scale filtering problem to one of multiple , smaller , independent filtering problems and on which each independent particle filter mostly exhibits an error that is well understood @xcite .",
    "since the general nature ( not the specific constants ) of the variance bound is the best one might expect , we will not focus on this bound going forward .",
    "the main error component of relevant interest here is that component introduced purely as a result of the blocking operation ( and not the sampling ) .",
    "therefore , going forward we are largely concerned with @xmath150 where @xmath125 and its ability to approximate the ideal , full , bayesian filter @xmath151 .",
    "the particle filter @xmath76 in this case can be thought of as an approximation of the ideal blocked filter @xmath128 and it is worth noting that other approximations to @xmath128 separate to particle - based approximations could be substituted .    in summary ,",
    "the main contribution is reduced to a study on adaptively blocked filtering and the error introduced through adaptive blocking when compared to the ideal bayesian filter .",
    "the particle filtering step is given to show how one may approximate the adaptively blocked filter in practice and the error given on this particle representation is noted for completeness ( this error is as expected even if it is non - trivial to derive ) .",
    "the bound on the bias introduced due to blocking is now stated .",
    "[ thm : biasbound ] suppose that @xmath152 for all @xmath39 and @xmath153 with @xmath154 .",
    "let @xmath155 . if @xmath156 , @xmath157 then for every @xmath39 we have @xmath158\\ ] ] for every @xmath143 and @xmath159 where @xmath160 and @xmath161 and @xmath88 .",
    "this is a time - uniform bound on the average bias over a time length of @xmath162 .",
    "both inequalities in theorem [ thm : biasbound ] imply that the bias introduced due to blocking can be spatially averaged ( smoothed ) across a cyclical application of a sequence of partitions .",
    "both inequalities collapse to the result of @xcite in the case @xmath103 . the second inequality , in general , over bounds the first inequality but",
    "may be more convenient for discussion as @xmath163 may be easier than @xmath164 to conceptualise . following the analysis in @xcite , the goal was to derive a similarly natured bound here , but which captured honestly the spatial smoothing effect .",
    "the spatial invariance of the error bound ( or more specifically the bias bound ) will be discussed in more detail in the next section .",
    "we simply note here that if @xmath165 where @xmath88 for all @xmath166 , then the bound really is spatially invariant .",
    "such a situation occurs for a particular class of graphs ( i.e. random fields ) discussed later .",
    "the more general case in which @xmath167 is also discussed .",
    "a simple corollary follows in which there exists an ordering of @xmath83 such that for a cyclical sequence of partitions @xmath156",
    ", @xmath157 we have @xmath168\\ ] ] for some ( at least one ) @xmath39 .",
    "the blocking operation contributes the bias term to the total error while the random sampling contributes the variance term . as previously noted , the nature of the variance bound is as expected and we do not focus on that going forward .",
    "the bias is determined by the blocking operator which conceptually , at any time , has little effect on those sites far removed from the block borders due to the local dynamical dependencies assumed .",
    "consequently , the bias is controlled at a sub - block level in that the bias at each site is controlled , on average , by that site s distance to the border of the blocks which contain it .",
    "if we can average this distance across the field through adaptive partitioning then we should be averaging the bias across the field .",
    "from the computational view point , the variance bound implies that one need only consider the size of the blocks ( not the entire random field ) when picking a value for @xmath71 to control the error . this ( ideally ) leads to a reduction in the computational requirements of the filtering problem and is the underlying motivation for blocking .",
    "this gain comes at a price , in the form of a bias introduced due to blocking . here",
    "we will consider adaptive blocking as a way of smoothing the bias error over the random field or controlling the bias in a more precise way .",
    "finally , we refer to @xcite for a discussion on the mixing assumption @xmath169 with the non - standard requirement @xmath140 with @xmath170 .",
    "the restrictions on @xmath140 are relaxed partially in @xcite .",
    "this term does not alter the significance of the result at the proof - of - concept stage .",
    "moreover , non - optimal restrictions on the system model in this form appear frequently in similar studies ; e.g. see @xcite .",
    "typically , the empirical evidence suggests a far more relaxed application of the algorithms in question is permissible .",
    "the bias and variance bounds are treated separately but lead to a total error bound .",
    "the proof strategy is adopted from rebeschini and van handel @xcite and much of the analysis required is identical and not repeated . the strategy in @xcite",
    "is inspired in part by the time - uniform particle filter convergence results @xcite .    in the case of the bias @xmath171 ,",
    "one first derives a local stability property for the filter @xmath59 which implies that the marginal over a local set @xmath49 of the initial state @xmath57 is forgotten exponentially fast .",
    "such a property also implies that any approximation errors in , say , the initial state are also forgotten .",
    "it then follows that if one can bound the one - step approximation error @xmath172 at any time , then in conjunction with the local stability property one will obtain a time - uniform bound on the bias over a local region of the field .    in the case of the variance @xmath173 , a similar idea",
    "is used except one first establishes stability for the ideal adaptively blocked filter @xmath128 .",
    "then , one must bound the one - step approximation error @xmath174 at any time . putting the stability property and",
    "the bound on the one - step approximation together , one achieves the desired time - uniform bound on the variance of a block in the adaptively blocked filter .",
    "we have obviously glossed over much of the intricacies involved in the proof in this summary .",
    "for example , in the case of the bias , the property introduced in @xcite and referred to as the decay of correlations must be established to hold uniformly in time for the ideal block filter @xmath128 .",
    "this property captures a notion of spatial stability where the state at some site in the random field is forgotten as one moves away from that site .",
    "rebeschini et al .",
    "provide a novel measure of this decay that allows them to establish local stability of the filter @xmath59 and to establish a bound on the one - step approximation error @xmath172 .",
    "conceptually , a property like the decay of correlations is necessary to establish such results .",
    "we refer the reader to @xcite for a broader , more insightful , discussion on the strategy .",
    "now , we note specifically what ideas must be altered to account for a change in partition from one time to the next .",
    "very roughly speaking the steps needed to prove the bound on the bias include : 1 ) . establishing the local stability property for the nonlinear filter @xmath59 ; and , 2 ) . establishing the decay of correlations property holds uniformly in time for the ideal adaptively",
    "blocked filter @xmath175 ; and then , 3 ) . establishing a bound on the one - step approximation error @xmath172 that holds at any time ; and finally , 4 ) putting it all together .",
    "the local stability of @xmath59 depends on the decay of correlations property assumed on @xmath57 and is otherwise independent of the blocking procedure .",
    "hence , we can take this result as a given @xcite .",
    "the one - step approximation error @xmath172 is dependent on the blocking procedure but only on the partition in effect during a single time step , and this partition is otherwise arbitrary , so we can take this result as given @xcite .",
    "to prove our case , we only need to establish that the decay of correlations property holds uniformly in time for the filter distribution @xmath175 when given the changing partitions .",
    "once this is established , it is just a matter of collecting the relevant results and finalising the bound on @xmath171 .",
    "we follow through with this last step and show how the spatial averaging effect of @xmath176 comes out during this procedure .    the detailed proof is given in a subsequent section drawing from @xcite as often as possible .      roughly again , the steps needed to prove the bound on the variance include : 1 ) . establishing the stability of the ideal adaptively blocked filter @xmath175 ; and , 2 ) . establishing a bound on the one - step approximation error @xmath177 that holds at any time ; and finally , 3 ) .",
    "putting it all together .",
    "firstly , we do not have to deal with any correlation - like properties in the case of the variance bound and as noted the final result is as expected .",
    "so things may appear simpler initially .",
    "unfortunately , proving stability for the ideal adaptively blocked filter @xmath175 is not trivial @xcite . because the stability of @xmath175 is dependent on the change of partition we must re -",
    "establish that this stability result holds in the case of adaptively changing partitions for completeness .",
    "moreover , for technical reasons related to the use of the norm @xmath126 , the authors in @xcite consider instead a two - step approximation error @xmath178 and bound this term at any time . because a change in partition comes into play over two steps ,",
    "we must re - establish that this two - step approximation error is bounded under a change of partition at any time .",
    "we then bring the relevant results together and finalise the bound on @xmath173 .",
    "as noted , this is the strategy taken to derive the variance bound but , since our main concern here is the spatial aspects of the filtering problem and the related ( adaptive ) blocking operation , we do not give the details .",
    "many of the technical lemmas involved in this analysis also follow directly from @xcite and those results requiring a modification to their proofs need only an arguably minor re - analysis and modification .",
    "the final result is as expected and the authors are available to provide the variance bound proof details on request .",
    "the main result in the previous section is a total error bound on @xmath179 , for all @xmath39 and can be decomposed ( and is actually derived ) in terms of a bound on the variance ( induced by the random monte carlo procedure ) and a bound on the bias ( induced by the blocking operator ) .",
    "we focus on the bias bound in this section and its relevance as it pertains to the dependence of the total error on the particular spatial site @xmath41 .",
    "the main point of interest in this work is the effect of the ( adaptive ) blocking operation on the total error bound which shows up purely via the systematic bias . to this end ,",
    "we compare the bound on the bias proposed here with the bias bound proposed in the motivating paper @xcite by rebeschini et al .",
    ", @xmath180 where rebeschini et al .",
    "only ever consider a single partition .",
    "here , @xmath181 and @xmath182 can be taken as the average of the bias over a time period of length @xmath162 .",
    "we remove any unnecessary constants from the expressions that cloud the conceptual discussion . here",
    ", we use @xmath183 to capture only that spatially dependent component of the bias bound noting that the constant ` out - the - front ' is equivalent in both cases @xcite . for conceptual , rather than technical , reasons we consider the slightly looser bound @xmath184 during discussion .",
    "we highlight that if @xmath160 for all @xmath166 then @xmath185 and the bound is truly spatially invariant .",
    "this is part of the motivation for this work and is explored in more detail now .",
    "consider figure [ fig : graph1 ] .",
    "( v0,v1 ) ( v2,v3,v4 ) ( v1,v2 ) ( v4,v0 )    ( v1,v2 ) ( v3,v4,v0 ) ( v2,v3 ) ( v0,v1 )    ( v2,v3 ) ( v4,v0,v1 ) ( v3,v4 ) ( v1,v2 )    ( v3,v4 ) ( v0,v1,v2 ) ( v4,v0 ) ( v2,v3 )    ( v4,v0 ) ( v1,v2,v3 ) ( v0,v1 ) ( v3,v4 )    pick any site @xmath39 in the graph depicted in figure [ fig : graph1 ] and note that @xmath186 .",
    "one then clearly has a spatially uniform bound on @xmath181 .",
    "consider now any single partition alone and note that for four out of the five sites we have @xmath187 and at one site we have @xmath188 which implies , as noted by rebeschini et al .",
    ", that the bound on @xmath182 is not spatially uniform .",
    "the adaptive blocking procedure is averaging the distance @xmath189 through the use of multiple partitions which results in a kind of spatial error smoothing .",
    "the bound on the bias of the cyclically blocked filter at every site @xmath39 is completely independent of the site @xmath41 in every case in which @xmath190 , @xmath109 .",
    "such cases may occur in practice ; e.g. a sequence of partitions on any regular lattice wrapped on a torus can be derived that obeys this property , see figure [ fig : graph2 ] .",
    "3[mesh , scatter , fill = white , opacity=0.5 , z buffer = sort , domain=0:2*pi , y domain=0:2*pi ] ( ( 4+cos(deg(x)))*cos(deg(y ) ) , ( 4+cos(deg(x)))*sin(deg(y ) ) , sin(deg(x ) ) ) ;    in the general case , in which @xmath191 for some @xmath39 , the spatial smoothing property of the cyclical blocking filter is still in effect and reduces , as compared to @xcite , the degree of spatial inhomogeneity ( on average ) as it applies to the bias bound",
    ". essentially , the sites on the borders of a block in @xmath80 are typically not on the borders of a block in @xmath192 , while the sites at the centre of a block in @xmath80 are typically not at the centre of a block in @xmath192 . given a sufficient number of well - chosen partitions of this type , then one can ensure the average distance of a site to a border is smoothed ( or spatially averaged ) across all sites .",
    "of course , this means that some particular sites may be worse off than they were under a single partition ( this is an obvious consequence averaging ) .",
    "for example , consider again the case in figure [ fig : graph1 ] but suppose only the four left most partitions are employed by the cyclically blocked particle filter .",
    "then @xmath193 while @xmath194 .",
    "clearly , one has a more desirable bound on the bias on average in this case than in the case in which only a single partition is considered , albeit complete spatial homogeneity is not achieved .",
    "considering additional partitions in a large - scale random field will be of even further benefit than that exposited in this toy example .",
    "note finally that in both rebeschini et al . and here the spatial uniformity of the error ( or the bias more specifically ) is often referred to via the bound on the bias and not the bias or error itself .",
    "this is a consequence of the technical analysis , but for all practical purposes it would appear obvious that the spatial homogeneity of the error itself is of the same nature as that noted by the bound applicable to that error ( even if such bounds are otherwise quite loose ) .",
    "that is , the blocked filter of rebeschini et al .",
    "@xcite would clearly seem to favour those sites far from the border of the individual blocks in terms of the actual performance of the filter , while the cyclically blocked filter proposed in this work is clearly , in some sense , averaging out this favouritism and its effect on the actual filter performance at any site .",
    "the point is that the spatial relationship of the error is typically noted in terms of the bias bounds but intuitively / conceptually the discussions on homogeneity ( or inhomogeneity ) of a particular filter apply ( seemingly ) also to the error / bias itself .",
    "rebeschini et al . @xcite introduced an important concept referred to as the decay of correlation which captures , in a very technical manner , the intuitive notion of spatial stability where the state at some site in the random field should be forgotten as one moves away from that site .",
    "this notion plays a crucial role in the convergence of the bias due to the blocking operation .",
    "it is important to note that the analysis and the spatial stability property put forth in @xcite is based in part on those ideas of temporal stability introduced in @xcite and used to establish time - uniform convergence results for the standard bootstrap particle filter .",
    "recall that the dynamics of the underlying process @xmath7 are local in the sense that @xmath42 depends only on @xmath43 where @xmath48 for @xmath49 . here",
    ", @xmath195 for some @xmath38 captures the local neighbourhood of sites on which site @xmath41 explicitly depends .",
    "we now briefly review the measure introduced in @xcite on the decay of correlations property . for any probability measure @xmath57 on @xmath0 and for @xmath153 with @xmath39 define @xmath196   = ~\\frac{\\int \\mathbf{1}_a(x^v)\\prod_{u\\in n(v)}p^u(x , z^u ) \\mu^v_x(dx^v)}{\\int \\prod_{u\\in n(v)}p^u(x , z^u ) \\mu^v_x(dx^v)}\\ ] ] for any @xmath197 and where @xmath198 then @xmath199 for @xmath37 .",
    "this quantity @xmath200 somehow captures the correlation between two sites @xmath37 in the random field under the assumed field model .",
    "a little more precisely , this term is measuring the maximal total variation at a site @xmath41 that may arise due to a perturbation at site @xmath201 .",
    "now define   @xmath202 with @xmath203 .",
    "this quantity @xmath204 is a measure on the total degree of correlation decay for the measure @xmath57 given a rate parameter @xmath205 .",
    "the site @xmath41 can be interpreted as the most sensitive site in the field . to understand this quantity",
    "@xmath204 a little more conceptually , suppose the most sensitive site @xmath41 is known a priori",
    ". then suppose that @xmath206 .",
    "it follows that the correlation between any two sites in the random field decays as a function of the distance between those two sites at an exponential rate defined by at least @xmath205 .    with @xmath204 defined as",
    "such we borrow directly from @xcite the local stability result on @xmath59 which requires only that the initial condition @xmath57 satisfy a decay of correlations property .",
    "recall that @xmath207 defines the size of the largest neighbourhood in @xmath16 .",
    "[ lem : localfilterstability ] suppose there exists @xmath208 such that @xmath209 for all @xmath39 and @xmath153 .",
    "let @xmath210 be probability measures on @xmath0 , and suppose that @xmath211 for a sufficiently small constant @xmath203 .",
    "then @xmath212 for every @xmath49 and @xmath213 .",
    "now consider any partition @xmath214 .",
    "we define a correlation depending on the given partition .",
    "fix a probability measure @xmath57 on @xmath0 and @xmath215 , @xmath216 , @xmath217 and then let @xmath218 = \\frac{\\int \\mathbf{1}_a(x^v ) \\prod_{u\\in n(v)\\cap k } p^u(x , z^u)\\mu^v_x(dx^v)}{\\int \\prod_{u\\in n(v)\\cap k } p^u(x , z^u)\\mu^v_x(dx^v)}\\ ] ] for any @xmath197 .",
    "now define @xmath219 and @xmath220 for any partition @xmath214 in the cyclically blocked particle filter sequence .",
    "one can interpret the block adapted measure @xmath221 in much the same way as @xmath204 .",
    "the reason behind formulating the measure @xmath204 in such a way is technical and follows from the program put forth in @xcite .",
    "here we are only modifying this program to account for changing partitions and in doing so we seek to draw on the detailed analysis put forth in @xcite as much as possible .",
    "the reason for introducing @xmath221 is now explained . in order to establish the one - step approximation error",
    "we must bound @xmath222 uniformly in time .",
    "the authors of @xcite note the difficulty in working directly with @xmath222 and instead propose to bound @xmath223 and then use the following result to indirectly control @xmath222 .        because @xmath175 is dependent on the changing partitions , which is central to the adaptively blocked filter",
    ", it follows that we must re - establish that a time - uniform bound on @xmath223 exists in the case of interest here .",
    "if @xmath230 or even @xmath231 then @xmath232 note @xmath233 implies @xmath234 for any @xmath235 .",
    "this leaves the case @xmath236 and @xmath237 . in this case @xmath238 by continuity in @xmath239 for any fixed @xmath240 .",
    "pick @xmath243 and @xmath244 with @xmath245 . then by definition @xmath246 } \\mu ^{v , k_j}_{x , z}(a ) \\\\",
    "& \\leq \\varepsilon^ { -4\\delta}\\mu ^{v , k_j}_{x , z}(a)\\end{aligned}\\ ] ] for @xmath197 .",
    "alternatively , @xmath247 noting that @xmath214 and @xmath248 are anyway arbitrary .",
    "fix @xmath197 and @xmath249 and suppose , without loss of generality , @xmath250 .",
    "it follows @xmath251 using lemma [ lem : minor ] .",
    "taking the supremum over @xmath197 gives @xmath252 and from this the result of the proposition follow easily .",
    "[ lem : tildecorrboundonestep ] suppose there exists @xmath208 such that @xmath253 for all @xmath39 and @xmath153 .",
    "for any probability measure @xmath224 , partition @xmath254 and sufficiently small @xmath203 such that @xmath255 then @xmath256 for any @xmath157 .",
    "now we are in a position to prove the time - uniform bound on @xmath223 .",
    "we have via lemma [ lem : tildecorrboundonestep ] a bound on the change in the correlation decay over any single time step .",
    "we have via proposition [ prop : tildecorrboundchangepartition ] a relationship between the decay of correlations for a measure under two different partitions .",
    "we combine these results and iterate to get the desired time - uniform bound as now shown .",
    "[ lem : tildecorrbounduniformtime ] assume @xmath253 for all @xmath39 and @xmath153 with @xmath257 let @xmath57 be a probability measure on @xmath0 and @xmath258 a partition of @xmath16 such that @xmath259 where @xmath260>0 $ ] .",
    "then @xmath261 for all @xmath262 .",
    "first , @xmath263 implies @xmath264 and @xmath265 .",
    "thus , given @xmath266 we have @xmath267 via proposition [ prop : tildecorrboundchangepartition ] . now , given @xmath268",
    "we have @xmath269 via lemma [ lem : tildecorrboundonestep ] .",
    "just restart the argument and iterate to get @xmath270 for any @xmath271 .",
    "now we have a time - uniform bound on @xmath272 , and we can use lemma [ lem : tildecorrboundcorr ] to arrive at the result we want which is a time - uniform bound on the decay of correlation measure of interest .      recall the program required to prove the bias bound : 1 ) . establish the local stability property for the nonlinear filter @xmath59 ; and , 2 ) .",
    "establish the decay of correlations property holds uniformly in time for the ideal cyclical blocked filter @xmath175 ; and then , 3 ) . establish a bound on the one - step approximation error @xmath172 that holds at any time ; and finally , 4 ) .",
    "put it all together .",
    "all that remains is to show that @xmath172 is bounded at any time and then to put it all together .",
    "since this one - step approximation error is independent of any change in partition we refer back to @xcite .",
    "[ lem : biasboundonestep ] suppose there exists @xmath208 such that @xmath275 for all @xmath39 and @xmath153 .",
    "let @xmath224 be a probability measure on @xmath0 , and suppose that @xmath276 for a sufficiently small constant @xmath203 .",
    "then @xmath277 for every @xmath157 and every @xmath278 with @xmath90 .",
    "furthermore , @xmath279 for every @xmath113 and @xmath280 .",
    "note @xmath281 and here we consider the case @xmath282 for some @xmath144 .",
    "firstly , note @xmath283 and @xmath273 such that lemma [ lemma : corrbounduniformtime ] holds .",
    "moreover , @xmath284 for every @xmath143 and lemma [ lem : biasboundonestep ] holds .",
    "finally , @xmath285 for every @xmath143 and lemma [ lem : localfilterstability ] holds .",
    "thus , the bound on the decay of correlations holds and implies local filter stability and the one - step blocking approximation error bound all hold under the given parameter hypotheses .",
    "following @xcite by application of lemma [ lem : localfilterstability ] and lemma [ lemma : corrbounduniformtime ] we easily find @xmath289 for every @xmath143 , @xmath159 and every @xmath290 such that @xmath287 for all @xmath157 .",
    "for any site @xmath41 consider the sequence @xmath291 for all @xmath157 .",
    "the bound just given becomes @xmath292 recall the hypothesis @xmath156 , @xmath157 .",
    "now averaging the bound gives @xmath293 and we define @xmath294 for @xmath157 to be zero .",
    "now @xmath295 where now we extend @xmath296 to all @xmath297 so @xmath298",
    ". then @xmath299 where we swapped the summation order and used the fact that @xmath300 for @xmath301 and all @xmath157 because of the cyclical partitioning sequence .",
    "now it follows that @xmath302 noting @xmath303 .",
    "let @xmath135 then the over bounding @xmath304\\ ] ] follows from slater s inequality and the proof is complete .",
    "again , both inequalities in theorem [ thm : biasbound ] imply that the bias introduced due to blocking can be spatially averaged ( or smoothed ) across a cyclical application of a sequence of partitions and both inequalities collapse to the result of @xcite in the case @xmath103 ."
  ],
  "abstract_text": [
    "<S> the typical particle filtering approximation error is exponentially dependent on the dimension of the model . </S>",
    "<S> therefore , to control this error , an enormous number of particles are required , which means a heavy computational burden that is often so great it is simply prohibitive . </S>",
    "<S> rebeschini and van handel ( 2013 ) consider particle filtering in a large - scale dynamic random field . through a suitable localisation operation , they prove that a modified particle filtering algorithm can achieve an approximation error that is mostly independent of the problem dimension . to achieve this feat , </S>",
    "<S> they inadvertently introduce a systematic bias that is spatially dependent ( in that the bias at one site is dependent on the location of that site ) . </S>",
    "<S> this bias consequently varies throughout field . in this work , a simple extension to the algorithm of rebeschini and van handel </S>",
    "<S> is introduced which acts to average this bias term over each site in the field through a kind of spatial smoothing . </S>",
    "<S> it is shown that for a certain class of random field it is possible to achieve a completely spatially uniform bound on the bias and that in any general random field the spatial inhomogeneity is significantly reduced when compared to the case in which spatial smoothing is not considered . while the focus is on spatial averaging in this work , the proposed algorithm seemingly exhibits other advantageous properties such as improved robustness and accuracy in those cases in which the underlying dynamic field is time varying . </S>"
  ]
}