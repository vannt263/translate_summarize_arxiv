{
  "article_text": [
    "although linear models have been the standard tool for time series analysis for a long time , their limitations have been underlined during the past twenty years .",
    "real data often exhibit characteristics that are not taken into account by linear models . financial series , for instance , alternate strong and weak volatility periods ,",
    "while economic series are often related to the business cycle and switch from recession to normal periods .",
    "several solutions such as heteroscedatic arch , garch models , threshold models , multilayer perceptrons or autoregressive switching markov models were proposed to overcome these problems .    in this paper , we consider models which allow the series to switch between regimes and more particularly we study the case of mixtures of multilayer perceptrons . in this frame , rather than using a single global model , we estimate several local models from the data . for the moment",
    ", we assume that switches between different models occur independently , the next step of this approach being to also learn how to split the input space and to consider the more general case of _ gated experts _ or _ mixtures of experts _ models ( jacobs et al . , 1991 ) .",
    "the problem we address here is how to select the number of components in a mixture of multilayer perceptrons .",
    "this is typically a problem of non - identifiability which leads to a degenerate fisher information matrix and the classical chi - square theory on the convergence of the likelihood ratio fails to apply .",
    "one possible method to answer this problem is to consider penalized criteria .",
    "the consistency of the bic criterion was recently proven for non - identifiable models such as mixtures of densities or hidden markov models ( keribin , 2000 and gassiat , 2002 ) .",
    "we extend these results to mixtures of nonlinear autoregressive models and prove the consistency of a penalized estimate for the number of components under some good regularity conditions .",
    "the rest of the paper is organized as follows : in section 2 we give the definition of the general model and state sufficient conditions for regularity .",
    "then , we introduce the penalized likelihood estimate for the number of components and state the result of consistency .",
    "section 3 is concerned with applying the main result to mixtures of multilayer perceptrons .",
    "some open questions , as well as some possible extensions are discussed in the conclusion .",
    "this section is devoted to the setting of the general theoretical frame : model , definition and consistency of the penalized - likelihood estimate for the number of components .",
    "* the model - definition and regularity conditions *    throughout the paper , we shall consider that the number of lags is known and , for ease of writing , we shall set the number of lags equal to one , the extension to @xmath0 time - lags being immediate .    let us consider the real - valued time series @xmath1 which verifies the following model    @xmath2    where    * @xmath3 is an iid sequence of random variables valued in a finite space @xmath4 and with probability distribution @xmath5 ; * for every @xmath6 , @xmath7 and + @xmath8 + is the family of possible regression functions .",
    "we suppose throughout the rest of the paper that @xmath9 are sublinear , that is they are continuous and there exist @xmath10 such that @xmath11 ; * for every @xmath6 , @xmath12 is an iid noise such that @xmath13 is independent of @xmath14 .",
    "moreover , @xmath13 has a centered gaussian density @xmath15 .",
    "the sublinearity condition on the regression functions is quite general and the consistency for the number of components holds for various classes of processes , such as mixtures of densities , mixtures of linear autoregressive functions or mixtures of multilayer perceptrons .",
    "let us also remark that besides its necessity in the proof of the theoretical result , the compactness hypothesis for the parameter space is also useful in practice .",
    "indeed , one needs to bound the parameter space in order to avoid numerical problems in the multilayer perceptrons such as hidden - unit saturation . in our case",
    ", @xmath16 seems to be an acceptable bound for the computations . on the other hand ,",
    "mixture probabilities are naturally bounded .",
    "the next example of a linear mixture illustrates the model introduced by ( 1 ) .",
    "the hidden process @xmath3 is a sequence of iid variables with bernoulli(0.5 ) distribution .",
    "we define @xmath1 as follows , using @xmath3 and a standard gaussian noise @xmath17 :    @xmath18    the penalized - likelihood estimate which we introduce in the next subsection converges in probability to the true number of components of the model under some regularity conditions on the process @xmath1 .",
    "more precisely , we need the following hypothesis which implies , according to yao and attali ( 2000 ) , strict stationarity and geometric ergodicity for @xmath1 :    * ( hs)@xmath19 * @xmath20    let us remark that hypothesis * ( hs ) * does not request every component to be stationary and that it allows non - stationary `` regimes '' as long as they do not appear too often . since multilayer perceptrons are bounded function , this hypothesis will be naturally fulfilled .",
    "* construction of the penalized likelihood criterion *    let us consider an observed sample @xmath21 of the time series @xmath1 .",
    "then , for every observation @xmath22 , the conditional density with respect to the previous @xmath23 and marginally in @xmath3 is    @xmath24    as the goal is to estimate @xmath25 , the number of regimes of the model , let us consider all possible conditional densities up to a maximal number of regimes @xmath26 , a fixed positive integer .",
    "we shall consider the class of functions    @xmath27    where @xmath28 , @xmath29 , @xmath30 and @xmath31 is a centered gaussian density .    for every @xmath32",
    "we define the number of regimes as    @xmath33    and let @xmath34 be the true number of regimes .",
    "we can now define the estimate @xmath35 as the argument @xmath36 maximizing the penalized criterion    @xmath37    where    @xmath38    is the log - likelihood marginal in @xmath39 and @xmath40 is a penalty term .    *",
    "convergence of the penalized likelihood estimate *    several statistical and probabilistic notions such as mixing processes , bracketing entropy or donsker classes will be used hereafter . for parcimony purposes we shall not remind them , but the reader may refer to doukhan ( 1995 ) and van der vaart ( 2000 ) for complete monographs on the subject .",
    "+ the consistency of @xmath41 is given by the next result , which in an extension of gassiat ( 2002 ) :    * _ theorem 1 _ * _ : consider the model @xmath42 defined by ( 1 ) and the penalized - likelihood criterion introduced in ( 2 ) .",
    "let us introduce the next assumptions : _",
    "* _ @xmath19(a1 ) _ * _ @xmath43 is an increasing function of @xmath44 , @xmath45 when @xmath46 for every @xmath47 and @xmath48 when @xmath46 for every @xmath44 _    * _ @xmath19(a2 ) _ * _ the model _ _ @xmath42 verifies the weak identifiability assumption _ * _ ( hi ) _ *    @xmath49    * _ @xmath19(a3 ) _ * _ the parameterization @xmath50 is continuous for every @xmath51 and there exists @xmath52 an integrable map with respect to the stationary measure of @xmath53 such that @xmath54 _    * _ @xmath19(a4 ) _ * _ @xmath55 is strictly stationary and geometrically @xmath56-mixing , and the family of generalized score functions associated to @xmath57 _    @xmath58    _ where @xmath59 is the stationary measure of _ @xmath53 _ and for every @xmath60 _    _ @xmath61}\\left(\\varepsilon , \\mathcal{s},\\left\\vert \\cdot \\right\\vert _ { 2}\\right)=\\mathcal{o}\\left(\\left|log\\ , \\varepsilon \\right|\\right),$ ] _    _",
    "@xmath61}\\left(\\varepsilon , \\mathcal{s},\\left\\vert \\cdot \\right\\vert _ { 2}\\right)$ ] being the bracketing entropy of @xmath62 with respect to the @xmath63-norm .",
    "_ then , under hypothesis ( a1)-(a4 ) , ( hs ) et ( hc ) , @xmath64 in probability .",
    "_    _ * proof of theorem 1 * _    first , let us show that @xmath41 does not overestimate @xmath25 .",
    "we shall need the following likelihood ratio inequality which is an immediate generalization of gassiat ( 2002 ) to multivariate dependent data .",
    "_ let @xmath65 be a parametric family of conditional densities containing the true model @xmath66 and consider the generalized score functions _",
    "@xmath67    _ where @xmath68 is the stationary measure of @xmath69 . then , _",
    "@xmath70    _ with _    @xmath71",
    ".    then we have :    @xmath72    @xmath73    @xmath74    under the hypothesis * ( hs ) * , there exists a unique strictly stationary solution @xmath55 which is also geometrically ergodic and this implies that @xmath55 is in particular geometrically @xmath75-mixing .",
    "then , by remarking that    @xmath76 we obtain that the bivariate series @xmath69 is also strictly stationary and geometrically @xmath75-mixing .",
    "this fact , together with the assumption on the @xmath77-bracketing entropy of @xmath62 with respect to the @xmath78 norm and the condition that @xmath79 ensures that theorem 4 in doukan , massart and rio ( 1995 ) holds and    @xmath80    is uniformly tight and verifies a functional central limit theorem .",
    "then ,    @xmath81    on the other hand , @xmath79 , thus @xmath82 and using the @xmath83-entropy condition @xmath84 is glivenko - cantelli . since @xmath69 is ergodic and strictly stationary ,",
    "we obtain the following uniform convergence in probability :    @xmath85    to finish the first part , let us prove that    @xmath86    if we suppose , on the contrary , that @xmath87 , then there exists a sequence of functions @xmath88 , @xmath89 such that @xmath90 .",
    "the @xmath63-convergence implies that @xmath91 in @xmath92 and a.s . for a subsequence @xmath93 . since @xmath94 and @xmath95 , where @xmath96 , we obtain that @xmath97 and thus @xmath98 in @xmath92 and a.s . for a subsequence @xmath99 .",
    "the hypothesis * ( a4 ) * ensures the existence of a square - integrable dominating function for @xmath62 and , finally , we get that a subsequence of @xmath100 converges to @xmath101 a.s . and in @xmath63 , which contradicts the fact that @xmath102 for every @xmath103 , so that :    @xmath104    then , by the uniform tightness above and the hypothesis * ( a1 ) * ,    @xmath105    let us now prove that @xmath35 does not underestimate @xmath25 :    @xmath106    @xmath107    now , @xmath108 and under the hypothesis * ( a3 ) * , the class of functions @xmath109 is @xmath110-glivenko - cantelli ( the general proof for a parametric family can be found in van der vaart , 2000 ) and since @xmath69 is ergodic and strictly stationary , we obtain the following uniform convergence in probability :    @xmath111    since @xmath112 and using assumption * ( a2 ) * , the limit is negative . by hypothesis * ( a1 ) * , @xmath113 converges to @xmath101 when @xmath114 , so we finally have that @xmath115 and the proof is done .",
    "in this section , we consider the model defined in ( 1 ) such that , for every @xmath117 , @xmath118 is a multilayer perceptron .",
    "since non - identifiability problems also arise in multilayer perceptrons ( see , for instance , rynkiewicz , 2006 ) , we shall simplify the problem by considering one hidden layer and a fixed number of units on every layer , @xmath119 .",
    "then , we have that for every @xmath117    @xmath120    where @xmath121 is the hyperbolic tangent and    @xmath122    is the true parameter with the true variance.let us check if the hypothesis of the main result of section 2 apply in the case of mixtures of multilayer perceptrons .    *",
    "hypothesis ( hs ) * : the stationarity and ergodicity assumption ( hs ) is immediately verified since the output of every perceptron is bounded , by construction . thus , every regime is stationary and the global model is also stationary .",
    "let us consider the class of all possible conditional densities up to a maximum number of components @xmath123 :    @xmath124 , where    * @xmath125 and we may suppose quite naturally that for every @xmath126 , @xmath127 * for every @xmath126 , @xmath128 is a multilayer perceptron    @xmath129 , where    @xmath130 belongs to a compact set .    *",
    "hypothesis ( a1 ) * : @xmath43 may be chosen , for instance , equal to the bic penalizing term , @xmath131 .    * hypothesis ( a2)-(a3 ) * : since the noise is normally distributed , the weak identifiability hypothesis is verified according to the result of teicher ( 1963 ) , while assumption ( a3 ) is a regularity condition verified by gaussian densities .",
    "* hypothesis ( a4 ) * : we consider the class of generalized score functions    @xmath132    the difficult part will be to show that @xmath61}\\left(\\varepsilon , \\mathcal{s},\\left\\vert \\cdot \\right\\vert _ { 2}\\right)=\\mathcal{o}\\left(\\left|log\\ , \\varepsilon \\right|\\right)$ ] for all @xmath60 which , since we are on a functional space , is equivalent to prove that `` the dimension '' of @xmath62 can be controlled . for @xmath133 ,",
    "let us denote @xmath134 and @xmath135 , so that the global parameter will be @xmath136 and the associated generalized score function @xmath137 .",
    "proving that a parametric family like @xmath62 verifies the condition on the bracketing entropy is usually immediate under good regularity conditions ( see , for instance , van der vaart , 2000 ) .",
    "a sufficient condition is that the bracketing number grows as a polynomial function of @xmath138 . in this particular case ,",
    "the problems arise when @xmath139 and the limits in @xmath140 of @xmath141 have to be computed .",
    "let us split @xmath62 into two classes of functions .",
    "we shall consider @xmath142 a neighborhood of @xmath143 such that @xmath144 and @xmath145 .",
    "on @xmath146 , it can be easily seen that    @xmath147    hence , on @xmath146 , it is sufficient that @xmath148 for @xmath149    now , @xmath146 is a parametric class . since the derivatives of the transfer functions are bounded , according to the example 19.7 of van der vaart ( 2000 ) , it exists a constant @xmath150 so that the bracketing number of @xmath151 is lower than @xmath152 where @xmath153 is the diameter of the smallest sphere of @xmath154 including the set of possible parameters .",
    "so , we get that @xmath155}\\left(\\varepsilon , \\mathcal{s}\\setminus \\mathcal{s}_{0},\\left\\vert \\cdot \\right\\vert _",
    "{ 2}\\right)=\\mathcal{o}\\left(\\frac{1}{\\varepsilon } \\right)^{6(k+1)p}$ ] , where @xmath155}\\left(\\varepsilon , \\mathcal{s}\\setminus \\mathcal{s}_{0},\\left\\vert \\cdot \\right\\vert _ { 2}\\right)$ ] is the number of @xmath156-brackets necessary to cover @xmath146 and the bracketing entropy is computed as @xmath157}\\left(\\varepsilon , \\mathcal{s}\\setminus \\mathcal{s}_{0},\\left\\vert \\cdot \\right\\vert _ { 2}\\right)$ ] .",
    "as for @xmath158 , the idea is to reparameterize the model in a convenient manner which will allow a taylor expansion around the identifiable part of the true value .",
    "for that , we shall use a slight modification of the method proposed by liu and shao ( 2003 ) .",
    "let us remark that when @xmath159 , the weak identifiability hypothesis * ( a2 ) * and the fact that for every @xmath126 , @xmath127 , imply that there exists a vector @xmath160 such that @xmath161 and , modulo a permutation , @xmath162 can be rewritten as follows : @xmath163 , @xmath164 , @xmath117 . with this remark , one can define in the general case @xmath165 and @xmath166 so that , for every @xmath117 , @xmath167 ,    @xmath168    and the new parameterization will be @xmath169 ,    @xmath170 , @xmath171 , with @xmath172 containing all the identifiable parameters of the model and @xmath173 the non - identifiable ones . then , for @xmath174 , we will have that    @xmath175{cccc }   ( \\underbrace{\\theta _ { 1}^{0}, ... ,\\theta _ { 1}^{0 } } & , ... , & \\underbrace{\\theta _",
    "{ p_{0}}^{0}, ... ,\\theta _ { p_{0}}^{0 } } , & \\underbrace{0, ... ,0}\\\\   t_{1 } &   & t_{p_{0}}-t_{p_{0}-1 } & p_{0}-1\\end{array})^{t}$ ]    this reparameterization allows to write a second - order taylor expansion of @xmath176 at @xmath177 . for ease of writing , we shall first denote    @xmath178    then , the density ratio becomes :    @xmath179    by remarking that when @xmath180 , @xmath181 does not vary with @xmath173 , we will study the variation of this ratio in a neighborhood of @xmath177 and for fixed @xmath173 .",
    "let us note @xmath182 the vector of derivatives of @xmath183 with respect of each components of @xmath184 and @xmath185 the vector of second derivatives of @xmath183 with respect of each components of @xmath184 .",
    "assuming that @xmath186 , @xmath187 and @xmath188 , where    @xmath189    are linearly independent in @xmath140 , one can prove the following :    * _ proposition 1 _ * _ : let us denote @xmath190 .",
    "for any fixed @xmath173 , there exists the second - order taylor expansion at @xmath177 : _",
    "@xmath191    _ with _",
    "@xmath192    _ and _    @xmath193    @xmath194\\ ] ]    _ moreover ,",
    "_    @xmath195    * _ proof of proposition 1 _ *    the first term in the developpement can be computed easily by remarking that the gradient of @xmath196 at @xmath197 is :    * for @xmath6 and @xmath198 , @xmath199 * for @xmath200 ,    @xmath201    the term of second order can be obtained by direct computations once the hessian in computed at @xmath197 :    * @xmath202 , @xmath203 and @xmath204 * @xmath205 , @xmath206 and @xmath207 * @xmath208 , @xmath209 * @xmath210 , @xmath211 and @xmath204 * @xmath212 , @xmath211 and @xmath213 * the other crossed derivatives of @xmath214 and @xmath215 are zero    it remains to prove that the rest is @xmath216 but this follows directly from yao ( 2000 ) and the fact that , since the noise is normally distributed , @xmath1 has moments of any order .",
    "@xmath116    using the taylor expansion above , for @xmath217 belonging to @xmath218 , @xmath219 is the sum of a linear combination of @xmath220    and of a term whose @xmath221 norm is negligible compared to the @xmath221 norm of this combination when @xmath77 goes to 0 . by assumption ( a3 ) , a strictly positive number @xmath222 exists so that for any vector of norm 1 with components @xmath223 and @xmath77 sufficiently small : @xmath224 since any function @xmath225 can be written : @xmath226 @xmath227 belongs to the set of functions : @xmath228 whose bracketing number is smaller or equal to @xmath229 .    and the assumptions of theorem 1 are verified @xmath116",
    "the theoretical result proven above may be applied in practice to compute the number of components in a mixture model on simulated or real data .",
    "some examples are presented below , illustrating the stability and the speed of convergence of the algorithm .",
    "let us first consider the particular case of linear models , corresponding to zero hidden - unit perceptrons .",
    "the examples are mixtures of two autoregressive models in which we vary the leading coefficients and the weights of the discrete mixing distribution . for every example",
    ", we simulate 20 samples of lengths @xmath230 and we fix @xmath231 the upper bound for the number of regimes .    the likelihood is maximized via the em algorithm ( see , for instance , dempster , laird and rubin ( 1977 ) or redner and walker , 1984 ) .",
    "this algorithm is well suited to find a sequence of parameters which increases the likelihood at each step , and so converges to a local maximum for a very wide class of models and for our model in particular .",
    "the idea of the em algorithm is to replace the latent variables of the mixture by their conditional expectation .",
    "a brief recall on the main steps of the algorithm is given below :    1 .",
    "let @xmath232 be the vector containing the component of the mixture and considered as a latent variable and let @xmath233 be the vector of observations .",
    "initialization : set @xmath234 and choose @xmath235 3 .",
    "e - step : set @xmath236 and compute @xmath237 with + @xmath238   $ ] where @xmath239 is the likelihood of the observations and the vector of mixture @xmath232 for the parameter @xmath217 .",
    "this step computes the probabilities of the mixture conditionally to the observations and with respect to the parameter @xmath240 4 .",
    "m - step : find : + @xmath241 5 .",
    "replace @xmath242 by @xmath243 , and go back to step 3 ) until a stopping criterion is satisfied ( i.e. when the parameters do nt seem to change anymore ) .",
    "the sequence @xmath244 gives nondecreasing values of the likelihood function up to a local maximum .",
    "@xmath245 is called conditional pseudo - log - likelihood .    to avoid local maxima ,",
    "the procedure is initialized several times with different starting values : in our case , ten different initializations provided good results .",
    "the stopping criteria applies when either there is no improvement in the likelihood value , either a maximum number of iterations , fixed at 200 here for reasonable computation time , is reached .",
    "the true conditional density is    @xmath246    with @xmath247 and @xmath248 for @xmath249 .    for every example",
    ", we pick equal standard errors @xmath250 , @xmath251 and @xmath252 and let vary the rest of the coefficients : @xmath253 , @xmath254 .",
    "let us focus in one particular example .",
    "figure 1 illustrates one out of the twenty samples in the case @xmath255 , @xmath256 , @xmath257 and @xmath258 .",
    "the observed values of the series @xmath1 are plotted on the first graph . on the second graph",
    ", we represent the convergence of the estimates for the mixture probabilities ( solid and dashed lines ) to the true values ( dotted lines ) .",
    "only the best result from the different initializations of the em algorithm was drawn .",
    "the summary of the results for all the examples is given by table 1 . in almost every case",
    ", the convergence is reached for the samples containing 2000 inputs . in practice",
    ", the results will be then more or less accurate , depending on the size of the sample , but also on the proximity of the components and on their frequency .",
    ".number of components for @xmath259 and @xmath260 [ cols=\"^,^,^,^,^,^,^,^,^,^,^ \" , ]     the results are clear for our model , the best model is the model with two experts .",
    "it is difficult to give an interpretation of the regimes because mixing probabilities remain constant over time .",
    "however , if we look at the prediction made by each expert , we can see that one expert seems to be specialized in the general regime of the series and the second one with the collapse regime .",
    "the proposed method gives an insight on the way to choose the number of experts in a mixture model for laser time series .",
    "however , since the probabilities of the mixture are constant , it would be better to choose probabilities depending on the previous value of the time series as in the gating expert of weigend et al .",
    "( weigend et .",
    "al , 1995 ) or of the time as in hybrid hidden markov / mlp models ( rynkiewicz , 2006 ) .",
    "the prediction error of this simple mixture model is not competitive with such more complex models , however we need to improve the theory to deal with such complex modeling .",
    "we have proven the consistency of the bic criterion for estimating the number of components in a mixture of multilayer perceptrons . in our opinion ,",
    "two important directions are to be studied in the future .",
    "the case of mixtures should be extended to the general case of gated experts which allow the probability distribution of the multilayer perceptrons to depend on the input and thus , to learn how to split the input space .",
    "the second possible extension should remove the hypothesis of a fixed number of units on the hidden layer .",
    "the problem of estimating the number of hidden units in one multilayer perceptron was solved in rynkiewicz ( 2006 ) , but it would be interesting to mix the two results and prove the consistency of a penalized criterion when there is a double non - identifiability problem : number of experts and number of hidden units ."
  ],
  "abstract_text": [
    "<S> bic criterion is widely used by the neural - network community for model selection tasks , although its convergence properties are not always theoretically established . in this paper </S>",
    "<S> we will focus on estimating the number of components in a mixture of multilayer perceptrons and proving the convergence of the bic criterion in this frame . </S>",
    "<S> the penalized marginal - likelihood for mixture models and hidden markov models introduced by keribin ( 2000 ) and , respectively , gassiat ( 2002 ) is extended to mixtures of multilayer perceptrons for which a penalized - likelihood criterion is proposed . </S>",
    "<S> we prove its convergence under some hypothesis which involve essentially the bracketing entropy of the generalized score - functions class and illustrate it by some numerical examples . </S>"
  ]
}