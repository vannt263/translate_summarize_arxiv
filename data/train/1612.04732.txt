{
  "article_text": [
    "word embeddings are representations that use vectors to represent word surface forms .",
    "they are known to be useful in improving the performance of many nlp tasks , from sequence labeling  @xcite , to language modeling  @xcite , to parsing  @xcite and morphological analysis  @xcite .",
    "cross - lingual tasks ( parsing , retrieval , translation ) have been particularly attractive as applications for word embeddings , see  @xcite for a good overview .",
    "continuous word embeddings use real - valued vectors , and are typically induced either via neural networks  @xcite , or neural - network  inspired models  @xcite , designed to learn word embeddings in an unsupervised manner .",
    "the embeddings that result from training such models exhibit certain desirable properties .",
    "one such property is _",
    "syntactic compositionality _ ,",
    "i.e. , the extent to which morpho - syntactic properties ( such as pluralization , past - tense , etc . ) can be represented as vectors in the same embedding space , using simple vector arithmetic to encode the application of this property , e.g. , @xmath0 .",
    "another property is _ semantic compositionality _ ,",
    "i.e. , the extent to which semantic properties ( such as `` maleness '' , `` royalty '' , etc . ) can be represented as vectors in the same embedding space , using simple vector arithmetic to encode the application of such a property , e.g. , @xmath1  @xcite .",
    "another desired property , _ semantic similarity _ , means that words with similar meaning are represented as vectors that are close ( cosine - wise ) in the embedding space .",
    "together with semantic compositionality , it allows for manipulating semantic concepts as points in such an embedding space .",
    "the extension of the semantic similarity property to spaces that embed words from more than one language results in _ multilingual semantic similarity _",
    ", i.e. , the extent to which words that are translations of each other are embedded to vectors that are close in the embedding space  @xcite .        in this paper , we present a unified framework for designing neural - network  inspired embedding models using multigraphs .",
    "the new framework introduces the flexibility to exploit both monolingual and bilingual text , without and with annotations ( e.g. , syntactic dependencies , word alignments ) .",
    "furthermore , it unifies existing models such as the skipgram model of mikolov et al .  , the dependency embedding model of levy and goldberg  , and the biskip model of luong et al .   as particular instances of this framework .",
    "our empirical evaluations show that this framework allows us to build models that yield embeddings with significantly higher accuracy on syntactic compositionality , semantic compositionality , and multilingual semantic similarity .",
    "we find that both syntactic and semantic compositionality accuracy of certain embedding spaces improves for a given language due to the presence of words from other language(s ) , embedded by the same model in a common embedding space .",
    "we also show that multilingual embedding spaces optimized for semantic similarity improve the end - to - end performance of a statistical mt system for english - spanish , english - french , and english - czech translations . in order to isolate the impact of this method on end - to - end translation performance , we perform these experiments on non - neural , phrase - based mt systems with standard dense features  @xcite , rather than the more recently - proposed neural mt systems  @xcite .",
    "the point of this evaluation is not to improve state - of - the - art in mt ; rather , it allows for a simple way to evaluate how good the translations induced by our multilingual embeddings are . to this end",
    ", we measure the impact of using our approach on handling words that are not present in the parallel data on which our mt systems are trained .",
    "potential translations for such words are automatically identified from our multilingual embeddings , and are subsequently used at decode time to produce improved translations for test sentences containing such words . although the _ method _ used to measure the quality of the induced translations ( based on enhancing phrase - tables ) can not be directly used for neural mt systems , the _ mapping _ of surface strings to their embeddings in a common multilingual embedding space _ can _ be used directly in fully - neural mt systems , as an orthogonal method to previously proposed ones , e.g. sub - word unit translations  @xcite .",
    "we describe here multigraph - based embedding models , using a formalism that extends on the skipgram model  @xcite . this extension is achieved by replacing the definition of context in skipgram with the formal definition of multigraph neighborhoods .",
    "we first revisit the skipgram model , then formally define the notion of multigraph and multigraph - based embedding models .      the skipgram neural embedding model of mikolov et al .",
    "uses a vocabulary @xmath2 and an @xmath3-dimensional real - valued space @xmath4 . for each word @xmath5 ,",
    "a vector @xmath6 is associated as the embedding of the word @xmath7 , learned as follows .",
    "we define @xmath8 to be the set of context words of @xmath7 , which is a window of @xmath9 words to the left and right of @xmath7 .",
    "@xmath10 is the complement for @xmath8 .",
    "for example , in the english text ( 1st line ) of figure  [ fig : ex2 ] , @xmath11 .",
    "the training objective is that for all words @xmath7 in the training data , the dot product @xmath12 is maximized for @xmath13 while minimized for @xmath14 .",
    "formally , this is defined as : @xmath15 where @xmath16 . due to its large size",
    ", @xmath10 is approximated via sampling : for each @xmath17 , we draw @xmath3 samples @xmath18 according to the distribution @xmath19 ( @xmath20 is the unigram distribution , @xmath21 the normalization constant ) , where @xmath3 is a model hyper - parameter .      the skipgram embedding models only make use of the context windows .",
    "however , there is much more information in both monolingual and bilingual text , e.g. syntactic dependencies and word alignments .",
    "for example , in figure  [ fig : ex2 ] , besides the dotted edges connecting neighbors in the text surface ( denoted as @xmath22 edges ) , the dashed edges connect words that are in a syntactic dependency relation ( denoted as @xmath23 edges ) , and solid edges connect nodes that represent translation - aligned words ( denoted as @xmath24 edges ) .    in order to make use of all three types of information ,",
    "we first define a set of labels @xmath25 , and a distance function @xmath26 by assigning a distance to each label .",
    "we use notation @xmath27 to represent @xmath28 , @xmath29 , @xmath30 .",
    "if any of the labels are mapped to distance 0 , we omit it from the notation ; for instance , @xmath31 means @xmath32 , @xmath33 , @xmath34 .",
    "next , we show how to incorporate those edges with different labels and distance functions into the same embedding model using multigraphs .",
    "a multigraph is a graph which is allowed to have multiple edges between the same nodes .",
    "we use here the particular notion of an undirected multigraph with identifiable edges ( i.e. , any edge has an identity that helps it being distinguished from a different edge that may share the same end nodes ) .",
    "formally , an undirected multigraph with identifiable edges is a 5-tuple @xmath35 , where @xmath2 is a set of vertices or nodes ; @xmath36 is a set of edges ; @xmath37 is a set of labels ; @xmath38 assigns end nodes to edges ; and @xmath39 assigns identity labels to edges .",
    "we use notation @xmath40 to represent the neighborhood of node @xmath7 in @xmath41 as given by a distance function @xmath42 .",
    "@xmath40 is defined as : @xmath43 where @xmath44 denotes the edges of a length-@xmath9 path from @xmath7 to @xmath45 .",
    "using the example from figure  [ fig : ex2 ] , we have @xmath46 , because there is a length-2 path with two @xmath47-type edges @xmath48 , with @xmath49 and @xmath50 . on the other hand , @xmath51 , because the path contains an @xmath47-type edge @xmath52 and a @xmath53-type edge @xmath54 , hence @xmath55 while @xmath56 .",
    "training the embedding variables @xmath57 is based on the neighborhoods @xmath58 for all words @xmath7 in the multigraph .",
    "the general form of the objective function can take any arbitrary function defined on @xmath58 and @xmath59 , where @xmath59 is the complement of @xmath40 .",
    "we use here a similar objective function to the one used by the skipgram model : @xmath60 where @xmath16 .",
    "due to its large size , @xmath59 is approximated similarly with @xmath61 : for each @xmath62 , we draw @xmath3 samples @xmath63 according to the unigram   distribution @xmath19 , where @xmath3 is a model hyper - parameter .      given sentence - level parallel data with word alignments , equation  [ eq : obj ] can be used to learn parameters for a model such as @xmath64 . under this model ,",
    "the graph neighborhood for @xmath65 in figure  [ fig : ex2 ] is @xmath66 @xmath67 .",
    "the neighborhood for @xmath68 is @xmath69 @xmath70 .",
    "if syntactic dependency relations are available ( for one or both languages in the parallel data ) , we can also learn parameters for model @xmath71 .",
    "for the multigraph in figure  [ fig : ex2 ] , the graph neighborhood for @xmath65 is @xmath72 @xmath73 .",
    "we can also create multigraph models for which the set of labels @xmath37 collapses some of the labels .",
    "for instance , collapsing labels t and a leads to a multigraph model such as @xmath74 , under which both text - based edges and alignment - based edges are traversed without differentiation ( up to distance 3 ) . in this case",
    ", the graph neighborhood for @xmath65 is @xmath75 @xmath76 @xmath77 . for @xmath68",
    ", it is @xmath78 @xmath79 @xmath80    note the  90% overlap between @xmath81 and @xmath82 . since the objective function from equation  [ eq : obj ] imposes that words that appear in similar contexts have similar embeddings , it follows that , under a model like @xmath74 , the embeddings for @xmath65 and @xmath68 should be similar .",
    "this leads to embeddings for words in multiple languages that keeps translations close to each other . on the other hand , models like @xmath64 and @xmath71 do not have this property , but nevertheless give rise to embeddings for words in multiple languages with properties that we analyze and quantify in section  [ sec : evaluation ] .",
    "distributed word representations have been used recently for tackling various tasks such as language modeling  @xcite , paraphrase detection  @xcite , sentiment analysis  @xcite , syntactic parsing  @xcite , and a multitude of cross - lingual tasks  @xcite .",
    "the introduction of the cbow and skipgram embedding models  @xcite has boosted this research direction .",
    "these models are simple and easy to implement , and can be trained orders of magnitude faster than previous models .",
    "subsequent research has proposed models that take advantage of additional textual information , such as syntactic dependencies  @xcite , global statistics  @xcite , or parallel data  @xcite .",
    "prior knowledge can also be incorporated to achieve improved lexical embeddings by modifying the objective function while allowing for the exploitation of existing resources such as wordnet  @xcite , or by modifying the model architecture while targeting specific tasks  @xcite .",
    "our paper describes a mechanism which unifies the way context signals from the training data are exploited .",
    "it exploits the information available in parallel data in an on - line training fashion ( bi / multi - lingual training ) , compared to the off - line matrix transformation proposal of mikolov et al .  . the bilbowa model  @xcite uses a parallel bag - of - word representation for the parallel data , while the biskip model  @xcite achieves bilingual training by exploiting word alignments .",
    "some of these proposals can be formulated as particular instances under our multigraph framework .",
    "for instance , the context window ( with window size @xmath9 ) @xmath83 of the skipgram model is equivalent to model @xmath84 in the multigraph formulation .",
    "the dependency - based embedding model of levy and goldberg   is equivalent to @xmath85 when word and context vocabulary are the same   and @xmath86 . ] .",
    "the biskip model  @xcite with a window of size @xmath9 is equivalent to model @xmath87 in our multigraph formulation .",
    "in addition to subsuming some of the previously - proposed methods , our approach comes with a mathematical foundation ( in terms of multigraphs ) for incorporating information from both monolingual and parallel data .",
    "this formulation allows us to understand and justify , from a formal perspective , some of the empirical results obtained by some of these models .",
    "moreover , our method allows for the exploitation of signals obtained via both unsupervised ( e.g. raw text , parallel text with unsupervised alignments ) and supervised learning ( e.g. , syntactic dependencies ) , while building a common embedding over arbitrary many languages , simply by treating the training data as a multigraph over potentially multiple languages , linked together via multiple bilingual alignments .",
    "a related approach for inducing multilingual embeddings is based on neural networks for automatic translation , either in conjunction with a phrase - table  @xcite , or a fully neural approach  @xcite .",
    "these approaches use signals similar to ours when exploiting parallel training data , but the resulting embeddings are optimized for translation accuracy ( according to the loss - function definition of these models , usually using a maximum - likelihood objective  @xcite or a reinforcement - learning",
    " inspired objective  @xcite ) .",
    "in addition , they do not directly allow for the exploitation of both parallel and monolingual data simultaneously at train time , or the exploitation of additional sources of linguistic information ( such as syntactic dependencies ) .",
    "because our approach enables us to exploit both monolingual and parallel data simultaneously , the resulting distributed representation can be successfully used to learn translations for terms that appear in the monolingual data only ( section  [ sec : ex - eval ] ) .",
    "this represents the neural word - embedding equivalent of a long line of research based on word - surface patterns , starting with earlier attempts  @xcite , and continuing with  @xcite , and complemented by approaches based on probabilistic models  @xcite .",
    "our approach has the advantages of achieving this effect in a completely unsupervised fashion , without exploiting surface patterns , and benefiting from the smoothness properties associated with continuous word representations .    [",
    "cols=\">,>,^,^\",options=\"header \" , ]     as the results in table  [ tab : eval7 ] show , the systems under the test condition score significantly higher compared to the systems under the base condition , in a side - by - side , randomized blind comparison .",
    "this evaluation is done using a pool of professional translators that see base - condition translations and test - condition translations side - by - side , in a randomized position , without knowledge of what change in the mt system is being evaluated ; each evaluator is allowed to score only 1% of the evaluation entries , to reduce bias ; the scale used is from 0 ( useless ) to 6 ( perfect ) .",
    "the values of the delta numbers in table  [ tab : eval7 ] are statistically significant at 95%-confidence using bootstrap resampling  @xcite .",
    "the results indicate that the translations induced using @xmath88 contribute significantly to the translation accuracy of sentences containing poov terms .",
    "moreover , the two examples in table  [ tab : eval7 ] illustrate how our approach is orthogonal to other approaches proposed for dealing with rare or oov words , such as sub - word unit translations  @xcite . whereas for example # 1 one can argue that a rare word like testy could be decomposed as test+y and subsequently translated correctly using sub - word translations , this can not happen with word elan in example # 2 .",
    "the proposed translation under the test condition , creatividad , may not be a perfect rendering of the word meaning for elan , but it is superior to copying the word in the target translation ( as under the base condition ) , or to any potential sub - word  based translation .",
    "this article introduces a novel framework for building continuous word representations as @xmath3-dimensional real - valued vectors .",
    "this framework utilizes multigraphs as an appropriate mathematical tool for inducing word representations from plain or annotated , monolingual or multilingual text .",
    "it also helps with unifying and generalizing several previously - proposed word embedding models  @xcite .",
    "we empirically show that this framework allows us to build models that yield word embeddings with significantly higher accuracy on syntactic and semantic compositionality , as well as multilingual semantic similarity .",
    "we also show that the latter improvement leads to better translation lexicons for words that do not appear in the parallel training data .",
    "the resulting translations are evaluated for end - to - end automatic translation accuracy , and succeed in significantly improving the performance of an automatic translation system .",
    "the family of models defined by our framework is much larger than what we presented here . by choosing different neighborhood or objective functions ,",
    "some of these models may further improve the performance on syntactic / semantic compositionality or similarity , while others may have other properties yet to be discovered .",
    "for instance , machine translation could potentially benefit from a thorough investigation study on the impact of various multigraph - based embeddings in pretraining , which can incorporate a variety of signals ( e.g. , dependency information in source and/or target context ) .    in general , the high accuracy on the intrinsic tasks that some of these models exhibit , combined with their attractive computational costs , makes them prime candidates for further exploring their properties and devising mechanisms to exploit them in end - to - end applications .",
    "ondej bojar , rajen chatterjee , christian federmann , barry haddow , matthias huck , chris hokamp , philipp koehn , varvara logacheva , christof monz , matteo negri , matt post , carolina scarton , lucia specia , and marco turchi .",
    "findings of the 2015 workshop on statistical machine translation . in _ proceedings of the tenth workshop on statistical machine translation _ , pages 146 .",
    "kyunghyun cho , bart van merrienboer , aglar glehre , dzmitry bahdanau , fethi bougares , holger schwenk , and yoshua bengio .",
    "2014 . learning phrase representations using rnn encoder - decoder for statistical machine translation . in _ proceedings of emnlp _ , pages 17241734 .",
    "jacob devlin , rabih zbib , zhongqiang huang , thomas lamar , richard schwartz , and john makhoul .",
    "fast and robust neural network joint models for statistical machine translation . in _ proceedings of acl_.        nikesh garera , chris callison - burch , and david yarowsky .",
    "2009 . improving translation lexicon induction from monolingual corpora via dependency contexts and part - of - speech equivalences . in _ proceedings of conll _ ,",
    "pages 129137 .",
    "kejun huang , matt gardner , evangelos papalexakis , christos faloutsos , nikos sidiropoulos , tom mitchell , partha  p. talukdar , and xiao fu .",
    "translation invariant word embeddings . in _ proceedings of emnlp _ , pages 10841088 .",
    "andriy mnih and geoffrey  e. hinton .",
    "three new graphical models for statistical language modelling . in _ machine learning , proceedings of the twenty - fourth international conference _ , pages 641648 .",
    "hendra setiawan , zhongqiang huang , jacob devlin , thomas lamar , rabih zbib , richard  m. schwartz , and john makhoul .",
    "statistical machine translation features with multitask tensor networks . in _ proceedings of acl _ , pages 3141 .",
    "richard socher , eric  h. huang , jeffrey pennington , andrew  y. ng , and christopher  d. manning .",
    "dynamic pooling and unfolding recursive autoencoders for paraphrase detection . in _ nips _ , pages 801809 .",
    "richard socher , jeffrey pennington , eric  h. huang , andrew  y. ng , and christopher  d. manning .",
    "semi - supervised recursive autoencoders for predicting sentiment distributions . in _ proceedings of emnlp _ , pages 151161 .",
    "ilya sutskever , oriol vinyals , and quoc v.  v le .",
    "sequence to sequence learning with neural networks . in _ advances in neural information processing systems 27 _ , pages 31043112 .",
    "curran associates , inc .",
    "y.  wu , m.  schuster , z.  chen , q.  v. le , m.  norouzi , w.  macherey , m.  krikun , y.  cao , q.  gao , k.  macherey , j.  klingner , a.  shah , m.  johnson , x.  liu , l.  kaiser , s.  gouws , y.  kato , t.  kudo , h.  kazawa , k.  stevens , g.  kurian , n.  patil , w.  wang , c.  young , j.  smith , j.  riesa , a.  rudnick , o.  vinyals , g.  corrado , m.  hughes , and j.  dean .",
    "google s neural machine translation system : bridging the gap between human and machine translation . , abs/1609.08144 ."
  ],
  "abstract_text": [
    "<S> we present a family of neural - network  inspired models for computing continuous word representations , specifically designed to exploit both monolingual and multilingual text . </S>",
    "<S> this framework allows us to perform unsupervised training of embeddings that exhibit higher accuracy on syntactic and semantic compositionality , as well as multilingual semantic similarity , compared to previous models trained in an unsupervised fashion . </S>",
    "<S> we also show that such multilingual embeddings , optimized for semantic similarity , can improve the performance of statistical machine translation with respect to how it handles words not present in the parallel data . </S>"
  ]
}