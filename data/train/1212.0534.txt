{
  "article_text": [
    "in this paper we develop a methodology we refer to as split sampling methods to provide more precise estimates for high dimensional expectations and rare event probabilities .",
    "we show that more precise estimators can be achieved by splitting the expectation of interest into a number of easier - to - estimate normalisation constants and then integrating those estimates to produce an estimate of the full expectation . to do this ,",
    "we employ an auxiliary variable mcmc approach with a family of splitting functions and a weighting function on the conditional distribution of the auxiliary random variable .",
    "we allow for an adaptive mcmc approach to specify our weighting function .",
    "we relate our method to the product estimator ( diaconis and holmes , 1994 , fishman , 1994 ) which splits the rare event probability into a set of relatively larger conditional probabilities which are easier to estimate and to nested sampling ( skilling , 2006 ) for the estimation of expectations .",
    "other variance reduction techniques , such as control variates will provide further efficiency gains ( see dellaportas and kontoyiannis , 2012 , mira et al , 2012 ) .",
    "there are two related approaches in the literature .",
    "one approach is for estimating expectations is nested sampling ( skilling , 2006 ) which sequentially estimates the quantiles of the likelihood function under the prior .",
    "other normalisation methods include bridge and path sampling ( meng and wong , 1996 , gelman and meng , 1998 ) , generalized versions of the wang - landau algorithm ( wang and landau , 2001 ) , and the tpa algorithm of huber and schott ( 2010 ) .",
    "serial tempering ( geyer , 2010 ) and linked importance sampling ( neal , 2005 ) provide ratios of normalisation constants for a discrete set of unnormalised densities .",
    "the second approach is cross entropy ( rubinstein and glynn , 2009 , asmussen et al , 2012 ) which sequentially constructs an optimal variance - reducing importance function for calculating rare event probabilities .",
    "we show that our adaptively chosen weighted mcmc algorithm can provide efficiency gains over cross entropy methods .",
    "the problem of interest is to calculate an expectation of interest , @xmath0 , where @xmath1 is a likelihood and @xmath2 is a prior measure . for rare event probabilities , @xmath3 , and",
    "the splitting functions @xmath4 are specified by level sets in the likelihood , namely @xmath5 .",
    "this occurs naturally in the rare event simulation literature and in nested sampling . to develop an efficient estimator",
    ", we define a joint split sampling split distribution , @xmath6 , on @xmath7 and an auxiliary variable @xmath8 that tilts the conditional distribution with a weighting function , @xmath9 .",
    "we provide a default setting for the weight function to match the sampling properties of the product estimator and of nested sampling .",
    "we also allow for the possibility of adaptively learning a weight function from the mcmc output . as with other ergodic monte carlo methods , we assume that the researcher can construct a fast mcmc algorithm for sampling our joint distribution",
    ".    the rest of the paper is outlined as follows .",
    "section 2 details our split sampling methodology . a key identity `` splits '' the expectation of interest into an integrated set of rare event normalisation constants .",
    "mcmc then provides an estimator of the marginal distribution of the auxiliary variable , which in turn provides our overall estimator .",
    "we provide a number of guidelines for specifying our weight function in both discrete and continuous settings .",
    "section 3 describes the relationship with the product estimator and nested sampling methods . in both cases ,",
    "we provide a theorem that provides a default choice of weight function to match the sampling behavior of the product estimator and nested sampling .",
    "section 4 applies our methodology to a shortest path rare event probability and to the calculation of a normalisation constant for a spike - and - slab mixture of gaussians .",
    "we illustrate the efficiency gains of split sampling over crude monte carlo , the product estimator and the cross entropy method .",
    "finally , section 5 concludes with directions for future research .",
    "we now introduce the notation to characterize the estimation problems and to develop our method .",
    "the central problem is to calculate an expectation of a non - negative functional of interest , which we denote as @xmath10 , under a @xmath11-dimensional probability distribution @xmath12 .",
    "we write this expectation as : @xmath13 the corresponding rare event probability is given by @xmath14 we interpret @xmath15 as a `` prior '' distribution , @xmath10 as a likelihood , and @xmath8 as an auxiliary variable . given a family of splitting functions , @xmath4 , their normalisation constants are , @xmath16 . for rare events , @xmath17 where @xmath8 is large . here",
    "we assume that @xmath10 is continuous with respect to the prior @xmath2 and @xmath18 .",
    "split sampling works as follows .",
    "we define the set of `` tilted '' distributions and corresponding normalisation constants by @xmath19 for rare events , @xmath20 and @xmath21 corresponds to conditioning on level sets of @xmath10 . the @xmath22 s",
    "correspond to `` rare '' event probabilities @xmath23 for the functional @xmath10 , the expectation of interest , @xmath24 , is an integration of the rare event probabilities . using fubini and writing @xmath25 we have the key identity @xmath26 we have `` split '' the computation of @xmath24 into a set of easier to compute normalisation constants @xmath22 .",
    "we will simultaneously provide an estimator @xmath27 and @xmath28 .    to do this ,",
    "we further introduce a weight function on the auxiliary variable , @xmath9 , and the cumulative weight @xmath29 .",
    "the joint split sampling density , @xmath30 , is defined with @xmath31 as @xmath32 where @xmath33 .",
    "the marginals on @xmath8 and @xmath7 are @xmath34 where @xmath35 .",
    "the key feature of our split sampling mcmc draws , @xmath36 , are that they provide an efficient rao - blackwellised estimate of the marginal @xmath37 without the knowledge of the @xmath22 s .",
    "we now show how to estimate @xmath38 and @xmath39 .    with @xmath40 ,",
    "the joint splitting density is @xmath41 the conditional posterior of the auxiliary index @xmath8 given @xmath7 is @xmath42 the density is proportional to the weight function @xmath9 on the interval @xmath43 $ ] .",
    "slice sampling corresponds to @xmath44 and uniform sampling on @xmath43 $ ] .",
    "this would lead to direct draws from the posterior distribution @xmath45 and the resultant estimator would be the harmonic mean .",
    "our approach will weight towards regions of smaller @xmath8 values to provide an efficient estimator of all the rare event probabilities @xmath22 and hence of @xmath24 .",
    "the marginal density estimator of @xmath8 is @xmath46 this is a re - weighted version of the initial weights @xmath9 .",
    "the function @xmath47 will be used to adaptively re - balance the initial weights @xmath9 in our adaptive version of the algorithm , see section 4 .",
    "we now derive estimators for @xmath22 and @xmath24 by exploiting a rao - blackwellised estimator for the marginal density , @xmath48 . from , we have @xmath49 and so an estimate of @xmath22 is given by @xmath50 with @xmath51 , this provides a new estimator @xmath38 , where @xmath52 , given by @xmath53 to find @xmath54 , we use the summation - integral counterpart to fubini and the fact that @xmath55 to yield @xmath56 therefore , we have our estimator @xmath57 we now describe our split sampling algorithm .    *",
    "algorithm : split sampling *    * draw samples @xmath58 by iterating @xmath59 and @xmath60 * estimate the marginal distribution , @xmath61 , via @xmath62 * estimate the individual normalisation constants , @xmath38 , via @xmath63 * compute a new estimate , @xmath64 , via @xmath65    a practical use of the algorithm will involve a discrete grid @xmath66 .",
    "we write the rare event probabilities @xmath67 and the weights @xmath68 .",
    "the marginal probabilities @xmath69 are estimated by rao - blackwellization as @xmath70 with @xmath71 , the estimator is @xmath72 for @xmath73 . in the next sections we provide a default choice of weights to match the sampling behaviour of the product estimator and nested sampling together with an adaptive mcmc scheme for estimating the weights .",
    "first , we turn to convergence issues .",
    "roberts and rosenthal ( 1997 ) , mira and tierney ( 2002 ) and hobert et al ( 2002 ) who provide general conditions for geometric ergodicity of slice sampling .",
    "geometric ergodicity will imply a monte carlo clt for calculating asymptotic distributions and standard errors .",
    "our chain is geometrically ergodic if @xmath74 is bounded , and there exists an @xmath75 such that @xmath76 is nonincreasing on @xmath77 for some @xmath78 where @xmath79 then we can apply a central limit theorem to ergodic averages of the functional @xmath80 which yields the condition @xmath81 we have a central limit theorem for @xmath82 at any @xmath83 , where @xmath84 @xmath85",
    "this argument also works at @xmath86 as long as @xmath87 .",
    "the standard monte carlo estimate of @xmath88 is @xmath89 where @xmath90 , with draws possibly obtained via mcmc .",
    "this is too inaccurate for rare event probabilities .",
    "von neumann s original view of importance sampling was as a variance reduction to improve this estimator . by viewing the calculation of an expectation as a problem of normalising a posterior distribution ,",
    "we can write @xmath91 importance sampling uses a blanket @xmath92 to compute @xmath93 picking @xmath94 to be the posterior distribution @xmath95 leads to the estimator @xmath96 with zero variance .",
    "while impractical , this suggests finding a class of importance blankets @xmath92 that are adaptive and depend on @xmath10 can exhibit good monte carlo properties .",
    "split sampling specifies a class of importance sampling blankets , indexed by @xmath9 , by @xmath97 the estimator @xmath38 in can be viewed as an importance sampling estimator where we average @xmath98 over the splitting set @xmath99 with @xmath52 .",
    "similarly we can express @xmath64 as an importance sampling estimator as in which uses a proposal distribution proportional to @xmath100 .",
    "a standard approach to calculating the rare event probability @xmath101 is the product estimator .",
    "we set @xmath102 for some @xmath103 and introduce a discrete grid @xmath104 of @xmath8-values starting at @xmath105 .",
    "the conditional probability estimator writes @xmath106 or equivalently @xmath107 .",
    "variance reduction is achieved by splitting @xmath108 into pieces @xmath109 of larger magnitude which are relatively easier to estimate . with @xmath110 ,",
    "we estimate @xmath111 given @xmath112 independent samples from the tilted distributions @xmath113 for each @xmath114 , we have @xmath115 with mean @xmath116 and variance @xmath117 .",
    "the product estimator , as well as the cross - entropy estimator , relies on a set of independent samples drawn in a sequential fashion .",
    "split sampling , on the other hand , uses a fast mcmc and ergodic averaging to provide an estimate @xmath118 .",
    "the monte carlo variation @xmath119 can be determined from the output of the chain .",
    "controlling the monte carlo error of this estimator is straightforward due to independent samples with relative mean squared error , see fishman ( 1994 ) and garvels et al ( 2002 ) ,      we can now compare the product estimator with split sampling .",
    "suppose @xmath120 and let @xmath104 be the grid points of @xmath10 for @xmath121 where @xmath122 . by construction ,",
    "@xmath104 are the @xmath123 @xmath124-quantile of @xmath10 , and we have @xmath125 .",
    "there are two versions of the product estimator .",
    "first , the standard product estimator has the long - run sampling distribution @xmath126 the second product estimator includes samples from the previous level generation that were above the threshold .",
    "this has the long - run sampling distribution @xmath127 we call this the product estimator with inclusion .    the two product estimators and the split sampling are related as follows .",
    "the standard product estimator corresponds to the ( discrete ) split sampling with @xmath128 .",
    "the product estimator with inclusion is equivalent to split sampling with cumulative weights @xmath129    split sampling with discrete knots @xmath104 , weights @xmath130 and @xmath131 has sampling distribution @xmath132 therefore , this is equivalent to the product estimator if and only if , for @xmath133 , @xmath134 as @xmath104 are the @xmath123 @xmath124-quantile of @xmath10 , we have @xmath135 and @xmath136 for the product estimator with inclusion , the equivalent split sampling weights are @xmath137      we now provide a comparison with nested sampling .",
    "we provide a specification of @xmath9 so that the sampling distribution of matches that of nested sampling .",
    "we can adaptively determine the values from our mcmc output .",
    "let @xmath138 be the @xmath139-quantile of the likelihood function @xmath10 under the prior @xmath12 .",
    "then , nested sampling expresses @xmath140 where @xmath141 .",
    "this integral can be approximated by quadrature @xmath142 the larger @xmath112 is , the more accurate the approximation , and so this suggests the following estimator @xmath143 where @xmath144 are the simulated @xmath145-quantiles of likelihoods with respect to the prior .",
    "here , @xmath146 is the total number of samples .",
    "nested sampling is a sequential simulation procedure for finding the @xmath147-quantiles , @xmath144 , by sampling @xmath148 .",
    "brewer et al ( 2011 ) propose a diffuse nested sampling approach to determine the levels @xmath149 . both nested and diffuse nested sampling are product estimator approaches .",
    "the quantiles @xmath150 are chosen so that each level @xmath149 occupies @xmath151 times as much prior mass as the previous level @xmath152 .",
    "diffuse nested sampling achieves this by sequentially sampling from a mixture importance sampler @xmath153 where the weights are exponential @xmath154 for some @xmath155 .",
    "mcmc methods are used to traverse this mixture distribution with a random walk step for the index @xmath156 that steps up or down a level with equal probability .",
    "a new level is added using the @xmath157-quantile of the likelihood draws . using diffuse",
    "nested sampling allows some chance of the samples escaping to lowered constrained levels and to explore the space more freely .",
    "one caveat is that a large contribution can come from values of @xmath158 near the origin and we have to find many levels @xmath159 to obtain an accurate approximation .",
    "murray et al ( 2006 ) provide single and multiple sample versions of nested sampling algorithms .",
    "if @xmath160 is known , we sample as follows :    1 .",
    "set @xmath161 .",
    "2 .   generate @xmath112 samples @xmath162 from @xmath163 and sort @xmath164 .",
    "3 .   repeat while @xmath165 : 1 .",
    "set @xmath166 and @xmath167 .",
    "2 .   generate @xmath168 and set @xmath169 .",
    "3 .   sort @xmath144 s and set @xmath170 .",
    "4 .   set @xmath171 and stop .",
    "if @xmath172 is not known , replace step 3 with :    1 .",
    "repeat while @xmath173 :      we now choose @xmath9 to match the sampling properties of nested sampling .",
    "the main difference between split and nested sampling is that in split sampling we specify a weight function @xmath9 for @xmath174 and sample from the full mixture distribution , rather than employing a sequential approach for grid selection which requires a termination rule .",
    "another difference is that split sampling estimator does not need to know the ordered @xmath144 s .",
    "we can now match the sampling distributions of split and nested sampling ( see , skilling , 2006 ) .",
    "the expected number of samples @xmath144 less than @xmath8 is @xmath175 .",
    "assume @xmath176 for a while .",
    "since @xmath177 are independent standard uniforms and @xmath178 the distribution of the number of samples @xmath144 less than @xmath8 is same as the number of arrivals before @xmath179 of a poisson process with rate @xmath180 . for general @xmath112",
    ", it only changes the arrival rate of the poisson process into @xmath112 .    if we pick the weights @xmath9 such that @xmath181 then the sampling distributions of split and nested sampling match .",
    "the sampling distribution of the nested sampling for finite @xmath146 is hard to calculate , but we can observe limiting results .",
    "as @xmath182 , if @xmath183 , then we have @xmath184 split sampling has marginal density of @xmath7 given by @xmath185 the tail distribution function @xmath186 is then @xmath187 we now find the importance splitting density that matches the nested sampling distributional properties in the sense that @xmath188 . since @xmath189 where @xmath190 .",
    "we therefore set @xmath191 .",
    "since @xmath22 is unknown , we are not ready to begin sampling . as a remedy we propose approximating @xmath22 .",
    "we estimate @xmath192 s for certain grid points @xmath193 and interpolate @xmath22 by , for example , a piecewise exponentially increasing function .",
    "as we assume no information on @xmath10 a priori , we have to start with a single grid point @xmath105 and build more grid points as sampling goes .",
    "when we have collected enough samples higher than the current top grid point , we add a new grid point and adjust the approximated @xmath194 function .",
    "for that purpose , we introduce a condition that lets us monitor the number of visits , @xmath195 , to the current top level of the likelihood before we construct a new level . specifically , we run    1 .",
    "set @xmath196 , @xmath197 , @xmath198 , and @xmath71 .",
    "2 .   while @xmath199 , set @xmath200 1 .   draws @xmath201 , and set @xmath202 .",
    "2 .   obtain @xmath203 if @xmath204 or @xmath205 otherwise , where @xmath206 .",
    "3 .   repeat ( a ) and ( b ) until we have @xmath195 visits to level @xmath207 .",
    "4 .   choose the @xmath208-quantile of likelihoods of level @xmath207 as @xmath209 .",
    "set @xmath210 and @xmath211 .    under the condition @xmath212",
    ", the chain will visit each level roughly uniformly .",
    "however , it may take a long time to reach the top level , and the uncertainty in @xmath213 may act like a hurdle for visiting upper levels . with these concerns , it is desirable to favor upper levels by replacing step ( d ) with    1 .   1 .",
    "set @xmath214 .",
    "we call @xmath215 the boosting factor as @xmath215 increases the preference for the upper levels .",
    "this reduces the search time and ensures the time complexity to be @xmath216 . to further expedite this procedure",
    ", we may put more weight on the top level @xmath159 by substituting ( d ) with the step :    1 .   1 .   set @xmath217 and @xmath218 .",
    "for example , if @xmath219 , the chain spends half of the time on the top level and the other half backtracking the other levels .",
    "once we identify all levels , our split sampling algorithm runs :    1 .",
    "set @xmath220 and @xmath221 for each @xmath222 .",
    "2 .   while @xmath223 , set @xmath170 .",
    "1 .   draw @xmath224 with @xmath225 and @xmath226 , and set @xmath202 .",
    "2 .   for each @xmath114 with @xmath227 , update @xmath228 .",
    "3 .   update @xmath229 and set @xmath230 .    from a practical perspective , it is critical to have nonzero initial values on @xmath231 .",
    "if we start with @xmath232 , the early @xmath233 and @xmath234 are unstable , and the whole procedure can become abortive .",
    "since , though not very accurate , @xmath235 is a reasonable initial estimate , we use them for the initial values of @xmath233 and @xmath234 .",
    "@xmath236 reflects the degree of dependence on those initial values .",
    "another point to make is even if the initial @xmath230 are not as accurate as needed to guarantee good mixing of our mcmc iterations , we dynamically refine @xmath234 as in step 2(c ) .",
    "the beauty of this algorithm is that this update makes the chain self - balanced .",
    "when @xmath234 is larger than it should be , or @xmath233 is smaller , the chain visits level @xmath114 more often . thus increasing @xmath233 and decreases @xmath234 , which helps @xmath233 converge more quickly to the true value .    at first sight , the time complexity appears to be @xmath237 since steps 2(b ) and 2(c ) cost @xmath216 operations .",
    "however , if the @xmath104 values are chosen so that @xmath192 are exponentially decreasing , the work can be done in @xmath238 time .",
    "the updates needed at steps 2(b ) and 2(c ) are only for the last several @xmath114 s since the increment @xmath239 becomes negligible very quickly relative to @xmath231 as @xmath114 decreases .",
    "the previous subsection assumed that @xmath9 is fixed .",
    "the `` correction factor '' @xmath241 in the construction of @xmath38 needs to be estimated as accurately as possible . to do this",
    "we will use an adaptive choice of the weight function , @xmath9 and use convergence results from the adaptive mcmc literature .",
    "a common initialisation is to set @xmath242 which leads to draws from the posterior .",
    "then , @xmath243 .",
    "this leads to an estimate of the marginal , @xmath244 , given by the measure @xmath245 the density estimate will be zero for @xmath246 by construction .",
    "we also have a set of estimates @xmath247 where @xmath248 which can be used to re - balance to weights @xmath249 .",
    "given an initial run of the algorithm , we can re - proportion the prior weight function to regions we have not visited frequently enough . to accomplish this ,",
    "let @xmath250 be a desired target distribution for @xmath251 , for example a uniform measure .",
    "then re - balance the weights inversely proportional to the visitation probabilities and set the new weights @xmath252 by @xmath253 this will only adjust our weights in the region where @xmath254 .",
    "as the algorithm proceeds we will sample regions of higher likelihood values and further adaptive our weight function .",
    "other choices for weights are available .",
    "for example , in many normalisation problems @xmath22 will be exponential in @xmath8 due to a laplace approximation argument .",
    "this suggests taking an exponential weighting @xmath255 for some @xmath256 . in this case",
    ", we have @xmath257 the marginal distribution is @xmath258 we can also specify @xmath9 to deal with the possibility that the chain might not have visited all states by setting a threshold @xmath259 which corresponds to the maximum allowable increase in the log - prior weights .",
    "this leads to a re - balancing rule @xmath260 where we have also re - normalised the value of the largest state to one .",
    "when @xmath261 is available , we set @xmath262 and @xmath263 for @xmath264 .",
    "to initialise @xmath9 , we use the harmonic mean @xmath265 for @xmath266 and an exponential interpolation for @xmath9 .",
    "drawing @xmath267 , we have @xmath268 the harmonic mean estimator ( raftery et al , 2007 ) is known to have poor monte carlo error variance properties ( polson , 2006 , wolpert and schmidler , 2012 ) although we are estimating @xmath9 and not its inverse .",
    "we can extend this insight to a fully adaptive update rule for @xmath269 , similar to stochastic approximation schemes . define a sequence of decreasing positive step sizes @xmath270 with @xmath271 .",
    "a practical recommendation is @xmath272 where @xmath273 $ ] , see e.g. sato and ishii ( 2000 ) .",
    "another approach is to wait until a `` flat histogram '' ( fh ) condition holds : @xmath274 for a pre - specified tolerance threshold , @xmath275 .",
    "the measure @xmath276 tracks our current estimate of the marginal auxiliary variable distribution .",
    "the rao - blackwellised estimate @xmath277 further reduces variance .",
    "the empirical measure can be used to update @xmath278 as the chain progresses .",
    "let @xmath279 denote the points at which @xmath280 will be decreased according to its schedule .",
    "then an update rule which guarantees convergence is to set @xmath281 jacob and ryder ( 2012 ) show that if @xmath282 is only updated on a sequence of values @xmath283 which correspond to times that a `` flat - histogram '' criterion is satisfied , then convergence ensues and the fh criteria is achieved in finite time . after updating @xmath284 , we re - set the counting measure @xmath285 and continue .",
    "other adaptive mcmc convergence methods are available in atchade and liu ( 2010 ) , liang et al ( 2007 ) , and zhou and wong ( 2008 ) .",
    "bornn et al ( 2012 ) provides a parallelisable algorithm for further efficiency gains .",
    "peskun ( 1973 ) provides theoretical results on optimal mcmc chains to minimise the variance of mcmc functionals .",
    "one desirable monte carlo property for an estimator is a bounded coefficient of variation . for simple functions ,",
    "@xmath286 and @xmath287 , mixture importance functions achieve such a goal , see iyengar ( 1991 ) and adler et al ( 2008 ) .",
    "madras and piccioni ( 1999 , section 4 ) hint at the efficiency properties of dynamically selected mixture importance blankets .",
    "gramacy et al ( 2010 ) propose the use of importance tempering .",
    "johansen et al ( 2006 ) use logit annealing implemented via a sequential particle filtering algorithm .",
    "we suggest a simple , sequential , empirical approach to selecting a `` cooling schedule '' in our approach .",
    "specifically , set @xmath105 , then given @xmath288 we sample @xmath289 .",
    "we order the realisations of the criteria function @xmath290 and set @xmath291 equal to the @xmath292-quantile of the @xmath293 samples .",
    "this provides a sequential approach to solving @xmath294 a number of authors have proposed `` optimal '' choices of @xmath295 , which implicitly defines a cooling schedule , @xmath104 , for @xmath296 .",
    "lecuyer et al ( 2006 ) and amrein and kunsch ( 2011 ) propose @xmath297 and @xmath298 , respectively .",
    "huber and schott ( 2010 ) define a well - balanced schedule as one that satisfies @xmath299 .",
    "they show that such a choice leads to fast algorithms .",
    "the difficulty is in finding the right order of magnitude of @xmath300 and the associated schedule @xmath104 that ensures that each slice @xmath301 is not exponentially small . for rare events , we sample until @xmath302 and then set @xmath303 .",
    "our initial estimate @xmath304 and our weights are @xmath305 .    in hard cases , such as the multimodal mixture of gaussians , the normalising constants @xmath22 are not exponential in @xmath8 .",
    "in such cases we initialize the weights by a piecewise exponential obtained by interpolating any point @xmath306 by @xmath307 where @xmath308 . for @xmath309 , we use @xmath310 . the final estimator is given by @xmath311 finally , our methodology can be viewed as an adaptive mixture importance sampler . as we rebalanced the weights we are adaptive changing the target distribution of our mcmc algorithm rather than the traditional adaptive proposal approaches with a fixed target .",
    "other similar approaches include umbrella sampling ( torrie and valleau , 1997 ) which can be seen as a precursor to many of the current advanced mc strategies such as the wang - landau algorithm and its generalisations for sampling high dimensional multimodal distributions .",
    "these algorithms exploit an auxiliary variable and by their adaptive nature improve estimates continuously as the simulation advances .",
    "the main difference is how each algorithm traverses low and high energy states .",
    "the wang - landau algorithm aims to achieve a uniform distribution on the auxiliary variable , thus spending more time in low energy states than high states states as opposed to multicanonical sampling ( berg and neuhaus , 1992 ) , @xmath312-ensemble sampling ( hesselbo and stinchcombe , 1995 ) or simulated tempering ( geyer and thompson , 1995 ) .",
    "calculating rare event probabilities is a common goal of many problems .",
    "rubinstein and kroese ( 2004 ) consider the total length of the shortest path on a weighted graph with random weights @xmath313 .",
    "suppose there are 4 vertices @xmath314 , @xmath315 , @xmath275 , and @xmath316 .",
    "the adjacent weight matrix is given by @xmath317.\\ ] ] each weight @xmath318 follows an independent exponential distribution with scale parameter @xmath319 with joint distribution given by @xmath320 the goal is to estimate the probability of the rare event corresponding to the length of the shortest path from @xmath314 to @xmath316 @xmath321 we will consider three cases : @xmath322 and @xmath323 where the true rare event probabilities are @xmath324 these can be estimated by the split sampler ( ss ) with @xmath325 and level breakpoints @xmath326 .",
    "@xmath327 is the estimator .",
    "we implement three other competing estimators .",
    "first , the crude monte carlo ( cmc ) estimator simulates @xmath328 and estimates the rare event probabilities by @xmath329 second , the conditional probability product ( cpp ) estimator @xmath330 calculates the @xmath208-quantile @xmath331 of @xmath332 samples of @xmath333 under @xmath334 for all @xmath335 with @xmath105 , @xmath336 , and @xmath337 .",
    "this estimator is defined as : @xmath338 to find @xmath339 we need to sample @xmath340 .",
    "we use gibbs sampling with complete conditionals @xmath341 given by truncated exponential distributions . by the lack of memory property , we have @xmath342 the other conditionals @xmath343 follow in a similar manner .",
    "third , the _ cross - entropy _ ( ce ) estimator ( de boer et al , 2005 ) calculates an `` optimal '' importance blanket , @xmath344 , parameterised by @xmath345 .",
    "then it draws @xmath346 samples of @xmath347 and estimates the shortest path probability @xmath348 the sequential algorithm for finding @xmath345 is similar in spirit to the product estimator approach : set @xmath349 and @xmath350 .",
    "choose @xmath295 ; typically @xmath351 .",
    "then perform    .[rare_tbl ] rare event probabilities simulation [ cols=\"^,^,^,^,^,^,^,^,^,^,^,^,^,^ \" , ]      the trace plot of visits to likelihood levels and changes of @xmath234 in de - centered gaussians case ]",
    "the advantage of the class of split sampling densities is that the resultant estimator of @xmath24 can be implemented via an auxiliary mcmc algorithm from a joint distribution @xmath352 indexed by a random auxiliary variable @xmath353 .",
    "moreover , it allows an adaptive choice of @xmath278 to reduce the monte carlo error of the resultant estimator .",
    "convergence results rely on adaptive mcmc literature",
    ". roberts ( 2010 ) observes that mcmc methods are likely to achieve the largest efficiency gains in rare event probabilities ( glasserman et al , 1999 , glynn et al , 2010 ) and in counting problems where the resultant chains can be hard to sample exactly .",
    "split sampling illustrates the adaptive importance sampling nature of nested sampling and cross - entropy methods .",
    "there is also a clear relationship with slice sampling ( polson , 1996 , neal , 2003 ) as one can view the sampling of the posterior , @xmath354 , as the marginal from the augmented distribution @xmath355 .",
    "the main difference is that split sampling runs a markov chain that traverses the whole space defined by @xmath356 to find regions where @xmath9 needs to be refined . both ce and ns methods using a sequential sampling procedure as in the cpp estimator to split the quantity of interest into estimable pieces .",
    "further research is required to tailor the specification of the weight function @xmath9 to the problem at hand .",
    "we leave open the question of an optimal choice of @xmath4 . here",
    "we have focused on @xmath357 , however , using logit - type functions might led to faster converging mcmc algorithms .",
    "the key to the efficiency of split sampling is being able to construct a rapidly mixing mcmc algorithm to sample the mixture distribution @xmath6 .",
    "we aim to report on direct applications in bayesian inference in future work .",
    "for example , murray et al ( 2006 ) shows that nested sampling performs well for markov random fields models and split sampling should have similar properties .",
    "adler , r.j . , j. blanchet and j. liu ( 2008 ) .",
    "efficient simulation for tail probabilities of gaussian random fields .",
    "_ proceedings of the winter simulation conference _ , 328 - 336 ."
  ],
  "abstract_text": [
    "<S> in this paper we develop a methodology that we call split sampling methods to estimate high dimensional expectations and rare event probabilities . </S>",
    "<S> split sampling uses an auxiliary variable mcmc simulation and expresses the expectation of interest as an integrated set of rare event probabilities . </S>",
    "<S> we derive our estimator from a rao - blackwellised estimate of a marginal auxiliary variable distribution . </S>",
    "<S> we illustrate our method with two applications . </S>",
    "<S> first , we compute a shortest network path rare event probability and compare our method to estimation to a cross entropy approach . </S>",
    "<S> then , we compute a normalisation constant of a high dimensional mixture of gaussians and compare our estimate to one based on nested sampling . we discuss the relationship between our method and other alternatives such as the product of conditional probability estimator and importance sampling . </S>",
    "<S> the methods developed here are available in the r package : splitsampling .    </S>",
    "<S> rare events , cross entropy , product estimator , slice sampling , mcmc , importance sampling , serial tempering , annealing , adaptive mcmc , wang - landau , nested sampling , bridge and path sampling . </S>"
  ]
}