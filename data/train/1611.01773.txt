{
  "article_text": [
    "since 2012 when alexnet won the first place in the imagenet competition  @xcite , convolutional neural networks ( cnns )  @xcite , have been producing state - of - the - art results in many of the most challenging vision tasks including image classification  @xcite , face recognition  @xcite , semantic segmentation  @xcite and object detection  @xcite .",
    "cnns have become the workhorse of many learning tasks and real - world applications beyond computer vision , such as natural language understanding and speech recognition  @xcite .",
    "recent studies have demonstrated both theoretically  @xcite and empirically  @xcite that the depth of neural networks is crucial for their representation ability with limited units  @xcite .",
    "however , in practice , a deeper network often does not necessarily lead to better performance mainly due to the implied training difficulties .",
    "it is shown in @xcite that a plain network with large depth often _ under - fits _ , instead of over - fitting , the training data , leading to a poor test accuracy . more critically , when the number of layers @xmath0 is very large , the network performance may be severely degraded , largely due to the training difficulty caused by gradient vanishing or exploding  @xcite .    recently , he proposed a new network architecture called the deep residual network ( resnet )  @xcite , which conducts forward inference by @xmath1 with @xmath2 being the input data , @xmath3 being the nonlinear activation function ( e.g. , the rectified linear unit ( relu ) : @xmath4  @xcite ) , and @xmath5 being the so called residual , a transformation function ( e.g. , the convolution operation  @xcite ) parameterized by @xmath6 .",
    "the term @xmath7 in eqn",
    ".   introduces what are called _ shortcut connections _ , and the model is reduced to a plain network when @xmath8 .",
    "the shortcut connections enable the features of any layer @xmath9 to be propagated to deeper layers , which helps to address the gradient vanishing issue .",
    "in fact , assuming @xmath10 and @xmath11 ( i.e. , @xmath12 is an identity activation function ) , and applying the recursive rule in eqn.([eq : residual ] ) , we achieve a simplified relation : @xmath13  @xcite . letting @xmath14",
    "be the loss of the final output , then for any shallow layer @xmath9 , the gradient of @xmath14 w.r.t .",
    "@xmath15 can be written as @xmath16 according to eqn.([eq : gradient_resnet ] ) , the gradient of a layer @xmath17 is unlikely to vanish even when the weights are arbitrarily small , which makes the training of very deep networks with hundreds or thousands of layers possible  @xcite . with such depth ,",
    "resnet has achieved state - of - the - art accuracy for several challenging tasks on imagenet  @xcite and ms coco competitions  @xcite . however , although gradient is maintained , the supervision signal may none - the - less vanish for very deep networks . in eqn.([eq : gradient_resnet ] ) , the term @xmath18 reflects the supervision information obtained from the loss ; while @xmath19 indicates the transformation of the network over @xmath20 .",
    "given a large @xmath0 , @xmath21 for a small @xmath9 can be very large and hence may dominate the resulting gradient .",
    "the supervision information reaching the shallower layers may thus not be sufficient to drive the desired behavior .",
    "this may result in difficulties in training the intermediate layers , and lead to model redundancy as network depth increases .",
    "[ t ]     indicates a fully connected layer , @xmath22 denotes softmax losses of auxiliary outputs , @xmath14 denotes softmax loss of the final output , @xmath23 denotes the width of network and @xmath24 denotes the weights on the auxiliary cost functions . ]    very deep cnns can also have other limitations in practice .",
    "first , given a very large number of layers , the model becomes more complex and the generalization ability may also suffer  @xcite . especially , when there is insufficient training data available , over - fitting is inevitable  @xcite .",
    "second , the number of model parameters increases dramatically , with depth , however , leading to a dramatic increase in inference cost . in real - world applications",
    ", there is a demand for more computationally efficient models , particularly for models capable of fast inference  @xcite .",
    "third , when the network is sufficiently deep , adding even a large number of layers effects only a marginal improvement in performance , which implies that it would be promising to train a compact network to achieve the same performance as the very deep model .    in this paper , we propose to address the above issues by devising a so called in which adaptively weighted auxiliary losses ( or outputs ) are introduced ( see figure [ fig : ausresnet ] and section [ sec : model_structure ] for more details ) .",
    "specifically , rooted in the resnet , each loss is connected to an intermediate layer in order to reduce model redundancy and improve the discriminative power of hidden layers . in this way",
    ", we can learn a much more compact model to represent images . while adding auxiliary outputs has been investigated by some previous work  @xcite , we draw the following new conclusions in our study .    \\1 .",
    "we propose three methods to train the network with multiple losses .",
    "in particular , we develop a multi - way training method which applies one forward propagation for all losses but conducts the gradient backpropagation for each loss separately and in series .",
    "this method effectively avoids the issue of supervision information vanishing , and yields the best result in terms of generalization performance .",
    "the proposed approach inherently produces multiple models of different depths , making model selection very simple .",
    "surprisingly , the intermediate models often outperform much deeper models , including the full - depth resnet equivalent .",
    "employing auxiliary classifiers has been investigated in deepid models for face recognition  @xcite , googlenet  @xcite , and deep supervised nets ( dsn )  @xcite . in  @xcite , the supervisory signals are connected to each convolution layer ; while in  @xcite they are added to each pooling layer .    in googlenet ,",
    "two auxiliary classifiers ( with weights equal to 0.3 ) are connected to intermediate layers to address the problem of vanishing gradients . in dsn",
    "@xcite , each convolution layer , followed by a _",
    "layer , is associated with a classifier . in the training , a joint objective function",
    "is constructed . to avoid the over - fitting issue",
    ", they keep the losses for a number of epochs , and discard all but the final loss to finish the rest epochs .",
    "these networks can not be very deep ( less than 25 layers ) .",
    "nevertheless , even with auxiliary classifiers , the issue of supervision information vanishing can still happen for these methods .",
    "moreover , unlike dsn , our method does not discard any loss , which helps to generate multiple models at the same time , with the ensuing benefits for model selection . in branchynet",
    "@xcite , auxiliary outputs are regarded as side branches for early exit . these outputs , not for training , but allow test samples to exit from shallow layers to achieve fast inference .",
    "[ sec : model_structure ]",
    "we see that the supervision information vanishing of long path propagation can cause model redundancy in even well trained very deep networks .",
    "we thus seek to add auxiliary outputs , ( each associated with a loss function ) , to some intermediate layers to improve the discriminative power of all the shallower layers . given its heritage , we have labelled the model .      rooted in resnet , the general architecture of is shown in figure [ fig : ausresnet ] .",
    "the original resnet is composed of a set of residual blocks , each containing two or more convolutional layers , followed by a batch normalization layer and an activation function ( see  @xcite for more details ) .",
    "very recently , he have stated that the pre - activation function could further improve the resnet and make the training of 1000-layers model possible  @xcite .",
    "compared with the resnet , one may extend the width of network by a factor of @xmath23 to improve the representation power of each block ( see wide resnets  @xcite ) . in ,",
    "in addition to the final output , we introduce additional @xmath25 losses weighted by @xmath26 to the intermediate residual blocks .",
    "thus it contains @xmath27 outputs in total , each for the same classification task . for convenience , hereafter we use -56 - 5 to denote with 56 layers and 5 outputs ( 4 auxiliary outputs ) .",
    "the positions of the auxiliary outputs are important . in general ,",
    "very shallow layers represent low - level structures with little discriminative power even for a well trained shallow model  @xcite .",
    "therefore , the outputs should not be added to very shallow layers .",
    "for example , the first layer , in fact , with low - level feature , has very little discriminative power .",
    "if adding an output to this layer , the loss and the gradients are always very large , meaning that the training may not converge . moreover , adding too many losses may cause severe over - fitting , which seriously influences the generalization ability of intermediate layers and results in great drop in terms of testing error ( see details in section  [ exp : num_outputs ] ) .",
    "therefore , we do not add outputs to each block or even each layer , which is very different from the approach proposed in  @xcite .",
    "* setting @xmath28*. the values of the @xmath28 are critical to the performance of the proposed method .",
    "first , since features at deep layers have more discriminative power , the losses at deeper layers should be more important , and hence weighted more highly .",
    "the output of the final layer @xmath0 is , of course , the most important , and hence we set its weight to 1 by default . for convenience ,",
    "let @xmath29 be the index of layer to which the @xmath30-th loss is applied .",
    "for the auxiliary losses , we suggest choosing @xmath31 , where @xmath32 .",
    "in fact , if @xmath33 , its effect can be negligible , so we set @xmath34 .",
    "the parameter @xmath35 reflects the decay rate of @xmath28 w.r.t .",
    "1 ) if setting @xmath36 , it follows @xmath37 , and that every loss is equally weighted . however , this setting is too aggressive since shallow layers which are often not very discriminative may hamper the performance .",
    "2 ) if setting @xmath38 , the weights of losses at shallower layers will be smaller , which improves overall performance .",
    "3 ) for a larger @xmath35 , the weights of the losses at shallow layers will decay faster to be negligible . in practice , we suggest choosing @xmath39 , which implies a fast decay for the weights of losses at shallower layers , but does not eliminate their effect entirely .      has the same forward propagation method as resnet , but the gradient backpropagation ( bp ) method must be modified to exploit the additional losses .",
    "we here develop three approaches to gradient backpropagation for .",
    "let @xmath40 be the loss of the @xmath30-th output and @xmath41 be the loss of the final output .",
    "the training process of with auxiliary outputs can be considered to minimize the following objective function as in  @xcite @xmath42 in backpropagation , the supervision information from intermediate auxiliary outputs is jointly propagated to the shallower layers .",
    "let @xmath43 be the gradient of all @xmath27 losses w.r.t .",
    "@xmath15 . for any shallow layer @xmath9 ( where @xmath44 ) , the gradient @xmath43 of @xmath45 w.r.t .",
    "@xmath15 is computed by @xmath46 it is worth mentioning that when @xmath47 is very large , the second term in eqn.([eq : joint_gradient ] ) can be very large .",
    "thus , the step size @xmath48 in the original resnet could be too large .",
    "to amend this , when @xmath49 , we can adjust the step size by @xmath50 to avoid convergence issues .",
    "the joint gradient backpropagation can alleviate the issue of supervision information vanishing to some extent .",
    "assuming @xmath11 , based on eqn.([eq : gradient_resnet ] ) , the gradient can be simplified as @xmath51 where @xmath52 , @xmath53 and @xmath54 .",
    "the term @xmath55 represents the original supervision information obtained from all the auxiliary losses .",
    "however , when @xmath0 is very large , since resnet can be considered as an ensemble of exponentially many shallow networks  @xcite , the term @xmath56 in @xmath57 can be exponentially large and dominate the gradient @xmath43 .",
    "therefore , the supervision information may still vanish when the network goes deeper .",
    "similarly , the supervision vanishing issue may also happen in dsn  @xcite .      to address the above issue , a straightforward pair - wise gradient backpropagation method can be applied . in this method , we sequentially conduct forward propagation and backpropagation for each output . for any epoch @xmath58 in sgd , suppose we deal with the outputs in an order of @xmath59 , where @xmath27 denotes the index of the final output . for the @xmath30-th output ,",
    "we first compute the features @xmath60 and the loss @xmath61 regarding this output based on the model updated by the @xmath62-th output , denoted by @xmath63 for simplicity . here , when @xmath64 , we have @xmath65 .",
    "after that , we compute the gradient @xmath66 for the @xmath30-th output at any layer @xmath9 that is lower than or equal to @xmath29 by @xmath67 according to eqn.([eq : pair - wise_gradient ] ) , for any layer @xmath9 , the supervision information of the nearest output can be successfully propagated to this layer , making the network easier to train .      in the pair - wise gradient backpropagation , for each output , the features and loss will be updated before doing the gradient backpropagation , which has two limitations .",
    "first , after backpropagation of the auxiliary output , the loss and the magnitude of the gradients for the next output shall decrease with very high probability , and this will affect the gradient propagation for the layers between the two outputs .",
    "second , the forward propagation for each output will take additional cost . to amend these",
    ", we propose a more aggressive backpropagation method , called multi - way gradient backpropagation . in this method",
    ", we apply only one forward propagation , in which we update the features for all layers and compute losses for all the outputs . keeping the features and losses fixed , we conduct the gradient backpropagation for each output separately in a sequential way .",
    "let @xmath68 be the features of layer @xmath9 which are fixed for all outputs . in this case , for the @xmath30-th output",
    ", the gradient @xmath69 for any layer @xmath9 that is lower or equal to @xmath29 shall be computed by @xmath70 assuming @xmath11 , we will have @xmath71 which is similar to eqn.([eq : gradient_resnet ] ) for resnet .",
    "the major characteristic of the multi - way method is the performance of multiple independent backpropagations , where only one forward propagation is applied . due to eqns.([eq : multipath_individual_gradient ] ) and ( [ eq : gradient - multi - way ] ) , intermediate layers can obtain sufficient supervision signal from independent backpropagations , and thus the issue of supervision information vanishing can be trivially addressed .      to better understand the effect on resnet of having supervision information vanishing , as well as the three gradient backpropagation methods , we compare resnet with 56 layers and with 56 layers and 4 auxiliary outputs ( at layers 15 , 25 , 35 , 45 ) .",
    "note that the latter two backpropagation methods are similar , here we only study multi - way backpropagation on the issue of supervision information vanishing . for simplicity , based on eqn.([eq : gradient - multi - way ] ) , for the @xmath30-th output of , we measure the supervision component of the whole gradient by @xmath72 at the final training epoch of sgd .",
    "similarly , based on eqns.([eq : gradient_resnet ] ) and ( [ eq : pair_gradient ] ) , we measure the component of the gradients by @xmath73 for resnet and @xmath74 for with joint gradient backpropagation , respectively .",
    "the ratios are shown in figure [ fig : ratio ] . from the figure",
    ", the supervision information for resnet and joint gradient backpropagation indeed diminishes very fast in shallower layers .",
    "however , for with multi - way gradient backpropagation , because of multiple individual backpropagations , the supervision component remains stable .",
    ".effects of different training methods for networks with auxiliary outputs on cifar-10 .",
    "[ cols=\"^,^,^,^\",options=\"header \" , ]     [ tab : lambda ]",
    "besides gradient vanishing , very deep networks also suffer from a vanishing supervision information issue . in the course of the investigation we developed a novel model , which we labelled .",
    "this model addresses the vanishing supervision information issue through the use of additional auxiliary outputs .",
    "we proposed three gradient backpropagation methods to train the network with multiple losses .",
    "in particular , we developed a multi - way gradient backpropagation method , which gives rise to models which outperform resnet .",
    "the proposed method reduces the internal redundancy and thus makes the models significantly more compact . because of the fact that it optimizes multiple intermediate models , also offers the opportunity for a form of model selection .",
    "x.  glorot and y.  bengio . understanding the difficulty of training deep feedforward neural networks . in _ international conference on artificial intelligence and statistics _ ,",
    "volume  9 , pages 249256 , 2010 .",
    "k.  he , x.  zhang , s.  ren , and j.  sun . delving deep into rectifiers : surpassing human - level performance on imagenet classification . in _ proceedings of the ieee international conference on computer vision _ , pages 10261034 , 2015 .",
    "j.  long , e.  shelhamer , and t.  darrell .",
    "fully convolutional networks for semantic segmentation . in _ proceedings of the ieee conference on computer vision and pattern recognition _ , pages 34313440 , 2015 .",
    "f.  schroff , d.  kalenichenko , and j.  philbin .",
    "facenet : a unified embedding for face recognition and clustering . in _ proceedings of the ieee conference on computer vision and pattern recognition _ , pages 815823 , 2015 .",
    "y.  sun , x.  wang , and x.  tang . deeply learned face representations are sparse , selective , and robust . in _ proceedings of the ieee conference on computer vision and pattern recognition _ ,",
    "pages 28922900 , 2015 .    c.  szegedy , w.  liu , y.  jia , p.  sermanet , s.  reed , d.  anguelov , d.  erhan , v.  vanhoucke , and a.  rabinovich .",
    "going deeper with convolutions . in _ proceedings of the ieee conference on computer vision and pattern recognition _ ,",
    "pages 19 , 2015 ."
  ],
  "abstract_text": [
    "<S> convolutional neural networks ( cnns ) with very deep architectures , such as the residual network ( resnet )  @xcite , have shown encouraging results in various tasks in computer vision and machine learning . </S>",
    "<S> their depth has been one of the key factors behind the great success of cnns , with the gradient vanishing issue having been largely addressed by resnet . </S>",
    "<S> however , there are other issues associated with increased depth . first , </S>",
    "<S> when networks get very deep , the supervision information may vanish due to the associated long backpropagation path . </S>",
    "<S> this means that intermediate layers receive less training information , which results in redundancy in models . </S>",
    "<S> second , when the model becomes more complex and redundant , inference becomes more expensive . </S>",
    "<S> third , very deep models require larger volumes of training data . </S>",
    "<S> we propose here instead an and a new training method to propagate not only gradients but also supervision information from multiple auxiliary outputs at intermediate layers . </S>",
    "<S> the proposed gives rise to a more compact network which outperforms its very deep equivalent ( i.e. resnet ) . </S>",
    "<S> for example , with 44 layers performs better than the original resnet with 110 layers on several benchmark data sets , i.e. cifar-10 , cifar-100 and svhn . </S>"
  ]
}