{
  "article_text": [
    "among the measures of association , the pearson ( product - moment ) correlation coefficient is widely applied in many fields of science due its simple computation and alleged ease of interpretation .",
    "indeed , the square of this correlation coefficient between two processes simply represents the proportion of variance of one process that can be linearly represented by the other @xcite .",
    "but what does this value say about how strong both processes are associated or dependent with each other in a multivariate process ? while it is a commonplace that correlation does not imply causation @xcite , the aim of this article is to further elucidate how the value of the lagged pearson correlation coefficient  in the following referred to as the correlation ( function )  between two causally dependent components of a multivariate process is to be interpreted .",
    "graphical models @xcite provide a well interpretable framework to study interactions in a multivariate process . here",
    "we utilise the derived concept of time series graphs @xcite to study the dependencies of cross correlation for the class of multivariate autoregressive time series models in a graph - theoretical way .",
    "we demonstrate that cross correlation can be rather misguiding as a measure of how _ strong _ two processes interact and is ambiguously influenced by other dependencies in the multivariate process .    based on the time series",
    "graph a certain partial correlation measure is introduced for which we prove very simple dependencies on the autoregressive coefficients , making it straightforward to be interpreted as the strength of dependence between these two components alone .",
    "we also introduce further partial correlation measures that capture different aspects of the dependence between two components .",
    "another commonly known problem of cross correlation is the estimation of its significance in the presence of strong autocorrelations in the time series .",
    "these dependencies violate the assumption of independent identically distributed samples and ` inflate ' the sampling distribution making an assessment of significance difficult . for the proposed partial correlation measure , on the other hand ,",
    "we show analytically and numerically that the is not affected by autocorrelation , as our theoretical results suggest .",
    "the article is structured as follows : in sect .",
    "[ sect : anamod ] we define time series graphs and their relation to autoregressive models . in sect .",
    "[ cc_deps ] the dependencies of the lagged correlation function are interpreted graph - theoretically . in sect .",
    "[ par_corr ] the novel partial correlation measures is introduced and some theoretical results are discussed .",
    "the properties of its sampling distribution are investigated in sect .",
    "[ numerics ] . finally , in sect .  [ application ]",
    "we compare the differences between the measures on a climatological example of temperature time series in the tropics .",
    "graphical models @xcite provide a tool to distinguish direct from indirect interactions between and within multiple processes .",
    "underlying is the concept of _ conditional independencies _ in a general multivariate process , which can be explained as follows .",
    "consider three processes where @xmath0 drives @xmath1 ( i.e. , @xmath1 is statistically dependent on @xmath0 at some lag in the past ) and @xmath1 drives @xmath2 as visualised in fig .",
    "[ fig : causality](a ) . here",
    "@xmath0 and @xmath2 are not directly but indirectly interacting and in a bivariate analysis @xmath0 and @xmath2 would be found to be dependent  implying that their correlation coefficient would be non - zero in the case of a linear dependency .",
    "the same holds for a common driver scheme in fig .",
    "[ fig : causality](b ) .",
    "if , however , the variable @xmath1 is included into the analysis , one finds that @xmath0 and @xmath2 are independent _ conditional _ on @xmath1 , written as @xmath3        this concept is now applied to define links in a time series graph @xcite of a multivariate stationary discrete - time process @xmath4 .",
    "each node in that graph represents a single random variable , i.e. , a subprocess , at a certain time @xmath5 .",
    "nodes @xmath6 and @xmath7 are connected by a directed link `` @xmath8 '' pointing forward in time if and only if @xmath9 and @xmath10 i.e. , if they are _ not _ independent conditionally on the past of the whole process denoted by @xmath11 . if @xmath12 , the link `` @xmath8 '' represents a _ coupling at lag _",
    "@xmath13 , while for @xmath14 it represents an _ autodependency at lag _ @xmath13 .",
    "further , nodes @xmath15 and @xmath7 are connected by an undirected contemporaneous link `` @xmath16 '' @xcite if and only if @xmath17 where also the contemporaneous present @xmath18 is included in the condition .",
    "note that for stationary processes it holds that `` @xmath8 '' whenever `` @xmath19 '' for any @xmath20 .",
    "these graphs can be linked to the concept of a lag - specific _ granger causality _ @xcite . in the original definition of granger causality @xmath21 _",
    "granger causes _ @xmath22 with respect to the past of the whole process @xmath4 if ( 1 ) events in @xmath0 occur before events in @xmath2 and ( 2 ) @xmath0 improves forecasting @xmath2 even if the past of the remaining process @xmath23 is known .",
    "the latter property is directly related to the conditional dependence between @xmath0 at some lag and @xmath2 given the past of the remaining process @xmath23 which defines links in the time series graph . in @xcite the range and conditions of application",
    "are further discussed .    for the following analysis the notion of _ parents _ @xmath24 and _ neighbors _ @xmath25 of a process @xmath7 in the time series graph will be important .",
    "they are defined as @xmath26 note , that also the past lags of @xmath2 can be part of the parents .",
    "the parents of all subprocesses in @xmath4 together with the contemporaneous links comprise the time series graph .",
    "while the definition of time series graphs was given for the large class of processes sufficing condition ( s ) in @xcite , in this article we consider the case of a stationary @xmath27-variate discrete - time process defined as @xmath28 i.e. , a vector autoregressive process of order @xmath29 where @xmath30 are @xmath31 matrices of coefficients for each lag @xmath32 and the @xmath27-vector @xmath33 is an independent identically distributed gaussian random variable with zero mean and covariance matrix @xmath34 .",
    "@xmath35 is sometimes referred to as the _ innovation term_. its variances on the main diagonal of @xmath34 we denote by @xmath36 and the covariances by @xmath37 for @xmath38 .    for this model class",
    "the directed and contemporaneous links of the corresponding time series graph are defined by non - zero entries in the coefficient matrix @xmath39 and the inverse of the innovation covariance matrix @xmath34 @xcite : @xmath40 an alternative definition of contemporaneous links is based on non - zero entries in @xmath41 @xcite .    as an example , consider the bivariate autoregressive model of order 1 @xmath42 and @xmath43 for @xmath44 .    ) as a time series graph .",
    "the labels indicate the coefficients in the matrices @xmath45 and @xmath34 .",
    "note , that a non - zero coefficient only determines the existence of absence of a link , but not a weight .",
    "note , that a non - zero @xmath46 only defines a contemporaneous link in the bivariate case , while it is non - zero entries in @xmath47 in the multivariate case . due to stationarity , links for @xmath5 imply links for all @xmath48 . process @xmath7",
    "( black node ) has one neighbor @xmath15 ( hatched node ) and two parents ( gray nodes ) . ]    in fig .",
    "[ fig : tsg_ar ] the corresponding time series graph is visualised . note , that a non - zero coefficient in the matrices @xmath49 or @xmath47 only defines the existence or absence of a link . in the next sections we address the question of how the _ weight _ of a link can be quantified .",
    "we are interested in the cross correlation lag function of stationary zero - mean random variables @xmath50 given by @xmath51}{\\sqrt{e[y_t y_t]}\\sqrt{e[x_t x_t]}},\\end{aligned}\\ ] ] which depends on the covariances and variances .",
    "thus , we will now give an interpretation of the lagged covariance structure of a multivariate autoregressive process in the framework of time series graphs .",
    "for an autoregressive process given by eq .",
    "( [ eq : var ] ) there exists an analytical expression of the lagged covariance in terms of @xmath39 ( * ? ? ?",
    "11.3 ) : @xmath52 = \\sum^\\infty_{n=0 } \\left(\\psi(n+\\tau ) \\sigma \\psi^\\top(n ) \\right)_{ij}\\end{aligned}\\ ] ] where @xmath53 can be recursively computed from matrix products : @xmath54 for example , @xmath55 where @xmath56 is the identity matrix .    now ,",
    "like a non - zero entry in @xmath39 corresponds to a link , an entry @xmath57 can be interpreted as a superposition of the contributions from different paths in the time series graph , each with total delay 3 : one direct path of only one link with lag 3 [ @xmath58 , paths composed of two links where the first has lag 1 and the second lag 2 [ ( @xmath59 and vice versa [ ( @xmath60 , and paths comprised of three links , each with lag 1 [ @xmath61 .",
    "for example , in the model eq .",
    "( [ eq : ar_model_matrix ] ) , @xmath53 is given by @xmath62 and a non - zero coefficient @xmath63_{yx}\\neq0 $ ] thus corresponds to all paths comprised of three links , each with lag 1 , e.g. , `` @xmath64 '' .",
    "these paths can be interpreted as an indirect causal chain as pictured in fig .",
    "[ fig : causality](a ) .",
    "the covariance @xmath65 , thus , is an infinite sum of products of @xmath66 , @xmath53 and @xmath34 and therefore a nonlinear polynomial combination of coefficients of _ all possible paths _ that end in @xmath67 and @xmath13-lags later in @xmath68 , emanating from nodes and their contemporaneous neighbors at all past lags .",
    "note , that possible paths via an intermediate node @xmath69 can only contain the motifs `` @xmath70 '' , `` @xmath71 '' or `` @xmath72 '' , but not `` @xmath73 '' or `` @xmath74 '' @xcite .",
    "in essence , most non - zero values in the covariance lag function are due to the common driver effect of the past ( fig .",
    "[ fig : causality](b ) ) or the indirect causality effect due to intermediate lags ( fig .",
    "[ fig : causality](a ) ) . therefore , the cross correlation as the covariance normalised by the variances , can not be related to the interaction between @xmath67 and @xmath68 alone , i.e. , the link `` @xmath75 '' in the time series graph .",
    "large cross correlation values between two nodes can simply be due to the superposition of indirect paths while the coefficient of the connecting link could be very small ( or even zero ) . in the application ( sect .",
    "[ application ] ) we give an example where this is the case .",
    "one can also characterise the dependencies of the covariance eq .",
    "( [ eq : anacov ] ) in terms of the parents in the time series graph .",
    "two univariate subprocesses @xmath76 of @xmath4 given by eq .  (",
    "[ eq : var ] ) with a link `` @xmath77 '' and @xmath78 can be written as @xmath79 with parents @xmath80 here the coefficient @xmath81 corresponds to the entry @xmath82 .    to simplify notation , eqns .",
    "( [ subx],[suby ] ) are expressed in vector notation @xmath83 where @xmath84 are scalar random processes , @xmath85 and @xmath86 are the coefficient vectors , and @xmath87 are possibly multivariate random processes of dimension @xmath88 and @xmath89 respectively , @xmath90 in the following , @xmath5 and @xmath13 will be dropped for ease of notation .      also in this interpretation",
    ", we find that the value of the cross correlation can not easily be related to the coefficient @xmath81 of the link between @xmath0 and @xmath2 in the time series graph and depends on the multiple interactions between the parents of @xmath0 and @xmath2 in the multivariate process .",
    "the knowledge of the ( linear ) conditional independence structure of the data encoded in the time series graph can be used to define a certain partial correlation measure with a straightforward graph - theoretical interpretation .",
    "partial correlation can be defined in the framework of regression analysis . if one regresses two variables @xmath50 on the same regressors @xmath100 , the cross correlation between the residuals @xmath101)^{-1 } e[\\mathbf{u}^\\top x ] \\nonumber\\\\ y_{\\mathbf{u } } &",
    "\\equiv y - \\mathbf{u } \\underbrace{(e[\\mathbf{u}^\\top \\mathbf{u}])^{-1 } e[\\mathbf{u}^\\top y]}_{\\substack{\\text{regression coefficient}\\\\\\text{(vector ) } \\mathbf{r } } } .\\end{aligned}\\ ] ] is the partial correlation @xmath102}{\\sqrt{e[y_{\\mathbf{u}}^\\top y_{\\mathbf{u}}]}\\sqrt{e[x_{\\mathbf{u}}^\\top x_{\\mathbf{u}}}}.\\end{aligned}\\ ] ] note , that this measure is not to be confused with the partial autocorrelation @xcite .",
    "the partial correlation measure introduced now is based on the parents @xmath2 _ and _ the parents of @xmath0 .    for two components @xmath50 of a stationary multivariate discrete - time process @xmath4 with parents @xmath24 and @xmath103 in the associated time series graph and @xmath9 , @xmath104    the name mit , short for _ momentary information transfer _ ,",
    "is used in analogy to the general case described in @xcite , which in the linear case should be understood as _",
    "momentary variance transfer_. the attribute _ momentary _ @xcite is used because mit measures the variance of the `` moment '' @xmath105 in @xmath0 that is transferred to @xmath7 .",
    "@xmath106 quantifies how much the variability in @xmath0 at the exact lag @xmath13 _ directly _ influences @xmath7 , irrespective of the pasts of @xmath6 and @xmath7 .",
    "one can also define a contemporaneous mit , which in the linear case is equivalent to the inverse covariance of the residuals after regressing each process on its parents @xcite .      as in sect .",
    "[ sect : cc_deps_par ] for the cross correlation , we now derive the dependencies of the partial correlation mit on the coefficients of a vector autoregressive model eq .",
    "( [ eq : var_vector ] ) .",
    "the equations for the subprocess @xmath2 can be written as @xmath107 where @xmath0 and the coefficient @xmath81 occurring in eq .",
    "( [ eq : var_vector ] ) is collapsed into @xmath99 and @xmath86 , respectively .",
    "for the autoregressive model eq .",
    "( [ eq : var_vector_reg ] ) , a multivariate regression for the dependent variable @xmath2 on @xmath108 , where @xmath109 are other regressors that are not part of the parents , i.e. , @xmath110 gives @xmath111    the proof is given in the appendix .",
    "for the partial correlation mit , the dependencies are slightly more complex .      the proof is given in the appendix .",
    "the ( co-)variances are comprised of two parts .",
    "the first one is simply the cross correlation between @xmath117 and @xmath118 .",
    "the second part is due to dependencies between @xmath117 and the parents of @xmath7 and non - zero only under certain conditions .",
    "more precisely , the schur complement @xmath115 can be interpreted as the conditional variance of @xmath99 given @xmath98 . on the other hand , the covariance @xmath119 $ ]",
    "can best be interpreted in the framework of time series graphs . in terms of the coefficient path matrices @xmath120 and the innovation s",
    "covariance @xmath34 it can be written as : @xmath121)_i = \\sum^n_{r=1 } \\psi_{w_ir}(\\tau - g_i ) \\sigma_{rx}.\\end{aligned}\\ ] ] this relation is derived in the appendix .",
    "@xmath122)_i$ ] is the linear combination of all paths of length @xmath123 emanating from @xmath15 or @xmath124 with @xmath125 to @xmath126 .",
    "it will be shown , that it can be understood as a `` sidepath '' covariance and is zero if there are no such paths .",
    "then , for @xmath119=0 $ ] , the @xmath106 becomes @xmath127 thus , if there are no sidepaths , the partial correlation measure mit of a link `` @xmath77 '' solely depends on the coefficient matrix entry @xmath49 and the innovation s variances @xmath128 and @xmath129 .",
    "the mit of an autoregressive process is , therefore , much better interpretable than the cross correlation as analysed in sects .  [ sect : cc_deps_paths ] and [ sect : cc_deps_par ] since its value is attributable to the interaction between @xmath67 and @xmath68 alone , i.e. , the link `` @xmath75 '' in the time series graph of @xmath4 .",
    "this theorem is the linear version of the _ coupling strength autonomy theorem _ that treats the general nonlinear case in the information - theoretic framework @xcite .",
    "the graph - theoretic perspective invites to define related measures that capture different aspects of the dependency between two components in a multivariate process .",
    "for example , we can also choose either one of the parents as a condition , which  dropping the attribute `` momentary ''  leads to the _ information transfers _ ity and itx @xmath130 ity only conditions out the influence of the parents of @xmath2 , but includes the aggregated influence of the parents of @xmath0 .",
    "like mit it is non - zero only for ( granger- ) causal dependent nodes and used in the algorithm to estimate the time series graph @xcite .",
    "itx , on the other hand , measures the part of variance originating in @xmath131 that reaches @xmath7 on any path and is , thus , not a ` causal ' measure of direct dependence , yet in many situations we might only be interested in the effect of @xmath0 on @xmath2 , no matter how this influence is mediated .",
    "for the case of sidepaths with @xmath119\\neq 0 $ ] the ( co-)variances in eq .",
    "( [ eq : mit_theorem ] ) depend on an additional term .",
    "as an example where one parent @xmath132 of @xmath2 ( apart from @xmath0 ) depends on @xmath0 , consider the following model : @xmath133 where for all @xmath134)_i=0 $ ] and also assume that additionally for all @xmath135 .",
    "as derived in the appendix , mit is then @xmath136 thus , the mit depends not only on @xmath81 , but also on all the coefficients along the paths @xmath137 , here only @xmath138 , and on the residual variance of @xmath132 given @xmath98 .",
    "this example points to the suggestion , that it might be more appropriate to `` leave open '' _ all _ paths from @xmath6 to @xmath7 by excluding from the conditions those parents of @xmath7 that are depending on @xmath6 .",
    "then the possible paths of variance transfer are either via the direct link `` @xmath139 '' or via the sidepaths `` @xmath140 '' ( the symbol `` @xmath141 '' denotes that the sidepath can start from @xmath6 either directed or contemporaneous , while the subsequent links of the path can only be directed ) . to isolate all of these paths , we suggest to additionally condition on the parents of the intermediate nodes on these sidepaths",
    ". these nodes can be characterised by @xmath142 where @xmath143 denotes the ancestors of @xmath7 , i.e. , the set of nodes with a directed path towards @xmath7 @xcite .",
    "we call the modified mit mits , where `` s '' stands for `` sidepath , '' @xmath144 in our sidepath example eq .",
    "( [ sidepath_model ] ) for the simpler special case @xmath145 and @xmath146 , mits evaluates to @xcite @xmath147 here the factor @xmath148 is the covariance along both paths , which can also vanish for @xmath149 , and seems like a more appropriate representation of the coupling between @xmath150 and @xmath7 .",
    "' ' - ) plots of sample estimates of cross correlation and the partial correlation measures ity and mit plotted against the student s t - distribution for different degrees of freedom ( dotted line for @xmath151 , dashed line for @xmath152 and solid line for @xmath153 ; the lines are almost identical ) .",
    "the diagonal line ( with 90% confidence intervals ) indicates a perfect match of theoretical and empirical distributions and the horizontal and vertical black lines denote the 5% and 95% quantiles of the theoretical distributions for different degrees of freedom . ]    in this section we study the properties of the sample estimate @xmath154 of the mit partial correlation .",
    "it is known , that the distribution of the partial correlation coefficient is the same as that of the cross correlation coefficient with the degrees of freedom reduced by the cardinality of the set of conditions @xmath155 @xcite .",
    "therefore , the distribution of @xmath156 is students-@xmath5 with @xmath157 degrees of freedom with @xmath155 being the dimension of @xmath100 . in the case of mit @xmath158 .",
    "the assumptions underlying this result are gaussianity and , importantly , independent and identically distributed samples .",
    "this assumption is , however , violated in many practical cases , especially for serially dependent samples with non - zero autocorrelations .",
    "consider the model eq .",
    "( [ eq : ar_model_matrix ] ) for @xmath159 and strong autocorrelations @xmath160 and where we assume the innovations to be uncorrelated , i.e. , @xmath34 is diagonal .",
    "the two processes are , therefore , independent , but the samples are serially dependent . as shown in fig .",
    "[ fig : qqplot ] for the cross correlation this effectively reduces the degrees of freedom @xcite and leads to an `` inflated '' sampling distribution .    since theorem [ thm ] implies , that mit `` filters out '' also autocorrelation , we expect that , conversely to the sample estimate of the cross correlation , the mit estimator is not `` inflated '' by autocorrelation .",
    "more precisely , since the condition on the parents removes the dependency of @xmath0 and @xmath2 on the past samples , the residuals @xmath161 and @xmath162 given by eq .",
    "( [ eq : residuals ] ) for a regression on both parents @xmath163 are @xmath164 and therefore indeed serially independent since both @xmath165 and @xmath166 are independent in time .",
    "note , that this only holds for links `` @xmath77 '' without sidepaths as discussed in the previous section .",
    "we also test the distribution of the partial correlation ity defined in eq .",
    "( [ eq : def_py ] ) where only the parents of @xmath2 are conditioned out . here",
    "the residuals are _ not _ independent and we expect the distribution to be still broadened due to less effective degrees of freedom . for model eq .",
    "( [ eq : ar_model_matrix ] ) the parents are @xmath167 and @xmath168 and @xmath169 .",
    "figure  [ fig : qqplot ] shows the quantile plots of the empirical distributions simulated with time series length @xmath170 plotted against the student s t - distribution with @xmath151 for @xmath171 , @xmath152 for @xmath172 and @xmath153 for @xmath154 .",
    "the plots demonstrate , that the cross correlation is strongly `` inflated '' , ity is still affected and only mit can be well described by the theoretical distribution within the confidence bounds , independent of the strength of autocorrelation .",
    "this feature can be used for independence tests since it allows for a more accurate significance test .",
    "note , however , that first the time series graph has to be estimated to infer the parents to condition on . in @xcite the measure ity",
    "is used in the estimation of the time series graph and we suggest to subsequently test the inferred links with mit to fully account for autocorrelations and dependencies also from parents of @xmath0 .",
    "as a climatological application we study two indices of monthly sea surface temperature anomalies @xcite for the period 1950  2012 .",
    "nino is the time series of the spatial average over the nino34 region ( 5n-5s and 170 - 120w ) in the east pacific and tna is the tropical north atlantic index @xcite averaged over ( 5.523.5n and 1557.5w ) .",
    "figure  [ fig : climate ] shows the time series and ( partial ) correlations .",
    "the time series graph was estimated using the pc - algorithm @xcite as described in @xcite with the theoretical significance test discussed above at the ( two - sided ) level @xmath173 . the estimated time series graph is comprised of a coupling link `` @xmath174 '' and autodependency links at lag 1 and 2 in nino and only at lag 1 in tna . on the other hand , the auto- and cross correlation lag functions shown in gray feature significant links for a large range of lags with a maximum of the cross correlation lag function @xmath175 at lag @xmath176 .",
    "this shift of the lag function s maximum is further investigated in @xcite .",
    "also the cross correlation value @xmath177 at lag @xmath178 is significantly larger than @xmath179 ( the `` @xmath180 '' values correspond to the 90% confidence interval estimated from a bootstrap test @xcite ) .",
    "the strong autodependency links with mit values of @xmath181 for lags 1 and 2 in nino and 0.7 for lag 1 in tna explain these ` significant ' cross correlation values at most lags , which according to eq .",
    "( [ eq : anacov ] ) , are due to the common driver effect of past nodes ( fig .",
    "[ fig : causality](b ) ) or the indirect causal effect due to intermediate lags ( fig .",
    "[ fig : causality](a ) ) . on the other hand ,",
    "since there are no sidepaths here , the small mit value reflects only the contributions from the coupling link and the residual s variances according to eq .",
    "( [ eq : mit_nosidepaths ] ) .",
    "the small value of mit shows , that the actual coupling mechanism by which nino influences tna is quite weak , but due to strong autocorrelations the overall contribution to tna s variance is larger becoming maximal in the peak at lag 5 .",
    "in @xcite this pacific ",
    "atlantic interaction is climatologically discussed .     for @xmath182 in light gray and the mit value at the significant link `` @xmath174 '' in black .",
    "note , that for autocorrelations ( on the diagonal ) the zero - lag is not drawn . ]",
    "with the goal to investigate how the value of cross correlation can be interpreted , we analysed how the cross correlation between two components of a multivariate autoregressive model depends on the model s coefficients .",
    "these dependencies can well be understood within the framework of time series graphs showing that the value of cross correlation at a certain lag stems from a superposition of paths from past and intermediate nodes in the graph .",
    "these complex dependencies on the model s coefficients make it hard to interpret the cross correlation as a measure of the strength of association between the two components alone .    on the other hand , for the recently introduced partial correlation measure mit",
    "we prove a simple formula depending solely on the coefficient belonging to the coupling lag of the two variables and the variance of their innovations .",
    "mit , thus , allows to separate the effect of other links , like strong autocorrelations , from the actual coupling link , making it better interpretable than cross correlation .",
    "we also suggest related measures that capture different aspects of the dependency between two components in a well interpretable way .",
    "additionally , an analysis of the sample estimate of mit shows , that it is not ` inflated ' by autocorrelations like cross correlation and , thus , suitable for significance tests that assume temporally independent samples .",
    "on http://tocsy.pik-potsdam.de/tigramite.php we provide a python program with a graphical user interface to estimate the time series graph and the partial correlation measures ity and mit as well as their information - theoretic counterparts .",
    "we appreciate the support by the german national academic foundation ( studienstiftung ) and dfg grant no . ku34 - 1 and thank jobst heitzig for helpful comments on an earlier version of the manuscript .",
    "for the model eq .",
    "( [ eq : var ] ) any regression of @xmath2 on regressors that include the parents @xmath24 yields the corresponding coefficients in @xmath39 for the parents and zeros for non - parents .",
    "more precisely , first the dependencies of a subprocess @xmath22 can be written as @xmath183 with parents @xmath184 to simplify notation , eq .",
    "( [ subyreg ] ) is expressed in vector notation @xmath185 where @xmath186 is the coefficient vector and @xmath187 is a possibly multivariate random process of dimension @xmath89 , on which @xmath2 depends at lags @xmath188 , @xmath189 in the following , @xmath5 and @xmath13 will be dropped for ease of notation .",
    "then a regression on @xmath108 , where @xmath109 are other regressors that are not part of the parents , i.e. , @xmath110 gives the coefficient vector @xmath190)^{-1 } e[\\mathbf{u}^\\top y ] \\nonumber\\\\     & = \\left ( \\begin{array}{cc } e[\\mathbf{w}^\\top\\mathbf{w } ] & e[\\mathbf{w}^\\top\\mathbf{v } ] \\\\",
    "e[\\mathbf{v}^\\top\\mathbf{w } ] & e[\\mathbf{v}^\\top\\mathbf{v } ] \\end{array } \\right)^{-1 } \\left(\\begin{array}{c } e[\\mathbf{w}^\\top y ] \\\\",
    "e[\\mathbf{v}^\\top y ] \\end{array } \\right).\\end{aligned}\\ ] ] now one can prove that @xmath191 which implies that any multivariate regression which contains the parents as regressors will recover the coefficients of the underyling model . to prove this relation ,",
    "the inverse can be treated via the matrix inversion lemma @xmath192 & e[\\mathbf{w}^\\top\\mathbf{v } ] \\\\",
    "e[\\mathbf{v}^\\top\\mathbf{w } ] & e[\\mathbf{v}^\\top\\mathbf{v } ] \\end{array } \\right)^{-1 } \\nonumber\\\\ & = \\left ( \\begin{smallmatrix } s^{-1}_v & - ( e[\\mathbf{w}^\\top \\mathbf{w}])^{-1 } e[\\mathbf{w}^\\top \\mathbf{v } ] s^{-1}_w \\\\ - ( e[\\mathbf{v}^\\top \\mathbf{v}])^{-1 } e[\\mathbf{v}^\\top \\mathbf{w } ] s^{-1}_v & s_w^{-1 }   \\end{smallmatrix } \\right),\\end{aligned}\\ ] ] where @xmath193 denotes the schur complements @xmath194 - e[\\mathbf{w}^\\top \\mathbf{v } ] ( e[\\mathbf{v}^\\top\\mathbf{v}])^{-1 } e[\\mathbf{v}^\\top \\mathbf{w } ] \\\\ s_w & =   e[\\mathbf{v}^\\top \\mathbf{v } ] - e[\\mathbf{v}^\\top \\mathbf{w } ] ( e[\\mathbf{w}^\\top\\mathbf{w}])^{-1 } e[\\mathbf{w}^\\top \\mathbf{v}].\\end{aligned}\\ ] ] @xmath195 can be interpreted as the conditional variance of @xmath99 given @xmath109 .",
    "@xmath196 can be further transformed using the woodbury matrix identity @xmath197)^{-1 } - ( e[\\mathbf{w}^\\top \\mathbf{w}])^{-1 } e[\\mathbf{w}^\\top \\mathbf{v}]\\times \\nonumber\\\\ & \\times \\underbrace{(-e[\\mathbf{v}^\\top \\mathbf{v } ] + e[\\mathbf{v}^\\top \\mathbf{w } ] ( e[\\mathbf{w}^\\top\\mathbf{w}])^{-1 } e[\\mathbf{w}^\\top \\mathbf{v}])^{-1}}_{=(-s^{-1}_w ) }    \\times\\nonumber \\\\ & ~~~~~~\\times e[\\mathbf{v}^\\top \\mathbf{w } ] ( e[\\mathbf{w}^\\top \\mathbf{w}])^{-1}.\\end{aligned}\\ ] ] the covariance vector in eq .",
    "( [ eq : reg_inv ] ) can be simplified by @xmath198 \\\\",
    "e[\\mathbf{v}^\\top y ] \\end{array } \\right ) & = \\left(\\begin{array}{c } e[\\mathbf{w}^\\top \\mathbf{w } ] b + e[\\mathbf{w}^\\top \\varepsilon_y ] \\\\",
    "e[\\mathbf{v}^\\top \\mathbf{w}]b +   e[\\mathbf{v}^\\top \\varepsilon_y ] \\end{array } \\right ) \\\\ & = \\left(\\begin{array}{c } e[\\mathbf{w}^\\top \\mathbf{w } ] b \\\\",
    "e[\\mathbf{v}^\\top \\mathbf{w}]b \\end{array } \\right),\\end{aligned}\\ ] ] where @xmath199=e[\\mathbf{w}^\\top \\varepsilon_y]=0 $ ] because @xmath95 is independent of past processes .",
    "then the regression coefficient @xmath200 given by @xmath201 \\mathbf{b } - \\nonumber\\\\ & ~~~~- ( e[\\mathbf{w}^\\top \\mathbf{w } ] ) ^{-1 }   e[\\mathbf{w}^\\top \\mathbf{v } ] s_w^{-1 }   e[\\mathbf{v}^\\top \\mathbf{w } ] \\mathbf{b }   \\end{aligned}\\ ] ] can be simplified by inserting eq .",
    "( [ eq : woodbury ] ) from which it follows that @xmath202 \\mathbf{b } =   \\nonumber\\\\ & \\mathbf{b } + ( e[\\mathbf{w}^\\top \\mathbf{w}])^{-1 } e[\\mathbf{w}^\\top \\mathbf{v } ] s_w^{-1 } e[\\mathbf{v}^\\top \\mathbf{w } ] \\mathbf{b},\\end{aligned}\\ ] ] and thus @xmath203 which proves the first part of the claim .    to prove the second part , now the analogue of eq .",
    "( [ eq : woodbury ] ) for @xmath204 is inserted into @xmath205 \\mathbf{b } - \\nonumber\\\\ & ~~~~- ( e[\\mathbf{v}^\\top \\mathbf{v } ] ) ^{-1 }   e[\\mathbf{v}^\\top \\mathbf{w } ] s_v^{-1 }   e[\\mathbf{w}^\\top \\mathbf{w } ] \\mathbf{b},\\end{aligned}\\ ] ] from which using @xmath206 \\mathbf{b } = ( e[\\mathbf{v}^\\top \\mathbf{v}])^{-1 } e[\\mathbf{v}^\\top \\mathbf{w } ] \\mathbf{b}\\times \\nonumber\\\\ & ~~~~~~~\\times ( e[\\mathbf{v}^\\top \\mathbf{v}]])^{-1 } e[\\mathbf{v}^\\top \\mathbf{w } ] s_v^{-1 } \\times \\nonumber \\\\ % & ~~~~~~~\\times \\underbrace{e[\\mathbf{w}^\\top \\mathbf{v } ] ( e[\\mathbf{v}^\\top \\mathbf{v}])^{-1}e[\\mathbf{v}^\\top \\mathbf{w}]}_{=e[\\mathbf{w}^\\top \\mathbf{w } ] - s_v } \\mathbf{b } , & ~~~~~~~\\times e[\\mathbf{w}^\\top \\mathbf{v } ] ( e[\\mathbf{v}^\\top \\mathbf{v}])^{-1}e[\\mathbf{v}^\\top \\mathbf{w } ] \\mathbf{b},\\end{aligned}\\ ] ] and @xmath207 ( e[\\mathbf{v}^\\top \\mathbf{v}])^{-1}e[\\mathbf{v}^\\top \\mathbf{w } ] = e[\\mathbf{w}^\\top \\mathbf{w } ] - s_v\\end{aligned}\\ ] ] one arrives at @xmath208 .",
    "first @xmath0 and @xmath2 are regressed on @xmath209 yielding the residuals @xmath210)^{-1 } e[\\mathbf{u}^\\top y ] \\\\",
    "x_{\\mathbf{u } } & \\equiv x - \\mathbf{u } ( e[\\mathbf{u}^\\top \\mathbf{u}])^{-1 } e[\\mathbf{u}^\\top x ] .\\end{aligned}\\ ] ] then the covariance and variances are @xmath211 & = e[y^\\top x]-     \\nonumber\\\\                 & ~~~- e[y^\\top \\mathbf{u } ] ( e[\\mathbf{u}^\\top \\mathbf{u}])^{-1 } e[\\mathbf{u}^\\top x]\\\\ e[y_{\\mathbf{u } } ^\\top y_{\\mathbf{u } } ] & = e[y^\\top y ] -    \\nonumber\\\\                 & ~~~- e[y^\\top \\mathbf{u } ] ( e[\\mathbf{u}^\\top \\mathbf{u}])^{-1 } e[\\mathbf{u}^\\top y]\\\\ e[x_{\\mathbf{u } } ^\\top x_{\\mathbf{u } } ] & = e[x^\\top x ] -    \\nonumber\\\\                 & ~~~- e[x^\\top \\mathbf{u } ] ( e[\\mathbf{u}^\\top \\mathbf{u}])^{-1 } e[\\mathbf{u}^\\top x].\\end{aligned}\\ ] ] the covariance can be evaluated as follows .",
    "first , writing @xmath212 the covariance @xmath91 $ ] is expressed in terms of @xmath100 as @xmath213 = \\nonumber\\\\ & \\left ( \\mathbf{b}^\\top , c \\mathbf{a}^\\top \\right ) e[\\mathbf{u}^\\top \\mathbf{u } ] \\left ( \\begin{smallmatrix } 0\\\\ \\mathbf{a } \\end{smallmatrix } \\right ) + c e[\\varepsilon^\\top_x \\mathbf{u } ] \\left ( \\begin{smallmatrix } 0\\\\ \\mathbf{a } \\end{smallmatrix } \\right ) + \\underbrace{e[\\varepsilon^\\top_y \\mathbf{u}]}_{=0}\\left ( \\begin{smallmatrix } 0\\\\ \\mathbf{a } \\end{smallmatrix } \\right ) \\nonumber\\\\ & + \\left ( \\mathbf{b}^\\top , c \\mathbf{a}^\\top",
    "\\right ) e[\\mathbf{u}^\\top \\varepsilon_x ] + c \\underbrace{e[\\varepsilon_x^\\top \\varepsilon_x]}_{\\sigma_x^2 } + \\underbrace{e[\\varepsilon_y^\\top \\varepsilon_x]}_{=0},\\end{aligned}\\ ] ] where @xmath214=e[\\varepsilon_y^\\top \\varepsilon_x]=0 $ ] because @xmath95 is i.i.d . and therefore independent of processes from the past .",
    "note , that the suppressed subscript of @xmath96 is @xmath105 for @xmath9 .",
    "further , @xmath215 $ ] becomes @xmath216 & = \\left ( \\mathbf{b}^\\top , c \\mathbf{a}^\\top \\right ) e[\\mathbf{u}^\\top \\mathbf{u } ] + c e[\\varepsilon_x^\\top \\mathbf{u } ] + \\underbrace{e[\\varepsilon^\\top_y \\mathbf{u}]}_{=0},\\end{aligned}\\ ] ] and @xmath217 & = e[\\mathbf{u}^\\top \\mathbf{u } ] \\left ( \\begin{smallmatrix } 0\\\\ \\mathbf{a } \\end{smallmatrix } \\right ) + e[\\mathbf{u}^\\top x].\\end{aligned}\\ ] ] then @xmath218 ( e[\\mathbf{u}^\\top \\mathbf{u}])^{-1 } e[\\mathbf{u}^\\top x]=\\nonumber\\\\ & \\left ( \\mathbf{b}^\\top , c \\mathbf{a}^\\top \\right ) e[\\mathbf{u}^\\top \\mathbf{u } ] \\left ( \\begin{smallmatrix } 0\\\\ \\mathbf{a } \\end{smallmatrix } \\right ) + c e[\\varepsilon^\\top_x \\mathbf{u } ] \\left ( \\begin{smallmatrix } 0\\\\ \\mathbf{a } \\end{smallmatrix } \\right ) + \\nonumber\\\\ & + \\left ( \\mathbf{b}^\\top , c \\mathbf{a}^\\top",
    "\\right ) e[\\mathbf{u}^\\top \\varepsilon_x ] + \\nonumber\\\\ & + c e[\\varepsilon_x^\\top \\mathbf{u } ] ( e[\\mathbf{u}^\\top \\mathbf{u}])^{-1 } e[\\mathbf{u}^\\top \\varepsilon_x].\\end{aligned}\\ ] ] thus , many terms in @xmath219 $ ] cancel , and it remains @xmath220 = c\\sigma_x^2 + \\nonumber\\\\ & -c\\underbrace{e[\\varepsilon_x^\\top \\mathbf{u } ] ( e[\\mathbf{u}^\\top \\mathbf{u}])^{-1 } e[\\mathbf{u}^\\top \\varepsilon_x]}_{(\\star)}.\\end{aligned}\\ ] ] treating the inverse covariance in the @xmath221-term with the matrix inversion lemma analogous to eq .",
    "( [ eq : inversion_lemma ] ) and noting that @xmath222 = \\left ( 0 , \\varepsilon_x^\\top \\mathbf{w } \\right),\\end{aligned}\\ ] ] because @xmath96 is independent from the parents @xmath98 of @xmath0 , the @xmath221-term becomes @xmath223 s_z^{-1 }   e[\\mathbf{w}^\\top \\varepsilon_x].\\end{aligned}\\ ] ] @xmath224 is again the inverted @xmath225 matrix of the conditional variance of @xmath99 given @xmath98 , @xmath226 - e[\\mathbf{w}^\\top \\mathbf{z } ] ( e[\\mathbf{z}^\\top\\mathbf{z}])^{-1 } e[\\mathbf{z}^\\top \\mathbf{w}].\\end{aligned}\\ ] ] along the same derivation the variances are evaluated . all together ,",
    "the covariances and variances are simplified to @xmath211 & = c \\sigma_x^2   \\nonumber\\\\                 & ~~~- c e[\\varepsilon_x^\\top\\mathbf{w } ] s_z^{-1 }    e[\\mathbf{w}^\\top \\varepsilon_x ] \\\\ e[y_{\\mathbf{u } } ^\\top y_{\\mathbf{u } } ] & = \\sigma_y^2 + c^2 \\sigma_x^2   \\nonumber\\\\      & ~~~- c^2 e[\\varepsilon_x^\\top\\mathbf{w } ] s_z^{-1 }    e[\\mathbf{w}^\\top \\varepsilon_x]\\\\ e[x_{\\mathbf{u } } ^\\top x_{\\mathbf{u } } ] & = \\sigma_x^2   \\nonumber\\\\                 & ~~~- e[\\varepsilon_x^\\top\\mathbf{w } ] s_z^{-1 }    e[\\mathbf{w}^\\top \\varepsilon_x].\\end{aligned}\\ ] ]    the `` sidepath '' contribution @xmath227 $ ] can be further analysed as follows .",
    "inserting @xmath5 and @xmath13 again , the entries of the vector @xmath119 $ ] can be written as @xmath121)_i = e[\\varepsilon_{x , t-\\tau } w_{t - g_i}^i],\\end{aligned}\\ ] ] a simple case where @xmath119 $ ] is zero is given if @xmath228 , i.e. , all parents of @xmath2 are in the past of @xmath0 .",
    "but it is interesting to further analyse more complex cases for @xmath229 for any @xmath230 .",
    "consider @xmath231 & = e[w^i_{t+\\tau - g_i } \\varepsilon_{x , t } ] \\nonumber \\\\ & = \\underbrace{e[w^i_{t+\\tau - g_i}x_t]}_{\\gamma_{w_ix}(\\tau - g_i ) } - \\nonumber \\\\ & ~~~ - \\sum^{n_x}_{j=1 } \\phi_{xz_j}(h_j ) \\underbrace{e[w^i_{t+\\tau - g_i}z^j_{t - h_j}]}_{\\gamma_{w_iz_j}(\\tau+h_j - g_i)}.\\end{aligned}\\ ] ] analyzing @xmath232 , @xmath233 the linear combination of paths in @xmath234 can be separated as they either all go through the parents of @xmath0 or are emanating from @xmath0 , i.e. , are of length @xmath235 : @xmath236 resulting in @xmath237 and thus @xmath121)_i = \\sum^n_{r=1 } \\psi_{w_ir}(\\tau - g_i ) \\sigma_{rx}.\\end{aligned}\\ ] ] @xmath122)_i$ ] is the linear combination of all paths of length @xmath123 emanating from @xmath15 or @xmath124 with @xmath125 to @xmath126 .    for @xmath238 , @xmath239 and thus for all @xmath230 @xmath122)_i=0 $ ] , confirming the first part of the theorem . but for all @xmath230 with @xmath229 , @xmath122)_i$ ] can still be zero if there are no such paths .",
    "if that holds for all @xmath230 , the vector @xmath119 $ ] is zero and the simple expression for mit is obtained .",
    "the mit for the sidepath example is derived as follows . in this example",
    "we have for all @xmath134)_i=0 $ ] and also assume that additionally for all @xmath135 .",
    "then @xmath240 and @xmath121)_k =   \\psi_{w_kx}(1 ) \\sigma_{xx } = d \\sigma_x^2\\end{aligned}\\ ] ] and with @xmath241 = d^2 a^\\top e[\\mathbf{z}^\\top \\mathbf{z}]+d^2 \\sigma_x^2 + \\sigma^2_{w_k}$ ] and @xmath242=d a^\\top e[\\mathbf{z}^\\top \\mathbf{z}]$ ] the conditional variance of @xmath132 is @xmath243 and therefore , since @xmath115 is a scalar ,",
    "@xmath244 s_z^{-1}e[\\mathbf{w}^\\top \\varepsilon_y ] = \\frac{d^2 \\sigma^4_x}{d^2 \\sigma_x^2 + \\sigma^2_{w_k}},\\end{aligned}\\ ] ] from which the sidepath mit follows .",
    "dahlhaus , r. , 2000 : graphical interaction models for multivariate time series .",
    "_ metrika _ , * 51  ( 2 ) * , 157172 .",
    "eichler , m. , 2005 : a graphical approach for evaluating effective connectivity in neural systems .",
    "_ philosophical transactions of the royal society of london .",
    "series b , biological sciences _ ,",
    "* 360  ( 1457 ) * , 95367 .",
    "eichler , m. , 2012 : graphical modelling of multivariate time series .",
    "_ probability theory and related fields _ , * 1 * , 233 .",
    "enfield , d.  b. , a.  m. mestas - nuez , d.  a. mayer , and l.  cid - serrano , 1999 : how ubiquitous is the dipole relationship in tropical atlantic sea surface temperatures ?",
    "_ journal of geophysical research _ , * 104  ( c4 ) * , 7841 .",
    "fisher , r. , 1924 : the distribution of the partial correlation coefficient .",
    "_ metron_.        pompe , b. and j.  runge , 2011 : momentary information transfer as a coupling measure of time series .",
    "e _ , * 83  ( 5 ) * , 112 .",
    "rayner , n. , d.  parker , e.  horton , c.  folland , l.  alexander , d.  rowell , e.  kent , and a.  kaplan , 2003 : global analyses of sea surface temperature , sea ice , and night marine air temperature since the late nineteenth century .",
    "_ j. geophys .",
    "res _ , * 108  ( d14 ) * , 4407 .",
    "runge , j. , j.  heitzig , n.  marwan , and j.  kurths , 2012 : quantifying causal coupling strength : a lag - specific measure for multivariate time series related to transfer entropy .",
    "e _ , * 86  ( 6 ) * , 115 .",
    "runge , j. , j.  heitzig , v.  petoukhov , and j.  kurths , 2012 : escaping the curse of dimensionality in estimating multivariate transfer entropy .",
    "_ physical review letters _ , * 108  ( 25 ) * , 14 .",
    "runge , j. , v.  petoukhov , and j.  kurths , 2013 : quantifying the strength and delay of climatic interactions : the ambiguities of cross correlation and a novel graphical models based measure .",
    "_ in press at journal of climate_."
  ],
  "abstract_text": [
    "<S> the dependencies of the lagged ( pearson ) correlation function on the coefficients of multivariate autoregressive models are interpreted in the framework of time series graphs . </S>",
    "<S> time series graphs are related to the concept of granger causality and encode the conditional independence structure of a multivariate process . </S>",
    "<S> the authors show that the complex dependencies of the pearson correlation coefficient complicate an interpretation and propose a novel partial correlation measure with a straightforward graph - theoretical interpretation . </S>",
    "<S> the novel measure has the additional advantage that its sampling distribution is not affected by serial dependencies like that of the pearson correlation coefficient . in an application to climatological time </S>",
    "<S> series the potential of the novel measure is demonstrated . </S>"
  ]
}