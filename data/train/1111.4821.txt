{
  "article_text": [
    "the p - value is commonly used as a measure of evidence in a data @xmath1 , against a hypothesis @xmath2 : the smaller the p - value , the stronger the evidence against @xmath2 in the data . recall that the p - value is the smallest level at which a test @xmath3 rejects @xmath2 . according to the typical calibration @xcite , @xcite , the p - value smaller than 0.01 suggests a very strong evidence against @xmath2 .    unlike the p - value , which measures evidence against a single hypothesis , the ratio of likelihoodsmeasures evidence in a data for a simple hypothesis @xmath2 , relative to a simple hypothesis @xmath4 . for a parametric model @xmath5 , the ratio of likelihoods @xmath6 measures evidence for @xmath2 relative to @xmath4 , in data @xmath1 .",
    "the value of @xmath7 above a certain threshold @xmath8 is taken as an evidence in favor of @xmath2 , and against @xmath4 .",
    "values of @xmath9 around  @xmath10 are suggested for a threshold , above which the evidence is considered very strong ( cf .",
    "@xcite , @xcite ) .",
    "statistics abounds criteria for assessing quality of estimators , tests , forecasting rules , classification algorithms , but besides the likelihood principle discussions ( cf .",
    "@xcite ) , it seems to be almost silent on what criteria should a good measure of evidence satisfy .",
    "schervish , in a notable exception@xcite , considers a requirement of coherence , borrowed from the multiple comparisons theory @xcite . if @xmath11 implies @xmath12 ( i.e. , @xmath13 ) , then the coherent measure of evidence gives at least as strong evidence to @xmath14 as it gives to @xmath0 .",
    "the p - value is not coherent ; cf .",
    "@xcite . in this note ,",
    "an asymptotic criterion of consistency is introduced , and it is demonstrated that the p - value is not consistent , while the ratio of likelihoods satisfies the consistency requirement .",
    "to set a formal framework , let @xmath15 be a random variable with the probability density ( or mass ) function @xmath5 , parametrized by @xmath16 , and such that if @xmath17 then @xmath18 .",
    "let @xmath19 form a partition of @xmath20 , and associate @xmath21 with the hypothesis @xmath22 , @xmath23 .",
    "let @xmath24 be a random sample from @xmath5 .",
    "a measure of evidence @xmath25 , in data @xmath1 , for the hypothesis @xmath26 where @xmath27 , relative to @xmath28 where @xmath29 , is a mapping @xmath30 .",
    "it usually goes with a calibration that partitions values of @xmath31 into intervals , or categories . in what follows ,",
    "the interest will concentrate on the category @xmath32 of the most extreme values of the evidence measure @xmath31 that correspond to the strongest evidence .",
    "finally , the measure of evidence against a hypothesis @xmath2 , relative to @xmath4 , in data @xmath1 , will be denoted @xmath33 .",
    "in @xcite , sellke , bayarri , and berger stress that in applications of an evidence measure , data sets may come from either @xmath2 or @xmath4 . the authors illustrate this important point by an example of testing drugs @xmath34 , for an illness , in a series of independent experiments .",
    "the measure of evidence applied to a data set from @xmath35-th experiment , is used to differentiate between the hypothesis @xmath2 that the drug @xmath36 has a negligible effect , and the alternative @xmath4 that the drug @xmath36 has a non - negligible effect .",
    "some drugs have negligible effects , some have the non - negligible one . in other words ,",
    "some experimental data @xmath1 come from @xmath2 , other data sets are from @xmath4 .",
    "this key aspect of applications of the evidence measure can be captured by the following two - level sampling mechanism :    1 .",
    "first , @xmath37 is drawn from a pdf ( or pmf ) @xmath38 .",
    "2 .   given @xmath37 , a random sample @xmath1 is drawn from @xmath5 .    as the sample size @xmath39 increases , it should hold , informally put , that among the data sets which , according to the measure of evidence strongly testify against @xmath2 , the relative number of those which in fact come from @xmath2 , should go to zero .",
    "this motivates the following requirement of consistency : we say that a measure of evidence @xmath33 against @xmath2 , relative to @xmath4 , is _ consistent _ , if @xmath40    the probability that @xmath37 is in @xmath41 , given that the measure of evidence @xmath33 strongly testifies against @xmath2 , relative to @xmath4 , should go to zero , as the sample size @xmath39 goes beyond any limit .",
    "the p - value is @xmath42 , where @xmath43 is a test statistic , @xmath44 is the size of the test , and @xmath45 is the rejection region for @xmath2 . in this section",
    "it is assumed that @xmath46 is a continuous random variable and the test statistic @xmath43 is such that it rejects @xmath2 when the observed value @xmath47 of @xmath43 is large .",
    "then the p - value is @xmath48 .",
    "the p - value @xmath49 as a measure of evidence against @xmath2 does not take @xmath4 into account .",
    "let @xmath50 be the interval of values that indicate the very strong evidence against @xmath2 .    before addressing the question of consistency of the p - value in general , consider an illustrative example of the gaussian random variable @xmath46 with the variance @xmath51 , and let @xmath52 , @xmath53 , @xmath54 .",
    "let @xmath55 , @xmath56 . and ,",
    "let @xmath57 be the test statistic , and @xmath58 be the rejection region , with @xmath59 denoting the @xmath60 quantile of the standard normal distribution .    under @xmath2 ,",
    "the p - value is a uniform random variable , so @xmath61 . under @xmath4 ,",
    "the power of the test is @xmath62 , where @xmath63 is the distribution function of the standard normal random variable .",
    "note that @xmath64 converges to @xmath65 , for @xmath54 .",
    "taken together , @xmath66 .",
    "thus , in this simple example , the p - value is not a consistent measure of evidence against @xmath2 .",
    "following the reasoning in the above example , it can be demonstrated that the p - value is inconsistent .",
    "let @xmath41 , @xmath67 form a partition of @xmath20 .",
    "let @xmath38 be such that @xmath68 is @xmath69 . and",
    ", let @xmath43 , @xmath45 , be such that @xmath70 , as @xmath71 ( i.e. , for @xmath29 , the power of the test @xmath43 converges to @xmath65 ) .",
    "then it holds that @xmath72    under @xmath2 , the p - value is uniformly distributed , so that @xmath73 , for @xmath27 . thus , @xmath74 .",
    "next , under the assumption that the power of the test @xmath43 goes to @xmath65 , as @xmath71 , the probability @xmath75 . taken together , it proves the proposition .",
    "since the right - hand side expression in is positive , the p - value is not a consistent measure of evidence .",
    "the limit of the probability becomes zero only at the extreme , uninteresting case of @xmath76 , i.e. , when no @xmath1 comes from @xmath2 . for the typical value of @xmath77 and @xmath78 ,",
    "the limit value of the probability is @xmath79 . for @xmath80 ,",
    "the probability is @xmath81 . for @xmath82 ,",
    "the probability is @xmath83 , and it converges to @xmath65 , as @xmath84 .",
    "the greater the relative presence of data sets from @xmath2 , the higher the asymptotic probability that the data come from @xmath2 , when the p - value strongly testifies against @xmath2 .",
    "for point sets @xmath41 , @xmath67 , the ratio of likelihoods @xmath7 of @xmath2 relative to @xmath4 is @xmath85 , where @xmath86 , for @xmath23 . the ratio @xmath7 measures the evidence in favor of @xmath2 ( and against @xmath4 ) , in data @xmath1 .",
    "the larger the @xmath7 , the stronger the evidence in favor of @xmath2 ( and against @xmath4 ) , so that @xmath87 , @xmath88 .",
    "first , consider the ratio of likelihoods @xmath89 in the example described above .",
    "clearly , @xmath90 , which , under the assumption @xmath54 , converges to @xmath91 , as @xmath71 . and , @xmath92 , which , under the assumption @xmath54 , converges to @xmath65 , as @xmath71 .",
    "thus , @xmath93 .",
    "hence , the ratio of likelihoods is a consistent measure of evidence , in this example .",
    "and the consistency is not accidental , as stated in the following proposition .    for point sets @xmath41 , @xmath67 , and @xmath94 , the ratio of likelihoods",
    "@xmath95 is a consistent measure of evidence , i.e. , @xmath96    the claim follows from the law of large numbers ( lln ) , applied to @xmath97 , and the fact that the kullback leibler divergence is positive for distinct distributions .",
    "recently , bickel @xcite proposed an extension of the ratio of likelihoods ( see also @xcite , @xcite ) to the case of general @xmath41 , @xmath67 : @xmath98 , and suggested its use as a measure of evidence .",
    "the extended ratio of likelihoods reduces to the ratio of likelihoods , when @xmath41 , @xmath67 are point sets . under additional assumptions",
    ", @xmath99 is a consistent measure of evidence . before stating the result ,",
    "recall that the maximum likelihood ( ml ) estimator @xmath100 of @xmath37 , restricted to @xmath101 , is @xmath102 .",
    "let @xmath5 and @xmath41 , @xmath67 be such that the maximum likelihood estimators @xmath103 , restricted to @xmath21 , are consistent estimators of @xmath37 , @xmath104 . and",
    "let the maximum likelihood estimators @xmath105 , restricted to @xmath106 , converge in probability to some finite @xmath107 , @xmath108 .",
    "let @xmath38 be such that @xmath109 .",
    "then the extended ratio of likelihoods @xmath110 is a consistent measure of evidence against @xmath2 , relative to @xmath4 .    under the assumed consistency and convergence of the constrained mls",
    ", the claim follows from the lln and the positivity of the kullback leibler divergence between two different distributions , applied to the probability @xmath111 in the upper bound @xmath112\\int_{\\theta_1}p(\\theta)}{\\int_{\\theta_2 } pr(r_{21}^e > k_s\\,|\\,\\theta)p(\\theta)}$ ] and the lower bound @xmath113\\int_{\\theta_1}p(\\theta)$ ] of @xmath114 .",
    "it is open to debate whether a measure of evidence can depend on a prior information .",
    "bayesians usually measure evidence for @xmath2 relative to @xmath4 by the bayes factor @xmath115 , where @xmath116 is the prior distribution .",
    "the bayes factor above @xmath117 is usually considered @xcite as the very strong evidence for @xmath2 . however , lavine and schervish @xcite note that the bayes factor does not satisfy the coherence requirement , while the posterior odds is coherent . both the bayes factor @xmath118 and the posterior odds @xmath119 are consistent measures of evidence against @xmath2 , relative to @xmath4 . also , in analogy with the proposition 3 , consistency of the ratio of posterior modes can be established .",
    "there are several measures of statistical evidence in use . among them",
    "is the fisherian p - value and its extensions , likelihood - based measures , such as the ratio of likelihoods and the extended ratio of likelihoods , as well as the bayes factor and the posterior odds .",
    "what are the criteria that a measure of evidence should satisfy ?",
    "coherence ( cf .",
    "1 ) is one such a criterion .",
    "it is a logical criterion . in this note ,",
    "the asymptotic criterion of consistency was introduced .",
    "besides being incoherent , the p - value is also inconsistent .",
    "the ratio of likelihoods and its extension are consistent and coherent measures . among the bayesian measures of evidence , for instance the posterior odds ratio is both coherent and consistent ."
  ],
  "abstract_text": [
    "<S> what are the criteria that a measure of statistical evidence should satisfy ? it is argued that a measure of evidence should be consistent . </S>",
    "<S> consistency is an asymptotic criterion : the probability that if a measure of evidence in data strongly testifies against a hypothesis @xmath0 , then @xmath0 is indeed not true , should go to one , as more and more data appear . </S>",
    "<S> the p - value is not consistent , while the ratio of likelihoods is .    statistical evidence , consistency , </S>",
    "<S> p - value , ratio of likelihoods , extended ratio of likelihoods , bayes factor , posterior odds </S>"
  ]
}