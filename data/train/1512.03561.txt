{
  "article_text": [
    "the different stages of information processing in large neural systems comprise multiple characteristic spatial and temporal scales .",
    "while the _ in - vivo _ recordings of single neurons indicate considerable subthreshold fluctuations and highly variable spike trains @xcite , the macroscopic measurements have revealed reliable and structured activity in many cortical areas @xcite .",
    "accounting for these two results is an outstanding theoretical issue , which requires one to develop analytically tractable models capable of capturing the functional organization and integration of single unit dynamics at different levels of complexity .",
    "this is typically resolved by invoking the mean - field approach to describe the coarse - grained activity and interactions of neural populations .",
    "given the often used assumption on population homogeneity , the approach is from biological view most appropriate for intermediate - scale ( mesoscopic ) assemblies , such as cortical columns @xcite .",
    "the latter assemblies incorporate on one hand a sufficiently large number of neurons for the averaging effects to occur , but on the other hand , are small enough to support the homogeneity assumption .",
    "the mean field approach has so far been implemented to network structures as well as spatially extended neural systems , with the pertaining models classified as activity - based or voltage - based depending on the type of the state variable @xcite .",
    "the seminal works of wilson and cowan @xcite , as well as amari @xcite , employed the heuristic continuum limit , providing the description of the temporal coarse - grained dynamics in neural fields . though deterministic in nature , such models recovered a number of highly relevant dynamical regimes including multistability @xcite , large - scale oscillations @xcite , stationary pulses or bumps @xcite , traveling fronts and pulses @xcite , spiral waves @xcite as well as spatially localized oscillations @xcite .",
    "nevertheless , given the aim to reconcile observations of highly variable local neuron activity and the substantially reliable activity patterns at the macroscopic scale , the key point emerging in recent research on mean - field models has been to account for the higher - order statistics @xcite .",
    "conceptually , the goal has become to demonstrate how the fluctuations and correlations from the single unit level translate to and are manifested at the assembly level .",
    "in general , the physical background of variability of single units may be related either to noise or the balanced recurrent excitatory and inhibitory inputs @xcite .",
    "our interest lies with the former scenario . in neural systems",
    ", noise may derive from a number of extrinsic and/or intrinsic sources @xcite .",
    "the external noise is mainly due to random inputs arriving from a large number of afferent neurons ( synaptic noise ) , whereas the internal noise is primarily linked to random opening of a finite number of ion channels ( ion - channel noise ) . in the present paper ,",
    "we consider a network of randomly connected units , where the local dynamics follows a rate - model and is affected both by the internal and the external noise . using the gaussian closure hypothesis @xcite , we will derive the stochastic mean - field model characterizing the macroscopic network activity in terms of the mean rate and the associated variance .",
    "the issue of how noise from the single unit level translates to noise at the macroscopic scale is highly nontrivial .",
    "so far , the stochastic mean - field models have been constructed either via the top - down or the bottom - up approaches . in the top - down approach ,",
    "the details of the local neuron dynamics are neglected , which typically leads to phenomenological stochastic neural field models .",
    "these are based either on langevin version of the deterministic equations , having introduced some form of spatiotemporal white noise @xcite , or on treating the neural field equations as the thermodynamic limit of the underlying master equation @xcite . in the latter case ,",
    "extensions of the deterministic mean - field model have been obtained by perturbation techniques , such as the system - size expansion @xcite , or via the field - theory methods , viz .",
    "the path integral formalism @xcite .",
    "the bottom - up construction of stochastic mean - field models has primarily concerned networks of integrate - and - fire neurons with two types of interaction topology , the global coupling scheme @xcite or the sparse connectivity @xcite . within the framework of population density method @xcite ,",
    "such networks have been shown to display the asynchronous state despite the fact that the local firing conforms to poissonian process . under such conditions ,",
    "the collective dynamics has been described by an effective mean - field rate equation with a characteristic gain function .",
    "nevertheless , the point that the asynchronous state is stable only in the thermodynamic limit has indicated that the finite - size effects @xcite may yield qualitatively novel phenomena and contribute as additional source of intrinsic noise at the network level .",
    "apart from considering the networks of spiking neurons , the bottom - up approaches to stochastic mean - field models have pursued the second line of research featuring local rate dynamics @xcite .",
    "this is consistent with the long standing debate on the precise temporal codes vs. rate codes as the main principles of information encoding in neural systems @xcite .",
    "the importance of rate code has been confirmed for a number of motor and sensory areas @xcite , whereby the potential advantage of the population rate code may lie in the lesser vulnerability to noise . for the class of models built on the rate - based neurons",
    ", hasegawa has introduced the augmented moment approach @xcite to analyze the mean - field dynamics of globally coupled finite - size populations where the units are subjected to additive and multiplicative noise . while we also consider the rate - based neurons , our model is distinct in that it accounts for the effects arising from the random network topology .",
    "also , the issue of how the effects of noise acting on single units are manifested at the assembly level is addressed in a more elaborate fashion , accounting for the origin of multiplicative noise in the mean - field dynamics .",
    "the paper is organized as follows . in sec .",
    "[ sec : derivation ] , we introduce the rate model of local activity and apply the gaussian closure hypothesis to derive the stochastic mean - field equations for the finite - size population of randomly connected units .",
    "section [ sec : thermo ] concerns the stability analysis of the introduced mean - field model in the thermodynamic limit , where noise intensities act as additional system parameters .",
    "apart from demonstrating the emergence of macroscopic bistable behavior , it is also shown how temporary changing of the level of noise may be used to control the network state in a hysteresis - like scenario . in sec .",
    "[ sec : finite ] we discuss the finite - size effects and determine the magnitude of fluctuations around the stationary states from the thermodynamic limit .",
    "section [ sec : summary ] provides a brief summary of the results obtained .",
    "we consider a network of @xmath0 excitatory neurons . the local activity is described in terms of firing rates @xmath1 $ ] , whose dynamics is given by : @xmath2 in the last equation",
    ", @xmath3 denotes the relaxation characterizing the inertness of units , @xmath4 is the gain function and @xmath5 stands for the coupling coefficient , and @xmath6 is the external current which is taken to be constant .",
    "the above form of rate model is considered paradigmatic @xcite , and a substantial amount of theoretical work has been carried out to analytically obtain the particular transfer functions for a range of spiking neuron models @xcite .     and @xmath7 .",
    "( b ) typical time series @xmath8 of three arbitrary units ( first three rows ) are compared to the time series of the mean rate @xmath9 ( bottom row ) .",
    "note that the fluctuations of the mean rate are much smaller than those for the local variables .",
    "the system parameters are @xmath10 , @xmath11 , @xmath12 , @xmath13 , @xmath14 , @xmath15.[fig : net],width=264 ]    each unit is influenced by the external ( synaptic ) white noise @xmath16 and the internal ( ion - channel ) white noise @xmath17 , whose respective intensities are @xmath18 and @xmath19 .",
    "the external and internal noise sources are assumed to be independent , whereas the random perturbations acting on different units are uncorrelated .",
    "the input @xmath20 which the neuron @xmath21 receives from the rest of the network is specified by @xmath22 where @xmath23 denote the elements of the adjacency matrix . throughout the paper , it is assumed that the interaction topology is random , conforming to the erds - rnyi type of network , see fig .",
    "[ fig : net](a ) .    in the remaining part of this section",
    ", we derive the mean - field model for the collective dynamics of the network given by the system - .",
    "our approach is essentially based on the well - known quasi - independence and gaussian approximations @xcite , and leads to the second - order mean - field model of the macroscopic dynamics . in other words",
    ", we use the moment approach with the gaussian closure hypothesis . the collective behavior is then described in terms of the mean ( assembly - averaged ) rate and the associated variance @xmath24 one naturally expects that the fluctuations of the mean - rate will be comparably smaller than the fluctuations for the local variables , see the sample series in fig .",
    "[ fig : net](b ) .",
    "this point will be confirmed during the derivation of the mean - field equations . in order to make the reading easier ,",
    "a summary of the most relevant notation used throughout the paper is provided in table [ tab : notation ] .",
    ".[tab : notation]summary of the introduced notation . [ cols= \" < , < \" , ]     before proceeding to the analytical part , let us explicitly state the approximations relevant for the derivation of the mean - field model .",
    "the first one concerns the requirement that the random variables @xmath8 at any moment @xmath25 and for sufficiently large @xmath0 satisfy @xmath26 $ ] , where @xmath27 $ ] denotes the expectation over the different stochastic realizations .",
    "the mathematical background of this approximation lies in the strong law of large numbers , which states that the sample average @xmath28 of @xmath0 independently and identically distributed random variables @xmath29 will almost surely converge to the expectation @xmath30 $ ] for @xmath31 .",
    "the form of convergence for large , but finite @xmath0 is specified by the central limit theorem . in physical terms , the outputs of neurons @xmath32 can be considered unbiased if the distribution of the number of incoming connections ( connectivity degrees ) over the population is sufficiently narrow .",
    "the second approximation is in a sense implicit for the validity of the first one , and consists in the requirement that the correlation between the outputs of neurons is negligible : @xmath33=[r_{i}(t)][r_{j}(t)].$ ] this is reasonably satisfied when the units share a small fraction of common input from the network @xcite .",
    "recall that we consider random erds - rnyi networks where the probability of connection between two neurons equals constant value @xmath34 .",
    "for such networks , the fraction of the shared input for two neurons is @xmath34 , while the coefficient of variation for the number of incoming connections equals @xmath35 .",
    "both values are small for @xmath36 .",
    "thus , in large sparsely connected random networks the approximations for the mean - field approach should be fulfilled .",
    "this provided , one can represent the output of each neuron as @xmath37 where @xmath38 are uncorrelated variables with zero mean and unit intensity , cf .",
    "@xcite .    proceeding to the derivation of the mean - field model ,",
    "let us for simplicity first introduce the notation @xmath39 for the total input to the @xmath21-th neuron .",
    "using , the latter can be written as @xmath40 where @xmath41 denotes the number of incoming connections to the @xmath21-th neuron , @xmath42 is the mean number of connections , @xmath43 and @xmath44 .",
    "the deviations @xmath45 are of the order of @xmath46 , and the independence of the variables @xmath38 implies @xmath47 . therefore , the second and the third term in the righthand side of ( [ eq : xi ] ) are of the order of @xmath48 , i.e. are small .",
    "if the external noise @xmath18 is weak as well , the function @xmath49 can be expanded into the taylor series around @xmath50 : @xmath51 in the last expression , we have introduced the notation @xmath52 , @xmath53 and @xmath54 .",
    "note that the products of noisy terms are replaced by the respective means : @xmath55 @xmath56 @xmath57 .",
    "inserting into , one arrives at the equation for the local rates @xmath58 where @xmath59 , @xmath60 and @xmath61 . taking the population average of the equation for microscopic dynamics ,",
    "we obtain the following for mean ( macroscopic ) rate @xmath62 :    @xmath63    where @xmath64 , and @xmath65 is the second central moment of the connectivity degree distribution .    note that eq .",
    "effectively includes three noisy terms .",
    "apart from the external and the internal noise , there is also the `` network noise '' due to variability in connectivity degrees . to estimate the network noise ,",
    "let us first rewrite the corresponding term as @xmath66 since @xmath67 and @xmath68 are not correlated , @xmath69 holds .",
    "taking this into account , the sum of noisy terms in ( [ eq : r1 ] ) can be rewritten as @xmath70 , which is equivalent to white noise with the intensity @xmath71 , where @xmath72 in the last expression , the terms of the order of @xmath73 have been neglected .",
    "now let us derive the equation for the variance @xmath74 .",
    "taking the appropriate it derivatives , one obtains @xmath75 it can readily be shown that the noisy terms completely cancel each other . using the assumption that the outputs @xmath38 are not correlated to the connectivity @xmath45 ,",
    "one arrives at the following equation for the variance @xmath76    taking into account , and , the stochastic mean - field model for the finite - size random network of rate - based neurons reads : @xmath77    before proceeding with the stability and bifurcation analysis , a brief remark is required regarding the numerical treatment of system , and the ensuing dynamics for the assembly average . in particular , the transfer function involves an argument with the stochastic term corresponding to external noise , which can not be resolved unless some approximation is introduced . during the derivation of the mean - field model",
    ", we have expanded the transfer function @xmath78 to taylor series up to second order around the assembly - averaged input @xmath50 , having verified that each of the terms contributing the deviation of the input @xmath79 , received by an arbitrary unit @xmath21 , from @xmath50 is small .",
    "the expansion up to second order may effectively be interpreted as gaussian approximation for the distribution of @xmath78 over the assembly .",
    "when numerically integrating the system , one can not hold that such an approximation holds _ a priori_. it has to be explicitly verified that the distribution of @xmath78 is indeed gaussian for the considered range of neuronal and network parameters . to this end , before running the simulations , we have calculated the distributions of the function @xmath80 for various @xmath81 and evinced that their skewness and excess kurtosis are small , consistent with the gaussian requirement .",
    "this allowed us to replace the term @xmath80 by a gaussian process with the same mean and variance .",
    "plane for three different values of @xmath18 .",
    "( b ) one - parameter bifurcation diagram showing the dependence of the mean rate @xmath62 against the bias current @xmath6 for @xmath82 , @xmath83 .",
    "( c ) the analogous bifurcation diagram as in ( b ) is displayed for @xmath84 .",
    "the solid lines indicate the stable branches , whereas the dashed line refers to the unstable branch .",
    "[ fig : thermo_bd],width=302 ]",
    "in this section , we analyze the stability and bifurcations of the mean - field model ( [ eq : r]-[eq : s ] ) in the thermodynamic limit @xmath85 . under such conditions , the stochastic term in ( [ eq : r ] )",
    "can be neglected , so that the network dynamics effectively becomes deterministic .",
    "the influence of noise is reduced to respective noise intensities @xmath18 and @xmath19 , which may be regarded as additional system parameters . for simplicity , let us further set @xmath86 and consider the activation function @xmath87 of the form @xmath88 consistent with the notation introduced above , cf .",
    ", one has @xmath89 , while the first- and second - order derivatives are @xmath90 , @xmath91 for @xmath92 .",
    "the dynamics of variance @xmath74 in the thermodynamic limit is governed by the equation @xmath93 following relaxation , the variance reaches the stationary value @xmath94 further note that the the dynamics of the mean rate @xmath62 , given by , becomes @xmath95 which is independent on the variance @xmath74 . taking into account that @xmath96 , where @xmath97 , one can rewrite as    @xmath98    the analysis of indicates that it always exhibits at least one stable stationary state . for the parameter values given by @xmath99    undergoes pitchfork bifurcation where two stable steady states are created separated by an unstable one .",
    "the stable states correspond to two distinct values of the mean firing rate which we further refer to as the `` low '' and the `` high '' state . for strong enough coupling @xmath100 ,",
    "the high ( low ) state emerges via the saddle - node bifurcation , which occurs at the parameter value @xmath101 where the minus sign corresponds to the high , and plus to the low state .",
    "for @xmath6 between these two values , the high and the low states coexist , such that the network is in bistable regime . the two - dimensional bifurcation diagram in fig.[fig : thermo_bd](a )",
    "shows the curves for different values of @xmath18 .",
    "one can see that the two curves form a `` tongue '' inside which the network is bistable .",
    "figures [ fig : thermo_bd](b ) and ( c ) display the one - dimensional bifurcation diagrams for parameter values outside and within the bistability tongue , respectively .",
    "we note the interesting role played by the intensity of external noise @xmath18 .",
    "it is found to influence the position of the bistability region , shifting it `` upwards '' toward the domain of stronger couplings .",
    "this observation instigated an idea of the potential network control mechanism via the noise intensity . in order to illustrate this mechanism ,",
    "we have analyzed in more detail how the network dynamics depends on @xmath18 . to this end",
    ", one can solve the equation @xmath102 with respect to @xmath18 and obtain the following expression @xmath103    for @xmath104 , the corresponding one - dimensional bifurcation diagram is provided in fig .",
    "[ fig : noise](a ) .",
    "the dependence @xmath105 is single - valued for @xmath106 , where @xmath107 . for such @xmath18 , only the high state of the network exists . for @xmath108 ,",
    "the saddle - node bifurcation takes place , whereby the low state is born .",
    "the latter state is found for @xmath109 . since only positive values of @xmath18",
    "are physically meaningful , the low state branch exist only for @xmath110 , which is equivalent to the condition @xmath111 for @xmath112 and @xmath6 satisfying , the network is bistable for @xmath109 and exhibits only the high state for @xmath106 .",
    "therefore , a pulse - like increase of @xmath18 may switch the network from the low to the high state via the hysteresis scenario .",
    "this effect is illustrated in fig .",
    "[ fig : noise](b ) , which shows the network dynamics before , during and after the pulse - like change of the external noise @xmath18 .",
    "prior to strengthening , the external noise level is @xmath113 , such that the network is bistable and is settled in the low lying state .",
    "as soon as the external noise intensity is temporarily increased to @xmath114 , the low state vanishes , and the network switches to the high state . when @xmath18 regains the initial value , admits bistable regime again , but the network remains in the high state",
    "thus , the temporary increase of @xmath18 has caused the network to switch from the low state to the high state .",
    "note that for @xmath115 , the inverse scenario is possible , where the network can switch from the high state to the low state by a pulse - like increase of the external noise .",
    "an example for such a scenario is illustrated in fig .",
    "[ fig : noise](c ) and fig .",
    "[ fig : noise](d ) .    interestingly enough , the external noise has effect not only on the stationary states of the network , but is found to influence its transient dynamics as well .",
    "the transient dynamics is important when the external input changes and the network has to track this change and adapt to its rate accordingly . in this scenario , short response time of a network is naturally considered as advantageous @xcite .",
    "our analysis shows that under certain conditions , introduction of the external noise may sufficiently reduce the response time . to understand this ,",
    "let us consider the situation when the input @xmath6 switches from some value @xmath116 to the new value @xmath117 . for simplicity",
    ", we assume that the other parameters are set so that the network is always monostable . then , the network will evolve from the previous stationary state @xmath118 to the new one @xmath119 . according to ( [ eq : x_thermo ] )",
    ", the rate @xmath120 of the system convergence to @xmath119 is determined by the absolute value of the derivative @xmath121 :    @xmath122    thus , strengthening of the external noise @xmath18 increases the rate @xmath120 and speeds up the network response .",
    "this finding is corroborated by numerical simulations illustrated in fig .",
    "[ fig : trans ] here , two networks are considered : the first one without the external noise ( @xmath82 , blue curve ) , and the second one with noise ( @xmath123 , red curve ) .",
    "the values of the other parameters are given in the caption to the figure .",
    "for both cases , the input @xmath6 changes its value at the moment @xmath124 so that the stationary value of @xmath50 changes from @xmath125 to @xmath126 .",
    "the estimate ( [ eq : rate_thermo ] ) then gives @xmath127 without noise and @xmath128 with noise , which implies a fourfold speedup of the network response .",
    "note that the numerical results show satisfactory agreement with the theoretical predictions .     .",
    "the change of external input occurs at @xmath124 .",
    "the response of the network for @xmath82 is shown by the blue ( light gray ) lines , and for @xmath123 by the red ( dark gray ) lines .",
    "the remaining network parameters are @xmath129 @xmath130 , @xmath131 . .",
    "the thick lines represent the numerical results , whereas the thin lines denote the theoretical estimates.[fig : trans],width=264 ]",
    "in this section we analyze the influence of the finite - size effects in case where the network is large but finite , viz . @xmath132 .",
    "then , the noise term in can no longer be considered zero , and can give rise to stochastic fluctuations of the mean rate around the values obtained for the thermodynamical limit .    to study the magnitude of fluctuations ,",
    "let us rewrite as follows : @xmath133 for large @xmath0 , the variables @xmath50 and @xmath74 are close to the respective values @xmath134 and @xmath135 from the thermodynamic limit , whereby @xmath134 is defined by the condition @xmath136 , and @xmath135 by ( [ eq : s0 ] ) . since the fluctuations @xmath137 are small , one can linearize and obtain @xmath138 since the state is stable in the thermodynamic limit , @xmath139 applies .",
    "the steady state s displacement due to the finite size effect equals @xmath140 this deviation is of the order of @xmath141 , while the random fluctuations of @xmath50 due to noise are of the order of @xmath48 .",
    "this allows one to neglect the second term in and obtain the following expression for the variance of @xmath50 over stochastic realizations : @xmath142=\\frac{2\\psi(x_{0},s_{0})}{-2nf^{\\prime}(x_{0})}=\\frac{\\left(d+bh_{1}^{2}\\right)\\left(2+\\alpha^{2}h_{1}^{2}\\right)}{2n\\left(12\\alpha b+1-\\alpha h_{1}\\right)}.\\label{eq : x2}\\ ] ] the expressions ( [ eq : x0 ] ) and ( [ eq : x2 ] ) both contain @xmath143 in the denominator . when the value of @xmath143 becomes small ,",
    "the two formulas lose validity since the linearization of ) is no longer adequate .",
    "note that such scenario corresponds to the parameter domain near the saddle - node bifurcations of the system in the thermodynamic limit .        in order to verify the validity and the accuracy of the developed mean - field approach , we have performed direct simulations of the network ( [ eq:1 ] ) and compared the results with the predictions of the theory .",
    "we find that for @xmath144 , @xmath145 and @xmath146 the theory holds quite well in most of the cases : the mean rate of the network is typically predicted with the accuracy no less than @xmath147% .",
    "the theory s validity reduces for the values of @xmath62 close to zero and unity since the second derivative @xmath148 has discontinuity at these points .    the comparison between the numerical and the theoretical results is provided in fig .",
    "[ fig : finite ] .",
    "the intention is to first consider a sufficiently large network @xmath149 , where the mean - field treatment is expected to hold , see fig .",
    "[ fig : finite](a ) .",
    "in particular , for each parameter value , the network is simulated for the period @xmath150 starting from @xmath151 different randomly chosen initial conditions . after the transient @xmath152",
    ", all the observed mean rates @xmath62 were saved and plotted versus the corresponding parameter value . the theoretical prediction for the mean",
    "is superimposed on this plot ( see the dashed lines ) . to check the predictions for the magnitude of the stochastic fluctuations",
    ", we have further plotted together the observed variance and the estimate , cf . figs .",
    "[ fig : finite](b),(c ) .",
    "since the network is bistable in a certain parameter interval , the results are plotted separately for the low and the high branches .",
    "as expected , the theory becomes inadequate close to the points where the branches vanish through the saddle - node bifurcations . in the rest of the parameter interval",
    "the theoretical estimate is quite precise .    the second row in fig .",
    "[ fig : finite ] illustrates the breakdown of theory for smaller system sizes . as an example",
    ", we consider the case @xmath153 .",
    "note that the upper branch of the mean rates substantially deviates from the theoretical prediction .",
    "one also finds that the magnitude of stochastic fluctuations are much larger than what is anticipated by the approximate model , because the assumptions behind no longer hold .",
    "note that the influence of the system finite - size on the value of the variance @xmath74 amounts only to its small change , which is of the order @xmath141 .",
    "namely , the stationary value of the variance for large @xmath0 equals @xmath154",
    "in this paper , we have considered a network of rate - based neurons with random connectivity and two types of noise . in order to study the macroscopic dynamics of the network ,",
    "we have developed the second - order mean - field approach which incorporates the gaussian closure hypothesis .",
    "the dynamics of the large , but finite network is described in terms of the assembly averaged firing rate and the associated variance , whose evolution is given by the system - .",
    "the main approximations relevant to the derivation of the model are that the outputs of the units are unbiased and uncorrelated .",
    "the analysis shows that these assumptions are valid for large networks with random sparse connectivity.in fact , such type of connectivity renders correlation between the outputs of units small , which is the point relevant for our derivation .    in the context of neuroscience ,",
    "random networks are often considered as the simplest model of connectivity of neural circuits @xcite . on the other hand ,",
    "most of the research so far dedicated to mean - field approach for stochastic systems has addressed the scenario of a fully connected network @xcite .",
    "however , recent experimental data provides evidence that the organization of synaptic connections in brain is nontrivial and differs drastically from both of the above models @xcite .",
    "the structure of neural networks appears to be inhomogeneous , in a sense that most of the connections are random and sparse , but some units are also organized into densely connected clusters @xcite .",
    "such clusters have already been established to play an important information - processing role in the cortex @xcite . within a broader research agenda",
    ", the results gained here for the case of random networks , if incorporated together with the previous work on fully connected networks , may ultimately allow us to derive the mean - field model appropriate for clustered networks .    in terms of research goals ,",
    "most of the early studies applying the mean - field approach have been focused on explaining the mechanisms behind the spontaneous activity characterized by irregular firing of neurons at low rates , typically found in a living cortex or living hippocampus .",
    "apart from gaining insight into the genesis and the self - sustaining property of these chaotic states , the aim has also been to explain why populations of highly nonlinear units display linear responses to external drive , reacting on time scales faster than the characteristic time scale of a single unit .",
    "the emergence of relevant cooperative states has been linked to several different ingredients , including the features of the units threshold function , the network connection topology and the scaling of synaptic strengths . in particular , for a fully connected network of rate - based units with random asymmetrical couplings similar to spin glasses , the onset of chaos has been associated to the gain parameter of the threshold function @xcite . for networks comprised of binary neuron - like units ,",
    "the most important finding has concerned the existence of a chaotic balanced state , where variability is achieved by the balance of excitatory and inhibitory inputs , each being much larger than the units threshold @xcite .",
    "necessary conditions for maintaining such a regime include random and sparse connectivity , as well as comparably strong synapses . under similar conditions",
    "the networks of integrate - and - fire neurons have been found to support a bistable regime between the spontaneous activity , uncorrelated with the received stimuli , and the working memory states , strongly correlated with the learned stimuli @xcite .",
    "further research have revealed importance of the weight distribution in random networks of integrate - and - fire neurons and its essential role for the spike - based communication @xcite .    at variance with theabove models , which typically do not consider at all or provide only a limited account of the effects of noise ,",
    "the central issue of research in recent years has become the point of how noise from the level of single units is translated to and reflected in the macroscopic - scale behavior .",
    "the present study aims to contribute to this line of research , and our main results can be summarized as follows . in the thermodynamic limit ,",
    "the network dynamics is deterministic in nature .",
    "we have determined the stationary levels of the network activity , showing that for strong enough coupling ( @xmath100 , see eq .",
    "( [ eq : pitchfork ] ) ) the network exhibits bistable regime , characterized by coexistence of the low and the high stable states . in terms of how noise from microscopic dynamics",
    "effectively impacts the collective behavior , our most important finding is that the external and the internal noise play essentially different roles in the mean - field dynamics . in particular , in the thermodynamic limit , the internal noise does not influence the macroscopic dynamics at all , while the external noise changes the position and the number of stable levels .",
    "we have demonstrated that this feature can be used to control the network dynamics via external noise in a hysteresis - like scenario , as illustrated in fig .",
    "[ fig : noise ] .",
    "we have also shown that the external noise influences the transient dynamics of a network , , at certain instances being able to speed up its response to the change of external drive ..    the developed theory has also allowed us to consider the finite - size effects on the network dynamics .",
    "the corresponding approximate model for large but finite networks effectively involves three sources of noisy behavior .",
    "apart from the internal and external noises , which manifest as the additive and multiplicative noise at the macroscopic level , we identify an additional term that derives from heterogeneity in the units connectivity degrees .",
    "we have found that the finite - size effects are twofold and consist in @xmath155 displacement of the stationary levels and @xmath156 in giving rise to stochastic fluctuations of the mean rate . since the change of the stationary values of @xmath62 and @xmath74 is of the order of @xmath141 , the most important are the stochastic fluctuations which have the magnitude of the order of @xmath48 .",
    "it has also been explicitly demonstrated that the developed approach provides satisfactory estimate of the magnitude of the fluctuations for the parameter domain sufficiently away from the bifurcations .",
    "we suspect that novel interesting effects may arise in sufficient vicinity of the pitchfork bifurcation , where the network possesses two stable activity levels that are relatively close to each other . in this case , the derivatives @xmath157 are close to zero for both states , and the estimate provided by eq . indicates large fluctuations of the mean rate .",
    "if one approaches close enough to the bifurcation , the magnitude of fluctuations may become of the order of the distance between the levels , which is likely to induce stochastic `` switching '' between the low and the high state .",
    "this phenomenon may be associated to high variability of firing rates often observed in neural networks and recently connected to clustering of synaptic connections @xcite . however , linearization of eq . in this case",
    "is no longer adequate , such that the full nonlinear equations ( [ eq : r]-[eq : s ] ) should be studied to capture the potential phenomenon of stochastic switchings .",
    "this will be one of the main goals for our future research .",
    "this work was supported in part by the russian foundation for basic research ( grant 14 - 02 - 00042 ) and the ministry of education and science of the republic of serbia under project no .",
    "the authors would like to thank prof .",
    "nikola buri and prof .",
    "vladimir nekorkin for valuable discussions during the different stages of the research .",
    "v. s. anishchenko , v. astakhov , a. neiman , t. vadivasova , l. schimansky - geier , _ nonlinear dynamics of chaotic and stochastic systems : tutorial and modern developments _ , ( springer - verlag , berlin heidelberg , 2007 ) ."
  ],
  "abstract_text": [
    "<S> we consider a network of randomly coupled rate - based neurons influenced by external and internal noise . we derive a second - order stochastic mean - field model for the network dynamics and use it to analyze the stability and bifurcations in the thermodynamic limit , as well as to study the fluctuations due to the finite - size effect . </S>",
    "<S> it is demonstrated that the two types of noise have substantially different impact on the network dynamics . </S>",
    "<S> while both sources of noise give rise to stochastic fluctuations in case of the finite - size network , only the external noise affects the stationary activity levels of the network in the thermodynamic limit . </S>",
    "<S> we compare the theoretical predictions with the direct simulation results and show that they agree for large enough network sizes and for parameter domains sufficiently away from bifurcations . </S>"
  ]
}