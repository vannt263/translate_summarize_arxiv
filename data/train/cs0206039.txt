{
  "article_text": [
    "in this paper we discuss the following problem of _ time series segmentation _ : _ _ given a time series , divide it into two or more  _ segments _ ( i.e. blocks of contiguous data ) such that each segment is homogeneous , but contiguous segments are heterogeneous .",
    "homogeneity / heterogeneity is described in terms of some appropriate statistics of the segments . the term _ change point detection _ is also used to describe the problem .",
    "examples of this problem arise in a wide range of fields , including engineering , computer science , biology and econometrics .",
    "the segmentation problem is also relevant to hydrology and environmetrics .",
    "for instance , in climate change studies it is often desirable to test a time series ( such as river flow , rainfall or temperature records ) for one or more sudden changes of its mean value .",
    "the time series segmentation problem has been studied in the hydrological literature .",
    "the reported approaches can be divided into two categories :  _ sequential _ and _ nonsequential_. sequential approaches often involve _ intervention models _ ; see for example @xcite and , for a critique of intervention models , @xcite .",
    "most of the nonsequential time segmentation work appearing in the hydrological literature involves _ two _ segments . in other words ,",
    "the goal is to detect the existence and estimate the location of a _ single _ change point .",
    "a classical early study of changes in the flow of nile appears in @xcite .",
    "buishand s work @xcite is also often cited .",
    "for some case studies see @xcite .",
    "bayesian approaches have recently generated considerable interest @xcite .",
    "it appears that the _ multiple _ change point problem has not been studied as extensively .",
    "hubert s segmentation procedure @xcite is an important step in this direction .",
    "the goodness of a segmentation is evaluated by the sum squared deviation of the data from the means of their respective segments ; in what follows we will use the term _ segmentation cost _ for this quantity . given a time series",
    ", hubert s procedure computes the _ minimal cost _ segmentation with @xmath02 , 3 , ... change points .",
    "the procedure gradually increases @xmath1 ; for every value of @xmath1 the best segmentation is computed ; the procedure is terminated when differences in the means of the obtained segments are no longer statistically significant ( as measured by scheffe s contrast criterion @xcite ) .",
    "hubert mentions that this procedure can segment time series with several tens of terms but is `` ... unable at the present state to tackle series of much more than a hundred terms ... '' because of the combinatorial increase of computational burden @xcite .",
    "the work reported in this paper has been inspired by hubert s procedure .",
    "our goal is to develop an algorithm which can locate multiple change points in hydrological and/or environmental time series with several hundred terms or more . to achieve this goal",
    ", we adapt some _ hidden markov models ( hmm ) _ algorithms which have originally appeared in the speech recognition literature .",
    "( a survey of the relevant literature is postponed to section [ sec0303 ] . )",
    "we introduce a hmm  of hydrological and/or enviromental time series with change points and describe an approximate _ expectation / maximization ( em )  algorithm _ which produces a converging sequence of segmentations .",
    "the algorithm also produces a sequence of estimates for the hmm  parameters .",
    "time series of several hundred points can be segmented in a few seconds ( see section [ sec04 ] ) , hence the algorithm can be used in an interactive manner as an exploratory tool . even for time",
    "series of several thousand points the segmentation time is in the order of seconds .",
    "this paper is organized as follows . in section [ sec02 ]",
    "we review hubert s formulation of the time series segmentation problem . in section [ sec03 ]",
    "we formulate the segmentation problem in terms of hidden markov models and present a segmentation algorithm ; also we compare the hidden markov model approach with that of hubert .",
    "we present some segmentation experiments in section [ sec04 ] . in section [ sec05 ]",
    "we summarize our results . finally , in the appendix we present an alternative , non - hmm segmentation method , which is more accurate but also slower .",
    "in this section we formulate time series segmentation as an optimization problem .",
    "we follow hubert s presentation , but we modify his notation .    given a time series  @xmath2  = ( @xmath3 , @xmath4 , ... , @xmath5 ) and a number @xmath1 , a _ segmentation _ is a sequence of times @xmath6 = @xmath7 , @xmath8 , ... ,",
    "@xmath9 which satisfy@xmath10 the intervals of integers @xmath11 , @xmath12 $ ] , @xmath13 $ ] , ... , @xmath14 $ ] are the _ segments _ ; the times @xmath15 , @xmath8 , ... , @xmath16 are the _ change points_. @xmath1 , the number of segments , is the _ order _ of the segmentation . the length of the @xmath17-th segment ( for @xmath18 ) is denoted by @xmath19 .",
    "the following notation is used for a given segmentation @xmath6 = @xmath7 , @xmath8 , ... , @xmath20 . for @xmath18 ,",
    "define@xmath21 define the _ cost _ of segmentation @xmath22 by@xmath23 if @xmath24 has a small value , then the segments are homogeneous , i.e. the @xmath25 s are close to @xmath26 for @xmath18 and for @xmath27 .",
    "now we can define the best @xmath1-th order segmentation @xmath28 to be the one minimizing @xmath29 and denote the minimal cost by @xmath30 = @xmath31 .",
    "note that for every @xmath1 we have @xmath32 @xcite .",
    "also , there is only one segmentation @xmath6 of order @xmath33 ; in this case every time instant @xmath34 is a segment by itself and @xmath35 .",
    "it can be seen @xcite that the number of possible segmentations grows exponentially with @xmath33 . to efficiently search the set of all possible segmentations , hubert uses a _",
    "branch - and - bound _ approach .",
    "even so , the computational load increases excessively with @xmath33 and this approach is not able currently ( in 2000 ) to segment series of much more than a hundred terms @xcite .",
    "minimization of @xmath24 can be achieved by several alternative ( and faster than branch - and - bound ) algorithms .",
    "a _ dynamic programming _ approach is presented in the appendix to obtain the globally minimum cost ; this is feasible for @xmath33 in the order of several hundreds and will be reported in greater length in a future publication @xcite . in this paper",
    "a different approach is followed , which is based on hmm s .",
    "we now present a hmm formulation of the time series segmentation problem .",
    "hmm s have been used for runoff modeling @xcite and the possibility of using them for hydrological time series segmentation has been mentioned in @xcite but , as far as the author knows , an actual implementation has not been presented yet . on the other hand , we have already mentioned that hmm s are used for segmentation of time series in several other fields ( see the discussion in section [ sec0303 ] ) .",
    "the term `` hidden markov model '' is used to denote a broad class of stochastic processes ; here we use a particular and somewhat restricted species of hmm to model a hydrological time series and present an approximate expectation / maximization ( em ) algorithm to perform _ maximum likelihood _ ( ml ) segmentation .",
    "in addition to the standard probabilistic interpretation of the algorithm , a numerical optimization point of view is also possible and we use the latter to prove the convergence of the algorithm .",
    "finally we discuss related algorithms and possible extensions .",
    "we will use a pair of stochastic processes @xmath36 to model a hydrological time series with change points .",
    "we start by considering a simple example .    the annual flow of a river is denoted by @xmath37 .",
    "we assume that , for the years @xmath38 , @xmath37 is a normally distributed random variable with mean @xmath39 and standard deviation @xmath40 . in year",
    "@xmath8 a _ transition _ takes place and , for the years @xmath41 , @xmath42 , @xmath37 is normally distributed with mean @xmath43 and standard deviation @xmath40 .",
    "this process continues with transitions taking place in years @xmath44 , @xmath45 , ... ,",
    "this process is illustrated in figure 1 .",
    "we indicate the _ states _ of the river flow by circles and the possible transitions from state to state by arrows ; note that the states are _",
    "unobservable_. we indicate the observable time series by the double arrows emanating from the states .    *",
    "figure 1 to appear here *    the above mechanism can be modeled by a pair of stochastic processes @xmath36 ( with @xmath47 ) defined as follows .",
    "1 .   @xmath48 , which is the _ state process _ , is a finite state markov chain with @xmath1 states ; it has initial probability vector @xmath49 and transition probability matrix @xmath50 .",
    "hence , for any @xmath33 , the joint probability function of  @xmath51 is@xmath52 for the specific example discussed above , it will also be true that :  ( a ) @xmath53 , @xmath54 for @xmath55 , ( b )  @xmath56 for @xmath18 and all @xmath57 other than @xmath17 , @xmath58 . the parameters of this process are @xmath1 and @xmath50 .",
    "2 .   @xmath37 , which is the _ observation process _ , is a sequence of _ conditionally independent , _ normally distributed random variables with mean @xmath59 and standard deviation @xmath40 .",
    "more precisely , for every @xmath34 , the joint probability density of  @xmath60 conditioned on @xmath61 is@xmath62 the parameters of this process are @xmath39 , @xmath43 , ... , @xmath63 and @xmath40 .",
    "we will often use the notation @xmath64 = [ @xmath39 , @xmath43 , ... , @xmath63 ] .",
    "the @xmath36 pair is a hmm , in particular a _ left - to - right continuous hmm _",
    "`` left - to - right '' refers to the structure of state transitions ( as depicted in figure 1 ) and `` continuous '' refers to the fact that the observation process is continuous valued .",
    "the model parameters are @xmath65 @xmath66 @xmath67 @xmath40 .",
    "there is a one - to - one correspondence between state sequences @xmath68 =  ( @xmath69 , @xmath70 , ... , @xmath71 ) and segmentations",
    "@xmath6 = ( @xmath72 , @xmath8 , ... , @xmath73 ) .",
    "for example , given a particular @xmath68 , we obtain the corresponding @xmath6  by locating the times @xmath74 such that @xmath75 , for @xmath76 ( and setting @xmath77 and @xmath78 ) .",
    "the postulated markov chain only allows left - to - right transitions , hence @xmath79 , i.e. there will be _ at most _",
    "@xmath80segments , and every segment will be uniquely associated with a state .",
    "the _ conditional likelihood _ of a state sequence @xmath68 ( _ given _ an observation sequence @xmath2 )  is denoted by @xmath81 and the _ joint likelihood _ of a state sequence @xmath68 _ and _ an observation sequence @xmath2 is denoted by @xmath82 @xmath83 and @xmath84 are understood as functions of @xmath68 = @xmath85 ; the observations @xmath2 =  ( @xmath3 , @xmath4 , ... , @xmath5 ) , the number of segments @xmath1 , and the length of the time series @xmath33 , as well as the parameters @xmath66 @xmath67 @xmath40 are assumed _ fixed_. in place of @xmath33 any @xmath34 can be used , to indicate the likelihood of the subsequence @xmath86 given @xmath87 etc . for example , we can write @xmath88 note also that@xmath89 where @xmath90 is a quantity independent of @xmath85 . finally , from ( [ eq01 ] ) , ( [ eq02 ] ) we have@xmath91 where @xmath92 = 1 , according to the previously stated assumption .",
    "the ml  segmentation @xmath28 can be obtained from the ml  state sequence @xmath93 = ( @xmath94 , @xmath95 , ... , @xmath96 ) .",
    "since state sequences are unobservable , we will estimate @xmath93 in terms of the observable sequence @xmath2  = ( @xmath3 , @xmath4 , ... , @xmath5 ) and @xmath97the  parameters @xmath65 @xmath66 @xmath67 @xmath40 .",
    "note that in practice @xmath65 @xmath66 @xmath67 @xmath40 will also be unknown .",
    "hence the computation of the maximum likelihood  hmm segmentation must be divided into two subtasks :  ( a )  estimating the hmm  parameters and ( b )  computing the actual segmentation .",
    "we follow the standard approach used in hmm  problems : a parameter estimation phase is followed by a time series segmentation phase and the process is repeated until convergence .",
    "this is the expectation / maximization ( em ) approach .",
    "first we discuss estimation and segmentation in more detail ; then we will return to a discussion of the em  approach .",
    "suppose , for the time being , that a segmentation @xmath6 = @xmath98 , ... , @xmath99 is given .",
    "a reasonable estimate of @xmath64 = [ @xmath39 , @xmath43 , ... , @xmath63 ] , _ dependent on the given segmentation _ , is ( for @xmath18)@xmath101 similarly we could use the following _ segmentation - dependent _ estimates of @xmath40 ( for @xmath18)@xmath102 however , to maintain compatibility with hubert s approach , we will use the _ segmentation - independent _",
    "estimate@xmath103 where @xmath104    let us now turn to the transition probability matrix @xmath50 . in a left - to - right hmm , for @xmath18 and all @xmath57 different from @xmath17 and @xmath58 , we will have @xmath56 . also , for @xmath105 we will have @xmath106 .",
    "hence @xmath50 only has @xmath107 free parameters , namely @xmath108 , @xmath109 , ... ,",
    "these could be estimated from the given segmentation .",
    "however , in this paper we use a simpler approach .",
    "namely , we assume@xmath111{cccccc}% p & 1-p & 0 & ... & 0 & 0\\\\ 0 & p & 1-p & ... & 0 & 0\\\\ ... & ... & ... & ... & ... & ... \\\\ 0 & 0 & 0 & ... & p & 1-p\\\\ 0 & 0 & 0 & 0 & 0 & 1 \\end{array } \\right ]   .",
    "\\label{eq31}%\\ ] ] hence @xmath50 is determined in terms of a single parameter @xmath112 , which will be chosen a priori , rather than estimated .",
    "we have found by numerical experimentation that the exact value of @xmath112 is not critical ; in all the examples of section [ sec04 ] , the segmentation algorithm performs very well using @xmath112 in the range [ 0.85,0.95 ] .",
    "finally , we must make a choice regarding the number of segments @xmath1 .",
    "we will use hubert s approach , and take a sequence of increasing values :  @xmath113 , @xmath114 , ... until a value of @xmath1 is reached which yields statistically nonsignificant segmentations ( statistical significance is evaluated by scheffe s contrast criterion , @xcite ) .      given observations @xmath115 and assuming the parameters @xmath1 , @xmath50 , @xmath64 * * , * * @xmath40 to be  known , the _ maximum likelihood ( ml ) _ state sequence is the @xmath93 = @xmath116 which maximizes @xmath117 as function of @xmath68",
    ". the ml  segmentation @xmath28 = ( @xmath118 , @xmath119 , ... , @xmath120 )  is obtained from @xmath93 .",
    "it will be seen in section [ sec03024 ] that , under certain circumstances , @xmath121  also minimizes the segmentation cost @xmath24 defined in section [ sec02 ] .",
    "@xmath93 @xmath122 @xmath123 can be found by the _ viterbi algorithm _",
    "@xcite , a computationally efficient dynamic programming approach . in view of ( [ eq05 ] )",
    "we have @xmath124 now , for @xmath125 @xmath97and _ _ @xmath18",
    "define@xmath126 it can be shown by standard dynamic programming arguments @xcite that both @xmath93",
    "@xmath122 @xmath127 , @xmath95 , @xmath128 , @xmath129 and the @xmath130 s of ( [ eq012 ] ) can be computed recursively as follows .    ' '' ''    * viterbi algorithm *    * * input * : the time series @xmath131 ; the parameters @xmath1 , @xmath50 , @xmath64 and @xmath40 . * * forward recursion * * set @xmath132 , @xmath133 . * for @xmath125 @xmath97 _ _ * * for @xmath18 @xmath134 * * end * end * * backtracking * * @xmath135 * @xmath136 . * for @xmath137@xmath138 * end    ' '' ''    upon completion of the forward recursion , @xmath139 , the maximum value of @xmath84 , is obtained .",
    "the backtracking phase produces the state sequence which maximizes @xmath84 ( and hence also @xmath83 ) .",
    "execution time is of order o(@xmath140 ) which is _ linear _ ( rather than exponential )  in the length of the time series @xmath33 .",
    "this makes the algorithm computationally feasible even for long time series . for more details on the viterbi algorithm",
    "see @xcite .      parameter estimation and segmentation can be combined in an algorithm which maximizes the likelihood viewed as a function of _ both _ the state sequence @xmath68 = @xmath141 , ... , @xmath142 and the parameters @xmath64 .",
    "the algorithm presented below is an iterative _ expectation / maximization _ ( em )  algorithm @xcite  which produces a converging sequence of segmentations .    ' '' ''    * hmm  segmentation algorithm *    * * input : *  the time series @xmath143 ; the parameters @xmath1 , @xmath50 ; a termination variable @xmath144 . *",
    "choose randomly a state sequence @xmath145 * * = ( * * @xmath146 , ... , @xmath147 ) . *",
    "compute @xmath148 from @xmath97([eq22 ] ) . * for @xmath149 @xmath97",
    "_ _ * * compute @xmath150from @xmath151 . * * compute @xmath152 from @xmath150and ( [ eq21 ] ) . * * compute @xmath153 by the viterbi algorithm using @xmath2 , @xmath1 , @xmath50 , @xmath154 and @xmath148 . * * if @xmath155@xmath156 , @xmath157,@xmath158 @xmath159 @xmath160 , @xmath161,@xmath162 . * * * @xmath93 = @xmath153 . * * * exit the loop * * endif * end    ' '' ''    in section [ sec03024 ] we will show that the above algorithm is a very close approximation to an em  algorithm and that , under certain conditions , every iteration increases the likelihood function . in all the examples presented in section [ sec04 ]",
    "the algorithm converges to the _ global _ maximum with very few iterations ( typically 3 or 4 ) . in other words , the outer loop of the algorithm",
    "is executed only a few times ; in each execution we perform a parameter reestimation according to ( [ eq21 ] ) ( with execution time o(@xmath33 ) )  and a segmentation by the viterbi algorithm ( with execution time o(@xmath140 ) ) . hence the total execution time for a fixed @xmath1 value is o(@xmath140 ) .    for a complete segmentation procedure",
    "the above algorithm is run for a sequence of increasing values @xmath163 @xmath164 first the algorithm is used to obtain the ml  segmentation of order @xmath02 ; the difference of the means of the two segments is tested for statistical significance by the scheffe criterion ( for details see @xcite and @xcite ) .",
    "if the difference is not significant , then it is concluded that the entire time series consists of a single segment .",
    "if the difference is significant , the algorithm is run with @xmath165 and the scheffe test is applied to the resulting segments .",
    "the process is continued until , for some value of @xmath1 , a segmentation is obtained which fails the scheffe test ( or until we reach @xmath166 , an unlikely case ) .    the use of scheffe s contrast criterion to determine the true value of @xmath1 is somewhat problematic .",
    "this point is discussed in some detail in @xcite .",
    "many methods for the determination of @xmath1 have been proposed in the literature , but none of these completely resolves the problem . in cases of doubt , a pragmatic approach would be to use human judgement to evaluate segmentations with different @xmath1 s . in the case of hydrological and environmental time series which involve a rather small number of segments ,",
    "this is relatively easy .",
    "the short execution time of the segmentation algorithm favors this approach , since experimentation in an `` interactive '' mode is feasible .",
    "the goal of this section is to show that , for a fixed @xmath1 , every iteration of the hmm  segmentation algorithm increases the likelihood ; since the likelihood is bounded above by one , this also implies that the algorithm converges@xmath164    two approaches can be used .",
    "the first approach is based upon the probabilistic interpretation of the algorithm ; since this is a routinely applied analysis of em  algorithms , it will be presented only in outline . in the second approach",
    ", the  segmentation algorithm is viewed from a numerical optimization point of view and convergence is proved without using any probabilistic assumptions ; furthermore this approach shows clearly the connection of our segmentation algorithm to hubert s procedure .",
    "* probabilistic approach*. as explained in @xcite , the basic ingredient of the em  family of algorithms is the iterative application of an expectation step followed by a likelihood maximization step . in our case",
    "the expectation step consists in estimating @xmath167by ( [ eq21 ] ) and the maximization step consists in finding @xmath168 by the viterbi algorithm .    while the viterbi algorithm computes exactly the global maximum of the likelihood ( viewed as a function of @xmath68 only ! ) , the estimation step used in this paper is approximate .",
    "the exact step would involve computing estimates of @xmath169 , @xmath170 , @xmath128 , @xmath171 for every possible segmentation and then combining these estimates in a sum weighted by the respective probability of each segmentation ( a similar approach should be used for @xmath40 , using the estimates of ( [ eq23 ] ) ) .",
    "this approach is used in @xcite and elsewhere ; while it is computationally more expensive than the approach used here , it is still viable . at any rate , in most cases the two approaches yield very similar results .    if it is assumed that the estimate of ( [ eq21 ] ) is a close approximation to the maximum likelihood estimate of @xmath64 , then convergence can be established by a standard em  argument presented in @xcite and several other places .",
    "this argument shows that a certain cross entropy @xmath172 is decreased by every iteration of an em algorithm .",
    "since @xmath173 is always nonnegative , it must converge to a nonnegative number , and this suffices for the algorithm to terminate .",
    "furthermore , by relating @xmath172 to the likelihood , it can be shown that the sequence @xmath174 is monotonically increasing .",
    "* numerical approach*. in what follows we will consider @xmath1 , @xmath50 , @xmath2 * * , * * @xmath175 to be fixed .",
    "we will denote the set of all possible state sequences by @xmath176 and the set of all state sequences with @xmath1 transitions by @xmath177 ; we will also use the standard notation @xmath178 for the set of all @xmath1-dimensional real vectors .    taking the negative logarithm of ( [ eq09 ] ) we obtain@xmath179 = -\\sum_{t=1}^{t}\\log\\left (   p_{z_{t-1},z_{t}}\\right )   + \\sum_{t=1}% ^{t}\\frac{\\left (   x_{t}-\\mu_{z_{t}}\\right )   ^{2}}{2\\sigma^{2}}.\\label{eq41}%\\ ] ]",
    "we define @xmath180 `` number of times @xmath181 '' ; in other words , @xmath182 is the number of transitions in the state sequence @xmath68 .",
    "if we limit ourselves to state sequences @xmath183 , then obviously @xmath184 .",
    "now , for all @xmath183 , ( [ eq41 ] )  becomes@xmath185 &   = -\\left (   ( t-\\phi(\\mathbf{z}))\\cdot\\log\\left (   p\\right )   + \\phi ( \\mathbf{z})\\cdot\\log\\left (   1-p\\right )   \\right )   + \\sum_{t=1}^{t}\\frac{\\left ( x_{t}-\\mu_{z_{t}}\\right )   ^{2}}{2\\sigma^{2}}\\label{eq42}\\\\ &   = -\\left (   ( t - k)\\cdot\\log\\left (   p\\right )   + k\\cdot\\log\\left (   1-p\\right ) \\right )   + \\sum_{t=1}^{t}\\frac{\\left (   x_{t}-\\mu_{z_{t}}\\right )   ^{2}}% { 2\\sigma^{2}}\\rightarrow\\label{eq43}\\\\ &   = c(t , k , p)+\\sum_{t=1}^{t}\\frac{\\left (   x_{t}-\\mu_{z_{t}}\\right )   ^{2}% } { 2\\sigma^{2}}.\\label{eq44}%\\end{aligned}\\ ] ] where @xmath186 = @xmath187   $ ] . now we define the function @xmath188 and note that @xmath189   + c(t , k , p)\\right ) .\\label{eq53}%\\ ] ] note that , for simplicity of notation , we write @xmath190 as a function only of @xmath191 ; the quantities @xmath33 , @xmath1 , @xmath50 , @xmath192 * * , * * @xmath175 can be considered fixed .    now consider a run of the segmentation algorithm which produces a sequence @xmath193 , @xmath194 , @xmath195 , @xmath168 , ... @xmath97 .",
    "_ suppose that for every _",
    "@xmath196 _ we have _ @xmath197 . by the reestimation formula for @xmath198 we will have for every @xmath196:@xmath199 furthermore , note that the viterbi algorithm yields the global maximum of the likelihood _ as a function of _ @xmath68 .",
    "hence , from ( [ eq53 ] ) and the reestimation formula for @xmath168 we will have for every @xmath200 : @xmath201 now , using first ( [ eq52 ] ) and then ( [ eq51 ] ) , we obtain@xmath202 and , from ( [ eq55 ] ) and ( [ eq53]),@xmath203    hence , _ if for every _",
    "@xmath200 _ we have _",
    "@xmath197 , then the sequence @xmath204 is increasing ; since it is also bounded from above by one , it must converge .",
    "it follows that the hmm  segmentation algorithm produces a sequence of segmentations with increasing and convergent likelihood ; from convergence of the likelihood we also conclude that the algorithm will eventually terminate . furthermore",
    ", if @xmath205 is the segmentation obtained from @xmath168 is easy to check that @xmath206 from ( [ eq51 ] ) , ( [ eq57 ] ) follows that _",
    "hubert s segmentation cost is decreased in every iteration _ of the hmm  segmentation algorithm .    for the above analysis to hold ,",
    "we have required that @xmath207 for every @xmath200 .",
    "this condition is easy to check ; it is usually satisfied in practice ; and it can be _ enforced _ by choosing the parameter @xmath112 to be not too close to 1 ( if @xmath208 , then the cost of state transitions is very high and transitions are avoided ) .    one way to interpret",
    "the above analysis is the following : using an appropriate value of @xmath112 , the segmentation algorithm presented here becomes an iterative , approximate way to find hubert s optimal segmentation .",
    "the approximation is usually very good , as will be seen in section [ sec04 ] .",
    "this interpretation is completely nonprobabilistic and does not depend on the use of the hidden markov model .",
    "* computational issues*. we must also mention that succesful implementation of the viterbi algorithm requires a normalization of the @xmath130 s to avoid numerical underflow ; alternatively one can work with the logarithms of the the @xmath130 s and perform additions rather than multiplications .",
    "an extensive mathematical , statistical and engineering literature covers both the theoretical and applied aspects of hmm s .",
    "the reader can use @xcite as starting points for a broader overview of the subject .",
    "em - like algorithms for hmm s were introduced in @xcite .",
    "the em  family of algorithms was introduced in great generality in @xcite ; work on hmm s also appears in the econometrics @xcite , as well as in the biological @xcite literature .",
    "these references are merely starting points ; the literature is very extensive .",
    "as already mentioned , the em  segmentation algorithm used here is a variation of algorithms which are well - established  in the field of speech recognition ; for example see @xcite . taking into account the extensive hmm  literature , as well as various ideas reported in the hydrological literature",
    ", the algorithm of section [ sec03024 ] can be extended in several directions .    1 .",
    "the assumption that the observations are normally distributed is not essential .",
    "other forms of probability density can be used in ( [ eq09 ] ) .",
    "similarly , by a simple modification of ( [ eq09 ] ) the algorithm can handle vector valued observations .",
    "2 .   a basic idea of the algorithm",
    "is that each segment must be _",
    "homogeneous_. assuming that the observations within a segment are generated independently and normally , segment homogeneity is evaluated by the deviation of @xmath209 from the segment mean @xmath26 . but",
    "alternative assumptions can be used .",
    "for example , assume that the observations are generated by an autoreggressive mechanism , i.e. that , for @xmath210 and @xmath18 , we have@xmath211 ( where @xmath212 is a white noise term ) .",
    "the segmentation algortithm can be used within this framework . in this case",
    "the reestimation phase computes the ar@xmath213coefficients @xmath214 , @xmath215 , @xmath128 , @xmath216 , which can be estimated from @xmath217 , @xmath218 , @xmath128 , @xmath219 using a least squares fitting algorithm .",
    "this approach is used in section [ sec0403 ] to fit a hmm  autoregressive model to global temperature data .",
    "similarly , it may be assumed that the observations are generated by a polynomial regression of the form ( for @xmath210 and @xmath18)@xmath220 where @xmath212 is a noise term .",
    "again , the coefficients @xmath221 , @xmath222 , @xmath128 , @xmath216 can be computed at every reestimation phase by a least squares fitting algorithm .",
    "additional constraints can be used to enforce continuity across segments . in the case of 1st order polynomials",
    "there are only two coefficients , @xmath221 , @xmath222 , which are determined by the continuity assumptions ; the iterative reestimation of the change points can still be performed .",
    "this case may be of interest for detection of trends .",
    "it has been mentioned in section [ sec03021 ] that @xmath50 can also be reestimated in every iteration of the em  algorithm .",
    "preserving the left - to - right structure implies that for @xmath18 and for all @xmath57 different from @xmath17 and @xmath58 , we have @xmath56 ; furthermore , for @xmath105 we have @xmath106 .",
    "the @xmath223 parameters can be estimated by @xmath224 .",
    "however , some preliminary experiments indicate that this approach does not yield improved segmentations .",
    "5 .   on the other hand ,",
    "the treatment of the state transition can be modified in a more substantial manner by dropping the left - to - right assumption . in the current model each state of the markov chain",
    "corresponds to a single segment and , because of the left - to - right structure , it is visited at most once .",
    "an alternate approach would be to assign some physical significance to the states .",
    "for instance , states could be chosen to correspond to climate regimes such as `` dry '' , `` wet '' etc . in this case a state could be visited more than once .",
    "this approach allows the choice of models which incorporate expert knowledge about the evolution of climate regimes . on the other hand , if the left - to - right structure is dropped , the number of free parameters in the @xmath50 matrix increases .",
    "these parameters could be estimated ( conditional on a particular state sequence )  by @xmath225 the enhancements of arbitrary transition structure and transition probability estimation are easily accommodated by our algorithm .",
    "in this section we evaluate the segmentation algorithm by numerical experiments .",
    "the first experiment involves an annual river discharge time series which contains 86 points .",
    "the second example involves the reconstructed annual mean global temperature time series and contains 282 points . both of these examples involve segmentation by minimization of total deviation from segment means .",
    "the third example again involves the annual mean global temperature time series , but performs segmentation by minimization of autoregressive prediction error .",
    "the fourth example involves artificially generated time series with up to 1500 points .      in this experiment",
    "we use the time series of the senegal river annual discharge data , measured at the bakel station for the years 1903 - 1988 .",
    "the length of the time series is 86 .",
    "the same data set has been used by hubert @xcite .",
    "the goal is to find the segmentation which is optimal with respect to total deviation from the segment means , has the highest possible order and is statistically significant according to scheffe s criterion .",
    "we run the segmentation algorithm for increasing values of @xmath1 . in the experiments reported here",
    "we have always used @xmath226 ( similar results are  obtained for other values of @xmath112 in the interval [ 0.85 , 0.95 ] . for every value of @xmath1 , convergence is achieved by the 3rd or 4th iteration of the algorithm .",
    "the optimal segmentations are presented in table 1 .",
    "the segmentations which were validated by the scheffe criterion appear in bold letters .",
    "[ c]|l|l|l|l|l|l|l|l|@xmath1 & + 1 & * 1902 * & * 1988 * & & & & & + 2 & * 1902 * & * 1967 * & * 1988 * & & & & + 3 & * 1902 * & * 1949 * & * 1967 * & * 1988 * & & & + 4 & * 1902 * & * 1917 * & * 1953 * & * 1967 * & * 1988 * & & + 5 & * 1902 * & * 1921 * & * 1936 * & * 1949 * & * 1967 * & * 1988 * & + 6 & 1902 & 1921 & 1936 & 1949 & 1967 & 1971 & 1988 +    * table 1 *    hence it can be seen that the optimal and statistically significant segmentation is that of order 5 , i.e. the segments are [ 1903,1921 ] , [ 1922,1936 ] , [ 1937,1949 ] , [ 1950,1967 ] , [ 1967,1988 ] . that this is the globally optimal  segmentation , has been shown by hubert in @xcite using his exact segmentation procedure . a plot of the time series , indicating the 5 segments and the respective means appears in figure 2 .",
    "* figure 2 to appear here *    we have verified that the hmm  algorithm finds the globally optimal segmentation for all values of @xmath1 ( as listed in table 1 ) .",
    "we performed this verification by use of the exact dynamic programming algorithm presented in the appendix .",
    "the conclusion is that , in this experiment , the hmm  segmentation algorithm finds the optimal segmentations considerably faster than the exact algorithm .",
    "specifically , running the entire experiment ( i.e. obtaining the hmm  segmentations of _ all _ orders ) with a matlab  implementation of the hmm  segmentation algorithm took 1.1 sec on a pentium iii 1 ghz personal computer ; we expect that a fortran  or c  implementation would take about 10% to 20% of this time .      in this experiment",
    "we use the time series of  annual mean global temperature for the years 1700  1981 . only the temperatures for",
    "the period 1902  1981 come from actual measurements ; the remaining temperatures were _ reconstructed _ according to a procedure described in @xcite and also at the internet address ` http://www.ngdc.noaa.gov/paleo/ei/ei_intro.html ` .",
    "the length of the time series is 282 .",
    "the goal is again to find the segmentation which is optimal with respect to total deviation from the segment - means , has the highest possible order and is statistically significant according to scheffe s criterion .",
    "we run the segmentation algorithm for @xmath227 , using @xmath226 .",
    "convergence takes place in 4 iterations or less .",
    "the optimal segmentations are presented in table 2 .",
    "the segmentations which were validated by scheffe s criterion appear in bold letters .",
    "[ c]|l|l|l|l|l|l|l|l|@xmath1 & + 1 & * 1700 * & * 1981 * & & & & & + 2 & * 1700 * & * 1930 * & * 1981 * & & & & + 3 & * 1700 * & * 1812 * & * 1930 * & * 1981 * & & & + 4 & * 1700 * & * 1720 * & * 1812 * & * 1930 * & * 1981 * & & + 5 & 1700 & 1720 & 1812 & 1926 & 1935 & 1981 & + 6 & 1700 & 1720 & 1812 & 1926 & 1934 & 1977 & 1981 +    * table 2 *    hence it can be seen that the optimal and statistically significant segmentation is of order 4 , i.e. the segments are [ 1700,1720 ] , [ 1721,1812 ] , [ 1813,1930 ] , [ 1931,1981 ] . a plot of the time series , indicating the 4 segments and the respective means appears in figure 3 .",
    "* figure 3 to appear here *    the _ total _ execution time for the experiment ( i.e. to obtain optimal segmentations of all orders ) is 2.97 sec .",
    "the segmentations of table 2 are the globally optimal ones , as we have verified using the dynamic programming segmentation algorithm .      in this experiment",
    "we again use the annual mean global temperature time series , but now we assume that it is generated by a _ switching regression _ hmm .",
    "specifically , we assume a model of the form @xmath228 where the parameters @xmath221 , @xmath214 , @xmath229 , @xmath230 are specific to the @xmath17-th state of the underlying markovian process . given a particular segmentation ,",
    "these parameters can be estimated by a least squares fitting algorithm .",
    "hence the segmentation algorithm can be modified to obtain the optimal segmentation with respect to the model of ( [ eq46 ] ) .",
    "once again we run the segmentation algorithm for @xmath227 , using @xmath226 .",
    "the optimal segmentations thus obtained are presented in table 3 .",
    "[ c]|l|l|l|l|l|l|l|l|@xmath1 & + 1 & * 1700 * & * 1981 * & & & & & + 2 & * 1700 * & * 1926 * & * 1981 * & & & & + 3 & * 1700 * & * 1833 * & * 1926 * & * 1981 * & & & + 4 & * 1700 * & * 1769 * & * 1833 * & * 1926 * & * 1981 * & & + 5 & 1700 & 1769 & 1833 & 1895 & 1926 & 1981 & + 6 & 1700 & 1769 & 1825 & 1877 & 1904 & 1926 & 1981 +    * table 3 *    in this case segment validation is not performed by the scheffe criterion ; instead we use a prediction error correlation criterion .",
    "this indicates the maximum statistically significant number of segments is @xmath1=4 and the segments are [ 1700,1769 ] , [ 1770,1833 ] , [ 1834,1926 ] , [ 1927,1981 ] . a plot of the time series , indicating the 4 segments and the respective autoregressions appears in figure 3 .    * figure 4 to appear here *",
    "recall that the segments obtained by means - based segmentation are [ 1700,1720 ] , [ 1721 , 1812 ] , [ 1813 , 1930 ] , [ 1931 , 1981 ] .",
    "this seems to be in reasonable agreement with the ar - based segmentation , excepting the discrepancy of 1720 and 1769 . from a numerical point of view",
    ", there is no a priori reason to expect that the ar - based segmentation and means - based segmentation should give the same results .",
    "the fact that the two segmentations are in reasonable agreement , supports the hypothesis that actual climate changes have occurred approximately at the transition times indicated by both segmentation methods .    finally , let us note that the _ total _ execution time for the experiment ( i.e. to obtain optimal segmentations of every order ) is 3.07 sec and that the segmentations of table 3 are the globally optimal ones , as we have verified using the dynamic programming segmentation algorithm .",
    "the goal of the final experiment is to investigate the scaling properties of the algorithm , specifically the scaling of execution time with respect to time series length @xmath33 and the scaling of accuracy with respect to noise in the observations . to obtain better control over these factors , artificial time series",
    "are used , which have been generated by the following mechanism .    the time series",
    "are generated by a 5-th order hmm .",
    "every time series is generated by running the hmm from state no.1 until state no.5 .",
    "hence , every time series involves 5 state transitions and , for the purposes of this experiment , this is assumed to be known a priori . on the other hand , it can be seen that the length of the time series is variable . with a slight change of notation , in this section @xmath33",
    "will denote the _ expected _ length of the time series , which can be controlled by choice of the probability @xmath112 .",
    "the values of @xmath112 were chosen to generate time series of average lengths 200 , 250 , 500 , 750 , 1000 , 1250 , 1500 .",
    "the observations are generated by a normal distribution with mean @xmath231 ( @xmath17= 1 , 2 , ... , 5 ) and standard deviation @xmath40 .",
    "in all experiments the values @xmath39=",
    "@xmath232= @xmath233= 1 , @xmath43= @xmath234= @xmath235 were used .",
    "several values of @xmath40 were used , namely @xmath40= 0.00 , 0.10 , 0.20 , 0.30 , 0.50 , 0.75 , 1.00 , 1.25 , 1.50 , 1.75 , 2.00 .    for each combination of @xmath33 and @xmath236 20 time series were generated and the hmm  segmentation algorithm was run on each one . for each run",
    "two quantities were computed : @xmath237 , accuracy of segmentation , and @xmath238 , execution time .",
    "segmentation accuracy is computed by the formula@xmath239 where the indicator function @xmath240 is equal to 1 when @xmath241 and equal to 0 otherwise .    from these data",
    "two tables are compiled .",
    "table 4 lists @xmath238 ( in seconds )  as a function of @xmath33 ( i.e. @xmath238 is averaged over all time series of the same @xmath33 ) .",
    "table 5 lists average segmentation accuracy @xmath237 as a function of @xmath33 and @xmath40 ( i.e. @xmath237 is averaged over the 20 time series with the same @xmath33 and @xmath40 ) .",
    "as expected , segmentation accuracy is generally a decreasing function of @xmath40 .",
    "[ c]|l|l|l|l|l|l|l|l|@xmath33 & 200 & 250 & 500 & 750 & 1000 & 1250 & 1500 + @xmath238 & 0.193 & 0.249 & 0.585 & 1.024 & 1.845 & 3.026 & 4.60 +    * table 4 . * average execution time @xmath238 ( in seconds )  as a function of average time series length @xmath33 .    [ c]|l|lllllll|@xmath33 & 200 & & & & & & + @xmath40 & + 0.00 & 1.0000 & 1.0000 & 1.0000 & 0.9692 & 1.0000 & 1.0000 & 0.9902 + 0.10 & 1.0000 & 1.0000 & 1.0000 & 0.9814 & 1.0000 & 1.0000 & 1.0000 + 0.20 & 1.0000 & 0.9806 & 1.0000 & 1.0000 & 1.0000 & 0.9716 & 1.0000 + 0.30 & 1.0000 & 1.0000 & 0.9999 & 0.9792 & 1.0000 & 0.9807 & 1.0000 + 0.50 & 0.9989 & 0.9993 & 0.9994 & 0.9997 & 1.0000 & 0.9997 & 1.0000 + 0.75 & 0.9945 & 0.9979 & 0.9663 & 0.9521 & 0.9988 & 0.9992 & 0.9991 + 1.00 & 0.9881 & 0.9880 & 0.9863 & 0.9974 & 0.9517 & 0.9981 & 0.9711 + 1.25 & 0.9778 & 0.9710 & 0.9762 & 0.9924 & 0.9965 & 0.9843 & 0.9781 + 1.50 & 0.9561 & 0.9701 & 0.9874 & 0.9341 & 0.9507 & 0.9362 & 0.9956 + 1.75 & 0.9337 & 0.8985 & 0.9494 & 0.9341 & 0.9708 & 0.9272 & 0.9942 + 2.00 & 0.8628 & 0.8617 & 0.8255 & 0.9141 & 0.8600 & 0.9523 & 0.8297 +    * table 5 . * average classif .",
    "accuracy @xmath237 as a function of average time series length @xmath33 and noise level @xmath40 .",
    "in this paper we have used hidden markov models to represent hydrological and enviromental time series with multiple change points . inspired by hubert s pioneering work and by methods of speech recognition , we have presented a fast iterative segmentation algorithm which belongs to the em  family .",
    "the quality of a particular segmentation is evaluated by the deviation from segment means , but extensions involving autoregressive hmm s , trend - generating hmm s etc . can also be used . because execution time is o(@xmath140 ) , our algorithm can be used to explore various possible segmentations in an interactive manner .",
    "we have presented a convergence analysis which shows that under appropriate conditions every iteration of our algorithm increases the likelihood of the resulting segmentation .",
    "furthermore , numerical experiments ( involving river flow and global temperature time series ) indicate that the algorithm can be expected to converge to the _ globally _ optimal segmentation .",
    "in this appendix we present an alternative time series segmentation algorithm which , unlike the hmm  algorithm , is _ guaranteed _ to produce the _ globally optimal _ segmentation of a time series .",
    "this superior performance , however , is obtained at the price of longer execution time .",
    "still , the algorithm is computationally viable for time series of several hundred terms .",
    "we describe the algorithm briefly here ; a more detailed report appears in @xcite .",
    "a _ generalization _ of the time series segmentation problem discussed in previous sections is the following . given a time series @xmath242 , @xmath4 , ... , @xmath243 and a fixed @xmath1 , find a sequence of times @xmath6 = @xmath7 , @xmath8 , ... ,",
    "@xmath9 which satisfies @xmath244 ...",
    "@xmath245 = @xmath33 , and minimizes@xmath246 @xmath247 consists of a sum of terms @xmath248 . for example , hubert s cost function can be obtained by setting@xmath249 hence hubert s segmentation cost ( [ eq99 ] ) is a special case of ( [ eq101 ] ) .",
    "similarly , consider _ autoregressive _ models of the form@xmath250 where @xmath251 , @xmath252 , ... , @xmath74 ) and @xmath253 , @xmath254 , @xmath255 , @xmath128 , @xmath256 $ ] , @xmath257 , @xmath258 , @xmath128 , @xmath259^{\\prime}$ ] ( the @xmath260 denotes transpose of a matrix ) .",
    "then we can set @xmath261 then the segmentation cost becomes@xmath262 the @xmath263 , @xmath258 , @xmath128 , @xmath264 ( elements of @xmath265 ) are unknown , but can be determined by least squares fitting on @xmath217 , @xmath266 , ... , @xmath219 .",
    "a similar formulation can be used for regressive models of the form @xmath267 where @xmath265 = @xmath268 , @xmath263 , @xmath128 , @xmath259^{\\prime}$ ] , @xmath269 = @xmath270 , @xmath271 , @xmath272 , @xmath128 , @xmath273 $ ] .",
    "hence we see that ( [ eq101 ] )  is sufficiently general to subsume  many cost functions of practical interest .            * * input : *  the time series @xmath274 ; a termination number @xmath1 . * * initialization * * for @xmath125 @xmath97 _ _ * * for @xmath275 @xmath97 _ _ * * * @xmath276 * * end * * @xmath277 * end * * minimization * * for @xmath18 * * for @xmath278 * * * for @xmath279 * * * * @xmath280 * * * end * * * @xmath281 * * * @xmath282 * * end * end * * backtracking * * for @xmath18 * * @xmath283 * * for @xmath284 * * * @xmath285 * * end * * @xmath286 * end      on termination , the dynamic programming segmentation algorithm has computed @xmath287 for @xmath18 ; in other words it has recursively solved a _ sequence _ of minimization problems . for @xmath18 ,",
    "the optimal segmentation @xmath288 = ( @xmath289 , @xmath290 , ... , @xmath291 ) has been obtained by backtracking .",
    "the recursive minimization is performed in the second part of the algorithm ; it is seen that computation time is o(@xmath292 ) .",
    "this is not as good as the o(@xmath293 ) obtained by the hmm algorithm ( note that usually @xmath1 is@xmath97significantly less than @xmath33 ) , but is still computationally viable for @xmath294 in the order of a few hundreds .",
    "the backtracking part of the algorithm has execution time o(@xmath295 ) .    however , in many cases the computationally most expensive part of the algorithm is the initialization phase , i.e. the computation of @xmath296 .",
    "this involves o(@xmath297 ) computations of @xmath276 and can increase the computation cost by one or more orders of magnitude .",
    "for example , if we apply the algorithm to detect changes in the mean , then @xmath298 which involves @xmath299 addittions ; if ( [ eq111 ] ) is used in the initialization phase , then this phase requires o(@xmath300 ) computations and this severely limits computational viability to relatively short time series .    hence , to enhance the computational viability of the dynamic programming segmentation algorithm , it is necessary to find efficient ways to perform the initialization phase . in the next two sections",
    ", we will deal with this question for two specific forms of @xmath301 : the first form pertains to the computation of means and the second to the computation of regressions and autoregressions .",
    "the computation of means can be performed recursively , as will now be shown . for @xmath125 , @xmath302",
    ", we must compute@xmath303 for @xmath125 , @xmath275 , define the following additional quantities:@xmath304 then we have@xmath305 and@xmath306 from ( [ eq141 ] ) , ( [ eq142 ] ) follows that ( for @xmath125 , @xmath302)@xmath307 the above computations can be implemented in time o(@xmath297 ) by the following algorithm .        *",
    "for @xmath125 @xmath97 _ _ * * @xmath308 * * @xmath309 * * for @xmath310 @xmath97 _ _ * * * @xmath311 * * * @xmath312 * * end * end * for @xmath125 @xmath97 _ _ * * for @xmath313 * * * @xmath314 * * end * end * for @xmath125 @xmath97 _ _ * * @xmath315 * * for @xmath316 @xmath97 _ _ * * * @xmath317 * * end * end      hence , if the above code replaces the initialization phase of the dynamic programming algorithm in section [ seca02 ] , we obtain an o(@xmath292 ) implementation of the entire algorithm . in other words",
    ", we obtain an algorithm which , given a time series of length @xmath33 , computes the global minimum of hubert s segmentation cost ( for all segmentations of orders @xmath318 ) in time o(@xmath292 )      consider now autoregressive models described by ( [ eq103 ] ) .",
    "as already mentioned , in this case we have@xmath319 hence @xmath320 is given by@xmath321 where @xmath253 , @xmath254 , @xmath255 , ... , @xmath256 $ ] and @xmath322",
    "is obtained by solving the least squares equation @xmath323",
    "with@xmath324{l}% x_{s}\\\\ x_{s+1}\\\\ ... \\\\ x_{t}% \\end{array } \\right ]   \\qquad\\text { and}\\qquad\\text { } u(s , t)=\\left [ \\begin{array } [ c]{l}% u_{s}\\\\ u_{s+1}\\\\ ... \\\\ u_{t}% \\end{array } \\right ]   .\\label{eq134}%\\ ] ] note that to solve ( [ eq133 ] ) the matrix multiplications @xmath325 , @xmath326 must be performed . for @xmath125 , @xmath275 ,",
    "these multiplications require o(@xmath327 ) time .",
    "however , the solution of ( [ eq133 ] )  can be approximated by a fast recursive algorithm reported in @xcite .",
    "choose some small number @xmath328 and set@xmath329 ( where @xmath330 is the @xmath331 unit matrix ) .",
    "then , consider the following recursion for @xmath332 and @xmath333:@xmath334,\\label{eq136a}\\\\ n &   = t - s,\\label{eq136b}\\\\ p_{n } &   = p_{n-1}-p_{n-1}\\cdot u_{t}^{\\prime}\\cdot u_{t}\\cdot p_{n-1}% \\cdot\\frac{1}{1+u_{t}\\cdot p_{n-1}\\cdot u_{t}^{\\prime}},\\label{eq136c}\\\\ \\widehat{a}(s , t ) &   = \\widehat{a}(s , t-1)+p_{n}\\cdot u_{t}^{\\prime}\\cdot\\left ( x_{t}-u_{t}\\cdot\\widehat{a}(s , t-1)\\right )   .\\label{eq136d}%\\end{aligned}\\ ] ] using the arguments of @xcite for a fixed @xmath196 and increasing @xmath34 it can be shown that @xmath335 converges _ very quickly _ to @xmath322 , the true solution of ( [ eq133 ] ) . furthermore",
    ", the computations of ( [ eq136a])-([eq136d ] ) can be implemented in time o(@xmath297 ) .",
    "hence , for the case of autoregressive models , the @xmath296 computation can be programmed as follows .        * for @xmath332 @xmath97 _ _ * * @xmath336=@xmath337 * * initialize @xmath338 randomly * * @xmath339=0 * * for @xmath340 * * * @xmath341 $ ] * * * @xmath342 * * * @xmath343 = @xmath344 * * * @xmath335 = @xmath345 * * * @xmath346 * * end * end      hence , if the above code replaces the initialization phase of the dynamic programming segmentation algorithm in section [ seca02 ] , we have an o(@xmath292 ) implementation of the entire algorithm for autoregressive models .",
    "a similar modification is possible for regressive models of the form ( [ eq103 ] ) .",
    "baum and j.a .",
    "`` an inequality with applications to statistical estimation for probabilistic functions of markov processes and to a model for ecology '' . _ bull .",
    "_ , vol.73 , pp.360363 , 1967 .",
    "p. hubert .",
    "`` change points in meteorological analysis '' . in _ applications of _",
    "_ time series analysis in astronomy and meteorology _ , t.subba rao , m.b .",
    "priestley and o. lessi ( eds . ) .",
    "chapman and hall , london , 1997 .",
    "l. perreault , m. hache , m. slivitzky and b. bobee .",
    "`` detection of changes in precipitation and runoff over eastern canada and u.s . using a bayesian approach '' .",
    "res . and risk ass .",
    "_ , vol . 13 , pp.201 - 216 , 1999 .    l. perreault , e. parent , j. bernier , b. bobee and m. slivitzky .",
    "`` retrospective multivariate bayesian change - point analysis : a simultaneous single change in the mean of several hydrological sequences '' .",
    "res . and risk ass .",
    "14 , pp.243 - 261 , 2000 .",
    "l. perreault , j. bernier , b. bobee and e. parent .",
    "`` bayesian change - point analysis in hydrometeorological time series .",
    "comparison of change - point models and forecasting '' .",
    "_ j. hydrol .",
    "235 , pp.242 - 263 , 2000 ."
  ],
  "abstract_text": [
    "<S> motivated by hubert s segmentation procedure @xcite , we discuss the application of hidden markov models ( hmm ) to the segmentation of hydrological and enviromental time series . </S>",
    "<S> we use a hmm algorithm which segments time series of several hundred terms in a few seconds and is computationally feasible for even longer time series . </S>",
    "<S> the segmentation algorithm computes the maximum likelihood segmentation by use of an expectation / maximization iteration . </S>",
    "<S> we rigorously prove algorithm convergence and use numerical experiments , involving temperature and river discharge time series , to show that the algorithm usually converges to the globally optimal segmentation . </S>",
    "<S> the relation of the proposed algorithm to hubert s segmentation procedure is also discussed . </S>"
  ]
}