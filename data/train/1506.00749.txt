{
  "article_text": [
    "proliferation of smart mobile devices , coupled with new types of wireless applications , has led to an exponential growth of wireless and mobile data traffic . in order to provide high - volume and diversified data services , ultra - dense wireless cooperative network architectures",
    "have been proposed for next generation wireless networks @xcite , e.g. , cloud - ran @xcite , and distributed antenna systems @xcite . to enable efficient interference management and resource allocation ,",
    "large - scale multi - entity collaboration will play pivotal roles in dense wireless networks .",
    "for instance , in cloud - ran , all the baseband signal processing is shifted to a single cloud data center with very powerful computational capability .",
    "thus the centralized signal processing can be performed to support large - scale cooperative transmission / reception among the radio access units ( raus ) .",
    "convex optimization serves as an indispensable tool for resource allocation and signal processing in wireless communication systems @xcite . for instance , coordinated beamforming @xcite often yields a direct convex optimization formulation , i.e. , second - order cone programming ( socp ) @xcite .",
    "the network max - min fairness rate optimization @xcite can be solved through the bi - section method @xcite in polynomial time , wherein a sequence of convex subproblems are solved .",
    "furthermore , convex relaxation provides a principled way of developing polynomial - time algorithms for non - convex or np - hard problems , e.g. , group - sparsity penalty relaxation for the np - hard mixed integer nonlinear programming problems @xcite , semidefinite relaxation @xcite for np - hard robust beamforming @xcite and multicast beamforming @xcite , and sequential convex approximation to the highly intractable stochastic coordinated beamforming @xcite .",
    "nevertheless , in dense wireless cooperative networks @xcite , which may possibly need to simultaneously handle hundreds of raus , resource allocation and signal processing problems will be dramatically scaled up .",
    "the underlying optimization problems will have high dimensions and/or large numbers of constraints ( e.g. , per - rau transmit power constraints and per - mu ( mobile user ) qos constraints ) .",
    "for instance , for a cloud - ran with 100 single - antenna raus and 100 single - antenna mus , the dimension of the aggregative coordinated beamforming vector ( i.e. , the optimization variables ) will be @xmath0 .",
    "most advanced off - the - shelf solvers ( e.g. , sedumi @xcite , sdpt3 @xcite and mosek @xcite ) are based on the interior - point method",
    ". however , the computational burden of such second - order method makes it inapplicable for large - scale problems .",
    "for instance , solving convex quadratic programs has cubic complexity @xcite .",
    "furthermore , to use these solvers , the original problems need to be transformed to the standard forms supported by the solvers .",
    "although the parser / solver modeling frameworks like cvx @xcite and yalmip @xcite can automatically transform the original problem instances into standard forms , it may require substantial time to perform such transformation @xcite , especially for problems with a large number of constraints @xcite .",
    "one may also develop custom algorithms to enable efficient computation by exploiting the structures of specific problems .",
    "for instance , the uplink - downlink duality @xcite is exploited to extract the structures of the optimal beamformers @xcite and enable efficient algorithms .",
    "however , such an approach still has the cubic complexity to perform matrix inversion at each iteration @xcite .",
    "first - order methods , e.g. , the admm algorithm @xcite , have recently attracted attention for their distributed and parallelizable implementation , as well as the capability of scaling to large problem sizes .",
    "however , most existing admm based algorithms can not provide the certificates of infeasibility @xcite .",
    "furthermore , some of them may still fail to scale to large problem sizes , due to the socp subproblems @xcite or semidefinite programming ( sdp ) subproblems @xcite needed to be solved at each iteration .    without efficient and scalable algorithms , previous studies of wireless cooperative networks either only demonstrate performance in small - size networks , typically with less than 10 raus , or resort to sub - optimal algorithms , e.g. , zero - forcing based approaches @xcite .",
    "meanwhile , from the above discussion , we see that the large - scale optimization algorithms to be developed should possess the following two features :    * to scale well to large problem sizes with parallel computing capability ; * to effectively detect problem infeasibility , i.e. , provide certificates of infeasibility .    to address these two challenges in a unified way , in this paper , we shall propose a two - stage approach as shown in fig .",
    "the proposed framework is capable to solve large - scale convex optimization problems in parallel , as well as providing certificates of infeasibility .",
    "specifically , the original problem @xmath1 will be first transformed into a standard cone programming form @xmath2 @xcite based on the smith form reformulation @xcite , via introducing a new variable for each subexpression in the disciplined convex programming form @xcite of the original problem .",
    "this will eventually transform the coupled constraints in the original problem into the constraint only consisting of two convex sets : a subspace and a convex set formed by a cartesian product of a finite number of standard convex cones .",
    "such a structure helps to develop efficient parallelizable algorithms and enable the infeasibility detection capability _ simultaneously _ via solving the homogeneous self - dual embedding @xcite of the primal - dual pair of the standard form by the admm algorithm .",
    "as the mapping between the standard cone program and the original problem only depends on the network size ( i.e. , the numbers of raus , mus and antennas at each rau ) , we can pre - generate and store the structures of the standard forms with different candidate network sizes .",
    "then for each problem instance , that is , given the channel coefficients , qos requirements , and maximum rau transmit powers , we only need to copy the original problem parameters to the standard cone programming data .",
    "thus , the transformation procedure can be very efficient and can avoid repeatedly parsing and re - generating problems @xcite .",
    "this technique is called _ matrix stuffing _",
    "@xcite , which is essential for the proposed framework to scale well to large problem sizes .",
    "it may also help rapid prototyping and testing for practical equipment development .",
    "= [ draw , fill = blue!20 , minimum size=2em ] = [ pin edge = to-,thin , black ]    [ node distance=3.7cm , auto,>=latex ] ( a ) transformation ; ( b ) [ left of = a , node distance=2 cm , coordinate ] a ; ( c ) [ right of = a ] admm solver ; ( end ) [ right of = c , node distance=2.0 cm ] ; ( b ) edge node @xmath1 ( a ) ; ( a ) edge node @xmath2 ( c ) ; ( c ) edge node @xmath3 ( end ) ;      the major contributions of the paper are summarized as follows :    1 .   we formulate main performance optimization problems in dense wireless cooperative networks into a general framework .",
    "it is shown that all of them can essentially be solved through solving one or a sequence of large - scale convex optimization or convex feasibility problems .",
    "2 .   to enable both the infeasibility detection capability and parallel computing capability , we propose to transform the original convex problem to an equivalent standard cone program .",
    "the transformation procedure scales very well to large problem sizes with the matrix stuffing technique .",
    "simulation results will demonstrate the effectiveness of the proposed fast transformation approach over the state - of - art parser / solver modeling frameworks .",
    "the operator splitting method is then adopted to solve the large - scale homogeneous self - dual embedding of the primal - dual pair of the transformed standard cone program in parallel .",
    "this first - order optimization algorithm makes the second stage scalable .",
    "simulation results will show that it can speedup several orders of magnitude over the state - of - art interior - point solvers .",
    "4 .   the proposed framework enables evaluating various cooperation strategies in dense wireless networks , and helps reveal new insights _",
    "numerically_. for instance , simulation results demonstrate a significant performance gain of optimal beamforming over sub - optimal schemes , which shows the importance of developing large - scale optimal beamforming algorithms .",
    "this work will serve the purpose of providing practical and theoretical guidelines on designing algorithms for generic large - scale optimization problems in dense wireless networks .",
    "the remainder of the paper is organized as follows .",
    "section [ probf ] presents the system model and problem formulations . in section [ hsde ] ,",
    "a systematic cone programming form transformation procedure is developed .",
    "the operator splitting method is presented in section [ opsp ] .",
    "the practical implementation issues are discussed in section [ pii ] .",
    "numerical results will be demonstrated in section [ num ] .",
    "finally , conclusions and discussions are presented in section [ concl ] . to keep the main text clean and free of technical details ,",
    "we divert most of the proofs , derivations to the appendix .",
    "in this section , we will first present two representative optimization problems in wireless cooperative networks , i.e. , the network power minimization problem and the network utility maximization problem .",
    "we will then provide a unified formulation for large - scale optimization problems in dense wireless cooperative networks .          consider a dense fully cooperative network with @xmath4 raus and @xmath5 single - antenna mus , where the @xmath6-th rau is equipped with @xmath7 antennas .",
    "the centralized signal processing is performed at a central processor , e.g. , the baseband unit pool in cloud - ran @xcite as shown in fig .",
    "the propagation channel from the @xmath6-th rau to the @xmath8-th mu is denoted as @xmath9 . in this paper , we focus on the downlink transmission for illustrative purpose .",
    "but our proposed approach can also be applied in the uplink transmission , as we only need to exploit the convexity of the resulting performance optimization problems .",
    "the received signal @xmath10 at mu @xmath8 is given by @xmath11 where @xmath12 is the encoded information symbol for mu @xmath8 with @xmath13=1 $ ] , @xmath14 is the transmit beamforming vector from the @xmath6-th rau to the @xmath8-th mu , and @xmath15 is the additive gaussian noise at mu @xmath8 .",
    "we assume that @xmath12 s and @xmath16 s are mutually independent and all the users apply single user detection .",
    "thus the signal - to - interference - plus - noise ratio ( sinr ) of mu @xmath8 is given by @xmath17 where @xmath18^{t}\\in\\mathbb{c}^{n}$ ] with @xmath19 , @xmath20^{t}\\in\\mathbb{c}^{n}$ ] and @xmath21^{t}\\in\\mathbb{c}^{nk}$ ] .",
    "we assume that each rau has its own power constraint , @xmath22 where @xmath23 is the maximum transmit power of the @xmath6-th rau . in this paper , we assume that the full and perfect csi is available at the central processor and all raus only provide unicast / broadcast services .",
    "network power consumption is an important performance metric for the energy efficiency design in wireless cooperative networks .",
    "coordinated beamforming is an efficient way to design energy - efficient systems @xcite , in which , beamforming vectors @xmath24 s are designed to minimize the total transmit power among raus while satisfying the qos requirements for all the mus .",
    "specifically , given the target sinrs @xmath25 for all the mus with @xmath26 , @xmath27 , we will solve the following total transmit power minimization problem : @xmath28 where @xmath29 is the intersection of the sets formed by transmit power constraints and qos constraints , i.e. , @xmath30 where @xmath31 s are feasible sets of @xmath32 that satisfy the per - rau transmit power constraints , i.e. , @xmath33 and @xmath34 s are the feasible sets of @xmath35 that satisfy the per - mu qos constraints , i.e. , @xmath36 as all the sets @xmath37 s and @xmath31 s can be reformulated into second - order cones as shown in @xcite , problem @xmath38 can be reformulated as an socp problem .",
    "however , in dense wireless cooperative networks , the mobile hauling network consumption can not be ignored . in @xcite , a two - stage group sparse beamforming ( gsbf ) framework",
    "is proposed to minimize the network power consumption for cloud - ran , including the power consumption of all optical fronthaul links and the transmit power consumption of all raus .",
    "specially , in the first stage , the group - sparsity structure of the aggregated beamformer @xmath32 is induced by minimizing the weighted mixed @xmath39-norm of @xmath32 , i.e. , @xmath40 where @xmath41^{t}\\in\\mathbb{c}^{n_lk}$ ] is the aggregated beamforming vector at rau @xmath6 , and @xmath42 is the corresponding weight for the beamformer coefficient group @xmath43 . based on the ( approximated ) group sparse beamformer @xmath44 , which is the optimal solution to @xmath45 , in the second stage , an rau selection procedure is performed to switch off some raus so as to minimize the network power consumption . in this procedure",
    ", we need to check if the remaining raus can support the qos requirements for all the mus , i.e. , check the feasibility of problem @xmath46 given the active raus .",
    "please refer to @xcite for more details on the group sparse beamforming algorithm .",
    "network utility maximization is a general approach to optimize network performance .",
    "we consider maximizing an arbitrary network utility function @xmath47 that is strictly increasing in the sinr of each mu @xcite , i.e. , @xmath48 where @xmath49 is the intersection of the sets of the per - rau transmit power constraints ( [ pconstraint ] ) .",
    "it is generally very difficult to solve , though there are tremendous research efforts on this problem @xcite .",
    "in particular , liu _",
    "et al . _ in @xcite proved that @xmath50 is np - hard for many common utility functions , e.g. , weighted sum - rate .",
    "please refer to ( * ? ? ?",
    "* table 2.1 ) for details on classification of the convexity of utility optimization problems .",
    "assume that we have the prior knowledge of sinr values @xmath51 that can be achieved by the optimal solution to problem @xmath52 .",
    "then the optimal solution to problem @xmath53 with target sinrs as @xmath54 is an optimal solution to problem @xmath50 as well @xcite .",
    "the difference between problem @xmath53 and problem @xmath50 is that the sinrs in @xmath53 are pre - defined , while the optimal sinrs in @xmath50 need to be searched . for the max - min fairness maximization problem ,",
    "optimal sinrs can be searched by the bi - section method @xcite , which can be accomplished in polynomial time . for the general increasing utility maximization problem @xmath50 , the corresponding optimal sinrs can be searched as follows @xmath55 where @xmath56 is the achievable performance region @xmath57 problem ( [ mon ] ) is a monotonic optimization problem @xcite and thus can be solved by the polyblock outer approximation algorithm @xcite or the branch - reduce - and - bound algorithm @xcite . the general idea of both algorithms is iteratively improving the lower - bound @xmath58 and upper - bound @xmath59 of the objective function of problem ( [ mon ] ) such that @xmath60 for a given accuracy @xmath61 in finite iterations . in particular , at the @xmath62-iteration",
    ", we need to check the convex feasibility problem of @xmath63})$ ] given the target sinrs @xmath64}=(\\gamma_{1}^{[m]},\\dots , \\gamma_k^{[m]})$ ] . however , the number of iterations scales exponentially with the number of mus @xcite .",
    "please refer to the tutorial ( * ? ? ?",
    "* section 2.3 ) for more details .",
    "furthermore , the network achievable rate region @xcite can also be characterized by the rate profile method @xcite via solving a sequence of such convex feasibility problems @xmath53 .      in dense wireless cooperative networks ,",
    "the central processor can support hundreds of raus for simultaneously transmission / reception @xcite .",
    "therefore , all the above optimization problems are shifted into a new domain with a high problem dimension and a large number of constraints . as presented previously , to solve the performance optimization problems , we essentially need to solve a sequence of the following convex optimization problem with different problem instances ( e.g. , different channel realizations , network sizes and qos targets ) @xmath65 where @xmath66 is convex in @xmath35 as shown in @xmath46 and @xmath67 . solving problem @xmath1",
    "means that the corresponding algorithm should return the optimal solution or the certificate of infeasibility .",
    "for all the problems discussed above , problem @xmath1 can be reformulated as an socp problem , and thus it can be solved in polynomial time via the interior - point method , which is implemented in most advanced off - the - shelf solvers , e.g. , public software packages like sedumi @xcite and sdpt3 @xcite and commercial software packages like mosek @xcite .",
    "however , the computational cost of such second - order methods will be prohibitive for large - scale problems .",
    "on the other hand , most custom algorithms , e.g. , the uplink - downlink approach @xcite and the admm based algorithms @xcite , however , fail to either scale well to large problem sizes or detect the infeasibility effectively .    to overcome the limitations of the scalability of the state - of - art solvers and the capability of infeasibility detection of the custom algorithms , in this paper , we propose to solve the homogeneous self - dual embedding @xcite ( which aims at providing necessary certificates ) of problem @xmath1 via a first - order optimization method @xcite ( i.e. , the operator splitting method ) .",
    "this will be presented in section [ opsp ] .",
    "to arrive at the homogeneous self - dual embedding and enable parallel computing , the original problem will be first transformed into a standard cone programming form as will be presented in section [ hsde ] .",
    "this forms the main idea of the two - stage based large - scale optimization framework as shown in fig .",
    "although the parser / solver modeling language framework , like cvx @xcite and yalmip @xcite , can automatically transform the original problem instance into a standard form , it requires substantial time to accomplish this procedure @xcite .",
    "in particular , for each problem instance , the parser / solver modeling frameworks need to repeatedly parse and canonicalize it . to avoid such modeling overhead of reading problem data and repeatedly parsing and canonicalizing , we propose to use the matrix stuffing technique @xcite to perform fast transformation by exploiting the problem structures .",
    "specifically , we will first generate the mapping from the original problem to the cone program , and then the structure of the standard form will be stored .",
    "this can be accomplished offline .",
    "therefore , for each problem instance , we only need to stuff its parameters to data of the corresponding pre - stored structure of the standard cone program .",
    "similar ideas were presented in the emerging parse / generator modeling frameworks like cvxgen @xcite and qcml @xcite , which aim at embedded applications for some specific problem families . in this paper",
    ", we will demonstrate in section [ num ] that matrix stuffing is essential to scale to large problem sizes for fast transformation at the first stage of the proposed framework .      in this section ,",
    "we describe a systematic way to transform the original problem @xmath1 to the standard cone program . to enable parallel computing ,",
    "a common way is to replicate some variables through either exploiting problem structures @xcite or using the consensus formulation @xcite .",
    "however , directly working on these reformulations is difficult to provide computable mathematical certificates of infeasibility .",
    "therefore , heuristic criteria are often adopted to detect the infeasibility , e.g. , the underlying problem instance is reported to be infeasible when the algorithm exceeds the pre - defined maximum iterations without convergence @xcite . to unify the requirements of parallel and scalable computing and to provide computable mathematical certificates of infeasibility , in this paper",
    ", we propose to transform the original problem @xmath1 to the following equivalent cone program @xmath2 : @xmath68 where @xmath69 and @xmath70 are the optimization variables , @xmath71 with @xmath72 as the standard second - order cone of dimension @xmath73 @xmath74 and @xmath75 is defined as the cone of nonnegative reals , i.e. , @xmath76 . here , each @xmath77 has dimension @xmath78 such that @xmath79 , @xmath80 , @xmath81 , @xmath82 .",
    "the equivalence means that the optimal solution or the certificate of infeasibility of the original problem @xmath1 can be extracted from the solution to the equivalent cone program @xmath2 . to reduce the storage and memory overhead , we store the matrix @xmath83 , vectors @xmath84 and @xmath85 in the sparse form @xcite by only storing the non - zero entries .",
    "the general idea of such transformation is to rewrite the original problem @xmath1 into a smith form by introducing a new variable for each subexpression in disciplined convex programming form @xcite of problem @xmath1 .",
    "the details are presented in the appendix .",
    "working with this transformed standard cone program @xmath2 has the following two advantages :    * the homogeneous self - dual embedding of the primal - dual pair of the standard cone program can be induced , thereby providing certificates of infeasibility .",
    "this will be presented in section [ hsd ] . *",
    "the feasible set @xmath29 ( [ intset ] ) formed by the intersection of a finite number of constraint sets @xmath86 s and @xmath34 s in the original problem @xmath1 can be transformed into two sets in @xmath2 : a subspace ( [ affset ] ) and a convex cone @xmath87 , which is formed by the cartesian product of second - order cones .",
    "this salient feature will be exploited to enable parallel and scalable computing , as will be presented in section [ opsp1 ] .",
    "inspired by the work @xcite on fast optimization code deployment for embedding second - order cone program , we propose to use the matrix stuffing technique @xcite to transform the original problem into the standard cone program quickly .",
    "specifically , for any given network size , we first generate and store the structure that maps the original problem @xmath1 to the standard form @xmath2 .",
    "thus , the pre - stored standard form structure includes the problem dimensions ( i.e. , @xmath62 and @xmath88 ) , the description of @xmath29 ( i.e. , the array of the cone sizes @xmath89 $ ] ) , and the symbolic problem parameters @xmath83 , @xmath84 and @xmath90 .",
    "this procedure can be done offline .    based on the pre - stored structure , for a given problem instance @xmath1",
    ", we only need to copy its parameters ( i.e. , the channel coefficients @xmath91 s , maximum transmit powers @xmath92 s , sinr targets @xmath93 s ) to the corresponding data in the standard form @xmath2 ( i.e. , @xmath83 and @xmath84 ) . details of the exact description of copying data for transformation are presented in the appendix . as the procedure for transformation only needs to copy memory , it thus is suitable for fast transformation and can avoid repeated parsing and generating as in parser / solver modeling frameworks like cvx .",
    "as shown in the appendix , the dimension of the transformed standard cone program @xmath2 becomes @xmath94 , which is much larger than the dimension of the original problem , i.e. , @xmath95 in the equivalent real - field .",
    "but as discussed above , there are unique advantages of working with this standard form , which compensate for the increase in the size , as will be explicitly presented in later sections .",
    "although the standard cone program @xmath2 itself is suitable for parallel computing via the operator splitting method @xcite , directly working on this problem may fail to provide certificates of infeasibility . to address this limitation , based on the recent work by odonoghue _",
    "@xcite , we propose to solve the homogeneous self - dual embedding @xcite of the primal - dual pair of the cone program @xmath2 .",
    "the resultant homogeneous self - dual embedding is further solved via the operator splitting method , a.k.a .",
    "the admm algorithm @xcite .      the basic idea of the homogeneous self - dual embedding is to embed the primal and dual problems of the cone program @xmath2 into a single feasibility problem ( i.e. , finding a feasible point of the intersection of a subspace and a convex set ) such that either the optimal solution or the certificate of infeasibility of the original cone program @xmath2 can be extracted from the solution of the embedded problem .    the dual problem of @xmath2 is given by @xcite @xmath96 where @xmath97 and @xmath98 are the dual variables",
    ", @xmath99 is the dual cone of the convex cone @xmath87 .",
    "note that @xmath100 , i.e. , @xmath87 is self dual .",
    "define the optimal values of the primal program @xmath2 and dual program @xmath101 are @xmath102 and @xmath103 , respectively .",
    "let @xmath104 and @xmath105 indicate primal infeasibility and unboundedness , respectively .",
    "similarly , let @xmath106 and @xmath107 indicate the dual infeasibility and unboundedness , respectively .",
    "we assume strong duality for the convex cone program @xmath2 , i.e. , @xmath108 , including cases when they are infinite .",
    "this is a standard assumption for practically designing solvers for conic programs , e.g. , it is assumed in @xcite .",
    "besides , we do not make any regularity assumption on the feasibility and boundedness assumptions on the primal and dual problems .      given the cone program @xmath2 , a main task is to detect feasibility . in ( * ? ? ? * theorem 1 ) , a sufficient condition for the existence of strict feasible solution was provided for the transmit power minimization problem without power constraints . however , for the general problem @xmath1 with per - mu qos constraints and per - rau transmit power constraints , it is difficult to obtain such a feasibility condition analytically . therefore , most existing works either assume that the underlying problem is feasible @xcite or provide heuristic ways to handle infeasibility @xcite .    nevertheless , the only way to detect infeasibility effectively is to provide a certificate or proof of infeasibility as presented in the following proposition .",
    "[ prop1 ] [ certificates of infeasibility ] the following system @xmath109 is infeasible if and only if the following system is feasible @xmath110 therefore , any dual variable @xmath111 satisfying the system ( [ cid ] ) provides a certificate or proof that the primal program @xmath2 ( equivalently the original problem @xmath1 ) is infeasible .    similarly , any primal variable @xmath112 satisfying the following system @xmath113 is a certificate of the dual program @xmath101 infeasibility .",
    "this result directly follows the theorem of strong alternatives ( * ? ? ?",
    "* section 5.8.2 ) .",
    "if the transformed standard cone program @xmath2 is feasible , then ( @xmath114 ) are optimal if and only if they satisfy the following karush - kuhn - tucker ( kkt ) conditions @xmath115 in particular , the complementary slackness condition ( [ kkt3 ] ) can be rewritten as @xmath116 which explicitly forces the duality gap to be zero",
    ".      we can first detect feasibility by proposition [ prop1 ] , and then solve the kkt system if the problem is feasible and bounded .",
    "however , the disadvantage of such a two - phase method is that two related problems ( i.e. , checking feasibility and solving kkt conditions ) need to be solved sequentially @xcite . to avoid such inefficiency , we propose to solve the following homogeneous self - dual embedding @xcite : @xmath117 to embed all the information on the infeasibility and optimality into a single system by introducing two new nonnegative variables @xmath118 and @xmath119 , which encode different outcomes .",
    "the homogeneous self - dual embedding thus can be rewritten as the following compact form @xmath120 where @xmath121}_{\\bf{y}}=\\underbrace{\\left [ \\begin{array}{ccc } { \\bf{0 } } & { \\bf{a}}^{t } & { \\bf{c } } \\\\ -{\\bf{a } } & { \\bf{0 } } & { \\bf{b } } \\\\ -{\\bf{c}}^{t } & -{\\bf{b}}^{t } & { \\bf{0 } } \\end{array } \\right]}_{\\bf{q } } \\underbrace{\\left [ \\begin{array}{c } { { \\boldsymbol}{\\nu } } \\\\ { \\boldsymbol{\\eta } } \\\\ { { \\tau } } \\end{array } \\right]}_{\\bf{x } } , \\end{aligned}\\ ] ] @xmath122 , @xmath123 , @xmath124 , @xmath125 and @xmath126 .",
    "this system has a trivial solution with all variables as zeros .",
    "the homogeneous self - dual embedding problem @xmath127 is thus a feasibility problem finding a nonzero solution in the intersection of a subspace and a convex cone .",
    "let @xmath128 be a non - zero solution of the homogeneous self - dual embedding .",
    "we then have the following remarkable trichotomy derived in @xcite :    * * case 1 * : @xmath129 , @xmath130 , then @xmath131 are the primal and dual solutions to the cone program @xmath2 . * * case 2 * : @xmath132 , @xmath133 ; this implies @xmath134 , then 1 .   if @xmath135 , then @xmath136 is a certificate of the primal infeasibility as @xmath137 2 .   if @xmath138 , then @xmath139 is a certificate of the dual infeasibility as @xmath140 * * case 3 * : @xmath141 ; no conclusion can be made about the cone problem @xmath2 .",
    "therefore , from the solution to the homogeneous self - dual embedding , we can extract either the optimal solution ( based on ( [ solv ] ) ) or the certificate of infeasibility for the original problem .",
    "furthermore , as the set ( [ hsd4 ] ) is a cartesian product of a finite number of sets , this will enable parallelizable algorithm design . with the distinct advantages of the homogeneous self - dual embedding , in the sequel",
    ", we focus on developing efficient algorithms to solve the large - scale feasibility problem @xmath127 via the operator splitting method .",
    "conventionally , the convex homogeneous self - dual embedding @xmath127 can be solved via the interior - point method , e.g. , @xcite . however , such second - order method has cubic computational complexity for the second - order cone programs @xcite , and thus the computational cost will be prohibitive for large - scale problems . instead , odonoghue _ et al . _",
    "@xcite develop a first - order optimization algorithm based on the operator splitting method , i.e. , the admm algorithm @xcite , to solve the large - scale homogeneous self - dual embedding .",
    "the key observation is that the convex cone constraint in @xmath127 is the cartesian product of standard convex cones ( i.e. , second - order cones , nonnegative reals and free variables ) , which enables parallelizable computing .",
    "furthermore , we will show that the computation of each iteration in the operator splitting method is very cheap and efficient .",
    "specifically , the homogeneous self - dual embedding @xmath127 can be rewritten as @xmath142 where @xmath143 is the indicator function of the set @xmath144 , i.e. , @xmath145 is zero for @xmath146 and it is @xmath147 otherwise . by replicating variables @xmath148 and @xmath149 , problem ( [ hsdf1 ] ) can be transformed into the following consensus form ( * ? ? ?",
    "* section 7.1 ) @xmath150 which is readily to be solved by the operator splitting method .    applying the admm algorithm (",
    "* section 3.1 ) to problem @xmath151 and eliminating the dual variables by exploiting the self - dual property of the problem @xmath127 ( please refer to ( * ? ? ?",
    "* section 3 ) on how to simplify the admm algorithm ) , the final algorithm is shown as follows : @xmath152}=({\\bf{i}}+{\\bf{q}})^{-1}({\\bf{x}}^{[i]}+{\\bf{y}}^{[i]})\\\\ { \\bf{x}}^{[i+1]}=\\pi_{\\mathcal{c}}(\\tilde{\\bf{x}}^{[i+1]}-{\\bf{y}}^{[i]})\\\\ { \\bf{y}}^{[i+1]}={\\bf{y}}^{[i]}-\\tilde{\\bf{x}}^{[i+1]}+{\\bf{x}}^{[i+1 ] } , \\end{array}\\right.\\end{aligned}\\ ] ] where @xmath153 denotes the euclidean projection of @xmath154 onto the set @xmath155 .",
    "this algorithm has the @xmath156 convergence rate @xcite with @xmath8 as the iteration counter ( i.e. , the @xmath61 accuracy can be achieved in @xmath157 iterations ) and will not converge to zero if a nonzero solution exists ( * ? ? ?",
    "* section 3.4 ) .",
    "empirically , this algorithm can converge to modest accuracy within a reasonable amount of time .",
    "as the last step is computationally trivial , in the sequel , we will focus on how to solve the first two steps efficiently .",
    "the first step in the algorithm @xmath158 is a subspace projection .",
    "after simplification ( * ? ? ? * section 4 ) , we essentially need to solve the following linear equation at each iteration , i.e. , @xmath159}_{\\bf{s}}\\underbrace{\\left [ \\begin{array}{c } { { \\boldsymbol}{\\nu } } \\\\ { -\\boldsymbol{\\eta } } \\end{array } \\right]}_{\\bf{x}}=\\underbrace{\\left [ \\begin{array}{c } { { \\boldsymbol}{\\nu}}^{[i ] } \\\\ { \\boldsymbol{\\eta}}^{[i ] }   \\end{array } \\right]}_{\\bf{b}},\\end{aligned}\\ ] ] for the given @xmath160}$ ] and @xmath161}$ ] at iteration @xmath162 , where @xmath163 with @xmath164 is a _ symmetric quasidefinite _ matrix @xcite . to enable quicker inversions and reduce memory overhead via exploiting the sparsity of the matrix @xmath165",
    ", the sparse permuted @xmath166 factorization @xcite method can be adopted .",
    "specifically , such factor - solve method can be carried out by first computing the sparse permuted @xmath166 factorization as follows @xmath167 where @xmath168 is a lower triangular matrix , @xmath169 is a diagonal matrix @xcite and @xmath170 with @xmath171 is a permutation matrix to fill - in of the factorization @xcite , i.e. , the number of nonzero entries in @xmath172 .",
    "such factorization exists for any permutation @xmath173 , as the matrix @xmath165 is symmetric quasidefinite ( * ? ? ?",
    "* theorem 2.1 ) .",
    "computing the factorization costs much less than @xmath174 flops , while the exact value depends on @xmath175 and the sparsity pattern of @xmath176 in a complicated way .",
    "note that such factorization only needs to be computed once in the first iteration and can be cached for re - using in the sequent iterations for subspace projections .",
    "this is called the _ factorization caching _ technique @xcite .",
    "given the cached factorization ( [ factor ] ) , solving subsequent projections @xmath177 ( [ lin ] ) can be carried out by solving the following much easier equations : @xmath178 which cost zero flops , @xmath179 flops by forward substitution with @xmath180 as the number of nonzero entries in @xmath168 , @xmath181 flops , @xmath179 flops by backward substitution , and zero flops , respectively ( * ? ? ?",
    "* appendix c ) .",
    "the second step in the algorithm @xmath182 is to project a point @xmath183 onto the cone @xmath155 .",
    "as @xmath155 is the cartesian product of the finite number of convex cones @xmath184 , we can perform projection onto @xmath155 by projecting onto @xmath184 separately and in parallel .",
    "furthermore , the projection onto each convex cone can be done with closed - forms .",
    "specifically , for nonnegative real @xmath185 , we have that ( * ? ? ?",
    "* section 6.3.1 ) @xmath186 where the nonnegative part operator @xmath187 is taken elementwise . for the second - order cone @xmath188 , we have that ( * ? ? ?",
    "* section 6.3.2 ) @xmath189    in summary , we have presented that each step in the algorithm @xmath158 can be computed efficiently . in particular , from both ( [ pronon ] ) and ( [ proxop ] ) , we see that the cone projection can be carried out very efficiently with closed - forms , leading to parallelizable algorithms .",
    "in previous sections , we have presented the unified two - stage framework for large - scale convex optimization in dense wireless cooperative networks . in this section , we will focus on the implementation issues of the proposed framework .      in the appendix",
    ", we describe a systematic way to transform the original problem to the standard cone programming form .",
    "the resultant structure that maps the original problem to the standard form can be stored and re - used for fast transforming via matrix stuffing .",
    "this can significantly reduce the modeling overhead compared with the parse / solver modeling frameworks like cvx .",
    "however , it requires tedious manual works to find the mapping and may not be easy to verify the correctness of the generated mapping .",
    "@xcite gave such an attempt intending to automatically generate the code for matrix stuffing .",
    "however , the corresponding software package qcml @xcite , so far , is far from complete and may not be suitable for our applications . extending the numerical - based transformation modeling frameworks like cvx to the symbolic - based transformation modeling frameworks like",
    "qcml is not trivial and requires tremendous mathematical and technical efforts . in this paper",
    ", we derive the mapping in the appendix manually and verify the correctness by comparing with cvx through extensive simulations .      theoretically , the presented operator splitting algorithm @xmath182 is compact , parameter - free , with parallelizable computing and linear convergence . practically , there are typically several ways to improve the efficiency of the algorithm .",
    "in particular , there are various tricks that can be employed to improve the convergence rate , e.g , over - relaxation , warm - staring and problem data scaling as described in @xcite . in the dense wireless cooperative networks with multi - entity collaborative architecture , we are interested in two particular ways to speed up the subspace projection of the algorithm @xmath182 , which is the main computational bottleneck .",
    "specifically , one way is to use the parallel algorithms for the factorization ( [ factor ] ) by utilizing the distributed computing and memory resources @xcite .",
    "for instance , in the cloud computing environments in cloud - ran , all the baseband units share the computing , memory and storage resources in a single baseband unit pool @xcite .",
    "another way is to leverage _ symbolic _ factorization ( [ factor ] ) to speed up the numerical factorization for each problem instance , which is a general idea for the code generation system cvxgen @xcite for realtime convex quadratic optimization @xcite and the interior - point method based socp solver @xcite for embedded systems . eventually , the admm solver in fig . [ lsco ] can be symbolic based so as to provide numerical solutions for each problem instance extremely fast and in a realtime way .",
    "however , this requires further investigation .",
    "in this section , we simulate the proposed two - stage based large - scale convex optimization framework for performance optimization in dense wireless cooperative networks . _ the corresponding matlab code that can reproduce all the simulation results",
    "using the proposed large - scale convex optimization algorithm is available online_.    we consider the following channel model for the link between the @xmath8-th mu and the @xmath6-th rau : @xmath190 where @xmath191 is the path - loss in db at distance @xmath192 as shown in ( * ? ? ?",
    "* table i ) , @xmath193 is the shadowing coefficient , @xmath194 is the antenna gain and @xmath195 is the small - scale fading coefficient .",
    "we use the standard cellular network parameters as showed in ( * ? ? ?",
    "* table i ) .",
    "all the simulations are carried out on a personal computer with 3.2 ghz quad - core intel core i5 processor and 8 gb of ram running linux .",
    "the reference implementation of the operator splitting algorithm scs is available online , which is a general software package for solving large - scale convex cone problems based on @xcite and can be called by the modeling frameworks cvx and cvxpy @xcite .",
    "the settings ( e.g. , the stopping criteria ) of scs can be found in @xcite .    the proposed two - stage approach framework , termed  \" , is compared with the following state - of - art frameworks :    * : this category adopts second - order methods .",
    "the modeling framework cvx will first automatically transform the original problem instance ( e.g. , the problem @xmath1 written in the disciplined convex programming form ) into the standard cone programming form and then call an interior - point solver , e.g. , sedumi @xcite , sdpt3 @xcite or mosek @xcite . * : in this first - order method based framework , cvx first transforms the original problem instance into the standard form and then calls the operator splitting solver scs .",
    "we define the  * modeling time * \" as the transformation time for the first stage , the  * solving time * \" as the time spent for the second stage , and the  * total time * \" as the time of the two stages for solving one problem instance . as the large - scale convex optimization algorithm should scale well to both the modeling part and the solving part simultaneously , the time comparison of each individual stage will demonstrate the effectiveness of the proposed two - stage approach .",
    "given the network size , we first generate and store the problem structure of the standard form @xmath2 , i.e. , the structure of @xmath83 , @xmath84 , @xmath90 and the descriptions of @xmath29 .",
    "as this procedure can be done offline for all the candidate network sizes , we thus ignore this step for time comparison .",
    "we repeat the following procedures to solve the large - scale convex optimization problem @xmath1 with different parameters and sizes using the proposed framework  \"",
    ":    1 .   copy the parameters in the problem instance @xmath1 to the data in the pre - stored structure of the standard cone program @xmath2 .",
    "2 .   solve the resultant standard cone programming instance @xmath2 using the solver scs .",
    "3 .   extract the optimal solutions of @xmath1 from the solutions to @xmath2 by the solver scs .",
    "finally , note that all the interior - point solvers are multiple threaded ( i.e. , they can utilize multiple threads to gain extra speedups ) , while the operator splitting algorithm solver scs is single threaded .",
    "nevertheless , we will show that scs performs much faster than the interior - point solvers .",
    "we also emphasize that the operator splitting method aims at scaling well to large problem sizes and thus provides solutions to modest accuracy within reasonable time , while the interior - point method intends to provide highly accurate solutions .",
    "furthermore , the modeling framework cvx aims at rapid prototyping and providing a user - friendly tool for automatically transformations for general problems , while the matrix - stuffing technique targets at scaling to large - scale problems for the specific problem family @xmath1 .",
    "therefore , these frameworks and solvers are not really comparable with different purposes and application capabilities .",
    "we mainly use them to verify the effectiveness and reliability of our proposed framework in terms of the solution time and the solution quality .    [ cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]     [ timecom ]      consider a network with @xmath196 @xmath197-antenna raus and @xmath5 single - antenna mus uniformly and independently distributed in the square region @xmath198 \\times [ -3000 , 3000]$ ] meters with @xmath199 .",
    "we consider the total transmit power minimization problem @xmath46 with the qos requirements for each mu as @xmath200 db , @xmath27 .",
    "table [ timecom ] demonstrates the comparison of the running time and solutions using different convex optimization frameworks .",
    "each point of the simulation results is averaged over @xmath201 randomly generated network realizations ( i.e. , one small scaling fading realization for each large - scale fading realization ) .    for the modeling time comparisons",
    ", this table shows that the value of the proposed matrix stuffing technique ranges between 0.01 and 30 seconds for different network sizes and can speedup about 15x to 60x compared to the parser / solver modeling framework cvx .",
    "in particular , for large - scale problems , the transformation using cvx is time consuming and becomes the bottleneck , as the  * modeling time * \" is comparable and even larger than the  * solving time * \" .",
    "for example , when @xmath202 , the  * modeling time * \" using cvx is about 3 minutes , while the matrix stuffing only requires about 10 seconds .",
    "therefore , the matrix stuffing for fast transformation is essential for solving large - scale convex optimization problems quickly .    for the solving time",
    "( which can be easily calculated by subtracting the  * modeling time * \" from the  * total time * \" ) using different solvers , this table shows that the operator splitting solver can speedup by several orders of magnitude over the interior - point solvers .",
    "for example , for @xmath203 , it can speedup about 20x and 130x over mosek and sdpt3 , respectively , while sedumi is inapplicable for this problem size as the running time exceeds the pre - defined maximum value , i.e. , one hour .",
    "in particular , all the interior - point solvers fail to solve large - scale problems ( i.e. , @xmath204 ) , denoted as  n / a \" , while the operator splitting solver scs can scale well to large problem sizes . for the largest problems with @xmath205 , the operator splitting solver can solve them in about 5 minutes .    for the quality of the solutions",
    ", this table shows that the propose framework can provide a solution to modest accuracy within much less time .",
    "for the two problem sizes , i.e. , @xmath206 and @xmath203 , which can be solved by the interior - point method based frameworks , the optimal values attained by the proposed framework are within @xmath207 of that obtained via the second - order method frameworks .    in summary , the proposed two - stage based large - scale convex optimization framework scales well to large - scale problem modeling and solving simultaneously .",
    "therefore , it provides an effective way to evaluate the system performance via large - scale optimization in dense wireless networks .",
    "however , its implementation and performance in practical systems still need further investigation . in particular , this set of results indicate that the scale of cooperation in dense wireless networks may be fundamentally constrained by the computation complexity / time .",
    "table feasibile_small_sdpt3.dat ; table feasibile_small_scs.dat ; table feasibile_small_matscs.dat ;    table feasibile_large_scs.dat ; table feasibile_large_matscs.dat ;    ( axis cs:6,0.46 ) ",
    "( axis cs:6.73,0.4 ) node[above left=2 mm ] ; ( axis cs:8.4,0.41 )  ( axis cs:9.13,0.35 ) node[above=4.4 mm , left=0.95 mm ] ;",
    "a unique property of the proposed framework is its infeasibility detection capability , which will be verified in this part .",
    "consider a network with @xmath203 single - antenna raus and @xmath208 single - antenna mus uniformly and independently distributed in the square region @xmath209 \\times [ -2000 , 2000]$ ] meters .",
    "the empirical probabilities of feasibility in fig .",
    "[ feasible ] show that the proposed framework can detect the infeasibility accurately compared with the second - order method framework  \" and the first - order method framework  \" .",
    "each point of the simulation results is averaged over @xmath210 randomly generated network realizations .",
    "the average (  * total time * \" ,  * solving time * \" ) for obtaining a single point with  \" ,  \" and  \" are ( @xmath211 , @xmath212 ) seconds , ( @xmath213 , @xmath214 ) seconds and ( @xmath215 , @xmath216 ) seconds , respectively .",
    "this shows that the operator splitting solver can speedup about 50x over the interior - point solver .",
    "we further consider a larger - sized network with @xmath217 single - antenna raus and @xmath218 single - antenna mus uniformly and independently distributed in the square region @xmath209 \\times [ -2000 , 2000]$ ] meters . as",
    "the second - order method framework fails to scale to this size , we only compare with the first - order method framework .",
    "[ feasible ] demonstrates that the proposed framework has the same infeasibility detection capability as the first - order method framework .",
    "this verifies the correctness and the reliability of the proposed fast transformation via matrix stuffing .",
    "each point of the simulation results is averaged over 200 randomly generated network realizations .",
    "the average (  * solving time * \" ,  * modeling time * \" ) for obtaining a single point with  \" and  \" are ( @xmath219 ) seconds and ( @xmath220 ) seconds , respectively .",
    "this shows that the matrix stuffing technique can speedup about 40x over the numerical based parser / solver modeling framework cvx .",
    "we also note that the solving time of the proposed framework is smaller than the framework  \"",
    ", the speedup is due to the warm - staring ( * ? ? ?",
    "* section 4.2 ) .",
    "table totalpower_small_sdpt3.dat ; table totalpower_small_scs.dat ; table totalpower_small_matscs.dat ;    table totalpower_large_scs.dat ; table totalpower_large_matscs.dat ;    ( axis cs:2.3,0.042 ) ",
    "( axis cs:1.8,0.0458 ) node[below = 5.5mm , right=-4 mm ] ;    ( axis cs:3.3,0.0151 )  ( axis cs:2.8,0.0169 )",
    "node[below = 7mm , right=-4 mm ] ;    in this part , we simulate the network power minimization problem using the group sparse beamforming algorithm ( * ? ? ?",
    "* algorithm 2 ) .",
    "we set each fronthaul link power consumption as @xmath221 and set the power amplifier efficiency coefficient for each rau as @xmath222 . in this algorithm ,",
    "a sequence of convex feasibility problems need to be solved to determine the active raus and one convex optimization problem needs to be solved to determine the transmit beamformers .",
    "this relies on the infeasibility detection capability of the proposed framework .",
    "consider a network with @xmath223 2-antenna raus and @xmath224 single - antenna mus uniformly and independently distributed in the square region @xmath225 \\times [ -1000 , 1000]$ ] meters .",
    "each point of the simulation results is averaged over 50 randomly generated network realizations .",
    "[ netp ] demonstrates the accuracy of the solutions in the network power consumption obtained by the proposed framework compared with the second - order method framework  \" and the first - order method framework  \" .",
    "the average (  * total time * \" ,  * solving time * \" ) for obtaining a single point with  \" ,  \" and  \" are ( @xmath226 , @xmath227 ) seconds , ( @xmath228 , @xmath229 ) seconds and ( @xmath230 , @xmath231 ) seconds , respectively . this shows that the operator splitting solver can speedup about 20x over the interior - point solver .",
    "we further consider a larger - sized network with @xmath203 2-antenna raus and @xmath208 single - antenna mus uniformly and independently distributed in the square region @xmath198 \\times [ -3000 , 3000]$ ] meters . as",
    "the second - order method framework is not applicable to this problem size , we only compare with the first - order method framework .",
    "each point of the simulation results is averaged over 50 randomly generated network realizations .",
    "[ netp ] shows that the proposed framework can achieve the same solutions in network power consumption as the first - order method framework  \" .",
    "the average (  * solving time * \" ,  * modeling time * \" ) for obtaining a single point with  \" and  \" are ( @xmath232 ) seconds and ( @xmath233 ) seconds , respectively .",
    "this shows that the matrix stuffing technique can speedup about 30x over the numerical based parser / solver modeling framework cvx .",
    "in summary , fig .",
    "[ netp ] demonstrates the capability of infeasibility detection ( as a sequence of convex feasibility problems need to be solved in the rau selection procedure ) , the accuracy of the solutions , and speedups provided by the proposed framework over the existing frameworks .",
    "table maxmin_opt_scs.dat ; table maxmin_opt_matscs.dat ; table maxmin_rzf_sdpt3.dat ; table maxmin_zfbf_sdpt3.dat ; table maxmin_mrt_sdpt3.dat ;    ( axis cs:-1.5,2.1 ) ",
    "( axis cs:1,1.3 ) node[above=7 mm , left=-6.3 mm ] ;    ( axis cs:2.5313,3 ) ",
    "( axis cs:8.5,1.09 ) node[above=15.5 mm , left=6.5 mm ] ;    ( axis cs:7,3.9 ) ",
    "( axis cs:16,1.02 ) node[above=22.5 mm , left=11 mm ] ;    ( axis cs:10.6875,4.8 )  ( axis cs:23.5,0.7 ) node[above=31.5 mm , left=18 mm ] ;    we will simulate the minimum network - wide achievable rate maximization problem using the max - min fairness optimization algorithm in ( * ? ? ?",
    "* algorithm 1 ) via the bi - section method , which requires to solve a sequence of convex feasibility problems .",
    "we will not only show the quality of the solutions and speedups provided by the proposed framework , but also demonstrate that the optimal coordinated beamformers significantly outperform the low - complexity and heuristic transmission strategies , i.e. , zero - forcing beamforming ( zfbf ) @xcite , regularized zero - forcing beamforming ( rzf ) @xcite and maximum ratio transmission ( mrt ) @xcite .    consider a network with @xmath234 single - antenna raus and @xmath235 single - antenna mus uniformly and independently distributed in the square region",
    "@xmath236 \\times [ -5000 , 5000]$ ] meters . fig .",
    "[ maxmin_snr ] demonstrates the minimum network - wide achievable rate versus different snrs ( which is defined as the transmit power at all the raus over the receive noise power at all the mus ) using different algorithms .",
    "each point of the simulation results is averaged over 50 randomly generated network realizations . for the optimal beamforming",
    ", this figure shows the accuracy of the solutions obtained by the proposed framework compared with the first - order method framework  \" .",
    "the average (  * solving time * \" ,  * modeling time * \" ) for obtaining a single point for the optimal beamforming with  \" and  \" are ( @xmath237 , @xmath238 ) seconds and ( @xmath239 , @xmath240 ) seconds , respectively .",
    "this shows that the proposed framework can reduce both the solving time and modelling time via warm - starting and matrix stuffing , respectively .",
    "furthermore , this figure also shows that the optimal beamforming can achieve quite an improvement for the per - user rate compared to suboptimal transmission strategies rzf , zfbf and mrt , which clearly shows the importance of developing optimal beamforming algorithms for such networks .",
    "the average (  * solving time * \" ,  * modeling time * \" ) for a single point using  \" for the rzf , zfbf and mrt are ( @xmath241 , @xmath242 ) seconds , ( @xmath243 , @xmath244 ) seconds and ( @xmath245 , @xmath246 ) seconds , respectively .",
    "note that the solving time is very small , which is because we only need to solve a sequence of linear programming problems for power control when the directions of the beamformers are fixed during the bi - section search procedure .",
    "the main time consuming part is from transformation using cvx .",
    "in this paper , we proposed a unified two - stage framework for large - scale optimization in dense wireless cooperative networks .",
    "we showed that various performance optimization problems can be essentially solved by solving one or a sequence of convex optimization or feasibility problems .",
    "the proposed framework only requires the convexity of the underlying problems ( or subproblems ) without any other structural assumptions , e.g. , smooth or separable functions .",
    "this is achieved by first transforming the original convex problem to the standard form via matrix stuffing and then using the admm algorithm to solve the homogeneous self - dual embedding of the primal - dual pair of the transformed standard cone program .",
    "simulation results demonstrated the infeasibility detection capability , the modeling flexibility and computing scalability , and the reliability of the proposed framework .    in principle , one may apply the proposed framework to any large - scale convex optimization problems and only needs to focus on the standard form reformulation as shown in appendix , as well as to compute the proximal operators for different cone projections in ( [ proxop ] ) .",
    "however , in practice , we need to address the following issues to provide a user - friendly framework and to assist practical implementation :    * although the parse / solver frameworks like cvx can automatically transform an original convex problem into the standard form _ numerically _ based on the graph implementation , extending such an idea to the _ automatic and symbolic _ transformation , thereby enabling matrix stuffing , is desirable but challenging in terms of reliability and correctness verification .",
    "* efficient projection algorithms are highly desirable .",
    "for the subspace projection , as discussed in section [ cfos ] , parallel factorization and symbolic factorization are especially suitable for the cloud computing environments as in cloud - ran @xcite . for the cone projection , although the projection on the second - order cone is very efficient , as shown in ( [ proxop ] ) , projecting on the semidefinite cone ( which is required to solve the semidefinite programming problems ) is computationally expensive , as it requires to perform eigenvalue decomposition @xcite .",
    "the structure of the cone projection should be exploited to make speedups .",
    "* it is interesting to apply the proposed framework to various non - convex optimization problems .",
    "for instance , the well - known majorization - minimization optimization provides a principled way to solve the general non - convex problems , whereas a sequence of convex subproblems need to be solved at each iteration .",
    "enabling scalable computation at each iteration will hopefully lead to scalability of the overall algorithm .",
    "we shall present a systematic way to transform the original problem to the standard convex cone programming form .",
    "we first take the real - field problem @xmath1 with the objective function @xmath247 as an example . at the end of this subsection",
    ", we will show how to extend it to the complex - field .    according to the principle of the disciplined convex programming @xcite",
    ", the original problem @xmath1 can be rewritten as the following disciplined convex programming form @xcite @xmath248 where @xmath249 with @xmath250\\in\\mathbb{r}^{n_{l}\\times n}$ ] , @xmath251 , @xmath252^{t}\\in\\mathbb{r}^{nk}$ ] , @xmath253^t\\in\\mathbb{r}^{k+1}$ ] , and @xmath254^{t}\\in\\mathbb{r}^{(k+1)\\times nk}$ ] with @xmath255 .",
    "it is thus easy to check the convexity of problem @xmath256 , following the disciplined convex programming ruleset @xcite .",
    "to arrive at the standard convex cone program @xmath2 , we rewrite problem @xmath256 as the following smith form @xcite by introducing a new variable for each subexpression in @xmath256 , @xmath257 where @xmath258 is the smith form reformulation for the transmit power constraint for rau @xmath6 ( [ cvx1 ] ) as follows @xmath259 and @xmath260 is the smith form reformulation for the qos constraint for mu @xmath8 ( [ cvx2 ] ) as follows @xmath261    nevertheless , the smith form reformulation ( [ smithr ] ) is not convex due to the non - convex constraint @xmath262 .",
    "we thus relax the non - convex constraint as @xmath263 , yielding the following relaxed smith form @xmath264 where @xmath265 it can be easily proved that the constraint @xmath263 has to be active at the optimal solution ; otherwise , we can always scale down @xmath266 such that the cost function can be further minimized while still satisfying the constraints .",
    "therefore , we conclude that the relaxed smith form ( [ rsm ] ) is equivalent to the original problem @xmath256 .",
    "now , the relaxed smith form reformulation ( [ rsm ] ) is readily to be reformulated as the standard cone programming form @xmath2 . specifically , define the optimization variables @xmath267 $ ] with the same order of equations as in @xmath268 , then @xmath268 can be rewritten as @xmath269+{{\\boldsymbol}{\\mu}}_0={\\bf{m}},\\end{aligned}\\ ] ] where the slack variables belong to the following convex set @xmath270 and @xmath271 and @xmath272 are given as follows @xmath273 , { { \\bf{m}}}=\\left [ \\begin{array}{c } 0\\\\ \\hline { \\bf{0}}_{nk } \\end{array } \\right],\\end{aligned}\\ ] ] respectively .",
    "define the optimization variables @xmath274 $ ] with the same order of equations as in @xmath258 , then @xmath258 can be rewritten as @xmath275+{{\\boldsymbol}{\\mu}}_1^l={\\bf{p}}^l,\\end{aligned}\\ ] ] where the slack variables @xmath276 belongs to the following convex set formed by the cartesian product of two convex sets@xmath277 and @xmath278 and @xmath279 are given as follows @xmath280 , { { \\bf{p}}}^l=\\left [ \\begin{array}{c } \\sqrt{p_l } \\\\",
    "\\hline 0\\\\ { \\bf{0}}_{kn_l } \\end{array } \\right],\\end{aligned}\\ ] ] respectively .",
    "define the optimization variables @xmath281 $ ] with the same order of equations as in @xmath260 , then @xmath260 can be rewritten as @xmath282+{{\\boldsymbol}{\\mu}}_2^k={\\bf{q}}^k,\\end{aligned}\\ ] ] where the slack variables @xmath283 belong to the following convex set formed by the cartesian product of two convex sets @xmath284 and @xmath285 and @xmath286 are given as follows @xmath287 ,   { { \\bf{q}}}^k=\\left [ \\begin{array}{c } 0 \\\\ \\hline 0\\\\ { \\bf{g}}_k \\end{array } \\right],\\end{aligned}\\ ] ] respectively .",
    "therefore , we arrive at the standard form @xmath2 by writing the optimization variables @xmath288 as follows @xmath289,\\end{aligned}\\ ] ] and @xmath290 $ ] . the structure of the standard cone programming @xmath2 is characterized by the following data @xmath291 where @xmath87 is the cartesian product of @xmath292 second - order ones , and @xmath83 and @xmath293 are given as follows : @xmath294,\\ !",
    "{ \\bf{b}}=\\left [ \\begin{array}{c } \\sqrt{p_1}\\\\ \\vdots\\\\ \\sqrt{p_l}\\\\ \\hline 0\\\\ \\vdots\\\\ 0\\\\ \\hline 0\\\\ { \\bf{0}}_{nk}\\\\ \\hline 0\\\\ { \\bf{0}}_{kn_1}\\\\ \\hline \\vdots\\\\ \\hline 0\\\\ { \\bf{0}}_{kn_1}\\\\ \\hline 0\\\\ { \\bf{g}}_1\\\\ \\hline \\vdots\\\\ \\hline 0\\\\ { \\bf{g}}_k \\end{array}\\right],\\end{aligned}\\ ] ] respectively .      given a problem instance @xmath1 , to arrive at the standard cone program form , we only need to copy the parameters of the maximum transmit power @xmath92 s to the data of the standard form , i.e. , @xmath295 s in @xmath293 , copy the parameters of the sinr thresholds @xmath296 to the data of the standard form , i.e. , @xmath297 s in @xmath83 , and copy the parameters of the channel realizations @xmath298 s to the data of the standard form , i.e. , @xmath299 s and @xmath300 s in @xmath301 .",
    "as we only need to perform copying the memory for the transformation , this procedure can be very efficient compared to the state - of - the - art numerical based modeling frameworks like cvx .      for @xmath302",
    ", we have @xmath303}_{\\tilde{\\bf{h}}_k}}^{t}\\underbrace{\\left [ \\begin{array}{c } \\mathfrak{r}({\\bf{v}}_i)\\ \\\\ \\mathfrak{j}({\\bf{v}}_i )   \\end{array } \\right]}_{\\tilde{\\bf{v}}_i},\\end{aligned}\\ ] ] where @xmath304 and @xmath305 .",
    "therefore , the complex - field problem can be changed into the real - field problem by the transformations : @xmath306 .",
    "the authors would like to thank dr .",
    "eric chu for his insightful discussions and constructive comments related to this work .",
    "s.  zhou , m.  zhao , x.  xu , j.  wang , and y.  yao , `` distributed wireless communication system : a new architecture for future public wireless access , '' _ ieee commun . mag .",
    "_ , vol .",
    "41 , no .  3 , pp .",
    "108113 , 2003 .",
    "a.  b. gershman , n.  d. sidiropoulos , s.  shahbazpanahi , m.  bengtsson , and b.  ottersten , `` convex optimization - based beamforming : from receive to transmit and network designs , '' _ ieee signal process . mag _ ,",
    "27 , no .  3 , pp .",
    "6275 , 2010 .              c.  shen , t .- h .",
    "chang , k .- y .",
    "wang , z.  qiu , and c .- y .",
    "chi , `` distributed robust multicell coordinated beamforming with imperfect csi : an admm approach , '' _ ieee trans . signal process .",
    "_ , vol .",
    "60 , pp .  29883003 , jun . 2012 .",
    "j.  cheng , y.  shi , b.  bai , w.  chen , j.  zhang , and k.  letaief , `` group sparse beamforming for multicast green cloud - ran via parallel semidefinite programming , '' _ ieee int .",
    "conf . on commun .",
    "( icc ) , london , uk _ ,",
    "2015 .",
    "e.  d. andersen and k.  d. andersen , `` the mosek interior point optimizer for linear programming : an implementation of the homogeneous algorithm , '' in _ high performance optimization _ , pp .  197232 , springer , 2000 .",
    "e.  bjornson , m.  bengtsson , and b.  ottersten , `` optimal multiuser transmit beamforming : a difficult problem with a simple solution structure [ lecture notes ] , '' _ ieee signal process .",
    "_ , vol .  31 , pp .  142148 , jul",
    ". 2014 .",
    "liao , m.  hong , y .- f .",
    "liu , and z .- q .",
    "luo , `` base station activation and linear transceiver design for optimal resource management in heterogeneous networks , '' _ ieee trans . signal process .",
    "_ , vol .",
    "62 , pp .  39393952 , aug .",
    "s.  boyd , n.  parikh , e.  chu , b.  peleato , and j.  eckstein , `` distributed optimization and statistical learning via the alternating direction method of multipliers , '' _ found .",
    "trends in mach .",
    "_ , vol .  3 , pp .",
    "1122 , jul .",
    "s.  k. joshi , m.  codreanu , and m.  latva - aho , `` distributed resource allocation for miso downlink systems via the alternating direction method of multipliers , '' _ eurasip journal on wireless communications and networking _ , vol .",
    "2014 , pp .  119 , jan .",
    "2014 .",
    "liu , y .- h .",
    "dai , and z .- q .",
    "luo , `` coordinated beamforming for miso interference channel : complexity analysis and efficient algorithms , '' _ ieee trans . signal process .",
    "_ , vol .  59 , pp .  11421157 , mar . 2011 .",
    "b.  odonoghue , e.  chu , n.  parikh , and s.  boyd , `` conic optimization via operator splitting and homogeneous self - dual embedding , '' _ arxiv preprint arxiv:1312.3039 _ , 2013 , [ online ] .",
    "available : http://arxiv.org/abs/1312.3039            j.  poulson , b.  marker , r.  a. van  de geijn , j.  r. hammond , and n.  a. romero , `` elemental : a new framework for distributed memory dense matrix computations , '' _ acm transactions on mathematical software ( toms ) _ , vol .",
    "39 , p.  13",
    ", feb . 2013 .",
    "c.  b. peel , b.  m. hochwald , and a.  l. swindlehurst , `` a vector - perturbation technique for near - capacity multiantenna multiuser communication - part i : channel inversion and regularization , '' _ ieee trans .",
    "_ , vol .",
    "53 , pp .  195202 , jan .",
    "f.  rusek , d.  persson , b.  k. lau , e.  larsson , t.  marzetta , o.  edfors , and f.  tufvesson , `` scaling up mimo : opportunities and challenges with very large arrays , '' _ ieee signal process . mag .",
    "_ , vol .  30 , no .  1 ,",
    "pp .  4060 , 2013 ."
  ],
  "abstract_text": [
    "<S> convex optimization is a powerful tool for resource allocation and signal processing in wireless networks . as the network density is expected to drastically increase in order to accommodate the exponentially growing mobile data traffic , performance optimization problems are entering a new era characterized by a high dimension and/or a large number of constraints , which poses significant design and computational challenges . in this paper </S>",
    "<S> , we present a novel two - stage approach to solve large - scale convex optimization problems for dense wireless cooperative networks , which can effectively detect infeasibility and enjoy modeling flexibility . in the proposed approach </S>",
    "<S> , the original large - scale convex problem is transformed into a standard cone programming form in the first stage via matrix stuffing , which only needs to copy the problem parameters such as channel state information ( csi ) and quality - of - service ( qos ) requirements to the pre - stored structure of the standard form . </S>",
    "<S> the capability of yielding infeasibility certificates and enabling parallel computing is achieved by solving the homogeneous self - dual embedding of the primal - dual pair of the standard form . in the solving stage , the operator splitting method , </S>",
    "<S> namely , the alternating direction method of multipliers ( admm ) , is adopted to solve the large - scale homogeneous self - dual embedding . </S>",
    "<S> compared with second - order methods , admm can solve large - scale problems in parallel with modest accuracy within a reasonable amount of time . </S>",
    "<S> simulation results will demonstrate the speedup , scalability , and reliability of the proposed framework compared with the state - of - the - art modeling frameworks and solvers .    </S>",
    "<S> dense wireless networking , large - scale optimization , matrix stuffing , operator splitting method , admm , homogeneous self - dual embedding . </S>"
  ]
}