{
  "article_text": [
    "in problems of high - dimensional nonparametric regression , it is known that the number of samples required to attain a reliable generalization error rapidly increases with the increase in dimensionality .",
    "this is also referred to as the curse of dimensionality and the following is an intuitive example for this phenomenon : for a hypercube with a fixed side length , as the dimension of the space increases the ratio of the volume of an inscribed hypersphere to the volume of the hypercube rapidly decreases to an infinitesimal value thereby indicating that in a uniform sample , the number of points that lie within a hypersphere around a fixed center and fixed radius happens to decrease with an increase in dimension thus requiring a greater sample complexity to generalize the phenomenon of the underlying distribution .",
    "recently there has been work around the assumption that the high - dimensional features may lie on a smooth manifold in a lower - dimension .",
    "a characterization of this assumption is presented in detail in  @xcite .",
    "there has been a significant research in developing methods like  @xcite ,  @xcite and  @xcite that try to recover the low - dimensional manifold from high - dimensional representations of data . in this paper",
    "we propose algorithms that instead focus on reducing the dimensionality of the covariates ( features / predictor variables ) in a regression setting while maximizing a measure of statistical dependency between the covariates and the response variable .",
    "the statistical dependency maximization approach presented in this paper is model - free and the dimensionality reduction does not require any prior assumption on the type of regression model that needs to be used in conjunction with the low - dimensional covariates produced by the method .",
    "this is in contrast to model - dependent supervised dimensionality reduction methods such as  @xcite where a supervised dimensionality reduction algorithm is presented based on an assumption that a generalized linear model would be applied on the appropriate features obtained after the dimensionality reduction .",
    "similarly , the model - based approaches in  @xcite and  @xcite aim to learn low - dimensional features that can be used to learn gaussian mixture and bayesian mixture based predictive models respectively . we shall first introduce basic notations used in the paper .",
    "let the high - dimensional covariates in a regression setting be represented by a matrix @xmath0 where the columns are the covariates and let the response be represented by the matrix @xmath1 .",
    "we use @xmath2 to represent the degree matrix of a weighted graph @xmath3 having an adjacenncy matrix @xmath4 where @xmath5 and the degree matrix is a diagonal matrix .",
    "similarly , the laplacian matrix of a graph @xmath3 denoted by @xmath6 is given by @xmath7 .",
    "given a matrix of features @xmath0 and a corresponding response @xmath1 we define the corresponding squared euclidean distance matrices @xmath8 and @xmath9 such that for @xmath10 , @xmath11 and @xmath12 .",
    "the framework presented in this paper might be generalized for prediction with multiple response variables - but we focus our evaluations in this paper in the case of a univariate response . a line of research which has a greater similarity to our proposed approach is the setting in sufficient dimensionality reduction methods like inverse regression estimation ( ire )  @xcite , principal hessian directions ( phd )  @xcite , sliced inverse regression ( sir )  @xcite and sliced average variance estimation(save )  @xcite- as these methods do not make assumptions about the regression model that can be applied over the low - dimensional features .",
    "these techniques are based on the principle of sufficiency as described in  @xcite and however assume that the distribution of the covariates is elliptic .",
    "we now give a brief overview about the structure of this paper . in section 2",
    "we cite the definitions for the population distance correlation and the corresponding sample statistics as proposed by  @xcite ,  @xcite . in section 3",
    "we propose a graph - theoretic formulation of sample distance correlation and sample distance covariance and show that they can be formulated using laplacian matrices . in section 4 we propose a loss function for supervised dimensionality reduction based on the laplacian formulation of sample distance correlation . in section 5",
    "we investigate the convexity and differentiability of this loss - function . in section 6 we present an update to optimize the proposed loss based on the convexity properties presented in section 5 .",
    "in section 7 we propose an algorithm for optimizing the loss without requiring a matrix inversion .",
    "in section 8 we investigate the convergence properties of the loss function based on spectral radius and the differentiability properties presented in section 5 . in section 9 we present experimental results on 5 regression datasets and compare our technique with other supervised dimensionality reduction techniques like ire , sir , phd , save and kdr .",
    "we evaluate the techniques by running regression techniques like support vector machines ( svm ) , random forests ( rf ) , lasso , node harvest ( nh ) , bayesian additive regression trees ( bart ) over the low - dimensional features learnt by the above mentioned dimensionality reduction techniques , and compare the cross - validated predictive performances of the regression methods across low - dimensional features produced by different dimensionality reduction techniques .",
    "we also present empirical results showing convergence along with some simple empirical results evaluating the proposed conditions required to achieve convergence .",
    "in section 10 we present the conclusion and some discussions on future work .",
    "pearson s product - moment correlation is a measure of monotone or linear dependencies between two random variables of the same dimension .",
    "distance correlation introduced by  @xcite ,  @xcite is a measure of monotone as well as nonlinear dependencies between random vectors of arbitrary dimensions . for random variables @xmath13 and @xmath14 , the population distance covariance for a suitable weight function @xmath15 proposed in @xcite , is given by @xmath16 where @xmath17 are the characteristic functions of @xmath18 and @xmath19 is the joint characteristic function .",
    "it is clear from the above definition that the distance covariance can be zero , only when @xmath20,@xmath21 are independent .",
    "the weight function @xmath15 has special properties that allow for @xmath22 to be defined as @xmath23 and hence a standardized version of the distance covariance was shown to be obtainable as @xmath24 and this is the distance correlation of @xmath18 .      the authors in  @xcite ,  @xcite propose a non - negative sample distance covariance , defined over a random sample @xmath25 of @xmath26 i.i.d random vectors @xmath27 from the joint distribution of random vectors @xmath20 in @xmath28 and @xmath21 in @xmath29 .",
    "for this they compute the euclidean distance matrices @xmath30 and @xmath31 where for @xmath10 the distance matrices are formed as @xmath32 and @xmath33 and the distance matrices @xmath34 are double - centered to make their row and column means to be zero .",
    "the double - centered euclidean distance matrices denoted by @xmath35 are obtained using the double - centering matrix , @xmath36 with @xmath37 being the identity matrix and @xmath38 , a vector of one s as @xmath39 and @xmath40 .",
    "this is equivalent to performing the following operation on the entries of @xmath41 : if we denote the row @xmath42 of matrix @xmath30 by @xmath43 and the column @xmath44 by @xmath45 and @xmath46 denotes the average of the elements in @xmath43 and similarly the average of all the elements in @xmath30 by @xmath47 then the entries in @xmath48 and @xmath49 can be represented as : @xmath50 and @xmath51 .",
    "given these representations , the sample distance covariance is defined as @xmath52 and the sample distance correlation @xmath53 is given by @xmath54 .",
    "in this section we propose a laplacian matrix based formulation of the sample distance covariance and sample distance correlation and in the next section we propose a loss - function for supervised dimensionality reduction based on the laplacian formulation we propose here .    we now give the main result of this section :    given matrices of squared euclidean distances @xmath8 and @xmath9 the square of the sample distance correlation , @xmath55 can be expressed using the graph laplacians @xmath56 and @xmath57 formed over adjacency matrices @xmath58 , @xmath59 as : @xmath60 , @xmath61 and any scalar @xmath62 where @xmath63 with @xmath64 as @xmath65    let @xmath66 and @xmath67 be the matrices obtained by double - centering @xmath8 and @xmath9 .",
    "@xmath68 are positive semi - definite and are related to @xmath69 as @xmath70 , @xmath71 . also , as @xmath72 and @xmath73 , the mean of each column in @xmath0 and @xmath1 is zero .",
    "@xmath74 and @xmath75 can be viewed as laplacian matrices constructed using the weighted adjacency matrices @xmath68 .",
    "now for any graph with a weighted adjacency matrix @xmath76 and a corresponding laplacian matrix @xmath77 along with a real matrix @xmath0 the term @xmath78 can be represented using euclidean distances between the rows in @xmath0 as : @xmath79 thus we can represent the term @xmath80 in terms of @xmath81 as @xmath82 since @xmath83 and @xmath81 are double centered , @xmath84 and therefore , @xmath85 with a similar argument we can express the sample distance covariance using both @xmath86 as @xmath87 and the sample distance variances can be expressed as @xmath88 and @xmath89 and so we can now represent the sample distance correlation in terms of @xmath90 as in[proofeqn ] @xmath91",
    "in this section we propose a loss - function which we minimize over a low - dimensional @xmath92 inorder to maximize the distance correlation @xmath93 and evaluate this setting in the later sections .",
    "the proposed loss function @xmath94 that we would like to minimize over @xmath92 with @xmath95 being a fixed scalar when given a feature matrix @xmath0 and a corresponding response variable @xmath1 is @xmath96 this formulation contains a difference of the trace terms observed in [ proofeqn ] . note that the @xmath74 and @xmath75 we use in this loss - function is computed over a high - dimensional @xmath0 and the corresponding response @xmath1 .",
    "this can also be expressed as @xmath97 - \\sum_{i , j}[\\langle y_{i.},y_{j . }",
    "\\rangle d_{ij}^2(\\hat{x})]\\ ] ] @xmath91 similar formulations for solving a maximization of a ratio of trace functions , but under orthogonality constraints were studied in the fisher linear discriminant analysis problem  @xcite where the ratio maximization is formulated as a minimization of a difference , just as in @xmath94 .",
    "@xcite proved that the maximum of the ratio of trace functions under an orthogonality constraint can be achieved by optimizing a difference based formulation as in [ diff ] .",
    "their iterative solution for the difference formulation under the orthogonality constraints requires an eigen decomposition at every iteration . in our proposed solution for the optimization of @xmath94",
    "we do not fix any orthogonality constraints and that is the key difference between the two settings .",
    "we use @xmath74 instead of @xmath98 because we are trying to find a euclidean embedding that preserves the neighborhood relations within the inner - products of the rows ( points ) in @xmath0 and @xmath1 while maximizing the distance correlation .",
    "we empirically show in the convergence plots in the later sections that minimizing the above loss which conts @xmath74 , maximizes the distance correlation between the optimal @xmath92 , @xmath1 . under @xmath98 , the first term @xmath99 in the loss function",
    "is non - convex but in the case where we use @xmath74 , this term becomes convex .",
    "this leads to the loss being a sum of convex and concave functions which we utilize inorder to minimize it using the concave convex procedure ( cccp ) @xcite .",
    "we will go into more details on optimizing this loss in later sections of this paper where we provide an iterative algorithm .",
    "we also choose the fixed number of iterations for which we run the optimization algorithm by cross - validation in a prediction setting .",
    "in an iterative optimization framework we represent the above loss function as a sum of a convex function @xmath100 and a concave function @xmath101 at any iteration @xmath102 as @xmath103\\ ] ] where the individual functions are @xmath104 and @xmath105 based on the concave - convex procedure  @xcite such a loss function can be iteratively minimized with gauaranteed monotone convergence to the minimum or a saddle point by the following update : @xmath106 which gives the update using the moore - penrose inverse as @xmath107",
    "in this section we formulate a solution for the proposed supervised learning loss , in such a way that the iterative update does not require a matrix inversion .",
    "+ we denote by @xmath108 , a diagonal matrix whose diagonal is the diagonal of @xmath74 .",
    "now , we can build a majorization function  @xcite over @xmath109 , based on the fact that @xmath110-{l_x}]$ ] is diagonally dominant .",
    "this leads to the following inequality for any matrix @xmath111 with real entries and of the same dimension as @xmath92 : @xmath112-l_x](\\hat{x}-m)\\succeq 0\\ ] ] we now get the following majorization inequality over @xmath113 , by separating it from the above inequality : @xmath114\\\\-2tr [ \\hat{x}^{t}(2diag({l_x})-{l_x})m]=\\lambda(\\hat{x},m)\\ ] ] which is quadratic in @xmath92 where , @xmath115 .",
    "let , @xmath116 .",
    "this leads to the following bound over our loss function with @xmath117 being a function that only depends on @xmath111 : @xmath118 that satisfies the supporting point requirement , and hence h ( . )",
    "touches the objective function at the current iterate and forms a majorization function .",
    "now the following majorization - minimization iteration holds true for an iteration @xmath102 : @xmath119 it is important to note that these inequalities occur amongst the presence of additive terms , @xmath117 that are independent of x unlike a typical majorization - minimization framework and hence , it is a relaxation .",
    "the majorization function @xmath120 can be expressed as a sum of a convex function @xmath121 and a concave function @xmath122 . by the concave - convex formulation",
    ", we get the iterative solution by solving for @xmath123 which gives us : @xmath124 and on applying the majorization update over @xmath125 , we get @xmath126x_{\\varphi-1}+x_{\\varphi-1}$ ] the iterative update can be represented using the gradient of @xmath94 as @xmath127 we choose @xmath95 at every iteration @xmath102 as @xmath128 as suggested in @xcite .",
    "notation : in this section we use @xmath129 to denote the spectral - radius as it has been a standard notation in literature .",
    "so , is the case with using @xmath129 for denoting distance correlation in the previous sections .",
    "we would like the reader to interpret notation in this case , based on context",
    ".    strong attraction",
    ". a fixed point @xmath130 of @xmath131 is said to be a point of strong attraction of the iteration if @xmath131 is differentiable at @xmath130 and the spectral radius @xmath132    we now study the conditions under which the twice fretchet differentiable iterative update in [ update ] converges with @xmath133    for any pair of real matrices @xmath69 along with a real scalar @xmath134 $ ] and laplacians @xmath135 constructed over @xmath136 respectively the spectral radius @xmath137 for any stationary point @xmath130 .",
    "the gradient of the iterative update of the majorization based approach can be represented using the hessian of @xmath94 as @xmath138\\end{aligned}\\ ] ] by the extension to the ostrowski s theorem in @xcite @xmath139 when @xmath140 on representing @xmath74 and @xmath75 in terms of @xmath0 and @xmath1 using equations [ lxeqn],[lyeqn ] we have the following two positive semi - definiteness conditions that need to be satisfied : @xmath141 and @xmath142 which we represent using the trace function as @xmath143 for any pair of real matrices we can find a real scalar @xmath144 that satisfies the above condition as @xmath145 rearranging the terms we have that choosing any @xmath144 from the interval @xmath146 $ ] would ensure that @xmath147 for any pair of real matrices @xmath69 and any stationary point @xmath148",
    "to evaluate our technique we ran experiments using 5 standard regression datasets .",
    "we performed a dimensionality reduction of the features in each of these datasets using our technique ( sdr - dcm ) and also with other supervised dimensionality reduction techniques like sliced average variance estimation ( save ) , principal hessian directions(phd ) , sliced inverse regression ( sir ) , inverse regression estimation ( ire ) , kernel sliced inverse regression ( ksir ) and no dimensionality reduction ( without dr ) giving us low - dimensional feature sets obtained from seven different technqiues across the 5 datasets .",
    "we then ran regression models using support vector machines ( svm ) , lasso , random forests ( rf ) , node harvest ( nh ) and bayesian additive regression trees ( bart ) utilizing the low - dimensional feature set obtained from each dimensionality reduction techniques across each dataset . the parameters of the regression models were tuned using 5-fold cross - validation .",
    "also , the 5-fold cross - validated root mean square error ( r.m.s.e ) was computed for each combination of the supervised dimensionality reduction technique , regression model and dataset .",
    "these cross - validated r.m.s.e values are presented in table  [ tb : tablename ] .",
    "also , for our proposed technique we choose the number of iterations for which we run the algorithm based on cross - validation .",
    "the overlaid vertical , black lines in figure 1 show the iteration at which the minimal cross - validation error was achieved for each of these datasets .",
    "the dimensionality to which the covariates in each dataset was reduced to is mentioned in the following sub - section .",
    "we now give a terse description of each of the 5 regression datasets used . + a ) * boston housing * is a dataset available at the uci ml repository .",
    "the data consists of 506 census tracts of boston from the 1970 census and contains information collected by the u.s census service and the task is to predict the median value of a home .",
    "this dataset cntains 14 features.the dimensionality was reduced to 3 covariates .",
    "+ b ) * concrete compressive strength * is a dataset also available from the uci ml repository and the task is to predict the concrete compressive strength based on the age and other ingredients .",
    "some of these ingredients include these ingredients include specific contents like cement , blast furnace slag , fly ash , water , superplasticizer , coarse aggregate , and fine aggregate .",
    "this dataset consists of 1090 samples and 9 features .",
    "the dimensionality was reduced to 3 covariates .",
    "+ c ) * windspeed * data at a candidate site in northern south dakota was collected every six hours for all of 2002 , except that of the month of may and a few other observations that are missing .",
    "wind speed and direction data corresponding to the candidate site was also collected at four reference sites .",
    "the task is to predict the wind speed at the candidate site .",
    "the data consists of 1114 observations and 14 variables and was collected by windlogics , inc .",
    "the dimensionality was reduced to 3 covariates .",
    "+ d ) * voting record * dataset was scraped from http://www.senate.gov/ and is also available on cran r repository .",
    "it consists of 598 samples and 96 variables .",
    "the task is to predict the voting record of the california democrat junior senator barbara boxer from the voting records of other senators .",
    "the senators included in the dataset consists of those who were in office for the entire session the dimensionality was reduced to 6 covariates .",
    "+ e ) * breast cancer gene expression * data from  @xcite was studied .",
    "it consists of gene signature data with 144 breast cancer patients and 77 covariates .",
    "the task is to predict the survival time of the patients based on the combination of gene expression measurements and the clinical covariates .",
    "the dimensionality was reduced to 6 covariates .",
    "as seen in the table 1 , our supervised dimensionality reduction technique ( sdr - dcm ) performed well in comparison to the other supervised dimensionality reduction techniques .",
    "figure 1 shows the convergence of the maximization of distance correlation , and in figure 2 the convergence plots of a gamma chosen from proposition 8.1 for the voting record dataset and three gamma choices outside the suggested interval are shown .",
    "the green line shows that convergence was reached when gamma was in the suggested interval unlike the rest of the choices of the gamma .",
    "the results produced by the proposed technique were reasonably competitive with regards to the results obtained on the without dimensionality reduction dataset apart from other techniques .",
    "as part of future - work , we believe that there may be a reasonable scope for generalizing this approach to prediction in a multi - task learning setting apart from applying this approach to classification problems .    , width=264,height=302 ]    0.95@ | c | c | c | c | c | c | c |r|   + method & sdr - dcm & save & ire & sir & phd & ksir & without dr + svm & 0.153 & 0.191 & 0.244 & 0.198 & 0.208 & 0.182 & 0.169 + lasso & 0.148 & 0.259 & 0.288 & 0.236 & 0.280 & 0.210 & 0.194 + node harvest & 0.163 & 0.241 & 0.277 & 0.221 & 0.269 & 0.174 & 0.184 + random forest & 0.166 & 0.213 & 0.257 & 0.186 & 0.220 & 0.192 & 0.170 + bart & 0.148 & 0.259 & 0.288 & 0.236 & 0.280 & 0.179 & 0.194 +   + method & sdr - dcm & save & ire & sir & phd & ksir & without dr + svm & 0.091 & 0.262 & 0.262 & 0.104 & 0.262 & 0.193 & 0.228 + lasso & 0.134 & 0.456 & 0.372 & 0.126 & 0.456 & 0.231 & 0.184 + node harvest & 0.106 & 0.287 & 0.290 & 0.091 & 0.287 & 0.227 & 0.198 + random forest & 0.162 & 0.237 & 0.294 & 0.240 & 0.236 & 0.186 & 0.187 + bart & 0.148 & 0.259 & 0.288 & 0.236 & 0.280 & 0.193 & 0.194 +   + method & sdr - dcm & save & ire & sir & phd & ksir & without dr + svm & 5.697 & 6.530 & 12.468 & 11.810 & 9.585 & 8.362 & 6.301 + lasso & 8.538 & 10.403 & 13.226 & 14.136 & 14.471 & 10.832 & 10.382 + node harvest & 6.381 & 10.064 & 13.219 & 12.690 & 12.401 & 9.917 & 8.386 + random forest & 6.216 & 8.040 & 12.273 & 12.403 & 9.653 & 7.806 & 5.341 + bart & 7.813 & 10.589 & 10.716 & 9.674 & 8.211 & 7.215 & 5.683 +   + method & sdr - dcm & save & ire & sir & phd & ksir & without dr + svm & 2.135 & 2.278 & 2.248 & 2.190 & 2.287 & 2.263 & 2.276 + lasso & 3.443 & 2.182 & 2.224 & 2.131 & 2.271 & 2.241 & 2.152 + node harvest & 1.837 & 2.437 & 2.517 & 2.259 & 2.497 & 2.972 & 2.275 + random forest & 2.051 & 2.291 & 2.354 & 2.227 & 2.295 & 2.085 & 2.176 + bart & 1.928 & 2.269 & 2.681 & 2.265 & 2.316 & 2.164 & 2.183 +   + method & sdr - dcm & save & ire & sir & phd & ksir & without dr + svm & 2.234 & 3.826 & 4.819 & 3.964 & 5.583 & 3.749 & 4.153 + lasso & 3.074 & 5.416 & 4.397 & 4.360 & 4.753 & 3.249 & 3.965 + node harvest & 2.160 & 4.813 & 4.265 & 4.361 & 3.436 & 4.186 & 3.702 + random forest & 2.203 & 3.261 & 3.974 & 4.132 & 4.924 & 3.134 & 3.986 + bart & 2.627 & 3.563 & 3.298 & 3.612 & 4.173 & 2.937 & 3.641 +",
    "ht z. zhang , h. zha _ principal manifolds and nonlinear dimensionality reduction via tangent space alignment _ , siam journal on scientific computing , volume 26 issue 1 , 2005 , pp",
    ".   313 338 .",
    "r.  r. coifman , s.  lafon , _ diffusion maps _ , applied and computational harmonic analysis , volume 21 , issue 1 , ( 2006 ) , pp .",
    "5-30 m. bernstein , v. de silva , j.  c. langford , j.  b. tenenbaum , _ graph approximations to geodesics on embedded manifolds _",
    "d.  l. donoho , c.   grimes , _ hessian eigenmaps : locally linear embedding techniques for high - dimensional data _",
    "usa , pnas , ( 2003 ) , vol .",
    "10 , pp .",
    "55915596 i. rish , g. grabarnik , g. cecchi , f. pereira , g.  j. gordon _ closed - form supervised dimensionality reduction with generalized linear models _ , 25th international conference on machine learning , ( 2008 ) .",
    "sajama , a. orlitsky _ supervised dimensionality reduction using mixture models _ , 22nd international conference on machine learning , ( 2005 ) .",
    "huan wang , shuicheng yan , dong xu , xiaoou tang , thomas huang , _ trace ratio vs. ratio trace for dimensionality reduction _ , ieee conference on computer vision and pattern recognition , ( 2007 ) .",
    "k. mao , f. liang , s. mukherjee , _ supervised dimension reduction using bayesian mixture modeling _ , artificial intelligence and statistics , ( 2010 ) , pp .",
    ".    r.  d. cook , _ journal of the american statistical association _",
    "93 , no . 441 ( 1998 ) , pp .",
    "8494 k.  c. li , _ sliced inverse regression for dimension reduction _ , journal of the american statistical association , vol .",
    "86 , no . 414 , ( 1991 ) , pp .",
    "316 - 327 .",
    "y.  li , l.  x. zhu , asymptotics for sliced average variance estimation , the annals of statistics vol .",
    "1 ( feb . , 2007 ) , pp .",
    "41 - 69 m. rizzo , n. bakirov _ measuring and testing dependence by correlation of distances _ , annals of statistics , ( 2007 ) vol .",
    "35 no . 6 , pp .",
    "27692794 . van de vijver , m.  j.  y.  d. he , l.   j. van t veer , h. dai , a.  a.   m. hart , d.   w. voskuil , g.   j. schreiber , j.   l. peterse , c. roberts , m.  j. marton , m. parrish ,  t. rutgers , s.  h. friend , and r. bernards , _ a gene- expression signature as a predictor of survival in breast cancer _",
    ", new england journal of medicine , ( 2002 ) , 347 ( 25 ) , pp .",
    ". y. jia , f. nie , c. zhang , _ trace ratio problem revisited _ , ieee transactions on neural networks , ( 2009 ) , volume : 20 , issue : 4 , pp .",
    "a.  l. yuille , a. rangarajan , _ the concave - convex procedure ( cccp ) _ , ( 2002 ) , advances in neural information processing systems .",
    "d.  r. hunter , k. lange , _ a tutorial on mm algorithms _ , the american statistician , ( 2004 ) , taylor and francis .",
    "y. zhang , r. tapia , l. velazquez , _ on convergence of minimization methods : attraction , repulsion , and selection _ , journal of optimization theory and applications , ( 2000 ) , vol .",
    "107 , no.3 , pp .   529546 .",
    "given that @xmath149 denotes the space of linear functionals on matrices over reals , @xmath150 in order for a function @xmath151 to be frechet differentiable at @xmath152 , it must satisfy for a direction @xmath153 , the condition @xmath154 for some linear map @xmath155 .",
    "we check the frechet differentiability of the loss function @xmath156 in this section .",
    "we use @xmath157 for brevity and also note that @xmath158 as the laplacian matrices are symmetric .",
    "we check the above stated differentiability condition over our loss function @xmath156 which gives us @xmath159 we have @xmath160 hence , @xmath161 and also the differential @xmath162 for showing that @xmath94 is twice differentiable , we have to prove that there is a linear @xmath163 such that for a direction @xmath164 @xmath165 but now as @xmath166 is linear , if we define @xmath167 for each @xmath92 , we have @xmath168 so , the loss - function @xmath156 is twice frechet differentiable ."
  ],
  "abstract_text": [
    "<S> in a regression setting we propose algorithms that reduce the dimensionality of the features while simultaneously maximizing a statistical measure of dependence known as distance correlation between the low - dimensional features and a response variable . </S>",
    "<S> this helps in solving the prediction problem with a low - dimensional set of features . </S>",
    "<S> our setting is different from subset - selection algorithms where the problem is to choose the best subset of features for regression . </S>",
    "<S> instead , we attempt to generate a new set of low - dimensional features as in a feature - learning setting . we attempt to keep our proposed approach as model - free and our algorithm does not assume the application of any specific regression model in conjunction with the low - dimensional features that it learns . </S>",
    "<S> the algorithm is iterative and is fomulated as a combination of the majorization - minimization and concave - convex optimization procedures . </S>",
    "<S> we also present spectral radius based convergence results for the proposed iterations </S>",
    "<S> .    [ section ] [ theorem]lemma [ theorem]proposition [ theorem]corollary [ section ] [ section ] [ section ] </S>"
  ]
}