{
  "article_text": [
    "in more and more burgeoning science and technology fields and with the help of rapid development in information technology , a huge amount of data is collected where the number of variables is usually large .",
    "however , most of traditional statistical tools deeply depend on the assumption of a large sample size @xmath3 compared to the number of variables @xmath1 ( data dimension ) . for high - dimensional data analysis , inevitably , these classical tools become inefficient , or even worse , inconsistent . for decades",
    ", statisticians devoted special efforts to seek for better approaches in such high - dimensional data case .",
    "for the two sample significance test problem in high dimensions , as early as in 1958 , @xcite proposed a so - called _ non - exact test _",
    "( net ) as a remedy to the failure of hotelling s @xmath7-test .",
    "a rigorous analysis of this net arises much later in @xcite using modern random matrix theory ( rmt ) .",
    "these authors have found necessary correction for the @xmath7-test to cope with high dimensional effects .",
    "recent work in high dimensional statistics include @xcite , @xcite and @xcite .",
    "these authors propose several procedures in the high - dimensional setting for testing that i ) a covariance matrix is an identity matrix , proportional to an identity matrix ( spherecity ) and is a diagonal matrix or ii ) several covariance matrices are equal .",
    "these procedures have the following common feature : their construction involves some well - chosen distance function between the null and the alternative hypotheses and rely on the first two spectral moments , namely the statistics tr@xmath8 and tr@xmath9 from sample covariance matrices @xmath8 . in a recent work @xcite , we have considered likelihood based tests about such high dimensional covariance matrices where the failure of the classical likelihood ratio test is explained using rmt .",
    "necessary corrections to these lrt s are then introduced to achieve consistency .",
    "this paper pursue the investigation of similar questions but for a multivariate regression model with high dimensional data , i.e. the dimensions of the dependent variable as well as the number of the regression variables are large compared to the sample size .",
    "more precisely , let a @xmath1-th dimensional regression model @xmath10 where @xmath11 is a sequence of i.i.d .",
    "zero - mean gaussian noise @xmath12 with covariance matrices @xmath13 , @xmath14 a @xmath15 matrix of regression coefficients , and @xmath16 a sequence of known regression variables of dimension @xmath2 . to simplify the presentation",
    ", we always assume that @xmath17 and that the rank of @xmath18 equals @xmath2 .",
    "let us define a block decomposition @xmath19 with @xmath20 and @xmath21 columns , respectively ( @xmath22 ) .",
    "a general linear hypothesis is defined as @xmath23 where @xmath24 is a given matrix .",
    "a well - studied example is the special case @xmath25 yielding a significance test for the first @xmath26 regression variables .    in the general case and under the alternative ,",
    "the maximum likelihood estimators of @xmath27 are @xmath28 and @xmath29 the corresponding likelihood maximum equals @xmath30    on the other hand , under the null hypothesis , by using a partition @xmath31 on @xmath26 and @xmath21 variables repectively , the maximum likelihood estimators of @xmath32 are @xmath33 and @xmath34 where @xmath35 .",
    "the associated likelihood maximum equals @xmath36 it follows that the likelihood ratio statistic for the test ( [ h02b ] ) equals @xmath37 where @xmath38 is the celebrated wilk s @xmath4 ( @xcite and @xcite ) .",
    "let us define a similar block decomposition for the sum @xmath39 and the matrix @xmath40 after some algebraic manipulations , we get ( see @xcite , page 302 ) @xmath41 where @xmath42 and @xmath43 is a @xmath44 matrix made of the first @xmath26 columns of @xmath45 .",
    "it is known that @xmath46 , a wishart distribution . moreover , under @xmath47 , @xmath48 and this statistic is independent of @xmath49 .",
    "therefore , @xmath47 will be rejected if @xmath50 for some critical value @xmath51 , or equivalently , when the matrix @xmath52 has some large enough eigenvalues .    under the gaussian assumptions made here",
    ", the exact distribution of @xmath53 is known under the null hypothesis .",
    "however in practice , it is usually a difficult task to compute the critical value @xmath51 even for moderately large @xmath1 and @xmath2 .",
    "for example , @xcite used complex analytical approximations and established tables for critical values with @xmath1 and @xmath2 smaller than 12 .",
    "on the other hand , in a large @xmath3 asymptotic scheme , one assumes @xmath1 and @xmath2 are fixed and then the null distribution of @xmath54 is approximated by a @xmath55 .",
    "note that for this chi - squared approximation , one generally uses a rescaled lrt statistic @xmath56 this correction is known as bartlett - box correction ( hereafter bbc ) due to @xcite and it is much less biased than the classical lrt @xmath54 , see section  [ sec : simulinear ] for a detailed comparison .",
    "however for high dimensional data where the dimensions @xmath1 and @xmath26 are large compared to the sample size @xmath3 , unfortunately the above @xmath55 approximation becomes useless .",
    "as an example , even for moderate @xmath1 , @xmath2 and @xmath3 with @xmath57 close to 1 , the celebrated marenko - pastur theorem tell us that the eigenvalues of @xmath58 tend to fill the whole interval @xmath59 $ ] .",
    "hence , a non - negligible proportion of these eigenvalues are close to zero .",
    "consequently , any statistic based on the inverse @xmath60 like @xmath53 becomes unstable and non robust .    in  [ sec :",
    "clinear ] , by using modern rmt , we introduce a correction to wilk s @xmath4 to cope with the mentioned high - dimensional effects .",
    "the corrected lrt is asymptotically gaussian and we will see that it has very satisfactory size and power , surely for the large @xmath61 and @xmath3 context , but also for moderate data dimensions like @xmath5 or @xmath6 .",
    "moreover , to assess the power of the corrected lrt , we examine two additional tests based on statistics of least - squares type as suggested in @xcite . a quite intensive simulation experiment is then conducted to compare these different procedures for testing ( [ h02b ] ) .",
    "next in  [ sec : application ] , we consider the classical multiple sample significance test problem but with high - dimensional data .",
    "as it is well - known , this problem can be embedded into a special instance of the general linear hypothesis ( [ h02b ] ) .",
    "therefore , by an application of general results of  [ sec : clinear ] , we obtain a valid lrt after necessary corrections .    all the proofs and technical derivations are postponed to ",
    "[ sec : proofs ] .",
    "we first recall a fundamental result from rmt for linear statistics of so - called random fisher matrices which will be used below . for any @xmath62 square matrix @xmath63 with real eigenvalues @xmath64",
    ", @xmath65 denotes the empirical spectral distribution ( esd ) of @xmath63 , that is , @xmath66 we will consider random matrices @xmath67 whose esd @xmath68 converges , in a sense to be precise and when @xmath69 , to a limiting spectral distribution ( lsd ) @xmath70 .",
    "assume we have to estimate some parameter of @xmath70 , say @xmath71 for some function @xmath72 , it is natural to use the empirical estimator @xmath73 which is a so - called linear spectral statistic ( lss ) of the random matrices @xmath74 .",
    "let @xmath75 and @xmath76 be two independent double arrays of @xmath77 complex variables with mean 0 and variance 1 . write @xmath78 and @xmath79 .",
    "also , for any positive integers @xmath80 , the vectors @xmath81 and @xmath82 can be thought as independent samples of size @xmath83 and @xmath84 , respectively , from some @xmath1-dimensional distributions .",
    "let @xmath85 and @xmath86 be the associated sample covariance matrices ,   @xmath87 @xmath88 then , the following so - called _ f - matrix _ generalizes the classical fisher - statistic to the present @xmath1-dimensional case , @xmath89 where we assume that @xmath90 . here",
    "we use the notation @xmath91 .",
    "let us also assume that @xmath92 under suitable moment conditions , the esd @xmath93 of @xmath94 has a lsd @xmath95 with the following density function , see p.72 of @xcite , @xmath96      & \\displaystyle{0 , \\quad\\quad \\quad\\quad \\mbox{otherwise } , }    \\end{array }    \\right.\\label{lsdden}\\ ] ] where @xmath97 let @xmath98 be an open subset of the complex plane which contains the interval @xmath99 $ ] and @xmath100 be the set of analytic functions @xmath101 define the empirical process @xmath102 indexed by @xmath100 @xmath103 ( dx ) , \\quad f",
    "\\in \\mathcal{a}. \\label{gdef2}\\ ] ] here @xmath104 is the distribution in ( [ lsdden ] ) with indexes @xmath105 ( instead of @xmath106 ) , k=1,2 .",
    "recently , @xcite establishes a general clt for lss of large - dimensional f matrix .",
    "the following theorem is a simplified one quoted from it . throughout the paper ,",
    "@xmath107 denotes a contour integral along a given contour .",
    "[ t2.1 ] let @xmath108 , and assume : + for each p , @xmath109 and @xmath110 variables are @xmath111 , @xmath112 @xmath113 @xmath114   @xmath115    * real case .",
    "assume moreover @xmath116 and @xmath117 are real , @xmath118 , then the random vector @xmath119 weakly converges to a k - dimensional gaussian vector with the mean vector @xmath120d\\zeta \\label{e1}\\\\      & & \\quad + \\frac{\\beta\\cdot y_1(1-y_2)^2}{2\\pi i \\cdot        h^2}\\oint_{|\\zeta|=1}f_j(z(\\zeta))\\frac{1}{(\\zeta+\\frac{y_2}{h})^3}d\\zeta\\label{e1betax}\\\\      & & \\quad + \\frac{\\beta\\cdot y_2(1-y_2)}{2 \\pi i \\cdot        h}\\oint_{|\\zeta|=1}f_j(z(\\zeta))\\frac{\\zeta +        \\frac{1}{h}}{(\\zeta+\\frac{y_2}{h})^3}d\\zeta , \\label{e1betay }      \\quad\\quad j=1 , \\cdots , k ,    \\end{aligned}\\ ] ] where @xmath121 , \\quad    h = \\sqrt{y_1+y_2-y_1y_2}$ ] , @xmath122 and the covariance function @xmath123 * complex case .",
    "assume moreover @xmath116 and @xmath117 are complex , @xmath124 then the conclusion of ( i ) also holds , except the means are @xmath125 and the covariance function is @xmath126 with @xmath127 .    we should point out that zheng s clt for @xmath70-matrices covers more general situations the those cited in theorem  [ t2.1 ] . in particular , the fourth moments @xmath128 and @xmath129 can be different .",
    "the following lemma will be used in  [ sec : clinear ] for an application of theorem [ t2.1 ] ( see ( [ teste ] ) and ( [ testvar ] ) ) . for a proof ,",
    "see @xcite .    for",
    "the function @xmath130 , let @xmath131 be the unique solution to the equations @xmath132 analogously , let @xmath133 be the constants similar to @xmath134 but for the function @xmath135 then , the mean and covariance functions in ( [ e1 ] ) and ( [ cov1 ] ) equal to @xmath136 [ lem1 ]",
    "the construction of a correct scaling for the lrt statistic @xmath53 of the test ( [ h02b ] ) will rely on the clt  [ t2.1 ] . recall that @xmath137 under @xmath47 , we have @xmath138 and they are independent . consequently , @xmath139 is exactly distributed as the @xmath70-matrix @xmath94 defined in ( [ f ] ) , where in addition all the variables are gaussian .",
    "our correction to the lrt statistic @xmath53 is given in the following theorem .",
    "[ t4.1 ] for the general linear hypothesis ( [ h02b ] ) in the regression model , let @xmath53 be wilk s lrt statistic given in ( [ lamda * ] ) .",
    "define also the function @xmath140 and assume that @xmath141 then , under the null , @xmath142 \\rightarrow { \\euscript{n}}\\left ( 0 , 1\\right),\\ ] ] where @xmath143 and @xmath144 are defined in ( [ teste])([testvar])and ( [ limit ] ) , respectively .    before giving a proof",
    ", it is worth mentioning that at a first look , the asymptotic framework depicted in seems complicated .",
    "indeed , this is a common set - up in rmt and simply requires that the degrees of freedom of the underlying wishart matrices grow to infinity in a proportional way with the sample size .",
    "since @xmath139 can be represented by a gaussian @xmath94 , we have @xmath145 define @xmath146 , by @xmath147 , also it can be written as @xmath148 from @xmath149 where @xmath150 and @xmath151 is the limiting distribution which has a density in ( [ lsdden ] ) but with @xmath105 instead of @xmath152 then we get @xmath153    by theorem [ t2.1 ] , @xmath154 weakly converges to a gaussian vector with mean @xmath155 and variance @xmath156 for the real case , where @xmath157 , c > d",
    ".    \\end{aligned}\\ ] ] this is calculated in ",
    "[ sec : proofs ] using lemma  [ lem1 ] . for the complex case , the mean @xmath158 is zero and",
    "the variance is half of @xmath159 .",
    "in other words , @xmath160 here @xmath161 where @xmath162 , c_n > d_n ,    \\end{aligned}\\ ] ] is derived in  [ sec : proofs ] using the density function of @xmath163 .",
    "then we get letting @xmath164 , @xmath165 \\rightarrow n \\left ( 0 ,    1\\right).\\ ] ]    we call _ corrected likelihood ratio test _ ( clrt ) for testing the test based on the statistic @xmath166 and its asymptotic distribution derived in the theorem above .",
    "moreover , it is worth noticing that in the above proof , we used the gaussian assumption for entry variables to fit @xmath52 to a gaussian @xmath70-matrix .",
    "however , theorem  [ t2.1 ] does not need this gaussian assumption .",
    "therefore , we can expect ( or conjecture ) that the asymptotic distribution for @xmath166 in theorem  [ t4.1 ] , hence the clrt , could be valid more generally .",
    "however , the kurtosis parameter @xmath167 appeared in theorem  [ t2.1 ] is no more null and it will appears in the asymptotic parameters @xmath168 and @xmath169 above .",
    "to evaluate the corrected lrt , we consider two additional procedures based on least - squares type statistics as suggested in @xcite . we first need to find the asymptotic distributions of these statistics .    by ( [ bmle ] ) and the partition of @xmath170",
    ", we obtain @xmath171 let @xmath172 because @xmath45 is a unbiased estimator of @xmath170 , then @xmath173 under the null hypothesis .",
    "thus @xmath174 where @xmath175 ^ 2,\\\\    \\beta_{z2 } & = &    \\sum\\limits_{i=1}^n\\left[(\\textbf{z}_{i,1}'-\\textbf{z}_{i,2}'\\textbf{a}_{22}^{-1}\\textbf{a}_{21 } )      \\textbf{a}_{11:2}^{-1 }      ( \\textbf{z}_{i,1}-\\textbf{a}_{12}\\textbf{a}_{22}^{-1}\\textbf{z}_{i,2})\\right]^2.\\\\\\end{aligned}\\ ] ] define @xmath176    [ th.mnk ] assuming that    1 .",
    "@xmath177 ; 2 .   as @xmath178 , @xmath179 ; 3 .",
    "@xmath180)$ ] ; 4 .",
    "@xmath181 are i.i.d .",
    "zero - mean random vectors such that for any @xmath182 , there exists a @xmath183 , such that @xmath184    then for @xmath185 and under @xmath47 in , @xmath186    consequently , to test , we can use any of the statistics @xmath187 and @xmath188 .",
    "these tests will be referred below as st1 and st2 .",
    "we set up a simulation experiment to compare five procedures for testing : the classical lrt with an asymptotic @xmath189 approximation , the associated bartlett - box correction ( bbc ) recalled in , our corrected lrt ( clrt ) introduced in  [ sec : clinear ] and the two tests st1 and st2 based on least - squares type statistics of  [ sec : st1st2 ] .",
    "denote the non - center parameter as  @xmath190 , where @xmath191 , and @xmath192 is a varying constant .",
    "then we consider the model ( [ linear model ] ) as the form @xmath193 assume that the elements of  @xmath194 follow the distribution @xmath195 .",
    "all the @xmath77 elements of @xmath196 in the model are sampled from @xmath197 .",
    "the errors @xmath198 in ( [ linear model ] ) have a multivariate normal distribution @xmath199 with @xmath200 therefore , @xmath201 measures the degree of correlations between the @xmath1 coordinates of the noise vectors . to understand the effect of these correlations on the test procedures , we consider two cases : @xmath202 and @xmath203 .    for different values of @xmath204 , we compute the realized sizes ( type - i errors ) of the five tests based on 1,000 independent replications .",
    "all the tests are defined with an nominal ( and asymptotic ) level @xmath205 .",
    "the powers of the tests are evaluated under alternative hypotheses obtained by varying the parameter @xmath192 .",
    "table  [ table00 ] gives the sizes ( line @xmath206 , in bold ) and the powers ( @xmath207 ) for the case @xmath203 and various choices of the dimensions @xmath204 .",
    "table  [ table09 ] displays analogous results for the case @xmath202 where the coordinates of the noise sequence are highly correlated .",
    "the important conclusions from these tables are as follows .",
    "test size : : :    * the lrt and bbc correction are highly inconsistent : in all    considered cases , the lrt and its bbc correction have a much higher    size than the nominal value 5% . in particular , the lrt systematically    rejects the null hypothesis , even for data dimension as small as    @xmath208 , while the bbc correction is just less biased as    expected .",
    "* in the case where the coordinates of the noise are uncorrelated    ( table  [ table00 ] ) , the three tests clrt , st1 and st2 which are based    on the rmt , achieve a correct level close to 5% .    +    in contrary , when these correlations are high ( table  [ table09 ] ) , as    the least - squares type tests st1 and st2 heavily depend on an assumed    non correlation between these coordinates , these two tests become    inconsistent .",
    "the power function : : :    in the case where the coordinates of the noise are uncorrelated    ( table  [ table00 ] ) , while being all consistent , clrt and st2 outperform    the test st1 .",
    "+    when these coordinates are highly correlated ( table  [ table09 ] ) and    despite their inconsistency , the tests st1 and st2 are outperformed by    the clrt .",
    "for example , in the case @xmath209 ,    the highest power of st1 and st2 are only 0.283 and 0.115 ,    respectively .    to summarize , among the five tests considered here , only the clrt displays an overall consistency and a generally satisfactory power .",
    "in particular , this test is robust with regard to the correlations between the coordinates of the noise process .",
    "lastly , figures 1 and 2 give a dynamic view of these comparisons by varying the non - central parameter @xmath192 for the cases @xmath203 and @xmath202 , respectively .",
    "note that the left - first point of all lines represent the realized sizes ( type i errors ) of the tests , and others are the powers .",
    "in this section we consider the following multiple sample significance test problem in a manova with high - dimensional data .",
    "for the two sample case , this problem has been considered by @xcite and @xcite . here",
    "we treat the general multiple sample case .",
    "consider @xmath2 gaussian populations @xmath210 of dimension @xmath1 , @xmath211 , and for each population , assume that we have a sample of size @xmath212 : @xmath213 .",
    "we wish to test the hypothesis @xmath214 high dimensional here means that both the number @xmath2 of the populations and the dimension @xmath1 of the observation vectors are large with respect to the sample sizes @xmath215 s .",
    "clearly , the observations can be put in the form @xmath216 where @xmath217 is an array of i.i.d .",
    "random vectors distributed as @xmath12 .",
    "we are going to embed the test into a special instance of the regression test . to this end , let @xmath218 be the canonical base of @xmath219 and we define the following regression vectors @xmath220 \\mathbf{1}_{\\{i < q\\ } } + e_q \\mathbf{1}_{\\{i = q\\ } } , \\quad    1\\le i\\leq q,~1\\le   k\\le n_i.\\ ] ] define moreover the @xmath15 matrix @xmath221 with @xmath222 note that the dimension @xmath2 is split to @xmath223 in the above decomposition .",
    "therefore , the observations follow a linear model @xmath224 the multiple sample test is equivalent to the following regression test @xmath225    in order to apply theorem  [ t4.1 ] , we now identify the likelihood ratio statistic @xmath53 defined in . here",
    "denote @xmath226 . under the null hypothesis , the likelihood estimates of @xmath227",
    "are ( see @xcite for details of computation ) @xmath228 on the other hand , under the alternative hypothesis , the likelihood estimates of @xmath229 are @xmath230 the likelihood ratio statistic @xmath231 readily follows .    by application of theorem  [ t4.1 ]",
    ", we have the following    [ prop : m ] for the multiple sample significance test , assume that @xmath232 , @xmath233 , @xmath234 , @xmath235 in such a manner that @xmath236 then , for the same function @xmath72 defined in ( [ f(x ) ] ) , we have @xmath237 \\rightarrow { \\euscript{n}}\\left ( 0 , 1\\right).\\ ] ] where @xmath238 and @xmath144 are defined in ( [ teste ] ) , ( [ testvar ] ) and ( [ limit ] ) respectively , with the values of @xmath239 defined in ( [ y1y2 ] ) .",
    "it is worth noticing here that the classical likelihood ratio test ( lrt ) for testing will rely on the following weak convergence theorem : under @xmath47 and assuming fixed @xmath1 and @xmath2 while letting @xmath233 , @xmath240 inevitably , in high dimensional case , @xmath241 will drifts to infinity by proposition  [ prop : m ] .",
    "consequently , this classical @xmath189-approximation will leads to a test size much higher than a given nominal test level , exactly as for the general linear hypothesis considered in ",
    "[ sec : clinear ] .",
    "because @xmath242 are gaussian variables , for real case , @xmath243**@xmath244**@xmath245 then ( [ e1betax ] ) , ( [ e1betay ] ) and ( [ cov1betax ] ) are all 0 .",
    "consider ( [ e1 ] ) and ( [ cov1 ] ) , as @xmath246 and during the process of lemma [ lem1 ] calculation , we will see that the constant and items approaching to zero do not effect on the the circle integration results , and in practice @xmath247 so we use @xmath248 instead of @xmath249 .",
    "make substitute @xmath250 , where @xmath251 , \\quad h = \\sqrt{y_1+y_2-y_1y_2}$ ] . because @xmath252 where @xmath253 , c > d,\\\\ a_0,b_0&=&\\frac{(1+h)^2}{(1-y_2)^2}.   \\end{aligned}\\ ] ] is the solution of the equation ( [ cd ] ) with @xmath254 then use lemma [ lem1 ] , we have @xmath255 for the real case .",
    "for this computation we drop the indexes @xmath257 and @xmath258 in the parameters @xmath259 and compute the integral @xmath260 . following a device designed in @xcite ( lemma a.2 ) ,",
    "let @xmath261 be the stieltjes transform of the distribution function @xmath262 for @xmath263 but very close to @xmath264 and @xmath265 , we use a change of variable @xmath266 which is implicitly defined by the formula @xmath267 and we have the following relations @xmath268 or equivalently , @xmath269 this shows that when @xmath244 anticlockwise runs along the unit circle , @xmath270 anticlockwise runs a contour which closely encloses the interval @xmath271 $ ] when @xmath272 is close to 1 where @xmath273 and @xmath274 .",
    "so we obtain @xmath275)\\\\ & = & \\frac{1}{2\\pi iy_{1}}\\oint_{|\\xi|=1}\\log|c+d\\xi|^2\\frac{\\xi^2 - 1}{\\xi(\\xi+\\frac1{h})(\\xi+\\frac{y_{2}}{h})}d\\xi\\\\ & = & \\frac{1}{2\\pi iy_{1}}\\oint_{|\\xi|=1}\\left(\\log(c+d\\xi)+\\log(c+d\\xi^{-1})\\right)\\frac{\\xi^2 - 1}{\\xi(\\xi+\\frac1{h } ) ( \\xi+\\frac{y_{2}}{h})}d\\xi\\\\ & & ~(\\mbox{making } \\xi^{-1}\\to\\xi \\mbox { in the second integral } ) \\\\ & = & \\frac{1}{2\\pi iy_{1}}\\oint_{|\\xi|=1}\\log(c+d\\xi ) \\left(\\frac{\\xi^2 - 1}{\\xi(\\xi+\\frac1{h})(\\xi+\\frac{y_{2}}{h } ) } -\\frac{h^2}{y_{2}}\\frac{\\xi^2 - 1}{\\xi(\\xi+h)(\\xi+\\frac{h}{y_{2}})}\\right)d\\xi\\\\ & = & \\frac{y_{2}-1}{y_{2}}\\log(c)+\\frac{y_{1}-1}{y_{1}}\\log(c - dh)+\\frac{y_{1}+y_{2}}{y_{1}y_{2}}\\log\\left(\\frac{ch - dy_2}{h}\\right)\\end{aligned}\\ ] ] where @xmath276          bai , z. d. jiang , d.d .",
    "yao , j.f . and",
    "zheng , s.r .",
    "corrections to lrt on large dimensional covariance matrix by rmt _ ann . statistics _ * 37 ( 6b ) * , 3822 - 3840 .",
    "a longer version with full proofs is available at ` arxiv:0902.0552[stat ] ` .",
    "bai , z. d. and saranadasa , h. ( 1996 ) .",
    "effect of high dimension comparison of significance tests for a high dimensional two sample problem .",
    "_ statistica sinica . _",
    "* 6 * , 311 - 329 .",
    "ledoit , o. and wolf , m. ( 2002 ) some hypothesis tests for the covariance matrix when the dimension is large compared to the sample size .",
    "_ _ ann . statist.__**30 * * , 1081 - 1102 .",
    "mathai , a.m. ( 1971 ) on the distribution of the likelihood ratio criterion for testing linear hypotheses on regression coefficients .",
    "math._*23 * , 181 - 197 .",
    "l|@rrrrr|@rrrrr @xmath203 & & +   + parameter @xmath192 & lrt&clrt&bbc&st1&st2 & lrt&clrt&bbc & st1&st2 +   + 0 & * 1 & * 0.056 & * 0.101 & * 0.070 & * 0.086 & * 1 & * 0.047 & * 0.672&*0.042 & * 0.072 + @xmath277 & 1 & 0.064 & 0.113 & 0.071 & 0.096 & 1 & 0.084 & 0.741&0.044 & 0.129 + @xmath278 & 1 & 0.083 & 0.150 & 0.080 & 0.136 & 1 & 0.203 & 0.879&0.050 & 0.395 + @xmath279 & 1 & 0.150 & 0.224 & 0.098 & 0.222 & 1 & 0.381 & 0.963&0.063 & 0.851 + @xmath280 & 1 & 0.247 & 0.342 & 0.125 & 0.387 & 1 & 0.583 & 0.992&0.091 & 0.998 + @xmath281 & 1 & 0.382 & 0.500 & 0.156 & 0.588 & 1 & 0.784 & 0.999&0.127 & 1 + @xmath282 & 1 & 0.574 & 0.676 & 0.200 & 0.792 & 1 & 0.914 & 1 & 0.173 & 1 + @xmath283 & 1 & 0.747 & 0.829 & 0.279 & 0.932 & 1 & 0.979 & 1 & 0.257 & 1 + @xmath284 & 1 & 0.885 & 0.925 & 0.375 & 0.988 & 1 & 0.996 & 1 & 0.374 & 1 + @xmath285 & 1 & 0.953 & 0.980 & 0.496 & 0.997 & 1 & 0.999 & 1 & 0.526 & 1 + @xmath286 & 1 & 0.986 & 0.990 & 0.624 & 1 & 1 & 1 & 1 & 0.681 & 1 + * * * * * * * * * *    l|@rrrrr|@rrrrr @xmath203 & & +   + parameter @xmath192 & lrt&clrt&bbc&st1&st2 & lrt&clrt&bbc & st1&st2 +   + 0 & * 1 & * 0.060 & * 0.178 & * 0.054 & * 0.062 & * 1 & * 0.056 & * 0.495&*0.036 & * 0.048 + @xmath287 & 1 & 0.062 & 0.190 & 0.055 & 0.065 & 1 & 0.063 & 0.551&0.040 & 0.065 + @xmath288 & 1 & 0.078 & 0.221 & 0.060 & 0.083 & 1 & 0.099 & 0.668&0.042 & 0.135 + @xmath289 & 1 & 0.106 & 0.276 & 0.068 & 0.123 & 1 & 0.210 & 0.797&0.048 & 0.372 + @xmath290 & 1 & 0.164 & 0.357 & 0.071 & 0.229 & 1 & 0.363 & 0.908&0.060 & 0.734 + @xmath291 & 1 & 0.232 & 0.462 & 0.082 & 0.352 & 1 & 0.560 & 0.972&0.073 & 0.974 + @xmath292 & 1 & 0.348 & 0.584 & 0.097 & 0.501 & 1 & 0.742 & 0.991&0.103 & 0.999 + @xmath293 & 1 & 0.483 & 0.725 & 0.131 & 0.715 & 1 & 0.871 & 0.998&0.152 & 1 + @xmath294 & 1 & 0.616 & 0.831 & 0.182 & 0.874 & 1 & 0.939 & 0.999&0.207 & 1 + @xmath295 & 1 & 0.771 & 0.911 & 0.241 & 0.970 & 1 & 0.984 & 1 & 0.304 & 1 + @xmath279 & 1 & 0.872 & 0.954 & 0.325 & 0.993 & 1 & 0.995 & 1 & 0.414 & 1 + * * * * * * * * * *    l|@rrrrr|@rrrrr @xmath202 & & +   + parameter @xmath192 & lrt&clrt&bbc&st1&st2 & lrt&clrt&bbc & st1&st2 +   + 0 & * 1 &",
    "* 0.056 & * 0.089 & * 0.105 & * 0.119 & * 1 & * 0.055 & * 0.681 & * 0.087 & * 0.155 + @xmath296 & 1 & 0.063 & 0.099 & 0.106 & 0.121 & 1 & 0.063 & 0.696 & 0.088 & 0.164 + @xmath297 & 1 & 0.078 & 0.123 & 0.107 & 0.124 & 1 & 0.089 & 0.762 & 0.089 & 0.187 + @xmath291 & 1 & 0.110 & 0.162 & 0.109 & 0.134 & 1 & 0.165 & 0.849 & 0.091 & 0.220 + @xmath298 & 1 & 0.164 & 0.234 & 0.111 & 0.143 & 1 & 0.261 & 0.923 & 0.093 & 0.261 + @xmath299 & 1 & 0.253 & 0.355 & 0.116 & 0.161 & 1 & 0.458 & 0.974 & 0.095 & 0.323 + @xmath300 & 1 & 0.388 & 0.491 & 0.118 & 0.182 & 1 & 0.690 & 0.999 & 0.099 & 0.408 + @xmath301 & 1 & 0.562 & 0.652 & 0.123 & 0.215 & 1 & 0.878 & 1 & 0.101 & 0.503 + @xmath302 & 1 & 0.724 & 0.811 & 0.130 & 0.250 & 1 & 0.963 & 1 & 0.105 & 0.610 + @xmath303 & 1 & 0.873 & 0.926 & 0.136 & 0.284 & 1 & 0.998 & 1 & 0.110 & 0.704 + @xmath304 & 1 & 0.951 & 0.979 & 0.144 & 0.343 & 1 & 1 & 1 & 0.115 & 0.801 + * * * * * * * * * *      l|@rrrrr|@rrrrr @xmath202 & & +   + parameter @xmath192 & lrt&clrt&bbc&st1&st2 & lrt&clrt&bbc & st1&st2 +   + 0 & * 1 & * 0.054 & * 0.181 & * 0.089 & * 0.105&*1 & * 0.059 & * 0.520&*0.098 & * 0.100 + @xmath305 & 1 & 0.059 & 0.197 & 0.090 & 0.106 & 1 & 0.060 & 0.536&0.099 & 0.107 + @xmath306 & 1 & 0.074 & 0.223 & 0.090 & 0.109 & 1 & 0.079 & 0.604&0.100 & 0.116 + @xmath288 & 1 & 0.113 & 0.288 & 0.091 & 0.115 & 1 & 0.140 & 0.697&0.101 & 0.136 + @xmath307 & 1 & 0.178 & 0.400 & 0.091 & 0.126 & 1 & 0.233 & 0.811&0.102 & 0.175 + @xmath297 & 1 & 0.287 & 0.530 & 0.092 & 0.140 & 1 & 0.409 & 0.913&0.104 & 0.230 + @xmath290 & 1 & 0.445 & 0.691 & 0.093 & 0.161 & 1 & 0.633 & 0.979&0.107 & 0.300 + @xmath308 & 1 & 0.643 & 0.840 & 0.097 & 0.180 & 1 & 0.826 & 0.993&0.114 & 0.379 + @xmath309 & 1 & 0.821 & 0.939 & 0.101 & 0.202 & 1 & 0.953 & 1 & 0.118 & 0.481 + @xmath292 & 1 & 0.937 & 0.986 & 0.107 & 0.238 & 1 & 0.992 & 1 & 0.125 & 0.597 + @xmath298 & 1 & 0.987 & 0.996 & 0.115 & 0.283 & 1 & 1 & 1 & 0.131 & 0.694 + * * * * * * * * * *    ) and powers ( @xmath310 ) of the four methods , which are the corrected lrt ( clrt ) , bartlett - box correction ( bbc ) and two least - squares type tests ( st1 and st2 ) , based on 1,000 independent replications using gaussian error variables from @xmath311 .",
    "top row :   @xmath312 and @xmath313 .",
    "bottom row :  @xmath314 and @xmath315 .",
    "[ fig:12 ] , title=\"fig:\",width=264 ] ) and powers ( @xmath310 ) of the four methods , which are the corrected lrt ( clrt ) , bartlett - box correction ( bbc ) and two least - squares type tests ( st1 and st2 ) , based on 1,000 independent replications using gaussian error variables from @xmath311 .",
    "top row :   @xmath312 and @xmath313 .",
    "bottom row :  @xmath314 and @xmath315 .",
    "[ fig:12 ] , title=\"fig:\",width=264 ] + ) and powers ( @xmath310 ) of the four methods , which are the corrected lrt ( clrt ) , bartlett - box correction ( bbc ) and two least - squares type tests ( st1 and st2 ) , based on 1,000 independent replications using gaussian error variables from @xmath311 .",
    "top row :   @xmath312 and @xmath313 .",
    "bottom row :  @xmath314 and @xmath315 .",
    "[ fig:12 ] , title=\"fig:\",width=264 ] ) and powers ( @xmath310 ) of the four methods , which are the corrected lrt ( clrt ) , bartlett - box correction ( bbc ) and two least - squares type tests ( st1 and st2 ) , based on 1,000 independent replications using gaussian error variables from @xmath311 .",
    "top row :   @xmath312 and @xmath313 .",
    "bottom row :  @xmath314 and @xmath315 .",
    "[ fig:12 ] , title=\"fig:\",width=264 ]    ) and powers ( @xmath310 ) of the four methods , which are the corrected lrt ( clrt ) , bartlett - box correction ( bbc ) and two least - squares type tests ( st1 and st2 ) , based on 1,000 independent replications using gaussian error variables from @xmath316 with the parameter @xmath202 .",
    "top row :   @xmath312 and @xmath313 .",
    "bottom row :  @xmath314 and @xmath315 .",
    "[ fig:34],title=\"fig:\",width=264 ] ) and powers ( @xmath310 ) of the four methods , which are the corrected lrt ( clrt ) , bartlett - box correction ( bbc ) and two least - squares type tests ( st1 and st2 ) , based on 1,000 independent replications using gaussian error variables from @xmath316 with the parameter @xmath202 .",
    "top row :   @xmath312 and @xmath313 .",
    "bottom row :  @xmath314 and @xmath315 .",
    "[ fig:34],title=\"fig:\",width=264 ] + ) and powers ( @xmath310 ) of the four methods , which are the corrected lrt ( clrt ) , bartlett - box correction ( bbc ) and two least - squares type tests ( st1 and st2 ) , based on 1,000 independent replications using gaussian error variables from @xmath316 with the parameter @xmath202 .",
    "top row :   @xmath312 and @xmath313 .",
    "bottom row :  @xmath314 and @xmath315 .",
    "[ fig:34],title=\"fig:\",width=264 ] ) and powers ( @xmath310 ) of the four methods , which are the corrected lrt ( clrt ) , bartlett - box correction ( bbc ) and two least - squares type tests ( st1 and st2 ) , based on 1,000 independent replications using gaussian error variables from @xmath316 with the parameter @xmath202 .",
    "top row :   @xmath312 and @xmath313 .",
    "bottom row :  @xmath314 and @xmath315 .",
    "[ fig:34],title=\"fig:\",width=264 ]"
  ],
  "abstract_text": [
    "<S> for a multivariate linear model , wilk s likelihood ratio test ( lrt ) constitutes one of the cornerstone tools . </S>",
    "<S> however , the computation of its quantiles under the null or the alternative requires complex analytic approximations and more importantly , these distributional approximations are feasible only for moderate dimension of the dependent variable , say @xmath0 . on the other hand , assuming that the data dimension @xmath1 as well as the number @xmath2 of regression variables are fixed while the sample size @xmath3 grows , several asymptotic approximations are proposed in the literature for wilk s @xmath4 including the widely used chi - square approximation . in this paper , we consider necessary modifications to wilk s test in a high - dimensional context , specifically assuming a high data dimension @xmath1 and a large sample size @xmath3 . </S>",
    "<S> based on recent random matrix theory , the correction we propose to wilk s test is asymptotically gaussian under the null and simulations demonstrate that the corrected lrt has very satisfactory size and power , surely in the large @xmath1 and large @xmath3 context , but also for moderately large data dimensions like @xmath5 or @xmath6 . as a byproduct </S>",
    "<S> , we give a reason explaining why the standard chi - square approximation fails for high - dimensional data . </S>",
    "<S> we also introduce a new procedure for the classical multiple sample significance test in manova which is valid for high - dimensional data .    </S>",
    "<S> [ section ] [ section ] [ section ] [ section ] </S>"
  ]
}