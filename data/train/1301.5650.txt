{
  "article_text": [
    "the main paradigm shift in language modelling more than 20 years ago moved the field from rule - based systems to statistical , learned models .",
    "the most popular such models are based on frequency statistics of short sequences of words , called n - grams .",
    "various smoothing techniques were proposed to solve the fundamental problem of language modelling : most words and combinations of words appear very rarely in language .",
    "in fact , from the point of view of n - gram techniques , language modelling is fundamentally a smoothing or in other words a regularization problem .",
    "more recently , models based on neural networks ( nnlms ) have been shown to result in better representations of language due to their lower dimensional parametrization and higher ability to generalize @xcite .",
    "recurrent nnlms are a subclass of nnlms that achieve even better performance with a clever parametrization of the predictive likelihood function through a recurrent neural network .",
    "however , to yield top perplexity scores , the neural network language models currently still need to be combined with n - gram based models , caching techniques and averaged over an ensemble of different models as shown in @xcite .",
    "furthermore , the rnn based lms already require very long training times individually and can be slow at run time if the average over an ensemble needs to be computed . here",
    "we show that random dropout based regularization @xcite improves the performance of rnn lms on small datasets .",
    "furthermore , a simpler model that we study here , the impulse - response lm ( irlm ) , achieves equal performance to the nonlinear rnn when both are regularized in this manner .",
    "the irlm is similar to the log - bilinear language model @xcite ( lbl ) and can also be seen as a learned long short term memory ( lstm ) network @xcite .",
    "special units in the lstm have a connection strength of 1 to themselves and no connections to the rest of the network , which alleviates the problem of decaying gradients in learning rnns .",
    "our irlm is composed exclusively of these special lstm units , with the generalization that the self - connection strength can be anything between -1 and 1 and it is learned together with the other parameters of the irlm . in practice , the irlm learns to incorporate information from very large contexts of up to 50 words , while also capturing local information .",
    "standard recurrent neural networks are functions of input data .",
    "we consider sequential data that comes in the form of discrete tokens , such that the tokens may for example be either characters or words .",
    "an rnn function takes the following form : @xmath0 where @xmath1 is the data and @xmath2 is the representation computed by the rnn .",
    "in the case of language modelling , @xmath1 is a one - hot encoding of the token in position @xmath3 , meaning @xmath1 is a vector of mostly zeros with just a one in the position of the active token .",
    "we call @xmath4 the encoding matrix and @xmath5 the recurrent or transformation matrix .",
    "@xmath6 is typically taken to be a sigmoidal nonlinearity such as the logistic @xmath7 .",
    "it is generally believed that such strong nonlinearities are necessary to model the apparently complicated dependencies in real - world data .",
    "the disadvantages of sigmoidal nonlinearities will become apparent when we consider the optimization problem below . to make a statistical language model out of an rnn",
    ", we define a soft - max probability over the tokens in the sequence , such that this probability depends only on the representation @xmath2 computed by the rnn : @xmath8 where @xmath9 is a decoding matrix .",
    "we use the terminology of encoding , transformation and decoding matrices for @xmath10 and @xmath9 due to the similarity with methods based on autoencoders , rbms or sparse coding for static data . to obtain the likelihood of a full sequence",
    "we multiply together the conditional probabilities defined by equation [ eq : pyx ] for every index @xmath11 from 1 to the length of the sequence",
    ".    the likelihood of the rnn lm can be optimized by gradient descent .",
    "notice that in order to learn @xmath4 and @xmath5 the gradient has to be backpropagated through time ( bptt ) over successive activations @xmath2 .",
    "the intermediate gradients at @xmath2 which we call @xmath12 can be computed incrementally in the reverse - time direction @xmath13 where `` @xmath14 '' is elementwise multiplication and @xmath15 is the derivative .",
    "this backward pass for bptt has a similar functional form to the forward pass of equation [ eq : pyx ] and the same computational complexity .",
    "it has been observed early on in @xcite that during training the contribution to the gradient of @xmath16 and @xmath17 from future times tends to vanish as it is backpropagated through several time steps .",
    "this can be understood by considering the jacobians @xmath18 of the transformations which the rnn is computing sequentially , and noticing that two main effects alter the size of the gradient / jacobian .",
    "first , the transformation matrix itself generally has eigenvalues less than 1 in absolute value in order to be stable .",
    "consequently , the projection of @xmath19 on the eigenvectors of @xmath5 decays exponentially over time steps , both in the forward direction when computing the likelihood and in the backward direction when computing the gradients , because @xmath5 and @xmath20 have the same eigenvalues .",
    "the second effect comes from the nonlinearity which further multiplies the gradient @xmath12 elementwise by the gradient of @xmath6 . typically during learning the sigmoid",
    "saturates either at 0 or 1 where the derivative @xmath15 is 0 , which drastically reduces the contribution to the gradient from future time steps .",
    "we hypothesized that this second effect has a much greater influence on bptt than the eigenvalues of the transformation matrix @xmath5 .",
    "in fact , with linear rnns where @xmath6 is just the identity , we find that the network easily learns matrices @xmath5 with 20% of their eigenvalues",
    "larger than 0.9 .",
    "we developed the rnn model used here on character - level language modelling where instead of using the rnn to predict words sequentially we use it to predict characters .",
    "rnns can be difficult to train and require many passes through the training data so it is important to have good optimization techniques . in this section",
    "we show that training rnns with very large values of momentum can optimize the difficult cost function associated with character - level language modelling .",
    "such cost functions were previously shown to be hard to optimize by @xcite and @xcite .",
    "the authors of those studies propose instead a hessian - free training method which uses second order information but can be computationally very demanding .",
    "the implementation of @xcite takes five days to train in parallel on eight high - end gpus .",
    "instead we push the momentum term to very high values , such that every gradient update is an effective average over the past approximately one million tokens .",
    "we also take advantage of gpus to run many gradient updates and find that the rnn optimizes to prediction levels comparable to those reported with hessian - free optimization in @xcite on the ` text8 ' dataset , in less than a day on a single high - end gpu .",
    "we use a version of rnn in which the nonlinearity @xmath6 has been replaced with a rectifier function of the form @xmath21 . unlike the sigmoidal nonlinearity ,",
    "rectifier functions have derivative 1 for positive input which means they fully propagate the gradient in equation [ eq : bptt ] .",
    "this architecture is also currently and independently investigated by @xcite , though we were not aware of their results when we ran our own experiments .",
    "we found that the standard rectifier nonlinearity they use there is relatively unstable during learning on character - level problems .",
    "furthermore , the results reported in @xcite for this architecture are well behind the state - of - the - art character level results reported in @xcite with the hessian - free optimized m - rnn of @xcite ( see table [ tab : pennchar ] ) .",
    "we adopted instead a smoothed version of the rectifier nonlinearity which is differentiable everywhere and still 0 when the input is negative @xmath22 and found that this simple smooth nonlinearity can be used stably with large learning rates .",
    "when @xmath23 is made small this function approaches the standard rectifier nonlinearity .",
    "the h - rnn can model highly nonlinear distributions of sequences as shown by its performance on the character - level modelling task .",
    "we also used the rnn in word - level experiments .",
    "there we reverted back to the standard rectifier nonlinearity , as we did not observe the same instabilities .",
    "to our knowledge this is the first published result showing that stochastic gradient descent ( sgd ) learning of nonlinear rnns can achieve the same performance as the second order optimization methods proposed by @xcite and evaluated on the text8 dataset by @xcite .",
    "the training cost of rnn with gradient descent is about 15h on a single high - end gpu card , while the hf - mrnn takes five days on 8 high - end gpus , so we observe a more than 50 fold speedup for the same predictive likelihoods . notice that while a very similar rnn architecture is evaluated in @xcite , their results are worse than ours , owing perhaps to the small size of the network they use ( 512 neurons as opposed to our 2048 ) and the non - smooth version of the rectifier nonlinearity used there .",
    "we did not find it necessary to clip the gradients as done in @xcite , because the very high momentum term smoothed the gradient sufficiently .",
    "although we did observe sharp falls of the cost function a few times during training , these were not associated with large changes in the parameters , and the network recovered with a few parameter updates to its previous value of the cost function .",
    "large momentum seemed however to be crucial for fast learning . without it , the network diverged even with much smaller learning rates .",
    "the analysis of @xcite also suggests momentum as an important technique for training rnns and deep neural networks , although the authors suggests it should be used in conjunction with a special initialization of the rnn as an echo state network @xcite but we did not investigate such an initialization . while we did initialize the parameters of the encoder and decoder matrices to relatively large values , we initialized the recurrent weights close to 0 .    in separate experiments we trained the same rnn on the raw version of the same first 100 mb of wikipedia .",
    "we found that sgd training diverged much more easily in that case and required clipping the gradients as is also done in @xcite . on closer inspection",
    ", we found that all such cases of divergence were associated with very rare events not related to language at all , such as multiple repeats of the same low - probability character like dashes and exclamation marks . with a minimum of preprocessing we removed such events and",
    "the rnn was able to learn successfully from raw wikipedia without clipping gradients in sgd .",
    "more research is needed to clarify the actual geometry of neural networks because as @xcite points out , the classical image of long narrow ravines might be quite wrong .    as a curious fact",
    ", we note that the rnn models we train discover an ingenious solution for maintaining stability during learning .",
    "this is further detailed in the supplemental material .",
    ".45    .results on text8 ( a , b ) , project gutenberg ( c ) and the mrsc ( c , d ) . [ cols=\"<,^\",options=\"header \" , ]     one interesting aspect of the irlm is that we can directly assess how large of a context the network uses by looking at the diagonal matrix @xmath5 .",
    "the effective contribution to @xmath2 from a timelag @xmath24 in the past is @xmath25 which then decays like @xmath26 .",
    "the timescales of the network are defined as @xmath27 , where the division operation is understood elementwise on the diagonal of @xmath5 . for an irlm trained on the penn corpus",
    "we plot these timescales in figure figure [ fig : lcus]a .",
    "one can see that the irlm has learned several very long timescales of up to 50 words .",
    "this reminds of the benefits offered by caching methods to language models : many language models are improved at test time if the predictive probability of the model is interpolated with a unigram model learned exclusively from the previous 50 - 100 words .",
    "we can see that nonetheless most of the timescales of the irlm are relatively short in order to model the local grammar of language .",
    "each experiment on the penn dataset took about three hours to run on a gtx 680 gpu .",
    "we also ran the same experiments on a much larger corpus known as text8 , consisting of the first 100 m characters of wikipedia .",
    "as a preprocessing step we considered only words that appear at least five times in the training set , converting all others to the special @xmath28 token .",
    "this left us with a large vocabulary of 67428 words and a training corpus of 15301600 words .",
    "training rnn models on this much data would require several days for a single experiment so we turned to the fast approximate training method called noise contrastive estimation ( nce ) , proposed by @xcite and adapted for neural language modelling by @xcite .",
    "the reader is advised to read @xcite for full details of the procedure .",
    "briefly , the method involves training a classifier to distinguish between real sampled sequences of words and sequences where one word is sampled from noise distributions , where the classifier is based on the generative model being trained .",
    "the perplexity results that we get with nce trained neural language models are significantly better than those of n - gram models .",
    "in addition , the rnn - lm achieves 10% lower perplexity than irlm on both training and test data , indicating that the rnn has a larger capacity for storing sequences .",
    "we also ran experiments on the training dataset for the microsoft sentence completion challenge @xcite .",
    "this dataset consists of about 500 19th century novels from the project gutenberg database .",
    "the rnn - lm was again able to capture many more patterns in the data compared to the irlm , resulting in a 20% perplexity difference .",
    "we find that dropout regularization no longer helps generalization with this large amount of training data . instead",
    ", the performance of the regularize and non - regularized moedls is almost the same , and much better ( @xmath29 35% ) than the vanilla n - gram model to which comparisons are usually made in the literature ( kneser - ney of order 5 ) .",
    "however , cache models significantly improve n - grams and in this case by a very large margin of @xmath29 25% perplexity .",
    "the neural language models offered improvements of 20 - 30% perplexity over that .",
    "the mrsc challenge consists of 1040 sat - style sentences with blanked words and five possible choices for each blank .",
    "the task is to choose the sentence that makes most sense out of the five possible ones .",
    "humans perform on this task at around @xmath29 90 % correct @xcite .",
    "the training set for language models on this task consists of the project gutenberg novels .",
    "n - gram models perform poorly in the mrsc challenge at around 40 % correct but rnn - lms perform at around 50% , while the lbl model of @xcite obtains 54.7% .",
    "although @xcite reports results for rnn - lms , it might be that the advantage of the lbl model is related to the more efficient training method , thus resulting in more epochs of learning .",
    "we trained both rnn - lms and irlms on the project gutenberg novels using nce , and report in table [ tab : perpgut ] their respective performance .    in agreement with @xcite",
    "we find that the nce - trained rnn - lm performs at @xmath29 50% correct . despite having a much lower perplexity ,",
    "the irlm performs slightly better at 52.5% , though not as well as the lbl model reported in @xcite .",
    "this ordering is highly surprising , because a 20% perplexity gap exists between the rnn - lm and the irlm .",
    "we conjecture that due to its restricted representation the irlm needs to focus more than the rnn on the long range dependencies in language , which help it have better semantic comprehension . to verify our conjecture , we ran an irlm experiment where all the units were initialized to 0.9 .",
    "these units effectively ` see ' about ten words into the past and we call these long context units ( lcus ) . during training , a proportion of about 25% of the units dropped in value to code for short context dependencies but the other units remained above 0.5 .",
    "this model scored 0.55% correct on the mrsc task .",
    "we then designed another irlm in which we enforced lcus to keep large values by always constraining them to lie between 0.7 and 1 .",
    "we used 128 short - context units initialized at 0 and 384 lcus .",
    "after training , we used only the values of the lcus to make predictions about which mrsc sentence is correct by setting the values of the short - context units to 0 .",
    "this is potentially highly disruptive for the normalization constant in each context , so we did not normalize the predictions of the lcus .",
    "the lcus alone achieved 60.8% correct which is a new state - of - the - art result on this task .",
    "after learning , most of the lcus were in the range of 0.7 to 0.9 ( figure [ fig : lcus]b ) , suggesting the relevant timescales analyzed are between 3 and 10 words in the past .",
    "we attempted similar experiments with rnn - lms . to this end",
    "we partitioned the recurrent matrix into two blocks of units and set connections between blocks to 0 .",
    "we initialized connections within the larger block of 384 units with 0.9 on the diagonal and the smaller block of 128 units with all zeros . this way , like the lcus",
    ", the 384 block of the rnn is biased by initialization to keep information from long contexts .",
    "however , it was not possible to enforce this constraint during learning .",
    "the best result we could get using the 384 unit block of the rnn was 55% correct , and only when using very small learning rates for the 384 unit block .",
    "initializing the 384 unit block to an echo state network as proposed in @xcite was even less succesful .",
    "we have presented language modelling experiments with rnns and irlms aimed at evaluating the two models intrinsic regularization properties , storage capacity and ability to capture long contexts . on a small dataset we found that the regularization method ( random dropout ) was far more important than the model used .",
    "the best model was an irlm that scored 102.5 perplexity on the test set , and used several regularization techniques : random dropout , column normalization and annealed learning rates . on large datasets ,",
    "the high capacity of the rnn allows it to store and recognize more patterns in language . nonetheless , on a sentence comprehension challenge that requires integrating information over long contexts we found that the irlm was slightly superior .",
    "in addition , the irlm s simple representation allowed us to use only the long context units for scoring sentences , which resulted in a large boost in accuracy to 60.8% correct .",
    "this represents a new best result on this dataset improving on the 54.7% from @xcite .",
    "while the same information from long contexts might in principle be embedded in the representation of the rnn , it is not straightforward to obtain .",
    "we see that there is reason to develop more easily accessible representations for neural language models and irlm constitues a first step in that direction .",
    "we thank andriy mnih and yoshua bengio for reading versions of this article and providing helpful comments . we also thank andriy for introducing us to neural language models and for suggesting the name irlm and we thank yoshua for pointing out that the nonlinear dynamics of the rnn really should matter for storing patterns and making predictions ."
  ],
  "abstract_text": [
    "<S> neural language models ( lms ) based on recurrent neural networks ( rnn ) are some of the most successful word and character - level lms . </S>",
    "<S> why do they work so well , in particular better than linear neural lms ? </S>",
    "<S> possible explanations are that rnns have an implicitly better regularization or that rnns have a higher capacity for storing patterns due to their nonlinearities or both . here </S>",
    "<S> we argue for the first explanation in the limit of little training data and the second explanation for large amounts of text data . </S>",
    "<S> we show state - of - the - art performance on the popular and small penn dataset when rnn lms are regularized with random dropout . nonetheless , we show even better performance from a simplified , much less expressive linear rnn model without off - diagonal entries in the recurrent matrix . </S>",
    "<S> we call this model an impulse - response lm ( irlm ) . using random dropout , column normalization and annealed learning rates , </S>",
    "<S> irlms develop neurons that keep a memory of up to 50 words in the past and achieve a perplexity of 102.5 on the penn dataset . on two large datasets </S>",
    "<S> however , the same regularization methods are unsuccessful for both models and the rnn s expressivity allows it to overtake the irlm by 10 and 20 percent perplexity , respectively . despite the perplexity gap </S>",
    "<S> , irlms still outperform rnns on the microsoft research sentence completion ( mrsc ) task . </S>",
    "<S> we develop a slightly modified irlm that separates long - context units ( lcus ) from short - context units and show that the lcus alone achieve a state - of - the - art performance on the mrsc task of 60.8% . </S>",
    "<S> our analysis indicates that a fruitful direction of research for neural lms lies in developing more accessible internal representations , and suggests an optimization regime of very high momentum terms for effectively training such models . </S>"
  ]
}