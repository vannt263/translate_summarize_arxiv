{
  "article_text": [
    "this study focuses on significantly improving the computational efficiency of * abc - boost * , a new line of boosting algorithms recently proposed for multi - class classification  @xcite .",
    "boosting  @xcite has been successful in machine learning and industry practice .    in prior studies , _",
    "abc - boost _ has been implemented as * abc - mart *  @xcite and * abc - logitboost *  @xcite .",
    "therefore , for completeness , we first provide a review of * logitboost *  @xcite and * mart * ( multiple additive regression trees )  @xcite .",
    "we denote a training dataset by @xmath4 , where @xmath5 is the number of feature vectors ( samples ) , @xmath6 is the @xmath7th feature vector , and @xmath8 is the @xmath7th class label , where @xmath9 in multi - class classification .    both _ logitboost _",
    "@xcite and _ mart _",
    "@xcite can be viewed as generalizations to the classical logistic regression , which models class probabilities @xmath10 as @xmath11 while logistic regression simply assumes @xmath12 ,  _ logitboost _ and _ mart _ adopt the flexible `` additive model , '' which is a function of @xmath13 terms : @xmath14 where @xmath15 , the base ( weak ) learner , is typically a regression tree .",
    "the parameters , @xmath16 and @xmath17 , are learned from the data , by maximizing the joint likelihood , which is equivalent to minimizing the following _ negative log - likelihood loss _ function : @xmath18 where @xmath19 if @xmath20 and @xmath21 otherwise . for identifiability , @xmath22 , i.e. , the * sum - to - zero * constraint ,",
    "is typically adopted @xcite .",
    "the _ logitboost _ algorithm  @xcite builds the additive model ( [ eqn_f_m ] ) by a greedy stage - wise procedure , using a second - order ( diagonal ) approximation of the loss function ( [ eqn_loss ] ) .",
    "the standard practice is to implement _ logitboost _ using regression trees .",
    "the _ mart _",
    "algorithm  @xcite is a creative combination of gradient descent and newton s method , by using the first - order information of the loss function ( [ eqn_loss ] ) to construct the trees and using both the first- & second - order derivatives to determine the values of the terminal nodes .",
    "therefore , both _",
    "logitboost _ and _ mart _ require the first two derivatives of the loss function ( [ eqn_loss ] ) with respective to the function values @xmath23 .",
    "@xcite used the following derivatives : @xmath24    the recent work named _ robust logitboost _  @xcite is a numerically stable implementation of _",
    "@xcite unified _ logitboost _ and _ mart _ by showing that their difference lies in the tree - split criterion for constructing the regression trees at each boosting iteration .",
    "consider @xmath5 weights @xmath25 , and @xmath5 response values @xmath26",
    ", @xmath27 to @xmath5 , which are assumed to be ordered according to the ascending order of the corresponding feature values .",
    "the tree - split procedure is to find the index @xmath28 , @xmath29 , such that the weighted square error ( se ) is reduced the most if split at @xmath28 .",
    "that is , we seek the @xmath28 to maximize the * gain * : @xmath30\\end{aligned}\\]]where @xmath31 @xcite showed the expression ( [ eqn_gain ] ) can be simplified to be @xmath32 ^ 2}{\\sum_{i=1}^s w_i}+\\frac{\\left[\\sum_{i = s+1}^n z_iw_i\\right]^2}{\\sum_{i = s+1}^{n } w_i}- \\frac{\\left[\\sum_{i=1}^n z_iw_i\\right]^2}{\\sum_{i=1}^n w_i}.\\end{aligned}\\ ] ]    for _ logitboost _ , @xcite used the weights @xmath33 and the responses @xmath34 , i.e. , @xmath35 ^ 2}{\\sum_{i=1}^s p_{i , k}(1-p_{i , k})}+\\frac{\\left[\\sum_{i = s+1}^n \\left(r_{i , k}- p_{i , k}\\right ) \\right]^2}{\\sum_{i = s+1}^{n } p_{i , k}(1-p_{i , k } ) } - \\frac{\\left[\\sum_{i=1}^n \\left(r_{i , k } - p_{i , k}\\right ) \\right]^2}{\\sum_{i=1}^n p_{i , k}(1-p_{i , k})}.\\end{aligned}\\ ] ]    for _ mart _ , @xcite used the weights @xmath36 and the responses @xmath37 , i.e. , @xmath38 ^ 2 + \\frac{1}{n - s}\\left[\\sum_{i = s+1}^n \\left(r_{i , k } - p_{i , k}\\right ) \\right]^2- \\frac{1}{n}\\left[\\sum_{i=1}^n \\left(r_{i , k } - p_{i , k}\\right ) \\right]^2.\\end{aligned}\\ ] ]      1 : @xmath39 , @xmath40 , @xmath41 to @xmath42 , @xmath43 to @xmath5 + 2 : for @xmath44 to @xmath13 do + 3 : for @xmath45 to @xmath42 do + 4 : @xmath46-terminal node regression tree from @xmath47 , with weights @xmath48 as in ( [ eqn_logit_gain ] ) + 5 : @xmath49 + 6 : @xmath50 + 7 : end + 8 : @xmath51 + 9 : end    alg .",
    "[ alg_robust_logitboost ] describes _ robust logitboost _ using the tree - split criterion ( [ eqn_logit_gain ] ) .",
    "in line 6 , @xmath52 is the shrinkage parameter and is normally set to be @xmath53 .",
    "note that after trees are constructed , the values of the terminal nodes are computed by @xmath54 which explains line 5 of alg .",
    "[ alg_robust_logitboost ] .",
    "the _ mart _ algorithm only uses the first derivative to construct the tree .",
    "once the tree is constructed , @xcite applied a one - step newton update to obtain the values of the terminal nodes .",
    "interestingly , this one - step newton update yields exactly the same equation as ( [ eqn_update ] ) . in other words , ( [ eqn_update ] ) is interpreted as weighted average in _ logitboost _ but it is interpreted as the one - step newton update in _",
    "mart_. thus , the _ mart _",
    "algorithm is similar to alg .",
    "[ alg_robust_logitboost ] ; we only need to change line 4 , by replacing ( [ eqn_logit_gain ] ) with ( [ eqn_mart_gain ] ) .",
    "developed by @xcite , the _ abc - boost _ algorithm consists of the following two components :    1 .   using the widely - used sum - to - zero constraint  @xcite on the loss function",
    ", one can formulate boosting algorithms only for @xmath42 classes , by using one class as the * base class*. 2 .   at each boosting iteration , * adaptively * select the base class according to the training loss ( [ eqn_loss ] ) .",
    "@xcite suggested an exhaustive search strategy .",
    "@xcite derived the derivatives of ( [ eqn_loss ] ) under the sum - to - zero constraint . without loss of generality",
    ", we can assume that class 0 is the base class .",
    "for any @xmath55 , @xmath56    @xcite combined the idea of _ abc - boost _ with _ mart _ to develop _ abc - mart _ , which achieved good performance in multi - class classification .",
    "more recently , @xcite developed _ abc - logitboost _ by combining _",
    "abc - boost _ with _ robust logitboost_.      alg .",
    "[ alg_abc - logitboost ] presents _ abc - logitboost _ , using the derivatives in ( [ eqn_abc_d1 ] ) and ( [ eqn_abc_d2 ] ) and the same exhaustive search strategy proposed in  @xcite .",
    "compared to alg .",
    "[ alg_robust_logitboost ] , _ abc - logitboost _ differs from _ ( robust ) logitboost _ in that they use different derivatives and _ abc - logitboost _ needs an additional loop to select the base class at each boosting iteration",
    ".    1 : @xmath39 ,   @xmath40 ,  @xmath41 to @xmath42 ,  @xmath43 to @xmath5 + 2 : for @xmath44 to @xmath13 do + 3 : for @xmath57 to @xmath42 , do + 4 : for @xmath45 to @xmath42 , @xmath58 , do + 5 : @xmath46-terminal node regression tree from @xmath59 with + : weights @xmath60 , in sec .  [ sec_split ] .",
    "+ 6 : @xmath61 +   + 7 : @xmath62 + 8 : end + 9 : @xmath63 + 10 : @xmath64 + 11 : @xmath65 + 12 : end + 13 : @xmath66 + 14 : @xmath67 + 15 : @xmath51 + 16 : end    again , _",
    "abc - logitboost _ differs from _ abc - mart _ only in the tree - split procedure ( line 5 in alg .",
    "[ alg_abc - logitboost ] ) .",
    "@xcite used the hessian matrix , to demonstrate why the choice of the base class matters .    the chose of the base class matters because of the diagonal approximation ; that is , fitting a regression tree for each class at each boosting iteration . to see this",
    ", we can take a look at the hessian matrix , for @xmath68 . using the original logitboost / mart derivatives ( [ eqn_mart_d1d2 ] ) ,",
    "the determinant of the hessian matrix is @xmath69 as expected , because there are only @xmath42 degrees of freedom .",
    "a simple fix is to use the diagonal approximation  @xcite .",
    "in fact , when trees are used as the weak learner , it seems one must use the diagonal approximation .",
    "now , consider the derivatives ( [ eqn_abc_d1 ] ) and ( [ eqn_abc_d2 ] ) used in _ abc - mart _ and _ abc - logitboost_. this time , when @xmath68 and @xmath45 is the base class , we only have a 2 by 2 hessian matrix , whose determinant is @xmath70    which is non - zero and is in fact independent of the choice of the base class ( even though we assume @xmath45 as the base in this example ) . in other words , the choice of the base class would not matter if the full hessian is used .",
    "however , because we will have to use diagonal approximation in order to construct trees at each iteration , the choice of the base class will matter .",
    "we will test * fast abc - boost * using a subset of the datasets in  @xcite , as listed in table [ tab_data ] . because the computational cost of _ abc - boost _ is not a concern for small datasets , this study focuses on fairly large datasets ( _ covertype _ and _ poker _ ) as well as datasets of moderate size ( _ mnist10k _ and _ m - image _ ) .",
    ".datasets [ cols=\"<,>,>,>,>\",options=\"header \" , ]     [ tab_data ]      for these two datasets , @xcite experimented with every combination of @xmath71 and @xmath72 .",
    "the four boosting algorithms were trained till the training loss ( [ eqn_loss ] ) was close to the machine accuracy , to exhaust the capacity of the learners , for reliable comparisons , up to @xmath73 iterations . since no obvious overfitting was observed , the test mis - classification errors at the last iterations were reported .",
    "table  [ tab_mnist10k ] and table  [ tab_m - image ] present the test mis - classification errors , which verify the consistent improvements of ( a ) _ abc - logitboost _ over _ ( robust ) logitboost _ , ( b ) _ abc - logitboost _ over _ abc - mart _ , ( c ) _ ( robust ) logitboost _ over _ mart _ , and ( d ) _",
    "abc - mart _ over _ mart_. the tables also verify that the performances are not too sensitive to the parameters ( @xmath74 and @xmath52 ) .",
    "[ tab_mnist10k ]    [ tab_m - image ]",
    "recall that , in _ abc - boost _ , the base class must be identified at each boosting iteration .",
    "the exhaustive search strategy used in  @xcite is obviously very expensive . in this paper ,",
    "our main contribution is a proposal for speeding up _ abc - boost _ by introducing * gaps * when selecting the base class .",
    "again , we illustrate our strategy using _ abc - mart _ and _ abc - logitboost _ , which are only two implementations of _ abc - boost _ so far .",
    "+ assuming @xmath13 boosting iterations , the computation cost of _ mart _ and _ logitboost _ is @xmath75 .",
    "however , the computation cost of _ abc - mart _ and _ abc - logitboost _ @xmath76 , which can be prohibitive . + the reason we need to select the _ base class _ is because we have to use the the diagonal approximation in order to fit a regression separately for each class at every boosting iteration .",
    "based on this insight , we really do not have to re - compute the base class for every iteration .",
    "instead , we only compute the base class for every @xmath0 steps , where @xmath0 is the _ gap _ and @xmath1 means we select the base class for every iteration . + after introducing _ gaps _ , the computation cost of _ fast abc - boost _ is reduced to @xmath77 .",
    "one can verify that when @xmath78 , the cost of _ fast abc - boost _ is at most twice as the cost of _ logitboost_. as we increases @xmath0 more , the additional computational overhead of _ fast abc - boost _ further diminishes .",
    "+ the parameter @xmath0 can be viewed as a new tuning parameter . our experiments ( in the following subsections )",
    "illustrate that when @xmath79 ( or @xmath3 ) , there would be no obvious loss of test accuracies in large datasets ( or moderate datasets ) .      as presented in  @xcite , on the _ poker _ dataset , _ abc - boost _ achieved very remarkable improvements over _ mart _ and _ logitboost _ , especially when the number of boosting iterations was not too large .",
    "in fact , even at @xmath80 iterations , the mis - classification error of _ mart _ ( or _ ( robust ) logitboost _ ) is 3 times ( or 1.5 times ) as large as the error of _ abc - mart _ ( or _ abc - logitboost _ ) ; see the rightmost panel of figure  [ fig_poker525k ] .    for all datasets , we experiment with @xmath1 ( i.e. , the original _ abc - boost _ ) , @xmath81 . as shown in figure",
    "[ fig_poker525k ] , using _ fast abc - boost _ with @xmath79 , there is no obvious loss of test accuracies on _ poker525k_. in fact , using _ abc - mart _ , even with @xmath82 , there is only very little loss of accuracy .",
    "+ note that it is possible for _ fast abc - boost _ to achieve smaller test errors than _ abc - boost _ ; for example , the ratios of test errors in the right panel of figure  [ fig_poker525k ] may be below 1.0 .",
    "this interesting phenomenon is not surprising .",
    "after all , @xmath0 can be viewed as tuning parameter and using @xmath83 may have some _ regularization _ effect because that would be less greedy . + figure  [ fig_poker275k ] presents the test error results on _ poker275k _ , which are very similar to the results on _ poker525k_.    figure  [ fig_covertype290k ] presents the test error results on _",
    "covertype290k_. for this dataset , even with @xmath82 , we notice essentially no loss of test accuracies .",
    "the situation is somewhat different on datasets that are not too large .",
    "recall , for these two datasets , we terminate the training if the training loss ( [ eqn_loss ] ) is to close to the machine accuracy , up to @xmath73 iterations .",
    "figure  [ fig_mnist_imgj20 ] and figure  [ fig_mnist10kj20 ] show that , on _ m - image _ and _ mnist10k _ , using _ fast abc - boost _ with @xmath84 can result in non - negligible loss of test accuracies compared to using @xmath1 . when @xmath0 is too large , e.g. , @xmath82 , it is possible that _ fast abc - boost _ may produce even larger test errors than _ mart _ or _",
    "logitboost_.    figure  [ fig_mnist_imgj20 ] and figure  [ fig_mnist10kj20 ] report the test errors for @xmath85 and two shrinkages , @xmath86 .",
    "it seems that , at the same @xmath0 , using smaller @xmath52 produces slightly better results .",
    "the above experiments always use @xmath85 , which seems to be a reasonable number of terminal tree nodes for large or moderate datasets .",
    "nevertheless , it would be interesting to experiment with other @xmath74 values .",
    "figure  [ fig_mnist10k ] presents the results on the _ mnist10k _",
    "dataset , for @xmath87 .    when @xmath74 is small ( e.g. , @xmath88 ) , using @xmath0 as large as 100 results in almost no loss of test accuracies .",
    "however , when @xmath74 is large ( e.g. , @xmath89 ) , even with @xmath90 may produce obviously less accurate results compared to @xmath1 .",
    "this study proposes _ fast abc - boost _ to significantly improve the training speed of _ abc - boost _ , which suffered from serious problems of computational efficiency . _",
    "abc - boost _ is a new line of boosting algorithms for improving multi - class classification , which was implemented as _ abc - mart _ and _ abc - logitboost _ in prior studies . _",
    "abc - boost _ requires that a _ base class _ must be identified at each boosting iteration .",
    "the computation of the base class was based on an expensive exhaustive search strategy in prior studies .    with _ fast abc - boost _",
    ", we only need to update the choice of the _ base class _ once for every @xmath0 iterations , where @xmath0 can be viewed as _ gaps _ and used as an additional tuning parameter .",
    "our experiments on fairly large datasets show that the test errors are not sensitive to the choice of @xmath0 , even with @xmath91 or 1000 . for datasets of moderate size , our experiments show that , when @xmath3 , there would be no obvious loss of test accuracies compared to the original _ abc - boost _ algorithms ( i.e. , @xmath1 ) ."
  ],
  "abstract_text": [
    "<S> * abc - boost * is a new line of boosting algorithms for multi - class classification , by utilizing the commonly used * sum - to - zero * constraint . to implement _ abc - boost _ </S>",
    "<S> , a * base class * must be identified at each boosting step . </S>",
    "<S> prior studies used a very expensive procedure based on exhaustive search for determining the base class at each boosting step . </S>",
    "<S> good testing performance of _ abc - boost _ ( implemented as * abc - mart * and * abc - logitboost * ) on a variety of datasets was reported .    for large datasets , however , the exhaustive search strategy adopted in prior _ abc - boost _ algorithms can be too prohibitive . to overcome this serious limitation , </S>",
    "<S> this paper suggests a heuristic by introducing * gaps * when computing the base class during training . </S>",
    "<S> that is , we update the choice of the base class only for every @xmath0 boosting steps ( i.e. , @xmath1 in prior studies ) . </S>",
    "<S> we test this idea on large datasets ( _ covertype _ and _ poker _ ) as well as datasets of moderate size . </S>",
    "<S> our preliminary results are very encouraging . on the large datasets , </S>",
    "<S> when @xmath2 ( or even larger ) , there is essentially no loss of test accuracy compared to using @xmath1 . on the moderate datasets , no obvious loss of test accuracy </S>",
    "<S> is observed when @xmath3 . </S>",
    "<S> therefore , aided by this heuristic of using _ gaps _ , it is promising that _ abc - boost _ will be a practical tool for accurate multi - class classification . </S>"
  ]
}