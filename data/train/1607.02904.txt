{
  "article_text": [
    "molecular dynamics ( md ) simulations trace the trajectory of thousands , millions , and even billions of particles over millions of timesteps , enabling materials science research at the atomistic level .",
    "such simulations are commonly run on highly parallel architectures , and take up a sizeable portion of the computing cycles provided by today s supercomputers . in this paper , we extend the lammps molecular dynamics simulator  @xcite with a new , optimized and portable implementation of the tersoff multi - body potential  @xcite .",
    "+ in many simulations , interactions among particles are assumed to occur in a pair - wise fashion ( particle - to - particle ) , as dictated by potentials such as the coulomb or lennard - jones ones . however , several applications in materials science require multi - body potential formulations @xcite . with these ,",
    "the force between two particles does not depend solely on their distance , but also on the relative position of surrounding particles .",
    "this added degree of freedom in the potential enables more accurate modeling but comes at the cost of additional complexity in its evaluation . because of this complexity , the optimization of multi - body potential is still largely unexplored .    by design ,",
    "lammps main parallelization scheme is an mpi - based domain decomposition ; this makes it possible to run on clusters and supercomputers alike , and to tackle large scale systems .",
    "optionally , lammps can also take advantage of shared - memory parallelism via openmp . however , support for vectorization is limited to intel architectures and only to a few kernels .",
    "given the well - established and well - studied mechanisms for parallelism in md codes  @xcite , our efforts mostly focus on vectorization , as a further step to fully exploit the computational power of the available hardware .",
    "on current architectures , simd processing contributes greatly to the system s peak performance ; simd units offer hardware manufacturers a way to multiply performance for compute - intense applications .",
    "this principle is most pronounced in accelerators such as intel s xeon phis and nvidia s teslas .",
    "several successful open - source md codes",
    "gromacs  @xcite , namd , lammps  @xcite , and ls1 mardyn  @xcite  already take advantage of vectorization in their most central kernels",
    ". however , these vectorized kernels usually do not include multi - body potentials .",
    "implementation methods for the kernels vary between hand - written assembly , intrinsics ( compiler - provided functions that closely map to machine instructions ) , and annotations that guide the compiler s optimization @xcite .",
    "furthermore , the majority of these kernels are not readily portable to different architectures ; on the contrary , for each target architecture , separate optimizations are required .",
    "our objective is an approach sufficiently general to attain high performance on a wide variety of architectures , and that requires changes localized in few , general building blocks .",
    "we identified such a solution for the tersoff potential .",
    "when transitioning from one architecture to another , the numerical algorithm stays fixed , and only the interface to the surrounding software components ( memory management , pre - processing ) needs to be tailored .",
    "we demonstrate our approach for performance portability on a range of architectures , including arm , westmere to broadwell , nvidia s teslas ( from the kepler generation ) , and two generations of intel xeon phi ( knights corner and knights landing ) . as already mentioned , the numerical algorithm",
    " built on top of platform - specific building blocks  stays the same across all architectures ; it is the building blocks that are implemented ( once and for all ) for each of the instruction sets .",
    "our evaluation ranges from single - threaded to a cluster of nodes containing two xeon phi s . depending on architecture and benchmark , we report speedups ranging from 2x to 8x .",
    "[ [ related - work ] ] related work + + + + + + + + + + + +    in addition to the aforementioned lammps , several other md codes are available , including gromacs  @xcite , namd  @xcite , dl_poly2  @xcite , and ls1_mardyn  @xcite .",
    "gromacs is well known for its single - threaded performance , provides support for the xeon phi  @xcite , and already contains a highly portable scheme for the vectorization of pair potentials  @xcite .",
    "all the other softwares also contain routines specific to certain platforms such as the phi  @xcite or cuda .",
    "lammps @xcite is a simulator written in c++ and designed to favor extensibility .",
    "it excels at scalability using mpi , and comes with a number of optional packages to run on a different platforms with different parallel paradigms .",
    "lammps supports openmp shared - memory programming , gpus  @xcite , kokkos  @xcite , and vector - enabled intel hardware  @xcite .",
    "the vectorization of pair - potential calculations was specifically addressed in an md application called minimd  @xcite ( proxy  @xcite for the short - ranged portion of lammps ) .",
    "the target of that work are various x86 instruction set extensions , including the xeon phi s imci , and the optimization of cache access patterns",
    ".    there have been efforts to speed up pair potentials on gpus  @xcite ; these techniques are similar to what one would use to achieve vectorization , and the pattern of communication between the gpu and the host is similar to what is needed to achieve high - performance with the first generation of the xeon phi . in general , gpus have been used to great effect for speeding up md simulations  @xcite .",
    "there exist implementations for gpus that support multi - body potentials such as eam  @xcite , stillinger - weber  @xcite and tersoff  @xcite .",
    "as opposed to our work , the tersoff implementation for the gpu requires explicit neighbor assignments and thus is only suitable for rigid problems ; by constrast , our approach is suitable for general scenarios .    in imd ,",
    "scalar optimizations similar to those we describe here are implemented  @xcite ; however , no sort of vectorization is included .",
    "our work is in part based on previous efforts to port md simulations , and lammps in particular , to the xeon phi  @xcite ; specifically , we use the same data and computation management scheme .",
    "[ [ organization - of - the - paper ] ] organization of the paper + + + + + + + + + + + + + + + + + + + + + + + + +    in sec .  [",
    "sec : md - over ] , we give a quick introduction to md simulations in general , while a discussion specific to the tersoff potential and its computational challenges comes in sec .",
    "[ sec : ters ] .",
    "we introduce our optimizations and vectorization schemes in sec .",
    "[ sec : opt ] , and then describe the techniques to achieve portability in sec .",
    "[ sec : impl ] . in sec .",
    "[ sec : res ] , we provide the results of our multi - platform evaluations , and in sec .",
    "[ sec : conc ] we draw conclusions .    [",
    "[ open - source ] ] open source + + + + + + + + + + +    the associated code is available at @xcite .",
    "a typical md simulation consists of a set of atoms ( particles ) and a sequence of timesteps . at each timestep ,",
    "the forces for each atom are calculated , and velocity and position are updated accordingly .",
    "the forces are modeled by a potential @xmath0 that depends solely on the positions @xmath1 of each atom @xmath2 .",
    "@xmath3 represents the potential energy of the system ; the force on an atom @xmath2 is then the negative derivative of @xmath3 with respect to the atom s position @xmath1 @xcite : @xmath4    most non - bonded potentials , such as lennard - jones and coulomb , are pair potentials : as such , they can be expressed as a double sum over the atoms , where the additive term depends only on the relative distance between the atoms : @xmath5    in practice , eq .",
    "[ eqn : pair - pot ] is computed by limiting the inner summation ( @xmath6 ) only to the set of atoms @xmath7known as the `` neighbor list''that are within a certain distance @xmath8 from atom @xmath2 : @xmath9 @xmath10 this simplification is based on the assumption that @xmath11 goes to zero as the distance @xmath12 increases .",
    "the assumption is valid for most pair potentials , even though some have to be augmented using long - ranged calculation schemes . with this second formulation , the complexity for the computation of @xmath3 decreases from quadratic ( in the number of atoms ) to linear , thus making large - scale simulations feasible .",
    "algorithm  [ algo : general - pair ] illustrates how , based on a potential @xmath3 as given in eq .",
    "[ eqn : pot - neigh ] , the potential energy @xmath3 and the forces @xmath13 on each atom @xmath2 can be evaluated .",
    "as opposed to pair potentials , multi - body potentials deviate from the form of eq .",
    "[ eqn : pair - pot ] . in particular ,",
    "@xmath11 is replaced by a term that depends on more than just the distance @xmath14 .",
    "instead , it might depend also on the distance of other atoms close to atom @xmath2 or @xmath6 , and on the angle between @xmath2 , @xmath6 and the surrounding atoms .    omitting trivial definitions , the tersoff potential  @xcite",
    "is defined as follows : @xmath15}^{v(i , j , \\zeta_{ij})},\\label{eqn : ters-1}\\\\ b_{ij } & = \\vphantom{\\sum_{r_j}}(1 + \\beta^\\eta\\zeta_{ij}^\\eta)^{-\\frac{1}{2\\eta } } , \\eta\\in\\mathbb{r}\\label{eqn : ters-2},\\\\ \\zeta_{ij } & = \\vphantom{\\sum_{r_j}}\\sum_{k\\in\\mathcal{n}_i\\setminus\\{j\\ } } \\underbrace{f_c(r_{ik } ) g(\\theta_{ijk } ) \\exp(\\lambda_3 ( r_{ij } - r_{ik}))}_{\\zeta(i , j , k)}.\\label{eqn : ters-3}\\end{aligned}\\ ] ] eq .",
    "[ eqn : ters-1 ] indicates that two forces act between each pair of atoms @xmath16 : an attractive force modeled by @xmath17 , and a repulsive force modeled by @xmath18 .",
    "both depend only on the distance @xmath14 between atom @xmath2 and atom @xmath6 .",
    "the bond - order factor @xmath19 , defined by eq .",
    "[ eqn : ters-2 ] , however , is a scalar that depends on all the other atoms @xmath20 in the neighbor list of atom @xmath2 , by means of their distance @xmath21 , and angle @xmath22 via @xmath23 ( from eq .",
    "[ eqn : ters-3 ] ) .",
    "since the contribution of the @xmath16 pair depends on other atoms @xmath20 , tersoff is a multi - body potential .",
    "@xmath24 is a cutoff function , smoothly transitioning from 1 to 0 ; @xmath25 describes the influence of the angle on the potential ; all other symbols in eq .",
    "[ eqn : ters-1][eqn : ters-3 ] are parameters that were empirically determined by fitting to known properties of the modelled material . although these parameters mean that many lookups are necessary , the functions within the potential ( @xmath26 ) are expensive to compute , thus making the tersoff potential a good target for vectorization .",
    "[ eqn : ters-1][eqn : ters-3 ] give rise to a triple summation ; this is mirrored by the triple loop structure of algorithm  [ algo : ters - lammps ] , which describes the implementation found in lammps in terms of the functions @xmath27 and @xmath28 , and calculates forces @xmath29 and potential energy @xmath30 : for all @xmath16 pairs of atoms , first @xmath23 is accumulated , and then the forces are updated in two stages , first with the contribution of the @xmath31 term , and finally with the contributions of the @xmath28 terms .    for the following discussions , it is important to keep in mind the loop structure of algorithm  [ algo : ters - lammps ] : it consists of an outer loop over all atoms ( denoted by the capital letter @xmath32 ) , an inner loop over a neighbor list ( denoted by the capital letter @xmath33 ) , and inside the latter , two more loops over the same neighbor list ( denoted by the capital letter @xmath34 ) .    as opposed to pair potentials ,",
    "many - body potentials are used with extremely short neighbor lists @xmath7 . in a representative simulation run ,",
    "@xmath7 rarely contains more than four atoms . assuming that the size of @xmath7 is @xmath35 , the algorithm accesses the atoms in @xmath7 a total of @xmath36 times . in practice , constructing the neighbor list on every timestep would be too expensive . instead",
    ", the cutoff radius @xmath8 is extended by a so - called `` skin distance '' .",
    "because atoms only move a certain distance per timestep , one can guarantee that no atom enters or exits the cutoff region for a certain number of timesteps by tracking all atoms also within the skin distance .",
    "consequently , the neighbor list also only needs to be rebuild after this many steps .",
    "we denote the extended neighbor list by @xmath37 instead of @xmath7 .",
    "given that the tersoff potential incorporates a cutoff function @xmath24 , the mathematical formulation is equivalent no matter if iterating through @xmath7 or @xmath37 .",
    "nevertheless , as little computation as possible should be performed on skin atoms . efficiently excluding skin atoms",
    "is one of the major challenges for vectorization .",
    "this section discusses the various optimizations that we applied to the algorithm described in the previous section .",
    "some are inherited from the libraries that we integrate with ( user - intel and kokkos ) , such as optimized neighbor list build , time integration , and data access ( e.g. alignment , packing , atomics ) .",
    "these optimizations are generic in that they apply to any potential that uses that particular package .",
    "we devised several other optimizations which are instead specific to the tersoff potential ; they are detailed here .    1 .   scalar optimizations",
    ". these improvements are useful whether one vectorizes or not .",
    "we improve parameter lookup by reducing indirection , and eliminate redundant calculation by inlining function calls .",
    "this group of optimizations also includes the method described in sec .",
    "[ ssec : prec ] , which aims to remove redundant calculations of the @xmath38 term .",
    "2 .   vectorization .",
    "we discuss details of our vectorization strategy in sec .",
    "[ ssec : vect ] , where we present different schemes , and describe their effectiveness for various vector lengths .",
    "optimizations that aid vectorization . as described in sec .",
    "[ ssec : avoid - mask ] and  [ ssec : fil ] , we aim at reduce the waste of computing resources on skin atoms .",
    "the first optimization we discuss consists in restructuring the algorithm so that @xmath38 and its derivatives are computed only once , in the first loop , and the product with @xmath39 is only performed in the second loop . since @xmath38 and its derivatives  naturally  share terms , this modification has a measurable impact on performance .",
    "indeed , @xmath38 can be calculated from intermediate results of the derivative evaluation at the cost of just one additional multiplication .",
    "however , the computation of the derivatives in the first loop over @xmath20 requires additional storage : while the derivatives with respect to the positions of atoms @xmath2 and @xmath6 can be accumulated , the derivatives for @xmath20 have to be stored separately , as they belong to different @xmath20 s . in our implementation , this list can contain up to a specified number @xmath40 of elements .",
    "should more than @xmath40 elements be necessary , the algorithm falls back to the original scheme , thus maintaining complete generality .",
    "algorithm  [ algo : deriv ] implements this idea .    ",
    "@xmath41 ; @xmath42 ; @xmath43 ; @xmath44 @xmath45     @xmath46 @xmath47          .3     ( 1,4 ) rectangle ( 1 + 1,4 + 1 ) ; ( 1.1 , 4.1 )  ( 1.9 , 4.9 ) ; at ( 1.3 , 4.7 ) @xmath48 ; at ( 1.7 , 4.3 ) @xmath49 ; ( 2,4 ) rectangle ( 2 + 1,4 + 1 ) ; ( 2.1 , 4.1 )  ( 2.9 , 4.9 ) ; at ( 2.3 , 4.7 ) @xmath48 ; at ( 2.7 , 4.3 ) @xmath50 ; ( 3,4 ) rectangle ( 3 + 1,4 + 1 ) ; ( 3.1 , 4.1 )  ( 3.9 , 4.9 ) ; at ( 3.3 , 4.7 ) @xmath48 ; at ( 3.7 , 4.3 ) @xmath51 ; ( 4,4 ) rectangle ( 4 + 1,4 + 1 ) ; ( 4.1 , 4.1 )  ( 4.9 , 4.9 ) ; at ( 4.3 , 4.7 ) @xmath48 ; at ( 4.7 , 4.3 ) @xmath52 ;     ( 1,3 ) rectangle ( 1 + 1,3 + 1 ) ; ( 1.1 , 3.1 )  ( 1.9 , 3.9 ) ; at ( 1.3 , 3.7 ) @xmath53 ; at ( 1.7 , 3.3 ) @xmath49 ; ( 2,3 ) rectangle ( 2 + 1,3 + 1 ) ; ( 2.1 , 3.1 )  ( 2.9 , 3.9 ) ; at ( 2.3 , 3.7 ) @xmath53 ; at ( 2.7 , 3.3 ) @xmath50 ; ( 3,3 ) rectangle ( 3 + 1,3 + 1 ) ; ( 3.1 , 3.1 )  ( 3.9 , 3.9 ) ; at ( 3.3 , 3.7 ) @xmath53 ; at ( 3.7 , 3.3 ) @xmath51 ; ( 4,3 ) rectangle ( 4 + 1,3 + 1 ) ; ( 4.1 , 3.1 )  ( 4.9 , 3.9 ) ; at ( 4.3 , 3.7 ) @xmath53 ; at ( 4.7 , 3.3 ) @xmath52 ;     ( 1,2 ) rectangle ( 1 + 1,2 + 1 ) ; ( 1.1 , 2.1 )  ( 1.9 , 2.9 ) ; at ( 1.3 , 2.7 ) @xmath54 ; at ( 1.7 , 2.3 ) @xmath49 ; ( 2,2 ) rectangle ( 2 + 1,2 + 1 ) ; ( 2.1 , 2.1 )  ( 2.9 , 2.9 ) ; at ( 2.3 , 2.7 ) @xmath54 ; at ( 2.7 , 2.3 ) @xmath50 ; ( 3,2 ) rectangle ( 3 + 1,2 + 1 ) ; ( 3.1 , 2.1 ) ",
    "( 3.9 , 2.9 ) ; at ( 3.3 , 2.7 ) @xmath54 ; at ( 3.7 , 2.3 ) @xmath51 ; ( 4,2 ) rectangle ( 4 + 1,2 + 1 ) ; ( 4.1 , 2.1 )  ( 4.9 , 2.9 ) ; at ( 4.3 , 2.7 ) @xmath54 ; at ( 4.7 , 2.3 ) @xmath52 ;     ( 1,1 ) rectangle ( 1 + 1,1 + 1 ) ; ( 1.1 , 1.1 )  ( 1.9 , 1.9 ) ; at ( 1.3 , 1.7 ) @xmath55 ; at ( 1.7 , 1.3 ) @xmath49 ; ( 2,1 ) rectangle ( 2 + 1,1 + 1 ) ; ( 2.1 , 1.1 )  ( 2.9 , 1.9 ) ; at ( 2.3 , 1.7 ) @xmath55 ; at ( 2.7 , 1.3 ) @xmath50 ; ( 3,1 ) rectangle ( 3 + 1,1 + 1 ) ; ( 3.1 , 1.1 ) ",
    "( 3.9 , 1.9 ) ; at ( 3.3 , 1.7 ) @xmath55 ; at ( 3.7 , 1.3 ) @xmath51 ; ( 4,1 ) rectangle ( 4 + 1,1 + 1 ) ; ( 4.1 , 1.1 )  ( 4.9 , 1.9 ) ; at ( 4.3 , 1.7 ) @xmath55 ; at ( 4.7 , 1.3 ) @xmath52 ;    ( 0.8,1.0 )  ( 0.8,5.0 ) node [ black , midway , xshift=-0.6cm , rotate=90 ] parallelism ; ( 1.0,5.2 )  ( 5.0,5.2 ) node [ black , midway , yshift=0.6 cm ] vectorization ;    .3     ( 1,4 ) rectangle ( 1 + 1,4 + 1 ) ; ( 1.1 , 4.1 )  ( 1.9 , 4.9 ) ; at ( 1.3 , 4.7 ) @xmath48 ; at ( 1.7 , 4.3 ) @xmath49 ; ( 2,4 ) rectangle ( 2 + 1,4 + 1 ) ; ( 2.1 , 4.1 )  ( 2.9 , 4.9 ) ; at ( 2.3 , 4.7 ) @xmath48 ; at ( 2.7 , 4.3 ) @xmath50 ; ( 3,4 ) rectangle ( 3 + 1,4 + 1 ) ; ( 3.1 , 4.1 )  ( 3.9 , 4.9 ) ; at ( 3.3 , 4.7 ) @xmath53 ; at ( 3.7 , 4.3 ) @xmath49 ; ( 4,4 ) rectangle ( 4 + 1,4 + 1 ) ; ( 4.1 , 4.1 )  ( 4.9 , 4.9 ) ; at ( 4.3 , 4.7 ) @xmath53 ; at ( 4.7 , 4.3 ) @xmath50 ;     ( 1,3 ) rectangle ( 1 + 1,3 + 1 ) ; ( 1.1 , 3.1 )  ( 1.9 , 3.9 ) ; at ( 1.3 , 3.7 ) @xmath54 ; at ( 1.7 , 3.3 ) @xmath49 ; ( 2,3 ) rectangle ( 2 + 1,3 + 1 ) ; ( 2.1 , 3.1 )  ( 2.9 , 3.9 ) ; at ( 2.3 , 3.7 ) @xmath54 ; at ( 2.7 , 3.3 ) @xmath50 ; ( 3,3 ) rectangle ( 3 + 1,3 + 1 ) ; ( 3.1 , 3.1 ) ",
    "( 3.9 , 3.9 ) ; at ( 3.3 , 3.7 ) @xmath54 ; at ( 3.7 , 3.3 ) @xmath51 ; ( 4,3 ) rectangle ( 4 + 1,3 + 1 ) ; ( 4.1 , 3.1 )  ( 4.9 , 3.9 ) ; at ( 4.3 , 3.7 ) @xmath55 ; at ( 4.7 , 3.3 ) @xmath49 ;     ( 1,2 ) rectangle ( 1 + 1,2 + 1 ) ; ( 1.1 , 2.1 )  ( 1.9 , 2.9 ) ; at ( 1.3 , 2.7 ) @xmath56 ; at ( 1.7 , 2.3 ) @xmath49 ; ( 2,2 ) rectangle ( 2 + 1,2 + 1 ) ; ( 2.1 , 2.1 )  ( 2.9 , 2.9 ) ; at ( 2.3 , 2.7 ) @xmath57 ; at ( 2.7 , 2.3 ) @xmath49 ; ( 3,2 ) rectangle ( 3 + 1,2 + 1 ) ; ( 3.1 , 2.1 )  ( 3.9 , 2.9 ) ; at ( 3.3 , 2.7 ) @xmath57 ; at ( 3.7 , 2.3 ) @xmath50 ; ( 4,2 ) rectangle ( 4 + 1,2 + 1 ) ; ( 4.1 , 2.1 )  ( 4.9 , 2.9 ) ; at ( 4.3 , 2.7 ) @xmath58 ; at ( 4.7 , 2.3 ) @xmath49 ;     ( 1,1 ) rectangle ( 1 + 1,1 + 1 ) ; ( 1.1 , 1.1 ) ",
    "( 1.9 , 1.9 ) ; at ( 1.3 , 1.7 ) @xmath59 ; at ( 1.7 , 1.3 ) @xmath49 ; ( 2,1 ) rectangle ( 2 + 1,1 + 1 ) ; ( 2.1 , 1.1 )  ( 2.9 , 1.9 ) ; at ( 2.3 , 1.7 ) @xmath59 ; at ( 2.7 , 1.3 ) @xmath50 ; ( 3,1 ) rectangle ( 3 + 1,1 + 1 ) ; ( 3.1 , 1.1 ) ",
    "( 3.9 , 1.9 ) ; at ( 3.3 , 1.7 ) @xmath59 ; at ( 3.7 , 1.3 ) @xmath51 ; ( 4,1 ) rectangle ( 4 + 1,1 + 1 ) ; ( 4.1 , 1.1 )  ( 4.9 , 1.9 ) ; at ( 4.3 , 1.7 ) @xmath59 ; at ( 4.7 , 1.3 ) @xmath52 ;    ( 0.8,1.0 )  ( 0.8,5.0 ) node [ black , midway , xshift=-0.6cm , rotate=90 ] parallelism ; ( 1.0,5.2 )  ( 5.0,5.2 ) node [ black , midway , yshift=0.6 cm ] vectorization ;    .3     ( 1,4 ) rectangle node @xmath48 ( 1 + 1,4 + 1 ) ; ( 2,4 ) rectangle node @xmath53 ( 2 + 1,4 + 1 ) ; ( 3,4 ) rectangle node @xmath54 ( 3 + 1,4 + 1 ) ; ( 4,4 ) rectangle node @xmath55 ( 4 + 1,4 + 1 ) ;     ( 1,3 ) rectangle node @xmath56 ( 1 + 1,3 + 1 ) ; ( 2,3 ) rectangle node @xmath57 ( 2 + 1,3 + 1 ) ; ( 3,3 ) rectangle node @xmath58 ( 3 + 1,3 + 1 ) ; ( 4,3 ) rectangle node @xmath59 ( 4 + 1,3 + 1 ) ;     ( 1,2 ) rectangle node @xmath60 ( 1 + 1,2 + 1 ) ; ( 2,2 ) rectangle node @xmath61 ( 2 + 1,2 + 1 ) ; ( 3,2 ) rectangle node @xmath62 ( 3 + 1,2 + 1 ) ; ( 4,2 ) rectangle node @xmath63 ( 4 + 1,2 + 1 ) ;     ( 1,1 ) rectangle node @xmath64 ( 1 + 1,1 + 1 ) ; ( 2,1 ) rectangle node @xmath65 ( 2 + 1,1 + 1 ) ; ( 3,1 ) rectangle node @xmath66 ( 3 + 1,1 + 1 ) ; ( 4,1 ) rectangle node @xmath67 ( 4 + 1,1 + 1 ) ;    ( 0.8,1.0 )  ( 0.8,5.0 ) node [ black , midway , xshift=-0.6cm , rotate=90 ] parallelism ; ( 1.0,5.2 )  ( 5.0,5.2 ) node [ black , midway , yshift=0.6 cm ] vectorization ;    in alg .",
    "[ algo : deriv ] ( and alg .",
    "[ algo : ters - lammps ] ) the iteration space ultimately is three - dimensional , corresponding to the three nested loops @xmath32 , @xmath33 and @xmath34 .",
    "this space needs to be mapped onto the available execution schemes , that is , data parallelism , parallel execution , and sequential execution .",
    "we propose three different mappings that are useful in different scenarios .",
    "for all of them , it is convenient to map the @xmath34 dimension onto sequential execution , because values calculated in the @xmath34 loop , the @xmath38 s , are used in the surrounding @xmath33 loop , and data computed in the @xmath33 loop , i.e. @xmath39 , is then used within the second @xmath34 loop .",
    "therefore , the problem boils down to mapping the @xmath32 and @xmath33 dimensions onto a combination of parallel execution , data parallelism , and if necessary , sequential execution . in our reasoning",
    ", we assume that the amount of available data - parallelism is unlimited .",
    "in practice , the program sequentially executes chunks , and each chunk takes advantage of data parallelism .    as shown in fig .",
    "[ fig : map ] , to perform the mapping sensibly we propose three schemes :    * @xmath32 is mapped to parallel execution , and @xmath33 to data parallelism .",
    "* @xmath32 is mapped to parallel execution , and @xmath32 and @xmath33 to data parallelism . *",
    "@xmath32 is mapped to parallel execution and data parallelism , and @xmath33 to sequential execution .",
    "scheme ( [ sfig : vec-1 ] ) is natural for vector architectures with short vectors , such as single precision sse and double precision avx . in these",
    ", it makes sense to map the @xmath6 iterations directly to vector lanes , as there is a good match among them : 3 - 4 iterations to 4 vector lanes .",
    "this scheme is most commonly used to vectorize pair potentials .",
    "the advantage of this approach is that the atom @xmath2 is constant across all lanes . while performing iterations in @xmath20 through the neighbor list of atom @xmath2 , the same neighbor list is traversed across all lanes , leading to an efficient vectorization .",
    "however , with long vectors and short neighbor lists , this approach is destined to fail on accelerators and cpus with long vectors .",
    "scheme ( [ sfig : vec-2 ] ) is best suited for vector widths ( 8 or 16 ) that exceed the iteration count of @xmath6 , as it handles the shortcomings of ( [ sfig : vec-2 ] ) . with this approach , iterations of @xmath2 and @xmath6",
    "are fused , and the fused loop is used for data parallelism . given that @xmath2 contains many iterations ( as many as atoms in the system ) , this scheme achieves an unlimited potential for data parallelism .",
    "however , in contrast to ( [ sfig : vec-1 ] ) , atom @xmath2 is not constant across all lanes ; consequently , the innermost loops iterates over the neighbor lists of different @xmath2 , leading to a more involved iteration scheme .",
    "even if this iteration is efficient , it can not attain the same performance of an iteration scheme where all vector lanes iterate over the same neighbor list .",
    "the vectorization of the @xmath2 loop invalidated a number of assumptions of the algorithm : @xmath2 and @xmath20 are always identical across all lanes , while @xmath6 s , coming from the same neighbor list , are always distinct . without these assumptions , special care has to be taken when accumulating the forces to avoid conflicts . for the program to be correct under all circumstances ,",
    "the updates have to be serialized . in the future ,",
    "avx-512 conflict detection support may change this .    whether the disadvantages of scheme ( [ sfig : vec-2 ] ) outweigh its advantages or not is primarily a question of amortization .",
    "the answer depends on the used floating point data type , the vector length , and the features of the underlying instruction set .",
    "scheme ( [ sfig : vec-3 ] ) is the natural model for the gpu , where data parallelism and parallel execution are blurred together .",
    "an iteration @xmath2 is assigned to each thread , and the thread sequentially works through the @xmath6 iterations .    to implement these schemes ,",
    "the algorithms are split into two components : a `` computational '' one , and a `` filter '' .",
    "the computational component carries out the numerical calculations , including the innermost loop and the updates to force and energy ; the input to this component are pairs of @xmath2 and @xmath6 for which the force and energy calculations are to be carried out .",
    "given that the majority of the runtime is spent in computation , this is the part of the algorithm that has to be vectorized .",
    "the filter component is instead responsible to feed work to the computational one ; its duty is to determine which pairs to pass . to this end",
    ", the data is filtered to make sure that work is assigned to as many vector lanes as possible before entering the vectorized part .",
    "this means that the interactions outside of the cutoff region never even reach the computational component .",
    "0.5   loop : green is ready - to - compute , red is not - ready - to - compute , and blue is actual calculation . the left is an unoptimized variant , where calculation takes place as soon as at least one lane is ready - to - compute , whereas on the right , the calculation is delayed until all lanes are ready - to - compute .",
    ", title=\"fig : \" ]       0.5   loop : green is ready - to - compute , red is not - ready - to - compute , and blue is actual calculation . the left is an unoptimized variant , where calculation takes place as soon as at least one lane is ready - to - compute , whereas on the right , the calculation is delayed until all lanes are ready - to - compute .",
    ", title=\"fig : \" ]    section  [ ssec : vect ] just described how we avoid calculation for skin atoms in the @xmath33 loop .",
    "the remaining issue is how we skip them in the @xmath34 loop .",
    "the same argument , that resources must not be wasted on calculation that does not contribute to the final result , applies here , too . as such , as many vector lanes as possible need to be active before entering numerical kernels such as those computing @xmath68 and @xmath27 .",
    "these computational kernels are almost entirely straight - line floating - point intense code , with some lookups for potential parameters in between .",
    "this optimization is most important for scheme ( [ sfig : vec-2 ] ) and ( [ sfig : vec-3 ] ) , as they traverse multiple neighbor lists in parallel .",
    "as such , no guarantee can be made that interacting atoms have the same position in all neighbor lists .",
    "this leads to sparse masks for the compute kernel : for example , in a typical simulation that uses a vector length of sixteen , no more than four lanes will be active at a time .",
    "on gpus , this effect is even worse , where 95% of the threads in a warp might be inactive .",
    "our optimization extends scheme ( [ sfig : vec-2 ] ) and ( [ sfig : vec-3 ] ) to fast forward through the @xmath34 loop , until as many lanes as possible can participate in the calculation of a numerical kernel .",
    "the idea is that the neighbor list is not traversed at equal speed for all the vector lanes ; instead , we manipulate the iteration index independently in the various lanes .",
    "[ fig : avoid ] visualizes the way this modification affects the behaviour of the algorithm . in that figure",
    ", the shade of the particular color roughly corresponds to that lane s progress through the loop .",
    "notice that on the left , calculation ( blue ) takes place as soon as at least one lane requires that calculation ( green ) .",
    "instead , on the right , a lane that is ready to compute ( green ) , idles ( does not change its shade ) , while the other lanes make further progress ( going through shades of red ) in search of the iteration where they become ready ( green ) . in our implementation",
    ", the calculation ( blue ) only takes place if all lanes are ready ( green ) .",
    "effectively , we `` fast - forward '' in each lane , until all of them have are ready to compute .",
    "to implement the idea from sec .",
    "[ ssec : avoid - mask ] , a lot of masking is necessary , because the subset of lanes that have to progress when fast - forwarding changes every time . on platforms where",
    "masking has non - trivial overhead , performance can be further optimized .",
    "observe that in fig .",
    "[ fig : avoid ] , lanes `` spin '' until computation is available .",
    "we can reduce the amount of spinning by filtering the neighbor list in the scalar segment of the program . to ensure correctness",
    ", the filtering is based on the maximum cutoff of all the types of atoms in the system .",
    "this means that atoms that physically play a role can not be accidentally excluded from the calculation .",
    "filtering with any other cutoff might lead to incorrect results in systems with multiple kinds of atoms , if the cutoff prescribed between any two atom kinds differs .    filtering",
    "the neighbor list is especially effective with avx , where the double precision implementation uses the mapping ( [ sfig : vec-1 ] ) , whereas the single precision variant uses ( [ sfig : vec-2 ] ) . without this change ,",
    "the overhead to spin is too big to lead to speedups with respect to the double precision version . with this change , most time is again spent in the numerical part of the algorithm .",
    "so far we kept the description of the algorithms as generic as possible ; in this section , we cover the actual implementation of the schemes and optimizations from the previous section , and their integration into lammps . from a software engineering standpoint ,",
    "the main challenge was to make the implementation maintainable , while achieving portable performance .",
    "to this end , openmp 4.5 s simd extensions would be the most appealing solution , but right now lack both compiler support and a number of critical features that are required for our implementation .",
    "we resorted to implementing these features ourself as modular and portable building blocks . in the following",
    ", we first introduce these building blocks , and then focus on a platform independent implementation .",
    "finally , we characterize the different execution modes supported by our code .",
    "we identified four groups of building blocks necessary for a portable implementation .",
    "\\(1 ) vector - wide conditionals .",
    "these conditionals check if a condition is true across all vector lanes ; since either all or no lanes enter these conditionals , excessive masking is prevented .",
    "\\(2 ) reduction operations . these are useful when all lanes accumulate to the same ( uniform - across - lanes ) memory location . in these cases , the reduction can be performed in - register , and only the accumulated value is written to memory .",
    "this behaviour can not be achieved with openmp s reduction clause , since it only supports reductions in which the target memory location is known a - priori , while this is not the case for our code .",
    "\\(3 ) conflict write handling .",
    "this feature allows vector code to write to non distinct memory locations ( see the discussion in the previous bullet ) . in the vectorization of md codes",
    ", it is often guaranteed that all lanes write to distinct memory locations ( since the atoms in a neighbor list are all distinct ) , which is the assumption that compilers typically make when performing user - specified vectorization .",
    "unfortunately , this guarantee does not hold for scheme ( [ sfig : vec-2 ] ) . by serializing the accesses ,",
    "the ` ordered simd ` clause of the openmp 4.5 standard provides a solution to this issue ; however , at time of writing , this directive is not yet supported by any major compiler .",
    "it is also questionable whether this approach will be `` future proof '' or not , as a conflict detection mechanism such as that in the avx-512 extensions might make serialization unnecessary .",
    "\\(4 ) adjacent gather optimizations .",
    "these provide improved performance on systems that do not support a native gather instruction or where such an instruction has a high latency .",
    "an adjacent gather is a sequence of gather operations that access adjacent memory locations . instead of using gather instructions or gather emulations here",
    ", it is possible to load continuously from memory into registers , and then permute the data in - register .",
    "this operation can lead to significant performance improvements in our code , because adjacent gathers are necessary to load the parameters of our potential ; it is also important for backwards - compatibility reasons , because old systems lack efficient native gather operations .      since our objective is to integrate with the lammps md simulator , support for different instruction sets and for different floating point precisions is necessary .",
    "it is crucial to support cpu instruction sets to balance the load between host and accelerator .",
    "additionally , such an abstraction enables us to evaluate the influence of vector lengths and instruction set features on performance . considering all combinations of instruction sets , data types and vectorization variants ,",
    "it becomes clear that it is infeasible to implement everything with intrinsics .",
    "we created a single algorithm , and paired it with a vectorization back - end .",
    "as a consequence , instead of coding the tersoff potential s algorithm @xmath69 times ( @xmath35 architectures and @xmath70 precision modes ) , we only had to implement the building blocks for the vectorization back - end .",
    "some of these building blocks provide the features described in sec .",
    "[ ssec : openmp ] , while others provide one - to - one mappings to intrinsics , mostly unary and binary operators .",
    "the vectorization back - end uses c++ templates , which are specialized for each targeted architecture .",
    "we developed back - ends for single , double and mixed precision using a variety of instruction set extensions : scalar , sse4.2 , avx , avx2 , imci ( the xeon phi knights corner instruction set ) , as well as experimental support for avx-512 , cilk array notation and cuda .",
    "the library is designed to be easily extended to new architectures . even though the tuning might take some time , it is simplified by the fact that a number of building blocks , such as wide adjacent - gather operations , can be optimized in one go .",
    "contrary to most other vector libraries , which allow the programmer to pick a vector length that may be emulated internally , our library only allows for algorithms that are oblivious of the used vector length .",
    "we use vanilla lammps mpi - based domain decomposition scheme and build upon optional packages that offer various optimizations and capabilities .",
    "specifically , all our x86 and arm implementations use the user - intel @xcite package , which collects optimizations for intel hardware , to manage offloading to the xeon phi , data - packing , alignment and simulation orchestration . for the gpu implementation",
    ", the same role is fulfilled by the kokkos package @xcite .",
    "since kokkos abstracts the data layout of the variables used in a simulation ( e.g.  position , velocity , mass , force ) , the code needs to be changed wherever data is accessed from memory .",
    "we also need to change the routine that feeds our algorithm , to conform with the model of parallelism that is used by kokkos . as a consequence ,",
    "comparisons between x86 and arm , and between x86 and the gpu implementation can not reasonably be drawn .",
    "furthermore , the kokkos package is still under development , while the user - intel package is more mature ; as such , we believe that the gpu results are likely to have room for improvement .          in addition to double precision , which is the default in lammps , we created versions that compute the tersoff potential in single and mixed precision . in order to validate these two implementations that use reduced precision , we measured the energy in a long - running simulation .",
    "[ fig : acc ] illustrates , for a system of 32000 atoms , the deviation is within 0.002% of the reference .    in part , this effect can be explained by the short neighbor lists which are characteristic of the tersoff potential : since only few different atoms interact with any given atom , and only these accumulate their contributions to the force , there is little chance for round - off error to accumulate .      in the following section , we present performance results for several hardware platforms , and four different codes : ref , opt - d , opt - s , opt - m .",
    "[ [ ref ] ] ref + + +    the reference for our optimization and testing is the implementation shipped with lammps itself , which performs all the calculations in double precision .    [",
    "[ opt - d ] ] opt - d + + + + +    the most accurate version of our code , which performs the calculations in double precision .",
    "it includes both the optimizations due to scalar improvements , and those due to vectorization .",
    "[ [ opt - s ] ] opt - s + + + + +    the least accurate version of our code , implemented entirely in single precision . as for opt - d , it includes both scalar improvements , and takes advantage of vectorization .",
    "the vector length typically is twice that of opt - d . referring to sec .",
    "[ ssec : acc ] , the accuracy of the single precision solver is perfectly in line with the one offered by _ opt - d _ or _",
    "ref_.    [ [ opt - m ] ] opt - m + + + + +    we also provide a mixed precision version of our code .",
    "this version performs all the calculations in single precision , except for accumulations .",
    "it is the default mode for code of the user - intel package , as it offers a compromise between speed and accuracy . from an software engineering perspective , the mixed precision version costs very little , as its can leverage the existing single and double precision codes ; indeed , our vector library performs this step ( from single and double implementation to mixed implementation ) automatically .",
    "in addition to these four modes , the algorithms are run on a single thread ( _ 1 t _ ) or on an entire node ( _ 1n _ ) .",
    "the single - threaded run gives the most pure representation of the speedup obtained by our optimizations ; the results for an entire node ( _ 1n _ ) and a cluster instead give a realistic assessment of the expected speedup in real - world applications .",
    "such parallel runs use mpi , as provided by lammps itself ; as a consequence , the parallelization scheme used in _ ref _ and _ opt _ is the same .",
    "in this section , we validate the effectiveness of our optimizations for the tersoff potential by presenting experimental results on a variety of hardware platforms , ranging from a low - power arm to the second generation xeon phi . as a test case",
    ", we use a standard lammps benchmark for the simulation of silicon atoms ; since the atoms are laid out in a regular lattice so that each of them has exactly four nearest neighbors , this test case captures well the scenario of small neighbor lists discussed in sec .",
    "[ sec : ters ] and sec .",
    "[ sec : opt ] .",
    "we start by presenting single - threaded and single - node results for the cpus ( and the respective instruction sets ) listed in table  [ tbl : cpu - hw ] ; we continue with measurements for two gpus ( table  [ tbl : gpu - hw ] ) , and conclude with results for the xeon phi ( table  [ tbl : phi - hw ] ) , in number of configurations .",
    "we also present data for a cluster of nodes , to demonstrate the degree to which the scalar and vector improvements lead to performance at scale .",
    ".hardware used for cpu benchmarks . [ cols=\"<,<,<,<\",options=\"header \" , ]     we conclude with a discussion of the portability of our optimizations on two generations of intel xeon phi accelerators``knights corner '' ( knc ) , and `` knights landing '' ( knl)scaling from a single accelerator to a cluster .",
    "[ fig : speedup - phi ] measures the impact of our optimizations while using all cores of a xeon phi accelerator . for a fair comparison",
    ", the benchmark is run on the device , without any involvement of the host in the calculation .",
    "on both platforms , the speedup of opt - m with respect to ref is roughly 5x .",
    "single - threaded measurements ( not reported ) indicate that the `` pure '' speedup in the kernel is even higher , at approximately 9x .    with a relative performance improvement of about 3x ,",
    "the difference between knc and knl is in line with our expectations ; in fact , the theoretical peak performance also roughly tripled , along with the bandwidth , which roughly doubled .",
    "we point out that no optimization specific to knl was incorporated in our code ; the speedup was achieved by simply making sure that the vector abstraction complied with avx-512 .",
    "additional optimizations for knl might take advantage of the avx-512cd conflict detection instructions and different code for gather operations .    to lead up to the scaling results across multiple xeon phi augmented nodes , fig .",
    "[ fig : nodes - phi ] measures the performance of individual such nodes as listed in table  [ tbl : phi - hw ] . like in a real simulation ,",
    "the workload is shared among cpu and accelerator . given",
    "that our knl system is self - hosted , we include it in this list .",
    "the measurements for cpu+knc , include both the overheads incurred in a single node execution ( such as mpi and threading ) , and the overhead due to offloading . in view of the performance of the cpu - only systems relative to knc , these performance numbers are then plausible .",
    "a single knc delivers higher simulation speed than the cpu - only sb node ; however , a cpu - only hw node is more powerful than the knc , and thus also noticeably contributes to the combined performance . adding a second accelerator also seems to improve performance , as seen in the iv+2knc measurement .",
    "the knl system delivers higher performance than the combination of two first - generation xeon phis and two ivy bridge cpus .",
    "the question with any kind of serial improvement is `` will it translate to similar speedups in a ( highly ) parallel environment ? '' . in theory ,",
    "sequential improvements multiply with the performance achieved from parallelism ; in practice , a good chunk of those improvements are eaten away by a collection of overheads , and a realistic assessment can only be made from measurement .",
    "figure  [ fig : supermic ] depicts results for up to eight nodes in a cluster of iv+2knc nodes . here ,",
    "overheads are not only due to the parallelism within a node , but also to communication among the nodes .",
    "the vector optimizations port to large scale computations seamlessly : without accelerator , the performance improvement for 196 mpi ranks is 2.5x ; when two accelerators are added per node , the performance improvement becomes 6.5x .",
    "coordinates ( knc , 0.526 ) ( knl , 1.382 ) ; coordinates ( knc , 2.475 ) [ 4.71x ] ( knl , 8.209 ) [ 5.94x ] ;    coordinates ( sb+knc , 2.029 ) ( hw+knc , 4.358 ) ( iv+2knc , 6.184 ) ( knl , 8.209 ) ;",
    "we discussed the problem of calculating the tersoff potential efficiently and in a portable manner ; we described a number of optimization schemes , and validated their effectiveness by means of realistic use cases .",
    "we showed that vectorization can achieve considerable speedups also in scenarios  such as a multi - body potential with a short neighbor list  which do not immediately lend themselves to the simd paradigm . to achieve portability",
    ", it proved useful to isolate target - specific code into a library of abstract operations ; this separation of concerns leads to a clean division between the algorithm implementation and the hardware support for vectorization .",
    "it also makes it possible to map the vector paradigm to gpus , while attaining considerable speedup .",
    "the ideas behind our optimizations were described , and their effectiveness was validated by means of realistic use cases . indeed , we observe speedups between 2x and 3x on most cpus , and between 3x and 5x on accelerators ; performance scales also to clusters and clusters of accelerators .",
    "finally , we believe that the main success of this work lies in the achieved degree of cross - platform code reuse , and in the portability of the proposed optimizations ; combined , these two features lead to a success story with respect to performance portability .    coordinates ( 1 , 0.155 ) ( 2 , 0.294 ) ( 4 , 0.608 ) ( 8 , 0.839 ) ; coordinates ( 1 , 0.394 ) ( 4 , 1.070 ) ( 8 , 2.096 ) ; coordinates ( 2 , 1.818 ) ( 4 , 3.356 ) ( 8 , 5.439 ) ;",
    "the authors gratefully acknowledge financial support from the deutsche forschungsgemeinschaft ( german research association ) through grant gsc 111 , and from intel via the intel parallel computing center initiative .",
    "we thank the rwth computing center and the leibniz rechenzentrum mnchen for computing resources to conduct this research .",
    "we would like to thank marcus schmidt for providing one of the benchmarks used in this work , and m. w. brown for conducting the benchmarks on the 2nd generation xeon phi hardware .",
    "1 s. plimpton , fast parallel algorithms for short - range molecular dynamics , j comp phys , 1995 .",
    "wolf et al , assessing the performance of openmp programs on the intel xeon phi , lecture notes in computer science , euro - par 2013 parallel processing , 2013 .",
    "brown et al , an evaluation of molecular dynamics performance on the hybrid cray xk6 supercomputer , procedia computer science , 2012 .",
    "brown at al , implementing molecular dynamics on hybrid high performance computers  three - body potentials , computer physics communications , 2013 .",
    "hou et al , efficient gpu - accelerated molecular dynamics simulation of solid covalent crystals , computer physics communications , 2013 .",
    "brown et al , optimizing legacy molecular dynamics software with directive - based offload , computer physics communications , 2015 .",
    "tersoff , new empirical approach for the structure and energy of covalent systems , phys .",
    "heinecke et al , supercomputing for molecular dynamics simulations , springer international publishing , 2 - 15 .",
    "pll et al , tackling exascale software challenges in molecular dynamics with gromacs , solving software challenges for exascale , lecture notes in computer science , 2015 .",
    "tian et al , compiling c / c++ simd extensions for function and loop vectorization on multicore - simd processors , ipdpsw , 2012 .",
    "e. bitzek , et al , recent developments in imd : interactions for covalent and metallic systems , high performance computing in science and engineering 2000 , 2001 . j. roth et al , imd - a massively parallel molecular dynamics package for classical simulations in condensed matter physics , high performance computing in science and engineering 99 , 2000 . s. j. plimpton and a. p. thompson , computational aspects of many - body potentials , mrs bulletin , 37 , 2012 .",
    "berendsen , d. van der spoel , r. van drunen , gromacs : a message - passing parallel molecular dynamics implementation , computer physics communications , 1995 .",
    "mark james abraham , teemu murtola , roland schulz , szilrd pll , jeremy c. smith , berk hess , erik lindahl , gromacs : high performance molecular simulations through multi - level parallelism from laptops to supercomputers , softwarex , september 2015",
    ". h. carter edwards , christian r. trott , daniel sunderland , kokkos : enabling manycore performance portability through polymorphic memory access patterns , j. parallel distrib .",
    "2014 . w. m. brown , p. wang , s. j. plimpton , a. n. tharrington , implementing molecular dynamics on hybrid high performance computers - short range forces , comp phys comm , 2011 brown , w.m . , carrillo , j .- m.y . , gavhane , n. , thakkar , f.m . ,",
    "plimpton , s.j .",
    ", optimizing legacy molecular dynamics software with directive - based offload , computer physics communications , to appear .",
    "o. e. b. messer , e. dazevedo , j. hill , w. joubert , s. laosooksathit and a. tharrington , developing miniapps on modern platforms using multiple programming models , cluster computing ( cluster ) , 2015 ieee international conference on , 2015 . s. pll , b. hess , a flexible algorithm for calculating pair interactions on simd architectures , computer physics communications , 2013 . s. j. pennycook , c. j. hughes , m. smelyanskiy , s.a .",
    "jarvis , exploring simd for molecular dynamics , using intelxeonprocessors and intelxeon phi coprocessors , ipdps 13 , 2013 .",
    "j. a. anderson , c. d. lorenz , a. travesset , general purpose molecular dynamics simulations fully implemented on graphics processing units , journal of computational physics , 2008 .",
    "d. c. rapaport , enhanced molecular dynamics performance with a programmable graphics processor , computer physics communications , 2011 .",
    "z. fan , t. siro , a. harju , accelerated molecular dynamics force evaluation on graphics processing units for thermal conductivity calculations , computer physics communications , 2013 . c. hou , j. xu , p. wang , w. huang , x. wang , efficient gpu - accelerated molecular dynamics simulation of solid covalent crystals , computer physics communications , 2013 .",
    "j. c. phillips , r. braun , w. wang , j. gumbart , e. tajkhorshid , e. villa , c. chipot , r. d. skeel , l. kale , k. schulten , scalable molecular dynamics with namd , journal of computational chemistry , 2005 . c. niethammer , s. becker , m. bernreuther , m. buchholz , w. eckhardt , a. heinecke , s. werth , h .- j .",
    "bungartz , c. w. glass , h. hasse , j. vrabec , m. horsch , ls1 mardyn : the massively parallel molecular dynamics code for large systems , journal of chemical theory and computation , 2014 .",
    "w. smith , i. t. todorov , a short description of dl_poly , molecular simulation , 2006 .",
    "f. w. j. olver , d. w. lozier , r. f. boisvert , c. w. clark , editors , nist handbook of mathematical functions , cambridge university press , 2010 .",
    "m. p. allen , d. j. tildesley , computer simulation of liquids , oxford university press , 1987 . ` http://github.com/hpac/lammps-tersoff-vector ` ."
  ],
  "abstract_text": [
    "<S> molecular dynamics simulations , an indispensable research tool in computational chemistry and materials science , consume a significant portion of the supercomputing cycles around the world . </S>",
    "<S> we focus on multi - body potentials and aim at achieving performance portability . compared with well - studied pair potentials , </S>",
    "<S> multibody potentials deliver increased simulation accuracy but are too complex for effective compiler optimization . because of this </S>",
    "<S> , achieving cross - platform performance remains an open question . by abstracting from target architecture and computing precision , </S>",
    "<S> we develop a vectorization scheme applicable to both cpus and accelerators . </S>",
    "<S> we present results for the tersoff potential within the molecular dynamics code lammps on several architectures , demonstrating efficiency gains not only for computational kernels , but also for large - scale simulations . on a cluster of intel xeon phi s </S>",
    "<S> , our optimized solver is between 3 and 5 times faster than the pure mpi reference . </S>"
  ]
}