{
  "article_text": [
    "during the past decade , @xmath1-body tree codes have been applied successfully to various problems in galaxy dynamics , galaxy formation and cosmological structure formation ( barnes & hut 1986 ; hernquist 1987 ; dubinski 1988 ) .",
    "computing time only scales as @xmath2 so they provide a relatively fast way to solve the general collisionless n - body problem . for the most part , the favored tree code in studies has been the barnes - hut ( bh ) code ( 1986 ) mainly because it was widely distributed by its authors but also because it has been easy to implement .",
    "different tree - based algorithms have also been used successfully for various problems ( appel 1985 ; jernigan & porter 1989 ; 1993 ; benz , bowers , cameron & press 1990 ; steinmetz & muller 1993 ) .",
    "tree codes have also been linked up with smoothed particle hydrodynamics ( sph ) ( hernquist & katz 1989 ; benz et al 1990 ; steinmetz & muller 1993 ) .",
    "tree code simulations running on the latest workstations and vector supercomputers are generally restricted to @xmath3 because of memory and time limitations .",
    "cosmological and hydrodynamical simulations require ever greater dynamic range , so there is a strong desire to increase these limits . during the past few years",
    ", there has been a paradigm shift in supercomputing with the movement from vector machines to the new massively parallel machines .",
    "parallel supercomputers are collections of hundreds to thousands of independent smaller commodity processors interconnected with an internal high speed communication network .",
    "the nsf supercomputing centers support several machines : the connection machine 5 , ( cm-5 ) , the intel paragon , the ibm sp/2 , and the cray t3d .",
    "each processor operates independently but communication software allows messages containing data to be exchanged rapidly with other processors . in principle",
    ", a problem can be partitioned among the @xmath1 processors and a maximum @xmath1-fold increase in computational speed can be realized . in practice , the speed up is smaller because of the extra time needed for exchanging data in message - passing algorithms .",
    "parallel machines offer a new route to very fast computation if algorithms can be redesigned to conform to the message - passing paradigm and communication can be minimized .",
    "the new algorithms are often very different than their sequential counterparts and require a considerable effort to redesign .",
    "the key to a successful algorithm is good _ load balance _ : both data and computational work must be distributed evenly among the processors .",
    "a common way of achieving load balance on parallel machines is through _ domain decomposition _",
    "( fox , williams & messina 1994 ) .",
    "the physical domain of the problem is partitioned into smaller subdomains and the physical quantities of these subdomains : either values of density , pressure and temperature at grid elements in an eulerian fluid code or collections of particles and their attributes in an n - body code are assigned to each processor .",
    "the trick in designing a parallel algorithm is finding a way of partitioning the domain so that data and work are evenly distributed and communication is minimized .",
    "there have been many efforts to parallelize the bh tree code .",
    "hillis and barnes ( 1987 ) and makino and hut ( 1989 ) ported the code to the connection machine-2 but at the time there was little gain over the existing vector machines . salmon ( 1990 )",
    "introduced a new parallel tree algorithm using a domain decomposition based on the orthogonal recursive bisection of the n - body volume into rectangular subvolumes for purposes of load balance .",
    "the code has been applied to dark halo formation with initial density fluctuations based on different power spectra ( warren et al .",
    "1992 ; warren 1994 ; zurek et al .",
    "warren & salmon ( 1993 ) have also recently designed a new algorithm which uses a load - balancing scheme based on a parallel hashed oct - tree ( also warren 1994 ) .",
    "dikaiakos and stadel ( 1995 ) also have developed another variant of salmon s algorithm in their parallel tree code pkdgrav .",
    "parallel n - body tree codes have also been discussed as a pure computer science problem ( bhatt et al .",
    "1992 ; pal singh 1993 ; pal singh et al .",
    "1995 ) .    in this paper",
    ", we present a new version of salmon s ( 1990 ) parallel tree algorithm . in  2 , we describe the bh tree code as implemented at the node level in the parallel code . in  3 , we describe a new portable implementation of salmon s parallel algorithm using the message passing interface ( mpi ) package to handle processor communication . in ",
    "4 , we examine the code s performance on the cray t3d , checking the accuracy , speed , and load balance . the term `` node ''",
    "is often used to refer to a single processor on a parallel computer but it is also used to refer to a data structure in a tree . to avoid confusion ,",
    "i will only use the term `` node '' in reference to tree structures and `` processor '' for a computational `` node '' .",
    "the n - body problem involves advancing the trajectories of @xmath1 particles according to their time evolving mutual gravitational field . in the simplest algorithm ,",
    "the force on each particle is determined by direct summation of the contributions from all of the n-1 neighbours . in a discrete integration , the forces at each timestep",
    "are then used to advance the particles along their trajectories according to a numerical integration scheme such as the leapfrog method .",
    "computational costs of direct summation scale as @xmath4 making this algorithm expensive . in collisionless systems like galaxies ,",
    "however , one can tolerate small errors in the force for improved performance by using techniques which approximate the gravitational field .",
    "tree codes are one class of methods which accomplish this and have the advantage of scaling only as @xmath5 in computational cost ( appel 1985 ; barnes & hut 1986 ; jernigan & porter 1989 ) .",
    "the essence of a tree code is the recognition that the gravitational potential of a distant group of particles can be well - approximated by a low - order multipole expansion .",
    "a set of particles can be arranged in a hierarchical system of groups in the form of a tree structure .",
    "the entire set can be subdivided into several groups and each of these groups can be broken down in succession in the hierarchy until the groups contain at most 1 particle .",
    "there are many different methods for grouping particles hierarchically in this way ( appel 1985 ; jernigan & porter 1989 ; barnes & hut 1986 ) .",
    "the evaluation of the potential at a point reduces to a descent through the tree .",
    "one sets a minimum distance a point can be from a group to use a multipole expansion .",
    "if the point is sufficiently distant from a group , the multipole expansion is used to find the potential from that group , while if the point is too close to the group , each of its child subgroups are examined .",
    "the procedure continues until all groups are broken down as far as they need be .      the barnes - hut ( 1986 ) algorithm works by grouping particles using a hierarchy of cubes arranged in oct - tree structure i.e. each node in the tree has 8 siblings .",
    "the system is first surrounded by a single cube or cell encompassing all of the particles .",
    "this main cell is subdivided into 8 subcells , each containing their own subset of the particles .",
    "the tree structure continues down in scale until cells contain only 1 particle .",
    "for each cell or node in the tree , we calculate the total mass , center of mass and higher order multipole moments ( typically only up to quadrupole order ) .",
    "this tree structure can be built very rapidly making it feasible to rebuild it at each time step .",
    "the tree can be constructed bottom - up i.e. by inserting one particle at a time ( barnes & hut 1986 ) or top - down by sorting particles across divisions .",
    "both methods take @xmath5 time and in practice only a few percent of total time per step .",
    "the force on a particle in the system can be evaluated by `` walking '' down the tree level by level beginning with the top cell . at each level",
    ", a cell is added to an interaction list if the cell is distant enough for a force evaluation .",
    "if the cell is too close , it is `` opened '' and the 8 subcells are either used for the force evaluation or opened further .",
    "the walk ends when all cells which pass the opening test and any single particles are acquired . the accumulated list of interacting cells and particles is then looped through to calculate the force on the given particle and this amounts to the bulk of the computation .",
    "in this way , the number of interactions computed is significantly smaller than a direct n - body method with the number scaling as @xmath5 ( barnes & hut 1986 ) . typically , there are @xmath6 interactions per particle on average in a simulation with @xmath7 particles making the algorithm significantly factor than a direct summation method .      there are various criteria for determining whether a cell is sufficiently distant for a force evaluation .",
    "the simplest criterion was introduced by barnes & hut ( 1986 ) based on an opening angle parameter , @xmath8 .",
    "if the size of a cell is @xmath9 and the distance of the particle from the cell center of mass is @xmath10 , we accept the cell for a force evaluation if @xmath11 smaller values of @xmath8 lead to more cell openings and more accurate forces .",
    "typically , @xmath12 gives accelerations with errors around 1% ( hernquist 1987 ) .",
    "salmon & warren ( 1994 ) showed , however , that this criterion for calculating the potential can cause gross errors in some pathological cases in which the center of mass is near the edge of the cell .",
    "they introduced various alternatives which avoid this problem as did barnes ( 1994 ) .",
    "after some experimentation with various opening criteria , we have adopted barnes ( 1994 ) modification which circumvents this problem .",
    "the new criterion for cell opening is @xmath13 where @xmath14 is the distance between the center of mass of a cell and its geometrical center ( figure [ fig - bhopen ] ) .",
    "this criterion guarantees that if the center of mass is near the cell edge only positions removed by an extra distance @xmath14 use the cell for a force evaluation , while if the center of mass is near the cell center it reverts to the old criterion .",
    "tree walks themselves include a sizeable overhead in computation since the cell opening criterion must be computed many times for each particle .",
    "barnes ( 1990 ) introduced one final improvement to reduce the number of tree walks for a net gain in computing speed .",
    "the strategy is to find all cells in the tree which contain less than @xmath15 particles where typically @xmath16 .",
    "the tree walk proceeds as before but instead of defining @xmath10 as the particle distance from a target cell s center of mass , @xmath10 is defined as the distance between the nearest edge of the cube encompassing the group to the target cell s center of mass .",
    "the accumulated interaction list is then used for all of the particles in a given group and the forces are guaranteed to be at least as accurate as those of an individual particle at the edge of the group s cube . in practice , this can lead to a considerable speedup for a fixed force accuracy ( barnes 1990 ) .",
    "when tree structures are part of an algorithm , it is natural to program functions recursively . unfortunately , there can be considerable overhead from recursive calls .",
    "the tree walk is called many times at each timestep and so it is best to eliminate recursion in this function .",
    "non - recursive tree walks can be accomplished by arranging the tree nodes in a linked list ( dubinski 1988 ; makino 1989 ) .",
    "each node is set up to contain a pointer to its first child and adjacent sibling . if a node is the last child in a level , its sibling pointer is assigned to its parent s sibling .",
    "a node which contains 1 particle will have only one adjacent sibling and no children .",
    "the nodes are resorted as follows ( figure [ fig - nonrecur ] ) .",
    "the first node in list is set to the root cell containing all of the particles .",
    "the descent of the tree always proceeds to the current node s first child which is then added to the list .",
    "if the node contains only one particle the walk proceeds to the node s sibling which is then added to the list . once the nodes are sorted this way , a tree walk for a force calculation then reduces to a scanning of this list . if a cell must be opened , one simply examines the next node in this list which is guaranteed to be the first child cell at the next level of the hierarchy .",
    "if a cell can be used , the walk to the sibling is simply a jump down the list to the appropriate cell labelled by the sibling pointer .",
    "the other main advantage of resorting the tree nodes in this way is that it becomes easy to prune and graft trees onto existing structures which is essential in the parallelization of the code .",
    "the bh tree algorithm as described above has been implemented on each processor in the parallel code .",
    "the node level code has been optimized using grouping and non - recursive tree walks and improve the tree code s efficiency considerably over simpler versions .",
    "the parallelization of the bh algorithm is not obvious since the inhomogeneous distribution of particles in the oct - tree structure does not immediately lend itself to a simple domain decomposition with load balance .",
    "the main difficulty is that both the particles and the tree structure must somehow be distributed in a balanced way among many independent processors .",
    "salmon ( 1990 ) describes one possible parallelization of the tree code which retains most of the features of the original tree code on the level of individual processors but introduces a new algorithm for distributing the particles and tree structure among the processors .",
    "we describe the algorithm below and its implementation on the cray t3d .",
    "the main difference between this new code and salmon s original code is the optimized implementation of the bh algorithm described in  2 .",
    "the parallel features are essentially the same .      in an n - body problem ,",
    "the system of particles can be enclosed by a rectangular box for isolated systems like individual galaxies or an exact cube in cosmological systems with periodic boundaries .",
    "salmon ( 1990 ) developed a parallel algorithm for decomposing this volume into rectangular sub - volumes using the method of orthogonal recursive bisection ( orb ) . in this technique ,",
    "a volume is represented as a hierarchy of rectangular boxes somewhat like the bh tree .",
    "the main volume is first cut along an arbitrary dimension at an arbitrary position . for a balanced tree ,",
    "the position is chosen so there are equal numbers of particles on each side of the cut .",
    "the resulting two volumes are cut again along the same or a different dimension again at an arbitrary position .",
    "the slicing of each subvolume continues for as many levels as required .",
    "the resulting process can be represented by a binary tree structure since each node points to two children .",
    "an orb tree of @xmath17 levels therefore results in @xmath18 subvolumes .",
    "most massively parallel computers are organized into partitions of @xmath18 processors ( @xmath19 in practice ) so that an @xmath17-level orb tree decomposition of an n - body volume maps conveniently onto a parallel machine .    at each level in the construction of the orb tree",
    ", we have the freedom to choose both the dimension and position of the volume subdivision .",
    "for example , if the initial volume is a cube , one can construct a orb tree analogue of the bh tree by cycling through the 3 dimensions and cutting only through the geometric center of each subvolume .",
    "however , with load balance in mind we wish to position our slice of the subvolume so that there are equal amounts of _ computational work _ on each side of the slice .",
    "this is the trick introduced by salmon to achieve load balance .",
    "we can keep track of the computational work for an n - body simulation by simply summing up the number of cell - particle interactions , the main sink of computing time . in this way , once the particles in each subvolume are assigned to a processor , the calculation of the acceleration should be load balanced .",
    "initially , the amount of computational work is unknown so it is assumed to be the same for each particle .",
    "the first step is therefore somewhat load imbalanced but in practice successive steps become more balanced .",
    "the choice of dimension for cutting the subvolumes in the orb tree construction is generally arbitrary but it is best to select the longest dimension for slicing . the tree code uses a multipole expansion to quadrupole order so we desire rectangular domains that are as `` spherical '' as possible . an orb domain decomposition on a parallel machine can be carried out by assuming the processors are layed out in a hypercube communication network . a machine containing @xmath18 processors",
    "can be arranged using the communication paradigm of an @xmath17-dimensional hypercube . in this arrangement",
    ", each processor resides at the vertex of this hypercube and can only communicate to @xmath20 neighbors along the lines on the edge of the hypercube .",
    "a domain decomposition can be carried out in @xmath17 steps by consecutive exchanges of particles across the @xmath17-dimensions of the hypercube network .",
    "processors are usually labeled by an integer between 0 and @xmath21 .",
    "if we represent processor @xmath22 in binary format , its hypercube partner across the @xmath23th dimension is @xmath24 where xor is the logical exclusive - or operation .",
    "this has the effect of flipping the @xmath23th bit from a 0 to 1 or vice versa .",
    "the orb domain decomposition proceeds as follows . a position and dimension",
    "is selected for cutting the entire domain into two subdomains .",
    "for the first hypercube dimension , each processor exchanges particles with its unique partner so that all the particles in a given processor are on one or the other side of the first cut .",
    "after this first iteration , half of the processors contain particles on one side of the domain and half on the other . at each successive iteration",
    ", each subdomain finds the splitting position and dimension and processors once more exchange particles as above with their corresponding partner across the particular hypercube dimension .",
    "finally , each processor contains its unique rectangular subdomain of particles ( figure [ fig - orb ] ) .",
    "if the criterion for splitting domains is the amount of computational work , each processor also has an allotment of particles which will take roughly the same amount of computational effort . in practice",
    ", the memory imbalance can be substantial with some processors containing approximately twice as many particles as others although this has been found to be tolerable .",
    "after the domain decomposition , a bh tree can be constructed locally in each processor using the particles contained within its independent subvolume .",
    "after constructing the local trees , we would ideally like to link them together to form the full tree describing the entire system .",
    "the top levels of the tree are simply the load - balanced orb tree structure created by the domain decomposition , while the processors contain the bh trees ( figure [ fig - kdtree ] ) .",
    "if a copy of this complete tree structure were available to each processor , they could proceed independently to evaluate the forces on their particles .",
    "unfortunately , the amount of memory required for a full copy of the tree at each node is prohibitive .",
    "salmon ( 1990 ) solved this problem by introducing the concept of a _ locally essential tree_. each processor does not require an entire copy of the tree .",
    "rather , only a significantly smaller subset of nodes is necessary since the bh - trees in distant volumes need only be opened to a lesser degree for _ all _ the particles in a given processor domain .",
    "the opening criterion can be applied to the entire group of particles in a processor by calculating the distance to the nearest edge of a processor volume in the same way as described above in the grouping scheme used to improve the efficiency of the force evaluations . therefore",
    ", a processor only needs to import significantly pruned trees from distant processors which it can graft on to its existing structure to create the locally essential tree .",
    "this pruned tree of the entire system contains all the nodes required to calculate the forces to the required tolerance specified by the opening angle criterion .    in practice ,",
    "the locally essential trees are constructed in the following manner .",
    "after building the local bh - tree , each processor imports the root nodes of the trees from all of the others in the pool ( figure [ fig - kdtree ] ) . a local binary tree is built from the base up in each processor using the imported root nodes .",
    "a walk through the orb tree using a group opening - criterion determines which subset of processors must be examined further to gather more tree nodes if necessary .",
    "tree walks are performed in the needed processors using the group opening criterion of the requesting processors .",
    "bh tree nodes which can be used are flagged .",
    "the nodes of the pruned trees are then gathered together and exported to the calling processor . once received the internal child and sibling links of the imported trees",
    "are reset and the pruned trees are grafted at the appropriate node of the orb binary tree .",
    "the result of this procedure is the locally essential tree which contains : the local bh - tree , the orb tree structure and the trees pruned to the appropriate levels imported from other processors .",
    "in summary , the parallel tree code works as follows . at the start",
    ", particles are distributed randomly among the @xmath18 processors . at each timestep ,",
    "the following procedures are carried out :    1 .",
    "orb domain decomposition across processors .",
    "2 .   construct the local bh tree .",
    "3 .   exchange tree nodes to construct the locally essential trees .",
    "walk through trees to calculate forces .",
    "move particles along their trajectories using these forces .",
    "only steps 1 . and 3 .",
    "require message passing . in both cases",
    ", messages are exchanged in a synchronous manner using the hypercube paradigm described above .",
    "the messages contain arrays of particle data ( masses , positions , and velocities ) and tree node data ( masses , positions , quadrupole moments and cell dimensions ) .",
    "to ease the problems of bookkeeping , the code is written in c and particle and tree node data are packaged in arrays of appropriate c structures .",
    "we use the message passing interface ( mpi ) software to handle the message passing .",
    "the mpi functions are supported on almost all parallel machines so the code is portable to different platforms .",
    "the code runs on the cray t3d , intel paragon , and the ibm sp/2 .",
    "in this section , we describe some results on the code performance on the cray t3d in terms of accuracy , speed and load balance .",
    "we use two test problems to profile the code : an isolated galaxy composed of a disk , bulge and a halo and a spherical cluster of galaxies following a hernquist density profile .",
    "the first case represents a smooth centrally concentrated mass distribution expected in galaxy simulations while the second case exemplifies a strongly , inhomogeneous clustered distribution which arises in a cosmological context ( figure [ fig - nbody ] ) .",
    "tree codes usually run most slowly for these sorts of problems so the performance study here should test the code most rigorously . more homogeneous systems run approximately twice as quickly in practice .",
    "we note that the code has already been applied to simulations of interacting and colliding galaxies containing @xmath25300 thousand particles and runs without problems for greater than 1000 timesteps ( dubinski , mihos , and hernquist 1996 ) .",
    "the code also runs on the intel paragon and the ibm sp2 and note that the performance on these machines is comparable to the t3d .",
    "we first measured the errors in acceleration , @xmath26 , for various values of the cell opening angle , @xmath8 in single galaxy and cluster models .",
    "we only use a single processor to measure these errors .",
    "all cell - particle interactions are calculated to quadrupole order .",
    "the galaxy contained 40000 particle while the cluster contained 128 galaxies of @xmath27 particles each plus a smooth cluster dark halo for a total of 120000 particles .",
    "accelerations were first calculated using @xmath28 . since the errors are less than 0.02% for this case the accelerations are effectively exact for comparison to those calculated with larger values of @xmath8 .",
    "figure [ fig - err ] shows plots of the distribution of acceleration errors for various values of @xmath8 along with the resulting cumulative distribution .",
    "even with @xmath29 , the median error in the acceleration is still less than 0.5% while 90% of the measured accelerations are less than 1% for both the galaxy and the cluster .",
    "the addition of @xmath14 in barnes new opening criterion causes many more cells to be opened than with the original criterion and therefore allows more accurate forces for larger values of @xmath8 .",
    "the old opening criterion gave similar error distributions for @xmath12 ( hernquist 1987 ) .",
    "we also found that as the number of processors increases , the mean acceleration errors are found to decline while the computing time per processor increases since more cell openings are required .",
    "the top cells of the tree in a multi - processor run created during the domain decomposition can have large aspect ratios ( figure [ fig - orb ] ) .",
    "the result is more accurate forces than for a cubic decomposition since the walk through the hybrid orb  bh tree structure must go to deeper levels than the pure bh tree to satisfy the cell - opening criterion . in principle , the extra accuracy is not needed but at least the slightly degraded performance results in a higher quality simulation .",
    "the memory requirements of the parallel tree code are quite large .",
    "the majority of the memory is assigned to the particle and cell arrays in for the local subset of particles with additional arrays for particles and cells imported for the locally essential trees .",
    "the requirements for the particles and cells are respectively 15 and 20 machine words ( 8 bytes ) with an additional arra for the tree nodes imported from other processors .",
    "the memory for imported tree nodes can increase substantially for small @xmath8 but in practice is @xmath30 1 megaword for @xmath31 in simulations that fully load the machine . in practice , there are about 6 mwords available on the t3d for local particles and cells so this amounts to a maximum of 170k particles per processor . the scheme used for load - balancing computing time can lead to a memory imbalance of up to a factor of 2 in the number of particles per processor .",
    "the practical value for simulations is therefore about @xmath32k particles per processor to allow for this imbalance .",
    "the memory is allocated and reallocated dynamically for added flexibility in managing the changing sizes of the particle and cell lists .",
    "a successful parallel algorithm should have good load balance with a minimal overhead for processor communication .",
    "we ran our two test cases for a few time steps using different values of @xmath8 and different numbers of processors to examine these properties .",
    "we used larger number of particles to fully load the machines : 640 thousand particles for the galaxy , and 1.1 million for the cluster .",
    "we timed the computational functions ( tree building / tree walking ) and communication functions ( domain decomposition and tree exchange and distribution ) separately to measure the load balance and overall communication costs of the algorithm .",
    "we define the load balance as : @xmath33 where @xmath34 is the pure computational cpu time for a given processor and @xmath35 is the communication time . for this code @xmath35",
    "is the same for each processor since message passing is done synchronously .",
    "the denominator is just the elapsed wall - clock time for each timestep .",
    "we define the communication overhead , @xmath36 , as the fraction of the total wall - clock time per step used for communication : @xmath37 the load balance and communication overhead for the test cases as a function of number of processors for are listed in table 1 .",
    "we show results for the first step and third step of an integration . in the first step",
    ", particles are assumed to have equal computational costs and are therefore arranged in an orb tree with equal numbers of particles per processor .",
    "load balance is in the range of 60%-70% while tolerable can still be improved considerably .",
    "we expect this first step to be poorly load balanced since we know that in practice particles will have a wide range of computational costs .    by the third step ,",
    "the system is distributed by orb according to computational load . for most cases ,",
    "the load balance has improved considerably to greater than 90% for @xmath38 .",
    "the communication overhead is generally less than 10% of the total time per step so the computation is being dominated by physical calculations . for processor numbers greater than 128",
    ", there does not appear to be a major improvement in the load balancing by the third step .",
    "communication costs become more significant and so the use of the number of cell interactions to estimate the load is not necessarily the best way to weight the particles . an additional term incorporating the communication cost in the load estimate may improve the balance . in any case , load balance is still better than 70% so there is not much room left for improvement .",
    "rllll +   + & & + & & & & + & & & +   + 16 & 0.77 & 0.03 &  &  + 32 & 0.71 & 0.04 & 0.85 & 0.06 + 64 & 0.66 & 0.09 & 0.84 & 0.12 + 128 & 0.64 & 0.16 & 0.87 & 0.28 + 256 & 0.69 & 0.32 & 0.87 & 0.38 +   + & & & +   + 16 & 0.95 & 0.04 &  &  + 32 & 0.95 & 0.06 & 0.93 & 0.04 + 64 & 0.92 & 0.09 & 0.94 & 0.06 + 128 & 0.80 & 0.12 & 0.93 & 0.11 + 256 & 0.73 & 0.21 & 0.88 & 0.15 +      we first measured the speed of the code in terms of the number of particles which could be evaluated per second as a function of the number of processors .",
    "speeds were calculated from the third step once the particles had settled into a load - balanced decomposition .",
    "the speed is simply , @xmath39 , where @xmath1 is the number of particles and @xmath40 is the elapsed wall - clock time per step .",
    "figure [ fig - speed ] shows the code speed , @xmath41 , for various cases .",
    "the code shows an approximate linear scaling versus @xmath17 for @xmath38 but beyond that it falls off . as @xmath17 increases",
    ", the shapes of domains in the orb decomposition have a larger range of aspect ratios forcing tree walks to proceed to deeper levels to satisfy the opening criterion .",
    "we found that the number of cell interactions grows with the number of processors because of this effect .    4.0 in    we also measured the speed simply as the number of cell interactions / second , the dominant source of computation , to see how well the raw computational speed of the code scales with @xmath17 .",
    "figure [ fig - qspeed ] again shows a nearly linear scaling for @xmath38 after which there is a drop off which can be attributed mainly to the added load imbalance and communication cost for large @xmath17 .",
    "this speed is independent of the model for @xmath38 showing that it is an approximate measure of the raw computational speed .",
    "there are 72 floating point calculations per quadrupole order interaction plus one square root .",
    "there is also some computation associated with the tree walk and some direct particle - particle interactions .",
    "there are therefore approximately 100 floating point operations of useful work per cell interaction .",
    "the code speed on the t3d is therefore in the range of 15 - 20 mflops / processor depending on @xmath17 .",
    "similar estimates are found using the `` apprentice '' profiling utility on the t3d .",
    "this value falls short of the peak theoretical speed of 150 mflops / node .",
    "the deficit results from the large of amount of bookkeeping needed in shuffling and resorting particles and tree nodes at various times before actually doing the useful work of computing the forces .    4.0 in    in practice , the code running on a 16 node partition performs almost as well as hernquist s ( 1990 ) vectorized tree code on the cray c90 .",
    "walker s 500,000 particle of satellite accretion ran at a rate of @xmath254000 particles per second with @xmath42 ( walker , mihos & hernquist 1996 ) .",
    "parallel treecodes running on the newest parallel machines therefore offer about a factor of 10 fold increase in problem size or speed over vectorized tree codes .",
    "we also note that the speed of this parallel code is comparable to warren & salmon s ( 1994 ) when comparing their homogeneous sphere benchmarks .",
    "a parallel tree code based on salmon s algorithm has been implemented using the mpi message passing software on 3 parallel machines : the cray t3d , paragon and the ibm sp/2 . in principle",
    ", the code can also run on a small network of workstations although we have not had the opportunity to test it this way .",
    "the code incorporates an improved version of the bh tree algorithm including particle grouping for tree walks , non - recursive tree walking and a `` safe '' cell opening criterion .",
    "the code has been applied successfully to simulations of galaxy interactions ( dubinski , mihos , & hernquist 1996 ) and is currently being applied to a variety of other problems in galaxy dynamics .",
    "the code works best with a small number of processors that are heavily loaded with particles .",
    "if @xmath3 , we find that the number of processors should be less than 64 to get the best use of computing time .",
    "bigger problems will of course require the added memory of more processors .",
    "this code s main disadvantage is its excessive demand for memory .",
    "the memory needs become excessive when @xmath8 becomes too small .",
    "the use of locally essential trees is the main culprit in this respect .",
    "perhaps , this method of load balancing can be replaced by an asynchronous message passing scheme in which tree walks in various processors are done on demand by distributing particles among the processors . at present , the code is still marginally time limited and available memory will be greater in the next generation of machines .",
    "we are now in the process of adapting the code for both cosmological simulations with periodic boundary conditions and smoothed particle hydrodynamics ( dave , dubinski & hernquist 1996 ) . with this new code",
    ", we should be able to increase the typical number of particles used in current cray c90 treesph ( hernquist & katz 1989 ) simulation by a factor of 10 - 100 .",
    "the improved dynamic range should allow simulations of galaxy formation in a full cosmological context .",
    "i would like to thank romeel dave , uffe hellsten , lars hernquist and guohong xu for useful comments .",
    "i acknowledge grants for supercomputing time at the national center for supercomputing applications , the pittsburgh supercomputing center , the san diego supercomputing center and the cornell theory center .",
    "appel , a.w .",
    "1985 , in siam journal on scientific and statistical computing , 6 , 85 barnes , j. & hut p. 1986 , nature , 324 , 446 barnes , j.e 1990 , j. of comp .",
    "phys . , 87 , 161 barnes , j.e .",
    "1994 , in computational astrophysics , eds .",
    "j. barnes et al .",
    "( springer - verlag : berlin ) benz , w. , bowers , r.l . , cammeron , a.g.w . & press , w.h .",
    "1990 , apj , 348 , 647 bhatt , s. ; chen , m. ; lin , c .- y . ;",
    "liu , p. 1992 , in proceedings scalable high performance computing conference ( los alimitos : ieee comput .",
    "soc . press ) dave , r. , dubinski , j. & hernquist , l. 1996 , in preparation dikaiakos , m.d .",
    "& stadel , j. 1995 , preprint dubinski , j. 1988 , m.sc .",
    "thesis , university of toronto dubinski , j. , mihos , j.c , & hernquist , l. 1996 , apj , in press fox , g.c . , williams , r.d , & messina , p.c .",
    "1994 , parallel computing works ! , san francisco , ca : morgan kaufmann publishers inc .",
    "hernquist , l & katz , n. 1989 , apjs , 70 , 419 hernquist , l. 1987 , apjss , , 64 , 715 hernquist , l. 1990 , j. comp .",
    "phys . , 87 , 137 hernquist , l. , bouchet , f.r . , & suto , y. 1991 , apjss , 75 , 231 hillis , w.d .",
    "& barnes , j. 1987 , nature , 326 , 27 jernigan , j. g. & porter , d. h 1989 , apjs , 71 , 871 makino , j. & hut , p. 1989",
    ", 9 , 199 pal singh , j. 1993 , ph .",
    "d. thesis , stanford university pal singh , j. , holt , c. , totsuka , t. , gupta , a. , & hennessy , j. 1995 , journal of parallel and distributed computing , 27 , 118 .",
    "salmon , j. 1990 , ph.d .",
    "thesis , `` parallel hierarchical n - body methods '' , california institute of technology .",
    "salmon , j.k . &",
    "warren , m.s .",
    "1994 , journal of computational physics , 111 , 136 salmon , j.k . , & warren , m.s .",
    "1994 , international journal of supercomputing applications , 8 , 129 steinmetz , m. & muller , e. 1993 , aa , 268 , 391 walker , i. , mihos , j.c .",
    "& hernquist , l. 1996 , apj , in press warren , m. , and salmon j. 1992 , apj , 401 , warren , m.s .",
    "1994 , ph.d thesis , univeristy of california , santa barbara warren , m.s . & salmon , j.k .",
    "1993 , in proc . of supercomputing 93 , 12"
  ],
  "abstract_text": [
    "<S> we describe a new implementation of a parallel n - body tree code . </S>",
    "<S> the code is load - balanced using the method of orthogonal recursive bisection to subdivide the n - body system into independent rectangular volumes each of which is mapped to a processor on a parallel computer . on the cray t3d , the load balance in the range of 70 - 90% depending on the problem size and number of processors . </S>",
    "<S> the code can handle simulations with @xmath0 10 million particles roughly a factor of 10 greater than allowed on vectorized tree codes . </S>"
  ]
}