{
  "article_text": [
    "let @xmath9 be a sample from a stationary process with values in @xmath10 .",
    "the @xmath11 matrix @xmath12 is the so - called lag@xmath4 _ sample auto - covariance matrix _ of the process ( here @xmath13 denotes the transpose of a vector or matrix @xmath14 ) . in a classical low - dimensional situation where the dimension @xmath6 is assumed much smaller than the sample size @xmath7",
    ", @xmath8 is very close to @xmath15 so that its asymptotic behavior when @xmath16 ( so @xmath6 is considered as fixed ) is well known . in the high - dimensional context",
    "where typically the dimension @xmath6 is of same order as @xmath7 , @xmath8 will not converge to @xmath17 and its asymptotic properties have not been well investigated . in this paper , we study the empirical spectral distribution ( esd ) of @xmath8 , namely , the distribution generated by its @xmath6 singular values . the main result of the paper is the establishment of the limit of this esd when @xmath18 is an independent sequence with elements having a finite fourth moments while @xmath6 and @xmath7 grow to infinity proportionally .    in order to understand the importance of limiting spectral distribution ( lsd ) of singular values of the auto - covariance matrix @xmath8",
    ", we describe a statistical problem where these distributions are of central interest . in a recent stimulating paper",
    ", @xcite considers the following dynamic factor model @xmath19 where @xmath20 is an observed @xmath6-dimensional sequence , @xmath21 a sequence of @xmath22-dimensional  latent factor \" ( @xmath23 ) uncorrelated with the error process @xmath24 and @xmath25 is the general mean .",
    "a particularly important question here is the determination of the number @xmath22 of factors . for any stationary process",
    "@xmath26 , let @xmath27 be its ( population ) lag-@xmath28 auto - covariance matrix , we have @xmath29 it turns out that @xmath30 has exactly @xmath22 non - null singular values so that based on a sample @xmath31 it seems natural to infer @xmath22 from the singular values of the sample lag-1 auto - covariance matrix @xmath32 because @xmath33 has rank @xmath22 , the first three terms all have rank bounded by @xmath22 and @xmath34 appears as a finite - rank perturbation of the lag-1 auto - covariance matrix @xmath35 which in general has rank @xmath36 .",
    "therefore , understanding the properties of the singular values of @xmath35 will be of primary importance for the understanding of the @xmath22 largest singular values of the matrix of @xmath34 which are , as said above , fundamental for the determination of the number of factors @xmath22 .",
    "notice however that this statistical problem is given here to describe a potential application of the theory established in this paper , but this theory on singular value distribution is general and can be applied to fields other than statistics .",
    "if we take @xmath37 in , the matrix @xmath38 is the sample covariance matrix from the observations .",
    "the theory for eigenvalue distributions of @xmath39 has been extensively studied in the random matrix literature dating back to the seminal paper @xcite . in this paper ,",
    "the famous mar@xmath40enko - pastur law as limit of eigenvalue distributions has been found for a wide class of sample covariance matrices .",
    "further development includes the almost sure convergence of these distributions ( @xcite ) and conditions for convergence of the largest and the smallest eigenvalues , see @xcite .",
    "meanwhile book - length analysis of sample covariance matrices can be found in @xcite , @xcite , @xcite .",
    "the situation of an auto - covariance matrix @xmath8 is completely different . to author",
    "s best knowledge , none of the existing literature in random matrix theory treats the sample auto - covariance matrix and the limit for its eigenvalue distribution found in this paper is new .",
    "there are basically two major differences between @xmath8 and @xmath39 .",
    "first , while @xmath39 is a non - negative symmetric random matrix , @xmath8 is even not symmetric and we must rely on singular value distributions which are in general much more involved than eigenvalue distributions .",
    "secondly , because of the positive lag @xmath41 , the summands in @xmath8 are no more independent as it is the case for the sample covariance matrix @xmath39 .",
    "this again makes the analysis of @xmath8 more difficult . as a consequence of these major differences ,",
    "several new techniques are introduced in the paper in order to complete the proofs , although the general strategy is common in the random matrix theory ( see @xcite ) .",
    "for example , the characterization of the stieltjes transform of the limiting distribution is obtained via a system of equations due to the time delay @xmath41 where for the case of sample covariance matrix , the characterization is given by a single equation(@xcite , @xcite ) .",
    "the rest of the paper is organized as follows .",
    "the main theorem of the paper is introduced in section  [ results ] .",
    "section  [ proofs ] details the proof of the main theorem when time lag @xmath42 . section  [ extension ] generalizes the proof from time lag @xmath42 to any given positive number .",
    "meanwhile , in contrast to other aspects discussed above , the preliminary steps of truncation , centralization and standardization of the matrix entries are similar to the case of a sample covariance matrix .",
    "they are given in appendix  [ app ] .",
    "to ease the reading of the proof , technical lemmas are grouped in section  [ lemmas ] .",
    "in this paper , we intend to derive the limiting singular value distribution of the lag@xmath4 auto - covariance matrix defined in .",
    "it will be done in two steps .",
    "we derive the main result first for the lag-1@xmath43 sample auto - covariance matrix @xmath44 .",
    "it turns out that the general case @xmath45 is essentially the same and the extension is easily obtained .",
    "the details of the extension are given in section  [ extension ] .",
    "therefore , we consider the lag-1 sample auto - covariance matrix @xmath46 . by definition ,",
    "it is equivalent to study the limiting spectral distribution(lsd ) of the matrix @xmath47 alternatively , @xmath48 where @xmath49 , @xmath50 .",
    "here we define a modified version of the a matrix , @xmath51 where @xmath52 is the j - th row of @xmath53 , and @xmath54 the j - th row of @xmath55 . as @xmath56 and @xmath57",
    "have same nonzero eigenvalues , the lsd of @xmath56 can be derived from the lsd of @xmath57 .    the main result of the paper is    [ th1 ] let the following assumptions hold :    * @xmath58 are independent p - dimensional real - valued random vectors with independent entries satisfying condition : @xmath59 for some constant @xmath60 and for any @xmath61 , @xmath62 * as @xmath63 , the sample size @xmath64 and @xmath65 .",
    "then ,    * as @xmath66 , almost surely , the empirical spectral distribution @xmath67 of @xmath57 , converges to a non - random probability distribution @xmath68 whose stieltjes transform @xmath69 , @xmath70 , satisfies the equation @xmath71 * moreover , for @xmath72 , equation admits an unique solution @xmath73 with positive imaginary part and the density function of the lsd @xmath68 is : + [ cols= \" < \" , ]     where @xmath74 , for @xmath75 and @xmath76 , for @xmath77 , with @xmath78 given in equation and @xmath79 given in equation .",
    "therefore , equation admits at least one solution @xmath73 that corresponds to this density function of the lsd @xmath68 . as for the uniqueness , suppose there exists another solution @xmath80 that satisfies equation , then there should be another density @xmath81 that corresponds to @xmath80 while @xmath82 .",
    "however , it can be seen from the previous deductions that the density function is unique .",
    "therefore , @xmath83 , @xmath84 .",
    "equation admits one unique solution .      under the same conditions in * theorem [ th1 ] * , the esd of @xmath56 converges to a non - random limit distribution @xmath85 with stieltjes transform @xmath86 . on the other hand",
    ", the esd of @xmath57 converges to @xmath68 with stieltjes transform @xmath69 satisfying @xmath87    since it s known that @xmath88 conclusively we have @xmath89 substituting into the equation of @xmath90 we can get the equation of @xmath91 , which is @xmath92",
    "so far in previous sections , we have focused on the singular value distribution of the lag-1 sample auto - covariance matrix @xmath93 , while in this section , for any given positive integer @xmath41 , we discuss the singular value distribution of the lag-@xmath41 sample auto - covariance matrix @xmath94 .    here",
    "we follow exactly the same strategy used in the derivation of the lsd of the lag-1 sample auto - covariance matrix .",
    "it s easy to see that the difference between @xmath35 and @xmath8 lies in that we have now for @xmath8 , @xmath95 meanwhile , the other matrices and notations remain the same using however the new definition of the @xmath96 above .",
    "consequently , equation becomes @xmath97 equation becomes    @xmath98    thus , according to equation and , we still have @xmath99 meanwhile , by lemma [ lem2 ] , we still have @xmath100 then by equation , we have @xmath101 similarly , as for the second equation satisfied by @xmath102 and @xmath103 , equation persists .",
    "@xmath104 therefore , the system of equations satisfied by @xmath102 and @xmath103 remains the same when the time lag changes from 1 to @xmath41 .",
    "in other words , for a given positive time lag @xmath41 , the singular value distribution of @xmath105 is the same with that of @xmath35 established in theorem [ th1 ] .",
    "[ lem3 ] under the same assumptions in * theorem [ th1 ] * , we have , @xmath106 , almost surely , @xmath107 @xmath108 @xmath109 @xmath110 where the @xmath111 terms are uniform in @xmath112 .",
    "we detail the proof of and the proofs of , and are very similar , thus omitted .",
    "denote @xmath113 by @xmath114 , @xmath115 , then we have @xmath116 where @xmath117 is the image part of @xmath118 .",
    "following the scheme of * lemma 9.1 * of @xcite it s easy to see that @xmath119 where @xmath120 what s more , @xmath121 @xmath122 consider a graph g with @xmath123 edges that link @xmath124 to @xmath125 and @xmath126 to @xmath127 , @xmath128 .",
    "it s easy to see that for any nonzero term , the vertex degrees of the graph are not less than 2 .",
    "write the non - coincident vertices as @xmath129 with degrees @xmath130 greater than 1 , then , similarly in * lemma 9.1 * of @xcite , we have , @xmath131 @xmath132 therefore , by the borel - cantelli lemma , we have , @xmath133 , @xmath134 where the @xmath111 terms are uniform in @xmath112 .    [ lem1 ] under the same assumptions in * theorem [ th1 ] * , we have , @xmath106 , @xmath135 , almost surely , @xmath136 @xmath137 where the @xmath111 terms are uniform in @xmath112 .",
    "notice that , for @xmath135 , @xmath138 here @xmath139 represents the power @xmath7 of the @xmath140 matrix @xmath141 , we use @xmath142 to denote the transpose of matrix @xmath141 .",
    "denote , for @xmath143 , @xmath144 @xmath145 it s easy to see that @xmath146 in addition , for any @xmath112 , @xmath147 now we can derive the recursion equations between @xmath148 and @xmath149 .",
    "firstly , for @xmath148 , @xmath135 , since @xmath150 taking trace and dividing @xmath7 on both sides of the equation , we get @xmath151+o_{a.s.}(1)\\\\ = & \\frac{p}{t}\\dfrac{1+y_0}{(1+y_0)^2-\\alpha x_1 ^ 2}\\left[\\frac{1}{t}tr(\\tilde{c}b^{-1}(\\alpha)p_1^k)-\\frac{\\alpha x_1}{1+y_0}\\cdot\\frac{1}{t}tr{\\left ( } b^{-1}(\\alpha)p_1^{k+1}{\\right ) } \\right]+o_{a.s.}(1),\\end{aligned}\\ ] ] i.e. @xmath152 particularly , for @xmath153 , we have @xmath154 similarly , for @xmath149 , @xmath143 , @xmath155+o_{a.s.}(1)\\\\ & = \\frac{p}{t}\\cdot \\dfrac{1+y_0}{(1+y_0)^2-\\alpha x_1 ^ 2}\\cdot\\left[\\frac{1}{t}tr(b^{-1}(\\alpha)p_1^k)-\\frac{x_1}{1+y_0}\\cdot\\frac{1}{t}tr{\\left ( } \\tilde{c}b^{-1}(\\alpha)p_1^{k-1}{\\right ) } \\right]+o_{a.s.}(1),\\end{aligned}\\ ] ] i.e. @xmath156    particularly , for @xmath157 , we have @xmath158 note that @xmath159 then we have either @xmath160 or @xmath161 .",
    "if @xmath160 , according to equation , we have @xmath162 , then according to equation , we have @xmath163 , recursively , we have for all @xmath135 , @xmath164 otherwise , if @xmath161 , according to equation , we have @xmath165 , then according to equation , we have @xmath166 , then according to equation , we have @xmath167 , recursively , we still have for all @xmath135 , @xmath164 therefore we have , @xmath106 , @xmath135 , almost surely , @xmath136 @xmath137 where the @xmath111 terms are uniform in @xmath112 .",
    "[ lem2 ] extension of lemma [ lem1 ] to time lag @xmath41 :    we have , @xmath106 , @xmath168 $ ] , almost surely , @xmath169 @xmath170 where the @xmath111 terms are uniform in @xmath112 .",
    "denote , for @xmath171 $ ] , @xmath144 @xmath172 it s easy to see that @xmath173 + 1}=y_{\\left[\\frac{t}{\\tau}\\right]+1}=0.\\ ] ] in addition , for any @xmath112 , @xmath174 now we can derive the recursion equations between @xmath148 and @xmath149 .",
    "firstly , for @xmath148 , @xmath171 $ ] , @xmath175,\\end{aligned}\\ ] ] i.e. @xmath176.\\ ] ]    similarly , for @xmath149 , @xmath171 + 1 $ ] , @xmath177+o_{a.s.}(1),\\end{aligned}\\ ] ] i.e. @xmath178 + 1.\\ ] ]    particularly , for @xmath179 + 1 $ ] , we have @xmath180 + 1}=\\frac{p}{t}\\cdot \\dfrac{1+y_0}{(1+y_0)^2-\\alpha x_1 ^ 2}\\cdot x_{\\left[\\frac{t}{\\tau}\\right]+1}-\\frac{p}{t}\\cdot \\dfrac{x_1}{(1+y_0)^2-\\alpha x_1 ^ 2}\\cdot y_{\\left[\\frac{t}{\\tau}\\right]}+o_{a.s.}(1).\\ ] ] note that @xmath173 + 1}=y_{\\left[\\frac{t}{\\tau}\\right]+1}=0,\\ ] ] following the same arguments in lemma [ lem1 ] , we have , @xmath106 , @xmath171 $ ] , almost surely , @xmath169 @xmath170 where the @xmath111 terms are uniform in @xmath112 .",
    "recall that @xmath181 , @xmath182 are independent real - valued random variables with @xmath183 , and we are interested in is the lsd of time - lagged covariance matrix                                                                  consequently , @xmath220 similarly , we can prove that the last term in equation tends to zero almost surely .",
    "as for the first term , we have @xmath221 therefore @xmath222 now , we consider @xmath223 , @xmath224 firstly , for @xmath225 , since @xmath226 , @xmath227 moreover , @xmath228 therefore @xmath229 , a.s .",
    "next for @xmath230 ,              according to theorem a.46 of @xcite , we have @xmath239\\\\    & \\cdot\\left[\\frac{1-\\hat{\\sigma}_{ij}^{-4}}{t^{2}}tr\\left(\\left(\\sum_{i=1}^{t}\\hat{\\varepsilon}_{i}\\hat{\\varepsilon}_{i-1}^{t}\\right)\\left(\\sum_{j=1}^{t}\\hat{\\varepsilon}_{j-1}\\hat{\\varepsilon}_{j}^{t}\\right)\\right)\\right]\\\\    & =    2\\left(1-\\hat{\\sigma}_{ij}^{-8}\\right)\\left[\\frac{1}{pt^{2}}tr\\left(\\left(\\sum_{i=1}^{t}\\hat{\\varepsilon}_{i}\\hat{\\varepsilon}_{i-1}^{t}\\right)\\left(\\sum_{j=1}^{t}\\hat{\\varepsilon}_{j-1}\\hat{\\varepsilon}_{j}^{t}\\right)\\right)\\right]^{2}.\\end{aligned}\\ ] ]              bai , z.d . and silverstein , j.w .",
    "no eigenvalues outside the support of the limiting spectral distribution of large - dimensional sample covariance matrices . _",
    "the annals of probability _ , vol .",
    "26 , no . 1 , 316 - 345 .      bai , z.d . and",
    "yin , y.q.(1993 ) .",
    "limit of the smallest eigenvalues of large dimensional covariance matrix .",
    "probab . _ * 21*(3 ) , 1275 - 1294 .",
    "lam , c. , yao , q. and bathia , n. ( 2011 ) . estimation of latent factors for high - dimensional time series .",
    "_ biometrika _ * 98(4 ) * , 901 - 918 .",
    "lam , c. and yao , q. ( 2012 ) .",
    "factor modeling for high - dimensional time series : inference for the number of factors .",
    "_ * 40(2 ) * , 694 - 726 ."
  ],
  "abstract_text": [
    "<S> let @xmath0 be a sequence of independent @xmath1dimensional random vectors and @xmath2 a given integer . from a sample @xmath3 of the sequence , </S>",
    "<S> the so - called lag@xmath4 auto - covariance matrix is @xmath5 . </S>",
    "<S> when the dimension @xmath6 is large compared to the sample size @xmath7 , this paper establishes the limit of the singular value distribution of @xmath8 assuming that @xmath6 and @xmath7 grow to infinity proportionally and the sequence satisfies a lindeberg condition . </S>",
    "<S> compared to existing asymptotic results on sample covariance matrices developed in random matrix theory , the case of an auto - covariance matrix is much more involved due to the fact that the summands are dependent and the matrix @xmath8 is not symmetric . </S>",
    "<S> several new techniques are introduced for the derivation of the main theorem .    , </S>"
  ]
}