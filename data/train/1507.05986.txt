{
  "article_text": [
    "the use of annotations to describe program properties for which run - time tests are to be generated has become frequent in dynamic programming languages , including assertion - based approaches in ( constraint ) logic programming ( ( c)lp )  @xcite , soft / gradual typing in functional programming  @xcite , and contract - based extensions in object - oriented programming  @xcite .",
    "however , run - time testing in these frameworks can generally incur high penalty in execution time and/or space over the standard program execution without tests .",
    "a number of techniques have been proposed to date to reduce this overhead , including simplifying the checks at compile time via static analysis  @xcite or reducing the frequency of checking , including for example testing only at a reduced number of points  @xcite .",
    "our objective is to develop an approach to run - time testing that is efficient while being minimally obtrusive and remaining exhaustive .",
    "we present an approach based on the use of memoization to cache intermediate results of check evaluation in order to avoid repeated checking of previously verified properties over the same data structure .",
    "memoization has of course a long tradition in ( c)lp in uses such as tabling resolution  @xcite , including also sharing and memoizing tabled sub - goals  @xcite , for improving termination .",
    "memoization has also been used in program analysis  @xcite , where tabling resolution is performed using abstract values . however , in tabling and analysis what is tabled are call - success patterns and in our case the aim is to cache the results of test execution .",
    "while the approach that we propose is general and system - independent , we will present it for concreteness in the context of the ciao run - time checking framework .",
    "the ciao model  @xcite is well understood , and different aspects of it have been incorporated in popular ( c)lp systems , such as ciao , swi , and xsb  @xcite . using this concrete model allows us to provide an operational semantics of programs with checks and caching , as well as a concrete implementation from which we derive experimental results .",
    "we also present a program transformation for implementing the run - time checks that is more efficient than previous proposals  @xcite .",
    "our experimental results provide evidence that using a relatively small cache leads to significant decreases in run - time checking overhead .",
    "[ [ basic - notation - and - standard - semantics . ] ] basic notation and standard semantics .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we recall some concepts and notation from standard ( c)lp theory .",
    "atom _ has the form @xmath0 where @xmath1 is a predicate symbol of arity @xmath2 and @xmath3 are terms .",
    "a _ constraint _ is a conjunction of expressions built from predefined predicates ( such as term equations or inequalities over the reals ) whose arguments are constructed using predefined functions ( such as real addition ) .",
    "literal _ is either an atom or a constraint .",
    "a _ goal _ is a finite sequence of literals .",
    "is of the form @xmath4 where @xmath5 , the _ head _ , is an atom and @xmath6 , the _ body _ , is a possibly empty finite sequence of literals . a _",
    "constraint logic program _ , or _",
    ", is a finite set of rules .",
    "the _ definition _ of an atom @xmath7 in a program , @xmath8 , is the set of variable renamings of the program rules s.t .",
    "each renaming has @xmath7 as a head and has distinct new local variables .",
    "we assume that all rule heads are _ normalized _",
    ", i.e. , @xmath5 is of the form @xmath9 where the @xmath10 are distinct free variables .",
    "let @xmath11 be the constraint @xmath12 restricted to the variables of the syntactic object @xmath13 .",
    "we denote _ constraint entailment _ by @xmath14 , so that @xmath15 denotes that @xmath16 entails @xmath17 .",
    "then , we say that @xmath17 is _ weaker _ than @xmath16 .",
    "the operational semantics of a program is given in terms of its `` derivations '' , which are sequences of `` reductions '' between `` states '' .",
    "a _ state _",
    "@xmath18 consists of a goal @xmath19 and a constraint store ( or _ store _ for short ) @xmath12 .",
    "we use : : to denote concatenation of sequences and we assume for simplicity that the underlying constraint solver is complete .",
    "a state @xmath20 where @xmath13 is a literal can be _ reduced _ to a state @xmath21 as follows :",
    "@xmath22 if @xmath13 is a constraint and @xmath23 is satisfiable .",
    "2 .   @xmath24 if @xmath13 is an atom of the form @xmath25 , + for some rule @xmath26 @xmath27 .",
    "we use @xmath28 to indicate that a reduction can be applied to state @xmath29 to obtain state @xmath21 .",
    "also , @xmath30 indicates that there is a sequence of reduction steps from state @xmath29 to state @xmath21 .",
    "a _ query _ is a pair @xmath31 , where @xmath13 is a literal and @xmath12 a store , for which the ( c)lp system starts a computation from state @xmath32",
    ". a finished derivation from a query @xmath31 is _ successful _ if the last state is of the form @xmath33 , where @xmath34 denotes the empty goal sequence . in that case",
    ", the constraint @xmath35 is an _ answer _ to @xmath29 .",
    "we denote by @xmath36 the set of answers to a query @xmath37 .",
    "[ [ assertions - and - their - semantics . ] ] -assertions and their semantics .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    assertions are linguistic constructions for expressing properties of programs .",
    "herein , we will use the -assertions of  @xcite , for which we follow the formalization of  @xcite .",
    "these assertions allow specifying certain conditions on the constraint store that must hold at certain points of program derivations .",
    "in particular , they allow stating sets of _ preconditions _ and _ conditional postconditions _ for a given predicate .",
    "a set of assertions for a predicate is of the form : + [ -8 mm ]    @xmath38    where @xmath39 is a normalized atom that denotes the predicate that the assertions apply to , and the @xmath40 and @xmath41 are ( dnf ) formulas that refer to the variables of @xmath39 .",
    "we assume the @xmath40 and @xmath41 to be dnf formulas of _ prop _ literals , which specify conditions on the constraint store .",
    "a prop literal @xmath13 _ succeeds trivially _ for @xmath12 in program @xmath42 , denoted @xmath43 , iff @xmath44 such that @xmath45 .    a set of assertions as above states that in any execution state @xmath46 at least one of the @xmath40 conditions should hold , and that , given the @xmath47 pair(s ) where @xmath40 holds , then , if @xmath39 succeeds , the corresponding @xmath41 should hold upon success .",
    "more formally , given a predicate represented by a normalized atom @xmath39 , and the corresponding set of assertions is @xmath48 , with @xmath49 @xmath50  such assertions are normalized into a set of _ assertion conditions _ @xmath51 , with : @xmath52    if there are no assertions associated with @xmath39 then the corresponding set of conditions is empty .",
    "the set of assertion conditions for a program is the union of the assertion conditions for each of the predicates in the program .",
    "the @xmath53 conditions encode the checks that ensure that the calls to the predicate represented by @xmath39 are within those admissible by the set of assertions , and we thus call them the _ calls assertion conditions_. the @xmath54 conditions encode the checks for compliance of the successes for particular sets of calls , and we thus call them the _ success assertion conditions_.    we now turn to the operational semantics with assertions , which checks whether assertion conditions hold or not while computing the derivations from a query . in order to keep track of any violated assertion conditions , we add labels to the assertion conditions .",
    "given the atom @xmath55 and the corresponding set of assertion conditions @xmath56 , @xmath57 denotes the set of _ labeled _ assertion condition instances for @xmath55 , where each is of the form @xmath58 , such that @xmath59 , @xmath60 ( or @xmath61 ) , @xmath62 is a renaming s.t .",
    "@xmath63 , @xmath64 ( or @xmath65 ) , and @xmath66 is an identifier that is unique for each @xmath67 .",
    "we also introduce an extended program state of the form @xmath68 , where @xmath69 denotes the set of identifiers for falsified assertion condition instances . for the sake of readability ,",
    "we write labels in _ negated _ form when they appear in the error set .",
    "we also extend the set of literals with syntactic objects of the form @xmath70 where @xmath13 is a literal and @xmath66 is an identifier for an assertion condition instance , which we call _ check literals_. thus , a _",
    "literal _ is now a constraint , an atom or a check literal .",
    "we can now recall the notion of _ reductions in programs with assertions _ from  @xcite , which is our starting point : a state @xmath71 , where @xmath13 is a literal , can be _ reduced _ to a state @xmath21 , denoted @xmath72 , as follows :    1 .   if @xmath13 is a constraint or @xmath73 , then @xmath74 where @xmath75 and @xmath76 are obtained in a same manner as in @xmath77 2 .   if @xmath13 is an atom and @xmath78 , then @xmath79 where : @xmath80 and @xmath81 such that + @xmath82 .",
    "3 .   if @xmath13 is a check literal @xmath83 , then @xmath84 where @xmath85",
    "the standard operational semantics with run - time checking revisited in the previous section has the same potential problems as other approaches which perform exhaustive tests : it can be prohibitively expensive , both in terms of time and memory overhead .",
    "for example , checking that the first argument of the ` length/2 ` predicate is a list at each recursive step turns the standard @xmath86 algorithm into @xmath87 .    as mentioned in the introduction ,",
    "our objective is to develop an effective solution to this problem based on memoizing property checks .",
    "an observation that works in our favor is that many of the properties of interest in the checking process ( such as , e.g. , regtype instantiation checking ) are monotonic .",
    "that is , we will concentrate on properties such that , for all property checks @xmath13",
    "if @xmath88 and @xmath89 then @xmath90 . in this context it clearly seems attractive to keep @xmath13 in the store so that it does not need to be recomputed .",
    "however , memoizing every checked property may also have prohibitive costs in terms of memory overhead .",
    "a worst - case scenario would multiply the memory needs by the number of call patterns to properties , which can be large in realistic programs .",
    "in addition , looking for stored results in the store obviously also has a cost that must be taken into account .",
    "[ [ operational - semantics - with - caching . ] ] operational semantics with caching .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we base our approach on an operational semantics which modifies the run - time checking to maintain and use a _",
    "cache store_. the _ cache store _",
    "@xmath91 is a special constraint store which temporarily holds results from the evaluation of _ prop _ literals w.r.t .",
    "the standard constraint store @xmath12 .",
    "we introduce an extended program state of the form @xmath92 and a _ cached _ version of `` _ _ succeeds trivially _ _ '' : given a prop literal @xmath13 , it succeeds trivially for @xmath12 and @xmath91 in program @xmath42 , denoted @xmath93 , iff either @xmath94 or @xmath95 .",
    "also , the cache store is updated based on the results of the prop checks , formalized in the following definitions :    let us consider a dnf formula @xmath96 , where each @xmath97 is a prop literal . by @xmath98,j\\in[0:m(i)]\\}$ ] we denote the set of all literals which appear in @xmath99 .",
    "the _ cache update _ operation is defined as a function @xmath100 such that : +    @xmath101    note that a precise definition of cache update is left open in this semantics .",
    "contrary to @xmath12 , updates to the cache store @xmath91 are not monotonic since we allow the cache to `` forget '' information as it fills up , i.e. , we assume from the start that @xmath91 is of limited capacity .",
    "however , that information can always be recovered via recomputation of property checks . in practice",
    "the exact cache behavior depends on parts of the low - level abstract machine state that are not available at this abstraction level .",
    "it will be described in detail in later sections .",
    "[ def : asr - cache - reductions ] a state @xmath102 , where @xmath13 is a literal , can be _ reduced _ to a state @xmath21 , denoted @xmath72 , as follows :    1 .   if @xmath13 is a constraint or @xmath73 , then @xmath103 where @xmath75 and @xmath76 are obtained in a same manner as in @xmath77 2 .",
    "if @xmath13 is an atom and @xmath78 , then + @xmath104 where : @xmath105 + @xmath106 and @xmath81 such that + @xmath107 .",
    "3 .   if @xmath13 is a check literal @xmath83 , then @xmath108 where @xmath109 and @xmath110",
    "we use the traditional definitional transformation  @xcite as a basis of our implementation of the operational semantics with cached checks .",
    "this consists of a program transformation that introduces _ wrapper _ predicates that check calls and success assertion conditions while running on a standard ( c)lp system .",
    "however , we propose a novel transformation that , in contrast to previous approaches , groups all assertion conditions for the same predicate together to produce optimized checks .    given a program @xmath111 , for every predicate @xmath1 the transformation replaces all clauses @xmath112 by @xmath113 , where @xmath114 is a new predicate symbol , and inserts the wrapper clauses given by @xmath115 .",
    "the wrapper generator is defined as follows : @xmath116 where @xmath117 and @xmath118 are the optimized compilation of pre- and postconditions @xmath119 and @xmath120 respectively , for @xmath121 @xmath122 ; and the additional _ status _",
    "variables @xmath123 are used to communicate the results of each @xmath40 evaluation to the corresponding @xmath124 check . this way , without any modifications to the literals calling @xmath1 in the bodies of clauses in @xmath111 ( and in any other modules that contain calls to @xmath1 ) , after the transformation run - time checks will be performed for all these calls to @xmath1 since @xmath1 ( now @xmath114 ) will be accessed via the wrapper predicate .",
    "the compilation of checks for assertion conditions emits a series of calls to a predicate , which accepts as the first argument a property and unifies its second argument with or , depending on whether the property check succeeded or not .",
    "the results of those reified checks are then combined and evaluated as boolean algebra expressions using bitwise operations and the prolog ` is/2 ` predicate .",
    "that is , the logical operators @xmath125 , @xmath126 , and @xmath127 used in encoding assertion conditions are replaced by their bitwise logic counterparts , , , respectively .",
    "the purpose of reification and this compilation scheme is to make it possible to optimize the logic formulae containing properties that result from the combination of several assertions ( i.e. , the assertion conditions ) .",
    "the optimization consists in reusing the reified status @xmath128 when possible , which happens in two ways .",
    "first , the _ prop _ literals which appear in @xmath129 or @xmath130 formulas are only checked once ( via ) and then their reified status @xmath128 is reused when needed .",
    "second , the reified status of each @xmath129 conjunction is reused both in @xmath117 and @xmath118 .    in practice",
    "the ( @xmath131,@xmath114 ) clause generator shares the minimum number of status variables and omits _ trivial _ assertion conditions , i.e. , those with ` true ` conditions in one of their parts .",
    "for instance , excluding @xmath132 preserves low - level optimizations such as last call optimization . and @xmath132 predicates follow the usual bytecode - based compilation path , note that they have a concrete structure that is amenable to further optimizations ( like specialized wam - level instructions or a dedicated interpreter ) . ]",
    "[ ex : asr - transform ] consider the following annotated program :    .... : - pred p(x , y ) : ( int(x ) , var(y ) ) = > ( int(x ) , int(y ) ) .",
    "% a1 : - pred p(x , y ) : ( int(x ) , var(y ) ) = > ( int(x ) , atm(y ) ) . % a2 : - pred p(x , y ) : ( atm(x ) , var(y ) ) = > ( atm(x ) , atm(y ) ) .",
    "p(1,42 ) .",
    "p(2,gamma ) .",
    "p(a , alpha ) . ....    from the set of assertions @xmath133 the following assertion conditions are constructed :    @xmath134    the resulting optimized program transformation is : + [ 1 mm ]    [ cols= \" < , < \" , ]     [ [ algorithm - for - checking - regular - types - with - caches . ] ] algorithm for checking regular types with caches .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we describe the regcheck algorithm for regtype checking using caches in algorithm  [ alg : regcheck ] .",
    "the predicate acts as the interface between regcheck and the runtime checking framework .",
    "find @xmath135 so that @xmath136 , otherwise false true true true false    the algorithm is derived from the standard definition of _ run _ on tree automata .",
    "a run of a tree automaton @xmath137 on a tree @xmath138 ( terms over @xmath139 ) is a mapping @xmath140 assigning a state to each occurrence ( subterm ) of @xmath141 of @xmath142 such that : @xmath143 a term @xmath142 is recognized by @xmath144 if @xmath145 . for deterministic top - down recognition ,",
    "the algorithm starts with the single state in @xmath146 ( which for simplicity , we will use to identify each regtype and its corresponding automata ) and follows the rules _ backwards_. the tree automata transition rules for a regtype are consulted with the functions @xmath147 , @xmath148 ( the i - th argument of a constructor or term @xmath149 ) , and @xmath150 ( the functor symbol , including arity , of a constructor or term @xmath149 ) . once there is a functor match , the regtypes of the arguments",
    "are checked recursively . to speed up checks ,",
    "the cache is consulted ( @xmath151 searches for @xmath152 ) before performing costly recursion , and _",
    "valid _ checks inserted ( @xmath153 inserts @xmath152 ) if needed ( e.g. , using heuristics , explained below ) .",
    "the cache for storing results of regular type checking is implemented as a _ set _ data structure that can efficiently insert and look up @xmath152 pairs , where @xmath142 is a term address and @xmath154 a regular type identifier .",
    "the specific implementation depends on the cache heuristics , as described below .",
    "[ [ complexity . ] ] complexity .",
    "+ + + + + + + + + + +    it is easy to show that complexity has @xmath155 best case ( if @xmath142 was cached ) and @xmath86 worst case , with @xmath2 being the number of tree nodes ( or term size ) . in practice",
    ", the caching heuristics can drastically affect performance .",
    "for example , assume a full binary tree of @xmath2 nodes .",
    "caching all nodes at levels multiple of @xmath156 will need @xmath157 entries , with a constant cost for the worst case check ( at most @xmath158 will be checked , independently of the size of the term ) .",
    "[ [ cache - implementation - and - heuristics . ] ] cache implementation and heuristics .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in order to decide what entries are added and what entries are evicted to make room for new entries on cache misses , we have implemented several caching heuristics and their corresponding data structures . entry eviction is controlled by _ replacement policies _ :    * least - recently used ( lru ) replacement and fully associative . implemented as a hash table whose entries are nodes of a doubly linked list .",
    "the most recently accessed element is moved to the head and new elements are also added to the head . if cache size exceeds the maximal size allowed , the cache is pruned .",
    "* direct - mapped cache with collision replacement , with a simple hash function based on modular arithmetic on the term address .",
    "this is simpler but less predictable .",
    "the insertion of new entries is controlled by the caching _ contexts _ , which include the regular type being checked and the location of the check :    * we do not cache simple properties ( like primitive type tests , e.g. , ` integer/1 ` , etc ) , where caching is more expensive than recomputing .",
    "* we use the check depth level in the cache interface for recursive regular types .",
    "checks beyond this threshold depth limit are not cached .",
    "this gives priority to roots of data structures over internal subterms which may pollute the cache .",
    "[ [ low - level - c - implementation . ] ] low - level c implementation .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + +    in our prototype , this algorithm is implemented in c with some specialized cases ( as required for our wam - based representation of terms , e.g. , to deal with atomic terms , list constructors , etc . ) .",
    "the regtype definition is encoded as a map between functors ( name and arity ) and an array of @xmath159 states for each argument .",
    "for a small number of functors , the map is implemented as an array .",
    "efficient lookup for many functors is achieved using hash maps .",
    "additionally , a number of implicit transition rules exist for primitive types ( any term to @xmath160 , integers to @xmath161 , etc . ) that are handled as special cases .",
    "to study the impact of caching on run - time overhead , we have evaluated the run - time checking framework on a set of 7 benchmarks , for regular types .",
    "we consider benchmarks where we perform a series of element insertions in a data structure .",
    "benchmarks , , , and ( binary ) were adapted from the ciao libraries ; benchmarks , and were adapted from the yap libraries .",
    "these benchmarks can be divided into 4 groups :    a.   simple list - based data structures : , ; b.   balanced tree - based structures that do not change the structural properties of their nodes on balancing : , ; c.   balanced tree - based structures that change node properties : ( changes the number of node children ) , ( changes node color ) ; d.   unbalanced tree structures ( ) .    for each run of the benchmark suite the following parameters were varied : cache replacement policy ( lru , direct mapping ) , cache size ( 1 to 256 cells ) , and check depth threshold ( 1 to 5 , and `` infinite '' threshold for unlimited check depth ) . table  [ tbl : bmks - results ] summarizes the results of the experiments . for each combination of the parameters it reports the optimal caching policy , lru ( l ) or direct mapping ( d ) . also , for each of the benchmarks it reports an interval within which the worst case check depth varies .",
    "r|c||rlrl|rlrl|rlrl|rl & & & & & & & + & & & & & & & + & & & & & & & + & & @xmath162 & 2 & @xmath162 & 2 & @xmath162 & 2 & @xmath162 & 2 & @xmath162 & 2 & @xmath162 & 2 & @xmath162 + & 256 & d & d & d & d & l & l & l & l & l & l & l & d & d & l + & 128 & d & d & d & d & l & l & l & l & d & d & l & d & d & d + & 64 & d & d & d & d & l & l & l & l & d & d & l & d & d & d + & 32 & d & d & d & d & d & d & d & d & d & d & d & d & d & d + & dm & & & & & & & + & lru & & & & & & & +    [ tbl : bmks - results ]    the experiments show that the overhead of checks with depth threshold 2 ( storing the regtype of the check argument and the regtypes of its arguments ) is smaller than or equal to the one obtained with unlimited depth limit  ( fig  [ fig : infworse2 ] ) .",
    "a depth limit of 1 does not allow checks to store enough useful information about terms of most of the data structures ( compare the overhead increase for with this and bigger limits ) , while unlimited checks tend to overwrite this information multiple times , so that it can not be reused .",
    "at the same time , for data structures represented by large nested terms ( e.g. , nodes of ) , deeper limits ( 3 or 4 ) for small inputs seem more beneficial for capturing such term structure .",
    "it can also be observed that the lower cost of element insert / lookup operations with the dm cache replacement policy results in having lower total overhead than with the lru policy .",
    "0.32 , and dm ( top row ) and lru ( bottom row ) policies in cache of 256 elements.,title=\"fig : \" ]       0.32 , and dm ( top row ) and lru ( bottom row ) policies in cache of 256 elements.,title=\"fig : \" ]       0.32 , and dm ( top row ) and lru ( bottom row ) policies in cache of 256 elements.,title=\"fig : \" ]    0.32 , and dm ( top row ) and lru ( bottom row ) policies in cache of 256 elements.,title=\"fig : \" ]       0.32 , and dm ( top row ) and lru ( bottom row ) policies in cache of 256 elements.,title=\"fig : \" ]       0.32 , and dm ( top row ) and lru ( bottom row ) policies in cache of 256 elements.,title=\"fig : \" ]    while even with caching the cost of the run - time checks still remains significant , caching does reduce overhead by 1 - 2 orders of magnitude with respect to the cost of run - time checking without caching  ( fig .",
    "[ fig : abs_rel ] ) . also , the slowdown ratio of programs with run - time checks using caching is almost constant , in contrast with the linear ( or worse ) growth in the case where caching is not used .",
    "an important issue that has to be taken into account here is that most of the benchmarks are rather simple , and that performing insert operations is much less costly that performing run - time checks on the arguments of this operation .",
    "this explains the observation that checking overhead is the highest for the benchmark  ( fig  [ fig : infworse2 ] ) , while it is one of the simplest used in the experiments .",
    "0.35        0.35     another factor that affects the overhead ratio is cache size . for smaller caches cell rewritings occur more often , and thus the optimal cache replacement policy in such cases is the one with the cheapest operations . for instance , for cache size 32",
    "the optimal policy for all benchmark groups is dm , while for other cache sizes lru is in some cases better as it allows optimizing cell rewritings .",
    "this observation is also confirmed by the maximal check depth in the worst case , which is almost half on average for the benchmarks for which lru is the optimal policy ( fig  [ fig : allrt_b_c ] ) .",
    "0.35        0.35     in the simple data structures of group ( a ) the experiments show that it is beneficial to have cheaper cache operations ( like those of caches with dm caching policy ) , since such structures do not suffer from cache cell rewritings as much as more complex structures .",
    "the same observation is still true for group ( d ) , where for some inputs the binary tree might grow high and regtype checks of leaves will pollute the cache with results of checks for those inner nodes on the path , that are not in the cache , overwriting cache entries with regtypes of previously checked nodes .",
    "the dm policy also happens to show better results for group ( c ) for a similar reason .",
    "since data structures in this group change essential node properties during the tree insertion operation , this in practice means that sub - terms that represent inner tree nodes are ( re-)created more often . as a result , with the lru caching policy the cache would become populated by check results for these recently created nodes , while the dm caching policy would allow preserving ( and reusing ) some of the previously obtained results .",
    "the only group that benefits from lru is ( b ) , where this policy helps preserving check results for the tree nodes that are closer to the root ( and are more frequently accessed ) and most of the overwrites happen to cells that store leaves .",
    "more plots are available in the online appendix ( appendix a ) .",
    "we have presented an approach to reducing the overhead implied by run - time checking of properties based on the use of memoization to cache intermediate results of check evaluation , avoiding repeated checking of previously verified properties .",
    "we have provided an operational semantics with assertion checks and caching and an implementation approach , including a more efficient program transformation than in previous proposals .",
    "we have also reported on a prototype implementation and provided experimental results that support that using a relatively small cache leads to very significant decreases in run - time checking overhead .",
    "the idea of using memoization techniques to speed up checks has attracted some attention recently  @xcite .",
    "their work ( developed independently from ours ) is based on adding fields to data structures to store the properties that have been checked already for such structures .",
    "in contrast , our approach has the advantage of not requiring any modifications to data structure representation , or to the checking code , program , or core run - time system . compared to the approaches that reduce checking frequency our proposal has the advantage of being exhaustive ( i.e.",
    ", all tests are checked at all points ) while still being much more efficient than standard run - time checking .",
    "our approach greatly reduces the overhead when tests are being performed , while allowing the parts for which testing is turned off to execute at full speed without requiring recompilation .",
    "while presented for concreteness in the context of the ciao run - time checking framework , we argue that the approach is general , and the results should carry over to other programming paradigms ."
  ],
  "abstract_text": [
    "<S> the use of annotations , referred to as assertions or contracts , to describe program properties for which run - time tests are to be generated , has become frequent in dynamic programing languages . </S>",
    "<S> however , the frameworks proposed to support such run - time testing generally incur high time and/or space overheads over standard program execution . </S>",
    "<S> we present an approach for reducing this overhead that is based on the use of memoization to cache intermediate results of check evaluation , avoiding repeated checking of previously verified properties . compared to approaches that reduce checking frequency , </S>",
    "<S> our proposal has the advantage of being exhaustive ( i.e. , all tests are checked at all points ) while still being much more efficient than standard run - time checking . </S>",
    "<S> compared to the limited previous work on memoization , it performs the task without requiring modifications to data structure representation or checking code . </S>",
    "<S> while the approach is general and system - independent , we present it for concreteness in the context of the ciao run - time checking framework , which allows us to provide an operational semantics with checks and caching . </S>",
    "<S> we also report on a prototype implementation and provide some experimental results that support that using a relatively small cache leads to significant decreases in run - time checking overhead . to appear in theory and practice of logic programming ( tplp ) , proceedings of iclp 2015 . </S>"
  ]
}