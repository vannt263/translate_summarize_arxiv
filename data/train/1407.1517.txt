{
  "article_text": [
    "inverse problems are ubiquitous in science and engineering .",
    "perhaps the most popular family of inverse problems is to determine a set of parameters ( or a function ) given a set of indirect observations , which are in turn provided by a parameter - to - observable map plus observation uncertainties .",
    "for example , if one considers the problem of determining the heat conductivity of a thermal fin given measured temperature at a few locations on the thermal fin , then : i ) the desired unknown parameter is the distributed heat conductivity , ii ) the observations are the measured temperatures , iii ) the parameter - to - observable map is the mathematical model that describes the temperature on the thermal fin as a function of the heat conductivity ; indeed the temperature distribution is a solution of an elliptic partial differential equation ( pde ) whose coefficient is the heat conductivity , and iv ) the observation uncertainty is due to the imperfection of the measurement device and/or model inadequacy .",
    "the bayesian inversion framework refers to a mathematical method that allows one to solve statistical inverse problems taking into account all uncertainties in a systematic and coherent manner .",
    "the bayesian approach does this by reformulating the inverse problem as a problem in statistical inference , incorporating uncertainties in the observations , the parameter - to - observable map , and prior information on the parameter .",
    "in particular , we seek a statistical description of all possible ( set of ) parameters that conform to the available prior knowledge and at the same time are consistent with the observations .",
    "the solution of the bayesian framework is the so - called posterior measure that encodes the degree of confidence on each set of parameters as the solution to the inverse problem under consideration .",
    "mathematically the posterior is a surface in high dimensional parameter space .",
    "the task at hand is therefore to explore the posterior by , for example , characterizing the mean , the covariance , and/or higher moments .",
    "the nature of this task is to compute high dimensional integrals for which most contemporary methods are intractable .",
    "perhaps the most general method to attack these problems is the markov chain monte carlo ( mcmc ) method which shall be introduced in subsequent sections .",
    "let us now summarize the content of the paper .",
    "we start with the description of the statistical inverse problem under consideration in section .",
    "it is an inverse steady state heat conduction governed by elliptic pdes .",
    "we postulate a gaussian measure prior on the parameter space to ensure that the inverse problem is well - defined .",
    "the prior itself is a well - defined object whose covariance operator is the inverse of an elliptic differential operator and with the mean function living in the cameron - martin space of the covariance .",
    "the posterior is given by its radon - nikodym derivative with respect to the prior measure , which is proportional to the likelihood .",
    "since the rmhmc simulation method requires the gradient , hessian , and the derivative of the fisher information operator , we discuss , in some depth , how to compute the derivatives of the potential function ( the misfit functional ) with pde constraints efficiently using the adjoint technique in section . in particular , we define a fisher information operator and show that it coincides with the well - known gauss - newton hessian of the misfit .",
    "we next present a discretization scheme for the infinite bayesian inverse problem in section . specifically",
    ", we employ a standard continuous @xmath0-conforming finite element ( fem ) method to discretize both the likelihood and the gaussian prior .",
    "we choose to numerically compute the truncated karhunen - love expansion which requires one to solve an eigenvalue problem with fractional laplacian . in order to accomplish this task ,",
    "we use a matrix transfer technique ( mtt ) which leads to a natural discretization of the gaussian prior measure . in section",
    ", we describe the riemannian manifold hamiltonian monte carlo ( rmhmc ) and its variants at length , and its application to our bayesian inverse problem .",
    "section presents a low rank approach to approximate the fisher information matrix and its inverse efficiently .",
    "this is possibly due to the fact that the gauss - newton hessian , and hence the fisher information operator , is a compact operator .",
    "various numerical results supporting our proposed approach are presented in section .",
    "we begin this section with an extensive study and comparison of riemannian manifold mcmc methods for problems with two parameters , and end the section with @xmath1-parameter problem .",
    "finally , we conclude the paper in section with a discussion on future work .",
    "in order to clearly illustrate the challenges arising in pde - constrained inverse problems for mcmc based bayesian inference , we consider the following heat conduction problem governed by an elliptic partial differential equation in the open and bounded domain @xmath2 : @xmath3 where @xmath4 is the forward state , @xmath5 the logarithm of distributed thermal conductivity on @xmath6 , @xmath7 the unit outward normal on @xmath8 , and @xmath9 the biot number .    in the forward problem ,",
    "the task is to solve for the temperature distribution @xmath4 given a description of distributed parameter @xmath5 . in the inverse problem ,",
    "the task is to reconstruct @xmath5 given some available observations , e.g , temperature observed at some parts / locations of the domain @xmath6 .",
    "we initially choose to cast the inverse problem in the framework of pde - constrained optimization . to begin ,",
    "let us consider the following additive noise - corrupted pointwise observation model is sufficiently regular , i.e. @xmath10 , so that @xmath11 is , by the virtue of the sobolev embedding theorem , continuous , and therefore it is meaningful to measure @xmath11 pointwise . ]",
    "@xmath12 where @xmath13 is the total number of observation locations , @xmath14 the set of points at which @xmath4 is observed , @xmath15 the additive noise , and @xmath16 the actual noise - corrupted observations . in this paper",
    "we work with synthetic observations and hence there is no model inadequacy in .",
    "concatenating all the observations , one can rewrite as @xmath17 with @xmath18 denoting the map from the distributed parameter @xmath5 to the noise - free observables , @xmath19 being random numbers normally distributed by @xmath20 with bounded covariance matrix @xmath21 , and @xmath22 . for simplicity , we take @xmath23 , where @xmath24 is the identity matrix .    our inverse problem can be now formulated as @xmath25    @xmath26    where @xmath27 denotes the weighted euclidean norm induced by the canonical inner product @xmath28 in @xmath29 .",
    "this optimization problem is however ill - posed .",
    "an intuitive reason is that the dimension of observations @xmath30 is much smaller than that of the parameter @xmath5 , and hence they provide limited information about the distributed parameter @xmath5 . as a result ,",
    "the null space of the jacobian of the parameter - to - observation map @xmath31 is non - empty .",
    "indeed , we have shown that the gauss - newton approximation of the hessian ( which is the square of this jacobian , and is also equal to the full hessian of the data misfit @xmath32 evaluated at the optimal parameter ) is a compact operator @xcite , and hence its range space is effectively finite - dimensional .    one way to overcome",
    "the ill - posedness is to use _ tikhonov regularization _ ( see , e.g. , @xcite ) , which proposes to augment the cost functional with a quadratic term , i.e. , @xmath33 where @xmath34 is a regularization parameter , @xmath35 some regularization operator , and @xmath36 some appropriate norm .",
    "this method is a representative of deterministic inverse solution techniques that typically do not take into account the randomness due to measurements and other sources , though one can equip the deterministic solution with a confidence region by post - processing ( see , e.g. , @xcite and references therein ) .",
    "it should be pointed out that if the regularization term is replaced by the cameron - martin norm of @xmath5 ( the second term in ) , the tikhonov solution is in fact identical to the maximum a posteriori point in .",
    "however , such a point estimate is insufficient for the purpose of fully taking the randomness into account .    in this paper",
    ", we choose to tackle the ill - posedness using a _",
    "framework @xcite .",
    "we seek a statistical description of all possible @xmath5 that conform to some prior knowledge and at the same time are consistent with the observations .",
    "the bayesian approach does this by reformulating the inverse problem as a problem in _ statistical inference _ , incorporating uncertainties in the observations , the forward map @xmath31 , and prior information .",
    "this approach is appealing since it can incorporate most , if not all , kinds of randomness in a systematic manner .",
    "to begin , we postulate a gaussian measure @xmath37 on @xmath5 in @xmath38 where @xmath39 with the domain of definition @xmath40 where @xmath41 is the usual sobolev space .",
    "assume that the mean function @xmath42 lives in the cameron - martin space of @xmath43 , then one can show ( see @xcite ) that the measure @xmath44 is well - defined when @xmath45 ( @xmath46 is the spatial dimension ) , and in that case , any realization from the prior distribution @xmath44 is almost surely in the hlder space @xmath47 with @xmath48 .",
    "that is , @xmath49 , and the bayesian posterior measure @xmath50 satisfies the radon - nikodym derivative @xmath51 if @xmath31 is a continuous map from @xmath52 to @xmath29 .",
    "note that the radon - nikodym derivative is proportional to the the likelihood defined by @xmath53    the maximum a posteriori ( map ) point is defined as @xmath54 where @xmath55 denotes the weighted @xmath38 norm induced by the @xmath38 inner product @xmath56 .",
    "in this section , we briefly present the adjoint method to efficiently compute the gradient , hessian , and the third derivative of the cost functional .",
    "we start by considering the weak form of the ( first order ) forward equation : @xmath57 with @xmath58 as the test function . using the standard reduced space approach ( see , e.g. , a general discussion in @xcite and a detailed derivation in @xcite ) one can show that the gradient @xmath59 , namely the frchet derivative of the cost functional @xmath60 , acting in any direction @xmath61 is given by @xmath62 where the ( first order ) adjoint state @xmath63 satisfies the adjoint equation @xmath64 with @xmath65 as the test function .",
    "on the other hand , the hessian , the frchet derivative of the gradient , acting in directions @xmath61 and @xmath66 ( superscript `` 2 '' means the second variation direction ) reads @xmath67 where the second order forward state @xmath68 obeys the second order forward equation @xmath69 and the second order adjoint state @xmath70 is governed by the second order adjoint equation @xmath71    we define the generalized fisher information operator acting in directions @xmath61 and @xmath66 as @xmath72 where the expectation is taken with respect to the likelihood  the distribution of the observation @xmath30 .",
    "now , substituting into and assuming that the integrals / derivatives can be interchanged we obtain @xmath73 where we have used the assumption that the parameter @xmath5 is independent of observation @xmath30 and the fact that @xmath4 and @xmath68 do not depend on @xmath30 .",
    "the next step is to compute @xmath74 and @xmath75 .",
    "to begin , let us take the expectation the first order adjoint equation with respect to @xmath76 to arrive at @xmath77 where the second equality is obtained from and the assumption @xmath78 .",
    "we conclude that @xmath79 on the other hand , if we take the expectation of the second order adjoint equation and use we have @xmath80 let us define @xmath81 then becomes @xmath82    as a result , the fisher information operator acting along directions @xmath61 and @xmath66 reads @xmath83 where @xmath84 is the solution of , a variant of the second order adjoint equation .",
    "the fisher information operator therefore coincides with the gauss - newton hessian of the cost functional .",
    "the procedure for computing the gradient acting on an arbitrary direction is clear .",
    "one first solves the first order forward equation for @xmath4 , then the first order adjoint for @xmath63 , and finally evaluate .",
    "similarly , one can compute the hessian ( or the fisher information operators ) acting on two arbitrary directions by first solving the second order forward equation for @xmath68 , then the second order adjoint equation ( or its variant ) for @xmath70 ( or @xmath84 ) , and finally evaluating ( or ) .",
    "one of the main goals of the paper is to study the riemann manifold hamiltonian monte carlo method in the context of bayesian inverse problems governed by pdes .",
    "it is therefore essential to compute the derivative of the fisher information operator .",
    "this task is obvious for problems with available closed form expressions of the likelihood and the prior , but it is not so for those governed by pdes .",
    "nevertheless , using the adjoint technique we can compute the third order derivative tensor acting on three arbitrary directions with three extra pde solves , as we now show . to that end ,",
    "recall that the fisher information operator acting on directions @xmath61 and @xmath66 is given by .",
    "the frchet derivative of the fisher information operator along the additional direction @xmath85 ( superscript `` 3 '' means the third variation direction ) is given by @xmath86 where @xmath87 , @xmath88 are the variation of @xmath4 and @xmath84 in the direction @xmath85 , respectively .",
    "one can show that @xmath85 satisfies another second order forward equation @xmath89 similarly , @xmath88 is the solution of the third order adjoint equation @xmath90 and @xmath91 , the variation of @xmath66 in direction @xmath85 , satisfies the following third order forward equation @xmath92 note that it would have required four extra pde solves if one computes the third derivative of the full hessian .",
    "it is important to point out that the operator @xmath93 is only symmetric with respect to @xmath61 and @xmath66 since the fisher information is symmetric , but not with respect to @xmath61 and @xmath85 or @xmath66 and @xmath85 .",
    "the full symmetry only holds for the derivative of the full hessian , that is , the true third derivative of the cost functional .",
    "as presented in section , we view our inverse problem from an infinite dimensional point of view . as such , to implement our approach on computers , we need to discretize the prior , the likelihood and hence the posterior .",
    "we choose to use the finite element method .",
    "in particular , we employ the standard @xmath94 finite element method ( fem ) to discretize the forwards and adjoints ( the likelihood ) , and the operator @xmath95 ( the prior ) .",
    "it should be pointed out that the cameron - martin space can be shown ( see , e.g. , @xcite ) to be a subspace of the usual fractional sobolev space @xmath96 , which is in turn a subspace of @xmath94 .",
    "thus , we are using a non - conforming fem approach ( outer approximation ) . for convenience ,",
    "we further assume that the discretized state and parameter live on the same finite element mesh . since fem approximation of elliptic operators is standard ( see , e.g. , @xcite )",
    ", we will not discuss it here .",
    "instead , we describe the matrix transfer technique ( see , e.g , @xcite and the references therein ) to discretize the prior .",
    "define @xmath97 , then the eigenpairs @xmath98 of @xmath99 define the karhunen - love ( kl ) expansion of the prior distribution as @xmath100 where @xmath101 .",
    "we need to solve @xmath102 or equivalently @xmath103    to solve using the matrix transfer technique ( mtt ) , let us denote by @xmath104 the mass matrix , and @xmath105 the stiffness matrix resulting from the discretization of the laplacian @xmath106 . the representation of @xmath95 in the finite element space ( see , e.g.",
    ", @xcite and the references therein ) is given by @xmath107 let bold symbols denote the corresponding vector of fem nodal values , e.g. , @xmath108 is the vector containing all fem nodal values of @xmath5 .",
    "if we define @xmath109 as eigenpairs for @xmath110 , i.e , @xmath111 where @xmath112 , and hence @xmath113 , @xmath114 is the kronecker delta function , and @xmath115 is the diagonal matrix with entries @xmath116 . since @xmath110 is similar to @xmath117 , a symmetric positive definite matrix , @xmath110 has positive eigenvalues . using mtt method",
    ", the matrix representation of reads @xmath118 where @xmath119 it follows that @xmath120 the galerkin fem approximation of the prior via truncated kl expansion reads @xmath121 with @xmath108 as the fem nodal value of the approximate prior sample @xmath5 and @xmath122 as the number of fem nodal points .",
    "note that for ease in writing , we have used the same notation @xmath5 for both infinite dimensional prior sample and its fem approximation .",
    "since @xmath123 , @xmath108 naturally lives in @xmath124 , the euclidean space with weighted inner product @xmath125 .",
    "a question arises : what is the distribution of @xmath108 ? clearly @xmath108 is a gaussian with mean @xmath126 since @xmath127 are .",
    "the covariance matrix @xmath128 for @xmath108 is defined by @xmath129 where we have used to obtain the second equality and @xmath130 is the diagonal matrix with entries @xmath131 .",
    "it follows that @xmath132 as a map from @xmath124 to @xmath124 , and its inverse can be shown to be @xmath133 whence the distribution of @xmath108 is @xmath134 as a result , the fem discretization of the prior can be written as @xmath135 thus , the fem approximation of the posterior is given by @xmath136 the detailed derivation of the fem approximation of infinite bayesian inverse problems in general and the prior in particular will be presented elsewhere @xcite .",
    "in this section we give a brief overview of the mcmc algorithms that we consider in this work . some familiarity with the concepts of mcmc",
    "is required by the reader since an introduction to the subject is out of the scope of this paper .      for a random vector @xmath137 with density @xmath138",
    "the metropolis - hastings algorithm employs a proposal mechanism @xmath139 and proposed moves are accepted with probability @xmath140 . tuning the metropolis - hastings algorithm involves selecting an appropriate proposal mechanism .",
    "a common choice is to use a gaussian proposal of the form @xmath141 , where @xmath142 denotes the multivariate normal density with mean @xmath143 and covariance matrix @xmath115 .",
    "selecting the covariance matrix however , is far from trivial in most of cases since knowledge about the target density is required .",
    "therefore a more simplified proposal mechanism is often considered where the covariance matrix is replaced with a diagonal matrix such as @xmath144 where the value of the scale parameter @xmath145 has to be tuned in order to achieve fast convergence and good mixing .",
    "small values of @xmath145 imply small transitions and result in high acceptance rates while the mixing of the markov chain is poor .",
    "large values on the other hand , allow for large transitions but they result in most of the samples being rejected .",
    "tuning the scale parameter becomes even more difficult in problems where the standard deviations of the marginal posteriors differ substantially , since different scales are required for each dimension , and when correlations between different variables exist . in the case of pde - constrained inverse problems in very high dimensions with strong nonlinear interactions inducing complex non - convex structures in the target",
    "posterior this tuning procedure is typically doomed to failure of convergence and mixing .",
    "there have been many subsequent developments of this basic algorithm however the most important with regard to inverse problems , arguably , is the formal definition of metropolis hastings in an infinite dimensional functional space .",
    "one of the main failings of metropolis hastings is the drop - off in acceptance probability as the dimension of the problem increases . by defining the metropolis acceptance probability in the appropriate hilbert space the acceptance probability",
    "should then be invariant to the dimension of the problem and this is indeed the case as is described in a number of scenarios by @xcite .",
    "furthermore the definition of a markov chain transition kernel directly in the hilbert space which exploits hamiltonian dynamics in the proposal mechanism followed in @xcite .",
    "these are important methodological advances for mcmc applied to inverse problems . as the infinite dimensional nature of",
    "the problem is a fundamental aspect of the problem it is sensible that this characteristic is embedded in the mcmc scheme . in a similar vein by noting that the statistical model associated with the specific inverse problem is generated from an underlying partial differential equation or system of ordinary differential equations a natural geometric structure structure on the space of probability distributions is induced .",
    "this structure provides a rich source of model specific information that can be exploited in devising mcmc schemes that are informed by the underlying structure of the model itself .    in @xcite a way around this situation",
    "was provided by accepting that the statistical model can itself be considered as an object with an underlying geometric structure that could be embedded into the proposal mechanism .",
    "a class of mcmc methods were developed based on the differential geometric concepts underlying riemannian manifolds .      denoting the log of the target density as @xmath146 ,",
    "the manifold metropolis adjusted langevin algorithm ( mmala ) method , @xcite , defines a langevin diffusion with stationary distribution @xmath138 on the riemann manifold of density functions with metric tensor @xmath147 . by employing a first order euler integrator for discretising the stochastic differential equation a proposal mechanism with density",
    "@xmath148 is defined , where @xmath145 is the integration step size , a parameter which needs to be tuned , and the @xmath149th component of the mean function @xmath150 is @xmath151 where @xmath152 are the christoffel symbols of the metric in local coordinates . note that we have used the christoffel symbols to express the derivatives of the metric tensor , and they are computed using the adjoint method presented in section .    due to the discretisation error introduced by the first order approximation convergence to the stationary distribution",
    "is not guaranteed anymore and thus the metropolis - hastings ratio is employed to correct for this bias . in @xcite a number of examples are provided illustrating the potential of such a scheme for challenging inference problems .",
    "one can interpret the proposal mechanism of rmmala as a local gaussian approximation to the target density where the effective covariance matrix in rmmala is the inverse of the metric tensor evaluated at the current position .",
    "furthermore a simplified version of the rmmala algorithm , termed srmmala , can also be derived by assuming a manifold with constant curvature thus cancelling the last term in equation ( [ eq : meanmmala ] ) which depends on the christoffel symbols . whilst this is a step forward in that much information about the target density",
    "is now embedded in the proposal mechanism it is still driven by a random walk . the next approach to be taken goes beyond the direct and scaled random walk by defining proposals which follow the geodesic flows on the manifold of densities and thus presents a potentially really powerful scheme to explore posterior distributions .",
    "the riemann manifold hamiltonian monte carlo ( rmhmc ) method defines a hamiltonian on the riemann manifold of probability density functions by introducing the auxiliary variables @xmath153 which are interpreted as the momentum at a particular position @xmath108 and by considering the negative log of the target density as a potential function .",
    "more formally the hamiltonian defined on the riemann manifold is @xmath154 where the terms @xmath155 and @xmath156 are the potential energy and kinetic energy terms respectively . and the dynamics given by hamiltons equations are @xmath157 \\nonumber\\\\ & + \\frac{1}{2}\\pb^t\\g(\\ub)^{-1}\\frac{\\partial \\g(\\ub)}{\\partial \\ub_k}\\g(\\ub)^{-1}\\pb   \\eqnlab{hamilton}\\end{aligned}\\ ] ]    these dynamics define geodesic flows at a particular energy level and as such make proposals which follow deterministically the most efficient path across the manifold from the current density to the proposed one . simulating the hamiltonian",
    "requires a time - reversible and volume preserving numerical integrator . for this purpose",
    "the generalised leapfrog algorithm can be employed and provides a deterministic proposal mechanism for simulating from the conditional distribution , i.e. @xmath158 .",
    "more details about the generalised leapfrog integrator can be found in @xcite . to simulate a path ( which turns out to be a local geodesic ) across the manifold , the leapfrog integrator is iterated @xmath159 times which along with the integration step size @xmath145 are parameters requiring tuning .",
    "again due to the discrete integration errors on simulating the hamiltonian in order to ensure convergence to the stationary distribution the metropolis - hastings acceptance ratio is applied .",
    "the rmhmc method has been shown to be highly effective in sampling from posteriors induced by complex statistical models and offers the means to efficiently explore the hugely complex and high dimensional posteriors associated with pde - constrained inverse problems .",
    "as presented in section , we use the fisher information matrix at the map point augmented with the hessian of the prior as the metric tensor in our hmc simulations .",
    "it is therefore necessary to compute the augmented fisher matrix and its inverse . in  @xcite , we have shown that the gauss - newton hessian of the cost functional , also known as the data misfit , is a compact operator , and that for smooth @xmath5 its eigenvalues decay exponentially to zero .",
    "thus , the range space of the gauss - newton hessian is effectively finite - dimensional even before discretization , i.e. , it is independent of the mesh . in other words ,",
    "the fisher information matrix admits accurate low rank approximations and the accuracy can be improved as desired by simply increasing the rank of the approximation .",
    "we shall exploit this fact to compute the augmented fisher information matrix and its inverse efficiently .",
    "we start with the augmented fisher information matrix in @xmath124 @xmath160 where @xmath161 is the fisher information matrix obtained from by taking @xmath61 and @xmath66 as fem basis functions .",
    "assume that @xmath161 is compact ( see , e.g. , @xcite ) , together with the fact that @xmath162 decays to zero , we conclude that the prior - preconditioned fisher information matrix @xmath163 also has eigenvalues decaying to zero .",
    "therefore it is expected that the eigenvalues of the prior - preconditioned matrix decays faster than those of the original matrix @xmath161 .",
    "indeed , the numerical results in section will confirm this observation .",
    "it follows that ( see , e.g. , @xcite for similar decomposition ) @xmath164 admits a @xmath165-rank approximation of the form @xmath166 where @xmath167 and @xmath168 ( diagonal matrix ) contain the first @xmath165 dominant eigenvectors and eigenvalues of @xmath164 , respectively . in this work ,",
    "similar to @xcite , we use the one - pass randomized algorithm in @xcite to compute the low rank approximation .",
    "consequently , the augmented fisher information matrix becomes @xmath169 from which we obtain the inverse , by using the woodbury formula @xcite , @xmath170 where @xmath171 is a diagonal matrix with @xmath172 .    in the rmhmc method , we need to randomly draw the momentum variable as @xmath173 . if one considers @xmath174 where @xmath175 , then one can show , by inspection , that @xmath176 is distributed by @xmath177 .",
    "for convenience , let us recall that the finite element ( fem ) approximation of the posterior is given as @xmath178 @xmath126 is the fem nodal value of the prior mean function @xmath42 , @xmath104 is the mass matrix , @xmath179 the matrix of eigenvectors defined in , @xmath130 the diagonal matrix introduced in , @xmath23 , @xmath30 vector of observation data , and @xmath180 the forward map given by the forward equation @xmath3 that is discretized by the @xmath0-conforming fem method .    in this section ,",
    "we study riemann manifold monte carlo methods and their variations to explore the posterior .",
    "in particular , we compare the performance of four methods : i ) srmmala obtained by ignoring the third derivative in rmmala , ii ) rmmala , iii ) srmhmc obtained by first computing the augmented fisher metric tensor at the map point and then using it as the constant metric tensor , iv ) rmhmc .",
    "for all methods , we form the augmented fisher information matrix exactly using with @xmath61 and @xmath66 as finite element basis vectors . for rmmala and rmhmc we also need the derivative of the metric tensor which is a third order tensor . it can be constructed exactly using with @xmath181 and @xmath85 as finite element basis vectors .",
    "we also need extra work for the rmhmc method since each stormer - verlet step requires an implicit solve for both the first half of momentum and full position . for inverse problems such as those considered in this paper , the fixed point approach proposed in @xcite does not seem to converge .",
    "we therefore have to resort to a full newton method .",
    "since we explicitly construct the metric tensor and its derivative , it is straightforward for us to develop the newton scheme .",
    "for all problems considered in this section , we have observed that it takes at most five newton iterations to converge .",
    "note that we limit ourselves in comparing these four methods in the riemannian manifold mcmc sampling family .",
    "clearly , other methods are available , we avoid `` unmatched comparison '' in terms of cost and the level of exploiting the structure of the problem . even in this limited family , rmhmc is most expensive since it requires not only third derivatives but also implicit solves , but the ability in generating almost independent samples is attractive and appealing as we shall show .",
    "though our proposed approach described in previous sections are valid for any spatial dimension @xmath46 , we restrict ourselves to a one dimensional problem , i.e. @xmath182 , to clearly illustrate our points and findings . in particular , we take @xmath183 , @xmath184 .",
    "we set @xmath185 for all examples . as discussed in section , for the gaussian",
    "prior to be well - defined , we take @xmath186 .",
    "we start our numerical experiments with two parameters .",
    "this will help demonstrate various aspects of rmhmc which are otherwise too computationally expensive for high dimensional problems .",
    "in particular , two - parameter example allows us to compute the complete third derivative tensor and perform the newton method for each stormer - verlet step .",
    "this in turn allows us to show the capability of the full rmhmc over its simplified variants in tackling challenging posterior densities in which the metric tensor changes rapidly .    in order to construct the case with two parameters we consider fem with one finite element .",
    "we assume that there is one observation point , i.e. @xmath187 , and it is placed at the left boundary @xmath188 . in the first example , we first take @xmath189 , @xmath190 , and @xmath191 . the posterior in this case",
    "is shown in figure .",
    "we start by taking a time step of @xmath192 with @xmath193 stormer - verlet steps for both srmhmc and rmhmc .",
    "the acceptance rate for both methods is @xmath194 .",
    "one would take a time step of @xmath195 for both srmmala and rmmala to be comparable with srmhmc and rmhmc , but the acceptance would be zero .",
    "instead we take time step of @xmath194 so that the acceptance rate is about @xmath196 for srmmala and @xmath197 for rmmala .",
    "the map point is chosen as the initial state for all the chains with @xmath198 sample excluding the first @xmath193 burn - ins .",
    "the result is shown in figure .    as can be seen ,",
    "the rmhmc chain is the best in terms of mixing by comparing the second column ( the trace plot ) and the third column ( the autocorrelation function acf ) .",
    "each rmhmc sample is almost uncorrelated to the previous ones .",
    "the srmhmc is the second best , but the samples are strongly correlated compared to those of rmhmc , e.g. one uncorrelated sample for every @xmath199 .",
    "it is interesting to observe that the full rmmala and srmmala have performance in terms of auto - correlation length that is qualitatively similar at least in the first @xmath198 samples .",
    "this is due to the rmmala schemes being driven by a single step random walk that can not exploit fully the curvature information available to the geodesic flows of rmhmc , see rejoinder of @xcite .",
    "note that it is not our goal to compare the behavior of the chains when they converge .",
    "rather we would like to qualitatively study how fast the chains are well - mixed ( mixing time ) .",
    "this is important for large - scale problems governed by pdes since `` unpredicted '' mixing time implies a lot of costly waste in pde solves which one must avoid .",
    "though rmhmc is expensive in generating a sample , the cost of generating an uncorrelated / independent sample seems to be comparable to srmhmc for this example .",
    "in fact , if we measure the cost in terms of the number of pde solves , the total number of pde solves for rmhmc is @xmath200 while it is @xmath201 for srmhmc , a factor of @xmath199 more expensive .",
    "however , the cost in generating an almost uncorrelated / independent sample is the same since srmhmc generates one uncorrelated sample out of @xmath199 while it is one out of one for rmhmc .    to see how each method distributes the samples we plot one for every five samples in figure .",
    "all methods seem to explore the high probability density region very well .",
    "this explains why the sample mean and the @xmath202 credibility region are similar for all methods in the first column of figure .    in the second example we consider the combination @xmath189 , @xmath203 , and @xmath204 which leads to the posterior shown in figure .",
    "for srmhmc and rmhmc , we take time step @xmath205 with @xmath206 time steps , while it is @xmath194 for both srmmala and rmmala . again",
    ", the acceptance rate is unity for both srmhmc and rmhmc while it is @xmath207 for srmmala and @xmath208 for rmmala , respectively .",
    "the result for four methods is shown in figure .",
    "as can be seen , this example seems to be easier than the first one since even though the time step is larger , the trace plot and the acf looks better .",
    "it is interesting to observe that srmhmc is comparable with rmhmc ( in fact the acf seems to be a bit better ) for this example . as a result , rmhmc is more expensive than srmhmc for less challenging posterior in figure . here ,",
    "by less challenging we mean that the posterior is quite well approximated by a gaussian at the map point , e.g. the metric tensor is almost constant .",
    "this is true for the posterior in figure in which the gaussian prior contribution is significant , i.e. , @xmath204 instead of @xmath191 .",
    "conversely , the posterior is challenging if the metric tensor changes rapidly . similar to the first example , one also see that the sample mean and the @xmath202 credibility region are almost the same for all methods .    in the third example we consider the combination @xmath189 , @xmath203 , and @xmath209 which leads to a skinny posterior with a long ridge as shown in figure .",
    "for srmhmc and rmhmc , we take time step @xmath210 with @xmath206 time steps , while it is @xmath194 for both srmmala and rmmala .",
    "again , the acceptance rate is unity for both srmhmc and rmhmc while it is @xmath211 for srmmala and rmmala .",
    "the result for four methods is shown in figure .",
    "for this example , the rmhmc is more desirable than srmhmc since the cost to generate an uncorrelated / independent sample is smaller for the former than the latter .",
    "the reason is that the total number of pdes solves for the former is @xmath199 times more than the latter , but one out of very sixty samples is uncorrelated / independent .      in this section",
    "we choose to discretize @xmath183 with @xmath212 elements , and hence the number of parameters is @xmath1 .",
    "for all simulations in this section , we choose @xmath189 , @xmath213 , and @xmath214 . for synthetic observations ,",
    "we take @xmath215 observations at @xmath216 , @xmath217 .",
    "clearly , using the full blown rmhmc is out of the question since it is too expensive to construct the third derivative tensor and newton method for each stomer - verlet step .",
    "for that reason , the srmhmc becomes the viable choice . as studied in section , though srmhmc loses the ability to efficiently sample from highly nonlinear posterior surfaces compared to the full rmhmc it is much less expensive to generate a sample since it does not require the derivative of the fisher information matrix .",
    "in fact srmhmc requires to ( approximately ) compute the fisher information at the map point and then uses it as the fixed constant metric tensor throughout all leap - frog steps for all samples . clearly , the gradient has to be evaluated at each leap - frog step , but it can be computed efficiently using the adjoint method presented in section .",
    "nevertheless , constructing the exact fisher information matrix requires @xmath218 pdes solves .",
    "this is impractical if the dimension of the finite element space increases , e.g. by refining the mesh .",
    "alternatively , due to the compactness of the hessian of the prior - preconditioned misfit as discussed in section , we can use the randomized singular value decomposition ( rsvd ) technique @xcite to compute its low rank approximations . shown in figure",
    "are the first 35 dominant eigenvalues of the fisher information matrix and its prior - preconditioned counterpart .",
    "we also plot 20 approximate eigenvalues of the prior - preconditioned fisher information matrix obtained from the rsvd method .",
    "as can be seen , the eigen spectrum of the prior - preconditioned fisher information matrix decays faster than that of the original one .",
    "this is not surprising since the prior - preconditioned fisher operator is a composition of the prior covariance , a compact operator , and the fisher information operator , also a compact operator .",
    "the power of the rsvd is clearly demonstrated as the rsvd result for the first 20 eigenvalues is very accurate .",
    "next , we perform the srmhmc method using three different constant metric tensors : i ) the low rank gauss - newton hessian , ii ) the exact gauss - newton hessian , and iii ) the full hessian . for each case , we start the markov chain at the map point and compute @xmath219 samples , the first @xmath193 of which is then discarded as burn - ins .",
    "the empirical mean ( red line ) , the exact distributed parameter used to generate the observation ( black line ) , and @xmath202 credibility region are shown in figure .",
    "as can be seen , the results from the three methods are indistinguishable .",
    "the first srmhmc is the most appealing since it requires @xmath220 pde solves to construct the low rank fisher information while the others need @xmath218 pde solves . for large - scale problems with computationally expensive pde solves ,",
    "the first approach is the method of choice .    to further compare the three methods we record the trace plot of the first two ( @xmath194 and @xmath195 ) and the last two ( @xmath221 and @xmath1 ) parameters in figure .",
    "as can be observed , the chains from the three methods seem to be well - mixed and it is hard to see the difference among them .",
    "we also plot the autocorrelation function for these four parameters .",
    "again , results for the three srmhmc methods are almost identical , namely , they generate almost uncorrelated samples .",
    "we therefore conclude that low rank approach is the least computational extensive , yet it maintains the attractive features of the original rmhmc .",
    "as such , it is the most suitable method for large - scale bayesian inverse problems with costly pde solves .",
    "we have proposed the adoption of a computationally inexpensive riemann manifold hamiltonian monte method to explore the posterior of large - scale bayesian inverse problems governed by pdes in a highly efficient manner .",
    "we first adopt an infinite dimensional bayesian framework to guarantee that the inverse formulation is well - defined . in particular",
    ", we postulate a gaussian prior measure on the parameter space and assume regularity for the likelihood .",
    "this leads to a well - defined posterior distribution .",
    "then , we discretize the posterior using the standard finite element method and a matrix transfer technique , and apply the rmhmc method on the resulting discretized posterior in finite dimensional parameter space .",
    "we present an adjoint technique to efficiently compute the gradient , the hessian , and the third derivative of the potential function that are required in the rmhmc context .",
    "this is at the expense of solving a few extra pdes : one for the gradient , two for a hessian - vector product , and four for the product of third order derivative with a matrix . for large - scale problems , repeatedly computing the action of the hessian and third order derivative is too computationally expensive and this motivates us to design a simplified rmhmc in which the fisher information matrix is computed once at the map point .",
    "we further reduce the effort by constructing low rank approximation of the fisher information using a randomized singular value decomposition technique .",
    "the effectiveness of the proposed approach is demonstrated on a number of numerical results up to @xmath1 parameters in which the computational gain is about two orders of magnitude while maintaining the quality of the original rmhmc method in generating ( almost ) uncorrelated / independent samples .    for more challenging inverse problems with significant change of metric tensor across the parameter space",
    ", we expect that srmhmc with constant metric tensor is inefficient . in that case",
    ", rmhmc seems to be a better option , but it is too computational extensive for large - scale problems .",
    "ongoing work is to explore approximation of the rmhmc methods in which we approximate the trace and the third derivative in using adjoint and randomized techniques .",
    "10                                          h.  pearl flath , lucas  c. wilcox , volkan akelik , judy hill , bart van bloemen  waanders , and omar ghattas .",
    "fast algorithms for bayesian uncertainty quantification in large - scale linear inverse problems based on low - rank partial hessian approximations .",
    ", 33(1):407432 , 2011 .",
    "tan bui - thanh , omar ghattas , james martin , and georg stadler . a computational framework for infinite - dimensional bayesian inverse problems part i : the linearized case , with application to global seismic inversion .",
    ", 35(6):a2494a2523 , 2013 ."
  ],
  "abstract_text": [
    "<S> we consider the riemann manifold hamiltonian monte carlo ( rmhmc ) method for solving statistical inverse problems governed by partial differential equations ( pdes ) . </S>",
    "<S> the bayesian framework is employed to cast the inverse problem into the task of statistical inference whose solution is the posterior distribution in infinite dimensional parameter space conditional upon observation data and gaussian prior measure . </S>",
    "<S> we discretize both the likelihood and the prior using the @xmath0-conforming finite element method together with a matrix transfer technique . </S>",
    "<S> the power of the rmhmc method is that it exploits the geometric structure induced by the pde constraints of the underlying inverse problem . </S>",
    "<S> consequently , each rmhmc posterior sample is almost uncorrelated / independent from the others providing statistically efficient markov chain simulation . </S>",
    "<S> however this statistical efficiency comes at a computational cost . </S>",
    "<S> this motivates us to consider computationally more efficient strategies for rmhmc . at the heart of our construction </S>",
    "<S> is the fact that , gaussian error structures the fisher information matrix coincides with the gauss - newton hessian . </S>",
    "<S> we exploit this fact in considering a computationally simplified rmhmc method combining state - of - the - art adjoint techniques and the superiority of the rmhmc method . </S>",
    "<S> specifically , we first form the gauss - newton hessian at the maximum _ a posteriori _ point and then use it as a fixed constant metric tensor throughout rmhmc simulation . </S>",
    "<S> this eliminates the need for the computationally costly differential geometric christoffel symbols which in turn greatly reduces computational effort at a corresponding loss of sampling efficiency . </S>",
    "<S> we further reduce the cost of forming the fisher information matrix by using a low rank approximation via a randomized singular value decomposition technique . </S>",
    "<S> this is efficient since a small number of hessian - vector products are required . </S>",
    "<S> the hessian - vector product in turn requires only two extra pde solves using adjoint technique . </S>",
    "<S> various numerical results up to @xmath1 parameters are presented to demonstrate the ability of the rmhmc method in exploring the geometric structure of the problem to propose ( almost ) uncorrelated / independent samples that are far away from each other , and yet the acceptance rate is almost unity . </S>",
    "<S> the results also suggest that for the pde models considered the proposed fixed metric rmhmc can attain almost as high a quality performance as the original rmhmc , i.e. generating ( almost ) uncorrelated / independent samples , while being two orders of magnitude less computationally expensive .    </S>",
    "<S> tbmacros.txt    _ keywords _ riemann manifold hamiltonian monte carlo , inverse problems , bayesian , gaussian measure , prior , likelihood , posterior , adjoint , hessian , fisher information operator , gauss - newton . </S>"
  ]
}