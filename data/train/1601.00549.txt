{
  "article_text": [
    "boosting is considered as one of the most important ensemble learning methods in the machine learning literature and it is extensively used in several different real life applications from classification to regression ( @xcite ) . as an ensemble learning method ( @xcite )",
    ", boosting combines several parallel running `` weakly '' performing algorithms to build a final `` strongly '' performing algorithm ( @xcite ) .",
    "this is accomplished by finding a linear combination of weak learning algorithms in order to minimize the total loss over a set of training data commonly using a functional gradient descent ( @xcite ) .",
    "boosting is successfully applied to several different problems in the machine learning literature including classification ( @xcite ) , regression ( @xcite ) , and prediction ( @xcite ) .",
    "however , significantly less attention is given to the idea of boosting in online regression framework .",
    "to this end , our goal is ( a ) to introduce a new boosting approach for online regression , ( b ) derive several different online regression algorithms based on the boosting approach , ( c ) provide mathematical guarantees for the performance improvements of our algorithms , and ( d ) demonstrate the intrinsic connections of boosting with the adaptive mixture - of - experts algorithms ( @xcite ) and data reuse algorithms ( @xcite ) .",
    "although boosting is initially introduced in the batch setting ( @xcite ) , where algorithms boost themselves over a fixed set of training data , it is later extended to the online setting ( @xcite ) . in the online setting ,",
    "however , we neither need nor have access to a fixed set of training data , since the data samples arrive one by one as a stream ( @xcite ) .",
    "each newly arriving data sample is processed and then discarded without any storing .",
    "the online setting is naturally motivated by many real life applications especially for the ones involving big data , where there may not be enough storage space available or the constraints of the problem require instant processing ( @xcite ) .",
    "therefore , we concentrate on the online boosting framework and propose several algorithms for online regression tasks .",
    "in addition , since our algorithms are online , they can be directly used in adaptive filtering applications to improve the performance of conventional mixture - of - experts methods ( @xcite ) . for adaptive filtering purposes ,",
    "the online setting is especially important , where the sequentially arriving data is used to adjust the internal parameters of the filter , either to dynamically learn the underlying model or to track the nonstationary data statistics ( @xcite ) .",
    "specifically , we have @xmath0 parallel running weak learners ( wl ) ( @xcite ) that receive the input vectors sequentially .",
    "each wl uses an update method , such as the second order newton s method ( nm ) or stochastic gradient descent ( sgd ) , depending on the target of the applications or problem constraints ( @xcite ) . after receiving the input vector ,",
    "each algorithm produces its output and then calculates its instantaneous error after the observation is revealed . in the most generic setting , this estimation / prediction error and the corresponding input vector are then used to update the internal parameters of the algorithm to minimize a priori defined loss function , e.g. , instantaneous error for the sgd algorithm .",
    "these updates are performed for all of the @xmath0 wls in the mixture .",
    "however , in the online boosting approaches , these adaptations at each time proceed in rounds from top to bottom , starting from the first wl to the last one to achieve the `` boosting '' effect ( @xcite ) . furthermore , unlike the usual mixture approaches ( @xcite ) , the update of each wl depends on the previous wls in the mixture . in particular , at each time @xmath1 , after the @xmath2 wl calculates its error over @xmath3 pair , it passes a certain weight to the next wl , the @xmath4 wl , quantifying how much error the constituent wls from @xmath5 to @xmath2 made on the current @xmath3 pair .",
    "based on the performance of the wls from @xmath6 to @xmath7 on the current @xmath3 pair , the @xmath4 wl may give a different emphasis ( importance weight ) to @xmath3 pair in its adaptation in order to rectify the mistake of the previous wls .    the proposed idea for online boosting",
    "is clearly related to the adaptive mixture - of - experts algorithms widely used in the machine learning literature , where several parallel running adaptive algorithms are combined to improve the performance . in the mixture methods ,",
    "the performance improvement is achieved due to the diversity provided by using several different adaptive algorithms each having a different view or advantage ( @xcite ) .",
    "this diversity is exploited to yield a final combined algorithm , which achieves a performance better than any of the algorithms in the mixture .",
    "although the online boosting approach is similar to mixture approaches ( @xcite ) , there are significant differences . in the online boosting notion , the parallel running algorithms are not independent , i.e. , one deliberately introduces the diversity by updating the wls one by one from the first wl to the @xmath8 wl for each new sample based on the performance of all the previous wls on this sample . in this sense , each adaptive algorithm , say the @xmath4 wl , receives feedback from the previous wls , i.e. , @xmath5 to @xmath2 , and updates its inner parameters accordingly . as an example , if the current @xmath3 is well modeled by the previous wls , then the @xmath4 wl performs minor update using @xmath3 and may give more emphasis ( importance weight ) to the later arriving samples that may be worse modeled by the previous wls .",
    "thus , by boosting , each adaptive algorithm in the mixture can concentrate on different parts of the input and output pairs achieving diversity and significantly improving the gain .",
    "the linear online learning algorithms , such as sgd or nm , are among the simplest as well as the most widely used regression algorithms in the real - life applications ( @xcite ) .",
    "therefore , we use such algorithms as base wls in our boosting algorithms . to this end , we first apply the boosting notion to several parallel running linear nm - based wls and introduce three different approaches to use the importance weights ( @xcite ) , namely `` weighted updates'',``data reuse '' , and `` random updates '' . in the first approach ,",
    "we use the importance weights directly to produce certain weighted nm algorithms .",
    "in the second approach , we use the importance weights to construct data reuse adaptive algorithms ( @xcite ) .",
    "however , data reuse in boosting , such as ( @xcite ) , is significantly different from the usual data reusing approaches in adaptive filtering ( @xcite ) .",
    "as an example , in boosting , the importance weight coming from the @xmath2 wl determines the data reuse amount in the @xmath4 wl , i.e. , it is not used for the @xmath2 filter , hence , achieving the diversity .",
    "the third approach uses the importance weights to decide whether to update the constituent wls or not , based on a random number generated from a bernoulli distribution with the parameter equal to the weight .",
    "the latter method can be effectively used for big data processing ( @xcite ) due to the reduced complexity .",
    "the output of the constituent wls is also combined using a linear mixture algorithm to construct the final output .",
    "we then update the final combination algorithm using the sgd algorithm ( @xcite ) .",
    "furthermore , we extend the boosting idea to parallel running linear sgd - based algorithm similar to the nm case .",
    "we start our discussions by investigating the related works in section [ sec : rel_work ] .",
    "we then introduce the problem setup and background in section [ sec : problem ] , where we provide individual sequence as well as mse convergence results for the nm and sgd algorithms .",
    "we introduce our generic boosted online regression algorithm in section [ sec : bora ] and provide the mathematical justifications for its performance .",
    "then , in sections [ sec : brls ] and [ sec : blms ] , three different variants of the proposed boosting algorithm are derived , using the nm and sgd , respectively .",
    "then , in section [ sec : analysis ] we provide the mathematical analysis for the computational complexity of the proposed algorithms .",
    "the paper concludes with extensive sets of experiments over the well known benchmark data sets and simulation models widely used in the machine learning literature to demonstrate the significant gains achieved by the boosting notion .",
    "adaboost is one of the earliest and most popular boosting methods , which has been used for binary and multiclass classifications as well as regression ( @xcite ) .",
    "this algorithm has been well studied and has clear theoretical guarantees , and its excellent performance is explained rigorously ( @xcite ) . however , adaboost can not perform well on the noisy data sets ( @xcite ) , therefore , other boosting methods have been suggested that are more robust against noise .    in order to reduce the effect of noise , smoothboost was introduced in ( @xcite ) in a batch setting .",
    "moreover , in ( @xcite ) the author proves the termination time of the smoothboost algorithm by simultaneously obtaining upper and lower bounds on the weighted advantage of all samples over all of the weak learners .",
    "we note that the smoothboost algorithm avoids overemphasizing the noisy samples , hence , provides robustness against noise",
    ". in ( @xcite ) , the authors extend bagging and boosting methods to an online setting , where they use a poisson sampling process to approximate the reweighting algorithm",
    ". however , the online boosting method in ( @xcite ) corresponds to adaboost , which is susceptible to noise . in ( @xcite ) , the authors use a greedy optimization approach to develop the boosting notion to the online setting and introduce stochastic boosting .",
    "nevertheless , while most of the online boosting algorithms in the literature seek to approximate adaboost , ( @xcite ) investigates the inherent difference between batch and online learning , extend the smoothboost algorithm to an online setting , and provide the mathematical guarantees for their algorithm .",
    "( @xcite ) points out that the online weak learners do not need to perform well on all possible distributions of data , instead , they have to perform well only with respect to smoother distributions .",
    "recently , in ( @xcite ) the authors have developed two online boosting algorithms for classification , an optimal algorithm in terms of the number of weak learners , and also an adaptive algorithm using the potential functions and boost - by - majority ( @xcite ) .",
    "in addition to the classification task , the boosting approach has also been developed for the regression ( @xcite ) . in ( @xcite ) , a boosting algorithm for regression",
    "is proposed , which is an extension of adaboost.r ( @xcite ) .",
    "moreover , in ( @xcite ) , several gradient descent algorithms are presented , and some bounds on their performances are provided . in ( @xcite )",
    "the authors present a family of boosting algorithms for online regression through greedy minimization of a loss function .",
    "also , in ( @xcite ) the authors propose an online gradient boosting algorithm for regression .    in this paper",
    "we propose a novel family of boosted online algorithms for the regression task using the `` online boosting '' notion introduced in ( @xcite ) , and investigate three different variants of the introduced algorithm .",
    "furthermore , we show that our algorithm can achieve a desired mean squared error ( mse ) , given a sufficient amount of data and a sufficient number of weak learners .",
    "in addition , we use similar techniques to ( @xcite ) to prove the correctness of our algorithm .",
    "we emphasize that our algorithm has a guaranteed performance in an individual sequence manner , i.e. , without any statistical assumptions on the data . in establishing our algorithm and its justifications ,",
    "we refrain from changing the regression problem to the classification problem , unlike the adaboost.r ( @xcite ) . furthermore , unlike the online smoothboost ( @xcite ) , our algorithm can learn the guaranteed mse of the weak learners , which in turn improves its adaptivity .",
    "all vectors are column vectors and represented by bold lower case letters .",
    "matrices are represented by bold upper case letters . for a vector @xmath9 ( or a matrix @xmath10 )",
    ", @xmath11 ( or @xmath12 ) is the transpose and tr(@xmath10 ) is the trace of the matrix @xmath10 . here ,",
    "@xmath13 and @xmath14 represent the identity matrix of dimension @xmath15 and the all zeros vector of length @xmath0 , respectively . except @xmath13 and @xmath14 ,",
    "the time index is given in the subscript , i.e. , @xmath16 is the sample at time @xmath1 .",
    "we work with real data for notational simplicity .",
    "we denote the mean of a random variable @xmath17 as @xmath18 $ ] . also , we show the cardinality of a set @xmath19 by @xmath20 .",
    "we sequentially receive @xmath21-dimensional input ( regressor ) vectors @xmath22 , @xmath23 , and desired data @xmath24 , and estimate @xmath25 by @xmath26 , where @xmath27 is an online regression algorithm . at each time",
    "@xmath1 the estimation error is given by @xmath28 and is used to update the parameters of the wl . for presentation purposes ,",
    "we assume that @xmath29 $ ] , however , our derivations hold for any bounded but arbitrary desired data sequences . in our framework , we do not use any statistical assumptions on the input feature vectors or on the desired data such that our results are guaranteed to hold in an individual sequence manner ( @xcite ) .",
    "the linear methods are considered as the simplest online modeling or learning algorithms , which estimate the desired data @xmath25 by a linear model as @xmath30 , where @xmath31 is the linear algorithm s coefficients at time @xmath1 .",
    "note that the previous expression also covers the affine model if one includes a constant term in @xmath32 , hence we use the purely linear form for notational simplicity .",
    "when the true @xmath25 is revealed , the algorithm updates its coefficients @xmath33 based on the error @xmath34 . as an example , in the basic implementation of the nm algorithm , the coefficients are selected to minimize the accumulated squared regression error up to time @xmath35 as @xmath36 where @xmath37 is a fixed vector of coefficients . the nm algorithm is shown to enjoy several optimality properties under different statistical settings ( @xcite ) .",
    "apart from these results and more related to the framework of this paper , the nm algorithm is also shown to be rate optimal in an individual sequence manner ( @xcite ) .",
    "as shown in ( @xcite ) ( section v ) , when applied to any sequence @xmath22 and @xmath24 , the accumulated squared error of the nm algorithm is as small as the accumulated squared error of the best batch least squares ( ls ) method that is directly optimized for these realizations of the sequences , i.e. , for all @xmath38 , @xmath22 and @xmath24 , the nm achieves @xmath39 the nm algorithm is a member of the follow - the - leader type algorithms ( @xcite ) ( section 3 ) , where one uses the best performing linear model up to time @xmath35 to predict @xmath25 .",
    "hence , follows by direct application of the online convex optimization results ( @xcite ) after regularization .",
    "the convergence rate ( or the rate of the regret ) of the nm algorithm is also shown to be optimal so that @xmath40 in the upper bound can not be improved ( @xcite ) .",
    "it is also shown in ( @xcite ) that one can reach the optimal upper bound ( with exact scaling terms ) by using a slightly modified version of @xmath41 note that the extension of is a forward algorithm ( section 5 of @xcite ) and one can show that , in the scalar case , the predictions of are always bounded ( which is not the case for ) ( @xcite ) .",
    "we emphasize that in the basic application of the nm algorithm , all data pairs @xmath42 , @xmath43 , receive the same `` importance '' or weight in .",
    "although there exists exponentially weighted or windowed versions of the basic nm algorithm ( @xcite ) , these methods weight ( or concentrate on ) the most recent samples for better modeling of the nonstationarity ( @xcite ) .",
    "however , in the boosting framework ( @xcite ) , each sample pair receives a different weight based on not only those weighting schemes , but also the performance of the boosted algorithms on this pair . as an example",
    ", if a wl performs worse on a sample , the next wl concentrates more on this example to better rectify this mistake . in the following sections , we use this notion to derive different boosted online regression algorithms",
    "although in this paper we use linear wls for the sake of notational simplicity , one can readily extend our approach to nonlinear and piecewise linear regression methods .",
    "for example , one can use tree based online regression methods ( @xcite ) as the weak learners , and boost them with the proposed approach .",
    "in this section we present the generic form of our proposed algorithms and provide the guaranteed performance bounds for that . regarding the notion of `` online boosting '' introduced in ( @xcite )",
    ", the online weak learners need to perform well only over smooth distributions of data points .",
    "we first present the generic algorithm in algorithm ( [ alg : bora ] ) and provide its theoretical justifications , then discuss about its structure and the intuition behind it .",
    "input : @xmath3 ( data stream ) , @xmath0 ( number of weak learners running in parallel ) , @xmath44 ( the modified desired mse ) , and @xmath45 ( the guaranteed achievable weighted mse ) .",
    "initialize the regression coefficients @xmath46 for each wl ; and the combination coefficients as @xmath47^t$ ] ; receive the regressor data instance @xmath32 ; compute the wls outputs @xmath48 ; produce the final estimate @xmath49^t$ ] ; receive the true output @xmath25 ( desired data ) ; @xmath50 ; @xmath51 ; @xmath52 ; update the wl@xmath53 , such that it has a weighted mse @xmath54 ; @xmath55 ;    @xmath56 $ ] ; update @xmath57 based on @xmath58 ;    in algorithm [ alg : bora ] , we have @xmath0 copies of an online wl , each of which is guaranteed to have a weighted mse of at most @xmath45 .",
    "we prove that the algorithm 1 can reach a desired mse , @xmath59 , through lemma 1 , lemma 2 , and theorem 1 .",
    "note that since we assume @xmath29 $ ] , the trivial solution @xmath60 incurs an mse of at most @xmath6 .",
    "therefore , we define a weak learner as an algorithm which has an mse less than @xmath6 .    * lemma 1 . * _ in algorithm [ alg : bora ] , if there is an integer @xmath61 such that @xmath62 for every @xmath63 , and also @xmath64 , where @xmath65 is arbitrarily chosen , it can reach a desired mse , @xmath59 . _ + * proof . *",
    "the proof of lemma 1 is given in appendix [ app : lem1 ] .",
    "* lemma 2 . *",
    "_ if the weak learners are guaranteed to have a weighted mse less than @xmath45 , i.e. , @xmath66 there is an integer @xmath61 that satisfies the conditions in lemma 1 . _",
    "* proof . *",
    "the proof of lemma 2 is given in appendix [ app : lem2 ] .",
    "* theorem 1 .",
    "* _ if the weak learners in line 11 of algorithm [ alg : bora ] achieve a weighted mse of at most @xmath67 , there exists an upper bound for @xmath0 such that the algorithm reaches the desired mse . _",
    "+ * proof .",
    "* this theorem is a direct consequence of combining lemma 1 and lemma 2 .",
    "note that although we are using copies of a base learner as the weak learners and seek to improve its performance , the constituent wls can be different .",
    "however , by using the boosting approach , we can improve the mse performance of the overall system as long as the wls can provide a weighted mse of at most @xmath45 .",
    "for example , we can improve the performance of mixture - of - experts algorithms ( @xcite ) by leveraging the boosting approach introduced in this paper .    as shown in fig .",
    "[ fig : boost_block ] , at each iteration @xmath1 , we have @xmath0 parallel running wls with estimating functions @xmath68 , producing estimates @xmath69 of @xmath25 , @xmath70 . as an example , if we use @xmath0 `` linear '' algorithms , @xmath71 is the estimate generated by the @xmath2 wl . the outputs of these @xmath0 wls",
    "are then combined using the linear weights @xmath57 to produce the final estimate as @xmath72 ( @xcite ) , where @xmath73^t$ ] is the vector of outputs .",
    "after the desired output @xmath25 is revealed , the @xmath0 parallel running wls will be updated for the next iteration .",
    "moreover , the linear combination coefficients @xmath74 are also updated using the normalized sgd ( @xcite ) , as detailed later in section  [ sec : final_rls ] .     to produce the final estimate @xmath75 .",
    "there are @xmath0 constituent wls @xmath76 , each of which is an online linear algorithm that generates its own estimate @xmath48 .",
    "the final estimate @xmath75 is a linear combination of the estimates generated by all these constituent wls , with the combination weights @xmath77 s corresponding to @xmath48 s .",
    "the combination weights are stored in a vector which is updated after each iteration @xmath1 . at time",
    "@xmath1 the @xmath2 wl is updated based on the values of @xmath78 and @xmath79 , and provides the @xmath4 filter with @xmath80 that is used to compute @xmath81 .",
    "the parameter @xmath82 indicates the weighted mse of the @xmath2 wl over the first @xmath1 estimations , and is used in computing @xmath78.,scaledwidth=80.0% ]    after @xmath25 is revealed , the constituent wls , @xmath83 , @xmath70 , are consecutively updated , as shown in fig .",
    "[ fig : boost_block ] , from top to bottom , i.e. , first @xmath84 is updated , then , @xmath85 and finally @xmath86 is updated . however , to enhance the performance , we use a boosted updating approach ( @xcite ) , such that the @xmath4 wl receives a `` total loss '' parameter , @xmath80 , from the @xmath2 wl , as @xmath87 , \\label{eq : loss}\\ ] ] to compute a weight @xmath78 .",
    "the total loss parameter @xmath88 , indicates the sum of the differences between the modified desired mse ( @xmath44 ) and the squared error of the first @xmath89 wls at time @xmath1 .",
    "then , we add the difference @xmath90 to @xmath88 , to generate @xmath80 , and pass @xmath80 to the next wl , as shown in fig . [ fig : boost_block ] . here , @xmath91 $ ] measures how much the @xmath2 wl is off with respect to the final mse performance goal .",
    "for example , in a stationary environment , if @xmath92 , where @xmath93 is a deterministic function and @xmath94 is the observation noise , one can select the desired mse @xmath59 as an upper bound on the variance of the noise process @xmath94 , and define a _ modified _",
    "desired mse as @xmath95 . in this sense ,",
    "@xmath88 measures how the wls @xmath96 are cumulatively performing on @xmath97 pair with respect to the final performance goal .",
    "we then use the weight @xmath78 to update the @xmath2 wl with the `` weighted updates '' , `` data reuse '' , or `` random updates '' method , which we explain later in sections [ sec : brls ] and [ sec : blms ] .",
    "our aim is to make @xmath78 large if the first @xmath89 wls made large errors on @xmath25 , so that the @xmath2 wl gives more importance to @xmath3 in order to rectify the performance of the overall system .",
    "we now explain how to construct these weights , such that @xmath98 . to this end , we set @xmath50 , for all @xmath1 , and introduce a weighting similar to ( @xcite ) .",
    "we define the weights as @xmath99 where @xmath45 is the guaranteed upper bound on the weighted mse of the weak learners .",
    "however , since there is no prior information about the exact mse performance of the weak learners , we use the following weighting scheme @xmath100 where @xmath101 indicates an estimate of the @xmath2 weak learner s mse , and @xmath102 is a design parameter , which determines the `` dependence '' of each wl update on the performance of the previous wls , i.e. , @xmath103 corresponds to `` independent '' updates , like the ordinary combination of the wls in adaptive filtering ( @xcite ) , while a greater @xmath104 indicates the greater effect of the previous wls performance on the weight @xmath78 of the current wl .",
    "note that including the parameter @xmath104 does not change the validity of our proofs , since one can take @xmath105 as the new guaranteed weighted mse .",
    "here , @xmath101 is an estimate of the `` weighted mean squared error '' ( wmse ) of the @xmath2 wl over @xmath22 and @xmath24 . in the basic implementation of the online boosting ( @xcite )",
    ", @xmath106 is set to the classification advantage of the weak learners ( @xcite ) , where this advantage is assumed to be the same for all weak learners . in this paper , to avoid using any a priori knowledge and to be completely adaptive , we choose @xmath101 as the weighted and thresholded mse of the @xmath2 wl up to time @xmath35 as @xmath107^{+}\\right)^2}{\\sum_{\\tau=1}^{t}\\lambda_{\\tau}^{(k ) } } \\nonumber \\\\ & = \\frac{\\lambda_{t-1}^{(k)}\\delta_{t-1}^{(k)}+\\frac{\\lambda_{t}^{(k)}}{4 } \\left(d_{t } -\\left[f_{t}^{(k)}({\\mbox{${\\mbox{\\boldmath${x}$}}$}}_{t})\\right]^{+}\\right)^2}{\\lambda_{t-1}^{(k)}+\\lambda_{t}^{(k ) } } , \\label{eq : delta}\\end{aligned}\\ ] ] where @xmath108 , and @xmath109^{+}$ ] thresholds @xmath110 into the range @xmath111 $ ] .",
    "this thresholding is necessary to assure that @xmath112 , which guarantees @xmath98 for all @xmath70 and @xmath1 .",
    "we point out that can be recursively calculated .    regarding the definition of @xmath78 ,",
    "if the first @xmath7 wls are `` good '' , we will pass less weight to the next wls , such that those wls can concentrate more on the other samples .",
    "hence , the wls can increase the diversity by concentrating on different parts of the data @xcite .",
    "furthermore , following this idea , in , the weight @xmath78 is larger , i.e. , close to 1 , if most of the wls , @xmath113 , have errors larger than @xmath44 on @xmath3 , and smaller , i.e. , close to 0 , if the pair @xmath3 is easily modeled by the previous wls such that the wls @xmath114 do not need to concentrate more on this pair .",
    "although in the proof of our algorithm , we assume a constant combination vector @xmath115 over time , we use a time varying combination vector in practice , since there is no knowledge about the exact number of the required week learners for each problem . hence , after @xmath25 is revealed , we also update the final combination weights @xmath74 based on the final output @xmath116 , where @xmath72 , @xmath117^t$ ] . to update the final combination weights",
    ", we use the normalized sgd algorithm @xcite yielding @xmath118      the choice of @xmath44 is a crucial task , i.e. , we can not reach any desired mse for any data sequence unconditionally . as an example , suppose that the data are generated randomly according to a known distribution , while they are contaminated with a white noise process .",
    "it is clear that we can not obtain an mse level below the noise power .",
    "however , if the wls are guaranteed to satisfy the conditions of theorem 1 , this would not happen . intuitively , there is a guaranteed upper bound ( i.e. , @xmath45 ) on the worst case performance , since in the weighted mse , the samples with a higher error have a more important effect . on the other hand ,",
    "if one chooses a @xmath44 smaller than the noise power , @xmath88 will be negative for almost every @xmath7 , turning most of the weights into 1 , and as a result the weak learners fail to reach a weighted mse smaller than @xmath45 .",
    "nevertheless , in practice we have to choose the parameter @xmath44 reasonably and precisely such that the conditions of theorem 1 are satisfied .",
    "for instance , we set @xmath44 to be an upper bound on the noise power .",
    "in addition , the number of weak learners , @xmath0 , is chosen regarding to the computational complexity constraints . however ,",
    "in our experiments we choose a moderate number of weak learners , @xmath119 , which successfully improves the performance .",
    "moreover , according to the results in section [ sec : parameter ] , the optimum value for @xmath104 is around 1 , hence , we set the parameter @xmath120 in our simulations .",
    "at each time @xmath1 , all of the wls ( shown in fig .",
    "[ fig : boost_block ] ) estimate the desired data @xmath25 in parallel , and the final estimate is a linear combination of the results generated by the wls .",
    "when the @xmath2 wl receives the weight @xmath78 , it updates the linear coefficients @xmath121 using one of the following methods .      here",
    ", we consider @xmath78 as the weight for the observation pair @xmath3 and apply a weighted nm update to @xmath121 .",
    "for this particular weighted nm algorithm , we define the hessian matrix and the gradient vector as @xmath123 where @xmath124 is the forgetting factor @xcite and @xmath125 can be calculated in a recursive manner as @xmath126 where @xmath127 , and @xmath128 , and @xmath129 .",
    "the complete algorithm is given in algorithm [ alg : boostedwrls ] with the weighted nm implementation in .",
    "+    input : @xmath3 ( data stream ) , @xmath0 ( number of wls ) and @xmath44 .",
    "initialize the regression coefficients @xmath46 for each wl ; and the combination coefficients as @xmath47^t$ ] ; and for all @xmath7 set @xmath130 .",
    "receive the regressor data instance @xmath32 ; compute the wls outputs @xmath131 ; produce the final estimate @xmath132^t$ ] ; receive the true output @xmath25 ( desired data ) ; @xmath50 ; @xmath51 ; @xmath133 ; update the regression coefficients @xmath134 by using the nm and the weight @xmath78 based on one of the introduced algorithms in section [ sec : brls ] ; @xmath55 ; @xmath135^{+}\\right)^2}{\\lambda_{t-1}^{(k)}+\\lambda_{t}^{(k)}}$ ] ; @xmath136 @xmath137 $ ] ; @xmath58 ; @xmath138 ;    [ alg : rls ]      another approach follows ozaboost ( @xcite ) . in this approach , from @xmath78 , we generate an integer , say @xmath139 , where @xmath140 is a design parameter that takes on positive integer values .",
    "we then apply the nm update on the @xmath3 pair repeatedly @xmath141 times , i.e. , run the nm update on the same @xmath3 pair @xmath141 times consecutively . note that @xmath140 should be determined according to the computational complexity constraints .",
    "however , increasing @xmath140 does not necessarily result in a better performance , therefore , we use moderate values for @xmath140 , e.g. , we use @xmath142 in our simulations . the final @xmath143 is calculated after @xmath144 nm updates . as a major advantage ,",
    "clearly , this reusing approach can be readily generalized to other adaptive algorithms in a straightforward manner .",
    "we point out that ozaboost ( @xcite ) uses a different data reuse strategy . in this approach",
    ", @xmath145 is used as the parameter of a poisson distribution and an integer @xmath144 is randomly generated from this poisson distribution .",
    "one then applies the nm update @xmath144 times .      in this approach ,",
    "we simply use the weight @xmath78 as a probability of updating the @xmath2 wl at time @xmath1 . to this end",
    ", we generate a bernoulli random variable , which is @xmath6 with probability @xmath78 and is @xmath146 with probability @xmath147 .",
    "then , we update the @xmath2 wl , only if the bernoulli random variable equals @xmath6 . with this method",
    ", we significantly reduce the computational complexity of the algorithm .",
    "moreover , due to the dependence of this bernoulli random variable on the performance of the previous constituent wls , this method does not degrade the mse performance , while offering a considerably lower complexity , i.e. , when the mse is low , there is no need for further updates , hence , the probability of an update is low , while this probability is larger when the mse is high .",
    "in this case , as shown in fig . [ fig : boost_block ] , we have @xmath0 parallel running wls , each of which is updated using the sgd algorithm .",
    "based on the weights given in and the total loss and mse parameters in and , we next introduce three sgd based boosting algorithms , similar to those introduced in section [ sec : brls ] .",
    "we note that by construction method in , @xmath148 , thus , these weights can be directly used to scale the learning rates for the sgd updates .",
    "when the @xmath2 wl receives the weight @xmath78 , it updates its coefficients @xmath121 , as @xmath149 where @xmath150 .",
    "note that we can choose @xmath151 for all @xmath7 , since the online algorithms work consecutively from top to bottom , and the @xmath2 wl will have a different learning rate @xmath152 .      in this scenario , for updating @xmath121",
    ", we use the sgd update @xmath139 times to obtain the @xmath143 as @xmath153 where @xmath140 is a constant design parameter .",
    "similar to the nm case , if we follow the ozaboost ( @xcite ) , we use the weights to generate a random number @xmath141 from a poisson distribution with parameter @xmath78 , and perform the sgd update @xmath141 times on @xmath121 as explained above .      again , in this scenario , similar to the nm case , we use the weight @xmath78 to generate a random number from a bernoulli distribution , which equals @xmath6 with probability @xmath78 , and equals @xmath146 with probability @xmath147",
    ". then we update @xmath31 using sgd only if the generated number is @xmath6 .",
    "in this section we provide the complexity analysis for the proposed algorithms .",
    "we prove an upper bound for the weights @xmath78 , which is significantly less than 1 .",
    "this bound shows that the complexity of the `` random updates '' algorithm is significantly less than the other proposed algorithms , and slightly greater than that of a single wl .",
    "hence , it shows the considerable advantage of `` boosting with random updates '' in processing of high dimensional data .",
    "here we compare the complexity of the proposed algorithms and find an upper bound for the computational complexity of random updates scenario ( introduced in section [ sec : rndrls ] for nm , and in section [ sec : rndlms ] for sgd updates ) , which shows its significantly lower computational burden with respect to two other approaches . for @xmath154 , each wl performs @xmath155 computations to generates its estimate , and if updated using the nm algorithm , requires @xmath156 computations due to updating the matrix @xmath157 , while it needs @xmath155 computations when updated using the sgd method ( in their most basic implementation ) .",
    "we first derive the computational complexity of using the nm updates in different boosting scenarios .",
    "since there are a total of @xmath0 wls , all of which are updated in the `` weighted updates '' method , this method has a computational cost of order @xmath158 per each iteration @xmath1 .",
    "however , in the `` random updates '' , at iteration @xmath1 , the @xmath2 wl may or may not be updated with probabilities @xmath78 and @xmath147 respectively , yielding @xmath159 where @xmath160 indicates the complexity of running the @xmath2 wl at iteration @xmath1 .",
    "therefore , the total computational complexity @xmath161 at iteration @xmath1 will be @xmath162 , which yields @xmath163 = e\\left[\\sum_{k=1}^{m } c_t^{(k)}\\right ] = \\sum_{k=1}^{m } e[\\lambda_t^{(k ) } ] o(r^2)\\ ] ] hence , if @xmath164 $ ] is upper bounded by @xmath165 , the average computational complexity of the random updates method , will be @xmath163 < \\sum_{k=1}^{m } \\tilde{\\lambda}^{(k ) } o(r^2).\\ ] ] in theorem 2 , we provide sufficient constraints to have such an upper bound .    furthermore , we can use such a bound for the `` data reuse '' mode as well . in this case , for each wl @xmath83 , we perform the nm update @xmath166 times , resulting a computational complexity of order @xmath167 < \\sum_{k=1}^{m } k\\ \\tilde{\\lambda}^{(k ) } ( o(r^2))$ ] . for the sgd updates ,",
    "we similarly obtain the computational complexities @xmath168 , @xmath169 , and @xmath170 , for the `` weighted updates '' , `` random updates '' , and `` data reuse '' scenarios respectively .",
    "the following theorem determines the upper bound @xmath171 for @xmath164 $ ] . + * theorem 2 . * _ if the wls converge and achieve a sufficiently small mse ( according to the proof following this theorem ) , the following upper bound is obtained for @xmath78 , given that @xmath44 is chosen properly ,",
    "@xmath172 \\leq \\tilde{\\lambda}^{(k ) } = \\left(\\gamma^{-2 \\sigma_m^2}(1 + 2 \\zeta^2 \\ln \\gamma)\\right)^{\\frac{1-k}{2}},\\\\\\ ] ] where @xmath173 $ ] and @xmath174 $ ] . _ + it can be straightforwardly shown that , this bound is less than @xmath6 for appropriate choices of @xmath44 , and reasonable values for the mse according to the proof .",
    "this theorem states that if we adjust @xmath44 such that it is achievable ,",
    "i.e. , the wls can provide a slightly lower mse than @xmath44 , the probability of updating the wls in the random updates scenario will decrease .",
    "this is of course our desired results , since if the wls are performing sufficiently well , there is no need for additional updates .",
    "moreover , if @xmath44 is opted such that the wls can not achieve a mse equal to @xmath44 , the wls have to be updated at each iteration , which increases the complexity .",
    "* proof : * for simplicity , in this proof , we have assumed that @xmath120 , however , the results are readily extended to the general values of @xmath104 .",
    "we construct our proof based on the following assumption : + * assumption : * assume that @xmath79 s are independent and identically distributed ( i.i.d ) zero - mean gaussian random variables with variance @xmath175 .",
    "+ we have @xmath176 & = e\\left[\\min \\left\\{1,\\left(\\delta_{t-1}^{(k)}\\right)^{l_t^{(k ) } } \\right\\}\\right]\\nonumber\\\\ & \\leq \\min\\left\\{1,e\\left[\\left(\\delta_{t-1}^{(k)}\\right)^{l_t^{(k)}}\\right]\\right\\}\\end{aligned}\\ ] ] now , we show that under certain conditions , @xmath177 $ ] will be less than 1 , hence , we obtain an upper bound for @xmath178 $ ] .",
    "we define @xmath179 , yielding @xmath180 = e \\left[e\\left [ \\exp\\big(s\\ { { \\l_t^{(k)}}}\\big ) \\big|s \\right]\\right ] = e\\left[m_{{\\l_t^{(k)}}}(s ) \\big| s \\right],\\end{aligned}\\ ] ] where @xmath181 is the moment generating function of the random variable @xmath182 . from the algorithm [ alg : rls ] , @xmath183 . according to the assumption",
    ", @xmath184 is a standard normal random variable .",
    "therefore , @xmath185 has a gamma distribution as @xmath186 ( @xcite ) , which results in the following moment generating function for @xmath182 @xmath187 in the above equality @xmath188 is a random variable , the mean of which is denoted by @xmath189 .",
    "we point out that @xmath189 will approach to @xmath175 in convergence .",
    "we define a function @xmath190 such that @xmath191 = e\\left[\\varphi\\left({\\delta_{t-1}^{(k)}}\\right)\\right]$ ] , and seek to find a condition for @xmath190 to be a concave function .",
    "then , by using the jenssen s inequality for concave functions , we have @xmath192 \\leq \\varphi(\\gamma).\\ ] ] inspired by , we define @xmath193 and @xmath194 . by these definitions we obtain    @xmath195 .",
    "\\end{gathered}\\ ] ]    considering that @xmath196 , in order for @xmath190 to be concave , it suffices to have @xmath197 which reduces to the following necessary and sufficient conditions : @xmath198 and @xmath199 where @xmath200 @xmath201 and @xmath202 under these conditions , @xmath190 is concave , therefore , by substituting @xmath190 in we achieve .",
    "this concludes the proof of the theorem 2 .",
    "in this section , we demonstrate the efficacy of the proposed boosting algorithms for nm and sgd linear wls under different scenarios . to this end",
    ", we first consider the `` online regression '' of data generated with a stationary linear model .",
    "then , we illustrate the performance of our algorithms under nonstationary conditions , to thoroughly test the adaptation capabilities of the proposed boosting framework . furthermore , since the most important parameters in the proposed methods are @xmath44 , @xmath104 , and @xmath0 , we investigate their effects on the final mse performance . finally , we provide the results of the experiments over several real and synthetic benchmark datasets .    throughout this section , `` sgd '' represents the linear sgd - based wl , `` nm '' represents the linear nm - based wl , and a prefix `` b '' indicates the boosting algorithms . in addition , we use the suffixes `` -wu '' , `` -ru '' , or `` -dr '' to denote the `` weighted updates '' , `` random updates '' , or `` data reuse '' modes , respectively , e.g. , the `` bsgd - ru '' represents the `` boosted sgd - based algorithm using random updates '' .    in order to observe the boosting effect , in all experiments",
    ", we set the step size of sgd and the forgetting factor of the nm to their optimal values , and use those parameters for the wls , too .",
    "in addition , the initial values of all of the weak learners in all of the experiments are set to zero .",
    "however , in all experiments , since we use @xmath142 in bsgd - dr algorithm , we set the step size of the wls in bsgd - dr method to @xmath204 , where , @xmath205 is the step size of the sgd . to compare the mse results ,",
    "we have provided the accumulated square error ( ase ) results .      in this experiment",
    ", we consider the case where the desired data is generated by a stationary linear model .",
    "the input vectors @xmath206 $ ] are 3-dimensional , where @xmath207 $ ] is drawn from a jointly gaussian random process and then scaled such that @xmath208^t \\in [ 0\\ 1]^2 $ ] .",
    "we include 1 as the third entry of @xmath209 to consider affine learners .",
    "specifically the desired data is generated by @xmath210^t\\ { \\mbox{${\\mbox{\\boldmath${x}$}}$}}_t + \\nu_t$ ] , where @xmath94 is a random gaussian noise with a variance of @xmath211 .    in our simulations",
    ", we use @xmath119 wls and @xmath212 for all sgd learners .",
    "in addition , for nm - based boosting algorithms , we set the forgetting factor @xmath213 for all algorithms .",
    "moreover , we choose @xmath214 for sgd - based algorithms and @xmath215 for nm - based algorithms , @xmath142 for data reuse approaches , and @xmath120 for all boosting algorithms .",
    "to achieve robustness , we average the results over 100 trials .    as depicted in fig .",
    "[ fig : stationary ] , our proposed methods boost the performance of a single linear sgd - based wl . nevertheless , we can not further improve the performance of a linear nm - based wl in such a stationary experiment since the nm achieves the lowest mse .",
    "we point out that the random updates method achieves the performance of the weighted updates method and the data reuse method with a much lower complexity .",
    "in addition , we observe that by increasing the data length , the performance improvement increases ( note that the distance between the ase curves is slightly increasing ) .     +      here , in order to show the tracking capability of our algorithms in nonstationary environments , we consider the case where the desired data is generated by the duffing map ( @xcite ) as a chaotic model . specifically , the data is generated by the following equation @xmath216 , where we set @xmath217 and @xmath218 .",
    "we consider @xmath219 as the desired data and @xmath220 $ ] as the input vector . in this experiment",
    ", each boosting algorithm uses 20 wls .",
    "the step sizes for the sgd - based algorithms are set to @xmath221 , the forgetting factor @xmath124 for the nm - based algorithms are set to @xmath222 , and the modified desired mse parameter @xmath44 is set to @xmath223 for bsgd methods , and @xmath224 for the bnm methods .",
    "note that although the value of @xmath44 is higher than the achieved mse , it can improve the performance significantly .",
    "this is because of the boosting effect , i.e. , emphasizing on the harder data patterns .",
    "the figures show the superior performance of our algorithms over a single wl ( whose step size is chosen to be the best ) , in this highly nonstationary environment .",
    "moreover , as shown in fig .",
    "[ fig : duffing ] , in the sgd - based boosted algorithms , the data reuse method shows a better performance relative to the other boosting methods .",
    "however , the random updates method has a significantly lower time consumption , which makes it desirable for larger data lengths . from the fig .",
    "[ fig : duffing ] , one can see that our method is truly boosting the performance of the conventional linear wls in this chaotic environment .    from the fig .",
    "[ fig : landa ] , we observe the approximate changes of the weights , in the bsgd - ru algorithm running over the duffing data .",
    "as shown in this figure , the weights do not change monotonically , and this shows the capability of our algorithm in effective tracking of the nonstationary data . furthermore , since we update the wls in an ordered manner , i.e. , we update the @xmath4 wl after the @xmath2 wl is updated , the weights assigned to the last wls are generally smaller than the weights assigned to the previous wls . as an example , in fig . [",
    "fig : landa ] we see that the weights assigned to the @xmath225 wl are larger than those of the @xmath226 and @xmath227 wls .",
    "furthermore , note that in this experiment , the dependency parameter @xmath104 is set to 1 . we should mention that increasing the value of this parameter , in general , causes the lower weights , hence , it can considerably reduce the complexity of the random updates and data reuse methods .",
    "+     +      in this section , we investigate the effects of the dependence parameter @xmath104 and the modified desired mse @xmath44 as well as the number of wls , @xmath0 , on the boosting performance of our methods in the duffing data experiment , explained in section [ sec : nonst ] . from the results in fig .",
    "[ fig : m ] , we observe that , increasing the number of wls up to @xmath228 can improve the performance significantly , while further increasing of @xmath0 only increases the computational complexity without improving the performance . in addition , as shown in fig .",
    "[ fig : c ] , in this experiment , the dependency parameter @xmath104 has an optimum value around @xmath6 .",
    "we note that choosing small values for @xmath104 reduces the boosting effect , and causes the weights to be larger , which in turn increases the computational complexity in random updates and data reuse approaches . on the other hand , choosing very large values for @xmath104 increases the dependency , i.e. , in this case the generated weights are very close to @xmath6 or @xmath146 , hence , the boosting effect is decreased .",
    "overall , one should choose values around @xmath6 for @xmath104 to avoid those extreme cases .    furthermore , as depicted in fig . [",
    "fig : sigma ] , there is an optimum value around @xmath229 for @xmath44 in this experiment .",
    "note that , choosing small values for @xmath44 results in large weights , thus , increases the complexity and reduces the diversity .",
    "however , choosing higher values for @xmath44 results in smaller weights , and in turn reduces the complexity .",
    "nevertheless , we note that increasing the value of @xmath44 does not necessarily enhance the performance . through the experiments , we find out that @xmath44 must be in the order of the mse amount to obtain the best performance .    0.5     0.5     0.5       in this section",
    ", we demonstrate the efficiency of the introduced methods over some widely used real life machine learning regression data sets .",
    "we have normalized each dimension of the data to the interval @xmath111 $ ] in all algorithms .",
    "we present the mse performance of the algorithms in table [ tab : result ] .",
    "these experiments show that our algorithms can successfully improve the performance of single linear wls .",
    "we now describe the experiments and provide the results : +     bnm - dr & bnm - ru + mv & 0.2711 & 0.2707 & 0.2706 & 0.2707 & 0.2592 & 0.2645 & 0.2587 & 0.2584 + puma8nh & 0.1340 & 0.1334 & 0.1332 & 0.1334 & 0.1296 & 0.1269 & 0.1295 & 0.1284 + kinematics & 0.0835 & 0.0831 & 0.0830 & 0.0831 & 0.0804 & 0.0801 & 0.0803 & 0.0801 + compactiv & 0.0606 & 0.0599 & 0.0608 & 0.0598 & 0.0137 & 0.0086 & 0.0304 & 0.0078 + protein tertiary & 0.2554 & 0.2550 & 0.2549 & 0.2550 & 0.2370 & 0.2334 & 0.2385 & 0.2373 + onp & 0.0015 & 0.0009 & 0.0009 & 0.0009 & 0.0009 & 0.0009 & 0.0009 & 0.0009 + california housing & 0.0446 & 0.0450 & 0.0452 & 0.0448 & 0.0685 & 0.0671 & 0.0579 & 0.0683 + ypmsd & 0.0237 & 0.0237 & 0.0233 & 0.0237 & 0.0454 & 0.0337 & 0.0302 & 0.0292 +    here , we briefly explain the details of the data sets :    1 .",
    "mv : this is an artificial dataset with dependencies between the attribute values",
    ". one can refer to ( @xcite ) for further details .",
    "there are @xmath230 attributes and one target value . in this dataset",
    ", we can slightly improve the performance of a single linear wl by using any of the proposed methods .",
    "puma dynamics ( puma8nh ) : this dataset is a realistic simulation of the dynamics of a puma @xmath231 robot arm ( @xcite ) .",
    "the task is to predict the angular acceleration of one of the robot arm s links .",
    "the inputs include angular positions , velocities and torques of the robot arm . according to the ase results in fig .",
    "[ fig : pumadyn ] , the bnm - wu has the best boosting performance in this experiment .",
    "nonetheless , the sgd - based methods also improve the performance .",
    "3 .   kinematics : this dataset is concerned with the forward kinematics of an 8 link robot arm ( @xcite ) .",
    "we use the variant 8 nm , which is highly non - linear and noisy . as shown in fig .",
    "[ fig : kinematics ] , our proposed algorithms slightly improve the performance in this experiment .",
    "computer activity ( compactiv ) : this real dataset is a collection of computer systems activity measures ( @xcite ) .",
    "the task is to predict usr , the portion of time that cpus run in user mode from all attributes ( @xcite ) .",
    "the nm - based boosting algorithms deliver a significant performance improvement in this experiment , as shown by the results in table [ tab : result ] .",
    "protein tertiary ( @xcite ) : this dataset is collected from critical assessment of protein structure prediction ( casp ) experiments @xmath232 . the aim is to predict the size of the residue using @xmath233 attributes over @xmath234 data instances .",
    "online news popularity ( onp ) ( @xcite ) : this dataset summarizes a heterogeneous set of features about articles published by mashable in a period of two years .",
    "the goal is to predict the number of shares in social networks ( popularity ) .",
    "california housing : this dataset has been obtained from statlib repository . they have collected information on the variables using all the block groups in california from the 1990 census . here , we seek to find the house median values , based on the given attributes .",
    "for further description one can refer to ( @xcite ) .",
    "year prediction million song dataset ( ypmsd ) ( @xcite ) : the aim is predicting the release year of a song from its audio features .",
    "songs are mostly western , commercial tracks ranging from 1922 to 2011 , with a peak in the year 2000s .",
    "we use a subset of the million song dataset ( @xcite ) . as shown in table",
    "[ tab : result ] and fig .",
    "[ fig : ypmsd ] , our algorithms can significantly improve the performance of the linear wl in this experiment .",
    "0.33   +    0.33   +    0.33   +",
    "we introduced a novel family of boosted online regression algorithms and proposed three different boosting approaches , i.e. , weighted updates , data reuse , and random updates , which can be applied to different online learning algorithms .",
    "we provide theoretical bounds for the mse performance of our proposed methods in a strong mathematical sense .",
    "we emphasize that while using the proposed techniques , we do not assume any prior information about the statistics of the desired data or feature vectors .",
    "we show that by the proposed boosting approaches , we can significantly improve the mse performance of the conventional sgd and nm algorithms .",
    "moreover , we provide an upper bound for the weights generated during the algorithm that leads us to a thorough analysis of the computational complexity of these methods .",
    "the computational complexity of the random updates method is remarkably lower than that of the conventional mixture - of - experts and other variants of the proposed boosting approaches , without degrading the performance .",
    "therefore , the boosting using random updates approach is an elegant alternative to the conventional mixture - of - experts method when dealing with real life large scale problems .",
    "we provide several results that demonstrate the strength of the proposed algorithms over a wide variety of synthetic as well as real data .",
    "we observe that according to algorithm [ alg : bora ] , @xmath235,\\\\ e_t & = \\frac{1}{m } \\sum_{k=1}^{m } e_t^{(k)},\\end{aligned}\\ ] ] in addition , we have @xmath236 and as a result , if @xmath237 , then @xmath238 , i.e. , @xmath239 .",
    "hence by defining a _ modified _ desired mse as @xmath95 , and @xmath240 $ ] for @xmath241 , we have @xmath242 finally we have @xmath243 this completes the proof of lemma 1 . @xmath203",
    "we have @xmath244 \\geq ( 1 - 4\\sigma^2 ) \\sum_{t=1}^{t } \\sum_{k=1}^{m } \\lambda_t^{(k)}.\\ ] ] moreover , since @xmath245 , following the similar lines as the proof of lemma 5 in ( @xcite ) , we find that @xmath244 \\leq -\\sigma^4 \\sigma_m^2 \\sum_{t=1}^{t } \\sum_{k=1}^{m } \\lambda_t^{(k ) } + \\frac{1}{\\sigma \\ln(1/\\sigma)}.\\ ] ] since @xmath246 , we conclude that @xmath247 this concludes the proof of lemma 2 .",
    "this work is supported in part by turkish academy of sciences outstanding researcher programme , tubitak contract no .",
    "113e517 , and turk telekom communications services incorporated .",
    "arenas - garcia , j. , azpicueta - ruiz , l.  a. , silva , m. t.  m. , nascimento , v.  h. , and sayed , a.  h. combinations of adaptive filters : performance and convergence properties .",
    "_ ieee signal processing magazine _ , 330 ( 1):0 120140 , jan 2016 .",
    "issn 1053 - 5888 .",
    "doi : 10.1109/msp.2015.2481746 .",
    "babenko , b. , yang , m.  h. , and belongie , s. a family of online boosting algorithms . in _",
    "computer vision workshops ( iccv workshops ) , 2009 ieee 12th international conference on _ , pages 13461353 , sept 2009 . doi : 10.1109/iccvw.2009.5457453 .",
    "bauer , e. and kohavi , r. an empirical comparison of voting classification algorithms : bagging , boosting , and variants . _ machine learning _ , 360 ( 1):0 105139 , 1999 .",
    "issn 1573 - 0565 .",
    "doi : 10.1023/a:1007515423169 .",
    "url http://dx.doi.org/10.1023/a:1007515423169 .",
    "ben - david , s. , kushilevitz , e. , and mansour , y. online learning versus offline learning .",
    "_ machine learning _",
    ", 290 ( 1):0 4563 , 1997 .",
    "issn 1573 - 0565 .",
    "doi : 10.1023/a:1007465907571 .",
    "url http://dx.doi.org/10.1023/a:1007465907571 .",
    "chapelle , o. , shivaswamy , p. , vadrevu , s. , weinberger , k. , zhang , y. , and tseng , b. boosted multi - task learning .",
    "_ machine learning _ , 850 ( 1):0 149173 , 2011 .",
    "issn 1573 - 0565 .",
    "doi : 10.1007/s10994 - 010 - 5231 - 6 .",
    "url http://dx.doi.org/10.1007/s10994-010-5231-6 .",
    "demiriz , a. , bennett , k.  p. , and shawe - taylor , j. linear programming boosting via column generation .",
    "_ machine learning _ , 460 ( 1):0 225254 , 2002 .",
    "issn 1573 - 0565 .",
    "doi : 10.1023/a:1012470815092 .",
    "url http://dx.doi.org/10.1023/a:1012470815092 .",
    "dietterich , t.  g. an experimental comparison of three methods for constructing ensembles of decision trees : bagging , boosting , and randomization .",
    "_ machine learning _ , 400 ( 2):0 139157 , 2000 .",
    "issn 1573 - 0565 .",
    "doi : 10.1023/a:1007607513941 .",
    "url http://dx.doi.org/10.1023/a:1007607513941 .",
    "fern , a. and givan , r. online ensemble learning : an empirical study .",
    "_ machine learning _ , 530 ( 1):0 71109 , 2003 .",
    "issn 1573 - 0565 .",
    "doi : 10.1023/a:1025619426553 .",
    "url http://dx.doi.org/10.1023/a:1025619426553 .",
    "freund , y. an adaptive version of the boost by majority algorithm .",
    "_ machine learning _ , 430 ( 3):0 293318 , 2001 .",
    "issn 1573 - 0565 .",
    "doi : 10.1023/a:1010852229904 .",
    "url http://dx.doi.org/10.1023/a:1010852229904 .",
    "jin , r. and zhang , j. multi - class learning by smoothed boosting .",
    "_ machine learning _ , 670 ( 3):0 207227 , 2007 .",
    "issn 1573 - 0565 .",
    "doi : 10.1007/s10994 - 007 - 5005-y .",
    "url http://dx.doi.org/10.1007/s10994-007-5005-y .",
    "khan , f. , kari , d. , karatepe , i.  a. , and kozat , s.  s. universal nonlinear regression on high dimensional data using adaptive hierarchical trees .",
    "_ ieee transactions on big data _ , 20 ( 2):0 175188 , june 2016 .",
    "doi : 10.1109/tbdata.2016.2555323 .",
    "kozat , s.  s. , singer , a.  c. , and zeitler , g.  c. universal piecewise linear prediction via context trees . _ ieee transactions on signal processing _ , 550 ( 7):0 37303745 , july 2007 .",
    "issn 1053 - 587x .",
    "doi : 10.1109/tsp.2007.894235 .",
    "lu , j. , zhao , p. , and hoi , s. c.  h. online passive - aggressive active learning",
    ". _ machine learning _ , 1030 ( 2):0 141183 , 2016 .",
    "issn 1573 - 0565 .",
    "doi : 10.1007/s10994 - 016 - 5555-y .",
    "url http://dx.doi.org/10.1007/s10994-016-5555-y .",
    "mannor , s. and meir , r. on the existence of linear weak learners and applications to boosting .",
    "_ machine learning _ , 480 ( 1):0 219251 , 2002 .",
    "issn 1573 - 0565 .",
    "doi : 10.1023/a:1013959922467 .",
    "url http://dx.doi.org/10.1023/a:1013959922467 .",
    "pereira , f. , machado , p. , costa , e. , and cardoso , a.",
    "_ progress in artificial intelligence : 17th portuguese conference on artificial intelligence , epia 2015 , coimbra , portugal_. lecture notes in computer science .",
    "springer international publishing , 2015 .",
    "isbn 9783319234854 .",
    "saigo , h. , nowozin , s. , kadowaki , t. , kudo , t. , and tsuda , k. gboost : a mathematical programming approach to graph classification and regression .",
    "_ machine learning _ , 750 ( 1):0 6989 , 2009 .",
    "issn 1573 - 0565 .",
    "doi : 10.1007/s10994 - 008 - 5089-z .",
    "url http://dx.doi.org/10.1007/s10994-008-5089-z .",
    "schapire , r.  e. and singer , y. improved boosting algorithms using confidence - rated predictions .",
    "_ machine learning _",
    ", 370 ( 3):0 297336 , 1999 .",
    "issn 1573 - 0565 .",
    "doi : 10.1023/a:1007614523901 .",
    "url http://dx.doi.org/10.1023/a:1007614523901 .",
    "shalev - shwartz , s. and singer , y. on the equivalence of weak learnability and linear separability : new relaxations and efficient boosting algorithms .",
    "_ machine learning _ , 800 ( 2):0 141163 , 2010 .",
    "issn 1573 - 0565 .",
    "doi : 10.1007/s10994 - 010 - 5173-z .",
    "url http://dx.doi.org/10.1007/s10994-010-5173-z .",
    "soltanmohammadi , e. , naraghi - pour , m. , and van  der schaar , m. context - based unsupervised ensemble learning and feature ranking .",
    "_ machine learning _ , 1050 ( 3):0 459485 , 2016 .",
    "issn 1573 - 0565 .",
    "doi : 10.1007/s10994 - 016 - 5576 - 6",
    ". url http://dx.doi.org/10.1007/s10994-016-5576-6 ."
  ],
  "abstract_text": [
    "<S> we investigate boosted online regression and propose a novel family of regression algorithms with strong theoretical bounds . </S>",
    "<S> in addition , we implement several variants of the proposed generic algorithm . </S>",
    "<S> we specifically provide theoretical bounds for the performance of our proposed algorithms that hold in a strong mathematical sense . </S>",
    "<S> we achieve guaranteed performance improvement over the conventional online regression methods without any statistical assumptions on the desired data or feature vectors . </S>",
    "<S> we demonstrate an intrinsic relationship , in terms of boosting , between the adaptive mixture - of - experts and data reuse algorithms . </S>",
    "<S> furthermore , we introduce a boosting algorithm based on random updates that is significantly faster than the conventional boosting methods and other variants of our proposed algorithms while achieving an enhanced performance gain . </S>",
    "<S> hence , the random updates method is specifically applicable to the fast and high dimensional streaming data . </S>",
    "<S> specifically , we investigate newton method - based and stochastic gradient descent - based linear regression algorithms in a mixture - of - experts setting , and provide several variants of these well known adaptation methods . </S>",
    "<S> however , the proposed algorithms can be extended to other base learners , e.g. , nonlinear , tree - based piecewise linear . </S>",
    "<S> furthermore , we provide theoretical bounds for the computational complexity of our proposed algorithms . </S>",
    "<S> we demonstrate substantial performance gains in terms of mean square error over the base learners through an extensive set of benchmark real data sets and simulated examples . </S>"
  ]
}