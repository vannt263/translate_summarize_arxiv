{
  "article_text": [
    "a functional regression model with functional response variable can be defined by @xmath0 where @xmath1 ( @xmath2 ) stands for @xmath3 batches ( or curves ) of functional data , @xmath4 is an unknown nonlinear function , depending on a set of functional covariates @xmath5 and a set of scalar covariates @xmath6 , and @xmath7 is the random error .",
    "a special case of such model is the following concurrent regression model with functional covariates @xmath5 ( see e.g. * ? ? ?",
    "* ) @xmath8 however , when the relationship between the response and the covariates can not be justified as linear , it is intractable to model the function @xmath4 nonparametrically for multi - dimensional @xmath5 since most nonparametric regression models suffer from the _ curse of dimensionality_. a variety of alternative approaches with special model structures have been proposed to overcome the problem ; examples include dimension reduction methods , the additive model ( see e.g. * ? ? ?",
    "* ) , varying - coefficient model ( see e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , and the neural network model ( see e.g. * ? ? ?",
    "@xcite proposed a gaussian process functional regression ( gpfr ) model , which is defined by @xmath9 where @xmath10 is the mean structure of the functional data and @xmath11 represents a gaussian process regression ( gpr ) model having zero mean and covariance kernel @xmath12 ( for the detailed definition of gaussian process regression models , see * ? ? ?",
    "* ; * ? ? ?",
    "this nonparametric concurrent functional regression model can address the regression problem with multi - dimensional functional covariates and model the mean structure and covariance structure simultaneously ; see the detailed discussion in @xcite .",
    "the aim of this paper is to extend the concurrent gpfr model to situations where the response variable , denoted by @xmath13 , is known to be non - gaussian .",
    "the work is motivated by the following example , concerning data collected during standing - up manoeuvres of paraplegic patients .",
    "the outputs are the human body s standing - up phases during rising from sitting position to standing position .",
    "specifically , @xmath13 takes value of either 0 , 1 or 2 , corresponding to the phases of ` sitting ' , ` seat unloading and ascending ' or ` stablising ' respectively , required for feeding back to a simulator control system .",
    "since it is usually difficult to measure the body position in practice , the aim of the example is to develop a model for reconstructing the position of the human body by using some easily measured quantities such as motion kinematic , reaction forces and torques , which are functional covariates denoted by @xmath14 .",
    "this is to investigate the regression relationship between the non - gaussian functional response variable @xmath13 and a set of functional covariates @xmath14 .",
    "since the standing - up phases are irreversible , @xmath13 is an ordinal response variable , taking value from three ordered categories .",
    "if we assume that there exists an unobservable latent process @xmath15 associated with @xmath14 and the response variable @xmath13 depends on this latent process , then by using a probit link function , we can define a model as follows : @xmath16 where @xmath17 , @xmath18 , and @xmath19 are the thresholds .",
    "now the problem becomes how to model @xmath20 by the functional covariates @xmath14 , or how to find a function @xmath21 such that @xmath22 .",
    "more discussion of this example is given in section [ para_data ] and appendix g of the supplementary materials .",
    "generally , letting @xmath23 be a given link function , a generalized linear regression model is defined as @xmath24 .",
    "@xcite proposed a generalized linear mixed model to deal with heterogeneity : @xmath25 where @xmath26 is the coefficient for the fixed effect and @xmath27 is a random vector representing random effect .",
    "however , if we have little practical knowledge on the relationship between the response variable and the covariates ( such as the case in the above paraplegia example ) , it is more sensible to use a nonparametric model . in this paper",
    ", we propose to use a gaussian process regression model to define such a nonparametric model , namely a concurrent generalized gaussian process functional regression ( ggpfr ) model .",
    "similar to gpfr model @xcite , the advantages of this model include : ( 1 ) it offers a nonparametric generalized concurrent regression model for functional data with functional response and multi - dimensional functional covariates ; ( 2 ) it provides a natural framework on modeling mean structure and covariance structure simultaneously and the latter can be used to model the individual characteristic for each batch ; and ( 3 ) the prior specification of covariance kernel enables us to accommodate a wide class of nonlinear functions .",
    "this paper is organized as follows .",
    "section 2 proposes the ggpfr model and describes how to estimate the hyper - parameters and how to calculate prediction , for which the implementation is mainly based on laplace approximation . the asymptotic properties , focusing on the information consistency ,",
    "are discussed in section 3 .",
    "several numerical examples are reported in section 4 .",
    "discussion and further development are given in section 5 .",
    "some technical details and more numerical examples are provided as the supplementary materials .",
    "let @xmath28 be a functional or longitudinal response variable for the @xmath29-th subject , namely the @xmath29-th batch .",
    "we assume that @xmath30 s are independent for different batches @xmath2 , but within the batch , @xmath31 and @xmath32 are dependent at different points .",
    "we suppose that @xmath30 has a distribution from an exponential family with the following density function @xmath33 where @xmath34 and @xmath35 are canonical parameter and dispersion parameter respectively , both functional .",
    "we have @xmath36 and @xmath37 , where @xmath38 and @xmath39 are the first two derivatives of @xmath40 with respect to @xmath41 .",
    "suppose that @xmath5 is a @xmath42-dimensional vector of functional covariates .",
    "nonparametric concurrent generalized gaussian process functional regression ( ggpfr ) models are defined by and the following @xmath43 here , the unobserved latent variable @xmath44 is modeled by a nonparametric gpr model via a gaussian process prior , depending on the functional covariates @xmath5 .",
    "the gpr model is specified by a covariance kernel @xmath45 , and by the karhunen - love expansion @xmath46 where @xmath47 , @xmath48 are the eigenvalues and @xmath49 are the associated eigenfunctions of the covariance kernel .",
    "one example of @xmath45 is the following squared exponential covariance function with a nonstationary linear term : @xmath50 where @xmath51 is a set of hyper - parameters involved in the gaussian process prior .",
    "the hyper - parameter @xmath52 corresponds to the smoothing parameters in spline and other nonparametric models .",
    "more specifically , @xmath53 is called the length - scale .",
    "the decrease in length - scale produces more rapidly fluctuating functions and a very large length - scale means that the underlying curve is expected to be essentially flat .",
    "more information on the relationship between smoothing splines and gaussian processes can be found in @xcite .",
    "we can use generalized cross - validation ( gcv ) or empirical bayesian method to choose the value of @xmath54 .",
    "when @xmath42 is large , gcv approach is usually inefficient .",
    "we will use the empirical bayesian method in this paper ; the details are given in the next subsection .",
    "some other covariance kernels such as powered exponential and matrn covariance functions can also be used ; see more discussion on the choice of covariance function in @xcite and @xcite",
    ".    in the model given by the response variable @xmath30 depends on @xmath5 at the current time only , therefore the proposed model can be regarded as a generalization of the concurrent functional linear model discussed in @xcite . in this model",
    "the common mean structure across @xmath3 batches is given by @xmath10 .",
    "if we use a linear mean function which depends on a set of @xmath55 scalar covariates @xmath6 only , can be expressed as @xmath56 in this case the regression relationship between the functional response @xmath30 and the functional covariates @xmath5 is modeled by the covariance structure @xmath57 .",
    "other mean structures , including concurrent form of functional covariates , can also be used .",
    "the proposed model has some features worth noting .",
    "in addition to those discussed in section 1 , we highlight that the ggpfr model is actually very flexible .",
    "it can model the regression relationship between the non - gaussian functional response and the multi - dimensional functional covariates nonparametrically .",
    "moreover , if we had known some prior information between @xmath30 ( or @xmath58 ) and some of the functional covariates , we could easily integrate it by adding a parametric mean part .",
    "for example we may define @xmath59 i.e. including a term in the ggpfr similar to the generalized linear mixed model @xcite ; an example of such models is provided in appendix g. the nonparametric part can still be modeled by @xmath44 via a gpr model .",
    "other nonparametric covariance structure can also be considered ; some examples can be found in @xcite , @xcite and @xcite .",
    "however , most of these methods are limited to small ( usually one ) dimensional @xmath14 or the covariance matrix with a special structure .    as an example of the ggpfr model",
    ", we consider a special case of binary data ( e.g. for classification problem with two classes ) . in this case , @xmath60 .",
    "if we use the logit link function , the density function is given by @xmath61 z_m(t ) \\ } } { 1 + \\exp \\{{\\mbox{{\\boldmath $ { u}$}$_{m}^{t}$ } } { \\mbox{\\boldmath $ { \\beta}$}}(t)+\\tau_m(t ) \\ } } .",
    "\\label{binden}\\ ] ] the marginal density function of @xmath30 is therefore given by @xmath62 where @xmath63 is the density function of @xmath44 , which is a multivariate normal distribution for any given points @xmath64 and depends on the functional covariates @xmath5 and the unknown hyper - parameter @xmath54 .",
    "the density functions for other distributions from the exponential families can be obtained similarly .",
    "now suppose that we have @xmath3 batches of data from @xmath3 subjects or experimental units . in the @xmath29-th batch ,",
    "@xmath65 observations are collected at @xmath66 .",
    "we denote @xmath67 , @xmath68 and @xmath69 by @xmath70 , @xmath71 and @xmath72 , respectively , for @xmath73 and @xmath74 .",
    "collectively , we denote @xmath75 and @xmath76 , and denote @xmath77 , @xmath78 , @xmath79 and @xmath80 in the same way .",
    "they are the realizations of @xmath30 , @xmath44 and @xmath81 at @xmath82 . a discrete ggpfr model is therefore given by @xmath83 for @xmath2 , where @xmath84 is a distribution from the exponential family and @xmath85 .",
    "@xmath86 has an @xmath65-variate normal distribution with zero mean and covariance matrix @xmath87 for @xmath88 .",
    "here we assume a fixed dispersion parameter @xmath89 , but the method developed in this paper can be applied to more general cases .",
    "we consider the estimation of @xmath90 first . to estimate the functional coefficient @xmath90 , we expand it by a set of basis functions ( see e.g. * ?",
    "* ) . in this paper",
    ", we use b - spline approximation .",
    "let @xmath91 be the b - spline basis functions , then the functional coefficient @xmath90 can be represented as @xmath92 , where the @xmath93-th column of @xmath94 , @xmath95 , is the b - spline coefficients for @xmath96 .",
    "thus , at the observation point @xmath82 , we have @xmath97 , where @xmath98 is an @xmath99 matrix with the @xmath100-th element @xmath101 . in practice ,",
    "the performance of the b - spline approximation may strongly depend on the choice of the knots locations and the number of basis functions .",
    "there are three widely - used methods for locating the knots : equally spaced method , equally spaced sample quantiles method and model selection based method .",
    "the guidance on which method is to use in different situations can be found in @xcite .",
    "the first method is used in our numerical examples in section 4 which all have equally - spaced time points and the second is adopted in the pbc data in the supplementary materials .",
    "the number of basis functions can be determined by generalized cross - validation or aic ( bic ) methods .",
    "more details on this issue can be found in @xcite .",
    "the covariance matrix @xmath87 of @xmath86 depends on @xmath79 and the unknown hyper - parameter @xmath54 .",
    "if we use covariance kernel ( [ covfun0 ] ) , its element @xmath102 is given by @xmath103 the covariance matrix involves the hyper - parameter @xmath104 , whose value is given based on the prior knowledge in conventional bayesian analysis .",
    "as discussed in @xcite , empirical bayesian learning method is preferable for gpr models when the dimension of @xmath54 is large .",
    "the idea of empirical bayesian learning is to choose the value of the hyper - parameter @xmath54 by maximizing the marginal density function .",
    "thus , @xmath54 as well as the unknown parameter @xmath94 can be estimated at the same time by maximizing the following marginal density @xmath105 or the marginal log - likelihood @xmath106 where @xmath107 is derived from the exponential family as defined in . for binomial distribution",
    ", it is given in .",
    "obviously the integral involved in the above marginal density is analytically intractable unless @xmath107 has a special form such as the density function of normal distribution .",
    "one method to address this problem is to use laplace approximation .",
    "we denote @xmath108 then the log - likelihood can be rewritten as @xmath109 let @xmath110 be the maximiser of @xmath111 , then by laplace approximation we have @xmath112 where @xmath113 is the second order derivative of @xmath114 with respect to @xmath115 and evaluated at @xmath110 ( see , for example , * ? ? ?",
    "* ; * ? ? ?",
    "the procedure of finding the maximiser @xmath110 can be carried out by the newton - raphson iterative method and is given in appendix a of the supplementary materials .    however , as pointed out in section 4.1 in @xcite , the error rate of the approximation may be @xmath116 since the dimension of @xmath115 increases with the sample size @xmath65 .",
    "a better method is to approximate @xmath117 in ( here and in the rest of the section the conditioning on @xmath79 is omitted for simplicity ) by @xmath118 where @xmath119 , @xmath120 is the gaussian approximation to the full conditional density @xmath121 , and is the mode of the full conditional density of @xmath86 for a given @xmath122 . here ,",
    "@xmath123 we approximate @xmath124 by taylor expansion to the second order @xmath125 where @xmath126 and @xmath127 depend on the first two derivatives of @xmath128 respectively and evaluated at @xmath129 .",
    "thus , @xmath130 where @xmath131 and @xmath132 .",
    "we can then use the following fisher scoring algorithm @xcite to find the gaussian approximation .",
    "starting with @xmath129 , the @xmath133-th iteration is given by    * find the solution @xmath134 from @xmath135 * update @xmath136 and @xmath137 using @xmath134 and repeat ( i ) .",
    "after the process converges , say at @xmath138 , we get the gaussian approximation @xmath120 which is the density function of the normal distribution @xmath139 we can then calculate @xmath140 by maximizing using the approximation .",
    "now we consider two types of prediction problems .",
    "first suppose that we have already observed some data for a subject , say @xmath141 observations in the @xmath133-th batch , and want to obtain prediction at other points .",
    "this can be for one of the batches @xmath142 or a completely new one .",
    "the observations are denoted by @xmath143 which are collected at @xmath144 .",
    "the corresponding input vectors are @xmath145 , and we also know the subject - based covariate @xmath146 .",
    "it is of interest to predict @xmath147 at a new point @xmath148 for the @xmath133-th subject given the test input @xmath149 .",
    "secondly we will assume there are no data observed from the subject of interest except the subject - based covariate and want to predict @xmath147 at a new point @xmath148 with the input @xmath150 .",
    "we use @xmath151 to denote all the training data and assume that the model itself has been trained ( i.e. all the unknown parameters have been estimated ) by the method discussed in the previous section .",
    "the main purpose in this section is to calculate @xmath152 and @xmath153 , which are used as the prediction and the predictive variance of @xmath147 .",
    "we now consider the first type of prediction .",
    "let @xmath154 be the underlying latent variable at @xmath148 , then @xmath155 ( for convenience we ignore the subscript ) and @xmath156 satisfy , and the expectation of @xmath147 conditional on @xmath155 is given by ( [ hmi ] ) : @xmath157 it follows that @xmath158 = \\int h({\\mbox{{\\boldmath $ { u}$}$_{k}^{t}$}}{\\hat{{\\mbox{\\boldmath $ { b}$}}}}^t{\\mbox{\\boldmath $ { \\phi}$}}(t^ * ) + \\tau^ * ) p(\\tau^*|{\\mbox{${\\cal d}$ } } ) d\\tau^*.     \\label{pred_mean0}\\ ] ] a simple method to calculate the above expectation is to approximate @xmath159 using a gaussian approximation @xmath160 as discussed around equation , that is , @xmath161 since it is assumed that both @xmath162 and @xmath155 come from the same gaussian process with covariance kernel @xmath163 , we have @xmath164}\\ ] ] where @xmath165 is the covariance matrix of @xmath162 , and @xmath166 is a vector of the covariances between @xmath162 and @xmath155 .",
    "thus , @xmath167 where @xmath168 and @xmath169 . from the discussion given in the last paragraph in section 2.2",
    ", we have @xmath170    the integrand in is therefore the product of two normal density functions . it is not difficult to prove ( see the details in appendix b of the supplementary materials ) that @xmath159 is still a normal density function @xmath171 then can be evaluated by numerical integration .    to calculate @xmath153 , we use the formula : @xmath172 + { \\mbox{var}}[{\\mbox{e}}(z^*|\\tau^*,{\\mbox{${\\cal d}$ } } ) ] .",
    "\\label{var1}\\ ] ] from the model definition , we have @xmath173 = { \\mbox{e}}\\big[{\\mbox{e}}(z^*|\\tau^*,{\\mbox{${\\cal d}$}})\\big]^2 - \\big({\\mbox{e}}[{\\mbox{e}}(z^*|\\tau^*,{\\mbox{${\\cal d}$}})]\\big)^2   \\nonumber",
    "\\\\ = & \\int \\big [ h({\\mbox{{\\boldmath $ { u}$}$_{k}^{t}$}}\\hat{b}^t { \\mbox{\\boldmath $ { \\phi}$}}(t^ * ) + \\tau^*)\\big]^2 p(\\tau^*|{\\mbox{${\\cal d}$ } } ) d\\tau^ * - \\big[{\\mbox{e}}(z^*|{\\mbox{${\\cal d}$}})\\big]^2 , \\label{var2 } \\end{aligned}\\ ] ] and @xmath174 = \\int { \\mbox{var}}(z^*|\\tau^*,{\\mbox{${\\cal d}$ } } ) p(\\tau^*|{\\mbox{${\\cal d}$ } } ) d\\tau^ * = \\int b''(\\hat{\\alpha}^ * ) a(\\phi ) p(\\tau^*|{\\mbox{${\\cal d}$ } } ) d\\tau^ * , \\label{var3}\\ ] ] where @xmath175 is a function of @xmath176 , and @xmath159 is given by .",
    "thus and can also be evaluated by numerical integration .",
    "the posterior density @xmath159 in is obtained based on the gaussian approximation @xmath160 to @xmath177 .",
    "it usually gives quite accurate results .",
    "the methods to improve gaussian approximation were discussed in @xcite .",
    "they can also be used to calculate @xmath159 from .",
    "an alternative way is to use the first integral in to replace @xmath159 in and perform a multi - dimensional integration using , for example , laplace approximation ; see appendix c of the supplementary materials for the details .",
    "the second type of prediction is to predict a completely new batch with subject - based covariate @xmath178 .",
    "we want to predict @xmath147 at @xmath179 . in this case , the training data @xmath180 are the data collected from the batches @xmath181 . since we have not observed any data for this new batch , we can not directly use the predictive mean and variance discussed above .",
    "a simple way is to predict @xmath147 by using @xmath182 , i.e. ignoring @xmath155 in .",
    "this approach however does not use the information of @xmath150 , the observed functional covariates .",
    "alternatively as argued in @xcite , batches @xmath183 to @xmath184 actually provide an empirical distribution of the set of all possible subjects .",
    "a similar idea is used here .",
    "we assume that , for @xmath2 , @xmath185 if we assume that the new batch or @xmath147 belongs to the @xmath29-th batch , we can calculate the conditional predictive mean by , formulated by @xmath186 the predictive mean @xmath187 in and the predictive variance @xmath188 in can be calculated as if the test data belong to the @xmath29-th batch . here",
    "both @xmath178 and @xmath150 are used .",
    "based on the above empirical assumption , the prediction of the response for the test input @xmath150 at @xmath148 in a completely new subject is @xmath189 and the predictive variance is @xmath190^{2 } - \\big[{\\mbox{e}}(z^*|{\\mbox{${\\cal d}$}})\\big]^2 .",
    "\\label{newpvar}\\ ] ] we usually use the equal weights , i.e. @xmath191 for @xmath2 . in general these @xmath3 batches may not provide equal information to the new batch . in this case",
    "varying weights may be considered ; see more discussion in @xcite .",
    "the consistency of gaussian process functional regression method involves two issues .",
    "one is related to the common mean @xmath192 in and the other is related to the curve @xmath193 itself ( @xmath194 or a new one ) .",
    "the common mean structure is estimated from the data collected from all @xmath3 subjects , and has been proved to be consistent in many functional linear models under suitable regularity conditions ( see * ? ? ?",
    "* ; * ? ? ?",
    "this paper focuses on the second issue , the consistency of @xmath195 to @xmath196 , one of the key features in nonparametric regression .",
    "this kind of problems for gpr related models have received increasing attention in recent years , see for example @xcite , @xcite and @xcite .",
    "@xcite considered the posterior consistency of gaussian process prior for normal response , @xcite proved the posterior consistency of gaussian process prior for nonparametric binary regression no matter what the mean function of gaussian process prior is set to , and @xcite extended the result to poisson distribution .",
    "but the consistency for general exponential family distributions is yet to be investigated .",
    "meanwhile , @xcite proved the information consistency via a regret bound on cumulative log loss . generally speaking ,",
    "if the sample size of the data collected from a certain curve is sufficiently large and the covariance function satisfies certain regularity conditions , the prediction based on a gpr model is consistent to the real curve , and the consistency does not depend on the common mean structure or the choice of the values of hyper - parameters involved in covariance function ; see more detailed discussion in @xcite .    in this section",
    ", we discuss the information consistency and extend the result of @xcite to a more general context such as @xmath197 following poisson distribution which has not been covered in the literature .",
    "similar to other gpr related models , the consistency of @xmath195 to @xmath196 depends on the observations collected from the @xmath133-th curve only .",
    "we assume that the underlying mean function for the @xmath133-th curve , denoted by @xmath198 , is known .",
    "the case where the mean function is estimated from data is discussed in the supplementary materials .",
    "for ease of presentation we omit the subscript @xmath133 in the rest of the section and denote the data by @xmath199 at the points @xmath200 , and the corresponding covariate values @xmath201 where @xmath202 are independently drawn from a distribution @xmath203 .",
    "let @xmath204 .",
    "we assume that @xmath205 is a set of samples taking values in @xmath206 and follows a distribution in exponential family , @xmath207 for an inverse link function @xmath208 , and the underlying process @xmath209 .",
    "therefore , the stochastic process @xmath210 induces a measure on space @xmath211 .",
    "suppose that the hyper - parameter @xmath212 in the covariance function of @xmath210 is estimated by empirical bayesian method and the estimator is denoted by @xmath213 .",
    "let @xmath214 be the true underlying function , i.e. the true mean of @xmath215 is given by @xmath216 .",
    "denote @xmath217 then @xmath218 is the bayesian predictive distribution of @xmath205 based on the gpr model .",
    "note that @xmath219 depends on @xmath220 since the hyper - parameter of @xmath210 is estimated from the data .",
    "it is said that @xmath218 achieves _ information consistency _",
    "if @xmath221 \\big ) \\rightarrow 0   \\quad\\text { as } n\\rightarrow \\infty ,   \\label{inf : con}\\ ] ] where @xmath222 denotes the expectation under the distribution of @xmath223 and @xmath224 $ ] is the kullback - leibler divergence between @xmath225 and @xmath226 , i.e. @xmath227=\\int p_0(z)\\log \\frac{p_0(z)}{p_{gp}(z)}dz.\\ ] ]    * theorem 1 : * under the ggpfr models and and the conditions given in lemma 1 of the supplementary materials , the prediction @xmath228 is information consistent to the true curve @xmath229 if the rkhs norm @xmath230 is bounded and the expected regret term @xmath231 .",
    "the error bound is specified in .    * remark 1 . *",
    "the condition @xmath232 in lemma 1 can be satisfied by a wide range of distributions , such as normal distribution where @xmath233 , binomial distribution ( with the number of trials @xmath29 ) where @xmath234 and poisson distribution where @xmath235 .    * remark 2 . *",
    "the regret term @xmath236 depends on the covariance function @xmath12 and the covariate distribution @xmath203 .",
    "it can be shown that for some widely used covariance functions , such as linear , squared exponential and matrn class , the expected regret terms are of order @xmath237 ; see @xcite for the detailed discussion .    * remark 3 .",
    "* lemma 1 requires that the estimator of the hyperparameter @xmath212 is consistent . in appendix d of the supplementary materials we proved that the estimator by maximizing the marginal likelihood based on laplace approximation satisfies this condition when the number of curves and the number of observations on each curve are sufficiently large",
    "this implies that the information consistency in ggpfr models is achieved for the covariance functions listed in remark 2 .",
    "a more general asymptotic analysis is to study the convergence rates of both the mean function estimation and the individual curves prediction when the number of curves and/or the number of observations on each curve tend to infinity , as discussed in @xcite for the maximum likelihood estimators of the parameters in mixed - effects models .",
    "the research along this direction is worth further development .",
    "in this section we demonstrate the proposed method with serveral examples .",
    "we first use simulated data and then consider the paraplegia data discussed in section 1 .",
    "more simulated and real examples are provided in the supplementary materials .      *",
    "( i ) simulation study . * the true model used to generate the latent process is @xmath238 , where , for each @xmath29",
    ", @xmath239 s are equally spaced points in @xmath240 and @xmath241 is a gaussian process with zero mean and the squared exponential covariance function defined in with @xmath242 , @xmath243 and @xmath244 . in this example , the covariate @xmath245 is the same as @xmath246 .",
    "the observations @xmath70 follow a binomial distribution @xmath247 with @xmath248    sixty curves , each containing @xmath65 data points , are generated and used as training data .",
    "we use a ggpfr model with binomial distribution and logit link function : @xmath249 where @xmath44 follows a gpr model .",
    "cubic b - spline approximation is used to estimate the mean curve @xmath250 , where the knots are placed at equally spaced points in the range and the number of basis functions is determined by bic which is given by bic@xmath251 with @xmath252 being the total number of parameters . a gaussian approximation method as specified around is used to calculate the empirical bayesian estimates of @xmath94 and @xmath54 .",
    "table_parameter ] lists the average estimates of the hyper - parameters @xmath253 for @xmath254 , 40 and 60 for ten replications .",
    "the empirical bayesian estimates are closer to the true values with @xmath65 increasing .",
    "the estimates of mean curve @xmath255 for different @xmath65 along with the true mean curves are presented in the left panels of figure [ simu_mean_curves ] . as discussed in section 3 ,",
    "the consistency of @xmath255 to @xmath250 depends on the observations obtained from all training curves .",
    "the figures show that the estimated mean curves by the ggpfr method are very close to the true one even for the case of @xmath254 .",
    "[ [ para_fig11]interpolation].,title=\"fig : \" ] [ [ para_fig12]interpolation].,title=\"fig : \" ] + [ [ para_fig13]extrapolation].,title=\"fig : \" ] [ [ para_fig14]extrapolation].,title=\"fig : \" ] + [ [ figbetat]].,title=\"fig : \" ]"
  ],
  "abstract_text": [
    "<S> in this paper we propose a generalized gaussian process concurrent regression model for functional data where the functional response variable has a binomial , poisson or other non - gaussian distribution from an exponential family while the covariates are mixed functional and scalar variables . </S>",
    "<S> the proposed model offers a nonparametric generalized concurrent regression method for functional data with multi - dimensional covariates , and provides a natural framework on modeling common mean structure and covariance structure simultaneously for repeatedly observed functional data . </S>",
    "<S> the mean structure provides an overall information about the observations , while the covariance structure can be used to catch up the characteristic of each individual batch . </S>",
    "<S> the prior specification of covariance kernel enables us to accommodate a wide class of nonlinear models . </S>",
    "<S> the definition of the model , the inference and the implementation as well as its asymptotic properties are discussed . </S>",
    "<S> several numerical examples with different non - gaussian response variables are presented . some technical details and more numerical examples as well as an extension of the model </S>",
    "<S> are provided as supplementary materials .    </S>",
    "<S> key words : covariance kernel , exponential family , concurrent regression models , nonparametric regression .    * author s footnote *    b. wang is lecturer in statistics , department of mathematics , university of leicester , leicester le1 7rh , uk ( e - mail : ` bw77@leicester.ac.uk ` ) . </S>",
    "<S> j. q. shi is reader in statistics , school of mathematics and statistics , newcastle university , newcastle ne1 7ru , uk ( e - mail : ` j.q.shi@ncl.ac.uk ` ) . </S>",
    "<S> the authors thank the associate editor and the reviewers for their constructive suggestions and helpful comments . </S>"
  ]
}