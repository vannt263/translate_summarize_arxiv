{
  "article_text": [
    "9 a comprehensive account of the existing techniques can be found in nightingale m p and umrigar c j 1999 _ quantum monte carlo methods in physics and chemistry _",
    "( the netherlands : kluwer academic publishers ) see for instance golub g h and van loan c f 1996 _ matrix computations _",
    "( baltimore : john hopkins university press ) davidson e r 1975 _ j. computational phys . _",
    "* 17 * 87 shavitt i , bender c f , pipano a , and hosteny r p 1973 _ j. computational phys . _",
    "* 11 * 90 ruhe a 1977 in _ lecture notes in mathematics _ * 527 * , edited by a. dold and b. eckmann , ( springer - verlag , berlin ) p 130 elsner u , mehrmann v , milde f , rmer r a , and schreiber m 1999 _ siam j. on scient . comp._*20 * 2089"
  ],
  "abstract_text": [
    "<S> we propose a new iterative algorithm for generating a subset of eigenvalues and eigenvectors of large matrices which generalizes the method of optimal relaxations . </S>",
    "<S> we also give convergence criteria for the iterative process , investigate its efficiency by evaluating computer storage and time requirements and by few numerical tests .    </S>",
    "<S> the increasing computational power has stimulated a growing interest toward developing and refining methods which allow to determine selected eigenvalues of a complex quantum system with extreme accuracy . </S>",
    "<S> widely adopted , specially for computing ground state properties , are the quantum monte carlo methods @xcite , where a properly defined function of the hamiltonian is used as a stochastic matrix which guides a markov process to sample the basis .    </S>",
    "<S> alternatively , one may resort to direct diagonalization methods , like the lanczos @xcite and davidson @xcite algorithms , of wide use in several branches of physics . </S>",
    "<S> the critical points of direct diagonalization methods are the amount of memory needed and the time spent in the diagonalization process . because of these limitations , several systems are still out of reach even with the computer power now available .    </S>",
    "<S> in this paper we present an iterative method , extremely easy to implement , for generating a subset of eigenvectors of a large matrix , give convergence criteria and show that it represents a generalization of the method of optimal relaxations @xcite .    </S>",
    "<S> we assume first that the matrix @xmath0 represents a self - adjoint operator @xmath1 in an orthonormal basis @xmath2 and is symmetric ( @xmath3 ) . </S>",
    "<S> for the sake of simplicity , we illustrate the procedure for a one - dimensional eigenspace . </S>",
    "<S> the algorithm consists of a first approximation loop and subsequent iterations of refinement loops . </S>",
    "<S> the first loop goes through the following steps :    * 1a ) * start with the first two basis vectors and diagonalize the matrix @xmath4 , where @xmath5 .    * </S>",
    "<S> 1b ) * select the eigenvalue @xmath6 and the corresponding eigenvector @xmath7 , where @xmath8 .    0.1truecm    * for * @xmath9    0.5truecm * 1c ) * compute @xmath10 .    0.5truecm * 1d ) * diagonalize the matrix   @xmath11    0.5truecm * 1e ) * select the eigenvalue @xmath12 and the corresponding eigenvector @xmath13 .    </S>",
    "<S> * end * @xmath14    0.2truecm    the first loop yields an approximate eigenvalue @xmath15 and an approximate eigenvector @xmath16 . with these new entries we start an iterative procedure which goes through the following refinement loops :    * for * @xmath17 till convergence    0.5truecm * for * @xmath18    1.0truecm * 2a ) * compute @xmath19 .    </S>",
    "<S> 1.0truecm * 2b ) * solve the eigenvalue problem in the general form @xmath20 \\ ; = \\;0\\ ] ] where the appearance of the metric matrix follows from the non orthogonality of the redifined basis @xmath21 and @xmath22 .    </S>",
    "<S> 1.0truecm * 2c ) * select the eigenvalue @xmath23 and the corresponding eigenvector @xmath24 .    0.5truecm </S>",
    "<S> * end * @xmath14    * end * @xmath25 .    </S>",
    "<S> 0.2truecm    the @xmath25-th loop yields an approximate eigenvalue @xmath26 . as for the eigenvector , at any step of the @xmath14-loop , we have @xmath27 with the appropriate normalization condition @xmath28 ^ 2\\;+\\;[q_j^{(n)}]^2\\;+\\;2\\;p_j^{(n)}q_j^{(n)}k_{j-1,j}^{(n ) } \\;=\\;1 $ ] . </S>",
    "<S> the iteration of eq . </S>",
    "<S> ( [ phi ] ) yields the @xmath25-th eigenvector @xmath29 where the numbers @xmath30 are @xmath31 the algorithm defines therefore the sequence of vectors ( 2 ) , whose convergence properties we can now examine . </S>",
    "<S> the @xmath32 and @xmath33 coefficients can be expressed as @xmath34^{1 \\over 2}}},\\nonumber\\\\ p_j^{(n ) } & \\;=\\;&(a_{jj}k_{j-1,j}^{(n)}-b_j^{(n ) } ) \\ ; { q_j^{(n ) } \\over b_j^{(n ) } } \\label{pj},\\end{aligned}\\ ] ] where @xmath35 \\;-\\ ; k_{j-1,j}^{(n ) }   \\big [   ( a_{jj}-\\lambda_{j}^{(n ) } ) ( \\lambda_{j-1}^{(n ) } - \\lambda_j^{(n ) } ) { \\big]}^{1 \\over 2}.\\ ] ] it is apparent from these relations that , if @xmath36 the sequence @xmath37 has a limit @xmath38 , which is an eigenvector of the matrix @xmath0 . </S>",
    "<S> in fact , defining the residual vectors @xmath39 a direct computation gives for their components @xmath40}^{1 \\over 2 }   + q_n^{(n ) } \\ ; \\big\\ { a_{ln } - \\lambda_{n}^{(n ) } \\delta_{ln } \\big\\ } \\nonumber \\\\ & - &   p_n^{(n ) } \\big\\ {   \\big ( \\lambda_{l-1}^{(n)}-\\lambda_{l}^{(n ) } \\big )   \\ ; k_{l , l-1}^{(n ) } + \\big ( \\lambda_{n-1}^{(n)}-\\lambda_{n}^{(n ) }   \\big ) \\ ; k_{l , n-1}^{(n ) } \\big\\}.\\end{aligned}\\ ] ] in virtue of ( [ conv ] ) , the norm of the @xmath25-th residual vector converges to zero , namely @xmath41 . </S>",
    "<S> equation ( [ conv ] ) gives therefore a necessary condition for the convergence of @xmath42 to an eigenvector @xmath43 of @xmath0 , with a corresponding eigenvalue @xmath44 . </S>",
    "<S> this condition holds independently of the prescription adopted for selecting the eigensolution . </S>",
    "<S> indeed , we never had to specify the selection rule in steps 1b ) , 1e ) and 2c ) . </S>",
    "<S> equation ( [ conv ] ) is not only a necessary but also a sufficient condition for the convergence to the lowest or the highest eigenvalue of @xmath0 . </S>",
    "<S> in fact , the sequence @xmath23 is monotonic ( decreasing or increasing , respectively ) , bounded from below or from above by the trace and therefore convergent .    </S>",
    "<S> the just outlined algorithm has a variational foundation . </S>",
    "<S> its variational counterpart is just the method of optimal relaxation @xcite . </S>",
    "<S> indeed , for the @xmath33 and @xmath32 given by eqs . </S>",
    "<S> ( [ pj ] ) , the @xmath45 derivative of the rayleigh quotient @xmath46 vanishes identically .    on the other hand </S>",
    "<S> the present matrix formulation allows in a straightforward way for the optimal relaxation of an arbitrary number @xmath47 of coordinates , thereby improving the convergence rate of the procedure . </S>",
    "<S> we only need to turn the two - dimensional into a @xmath48-dimensional eigenvalue problem in steps 1d ) and 2b ) , compute t elements @xmath49 in steps 1c ) and 2a ) , and accordingly redefine the @xmath14-loops . </S>",
    "<S> the current eigenvector is still defined by the iterative relation ( @xmath50 ) @xmath51 which automatically fulfils the extremal conditions @xmath52 moreover , the algorithm can be naturally extended to generate at once an arbitrary number @xmath53 of lowest eigenstates . </S>",
    "<S> we have simply to replace the two - dimensional matrices with multidimensional ones having the following block structure : a @xmath54 submatrix diagonal in the selected @xmath53 eigenvalues , which replaces @xmath55 , a @xmath56 submatrix corresponding to @xmath57 and two @xmath58 off - diagonal blocks replacing @xmath59 or @xmath60 . </S>",
    "<S> this new formulation amounts to an optimal relaxation method of several coordinates into a multidimensional subspace . </S>",
    "<S> it avoids therefore the use of deflation or shift techniques for the computation of higher eigenvalues and eigenvectors .    </S>",
    "<S> it remains now to investigate the practical feasibility of the method . </S>",
    "<S> the main issues to be faced are the storage and time requirements . in the one - dimensional case </S>",
    "<S> , we need to store a single @xmath61-dimensional vector ( the eigenvector ) . the time is mainly determined by the @xmath14 loop . </S>",
    "<S> this requires @xmath61 operations for implementing point 2a ) plus @xmath62 remaining operations . since @xmath63 and @xmath64 </S>",
    "<S> , the algorithm requires altogether @xmath65 operations . </S>",
    "<S> it follows that , for large dimensional matrices , the number of operations grows like @xmath66 . for sparse matrices with an average number @xmath67 of non zero matrix elements , </S>",
    "<S> the required number of operations is @xmath68 and therefore grows linearly with @xmath61 . in the multidimensional case </S>",
    "<S> we need to store @xmath53 @xmath61-dimensional vectors . if necessary , however , we can keep only one at a time and store the remaining @xmath69 vectors in a secondary storage . </S>",
    "<S> this latter feature clearly shows that the algorithm lends itself to a straightforward parallelization . </S>",
    "<S> also in the multidimensional case , the number of operations grows as @xmath70 .    </S>",
    "<S> the algorithm has other remarkable properties : i ) it works perfectly even in case of degeneracy of the eigenvalues . </S>",
    "<S> ii ) the diagonalization of the submatrices of order @xmath71 insures the orthogonalization of the full n - dimensional eigenvectors at each step . </S>",
    "<S> therefore , no _ ghost _ eigenvalues occur . </S>",
    "<S> iii ) the range of validity of the algorithm can be easily enlarged if we remove some of the initial assumptions . </S>",
    "<S> clearly , the iterative procedure applies to a non - orthogonal basis . </S>",
    "<S> we simply need to substitute steps 1a ) and 1d ) of the first loop with the appropriate generalized eigenvalue problem . </S>",
    "<S> it applies also to non symmetric matrices . </S>",
    "<S> we have only to update both right and left eigenvectors and perform steps 1c ) and 2a ) for both non - diagonal matrix elements .    in order to test the efficiency and the convergence rate of the iterative procedure , we have applied the method to several examples . </S>",
    "<S> the first is a 5-point finite difference matrix arising from the two - dimensional laplace equation @xcite . </S>",
    "<S> this is a block - tridiagonal matrix of @xmath72 @xmath73-dimensional blocks , whose eigenvalues are @xmath74 where @xmath75 and @xmath76 . as in @xcite , we considered a block - matrix with @xmath77 and @xmath78 . </S>",
    "<S> we have tested the one - dimensional as well as the multidimensional version of the algorithm . as shown in figure [ fig1 ] , the iterative procedure converges much faster in the multidimensional case . </S>",
    "<S> in fact , the convergence rate increases with the number @xmath79 of generated eigenvalues and is considerably faster than in lanczos . </S>",
    "<S> it is also to be stressed that our algorithm allows for an arbitrarily high accuracy , up to the machine precision limit . </S>",
    "<S> the method , specially in its multidimensional extension , is quite effective even if applied to the same matrix with @xmath80 so as to allow for degeneracy . for @xmath81 , </S>",
    "<S> it yields the lowest seven roots , including two couples of degenerate eigenvalues , with an accuracy of @xmath82 .    </S>",
    "<S> a second example , still taken from @xcite , is a one - dimensional biharmonic band matrix whose eigenvalues @xmath83 are small and poorly separated from each other . </S>",
    "<S> a similarly high density of levels occurs in the anderson model of localization @xcite . </S>",
    "<S> because of this peculiarity , the limit of the machine precision is reached for a modest increase of the dimension @xmath61 of the matrix . </S>",
    "<S> our method reproduces perfectly any number of eigenvalues with the required accuracy after a small number of iterations . in the specific example discussed in @xcite ( @xmath84 ) we attained the highest accuracy after eight iterations , much less than all methods discussed there . </S>",
    "<S> we have checked that , unlike others , our method works without any modification even if we increase the dimension @xmath61 up to the limit compatible with the machine precision . in this case </S>",
    "<S> the number of iterations needed increase by an order of magnitude , in any case , below 100 .    a third example is provided by a matrix with diagonal matrix elements @xmath85 and off - diagonal ones @xmath86 or @xmath87 according that they fall within or outside a band of width @xmath88 . such a matrix simulates a pairing hamiltonian relevant to many branches of physics . </S>",
    "<S> we have considered a matrix of dimension @xmath89 and half - band width @xmath90 . </S>",
    "<S> we found convergence after @xmath91 iterations , reaching an accuracy of @xmath92 for the eigenvalues . </S>",
    "<S> the time required to compute the lowest eigenvalue through the one - dimensional algorithm is @xmath93 s for a workstation of @xmath94 mhz and @xmath95 mb of ram .    </S>",
    "<S> finally , we generalize the latter example by considering a full matrix of dimension @xmath96 with matrix elements @xmath97 . </S>",
    "<S> their alternating signs are also to be noticed , since they decrease somewhat the rate of convergence of the process . </S>",
    "<S> we reproduce the lowest eigenvalue with an accuracy of @xmath98 after @xmath99 iterations respectively .    in conclusion , </S>",
    "<S> the present diagonalization algorithm is a generalization of the variational optimal relaxation method and , on the ground of the examples discussed , appears to be more competitive than the methods currently adopted . </S>",
    "<S> it seems to be faster and to require a minimal amount of computer storage . </S>",
    "<S> it is extremely simple to be implemented and is _ robust _ , yielding always stable numerical solutions . </S>",
    "<S> moreover , it is free of _ ghost _ eigenvalues . </S>",
    "<S> because of these features , we are confident that it can be applied quite effectively to physical systems , like medium - light nuclei or quantum dots with few electrons . </S>"
  ]
}