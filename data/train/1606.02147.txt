{
  "article_text": [
    "recent interest in augmented reality wearables , home - automation devices , and self - driving vehicles has created a strong need for semantic - segmentation ( or visual scene - understanding ) algorithms that can operate in real - time on low - power mobile devices .",
    "these algorithms label each and every pixel in the image with one of the object classes . in recent years",
    ", the availability of larger datasets and computationally - powerful machines have helped deep convolutional neural networks ( cnns ) @xcite surpass the performance of many conventional computer vision algorithms @xcite .",
    "even though cnns are increasingly successful at classification and categorization tasks , they provide coarse spatial results when applied to pixel - wise labeling of images .",
    "therefore , they are often cascaded with other algorithms to refine the results , such as color based segmentation @xcite or conditional random fields @xcite , to name a few .",
    "in order to both spatially classify and finely segment images , several neural network architectures have been proposed , such as segnet @xcite or fully convolutional networks @xcite .",
    "all these works are based on a vgg16 @xcite architecture , which is a very large model designed for multi - class classification .",
    "these references propose networks with huge numbers of parameters , and long inference times . in these conditions , they become unusable for many mobile or battery - powered applications , which require processing images at rates higher than 10 fps .    in this paper",
    ", we propose a new neural network architecture optimized for fast inference and high accuracy .",
    "examples of images segmented using enet are shown in figure [ fig : intro ] . in our work , we chose not to use any post - processing steps , which can of course be combined with our method , but would worsen the performance of an end - to - end cnn approach .    in section [ architecture ]",
    "we propose a fast and compact encoder - decoder architecture named enet .",
    "it has been designed according to rules and ideas that have appeared in the literature recently , all of which we discuss in section [ design ] .",
    "proposed network has been evaluated on cityscapes @xcite and camvid @xcite for driving scenario , whereas sun dataset @xcite has been used for testing our network in an indoor situation .",
    "we benchmark it on nvidia jetson tx1 embedded systems module as well as on an nvidia titan x gpu .",
    "the results can be found in section [ results ] .",
    "width=1.0,center    cmmm input image & & & + enet output & & & +    [ fig : intro ]",
    "semantic segmentation is important in understanding the content of images and finding target objects .",
    "this technique is of utmost importance in applications such as driving aids and augmented reality .",
    "moreover , real - time operation is a must for them , and therefore , designing cnns _ carefully _ is vital .",
    "contemporary computer vision applications extensively use deep neural networks , which are now one of the most widely used techniques for many different tasks , including semantic segmentation .",
    "this work presents a new neural network architecture , and therefore we aim to compare to other literature that performs the large majority of inference in the same way .",
    "state - of - the - art scene - parsing cnns use two separate neural network architectures combined together : an encoder and a decoder . inspired by probabilistic auto - encoders @xcite",
    ", encoder - decoder network architecture has been introduced in segnet - basic @xcite , and further improved in segnet @xcite .",
    "the encoder is a vanilla cnn ( such as vgg16 @xcite ) which is trained to classify the input , while the decoder is used to upsample the output of the encoder @xcite .",
    "however , these networks are slow during inference due to their large architectures and numerous parameters . unlike in fully convolutional networks",
    "( fcn ) @xcite , fully connected layers of vgg16 were discarded in the latest incarnation of segnet , in order to reduce the number of floating point operations and memory footprint , making it the smallest of these networks .",
    "still , none of them can operate in real - time .",
    "other existing architectures use simpler classifiers and then cascade them with conditional random field ( crf ) as a post - processing step @xcite . as shown in @xcite ,",
    "these techniques use onerous post - processing steps and often fail to label the classes that occupy fewer number of pixels in a frame .",
    "cnns can be also combined with recurrent neural networks @xcite to improve accuracy , but then they suffer from speed degradation .",
    "also , one has to keep in mind that rnn , used as a post - processing step , can be used in conjunction with any other technique , including the one presented in this work .",
    "the architecture of our network is presented in table [ tab : structure ] .",
    "it is divided into several stages , as highlighted by horizontal lines in the table and the first digit after each block name .",
    "output sizes are reported for an example input image resolution of @xmath1 .",
    "we adopt a view of resnets @xcite that describes them as having a single main branch and extensions with convolutional filters that separate from it , and then merge back with an element - wise addition , as shown in figure [ fig : bottleneck ] .",
    "each block consists of three convolutional layers : a @xmath2 projection that reduces the dimensionality , a main convolutional layer ( ` conv ` in figure [ fig : bottleneck ] ) , and a @xmath2 expansion .",
    "we place batch normalization @xcite and prelu @xcite between all convolutions . just as in the original paper",
    ", we refer to these as bottleneck modules .",
    "if the bottleneck is downsampling , a max pooling layer is added to the main branch .",
    "r2.75 in    [ tab : structure ]    @l > m0.7 in c @ name & type & output size + initial & & @xmath3 + bottleneck1.0 & downsampling & @xmath4 + @xmath5 bottleneck1.x & & @xmath4 + bottleneck2.0 & downsampling & @xmath6 + bottleneck2.1 & & @xmath6 + bottleneck2.2 & dilated @xmath7 & @xmath6 + bottleneck2.3 & asymmetric @xmath8 & @xmath6 + bottleneck2.4 & dilated @xmath9 & @xmath6 + bottleneck2.5 & & @xmath6 + bottleneck2.6 & dilated @xmath10 & @xmath6 + bottleneck2.7 & asymmetric @xmath8 & @xmath6 + bottleneck2.8 & dilated @xmath11 & @xmath6 +   + bottleneck4.0 & upsampling & @xmath4 + bottleneck4.1 & & @xmath4 + bottleneck4.2 & & @xmath4 + bottleneck5.0 & upsampling & @xmath3 + bottleneck5.1 & & @xmath3 + fullconv & & @xmath12 +    also , the first @xmath2 projection is replaced with a @xmath13 convolution with stride @xmath7 in both dimensions .",
    "we zero pad the activations , to match the number of feature maps . `",
    "conv ` is either a regular , dilated or full convolution ( also known as deconvolution or fractionally strided convolution ) with @xmath14 filters .",
    "sometimes we replace it with asymmetric convolution i.e. a sequence of @xmath15 and @xmath16 convolutions . for the regularizer",
    ", we use spatial dropout @xcite , with @xmath17 before bottleneck2.0 , and @xmath18 afterwards .",
    ".5   windows , and the convolution has 13 filters , which sums up to 16 feature maps after concatenation .",
    "this is heavily inspired by @xcite .",
    "( b ) enet bottleneck module .",
    "` conv ` is either a regular , dilated , or full convolution ( also known as deconvolution ) with @xmath14 filters , or a @xmath19 convolution decomposed into two asymmetric ones.,title=\"fig:\",width=172 ] +    .5   windows , and the convolution has 13 filters , which sums up to 16 feature maps after concatenation .",
    "this is heavily inspired by @xcite .",
    "( b ) enet bottleneck module .",
    "` conv ` is either a regular , dilated , or full convolution ( also known as deconvolution ) with @xmath14 filters , or a @xmath19 convolution decomposed into two asymmetric ones.,title=\"fig:\",height=172 ]    the initial stage contains a single block , that is presented in figure [ fig : initial ] .",
    "stage 1 consists of @xmath8 bottleneck blocks , while stage 2 and 3 have the same structure , with the exception that stage 3 does not downsample the input at the beginning ( we omit the @xmath20th bottleneck ) .",
    "these three first stages are the encoder .",
    "stage 4 and 5 belong to the decoder .",
    "we did not use bias terms in any of the projections , in order to reduce the number of kernel calls and overall memory operations , as cudnn @xcite uses separate kernels for convolution and bias addition .",
    "this choice did nt have any impact on the accuracy . between each convolutional layer and following non - linearity we use batch normalization @xcite .",
    "in the decoder max pooling is replaced with max unpooling , and padding is replaced with spatial convolution without bias .",
    "we did not use pooling indices in the last upsampling module , because the initial block operated on the @xmath21 channels of the input frame , while the final output has @xmath22 feature maps ( the number of object classes ) .",
    "also , for performance reasons , we decided to place only a bare full convolution as the last module of the network , which alone takes up a sizeable portion of the decoder processing time .",
    "in this section we will discuss our most important experimental results and intuitions , that have shaped the final architecture of enet .",
    "[ [ feature - map - resolution ] ] feature map resolution + + + + + + + + + + + + + + + + + + + + + +    downsampling images during semantic segmentation has two main drawbacks .",
    "firstly , reducing feature map resolution implies loss of spatial information like exact edge shape .",
    "secondly , full pixel segmentation requires that the output has the same resolution as the input .",
    "this implies that strong downsampling will require equally strong upsampling , which increases model size and computational cost .",
    "the first issue has been addressed in fcn @xcite by adding the feature maps produced by encoder , and in segnet @xcite by saving indices of elements chosen in max pooling layers , and using them to produce sparse upsampled maps in the decoder .",
    "we followed the segnet approach , because it allows to reduce memory requirements .",
    "still , we have found that strong downsampling hurts the accuracy , and tried to limit it as much as possible .    however , downsampling has one big advantage .",
    "filters operating on downsampled images have a bigger receptive field , that allows them to gather more context .",
    "this is especially important when trying to differentiate between classes like , for example , rider and pedestrian in a road scene .",
    "it is not enough that the network learns how people look , the context in which they appear is equally important . in the end",
    ", we have found that it is better to use dilated convolutions for this purpose @xcite .",
    "[ [ early - downsampling ] ] early downsampling + + + + + + + + + + + + + + + + + +    one crucial intuition to achieving good performance and real - time operation is realizing that processing large input frames is very expensive .",
    "this might sound very obvious , however many popular architectures do not to pay much attention to optimization of early stages of the network , which are often the most expensive by far .",
    "enet first two blocks heavily reduce the input size , and use only a small set of feature maps . the idea behind it , is that visual information is highly spatially redundant , and thus can be compressed into a more efficient representation .",
    "also , our intuition is that the initial network layers should not directly contribute to classification .",
    "instead , they should rather act as good feature extractors and only preprocess the input for later portions of the network .",
    "this insight worked well in our experiments ; increasing the number of feature maps from @xmath11 to @xmath23 did not improve accuracy on cityscapes @xcite dataset .",
    "[ [ decoder - size ] ] decoder size + + + + + + + + + + + +    in this work we would like to provide a different view on encoder - decoder architectures than the one presented in @xcite .",
    "segnet is a very symmetric architecture , as the encoder is an exact mirror of the encoder .",
    "instead , our architecture consists of a large encoder , and a small decoder .",
    "this is motivated by the idea that the encoder should be able to work in a similar fashion to original classification architectures , i.e. to operate on smaller resolution data and provide for information processing and filtering . instead , the role of the the decoder , is to upsample the output of the encoder , only fine - tuning the details .    [",
    "[ nonlinear - operations ] ] nonlinear operations + + + + + + + + + + + + + + + + + + + +    a recent paper @xcite reports that it is beneficial to use relu and batch normalization layers before convolutions .",
    "we tried applying these ideas to enet , but this had a detrimental effect on accuracy .",
    "instead , we have found that removing most relus in the initial layers of the network improved the results .",
    "it was quite a surprising finding so we decided to investigate its cause .",
    "we replaced all relus in the network with prelus @xcite , which use an additional parameter per feature map , with the goal of learning the negative slope of non - linearities .",
    "we expected that in layers where identity is a preferable transfer function , prelu weights will have values close to @xmath24 , and conversely , values around @xmath20 if relu is preferable . results of this experiment can be seen in figure [ fig : prelu ] .",
    "initial layers weights exhibit a large variance and are slightly biased towards positive values , while in the later portions of the encoder they settle to a recurring pattern .",
    "all layers in the main branch behave nearly exactly like regular relus , while the weights inside bottleneck modules are negative i.e. the function inverts and scales down negative values .",
    "we hypothesize that identity did not work well in our architecture because of its limited depth .",
    "the reason why such lossy functions are learned might be that that the original resnets @xcite are networks that can be hundreds of layers deep , while our network uses only a couple of layers , and it needs to quickly filter out information .",
    "it is notable that the decoder weights become much more positive and learn functions closer to identity .",
    "this confirms our intuitions that the decoder is used only to fine - tune the upsampled output .",
    "[ fig : prelu ]    [ [ information - preserving - dimensionality - changes ] ] information - preserving dimensionality changes + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    as stated earlier , it is necessary to downsample the input early , but aggressive dimensionality reduction can also hinder the information flow .",
    "a very good approach to this problem has been presented in @xcite .",
    "it has been argued that a method used by the vgg architectures , i.e. as performing a pooling followed by a convolution expanding the dimensionality , however relatively cheap , introduces a representational bottleneck ( or forces one to use a greater number of filters , which lowers computational efficiency ) . on the other hand , pooling after a convolution , that increases feature map depth , is computationally expensive .",
    "therefore , as proposed in @xcite , we chose to perform pooling operation in parallel with a convolution of stride 2 , and concatenate resulting feature maps .",
    "this technique allowed us to speed up inference time of the initial block @xmath25 times .",
    "additionally , we have found one problem in the original resnet architecture .",
    "when downsampling , the first @xmath2 projection of the convolutional branch is performed with a stride of @xmath7 in both dimensions , which effectively discards @xmath26 of the input . increasing the filter size to @xmath13 allows to take the full input into consideration , and",
    "thus improves the information flow and accuracy .",
    "of course , it makes these layers @xmath5 more computationally expensive , however there are so few of these in enet , that the overhead is unnoticeable .    [ [ factorizing - filters ] ] factorizing filters + + + + + + + + + + + + + + + + + + +    it has been shown that convolutional weights have a fair amount of redundancy , and each @xmath27 convolution can be decomposed into two smaller ones following each other : one with a @xmath28 filter and the other with a @xmath29 filter @xcite .",
    "this idea has been also presented in @xcite , and from now on we adopt their naming convention and will refer to these as asymmetric convolutions .",
    "we have used asymmetric convolutions with @xmath30 in our network , so cost of these two operations is similar to a single @xmath14 convolution .",
    "this allowed to increase the variety of functions learned by blocks and increase the receptive field .",
    "what s more , a sequence of operations used in the bottleneck module ( projection , convolution , projection ) can be seen as decomposing one large convolutional layer into a series of smaller and simpler operations , that are its low - rank approximation .",
    "such factorization allows for large speedups , and greatly reduces the number of parameters , making them less redundant @xcite .",
    "additionally , it allows to make the functions they compute richer , thanks to the non - linear operations that are inserted between layers .",
    "[ [ dilated - convolutions ] ] dilated convolutions + + + + + + + + + + + + + + + + + + + +    as argued above , it is very important for the network to have a wide receptive field , so it can perform classification by taking a wider context into account .",
    "we wanted to avoid overly downsampling the feature maps , and decided to use dilated convolutions @xcite to improve our model .",
    "they replaced the main convolutional layers inside several bottleneck modules in the stages that operate on the smallest resolutions .",
    "these gave a significant accuracy boost , by raising iou on cityscapes by around @xmath9 percentage points , with no additional cost .",
    "we obtained the best accuracy when we interleaved them with other bottleneck modules ( both regular and asymmetric ) , instead of arranging them in sequence , as has been done in @xcite .",
    "[ [ regularization ] ] regularization + + + + + + + + + + + + + +    most pixel - wise segmentation datasets are relatively small ( on order of @xmath31 images ) , so such expressive models as neural networks quickly begin to overfit them . in initial experiments , we used l2 weight decay with little success .",
    "then , inspired by @xcite , we have tried stochastic depth , which increased accuracy .",
    "however it became apparent that dropping whole branches ( i.e. setting their output to @xmath20 ) is in fact a special case of applying spatial dropout @xcite , where either all of the channels , or none of them are ignored , instead of selecting a random subset .",
    "we placed spatial dropout at the end of convolutional branches , right before the addition , and it turned out to work much better than stochastic depth .",
    "we benchmarked the performance of enet on three different datasets to demonstrate real - time and accurate for practical applications .",
    "we tested on camvid and cityscapes datasets of road scenes , and sun rgb - d dataset of indoor scenes .",
    "we set segnet @xcite as a baseline since it is one of the fastest segmentation models , that also has way fewer parameters and requires less memory to operate than fcn .",
    "all our models , training , testing and performance evaluation scripts were using the torch7 machine - learning library , with cudnn backend .",
    "to compare results , we use class average accuracy and intersection - over - union ( iou ) metrics .",
    "we report results on inference speed on widely used nvidia titan x gpu as well as on nvidia tx1 embedded system module .",
    "enet was designed to achieve more than @xmath25 fps on the nvidia tx1 board with an input image size @xmath32 , which is adequate for practical road scene parsing applications . for inference",
    "we merge batch normalization and dropout layers into the convolutional filters , to speed up all networks .",
    "[ tab : speed ]    .performance comparison . [ cols= \" < , > , > , > , > , > , > , > , > , > , > , > , > , > , > \" , ]     [ [ sun - rgb - d ] ] sun rgb - d + + + + + + + + +    the sun dataset consists of 5285 training images and 5050 testing images with 37 indoor object classes .",
    "we did not make any use of depth information in this work and trained the network only on rgb data . in table",
    "[ tab : sun1 ] we compare the performance of enet with segnet @xcite , which is the only neural network model that reports accuracy on this dataset .",
    "our results , though inferior in global average accuracy and iou , are comparable in class average accuracy . since global average accuracy and iou are metrics that favor correct classification of classes occupying large image patches , researchers generally emphasize the importance of other metrics in case of semantic segmentation .",
    "one notable example is introduction of iiou metric @xcite .",
    "comparable result in class average accuracy indicates , that our network is capable of differentiating smaller objects nearly as well as segnet . moreover , the difference in accuracy should not overshadow the huge performance gap between these two networks .",
    "enet can process the images in real - time , and is nearly @xmath33 faster than segnet on embedded platforms .",
    "example predictions from sun test set are shown in figure [ fig : sun ] .",
    "width=1.0,center    cmmmm input image & & & & + ground truth & & & & + enet output & & & & +    [ fig : cityscapes ]    width=1.0,center    cmmmm input image & & & & + ground truth & & & & + enet output & & & & +    [ fig : camvid ]    width=1.0,center    cmmmm input image & & & & + ground truth & & & & + enet output & & & & +    [ fig : sun ]",
    "we have proposed a novel neural network architecture designed from the ground up specifically for semantic segmentation .",
    "our main aim is to make efficient use of scarce resources available on embedded platforms , compared to fully fledged deep learning workstations .",
    "our work provides large gains in this task , while matching and at times exceeding existing baseline models , that have an order of magnitude larger computational and memory requirements .",
    "the application of enet on the nvidia tx1 hardware exemplifies real - time portable embedded solutions .",
    "even though the main goal was to run the network on mobile devices , we have found that it is also very efficient on high end gpus like nvidia titan x. this may prove useful in data - center applications , where there is a need of processing large numbers of high resolution images .",
    "enet allows to perform large - scale computations in a much faster and more efficient manner , which might lead to significant savings .",
    "this work is partly supported by the office of naval research ( onr ) grants n00014 - 12 - 1 - 0167 , n00014 - 15 - 1 - 2791 and muri n00014 - 10 - 1 - 0278 .",
    "we gratefully acknowledge the support of nvidia corporation with the donation of the tx1 , titan x , k40 gpus used for this research ."
  ],
  "abstract_text": [
    "<S> the ability to perform pixel - wise semantic segmentation in real - time is of paramount importance in mobile applications . </S>",
    "<S> recent deep neural networks aimed at this task have the disadvantage of requiring a large number of floating point operations and have long run - times that hinder their usability . in this paper , we propose a novel deep neural network architecture named enet ( efficient neural network ) , created specifically for tasks requiring low latency operation . </S>",
    "<S> enet is up to 18@xmath0 faster , requires 75@xmath0 less flops , has 79@xmath0 less parameters , and provides similar or better accuracy to existing models . </S>",
    "<S> we have tested it on camvid , cityscapes and sun datasets and report on comparisons with existing state - of - the - art methods , and the trade - offs between accuracy and processing time of a network . </S>",
    "<S> we present performance measurements of the proposed architecture on embedded systems and suggest possible software improvements that could make enet even faster . </S>"
  ]
}