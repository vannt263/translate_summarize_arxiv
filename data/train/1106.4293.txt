{
  "article_text": [
    "real - world data such as those obtained from neuroscience , chemometrics , data mining or sensor - rich environments are often extremely high - dimensional , severely underconstrained ( few data samples compared to the dimensionality of the data ) and interspersed with a large number of irrelevant or redundant features .",
    "furthermore , in most situations the data is contaminated by noise , making it even more difficult to retrieve useful information from the data .",
    "relevant variable selection is a compelling approach for addressing statistical issues in the scenario of high - dimensional and noisy data with small sample size . starting from mallows  @xcite , akaike  @xcite , schwarz @xcite who introduced , respectively ,",
    "the famous criteria @xmath7 , aic and bic , the problem of variable selection was extensively studied in the statistical and machine learning literature both from the theoretical and algorithmic viewpoints .",
    "it appears , however , that the theoretical limits of performing variable selection in the context of nonparametric regression are still poorly understood , especially when the number of variables , denoted by @xmath0 and referred to as ambient dimension , is much larger than the sample size @xmath2 .",
    "the purpose of the present work is to explore this setting under the assumption that the number of relevant variables , hereafter called intrinsic dimension and denoted by @xmath8 , may grow with the sample size but remains much smaller than @xmath0 .    in the important particular case of linear regression ,",
    "the latter scenario was the subject of a number of recent studies .",
    "many of them rely on @xmath9-norm penalization  @xcite and constitute an attractive alternative to iterative variable selection procedures  @xcite and to marginal regression or correlation screening @xcite .",
    "promising results for feature selection are also obtained by conformal prediction  @xcite , ( minimax ) concave penalties  @xcite , bayesian approach  @xcite and higher criticism @xcite .",
    "extensions to other settings including logistic regression , generalized linear model and ising model were carried out in  @xcite , respectively .",
    "variable selection in the context of groups of variables with disjoint or overlapping groups was studied by  @xcite .",
    "hierarchical procedures for selection of relevant variables were proposed by  @xcite .",
    "it is now well understood that in the gaussian sequence model and in the high - dimensional linear regression with a gram matrix satisfying some variant of irrepresentable condition , consistent estimation of the pattern of relevant variables  also called the sparsity pattern  is possible under the condition @xmath10 as @xmath4  @xcite .",
    "furthermore , it is well known that if @xmath11 remains bounded from below by some positive constant when @xmath4 , then it is impossible to consistently recover the sparsity pattern  @xcite .",
    "thus , a tight condition exists that describes in an exhaustive manner the interplay between the quantities  @xmath8 , @xmath0 and @xmath2 that guarantees the existence of consistent estimators .",
    "the situation is very different in the case of nonlinear regression , since , to our knowledge , there is no result providing tight conditions for consistent estimation of the sparsity pattern .",
    "lafferty and wasserman  @xcite and bertin and lecu ' e  @xcite , in papers closely related to the present work , considered the problem of variable selection in nonparametric gaussian regression model .",
    "they proved the consistency of the proposed procedures under some assumptions that  in the light of the present work  turn out to be suboptimal .",
    "more precisely , lafferty and wasserman  @xcite assumed the unknown regression function to be four times continuously differentiable with bounded derivatives .",
    "the algorithm they proposed , termed rodeo , is a greedy procedure performing simultaneously local bandwidth choice and variable selection .",
    "rodeo is shown to converge when the ambient dimension @xmath0 is @xmath12 while the intrinsic dimension @xmath8 does not increase with @xmath2 .",
    "on the other hand , bertin and lecu  @xcite proposed a procedure based on the @xmath9-penalization of local polynomial estimators and proved its consistency when @xmath13 , but @xmath0 is allowed to be as large as @xmath6 , up to a constant .",
    "they also had a weaker assumption on the regression function merely assumed to belong to the holder class with smoothness @xmath14 . to complete the picture ,",
    "let us mention that estimation and hypotheses testing problems for high - dimensional nonparametric regression under sparse additive modeling were recently addressed in  @xcite .",
    "this brief review of the literature reveals that there is an important gap in consistency conditions for the linear regression and for the nonlinear one .",
    "for instance , if the intrinsic dimension @xmath8 is fixed , then the condition guaranteeing consistent estimation of the sparsity pattern is @xmath15 in linear regression , whereas it is @xmath16 in the nonparametric case .",
    "while it is undeniable that the nonparametric regression is much more complex than the linear one , it is , however , not easy to find a justification to such an important gap between two conditions .",
    "the situation is even worse in the case where @xmath17 .",
    "in fact , for the linear model with at most polynomially increasing ambient dimension @xmath18 , it is possible to estimate the sparsity pattern for intrinsic dimensions @xmath8 as large as @xmath19 , for some @xmath20 . in other words",
    ", the sparsity index can be almost on the same order as the sample size .",
    "in contrast , in nonparametric regression , there is no procedure that is proved to converge to the true sparsity pattern when both @xmath2 and @xmath8 tend to infinity , even if @xmath8 grows extremely slowly .    in the present work",
    ", we fill this gap by introducing a simple variable selection procedure that selects the relevant variables by comparing some quadratic functionals of empirical fourier coefficients to prescribed significance levels .",
    "consistency of this procedure is established under some conditions on the triplet @xmath21 , and the tightness of these conditions is proved .",
    "the main take - away messages deduced from our results are the following :    * when the number of relevant variables @xmath8 is fixed and the sample size @xmath2 tends to infinity , there exist positive real numbers @xmath22 and @xmath23 such that ( a ) if @xmath24 the estimator proposed in section  [ sec3 ] is consistent and ( b ) no estimator of the sparsity pattern may be consistent if @xmath25 .",
    "* when the number of relevant variables @xmath8 tends to infinity with @xmath4 , then there exist real numbers @xmath26 and @xmath27 , @xmath28 such that @xmath29 , @xmath30 and ( a )  if @xmath31 the estimator proposed in section  [ sec3 ] is consistent and ( b )  no estimator of the sparsity pattern may be consistent if @xmath32 . * in particular ,",
    "if @xmath0 grows not faster than a polynomial in @xmath2 , then there exist positive real numbers @xmath33 and @xmath34 such that ( a ) if @xmath35 , the estimator proposed in section  [ sec3 ] is consistent , and ( b ) no estimator of the sparsity pattern may be consistent if @xmath36 .    in the regime of a growing intrinsic dimension @xmath17 and a moderately large ambient dimension @xmath37 , for some @xmath38 , we make a concentrated effort to get the constant @xmath39 as close as possible to the constant @xmath40 .",
    "this goal is reached for the model of gaussian white noise and , very surprisingly , it required from us to apply some tools from complex analysis , such as the jacobi @xmath41-function and the saddle point method , in order to evaluate the number of lattice points lying in a ball of an euclidean space with increasing dimension .",
    "the rest of the paper is organized as follows .",
    "the notation and assumptions necessary for stating our main results are presented in section  [ sec2 ] . in section",
    "[ sec3 ] , an estimator of the set of relevant variables is introduced and its consistency is established , in the case where the data come from the gaussian white noise model . the main condition required in the consistency result involves the number of lattice points in a ball of a high - dimensional euclidean space . an asymptotic equivalent for this number is presented in section  [ sec4 ] .",
    "results on impossibility of consistent estimation of the sparsity pattern are derived in section  [ sec5 ] .",
    "section  [ secadapt ] is devoted to exploring adaptation to the unknown parameters ( smoothness and degree of significance ) and recovering minimax rates of separation .",
    "then , in section  [ sec6 ] , we show that some of our results can be extended to the model of nonparametric regression .",
    "the relations between consistency and inconsistency results are discussed in section  [ sec7 ] .",
    "the technical parts of the proofs are postponed to the .",
    "we are interested in the variable selection task ( also known as model selection , feature selection , sparsity pattern estimation ) in the context of high - dimensional nonlinear regression .",
    "let @xmath42^d\\to\\rr$ ] denote the unknown regression function .",
    "we assume that the number of variables @xmath0 is very large , possibly much larger than the sample size @xmath2 , but only a small number of these variables contribute to the fluctuations of the regression function @xmath43 .    to be more precise",
    ", we assume that for some small subset @xmath44 of the index set @xmath45 satisfying @xmath46 , there is a function @xmath47 such that @xmath48 where @xmath49 stands for the subvector of @xmath50 obtained by removing from @xmath50 all the coordinates with indices lying outside @xmath44 . in what follows ,",
    "we allow @xmath0 and @xmath8 to depend on @xmath2 , but we will not always indicate this dependence in notation .",
    "note also that the genuine intrinsic dimension is @xmath51 ; @xmath8 is merely a known upper bound on the intrinsic dimension . in what follows ,",
    "we use the standard notation for the vector and sequence norms : @xmath52 .",
    "let us stress right away that the primary aim of this work is to understand when it is possible to estimate the sparsity pattern @xmath44 ( with theoretical guarantees on the convergence of the estimator ) and when it is impossible .",
    "the estimator that we will define in the next sections is intended to show the possibility of consistent estimation , rather than to provide a practical procedure for recovering the sparsity pattern .",
    "therefore , the estimator will be allowed to depend on different constants appearing in conditions imposed on the regression function @xmath43 and on some characteristics of the noise .    to make the consistent estimation of the set @xmath44 realizable , we impose some smoothness and identifiability assumptions on @xmath43 . in order to describe the smoothness assumption imposed on @xmath43 , let us introduce the trigonometric fourier basis , @xmath53 and @xmath54 where @xmath55 denotes the set of all @xmath56 such that the first nonzero element of @xmath57 is positive , and @xmath58 stands for the usual inner product in @xmath59 . in what follows ,",
    "we use the notation @xmath60 for designing the scalar product in @xmath61^d;\\rr)$ ] , that is , @xmath62^d } \\sfh(\\mathbf{x})\\tilde\\sfh(\\mathbf{x } ) \\,d\\mathbf{x}$ ] for every @xmath63^d;\\rr)$ ] . using this orthonormal fourier basis ,",
    "we define @xmath64 to ease notation , we set @xmath65=\\langle\\sff,\\varphi_\\bk \\rangle$ ] for all @xmath66 .",
    "in addition to the smoothness , we need also to require that the relevant variables are sufficiently relevant for making their identification possible .",
    "this is done by means of the following condition .",
    "the regression function @xmath43 belongs to @xmath67 .",
    "furthermore , for some subset @xmath68 of cardinality @xmath69 , there exists a function @xmath70 such that @xmath71 , @xmath72 , and it holds that @xmath73=\\sum_{\\bk : k_j\\neq0 } \\theta_\\bk[\\sff]^2\\geq\\kappa \\qquad\\forall j\\in j.\\ ] ]    one easily checks that @xmath74=0 $ ] for every @xmath75 that does not lie in the sparsity pattern .",
    "this provides a characterization of the sparsity pattern as the set of indices of nonzero coefficients of the vector @xmath76=(q_1[\\sff],\\ldots , q_d[\\sff])$ ] .",
    "prior to describing the procedures for estimating @xmath44 , let us comment on condition  [ c1 ] .",
    "it is important to note that the identifiability assumption ( [ ident ] ) can be rewritten as @xmath77^d } ( \\sff(\\mathbf{x})-\\int_0 ^ 1\\sff(\\mathbf{x } ) \\,dx_j ) ^2 \\,d\\mathbf{x}\\ge\\kappa$ ] and , therefore , is not intrinsically related to the basis we have chosen . in the case of continuously differentiable and @xmath78-periodic function @xmath43 , the smoothness assumption @xmath79 as well can be rewritten without using the trigonometric basis , since @xmath80 ^ 2=(2\\pi ) ^{-2}\\int_{[0,1]^d}[\\partial_j \\sff(\\mathbf{x})]^2 \\,d\\mathbf{x}$ ] .",
    "thus condition [ c1 ] is essentially a constraint on the function @xmath43 itself and not on its representation in the specific basis of trigonometric functions .",
    "the results of this work can be extended with minor modifications to other types of smoothness conditions imposed on @xmath43 , such as hlder continuity or besov - regularity . in these cases",
    "the trigonometric basis ( [ trig ] ) should be replaced by a basis adapted to the smoothness condition ( spline , wavelet , etc . ) .",
    "furthermore , even in the case of sobolev smoothness , one can replace the set @xmath67 corresponding to smoothness order @xmath78 by any sobolev ellipsoid of smoothness @xmath81 ; see , for instance , @xcite where the case @xmath82 is explored .",
    "roughly speaking , the role of the smoothness assumption is to reduce the statistical model with infinite - dimensional parameter @xmath43 to a finite - dimensional model having good approximation properties .",
    "any value of smoothness order @xmath81 leads to this reduction .",
    "the value @xmath83 is chosen for simplicity of exposition only .",
    "to convey the main ideas without taking care of some technical details , we start by focusing our attention on the gaussian white noise model that was proved to be asymptotically equivalent to the model of regression @xcite , as well as to other nonparametric models  @xcite .",
    "thus , we assume that the available data consists of the gaussian process @xmath84^d;\\rr)\\}$ ] such that @xmath85=\\int_{[0,1]^d } \\sff ( \\mathbf{x } ) \\phi ( \\mathbf{x } ) \\,d\\mathbf{x},\\qquad \\operatorname{cov}_f \\bigl(\\by(\\phi),\\by\\bigl(\\phi'\\bigr)\\bigr)=\\frac1n\\int _ { [ 0,1]^d } \\phi ( \\mathbf{x})\\phi'(\\mathbf{x } ) \\,d \\mathbf{x}.\\ ] ] it is well known that these two properties uniquely characterize the probability distribution of a gaussian process . an alternative representation of @xmath86 is @xmath87^d,\\ ] ] where @xmath88 is a @xmath0-parameter brownian sheet .",
    "note that minimax estimation and detection of the function @xmath43 in this set - up ( but without sparsity assumption ) was studied by  @xcite .",
    "we intend to tackle the variable selection problem by multiple hypotheses testing ; each hypothesis concerns a group of the fourier coefficients of the observed signal and suggests that all the elements within the group are zero .",
    "the rationale behind this approach is the following simple observation : since the trigonometric basis is orthonormal and contains the constant function , @xmath89=\\langle f,\\varphi_\\bk \\rangle=0 \\qquad\\forall\\bk \\mbox { s.t . }",
    "k_j \\not=0.\\ ] ] this observation entails that if the intrinsic dimension @xmath90 is small as compared to  @xmath0 , then the sequence of fourier coefficients is sparse .",
    "furthermore , as explained below , there is a sort of group sparsity with overlapping groups .    for every @xmath91 ,",
    "we denote by @xmath92 the set of all subsets @xmath93 of @xmath45 having exactly @xmath94 elements : @xmath95 . for every multi - index @xmath66",
    ", we denote by @xmath96 the set of indices corresponding to nonzero entries of @xmath57 . to define the blocks of coefficients @xmath97",
    "that will be tested for significance , we introduce the following notation : for every @xmath98 and for every @xmath99 , we set @xmath100= \\bigl(\\theta_\\bk[\\sff]\\dvtx j \\in\\supp(\\bk)\\subset i \\bigr).\\ ] ] it follows from ( [ crit1 ] ) that the characterization @xmath101 \\bigr\\|_p=0,\\ ] ] holds true for every @xmath102 $ ] .",
    "furthermore , again in view of ( [ crit1 ] ) , the maximum over @xmath93 of the norms @xmath103 \\|_p$ ] is attained when @xmath104 and is equal to the maximum over all subsets @xmath93 such that @xmath105 . summarizing these arguments",
    ", we can formulate the problem of variable selection as a problem of testing @xmath0 null hypotheses @xmath106 \\bigr\\|_p=0 \\qquad \\forall i\\subset\\{1,\\ldots , d\\ } \\mbox { such that } \\operatorname{card}(i)\\le d^*.\\ ] ] if the hypothesis @xmath107 is rejected , then the @xmath75th covariate is declared as relevant . note that by virtue of assumption [ c1 ] , the alternatives can be written as @xmath108 \\bigr\\|_2 ^ 2\\ge\\kappa\\qquad\\mbox{for some } i\\subset\\{1,\\ldots , d\\ } \\mbox { such that } \\operatorname{card}(i)\\le d^*.\\hspace*{-35pt}\\ ] ] our estimator is based on this characterization of the sparsity pattern . if we denote by @xmath109 the observable random variable @xmath110 , we have @xmath111+n^{-1/2 } \\xi_\\bk,\\qquad \\theta_\\bk=\\langle f,\\varphi_\\bk \\rangle , \\bk\\in\\zz^d,\\ ] ] where @xmath112 form a countable family of independent gaussian random variables with zero mean and variance equal to one .",
    "according to this property , @xmath109 is a good estimate of @xmath65 $ ] : it is unbiased and with a mean squared error equal to @xmath113 .",
    "using the plug - in argument , this suggests to estimate @xmath114 by @xmath115 and the norm of @xmath114 by the norm of @xmath116 .",
    "however , since this amounts to estimating an infinite - dimensional vector , the error of estimation will be infinitely large . to cope with this issue",
    ", we restrict the set of indices for which @xmath117 is estimated by @xmath109 to a finite set , outside of which @xmath117 will be merely estimated by @xmath118 .",
    "such a restriction is justified by the fact that @xmath43 is assumed to be smooth : fourier coefficients corresponding to very high frequencies are very small .",
    "let us fix an integer @xmath119 , the cut - off level , and denote , for @xmath120 , @xmath121 since the alternatives @xmath122 are concerned with the 2-norm , we build our test statistic on an estimate of the norm @xmath123\\|_2 $ ] . to this end , we introduce @xmath124 which is an unbiased estimator of @xmath125 .",
    "note that when @xmath126 , the quantity @xmath127 approaches @xmath123\\|_2 ^ 2 $ ] .",
    "it is clear that larger values of @xmath128 lead to a smaller bias while the variance get increased .",
    "moreover , the variance of @xmath129 is proportional to the cardinality of the set @xmath130 .",
    "the latter is an increasing function of @xmath131 .",
    "therefore , if we aim at getting comparable estimation accuracies when estimating the functionals @xmath123\\|_2 ^ 2 $ ] by @xmath129 for various @xmath93 s , it is reasonable to make the cut - off level @xmath128 vary with the cardinality of @xmath93 .",
    "thus , we consider a multivariate cut - off @xmath132 . for a subset",
    "@xmath93 of cardinality @xmath133 , we test significance of the vector @xmath134 $ ] by comparing its estimate @xmath135 with a prescribed threshold @xmath136 .",
    "this leads us to define an estimator of the set @xmath44 by @xmath137 where @xmath138 and @xmath139 are two vectors of tuning parameters .",
    "as already mentioned , the role of @xmath140 is to ensure that the truncated sums @xmath141 do not deviate too much from the complete sums @xmath142 .",
    "quantitatively speaking , for a given @xmath143 , we would like to choose @xmath144 s so that @xmath145 , where @xmath146 .",
    "this guarantee can be achieved due to the smoothness assumption .",
    "indeed , as proved in ( [ minor ] ) ( cf .",
    "appendix  [ appa1 ] ) , it holds that @xmath147 therefore , choosing @xmath148 , for every @xmath149 , entails the inequality @xmath150 , which indicates that the relevance of variables is not affected too much by the truncation .",
    "pushing further the analogy with the hypotheses testing , we define type i error of an estimator @xmath151 of @xmath44 as the one of having @xmath152 , that is , classifying some irrelevant variables as relevant .",
    "the type ii error is then that of having @xmath153 , which amounts to classifying some relevant variables as irrelevant .",
    "as in the testing problem , handling the type i error is easier since the distribution of the test statistic is independent of @xmath43 .",
    "in fact , this is the max of a finite family of random variables drawn from translated and scaled @xmath154-distributions . using the bonferroni adjustment leads to the following control of the first kind error .",
    "[ prop0 ] let us denote by @xmath155 the cardinality of the set @xmath156 .",
    "if for some @xmath157 and for every @xmath149 , @xmath158 then the type i error @xmath159 is upper - bounded by @xmath160 , and therefore tends to 0 as @xmath161 .",
    "this proposition shows that the type i error of a variable selection procedure may be made small by choosing a sufficiently high threshold . by doing this",
    ", we run the risk to reject @xmath107 very often and to drastically underestimate the set of relevant variables .",
    "the next result establishes a necessary condition , which will be shown to be tight , ensuring that such an underestimation does not occur .",
    "[ prop2 ] let condition be satisfied with some known constants @xmath162 and @xmath163 , and let @xmath146 . for some real numbers @xmath143 and @xmath157 , set @xmath148 , @xmath149 , and define @xmath136 to be equal to the right - hand side of ( [ lambda ] ) .",
    "if the condition @xmath164 is fulfilled , then @xmath165 is consistent and satisfies the inequalities @xmath166 and @xmath167 .",
    "condition ( [ ordre2 ] ) , ensuring the consistency of the variable selection procedure @xmath151 , admits a very natural interpretation : it is possible to detect relevant variables if the degree of relevance @xmath168 is larger than a multiple of the threshold @xmath169 , the latter being chosen according to the noise level .",
    "a first observation is that this theorem provides interesting insight to the possibility of consistent recovery of the sparsity pattern @xmath44 in the context of fixed intrinsic dimension .",
    "in fact , when @xmath8 remains bounded from above when @xmath170 and @xmath171 , then we get that @xmath172 provided that=-1 @xmath173=0 although we did not find ( exactly ) this result in the statistical literature on variable selection , it can be checked that ( [ eq9a ] ) is a necessary and sufficient condition for recovering the sparsity pattern @xmath44 in linear regression with fixed sparsity @xmath8 and growing dimension @xmath0 and sample size @xmath2 .",
    "thus , in the regime of fixed or bounded  @xmath8 , the sparsity pattern estimation in nonparametric regression is not more difficult than in the parametric linear regression , as far as only the consistency of estimation is considered and the precise value of the constant in ( [ eq9a ] ) is neglected .",
    "furthermore , there is a simple estimator @xmath174 of @xmath44 ( cf . equation ( 3 ) in @xcite ) , which is provably consistent under condition ( [ eq9a ] ) .",
    "this estimator can be seen as a procedure of testing hypotheses @xmath107 of form ( [ h0 ] ) with @xmath175 and , therefore , it does not really exploit the structure of the fourier coefficients of the regression function .",
    "to some extent , this is the reason why in the regime of growing intrinsic dimension @xmath17 , the estimator @xmath176 proposed by @xcite is no longer optimal .",
    "in fact , when @xmath17 , the term @xmath177 present in ( [ ordre2 ] ) tends to infinity as well . furthermore , as we show in section  [ sec4 ] , this convergence takes place at an exponential rate in @xmath8 . therefore , in this asymptotic set - up it is crucial to have the right order of @xmath177 in the condition that ensures the consistency . as shown in section  [ sec5 ] , this is the case for condition ( [ ordre2 ] ) .",
    "an apparent drawback of the estimator @xmath151 is the large dimensionality of tuning parameters involved in @xmath178 .",
    "however , theorem  [ prop2 ] reveals that for achieving good selection power , it is sufficient to select the @xmath179-dimensional tuning parameter @xmath180 on a one - dimensional curve parameterized by @xmath181 .",
    "indeed , once the value of @xmath182 is given , theorem  [ prop2 ] advocates for choosing @xmath183 \\\\[-8pt ] \\nonumber \\lambda_\\ell&=&\\frac { 2\\sqrt{a n(\\ell,\\vartheta)d^*\\log(2ed / d^*)}+2ad^*\\log(2ed / d^*)}{n}\\end{aligned}\\ ] ] for every @xmath149 . as discussed in section [ ssecadapt ] , this property allows us to relax the requirement that the values @xmath184 and @xmath168 involved in [ c1 ] are known in advance .",
    "the result of the last theorem is in some sense adaptive w.r.t .",
    "the unknown sparsity . indeed ,",
    "while the estimator @xmath178 involves @xmath8 , which is merely a known upper bound on the true sparsity @xmath146 and may be significantly larger than @xmath3 , it is the true sparsity @xmath3 that appears in condition ( [ ordre2 ] ) as a first argument of the quantity @xmath185 .",
    "this point is important given the exponential rate of divergence of @xmath186 when its first argument tends to infinity .",
    "on the other hand , if condition ( [ ordre2 ] ) is satisfied with @xmath187 instead of @xmath188 , then the consistent estimation of @xmath44 can be achieved by a slightly simpler procedure , @xmath189 the proof of this statement is similar to that of theorem  [ prop2 ] and will be omitted .",
    "the aim of the present section is to investigate the properties of the quantity @xmath190 that is involved in the conditions ensuring the consistency of the proposed procedures .",
    "quite surprisingly , the asymptotic behavior of @xmath190 turns out to be related to the jacobi @xmath41-function . to show this ,",
    "let us introduce some notation .",
    "for a positive number @xmath191 , we set @xmath192 along with @xmath193 and @xmath194 . in simple words , @xmath195 is the number of lattice points lying in the @xmath8-dimensional ball with radius @xmath196 and centered at the origin , while @xmath197 is the number of ( integer ) lattice points lying in the @xmath198-dimensional ball with radius @xmath199 and centered at the origin ; see figure  [ fig01 ] for an illustration . with this notation",
    ", the quantity @xmath200 of theorem  [ prop2 ] can be written as @xmath201 . by volumetric arguments",
    ", one can check that @xmath202 , where @xmath203 is the volume of the unit ball in @xmath204 .",
    "furthermore , similar bounds hold true for @xmath197 as well . unfortunately , when @xmath17 , these inequalities are not accurate enough to yield nontrivial results in the problem of variable selection we are dealing with .",
    "this is especially true for the results on impossibility of consistent estimation stated in section  [ sec5 ] .     in the three dimensional space ( @xmath205 ) .",
    "red points are those of @xmath206 while blue stars are those of @xmath207 . in this example , @xmath208 . ]    in order to determine the asymptotic behavior of @xmath195 and @xmath197 when @xmath8 tends to infinity , we will rely on their integral representation through jacobi s @xmath209-function .",
    "recall that the latter is given by @xmath210 , which is well defined for any complex number @xmath211 belonging to the unit ball @xmath212 . to briefly explain where the relation between @xmath213 and the @xmath41-function comes from ,",
    "let us denote by @xmath214 the sequence of coefficients of the power series of @xmath215 , that is , @xmath216 .",
    "one easily checks that @xmath217 , @xmath218 .",
    "thus , for every @xmath219 such that @xmath220 is integer , we have @xmath221 . as a consequence of cauchy s theorem",
    ", we get @xmath222 where the integral is taken over any circle @xmath223 with @xmath224 . exploiting this representation and applying the saddle - point method thoroughly described in  @xcite",
    ", we get the following result .",
    "[ prop1 ] let @xmath225 be an integer and let @xmath226 .",
    "there is a unique solution @xmath227 in @xmath228 to the equation @xmath229 .",
    "furthermore , the function @xmath230 is increasing and @xmath231 .    for @xmath28 , the following equivalences hold true : @xmath232 as @xmath8 tends to infinity .",
    "hereafter , it will be useful to note that the second part of proposition  [ prop1 ] yields @xmath233 \\\\[-8pt ] \\eqntext{\\mbox{as } d^*\\to\\infty,}\\end{aligned}\\ ] ] with @xmath234 .",
    "furthermore , while the asymptotic equivalences of proposition [ prop1 ] are established for integer values of @xmath225 , relation @xmath235 holds true for any positive real number @xmath219  @xcite . in order to get an idea of how the terms @xmath227 and @xmath236 depend on  @xmath219 , we depicted in figure [ fig1 ] the plots of these quantities as functions of @xmath225 .     and @xmath237 .",
    "one can observe that both functions are increasing , the first one converges to @xmath78 very rapidly , while the second one seems to diverge very slowly . ]    combining relation ( [ eqlog ] ) with theorem  [ prop2 ] , we get the following result .",
    "[ cor2 ] let condition be satisfied with some known constants @xmath162 and @xmath163 . consider the asymptotic set - up in which both @xmath238 and @xmath239 tend to infinity as @xmath240 .",
    "assume that @xmath0 grows at a sub - exponential rate in @xmath2 , that is , @xmath241 . if @xmath242 with @xmath243 , then consistent estimation of @xmath44 is possible and can be achieved , for instance , by the estimator @xmath178 .",
    "in this section , we focus our attention on the functional class @xmath244 of all functions satisfying assumption [ c1(@xmath245 ) ] . for emphasizing that @xmath44 is the sparsity pattern of the function @xmath43 , we write @xmath246 instead of @xmath44 .",
    "we assume that @xmath247 .",
    "the goal is to provide conditions under which the consistent estimation of the sparsity support is impossible , that is , there exists a constant @xmath248 and an integer @xmath249 such that , if @xmath250 , @xmath251 where the @xmath252 is over all possible estimators of @xmath246 . to this end",
    ", we introduce a set of @xmath253 probability distributions @xmath254 on @xmath255 and use the fact that @xmath256 these measures @xmath257 will be chosen in such a way that for each @xmath258 there is a set @xmath259 of cardinality @xmath8 such that @xmath260 and all the sets @xmath261 are distinct .",
    "the measure @xmath262 is the dirac measure in @xmath118 . considering these @xmath257s as `` priors '' on @xmath255 and defining the corresponding `` posteriors '' @xmath263 by @xmath264 we can write inequality ( [ minor1 ] ) as @xmath265 where the @xmath252 is taken over all random @xmath266 taking values in @xmath267 .",
    "the latter @xmath252 will be controlled using a suitable version of the fano lemma . to state it ,",
    "we denote by @xmath268 the kullback ",
    "leibler divergence between two probability measures  @xmath269 and @xmath270 defined on the same probability space .",
    "[ lemfano ] let @xmath271 be an integer , @xmath272 be a measurable space and let @xmath273 be probability measures on @xmath272 .",
    "let us set @xmath274 , where the @xmath252 is taken over all measurable functions @xmath275 .",
    "if for some @xmath276 , @xmath277 , then @xmath278 .",
    "we apply this lemma with @xmath279 being the set of all arrays @xmath280 such that for some @xmath281 the entries @xmath282 for every @xmath57 larger than @xmath283 in @xmath284-norm .",
    "it follows from fano s lemma that one can deduce a lower bound on @xmath285 , the quantity we are interested in , from an upper bound on the average kullback ",
    "leibler divergence between @xmath286 and @xmath287 .",
    "with these tools at hand , we are in a position to state the main result on the impossibility of consistent estimation of the sparsity pattern in the case when the conditions of theorem [ prop2 ] are violated .",
    "[ thm3 ] assume that @xmath288 and @xmath289 .",
    "let @xmath290 be the largest integer satisfying @xmath291 , where the jacobi @xmath41-function @xmath292 and @xmath293 are those defined in section  [ sec4 ] .    if for some @xmath294 , @xmath295 then , for @xmath8 large enough , @xmath296 .    if for some @xmath294 , @xmath297 then @xmath296",
    "it is worth stressing here that condition ( [ ordre3 ] ) is the converse of condition ( [ ordre2 ] ) of theorem  [ prop2 ] in the case @xmath17 , in the sense that condition ( [ ordre2 ] ) amounts to requiring that the left - hand side of ( [ ordre3 ] ) is smaller than some constant .",
    "there is , however , one difference between the quantities involved in these conditions : the term @xmath298 of ( [ ordre2 ] ) is replaced by @xmath299 in condition ( [ ordre3 ] ) .",
    "one can wonder how close @xmath300 is to @xmath182 . to give a qualitative answer to this question , we plotted in figure  [ fig2 ] the curve of the mapping @xmath301 along with the bisector @xmath302 .",
    "we observe that the difference between two curves is small compared to @xmath182 . as we discuss it later",
    ", this property shows that the constants involved in the necessary condition and in the sufficient condition for consistent estimation of @xmath44 are very close , especially for large values of @xmath182 .",
    "( blue ) and the bisector ( red ) . ]",
    "the estimator @xmath303 we have introduced in section  [ sec3 ] is clearly nonadaptive : the tuning parameters @xmath304 recommended by the developed theory involve the values @xmath184 and @xmath168 , which are generally unknown .",
    "fortunately , we can take advantage of the fact that the choice of @xmath305 and  @xmath306 is governed by the one - dimensional parameter @xmath181 . therefore , it is realistic to assume that a finite grid of values @xmath307 is available containing a true value of @xmath182 .",
    "the following result provides an adaptive procedure of variable selection with guaranteed control of the error .",
    "[ propadapt ] let @xmath307 and @xmath143 be given values , and set . ] @xmath308 for every @xmath309 , let us denote @xmath310 with @xmath311 and @xmath312 if the condition @xmath313 is fulfilled , then the estimator @xmath314 satisfies @xmath315 .    in simple words ,",
    "if the grid of possible values @xmath316 has a cardinality @xmath283 which is not too large [ i.e. , @xmath317 , then declaring a variable relevant if at least one of the procedures @xmath318 suggests its relevance provides a consistent and adaptive variable selection strategy .",
    "the proof of this statement follows immediately from proposition  [ prop0 ] and theorem  [ prop2 ] .",
    "indeed , applying proposition  [ prop0 ] with @xmath319 yields @xmath320 , while theorem  [ prop2 ] ensures that @xmath321 .",
    "since the methodology of section  [ sec3 ] takes its roots in the theory of hypotheses testing , one naturally wonders what are the minimax rates of separation in the problem of variable selection .",
    "the results stated in foregoing sections allow us to answer this question in the case of sobolev smoothness 1 and alternatives separated in @xmath322-norm .",
    "the following result , the proof of which is postponed to the appendix  [ appprop5 ] provides minimax rates .",
    "we assume herein that the true sparsity @xmath146 and its known upper estimate @xmath8 are such that @xmath323 is bounded from above by some constant .",
    "[ propmmx ] @xmath324there is a constant @xmath325 depending only on @xmath184 such that  if @xmath326 then there exists a consistent estimator of @xmath44 .",
    "furthermore , the consistency is uniform in @xmath327 . on the other hand",
    ", there is a constant @xmath328 depending only on @xmath184 such that if @xmath329 then uniformly consistent estimation of @xmath44 is impossible .",
    "borrowing the terminology of the theory of hypotheses testing , we say that @xmath330 is the minimax rate of separation in the problem of variable selection for sobolev smoothness one .",
    "these results readily extend to sobolev smoothness of any order @xmath331 , in which case the rate of separation takes the form @xmath332 .",
    "the first term in this maximum coincides , up to the logarithmic term , with the minimax rate of separation in the problem of detection of an @xmath3-dimensional signal @xcite . note , however , that in our case this logarithmic inflation is unavoidable .",
    "it is the price to pay for not knowing in advance which @xmath3 variables are relevant .",
    "so far , we have analyzed the situation in which noisy observations of the regression function @xmath333 are available at all points @xmath334^d$ ] .",
    "let us turn now to the more realistic model of nonparametric regression , when the observed noisy values of @xmath43 are sampled at random in the unit hypercube @xmath335^d$ ] .",
    "more precisely , we assume that @xmath2 independent and identically distributed pairs of input - output variables @xmath336 , @xmath337 are observed that obey the regression model @xmath338 the input variables @xmath339 are assumed to take values in @xmath59 while the output variables @xmath340 are scalar . as usual , @xmath341 are such that @xmath342=0 $ ] , @xmath337 ; additional conditions will be imposed later . without requiring from @xmath43 to be of a special parametric form",
    ", we aim at recovering the set @xmath68 of its relevant variables . the noise magnitude @xmath343 is assumed to be known .",
    "it is clear that the estimation of @xmath44 can not be accomplished without imposing some further assumptions on @xmath43 and on the distribution @xmath344 of the input variables . roughly speaking",
    ", we will assume that @xmath43 is differentiable with a squared integrable gradient and that @xmath344 admits a density which is bounded from below .",
    "more precisely , let @xmath345 denote the density of @xmath344 w.r.t . the lebesgue measure .",
    "@xmath346 for any @xmath347^d$ ] and that @xmath348 for any @xmath334^d$ ] .",
    "the next assumptions imposed to the regression function and to the noise require their boundedness in an appropriate sense .",
    "these assumptions are needed in order to prove , by means of a concentration inequality , the closeness of the empirical coefficients to the true ones .",
    "the @xmath349^d,\\rr , p_x)$ ] and @xmath61^d,\\rr , p_x)$ ] norms of the function @xmath43 are bounded from above , respectively , by @xmath350 and @xmath351 , that is , @xmath352 and @xmath353\\le l_2 ^ 2 $ ] .",
    "the noise variables satisfy a.e .",
    "e^{t^2/2}$ ] for all @xmath355 .",
    "we stress once again that the primary aim of this work is merely to understand when it is possible to consistently estimate the sparsity pattern .",
    "the estimator that we will define is intended to show the possibility of consistent estimation , rather than being a practical procedure for recovering the sparsity pattern .",
    "therefore , the estimator will be allowed to depend on the parameters @xmath356 , @xmath184 , @xmath168 and @xmath351 appearing in conditions [ c1][c3 ] .",
    "the estimator of the sparsity pattern @xmath44 that we are going to introduce now is based on the following simple observation : if @xmath357 , then @xmath65=0 $ ] for every @xmath57 such that @xmath358 .",
    "in contrast , if @xmath359 , then there exists @xmath66 with @xmath358 such that @xmath360|>0 $ ] . to turn this observation into an estimator of @xmath44 , we start by estimating the fourier coefficients @xmath65 $ ] by their empirical counterparts , @xmath361 then , for every @xmath362 and for any @xmath225 , we introduce the notation @xmath363 .",
    "the estimator of @xmath44 is defined by @xmath364 where @xmath128 and @xmath365 are some parameters to be defined later .",
    "the next result , the proof of which is placed in the supplementary material  @xcite , provides consistency guarantees for @xmath366 .",
    "[ thm1 ] let conditions be fulfilled with some known values @xmath367 , @xmath368 and @xmath351 .",
    "assume furthermore that the design density @xmath345 and an upper estimate on the noise magnitude @xmath343 are available .",
    "set @xmath369 and @xmath370 .",
    "if the following conditions are satisfied : @xmath371 \\\\[-8pt ] \\nonumber   \\frac{128({\\sigma}+l_2)^2d^*n(d^*,\\vartheta)\\log(24\\sqrt{\\vartheta } d / d^*)}{n\\gmin^2 } & < & \\kappa,\\end{aligned}\\ ] ] then the estimator @xmath372 satisfies @xmath373 .",
    "if we take a look at the conditions of theorem  [ thm1 ] ensuring the consistency of @xmath374 , it becomes clear that the strongest requirement is the second inequality in  ( [ cond3 ] ) .",
    "roughly speaking , this condition requires that @xmath375 is bounded from above by some constant .",
    "according to results stated in section  [ sec4 ] , @xmath187 diverges exponentially fast , making inequality ( [ cond3 ] ) impossible for @xmath8 larger than @xmath376 up to a multiplicative constant .",
    "it is also worth stressing that although we require the @xmath344-a.e .",
    "boundedness of  @xmath43 by some constant @xmath350 , this constant is not needed for computing the estimator proposed in theorem  [ thm1 ] .",
    "only constants related to some quadratic functionals of the sequence of fourier coefficients @xmath65 $ ] are involved in the tuning parameters @xmath128 and @xmath377 .",
    "this point might be important for designing practical estimators of @xmath44 , since the estimation of quadratic functionals is more realistic ( see , e.g. ,  @xcite ) than the estimation of @xmath378-norm .",
    "theorem  [ thm1 ] can be reformulated to characterize the level of relevance @xmath168 for the relevant components of @xmath379 making their identification possible .",
    "in fact , an alternative way of stating theorem  [ thm1 ] is the following : under conditions [ c1][c4 ] if @xmath182 is an arbitrary tuning parameter satisfying the first inequality in ( [ cond3 ] ) , then the estimator @xmath380with @xmath128 and @xmath377 chosen as in theorem  [ thm1]satisfies @xmath381 if the smallest level of relevance @xmath168 for components @xmath382 of @xmath379 with @xmath359 is not smaller than @xmath383 .",
    "this statement can be easily deduced from the proof of theorem [ thm1 ] ; cf .",
    "the supplementary material  @xcite .",
    "a natural question is now to check that the assumptions of theorem [ thm1 ] are tight in the asymptotic regimes of fixed sparsity and increasing ambient dimension , as well as increasing sparsity .",
    "we will only establish an analogue of claim ( ii ) of theorem [ thm3 ] .",
    "an attempt to prove a result similar to claim ( i ) of theorem  [ thm3 ] was done in  @xcite , theorem 2 .",
    "however , the result of  @xcite involves a stringent assumption on the empirical gram matrix ( cf .",
    "condition ( 6 ) in  @xcite ) and , unfortunately , we are unable to prove the existence of a sampling scheme for which this assumption is fulfilled .",
    "we assume that the errors @xmath384 are i.i.d .",
    "standard gaussian , and we focus our attention on the functional class @xmath255 .",
    "the following simple result shows that the conditions of theorem  [ thm1 ] are tight in the case of fixed intrinsic dimension .",
    "[ prop4 ] let the design @xmath385^d$ ] be either deterministic or random .",
    "if for some positive @xmath386 , the inequality @xmath387 = 0.19em plus 0.05em minus 0.02em holds true , then there is a constant @xmath388 such that .",
    "the results proved in previous sections almost exhaustively answer the questions on the existence of consistent estimators of the sparsity pattern in the model of gaussian white noise and , to a smaller extent , in nonparametric regression . in fact",
    "as far as only rates of convergence are of interest , the result obtained in theorem  [ prop2 ] is shown in section  [ sec5 ] to be unimprovable .",
    "thus only the problem of finding sharp constants remains open . to make these statements more precise ,",
    "let us consider the simplified set - up @xmath389 and define the following two regimes :    * the regime of fixed sparsity , that is , when the sample size @xmath2 and the ambient dimension @xmath0 tend to infinity but the intrinsic dimension @xmath8 remains constant or bounded .",
    "* the regime of increasing sparsity , that is , when the intrinsic dimension @xmath8 tends to infinity along with the sample size @xmath2 and the ambient dimension @xmath0 . for simplicity",
    ", we will assume that @xmath390 for some @xmath20 .    in the fixed sparsity regime , in view of theorems",
    "[ prop2 ] and [ thm1 ] , consistent estimation of the sparsity pattern can be achieved both in the gaussian white noise model and nonparametric regression as soon as @xmath391 , where @xmath392 is the constant defined by @xmath393 for the gaussian white noise model and @xmath394 for the regression model . on the other hand , by theorem  [ thm3 ] and proposition  [ prop4 ] , consistent estimation of the sparsity pattern is impossible if@xmath395 with @xmath396 .",
    "thus , up to multiplicative constants @xmath392 and @xmath397 ( which are clearly not sharp ) , the results of theorems  [ prop2 ] and  [ thm1 ] can not be improved in the regime of fixed sparsity .    in the regime of increasing sparsity ,",
    "the results we get in the model of gaussian white noise are much stronger than those for nonparametric regression . in the former model , taking the logarithm of both sides of inequality ( [ ordre2 ] ) and using formula  ( [ eqlog ] ) for @xmath398 , we see that consistent estimation of @xmath44 is possible when , for some @xmath143 and for all @xmath2 , the following two conditions are fulfilled : @xmath399 with some constants @xmath400 and @xmath401 .",
    "on the other hand , theorem  [ thm3 ] yields that there are some constants @xmath402 and @xmath403 such that it is impossible to consistently estimate @xmath44 if either one of the conditions @xmath404 is satisfied .",
    "first note that the left - hand side of the second condition in ( [ eq10 ] ) is exactly the same as the left - hand side of ( [ eq12b ] ) .",
    "if we compare now the left - hand side of the first condition in ( [ eq10 ] ) with the left - hand side of ( [ eq11 ] ) , we see that only the coefficients of @xmath8 differ . to measure the degree of difference of these two coefficients we draw in figure  [ fig4 ] the plots of the functions @xmath405 and @xmath406 , with @xmath407 as is theorem  [ thm3 ] .",
    "one can observe that the two curves are very close , especially for relatively large values of @xmath184 .",
    "this implies that the conditions in ( [ eq10 ] ) are tight .",
    "a  simple consequence of inequalities ( [ eq10 ] ) and ( [ eq11 ] ) is that the consistent recovery of the sparsity pattern is possible under the condition @xmath408 and impossible for @xmath409 as @xmath4 , provided that @xmath410 .",
    "( blue curve ) and @xmath406 ( red curve ) . ]    still in the regime of increasing sparsity , but for nonparametric regression , we proved that consistent estimation of the sparsity pattern is possible whenever @xmath411 with some constants @xmath412 and @xmath413 . as we have already mentioned , the second condition in ( [ eq10b ] ) is tight , up to the choice of @xmath414 , in view of proposition  [ prop4 ] .",
    "it is natural to expect that the first condition is tight as well , since it is in the model of gaussian white noise , which has the reputation of being simpler than the model of nonparametric regression .",
    "however , we do not have a mathematical proof of this statement .",
    "let us stress now that , all over this work , we have deliberately avoided any discussion on the computational aspects of the variable selection in nonparametric regression .",
    "the goal in this paper was to investigate the possibility of consistent recovery without paying attention to the complexity of the selection procedure .",
    "this lead to some conditions that could be considered a benchmark for assessing the properties of sparsity pattern estimators . as for the estimators proposed in section  [ sec3 ] ,",
    "it is worth noting that their computational complexity is not always prohibitively large .",
    "a recommended strategy is to compute the coefficients @xmath415 in a stepwise manner ; at each step @xmath416 only the coefficients @xmath415 with @xmath417 need to be computed and compared with the threshold .",
    "if some @xmath415 exceeds the threshold , then all the variables @xmath418 corresponding to nonzero coordinates of @xmath57 are considered as relevant .",
    "we can stop this computation as soon as the number of variables classified as relevant attains @xmath8 .",
    "while the worst - case complexity of this procedure is exponential , there are many functions @xmath43 for which the complexity of the procedure will be polynomial in @xmath0 .",
    "for example , this is the case for additive models in which @xmath419 for some univariate functions @xmath420 .",
    "note also that in the present study we focused exclusively on the consistency of variable selection without paying any attention to the consistency of regression function estimation .",
    "a thorough analysis of the latter problem being left to a future work , let us simply remark that in the case of fixed @xmath8 , under the conditions of theorem [ thm1 ] , it is straightforward to construct a consistent estimator of the regression function .",
    "in fact , it suffices to use a projection estimator with a properly chosen truncation parameter on the set of relevant variables .",
    "the situation is much more delicate in the case when the sparsity @xmath8 grows to infinity along with the sample size @xmath2 . presumably , condition ( [ eq10 ] ) is no longer sufficient for consistently estimating the regression function .",
    "the rationale behind this conjecture is that the minimax rate of convergence for estimating @xmath43 in our context , if we assume in addition that the set of relevant variables is known , is equal to @xmath421 .",
    "if the left - hand side of ( [ eq10 ] ) is equal to a constant and @xmath241 , then the aforementioned minimax rate does not tend to zero , making thus the estimator inconsistent .",
    "finally , we would like to mention that the selection of relevant variables is a challenging statistical task , which might be useful to perform independently of the task of regression function estimation .",
    "indeed , if we succeed in identifying relevant variables on a data - set having a small sample size , we can continue the data collection process more efficiently by recording only the values of relevant variables .",
    "this may considerably reduce the memory costs related to the data storage and the financial costs necessary for collecting new data . then",
    ", the regression function may be estimated more accurately on the base of this new ( larger ) data - set .",
    "to ease notation , we write @xmath151 instead of @xmath422 .",
    "it is clear that @xmath423 if and only if @xmath424 such that @xmath425 , where @xmath426 .",
    "for every @xmath427 , let us set @xmath428 and @xmath429 so that @xmath430 for @xmath431 , the first two terms of the last sum vanish and , therefore , we have @xmath432 where the last equality results from the fact that @xmath433 if @xmath434 . the random variable @xmath435 , being a centered sum of squares of independent standard gaussian random variables , follows a translated @xmath154-distribution .",
    "the tails of this distribution can be evaluated using the following result .",
    "[ lemmassart ] let @xmath436 be independent standard gaussian random variables .",
    "for every @xmath437 and for every vector @xmath438 , the following inequalities hold true : @xmath439    we apply this lemma to @xmath440 , for which @xmath441 and @xmath442 . setting @xmath443 and using the union bound ,",
    "we get @xmath444 one checks that @xmath445 holds true for every pair of integers @xmath446 such that @xmath447 ; cf .",
    "the supplementary material  @xcite for a proof .",
    "hence , for @xmath448 , we get @xmath449 .",
    "we begin with proving a stronger result that implies the claim of theorem  [ prop2 ] .",
    "[ propnew ] let @xmath450 be a real number from @xmath228 .",
    "if for every @xmath359 and for @xmath146 the inequality @xmath451^{1/2}+ \\biggl [ \\frac{2\\log(2s/\\alpha)}{n } \\biggr]^{1/2 } \\biggr\\}^2\\hspace*{-35pt}\\ ] ] holds true , then @xmath452 .    to bound from above the probability of type ii error , we rely on  the  equivalence : @xmath453 if and only if @xmath454 such that@xmath455 . recall that @xmath146 . using bonferroni s inequality ,",
    "we get @xmath456 \\\\[-8pt ] \\nonumber & \\le&\\sum_{j\\in j } \\pb",
    "\\bigl(\\widehat q_{m_{s},j}^j\\le\\lambda_s \\bigr ) \\le s \\max_{j\\in j}\\pb \\bigl(\\widehat",
    "q_{m_{s},j}^j\\le \\lambda_s \\bigr).\\end{aligned}\\ ] ] by virtue of decomposition ( [ notation1 ] ) , @xmath457 one checks that @xmath458 is a drawn from @xmath154-distribution  with @xmath459 degrees of freedom . therefore ,",
    "using lemma [ lemmassart ] stated in previous section , we get @xmath460 .",
    "therefore , @xmath461 is upper - bounded by @xmath462 using the condition of the proposition , we get @xmath463 . combining this inequality with ( [ bonf2 ] ) , we get the result of proposition  [ propnew ] .",
    "to deduce the claim of theorem  [ prop2 ] from that of proposition  [ propnew ] , we use the following lower bound : @xmath464 \\\\[-8pt ] \\nonumber & \\geq&\\kappa- m_s^{-2}\\sum_{j\\in\\supp(\\bk)\\subset j } \\theta_{\\bk } ^2\\| \\bk\\|_2 ^ 2 \\geq \\kappa- m_s^{-2 } ls\\end{aligned}\\ ] ] for every @xmath359 . our choice of @xmath465 , @xmath466 , ensures that @xmath467 .",
    "finally , using a very rough bound ( which is sufficient for our purposes ) , the right - hand side in ( [ qq1 ] ) can be upper - bounded by @xmath468 if @xmath450 is chosen to be equal to @xmath469 .",
    "therefore , if @xmath470 then ( [ qq1 ] ) holds true with @xmath471 and , therefore , the type ii error has a probability less than or equal to @xmath469 .",
    "_ proof of the first assertion_. this proof can be found in  @xcite ; we repeat here the arguments therein for the sake of keeping the paper self - contained . recall that @xmath195 admits an integral representation with the integrand @xmath472.\\ ] ] for any @xmath473",
    ", we define @xmath474 in such a way that @xmath475 by virtue of the cauchy ",
    "schwarz inequality , it holds that @xmath476 , @xmath477 , implying that @xmath478 for all @xmath479 , that is , @xmath480 is strictly decreasing .",
    "furthermore , @xmath480 is obviously continuous with @xmath481 and @xmath482 .",
    "these properties imply the existence and the uniqueness of @xmath483 such that @xmath484 . furthermore , as the inverse of a decreasing function , the function @xmath485 is decreasing as well .",
    "we set @xmath486 so that @xmath230 is increasing .",
    "we also have @xmath487    _ proof of the second assertion_. we apply the saddle - point method to the integral representing @xmath488 ; see , for example , chapter ix in  @xcite .",
    "it holds that @xmath489 \\\\[-8pt ] \\nonumber & = & \\frac{1}{2\\pi i}\\oint_{|z|=\\zb } \\bigl\\{z(1-z)\\bigr\\}^{-1}e^{d^*\\sfl_\\gamma(z)}{\\,dz}.\\end{aligned}\\ ] ] the first assertion of the proposition provided us with a real number @xmath227 such that @xmath490 and @xmath491 .",
    "the tangent to the steepest descent curve at @xmath227 is vertical .",
    "the path we choose for integration is the circle with center 0 and radius @xmath227 . as this circle and the steepest descent curve",
    "have the same tangent at @xmath227 , applying formula ( 1.8.1 ) of @xcite [ with @xmath492 since @xmath493 is real and positive ] , we get that @xmath494 when @xmath17 , as soon as the condition stands for the real part of the complex number @xmath495 . ]",
    "@xmath496\\leq-\\mu$ ] is satisfied for some @xmath497 and for any @xmath211 belonging to the circle @xmath498 and lying not too close to @xmath227 . to check that this is indeed the case , we remark that @xmath499=\\log |\\frac{\\sfh(z)}{z^\\gamma } |$ ] . hence , if @xmath500 with @xmath501 $ ] for some @xmath5020,\\pi[$ ]",
    ", then @xmath503 therefore @xmath504\\leq-\\mu$ ] with @xmath505 .",
    "this completes the proof for the term @xmath195 .",
    "the term @xmath197 can be dealt in the same way .",
    "to prove ( i ) we apply lemma  [ lemfano ] with @xmath506 in conjunction with a standard result , the proof of which can be found in  @xcite and in the supplementary material  @xcite .",
    "[ lem2 ] let @xmath507 be a subset of @xmath508 of cardinality @xmath509 and @xmath510 be a constant .",
    "define @xmath511 as a discrete measure supported on the finite set of functions @xmath512 such that @xmath513 for every @xmath514 .",
    "if we define the probability measure @xmath515 by @xmath516 , for every measurable set @xmath517 , and @xmath518 , then @xmath519 .    without loss of generality , we can assume @xmath520 ( the general case can be reduced to this one by replacing @xmath184 and @xmath2 , respectively , by @xmath521 and @xmath522 ) .",
    "thus , @xmath523 . we denote the set @xmath524 by @xmath67 and choose @xmath525 as follows : @xmath262 is the dirac measure @xmath526 , @xmath527 is defined as in lemma  [ lem2 ] with @xmath528 and @xmath529}^{-1/2}$ ] .",
    "the measures @xmath530 are defined similarly and correspond to the @xmath531 remaining sparsity patterns of cardinality @xmath8 .    in view of inequality ( [ minor2 ] ) and",
    "lemma  [ lemfano ] , it suffices to show that the measures @xmath257 satisfy @xmath532 and @xmath533 . combining lemma  [ lem2 ] with @xmath534 and inequality ( [ ordre3 ] )",
    ", we get @xmath535 . now , let us show that@xmath536 . by symmetry , this will imply that @xmath532 for every @xmath94 . since @xmath527 is supported by the set @xmath537 ,",
    "it is clear that @xmath538=a^2[n_1(d^*,\\gamma_l)-n_2(d^*,\\gamma_l)]=1 $ ] and @xmath539&=&\\sum _ { \\bk \\in \\mathcal c_1(d^*,\\gamma_l ) } k_j^2 a^2= \\frac1{d^ * } \\sum_{j=1}^{d^*}\\sum _ { \\bk\\in\\mathcal c_1(d^*,\\gamma_l ) } k_j^2 a^2\\leq a^2 \\gamma_l n_1\\bigl(d^ * , \\gamma_l\\bigr ) \\\\ &",
    "\\le&\\gamma_l \\frac{n_1(d^*,\\gamma_l ) } { n(d^*,\\gamma_l ) } , \\qquad   j=1,\\ldots , d^ * .\\end{aligned}\\ ] ] the results stated in section  [ sec4 ] imply that @xmath540 .",
    "our choice of @xmath541 ensures that , for @xmath8 large enough , @xmath542 .",
    "this completes the proof of claim ( i ) . to prove ( ii ) , we still use lemma  [ lemfano ] with @xmath543 and @xmath544 , where for every @xmath545 , @xmath546 is chosen as follows .",
    "let @xmath547 be all the subsets of @xmath45 containing exactly @xmath8 elements .",
    "we define @xmath546 , for @xmath548 , by its fourier coefficients @xmath549 as follows : @xmath550 obviously , all the functions @xmath546 belong to @xmath551 and , moreover , each @xmath552 has @xmath553 as sparsity pattern . one easily checks that our choice of @xmath552 implies @xmath554 . therefore , if @xmath555 , the desired inequality is satisfied .",
    "to conclude , it suffices to note that @xmath556 .",
    "in view of theorem  [ prop2 ] , applied with @xmath319 and @xmath557 , the consistent [ uniformly in @xmath327 ] estimation of @xmath44 is possible if @xmath559 since @xmath323 is upper - bounded by some constant , there is a constant @xmath560 such that the left - hand side of the last display is upper - bounded by @xmath561 as proved in lemma  [ lemcard0 ] below , @xmath562 .",
    "thus there is a constant @xmath563 such that @xmath564 combining these results , we see that under the conditions @xmath565 and @xmath566 consistent estimation of @xmath44 is possible . taking @xmath567 , we complete the proof of the first claim of the proposition .",
    "to prove the second assertion , we apply theorem  [ thm3 ] . since it holds that @xmath568 , we deduce from theorem  [ thm3 ] that there are some constants @xmath569 and @xmath570 such that if @xmath571 then consistent estimation of @xmath44 is impossible . since the @xmath3-dimensional @xmath351 ball with radius @xmath572 contains the @xmath350 ball of radius @xmath573 , @xmath574 for some constant @xmath575 . by rearranging different terms",
    ", we get the desired result .",
    "[ lemcard0 ] for every @xmath576 and @xmath577 , @xmath578 .",
    "one readily checks that if @xmath579 , then the hypercube centered at @xmath57 with side of length @xmath78 is included in the ball centered at the origin and having radius @xmath580 .",
    "therefore , @xmath581 $ ] , where @xmath582 $ ] stands for the volume of the unit ball in @xmath204 . using the well - known formula for the latter and the stirling approximation , for every @xmath583",
    ", we get @xmath584= \\frac{2\\pi^{d^*/2}}{d^*\\gamma(d^*/2 ) } \\le0.4\\frac{(4\\pi e / d^*)^{d^*/2}}{\\sqrt{2d^*}}$ ] .",
    "this implies that @xmath585 and the result follows .",
    "the authors would like to thank the reviewers for very useful remarks ."
  ],
  "abstract_text": [
    "<S> we address the issue of variable selection in the regression model with very high ambient dimension , that is , when the number of variables is very large . </S>",
    "<S> the main focus is on the situation where the number of relevant variables , called intrinsic dimension , is much smaller than the ambient dimension  @xmath0 . without assuming any parametric form of the underlying regression function , we get tight conditions making it possible to consistently estimate the set of relevant variables . these conditions relate the intrinsic dimension to the ambient dimension and to the sample size . </S>",
    "<S> the procedure that is provably consistent under these tight conditions is based on comparing quadratic functionals of the empirical fourier coefficients with appropriately chosen threshold values .    </S>",
    "<S> the asymptotic analysis reveals the presence of two quite different re gimes . </S>",
    "<S> the first regime is when the intrinsic dimension is fixed . in this case </S>",
    "<S> the situation in nonparametric regression is the same as in linear regression , that is , consistent variable selection is possible if and only if @xmath1 is small compared to the sample size @xmath2 . </S>",
    "<S> the picture is different in the second regime , that is , when the number of relevant variables denoted by @xmath3 tends to infinity as @xmath4 . </S>",
    "<S> then we prove that consistent variable selection in nonparametric set - up is possible only if @xmath5 is small compared to @xmath6 . </S>",
    "<S> we apply these results to derive minimax separation rates for the problem of variable selection . </S>"
  ]
}