{
  "article_text": [
    "a large variety of applied fields collect and need to recover information from high - dimensional data . among",
    "these we can cite communications and signal theory ( functional magnetic resonance imaging , spectroscopic imaging ) , econometrics , climate studies , biology ( gene expression micro - array ) and finance ( portfolio allocation ) .",
    "testing large covariance matrix is an important problem and has recently been approached via several techniques : corrected likelihood ratio test using the theory of large random matrices , methods based on the sample covariance matrix and so on .",
    "let @xmath14 , be @xmath1 independent and identically distributed @xmath0-vectors following a multivariate normal distribution @xmath15 , where @xmath16 _ { 1\\leq i , j \\leq p}$ ] is the normalized covariance matrix , with @xmath17 , for all @xmath18 to @xmath0 .",
    "let us denote by @xmath19 for all @xmath20 . in this paper",
    "we also assume that the size @xmath0 of the vectors grows to infinity as well as the sample size @xmath1 , @xmath21 and @xmath22 .",
    "we consider the following goodness - of - fit test , where we test the null hypothesis @xmath23 against the composite alternative hypothesis @xmath24 for any covariance matrix @xmath25 , we recall that the frobenius norm is computed as @xmath26= 2 \\sum_{1 \\leq i < j \\leq p}\\sigma_{ij}^2.\\ ] ] the class of matrices @xmath27 is defined as follows , for @xmath28 , @xmath29 in order to test @xmath30 , for some given non negative definite covariance matrix @xmath31 , we suggest rescaling the data @xmath32 and then apply the same test procedure provided that @xmath33 belongs to @xmath34 .",
    "let us denote by @xmath35 where @xmath36 is related to @xmath1 and @xmath0 , but also to @xmath12 and @xmath37 assumed fixed .",
    "the set of covariance matrices under the alternative hypothesis consists of matrices of size @xmath38 , whose elements decrease polynomially when moving away from the diagonal .",
    "this assumption is natural for covariances matrices and has been considered for estimation problems , see e.g @xcite , @xcite .",
    "regularization techniques , originally used for nonparametric estimation of functions , were successfully employed to the estimation of large covariance matrices . among these works ,",
    "let us mention minimax and adaptive minimax results : via banding the covariance matrix @xcite , thresholding the entries of the empirical covariance matrix @xcite , block - thresholding @xcite , tapering @xcite , @xmath39-estimation @xcite and so on .",
    "unlike the estimation of the covariance matrix , there are very few works for testing in a minimax setup in the existing literature .",
    "several types of test statistics have been proposed in the literature in order to test the null hypothesis .",
    "the likelihood ratio ( lr ) statistic , was first designed for fixed @xmath0 and @xmath40 . to treat the high dimensional case when @xmath41 , @xcite proposed a correction to the lr statistic and showed its convergence in law under the null hypothesis , as soon as @xmath42 , for some fixed @xmath43 .",
    "indeed , this correction is based on the asymptotic behavior of the spectrum of the covariance matrix .",
    "another approach is based on the largest magnitude of the off - diagonal entries of the sample correlation matrix and was introduced by @xcite .",
    "later , @xcite and @xcite show an original limit behavior of gumbel type for a self - normalized version of the maximum deviation of the sample covariance matrix .",
    "we also note that a non - asymptotic sphericity test for gaussian vectors was studied by @xcite .",
    "the alternative is given by a model with rank - one and sparse additive perturbation in the variance .",
    "furthermore , an approach based on the quadratic form @xmath44 , where @xmath45 is the sample covariance matrix , was proposed by @xcite , to test .",
    "later , @xcite shows that the test of @xmath46 based on @xmath47 is not consistent for large @xmath0 .",
    "they introduce a corrected version of @xmath47 and study its asymptotic behavior when @xmath48 and @xmath49 . in order to deal with non gaussian random vectors , and without specifying any relation between @xmath1 and @xmath0",
    ", @xcite proposed a u - statistic of order 2 , as a new correction of the previous quadratic form .",
    "they do moment assumptions in order to show the asymptotic behavior of their u - statistic , under the null and under a fixed alternative hypothesis .",
    "motivated by their work , @xcite used the u - statistic given in @xcite to test from a sample of gaussian vectors , and studied the testing problem from a minimax point of view .",
    "they consider the alternative hypothesis @xmath50 and they establish the minimax rates of order @xmath51 in this case . in our setup the restriction to the ellipsoid @xmath34 leads to different rates for testing .    in this paper , we introduce a u - statistic , which is weighted in an optimal way for our problem .",
    "this can also be seen as a regularization technique for estimating a quadratic functional , as it is often the case in minimax nonparametric test theory ( see @xcite ) .",
    "we use this test statistic to construct an asymptotically minimax test procedure .",
    "let us stress the fact that we study the type ii error probability uniformly over the set of all matrices @xmath52 under the alternative and that induces a separation rate saying how close @xmath52 can be to the identity matrix @xmath53 and still be distinguishable from @xmath53 .",
    "we describe the sharp separation rates for fixed unknown @xmath12 and give an adaptive procedure free of @xmath12 that allows to test at the price of a logarithmic loss in the rate .",
    "we describe here the rate asymptotics of the error probabilities from the minimax point of view .",
    "we recall that a test procedure @xmath54 is a measurable function with respect to the observations , taking values in @xmath55 $ ] .",
    "set @xmath56 its type i error probability , @xmath57 its maximal type ii error probability over the set @xmath58 , and by @xmath59 the total error probability of @xmath54 .",
    "let us denote by @xmath60 the minimax total error probability over @xmath61 which is defined by @xmath62 where the infimum is taken over all test procedures .",
    "we want to describe the separation rate @xmath63 such that , on the one hand , @xmath64 in this case we say that we can not distinguish between the two hypotheses . on the other hand",
    ", we exhibit an explicit test procedure @xmath65 such that its total error probability tends to @xmath66 @xmath67 we say that @xmath65 is asymptotically minimax consistent test and @xmath68 is the asymptotically minimax separation rate .    in this paper , we find asymptotically minimax rates for testing over the class @xmath69 .",
    "the minimax consistent test procedure is based on a u - statistic of second order , weighted in an optimal way . in this , our procedure is very different from known corrected procedures based on quadratic forms of the sample covariance matrix , see e.g. @xcite .",
    "this is the first time a weighted test - statistic is used for testing covariance matrices .",
    "moreover , our rates are sharp minimax .",
    "we show a gaussian asymptotic behaviour of the test statistic in the neighbourhood of the separation rate . we get the following sharp asymptotic expression for the maximal type ii probability error , under some assumptions relating @xmath70 , @xmath1 and @xmath0 , @xmath71 where @xmath72 denotes",
    "the cumulative distribution function ( cdf ) of the standard gaussian distribution and @xmath73 is the @xmath74 quantile of the standard gaussian distribution for any @xmath75 .",
    "we deduce that the sharp minimax total error probability is of the type @xmath76 where @xmath77 as @xmath78 , @xmath79 is explicitly given .",
    "it is usual to call the asymptotically sharp minimax rate @xmath80 corresponding to @xmath81 and to the asymptotic testing constant @xmath79 .",
    "analogous results were obtained by @xcite in the particular case where the covariance matrix is toeplitz , that is @xmath82 for all different @xmath83 and @xmath84 from 1 to @xmath0 .",
    "we note a gain of a factor @xmath0 in the minimax rate .",
    "the asymptotically sharp minimax rate for toeplitz covariance matrices is @xmath85 this additional factor @xmath0 can be heuristically explained by the number of parameters @xmath86 for a toeplitz matrix , instead of @xmath87 for an arbitrary covariance matrix . for @xmath88",
    "the test problem for toeplitz covariance matrices was solved in the sharp asymptotic framework , as @xmath21 , by @xcite .",
    "let us also recall that the adaptive rates ( to @xmath12 ) for minimax testing are obtained for the spectral density problem by @xcite by a non constructive method using the asymptotic equivalence with a gaussian white noise model .",
    "we also give an adaptive procedure for testing without prior knowledge on @xmath12 , for @xmath12 belonging to a closed subset of @xmath89 .",
    "important generalizations of this problem include testing in a minimax setup of composite null hypotheses like sphericity , @xmath90 , for unknown @xmath91 in some compact set separated from 0 , or bandedness , @xmath92 such that @xmath93_{ij } = 0 $ ] for all @xmath94 with @xmath95 .",
    "our proofs rely on the gaussian distribution of gaussian vectors .",
    "generalizations to non gaussian distributions with finite moments of some order can be proposed under additional assumptions on the behaviour of higher order moments , like e.g. @xcite .",
    "section  [ sec : test ] introduces the test statistic and studies its asymptotic properties .",
    "next we give upper bounds for the maximal type ii error probability and for the total error probability and refine these results to sharp asymptotics under the condition that @xmath96 . in section  [ simu ]",
    "we implement our test procedure and estimate its power . in section  [ sec : lowerbounds ] we prove sharp asymptotic optimality and deduce the optimality of the minimax separation rates for all @xmath10 and as soon as @xmath97 . in section",
    "[ sec : inverse ] we present the rate minimax ressults for testing the inverse of the covariance matrix . in section [ adaptivity ]",
    "we define an adaptive test procedure and show that the price of adaptation is a loss of @xmath98 in the separation rate .",
    "proofs are given in section  [ sec : proofs ] and in the appendix .",
    "in the minimax theory of tests developed since @xcite it is well understood that optimal test statistics are estimators ( suitably normalized and tuned ) of the functional which defines the separation of an element in the alternative from the element of the null hypothesis . in our case this is the frobenius norm @xmath99 $ ] .",
    "weighting the elements of the sample covariance matrix appeared first as hard thresholding in minimax estimation of large covariance matrices .",
    "let us mention @xcite for banding i.e. truncation of the matrix to its @xmath100 first diagonals ( closest to the main diagonal ) , @xcite for hard thresholding , then @xcite where tapering was studied .",
    "it is a natural idea when coming from minimax nonparametric estimation .",
    "however , that was never used for tests concerning large covariance matrices . in this section ,",
    "we introduce a weighted u - statistic of order 2 for testing large covariance matrices , study its asymptotic properties and give asymptotic upper bounds for the minimax rates of testing .    from now on asymptotics and symbols @xmath101 , @xmath102 , @xmath103 and @xmath104",
    "are considered @xmath1 and @xmath0 tend to infinity .",
    "recall that , given sequences of real numbers @xmath105 and real positive numbers @xmath106 , we say that they are asymptotically equivalent , @xmath107 , if @xmath108 .",
    "moreover , we say that the sequences are asymptotically of the same order , @xmath109 , if there exist two constants @xmath110 such that @xmath111 and @xmath112 .",
    "our test statistic is a weighted u - statistic of order 2 .",
    "it can be also seen as a weighted functional of the sample covariance matrix .",
    "the weights @xmath113 are constant on each diagonal ( they depend on @xmath83 and @xmath84 only through @xmath114 ) , non - zero only for @xmath115 for some large integer @xmath4 and decreasing polynomially for elements further from the main diagonal ( as @xmath116 is increasing ) . more precisely , we consider the following test statistic : @xmath117 where @xmath118 with @xmath119    the weights @xmath120 and the parameters @xmath121 are obtained by solving the following optimization problem : @xmath122 indeed our test statistic @xmath123 will concentrate asymptotically around the value @xmath124 which is @xmath66 for @xmath125 .",
    "the minimax paradigm considers the worst parameter @xmath126 in the class @xmath58 , that will give the smallest value @xmath127 and then finds the parameters @xmath128 of the test statistic to provide the largest value @xmath129 .",
    "such procedure performs uniformly well over all parameters @xmath130 .",
    "that explains why we solve the optimization problem .",
    "note that the weights in ( [ weights ] ) have further properties : @xmath131    the following proposition gives the moments of @xmath132 under the null and their bounds under the alternative hypothesis , respectively , as well as the asymptotic normality under the null hypothesis .",
    "[ prop : espvar ] the test statistic @xmath132 defined by ( [ est ] ) with parameters given by ( [ weights ] ) and ( [ constants ] ) has the following moments , under the null hypothesis : @xmath133 also we have that @xmath134 moreover , under the alternative , if we assume that @xmath135 and @xmath2 , we have , for all @xmath52 in @xmath58 : @xmath136 where @xmath137    note that , under the alternative , we have the additional assumption that @xmath138 , when @xmath0 grows to infinity .",
    "this is natural in order to a have a meaningful weighted statistic .",
    "let us look closer at the optimization problem ( [ optprob ] ) : for given @xmath139 , @xmath140 is the least value that @xmath141 can take over @xmath52 in the alternative set of hypotheses .    under the alternative",
    ", we shall establish the asymptotic normality under additional conditions that the underlying covariance matrix is close to the border of @xmath69 .",
    "this will be sufficient to give upper bounds of the total error probability of gaussian type in next section .",
    "[ an ] the test statistic @xmath132 defined by ( [ est ] ) with parameters given by ( [ weights ] ) and ( [ constants ] ) , such that @xmath78 , @xmath142 and under the aditionnal assumption that @xmath143 , is asymptotically normal : @xmath144 for any @xmath52 in @xmath58 such that @xmath145 .      in order to distinguish between the two hypothesis @xmath46 and @xmath146 defined previously , we propose the following test procedure @xmath147 where @xmath148 is the estimator defined in ( [ est ] ) .",
    "the following theorem proves that the previously defined test procedure is minimax consistent if @xmath149 is conveniently chosen .",
    "[ theo : bornesupminimax ] the test procedure @xmath65 defined in ( [ test * ] ) with @xmath150 has the following properties :    type i error probability : if @xmath151 then @xmath152 .",
    "type ii error probability : if @xmath2 and if @xmath153 then , uniformly over @xmath149 such that @xmath154 , for some constant @xmath155 in @xmath156 , we have @xmath157 if @xmath149 verifies all previous assumptions , then @xmath158 is asymptotically minimax consistent : @xmath159    in the next theorem we give sharp upper bounds of error probabilities of gaussian type .",
    "the proof of this result explains the choice of the weights as solution of the optimization problem .",
    "moreover , we will see that the gaussian behavior is obtained near the separation rates .",
    "recall that @xmath72 is the cumulative distribution function ( cdf ) of standard gaussian random variable and , for any @xmath75 , @xmath73 is defined by @xmath160 .    [",
    "theo : bornesup ] the test procedure @xmath65 defined in ( [ test * ] ) with @xmath150 has the following properties :    type i error probability : we have @xmath161 .",
    "type ii error probability : if @xmath2 and if @xmath162 then , uniformly over @xmath149 , we have @xmath163    in particular , for @xmath164 such that @xmath165 we have @xmath166 and @xmath167 another important consequence of the previous theorem , is that the test procedure @xmath168 , with @xmath169 has total error probability @xmath170      we include two examples , to illustrate the numerical behavior of our test procedure .",
    "first , we test the null hypothesis @xmath171 against the alternative hypothesis defined by the symmetric positive matrices @xmath172 .",
    "we implement the test statistic @xmath148 defined in and for @xmath173 , and @xmath174 .",
    "we choose the threshold @xmath149 of the test empirically , under the null hypothesis @xmath175 , from 1000 repetead samples of size @xmath1 , such that the type i error probability is fixed at 0.05 .",
    "we use @xmath149 to estimate the type ii error probability , also from 1000 repetitions and then plot the power as function of @xmath176 .",
    "figure [ plusieursp ] shows that the power is an increasing function of @xmath176 .",
    "also , we can see that for a fixed value of @xmath176 the power increases with @xmath0 .",
    "indeed , our procedure benefits from large values of @xmath0 , which is not a nuisance parameter here .",
    "second we consider the tridiagonal matrices @xmath177 , for @xmath178 $ ] under the alternative hypothesis .",
    "we compare our test procedure to the one given in @xcite , which is based on a @xmath179-statistic of order 2 , we denote it cm - test .",
    "moreover , the matrices @xmath180 are toeplitz , we also compare our the procedure to the one proposed in @xcite for toeplitz covariance matrices , that we denote by bz - test .",
    "the thresholds are evaluated empirically for each procedure at type i error probability smaller than 0.05 .",
    "we finally plot the powers curves of the three test procedures .",
    "figure [ comparaison ] shows that , when the alternative hypothesis consists of toeplitz matrices the bz - test has the better performance .",
    "however if we miss the information that the matrix is toeplitz , we see that the @xmath54-test is not bad and its power dominates the power of the cm - test .",
    "in this section , we first state the lower bound for testing , which , in addition to the test procedure exhibited in the previous section , shows that the asymptotically minimax separation rate is @xmath181 where the constant @xmath79 is given by ( [ constants ] ) .    [",
    "theo : borneinfo ] assume that , either @xmath182 , or @xmath183 and @xmath184 . if @xmath185 then @xmath186 where the infimum is taken over all test statistics @xmath54 .",
    "together with theorem  [ theo : bornesupminimax ] , the proof that @xmath68 is asymptotically minimax , under our assumptions , is complete .",
    "note that the condition @xmath187 is verified when @xmath188 for all @xmath1 and @xmath189 giving a general result in this case .",
    "when @xmath190 , the same condition holds for @xmath191 .",
    "this result is proven by showing that the @xmath192 distance between the null hypothesis and an averaged likelihood under the alternative ( that we explicitly construct ) tends to 0 .",
    "moreover , we give a sharp lower bound for the type ii error probability which is of gaussian type .",
    "[ theo : borneinf ] assume that @xmath10 and if @xmath193 @xmath194 where the infimum is taken over all test statistics @xmath54 with type i error probability less than or equal to @xmath195 .",
    "moreover , @xmath196    theorems [ theo : bornesup ] and [ theo : borneinf ] imply that for @xmath10 , the sharp separation rate for minimax testing is @xmath197 , under the additionnal assumptions ( [ conditionbornesup ] ) and ( [ conditionborneinf ] ) .",
    "note that a sufficient condition is that the separation rate verifies these assumptions , in particular @xmath198 holds if @xmath96 , and @xmath199 holds if @xmath200 .",
    "note that , there is a more general test procedure independent of @xmath70 , for which it is possible to derive the upper bounds as in theorems [ theo : bornesupminimax ] and [ theo : bornesup ] .",
    "it suffices to use the test statistic @xmath201 with the weights @xmath202 replaced by the weights @xmath203 defined as in and for @xmath70 replaced by @xmath68 .",
    "for more details see section 4.2 in @xcite .",
    "the proof of the lower bounds is given in section  [ sec : proofs ] .",
    "we construct a family of @xmath1 large centered gaussian vectors with covariance matrices based on @xmath204 given by the optimization problem and a prior measure @xmath205 on these covariance matrices .",
    "the logarithm of the likelihood ratio associated to an arbitrary @xmath52 with respect to @xmath53 under the null hypothesis is known to drift away to infinity ( see @xcite , who corrected this ratio to get a proper limit )",
    ". however , we show that the logarithm of the bayesian likelihood ratio with our prior measure @xmath205 verifies @xmath206 where @xmath207 , @xmath208 is asymptotically distributed as a standard gaussian distribution and @xmath209 is a random variable which converges to zero under @xmath210 probability .",
    "let us consider the same model , but the following test problem @xmath211 against the alternative @xmath212 where @xmath213 is the class of covariance matrices @xmath52 in @xmath34 with the additional constraint that the eigenvalues @xmath214 are bounded from below by some @xmath215 for all @xmath83 from 1 to @xmath0 and all @xmath52 in the set .",
    "we prove here that previous results apply to this setup and we get the same rates , but not the sharp asymptotics .",
    "note that , the additional hypothesis is mild enough so that it does not change the rates for testing .",
    "indeed , we see this case as a well - posed inverse problem .",
    "the cases of ill - posed inverse problem where the smallest eigenvalue can be allowed to tend to 0 will most certainly imply a loss in the rate and is beyond the scope of this paper .",
    "suppose @xmath188 , @xmath3 and @xmath215 .",
    "if @xmath1 and @xmath0 tend to infinity , such that @xmath216 , then @xmath217 defined in ( [ phitilde ] ) is the asymptotically minimax rate for the previous test .",
    "note that @xmath218 if and only if @xmath219 .",
    "moreover , if @xmath52 belongs to @xmath213 such that @xmath220 , then @xmath52 obviously belongs to @xmath34 and is such that @xmath221 thus we can proceed with our former test procedure , with @xmath70 replaced by @xmath222 and we obtain the upper bounds in the definition of the separation rates .",
    "the lower bounds in the previous section will also remain valid .",
    "indeed , this proof is based on the construction of a subfamily @xmath223 on the set of alternatives .",
    "we have proven in proposition  [ cor1 ] , that @xmath224 and we have @xmath10 and @xmath225 as @xmath226 and therefore , @xmath227 for @xmath228 small enough .",
    "thus , this family belongs to the set of alternatives we consider here , as well .",
    "moreover , proposition  [ cor1 ] proves also that @xmath229 for some fixed @xmath230 free of @xmath12 and @xmath37 .",
    "thus , @xmath231 thus we proceed the same way with @xmath70 replaced by @xmath232 .",
    "we want to built a test procedure of @xmath46 in which is free of the parameter @xmath12 belonging to some closed interval @xmath233 \\subset ( 1 , + \\infty)$ ] .",
    "the radius @xmath37 plays a minor role in the procedure and we suppose that it is known ( w.l.o.g we assume that @xmath234 ) .",
    "such a procedure is called adaptive and it solves the test problem @xmath46 in against a much larger set of alternative hypotheses : @xmath235 where @xmath236 is a large enough positive constant and @xmath237 depend on @xmath1 and @xmath0 , but also on @xmath12 . in order to construct the adaptive test procedure , we define a finite regular grid over the set @xmath238 $ ] : @xmath239 to each @xmath240 , we associate the weights : @xmath241 where the parameters @xmath242 and @xmath243 are given in and with @xmath12 replaced by @xmath244 and @xmath70 by @xmath245 .",
    "define the adaptive test procedure , for some constant @xmath246 large enough @xmath247 and where @xmath248 is the test statistic in with weights @xmath249 .",
    "note that the test @xmath250 rejects the null hypothesis as soon as there exists at least on @xmath251 for which @xmath252 .",
    "[ theo : adaptivity ] assume that @xmath253 the test statistic defined in with @xmath254 large enough verifies : @xmath255 for all @xmath256 , where @xmath245 is given in and @xmath257 .",
    "the proof that the adaptive procedure we propose attains the above rate is given in section [ sec : proofs ] .",
    "by analogy to nonparametric testing of functions , we expect the loss @xmath258 to be optimal uniformly over the class in the alternative hypothesis .",
    "[ sectionproofs ]    the proof is based on the proposition [ prop : espvar ] and the asymptotic normality of the weighted test statistic @xmath259 in proposition  [ an ] .",
    "we get for the type i error probability of @xmath65 @xmath260    for the type ii error probability of @xmath65 , uniformly in @xmath25 over @xmath61 , we have @xmath261 for @xmath262 and @xmath263 .",
    "it implies that @xmath264 .",
    "therefore , we distinguish the cases where @xmath265 tends to infinity or is bounded .",
    "we use the fact that , under the alternative , @xmath266 .",
    "we bound from below as follows : @xmath267 then , it gives @xmath268 let us bound from above @xmath269 using ( [ t_1 ] ) : @xmath270 we have @xmath271 which proves that : @xmath272 which tends to @xmath66 provided that @xmath273 .",
    "we will see using ( [ t_2 ] ) that the term @xmath274 tends to 0 as well : @xmath275    & = & o(1 ) \\text { for all } \\alpha > 1/2 , \\text { as soon as } n \\ds\\sqrt{p } b(\\varphi ) \\to + \\infty .",
    "\\end{array}\\ ] ]    now , if @xmath70 is close to the separation rate : @xmath276 , we see that whenever @xmath277 tends to infinity , the bound is trivial ( @xmath278 ) .",
    "the nontrivial bound is obtained when @xmath52 under the alternative is close to the optimal matrix @xmath279 , in the sense that @xmath280 together with the fact that @xmath70 is close to the separation rate : @xmath143 .",
    "we apply proposition  [ an ] to get the asymptotic normality @xmath281 thus , @xmath282 at this point , choosing optimal weights translates into @xmath283 after solving the optimization problem in the appendix , which ends the proof of the theorem .",
    "the first step of the proof is to reduce the set of parameters to a convenient parametric family .",
    "let @xmath284_{1 \\leq i , j \\leq p}$ ] be the matrix which has 1 on the diagonal and off - diagonal entries @xmath285 where @xmath286 with @xmath287 and @xmath4 are given by ( [ weights ] ) and ( [ constants ] ) .",
    "let us define @xmath288 a subset of @xmath289 as follows @xmath290_{ij } = i(i = j)+u_{ij}\\sigma_{ij}^*i(i \\neq j ) \\text { for all } 1 \\leq i , j \\leq p~ ,   ~u = [ u_{ij}]_{1 \\leq i , \\ , j\\leq p } \\in \\mathcal{u \\ } } , \\ ] ] where @xmath291_{1 \\leq i , \\ ,",
    "j\\leq p } :   u_{ii } = 0 , \\forall i \\mbox { and } \\ , u_{ij } = u_{j\\ , i}= \\pm 1 \\cdot i(|i - j|\\leq t ) , \\mbox { for } i\\neq j \\}.\\ ] ] the cardinality of @xmath292 is @xmath293 .",
    "[ cor1 ] for @xmath294 , the symmetric matrix @xmath295_{1 \\leq i , j \\leq p}$ ] , with @xmath296 , for all @xmath83 from 1 to @xmath0 , and @xmath297 defined in ( [ sigma * ] ) is non - negative definite , for @xmath298 small enough , and for all @xmath299 .",
    "moreover , denote by @xmath300 the eigenvalues of @xmath301 , then @xmath302 , for all @xmath83 from 1 to @xmath0 .",
    "we deduce that @xmath303 indeed , @xmath304 and @xmath305 has eigenvalues @xmath306 .",
    "proposition  [ cor1 ] shows that for all @xmath307 , @xmath308 is non - negative definite , for @xmath139 small enough .",
    "assume that @xmath309 under the null hypothesis and denote by @xmath210 the likelihood of these random variables .",
    "we assume that @xmath310 , under the alternative , and we denote @xmath311 the associated likelihood . in addition let @xmath312 be the average likelihood over @xmath288",
    ".    the problem can be reduced to the test @xmath313 against the averaged distribution @xmath314 , in the sense that @xmath315 and that @xmath316 it is , therefore , sufficient to show that , when @xmath317 , @xmath318 and that @xmath319 while , for @xmath320 , we need to show that @xmath321 in order to obtain ( [ beta ] ) and ( [ gamma ] ) , we apply results in section 4.3.1 of @xcite giving the sufficient condition that , in @xmath210 probability : @xmath322 where @xmath323 , @xmath324 , @xmath208 is asymptotically distributed as a standard gaussian distribution and @xmath209 is a random variable which converges to zero under @xmath210 probability . moreover , to show , it suffices to show that @xmath325 since @xmath326 we first begin by showing , in order to finish the proof of theorem [ theo : borneinfo ] .",
    "let , @xmath327 we have @xmath328 we define @xmath329 and note that @xmath330 . as the matrix @xmath331 is not necessarily symmetric",
    ", we write @xmath332 where @xmath333 is symmetric .",
    "moreover , we prove that for all @xmath334 and @xmath335 the eigenvalues of @xmath336 are in @xmath337 for all @xmath2 and @xmath70 small enough .",
    "indeed , by gershgorin s theorem , for each eigenvalue @xmath338 of @xmath336 there exists at least one @xmath339 such that @xmath340 we can show that @xmath341 and @xmath342 .",
    "thus , @xmath343 the taylor expansion for the logdet of a symmetric matrix writes @xmath344 in more details , @xmath345 & & - \\ds\\frac{1}{4}tr(\\delta_u \\delta^{2}_v \\delta^{2}_u \\delta_v ) - \\ds\\frac{1}{4 } tr(\\delta_v \\delta^{2}_u \\delta^{2}_v \\delta_u ) \\end{aligned}\\ ] ] recall that @xmath346 we have @xmath347 . for all @xmath348",
    ", we use the last inequality and the cauchy - schwarz inequality to get @xmath349 @xmath350 finally , using similar arguments we can show that @xmath351 thus , @xmath352 now we develop the terms on the right hand side of the previous equation .",
    "we obtain @xmath353 and @xmath354 now , we can write as follows : @xmath355 we explicit the expected value with respect to the i.i.d rademacher random variables @xmath356 , @xmath357 and @xmath358 pairwise distinct and independent : @xmath359 we use the inequality @xmath360 and get @xmath361 or , @xmath362 and since @xmath78 we have that @xmath363 and @xmath364 finally , @xmath365 as soon as @xmath366 and @xmath9 or @xmath367 and @xmath368 .    as consequence , if @xmath369 with the additional conditions on @xmath370 and @xmath0 given previously , we get @xmath371 which ends the proof of theorem  [ theo : borneinfo ] .",
    "now , we show ( [ lan ] ) in order to finish the proof of theorem [ theo : borneinf ] .",
    "more explicitly , @xmath372 where @xmath179 is seen as a randomly chosen matrix with uniform distribution over the set @xmath292 .",
    "let us denote @xmath373 and recall that proposition [ cor1 ] implies that @xmath374 for all @xmath294 .",
    "we write the following approximations obtained by matrix taylor expansion : @xmath375 note that , @xmath376 and that @xmath377 does not depend on @xmath179 .",
    "moreover , @xmath378 for all @xmath379 and when @xmath380 and @xmath381 . also ,",
    "for all @xmath382 @xmath383 in conclusion , we use @xmath384 for any sequence of random variables @xmath385 , to get @xmath386 we get @xmath387 from @xmath388 and 5 , we treat similarly the terms @xmath389 by , we have @xmath390 , similarly we obtain @xmath391 for @xmath392 and 5 . thus becomes : @xmath393 we have @xmath394 and we decompose @xmath395 note that @xmath396 if @xmath397 and @xmath398 . as for the last term : @xmath399 the first two terms in the decomposition of @xmath400 group with @xmath401 with extra factor @xmath402 , therefore we ignore these terms in further calculations .",
    "let us denote by @xmath403 , then @xmath404 @xmath405_{ij } w_{ij } = \\sum_{1 \\leq i \\ne j \\leq p } \\sum_{h \\notin \\ { i , j \\ } } u_{ih } u_{hj } \\sigma_{ih}^ * \\sigma_{hj}^ * w_{ij } + \\sum_{i=1}^p \\sum_{h \\ne i } \\sigma_{ih}^{*2 } w_{ii}\\end{aligned}\\ ] ] and @xmath406 .",
    "then , from we get @xmath407 now , we explicit the expected value with respect to the i.i.d rademacher random variables @xmath408 for all @xmath409 pairwise distinct .",
    "indeed , products of independent rademacher random variables are still rademacher and independent .",
    "thus , @xmath410 we shall use repeatedly the taylor expansion of @xmath411 as @xmath412 .",
    "indeed , @xmath413 and @xmath414 , giving that @xmath415 .",
    "thus @xmath416 similarly , using the first order taylor expansion , we get @xmath417 and for @xmath418 and 5 , @xmath419 recall now that @xmath420 , for all @xmath421 such that @xmath422 and @xmath423 . then , @xmath424 as soon as @xmath425 and @xmath426 . in conclusion ,",
    "as the convergence in @xmath427 implies convergence in @xmath210 probability , we get @xmath428 moreover , for @xmath429 and 5 , @xmath430 using and , gives @xmath431 we further decompose as follows : @xmath432 with our definition : @xmath433 , we can write @xmath434 and we put @xmath435 which has asymptotically standard gaussian under @xmath210 probability , by proposition  [ prop : espvar ] .    by proposition [ prop : wijproperty ] given in the appendix",
    ", we have @xmath436 , then @xmath437 moreover , @xmath438 we deduce that , @xmath439 remaining terms in ( [ last ] ) can be grouped as follows : @xmath440 since the random variable in the previous display is centered and @xmath441 which concludes the proof of ( [ lan ] ) .    the type i error probability tends to @xmath66 as a consequence of the berry - essen type inequality in lemma 1 in the appendix applied to the degenerate u - statistic @xmath442 .",
    "we have that , for some @xmath443 and any @xmath150 : @xmath444 we use the relation @xmath445 for all @xmath446 , to deduce that @xmath447    we use this previous result to show that the type i error probability tends to 0 . see that for all @xmath448 , @xmath449 , where @xmath450 .",
    "thus since @xmath451 , we obtain that @xmath452 for all @xmath453 recall that @xmath454 , therefore @xmath455 see that : @xmath456 moreover if @xmath457 , then @xmath458 , and if @xmath459 we obtain @xmath460    now , we move to the type ii error probability .",
    "let us consider @xmath461 such that @xmath462 for some @xmath463 .",
    "we defined @xmath464 as the smallest point on the grid such that @xmath465 .",
    "we denote by @xmath466 , @xmath467 , @xmath468 , @xmath469 and @xmath470 the test statistic , the threshold and the parameters depending on @xmath464 .",
    "also we define @xmath471 , @xmath472 and @xmath473 the constants define in for @xmath464 instead of @xmath12 and @xmath234 .",
    "we have @xmath474 and @xmath475 , for all @xmath476 .",
    "the type ii error probability is bounded from above as follows , @xmath477 $ ] and @xmath478 : @xmath479 first we have @xmath480 now we show that , since @xmath481 we have , @xmath482 moreover , use that @xmath483 , to obtain @xmath484 & = &    \\exp \\big\\ { \\frac{4 ( \\alpha - \\alpha_{r_0 } ) } { 4 \\alpha_{r_0 } + 1 } \\cdot \\ln ( ( n \\sqrt{p } ) / \\rho_{n , p } ) \\big\\ } \\nonumber \\\\ &   \\geq &     \\exp \\big\\ { - \\frac { 4 ( \\bar{\\alpha } - \\underline{\\alpha})}{4 \\underline{\\alpha } + 1 } ( 1 + o(1 ) ) \\big\\ } \\geq c ( \\underline{\\alpha } , \\bar{\\alpha } ) .\\nonumber\\end{aligned}\\ ] ] we deduce that , @xmath485 let us denote by @xmath486 and @xmath487 the right - hand side termes in ( [ t1 ] ) and ( [ t2 ] ) , respectively . then by markov inequality , for @xmath488 , we get @xmath489 & \\leq & \\ds\\frac{{\\text{var}}_\\sigma(\\widehat{\\mathcal{d}}_{n , r_0 } ) } { \\big ( \\mathbb{e}_{\\sigma}(\\widehat{\\mathcal{d}}_{n , r_0 } ) -    \\mathcal{c}^ *   t_{r_0 } \\big)^2 }   \\leq    \\ds\\frac{\\big ( \\mathcal{c } - \\ds\\frac{1 } { c ( \\underline{\\alpha } , \\bar{\\alpha } ) }   \\big)^2 { \\text{var}}_\\sigma(\\widehat{\\mathcal{d}}_{n , r_0 } ) } {   \\big ( \\mathcal{c } - \\ds\\frac{1 } { c ( \\underline{\\alpha } , \\bar{\\alpha } ) }   -   \\mathcal{c}^ * \\big)^2 \\mathbb{e}^2_{\\sigma}(\\widehat{\\mathcal{d}}_{n , r_0 } ) } \\\\[0.3 cm ] & \\leq & \\ds\\frac{\\big ( \\mathcal{c } - \\ds\\frac{1 } { c ( \\underline{\\alpha } , \\bar{\\alpha } ) }   \\big)^2 \\cdot ( \\mathcal{t}_1 + ( n-1)\\mathcal{t}_2)}{n(n-1)p^2     \\big ( \\mathcal{c } - \\ds\\frac{1 } { c ( \\underline{\\alpha } , \\bar{\\alpha } ) }   -   \\mathcal{c}^ * \\big)^2 \\mathbb{e}^2_{\\sigma}(\\widehat{\\mathcal{d}}_{n , r_0 } ) }   : =   f_1 + f_2.\\end{aligned}\\ ] ] we use to show that @xmath490 tends to zero .",
    "@xmath491 since @xmath492 for @xmath493 .",
    "similarly we use to show that @xmath494 thus we get , for @xmath488 , @xmath495 } \\sup\\limits _ { \\substack {    \\sigma \\in \\mathcal{f}(\\alpha , l ) \\ ,   ; \\\\   \\frac 1{2p } \\| \\sigma -i\\|_f^2   \\geq \\mathcal{c}^2 \\psi^2 _ { \\alpha } } } \\mathbb{p}_{\\sigma } ( \\delta^*_{ad } = 0 ) = o(1).\\ ] ]    99    z.  bai , d.  jiang , j .-",
    "f yao , and s.  zheng .",
    "corrections to lrt on large - dimensional covariance matrix by rmt . , 37(6b):38223840 , 2009 .",
    "q.  berthet and ph .",
    "optimal detection of sparse principal components in high dimension .",
    ", 41(4):17801815 , 2013 .",
    "peter  j. bickel and elizaveta levina .",
    "covariance regularization by thresholding .",
    ", 36(6):25772604 , 2008 .",
    "peter  j. bickel and elizaveta levina .",
    "regularized estimation of large covariance matrices .",
    ", 36(1):199227 , 2008 .    c.  butucea and g.  gayraud . . , jan 2013",
    ".    c.  butucea and r.  zgheib",
    "sharp minimax tests for large toeplitz covariance matrices with repeated observations . , 2015 .",
    "cristina butucea , catherine matias , and christophe pouet . adaptive goodness - of - fit testing from indirect observations .",
    ", 45(2):352372 , 05 2009 .",
    "t.  t. cai and t.  jiang . limiting laws of coherence of random matrices with applications to testing covariance structure and construction of compressed sensing matrices . , 39(3):14961525 , 2011 .",
    "t.  t. cai and z.  ma .",
    "optimal hypothesis testing for high dimensional covariance matrices .",
    ", 19(5b):23592388 , 11 2013 .    t.  t. cai , c .- h zhang , and h.  h. zhou . optimal rates of convergence for covariance matrix estimation . , 38(4):21182144 , 2010 .",
    "t.  t. cai and h.  h. zhou .",
    "minimax estimation of large covariance matrices under @xmath39-norm .",
    ", 22(4):13191349 , 2012 .",
    "t.  tony cai and ming yuan .",
    "adaptive covariance matrix estimation through block thresholding . , 40(4):20142042 , 2012 .",
    "s.  x. chen , l .- x .",
    "zhang , and p .- s .",
    "tests for high - dimensional covariance matrices .",
    ", 105(490):810819 , 2010 .",
    "m.  s. ermakov . a minimax test for hypotheses on a spectral density .",
    ", 68(4):475483 , 1994 .",
    "golubev , m.  nussbaum , and h.h .",
    "asymptotic equivalence of spectral density estimation and gaussian white noise .",
    ", 38:181214 , 2010 .",
    "p.  hall .",
    "central limit theorem for integrated square error of multivariate nonparametric density estimators .",
    ", 14(1):116 , 1984 .",
    "i. ingster .",
    "asymptotically minimax hypothesis testing for nonparametric alternatives .",
    "i. , 2:85114 , 171189 , 249268 , 1993 .",
    ".  i. ingster and i.  a. suslina . , volume 169 of _ lecture notes in statistics_. springer - verlag , new york , 2003 .",
    "tiefeng jiang .",
    "the asymptotic distributions of the largest entries of sample correlation matrices . , 14(2):865880 , 2004 .",
    "o.  ledoit and m.  wolf .",
    "some hypothesis tests for the covariance matrix when the dimension is large compared to the sample size .",
    ", 30(4):10811102 , 2002 .    h.  nagao . on some test criteria for covariance matrix .",
    ", 1(4):700709 , 07 1973 .",
    "h.  xiao and w.b .",
    "asymptotic theory for maximum deviations of sample covariance matrix estimation .",
    ", 123:28992920 , 2013 .",
    "we recall that under the null hypothesis the coordinates of the vector @xmath496 are independent , so using this fact we have :    @xmath497 for @xmath498 , @xmath499       & = \\ds\\frac{1}{p } \\underset{i < j } { \\ds\\sum_{i=1}^p \\sum_{j=1}^p } w^*_{ij}\\mathbb{e}(x_{1,i}x_{1,j})\\mathbb{e}(x_{2,i}x_{2,j } )       =   \\ds\\frac{1}{p } \\underset{i < j } { \\ds\\sum_{i=1}^p \\sum_{j=1}^p } w^*_{ij } \\sigma_{ij}^2 \\\\[0.5 cm ] \\end{array}\\ ] ] remark that @xmath500 can be written as the following form @xmath501 then the variance of the estimator @xmath132 is a sum of two uncorrelated terms @xmath502 now we will give an upper bound for the first term on the right - hand side of ( [ var2termes ] ) .",
    "denote by @xmath503 we shall distinguish three terms in the previous sum , that is @xmath504 where @xmath505 form a partition of the set@xmath506 .",
    "more precisely in @xmath507 we have @xmath508 or @xmath509 , in @xmath510 we have three different indices @xmath511 or @xmath512 or @xmath513 or @xmath514 and finally in @xmath515 the indices are pairewise distinct .",
    "first , when @xmath516 , we use that @xmath517 , to get @xmath518 and this is @xmath519 since @xmath520 when the indices are in @xmath510 , we have three indices out of four which are equal .",
    "we assume @xmath521 , therefore it is sufficient to check that , @xmath522 now let us bound from above the first term of @xmath523 , @xmath524 \\end{array}\\ ] ] again we will treat each term of @xmath525 separately .",
    "we recall that the weights @xmath202 verify the following properties @xmath526 in the rest of the proof we denote by @xmath527 different constants that dependent only on @xmath528 and/or on @xmath37 .",
    "we have for @xmath529 , @xmath530 for the second term in ( [ t_1,2,1 ] ) , where @xmath531 , we use the following bound : @xmath532 then we prove that , @xmath533 note that @xmath534 .",
    "the second term of @xmath535 , is bounded as follows : @xmath536 as a consequence of ( [ t_1,2,1,1 ] ) to ( [ t_1,2,2 ] ) , @xmath537 the last case , where @xmath538 vary in @xmath515 , the indices are pairwise distinct , @xmath539 as the two previous terms have the same upper bound , let us deal with the first one say @xmath540 .",
    "we should distinguish two cases , the first when @xmath541 and the second when @xmath542 .",
    "we begin by the first case , which in turn will be decomposed into three terms .",
    "first , @xmath543 then , @xmath544 finally , using cauchy - schwarz inequality , we have , @xmath545 now we suppose that we have @xmath546 , then , @xmath547 finally we obtain , from ( [ t_1,3,1,1 ] ) to ( [ t_1,3,2 ] ) : @xmath548 put together ( [ t_1,1 ] ) , ( [ t_1,2 ] ) and ( [ t_1,3 ] ) to obtain ( [ t_1 ] ) .",
    "let us give an upper bound for the second term of ( [ var2termes ] ) , @xmath549 proceeding similarly , we shall distinguish three kind of terms .",
    "let us begin by the case when the indices belong to @xmath507 , @xmath550 = 2 \\underset{i \\neq j}{\\ds\\sum_{i=1}^p \\ds\\sum_{j=1}^p } w^{*2}_{ij } \\sigma_{ij}^2 ( 1 + \\sigma_{ij}^2 ) \\nonumber \\\\ & \\leq & 4 ( \\sup\\limits_{i , j } w^*_{ij } ) \\underset{i \\neq j}{\\ds\\sum_{i=1}^p \\ds\\sum_{j=1}^p } w^{*}_{ij } \\sigma_{ij}^2 = 8 ( \\sup\\limits_{i , j } w^*_{ij } ) \\cdot p \\cdot \\mathbb{e}_{\\sigma}(\\widehat{\\mathcal{d}}_n ) =   o(1 ) \\cdot p \\cdot \\mathbb{e}_{\\sigma}(\\widehat{\\mathcal{d}}_n ) .",
    "\\label{t_{2,1}}\\end{aligned}\\ ] ] next , when @xmath551 , @xmath552 we bound from each term of @xmath553 separately .",
    "using cauchy - schwarz inequality two times we obtain , @xmath554 the second term in @xmath553 is @xmath555 and therefore , @xmath556    finally , when @xmath557 , we have to bound from above @xmath558 these last two terms , in @xmath559 , are treated similarly , so let us deal with : @xmath560 using the upper bound of @xmath561 obtained previously , we have          we use the decomposition ( [ decomp ] ) in the proof of the proposition  [ prop : espvar ] and we treat each term separately .",
    "recall that , by our assumptions , @xmath563 .",
    "use ( [ t_2 ] ) to get @xmath564 this tends to 0 , since @xmath565 , which is true for all @xmath2 .",
    "it follows that , for proving the asymptotic normality , it is sufficient to prove the asymptotic normality of @xmath566 we study @xmath567 centered , 1-degenerate u - statistic , with symmetric kernel @xmath568 defined as follows @xmath569 we apply theorem 1 of @xcite .",
    "therefore we check that @xmath570 and that @xmath571 where @xmath572 , for @xmath573 .",
    "we compute @xmath574 since @xmath575 , and from the inequality ( [ t_1 ] ) , we have @xmath576 in order to prove that @xmath577 , it is sufficient to show that @xmath578 in fact , @xmath579   & = & \\underset{1 \\leq i_1<j_1 \\leq p } { \\ds\\sum } ~ \\underset{1 \\leq i_1 ' < j_1 ' \\leq p } { \\ds\\sum }   ~ \\underset{1 \\leq i_2 < j_2 \\leq p } { \\ds\\sum } ~ \\underset{1 \\leq i'_2 < j'_2 \\leq p } { \\ds\\sum }    w^{*}_{i_1j_1 } w^{*}_{i'_1j'_1 } w^{*}_{i_2 j_2 } w^{*}_{i_2'j_2 ' } ( \\sigma_{i_1 i'_1 } \\sigma_{j_1 j'_1 } + \\sigma_{i'_1 j_1 } \\sigma_{i_1j'_1 } )   ( \\sigma_{i_2 i_2 ' } \\sigma_{j_2 j_2 ' } \\nonumber \\\\ & & + \\sigma_{i_2'j_2 } \\sigma_{i_2 j_2 ' } )   \\cdot \\mathbb{e}[(x_{1,i_1 } x_{1,j_1 } - \\sigma_{i_1j_1})(x_{1,i_2 } x_{1,j_2 } - \\sigma_{i_2j_2 } ) ]   \\mathbb{e}[(x_{2,i'_1 } x_{2,j'_1 } - \\sigma_{i'_1j'_1})(x_{2,i_2 ' } x_{2,j_2 ' } - \\sigma_{i_2'j_2 ' } ) ] \\nonumber \\\\[0.5 cm ] & = & \\underset{1 \\leq i_1 < j_1 \\leq p } { \\ds\\sum } ~ \\underset{1 \\leq i'_1 < j'_1 \\leq p } { \\ds\\sum }   ~ \\underset{1 \\leq i_2 < j_2 \\leq p } { \\ds\\sum } ~ \\underset{1 \\leq i'_2 < j'_2 \\leq p } { \\ds\\sum }    w^{*}_{i_1 j_1 } w^{*}_{i'_1j'_1 } w^{*}_{i_2 j_2 } w^{*}_{i_2'j_2 ' } ( \\sigma_{i_1i'_1 } \\sigma_{j_1j'_1 } + \\sigma_{i'_1j_1 } \\sigma_{i_1j'_1 } ) \\nonumber \\\\ & &   \\cdot   ( \\sigma_{i_2 i_2 ' } \\sigma_{j_2 j_2 ' } + \\sigma_{i_2'j_2 } \\sigma_{i_2 j_2 ' } )   ( \\sigma_{i_1i_2 } \\sigma_{j_2j_1 } + \\sigma_{i_1j_2 } \\sigma_{i_2j_1 } ) ( \\sigma_{i'_1i'_2 } \\sigma_{j_2'j'_1 } + \\sigma_{i'_1j_2 ' } \\sigma_{i_2'j'_1 } )   \\label{eg^2}\\end{aligned}\\ ] ] to bound from above ( [ eg^2 ] ) , we shall distinguish four cases .",
    "the first one is when all couples of indices are equal , @xmath580 the second one is when we have two different pairs of couples of indices , which can be obtained by two different combinations of the couples of indices .",
    "when we have equal pairs of couples of indices , as for example @xmath581 , @xmath582 and @xmath583 , we get @xmath584 & \\leq &   ( \\sup\\limits_{i_1,j_1}w_{i_1j_1}^{*^2 } ) \\cdot ( \\sup\\limits_{i_1,j_1}(1 + \\sigma_{i_1j_1}^2)^2 ) \\cdot \\underset{1 \\leq i_1 < j_1 \\leq p } { \\ds\\sum } ~ \\underset{1 \\leq i'_1 < j'_1 \\leq p } { \\ds\\sum }   w^{*}_{i_1j_1 } w^{*}_{i'_1j'_1 } ( \\sigma_{i_1i'_1 } \\sigma_{j_1j'_1 } + \\sigma_{i'_1j_1 } \\sigma_{i_1j'_1})^2 \\\\[0.5 cm ] & \\leq & 4 \\cdot   ( \\sup\\limits_{i_1,j_1}w_{i_1j_1}^{*^2 } )   \\cdot n^2   p \\cdot   \\mathbb{e}_{\\sigma}(h_n^2(x_1 , x_2 ) ) = 4 \\cdot   ( \\sup\\limits_{i_1,j_1}w_{i_1j_1}^{*^2 } )   \\cdot p = o(p^2 ) .",
    "\\end{array}\\ ] ] when we have three couples of indices equal , for example @xmath585 and @xmath586 , we get @xmath587 for the third case , there are three different couples of pairs of indices , for example , @xmath588 and @xmath589 . using cauchy - schwarz inequality",
    "several times we obtain , @xmath590 & \\leq & \\underset{1 \\leq i'_1 < j'_1 \\leq p } { \\ds\\sum }   ~ \\underset{1 \\leq i_2 < j_2 \\leq p } { \\ds\\sum } w^{*}_{i'_1j'_1 } w^{*2}_{i_2 j_2 } ( \\sigma_{i'_1i_2 } \\sigma_{j_2j'_1 } + \\sigma_{i'_1j_2 } \\sigma_{i_2j'_1 } ) ( 1 + \\sigma_{i_2,j_2}^2 ) \\\\   & & \\cdot \\big (   \\underset{1 \\leq i_1 < j_1 \\leq p } { \\ds\\sum }    w^{*}_{i_1j_1 } ( \\sigma_{i_1i'_1 } \\sigma_{j_1j'_1 } + \\sigma_{i'_1j_1 } \\sigma_{i_1j'_1})^2 \\big)^{1/2 }   \\big (   \\underset{1 \\leq i_1 < j_1 \\leq p } { \\ds\\sum }    w^{*}_{i_1j_1 }   ( \\sigma_{i_1i_2 } \\sigma_{j_2j_1 } + \\sigma_{i_1j_2 } \\sigma_{i_2j_1})^2 \\big)^{1/2 } \\\\[0.5 cm ] & \\leq &   \\underset{1 \\leq i_2 < j_2 \\leq p } { \\ds\\sum }   w^{*2}_{i_2 j_2 } ( 1 + \\sigma_{i_2,j_2 } ) ^2 \\big(\\underset{1 \\leq i'_1 < j'_1 \\leq p } { \\ds\\sum } w_{i'_1j'_1}^ *   ( \\sigma_{i'_1i_2 } \\sigma_{j_2j'_1 } + \\sigma_{i'_1j_2 } \\sigma_{i_2j'_1})^2\\big)^{1/2 } \\\\ & & \\cdot \\big(\\underset{1 \\leq i'_1 < j'_1 \\leq p } { \\ds\\sum }   \\underset{1 \\leq i_1 < j_1 \\leq p } { \\ds\\sum } w_{i'_1j'_1}^ *    w^{*}_{i_1j_1 }   ( \\sigma_{i_1i_1 ' } \\sigma_{j_1'j_1 } + \\sigma_{i_1j_1 ' } \\sigma_{i_1'j_1})^2 \\big)^{1/2 } \\\\ & & \\cdot    \\big (   \\underset{1 \\leq i_1 <",
    "j_1 \\leq p } { \\ds\\sum }    w^{*}_{i_1j_1 }   ( \\sigma_{i_1i_2 } \\sigma_{j_2j_1 } + \\sigma_{i_1j_2 } \\sigma_{i_2j_1})^2 \\big)^{1/2 } .\\\\[0.5 cm ] \\end{array}\\ ] ] moreover , we recognize in these bounds @xmath591 which is @xmath592 .",
    "thus , @xmath593 now we will treat the last case , when the pairs of indices are pairwise distinct , in this case , we have 16 terms to handle .",
    "as all terms are treated the same way , let us deal with : @xmath594 in order to find an upper bound for @xmath595 , we decompose the previous sums , into several sums , similarly to the upper bound of ( @xmath596 .",
    "that is @xmath597 , where @xmath598 , form a partition of the set @xmath599 .",
    "let us define , @xmath600 @xmath601 and so on , for all @xmath602 .",
    "to bound from above the sum over @xmath603 , we partition again @xmath603 , @xmath604 such that , @xmath605 and so on , until we get the partition of @xmath603 .",
    "again , by our assumption that @xmath607 , we can see that : @xmath608 where , from now on , @xmath609 denote constants that depend on @xmath610 and @xmath37 .",
    "now , we define @xmath611 , @xmath612 , @xmath613 and @xmath614 , thus we have , @xmath615 therefore , @xmath616 using similar arguments , we can prove that all remaining terms tend to zero . in consequence , @xmath617",
    "now let us prove that , @xmath618 , @xmath619 \\nonumber \\\\ \\nonumber\\end{aligned}\\ ] ] the above squared expected value is a sum of a large number of terms that are all treated similarly .",
    "let us consider examples of terms containing squared terms and products of terms , respectively . for @xmath620 , @xmath621",
    "the terms containing no squared values are treated as , e.g. , @xmath622 we can see that @xmath623 coincides with @xmath624 . then we can deduce that , @xmath625 finally we can apply @xcite , and we obtain : @xmath626 combining ( [ convenproba ] ) and ( [ convenloi ] ) , we have by slutsky theorem that : @xmath627    let us check the case where @xmath628 for all @xmath629 such that @xmath630 and the generalization to all @xmath179 in @xmath292 will be obvious . using gershgorin s theorem",
    "we get that each eigenvalue of @xmath631_{1 \\leq i , j \\leq p } $ ] lies in one of the disks centered in @xmath17 and radius @xmath632 .",
    "we have , @xmath633 & \\leq & 2 \\sqrt{\\lambda } \\big ( \\ds\\sum_{k=1}^t ( 1 - ( \\ds\\frac{k}{t})^{2\\alpha } )   \\big)^{\\frac{1}{2 } } t^{^{\\frac{1}{2 } } } = o(1 ) t\\sqrt{\\lambda } \\\\ & \\leq &   o(1 ) \\varphi^{1 - \\frac{1}{2 \\alpha } } \\to 0 \\text { provided that } \\alpha > 1/2 .",
    "\\end{array}\\ ] ] we deduce that the smallest eigenvalue is bounded from below by @xmath634 which is strictly positive for @xmath139 small enough .    [",
    "prop : wijproperty ] for all @xmath635 , @xmath636 is a centered random variable with variance , @xmath637 . moreover , for @xmath638 , we have @xmath639 also we have that @xmath640 . note that if we have @xmath641 and @xmath642 , then @xmath643 and @xmath644 are not correlated for @xmath645 finite integer . moreover , for all @xmath646 , the random variables @xmath647 are such that , @xmath648    to show the results we use lemma 3 and some technical computation of @xcite .",
    "@xmath649 or @xmath650 , where @xmath651 @xmath652 then we obtain that @xmath653 also we have that @xmath654 we use similar arguments to calculate the moments of @xmath647 .      for each @xmath656",
    ", @xmath248 is a degenerated u - statistic of order 2 , and can be written as follows : @xmath657 define , @xmath658 where @xmath659 is the @xmath660-field generated by the random variables @xmath661 . moreover , fix @xmath662 , and define @xmath663 then by theorem 3 of @xcite we get that , there exists a positive constant @xmath100 depending only on @xmath664 such that for any @xmath665 and any real @xmath149 , @xmath666 now , we give upper bounds for @xmath667 and @xmath668 for @xmath669 and get , @xmath670 similarly we can show that @xmath671 . thus we obtain the desired result ."
  ],
  "abstract_text": [
    "<S> we consider the detection problem of correlations in a @xmath0-dimensional gaussian vector , when we observe @xmath1 independent , identically distributed random vectors , for @xmath1 and @xmath0 large . </S>",
    "<S> we assume that the covariance matrix varies in some ellipsoid with parameter @xmath2 and total energy bounded by @xmath3 .    </S>",
    "<S> we propose a test procedure based on a u - statistic of order 2 which is weighted in an optimal way . </S>",
    "<S> the weights are the solution of an optimization problem , they are constant on each diagonal and non - null only for the @xmath4 first diagonals , where @xmath5 . we show that this test statistic is asymptotically gaussian distributed under the null hypothesis and also under the alternative hypothesis for matrices close to the detection boundary . </S>",
    "<S> we prove upper bounds for the total error probability of our test procedure , for @xmath6 and under the assumption @xmath5 which implies that @xmath7 . </S>",
    "<S> we illustrate via a numerical study the behavior of our test procedure .    </S>",
    "<S> moreover , we prove lower bounds for the maximal type ii error and the total error probabilities . </S>",
    "<S> thus we obtain the asymptotic and the sharp asymptotically minimax separation rate @xmath8 , for @xmath9 and for @xmath10 together with the additional assumption @xmath11 , respectively . </S>",
    "<S> we deduce rate asymptotic minimax results for testing the inverse of the covariance matrix .    </S>",
    "<S> we construct an adaptive test procedure with respect to the parameter @xmath12 and show that it attains the rate @xmath13 .    </S>",
    "<S> * mathematics subject classifications 2000 : * 62g10 , 62h15 , 62g20 + * key words : * adaptive test , covariance matrix , goodness - of - fit tests , high - dimensional data , minimax separation rate , sharp asymptotic rate , u - statistic + </S>"
  ]
}