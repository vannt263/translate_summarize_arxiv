{
  "article_text": [
    "a standard task in high energy physics experiments is the measurement of a distribution @xmath0 of some kinematical quantity @xmath1 . with an ideal detector",
    "one could measure the quantity @xmath1 in every event and could obtain @xmath0 by a simple histogram of the quantity @xmath1 . with _",
    "real _ detectors the determination of @xmath0 is complicated by three effects :    * * limited acceptance : * the probability to observe a given event , the _ detector acceptance _ , is less than 1 .",
    "the acceptance depends on the kinematical variable @xmath1 .",
    "* * transformation : * instead of the quantity @xmath1 a different , but related quantity @xmath2 is measured . the transformation from @xmath1 to @xmath2",
    "can be caused by the non - linear response of a detector component .",
    "* * finite resolution : * the measured quantity @xmath2 is smeared out due to the finite resolution ( or limited measurement accuracy ) of the detector . thus there is only a _ statistical _ relation between the true kinematical variable @xmath1 and the measured quantity @xmath2 .",
    "the really difficult effect in the data correction for experimental effects , or data transformation from @xmath2 to @xmath1 is the * finite resolution * , causing a _ smearing _ of the measured quantities .",
    "mathematically the relation between the distribution @xmath0 of the true variable @xmath1 , to be determined in an experiment , and the measured distribution @xmath3 of the measured quantity @xmath2 is given by the integral equation , @xmath4 called a fredholm integral equation of the first kind . in practice",
    "often a known ( measured or simulated ) background contribution @xmath5 has to be added to the right - hand side of equation ; this contribution is ignored in this paper .",
    "the resolution function @xmath6 represents the effect of the detector . for a given value @xmath7",
    "the function @xmath8 describes the response of the detector in the variable @xmath9 for that fixed value @xmath10 .",
    "the problem to determine the distribution @xmath0 from measured distributions @xmath3 is called _ unfolding _ ; it is called an inverse problem .",
    "unfolding of course requires the knowledge of the resolution function @xmath6 , i.e. all the effects of limited acceptance , transformation and finite resolution .",
    "in addition to the imperfections of the detector , there may be further effects between @xmath1 and @xmath9 , which are _ outside of the experimental control _",
    ", even with an ideal detector .",
    "one example are radiative effects , which in experiments are often corrected afterwards _",
    "( radiative corrections ) _ , but are in their effect similar to detector effects . if the true kinematical quantity is defined at the _ parton level _ , further effects from the fragmentation process of partons to the ( observable ) hadrons influence the measured quantity @xmath9 .",
    "all these effects are of statistical nature .",
    ", width=302 ]    a typical example for these effects is shown in figure [ fig : fmeas ] taken from a monte carlo simulation of all three effects . by unfolding an estimate of the original distribution has to be determined from the distorted measured distribution .",
    "details on the monte carlo simulation are given later in section [ sec : proposed ] , where the unfolding in this example is discussed in detail .",
    "for the numerical solution of equation the distributions have to represented by a finite set of parameters .",
    "one posssibility is to represent the distributions by histograms , and the resolution function by a matrix .",
    "equation can then be represented by the matrix equation @xmath11 which has to be solved for the vector @xmath12 , given the vector @xmath13 ( data histogram ) .",
    "the vector @xmath13 with @xmath14 elements represents a histogram of the measured quantity @xmath2 , and the distribution @xmath0 is represented by a histogram of the vector @xmath12 with @xmath15 elements .",
    "the variables @xmath2 and @xmath1 may be multidimensional , and the multidimensional histograms can be mapped to @xmath14-bin ( @xmath12 ) and @xmath15-bin histograms ( @xmath13 ) , respectively .",
    "the transition from @xmath12 to @xmath13 is described by the @xmath14-by-@xmath15 matrix @xmath16 . the element @xmath17 is related to the probability to observe an entry in histogram bin @xmath18 of the histogram @xmath13 , if the true value @xmath1 is from histogram bin @xmath19 of the histogram @xmath12 .",
    "problems with standard solutions are discussed in the next section .    in high energy physics experiments",
    "the problems is even more difficult than in other fields .",
    "often the statistics of the measurement is not high and every @xmath13-bin content ( which is distributed due to the poisson distribution around the expected value ) has a large statistical fluctuation .",
    "furthermore the resolution function @xmath20 ( or the matrix @xmath16 ) is not known analytically , but is represented by a data set from monte carlo simulation of the process , based on some assumed distribution @xmath21 , @xmath22 and is also statistically limited .",
    "standard methods for the solution of integral equations or linear equations can not be used in this case .",
    "a simple method like the so - called _ bin - by - bin correction _ may be meaningful if the measurements @xmath2 are very close to the true values @xmath1 .",
    "real _ unfolding _ methods , taking all the correlations into account , are essential if there are larger effects of _ transformation _ and _ finite resolution_. a solution @xmath12 has to be found , with small deviations between the elements of @xmath23 and the elements of the actually measured histogram @xmath24 . in the maximum likelihood method a function @xmath25 is constructed as the negative log of the likelihood function , which describes the statistical relations between data and result : @xmath26 and the minimum of @xmath25 is determined . wildly fluctuating results @xmath12 are due to large ( negative ) correlations between adjacent bins and are not acceptable . the approach to get a more reasonable solution is to impose a measure of the smoothness on the result @xmath12 ; this method is called * regularization*. this technique was proposed independently by phillips @xcite and by thikhonov @xcite . for a function",
    "@xmath0 the integrated square of the second derivative @xmath27 is often used in the regularization which in the linearized version of the problem can be expressed by a quadratic form @xmath28 with a positive - semidefinite matrix @xmath29 ( derivatives are replaced by finite differences ) .",
    "equation is then modified to the form @xmath30 with a factor @xmath31 called regularization parameter .",
    "the result of the minimization of the modified function @xmath32 of equation will show smaller fluctuations than the result obtained from equation and may be more useful to compare the measurement with theoretical predictions .",
    "however it is clear that unavoidably the regularization introduces a bias .",
    "the magnitude of the bias depends on the value of regularization parameter @xmath31 .",
    "a very large value would result in a _ linear _ function @xmath0 or distribution @xmath12 , respectively .",
    "it is clear that the method requires an a - priori knowledge about a smooth behaviour of @xmath0 .",
    "the function @xmath21 used in the monte carlo simulation of equation is often very close to the final result @xmath0 , i.e. the ratio is rather smooth .",
    "this suggests to express @xmath0 in the form @xmath33 and to rewrite equation in the form @xmath34",
    "f_{\\text{mult}}(x )   { \\,\\text{d}}x   \\ ; .\\ ] ] for the discretized form the function @xmath21 can be absorbed in a redefinition of matrix @xmath16 and the vector @xmath12 is interpreted as discretization of the hopefully _ smooth _",
    "function @xmath35 . with this redefinition",
    "the equation can remain unchanged .",
    "the program run @xcite for regularized unfolding is available since almost two decades and has been used in many experiments ; early applications are @xcite and @xcite .",
    "it is based on the reinterpretation of matrix @xmath16 and @xmath12 , as described above , and includes a method for the determination of the regularization parameter @xmath31 based on the available degrees of freedom . in the method",
    "described later in this paper some details are treated differently .",
    "the problems inherent to unfolding are discussed in a simple special case , assuming a resolution matrix @xmath16 with some smearing of data into neighbour bins . assuming a true vector @xmath12 the product @xmath36 describes the distribution expected due to the migration effect . with the same dimensions for the vectors @xmath12 and @xmath13 the matrix @xmath16 is a square matrix and in the example later in this section the following simple symmetric form is assumed for the matrix @xmath16 , which depends on a single parameter @xmath37 ( @xmath37 = migration parameter ) ; for a 5-by-5 matrix the form is @xmath38 a direct solution for @xmath12 , given a measurement @xmath39 differing from the expectation @xmath40 with the true vector @xmath12 by statistical fluctuations , is possible with inversion of the matrix @xmath16 : @xmath41 the result has certain good statistical properties , for example it has no bias : @xmath42 = { \\mbox{\\boldmath${a}$}}^{-1 } e\\left [ { \\mbox{\\boldmath${y}$ } } \\right ]    = { \\mbox{\\boldmath${a}$}}^{-1 }   { \\mbox{\\boldmath${a}$ } }   e \\left [ { \\mbox{\\boldmath${x}$ } } \\right ]      =   { \\mbox{\\boldmath${x}$}}$ ] . in practice",
    "the result is however satisfactory only for a matrix @xmath16 with dominating diagonal ; the result looks terrible if the matrix @xmath16 describes a large migration to neighbour bins .",
    "consequently the problem is called an ill - posed problem . in the following the solution of the equation @xmath43 using an orthogonal decomposition",
    "is discussed ; this will allow some insight into the unfolding problem .",
    "the symmetric matrix @xmath16 is expressed by @xmath44 with a transformation matrix @xmath45 with property @xmath46 , and a diagonal matrix @xmath47 , where the diagonal elements of matrix @xmath47 are the eigenvalues @xmath48 of matrix @xmath16 ( in the order of decreasing value ) .",
    "the transformation matrix @xmath45 contains the corresponding eigenvectors with the eigenvector @xmath49 in the @xmath19-th column .",
    "the condition number @xmath50 of a matrix is defined by the ratio of eigenvectors @xmath51 ; the value of @xmath50 is important for the quality of unfolding ( see below ) . for values above @xmath52",
    "the condition number @xmath50 is very rapidly increasing .",
    "a transformation of equation @xmath43 to a new basis is done by multiplication with matrix @xmath53 ( which is a rotation in the @xmath14-dimensional space ) : @xmath54 the matrix @xmath45 of eigenvectors @xmath55 allows to transform the vectors @xmath12 and @xmath13 to vectors @xmath56 and @xmath57 , and to transform these vectors back by @xmath58 and @xmath59 .",
    "the transformed equation @xmath60 with the diagonal matrix @xmath47 shows , that each of the coefficients @xmath61 and @xmath62 is transformed independently of any other coefficient by the simple relation @xmath63 .",
    "this operation does not depend on any assumption of the solution @xmath12 , and depends only on the properties of the matrix @xmath16 .",
    "folding ( @xmath64 ) and unfolding ( @xmath65 ) is multiplication and division by the eigenvalues @xmath66 , respectively , of the coefficients in the transformed space .    in order to unfold a measured vector @xmath13 ,",
    "the vector is transformed by @xmath67 to coefficients @xmath62 , which have values influenced by statistical fluctuations of the elements of vector @xmath13 . in the unfolding the coefficients",
    "@xmath62 are divided by the eigenvalues @xmath66 to obtain @xmath68 ; the statistical fluctuation of coefficient @xmath62 is magnified for small eigenvalues @xmath66",
    "( i.e. @xmath69 ) . eventually , for very small eigenvalues @xmath66",
    ", the final result @xmath70 will be dominated by one or by few of the coefficients @xmath61 with small eigenvalues and large statistical errors , and the complete result is unsatisfactory .    , title=\"fig:\",width=200],title=\"fig:\",width=200],title=\"fig:\",width=200 ]    * example . * in a numerical example the matrix @xmath16 has the form of equation with @xmath71 and a value of the migration parameter of @xmath72 .",
    "the first eigenvalue is @xmath73 , and the last one is @xmath74 , giving a condition number @xmath75 .",
    "for @xmath12 the ideal distribution of figure [ fig : trueunf]a is assumed ; the underlying function is of the form @xmath76 .",
    "the decomposition of the matrix @xmath16 according to equation is performed and the coefficients @xmath61 and @xmath62 are calculated .",
    "these coefficients are shown in figure [ fig : coeffs]a ( with @xmath77 ) .",
    "in addition this figure shows , calculated by standard error propagation , the almost constant error level of the coefficients , of the folded distribution of figure [ fig : trueunf]a with poisson distributed bin contents .",
    "figure [ fig : coeffs]a shows , that the coefficients @xmath61 of the true distribution decrease rapidly with increasing value @xmath19 of the index of the coefficient , by roughly three orders of magnitude .",
    "the coefficients @xmath62 of the folded distribution drop even faster , because it is more smooth due to the migration effect .",
    "of course the relation @xmath78 is valid .",
    "the last coefficient @xmath61 in figure [ fig : coeffs]a is reduced to @xmath62 by the inverse of the condition number of the matrix , which is @xmath75 in this case .     and @xmath62 are shown for @xmath79 . the coeffients @xmath61 and @xmath62 for the true distribution and the folded distribution ( without measurement errors ) are shown in ( a ) , together with the ( almost constant ) error estimate for the coeffients @xmath61 calulated by error propagation .",
    "the coeffients @xmath62 from the simulated measured distribution are shown in ( b ) , together with the error estimate .",
    "for @xmath19 above @xmath80 the smaller coefficients of the folded distribution become smaller than the statistical error . in",
    "( b ) the coefficients for @xmath19 above @xmath80 are dominated by statistical errors and even the sign is not determined by the data .",
    "[ fig : coeffs],title=\"fig:\",width=302 ] and @xmath62 are shown for @xmath79 .",
    "the coeffients @xmath61 and @xmath62 for the true distribution and the folded distribution ( without measurement errors ) are shown in ( a ) , together with the ( almost constant ) error estimate for the coeffients @xmath61 calulated by error propagation .",
    "the coeffients @xmath62 from the simulated measured distribution are shown in ( b ) , together with the error estimate . for @xmath19 above @xmath80 the smaller coefficients of the folded distribution become smaller than the statistical error . in ( b ) the coefficients for @xmath19 above @xmath80 are dominated by statistical errors and even the sign is not determined by the data .",
    "[ fig : coeffs],title=\"fig:\",width=302 ]    the components of the first eigenvector @xmath81 ( eigenvalue = 1 ) are all the same .",
    "thus the coefficients @xmath82 and @xmath83 are identical , and proportional to the total sum of the measured distribution , not at all influenced by the migration .",
    "if visualized by functions , interpolating the components the eigenvector @xmath84 ( eigenvalue @xmath66 ) has @xmath85 zeros , and the curvature of the visualized eigenvectors is rapidly increasing with index @xmath19 .",
    "the components of the last eigenvector @xmath86 have alternating sign for the bins ; it has a small absolute value and its measured value will have a large relative statistical error .",
    "the value of @xmath87 is obtained by @xmath88 in unfolding , introducing a large bin - to - bin oscillation into the result of unfolding .    in a simulation poisson",
    "distributed bin contents are assumed in the measurement vector @xmath13 .",
    "the coefficents for this measured distribution are shown in figure [ fig : coeffs]b , together with the level of the statistical error . as expected from the size of the errors all coefficients with an index above about @xmath89",
    "are dominated by the statistical error and therefore do not significantly contribute to the information content of the measurement . for indices above @xmath89 even the sign of the coeffient can not be determined by the measurement .    using all the  measured ",
    "coefficients for the unfolding the result of figure [ fig : trueunf]b is obtained .",
    "this result shows large fluctuations around the expected values shown by the curve .",
    "the fluctuations are due to the contributions from indices above @xmath89 , which represent noise and are magnified in the unfolding because of the large values of their inverse eigenvalues .",
    "the result is clearly unsatisfactory .",
    "because all measured coefficients @xmath62 with @xmath19 above a value of 12 are dominated by statistical errors ( noise ) their use in the unfolding makes no sense . a sharp cut - off after index @xmath89 or even after index @xmath90 will not remove any useful information from the measurement .",
    "the unfolding result using only measured coefficients @xmath62 up to @xmath90 is shown in figure [ fig : trueunf]c ; compared to figure [ fig : trueunf]b the large fluctuations are suppressed and the results seems to be acceptable .",
    "of course the fine structure of the true distribution expressed by the true coefficients @xmath61 with @xmath91 is not included in the solution and this may represent a bias .",
    "it is an unavoidable bias because these coefficients can not be measured .",
    "the covariance matrix of the result can be calculated by standard error propagation .",
    "however it is clear that the covariance matrix is singular and has only rank 10 in this case , because the 20 bins are obtained from 10 measured coefficients ( 10 degrees of freedom ) .",
    "this property is inherent to the cut - off method and to the regularization method , and was already mentioned in @xcite .",
    "such singularity of the covariance matrix can be avoided if the final transformation is to a number of bins identical to the degree of freedoms ; only a limited number of bins can be obtained in a measurement with large miration effects .",
    "this method of using a sharp cut - off has to be compared to the regularization method .",
    "it has been shown @xcite that the use of a regularization function of the type of equation is equivalent to a _ smooth _ cut - off ; essentially the measured coefficients @xmath62 are multiplied by a factor depending on the curvature of the orthogonal contributions ( see section [ sec : proposed ] ) .",
    "is proposed in the literature without explicit regularization , starting from a  good  initial distribution for @xmath12 .",
    "of course equations of this type ( with a square matrix ) have a unique solution and iterative solutions are slow compared to the direct solution ; after a large number of iterations with convergence the same unsatisfactory result as by direct solution will be obtained .",
    "however in these proposals only a small number of iterations is recommended .",
    "it can be shown that iterative methods can in fact include an implicit regularization @xcite : there is a different speed of convergence for the various orthogonal contributions and the small contributions with a small eigenvalue will converge very slowly .",
    "thus after a few iterations the ( large ) coefficients with large eigenvalues are already accurate ; the remaining coefficients are still almost unchanged and thus , for a stop after few iterations , their values are still close to the starting values .",
    "there is of course some subjectivity in stopping ",
    "early  enough . ]",
    "the proposed method is similar to the method used in run ; the differences are emphasized in this section .",
    "it is expected that the proposed modifications results in more stable solutions .",
    "the proposed method requires large dimension parameters in the resolution matrix @xmath16 .",
    "like in run the regularization is determined by the required number of degrees of freedom , which determines the regularization parameter .",
    "figures in this section refer to the example already mentioned in section [ sec : unfolding ] in a monte carlo calculation of all three effects , limited ( @xmath1-dependent ) acceptance , non - linear transformation and finite resolution are simulated .",
    "details on the function and the distorting effects are identical to the published examples @xcite . in",
    "total 100  000  events  are simulated for  data  and for the mc defining matrix @xmath16 .",
    "the input function @xmath21 ( equation ) is a constant .    in run the discretization for @xmath0 and for @xmath92",
    "was done using cubic b - spline functions ; the effect is the same as for simple histograms namely the integral equation is transformed to a system of linear equations , however the elements of the vectors are b - spline coefficients instead of bin contents .",
    "the advantage is that the parametrized solution is a _",
    "smooth _ function and the curvature as defined by equation can be exactly written as a quadratic form .",
    "however the accurate determination of matrix @xmath16 requires a good monte carlo statistic . in run statistical fluctuations of the elements of matrix",
    "@xmath16 could not be treated .",
    "simple histograms are instead proposed here ; the elements of the vector @xmath13 are bin contents ( integer numbers ) .",
    "the curvature of the solution is constructed by finite differences : the second derivative in bin @xmath19 is proportional to @xmath93 . in a histogram",
    "some resolution is lost if bins with a width as large as expected for the final resolution would be used .",
    "it is recommended to use initially @xmath94 bins for @xmath12 for a final number of degrees of freedom of @xmath95 . for @xmath2",
    "a larger number of bins @xmath14 ( @xmath96 ) is recommended , to avoid a loss of resolution .",
    "thus the number @xmath97 of elements is large , and a large sample of monte carlo events is required to _ fill _ matrix @xmath16 .",
    "the statistical error of the elements @xmath17 eventually can not be neglected .    *",
    "standard poisson maximum likelihood fit .",
    "* ignoring initially eventual statistical errors of the elements @xmath17 the expected number of events in bin @xmath18 of @xmath13 is given by @xmath98 . for the expected number @xmath99 , as given by this expression , the observed values @xmath100 follows the poisson distribution .",
    "optimal estimates for the elements @xmath101 are obtained by minimizing the ( negative ) logarithm of the total likelihood with respect to the elements @xmath101 of vector @xmath12 , assuming the poisson distribution : @xmath102       = \\sum_{i=1}^n \\left (   y_i - \\widehat{y}_i \\cdot \\ln y_i      \\right )    + \\text{const . } \\ ; , \\ ] ] where the constant term containing e.g @xmath103 can be ommited .",
    "this expression correctly accounts also for bins with a small number of histogram entries @xmath100 .",
    "an alternative would be to use the ( linear ) least squares method with singular value decomposition for the fit .",
    "however for small number of entries the use of the poisson distribution seems to be essential .",
    "furthermore the diagonalization used later in the method is almost equivalent to singular value decomposition ( eigenvalues are the squares of the singular values ) .    *",
    "fitting with finite monte carlo samples . *",
    "the problem of statistical fluctuations of the elements @xmath17 has been neglected so far . a method to treat the problem within the maximum - likelihood method has been developed by r.barlow and chr.beeston @xcite . in this method",
    "there is for each source bin @xmath101 some ( unknown ) expected number of events @xmath104 . for each element",
    "@xmath104 the corresponding number @xmath17 from the monte carlo sample is generated by a distribution which is taken to be poisson too .",
    "the nice feature of this method is that a bias which would be introduced by ignoring the statistical character of the values of the elements @xmath17 is avoided and the maximum likelihood error is more realistic .",
    "a large number of slack variables ( one for each bin ) is introduced and has to be treated in the optimzation .",
    "a new fast numerical solution method has been developed ( see @xcite ) .    * combining bins . *",
    "the likelihood function is a sum over all bins .",
    "combining almost empty bins does not introduce a systematic error .",
    "the total number of elements of the matrix may be large , especially if @xmath1 and/or @xmath2 are multidimensional , and a small number of entries ( or even zero ) in an element may not be uncommon .",
    "the combination of almost empty bins is done with a cluster algorithm , taking into account the distance between bins in one , two or three dimensions .    *",
    "first option : sharp cut - off of orthogonal contributions .",
    "* this method is rather similar to the method discussed in section [ sec : ill ] .",
    "the computational problem is to determine the minimum of @xmath25 ( see equation ) .",
    "the standard iterative method is based on the representation for the correction @xmath105 @xmath106 with the hessian @xmath107 ( matrix of second derivatives of @xmath108 ) and the gradient vector ( first derivatives of @xmath108 ) .",
    "a newton step is then calulated from equation @xmath109 .",
    "convergence is usually fast for good starting values and the covariance matrix is equal to the inverse of the hessian .",
    "the starting values can be calculated by a linear least square fit , based on the approximation of the poisson distribution by a gaussian distribution for each bin .",
    "a sharp cut - off as discussed in the example of section [ sec : ill ] requires a diagonalization of the symmetric matrix @xmath107 by @xmath110 with a diagonal matrix @xmath47 and a transformation matrix @xmath45 . by a transformation ( rotation ) in @xmath12-space linear combinations of the @xmath12-components",
    "are obtained with a diagonal covariance matrix , with variances of the linear combinations given by the inverse of the eigenvalues of matrix @xmath47 .",
    "a cut - off is done at some index @xmath19 followed by backtransformation to the @xmath12-space of bin - contents using the transformation matrix @xmath45 .    *",
    "second option : regularization .",
    "* in this option the regularization is based on the second derivative of the result according to equation , which can be expressed by a quadratic form @xmath111 with a positive - semidefinite matrix @xmath29 . in principle",
    "the same procedure is used as in run ; the mathematical details are given elsewhere @xcite . here a simple explanation is given on the standard mathematical operations used .",
    "regularization is done by adding the term @xmath112 to the function @xmath108 of equation .",
    "exactly as in the first option the hessian is diagonalized .",
    "@xmath113                    \\left [ { \\mbox{\\boldmath${d}$}}^{-1/2 } { \\mbox{\\boldmath${u}$}}^t \\right ]   \\ ; .\\ ] ] up to this step everything is identical to the cut - off option . using transformation matrix @xmath114 the vector @xmath12 is transformed to linear combinations @xmath115 , which are orthogonal , with all variances equal to 1 ( unit covariance matrix ) . because the covariance matrix is equal to the unit matrix",
    ", every additional pure rotation will not change the ( unit ) covariance matrix . in terms of the transformed vector the regularization term can now be written in the form @xmath116 , where @xmath117 is the transformed curvature matrix @xmath29 .",
    "now another diagonalization can be done of matrix @xmath117 : @xmath118 with a diagonal matrix @xmath119 and a rotation matrix @xmath120 .",
    "this diagonalization can be used to define a pure rotation from the linear combination @xmath115 to another linear combination @xmath121 @xmath122 the components of the new vector @xmath121 still have the unit matrix as covariance matrix . the complete transformation from @xmath12 to @xmath121",
    "is the effect of the transformation by @xmath114 and by @xmath120 .",
    "the algebra can be explained in other words : the error ellipsoid related to the hessian is first rotated to have the axes parallel to the axes of the new system . by a change of the scales the ellipsoid",
    "is transformed to a sphere , which will remain a sphere for any further rotation .",
    "a last rotation is done to bring the axes into the order of increasing curvature .    ,",
    "@xmath123 and @xmath124 .",
    "visualization is done by curves interpolating the components .",
    "the amplitude associated which each vector all have the same standard deviation of 1 .",
    "[ fig : feig],width=302 ]    some columns of the complete ( product ) transformation are shown in figure [ fig : feig ] .",
    "all linear combinations obtained have the same precision ( standard deviation of the coefficient is one ) . as seen in the figure linear combinations with large index @xmath19 are oscillating with large amplitude .",
    "the diagonal elements @xmath125 are the ( statistically independent ) contributions of the elements of @xmath121 to the total curvature . sorted according to increasing value of @xmath125 the value of @xmath125 will increase rather fast with increasing index @xmath19 .",
    "the spectrum of eigenvalues @xmath125 is shown in figure [ fig : eicu ] . in terms of the linear combinations @xmath121 regularization",
    "is simply given by @xmath126 and this simple form is the reason for the transformations made before .",
    "[ fig : eicu],title=\"fig:\",width=302 ] [ fig : eicu],title=\"fig:\",width=302 ]    * determination of the regularization parameters @xmath31 .",
    "* the first factors ( small @xmath19 ) on the right - hand - side of equation will be close to 1 ; for a value @xmath127 the factor will be 1/2 and for indices @xmath128 will rapidly decrease towards zero .",
    "the sum of all factors can be called the _ effective _ number of degrees of freedom , and can be used to determine the value of the regularization parameter @xmath31 from the required number of degrees of freedom , i.e. the regularization parameter @xmath31 is determined from the value of @xmath95 in the equation @xmath129 thus the required number of degrees of freedom has to be specified and determines the degree of regularization .",
    "this number can be taken from the spectrum of the coefficients or amplitudes , shown in figure [ fig : famp ] .",
    "the insignificant part ( large @xmath19 ) is clearly visible in the spectrum and separated from the significant part ( small @xmath19 ) .",
    "the selected value of @xmath130 should be equal to or larger than the number of significant terms .",
    "the unregularized amplitudes , which have standard deviation one , are shown by the left bars ; amplitudes above index 15 are compatible with one and represent noise .",
    "they would however make a large contribution to the solution , because the corresponding column vectors ( figure [ fig : feig ] ) are large .",
    "the regularization effectively damps the amplitude ( right bars ) around and above index 15 , which has been chosen as the degree of freedom here .",
    "the significant amplitudes are not affected by the regularization .",
    "are used as input .",
    "[ fig : fres],title=\"fig:\",width=302 ] are used as input .",
    "[ fig : fres],title=\"fig:\",width=302 ]    the final result of the example ( measured distribution in figure [ fig : fmeas ] ) is shown in figure [ fig : fres ] .",
    "the left figure shows 30 data points with error bars together with the original ( true ) distribution ; within errors the original distribution is nicely reproduced .",
    "the rank of the covariance matrix is about 15 , which was chosen as the effective number of degrees of freedom ; thus inversion of the covariance matrix , needed e.g. for a least - square fit of a model to the data , is not possible .",
    "although the large number of 30 data points seems to be attractive , the data points should be reduced to 15 data points by combining two bins to one , which then have a full - rank covariance matrix .",
    "this set of data points is shown in figure [ fig : fres ] ( right ) .",
    "the broader bins of this set of data points are a consequence of the limited acceptance and finite resolution of the measurement .",
    "i would like to thank the organizers of the conference on advanced statistical techniques in particle physics for their hospitality and the stimulating atmosphere in durham .",
    "99 a more detailed text is available via ` http://www.desy.de/~blobel/ ` .",
    "d. l. phillips , a technique for the numerical solution of certain integral equations of the first kind , j. assoc . comput . mach .",
    "* 9 * , 84 - 97 ( 1962 ) a.n .",
    "thikhonov , on the solution of improperly posed problems and the method of regularization , _ sov .",
    "math . _ * 5 * , 1035 ( 1963 ) v. blobel , unfolding methods in high energy physics experiments , in _ proceedings of the 1984 cern school of computing _",
    ", cern 85 - 09 ( 1985 ) and desy 84 - 114 v. blobel , the @xmath131 manual , regularized unfolding for high - energy physics experiments , opal technical note tn361 ( 1996 ) m. jonker et al .",
    "( charm collaboration ) , experimental study of differential cross sections @xmath132 in neutral current neutrino and antineutrino interactions physics letters * 102 b * , 62 - 72 ( 1981 ) m. jonker et al .",
    "( charm collaboration ) , experimental study of @xmath1-distributions in semileptonic neutral current neutrino interactions , physics letters * 128 b * , 117 - 123 ( 1983 ) ch .",
    "berger et al .",
    "( pluto collaboration ) , measurement of the photon structure function @xmath133 , physics letters * 142 b * , 111 - 118 ( 1984 ) ch .",
    "berger et al . ( pluto collaboration ) , measurement of deep inelastic electron scattering off virtual photons , physics letters * 142 b * , 119 - 124 ( 1984 ) a.k .",
    "louis , inverse und schlecht gestellte probleme , teubner , stuttgart und leipzig ( 1989 ) roger barlow and christine beeston , fitting finite monte carlo samples , _ computer physics communications _ * 77 * , 219228 ( 1993 )"
  ],
  "abstract_text": [
    "<S> finite detector resolution and limited acceptance require to apply unfolding methods in high energy physics experiments . </S>",
    "<S> information on the detector resolution is usually given by a set of monte carlo events . </S>",
    "<S> based on the experience with a widely used unfolding program ( run ) a modified method has been developed .    </S>",
    "<S> the first step of the method is a maximum likelihood fit of the monte carlo distributions to the measured distribution in one , two or three dimensions ; the finite statistic of the monte carlo events is taken into account by the use of barlows method with a new method of solution . </S>",
    "<S> a clustering method is used before to combine bins in sparsely populated areas . in the second step a regularization </S>",
    "<S> is applied to the solution , which introduces only a small bias . </S>",
    "<S> the regularization parameter is determined from the data after a diagonalization and rotation procedure . </S>"
  ]
}