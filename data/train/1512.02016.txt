{
  "article_text": [
    "link prediction is a fundamental task in statistical network analysis . for static networks ,",
    "it is defined as predicting the missing links from a partially observed network topology ( and some attributes if exist ) .",
    "existing approaches include : 1 ) unsupervised methods that design good proximity / similarity measures between nodes based on network topology features  @xcite , e.g. , common neighbors , jaccard s coefficient  @xcite , adamic / adar  @xcite , etc ; 2 ) supervised methods that learn classifiers on labeled data with a set of manually designed features  @xcite ; 3 ) others @xcite that use random walks to combine the network structure information with node and edge attributes .",
    "one possible limitation for such methods is that they rely on well - designed features or measures , which can be time demanding to get and/or application specific .",
    "latent variable models @xcite have been widely applied to discover latent structures from complex network data , based on which prediction models are developed for link prediction .",
    "although these models work well , one remaining problem is how to determine the unknown number of latent classes or features . a typical way using model selection , e.g. , cross - validation or likelihood ratio test @xcite , can be computationally prohibitive by comparing many candidate models .",
    "bayesian nonparametrics has shown promise in bypassing model selection by imposing an appropriate stochastic process prior on a rich class of models  @xcite . for link prediction , the infinite relational model ( irm",
    ") @xcite is class - based and uses bayesian nonparametrics to discover systems of related concepts .",
    "one extension is the mixed membership stochastic blockmodel ( mmsb ) @xcite , which allows entities to have mixed membership . @xcite and @xcite developed nonparametric latent feature relational models ( lfrm ) by incorporating indian buffet process ( ibp ) prior to resolve the unknown dimension of a latent feature space .",
    "though lfrm has achieved promising results , exact inference is intractable due to the non - conjugacy of the prior and link likelihood .",
    "one has to use metropolis - hastings @xcite , which may have low accept rates if the proposal distribution is not well designed , or variational inference @xcite with truncated mean - field assumptions , which may be too strict in practice .    in this paper , we develop discriminative nonparametric latent feature relational models ( dlfrm ) by exploiting the ideas of data augmentation with simpler gibbs sampling @xcite under the regularized bayesian inference ( regbayes ) framework @xcite .",
    "our major contributions are : 1 ) we use the regbayes framework for dlfrm to deal with the imbalance issue in real networks and naturally analyze both the logistic log - loss and the max - margin hinge loss under a unified setting ; 2 ) we explore data augmentation techniques to develop a simple gibbs sampling algorithm , which is free from unnecessary truncation and assumptions that typically exist in variational approximation methods ; 3 ) we develop an approximate gibbs sampler using stochastic gradient langevin dynamics , which can handle large networks with hundreds of thousands of entities and millions of links ( see table  [ table : data ] ) , orders of magnitude larger than what the existing lfrm models  @xcite can process ; and 4 ) finally , we conduct experimental studies on a wide range of real networks and the results demonstrate promising results of our methods .",
    "we consider static networks with @xmath0 entities .",
    "let @xmath1 be the @xmath2 binary link indicator matrix , where @xmath3 denotes the existence of a link from entity @xmath4 to @xmath5 , and @xmath6 denotes no link from @xmath4 to @xmath5 .",
    "@xmath1 is not fully observed .",
    "r0.18     our goal is to learn a model from the partially observed links and predict the values of the unobserved entries of @xmath1 .",
    "[ fig : model ] illustrates a latent feature relational model ( lfrm ) , where each entity is represented by @xmath7 latent features .",
    "let @xmath8 be the @xmath9 feature matrix , each row is associated with an entity and each column corresponds to a feature .",
    "we consider the binary features , where @xmath10 is the real - valued vector representing the amplitudes of each feature while the binary vector @xmath11 represents the presence of each feature . ] : if entity @xmath4 has feature @xmath12 , then @xmath13 , otherwise @xmath14 .",
    "let @xmath11 be the feature vector of entity @xmath4 , @xmath15 be a @xmath16 real - valued weight matrix , @xmath17 and @xmath18 , where @xmath19 is a vector concatenating the row vectors of matrix @xmath20 .",
    "note that @xmath21 and @xmath22 are column vectors , while @xmath11 is a row vector .",
    "then the probability of the link from entity @xmath4 to @xmath5 is @xmath23where is the sigmoid function .",
    "we assume that links are conditionally independent given @xmath8 and @xmath15 , then the link likelihood is , where @xmath24 is the set of training links ( observed links ) .    in the above formulation ,",
    "we assume that the dimensionality of the latent features @xmath7 is known a priori . however , this assumption is often unrealistic especially when dealing with large - scale applications .",
    "the conventional approaches that usually need a model selection procedure ( e.g. , cross validation ) to choose an appropriate value by trying on a large set of candidates can be expensive and often require extensive human efforts on guiding the search .",
    "recent progress on bayesian optimization  @xcite provides more effective solution to searching for good parameters , but still needs to learn many models under different configurations of the hyper - parameter @xmath7 .    in this paper",
    ", we focus on the nonparametric bayesian methods  @xcite for link prediction . the recently developed nonparametric latent feature relational models ( lfrm )",
    "@xcite leverage the advancement of bayesian nonparametric methods to automatically resolve the unknown dimensionality of the feature space by applying a flexible nonparametric prior .",
    "it assumes that each entity @xmath4 has an infinite number of binary features , that is @xmath25 , and the indian buffet process ( ibp ) @xcite is used as a prior of @xmath8 to produce a sparse latent feature vector for each entity .",
    "we treat the weight matrix @xmath15 as random and put a prior on it for fully bayesian inference .",
    "then with bayes theorem , the posterior distribution is @xmath26where the prior @xmath27 is an ibp and @xmath28 is often assumed to be an isotropic gaussian prior .      the conventional bayesian inference as above relies on bayes rule to infer the posterior distribution . in fact ,",
    "this procedure can be equivalently formulated as solving an optimization problem .",
    "for example , the bayes posterior in eq .",
    "( [ e2 ] ) is equivalent to the solution of the following problem : @xmath29,\\end{aligned}\\]]where @xmath30 is the space of well - defined distributions and @xmath31 is the kullback - leibler ( kl ) divergence from @xmath32 to @xmath33 .",
    "such an optimization view has inspired the development of regularized bayesian inference ( regbayes ) which solves : @xmath34where @xmath35 is a posterior regularization defined on the target posterior distribution and @xmath36 is a non - negative regularization parameter that balances the prior part and the posterior regularization part .",
    "we refer the readers to @xcite for more details on a generic representation theorem of the solution and its application  @xcite to learn latent feature models for classification .",
    "below , we explore the ideas to develop effective latent feature relational models for link prediction .",
    "although we could define an averaging classifier and make predictions using the sign rule @xmath37 ) $ ] , the resulting problem needs to be approximately solved by truncated variational methods , which can be inaccurate in practice . here , we propose to define a gibbs classifier , which admits simple and efficient sampling algorithms that are guaranteed to be accurate .",
    "our gibbs sampler randomly draws the latent variables @xmath38 from the unknown but pre - assumed to be given posterior distribution @xmath39 .",
    "once @xmath8 and @xmath15 are given , we can make predictions using the sign rule @xmath40 and measure the training error @xmath41 , where @xmath42 is an indicator function .",
    "since the training error is non - smooth and non - convex , it is often relaxed by a well - behaved loss function .",
    "let @xmath43 , two well - studied examples are the logistic log - loss @xmath44 and the hinge loss @xmath45 : where @xmath46 , @xmath47 , @xmath48 is the pre - defined cost to penalize a wrong prediction , and @xmath49 so that @xmath50 refers to a negative link instead of @xmath51 . to account for the uncertainty of the latent variables",
    ", we define the posterior regularization as the expected loss : @xmath52 \\nonumber , \\mathcal{r}_2\\ !",
    "( q(u , z ) ) \\!= \\ ! { \\mathbb{e}}_q [ r_2(z , u ) ] .",
    "\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\nonumber\\end{aligned}\\]]with these posterior regularization functions , we can do the regbayes as in problem  ( [ ee3 ] ) , where the parameter @xmath36 balances the influence between the prior distribution ( i.e. , @xmath53 divergence ) and the observed link structure ( i.e. , the loss term ) .",
    "we define the un - normalized pseudo link likelihood : @xmath54then problem  ( [ ee3 ] ) can be written in the equivalent form : @xmath55,\\]]where @xmath56 and @xmath57 can be @xmath58 or @xmath59 . then the optimal solution of ( [ ee3 ] ) or ( [ ee6 ] ) is the following posterior distribution with link likelihood : @xmath60notice that if adopting the logistic log - loss , we actually obtain a generalized pseudo - likelihood which is a powered form of likelihood in eq .",
    "( [ eq : likelihood ] ) .    for real networks , positive links are often highly sparse as shown in table  [ table : data ] .",
    "such sparsity could lead to serious imbalance issues in supervised learning , where the negative examples are much more than positive examples . in order to deal with the imbalance issue in network data and make the model more flexible",
    ", we perform regbayes by controlling the regularization parameter . for example",
    ", we can choose a larger @xmath36 value for the fewer positive links and a relatively smaller @xmath36 for the larger negative links .",
    "this strategy has shown effective in dealing with imbalanced data in  @xcite .",
    "we will provide experiments to demonstrate the benefits of regbayes on dealing with imbalanced networks when learning nonparametric lfrms .",
    "as we do not have a conjugate prior on @xmath15 , exact posterior inference is intractable .",
    "previous inference methods for nonparametric lfrm use either metropolis - hastings  @xcite or variational techniques  @xcite which can be either inefficient or too strict in practice .",
    "we explore the ideas of data augmentation to give the pseudo - likelihood a proper design , so that we can directly obtain posterior distributions and develop efficient gibbs sampling algorithms .",
    "specifically , our algorithm relies on the following unified representation lemma .",
    "both @xmath58 and @xmath59 can be represented as where for @xmath58 we have while for @xmath59 , let @xmath61 , we have    we have used @xmath62 to denote a polya - gamma distribution  @xcite and @xmath63 to denote a generalized inverse gaussian distribution .",
    "we defer the proof to appendix a , which basically follows  @xcite with some algebraic manipulation on re - organizing the terms .",
    "draw @xmath8 from ibp , @xmath15 from @xmath64 ; set @xmath65 .",
    "draw @xmath66 from eq .",
    "( [ ee8 ] ) ; draw @xmath67 using eq .",
    "( [ e6 ] ) .",
    "update @xmath68 , update new weights ; draw @xmath15 using eq .",
    "( [ e7 ] ) and draw @xmath69 using eq .",
    "( [ e8 ] ) .",
    "lemma 1 suggests that the pseudo - likelihood @xmath57 can be considered as the marginal of a higher dimensional distribution that includes the augmented variables @xmath69 : @xmath70which is a mixture of gaussian components of @xmath15 once @xmath8 is given , suggesting that we can effectively perform gibbs sampling if a conjugate gaussian prior is imposed on @xmath15 .",
    "we also construct the complete posterior distribution : @xmath71such that our target posterior @xmath72 is a marginal distribution of the complete posterior . therefore ,",
    "if we can draw a set of samples @xmath73 from the complete posterior , by dropping the augmented variables , the rest samples @xmath74 are drawn from the target posterior @xmath75 .",
    "this technique allows us to sample the complete posterior via a gibbs sampling algorithm , as outlined in alg .",
    "[ alg1 ] and detailed below .    * for @xmath8 : * we assume the indian buffet process ( ibp ) prior on the latent feature @xmath8 .",
    "although the total number of latent features is infinite , every time we only need to store @xmath7 active features that are not all zero in the columns of @xmath8 . when sampling the @xmath76-th row , we need to consider two cases , due to the nonparametric nature of ibp .",
    "first , for the active features , we sample @xmath77 in succession from the following conditional distribution @xmath78where @xmath79 and @xmath80 is the number of entities containing feature @xmath12 except entity @xmath81 .",
    "second , for the infinite number of remaining all - zero features , we sample @xmath67 number of new features and add them to the @xmath76th row . then we get the new @xmath82 matrix @xmath83 which becomes old when sampling the @xmath84-th row .",
    "every time when the number of features changes , we also update @xmath15 and extend it to a @xmath85 matrix @xmath86",
    ". let @xmath87 and @xmath88 be the parts of @xmath83 and @xmath86 that correspond to the @xmath67 new features .",
    "also , we define @xmath89 . during implementation , we can delete the all - zero columns after every resampling of @xmath8 , but here we ignore it .",
    "let @xmath90 follow the isotropic normal prior @xmath64 .",
    "now the conditional distribution for @xmath91 is @xmath92 , and the probability of @xmath93 is @xmath94where @xmath95 is from the ibp prior , @xmath96 is the dimension of @xmath97 and the mean @xmath98 , covariance @xmath99 .",
    "we compute the probabilities for @xmath100 , do normalization and sample from the resulting multinomial . here , @xmath101 is the maximum number of features to add .",
    "once we have added @xmath102 new features , we should also sample their weights @xmath97 , which follow a @xmath103 dimensional multivariate gaussian , in order to resample the next row of @xmath8 .    * for @xmath15 : * after the update of @xmath8 , we resample @xmath15 given the new @xmath8 .",
    "let @xmath104 and @xmath21 follow the isotropic normal prior @xmath105 . then the posterior is also a gaussian distribution @xmath106with the mean @xmath107 and the convariance @xmath108 .    * for @xmath109 : * since the auxiliary variables are independent given the new @xmath8 and @xmath15 , we can draw each @xmath110 separately . from the unified representation",
    ", we have @xmath111by doing some algebra , we can get the following equations . for @xmath58",
    ", @xmath110 still follows a polya - gamma distribution @xmath112 , from which a sample can be efficiently drawn . for @xmath59",
    ", @xmath110 follows a generalized inverse gaussian distribution @xmath113 , where @xmath114 .",
    "then @xmath115 follows an inverse gaussian distribution @xmath116 , from which a sample can be easily drawn in a constant time .",
    "[ alg1 ] needs to sample from a @xmath117-dim gaussian distribution to get @xmath15 , where @xmath7 is the latent feature dimension .",
    "this procedure is prohibitively expensive for large networks when @xmath7 is large ( e.g. , @xmath118 ) . to address this problem",
    ", we employ stochastic gradient langevin dynamics ( sgld ) @xcite , an efficient gradient - based mcmc method that uses unbiased estimates of gradients with random mini - batches .",
    "let @xmath119 denote the model parameters and @xmath120 is a prior distribution .",
    "given a set of i.i.d data points @xmath121 , the likelihood is @xmath122 . at each iteration @xmath123 ,",
    "the update equation for @xmath119 is : @xmath124where @xmath125 is the step size , @xmath126 is a subset of @xmath127 with size @xmath128 and @xmath129 is the gaussian noise . when the stepsize is annealed properly , the markov chain will converge to the true posterior distribution .",
    "let @xmath130 be a subset of @xmath131 with size @xmath128 .",
    "we can apply sgld to sample @xmath21 ( i.e. , @xmath15 ) . specifically , according to the true posterior of @xmath21 as in eq .",
    "( [ e7 ] ) , the update rule is : @xmath132where @xmath133 is a @xmath117-dimensional vector and each entry is a gaussian noise . after a few iterations , we will get the approximate sampler of @xmath21 ( i.e. , @xmath15 ) very efficiently .",
    "we present experimental results to demonstrate the effectiveness of dlfrm on five real datasets as summarized in table  [ table : data ] , where * nips * contains @xmath134 authors who have the most coauthor - relationships with others from nips @xmath135-@xmath136 ; * kinship * includes @xmath137 relationships of @xmath138 people in the alyawarra tribe in central australia ; * webkb * contains @xmath139 webpages from the cs departments of different universities , where the dictionary has @xmath140 unique words ; * astroph * contains collaborations between @xmath141 authors of papers submitted to arxiv astro physics in the period from jan .",
    "1993 to apr .",
    "2003  @xcite ; and * gowalla * contains @xmath142 people and their friendships on gowalla social website  @xcite .",
    "all these real networks have very sparse links .",
    "we evaluate three variants of our model : ( 1 ) * dlfrm * : to overcome the imbalance issue , we set @xmath143 as in @xcite , where @xmath144 is the regularization parameter for positive links and @xmath145 for negative links .",
    "we use a full asymmetric weight matrix @xmath15 ; ( 2 ) * stodlfrm * : the dlfrm model that uses sgld to sample weight matrix @xmath15 , where the stepsizes are set by @xmath146 for log - loss and adagrad  @xcite for hinge loss ; ( 3 ) * diagdlfrm * : the dlfrm that uses a diagonal weight matrix @xmath15 .",
    "each variant can be implemented with the logistic log - loss or hinge loss , denoted by the superscript @xmath147 or @xmath148 .",
    "we randomly select a development set from training set with almost the same number of links as testing set and choose the proper hyper - parameters , which are insensitive in a wide range .",
    "all the results are averaged over @xmath149 runs with random initializations and the same group of parameters .",
    "we first report the prediction performance ( auc scores ) on three relatively small networks .",
    "for fair comparison , we follow the previous settings to randomly choose @xmath150 of the links for training and use the remaining @xmath151 for testing .",
    "auc score is the area under the receiver operating characteristic ( roc ) curve ; higher is better .",
    "table [ table : nipsauc ] shows the auc scores on nips dataset , where the results of baselines ( i.e. , lfrm , irm , mmsb , medlfrm and bayesmedlfrm ) are cited from @xcite .",
    "we can see that both dlfrm@xmath152 and dlfrm@xmath153 outperform all other models , which suggests that our exact gibbs sampling with data augmentation can lead to more accurate models than medlfrm / bayesmedlfrm that uses the variational approximation methods with truncated mean - field assumptions .",
    "the stodlfrms obtain comparable results to dlfrms , which suggests that approximate sampler for @xmath21 using sgld is very effective . with sgld",
    ", we can improve efficiency without sacrificing performance which we will discuss later with table  [ table : aptime ] .",
    "furthermore , diagdlfrm@xmath152 and diagdlfrm@xmath153 also perform well , as they beat all other methods except ( sto)dlfrms . by using a lower dimensional @xmath21 derived from the diagonal weight matrix @xmath15 , diagdlfrm has the advantage of being computationally efficient , as shown in fig .",
    "[ fig : experiments](d ) .",
    "the good performance of stodlfrms and diagdlfrms suggests that we can use sgld with a full weight matrix or simply use a diagonal weight matrix on large - scale networks .",
    "[ table : nipsauc ]      for multi - relational kinship dataset , we consider the ",
    "single \" setting  @xcite , where we infer an independent set of latent features for each relation .",
    "the overall auc is obtained by averaging the results of all relations .",
    "as shown in table  [ table : nipsauc ] , both ( sto)dlfrm@xmath152 and ( sto)dlfrm@xmath153 outperform all other methods , which again proves the effectiveness of our methods .",
    "furthermore , the diagonal variants also obtain fairly good results , close to the best baselines .",
    "finally , the better results by the discriminative methods in general demonstrate the effect of regbayes on using various regularization parameters to deal with the imbalance issue ; fig .",
    "[ fig : experiments](b ) provides a detailed sensitivity analysis .",
    "we also examine how dlfrms perform on webkb network , which has rich text attributes  @xcite .",
    "our baselines include : 1 ) * katz * : a proximity measure between two entities  it directly sums over all collection of paths , exponentially damped by the path length to count short paths more heavily @xcite ; 2 ) * linear svm * : a supervised learning method using linear svm , where the feature for each link is a vector concatenating the bag - of - words features of two entities ; 3 ) * rbf - svm * : svm with the rbf kernel on the same features as the linear svm .",
    "we use svm - light  @xcite to train these classifiers ; and 4 ) * medlfrm * : state - of - the - art methods on learning latent features for link prediction  @xcite .",
    "note that we do nt compare with the relational topic models  @xcite , whose settings are quite different from ours . table  [ table : webkbauc ] shows the auc scores of various methods .",
    "we can see that : 1 ) both medlfrm and dlfrms perform better than svm classifiers on raw bag - of - words features , showing the promise of learning latent features for link prediction on document networks ; 2 ) dlfrms are much better than medlfrm 20 due to its inefficiency .",
    "] , suggesting the advantages of using data augmentation techniques for accurate inference over variational methods with truncated mean - field assumptions ; and 3 ) both stodlfrms and diagdlfrms achieve competitive results with faster speed .",
    "we now present results on two much larger networks .",
    "as the networks are much sparser , we randomly select @xmath154 of the positive links for training and the number of negative training links is @xmath155 times the number of positive training links . the testing set contains the remaining @xmath156 of the positive links and the same number of negative links , which we uniformly sample from the negative links outside the training set .",
    "this test setting is the same as that in @xcite .",
    "[ table : aptime ]      fig .",
    "[ fig : apauc ] presents the test auc scores , where the results of the state - of - the - art nonparametric models ammsb ( assortative mmsb ) and ahdpr ( assortative hdp relational model , a nonparametric generalization of ammsb ) are cited from @xcite .",
    "we can see that dlfrms achieve significantly better aucs than ammsb and ahdpr , which again demonstrates that our models can not only automatically infer the latent dimension , but also learn the effective latent features for entities .",
    "furthermore , stodlrms and diagdlfrms show larger benefits on the larger networks due to the efficiency .",
    "as shown in table  [ table : aptime ] , the time for sampling @xmath15 is greatly reduced with sgld .",
    "it only accounts for @xmath157 of the whole time for stodlfrm@xmath152 , while the number is @xmath158 for dlfrm@xmath152 .",
    "[ table : gowallaauc ]      finally , we test on the largest gowalla network , which is out of reach for many state - of - art methods , including lfrm , medlfrm and our dlfrms without sgld .",
    "some previous works combine the geographical information of gowalla social network to analyze user movements or friendships  @xcite , but we are not aware of any fairly comparable results for our setting of link prediction . here",
    ", we present the results of some proximitiy - measure based methods , including common neighbors ( * cn * ) , * jaccard * coefficient , and * katz*. as the network is too large to search for all the paths , we only concern the paths that shorter than @xmath159 for katz . as shown in previous results and fig .",
    "[ fig : experiments](d ) , dlfrms with logistic log - loss are more efficient and have comparable results of dlfrms with hinge loss , so we only show the results of stodlfrm@xmath152 and diagdlfrm@xmath152 .",
    "the auc scores and training time are shown in table  [ table : gowallaauc ] .",
    "we can see that stodlfrm@xmath152 outperforms all the other methods and diagdlfrm@xmath152 obtain competitive results .",
    "our diagdlfrm@xmath152 gets much better performance than the best baseline with less time .",
    "it shows that our models can also deal with the large - scale networks .",
    "we use nips network as an example to provide closer analysis .",
    "similar observations can be obtained in larger networks ( e.g. , astroph in appendix b ) , but taking longer time to run .",
    "[ fig : experiments](a ) shows the test auc scores w.r.t . the number of burn - in steps .",
    "we can see that all our variant models converge quickly to stable results .",
    "the diagdlfrm@xmath152 is a bit slower , but still within @xmath160 steps .",
    "these results demonstrate the stability of our gibbs sampler .",
    "to study how the regularization parameter @xmath36 handles the imbalance in real networks , we change the value of @xmath161 for dlfrm@xmath152 from @xmath135 to @xmath162 ( with all other parameters selected by the development set ) ; and report auc scores in fig .",
    "[ fig : experiments](b ) . the first point ( i.e. , @xmath163 ) corresponds to lfrm with our gibbs sampler , whose lower auc demonstrates the effectiveness of a larger @xmath161 to deal with the imbalance issue .",
    "we can see that the auc score increases when @xmath161 becomes larger and the prediction performance is stable in a wide range ( e.g. , @xmath164 ) .",
    "how large @xmath161 a network needs depends on its sparsity .",
    "a rule of thumb is that the sparser a network is , the larger @xmath161 it may prefer . the results also show that our setting ( @xmath165 ) is reasonable .",
    "[ fig : experiments](c ) shows the number of latent features automatically learnt by variant models .",
    "we can see that diagdlfrms generally need more features than dlfrms because the simplified weight matrix @xmath15 does nt consider pairwise interactions between features .",
    "moreover , dlfrm@xmath153 needs more features than dlfrm@xmath152 , possibly because of the non - smoothness nature of hinge loss .",
    "the small variance of each method suggests that the latent dimensions are stable in independent runs with random initializations .",
    "[ fig : experiments](d ) compares the training time .",
    "it demonstrates all our variant models are more efficient than medlfrm and bayesmedlfrm @xcite that use truncated mean - field approximation .",
    "compared to dlfrm@xmath152 , dlfrm@xmath153 takes more time to get the good auc .",
    "the reason is that dlfrm@xmath153 often converges slower ( see fig .",
    "[ fig : experiments](a ) ) with a larger latent dimension @xmath7 ( see fig . [",
    "fig : experiments](c ) ) .",
    "stodlfrms are more effective as we have discussed before .",
    "diagdlfrms are much more efficient due to the linear increase of training time per iteration with respect to @xmath7 .",
    "the testing time for all the methods are very little , omitted due to space limit .",
    "overall , dlfrms improve prediction performance and are more efficient in training , compared with other state - of - the - art nonparametric lfrms .",
    "we present discriminative nonparametric lfrms for link prediction , which can automatically resolve the unknown dimensionality of the latent feature space with a simple gibbs sampler using data augmentation ; unify the analysis for both logistic log - loss and hinge loss ; and deal with the imbalance issue in real networks .",
    "experimental results on a wide range of real networks demonstrate superior performance and scalability . for future work , we are interested in developing more efficient algorithms ( e.g. , using distributed computing ) to solve the link prediction problem in web - scale networks .",
    "the work was supported by the national basic research program ( 973 program ) of china ( nos . 2013cb329403 , 2012cb316301 ) , national nsf of china ( nos .",
    "61305066 , 61322308 , 61332007 ) , tnlist big data initiative , and tsinghua initiative scientific research program ( nos . 20121088071 , 20141080934 )",
    "for the case with logistic log - loss , we directly follow the data - augmentation strategy from  @xcite .",
    "let @xmath166 follow a polya - gamma distribution , denoted by @xmath167 , that is @xmath168where @xmath169 and @xmath170 are parameters and each @xmath171 is an independent gamma random variable .",
    "the main result of @xcite provides an alternative expression for the form of @xmath58 in eq .",
    "( [ fai1 ] ) by incorporating an augmented variable @xmath69 : @xmath172where @xmath173 and @xmath174 .    for the case with hinge loss , we take the advantage of data augmentation for support vector machines @xcite and @xmath59 in eq .",
    "( [ fai2 ] ) can be represented as a scale mixture of gaussian distributions : @xmath175where @xmath176 and @xmath110 is the augmented variable . by reformulating similar terms in eq .",
    "( [ hh1 ] ) , we have : @xmath177where @xmath178 and @xmath179 . given the results of eq .",
    "( [ proof1 ] ) and eq .",
    "( [ proof2 ] ) , lemma 1 holds true .",
    "[ fig : apexperiments](a ) shows the auc scores on testing data with respect to the number of burn - in steps on astroph dataset .",
    "we can observe that all our variant models converge quickly to stable results , similar as on nips dataset .",
    "our dlfrms with full weight matrix ( e.g. , dlfrm@xmath152 , dlfrm@xmath153 , stodlfrm@xmath152 and stodlfrm@xmath153 ) converge quickly within @xmath155 steps .",
    "the diagdlfrms need more steps to converge , but still within @xmath180 steps to converge to stable results .",
    "these results demonstrate the stability of our gibbs sampling algorithm .",
    "we analyze how the regularization parameter @xmath36 handles the imbalance in real networks using diagdlfrm@xmath152 , which is very efficient ( see fig .",
    "[ fig : apexperiments](d ) ) . following the settings on nips dataset",
    ", we change the ratio of @xmath161 for diagdlfrm@xmath152 from @xmath135 to @xmath162 with all the parameters selected by the development set . as shown in fig .",
    "[ fig : apexperiments](b ) , the auc score increases when @xmath161 becomes larger and the prediction performance is stable in a wide range ( e.g. , @xmath164 ) .",
    "these observations again demonstrate that using a larger @xmath144 than @xmath145 can effectively deal with the imbalance issue and our setting ( @xmath165 ) is reasonable .",
    "our variant models take the advantage of nonparametric technique to automatically learn the dimension of the latent features as shown in fig .",
    "[ fig : apexperiments](c ) .",
    "we can see that diagdlfrms generally need more features than dlfrms because the simplified weight matrix @xmath15 does not consider pairwise interactions between features .",
    "moreover , dlfrm@xmath153 needs more features than dlfrm@xmath152 , possibly because of the non - smoothness nature of hinge loss .",
    "the small variance of each method suggests that the latent dimensions are stable in independent runs with random initializations .",
    "the training time of our variant models on astroph dataset is shown in fig .  [ fig : apexperiments](d ) .",
    "we can see that for this relatively large network ( with tens of thousands of entities and millions of links ) , the least time we need to obtain the good auc score is only about @xmath181 seconds . as on nips dataset , dlfrm@xmath153 takes more time for training than dlfrm@xmath152 and this phenomenon is more obvious here due to the scalability of the network .",
    "the reason is that dlfrm@xmath153 often converges slower ( see fig  [ fig : apexperiments](a ) ) with a larger latent dimension @xmath7 ( see fig .",
    "[ fig : apexperiments](c ) ) . as discussed before ,",
    "stodlfrms are more effective .",
    "when a full weight matrix @xmath15 is used , training time per iteration increases exponentially with respect to @xmath7 .",
    "therefore , diagdlfrms are much more efficient due to the linear increase of training time per iteration with respect to @xmath7 ."
  ],
  "abstract_text": [
    "<S> we present a discriminative nonparametric latent feature relational model ( lfrm ) for link prediction to automatically infer the dimensionality of latent features . under the generic regbayes ( regularized bayesian inference ) </S>",
    "<S> framework , we handily incorporate the prediction loss with probabilistic inference of a bayesian model ; set distinct regularization parameters for different types of links to handle the imbalance issue in real networks ; and unify the analysis of both the smooth logistic log - loss and the piecewise linear hinge loss . </S>",
    "<S> for the nonconjugate posterior inference , we present a simple gibbs sampler via data augmentation , without making restricting assumptions as done in variational methods . </S>",
    "<S> we further develop an approximate sampler using stochastic gradient langevin dynamics to handle large networks with hundreds of thousands of entities and millions of links , orders of magnitude larger than what existing lfrm models can process . </S>",
    "<S> extensive studies on various real networks show promising performance . </S>"
  ]
}