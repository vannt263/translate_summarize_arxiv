{
  "article_text": [
    "modern cosmology is one of the most exciting areas in physical science .",
    "decades of surveying the sky have culminated in a cross - validated , `` cosmological standard model '' . yet ,",
    "key pillars of the model  dark matter and dark energy  together accounting for 95% of the universe s mass - energy remain mysterious  @xcite .",
    "deep fundamental questions demand answers : what is the dark matter",
    "? why is the universe s expansion accelerating ? what is the nature of primordial fluctuations ? should general relativity be modified ?        to address these questions , ground and space - based observatories operating at multiple wavebands  @xcite are aiming to unveil the true nature of the `` dark universe '' . driven by advances in semiconductor technology ,",
    "surveys follow a version of moore s law , in terms of ccd pixels or surveyed galaxies per year . in a major leap forward , current cosmological constraints will soon be improved by an order of magnitude  @xcite . as an example",
    ", the large synoptic survey telescope ( lsst )  @xcite can be compared to today s observations from the sloan digital sky survey ( sdss )  @xcite : in one night lsst will capture data equivalent to five years of sdss imaging ( fig .",
    "[ dls ] ) !",
    "interpreting future observations will be impossible without a modeling and simulation effort as revolutionary as the new surveys : the desired size and performance improvements for simulations over the next decade are measured in orders of magnitude  @xcite . because the simulations to be run are memory - limited on even the largest machines available and a large number of them are necessary , very stringent requirements are simultaneously imposed on code performance and efficiency .",
    "we show below how hacc meets these exacting conditions by attaining unprecedented sustained levels of performance , reaching up to @xmath0 of peak on certain bg / q partition sizes .",
    "cosmic structure formation is described by the gravitational vlasov - poisson equation in an expanding universe  @xcite , a 6-d pde for the liouville flow ( [ le ] ) of the phase space pdf where self - consistency is imposed by the poisson equation ( [ pe ] ) : @xmath1 the expansion of the universe is encoded in the time - dependence of the scale factor @xmath2 governed by the cosmological model , the hubble parameter , @xmath3 , @xmath4 is newton s constant , @xmath5 is the critical density , @xmath6 , the average mass density as a fraction of @xmath5 , @xmath7 is the local mass density , and @xmath8 is the dimensionless density contrast , @xmath9 the vlasov - poisson equation is very difficult to solve directly because of its high dimensionality and the development of structure  including complex multistreaming  on ever finer scales , driven by the gravitational jeans instability .",
    "consequently , n - body methods , using tracer particles to sample @xmath10 are used ; the particles follow newton s equations in an expanding universe , with the forces given by the gradient of the scalar potential as computed from eq .",
    "( [ pe ] )  @xcite .    under the jeans instability ,",
    "initial perturbations given by a smooth gaussian random field evolve into a ` cosmic web ' comprising of sheets , filaments , and local mass concentrations called halos  @xcite .",
    "the first stars and galaxies form in halos and then evolve as the halo distribution also evolves by a combination of dynamics , mass accretion and loss , and by halo mergers . to capture this complex behavior , cosmological n - body simulations",
    "have been developed and refined over the last three decades  @xcite .",
    "in addition to gravity , gasdynamic , thermal , radiative , and other processes must also modeled , e.g. , sub - grid modeling of star formation .",
    "large - volume simulations usually incorporate the latter effects via semi - analytic modeling .    to understand the essential nature of the challenge posed by future surveys , a few elementary arguments suffice .",
    "survey depths are of order a few gpc ( 1  @xmath11  light - years ) ; to follow typical galaxies , halos with a minimum mass of @xmath12@xmath13  m@xmath14 ( @xmath15  solar mass ) must be tracked . to properly resolve these halos ,",
    "the tracer particle mass should be @xmath12@xmath16  m@xmath14 and the force resolution should be small compared to the halo size , i.e. , @xmath12kpc .",
    "this last argument immediately implies a dynamic range ( ratio of smallest resolved scale to box size ) of a part in @xmath17 ( @xmath12gpc / kpc ) everywhere in the _ entire _ simulation volume ( fig .",
    "[ zoom ] ) .",
    "the mass resolution can be specified as the ratio of the mass of the smallest resolved halo to that of the most massive , which is @xmath12@xmath18 . in terms of the number of simulation particles ,",
    "this yields counts in the range of hundreds of billions to trillions .",
    "time - stepping criteria follow from a joint consideration of the force and mass resolution  @xcite .",
    "finally , stringent requirements on accuracy are imposed by the very small statistical errors in the observations ",
    "certain quantities such as lensing shear power spectra must be computed at accuracies of a _ fraction _ of a percent  @xcite .        for a cosmological simulation to be considered `` high - resolution '' , _ all _ of the above demands",
    "must be met .",
    "in addition , throughput is a significant concern .",
    "scientific inference from sets of cosmological observations is a statistical inverse problem where many runs of the forward problem are needed to obtain estimates of cosmological parameters via markov chain monte carlo methods .",
    "for many analyses , hundreds of large - scale , state of the art simulations will be required  @xcite .",
    "the structure of the hacc framework is based on the realization that it must not only meet the challenges of spatial dynamic range , mass resolution , accuracy , and throughput , but also overcome a final hurdle , i.e. , be fully cognizant of coming disruptive changes in computational architectures . as a validation of its design philosophy ,",
    "hacc was among the pioneering applications proven on the heterogeneous architecture of roadrunner  @xcite , the first supercomputer to break the petaflop barrier .",
    "hacc s multi - algorithmic structure also attacks several weaknesses of conventional particle codes including limited vectorization , indirection , complex data structures , lack of threading , and short interaction lists .",
    "it combines mpi with a variety of local programming models ( opencl , openmp ) to readily adapt to different platforms .",
    "currently , hacc is implemented on conventional and cell / gpu - accelerated clusters , on the blue gene architecture , and is running on prototype intel mic hardware .",
    "hacc is the first , and currently the only large - scale cosmology code suite world - wide , that can run at scale ( and beyond ) on _ all _ available supercomputer architectures .    to showcase this flexibility , we present scaling results for two systems aside from the bg / q in section  [ sec : results ]",
    "; on the entire anl bg / p system and over all of roadrunner .",
    "recent hacc science results on roadrunner include a suite of 64  billion particle runs for baryon acoustic oscillations predictions for boss ( baryon oscillation spectroscopic survey )  @xcite and a high - statistics study of galaxy cluster halo profiles  @xcite .",
    "hacc s performance and flexibility are not dependent on vendor - supplied or other high - performance libraries or linear algebra packages ; the 3-d parallel fft implementation in hacc couples high performance with a small memory footprint as compared to available libraries .",
    "unlike some other high - performance n - body codes , hacc does not use any special hardware .",
    "the implementation for the bg / q architecture has far more generally applicable features than ( the hacc or other ) cpu / gpu short - range force implementations .",
    "the cosmological n - body problem is typically treated by a mix of grid and particle - based techniques .",
    "the hacc design accepts that , as a general rule , particle and grid methods both have their limitations . for physics and algorithmic reasons ,",
    "grid - based techniques are better suited to larger ( ` smooth ' ) lengthscales , with particle methods having the opposite property .",
    "this suggests that higher levels of code organization should be grid - based , interacting with particle information at a lower level of the computational hierarchy .    following this central idea",
    ", hacc uses a hybrid parallel algorithmic structure , splitting the gravitational force calculation into a specially designed grid - based long / medium range spectral particle - mesh ( pm ) component that is common to all architectures , and an architecture - tunable particle - based short / close - range solver ( fig .  [ haccforce ] ) .",
    "the grid is responsible for 4 orders of magnitude of dynamic range , while the particle methods handle the critical 2 orders of magnitude at the shortest scales where particle clustering is maximal and the bulk of the time - stepping computation takes place .",
    "the computational complexity of the pm algorithm  @xcite is @xmath19+@xmath20 , where @xmath21 is the total number of particles , and @xmath22 the total number of grid points .",
    "the short - range tree algorithms  @xcite in hacc can be implemented in ways that are either @xmath23 or @xmath24 , where @xmath25 is the number of particles in individual spatial domains ( @xmath26 ) , while the close - range force computations are @xmath27 where @xmath28 is the number of particles in a tree leaf node within which all direct interactions are summed .",
    "@xmath28 values can range from @xmath29 in a ` fat leaf ' tree , to as large as @xmath30 in the case of a cpu / gpu implementation ( no mediating tree ) .",
    "hacc uses mixed precision computation ",
    "double precision is used for the spectral component of the code , whereas single precision is adequate for the short / close - range particle force evaluations and particle time - stepping .",
    "hacc s long / medium range algorithm is based on a fast , spectrally filtered pm method .",
    "the density field is generated from the particles using a cloud - in - cell ( cic ) scheme  @xcite , but is then smoothed with the ( isotropizing ) spectral filter @xmath31^{n_s } , \\label{filter}\\ ] ] with the nominal choices @xmath32 , @xmath33 .",
    "this reduces the anisotropy `` noise '' of the cic scheme by over an order of magnitude without requiring complex and inflexible higher - order spatial particle deposition methods .",
    "the noise reduction allows matching the short and longer - range forces at a spacing of 3 grid cells , with important ramifications for performance .",
    "the poisson solver uses a sixth - order , periodic , influence function ( spectral representation of the inverse laplacian )  @xcite .",
    "the gradient of the scalar potential is obtained using higher - order spectral differencing ( fourth - order super - lanczos  @xcite ) .",
    "the `` poisson - solve '' in hacc is the composition of all the kernels above in one single fourier transform ; each component of the potential field gradient then requires an independent fft .",
    "hacc uses its own scalable , high performance 3-d fft routine implemented using a 2-d pencil decomposition ( details are given in section  [ sec : results ] . )    to obtain the short - range force , the filtered grid force is subtracted from the exact newtonian force .",
    "the filtered grid force was obtained numerically to high accuracy using randomly sampled particle pairs and then fitted to an expression with the correct large and small distance asymptotics . because this functional form is needed only over a small , compact region , it can be simplified using a fifth - order polynomial expansion to speed up computations in the main force kernel ( section  [ sec : bg ] ) .",
    "hacc s spatial domain decomposition is in regular ( non - cubic ) 3-d blocks , but unlike the guard zones of a typical pm method , full particle replication",
    " termed ` particle overloading '  is employed across domain boundaries ( fig",
    ".  [ overload ] ) .",
    "the typical memory overhead cost for a large run is @xmath34 .",
    "the point of overloading is to allow essentially exact medium / long - range force calculations with no communication of particle information and high - accuracy local force calculations with relatively sparse refreshes of the overloading zone ( for details , see ref .",
    "the second advantage of overloading is that it frees the local force solver from handling communication tasks , which are taken care of by the long / medium - range force framework .",
    "thus new ` on - node ' local methods can be plugged in with guaranteed scalability and only local optimizations are necessary .",
    "note that all short - range methods in hacc are local to the mpi - rank and the locality can be fine - grained further .",
    "this locality has the key advantage of lowering the number of levels in tree algorithms and being able to parallelize across fine - grained particle interaction sub - volumes .",
    "the time - stepping in hacc is based on a 2nd - order split - operator symplectic scheme that sub - cycles the short / close - range evolution within long / medium - range ` kick ' maps where particle positions do not change but the velocities are updated .",
    "the relatively slowly evolving longer range force is effectively frozen during the shorter - range time steps , which are a symmetric ` sks ' composition of stream ( position update , velocity fixed ) and kick maps for the short / close - range forces  @xcite : @xmath35 the number of sub - cycles can vary , depending on the force and mass resolution of the simulation , from @xmath36 .    the long / medium - range solver remains unchanged across all architectures .",
    "the short / close - range solvers are chosen and optimized depending on the target architecture .",
    "these solvers can use direct particle - particle interactions , i.e. , a p@xmath37 m algorithm  @xcite , as on roadrunner , or use both tree and particle - particle methods as on the ibm bg / p and bg / q ( ` pptreepm ' ) .",
    "the availability of multiple algorithms within the hacc framework allows us to carry out careful error analyses , for example , the p@xmath37 m and the pptreepm versions agree to within @xmath38 for the nonlinear power spectrum test in the code comparison suite of ref .",
    "@xcite .    for heterogeneous systems such as roadrunner , or in the near future , titan at olcf",
    ", the long / medium - range spectral solver operates at the cpu layer . depending on the memory balance between the cpu and the accelerator , we can choose to specify two different modes , 1 ) grids held on the cpu and particles on the accelerator , or 2 ) a streaming paradigm with grid and particle information primarily resident in cpu memory with computations streamed through the accelerator . in both cases , the local force solve is a direct particle - particle interaction , i.e. , the whole is a p@xmath37 m code with hardware acceleration . for a many - core system ,",
    "the top layer of the code remains the same , but the short - range solver changes to a tree - based algorithm which is much better suited to the blue gene and mic architectures",
    ". we will provide an in - depth description of our blue gene / q - specific implementation in section  [ sec : bg ] .",
    "to summarize , the hacc framework integrates multiple algorithms and optimizes them across architectures ; it has several interesting performance - enhancing features , e.g. , overloading , spectral filtering and differentiation , mixed precision , and compact local trees .",
    "hacc attacks the weaknesses of conventional particle codes in ways made fully explicit in the next section  lack of vectorization , indirection , complex data structures , lack of threading , and short interaction lists .",
    "finally , weak scaling is a function only of the spectral solver ; hacc s 2-d domain decomposed fft guarantees excellent performance and scaling properties ( see section  [ sec : results ] ) .",
    "the bg / q is the third generation of the ibm blue gene line of supercomputers targeted primarily at large - scale scientific applications , continuing the tradition of optimizing for price performance , scalability , power efficiency , and system reliability  @xcite .",
    "the new bg / q compute chip ( bqc ) is a system - on - chip ( soc ) design combining cpus , caches , network , and messaging unit on a single chip  @xcite .",
    "a single bg / q rack contains 1024 bg / q nodes like its predecessors .",
    "each node contains the bqc and 16  gb of ddr3 memory .",
    "each bqc uses 17 augmented 64-bit powerpc a2 cores with specific enhancements for the bg / q : 1 ) 4 hardware threads and a simd quad floating point unit ( quad processor extension , qpx ) , 2 ) a sophisticated l1 prefetching unit ( l1p ) with both stream and list prefetching , 3 ) a wake - up unit to reduce certain thread - to - thread interactions , and 4 ) transactional memory and speculative execution . of the 17 bqc cores , 16 are for user applications and one for handling os interrupts and other system services .",
    "each core has access to a private 16  kb l1 data cache and a shared 32  mb multi - versioned l2 cache connected by a crossbar .",
    "the a2 core runs at 1.6  ghz and the qpx allows for 4 fmas per cycle , translating to a peak performance per core of 12.8  gflops , or 204.8  gflops for the bqc chip .",
    "the bg / q network has a 5-d torus topology ; each compute node has 10 communication links with a peak total bandwidth of 40  gb / s  @xcite .",
    "the internal bqc interconnect has a bisection bandwidth of 563  gb / s .    in order to evaluate the short - range force on non - accelerated systems , such as the bg / q ,",
    "hacc uses a recursive coordinate bisection ( rcb ) tree in conjunction with a highly - tuned short - range polynomial force kernel . the implementation of the rcb tree , although not the force evaluation scheme , generally follows the discussion in ref .",
    "two core principles underlie the high performance of the rcb tree s design .",
    "_ spatial locality . _",
    "the rcb tree is built by recursively dividing particles into two groups .",
    "the dividing line is placed at the center of mass coordinate perpendicular to the longest side of the box . once this line is chosen , the particles are partitioned such that particles in each group occupy disjoint memory buffers .",
    "local forces are then computed one leaf node at a time .",
    "the net result is that the particle data exhibits a high degree of spatial locality after the tree build ; because the computation of the short - range force on the particles in any given leaf node , by construction , deals with particles only in nearby leaf nodes , the cache miss rate during the force computation is extremely low .    _",
    "walk minimization .",
    "_ in a traditional tree code , an interaction list is built and evaluated for each particle . while the interaction list size scales only logarithmically with the total number of particles ( hence the overall @xmath39 complexity ) ,",
    "the tree walk necessary to build the interaction list is a relatively slow operation .",
    "this is because it involves the evaluation of complex conditional statements and requires `` pointer chasing '' operations .",
    "a direct @xmath40 force calculation scales poorly as @xmath41 grows , but for a small number of particles , a thoughtfully - constructed kernel can still finish the computation in a small number of cycles .",
    "the rcb tree exploits our highly - tuned short - range force kernels to decrease the overall force evaluation time by shifting workload away from the slow tree - walking and into the force kernel .",
    "up to a point , doing this actually speeds up the overall calculation : the time spent in the force kernel goes up but the walk time decreases faster .",
    "obviously , at some point this breaks down , but on many systems , tens or hundreds of particles can be in each leaf node before the crossover is reached .",
    "we point out that the force kernel is generally more efficient as the size of the interaction list grows : the relative loop overhead is smaller , and more of the computation can be done using unrolled vectorized code .",
    "in addition to the performance benefits of grouping multiple particles in each leaf node , doing so also increases the accuracy of the resulting force calculation : the local force is dominated by nearby particles , and as more particles are retained in each leaf node , more of the force from those nearby particles is calculated exactly . in highly - clustered regions ( with very many nearby particles ) ,",
    "the accuracy can increase by several orders of magnitude when keeping over 100 particles per leaf node .",
    "another important consideration is the tree - node partitioning step , which is the most expensive part of the tree build .",
    "the particle data is stored as a collection of arrays  the so - called structure - of - arrays ( soa ) format .",
    "there are three arrays for the three spatial coordinates , three for the velocity components , in addition to arrays for mass , a particle identifier , etc .",
    "our implementation in hacc divides the partitioning operation into three phases .",
    "the first phase loops over the coordinate being used to divide the particles , recording which particles will need to be swapped .",
    "next , these prerecorded swapping operations are performed on six of the arrays .",
    "the remaining arrays are identically handled in the third phase .",
    "dividing the work in this way allows the hardware prefetcher to effectively hide the memory transfer latency during the particle partitioning operation and reduces expensive read - after - write dependencies .",
    "we now turn to the evaluation of the bg / q - specific short - range force kernel , where the code spends the bulk of its computation time .",
    "due to the compactness of the short - range interaction ( cf .",
    "section  [ sec : hacc ] ) , the kernel can be represented as @xmath42 where @xmath43 , @xmath44(s)$ ] , and @xmath45 is a short - distance cutoff .",
    "this computation must be vectorized to attain high performance ; we do this by computing the force for every neighbor of each particle at once .",
    "the list of neighbors is generated such that each coordinate and the mass of each neighboring particle is pre - generated into a contiguous array .",
    "this guarantees that 1 ) every particle has an independent list of particles and can be processed within a separate thread ; and 2 ) every neighbor list can be accessed with vector memory operations , because contiguity and alignment restrictions are taken care of in advance .",
    "every particle on a leaf node shares the interaction list , therefore all particles have lists of the same size , and the computational threads are automatically balanced .    the filtering of @xmath46 , i.e. , checking the short - range condition , can be processed during the generation of the neighbor list or during the force evaluation itself ; since the condition is likely violated only in a number of `` corner '' cases , it is advantageous to include it into the force evaluation in a form where ternary operators can be combined to remove the need of storing a value during the force computation .",
    "each ternary operator can be implemented with the help of the instruction , which also has a vector equivalent .",
    "even though these alterations introduce an ( insignificant ) increase in instruction count , the entire force evaluation routine becomes fully vectorizable .    on the bg / q ,",
    "the instruction latency is 6 cycles for most floating - point instructions ; latency is hidden from instruction dependencies by a combination of placing the dependent instructions as far as 6 instructions away by using 2-fold loop unrolling and running 4 threads per core .",
    "register pressure for the 32 vector floating - point registers is the most important design constraint on the kernel .",
    "half of these registers hold values common to all iterations , 6 of which store the coefficients of the polynomial .",
    "the remaining registers hold iteration - specific values .",
    "because of the 2-fold unrolling , this means that we are restricted to 8 of these registers per iteration . evaluating the force in eq .",
    "( [ force ] ) requires a reciprocal square root estimate and evaluating a fifth - order polynomial , but these require only 5 and 2 iteration - specific registers respectively .",
    "the most register - intensive phase of the kernel loop is actually the calculation of @xmath46 , requiring 3 registers for the particle coordinates , 3 registers for the components of @xmath47 , and one register for accumulating the value of @xmath46 .",
    "there is significant flexibility in choosing the number of mpi ranks versus the number of threads on an individual bg / q node . because of the excellent performance of the memory sub - system and the low overhead of context switching ( due to use of the bqc wake - up unit ) , a large number of openmp threads ",
    "significantly larger than is considered typical  can be run to optimize performance .",
    "figure  [ threads ] shows how increasing the number of threads per core increases the performance of the force kernel as a function of the size of the particle neighbor list .",
    "the best performance is attained when running the maximum number of threads ( 4 ) per core , and at large neighbor list size . the optimum value for the current hacc runs turns out to be 16/4 as it allows for short tree walks , efficient fft computation , and a large fraction of time devoted to the force kernel . the numbers in fig .",
    "[ threads ] show that for runs with different parameters , such as high particle loading , the broad performance plateau allows us to use smaller or higher rpn / thread ratios as appropriate .",
    "the neighbor list sizes in representative simulations are of order 500 - 2500 . at the lower end of this neighbor list size",
    ", performance can be further improved using assembly - level programming , if desired .    at the chosen 16/4 operating point",
    ", the code spends 80% of the time in the highly optimized force kernel , 10% in the tree walk , and 5% in the fft , all other operations ( tree build , cic deposit ) adding up to another 5% .",
    "note that the actual fraction of peak performance attained in the force kernel is close to 80% as against a theoretical maximum value of 81% .",
    "the 26 instructions in the kernel correspond to a maximum of 208 flops if they were all fmas , whereas in the actual implementation , 16 of them are fmas yielding a total flop count of @xmath48 implying a theoretical maximum value of @xmath49 or 81% of peak .",
    "the high efficiency is due to successful use of the stream prefetch ; we have measured the latency of the l2 cache to be approximately 45 clock cycles , thus even a single miss per iteration would be enough to significantly degrade the kernel performance .",
    "we present performance data for three cases : 1 ) weak scaling of the poisson solver up to 131,072 ranks on different architectures for illustrative purposes ; 2 ) weak scaling at @xmath50 parallel efficiency for the full code on the bg / q with up to 1,572,864 cores ( 96 racks ) ; and 3 ) strong scaling of the full code on the bg / q up to 16,384 cores with a fixed - size realistic problem to explore future systems with lower memory per core .",
    "all the timing data were taken over many tens of sub - steps , each individual run taking between 4 - 5 hours .    to summarize our findings ,",
    "both the long / medium range solver and the full code exhibit perfect weak scaling out to the largest system we have had access to so far ; we achieved a performance of 13.94  pflops on 96 racks , at around 67 - 69% of peak in most cases ( up to 69.75% ) . the full code demonstrates strong scaling up to one rack on a problem with 1024@xmath37 particles .",
    "finally , the biggest test run evolved more than 3.6  trillion particles ( 15,360@xmath37 ) , exceeding by more than an order of magnitude , the largest high - resolution cosmology run performed to date . as discussed in section  [ science ] , the results of runs at this scale can be used for many scientific investigations .      .fft scaling on up to 10240@xmath37 grid points on the bg / q [ cols=\"^,^,^\",options=\"header \" , ]     the evolution of many - core based architectures is strongly biased towards a large number of ( possibly heterogeneous ) cores per compute node .",
    "it is likely that the ( memory ) byte / flop ratio could easily evolve to being worse by a factor of 10 than it is for the bg / q , and this continuing trend will be a defining characteristic of exascale systems . for these future - looking reasons  the arrival of the strong - scaling barrier for large - scale codes  and for optimizing wall - clock for fixed problem size , it is important to study the robustness of the strong scaling properties of the hacc short / close - range algorithms",
    ".    we designed the test with a @xmath51 particle problem running on one rack from 512 to 16384 cores , spanning a per node memory utilization factor of approximately 57% , a typical production run value , to as low as 7% .",
    "the actual memory utilization factor scales by a factor of 8 , instead of 32 , because on 16384 nodes we are running a ( severely ) unrealistically small simulation volume per rank with high particle overloading memory and compute cost . despite this ` abuse ' of the hacc algorithms , which are designed to run at @xmath52 of per node memory utilization to about a factor of 4 less ( @xmath53 ) , the strong scaling performance , as depicted in fig .",
    "7 ( associated data in table  [ tab : perf3 ] ) is impressive .",
    "the performance stays near - ideal throughout , as does the push - time per particle per step up to 8192 cores , slowing down at 16384 cores , only because of the extra computations in the overloaded regions .",
    "therefore , we expect the basic algorithms to work extremely well in situations where the byte / flop ratio is significantly smaller than that of the current optimum plateau for the bg / q .",
    "dark energy is one of the most pressing puzzles in physics today  @xcite .",
    "hacc will be used to investigate the signatures of different dark energy models in the detail needed to analyze upcoming cosmological surveys .",
    "the cosmology community has mostly focused on one cosmological model and a handful of ` hero ' runs to study it  @xcite . with hacc",
    ", we aim to systematically study dark energy model space at extreme scales and derive not only qualitative signatures of different dark energy scenarios but deliver quantitative predictions of unprecedented accuracy urgently needed by the next - generation surveys .",
    "the simulations can be used to interpret observations of various kinds , such as weak gravitational lensing measurements to map the distribution of dark matter in the universe , measurements of the distribution of galaxies and clusters , from the largest to the smallest scales , measurements of the growth and distribution of cosmic structure , gravitational lensing of the cosmic microwave background , and many more .",
    "we now show illustrative results from a science test run on 16 racks of mira , the bg / q system now under acceptance testing at the alcf . in this simulation , 10240@xmath37 particles",
    "are evolved in a ( 9.14  gpc)@xmath37 volume box .",
    "this leads to a particle mass , @xmath54  m@xmath14 , allowing us to resolve halos that host , e.g. , luminous red galaxies ( lrgs ) , a main target of the sloan digital sky survey .",
    "the test simulation was started at an initial redshift of @xmath55 ( our actual science runs have @xmath56 ) and evolved until today ( redshift @xmath57 ) .",
    "we stored a slice of the three - dimensional density at the final time ( only a small file system was available during this run ) , as well as a subset of the particles and the mass fluctuation power spectrum at 10 intermediate snapshots .",
    "the total run time of the simulation was approximately 14 hours .",
    "as the evolution proceeds , the particle distribution transitions from essentially uniform to extremely clustered ( see fig .  [ evolv ] ) .",
    "the local density contrast @xmath58 can increase by five orders of magnitude during the evolution .",
    "nevertheless , the wall - clock per time step does not change much over the entire simulation .",
    "the large dynamic range of the simulation is demonstrated in fig .",
    "the outer image shows the full simulation volume ( 9.14  gpc on a side ) . in this case , structures are difficult to see because the visualization can not encompass the dynamic range of the simulation . successively zooming into smaller regions , down to a ( 7  mpc)@xmath37 sub - volume holding a large dark matter - dominated halo gives some impression of the enormous amount of information contained in the simulation .",
    "the zoomed - in halo corresponds to a cluster of galaxies in the observed universe . note that in this region the actual ( formal ) force resolution of the simulation is 0.007  mpc , a further factor of 1000 smaller than the sub - volume size !",
    "a full simulation of the type described is extremely science - rich and can be used in a variety of ways to study cosmology as well as to analyze available observations .",
    "below we give two examples of the kind of information that can be gained from large scale structure simulations .",
    "( we note that the test run is more than three times bigger than the largest high - resolution simulation available today . )",
    "clusters are very useful probes of cosmology  as the largest gravitationally bound structures in the universe , they form very late and are hence sensitive probes of the late - time acceleration of the universe  @xcite .",
    "figure  [ cluster ] gives an example of the detailed information available in a simulation , allowing the statistics of halo mergers and halo build - up through sub - halo accretion to be studied with excellent statistics .",
    "in addition , the number of clusters as a function of their mass ( the mass function ) , is a powerful cosmological probe .",
    "simulations provide precision predictions that can be compared to observations .",
    "the new hacc simulations will not only predict the mass function ( as a function of cosmological models ) at unprecedented accuracy , but also the probability of finding very massive clusters in the universe .",
    "large - volume simulations are required to determine the abundance of these very rare objects reliably  @xcite .",
    "cosmological information resides in the nature of material structure and also in how structures grow with time . to test whether general relativity correctly describes the dynamics of the universe ,",
    "information related to structure growth ( evolution of clustering ) is essential .",
    "figure  [ evolv ] shows how structure evolves in the simulation .",
    "large - volume simulations are essential in producing predictions for statistical quantities such as galaxy correlation functions and the associated power spectra  with small statistical errors  in order to compare the predictions against observations .",
    "figure  [ power ] shows how the power spectrum evolves as a function of time in the science test run . at small wavenumbers ,",
    "the evolution is linear , but at large wavenumbers it is highly nonlinear , and can not be obtained by any method other than direct simulation .    to summarize , armed with large - scale simulations we can study and evaluate many cosmological probes .",
    "these probes involve the statistical measurements of the matter distribution at a given epoch ( such as the power spectrum and the mass function ) as well as their evolution .",
    "in addition , the occurrence of rare objects such as very massive clusters can be investigated in the simulations we will carry out with hacc .",
    "these are exciting times for users of the bg / q : in the us , two large systems are undergoing acceptance at livermore ( sequoia , 96 racks ) and at argonne ( mira , 48 racks ) .",
    "as shown here , hacc scales easily to 96 racks .",
    "our next step is to exploit the power of the new systems with the aim of carrying out a suite of full science runs with hundreds of billions to trillions of simulation particles .",
    "because hacc s performance and scalability do not rely on the use of vendor - supplied or other ` black box ' high - performance libraries or linear algebra packages , it retains the key advantage of allowing code optimization to be a continuous process ; we have identified several options to enhance the performance as reported here .",
    "an initial step is to fully thread all the components of the long - range solver , in particular the forward cic algorithm .",
    "next , we will improve ( nodal ) load balancing by using multiple trees at each rank , enabling an improved threading of the tree - build . while the force kernel already runs at very high performance",
    ", we can improve it further with lower - level implementations in assembly .",
    "many of the ideas and methods presented here are relatively general and can be re - purposed to benefit other hpc applications .",
    "in addition , hacc s extreme continuous performance contrasts with the more bursty stressing of the bg / q architecture by linpack ; this feature has allowed hacc to serve as a valuable stress test in the mira and sequoia bring - up process .    to summarize ,",
    "we have demonstrated outstanding performance at close to 14  pflops on the bg / q ( 69% of peak ) using more than 1.5 million cores and mpi ranks , at a concurrency level of 6.3 million .",
    "we are now ready to carry out detailed large - volume n - body cosmological simulations at the size scale of trillions of particles .",
    "we are indebted to bob walkup for running hacc on a prototype bg / q system at ibm and to dewey dasher for help in arranging access . at anl",
    ", we thank susan coghlan , paul messina , mike papka , rick stevens , and tim williams for obtaining allocations on different blue gene systems . at llnl , we are grateful to brian carnes , kim cupps , david fox , and michel mccoy for providing access to sequoia .",
    "we thank ray loy and venkat vishwanath for their contributions to system troubleshooting and parallel i / o .",
    "we acknowledge the efforts of the alcf operations team for their assistance in running on mira and the veas bg / q system , in particular , to paul rich , adam scovel , tisha stacey , and william scullin for their tireless efforts to keep mira and veas up and running and helping us carry out the initial long - duration science test runs .",
    "this research used resources of the alcf , which is supported by doe / sc under contract de - ac02 - 06ch11357 .",
    "albrecht ,  a. , g.  bernstein , r.  cahn , w.l .",
    "freedman , j.  hewitt , w.  hu , j.  huth , m.  kamionkowski , e.w .",
    "kolb , l.  knox , j.c .",
    "mather , s.  staggs , n.b .",
    "suntzeff , dark energy task force report , arxiv : astro - ph/0609591v1 .",
    "chen ,  d. , n.a .",
    "eisley , p.  heidelberger , r.m .",
    "senger , y.  sugawara , s.  kumar , v.  salapura , d.l .",
    "satterfield , b.  steinmacher - burowin , and j.j .",
    "parker , in sc11 , proceedings of the 2011 int .",
    "conf . high performance computing networking storage and analysis ( 2011 ) .",
    "doe ascr and doe hep joint extreme scale computing report ( 2009 ) ",
    "`` challenges for understanding the quantum universe and the role of computing at the extreme scale '' , http://extremecomputing.labworks.org/highenergyphysics/index.stm                    haring ,  r.a . , m.  ohmacht , t.w .",
    "fox , m.k .",
    "gschwind , p.a .",
    "boyle , n.h .",
    "christ , c.  kim , d.l .",
    "satterfield , k.  sugavanam , p.w .",
    "coteus , p.  heidelberger , m.a .",
    "blumrich , r.w .",
    "wisniewski , a.  gara , and g.l .",
    "chiu , ieee micro * 32 * , 48 ( 2012 ) .",
    "menanteau ,  f. , j.p .",
    "hughes , c.  sifon , m.  hilton , j.  gonzalez , l.  infante , l.f .",
    "barrientos , a.j .",
    "baker , j.r .",
    "bond , s.  das , m.j .",
    "devlin , j.  dunkley , a.  hajian , a.d .",
    "hincks , a.  kosowsky , d.  marsden , t.a .",
    "marriage , k.  moodley , m.d .",
    "niemack , m.r .",
    "nolta , l.a .",
    "page , e.d .",
    "reese , n.  sehgal , j.  sievers , d.n .",
    "spergel , s.t .",
    "staggs , e.  wollack , astrophys .",
    "j. * 748 * , 1 ( 2012 ) .",
    "pfalzner ,  s. , and p.  gibbon , _ many - body tree methods in physics _ ( cambridge university press , 1996 ) ; see also barnes ,  j. , and p.  hut , nature * 324 * , 446 ( 1986 ) ; warren ,  m.s . and j.k",
    ".  salmon , technical paper , supercomputing 1993 .",
    "wittman ,  d.m .",
    "tyson , i.p .",
    "dellantonio , a.c .",
    "becker , v.e .",
    "margoniner , j.  cohen , d.  norman , d.  loomba , g.  squires , g.  wilson , c.  stubbs , j.  hennawi , d.  spergel , p.  boeshaar , a.  clocchiatti , m.  hamuy , g.  bernstein , a.  gonzalez , p.  guhathakurta , w.  hu , u.  seljak , and d.  zaritsky , arxiv : astro - ph/0210118 ."
  ],
  "abstract_text": [
    "<S> remarkable observational advances have established a compelling cross - validated model of the universe . yet </S>",
    "<S> , two key pillars of this model  dark matter and dark energy  remain mysterious . </S>",
    "<S> sky surveys that map billions of galaxies to explore the ` dark universe ' , demand a corresponding extreme - scale simulation capability ; the hacc ( hybrid / hardware accelerated cosmology code ) framework has been designed to deliver this level of performance now , and into the future . with its novel algorithmic structure , hacc allows flexible tuning across diverse architectures , including accelerated and multi - core systems .    on the ibm bg / q </S>",
    "<S> , hacc attains unprecedented scalable performance  </S>",
    "<S> currently 13.94 pflops at 69.2% of peak and 90% parallel efficiency on 1,572,864 cores with an equal number of mpi ranks , and a concurrency of 6.3 million . </S>",
    "<S> this level of performance was achieved at extreme problem sizes , including a benchmark run with more than 3.6 trillion particles , significantly larger than any cosmological simulation yet performed . </S>"
  ]
}