{
  "article_text": [
    "several new estimation methods have been recently proposed for the linear regression model with observation error in the design .",
    "such problems arise in a variety of applications , see @xcite . in this work",
    "we consider the following regression model with observation error in the design : @xmath2 here the random vector @xmath3 and the random @xmath4 matrix @xmath5 are observed , the @xmath4 matrix @xmath6 is unknown , @xmath7 is an @xmath4 random noise matrix , and @xmath8 is a random noise vector .",
    "the vector of unknown parameters of interest is @xmath9 which is assumed to belong to a given convex subset @xmath10 of @xmath11 characterizing some prior knowledge about @xmath9 ( potentially @xmath12 ) .",
    "similarly to the recent literature on this topic , we consider the setting where the dimension @xmath13 can be much larger than the sample size @xmath14 and the vector @xmath9 is @xmath15-sparse , which means that it has not more than @xmath15 non - zero components .",
    "+ the need for new estimators under errors in the design arises from the fact that standard estimators ( e.g. lasso and dantzig selector ) might become unstable , see @xcite . to deal with this framework ,",
    "various assumptions have been considered , leading to different estimators . +",
    "a classical assumption in the literature is a uniform boundedness condition on the errors in the design , namely , @xmath16 denotes the @xmath17-norm for @xmath18 .",
    "note that this assumption allows for various dependences between the errors in the design . in this",
    "setting , the matrix uncertainty selector ( mu selector ) , which is robust to the presence of errors in the design , is proposed in @xcite .",
    "the mu selector @xmath19 is defined as a solution of the minimization problem @xmath20 where the parameters @xmath21 and @xmath22 depend on the level of the noises of @xmath7 and @xmath23 respectively . under appropriate choices of these parameters and suitable assumptions on @xmath6 ,",
    "it was shown in @xcite that with probability close to 1 , @xmath24 different positive constants that do not depend on @xmath9 , @xmath15 , @xmath14 , @xmath13 , @xmath25 .",
    "the result ( [ rtmu ] ) implies consistency as the sample size @xmath14 tends to infinity provided that the error in the design goes to zero sufficiently fast to offset @xmath26 , and the number of variables @xmath13 and the sparsity @xmath15 of @xmath27 do not grow too fast relative to the sample size @xmath14 .",
    "+ an alternative assumption considered in the literature is that the entries of the random matrix @xmath7 are independent with zero mean , the values @xmath28 are finite , and data - driven estimators @xmath29 of @xmath30 are available converging with an appropriate rate .",
    "this assumption motivated the idea to compensate the bias of using the observable @xmath31 instead of the unobservable @xmath32 in thanks to the estimates of @xmath30 .",
    "this compensated mu selector , introduced in @xcite and denoted as @xmath33 , is defined as a solution of the minimization problem @xmath34 where @xmath35 is the diagonal matrix with entries @xmath36 and @xmath37 and @xmath38 are constants chosen according to the level of the noises and the accuracy of the @xmath39 .",
    "+ rates of convergence of the compensated mu selector were established in @xcite .",
    "importantly , the compensated mu selector can be consistent as the sample size @xmath14 increases even if the error in the design does not vanish .",
    "this is in contrast to the case of the mu selector , where the bounds are small only if the bound on the design error @xmath0 is small . in particular , under regularity conditions , when @xmath27 is @xmath15-sparse , it is shown in @xcite that with probability close to 1 @xmath40 has been recently proposed and analyzed in @xcite .",
    "the estimator @xmath41 is defined as the first component of any solution of the optimization problem @xmath42 where @xmath43 , @xmath21 and @xmath22 are some positive tuning constants .",
    "akin to @xmath33 , this estimator compensates for the bias by using the estimators @xmath36 of @xmath30 .",
    "however it exploits a combination of @xmath44 and @xmath45-norm regularization to be more adaptive .",
    "it was shown to attain a bound as in ( [ rt13 ] ) and to be computationally feasible since it is cast as a tractable convex optimization problem ( a second order cone programming problem ) . moreover ,",
    "under mild additional conditions , with probability close to 1 , the estimator ( [ conic ] ) achieves improved bounds of the form @xmath46 converges to @xmath47 in sup - norm with the rate @xmath48 .",
    "it is shown in @xcite that the rate of convergence in is minimax optimal in the considered model .",
    "+ there have been other approaches to the errors - in - variables model , usually exploiting some knowledge about the vector @xmath27 , see @xcite .",
    "assuming @xmath49 is known , @xcite proposed an estimator @xmath50 defined as the solution of a non - convex program which can be well approximated by an iterative relaxation procedure . in the case where the entries of the regression matrix @xmath6 are zero - mean subgaussian and @xmath27 is @xmath15-sparse , under appropriate assumptions",
    ", it is shown in @xcite that for the error in @xmath45-norm ( @xmath51 ) , @xmath52 depends on @xmath9 , so that there is no guarantee that the estimator attains the optimal bound as in . assuming that the sparsity @xmath15 of @xmath27 is known and the non - zero components of @xmath27 are separated from zero in the way that @xmath53 an orthogonal matching pursuit algorithm to estimate @xmath9 is introduced in @xcite .",
    "focusing as in @xcite on the particular case where the entries of the regression matrix @xmath6 are zero - mean subgaussian , it is shown in @xcite that this last estimator satisfies a bound analogous to ( [ 6 ] ) , as well as a consistent support recovery result .",
    "+ the main purpose of this work is to show that an additional regularization term based on the @xmath1-norm leads to improved rates of convergence in several situations .",
    "we propose two new estimators for @xmath27 .",
    "the first proposal is applicable under a new combination of the assumptions mentioned above .",
    "namely , we assume that the components of the errors in the design are uniformly bounded by @xmath25 as in ( [ wbound ] ) , and that the rows of @xmath7 are independent and with zero mean .",
    "however , we will neither assume that a data - driven estimator @xmath54 is available , nor that specific features of @xmath27 are known ( e.g. @xmath15 or @xmath49 ) .",
    "the estimator is defined as a solution of a regularized optimization problem which uses simultaneously @xmath44 , @xmath45 , and @xmath1 regularization functions .",
    "it can be cast as a convex optimization problem and the solution can be easily computed .",
    "we study its rates of convergence in various norms in section [ first ] .",
    "one of the conclusions is that for @xmath55 the new estimator has improved rates of convergence compared to the mu selector .",
    "furthermore , note that the conic estimator @xmath56 studied in @xcite can be also applied .",
    "indeed , our setting can be embedded into that of @xcite with @xmath57 being the identically zero @xmath58 matrix , which means that we have an estimator of each @xmath30 with an error bounded by @xmath59 . comparing the bounds",
    "yields that the conic estimator @xmath56 achieves the same rate as our new estimator if @xmath25 is smaller than or of the order @xmath60 . however , there is no bound for @xmath56 available when @xmath61 .",
    "+ the second estimator we propose applies to the same setting as in @xcite .",
    "the idea of taking advantage of an additional @xmath1-norm regularization can be used to improve the conic estimator @xmath56 of @xcite whenever the rate of convergence of the estimator @xmath57 for @xmath30 , @xmath62 , is slower than @xmath48 .",
    "this motivates us to propose and analyze a modification of the conic estimator .",
    "we derive new rates of convergence that can lead to improvements . however , we acknowledge that in the case considered in @xcite , where the rate of convergence of @xmath57 is @xmath48 , there is no gain in the rates of convergence when using the additional @xmath1-norm regularization . + the paper is organized as follows .",
    "section [ sec : assump ] contains the notation , main assumptions and some preliminary lemmas needed to determine threshold constants in the algorithms . the definition and properties of our first estimator",
    "are given in section [ first ] whereas those of our second procedure can be found in section [ second ] .",
    "section 5 contains simulation results .",
    "some auxiliary lemmas are relegated to an appendix .",
    "in this section , we introduce the assumptions which will be required to derive the rates of convergence of the proposed estimators .",
    "one set of conditions pertains to the design matrix and the second to the errors in the model .",
    "we also state preliminary lemmas related to the stochastic error terms .",
    "we start by introducing some notation .",
    "let @xmath63 be a set of integers .",
    "we denote by @xmath64 the cardinality of @xmath65 . for a vector @xmath66 in @xmath67 , we denote by @xmath68 the vector in @xmath67 whose @xmath69th component satisfies @xmath70 if @xmath71 , and @xmath72 otherwise . for @xmath73 ,",
    "the random variable @xmath74 is said to be _ sub - gaussian with variance parameter _",
    "@xmath75 ( or shortly _ @xmath76-sub - gaussian _ ) if , for all @xmath77 , @xmath78\\leq \\text{exp}(\\gamma^2t^2/2).\\ ] ] a random vector @xmath79 is said to be _ sub - gaussian with variance parameter _ @xmath75 if the inner products @xmath80 are @xmath76-sub - gaussian for any @xmath81 with @xmath82 .      the performance of the estimators that we consider below is influenced by the properties of the gram matrix @xmath83 we will assume that :    _ _    * the matrix @xmath6 is deterministic .    in order to characterize the behavior of the design matrix , we set @xmath84 where the @xmath85 are the elements of matrix @xmath6 and we consider the sensitivity characteristics related to the gram matrix @xmath86 . for @xmath87",
    ", define the cone @xmath88 where @xmath65 is a subset of @xmath89 . for @xmath90 $ ] and an integer @xmath91 $ ] , the _ @xmath17-sensitivity _ ( cf .",
    "@xcite ) is defined as follows : @xmath92 like in @xcite , we use here the sensitivities to derive the rates of convergence of estimators under sparsity .",
    "importantly , as shown in @xcite , the approach based on sensitivities is more general than that based on the restricted eigenvalue or the coherence conditions , see also @xcite . in particular , under those conditions , we have @xmath93 for some constant @xmath94 , which implies the usual optimal bounds for the errors .",
    "next we turn to the error @xmath7 in the design and the error @xmath23 in the regression equation .",
    "we will make the following assumptions .",
    "+    _ _    * the elements of the random vector @xmath23 are independent zero - mean sub - gaussian random variables with variance parameter  @xmath95 . *",
    "the rows @xmath96 , @xmath97 , of the noise matrix @xmath7 are independent zero - mean sub - gaussian random vectors with variance parameter  @xmath98 .",
    "furthermore , @xmath7 is independent of @xmath23 .",
    "we now state some useful lemmas from @xcite and @xcite that provide bounds to various stochastic error terms that play a role in our analysis .",
    "we state them here because they introduce the thresholds @xmath99 that will be used in the definition of the estimators . in",
    "what follows , @xmath47 is the diagonal matrix with diagonal elements @xmath30 , @xmath100 , and for a square matrix @xmath101 , we denote by @xmath102 the matrix with the same dimensions as @xmath101 , the same diagonal elements , and all off - diagonal elements equal to zero .    [ lem2 ]",
    "let @xmath103 and assume ( a1)-(a3 ) . then , with probability at least @xmath104 ( for each event ) , @xmath105 where @xmath106 and for an integer @xmath107 , @xmath108 where @xmath109 are positive constants depending only on @xmath110 .",
    "[ lem3a ] let @xmath103 , @xmath111 and assume ( a1)-(a3 ) . then , with probability at least @xmath104 , @xmath112 where @xmath113 .",
    "in addition , with probability at least @xmath104 , @xmath114 where @xmath115 and @xmath116 are positive constants depending only on @xmath117 .",
    "the proofs of lemmas [ lem2 ] and [ lem3a ] can be found in @xcite and @xcite respectively .",
    "in this section , we define and analyze our first estimator .",
    "it can be seen as a compromise between the mu selector ( [ def : mu ] ) and the conic estimator ( [ conic ] ) achieved thanks to an additional @xmath1-norm regularization . in the setting that we consider now , the estimate @xmath54 is not available but the rows of the design error matrix @xmath7 are independent with mean 0 , and its entries are uniformly bounded .",
    "formally , in this section , we make the following assumption .    _",
    "_    * almost surely , @xmath119 .",
    "thus , assumptions ( a1)-(a4 ) imply the assumptions in @xcite .",
    "however , they neither imply or are implied by the assumptions in @xcite .",
    "that is , it is an intermediary set of conditions relative to the original assumptions for the mu selector in @xcite and to those for the compensated mu selector in @xcite .",
    "importantly , we do not assume that there are some accurate estimators of the @xmath30 .",
    "+ we consider the estimator @xmath120 such that @xmath121 is a solution of the following minimization problem @xmath122 where @xmath123 and @xmath124 are tuning constants and the minimum is taken over @xmath125 .",
    "this estimator @xmath120 will be further referred to as the @xmath118-mu selector .",
    "+ the estimator above attempts to mimic the conic estimator ( [ conic ] ) without an estimator @xmath57 for @xmath30 , @xmath62 . in order to make @xmath27 feasible for ( [ conicmunew ] ) , the contribution of the unknown term @xmath126 needs to be bounded .",
    "this is precisely the role of the extra term @xmath127 in the constraint since @xmath128 and @xmath129 almost surely .",
    "note that the use of @xmath130 and @xmath131 instead of @xmath132 and @xmath133 in the constraint makes ( [ conicmunew ] ) a convex programming problem .",
    "+ this new estimator exploits assumptions ( a2)-(a4 ) to achieve a rate of convergence that is intermediary relative to the rate of the mu selector and to that of the conic estimator .",
    "+ set @xmath134 and @xmath135 .",
    "note that @xmath21 and @xmath22 are of order @xmath48 .",
    "the next theorem summarizes the performance of the estimator defined by solving ( [ conicmunew ] ) .",
    "[ th : munew ] let assumptions ( a1)-(a4 ) hold .",
    "assume that the true parameter @xmath27 is @xmath15-sparse and belongs to @xmath136 .",
    "let @xmath103 , @xmath137 and @xmath138 , and let @xmath120 be the @xmath118-mu selector . if @xmath139 for some constant @xmath94 then , with probability at least @xmath140 , @xmath141 for some constants @xmath142 and @xmath143 ( here we set @xmath144 ) .",
    "+ if in addition , @xmath145 for some small enough constant  @xmath146 then , with the same probability we have @xmath147 for some constants @xmath142 and @xmath143 .",
    "+ under the same assumptions with @xmath148 , the prediction error admits the following bound , with the same probability : @xmath149    we proceed in three steps .",
    "step 1 establishes initial relations and the fact that @xmath150 belongs to @xmath151 .",
    "step 2 provides a bound on @xmath152 .",
    "step 3 establishes the rates of convergence stated in the theorem .",
    "we work on the event of probability at least @xmath153 where all the inequalities in lemmas [ lem2 ] and [ lem3a ] are realized . throughout the proof ,",
    "@xmath154 we often make use of the inequalities @xmath155 + _ step 1 .",
    "_ we first note that @xmath156 with probability at least @xmath157 by lemma [ lem2 ] .",
    "next , lemma [ lem3a ] and the fact that , due to ( [ wbound ] ) , we have @xmath158 imply @xmath159    combining and we get that @xmath160 is feasible for the problem ( [ conicmunew ] ) , so that @xmath161 arguments similar to lead to @xmath162    _ step 2 .",
    "_ we have @xmath163 the results of step 1 and of lemmas [ lem2 ] and [ lem3a ] imply the following bounds @xmath164    these relations and the inequality @xmath165 yield that @xmath166    _ step 3 . _",
    "next note that @xmath167 letting @xmath168 we have @xmath169 by the definition of the @xmath17-sensitivity , @xmath170 now , follows by combining the last two displays and the assumption on @xmath171 . to prove",
    ", we use that @xmath172    under our conditions , @xmath173 for some @xmath174 .",
    "thus , we have @xmath175 which implies in view of the definition of the @xmath17-sensitivity and the assumption on @xmath171 . + to show ,",
    "note first that @xmath176 by with @xmath148 , @xmath177 combining this inequality with and proves .",
    "we have stated theorem  [ th : munew ] under assumption ( a4 ) to make the analysis streamlined with the previous literature , see @xcite .",
    "however , inspection of the proofs shows that a more general condition can be used .",
    "the results of theorem  [ th : munew ] hold with probability at least @xmath178 if instead of assumption ( a4 ) we require @xmath7 to satisfy : @xmath179 with probability at least @xmath180 , for some @xmath181 .    compared to @xcite , the results in theorem [ th : munew ] exploit the zero mean condition on the noise matrix  @xmath7 . as in @xcite ,",
    "the estimator is consistent as @xmath25 goes to zero . in order to compare the rates in theorem [ th : munew ] with those for the mu selector",
    ", we recall that , by theorem 3 in @xcite , the mu selector satisfies @xmath182 with probability close to 1 .",
    "while both rates share some terms , a term of order @xmath183 appears only in the rate for the mu selector whereas a term of the order @xmath184 appears only for the @xmath118-mu selector .",
    "therefore , the improvement upon the original mu selector is achieved whenever @xmath185 .",
    "+ if the additional condition @xmath186 holds , we can use the bound and a better accuracy is achieved by the proposed estimator . in particular , @xmath187 no longer drives the rate of convergence .",
    "the impact of @xmath25 on this rate is in the term @xmath188 for the mu selector .",
    "furthermore , the rate of convergence of the new estimator also has a term of the form @xmath189 .",
    "thus the new estimator obtains a better accuracy by exploiting additional assumptions together with the fact that @xmath190 is of larger order than @xmath191 , which holds whenever @xmath185 . finally , the impact of going down from the @xmath44-norm to the @xmath45- or @xmath1-norms is not negligible neither .",
    "for example , if all non - zero components of @xmath9 are equal to the same constant @xmath192 , we have @xmath193 while @xmath194 , and @xmath195 .",
    "then , the comparison in is reduces to comparing @xmath196 featuring the maximum contrast between the two rates .",
    "+ finally , note that the conic estimator @xmath56 studied in @xcite can be also applied under the assumptions of this section .",
    "indeed , our setting can be embedded into that of @xcite with @xmath57 being the identically zero @xmath58 matrix , which means that we have an estimator of each @xmath30 with an error bounded by @xmath197 .",
    "the results in @xcite assume @xmath198 but they do not apply to designs with @xmath199 of larger order . comparing the bound in theorem  [ th : munew ] to the bound yields that the conic estimator @xmath56 achieves the same rate as our new estimator whenever @xmath25 is smaller than or of the order @xmath60 . however , there is no bound for @xmath56 available when @xmath61 .",
    "[ sec : conic ]    in this section , we discuss a modification of the conic estimator proposed in @xcite .",
    "we introduce an additional @xmath1-norm regularization to better adapt to the estimation error in @xmath57 . as discussed in the introduction , this is beneficial when the rate of convergence of @xmath57 to @xmath47 is slower than @xmath48 , which is not covered by @xcite . here",
    "we consider the same assumptions as in @xcite with the only difference that now we allow for any rate of convergence of @xmath57 to @xmath47 .",
    "thus , we replace assumption ( a4 ) by the following assumption on the availability of estimators for @xmath30 , @xmath62 .    _",
    "_    * there exist statistics @xmath29 and positive numbers @xmath200 such that for any @xmath201 , we have @xmath202\\leq \\varepsilon.\\ ] ]    in what follows , we fix @xmath203 and set @xmath204 we are particularly interested in cases where @xmath205 is of larger order than @xmath48 . to define the estimator , we consider the following minimization problem :    @xmath206    here , @xmath123 and @xmath124 are tuning constants and the minimum is taken over @xmath125 .",
    "let @xmath207 be a solution of ( [ conicnew ] ) .",
    "we take @xmath120 as estimator of @xmath9 and we call it the @xmath118-compensated mu selector . the rates of convergence of this estimator are given in the next theorem .",
    "[ th : conic ] let assumptions ( a1)-(a3 ) , and ( a5 ) hold .",
    "assume that the true parameter @xmath27 is @xmath15-sparse and belongs to @xmath136 .",
    "let @xmath103 and @xmath137 .",
    "suppose also that @xmath208 for some constant @xmath94 and that @xmath209 for some small enough constant @xmath210 .",
    "let @xmath120 be the @xmath118-compensated mu selector . then , with probability at least @xmath211 , latexmath:[\\[\\begin{aligned } \\label{11a }    for some constants @xmath142 and @xmath143 ( here we set @xmath144 ) .",
    "+ under the same assumptions with @xmath148 , the prediction error admits the following bound , with the same probability : @xmath213    throughout the proof , we assume that we are on the event of probability at least @xmath214 where the results of lemmas [ lem3 ] , [ lem4 ] and [ lem5 ] in the appendix hold .",
    "property ( [ c1 ] ) in lemma [ lem4 ] implies that @xmath215 is in the cone @xmath151 , where @xmath154 therefore , by the definition of the @xmath17-sensitivity and lemma  [ lem5 ] , we have @xmath216 where @xmath217 and @xmath218 are of order @xmath219 , and @xmath220 and @xmath221 are of order @xmath222 . using again ( [ c1 ] ) , we have @xmath223 it follows that @xmath224 which implies , by , @xmath225 in view of the assumptions of the theorem .",
    "recall that @xmath226 , where @xmath192 is a constant .",
    "therefore , since we assume that @xmath227 , ( [ 11a ] ) follows if @xmath146 is small enough .",
    "+ to prove ( [ 11c ] ) , we use .",
    "remark that from ( [ 11a ] ) with @xmath148 , we have @xmath228 lemma [ lem5 ] in the appendix yields @xmath229 combining the above bound for @xmath230 and ( [ sb ] ) , we get @xmath231 since @xmath232 for some constant @xmath233 under our assumptions .",
    "this proves ( [ 11c ] ) .",
    "theorem  [ th : conic ] generalizes the results in @xcite to estimators @xmath57 that converge with rate @xmath234 of larger order than @xmath48 . at the same time , if @xmath234 is smaller than @xmath48 , both the conic estimator @xmath56 of @xcite and the @xmath118-compensated mu selector achieve the same rate of convergence .",
    "+ for such designs that condition ( [ hyps ] ) does not hold , the conclusions of theorem [ th : conic ] need to be slightly modified as shown in the next theorem .",
    "+    [ th : conicrelaxed ] let assumptions ( a1)-(a3 ) , and ( a5 ) hold .",
    "assume that the true parameter @xmath27 is @xmath15-sparse and belongs to @xmath136 .",
    "let @xmath103 and @xmath137 .",
    "let @xmath120 be the @xmath118-compensated mu selector . then , with probability at least @xmath211 , latexmath:[\\[\\begin{aligned }",
    "\\label{eq : th : conicrelaxed:1 }    for some constants @xmath142 and @xmath143 , and the prediction error admits the following bound , with the same probability : @xmath236    again , throughout the proof , we assume that we are on the event of probability at least @xmath214 where the results of lemmas [ lem3 ] , [ lem4 ] and [ lem5 ] in the appendix hold . property ( [ c1 ] ) in lemma [ lem4 ] implies that @xmath215 is in the cone @xmath151 , where @xmath154 since @xmath237 we obtain @xmath238 therefore @xmath239 which implies .",
    "note also that , due to , the above displays immediately imply the bound on the prediction risk given by the second term under the minimum in .",
    "the first term under the minimum in is obtained by combining , with @xmath148 , and .",
    "this section aims to illustrate the finite sample performance of the proposed estimators .",
    "we will focus on the @xmath118-compensated mu selector only .",
    "we consider the following data generating process @xmath240 here , @xmath241 are independent and @xmath242 , @xmath243 , @xmath244 where @xmath245 is the identity matrix and @xmath246 is @xmath247 matrix with elements @xmath248 .",
    "we consider the vector of unknown parameters @xmath249 .",
    "we set @xmath250 , @xmath251 , and @xmath252 .",
    "we assume that @xmath253 is known and we set @xmath254 .",
    "the penalty parameters are set as @xmath255 , @xmath256 , for @xmath257 .",
    "+ in our first set of simulations , we illustrate the finite sample performance of the proposed estimator by setting @xmath258 .",
    "the @xmath118-compensated mu selector will be denoted by @xmath118 .",
    "we compare its performance with other recent proposals in the literature , namely the conic estimator ( denoted as conic ( @xmath43 ) for @xmath259 ) , and the compensated mu selector ( cmu ) .",
    "we also provide the ( infeasible ) dantzig selector which knows @xmath6 ( dantzig x ) and the dantzig selector that uses only @xmath5 ( dantzig z ) as additional benchmark for the performance .",
    ".simulation results for 100 replications . for each estimator",
    "we provide average bias ( bias ) , average root - mean squared error ( rmse ) , and average prediction risk ( pr ) .",
    "[ cols=\"<,^,^,^,^,^,^ \" , ]     tables [ table : mc3 ] and [ table : mc4 ] show the performance for different values of @xmath43 and @xmath260 .",
    "we note that these parameters seem to have different impact on the finite sample performance even if @xmath261 is kept constant .",
    "importantly , we observe that the addition of safeguard constraints virtually always leads to improvements although small ( even zero sometimes ) for most of the tested parameter values . in the case",
    "@xmath262 , using safeguard constraints makes almost no difference and overall performance of both estimators is better .",
    "in contrast , the estimators perform worse when @xmath263 and the safeguard constraints lead to improvements .",
    "finally , as expected , the safeguard constraints improve substantially the performance when @xmath264 . in that case , the performance becomes comparable to that of the cmu estimator . essentially , the safeguard constraints help to avoid severe underfitting .",
    "they are very helpful when the performance is below of what can be achieved .",
    "nonetheless , we recommend to keep them in all cases as it does not impact negatively the estimator and the additional computational burden seems minimal .",
    "in what follows , we write for brevity @xmath265 , @xmath266 , and we set @xmath215 , @xmath154    [ lem3 ] assume ( a1)-(a3 ) and ( a5 ) .",
    "then with probability at least @xmath267 , the pair @xmath268 belongs to the feasible set of the minimization problem ( [ conicnew ] ) .",
    "first , note that @xmath269 is equal to @xmath270 by definition of @xmath271 and @xmath199 , with probability at least @xmath272 , we have @xmath273 where in ( [ lem3.3 ] ) and ( [ lem3.4 ] ) we have used that the considered matrices are diagonal .",
    "also , by lemma [ lem3a ] , with probability at least @xmath157 , we have @xmath274 combining the decomposition of @xmath269 together with ( [ lem3.2])-([ineg1 ] ) , we find that @xmath275 with probability at least @xmath267 , which implies the lemma .",
    "@xmath276 +    [ lem4 ] let @xmath277 be the @xmath118-compensated mu - selector .",
    "assume ( a1)-(a3 ) and ( a5 ) .",
    "then with probability at least @xmath267 ( on the same event as in lemma [ lem3 ] ) , we have @xmath278    set @xmath150 . on the event of lemma [ lem3 ]",
    ", @xmath279 belongs to the feasible set of the minimization problem ( [ conic ] ) .",
    "consequently , @xmath280 this implies @xmath281 and so @xmath282 and ( [ c1 ] ) follows . to prove ( [ c2 ] )",
    ", it suffices to note that ( [ c3 ] ) implies @xmath283 and the result follows since @xmath284 and @xmath285 .",
    "similar calculations yield the bound for @xmath286 .",
    "[ lem5 ] let @xmath277 be the @xmath118-compensated mu - selector .",
    "assume ( a1)-(a3 ) and ( a5 ) .",
    "then , on a subset of the event of lemma [ lem3 ] having probability at least @xmath214 , we have @xmath287 where @xmath288 , @xmath289 , @xmath290 , @xmath291 .",
    "note that @xmath217 and @xmath218 are of order @xmath219 , and @xmath220 and @xmath221 are of order @xmath222 .    throughout the proof",
    ", we assume that we are on the event of probability at least @xmath267 where inequalities ( [ lem3.2 ] )  ( [ ineg1 ] ) hold and @xmath292 belongs to the feasible set of the minimization problem ( [ conicnew ] ) .",
    "we have @xmath293 using the fact that @xmath207 belongs to the feasible set of the minimization problem ( [ conic ] ) together with ( [ c2 ] ) , we obtain @xmath294 using that @xmath295 , assumption ( a5 ) together with ( [ lem3.4 ] ) yields that @xmath296 now remark that @xmath297 . in view of lemma [ lem3a ] and ( [ lem3.3 ] ) , on the initial event of probability at least @xmath298",
    ", @xmath299 moreover , we have @xmath300 therefore , @xmath301 with probability at least @xmath211 ( since we intersect the initial event of probability at least @xmath298 with the event of probability at least @xmath302 where the bounds @xmath303 and @xmath304 hold for the corresponding terms ) . next , on the same event of probability at least @xmath211 , @xmath305 to complete the proof , it suffices to plug ( [ c9 ] )  ( [ c8 ] ) in the last inequality for @xmath306 and to obtain @xmath307",
    "the work of the third author is supported by genes , and by the french national research agency ( anr ) as part of idex grant anr -11- idex-0003 - 02 , labex ecodec ( anr - 11-labex-0047 ) , and ipanema grant ( anr-13-bsh1 - 0004 - 02 ) .",
    "e.  gautier and a.  b.  tsybakov ( 2013 ) pivotal estimation in high - dimensional regression via linear programming . in : _ empirical inference",
    " a festschrift in honor of vladimir n. vapnik _ ,",
    "b.schlkopf , z. luo , v. vovk eds . , 195 - 204 .",
    "springer , new york e.a .",
    "m.  rosenbaum and a.b .",
    "tsybakov ( 2013 ) improved matrix uncertainty selector . in : _ from probability to statistics and back : high - dimensional models and processes  a festschrift in honor of jon a. wellner _ ,",
    "m.banerjee et al .",
    "eds . _ ims collections _ , vol.9 , 276290 , institute of mathematical statistics ."
  ],
  "abstract_text": [
    "<S> several new estimation methods have been recently proposed for the linear regression model with observation error in the design . </S>",
    "<S> different assumptions on the data generating process have motivated different estimators and analysis . </S>",
    "<S> in particular , the literature considered ( 1 ) observation errors in the design uniformly bounded by some @xmath0 , and ( 2 ) zero mean independent observation errors . under the first assumption , </S>",
    "<S> the rates of convergence of the proposed estimators depend explicitly on @xmath0 , while the second assumption has been applied when an estimator for the second moment of the observational error is available . </S>",
    "<S> this work proposes and studies two new estimators which , compared to other procedures for regression models with errors in the design , exploit an additional @xmath1-norm regularization . </S>",
    "<S> the first estimator is applicable when both ( 1 ) and ( 2 ) hold but does not require an estimator for the second moment of the observational error . </S>",
    "<S> the second estimator is applicable under ( 2 ) and requires an estimator for the second moment of the observation error . </S>",
    "<S> importantly , we impose no assumption on the accuracy of this pilot estimator , in contrast to the previously known procedures . </S>",
    "<S> as the recent proposals , we allow the number of covariates to be much larger than the sample size . </S>",
    "<S> we establish the rates of convergence of the estimators and compare them with the bounds obtained for related estimators in the literature . </S>",
    "<S> these comparisons show interesting insights on the interplay of the assumptions and the achievable rates of convergence . </S>"
  ]
}