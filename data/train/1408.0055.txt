{
  "article_text": [
    "restricted boltzmann machines ( rbms ) have proved to be a versatile tool for a wide variety of machine learning tasks and as a building block for deep architectures @xcite .",
    "the original proposals mainly handle binary visible and hidden units .",
    "whilst binary hidden units are broadly applicable as feature detectors , non - binary visible data requires different designs .",
    "recent extensions to other data types result in type - dependent models : the gaussian for continuous inputs @xcite , beta for bounded continuous inputs @xcite , poisson for count data @xcite , multinomial for unordered categories @xcite , and ordinal models for ordered categories @xcite .",
    "the boltzmann distribution permits several types to be jointly modelled , thus making the rbm a good tool for multimodal and complex social survey analysis .",
    "the work of @xcite combines continuous ( e.g. , visual and audio ) and discrete modalities ( e.g. , words ) .",
    "the work of @xcite extends the idea further to incorporate ordinal and rank data .",
    "however , there are conceptual drawbacks : first , conditioned on the hidden layer , they are still separate type - specific models ; second , handling ordered categories and ranks is not natural ; and third , specifying direct correlation between these types remains difficult .",
    "the main thesis of this paper is that many data types can be captured in one unified model .",
    "the key observations are that ( i ) type - specific properties can be modelled using one or several _ underlying continuous variables _ , in the spirit of thurstonian models @xcite , and ( ii ) evidences be expressed in the form of one or several _ inequalities _ of these underlying variables .",
    "for example , a binary visible unit is turned on if the underlying variable is beyond a threshold ; and a category is chosen if its utility is the largest among all those of competing categories .",
    "the use of underlying variables is desirable when we want to explicitly model the generative mechanism of the data . in psychology and economics , for example",
    ", it gives much better interpretation on why a particular choice is made given the perceived utilities @xcite .",
    "further , it is natural to model the correlation among type - specific inputs using a covariance structure on the underlying variables .",
    "the inequality observation is interesting in its own right : instead of learning from assigned values , we learn from the _ inequality expression of evidences _ , which can be much more relaxed than the value assignments .",
    "this class of evidences indeed covers a wide range of practical situations , many of which have not been studied in the context of boltzmann machines , as we shall see throughout the paper .    to this end",
    ", we propose a novel class of models called _ thurstonian boltzmann machine _",
    "( @xmath0 ) .",
    "the @xmath0 utilises the gaussian restricted boltzmann machine ( grbm ) : the top layer consists of binary hidden units as in standard rbms ; the bottom layer contains a collection of gaussian variable groups , one per input type .",
    "the main difference is that @xmath0 does not require valued assignments for the bottom layer _ but a set of inequalities expressing the constraints imposed by the evidences_. except for a limiting case of point assignments where the inequalities are strictly equalities , the gaussian layer is never fully observed .",
    "the @xmath0 supports more data types in a unified manner than ever before : for any combination of the point assignments , intervals , censored values , binary , unordered categories , multi - categories , ordered categories , ( in)-complete ranks with and without ties , all we need to do is to supply relevant subset of inequalities .",
    "we evaluate the proposed model on three applications of very different natures : handwritten digit recognitions , collaborative filtering and complex survey analysis . for the first two applications ,",
    "the performance is competitive against methods designed for those data types . on the last application",
    ", we believe we are among the first to propose a scalable and generic machinery for handle those complex data types .",
    "let @xmath1 be a vector of input variables .",
    "let @xmath2 be a set of hidden factors which are designed to capture the variations in the observations . the input layer and the hidden layer form an undirected bipartite graph , i.e. , only cross - layer connections are allowed .",
    "the model admits the boltzmann distribution    @xmath3    where @xmath4 is the normalising constant and @xmath5 is the state energy .",
    "the energy is decomposed as    @xmath6    where@xmath7 are free parameters and @xmath8 denotes the @xmath9-th row",
    ".    given the input @xmath10 , the posterior has a simple form    @xmath11    where @xmath12 denotes the @xmath13-th column .",
    "similarly , the generative process given the binary factor @xmath14 is also factorisable    @xmath15    where @xmath16 is the normal distribution of mean @xmath17 and unit deviation .",
    "we now generalise the gaussian rbm into the thurstonian boltzmann machine ( @xmath0 ) . denote by @xmath18 an observed _ evidence _ of @xmath10 .",
    "standard evidences are the point assignment of @xmath10 to some specific real - valued vector , i.e. , @xmath19 .",
    "generalised evidences can be expressed using _ inequality constraints _",
    "@xmath20 for some transform matrix @xmath21 and vectors @xmath22 , where @xmath23 denotes element - wise inequalities .",
    "thus an evidence can be completely realised by specifying the triple @xmath24 .",
    "for example , for the point assignment , @xmath25 , where @xmath26 is the identity matrix . in what follows",
    ", we will detail other useful popular realisations of these quantities .",
    "this refers to the case where input variables are independently constrained , i.e. , @xmath27 , and thus we need only to specify the pair @xmath28 .    [ [ censored - observations . ] ] censored observations .",
    "+ + + + + + + + + + + + + + + + + + + + + +    this refers to situation where we only know the continuous observation beyond a certain point , i.e. , @xmath29 and @xmath30 .",
    "for example , in survival analysis , the life expectancy of a person might be observed up to a certain age , and we have no further information afterward .    [ [ interval - observations . ] ] interval observations__. _ _ + + + + + + + + + + + + + + + + + + + + + + + + + + +    when the measurements are imprecise , it may be better to specify the range of possible observations with greater confidence rather than a singe point , i.e. , @xmath31 and @xmath32 for some pair @xmath33 .",
    "for instance , missile tracking may estimate the position of the target with certain precision .",
    "[ [ binary - observations . ] ] binary observations__. _ _ + + + + + + + + + + + + + + + + + + + + + + + +    a binary observation @xmath34 can be thought as a result of clipping @xmath35 by a threshold @xmath36 , that is @xmath37 if @xmath38 and @xmath39 otherwise . the boundaries in eq .",
    "( [ sub : inequality - constraints ] ) become : @xmath40 thus , this model offers an alternative . ] to standard binary rbms of @xcite .    [",
    "[ ordinal - observations . ] ] ordinal observations .",
    "+ + + + + + + + + + + + + + + + + + + + +    denote by @xmath41 the set of ordinal observations , where each @xmath34 is drawn from an ordered set @xmath42 .",
    "the common assumption is that the ordinal level @xmath43 is observed given @xmath44 $ ] for some thresholds @xmath45 .",
    "the boundaries thus read @xmath46 this offers an alternative to the ordinal rbms of @xcite .",
    "[ [ categorical - observations . ] ] categorical observations__. _ _",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + +    this refers to the situation where out of an _ unordered _ set of categories , we observe only one category at a time .",
    "this can be formulated as follows .",
    "each category is associated with a `` utility '' variable .",
    "the category @xmath47 is observed ( i.e. , @xmath48 ) _ _ if it has the largest utility , that is @xmath49 .",
    "thus , @xmath50 is the upper - threshold for all other utilities . on the other hand",
    ", @xmath51 is the lower - threshold for @xmath50 .",
    "this suggests an _ em - style _ procedure : ( _ i _ ) fix @xmath50 ( or treat it as a threshold ) and learn the model under the intervals @xmath52 for all @xmath53 , and ( _ ii _ ) fix all categories other than @xmath47 , learn the model under the interval @xmath49 .",
    "this offers an alternative to the multinomial logit treatment in @xcite .    to illustrate the point , suppose there are only four variables @xmath54 , and @xmath55",
    "is observed , then we have @xmath56 .",
    "this can be expressed as @xmath57 and @xmath58 .",
    "these are equivalent to @xmath59;\\quad\\bb=\\boldsymbol{0};\\quad\\cb=+\\infty\\right\\rangle\\ ] ]    [ [ imprecise - categorical - observations . ] ] imprecise categorical observations__. _ _ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    _ this generalises the categorical case _ : the observation is a subset of a set , where _ any _ member of the subset can be a possible observation . for example , when asked to choose the best sport team of interest , a person may pick two teams without saying which is the best . for instance , suppose the subset is @xmath60 , then @xmath61 , which can be expressed as @xmath62 , @xmath58 and @xmath63 .",
    "this translates to the following triple @xmath64;\\quad\\bb=\\boldsymbol{0};\\quad\\cb=+\\infty\\right\\rangle\\ ] ]    [ [ rank - with - ties - observations . ] ] rank ( with ties ) observations__. _ _ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    _ this generalises the imprecise categorical cases _ : here we have a ( partially ) ranked set of categories .",
    "assume that the rank is produced in a stagewise manner as follows : the best category subset is selected out of all categories , the second best is selected out of all categories except for the best one , and so on .",
    "thus , at each stage we have an imprecise categorical setting , but now the utilities of middle categories are constrained from both sides  the previous utilities as the upper - bound , and the next utilities as the lower - bound .    as an illustration ,",
    "suppose there are four variables @xmath54 and a particular rank ( with ties ) imposes that @xmath65 .",
    "this be rewritten as @xmath66 , which is equivalent to @xmath67;\\quad\\bb=\\boldsymbol{0};\\quad\\cb=+\\infty\\right\\rangle\\ ] ]",
    "under the @xmath0 , mcmc - based inference without evidences is simple : we alternate between @xmath68 and @xmath69 .",
    "this is efficient because of the factorisations in eqs .",
    "( [ eq : posterior - factor],[eq : generative - factor ] ) .",
    "inference with inequality - based evidence @xmath18 is , however , much more involved except for the limiting case of point assignments .",
    "denote by @xmath70 the constrained domain of @xmath10 defined by the evidence @xmath18 .",
    "now we need to specify and sample from the constrained distribution @xmath71 defined on @xmath72 .",
    "sampling @xmath68 remains unchanged , and in what follows we focus on sampling from @xmath73 .      for _ boxed constraints _",
    "( section  [ sub : boxed - constraints ] ) , due to the conditional independence , we still enjoy the factorisation @xmath74 .",
    "we further have @xmath75 where @xmath76 is the normal cumulative distribution function of @xmath77 .",
    "now @xmath78 is a truncated normal distribution , from which we can sample using the simple rejection method , or more advanced methods such as those in @xcite .      for general _ inequality constraints _ ( section  [ sub : inequality - constraints ] ) ,",
    "the input variables are interdependent due to the linear transform @xmath79 .",
    "however , we can specify the conditional distribution @xmath80 ( here @xmath81 ) by realising that    @xmath82 where @xmath83 for @xmath84 . in other words ,",
    "@xmath35 is conditionally box - constrained given other variables .",
    "this suggests a gibbs procedure by looping through @xmath85 . with some abuse of notation ,",
    "let @xmath86 and @xmath87 .",
    "the constraints can be summarised as @xmath88\\\\   & = & \\left[\\max_{m}\\min\\left\\ { \\tilde{b}_{mi},\\tilde{c}_{mi}\\right\\ } , \\min_{m}\\max\\left\\ { \\tilde{b}_{mi},\\tilde{c}_{mi}\\right\\ } \\right]\\end{aligned}\\ ] ] the @xmath89 and @xmath90 operators are needed to handle change in inequality direction with the sign of @xmath91 , and the join operator is due to multiple constraints .    for more sophisticated gibbs procedures",
    ", we refer to the work in @xcite .",
    "we are often interested in the posteriors @xmath92 , e.g. , for further processing . unlike the standard rbms",
    ", the binary latent variables here are coupled through the unknown gaussians and thus there are no exact solutions unless the evidences are all point assignments .",
    "the mcmc - based techniques described above offer an approximate estimation by averaging the samples @xmath93 .",
    "for the case of boxed constraints , mean - field offers an alternative approach which may be numerically faster .",
    "in particular , the mean - field updates are recursive :    @xmath94    where @xmath95 is the probability of the unit @xmath13 being activated , @xmath96 is the mean of the normal distribution truncated in the interval @xmath97 $ ] , @xmath98 is the probability density function , and @xmath99 is normal cumulative distribution function .",
    "interested readers are referred to the supplement for more details .      given the hidden states @xmath14 we want to estimate the probability that hidden states generate a particular evidence @xmath18    @xmath100 for boxed constraints , analytic solution is available since the gaussian variables are decoupled , i.e. , @xmath101 , where @xmath102 .",
    "for general inequality constraints , however , these variables are coupled by the inequalities .",
    "the general strategy is to sample from @xmath69 and compute the portion of samples falling into the constrained domain @xmath72 . for certain classes of inequalities",
    "we can approximate the gaussian by appropriate distributions from which the integration has the closed form .",
    "in particular , those inequalities imposed by the categorical and rank evidences can be dealt with by using the _ extreme value distributions_. the integration will give the logit form on distribution of categories and plackett - luce distribution of ranks .",
    "for details , we refer to the supplement .",
    "learning is based on maximising the evidence likelihood    @xmath103    where @xmath104 is defined in eq .",
    "( [ eq : model - def ] ) .",
    "let @xmath105 , then @xmath106 . the gradient w.r.t .",
    "the mapping parameter reads    @xmath107-\\mathbb{e}_{p(x_{i},h_{k})}\\left[x_{i}h_{k}\\right]\\label{eq : ll - grad}\\end{aligned}\\ ] ]    the derivation is left to the supplement .",
    "the _ data - dependent _ statistics @xmath108 $ ] and the _ data - independent _ statistics + @xmath109 $ ] are not tractable to compute in general , and thus approximations are needed .    [ [ data - dependent - statistics . ] ] data - dependent statistics .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + +    under the box constraints , the mean - field technique ( section  [ sub : estimating - the - posteriors ] ) can be employed as follows @xmath110\\approx\\hat{\\mu}_{i}q_{k}\\ ] ] for general cases , sampling methods are applicable .",
    "in particular , we maintain one persistent markov chain @xcite per data instance and estimate the statistics after a very short run .",
    "this would explore the space of the data - dependent distribution @xmath71 by alternating between @xmath68 and @xmath73 using techniques described in section  [ sub : generalised - inference ] .",
    "[ [ data - independent - statistics . ] ] data - independent statistics .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + +    mean - field distributions are not appropriate for exploring the entire state space because they tend to fit into one mode . one practical solution is based on the idea of hinton s contrastive divergence ( cd ) , where we create another markov chain on - the - fly starting from the latest state of the clamped chain .",
    "this chain will be discarded after each parameter update .",
    "this is particular useful when the models are instance - specific , e.g. , in collaborative filtering , it is much cheaper to build one model per user , all share the same parameters .",
    "if it is not the case , then we can maintain a moderate set of parallel chains and collect the samples after a short run at every updating step @xcite .      in the case of boxed constraints , sometimes it is helpful to learn the boundaries @xmath111 themselves .",
    "the gradient of the log - likelihood w.r.t .",
    "the lower boundaries reads @xmath112}p\\left(\\xb\\mid\\hb^{(s)},\\eb\\right)d\\xb_{\\neg i}dx_{i}\\\\   & = & -\\frac{1}{s}\\sum_{\\hb}p\\left(x_{i}=b_{i}\\mid\\hb^{(s)},\\eb\\right)\\end{aligned}\\ ] ] where @xmath93 are samples collected during the mcmc procedure running on the data - dependent distribution @xmath113 .",
    "similarly we would have the gradient w.r.t .",
    "the upper boundaries : @xmath114",
    "in this section , we describe applications of the @xmath0 for three realistic domains , namely handwritten digit recognition , collaborative filtering and worldwide survey analysis . before going to the details ,",
    "let us first address key implementation issues ( see supplement for more details ) .",
    "one observed difficulty in training the @xmath0 is that the hidden samples can get stuck in one of the two ends and thus learning can not progress .",
    "the reasons might be the large mapping parameters or the unbounded nature of the underlying gaussian variables , which can saturate the hidden units .",
    "we can control the norm of the mapping parameters , either by using the standard @xmath115-norm regularisation , or by rescaling the norm of the parameter vector for each hidden unit . to deal with the non - boundedness of the gaussian variables",
    ", then we can restrict their range , making the model bounded .",
    "another effective solution is to impose a constraint on the posteriors by adding the regularising term to the log - likelihood , e.g. , @xmath116\\right\\}\\ ] ] where @xmath117 is the expected probability that a hidden unit will turn on given the evidence and @xmath118 is the regularisation weight . maximising",
    "this quantity is essentially minimising the kullback - leibler divergence between the expected posteriors and the true posteriors . in our experiments",
    ", we found @xmath119 and @xmath120 gave satisfying results .",
    "the main technical issue is that @xmath121 does not have a simple form due to the integration over all the constrained gaussian variables .",
    "approximation is thus needed .",
    "the use of mean - field methods will lead to the simple sigmoid form , but it is only applicable for boxed constraints since it breaks down deterministic constraints among variables ( section  [ sub : estimating - the - posteriors ] ) .",
    "however , we can estimate the `` mean '' truncated gaussian @xmath96 by averaging the recent samples of the gaussian variables in the data - dependent phase .",
    "once these safeguards are in place , learning can greatly benefit from quite large learning rate and small batches as it appears to quickly get the samples out off the local energy traps by significantly distorting the energy landscape . depending on the problem sizes , we vary the batch sizes in the range @xmath122 $ ] .",
    "we use the name probit rbm to denote the special case of @xmath0 where the observations are binary ( i.e. , boxed constraints , see section  [ sub : boxed - constraints ] ) .",
    "the threshold @xmath36 for each visible unit @xmath9 is chosen so that under the zero mean , the probability of generating a binary evidence equals the empirical probability , i.e. , @xmath123 , and thus @xmath124 . since any mismatch in thresholds can be corrected by shifting the corresponding biases , we do not need to update the thresholds further .",
    "mnist feature weights ( one image per hidden unit ) learned by probit rbm ( left ) and rbm with cd-1 ( right).[fig : learned - mnist - features ] , title=\"fig:\"]@xmath125 mnist feature weights ( one image per hidden unit ) learned by probit rbm ( left ) and rbm with cd-1 ( right).[fig : learned - mnist - features ] , title=\"fig : \" ]    we report here the result of the mean - field method for computing data - dependent statistics , which are averaged over a random batch of @xmath126 images . for the data - independent statistics , @xmath126 persistent chains",
    "are run in parallel with samples collected after every @xmath127 gibbs steps .",
    "the sparsity level @xmath128 is set to @xmath129 and the sparseness weight @xmath130 is set to @xmath131 .",
    "once the model has been learned , mean - field is used to estimate the hidden posteriors .",
    "typically this mean - field is quite fast as it converges in a few steps .",
    ", title=\"fig : \" ]   , title=\"fig : \" ]    we take the data from mnist and binarize the images using a mid - intensity threshold .",
    "the learned representation is shown in figure  [ fig : t - sne - visualisation - of - mnist ] .",
    "most digits are well separated in 2d except for digits @xmath132 and @xmath133 .",
    "the learned representation can be used for classifications , e.g. , by feeding to the multiclass logistic classifier . for @xmath126 hidden units ,",
    "the probit rbm achieves the error rate of @xmath134% , comparable with those obtained by the rbm trained with cd-@xmath135 ( @xmath136% ) , and much better than the raw pixels ( @xmath137% ) .",
    "the features discovered by the probit rbm and rbm with cd-@xmath135 are very different ( figure  [ fig : learned - mnist - features ] ) , and this is expected because they operate on different input representations .",
    "the energy surface learned by the probit rbm is smooth enough to allow efficient exploration of modes , as shown in figure  [ fig : samples - generated - from - probit - rbm ] .     ,",
    "title=\"fig:\"]@xmath125 , title=\"fig : \" ]      in collaborative filtering , one of the main goals is to produce a personalized ranked list of items . until very recently , the majority in the area , on the other hand , focused on predicting the ratings , which are then used for ranking items .",
    "it can be arguably more efficient to learn a rank model directly instead of going through the intermediate steps .",
    "we build one @xmath0 for ranking with ties ( i.e. , inequality constraints , see section  [ sub : inequality - constraints ] ) per user due to the variation in item choices but all the @xmath0s share the same parameter set .",
    "the handling of ties is necessary because during training , many items share the same rating .",
    "unseen items are simply not accounted for in each model : we only need to compare the utilities between the items seen by each person .",
    "the result is that the models are very sparse and fast to run . for the data - dependent statistics , we maintain one markov chain per user .",
    "since there is no single model for all data instances , the data - independent statistics can not be estimated from a small set of markov chains .",
    "rather we also maintain a data - independent chain per data instance , which can be persistent on their own , or restarted from the data - dependent chains after every parameter updating step .",
    "the latter case , which is reported here , is in the spirit of the hinton s contrastive divergence , where the data - independent chain is just a few steps away from the data - dependent chain .    once the model has been trained , the hidden posterior vector @xmath138 , where @xmath139 , is used as the new representation of the tastes of user @xmath140 .",
    "the rank of unseen movies is the mode of the distribution @xmath141 , where @xmath142 are the rank - based evidences ( see section  [ sub : estimating - probability - of - evidences ] ) .",
    "for fast computation of @xmath141 , we approximate the gaussian by a gumbel distribution , which leads to a simple way of ranking movies using the mean `` utility '' @xmath143 for user @xmath140 ( see the supplement for more details ) .    the data used in this experiment is the movielens , which contains @xmath135 m ratings by approximately @xmath144k users on @xmath132k movies . to encourage diversity in the rank lists , we remove the top @xmath145% most popular movies .",
    "we then remove users with less than @xmath146 ratings on the remaining movies .",
    "the most recently rated @xmath145 movies per user are held out for testing , the next most recent @xmath127 movies are used for tuning hyper - parameters , and the rest for training .    for comparison",
    ", we implement a simple baseline using item popularity for ranking , and thus offering a naive non - personalized solution . for personalized alternatives , we implement two recent rank - based matrix factorisation methods , namely listrank.mf @xcite and pmop @xcite .",
    "two ranking metrics from the information retrieval literature are used : the _ err _ @xcite and the _",
    "ndcg@t _ @xcite . these metrics place more emphasis on the top - ranked items .",
    "table  [ tab : movielens-1m - rank ] reports the movie ranking results on test subset ( each user is presented with a ranked list of unseen movies ) , demonstrating that the @xmath0 is a clear winner in all metrics .",
    ".item ranking results on movielens  the higher the better ( @xmath147 ) . here _",
    "n@t _ is a shorthand for _ ndcg@t_. [ tab : movielens-1m - rank ] [ cols=\">,^,^,^,^\",options=\"header \" , ]      finally , we demonstrate the @xmath0 on mixed evidences .",
    "the data is from the survey analysis domain , which mostly consists of multiple questions of different natures such as basic facts ( e.g. , ages and genders ) and opinions ( e.g. , binary choices , single choices , multiple choices , ordinal judgments , preferences and ranks ) .",
    "the standard approach to deal with such heterogeneity is to perform the so - called `` coding '' , which converts types into some numerical representations ( e.g. , ordinal scales into stars , ranks into multiple pairwise comparisons ) so that standard processing tools can handle .",
    "however , this coding process breaks the structure in the data and thus significant information will be lost .",
    "thus our @xmath0 offers a scalable and generic machinery to process the data in its native format and then convert the mixed types into a more homogeneous posterior vector .",
    "we use the global attitude survey dataset collected by the pewresearch centre .",
    "the survey was conducted on @xmath148 people from @xmath149 countries during the period of march 17 april 21 , 2008 on a variety of topics concerning people s life , opinions on issues in their countries and around the world as well as future expectations .",
    "there are @xmath150 binary , @xmath151 categorical ( of variable category sizes ) , @xmath152 continuous , @xmath153 ordinal ( of variable level sizes ) question types .    like the case of collaborative filtering",
    ", we build one @xmath0 per respondent due to the variation in questions and answers but all the @xmath0s share the same parameter set .",
    "unanswered / inappropriate questions are ignored . for each respondent , we maintain @xmath154 persistent and non - interacting markov chains for the data - dependent statistics and the data - independent statistics , respectively .",
    "hidden units ) on 2d using t - sne .",
    "a dot represents one respondent .",
    "best viewed in colours .",
    "[ fig : distribution - of - world - opinions ] ]    figure  [ fig : distribution - of - world - opinions ] shows the 2d distribution of respondents from @xmath149 countries obtained by feeding the posteriors to the t - sne @xcite ( here no explicit information of countries is used ) .",
    "it is interesting to see the cultural / social clustering and gaps between countries as opposed to the geographical distribution ( e.g. , between indonesia and egypt , australia and uk and the relative separation of the china , pakistan , turkey and the us from the rest ) . to predict the @xmath149 countries , we feed the posteriors into the standard multiclass logistic regression and achieve an error rate of @xmath155 , suggesting that the @xmath0 has captured the intrastate regularities and separated the interstate variations well .",
    "latent multivariate gaussian variables have been widely studied in statistical analysis , initially to model correlated binary data @xcite then now used for a variety of data types such as ordered categories @xcite , unordered categories @xcite , and the mixture of types _ _ @xcite__. _ _ learning with the underlying gaussian model is notoriously difficult for large - scale setting : independent sampling costs cubic time due to the need of inverting the covariance matrix , while mcmc techniques such as gibbs sampling can be very slow if the graph is dense and the interactions between variables are strong .",
    "this can be partly overcome by adding one more layer of latent variables as in factor analysis @xcite and probabilistic principle component analysis @xcite .",
    "the main difference from our @xmath0 is that those models are directed with continuous factors while ours is undirected with binary factors .",
    "gaussian rbms have been used for modelling continuous data such as visual features @xcite , where the evidences are the value assignments , and thus a limiting case of our evidence system .",
    "some restrictions to the continuous boltzmann machines have been studied : in @xcite , gaussian variables are assumed to be non - negative , and in @xcite , continuous variables are bounded .",
    "however , we do not make these restrictions on the model but rather placing restrictions during the training phase only .",
    "grbms that handle ordinal evidences have been studied in @xcite , which is an instance of the boxed - constraints in our @xmath0 .",
    "since the underlying variables of the @xmath0 are gaussian , various extensions can be made without much difficulty .",
    "for example , direct correlations among variables , _ regardless of their types _ , can be readily modelled by introducing the non - identity covariance matrix @xcite .",
    "this is clearly a good choice for image modelling since nearby pixels are strongly correlated .",
    "another situation is when the input units are associated with their own attributes .",
    "each unit can be extended naturally by adding a linear combination of attributes to the mean structure of the gaussian .",
    "the additive nature of the mean - structure allows the natural extension to matrix modelling ( e.g. , see @xcite ) .",
    "that is , we do not distinguish the role of rows and columns , and thus each row and column can be modelled using their own hidden units ( the row parameters and columns parameters are different ) . conditioned on the row - based hidden units , we return to the standard @xmath0 for column vectors .",
    "inversely , conditioned on the column - based hidden units , we have the @xmath0 for row vectors .    to sum up",
    ", we have proposed a generic class of models called thurstonian boltzmann machine ( @xmath0 ) to unify many type - specific modelling problems and generalise them to the general problem of learning from multiple groups of inequalities .",
    "our framework utilises the gaussian restricted boltzmann machines , but the gaussian variables are never observed except for one limiting case .",
    "rather , those variables are subject to inequality constraints whenever an evidence is observed . under this representation",
    ", the @xmath0 supports a very wide range of evidences , many of which were not possible before in the boltzmann machine literature , _ without _ the need to specify type - specific models .",
    "in particular , the @xmath0 supports any combination of the point assignments , intervals , censored values , binary , unordered categories , multi - categories , ordered categories , ( in)-complete ranks with and without ties .",
    "we demonstrated the @xmath0 on three applications of very different natures , namely handwritten digit recognition , collaborative filtering and complex survey analysis .",
    "the results are satisfying and the performance is competitive with those obtained by type - specific models .",
    "the model potential is then the product of all local potentials @xmath157\\left[\\prod_{ik}\\psi_{ik}(\\x_{i},h_{k})\\right]\\left[\\prod_{k}\\phi_{k}(h_{k})\\right]\\label{eq : potential - factorise}\\ ] ] the partition function can be rewritten as @xmath158 where @xmath159 .",
    "we now proceed to compute @xmath160 : @xmath161\\int_{\\ub}\\left[\\prod_{i}\\phi_{i}(\\x_{i})\\right]\\left[\\prod_{ik}\\psi_{ik}(\\x_{i},h_{k})\\right]d\\ub\\\\   & = & \\left[\\prod_{k}\\phi_{k}(h_{k})\\right]\\prod_{i}\\int_{x_{i}}\\exp\\left\\ { -\\frac{\\x_{i}^{2}}{2}+(\\alpha_{i}+\\sum_{k}w_{ik}h_{k})\\x_{i}\\right\\ } dx_{i}\\\\   & = & \\left[\\prod_{k}\\phi_{k}(h_{k})\\right]\\prod_{i}c_{i}\\int_{x_{i}}\\exp\\left\\ { -\\frac{\\left(x_{i}-\\mu_{i}(\\hb)\\right)^{2}}{2}\\right\\ } dx_{i}\\\\   & = & \\left[\\prod_{k}\\phi_{k}(h_{k})\\right]\\prod_{i}c_{i}\\sqrt{2\\pi\\sigma_{i}^{2}}\\end{aligned}\\ ] ] where          let @xmath166 be the ( slowly ) increasing sequence of temperature , where @xmath167 and @xmath168 , that is @xmath169 . at @xmath167 , we have a uniform distribution , and at @xmath168 , we obtain the desired distribution . at each step",
    "@xmath170 , we draw a sample @xmath171 from the distribution @xmath172 ( e.g. using some metropolis - hastings procedure ) .",
    "let @xmath173 be the unnormalised distribution of @xmath174 , that is @xmath175 .",
    "the final weight after the annealing process is computed as @xmath176    the above procedure is repeated @xmath177 times . finally , the normalisation constant at @xmath178 is computed as @xmath179 where @xmath180 , which is the number of configurations of the hidden variables @xmath14 .",
    "recall that for evidence @xmath18 we want to estimate posteriors @xmath181 .",
    "assume that the evidences can be expressed in term of boxed constraints , which lead to the following factorisation @xmath182 this factorisation is critical because it ensures that there are no deterministic constraints among @xmath183 , which are the conditions that variational methods such as mean - fields would work well .",
    "this is because mean - field solution will generally not satisfy deterministic constraints , and thus may assign non - zeros probability to improbably areas .    to be more concrete",
    ", the mean - field approximation would be @xmath184 @xmath185 the best mean - field approximation will be the minimiser of the kullback - leibler divergence @xmath186-\\sum_{\\hb}\\sum_{\\xb\\in\\domainb(\\eb)}q(\\hb,\\xb)\\log p(\\hb,\\xb\\mid\\eb)\\label{eq : kl - mean - field}\\end{aligned}\\ ] ] where @xmath187 $ ] is the entropy function .",
    "now first , exploit the fact that @xmath188 is factorisable , and thus its entropy is decomposable , i.e. , @xmath189=\\sum_{k}h\\left[q_{k}(h_{k})\\right]+\\sum_{i}h\\left[q_{i}(x_{i})\\right]\\label{eq : entropy - mean - field - decompose}\\ ] ]        combining this decomposition and eq .",
    "( [ eq : entropy - mean - field - decompose ] ) , we have completely decomposed the kullback - leibler divergence in eq .",
    "( [ eq : kl - mean - field ] ) into local terms : @xmath196-\\sum_{k}h\\left[q_{k}(h_{k})\\right]\\end{aligned}\\ ] ]        * let us compute the partial derivative w.r.t .",
    "@xmath202 : @xmath203 where @xmath204 is a short hand for @xmath205 and we have made use of the fact that @xmath206 . setting this gradient to zero yields @xmath207 for @xmath208 .",
    "normalising this distribution would lead to the truncated form of the normal distribution those the mean is @xmath209 * in a similar way , the partial derivative w.r.t .",
    "@xmath210 would be @xmath211 equating the gradient to zero , we have @xmath212 where @xmath96 is the mean of the truncated normal distribution @xmath213 normalising @xmath210 would lead to @xmath214^{-1}\\label{eq : mean - field - sigmoid}\\ ] ] * finally , combining these findings in eqs .",
    "( [ eq : mean - field - normal],[eq : mean - field - mean],[eq : mean - field - sigmoid ] ) , and letting @xmath215 $ ] be the boxed constraint , and using the fact that the mean of the truncated distribution is @xmath216 we would arrive at the three recursive equations @xmath217 where @xmath218 is a short hand for @xmath219 and@xmath98 is the normal probability density function , and @xmath99 is the cumulative distribution function @xmath220      once the model has been learned , samples can be generated straightforwardly by first sampling the underlying gaussian rbm and then collect the true samples that satisfy the inequalities of interest .",
    "for example , for binary samples , if the generated gaussian value for a visible unit is larger than the threshold , then we have an active sample . likewise , rank samples",
    ", we only need to rank the sampled gaussian values",
    ".    however , this may suffer from the poor mixing if we use standard gibbs sampling , that is the markov chain may get stuck in some energy traps . to jump out of the trap",
    "we propose to periodically raise the temperature to a certain level ( e.g. , @xmath145 ) and then slowly cool down to the original temperature ( which is @xmath135 ) . in our experiment , the cooling is scheduled as follows @xmath221 where @xmath222 is estimated so that for @xmath223 steps , the temperature will drop from @xmath224 to @xmath225 .",
    "that is , @xmath226 , leading to @xmath227 .    to locate a basis of attraction , we can lower the temperature further ( e.g. , to @xmath228 ) to trap the particles there",
    ". then we collect @xmath13 successive samples and take the average to be the representative sample . in our experiments , @xmath229 .",
    "the log - likelihood of an evidence is @xmath230 where @xmath105 .",
    "the gradient of @xmath192 w.r.t .",
    "the mapping parameter @xmath231 reads @xmath232 where we have moved the constant @xmath233 into the sum and integration and make use of the fact that @xmath234 where the domain of the gaussian is constrained to @xmath235 .    from the definition of the energy function in eq .",
    "( [ eq : energy - def ] ) , we know that the energy is decomposable , and thus the gradient w.r.t .",
    "@xmath231 only involves the pair @xmath236 .",
    "in particular @xmath237          one undesirable feature of the mcmc chains used in learning we have experiences so far is the tendency for the binary hidden states to get stuck , i.e. , after some point they do not flip their assignments as learning progresses .",
    "we conjecture that this phenomenon may be due to the _ saturation effect _ inherent in the factor posterior : @xmath240 i.e. , once the collected value to a node @xmath241 is too high or too low , it is very hard to over turn .",
    "fortunately , there is a known technique to regularise the chain : we enforce that at a time , there should be only a fraction @xmath128 of nodes which are active , where @xmath117 .",
    "one way is to maximise the following objective function @xmath242p(\\ub\\mid\\vb)d\\ub\\end{aligned}\\ ] ] where @xmath118 is the weighting factor , @xmath243 if @xmath205 and @xmath244 otherwise .",
    "@xmath246p(\\ub|\\vb)d\\ub\\\\   & = & \\partial_{g_{k}}\\ll+\\lambda\\int\\left[\\rho - p(h_{k}^{1}\\mid\\ub)\\right]p(\\ub|\\vb)d\\ub\\\\   & \\approx & \\partial_{g_{k}}\\ll+\\frac{1}{s}\\lambda\\sum_{s}\\left[\\rho - p(h_{k}^{1}\\mid\\ub^{(s)})\\right]\\end{aligned}\\ ] ]        [ [ online - estimation - of - posteriors - subonline - estimation - of - posteriors ] ] online estimation of posteriors [",
    "sub : online - estimation - of - posteriors ] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^    for tasks such as data completion ( e.g. , collaborative filtering ) we need the posteriors @xmath251 for the prediction phase .",
    "one way is to run the markov chain or doing mean - field from scratch .",
    "here we suggest a simple way to obtain an approximation directly from the training phase without any further cost . the idea is to update the estimated posterior @xmath252 at each learning step @xmath253 in an _ exponential smoothing _ fashion :          it is often of practical importance to track the learning progress , either by the reconstruction errors or by the data likelihood .",
    "the data likelihood can be estimated as @xmath260 where @xmath261 are those samples collected as learning progressed in the data - independent phase , and the integration can be carried out using the technique described in the main text .",
    "[ [ gumbel - distribution - for - categorical - choices - subgumbel - distribution - cat ] ] gumbel distribution for categorical choices [ sub : gumbel - distribution - cat ] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^      using laplace s approximation ( e.g. , via taylor s expansion of @xmath264 using the second - order polynomial around @xmath17 ) , we have @xmath265 renormalising this distribution , e.g. , by replacing @xmath266 by @xmath267 we obtain the standard gaussian distribution .",
    "thus , _ we can use the gumbel as an approximation to the gaussian distribution_.    now we turn to the categorical model using gumbel variables .",
    "we maintain one variable per category , which plays the role of the utility for the category .",
    "assume that all utilities share the same scale parameter @xmath263 .",
    "the existing literature @xcite asserts that the probability of choosing the @xmath268-th category is @xmath269 where @xmath270 is the location of the @xmath47-th utility .",
    "we rewrite the gumbel density function by changing variable from @xmath280 to @xmath281 : @xmath282 for @xmath283 .",
    "thus @xmath284 now , by changing variable under the integration from @xmath280 to @xmath281 , we have @xmath285      we now extend the case of categorical evidences rank evidences .",
    "again we maintain one gaussian variable per category .",
    "without loss of generality , for a particular rank @xmath286 we assume that we must ensure that @xmath287 .",
    "this is equivalent to @xmath288\\cap\\left[x_{2}>\\max_{l>2}x_{l}\\right]\\cap ...",
    "\\cap\\left[x_{d-1}>x_{d}\\right]\\ ] ] the probability of this is essentially @xmath289          * * q4 * ( _ ordinal _ ) : [ ... ] how would you describe the current economic situation in ( survey country ) \\{_very good , somewhat good , somewhat bad , or very bad _ } ? * * q11a * ( _ binary _ ) : how do you think people in other countries of the world feel about china ? ",
    "\\{_like , disliked _ } ? * * q35,35a * ( _ category - ranking _ ) : which one of the following , if any , is hurting the world s environment the most / second - most \\{_india , germany , china , brazil , japan , united states , russia , other _ } ? * * q76 * ( _ continuous _ ) : how old were you at your last birthday ? * * q85 * ( _ categorical _ ) : what is your current employment situation \\{_a list of employment categories _ } ?",
    "laplace approximation is the technique using a gaussian distribution to approximate another distribution . for the univariate case , assume that the original density distribution has the form",
    "@xmath291 first we find the mode @xmath17 of @xmath292 or equivalently the minimiser of @xmath293 given it exists .",
    "then we apply taylor s expansion @xmath294 the gaussian approximation has the form @xmath295 where @xmath296 is the new variance .",
    "[ [ some - properties - of - the - truncated - normal - distribution - subsome - properties - of - truncated - normal ] ] some properties of the truncated normal distribution [ sub : some - properties - of - truncated - normal ] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^      @xmath299}(x\\mid,\\mu,\\sigma)=\\frac{q(x^{*})}{\\sigma\\left[\\phi(\\beta^{*})-\\phi(\\alpha^{*})\\right]}\\ ] ] where @xmath300 and @xmath301 and @xmath302 are the probability density function and the cumulative distribution of the standard normal distribution , respectively . in particular , we are interested in the mean of the distribution @xmath303}$ ] : @xmath304 some special cases :    * when @xmath305 , this distribution reduces to the dirac s delta . * when @xmath306 , we have a one - sided truncation from above since @xmath307 . *",
    "when @xmath308 , we obtain a one - sided truncation form below since @xmath309 .",
    "j.  geweke .",
    "efficient simulation from the multivariate normal and student - t distributions subject to linear constraints and the evaluation of constraint probabilities . in _ computing science and statistics : proceedings of the 23rd symposium on the interface _ , pages 571578 , 1991 .",
    "t.  truyen , d.q phung , and s.  venkatesh .",
    "probabilistic models over ordered partitions with applications in document ranking and collaborative filtering . in _ proc . of siam conference on data mining ( sdm )",
    "_ , mesa , arizona , usa , 2011 ."
  ],
  "abstract_text": [
    "<S> we introduce _ thurstonian boltzmann machines _ ( @xmath0 ) , a unified architecture that can naturally incorporate a wide range of data inputs at the same time . </S>",
    "<S> our motivation rests in the thurstonian view that many discrete data types can be considered as being generated from a subset of underlying latent continuous variables , and in the observation that each realisation of a discrete type imposes certain inequalities on those variables . thus learning and inference in @xmath0 reduce to making sense of a set of inequalities . </S>",
    "<S> our proposed @xmath0 naturally supports the following types : gaussian , intervals , censored , binary , categorical , muticategorical , ordinal , ( in)-complete rank with and without ties . </S>",
    "<S> we demonstrate the versatility and capacity of the proposed model on three applications of very different natures ; namely handwritten digit recognition , collaborative filtering and complex social survey analysis .    </S>",
    "<S>  x </S>"
  ]
}