{
  "article_text": [
    "partitioned global address space ( pgas ) devises a method of parallel programming by introducing a unified global view of address space ( like purely shared memory systems ) and presiding over the distribution of data ( similar to a distributed memory system ) , in order to provide a programmer with ease of use and a locality - aware paradigm . to distribute the data across the system , pgas implementations use one  sided communication substrates , which are hidden from the application developer .",
    "unified parallel c ( upc ) is an implementation of the pgas model .",
    "upc has a single shared address space , which is partitioned among upc threads , such that a portion of shared address space resides in the memory local to a upc thread .",
    "upc provides mechanisms to distinguish between local and remote data accesses , thus allowing to capitalize on data locality .",
    "however , a programmer needs to perform custom coding , potentially even building advanced data distribution schemes , to exploit locality efficiently .",
    "dash is a c++ library that delivers distributed data structures and algorithms designed for modern hpc machines , which are well - suited for hierarchical network interconnections and memory stratum @xcite .",
    "dash aims at various domains of scientific applications , providing the programmer with advanced data structures and algorithms , consequently reducing the need for custom coding .",
    "dash calls dash run - time ( dart ) , which provides basic functionalities to build a pgas model using state of the art one - sided communication libraries @xcite .",
    "these functionalities include :    * global memory management and optimization for accessing data that reside on shared memory system @xcite * creation , destruction and management of teams and groups * collective and non - collective communication routines * synchronization primitives    this paper presents dash as an alternative to traditional pgas implementations like upc .",
    "short - comings of upc and other traditional pgas implementations are discussed , which in many cases prevent an effective use of the pgas paradigm .",
    "furthermore , features of dash overcoming these short  comings are presented .",
    "the main contributions of this paper are :    1 .",
    "we present an advanced local copy feature in dash 2 .",
    "we evaluate the throughput of the local copy feature 3 .",
    "we present an automatic hierarchical units mapping mechanism for applications having a nearest neighbor communication pattern 4 .",
    "we evaluate the applicability of automatic hierarchical units mapping mechanism on a 3d stencil communication kernel on cray xc40 hazel hen machine at hlrs",
    "from our experience with upc for our in  house molecular dynamics code , we highlighted three major issues , resulting in severe performance degradation @xcite .",
    "these issues  and a further problem regarding hardware topology  are :    1 .",
    "manual pointer optimization is necessary for fast access to local data ( using local pointer ) 2 .",
    "non - trivial data distribution schemes need to be implemented by hand 3 .",
    "communication is performed at the same granularity as data access 4 .",
    "no mechanism available for co  locating strongly interacting units on the given hierarchical hardware topology    the first issue regards the failure of upc compilers to automatically distinguish between shared and distributed memory data accesses , even though the complete data layout is available .",
    "this holds true for both static and dynamic allocation of data ( as the block size of a distributed shared array needs to be a compile  time constant ) .",
    "this can lead to a significant performance degradation and can only be avoided by expert programmer intervention .",
    "manual optimization requires checking all parts of the code where significant data accesses are performed and switching to local pointers for local memory access .",
    "the second issue is that upc provides only round robin and blocked data distribution schemes .",
    "these schemes are suboptimal for many applications featuring some sort of short range geometric data accessing patterns , e.g. stencil patterns .",
    "this will lead to a unnecessarily high amount of communication traffic and percentage of remote communication . to avoid this problem",
    ", the programmer has to write specific data mapping routines .",
    "the third issue is associated with the communication granularity . in",
    "shared memory address space , the programmer can directly access and modify the shared data .",
    "the pgas paradigm also provides these attributes for its global address space .",
    "however , accessing and modifying remote data is expensive , especially if it leads to many small communications . as upc does not change the granularity",
    ", this often leads to a vast number of tiny communications . to avoid this problem ,",
    "the programmer needs to take the underlying distributed memory architecture into account and perform the necessary optimizations for packing communications manually .",
    "the fourth issue addresses the difficulty of adapting the behavior of an application to the machine topology .",
    "for example , reordering the placement of software units that may allow to reduce the communication cost by placing the interacting partners closer to each other on the physical hardware .    to summarize : in order to achieve a near optimal performance using traditional pgas implementations , the programmer needs to take care of a variety of issues manually .",
    "this contradicts the driving idea behind pgas : ease of programmability .",
    "the good news is , dash is tackling these issues by providing automatic optimization for faster local data accesses @xcite , advanced data distribution schemes @xcite , algorithm specific routines for pre - fetching and packing of data and automatic hierarchical units mapping .",
    "the following section provides a detailed illustration of these advance features .",
    "dash resolves the short  comings of the traditional pgas implementations with its automatic optimizations , advanced data structures and algorithms .",
    "we will now explain a few specific features of dash which address the problems highlighted in the previous section .",
    "the automatic detection of local vs. remote data access in dash demonstrates an effective use of pgas paradigm : every data access is performed in the most efficient way available .",
    "this automatic behavior is achieved by capitalizing on the shared memory window feature of mpi-3 @xcite used in the mpi version of dart ( dart  mpi ) .",
    "the shared memory window can be accessed directly by local mpi processes using load / store operations ( zero  copy model ) , allowing the processes to circumvent the single - copy model of the mpi layer .",
    "dart - mpi maps both global and shared memory windows to the same shared memory region , thus allowing the dart units on shared memory to directly access the local memory region .",
    "the dart units that are not part of the shared memory window , perform rma operations using a global window .",
    "furthermore , the use of the zero copy model for intra - node communication in dart scales down the memory bandwidth problem .",
    "we have demonstrated in @xcite , our optimization of mapping both shared and global memory windows to the same memory region on shared memory , enables faster intra - node communication .",
    "this allows dash programmers to iterate over distributed data structures without worrying about slow local data accesses , unlike upc where manual pointer optimizations are necessary to avoid less efficient local data accesses @xcite .",
    "dash features several data distribution schemes ( _ patterns _ ) that provide highly flexible configurations depending on data extents and logical unit topology .",
    "new pattern types are continuously added to dash .",
    "this flexibility leads to a large number of data distributions that can be used for a single use case .",
    "the preferable pattern configurations depend on the specific use case .",
    "algorithms operating on global address space strictly depend on domain decomposition as they expect data distributions that satisfy specific properties .    without methods that help to configure data distributions",
    ", programmers must learn about the differences between all pattern implementations and their restrictions .",
    "we therefore provide high - level functions to automatically optimize data distribution for a given algorithm and vice - versa .",
    "these mechanism are described in detail in @xcite . in this",
    "we present a classification of data distribution schemes based on well - defined properties in the three mapping stages of domain decomposition : partitioning , mapping , and memory layout .",
    "this classification system serves two purposes :    * provides a vocabulary to formally describe general data distributions by their semantic properties ( _ pattern traits _ ) . *",
    "specifies constraints on expected data distribution semantics .    as an example , the _ balanced partitioning _",
    "property describes that data is partitioned into blocks of identical size .",
    "an algorithm that is optimized for containers that are evenly distributed among units can declare the _ balanced partitioning _ and _ balanced mapping _ properties as constraints .",
    "a mapping is balanced if the same number of blocks is mapped to every unit .    when applying the algorithm on a container , its distribution is then checked against the algorithm s constraints already at compile time to preventing inefficient usage :    .... static_assert (     dash::pattern_contraints <         pattern ,         partitioning_properties <           partitioning_tag::balanced > ,         mapping_properties <",
    "mapping_tag::balanced > ,         layout_properties < ... >     > : : satisfied::value ) ; ....    finally , pattern traits also allow to implement high - level functions that resolve data distribution automatically . for this , we use simple constrained optimization based on type traits in c++11 to create an instance of a initially unspecified pattern type that is optimized for a set of property constraints . to be more specific ,",
    "the automatic resolution of a data distribution involves two steps : at compile time , the pattern type is deduced from constraints that are declared as type traits .",
    "then , an optimal instantiation of this pattern type is resolved from distribution constraints and run - time parameters such as data extents and team size .    ....",
    "teamspec<2 > logical_topo(16 , 8) ; sizespec<2 > data_extents(extent_x , extent_y ) ; // deduce pattern : auto pattern =    dash::make_pattern <      partitioning_properties <        partitioning_tag::balanced > ,      mapping_properties <",
    "mapping_tag::balanced >    > ( sizespec , teamspec ) ; ....    deduction of data distribution can also interact with team specification to find a suitable logical cartesian arrangement of units . as for domain decomposition",
    ", dash provides traits to specify preferences for logical team topology such as  compact \" or ",
    "node - balanced \" . as a result ,",
    "application developers only need to state a use case , such as dgemm , and let unit arrangement and data distribution be resolved automatically .",
    "while no automation can possibly do away with the need for manual optimization in general , automatic deduction as provided by dash greatly simplifies finding a configuration that is suitable as a starting point for performance tuning . in comparison ,",
    "finding practicable blocking factors and process grid extents for scalapack , even in seemingly trivial use cases , is a challenging task for non - experts .",
    "algorithms and container types in dash follow the concepts and semantics of their counterparts in the c++ standard template library ( stl ) and are consequently based on the iterator concept .",
    "algorithms provided by the stl can also be applied to dash containers and most have been ported to dash providing identical semantics .",
    "programmers will therefore already be familiar with most of the api concepts .",
    "for copying data ranges , the standard library provides the function interface ` std::copy ` . in dash , a corresponding interface ` dash::copy ` is provided for copying data in pgas .",
    "this section presents the concept of the the functions ` dash::copy ` and ` dash::copy_async ` , first - class citizens in the dash algorithm collection which represent a uniform , general interface for copy operations within global address space .    as an example , compare how an array segment is copied using standard library and using dash :    .... double range_copy[ncopy ] ; //",
    "stl variant : std::copy(arr.begin ( ) , arr.begin ( ) + ncopy ,            range_copy ) ; //",
    "dash variant , on a global array : dash::copy(arr.begin ( ) , arr.begin ( ) + ncopy ,             range_copy ) ; ....    asynchronous variants of data movement operations are essential to enable overlap of communication and computation .",
    "they employ the _ future _ concept also known from the standard library :    .... double range_copy[ncopy ] ; auto fut_copy_end = dash::copy_async (                        arr.begin ( ) ,                        arr.begin ( ) + ncopy ,                        range_copy ) ; //",
    "blocks until completion and returns result : double * copy_end = fut_copy_end.wait ( ) ; ....    copying data from global into to local memory space is a frequent operation in pgas applications",
    ". it can involve complex communications pattern as some segments of the copied range might be placed in memory local to the requesting unit s core while others are owned by units on distant processing nodes .",
    "performance of copy operations in partitioned global address space is optimized by avoiding unnecessary data movement and scheduling communication such that interconnect capacity is optimally exploited .",
    "we use the following techniques in dash , among others :    shared memory : :    for segments of the copied data range that are located on the same    processing node as the destination range , ` std::copy ` is used to copy    data in shared memory .",
    "this reduces calls to the communication    back - end to the unavoidable minimum .",
    "this is not restricted to    copying : dash algorithms in general automatically distinguish between    accesses in shared and distributed memory . and",
    "even when not using    dash algorithms , the dash runtime automatically resorts to shared    window queries and ` memcpy ` instead of mpi communication primitives    for data movement within a processing node .",
    "chunks : :    to achieve optimal throughput between processing nodes , communication    of data ranges is optimized for transmission buffer sizes by splitting    the data movement into chunks .",
    "adequate chunk sizes are obtained from    auto tuning or interconnect buffer sizes provided in environment    variables by some mpi runtimes .",
    "communication scheduling : :    parallel transmission capacity is exploited whenever possible : if the    data source range spans the address space of multiple units , separate    asynchronous transmissions for every unit are initiated instead of a    sequence of blocking transmissions .",
    "also , the single asynchronous    operations are then ordered in a schedule such that communication is    balanced among all participating units to fully utilize interconnect    capacity .",
    "the shared memory optimization technique require means to logically partition a global data range into local and remote segments .",
    "for this , dash provides the utility function ` dash::local_range ` that partitions a global iterator range into local and remote sub - ranges :    .... dash::array < t > a(size ) ; //",
    "get iterator ranges of local segments in //",
    "global array : std::vector < dash::range < t > > l_ranges =    dash::local_range(a.begin ( ) + 42 , a.end ( ) ) ; // iterates local segments in array : for ( l_range : l_ranges ) {    // iterates values in local segments :    for ( l_value : l_range ) {      // ...    } } ....    the global iterator range to partitioned into locality segments may be multidimensional .",
    "in addition , dash containers provide methods to access blocks mapped to units directly so that programmers do not have to resolve partitions from domain decomposition themselves . for example , sub - matrix blocks can be copied in the following way :    .... dash::matrix<2 , double > matrix ; //",
    "first block in matrix assigned to //",
    "active unit , i.e. in local memory space : auto l_block = matrix.local.block(0 ) ; //",
    "specify matrix block by global block //",
    "coordinates , i.e. in global memory space : auto m_block = matrix.block ( { 3 , 5 } ) ; // copy local block to global block : dash::copy(l_block.begin ( ) ,             l_block.end ( ) ,             m_block.begin ( ) ) ; ....    the ` dash::copy ` function interface and the ` dash::matrix ` concept greatly simplify the implementation of linear algebra operations .",
    "efficiency of the underlying communication is achieved without additional effort of the programmer due to the optimization techniques presented in this section .",
    "the mapping of an application s software units ( or threads / processes / tasks ) to the physical cores on an hpc machine is becoming increasingly important due to the rapid increase in the number of cores on a machine @xcite@xcite .",
    "this also leads to increasingly hierarchical networks and  depending on the underlying system and the submitted job  sparse core allocations .",
    "therefore , if two units of an application which are interacting partners ( or communicating more frequently than average ) are placed far from each other in the network , they will have to communicate through several levels in the network hierarchy .",
    "the placement of these units not only has repercussions in their communication latency and bandwidth , but may also result in the congestion of the network links .    as the units mapping plays a vital role , dart - mpi provides dash with the mean to automatically reorder the units which respect both the communication pattern of an application and the topology of allocated nodes on an hpc machine .",
    "this results in reduced communication and overall execution time of an application .",
    "currently we counter this problem for a specific set of applications which are based on nearest neighbor communication .",
    "the programmer only needs to inform dash that the application to be executed has a nearest neighbor communication pattern .",
    "the automatic mapping routine then gathers the required hardware topology , computes a new hierarchical unit mapping and registers it in the system . the new mapping is determined based on an approach similar to a hilbert space filling curve ( hsfc ) partitioning @xcite .",
    "an example of hsfc is shown in figure [ hilbert ] .",
    "hsfc is chosen due to its property of preserving the locality .",
    "the algorithm however does not fix the length ( number of elements to iterate over in a dimension ) of hsfc for multiple levels as the lengths are dependant upon the number of nodes corresponding to each level in the network hierarchy .          before discussing the steps in the automatic hierarchical unit mapping algorithm",
    ", we briefly explain the hierarchical network levels of the cray xc40 supercomputer hazel hen in the following .",
    "the first off - node network hierarchical level is a _",
    "compute blade_. a compute blade is composed of four nodes which share an aries chip .",
    "the aries chip connects these four nodes with the network interconnect .",
    "this is the fastest connection between nodes .",
    "the second level is the _ rank 1 network _ ( or backplane ) .",
    "the rank 1 network is used for inter - compute blades communication within a chassis  set of 16 compute blades ( 64 nodes ) .",
    "the rank 1 network has adaptive routing per packet .",
    "any two nodes at this level communicate with each other by going first through their aries chip , then through the backplane , and finally to the aries chip of the target node .",
    "this is an all - to - all pc board network .    the _ rank 2 network _",
    "is used for inter - backplane communication ( nodes on distinct chassis ) in a two cabinet group ( a group is composed of 384 nodes ) .",
    "the backplanes are connected through copper cables .",
    "all copper and backplane signals run at 14 gbps .",
    "the minimal route between two nodes on distinct chassis is two hops , whereas the longest route requires four hops .",
    "the aries adaptive routing algorithm is used to select the best route from four routes in a routing table .",
    "the rank 2 network also has an all - to - all connection , connecting 6 chassis ( two cabinets ) .",
    "the last off - node network level is the _ rank 3 network _ , which is used for communication between different groups .",
    "the rank 3 network has all - to - all routing using optical cables .",
    "if minimal path between two groups is congested , traffic can be hopped through any other intermediate group ( 1 or 2 hops ) .",
    "the layered layout of network hierarchy of hazel hen is show in figure [ hazel - hen ] .",
    "the automatic hierarchical units mapping algorithm is executed by every dash unit and assumes that the user has performed a binding of the units to the cpu cores , such that the units do not migrate from one cpu to another . every unit performs the following steps :    1 .   acquires the _ total number of units _ in the team ( to be mapped on the hierarchical topology ) .",
    "2 .   acquires its processor name and parses it to obtain the _",
    "node i d _ on which the unit resides .",
    "participates in the collective allgather operation to obtain the node ids of all units ( units on the same node have the same node i d ) and uses the node i d as a key to look for the placement information string of the node inside a topology file of the machine .",
    "4 .   reads topology file to acquire placement information string , number of sockets and number of cores per socket , for each allocated node .",
    "parses the placement information string of each node in order to obtain the value of each hierarchical level of the machine corresponding to each node .",
    "sorts the nodes with respect to all levels in the _ network hierarchy _",
    "i.e. at first performing the sorting according to the values of every node on level[4 ] , then on level[3 ] and so on .",
    "for example : + level(4 , 3 , 2 , 1 , 0 ) = ( 0 , 0 , 1 , 12 , 3)level(4 , 3 , 2 , 1 , 0 ) = ( 0 , 0 , 1 , 13 , 0)level(4 , 3 , 2 , 1 , 0 ) = ( 0 , 0 , 1 , 13 , 1)level(4 , 3 , 2 , 1 , 0 ) = ( 9 , 1 , 2 , 1 , 1 ) 7 .",
    "determines balanced distribution of total number of units in a cartesian grid .",
    ".   performs balanced distribution of units per node , to form multi ",
    "core groups in order to reduce inter - node communication ( for example : a balanced distribution for 24 cores as in hazel hen would be @xmath0 .",
    "the lengths of coordinate directions of the 3d cartesian grid ( x , y , z ) of total number of units should be divisible by the cartesian grid of units per node ( e.g. ( 4,3,2 ) ) .",
    "this is necessary as our reordering method ( algorithm [ mapping - algorithm ] ) performs multi  core grouping at the node level and therefore the number of groups in each coordinate direction should fit the cartesian grid of total number of units .",
    ".   assigns new unit i d to each unit taking into consideration the multi ",
    "level network hierarchy , i.e. multicore groups of units are mapped as close as possible in the network hierarchy in order to reduce communication between distinct network hierarchy levels .",
    "finally , the reordered unit ids are registered in the system .",
    "after the last step , the new mapping is completed .",
    "the algorithm will result in an optimal units mapping if the node allocation is contiguous on all network hierarchy levels .",
    "optimal being the minimal surface area , which results in minimal communication traffic on all network hierarchy levels .",
    "if the nodes are allocated in a sparse manner , the algorithm attempts to preserve the locality through its hsfc  like implementation .",
    "figure [ fig : units - mapping ] shows an example of automatic hierarchical units mapping for a 2d nearest neighbor communication pattern .",
    "please note that the example is just for elaborating the hierarchical units mapping and does not reflect the actual number of hardware instances at each network hierarchy level . ]",
    "@xmath1 @xmath2 $ ] @xmath3 $ ] @xmath4 $ ] @xmath5 $ ] @xmath6 $ ] @xmath7 $ ]",
    "optimization techniques employed in copying data in global memory space have been discussed in [ subsec : local - copy ] . in the following ,",
    "we evaluate the _ local copy _ use case where a unit creates a local copy of a data range in global memory .",
    "we consider the following scenarios , named by the location of the copied data range :    local : :    both source- and destination range are located at the same unit .",
    "this    scenario does not involve communication as ` dash::copy ` resorts to    copying data directly in shared memory . to illustrate the maximum    achievable throughput",
    ", this scenario is also evaluated using    ` std::copy ` .",
    "socket : :    the data range to be copied and the copy target range are owned by    units mapped to different sockets on the same processing node . in this    case",
    ", communication is avoided by dart recognizing the data movement    as an operation on shared memory windows .",
    "remote : :    the source range is located on a remote processing unit and is copied    in chunks using ` mpi_get ` .    for meaningful measurements ,",
    "it is essential to avoid a pitfall regarding cache effects : if the copied data range has been initialized by a unit placed on the same processing node as the unit creating the local copy , the data to be copied is stored in l3 data cache shared by both units . in this case , the ` local ` and ` socket ` scenarios would effectively measure cache bandwidth instead of the more common and less convenient case where copied data is not available from cache .",
    "the local copy benchmark has been executed on supermuc phase 2 nodes for the available mpi implementations intel mpi , ibm mpi , and openmpi .",
    "the mpi variants each exhibit specific advantages and disadvantages :    the installation of ibm mpi does not support mpi shared windows , effectively disabling the optimization in the dash runtime for the _ socket _ scenario , but offers the most efficient non - blocking rdma .",
    "intel mpi requires additional polling processes for asynchronous rdma which increases overall communication latency .",
    "the benchmark application has been compiled using the intel compiler ( icc ) version 15.0 . apart from being linked with different mpi libraries , the build environment is identical for every scenario .",
    "the results from all scenarios for the three mpi implementations is shown in figure [ fig : local - copy ] .    as a first observation , performance of ` std::copy ` varies with the mpi implementation used .",
    "this is because different c standard libraries must be linked for the respective mpi library .",
    "this also explains why cache effects become apparent for different range sizes in the _ local _ scenarios . in general , performance of local copying",
    "is expected to decrease for ranges greater than 32 kb which is the capacity of l1 data cache on supermuc haswell nodes .",
    "the c standard library linked for openmpi sustains better performance for larger data sizes compared to the other evaluated mpi variants .    when copying very small ranges in the _ local _ scenario the constant overhead in `",
    "dash::copy ` introduced by index calculations outweighs communication cost .",
    "still , the employed shared memory optimization leads to improved throughput compared to mpi operations used in the _ remote _ scenario .    for copied data sizes of roughly 64 kb and greater , `",
    "dash::copy ` achieves the maximum throughput measured using ` std::copy ` .",
    "this corresponds approximately to a minimum of a @xmath8 block of double - precision floating point values and is far below common data extents in real - world use cases .",
    "as expected , achieved throughput in the _ socket _ and _ remote _ scenarios are comparable for ibm mpi as shared window optimizations are not available and mpi communication is used to move data within the same node . fortunately , ibm mpi also exhibits the best performance for mpi communication . for ranges of 1 mb and larger , there is no significant difference between local and remote copying .    it might seem surprising that throughput in the _ socket _ scenario , where data is copied between numa domains , exceeds throughput in scenario _ local _ in some cases . however , in the _ socket _ scenario , data is copied in the dash runtime using ` memcpy ` instead of ` std::copy ` .",
    "the different low - level variants are expected to yield different performance and again depend on the c standard library linked .",
    "figure [ fig : local - copy - remote ] summarizes achieved throughput in the _ remote _ scenario of all mpi implementations in a single plot for comparison .",
    "results from this micro - benchmark can serve to auto - tune partition sizes used to distribute container elements among units .",
    "for example , a minimum block size of 1 mb is preferable for ibm mpi while block sizes between 1 to 16 mb should be avoided for intel mpi and openmpi as numa effects decrease performance otherwise .",
    "we now evaluate the performance of 3d stencil communication kernel with and without using the automatic hierarchical units mapping feature . in order to measure solely the impact of hierarchical units mapping on the performance",
    ", we have disabled the shared memory window feature of dart - mpi for the benchmark shown later .      in this stencil communication kernel ,",
    "each unit communicates with six neighbors ( left , right , upper , lower , front , and back ) .",
    "we use the blocking dart put operation for transferring the messages from one unit to another and the size of the messages is varied exponentially from 1 byte to 2 megabytes .",
    "we are interested in evaluating the _ relative performance improvement factor _ , which is computed by taking the ratio of the average execution times ( ten  thousand iterations ) of stencil communication kernel using default ( as performed by the job launcher on the cray machine ) against hierarchical units mapping .",
    "the benchmarks are carried out on the cray xc40 machine hazel hen at hlrs .",
    "each node on hazel hen is based on intel xeon cpu e5 - 2680 v3 ( 30 m cache , 2.50 ghz ) processors and comprises 24 cores ( 12 cores per socket ) .",
    "cray s aries interconnect provides node - node connectivity with multiple hierarchical network levels .",
    "we have two on node memory hierarchy levels , which are uniform memory access ( uma )  intra - socket communication  and non - uniform memory access ( numa )  inter - socket communication .",
    "it s easy to exploit the on node hierarchical levels . in this paper",
    "we are more interested in showing results by exploiting the off - node network hierarchical levels ( section [ network - hierarchy ] ) .",
    "figure [ hazel - hen - latency ] shows the average latency of messages up to the rank 1 network of hazel hen , highlighting the impact of different network levels .",
    "figure [ mapping - result ] shows the relative performance improvement factor of the 3d stencil communication kernel on 384 nodes ( 9,216 units ) of hazel hen .",
    "the nodes were allocated in a sparse manner by cray s job launcher , having small contiguous blocks of nodes .",
    "it can be seen that our units mapping provides an average performance improvement by a factor of 1.4 to 2.2 .",
    "in this paper we have presented specific features of dash which resolve some issues we observed in traditional pgas implementations .",
    "we have shown in section [ sec : eval - stencil ] that our automatic hierarchical units mapping provides a notable performance improvement over default units mapping .",
    "a user can take advantage of this self  adapting behavior without putting any effort into understanding the complex machine hierarchy or performing any custom coding .",
    "furthermore , new features are presented enabling a user to represent the computation and communication patterns of scientific applications at a very high level of abstraction in dash , while dash will take care of necessary code transformations .",
    "we are currently working on extending methods for automatic data distribution to data flow scenarios .",
    "however , automatic optimization in many data flow use cases is conceptually equivalent to integer programming and thus proven to be np - hard .",
    "we assume that solutions for a useful subset of scenarios can be found using linear programming techniques like the simplex algorithm .",
    "karl frlinger , colin glass , jose gracia , andreas knpfer , jie tao , denis hnich , kamran idrees , matthias maiterth , yousri mhedheb , and huan zhou .",
    "dash : data structures and algorithms with support for hierarchical locality . in _ euro - par 2014 : parallel processing workshops _ , pages 542552 .",
    "springer , 2014 .",
    "kamran idrees , christoph niethammer , aniello esposito , and colin  w glass .",
    "performance evaluation of unified parallel c for molecular dynamics . in _ proceedings of the 7th international conference on pgas programming models _",
    ", page 237 .",
    "huan zhou , yousri mhedheb , kamran idrees , colin  w glass , jos gracia , and karl frlinger .",
    "dart - mpi : an mpi - based implementation of a pgas runtime system in _ proceedings of the 8th international conference on partitioned global address space programming models _",
    ", page  3 .",
    "acm , 2014 .",
    "mehmet deveci , sivasankaran rajamanickam , vitus  j leung , kevin pedretti , stephen  l olivier , david  p bunde , umit  v catalyurek , and karen devine .",
    "exploiting geometric partitioning in task mapping for parallel computers . in _",
    "parallel and distributed processing symposium , 2014 ieee 28th international _ , pages 2736 .",
    "ieee , 2014 ."
  ],
  "abstract_text": [
    "<S> dash is a library of distributed data structures and algorithms designed for running the applications on modern hpc architectures , composed of hierarchical network interconnections and stratified memory . </S>",
    "<S> dash implements a pgas ( partitioned global address space ) model in the form of c++ templates , built on top of dart  a run - time system with an abstracted tier above existing one - sided communication libraries .    in order to facilitate the application development process for exploiting the hierarchical organization of hpc machines </S>",
    "<S> , dart allows to reorder the placement of the computational units . in this paper </S>",
    "<S> we present an automatic , hierarchical units mapping technique ( using a similar approach to the hilbert curve transformation ) to reorder the placement of dart units on the cray xc40 machine hazel hen at hlrs . to evaluate the performance of new units mapping which takes into the account the topology of allocated compute nodes , we perform latency benchmark for a 3d stencil code . </S>",
    "<S> the technique of units mapping is generic and can be be adopted in other dart communication substrates and on other hardware platforms .    furthermore , high  </S>",
    "<S> level features of dash are presented , enabling more complex automatic transformations and optimizations in the future .    </S>",
    "<S> = 1    dash , dart , pgas , upc , mpi , self - adaptation , code transformations </S>"
  ]
}