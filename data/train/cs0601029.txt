{
  "article_text": [
    "we consider the situation where a memoryless bi - variate gaussian source is to be transmitted over an additive white gaussian multiple - access channel with two transmitting terminals and one receiving terminal .",
    "each of the two source components is fed to a different average - power constrained encoder .",
    "our interest lies in the achievable expected squared - error distortion region .",
    "we show that in the symmetric case , where the source components are of the same variance and the transmitting terminals are subjected to the same average power constraint , uncoded transmission is optimal below a threshold signal - to - noise ratio ( snr ) that is determined by the correlation between the source components . for snrs above this threshold we provide outer and inner bounds on the achievable distortions .",
    "the problem at hand can be viewed as the gaussian version of the problem addressed by cover , el gamal and salehi @xcite ( see also @xcite and @xcite ) .",
    "it also appears to be closely related to the quadratic gaussian ceo problem @xcite and the quadratic gaussian two - terminal source - coding problem @xcite .",
    "however , it differs in character from the ceo problem and from the two - terminal source coding problem in that no error - free bit - pipes of finite rates can be assumed .",
    "this is due to the fact that the source - channel separation theorem does not apply to our situation .",
    "furthermore , the ceo problem focuses on the reconstruction of a single gaussian random variable , whereas in our case the interest lies in the reconstruction of both source components .",
    "the time-@xmath0 output @xmath1 of the discrete - time two - user additive white gaussian multiple - access channel is given by @xmath2 where @xmath3 denotes the time-@xmath0 symbol transmitted by the first transmitter , @xmath4 is the time-@xmath0 symbol transmitted by the second transmitter , and @xmath5 denotes the time-@xmath0 noise term .",
    "the noise terms @xmath6 are independent identically distributed ( iid ) zero - mean variance-@xmath7 gaussian random variables that are independent of the input sequences @xmath8 .",
    "we shall consider the case where transmitter  1 and transmitter  2 are average - power limited to @xmath9 and @xmath10 respectively .",
    "see ( [ eq : power ] ) ahead .    at time",
    "@xmath0 the source emits the pair @xmath11 where the @xmath12 are iid zero - mean gaussians of covariance @xmath13 with @xmath14 $ ] , and @xmath15 , @xmath16 .",
    "the sequence @xmath17 is fed to transmitter  1 and the sequence @xmath18 is fed to transmitter  2 .",
    "based on the channel output we wish to reconstruct the source vector .",
    "the performance criterion we focus on is the expected squared - error distortions in reconstructing each of the components of the source vector .",
    "given @xmath19 , @xmath14 $ ] , and @xmath20 we say that the tuple @xmath21 is achievable if there exists a sequence of encoder pairs @xmath22 @xmath23 and a sequence of reconstruction pairs @xmath24 @xmath25 such that the average power constraints are satisfied @xmath26 and @xmath27 \\leq d_{i},\\\\ \\quad i=1,2,\\end{gathered}\\ ] ] whenever @xmath28 are iid zero - mean bi - variate gaussian vectors of covariance matrix @xmath29 as above and @xmath6 are iid zero - mean variance-@xmath7 random variables that are independent of @xmath28",
    ". here we used the shorthand notation where @xmath30 denotes @xmath31 and similarly for @xmath32 .",
    "the problem we address here is , for given @xmath33 , to find the set of pairs @xmath34 such that @xmath35 is achievable .    by the symmetric version of this problem",
    "we shall refer to the case where @xmath36 , where @xmath37 , and where we seek the set of pairs @xmath38 that are achievable .",
    "that is , if we set @xmath39 and @xmath40 then we are interested in @xmath41",
    "before discussing our results , we make three remarks regarding the general nature of the problem .",
    "the firs two remarks show that there is no loss in generality by assuming that the correlation coefficient is non - negative and that the source components are of equal variance . as a consequence we shall assume for the remainder that @xmath42 and that @xmath43 $ ] .",
    "the third remark addresses a convexification issue of the distortion regions .    1 .",
    "the optimal distortion region depends on the correlation coefficient only via its absolute value @xmath44 .",
    "that is , the tuple @xmath45 is achievable if , and only if , the tuple @xmath46 is achievable .",
    "+ to see this note that if @xmath47 achieves the distortion @xmath34 for the source of correlation coefficient @xmath48 , then @xmath49 where @xmath50 and @xmath51 achieves @xmath34 on the source with correlation coefficient @xmath52 .",
    "2 .   the optimal distortions scale linearly with the source variances .",
    "that is , if @xmath53 are positive then @xmath54 is achievable if , and only if , @xmath55 is achievable .",
    "consequently , there is a simple linear transformation from the set of tuples @xmath34 for which @xmath56 is achievable and the set of tuples @xmath57 for which @xmath58 is achievable .",
    "+ to see this note that if @xmath47 demonstrate the achievability of @xmath59 then the encoders @xmath60 and the reconstructions @xmath61 demonstrate the achievability of the tuple @xmath62 .",
    "+ applying the same argument in the other direction with scalings by @xmath63 and @xmath64 concludes the proof .",
    "3 .   the achievable distortion is a convex function of the power constraints @xmath65 .",
    "that is , if @xmath66 and @xmath67 are achievable then @xmath68 is achievable for any @xmath69 $ ] , where @xmath70 .",
    "+ this follows by a simple time - sharing argument",
    "we present necessary conditions as well as sufficient conditions for achievability . in certain cases they agree .",
    "the proofs of those conditions will be discussed in the next section .",
    "our first result is a necessary condition for the achievability of @xmath71 .",
    "[ thm : converse ] a necessary condition for the achievability of @xmath72 is that @xmath73 where the expression for @xmath74 varies , depending on the values of @xmath75 .",
    "there are three cases .",
    "if @xmath75 are in the set @xmath76 then @xmath77 if @xmath75 are in the set @xmath78 then @xmath79 and if @xmath75 are in the set @xmath80 then @xmath81    [ cor:1 ] in the symmetric case where @xmath82 , we obtain @xmath83\\\\[5 mm ] \\sigma^2 \\sqrt{\\frac{(1-\\rho^2)n}{2p(1+\\rho ) + n } } & \\text{for } \\frac{p}{n } > \\frac{\\rho}{1-\\rho^2}. \\end{array } \\right.\\ ] ]    * note : * theorem  [ thm : converse ] can be easily extended to a much wider class of sources and distortions . indeed , if the source is any memoryless bi - variate source ( not necessarly zero - mean gaussian ) and if the fidelity measures @xmath84 that are used to measure the distortion in reconstructing each of the source components are arbitrary , then the pair @xmath34 is achievable with powers @xmath85 only if @xmath86 \\text{such that } & \\e{(s_1 -\\widehat{s}_1)^2 } \\leq d_1 , \\nonumber \\\\ & \\e{(s_2 -\\widehat{s}_2)^2 } \\leq d_2 , \\nonumber\\end{aligned}\\ ] ] does not exceed @xmath87 where @xmath88 is the hirschfeld - gebelein - rnyi maximal correlation between @xmath89 and @xmath90 : @xmath91 where the supremum is over all functions @xmath92 under which @xmath93    we next present two sufficient conditions for the achievability of @xmath94 .",
    "the first is obtained by analyzing uncoded transmission .",
    "[ thm : uncoded ] for @xmath94 to be achievable it suffices that both of the following conditions hold : @xmath95 d_2 & \\geq \\sigma^2 \\left ( \\frac{2p_2 + 4\\rho \\sqrt{p_1 p_2 } +    ( 1+\\rho^2)p_1 + n -2\\sqrt{p_2 + 2\\rho \\sqrt{p_1 p_2 } + \\rho^2 p_1 }    ( \\sqrt{p_2}+ \\rho \\sqrt{p_1})}{p_1 + p_2 + \\rho \\sqrt{p_1 p_2 } +    n } \\right).\\end{aligned}\\ ] ]    [ cor:2 ] in the symmetric case @xmath96    combining corollary [ cor:1 ] and corollary [ cor:2 ] , we obtain :    [ cor:3 ] for the symmetric case , @xmath97 i.e. , uncoded transmission is optimal for all @xmath98 .",
    "the second sufficient condition follows from analyzing the scheme where the encoding functions @xmath99 , @xmath100 , are randomly generated independent rate-@xmath101 vector quantizers , i.e.  the channel inputs are the rate-@xmath101 vector quantized source sequences .",
    "[ thm : coded ] the tuple @xmath94 is achievable whenever there exist rates @xmath102 and @xmath103 such that all of the following hold : @xmath104 r_2 & < \\frac{1}{2 } \\log_2 \\left (    \\frac{p_2(1-\\tilde{\\rho}^2)+n}{n(1-\\tilde{\\rho}^2 ) } \\right)\\\\[3 mm ] r_1 + r_2 & < \\frac{1}{2 } \\log_2 \\left ( \\frac{p_1+p_2      + 2\\tilde{\\rho}\\sqrt{p_1p_2}+n}{n(1-\\tilde{\\rho}^2 ) } \\right)\\\\[3 mm ] d_1 & > \\sigma^2 2^{-2r_1 } \\cdot \\frac{1-    \\rho^2(1 - 2^{-2r_2})}{1-\\tilde{\\rho}^2}\\\\ d_2 & >",
    "\\sigma^2 2^{-2r_2 } \\cdot \\frac{1-    \\rho^2(1 - 2^{-2r_1})}{1-\\tilde{\\rho}^2}.\\end{aligned}\\ ] ] where @xmath105 .",
    "[ cor:4 ] in the symmetric case @xmath106 is achievable if there exists some @xmath107 satisfying @xmath108 d & > \\sigma^2 2^{-2r } \\cdot \\frac{1-    \\rho^2(1 - 2^{-2r})}{1-\\rho^2(1 - 2^{-2r})^2}. \\label{eq : dist - vect - sym}\\end{aligned}\\ ] ]    here the rhs of ( [ eq : dist - vect - sym ] ) is monotonically decreasing in @xmath109 . evaluating corollary [ cor:4 ] and corollary [ cor:1 ] for @xmath110",
    "we get :    [ cor:5 ] in the symmetric case @xmath111    we conclude this section with a note on the superposition of the two discussed coding schemes .    *",
    "note : * we have analyzed two coding schemes ; uncoded transmission and transmission of vector - quantized source sequences .",
    "the superposition of those two schemes , analogous to the scheme discussed for the single - user case in @xcite , seems to yield strict improvements of the above discussed achievable @xmath112 .",
    "detailed results are to follow .",
    "in this section we shall try to sketch the ideas behind the proofs of the main results .",
    "the proof of theorem  [ thm : converse ] consists on one hand of upper bounding the mutual information between the the source vectors and the reconstructions , and on the other hand evaluating the rate distortion function for a bi - variate gaussian source .",
    "the key to upper bounding the mutual information between source and reconstructions is to use the average power constraints and the limited correlation between the source components to obtain the upper bound @xmath113 where @xmath114 is the @xmath0-th component of @xmath115 and where @xmath116 is analogously defined .",
    "once this bound is established for all encoders @xmath117 , @xmath118 satisfying the power constraints , one can derived necessary conditions for achievability by using the data processing inequality to upper bound the mutual information between the source vectors and their reconstructions by the mutual information between the transmitted waveforms and the received waveform .",
    "this latter mutual information is upper bounded by the capacity of the additive gaussian noise channel subject to the power constraint @xmath119 .",
    "the rate distortion function is obtained from evaluating under the given distortion constraints and for the given source law @xmath120 . from the maximum mutual information theorem",
    "it follows that this minimum is achieved if and only if @xmath121 are jointly gaussian .",
    "the minimization problem is then reduced to a minimization over the set of covariance matrices of @xmath122 that satisfy the distortion constraints and where the submatrix in @xmath123 is the covariance matrix of the source .",
    "the minimizing covariance matrix can be found by noticing that every relevant distortion pair can be achieved , with minimal necessary rate , by combining a scaling of the source with reverse waterfilling .",
    "let @xmath124 be the set of all distortion pairs @xmath125 that can be achieved on the source pair @xmath126 with rate @xmath109 , and let @xmath127 be the set of @xmath125 that can be achieved with rate @xmath109 on the scaled source @xmath128 .",
    "the region @xmath127 corresponds to the region @xmath124 scaled by a factor @xmath129 on the @xmath130-axis .",
    "reverse waterfilling at rate @xmath109 on the unitarily decorrelated pair @xmath131 of @xmath128 achieves the point @xmath132 of minimal sum @xmath133 . and",
    "since @xmath109 is the minimal rate needed to achieve @xmath134 on @xmath128 , and @xmath135 the rate @xmath109 is also the minimal rate needed to achieve @xmath136 on @xmath126 .",
    "hence , by choosing the appropriate scaling @xmath137 , we can get any relevant point on the boundary of @xmath124 .",
    "the covariance matrix of @xmath138 that achieves @xmath139 now follows from the covariance matrix of @xmath140 , where @xmath141 result from reverse waterfilling at rate @xmath109 on @xmath131 .",
    "the proof of theorem  [ thm : uncoded ] is straightforward .",
    "one merely considers the uncoded scheme where @xmath142 and then analyzes the linear minimum mean squared - error estimators of @xmath143 from @xmath144 .",
    "the proof of theorem  [ thm : coded ] involves an analysis of randomly generated independent vector quantizers for the two components .",
    "the proposed scheme is conceptually simple , but its analysis gets involved by the included epsilons and deltas . for the sake of clarity and brevity",
    "we shall omit these epsilons and deltas here .",
    "the encoder for the @xmath145-th , @xmath100 , source component is a rate-@xmath101 gaussian vector quantizer that scales the quantized sequence to meet the channel input power constraint .",
    "its codebook @xmath146 consists of @xmath147 codewords that are chosen iid uniformly on the surface of an @xmath148-sphere of center at the origin and radius @xmath149 .",
    "encoder  @xmath145 chooses the codeword @xmath150 in the codebook @xmath146 that is closest ( in euclidean distance ) to the source sequence @xmath151 , and transmits its scaled version @xmath152 where @xmath153 and where @xmath154 denotes the standard inner product in @xmath148 .",
    "the distance @xmath155 between the source sequence @xmath156 and its closest codeword @xmath150 approaches , with high probability , @xmath157 as the blocklength @xmath158 tends to infinity .",
    "it can be shown that , for large @xmath158 , the correlation coefficient between the chosen codewords @xmath159 and @xmath160 is , with very high probability , close to @xmath161 this coefficient @xmath162 plays a central role in this coding scheme .",
    "the decoding is performed in two parts .",
    "first the transmitted codeword pair is recovered , and then this codeword pair is used to make linear estimates of the source sequences . to recover the transmitted pair @xmath163",
    ", the decoder seeks , among all `` jointly typical '' pairs @xmath164 , i.e among all pairs satisfying @xmath165 the codeword pair @xmath166 whose weighted sum @xmath167 has the smallest angle to the channel output @xmath168 , i.e. @xmath169 the corresponding source estimates are then @xmath170 where the coefficients @xmath171 , @xmath172 , @xmath173 , @xmath174 are chosen such that @xmath175 would form the minimum mean squared - error estimates of @xmath176 if @xmath177 were zero - mean joint gaussians with correlation coefficients @xmath178 \\rho(s_1,u_2^{\\ast } ) = \\rho\\sqrt{1 - 2^{-2r_2 } } , & \\rho(s_2,u_1^{\\ast } ) = \\rho\\sqrt{1 - 2^{-2r_1 } } \\\\[3 mm ] \\rho(s_2,u_2^{\\ast } ) = \\sqrt{1 - 2^{-2r_2 } } , & \\rho(u_1^{\\ast},u_2^{\\ast } ) = \\tilde{\\rho}. \\end{array}\\ ] ]    the analysis of the three error events @xmath179 , @xmath180 , and @xmath181 gives that reliable transmission of the pair @xmath182 is possible for all rates @xmath183 in the region @xmath184 & \\qquad \\qquad \\ : \\ : \\ :",
    "r_2 < \\frac{1}{2 } \\log_2 \\left (    \\frac{p_2(1-\\tilde{\\rho}^2)+n}{n(1-\\tilde{\\rho}^2 ) } \\right)\\\\[3 mm ] & \\qquad \\ ; r_1 + r_2 < \\frac{1}{2 } \\log_2 \\left ( \\frac{p_1+p_2      + 2\\tilde{\\rho}\\sqrt{p_1p_2}+n}{n(1-\\tilde{\\rho}^2 ) } \\right ) \\bigg\\}.\\end{aligned}\\ ] ] it can then be shown that for all @xmath185 , the proposed sequence of schemes achieves the distortions @xmath186",
    "s.  bross , a.  lapidoth , s.  tinguely , `` superimposed coded and uncoded transmissions of a gaussian source over the gaussian channel '' , in _ proceedings ieee international symposium on information theory _ ,",
    "july 9 - july 14 , 2006 ."
  ],
  "abstract_text": [
    "<S> we consider a problem where a memoryless bi - variate gaussian source is to be transmitted over an additive white gaussian multiple - access channel with two transmitting terminals and one receiving terminal . </S>",
    "<S> the first transmitter only sees the first source component and the second transmitter only sees the second source component . </S>",
    "<S> we are interested in the pair of mean squared - error distortions at which the receiving terminal can reproduce each of the source components .    it is demonstrated that in the symmetric case , below a certain signal - to - noise ratio ( snr ) threshold , which is determined by the source correlation , uncoded communication is optimal . </S>",
    "<S> for snrs above this threshold we present outer and inner bounds on the achievable distortions . </S>"
  ]
}