{
  "article_text": [
    "in reinforcement learning , an agent learns to maximize its discounted future rewards @xcite .",
    "the structure of the environment is initially unknown , so the agent must both learn the rewards associated with various action - sequence pairs and optimize its policy .",
    "a natural approach is to tackle the subproblems separately via a critic and an actor @xcite , where the critic estimates the value of different actions and the actor maximizes rewards by following the policy gradient @xcite .",
    "policy gradient methods have proven useful in settings with high - dimensional continuous action spaces , especially when task - relevant _ policy representations _ are at hand @xcite .",
    "we tackle the problem of learning actor ( policy ) and critic representations . in the supervised setting , representation or deep learning algorithms",
    "have recently demonstrated remarkable performance on a range of benchmark problems .",
    "however , the problem of learning features for reinforcement learning remains comparatively underdeveloped .",
    "the most dramatic recent success uses @xmath1-learning over finite action spaces , and essentially build a neural network critic @xcite . here",
    ", we consider _ continuous _ action spaces , and develop an algorithm that simultaneously learns the value function and its gradient , which it then uses to find the optimal policy .",
    "this paper presents value - gradient backpropagation ( @xmath0 ) , a deep actor - critic algorithm for continuous action spaces with compatible function approximation .",
    "our starting point is the deterministic policy gradient and associated compatibility conditions derived in @xcite . roughly speaking , the compatibility conditions are that    1",
    ".   the critic approximate the gradient of the value - function and 2 .",
    "the approximation is closely related to the gradient of the policy .",
    "see theorem  [ t : compat ] for details .",
    "we identify and solve two problems with prior work on policy gradients  relating to the two compatibility conditions :    1 .",
    "_ temporal difference methods do not directly estimate the gradient of the value function . _ + instead , temporal difference methods are applied to learn an approximation of the form @xmath2 , where @xmath3 estimates the value of a state , given the current policy , and @xmath4 estimates the _ advantage _ from deviating from the current policy @xcite .",
    "although the advantage is related to the gradient of the value function , it is not the same thing .",
    "2 .   _ the representations used for compatible approximation scale badly on neural networks . _",
    "+ the second problem is that prior work has restricted to advantage functions constructed from a particular state - action representation , @xmath5 , that depends on the gradient of the policy .",
    "the representation is easy to handle for linear policies .",
    "however , if the policy is a neural network , then the standard state - action representation ties the critic too closely to the actor and depends on the internal structure of the actor , example  [ eg : deep_advantage ] . as a result",
    ", weight updates can not be performed by backpropagation , see section  [ sec : problem ] .",
    "the paper makes three novel contributions .",
    "the first two contributions relate directly to problems p1 and p2 .",
    "the third is a new task designed to test the accuracy of gradient estimates .",
    "[ [ method - to - directly - learn - the - gradient - of - the - value - function . ] ] method to directly learn the gradient of the value function .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the first contribution is to modify temporal difference learning so that it directly estimates the gradient of the value - function .",
    "the _ gradient perturbation trick _ , lemma  [ lem : gradient ] , provides a way to simultaneously estimate both the value of a function at a point and its gradient , by perturbing the function s input with uncorrelated gaussian noise .",
    "plugging in a neural network instead of a linear estimator extends the trick to the problem of learning a function and its gradient over the entire state - action space .",
    "moreover , the trick combines naturally with temporal difference methods , theorem  [ thm : extension ] , and is therefore well - suited to applications in reinforcement learning .    [ [ deviator - actor - critic - dac - model - with - compatible - function - approximation . ] ] deviator - actor - critic ( dac ) model with compatible function approximation .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the second contribution is to propose the deviator - actor - critic ( dac ) model , definition  [ def : beh_crit ] , consisting in three coupled neural networks and value - gradient backpropagation ( @xmath0 ) , algorithm  [ alg : qprop ] , which backpropagates three different signals to train the three networks .",
    "the main result , theorem  [ thm : main ] , is that @xmath0 has compatible function approximation when implemented on the dac model when the neural network consists in linear and rectilinear units .",
    "the proof relies on decomposing the actor - network into individual units that are considered as actors in their own right , based on ideas in @xcite .",
    "it also suggests interesting connections to work on structural credit assignment in multiagent reinforcement learning @xcite .",
    "[ [ contextual - bandit - task - to - probe - the - accuracy - of - gradient - estimates . ] ] contextual bandit task to probe the accuracy of gradient estimates .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    a third contribution , that may be of independent interest , is a new contextual bandit setting designed to probe the ability of reinforcement learning algorithms to estimate gradients .",
    "a supervised - to - contextual bandit transform was proposed in @xcite as a method for turning classification datasets into @xmath6-armed contextual bandit datasets .",
    "we are interested in the _ continuous _ setting in this paper .",
    "we therefore adapt their transform with a twist .",
    "the sarcos and barrett datasets from robotics have features corresponding to the positions , velocities and accelerations of seven joints and labels corresponding to their torques .",
    "there are 7 joints in both cases , so the feature and label spaces are 21 and 7 dimensional respectively .",
    "the datasets are traditionally used as regression benchmarks labeled sarcos1 through sarcos7 where the task is to predict the torque of a single joint  and similarly for barrett .",
    "we convert the two datasets into two continuous contextual bandit tasks where the reward signal is the negative distance to the correct label 7-dimensional .",
    "the algorithm is thus `` told '' that the label lies on a sphere in a 7-dimensional space .",
    "the missing information required to pin down the label s position is precisely the gradient . for an algorithm to make predictions that are competitive with fully supervised methods ,",
    "it is necessary to find extremely accurate gradient estimates .",
    "[ [ experiments . ] ] experiments .",
    "+ + + + + + + + + + + +    section  [ sec : experiments ] evaluates the performance of @xmath0 on the contextual bandit problems described above and on the challenging octopus arm task @xcite .",
    "we show that @xmath0 is able to simultaneously solve seven nonparametric regression problems without observing any labels  instead using the distance between its actions and the correct labels .",
    "it turns out that @xmath0 is competitive with recent _ fully supervised _ learning algorithms on the task .",
    "finally , we evaluate @xmath0 on the octopus arm benchmark , where it achieves the best performance reported to date .",
    "an early reinforcement learning algorithm for neural networks is @xmath7 @xcite .",
    "a disadvantage of @xmath7 is that the entire network is trained with a single scalar signal .",
    "our proposal builds on ideas introduced with deep @xmath1-learning @xcite , such as replay .",
    "however , deep @xmath1-learning is restricted to finite action spaces , whereas we are concerned with _",
    "continuous _ action spaces .",
    "policy gradients were introduced in @xcite and have been used extensively @xcite .",
    "the deterministic policy gradient was introduced in @xcite , which also proposed the algorithm @xmath8 .",
    "the relationship between @xmath0 and @xmath8 is discussed in detail in section  [ sec : problem ] .",
    "an alternate approach , based on the idea of backpropagating the gradient of the value function , is developed in @xcite .",
    "unfortunately , these algorithms do not have compatible function approximation in general , so there are no guarantees on actor - critic interactions .",
    "see section  [ sec : problem ] for further discussion .",
    "the analysis used to prove compatible function approximation relies on decomposing the actor neural network into a collection of agents corresponding to the units in the network .",
    "the relation between @xmath0 and the difference - based objective proposed for multiagent learning @xcite is discussed in section  [ sec : local_actors ] .",
    "we use boldface to denote vectors , subscripts for time , and superscripts for individual units in a network .",
    "sets of parameters are capitalized ( @xmath9 , @xmath10 , @xmath11 ) when they refer to matrices or to the parameters of neural networks .",
    "this section recalls previous work on policy gradients . the basic idea is to simultaneously train an actor and a critic .",
    "the critic learns an estimate of the value of different policies ; the actor then follows the gradient of the value - function to find an optimal ( or locally optimal ) policy in terms of expected rewards .",
    "the environment is modeled as a markov decision process consisting of state space @xmath12 , action space @xmath13 , initial distribution @xmath14 on states , stationary transition distribution @xmath15 and reward function @xmath16 .",
    "policy _ is a function @xmath17 from states to actions .",
    "we will often add noise to policies , causing them to be stochastic . in this case , the policy is a function @xmath18 , where @xmath19 is the set of probability distributions on actions .",
    "let @xmath20 denote the distribution on states @xmath21 at time @xmath22 given policy @xmath23 and initial state @xmath24 at @xmath25 and let @xmath26 .",
    "let @xmath27 be the discounted future reward . define the @xmath28\\quad\\text{and }      \\label{e : q }      \\\\",
    "\\text{value of a policy : } \\qquad      &   j({{\\ensuremath{\\boldsymbol\\mu}}_\\theta } ) = \\operatorname*{\\mathbb e}_{{\\mathbf{s}}\\sim \\rho^{{\\ensuremath{\\boldsymbol\\mu } } } , { \\mathbf{a}}\\sim { \\ensuremath{\\boldsymbol\\mu}}_\\theta}[q^{{\\ensuremath{\\boldsymbol\\mu}}_\\theta}({\\mathbf{s}},{\\mathbf{a } } ) ] .",
    "\\label{eq : j}\\end{aligned}\\ ] ] the aim is to find the policy @xmath29 with maximal value .",
    "a natural approach is to follow the gradient @xcite , which in the deterministic case can be computed explicitly as    [ t : dpg]@xmath30 +   under reasonable assumptions on the regularity of the markov decision process the policy gradient can be computed as @xmath31 .",
    "\\end{aligned}\\ ] ]    see @xcite .      since the agent does not have direct access to the value function @xmath32 , it must instead learn an estimate @xmath33 . a sufficient condition for when plugging an estimate @xmath4 into the policy gradient",
    "@xmath34 $ ] yields an unbiased estimator was first proposed in @xcite .",
    "a sufficient condition in the deterministic setting is :    [ t : compat]@xmath30 +   the value - estimate @xmath35 satisfies is compatible with the policy gradient , that is @xmath36\\ ] ] if the following conditions hold :    1 .",
    "* @xmath37 approximates the value gradient : * + the weights learned by the approximate value function must satisfy @xmath38 , where @xmath39                  \\label{eq : val_estimate}\\ ] ] is the mean - square difference between the gradient of the true value function @xmath32 and the approximation @xmath40 .",
    "* @xmath37 is policy - compatible : * + the gradients of the value - function and the policy must satisfy @xmath41    see @xcite .",
    "having stated the compatibility condition , it is worth revisiting the problems that we propose to tackle in the paper .",
    "the first problem is to directly estimate the gradient of the value function , as required by eq .   in condition _",
    "c1_. the standard approach used in the literature is to estimate the value function , or the closely related advantage function , using temporal difference learning , and then compute the derivative of the estimate .",
    "the next section shows how the gradient can be estimated directly .",
    "the second problem relates to the compatibility condition on policy and value gradients required by eq .   in condition _",
    "c2_. the only function approximation satisfying _",
    "c2 _ that has been proposed is    [ eg : advantage]@xmath30 +   let @xmath42 be an @xmath43-dimensional feature representation on states and set @xmath44",
    ". then the value function approximation @xmath45 satisfies condition _ c2 _ of theorem  [ t : compat ] .",
    "the approximation in example  [ eg : advantage ] encounters serious problems when applied to _ deep _ policies , see discussion in section  [ sec : problem ] .",
    "in this section , we tackle the first problem by modifying temporal - difference ( td ) learning so that it directly estimates the gradient of the value function .",
    "first , we developed a new approach to estimating the gradient of a black - box function at a point , based on perturbing the function with gaussian noise .",
    "it turns out that the approach extends easily to learning the gradient of a black - box function across its entire domain .",
    "moreover , it is easy to combine with neural networks and temporal difference learning .",
    "gradient estimates have been intensively studied in bandit problems , where rewards ( or losses ) are observed but labels are not .",
    "thus , in contrast to supervised learning where it is possible to compute the gradient of the loss , in bandit problems the gradient must be estimated .",
    "more formally , consider the following setup .",
    "[ def : bb]@xmath30 +   a function @xmath46 is a * zeroth - order black - box * if it can only be queried for _ zeroth - order _ information .",
    "that is , user can request the value @xmath47 of @xmath48 at any point @xmath49 , but can not request the gradient of the function .",
    "we use the shorthand _ black - box _ in what follows .    the black - box model for optimization",
    "was introduced in @xcite , see @xcite for a recent exposition . in those papers ,",
    "a black - box consists in a _ first - order oracle _ that can provide both zeroth - order information ( the value of the function ) and first - order information ( the gradient or subgradient of the function ) .",
    "@xmath30 +   the reward function @xmath50 is a black box since nature does not provide gradient information .",
    "the value function @xmath51 $ ] is _ not even _ a black - box : it can not be queried directly since it is defined as the expected discounted _ future _ reward .",
    "it is for this reason the gradient perturbation trick must be combined with temporal difference learning , see section  [ sec : tdgl ] .",
    "an important insight is that the gradient of an unknown function at a specific point can be estimated by perturbing its input @xcite .",
    "for example , for small @xmath52 the gradient of @xmath46 is approximately @xmath53 $ ] where the expectation is over vectors sampled uniformly from the unit sphere .",
    "the following lemma provides a simple method for estimating the gradient of a function _ at a point _ based on gaussian perturbations :    [ lem : gradient]@xmath30 +",
    "the gradient of differentiable @xmath46 at @xmath54 is @xmath55\\right\\}.\\ ] ]    by taking sufficiently small variance , we can assume that @xmath48 is locally linear .",
    "setting @xmath56 yields a line through the origin .",
    "it therefore suffices to consider the special case @xmath57 .    setting @xmath58,\\ ] ]",
    "we are required to show that @xmath59 .",
    "the problem is convex , so setting the gradient to zero requires to solve @xmath60 $ ] , which reduces to solving the set of linear equations @xmath61 = ( w^j - v^j)\\operatorname*{\\mathbb e}[(\\epsilon^j)^2]=(w^j - v^j)\\cdot \\sigma^2=0\\qquad \\text { for all $ j$}.\\ ] ] the first equality holds since @xmath62=0 $ ] .",
    "it follows immediately that @xmath59 .",
    "the solution to the optimization problem in eq .",
    "is the gradient @xmath63 of @xmath48 at a particular @xmath54 .",
    "the next step is to learn a function @xmath64 that approximates the gradient across a range of values .    more precisely , given a sample @xmath65 of points , we aim to find @xmath66 .",
    "\\label{eq : grad_optimal}\\ ] ] the next lemma considers the case where @xmath67 and @xmath68 are linear estimates , of the form @xmath69 and @xmath70 for fixed representations @xmath71 and @xmath72 .",
    "[ lem : gradl]@xmath30 +   let @xmath46 be a differentiable function .",
    "suppose that @xmath73 and @xmath72 are representations such that there exists an @xmath43-vector @xmath74 and a @xmath75-matrix @xmath76 satisfying @xmath77 and @xmath78 for all @xmath79 in the sample .",
    "if we define loss function @xmath80.\\ ] ] then @xmath81.\\ ] ]    follows from lemma  [ lem : gradient ] .",
    "in short , the lemma reduces gradient estimation to a simple optimization problem _ given a good enough representation_. jumping ahead slightly to section  [ sec : dpg ] , we ensure that our model has good enough representations by constructing two neural networks to learn them .",
    "the first neural network , @xmath82 , learns an approximation to @xmath83 that plays the role of the baseline @xmath84 .",
    "the second neural network , @xmath85 learns an approximation to the gradient .",
    "recall that @xmath86 is the expected value of a state - action pair given policy @xmath23 .",
    "it is never observed directly , since it is computed by discounting over future rewards .",
    "td - learning is a popular approach to estimating @xmath32 through dynamic programming @xcite .",
    "we quickly review td - learning .",
    "let @xmath87 be a fixed representation .",
    "the goal is to find a value - estimate @xmath88 where @xmath89 is an @xmath43-dimensional vector , that is as close as possible to the true value function .",
    "if the value - function were known , we could simply minimize the mean - square error with respect to @xmath89 : @xmath90.\\ ] ] unfortunately , it is impossible to minimize the mean - square error directly , since the value - function is the expected discounted future reward , rather than the reward .",
    "that is , the value function is not provided explicitly by the environment  not even as a black - box .",
    "the bellman error is therefore used a substitute for the mean - square error : @xmath91\\ ] ] where @xmath21 is the state subsequent to @xmath24 .",
    "let @xmath92 be the td - error .",
    "td - learning updates @xmath89 according to @xmath93 where @xmath94 is a sequence of learning rates .",
    "the convergence properties of td - learning and related algorithms have been studied extensively , see @xcite .      finally , we apply temporal difference methods to estimate the _ _ gradient _ _ of the value function , as required by condition _",
    "c1 _ of theorem  [ t : compat ] .",
    "we are interested in gradient approximations of the form @xmath95 where @xmath96 and @xmath10 is a @xmath75-dimensional matrix .",
    "the goal is to find @xmath76 such that @xmath97 for all sampled state - action pairs .",
    "it is convenient to introduce notation @xmath98 and shorthand @xmath99 .",
    "then , analogously to the mean - square , define the perturbed gradient error : @xmath100,\\ ] ] given a good enough representation , lemma  [ lem : gradl ] guarantees that minimizing the perturbed gradient error yields the gradient of the value function .",
    "unfortunately , as discussed above , the value function can not be queried directly .",
    "we therefore introduce the bellman gradient error as a proxy @xmath101.\\ ] ] set the tdg - error as @xmath102 and , analogously to eq .",
    ", define the tdg - updates @xmath103 where @xmath104 is the @xmath75 matrix given by the outer product .",
    "we refer to @xmath105 as the * perturbed tdg - error*.    the following _ extension theorem _ allows us to import guarantees from temporal - difference learning to temporal - difference gradient learning .",
    "[ thm : extension]@xmath30 +   guarantees on td - learning extend to tdg - learning .",
    "the idea is to reformulate tdg - learning as td - learning , with a slightly different reward function and function approximation . since the function approximation is still linear , any guarantees on convergence for td - learning transfered automatically to tdg - learning .",
    "first , we incorporate @xmath106 into the state - action pair .",
    "define @xmath107 and @xmath108 second , we define a dot product on matrices of equal size by flattening them down to vectors .",
    "more precisely , given two matrices @xmath109 and @xmath110 of the same dimension @xmath111 , define the dot - product @xmath112 .",
    "it is easy to see that @xmath113 the tdg - error can then be rewritten as @xmath114 where @xmath115 is a linear function approximation .",
    "if we are in a setting where td - learning is guaranteed to converge to the value - function , it follows that tdg - learning is also guaranteed to converge  since it is simply a different linear approximation .",
    "thus , @xmath116 and the result follows by lemma  [ lem : gradl ] .",
    "this section presents our model , which consists of three coupled neural networks that learn to estimate the value function , its gradient , and the optimal policy respectively .",
    "[ def : beh_crit]@xmath30 +   the * deviator - actor - critic ( dac ) * model consists in three neural networks :    * * actor - network * with policy @xmath117 ; * * critic - network * , @xmath118 , that estimates the value function ; and * * deviator - network * , @xmath119 , that estimates the gradient of the value function .",
    "gaussian noise is added to the policy during training resulting in actions @xmath120 where @xmath121 . the outputs of the critic and deviator are combined as @xmath122    the gaussian noise plays two roles .",
    "firstly , it controls the explore / exploit tradeoff by controlling the extent to which actor deviates from its current optimal policy . secondly , it controls the `` resolution '' at which deviator estimates the gradient .",
    "the three networks are trained by backpropagating three different signals .",
    "critic , deviator and actor backpropagate the tdg - error , the perturbed tdg - error , and deviator s gradient estimate respectively ; see algorithm  [ alg : qprop ] .",
    "an explicit description of the weight updates of individual units is provided in appendix  [ sec : explicit ] .",
    "deviator estimates the gradient of the value - function with respect to _",
    "deviations @xmath106 from the current policy_. backpropagating the gradient through actor allows to estimate the influence of actor - parameters on the value function as a function of their effect on the policy .",
    "critic and deviator learn representations suited to estimating the value function and its gradient respectively .",
    "note that even though the gradient is a linear function _ at a point _ , it can be a highly nonlinear function in general .",
    "similarly , actor learns a _ policy _ representation .",
    "we set the learning rates of critic and deviator to be equal @xmath123 in the experiments in section  [ sec : experiments ] .",
    "however , the perturbation @xmath106 has the effect of slowing down and stabilizing deviator updates :    [ rem : stability]@xmath30 +   the magnitude of deviator s weight updates depend on @xmath121 since they are computed by backpropagating the perturbed tdg - error @xmath124 .",
    "thus as @xmath125 , deviator s learning rate essentially tends to zero . in general",
    ", deviator learns more slowly than critic .",
    "this has a stabilizing effect on the policy since actor is insulated from critic  its weight updates only depend ( directly ) on the output of deviator .",
    "our main result is that the deviator s value gradient is compatible with the policy gradient of each unit in the actor - network  considered as an actor in its own right :    [ thm : main]@xmath30 +   suppose that all units are rectilinear or linear .",
    "then for each actor - unit in the actor - network there exists a reparametrization of the value - gradient approximator , @xmath68 , that satisfies the compatibility conditions in theorem  [ t : compat ] .",
    "the actor - network is thus a collection of interdependent agents that individually follow the correct policy gradients .",
    "the experiments below show that they also collectively converge on useful behaviors .",
    "[ [ overview - of - the - proof . ] ] overview of the proof .",
    "+ + + + + + + + + + + + + + + + + + + + + +    the next few subsections prove theorem  [ thm : main ] .",
    "we provide a brief overview before diving into the details .",
    "guarantees for temporal difference learning and policy gradients are typically based on the assumption that the value - function approximation is a _ linear _ function of the learned parameters . however , we are interested in the case where actor , critic and deviator are all neural networks , and are therefore highly nonlinear functions of their parameters .",
    "the goal is thus to relate the representations learned by neural networks to the prior work on linear function approximations .",
    "to do so , we build on the following observation , implicit in @xcite :    [ rem : active]@xmath30 +   a neural network of @xmath126 linear and rectilinear units can be considered as a set of @xmath127 submodels , corresponding to different subsets of units .",
    "the active submodel at time @xmath22 consists in the active units ( that is , the linear units and the rectifiers that do not output 0 ) .    the active submodel has two important properties :",
    "* it is a _ linear _ function from inputs to outputs , since rectifiers are linear when active , and * at each time step , learning only occurs over the active submodels , since only active units update their weights .",
    "the feedforward sweep of a rectifier network can thus be disentangled into two steps @xcite .",
    "the first step , which is highly nonlinear , applies a gating operation that selects the active submodel  by rendering various units inactive .",
    "the second step computes the output of the neural network via matrix multiplication .",
    "it is important to emphasize that although the active submodel is a linear function from inputs to outputs , it is not a linear function of the weights .",
    "the strategy of the proof is to decompose the actor - network in an interacting collection of agents , referred to as actor - units .",
    "that is , we model each unit in the actor - network as an actor in its own right that . on each time step",
    "that an actor - unit is active , it interacts with the deviator - submodel corresponding to the current active submodel of the deviator - network .",
    "the proof shows that each actor - unit has compatible function approximation .",
    "first , we recall some basic facts about backpropagation in the case of _ rectilinear _ units .",
    "recent work has shown that replacing sigmoid functions with rectifiers @xmath128 improves the performance of neural networks @xcite .",
    "let us establish some notation .",
    "the output of a rectifier with weight vector @xmath129 is @xmath130 the rectifier is * active * if @xmath131 .",
    "we use rectifiers because they perform well in practice and have the nice property that units are _ linear _ when they are active .",
    "the rectifier subgradient is the indicator function @xmath132    consider a neural network of @xmath126 units , each equipped with a weight vector @xmath133 .",
    "hidden units are rectifiers ; output units are linear .",
    "there are @xmath126 units in total .",
    "it is convenient to combine all the weight vectors into a single object ; let @xmath134 where @xmath135 .",
    "the network is a function @xmath136 .",
    "the network has error function @xmath137 with gradient @xmath138 .",
    "let @xmath139 denote the output of unit @xmath140 and @xmath141 denote its input , so that @xmath142 .",
    "note that @xmath143 depends on @xmath10 ( specifically , the weights of lower units ) but this is supressed from the notation .",
    "@xmath30 +   the * _ influence _ * of unit @xmath140 on unit @xmath144 at time @xmath22 is @xmath145 @xcite .",
    "the influence of unit @xmath140 on the output layer is the vector @xmath146 .",
    "the following lemma summarizes an analysis of the feedforward and feedback sweep of neural nets .",
    "[ lem : structure]@xmath30 +   the following properties hold    a.   * influence . * + a path is * _ active _ * at time @xmath22 if all units on the path are firing .",
    "the influence of @xmath140 on @xmath144 is the sum of products of weights over all active paths from @xmath140 to @xmath144 : @xmath147 where @xmath148 refer to units along the path from @xmath140 to @xmath144 . b.   * output decomposition .",
    "* + the output of a neural network decomposes , relative to the output of unit @xmath140 , as @xmath149 where @xmath150 is the @xmath151-matrix whose @xmath152 entry is the sum over all active paths from input unit @xmath153 to output unit @xmath144 that do not intersect unit @xmath140 . c.   * output gradient . *",
    "+ fix an input @xmath154 and consider the network as a function from parameters to outputs @xmath155 whose gradient is an @xmath156-matrix .",
    "the @xmath157-entry of the gradient is the input to the unit times its influence : @xmath158 d.   * backpropagated error . *",
    "+ fix @xmath154 and consider the function @xmath159 . let @xmath160 .",
    "+ the gradient of the error function is @xmath161 where the backpropagated error signal @xmath162 received by unit @xmath140 decomposes as @xmath163 .",
    "direct computation .",
    "the lemma holds generically for networks of rectifier and linear units .",
    "we apply it to actor , critic and deviator networks below .",
    "this subsection proves condition _",
    "c1 _ of compatible function approximation for a minimal , linear deviator - actor - critic model .",
    "the next subsection shows how the minimal model arises at the level of actor - units .",
    "@xmath30 +   the * minimal model * of a deviator - actor - critic consists in an actor with linear policy @xmath164 , where @xmath165 is an @xmath43-vector and @xmath166 is a noisy scalar .",
    "the critic and deviator together output : @xmath167 where @xmath89 is an @xmath43-vector , @xmath168 is a scalar , and @xmath169 is simply scalar multiplication .",
    "the critic in the minimal model is standard .",
    "however , the deviator has been reduced to almost nothing : it learns a single scalar parameter , @xmath168 , that is used to train the actor .",
    "the minimal model is thus too simple to be much use as a standalone algorithm .",
    "[ lem : loc_compat]@xmath30 +   there exists a reparametrization of the gradient estimate of the minimal model @xmath170 such that compatibility condition _",
    "c1 _ in theorem  [ t : compat ] is satisifed : @xmath171    let @xmath172 and construct @xmath173 .",
    "clearly , @xmath174 observe that @xmath175 and that , similarly , @xmath176 as required .",
    "the proof proceeds by showing that the compatibility conditions in theorem  [ t : compat ] hold for each actor - unit .",
    "the key step is to relate the actor - units to the minimal model introduced above .",
    "[ lem : reduction]@xmath30 +   actor - units in a dac neural network are equivalent to minimal model actors .",
    "let @xmath177 denote the influence of unit @xmath140 on the output layer of the actor - network at time @xmath22 .",
    "when unit @xmath140 is active , lemma  [ lem : structure]ab implies we can write @xmath178 , where @xmath179 is the sum over all active paths from the input to the output of the actor - network that do not intersect unit @xmath140 .",
    "following remark  [ rem : active ] , the active subnetwork of the deviator - network at time @xmath22 is a linear transform which , by abuse of notation , we denote by @xmath180 .",
    "combine the last two points to obtain @xmath181 observe that @xmath182 is a @xmath183-vector .",
    "we have therefore reduced actor - unit @xmath140 s interaction with the deviator - network to @xmath183 copies of the minimal model .",
    "theorem  [ thm : main ] follows from combining the above lemmas .    compatibility condition _",
    "c1 _ follows from lemmas  [ lem : loc_compat ] and [ lem : reduction ] .",
    "compatibility condition _",
    "c2 _ holds since the critic and deviator minimize the bellman gradient error with respect to @xmath10 and @xmath11 which also , implicitly , minimizes the bellman gradient error with respect to the corresponding reparametrized @xmath184 s for each actor - unit .",
    "theorem  [ thm : main ] shows that each actor - unit satisfies the conditions for compatible function approximation and so follows the correct gradient when performing weight updates .",
    "it is interesting to relate our approach to the literature on multiagent reinforcement learning @xcite . in particular , @xcite consider the _",
    "structural credit assignment problem _ within populations of interacting agents : how to reward individual agents in a population for rewards based on their collective behavior ? they propose to train agents within populations with a _ difference - based objective _ of the form @xmath185 where @xmath1 is the objective function to be maximized ; @xmath186 and @xmath187 are the system variables that are and are not under the control of agent @xmath140 respective , and @xmath188 is a fixed counterfactual action .    in our setting",
    ", the gradient used by actor - unit @xmath140 to update its weights can be described explicitly :    @xmath30 +   actor - unit @xmath140 follows policy gradient @xmath189 = \\operatorname*{\\mathbb e}\\left[\\operatorname*{\\nabla}_{\\theta^j } { \\ensuremath{\\boldsymbol\\mu}}_{\\theta^j}(s)\\cdot \\big\\langle { \\ensuremath{\\boldsymbol\\pi}}^j,{\\mathbf{g}}^{\\mathbf{w}}(\\tilde{{\\mathbf{s}}})\\big\\rangle\\right],\\ ] ] where @xmath190 is deviator s estimate of the directional derivative of the value function in the direction of actor - unit @xmath140 s influence",
    ".    follows from lemma  [ lem : structure]b .",
    "notice that @xmath191 in eq .  .",
    "it follows that training the actor - network via @xmath0 causes the actor - units to optimize the difference - based objective  without requiring to compute the difference explicitly .",
    "although the topic is beyond the scope of the current paper , it is worth exploring how suitably adapted variants of backpropagation can be applied to the reinforcement learning problems in the multiagent setting .      [ [ comparison - with - textttcopdac - mathttq . ] ] comparison with @xmath8 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    extending the standard value function approximation in example  [ eg : advantage ] to the setting where actor is a neural network yields the following representation , which is used in @xcite when applying @xmath8 to the octopus arm task :    [ eg : deep_advantage]@xmath30 +   let @xmath192 and @xmath193 be an actor and critic neural network respectively .",
    "suppose the actor - network has @xmath194 parameters ( i.e. the total number of entries in @xmath9 ) .",
    "it follows that the jacobian @xmath195 is an @xmath156-matrix .",
    "the value function approximation is then @xmath196 where @xmath129 is an @xmath194-vector .",
    "weight updates under @xmath8 , with the function approximation above , are therefore as described in algorithm  [ alg : copdac ] .",
    "let us compare @xmath0 with @xmath8 , considering the three updates in turn :    * _ actor updates . _ + under @xmath0 , the actor backpropagates the value - gradient estimate .",
    "in contrast under @xmath8 the actor performs a complicated update that combines the policy gradient @xmath197 with the advantage function s weights  and differs substantively from backprop . * _ deviator / advantage - function updates . _",
    "+ under @xmath0 , the deviator backpropagates the perturbed tdg - error .",
    "in contrast , @xmath8 uses the gradient of the _ actor _ to update the weight vector @xmath129 of the advantage function . + by lemma  [ lem : structure]d",
    ", backprop takes the form @xmath198 where @xmath199 is a @xmath183-vector .",
    "in contrast , the advantage function requires computing @xmath200 , where @xmath129 is an @xmath194-vector .",
    "although the two formulae appear similarly superficially , they carry very different computational costs .",
    "+ the first consequence is that the parameters of @xmath129 must exactly line up with those of the policy .",
    "the second consequence is that , by lemma  [ lem : structure]c , the advantage function requires access to @xmath201 where @xmath202 is the input from unit @xmath153 to unit @xmath140 .",
    "thus , the advantage function requires access to the input @xmath203 and the influence @xmath204 of every unit in the actor - network . * _ critic updates . _",
    "+ the critic updates for the two algorithms are essentially identical , with the td - error replaced with the tdg - error .    in short ,",
    "the approximation in example  [ eg : deep_advantage ] that is used by @xmath8 is thus not well - adapted to deep learning .",
    "the main reason is that learning the advantage function requires coupling the vector @xmath129 with the parameters @xmath9 of the actor .",
    "[ [ comparison - with - computing - the - gradient - of - the - value - function - approximation . ] ] comparison with computing the gradient of the value - function approximation .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    perhaps the most natural approach to estimating the gradient is to simply estimate the value function , and then use its gradient as an estimate of the derivative @xcite .",
    "the main problem with this approach is that , to date , it has not been show that the resulting updates of the critic and the actor are compatible .",
    "there are also no guarantees that the gradient of the critic will be a good approximation to the gradient of the value function ",
    "although it is intuitively plausible .",
    "the problem becomes particularly severe when the value - function is estimated via a neural network that uses activation functions that are _ not smooth _ such as rectifers .",
    "rectifiers are becoming increasingly popular due to their superior empirical performance @xcite .",
    "we evaluate @xmath0 on three tasks : two highly nonlinear contextual bandit tasks constructed from benchmark datasets for nonparametric regression , and the octopus arm .",
    "we do not evaluate @xmath0 on other standard reinforcement learning benchmarks such as mountain car , pendulum or puddle world , since these can already be handled by _ linear _ actor - critic algorithms .",
    "the contribution of @xmath0 is the ability to learn representations suited to nonlinear problems .    [",
    "[ cloning - and - replay . ] ] cloning and replay .",
    "+ + + + + + + + + + + + + + + + + + +    temporal difference learning can be unstable when run over a neural network . a recent innovation introduced in @xcite that _ stabilizes _ td - learning is to clone a separate network @xmath205 to compute the targets @xmath206 .",
    "the parameters of the cloned network are updated periodically .",
    "we implement a similar modification of the tdg - error in algorithm  [ alg : qprop ] .",
    "we also use experience replay @xcite .",
    "@xmath0 is well - suited to replay , since the critic and deviator can learn values and gradients over the full range of previously observed state - action pairs offline .",
    "cloning and replay were also applied to @xmath8 .",
    "both algorithms were implemented in theano @xcite .",
    "the goal of the contextual bandit tasks is to probe the ability of reinforcement learning algorithms to accurately estimate gradients .",
    "the experimental setting may thus be of independent interest .",
    "[ [ description . ] ] description .",
    "+ + + + + + + + + + + +    we converted two robotics datasets , sarcos and barrett wam , into contextual bandit problems via the supervised - to - contextual - bandit transform in @xcite .",
    "the datasets have 44,484 and 12,000 training points respectively , both with 21 features corresponding to the positions , velocities and accelerations of seven joints .",
    "labels are 7-dimensional vectors corresponding to the torques of the 7 joints .    in the contextual bandit task , the agent samples 21-dimensional state vectors i.i.d . from either the sarcos or barrett training data and",
    "executes 7-dimensional actions .",
    "the reward @xmath207 is the negative mean - square distance from the action to the label .",
    "note that the reward is a scalar , whereas the correct label is a 7-dimensional vector .",
    "the gradient of the reward @xmath208 is the direction from the action to the correct label . in the supervised setting ,",
    "the gradient can be computed . in the bandit",
    "setting , the reward is a zeroth - order black box .",
    "the agent thus receives far less information in the bandit setting than in the fully supervised setting .",
    "intuitively , the negative distance @xmath50 `` tells '' the algorithm that the correct label lies on the surface of a sphere in the 7-dimensional action space that is centred on the most recent action .",
    "by contrast , in the supervised setting , the algorithm is given the position of the label in the action space . in the bandit",
    "setting , the algorithm must estimate the position of the label on the surface of the sphere .",
    "equivalently , the algorithm must estimate the label s direction relative to the center of the sphere  which is given by the _ gradient _ of the value function .",
    "the goal of the contextual bandit task is thus to _ simultaneously solve seven nonparametric regression problems when observing distances - to - labels instead of directly observing labels_. the value function is relatively easy to learn in contextual bandit setting since the task is not sequential .",
    "however , both the value function and its gradient are highly nonlinear , and it is precisely the gradient that specifies where labels lie on the spheres .    [ [ network - architectures . ] ] network architectures .",
    "+ + + + + + + + + + + + + + + + + + + + + +    @xmath0 and @xmath8 were implemented on an actor and deviator network of two layers ( 300 and 100 rectifiers ) each and a critic with a hidden layers of 100 and 10 rectifiers .",
    "updates were computed via rmsprop with momentum .",
    "the variance of the gaussian noise @xmath209 was set to decrease linearly from @xmath210 until reaching @xmath211 at which point it remained fixed .    [ [ performance . ] ] performance .",
    "+ + + + + + + + + + + +    figure  [ f : robo ] compares the test - set performance of policies learned by @xmath0 against @xmath8 .",
    "the final policies trained by @xmath0 achieved average mean - square test error of 0.013 and 0.014 on the seven sarcos and barrett benchmarks respectively .    remarkably , @xmath0 is competitive with fully - supervised nonparametric regression algorithms on the sarcos and barrett datasets , see figure  2bc in @xcite and the results in @xcite .",
    "it is important to note that the results reported in those papers are for algorithms that are given the labels and that solve _ one regression problem at a time_. to the best of our knowledge , there are no prior examples of a bandit or reinforcement learning algorithm that is competitive with fully supervised methods on regression datasets .    for comparison , we implemented backprop on the actor - network under full - supervision .",
    "backprop converged to .006 and .005 on sarcos and barrett , compared to 0.013 and 0.014 for @xmath0 .",
    "note that backprop is trained on 7-dim labels whereas @xmath0 receives 1-dim rewards .",
    "[ [ accuracy - of - gradient - estimates . ] ] accuracy of gradient - estimates .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the true value - gradients can be computed and compared with the algorithm s estimates on the contextual bandit task .",
    "[ f : robo_grad ] shows the performance of the two algorithms .",
    "gradient - error converges to @xmath212 on both tasks .",
    "s gradient estimate , implicit in the advantage function , converges to 0.03 ( sarcos ) and 0.07 ( barrett ) .",
    "this confirms that @xmath0 yields significantly better gradient estimates .",
    "@xmath8 s estimates are significantly worse for barrett compared to sarcos , in line with the worse performance of @xmath8 on barrett in fig .",
    "[ f : robo ] .",
    "it is unclear why @xmath8 s gradient estimate gets worse on barrett for some period of time . on the other hand ,",
    "since there are no guarantees on @xmath8 s estimates , it follows that its erratic behavior is perhaps not surprising .",
    "[ [ comparison - with - bandit - task - in- . ] ] comparison with bandit task in @xcite .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    note that although the contextual bandit problems investigated here are lower - dimensional ( with 21-dimensional state spaces and 7-dimensional action spaces ) than the bandit problem in @xcite ( with no state space and 10 , 25 and 50-dimensional action spaces ) , they are nevertheless much harder .",
    "the optimal action in the bandit problem , in all cases , is the constant vector @xmath213 $ ] consisting of only 4s .",
    "in contrast , sarcos and barrett are nontrivial benchmarks even when fully supervised .",
    "the octopus arm task is a challenging environment that is high - dimensional , sequential and highly nonlinear .",
    "[ [ desciption . ] ] desciption .",
    "+ + + + + + + + + + +    the objective is to learn to hit a target with a simulated octopus arm @xcite .",
    "settings are taken from @xcite .",
    "importantly , the action - space is _ not _ simplified using `` macro - actions '' .",
    "the arm has @xmath214 compartments attached to a rotating base .",
    "there are @xmath215 state variables ( @xmath216 , @xmath217 position / velocity of nodes along the upper / lower side of the arm ; angular position / velocity of the base ) and @xmath218 action variables controlling the clockwise and counter - clockwise rotation of the base and three muscles per compartment .    after each step , the agent receives a reward of @xmath219 , where @xmath220 is the change in distance between the arm and the target .",
    "the final reward is @xmath221 if the agent hits the target .",
    "an episode ends when the target is hit or after 300 steps .",
    "the arm initializes at eight positions relative to the target : @xmath222 .",
    "see appendix  [ sec : octopus_details ] for more details .",
    "[ [ network - architectures.-1 ] ] network architectures .",
    "+ + + + + + + + + + + + + + + + + + + + + +    we applied @xmath0 to an actor - network with @xmath223 hidden rectifiers and linear output units clipped to lie in @xmath224 $ ] ; and critic and deviator networks both with two hidden layers of @xmath223 and @xmath225 rectifiers , and linear output units .",
    "updates were computed via rmsprop with step rate of @xmath226 , moving average decay , with nesterov momentum  @xcite penalty of @xmath227 and @xmath227 respectively , and discount rate @xmath228 of @xmath229 .",
    "the variance of the gaussian noise was initialized to @xmath210 .",
    "an explore / exploit tradeoff was implemented as follows .",
    "when the arm hit the target in more than 300 steps , we set @xmath230 ; otherwise @xmath231 .",
    "a hard lower bound was fixed at @xmath232 .",
    "we implemented copdac - q on a variety of architectures ; the best results are shown ( also please see figure  3 in @xcite ) .",
    "they were obtained using a similar architecture to @xmath0 , with sigmoidal hidden units and sigmoidal output units for the actor .",
    "linear , rectilinear and clipped - linear output units were also tried . as for @xmath0 , cloning and experience replay",
    "were used to increase stability .",
    "[ [ performance.-1 ] ] performance .",
    "+ + + + + + + + + + + +    figure [ f : oct ] shows the steps - to - target and average - reward - per - step on ten training runs .",
    "@xmath0 converges rapidly and reliably ( within @xmath233 steps ) to a stable policy that uses less than 50 steps to hit the target on average ( see supplementary video for examples of the final policy in action ) .",
    "@xmath0 converges quicker , and to a better solution , than @xmath8 .",
    "the reader is strongly encouraged to compare our results with those reported in @xcite . to the best of our knowledge",
    ", @xmath0 achieves the best performance to date on the octopus arm task .",
    "[ [ stability . ] ] stability .",
    "+ + + + + + + + + +    it is clear from the variability displayed in the figures that both the policy and the gradients learned by @xmath0 are more stable than @xmath8 .",
    "note that the higher variability exhibited by @xmath0 in the right - hand panel of fig .",
    "[ f : oct ] ( rewards - per - step ) is misleading .",
    "it arises because dividing by the number of steps  which is lower for @xmath0 since it hits the target more quickly after training  inflates @xmath0 s apparent variability .",
    "value - gradient backpropagation ( @xmath234 is the first deep reinforcement learning algorithm with compatible function approximation for continuous policies .",
    "it builds on the deterministic actor - critic , @xmath8 , developed in @xcite with two decisive modifications .",
    "first , we incorporate an explicit estimate of the value gradient into the algorithm .",
    "second , we construct a model that decouples the internal structure of the actor , critic , and deviator  so that all three can be trained via backpropagation .",
    "@xmath0 achieves state - of - the - art performance on two contextual bandit problems where it simultaneously solves seven regression problems without observing labels .",
    "note that @xmath0 is competitive with recent _ fully supervised _",
    "methods that solve a _",
    "single _ regression problem at a time .",
    "further , @xmath0 outperforms the prior state - of - the - art on the octopus arm task , quickly converging onto policies that rapidly and fluidly hit the target .",
    "[ [ acknowledgements . ] ] acknowledgements .",
    "+ + + + + + + + + + + + + + + + +    we thank nicolas heess for sharing the settings of the octopus arm experiments in @xcite .",
    "45 [ 1]#1 [ 1]`#1 ` urlstyle [ 1]doi : # 1    adrian  k agogino and kagan tumer .",
    "unifying temporal and structural credit assignment problems . in _ aamas _ , 2004 .",
    "adrian  k agogino and kagan tumer . analyzing and visualizing multiagent rewards in dynamic and stochastic environments .",
    "_ journal of autonomous agents and multi - agent systems _ , 170 ( 2):0 320338 , 2008 .",
    "l  c baird .",
    "residual algorithms : reinforcement learning with function approximation . in _ icml _",
    ", 1995 .",
    "david balduzzi . .",
    "in _ arxiv:1509.01851 _ , 2015 .",
    "david balduzzi , hastagiri vanchinathan , and joachim buhmann .",
    "kickback cuts backprop s red - tape : biologically plausible credit assignment in neural networks . in _ aaai _ , 2015 .",
    "andrew  g barto , richard  s sutton , and charles  w anderson .",
    "neuronlike adapative elements that can solve difficult learning control problems .",
    "_ ieee trans .",
    "systems , man , cyb _ , 130 ( 5):0 834846 , 1983 .",
    "f  bastien , p  lamblin , r  pascanu , j  bergstra , i  goodfellow , a  bergeron , n  bouchard , and y  bengio . .",
    "in _ nips workshop : deep learning and unsupervised feature learning _ , 2012 .",
    "j  bergstra , o  breuleux , f  bastien , p  lamblin , r  pascanu , g  desjardins , j  turian , d  warde - farley , and yoshua bengio .",
    "theano : a cpu and gpu math expression compiler . in _ proc .",
    "python for scientific comp .",
    "( scipy ) _ , 2010 .",
    "george  e dahl , tara  n sainath , and geoffrey hinton .",
    "improving deep neural networks for lvcsr using rectified linear units and dropout . in _",
    "ieee int conf on acoustics , speech and signal processing ( icassp ) _ , 2013 .",
    "christoph dann , gerhard neumann , and jan peters .",
    "policy evaluation with temporal differences : a survey and comparison .",
    "_ jmlr _ , 15:0 809883 , 2014 .",
    "marc  peter deisenroth , gerhard neumann , and jan peters . .",
    "_ foundations and trends in machine learning _ , 20 ( 1 - 2):0 1142 , 2011 .    miroslav dudk , dumitru erhan , john langford , and lihong li . .",
    "_ statistical science _ , 290 ( 4):0 485511 , 2014",
    ".    y  engel , p  szab , and d  volkinshtein . learning to control an octopus arm with gaussian process temporal difference methods . in _ nips _ , 2005 .",
    "michael fairbank and eduardo alonso . .",
    "ieee world conference on computational intelligence ( wcci ) _ , 2012 .",
    "michael fairbank , eduardo alonso , and daniel  v prokhorov . .",
    "_ ieee trans .",
    "_ , 240 ( 12):0 20882100 , 2013 .",
    "abraham flaxman , adam kalai , and h  brendan mcmahan .",
    "online convex optimization in the bandit setting : gradient descent without a gradient . in _ soda _ , 2005 .",
    "xavier glorot , antoine bordes , and yoshua bengio . eep sparse rectifier neural networks . in _ proc .",
    "14th int conference on artificial intelligence and statistics ( aistats ) _ , 2011 .",
    "carlos guestrin , michail lagoudakis , and ronald parr .",
    "coordinated reinforcement learning . in _",
    "icml _ , 2002 .",
    "roland hafner and martin riedmiller .",
    "reinforcement learning in feedback control : challenges and benchmarks from technical process control .",
    "_ machine learning _ , 84:0 137169 , 2011 .",
    "g  hinton , nitish srivastava , and kevin swersky .",
    "lecture 6a : overview of minibatch gradient descent .",
    "chris holmesparker , adrian  k agogino , and kagan tumer .",
    "combining reward shaping and hierarchies for scaling to large multiagent systems . _ the knowledge engineering review _",
    ", 2014 .",
    "michael  i jordan and r  a jacobs . .",
    "in _ nips _ , 1990 .",
    "sham kakade .",
    "a natural policy gradient . in _ nips _ , 2001",
    ".    vijay  r konda and john  n tsitsiklis .",
    "actor - critic algorithms . in _ nips _ , 2000 .",
    "samory kpotufe and abdeslam boularias .",
    "gradient weights help nonparametric regressors . in _ advances in neural information processing systems ( nips ) _ , 2013 .",
    "sergey levine , chelsea finn , trevor darrell , and pieter abbeel . .",
    "_ arxiv:1504.00702 _ , 2015 .",
    "volodymyr mnih , koray kavukcuoglu , david silver , andrei  a. rusu , joel veness , marc  g. bellemare , alex graves , martin riedmiller , andreas  k. fidjeland , georg ostrovski , stig petersen , charles beattie , amir sadik , ioannis antonoglou , helen king , dharshan kumaran , daan wierstra , shane legg , and demis hassabis .",
    "human - level control through deep reinforcement learning .",
    "_ nature _ , 5180 ( 7540):0 529533 , 02 2015 .",
    "vinod nair and geoffrey hinton .",
    "ectified linear units improve restricted boltzmann machines . in _",
    "icml _ , 2010",
    ".    a  s nemirovski and d  b yudin .",
    "_ problem complexity and method efficiency in optimization_. wiley - interscience , 1983 .",
    "duy nguyen - tuong , jan peters , and matthias seeger . .",
    "in _ nips _ , 2008 .",
    "jan peters and stefan schaal .",
    "policy gradient methods for robotics . in _ proc .",
    "ieee / rsj int .",
    "robots syst .",
    ".    daniel  v prokhorov and donald  c wunsch . .",
    "_ ieee trans .",
    "_ , 80 ( 5):0 9971007 , 1997 .    maxim raginsky and alexander rakhlin . .",
    "_ ieee trans .",
    "inf . theory _ , 570 ( 10):0 70367056 , 2011 .    david silver , guy lever , nicolas heess , thomas degris , daan wierstra , and martin riedmiller .",
    "deterministic policy gradient algorithms . in _ icml _ , 2014 .",
    "nitish srivastava , geoffrey hinton , alex krizhevsky , ilya sutskever , and ruslan salakhutdinov .",
    "dropout : a simple way to prevent neural networks from overfitting .",
    "_ jmlr _ , 15:0 19291958 , 2014 .",
    "r  s sutton and a  g barto .",
    "_ reinforcement learning : an introduction_. mit press , 1998 .",
    "richard sutton , david mcallester , satinder singh , and yishay mansour .",
    "policy gradient methods for reinforcement learning with function approximation . in _ nips _ , 1999 .",
    "richard sutton , hamid  reza maei , doina precup , shalabh bhatnagar , david silver , csaba szepesvri , and eric wiewiora .",
    "fast gradient - descent methods for temporal - difference learning with linear function approximation . in _ icml _ , 2009 .",
    "richard sutton , csaba szepesvri , and hamid  reza maei . a convergent @xmath235 algorithm for off - policy temporal - difference learning with linear function approximation . in _ adv in neural information processing systems ( nips )",
    "_ , 2009 .",
    "shubhendu trivedi , jialei wang , samory kpotufe , and gregory shakhnarovich . .",
    "in _ uai _ , 2014 .",
    "john tsitsiklis and benjamin  van roy .",
    "an analysis of temporal - difference learning with function approximation .",
    "_ ieee trans .",
    ", 420 ( 5):0 674690 , 1997 .",
    "niklas wahlstrm , thomas  b. schn , and marc  peter deisenroth . .",
    "_ arxiv:1502.02251 _ , 2015 .",
    "y  wang and j  si . .",
    "_ ieee trans .",
    "_ , 120 ( 2):0 264276 , 2001 .",
    "ronald  j williams .",
    "simple statistical gradient - following algorithms for connectionist reinforcement learning .",
    "_ machine learning _ , 8:0 229256 , 1992 .",
    "m  d zeiler , m  ranzato , r  monga , m  mao , k  yang , q  v le , p  nguyen , a  senior , v  vanhoucke , j  dean , and g  hinton . on rectified linear units for speech processing . in _",
    "icassp _ , 2013 .",
    "* appendices *",
    "it is instructive to describe the weight updates under @xmath0 more explicitly .",
    "let @xmath236 , @xmath237 and @xmath238 denote the weight vector of unit @xmath140 , according to whether it belongs to the actor , deviator or critic network .",
    "similarly , in each case @xmath239 or @xmath240 denotes the influence of unit @xmath140 on the network s output layer , where the influence is vector - valued for actor and deviator networks and scalar - valued for the critic network .",
    "weight updates in the deviator - actor - critic model , where all three networks consist of rectifier units performing stochastic gradient descent , are then per algorithm  [ alg : pb ] .",
    "units that are not active on a round do not update their weights that round .",
    "listing 1 summarizes technical information with respect to the physical description and task setting used in the octopus arm simulator in xml format .",
    "< constants > < frictiontangential>0.4</frictiontangential > < frictionperpendicular>1</frictionperpendicular > <",
    "pressure>10</pressure > < gravity>0.01</gravity > < surfacelevel>5</surfacelevel > < buoyancy>0.08</buoyancy >",
    "< muscleactive>0.1</muscleactive > < musclepassive>0.04</musclepassive >",
    "< musclenormalizedminlength>0.1</musclenormalizedminlength > < muscledamping>-1</muscledamping > < repulsionconstant>.01</repulsionconstant > < repulsionpower>1</repulsionpower > < repulsionthreshold>0.7</repulsionthreshold >",
    "< torquecoefficient>0.025</torquecoefficient > < /constants >"
  ],
  "abstract_text": [
    "<S> this paper proposes @xmath0 , a deep reinforcement learning algorithm for continuous policies with compatible function approximation . </S>",
    "<S> the algorithm is based on two innovations . </S>",
    "<S> firstly , we present a temporal - difference based method for learning the _ gradient _ of the value - function . secondly , we present the deviator - actor - critic ( dac ) model , which comprises three neural networks that estimate the value function , its gradient , and determine the actor s policy respectively .    </S>",
    "<S> we evaluate @xmath0 on two challenging tasks : a contextual bandit problem constructed from nonparametric regression datasets that is designed to probe the ability of reinforcement learning algorithms to accurately estimate gradients ; and the octopus arm , a challenging reinforcement learning benchmark . </S>",
    "<S> @xmath0 is competitive with _ fully supervised methods _ on the bandit task and achieves the best performance to date on the octopus arm .    </S>",
    "<S> policy gradient , reinforcement learning , deep learning , gradient estimation , temporal difference learning </S>"
  ]
}