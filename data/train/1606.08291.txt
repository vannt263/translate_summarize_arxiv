{
  "article_text": [
    "in time series portfolio analysis as in other areas of multivariate dynamic modeling and decision analysis in econometrics and finance , sparse models and efficient computation are critical to successfully scaling analyses to higher dimensional problems . with a focus on forecasting in financial time",
    "series , some of the recent progress with bayesian sparsity modeling approaches  such as copula - based dynamic models  ( e.g. * ? ? ?",
    "* ) , dynamic graphical models  ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) and sparse factor models  ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) have been demonstrably useful .",
    "forecasting improvements can be generated by data - relevant and informed sparsity , coupled with time - varying model parameters and relevant approaches to representing multivariate stochastic volatility  ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "such advances can then be expected to aid in improved characterizations of risk and in outcomes of sequentially revised portfolio decision strategies  ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "examples in the above referenced papers and others in recent times typically involve series in just a few dimensions , although some show simulations and empirical results in up to 50 dimensions . for both institutional and personal implementations for quantitative investing and automated trading , and also in view of regulatory requirements on banks to assess market risk through value - at - risk or other metrics (",
    "* paragraph 738 ) , there is increasing interest in scaling methodology to substantially higher dimensions , at least to hundreds of series .",
    "recently introduced simultaneous graphical dynamic linear models  ( sgdlms : * ? ? ?",
    "* ) address scalability .",
    "these involve : ( i ) a sets of _ decoupled _ univariate dynamic linear models ( dlms ) for individual series , allowing a range of time - varying parameter models and univariate volatilities , and for which standard theory and resulting efficient forward filtering / forecasting algorithms apply ; ( ii ) exploitation of a simultaneous equations formulation with sparse graphical modeling ideas that _ recouple _ the series and define rich yet sparse representations of multivariate stochastic volatility ; and ( iii ) variational bayes methods combined with importance sampling to coherently integrate / couple the series for forecasting and decisions .",
    "parallel , gpu - based implementation enables on - line analysis of increasingly high - dimensional time series .",
    "this paper defines and illustrates methodological advances in sgdlms addressing core questions of variable selection underlying the dynamics in structure of large multivariate volatility matrices .",
    "we develop and showcase this in a case study in financial forecasting and portfolio optimization with a 401-dimensional series of daily s&p 500 stock prices and index over 20032014 .",
    "the new methodology defines a strategy for sequential , adaptive selection of simultaneous / contemporaneous parental predictor series of each index , and its use in the case study highlights the utility in bayesian forecasting and portfolio decisions .",
    "the s&p analysis includes benchmarks of forecast performance as well as portfolio returns and risk metrics , including comparisons to the standard multivariate wishart dlm ( wdlm )  @xcite .",
    "this is the appropriate benchmark as it has been a standard model in bayesian financial time series and portfolio analysis  in industry and academic research  for years , being quite flexible and trivially implemented , and remains a mainstay component of many models .",
    "section  [ sgdlm2:sec : forecast - models ] introduces notation of dlms , and briefly summarizes the key concepts and technical elements of the sgdlm .",
    "the sgdlm requires specification of a set of `` parental '' time series to use as contemporaneous predictors of each univariate series in a simultaneous equations formulation ; to address this , section  [ sgdlm2:sec : sgdlm - bayes - hotspot ] introduces a novel and practicable selection strategy for the parental sets .",
    "section  [ sgdlm2:sec : investment - strategy ] discusses several quantitative investment rules based on various portfolio utility functions of practical interest .",
    "section  [ sgdlm2:sec : data - example ] presents a portfolio manager s view of managing a 400-asset portfolio using the sgdlm combined with such rules to drive investment decisions .",
    "some summary comments appear in section  [ sgdlm2:sec : final - remarks ] . supporting technical material on wdlms and sgdlms , together with additional summaries from the case study ,",
    "appear in the appendix .",
    "dlms @xcite are fully bayesian state - space models that are widely used in forecasting financial time series due to flexibility in model specification , ability to adapt to changing market dynamics and to incorporate external / intervention information .",
    "the standard univariate dlm combines a normal linear observation equation , @xmath0 with a conditionally normal , multivariate linear system equation to govern the state evolutions of @xmath1 from time @xmath2 to @xmath3 , @xmath4 here the observation errors @xmath5 follow a normal distribution with precision @xmath6 , and the state innovations @xmath7 are multivariate normally distributed with covariance @xmath8 . in financial time",
    "series , the necessity of volatility models is well - understood , and standard extensions of the basic dlm to include the beta - gamma stochastic evolution of the precisions @xmath6 are widely used ; see key source and references in  ( * ? ? ?",
    "10.8 ) and ( * ? ? ?",
    "details applied to the sgdlm are elaborated in the following section .",
    "conjugate analysis enables fast , on - line learning , so that models are updated dynamically , responding to the latest market events , while being open to user - intervention at all times .    the widely - used , benchmark multivariate dlms with dynamic volatility matrices",
    "extend the above univariate model to a vector time series in which the variance matrix of observation errors evolves according to a multivariate beta - wishart process .",
    "again theory is standard ; see  ( * ? ? ?",
    "16.4 ) and @xcite .",
    "we denote this model by wdlm , and give key summary details below in [ sgdlm2:subsec : wdlm ] .",
    "the sgdlm combines univariate dlms for each series to define a multivariate model , and does this via contemporaneous regressions of each series on a subset of the other series .",
    "this allows for parsimonious modeling of multivariate dependence for enhanced scalability , and was recently introduced in  @xcite .",
    "we summarize the essential details here .",
    "each of the @xmath9 univariate series @xmath10 , @xmath11 , follows a univariate dlm with observation equation @xmath12 where :    * the predictor vector @xmath13 consists of @xmath14 external predictors @xmath15 to model the local level of @xmath10 , together with the values of @xmath16 _ contemporaneous _ series @xmath17 with indices in the _ simultaneous parental set _",
    "@xmath18 of size @xmath19 .",
    "the latter allows for effective modeling of cross - series , time - varying conditional dependencies and across @xmath20 this defines a simultaneous equations formulation of multivariate volatility .",
    "* the state vector @xmath21 is partitioned accordingly : @xmath22 is the @xmath14-regression vector of @xmath23 and @xmath24 that for the @xmath16 simultaneous parental coefficients @xmath17 .",
    "* the precision process @xmath25 allows modeling of residual stochastic volatility patterns over time . *",
    "conditional on the volatility processes , the zero - mean normal observation errors @xmath26 are independent across series @xmath20 and time @xmath27    write @xmath28 , where @xmath29 , and @xmath30 .",
    "furthermore , write @xmath31 for the matrix that contains the elements of the simultaneous parental coefficients @xmath24 , with extension to @xmath32 if @xmath33 .",
    "conditional on these quantities , the multivariate series is conditionally normal , @xmath34 where @xmath35 and @xmath36 .",
    "the sgdlm allows for modeling flexibility in that different external predictors @xmath23 can be selected for each series .",
    "furthermore , the simultaneous parental specification of the volatility matrix @xmath37 allows for sparse models since the sizes @xmath16 of the parental sets @xmath38 can be chosen much smaller than @xmath9 .",
    "the states and precisions evolve according to a standard dlm  @xcite with a linear , gaussian state evolution for @xmath39 coupled to a discount volatility model for @xmath40 enabling closed - form computations for sequential filtering and forecasting .",
    "full specification involves cumulated information summarized in conditionally conjugate distributions , as follows .    independently across series , the prior for the series @xmath20 state vector and precision is normal / gamma @xmath41 in this @xmath42 notation , @xmath43 and @xmath44 with shape @xmath45 , rate @xmath46 and mean @xmath47 .",
    "the implied @xmath39 margin is multivariate t with @xmath48 degrees of freedom , mode @xmath49 and scale matrix @xmath50 .    with @xmath51 $ ] and @xmath30 the joint prior across series has density @xmath52    from @xmath53 to @xmath54 the state evolves conditional on @xmath25 and @xmath55 via @xmath56 based on evolution matrices @xmath57 and",
    "innovations @xmath58 having conditional variance matrices @xmath59 scaled by @xmath60 .",
    "conditional on the model states , volatility processes , evolution transition and variance matrices , the zero - mean observation errors @xmath26 and state innovations @xmath58 are independent and mutually independent across series @xmath20 and over time @xmath2 .",
    "the @xmath59 matrices are specified using discount factors  @xcite as detailed further below .    the one - step ahead predictive distributions are efficiently evaluated by simulation .",
    "draw from the set of @xmath9 independent normal / gamma priors of   to define a simulation sample @xmath61 , where @xmath62 indexes monte carlo samples for prediction .",
    "each sample defines monte carlo values of one - step forecast moments @xmath63 , which can be used to simulate from the predictive distribution of @xmath64 using the conditionally normal form of .",
    "step - ahead forecasting more than one period is similarly easily done via simulation .",
    "the exact posterior is @xmath65 where each @xmath66 factor is of a normal / gamma form @xmath67 that arises from standard analytic updating of each series individually .",
    "the parameters are obtained as @xmath68 , @xmath69 , @xmath70 and @xmath71 , after first computing the forecast error @xmath72 , forecast variance factor @xmath73 , adaptive coefficient vector @xmath74 and volatility update factor @xmath75 .",
    "the determinant term appearing in the exact posterior above theoretically _",
    "recouples _ the prior - independent states to account for between - series dependence effects .",
    "@xcite show the efficacy of importance sampling to evaluate characteristics of the exact posterior of .",
    "samples from the independent normal / gammas @xmath76 are importance - weighted by the resulting values of the determinant term @xmath77 to define the monte carlo approximation to the full joint posterior .",
    "to enable independent parallel processing of prior evolutions across series @xmath20 , the exact posterior is _ decoupled _ into a product of conjugate forms across the series @xmath11 .",
    "this uses a standard mean - field variational bayes ( vb ) approach  ( e.g. , ( * ? ? ?",
    "12.3 ) ; @xcite ) that emulates the exact posterior by a product of normal / gammas @xmath78 the vb strategy selects the parameters in   to minimize the kullback - leibler divergence of the product form @xmath79 from the exact joint posterior @xmath80 . [ sgdlm2:app : vb - decoupling ] gives summary equations .",
    "moving ahead one time point , states and volatilities undergo evolutions . for each @xmath20 ,",
    "the @xmath25 first evolves to @xmath81 according to the standard gamma / beta stochastic volatility model ; see , for example ,  ( * ? ? ?",
    "10.8 ) ,  ( * ? ? ?",
    "this is based on a series - specific discount factor @xmath82 , typically close to 1 . following this",
    ", the state vector @xmath39 evolves to @xmath83 according to the state evolution of   but with @xmath2 updated to @xmath84 the specification is such that the evolved priors at time @xmath3 maintain the normal / gamma form , enabling fast , exact analysis ; resulting priors are precisely as in   with @xmath2 updated to @xmath3 .",
    "the parameter evolutions @xmath85 , @xmath86 and @xmath87 follow standard dlm theory and notation as in the above references .",
    "model completion requires specification of the @xmath88 and @xmath89 matrices . in the case study of section  [ sgdlm2:sec : data - example ] , each @xmath90 and we use block discounting to specify the @xmath89  ( * ? ? ?",
    "* sect . 6.3 ) . for series",
    "@xmath91 this uses two discount factors : @xmath92 for the external predictor state vector , and @xmath93 for the parental state vector , with values satisfying @xmath94 with @xmath95 , this defines @xmath96 as a partitioned matrix with upper - left block diagonal @xmath97 , lower - right block diagonal @xmath98 , and upper - right ( covariance ) block @xmath99 .",
    "recoupling of the posteriors using importance sampling is the only computationally demanding step .",
    "this is well - suited to gpu - based massive parallelization since the @xmath9 model simulations are decoupled . as shown in  @xcite , this makes fully bayesian , real - time analysis with @xmath9 in the hundreds to thousands feasible . on standard 2014 gpu - enabled desktop machines , one full iteration of evolution / forecasting / updating takes less than 10 seconds with @xmath100 and modest dimensional models .",
    "the software discussed in that reference is used here .",
    "we will typically have @xmath101 much smaller than @xmath9 in problems where @xmath9 is at all large . with @xmath102 in our s&p case study ( section  [ sgdlm2:sec : data - example ] ) ,",
    "there are many patterns of time - varying dependencies among stocks , but it is inappropriate to expect real practical value in estimating co - volatilities from models with more than , say , 20 or so simultaneous predictors . that is , the implied dynamic graphical model  represented by zeros",
    "/ non - zeros in @xmath103 and @xmath104will typically be quite sparse .",
    "collinearities among potential simultaneous parental series will typically mean that many possible choices of a ( smallish ) parental set for any one series will yield similar predictions , so working with one set of selected @xmath38 over short time periods is desirable .",
    "the perspective here is critical : we are not interested in formal inference on parental sets , and such sets will not typically be stable over time or practically identifiable in problems with many series .",
    "choices of parental sets are only interesting as vehicles to improved forecasts and decisions . in larger problems ,",
    "any choice of a set of , say 10 parents for one series for a particular short time period will be practically indistinguishable from multiple other candidate sets in which some of the parents are replaced by strongly collinear alternatives . rather , the perspective is to identify small parental sets and adaptively revise them over time to capture and characterize the structure and dynamics of resulting ( precision and variance ) volatility matrices .",
    "our goals and interests are forecasting and portfolio decisions , and the dynamic precision matrices drive core aspects of the overall bayesian decision analysis .",
    "hence , we define a novel strategy to systematically and adaptively select / revise the @xmath38 over time , applied separately in parallel to each series .",
    "standing at a given time @xmath54 partition each set @xmath38 into three categories : a dynamic _ core _ group of simultaneous parents , @xmath105 ; a set of candidate simultaneous parents @xmath106 ; and a set of outgoing simultaneous parents @xmath107 .",
    "the core simultaneous parents define the current sparsity structure of the sgdlm s and underlie cross - series links in terms of precisions / co - volatilities .",
    "the warm - up groups serve to inform learning on the dynamic posterior distribution of the simultaneous regression coefficients @xmath108 of recently added simultaneous parental series in combination with the existing simultaneous parents as the full sgdlm is filtered forward .",
    "the outgoing group contains parents that are eliminated from either the warm - up group or core group , and are phased - out over several time steps by gradually shrinking their coefficients @xmath109 to zero .",
    "the dimensions of the three simultaneous parental sets are defined by the modeller . while we see merit in the use of approaches such as dynamic latent thresholding  ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ) that set these dimensions autonomously , such approaches are simply not adaptable to forward filtering and forecasting contexts with higher dimensional series .",
    "computational issues are a barrier , but ",
    "more importantly the perspective that we care only about useful predictive models , and not at all about specific parental sets that may be playing roles , indicates that such approaches are overkill .",
    "we focus on a more direct selection strategy that is consistent with this perspective and that provides an elegant solution to several typical problems of dynamic model selection . our bayesian strategy has a number of practically key features , now noted and then elaborated in following discussion . specifically :    * forward filtering selection requires very little additional computation time , and is substantially better - suited for on - line application than conducting any kind of formal bayesian model search at every step @xmath2 .",
    "methods based on markov chain , sequential monte carlo algorithms or related stochastic search methods are simply infeasible ( technically and ) computationally , as well as philosophically directed towards goals that are not relevant in our contexts .",
    "* the idea of a warm - up period to phase in new simultaneous parents uses data - informed posterior learning ( over several time steps ) and eliminates the need for delicate specification of initial priors of new simultaneous regression coefficients . *",
    "phasing - in new simultaneous parents from neutral zero - mean initial priors , and phasing out existing simultaneous parents by gradual shrinkage to zero makes resulting forecasts of multivariate volatility patterns robust by , in part , inducing smooth \" changes in the structure of resulting dynamic predictive precision matrices .",
    "at each @xmath2 , we allow for changes in the current \" parental set for each series .",
    "a key part of this is that candidate simultaneous parents lie in the warm - up sets @xmath106 .",
    "the size of each of these sets is a fixed value @xmath110 ; this value also determines how many time steps each simultaneous parent is granted before it will either be included in @xmath111 , or be gradually eliminated via assignment to @xmath112 .    linking to formal and mcmc - based bayesian variable selection , note that mcmc sampling consists of repeated performance of a proposal step and an acceptance / rejection step for candidate variables to include or exclude .",
    "our forward filtering selection builds on this underlying mcmc concept , adapting it to the forward / sequential analysis with our explicit decision focus .",
    "the first modification is that forward filtering selection uses only one proposal at each time point @xmath2 , while mcmc sampling generates as many proposals as there are mcmc iterations . in our parental selection selection ,",
    "the decision to accept or reject the time @xmath2 proposal is delayed to time @xmath113 , at which point the proposed additional parents have been tentatively included in the @xmath114 , @xmath115 , since time @xmath2 . by separating proposal generation from the acceptance decision , posterior information from joint updates of the regression coefficients of the proposed and existing simultaneous parents can be factored into the decision , and",
    "the choice of initial priors for newly added simultaneous parents becomes mostly irrelevant .",
    "then , the acceptance rule is different from the typical metropolis ",
    "hastings acceptance probability , as the goal is to select between two alternatives and not to estimate posterior probabilities of every possible model specification .",
    "our new strategy adaptively revises simultaneous parental sets based on a parallel analysis of the data using a standard wdlm . while this standard analysis is limited in terms of scalability and in its potential to predict changes in multivariate volatility patterns , it is able to track and adapt to such changes , so providing an obvious proposal `` model for generating insights into parental structure . the simple conjugate / analytic sequential analysis of the wdlm tracks and estimates the @xmath116 time - varying precision matrix @xmath104 without constraints .",
    "inference on @xmath104 allows interrogation of resulting posterior wishart distributions as they evolve over time . at any time",
    "@xmath54 off - diagonal elements in row @xmath20 define conditional regression coefficients of all @xmath117 series @xmath118 in predicting @xmath10 .",
    "larger absolute values of the precision elements in row @xmath20 thus suggest candidates for inclusion in the parental set @xmath38 . in our case study , we consider series @xmath119 for inclusion in @xmath38 if the absolute value of the @xmath120 precision element is among the largest @xmath121 values in row @xmath20 .",
    "each such series @xmath122 not already in the warm - up or core parental sets becomes a candidate in the warm - up set ; i.e. , each series @xmath122 among these top '' @xmath123 is added to @xmath106 if @xmath124 .",
    "this inclusion of series @xmath122 as a candidate `` parent of series @xmath20 involves specifying prior ( at the current time @xmath2 ) information for the corresponding coefficient @xmath125 we take this as having zero mean and a specified variance . once the candidate parental series is embedded in the model , posterior information on its contribution and relevance is generated during the regular evolution and updates over times @xmath126 after this learning period , the candidate parent is promoted into the core set @xmath127 . if this addition grows the core set beyond its target size , the additional as well as incumbent parental series are reviewed to drop ( or retire '' ) one or more parents , as follows .",
    "at each time @xmath2 and for each series @xmath91 simultaneous parents are retired from @xmath105 if this set exceeds its target size through the addition of a new parents .",
    "this process involves two steps : the selection of the series that will be dropped , and then the phasing out of the selected series .",
    "we target series @xmath122 for dropping based on inference on current values of parental coefficients @xmath128 , using standardized posterior values ( a.k.a .",
    "signal - to - noise ratios ) @xmath129 parental predictors with small values of these ratios are candidates for the retirement , i.e. , for inclusion in the phase - out set @xmath107 .",
    "elimination of regression effects with small signal - to - noise ratios is conducive to improving forecast performance and reliability .",
    "the simultaneous coefficients @xmath109 of outgoing parents in @xmath107 are gradually shrunk to zero over the next @xmath130 time steps .",
    "shrinkage is implemented as prior intervention through the state evolution matrices @xmath57 by appropriately scaling the corresponding diagonal entries as @xmath131 relative to that time @xmath132 at which series @xmath122 was added to @xmath133 . the sequential shrinkage in   results in stochastic reduction of the role of series @xmath122 to zero in @xmath130 steps .",
    "shrinkage over several time steps allows for the roles of other simultaneous parents to adjust , and makes forecasts of the precision and covariance matrices of @xmath64 more robust via the resulting smooth \" transitions of parental predictors included / excluded .",
    "section  [ sgdlm2:sec : data - example ] involves assessments of a range of dynamically optimized and updated portfolios based on bayesian decision analysis under several portfolio utility functions  ( * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "we explore portfolio utilities that represent currently topical and relevant approaches in modern quantitative investment management , all being extensions of traditional penalized mean - variance decision rules .",
    "the analysis models daily log - returns on stocks and sequentially updates the portfolio allocation across these stocks via bayesian decision analysis using chosen portfolio utility functions .",
    "mean - variance optimization aims to control risk while aiming for positive returns , and modified utilities overlay additional , practically relevant constraints .",
    "in addition to specific _ target return _ portfolios , we consider utility functions that incorporate a benchmark index and require that optimized portfolios be , in expectation , uncorrelated with the benchmark in addition to target return and risk components .",
    "our models are applied to the vector of daily log - returns @xmath64 . in all models , the mean and variance matrix of the one - step ahead forecast distribution @xmath134 are key ingredients .",
    "denote these by @xmath135 and @xmath136 . in the sgdlm ,",
    "these are computed via monte carlo simulation .",
    "a portfolio weight vector @xmath137 defines the allocation of capital across the @xmath9 assets .",
    "the decision is to choose @xmath138 at market close on day @xmath53 , and then act on that reallocation ; on day @xmath2 , the new closing prices are realized and the process repeats on the following day .",
    "based on the forecast distribution of log - returns , the implied one - step ahead forecast mean and variance of the portfolio for any specific weight vector @xmath138 are @xmath139 and @xmath140 , respectively .",
    "the standard or baseline minimum variance portfolio chooses @xmath138 as that vector minimizing the expected portfolio variance @xmath141 subject to @xmath142 .",
    "the optimal weight vector is trivially computed .",
    "more practically relevant portfolio strategies overlay additional constraints , as follows .",
    "the original  @xcite mean - variance portfolio rule minimizes the risk  again in terms of portfolio variance  for a given , desired target return @xmath143 .",
    "the relevant decision analysis simply modifies the minimum variance portfolio optimization by adding the constraint @xmath144 , or its practical equivalent @xmath145 .",
    "note that the targets @xmath143 can vary over time , and be chosen adaptively by either direct specification or an automated rule .",
    "this refinement mandates that the portfolio be uncorrelated , in expectation , at each step with a selected benchmark time series . to implement this , joint 1-step ahead forecast distributions",
    "are required for the assets of interest together with the benchmark series . with no loss of generality",
    ", we do this by taking the selected benchmark series as @xmath146 .",
    "the relevant decision analysis then simply modifies the portfolio optimization rules above by adding the constraints @xmath147 and @xmath148 , where @xmath149 is the first column of @xmath150 containing the covariances of all series with the benchmark .",
    "we use data on the s&p 500 stock market index ( spx ) and 400 s&p 500 member stocks that have been continuously listed since 2002 .",
    "the full data set covers the years 2002 through q3 - 2013 .",
    "we are interested in among other things comparisons using benchmark neutral portfolios , and take spx as the benchmark ; our models are thus for the @xmath102-dimensional vector of returns comprising spx as the first entry , followed by the 400 stocks . for each series @xmath20 , daily log - returns are @xmath151 using the daily closing prices . for clarity ,",
    "as the spx series is of particular interest as a benchmark , we label the first return series accordingly : @xmath152 our comparative analyses assume that there are no bid - ask spreads , and that trading costs are in the amount of 10 basis points of the traded volume .",
    "we assume that all trades can be executed at the daily closing price and that short - selling is possible .",
    "our calculations of annualized returns and volatilities assume that a year consists of 252 trading days .",
    "we study analysis of several variants of the sgdlm of section  [ sgdlm2:sec : forecast - models ] , based on different choices of exogenous predictors and discount factors .",
    "table  [ sgdlm2:tb : data - example : dlms ] provides a full summary of the models used .",
    "each sgdlm has fixed , core parental set sizes @xmath153 for each series @xmath91 and the adaptive parental strategy is based on @xmath154    .list of sgdlms in case study : predictors column indicates the model equation ; @xmath155 are values of model discount factors ; @xmath156 gives values of overall predictive log - likelihoods from the data analyses over 20032013 ; @xmath157 gives corresponding mean absolute deviations of one - step ahead point forecast errors averaged over all 401 series and across the period .",
    "[ cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]     gruberwest2016arxiv.bbl",
    "research presented here was partially developed while the first author was a visiting scholar in the department of statistical science at duke university .",
    "partial financial support was provided by the fulbright foundation through the fulbright program for foreign students ( l.f.g . ) .",
    "all opinions , findings and conclusions or recommendations expressed in this work are those of the authors and do not necessarily reflect the views of the fulbright foundation .      _ lutz gruber _ is senior analyst at e - commerce analytics firm quantco , germany .",
    "his main research foci are in on - line learning of financial time series , dependence analysis with copulas , and statistical econometric modeling .",
    "lutz received his ms in mathematical finance and actuarial science at the technical university of munich ( tum ) in 2011 , followed by his phd in statistics at tum in 2015 .",
    "_ mike west _ ( http://www.stat.duke.edu/~mw[www.stat.duke.edu/@xmath158mw ] ) is the arts & sciences professor of statistics & decision sciences in the department of statistical science at duke university .",
    "mike led development of the department from 1990 to 2001 , has served in the establishment and as board member of several national research institutes and companies , and is past president of the international society for bayesian analysis .",
    "mike works in theory and applications of bayesian statistics , with highlights in dynamic modeling , time series analysis and forecasting .",
    "mike has advised nearly 60 phd students and postdoctoral associates , and numerous undergraduate and ms students ."
  ],
  "abstract_text": [
    "<S> the recently introduced class of simultaneous graphical dynamic linear models ( sgdlms ) defines an ability to scale on - line bayesian analysis and forecasting to higher - dimensional time series . </S>",
    "<S> this paper advances the methodology of sgdlms , developing and embedding a novel , adaptive method of simultaneous predictor selection in forward filtering for on - line learning and forecasting . </S>",
    "<S> the advances include developments in bayesian computation for scalability , and a case study in exploring the resulting potential for improved short - term forecasting of large - scale volatility matrices . </S>",
    "<S> a case study concerns financial forecasting and portfolio optimization with a 400-dimensional series of daily stock prices . </S>",
    "<S> analysis shows that the sgdlm forecasts volatilities and co - volatilities well , making it ideally suited to contributing to quantitative investment strategies to improve portfolio returns . </S>",
    "<S> we also identify performance metrics linked to the sequential bayesian filtering analysis that turn out to define a leading indicator of increased financial market stresses , comparable to but leading the standard st . </S>",
    "<S> louis fed financial stress index ( stlfsi ) measure . </S>",
    "<S> parallel computation using gpu implementations substantially advance the ability to fit and use these models .    </S>",
    "<S> bayesian forecasting and portfolio optimization ; dynamic graphical models ; financial risk index ; gpu computation ; high - dimensional time series ; sparse multivariate stochastic volatility models . </S>"
  ]
}