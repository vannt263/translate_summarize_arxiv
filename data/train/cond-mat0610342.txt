{
  "article_text": [
    "the problem of effectively combining _ unlabeled _ data with _ labeled _ data , semi - supervised learning , is of central importance in machine learning ; see , for example , @xcite and references therein .",
    "semi - supervised learning methods usually assume that adjacent points and/or points in the same structure ( group , cluster ) should have similar labels ; one may assume that data are situated on a low dimensional manifold which can be approximated by a weighted discrete graph whose vertices are identified with the empirical ( labeled and unlabeled ) data points .",
    "this can be seen as a form of regularization @xcite . a common feature of these methods ,",
    "see also @xcite , is that , as the number of labeled points vanishes , the solution tends to the constant vector .",
    "an interesting survey on semi - supervised learning literature may be found on the web @xcite .",
    "improving regression with unlabeled data is the problem considered in @xcite , where co - training is achieved using k - nn regressors .",
    "a statistical physics approach , based on the potts model , is described in @xcite .",
    "an issue closely related to semi - supervised learning is active - learning : some attempts to combine active learning and semi - supervised learning has been made @xcite .",
    "the purpose of this work is to introduce a semi - supervised learning estimator which , as the number of labeled points vanishes , tends to the first kernel principal component @xcite ; when a suitable number of labeled points is available , it may be used for transductive inference @xcite .",
    "our approach is based on the following fact .",
    "given an unlabeled data set , its first kernel principal component is such that , treating it as target vector , supervised kernel ridge regression provides the minimum training error .",
    "now , suppose that you are given a partially labeled data set : still one may look for the target vector minimizing the training error .",
    "this optimal target vector may be seen as the generalization of the first kernel principal component to the semi - supervised case .",
    "the paper is organized as follows . in the next section",
    "we describe our approach , while in section 3 the experiments we performed are described .",
    "some conclusions are drawn in section 4 .",
    "we briefly recall the properties of kernel ridge regression ( krr ) , while referring the reader to @xcite for further technical details .",
    "let us consider a set of @xmath3 independent , identically distributed data @xmath4 , where @xmath5 is the @xmath6-dimensional vector of input variables and @xmath7 is the scalar output variable .",
    "data are drawn from an unknown probability distribution ; we assume that both @xmath0 and @xmath2 have been centered , i.e. they have been linearly transformed to have zero mean .",
    "the regularized linear predictor is @xmath8 , where @xmath9 minimizes the following functional : @xmath10 here @xmath11 and @xmath12 is the regularization parameter . for @xmath13 , predictor ( [ lagrangian - function - without - b ] )",
    "is invariant when new variables , statistically independent of input and target variables , are added to the set of input variables ( iiv property , @xcite ) .",
    "one may show that this invariance property holds , for ( [ lagrangian - function - without - b ] ) , also at finite @xmath14 .",
    "krr is the _ kernel _ version of the previous predictor .",
    "calling @xmath15 = @xmath16 the vector formed by the @xmath3 values of the output variable and @xmath17 being a positive definite symmetric function , the predictor has the following form : @xmath18 where coefficients @xmath19 are given by @xmath20 @xmath21 being the @xmath22 matrix with elements @xmath23 .",
    "equation ( [ notlinear ] ) may be seen to correspond to a linear predictor in the feature space @xmath24 where @xmath25 and @xmath26 are the eigenvalues and eigenfunctions of the integral operator with kernel @xmath27 .",
    "one may show @xcite that , for krr predictors with nonlinear kernels , the iiv property does not generically hold , even for those kernels , discussed in @xcite , for which the property holds at @xmath28 .",
    "regularization breaks the iiv invariance in those cases .",
    "due to ( [ notlinear ] ) and ( [ w2 ] ) , the predicted output vector @xmath29 , in correspondence of the _ true _ target vector @xmath15 , is given by @xmath29@xmath30 , where the symmetric matrix @xmath31 is given by @xmath32 note that matrix @xmath31 depends only on the distribution of @xmath33 values : @xmath31 embodies information about the structures present in @xmath33 data set .",
    "indeed , for @xmath34 , the matrix element @xmath35 quantifies how much the target value of the @xmath36 point influences the estimate of the target of point @xmath37 .",
    "let us now consider the leave - one - out scheme ; let data point @xmath37 be removed from the data set and the model be trained using the remaining @xmath38 points .",
    "we denote @xmath39 the target value thus predicted , in correspondence of @xmath40 .",
    "it is well known @xcite that the leave - one - out - error @xmath41 and the training error obtained using the whole data set @xmath42 satisfy : @xmath43 this formula shows that the closer @xmath44 to one , the farther the leave - one - out predicted value from those obtained using also point @xmath37 in the training stage .",
    "consider a point @xmath37 in a dense region of the feature space : one may expect that removing this point from the data - set would not change much the estimate since it can be well predicted on the basis of values of neighboring points .",
    "therefore points in low density regions of the feature space are characterized by diagonal values @xmath44 close to one , while @xmath44 is close to zero for points @xmath45 in dense regions : the diagonal elements of @xmath31 thus convey information about the structure of points in the feature space .",
    "it is worth stressing that , given a kernel function , the corresponding features @xmath46 are not centered in general .",
    "one can show @xcite that centering the features ( @xmath47 , for all @xmath48 ) amounts to perform the following transformation on the kernel matrix : @xmath49 where @xmath50 , and to work with the centered kernel @xmath51 . in the following",
    "we will assume that the kernel matrix @xmath52 has been centered .",
    "the training error of the krr model is proportional to @xmath53 where @xmath54 is a symmetric and positive matrix . in the unsupervised case the data set",
    "is made of @xmath55 points , @xmath56 , the target function @xmath1 is missing .",
    "however we may pose the following question : what is the vector @xmath57 such that treating it as the target vector leads to the best fit , i.e. the minimum training error @xmath58 ?",
    "we expect that this _ optimal _ target vector would bring information about the structures present in the data . to avoid the trivial solution @xmath59 , we constrain the target vector to have unit norm , @xmath60 ; it follows that the optimal vector is the normalized eigenvector of @xmath61 with the smallest eigenvalue . on the other hand ,",
    "matrix @xmath61 is a function of matrix @xmath52 : hence it has the same eigenvectors of @xmath52 while the corresponding eigenvalues @xmath62 and @xmath63 are related by the following monotonically decreasing correspondence : @xmath64 therefore , independently of @xmath65 , the smallest eigenvalue of @xmath61 corresponds to the largest eigenvalue of @xmath52 , and the optimal vector coincides with the first kernel principal component . to conclude this subsection ,",
    "we have shown that the method in [ 10 ] may be motivated also as the search for the optimal target vector .",
    "the notion of optimal target vector has been introduced in @xcite , where a kernel method for dichotomic clustering has been proposed , consisting in finding the ground state of a class of ising models .",
    "now we consider the case that we are given a set @xmath66 of data points with unknown targets @xmath67 , and a set @xmath68 , where @xmath69 , of input - output data . without loss of generality",
    "we assume that the labeled points belong to two classes , and take @xmath70 for all @xmath71 s .",
    "the @xmath72 dimensional full vector of targets @xmath1 is obtained appending @xmath73 ( unknown ) and @xmath74 ( known ) values : @xmath75 keeping the kernel and @xmath65 fixed , we look for the unit norm target vector @xmath1 minimizing the training error @xmath76 .",
    "the @xmath77 matrix @xmath61 has the block structure @xmath78 where @xmath79 is an @xmath22 matrix . neglecting a constant term , the optimal vector is determined by the vector @xmath80 minimizing @xmath81 under the constraint @xmath82 .",
    "the first term of @xmath83 favors projections of the @xmath3 points with great variance , whereas the second term measures their consistency with labeled points .",
    "let us denote @xmath84 and @xmath85 the eigenvectors and eigenvalues of @xmath79 , sorted into increasing @xmath86 .",
    "we express @xmath87 .",
    "the coefficients @xmath88 for the minimum are given by @xmath89 where @xmath90 , and @xmath91 is a lagrange multiplier which must to be tuned to satisfy : @xmath92    equation ( [ csi ] ) has always at least one solution with @xmath93 , see figure 1 , and usually this is the one minimizing @xmath83 .",
    "however all the solutions of ( [ csi ] ) must be compared according to their _ energies _ @xmath83 ; those corresponding to the lowest @xmath83 , @xmath94 , is then selected .",
    "clearly as @xmath95 one recovers the first eigenvector of @xmath79 , i.e. the first kernel principal component : @xmath94 thus constitutes a generalization of the latter to the semi - supervised case . to construct the other generalized kernel principal components",
    ", we make the following transformation on matrix @xmath61 : @xmath96 where @xmath97 is the projector on the linear subspace spanned by @xmath94 .",
    "the symmetric matrix @xmath98 has the lowest eigenvalue equal to zero and corresponding to eigenvector @xmath94 .",
    "the system of eigenvectors of @xmath98 constitutes a generalization of kernel principal components to the semi - supervised case .",
    "now we present some simulations of the proposed method , focusing on the dimensionality reduction issue and comparing with fully unsupervised kernel principal component analysis .",
    "we consider three well known data sets : iris ( 100 points in a four - dimensional space , second and third classes , versicolor and virginica ) ; colon cancer data set of @xcite , consisting in 40 tumor and 22 normal colon tissues samples , each sample being described by the @xmath99 most discriminant genes ; the leukemia data set of @xcite , consisting of samples of tissues of bone marrow samples , @xmath100 affected by acute myeloid leukemia ( aml ) and @xmath101 by acute lymphoblastic leukemia ( all ) , each sample being described by the @xmath102 most discriminant genes .",
    "the following question is addressed : is @xmath94 more correlated to the true labels than the fully unsupervised first kernel principal component ? here we restrict our analysis to the linear kernel .",
    "we start with iris and proceed as follows .",
    "we randomly select @xmath103 points and , treating them as labeled , we find the system of eigenvectors of @xmath98",
    ". then we evaluate the linear correlation @xmath104 between the eigenvectors and the true labels of the whole data - set .",
    "the distributions of @xmath104 for the four eigenvectors are depicted in figure 2 .",
    "we observe that in most cases the vector @xmath94 is more correlated with the true classes than the fully unsupervised principal component : the one - dimensional projection of data onto @xmath94 is more informative than the first principal component .",
    "however there are situations where use of labeled points leads to poor results ; a typical example is depicted in figure 3 . in figure 4 a situation",
    "is depicted where knowledge of labeled points leads to a relevant improvement .    in general",
    ", we denote @xmath105 the fraction of instances such that @xmath94 is more correlated to the true labels than the first principal component . in figure 5",
    "we depict @xmath105 as a function of @xmath106 for the three data sets here considered . at @xmath107 @xmath105",
    "is already nearly one .",
    "the semi - supervised method here proposed outperforms principal components almost always for large @xmath108 .      in this subsection",
    "we demonstrate the effectiveness of the proposed approach for estimating the values of a function at a set of test points , given a set of input - output data points , without estimating ( as an intermediate step ) the regression function .",
    "the boston data set is a well - known problem where one is required to estimate house prices according to various statistics based on @xmath109 locational , economic and structural features from data collected by u.s .",
    "census service in the boston massachusetts area . for @xmath110 ,",
    "we partition the data - set of @xmath111 observations randomly 100 times into a training set of @xmath112 observations and a testing set of @xmath3 observations .",
    "we use a gaussian kernel with @xmath113 and set @xmath114 ; results are stable against variations of these parameters . in table 1",
    "we report the mean squared error ( mse ) on the test set averaged over the 100 runs , for each value of @xmath3 , we obtain using the optimal target vector @xmath94 . in table 1",
    "we also report the mse obtained using the classical krr in the two step procedure : ( i ) estimation of the regression function using the training data - set ( ii ) calculation of the regression function at points of interest ( test data - set ) .",
    "the improvement achieved using the optimal target approach , over classical krr , is clear .",
    ".[tab : table1]the mean square error on the boston data set obtained using the optimal target ( ot ) approach and the classical kernel ridge regression ( krr ) method .",
    "the size of the test set is @xmath3 . [",
    "cols=\"<,^,>\",options=\"header \" , ]     it is worth stressing that our results are obtained without a fine - tuning of parameters .",
    "in particular , note that our definition of optimal target vector fixes the relative importance of the two terms in equation ( [ eeee ] ) .",
    "we have presented a new approach to semi - supervised learning based on the notion of optimal target vector , the target vector such that krr provides the minimum training error over all the possible target vectors .",
    "the proposed algorithm is characterized by the fact that the first kernel principal component is recovered as the cardinality of labeled points vanishes ; hence it may be seen as a semi - supervised generalization of kernel principal components analysis . the effectiveness of the proposed approach for transductive inference has also been demonstrated .",
    "* acknoledgements . *",
    "the authors thank olivier chapelle for a valuable correspondence on the subject of this paper .",
    "discussions on semi - supervised learning with eytan domany and noam shental are warmly acknowledged .",
    "d. zhu , o. bousquet , t.n .",
    "lal , j. weston , and b. scholkopf .",
    "learning with local and global consistency .",
    "_ advances in neural information processing systems _ , 16 , s. thrun et al .",
    "( eds . ) , mit press , cambridge , ma , 2004 .",
    "a. argyriou , m. herbster , m. pontil , combining graph laplacians for semi - supervised learning , _ advances in neural information processing systems _ , 18 , y. weiss and b. schlkopf and j. platt ( eds . ) , mit press , cambridge , ma , 2006 .",
    "g. getz , n. shental , e. domany , semi - supervised learning - a statistical physics approach .",
    "proceedings of the 22nd icml workshop on learning with partially classified training data .",
    "bonn , germani 2005 .",
    "x. zhu , j. lafferty , z. ghaharamani , combining active learning and semi - supervised learning using gaussian fields and harmonic functions .",
    "icml 2003 workshop on the continuum from labeled to unlabeled data in machine learning and data mining ."
  ],
  "abstract_text": [
    "<S> we introduce a semi - supervised learning estimator which tends to the first kernel principal component as the number of labeled points vanishes . </S>",
    "<S> our approach is based on the notion of optimal target vector , which is defined as follows . </S>",
    "<S> given an input data - set of @xmath0 values , the optimal target vector @xmath1 is such that treating it as the target and using kernel ridge regression to model the dependency of @xmath2 on @xmath0 , the training error achieves its minimum value . for an unlabeled data </S>",
    "<S> set , the first kernel principal component is the optimal vector . in the case one is given a partially labeled data set , still one may look for the optimal target vector minimizing the training error . </S>",
    "<S> we use this new estimator in two directions . as a substitute of kernel principal component analysis , in the case one has some labeled data , to produce dimensionality reduction . </S>",
    "<S> second , to develop a semi - supervised regression and classification algorithm for transductive inference . </S>",
    "<S> we show application of the proposed method in both directions . </S>"
  ]
}