{
  "article_text": [
    "mdl @xcite , is one of the most successful methods for determining the number of present signals in array processing and channel order detection @xcite .",
    "mdl is a low complexity information theoretic criteria which does not need any subjective threshold setting usual in detection theoretic criteria .",
    "other statistical properties , specially its asymptotic consistency @xcite , makes it a favorable choice for source enumeration .",
    "unfortunately , only few approximate finite - sample performance analysis are available on the mdl method @xcite . in @xcite , a simple asymptotic statistical model for the eigenvalues of the sample correlation matrix",
    "unfortunately , the theoretical results showed persistent bias from the simulation results @xcite .",
    "the next work @xcite , gives a computational approach for calculation of the probability of false alarm @xmath0 . in calculating the probability of missed detection @xmath1",
    ", the same inaccurate statistical model is used as in @xcite . in @xcite , instead of exact performance estimation , theoretical bounds for performance were presented . a qualitative performance evaluation in terms of gap between noise and signal eigenvalues and also the dispersion of each group",
    "is given in @xcite .",
    "in a recent work @xcite , a significantly different approach was used .",
    "our simulation results show improved results of @xcite in comparison with @xcite .",
    "the performance analysis was generalized to the non - gaussian signals while it was shown that the results reduce to the results of @xcite in gaussian signals .",
    "we will show that the same modelling errors have degraded the analysis in @xcite as in @xcite .    in this correspondence",
    ", we use an approach very similar to @xcite to estimate @xmath1 , including in the analysis the finite sample @xmath2 biases of the eigenvalues .",
    "the noise subspace eigenvalue spread is taken into account which prevents the signal subspace eigenvalues to approach @xmath3 , the noise variance .",
    "the bias of the noise power estimator in mdl is calculated to get excellent match between theoretical and simulation results .",
    "we will not calculate @xmath0 which is negligible .    in the previous works ,",
    "only the case of stochastic signal has been considered . here",
    ", we use a perturbation analysis to calculate biases and variances of the eigenvalues under deterministic signal , too . using these results ,",
    "we show that the performance of source enumeration methods are approximately the same in both stochastic and deterministic signal models .",
    "this is a natural complementary result for the known fact that the performance of the doa ( direction of arrival ) estimation methods in array processing is the same under stochastic and deterministic signal models @xcite .    from a sensor array of @xmath4 elements , @xmath5 observations @xmath6",
    "is made , which is a linear transformation of @xmath7 source signals @xmath8 , plus noise @xmath9 @xmath10 where @xmath11 , the steering matrix , is composed of @xmath12 linearly independent column vectors of array response @xmath13 .",
    "let @xmath14 $ ] and @xmath15 and @xmath16 be defined in the same way .",
    "signal and noise are assumed to be iid and uncorrelated random variables . a compact form for the model will be @xmath17 noise",
    "is assumed to be circular gaussian .",
    "signal can be modelled either as a zero - mean circular gaussian random sequence or an unknown deterministic sequence .",
    "the distribution of @xmath18 will be as @xmath19 where @xmath20 in the _ stochastic _ signal model , and as @xmath21 in the _ deterministic _ signal model .    to estimate the number of present signals @xmath12 , eigenvalues of the correlation matrix @xmath22",
    "are used . note that @xmath23 and @xmath24 .",
    "the eigendecomposition of the correlation matrix is @xmath25 and we have @xmath26 .",
    "source enumeration methods are based on a spherity test on the sample correlation matrix defined as @xmath27 eigendecomposition of @xmath28 is defined as @xmath29 in which @xmath30 .",
    "the mdl estimator of @xmath12 is the minimizer of the following criterion @xmath31 where @xmath32 @xmath33 the first term in is the generalized likelihood ratio for the test of spherity and the second term is a penalty function preventing over - modelling .",
    "first of all , we derive a result useful for statistical characterization of the signal eigenvalues in the deterministic signal model .",
    "let @xmath34 be i.i.d .",
    "observations and @xmath35 .",
    "note that @xmath36 , where @xmath37 is the kronecker product and @xmath38 is the vectorizing operator stacking columns of @xmath39 in a single column vector .",
    "let @xmath40 be constant vectors . the brillinger result states that @xcite : @xmath41 we generalize the brillinger result to the nonzero - mean case . to the best of our knowledge",
    "the following result is new to the literature .",
    "[ lem : bril_nc ] let @xmath42 , where @xmath43 $ ] and @xmath44 $ ] .",
    "then for @xmath45 and constant vectors @xmath46 , we will have @xmath47    see appendix [ app : bril_nc ] .",
    "we first briefly state useful available results .",
    "[ th : girshick ] let @xmath36 .",
    "then the signal eigenvalues of @xmath48 in the asymptotic region of @xmath49 has limiting gaussian distribution and we have @xcite , @xcite @xmath50    where @xmath51 is the kronecker delta function .",
    "now we generalize theorem [ th : girshick ] to the non - central case .",
    "[ theorem ] let @xmath52 .",
    "then asymptotically for the signal eigenvalues of @xmath28 we will have @xmath53    see appendix [ app : thrm ] .",
    "the eigenvalues associated with the noise subspace come from a spherical subspace .",
    "therefore , they are not sufficiently separated , but placed tight together around the noise power @xmath3 . then , the perturbation analysis in appendix [ app : thrm ] is no longer true , since their eigenvectors change dramatically with a small perturbation in @xmath54 .",
    "the distribution of the noise eigenvalues is identical to the noise - only observations in an @xmath55 dimensional noise subspace with a small negative bias introduced by signal eigenvalues @xcite . here ,",
    "we introduce two statistical distributions to show that some noise eigenvalues are considerably larger than @xmath3 .",
    "this invalidates the approximations used in @xcite for calculating @xmath1 . in low snrs , the weakest signal eigenvalue approaches the largest noise eigenvalue but can not pass it due to the ordering of the eigenvalues . in this subsection , we assume @xmath56 .      for sufficiently large @xmath5 and @xmath4 , with @xmath57 and in the null case , the distribution of unordered noise eigenvalues is @xcite @xmath58 where @xmath59 , @xmath60 , as depicted in fig . [",
    "fig : pastur ] . note that @xmath61 is a univariate distribution since it expresses the _ bulk _ distribution @xcite of the eigenvalues , i.e. , in the null case , the eigenvalues of the covariance matrix are @xmath4 independent samples of this distribution .",
    "the largest eigenvalue of a complex correlation matrix in the null case has a bell - shaped distribution called @xmath62 with moments @xcite @xmath63 @xmath64 in which @xmath65 @xmath66 let s see a numerical example",
    ". assume @xmath67 and @xmath68 , then @xmath69 and @xmath70 which implies that @xmath71 with high probability .",
    "we conclude that the signal eigenvalues should be well larger than @xmath3 .",
    "in this subsection , using the statistical tools developed in the previous section , we calculate @xmath1 for mdl method .",
    "@xmath0 is negligible in moderate values of @xmath5 and @xmath4 .",
    "for example , in @xmath72 and @xmath73 , @xmath74 and decays rapidly when @xmath5 and @xmath4 increase .",
    "@xmath1 can be used to estimate the minimum energy level of a source to be detectable by the system .",
    "it can also be used to determine the system capability for resolving very close sources .",
    "then , we concentrate on the @xmath75 and @xmath76 , although our method can be used for the general scenario .",
    "let @xmath77 denote the situation in which only one source is present @xmath78 using and rearranging the terms in we get @xmath79 by the definition of @xmath80 in , we can write @xmath81 similarly , for the geometric mean using we have @xmath82 substituting and in , we get @xcite @xmath83 where @xmath84 and @xmath85 in @xcite , the function @xmath86 is approximated by its second order taylor series near @xmath87 .",
    "this is one source of avoidable error in the method .",
    "the smallest eigenvalue of the signal subspace is greater than the largest eigenvalue of the noise subspace , which is , from subsection [ subsec : stat_noise ] , larger than @xmath3 .",
    "also recall that @xmath88 , we conclude that @xmath89 .",
    "it is evident that the function @xmath90 is uniformly increasing in the region @xmath89 , therefore we can translate the inequality in to a simpler one @xmath91 where @xmath92 using , two steps are required for calculation of @xmath1 , computing @xmath93 from and determining the statistics of @xmath94 in .    unfortunately , can not be solved analytically for @xmath93 , then we find an approximate solution in the first step .",
    "rearrange to get @xmath95 expanding the left - hand - side of to the second order , assuming @xmath4 is sufficiently large and solving the resulting quadratic equation , gives a first approximation for @xmath96 @xmath97 now since the function in l.h.s . of",
    "is smooth , we can use a first order taylor series around the solution in to get closer to the exact solution @xmath98 where @xmath99 depends on @xmath100 through .",
    "application of for a few times gives a very accurate solution .",
    "note that computation of @xmath96 is done after setting @xmath5 and @xmath4 , but is not dependent on the snr .",
    "the next step in calculating @xmath101 is determining the statistics of @xmath102 . from and , we can see that @xmath103 is distributed as @xmath104 in @xcite , the bias term of @xmath103 is not considered , while a numerical example can clarify the point .",
    "assume that @xmath67 , @xmath68 , and @xmath105 .",
    "in the snr in which @xmath101 starts to become large , @xmath106 , @xmath107 , and @xmath108 .",
    "therefore , overlooking the bias term ( @xmath109 ) introduces large error to the analysis . since in the critical snrs ,",
    "the signal eigenvalue get closer to the noise eigenvalues , the denominator in reduces and the bias term gets large .    in the null case , @xmath110 , which recommends that @xmath111 . but a signal eigenvalue can cause a negative bias on @xmath112 , numerically about 2% . then , although we neglect the variance of @xmath112 which is very small compared to the variance of @xmath103 , we should take into account the bias to achieve an exact performance evaluation .",
    "in fact , the variances of the eigenvalues ( regardless of being a noise eigenvalue or a signal one ) increases with the mean of the eigenvalue .",
    "this can be seen in the simulations and can be justified for the noise eigenvalues with noticing the decay of the marcenko - pastur distribution in fig .",
    "[ fig : pastur ] which results in increasing variance of its order statistics .",
    "the variance of any order statistic of a distribution is inversely proportional to the squared value of the distribution in the vicinity of the mean value of that order statistics .",
    "a classical example of this fact is the variance of the median . for the signal eigenvalues ,",
    "this is already shown in and .",
    "this fact , along with the averaging in the calculation of @xmath112 shows that its variance is negligible in the analysis . to calculate the bias , note that @xmath113 .",
    "this besides gives @xcite : @xmath114 using and , the distribution of @xmath102 is determined as a gaussian random variable with known mean @xmath115 and variance @xmath116 .",
    "then , @xmath101 can be calculated as @xmath117 in which @xmath118 the same procedure can be used to calculate @xmath119 . the following approximation is widely used and justified in the literature ( * ? ? ?",
    "* eq . ( 24 ) ) , ( * ? ? ?",
    "( ii.3a ) ) : @xmath120 it basically states that the probability of missing one of the sources is very larger than missing both of them . we drop the details and just give some of the points important in the calculation of @xmath119 : @xmath121 in which the threshold @xmath122 and the function @xmath123 are defined as @xmath124 @xmath125 @xmath126 the recursive equation to estimate the threshold @xmath127 will be @xmath128 the distribution of @xmath129 will be @xmath130 @xmath131 will have a negligible variance and can be estimated by its mean value : @xmath132 now , using and , the distribution of @xmath102 in can be found and @xmath119 is achieved as in .",
    "the same procedure can be used for determining @xmath1 in any number of sources .",
    "although the first- and second - order statistical properties of the signal subspace eigenvalues are different under stochastic and deterministic signal models , the performance of the mdl is the same under two models .",
    "as explained in section [ sec : p_miss_calc ] , @xmath1 depends on the statistics of the weakest signal eigenvalue @xmath133 .",
    "we show that these statistics grow similar under two models when @xmath133 approaches the noise eigenvalues .",
    "note that , for a fair comparison of the two signal models , the signal second - order characteristics should be the same ( see e.g. @xcite ) .",
    "therefore , we have @xmath134 , which results in @xmath135 and hence @xmath136 . in the situations where @xmath1 starts to grow large ,",
    "@xmath133 is barely larger than the noise eigenvalues , @xmath137 , then from we have @xmath138 which is the same as in stochastic signal model .",
    "for the variances , we assume that @xmath139 has approached the upper limit of the noise eigenvalues @xmath140 which is the upper limit of the marcenko - pastur distribution in . note that , as signal power reduces , its eigenvalue approaches the noise eigenvalues roughly about @xmath3 .",
    "but @xmath139 can not be smaller than the largest noise eigenvalue due to the sorting of the eigenvalues . then as the snr reduces , @xmath139 approaches the upper limit of the noise eigenvalues about .",
    "in fact , we are using a better approximation for @xmath139 in calculating the variance in rather than in calculating the expectation in . assuming @xmath141 , a first order expansion of can be used in to give @xmath142 and in to give @xmath143\\end{aligned}\\ ] ] which reduces to the result in and",
    "we can conclude that the variance of @xmath133 is the same under two models in low snrs .",
    "hence , @xmath1 is approximately the same under two signal models .",
    "this is in harmony with the same result in the doa estimation problem , where the performance of the estimators are the same under two signal model @xcite .",
    "in this section , simulation results are presented to support the theoretical derivations .",
    "we consider @xmath1 in different conditions of number of snapshots @xmath5 , and number of sensors @xmath4 in a uniform linear array with half - wavelength inter - element distance .",
    "our estimate is compared with @xcite and @xcite .",
    "results are presented for two closely spaced sources in @xmath119 , and one source in @xmath101 . when the sources get closer to each other , the weaker signal eigenvalue approaches the noise eigenvalues and possibly miss will occur .",
    "therefore , for a fixed angular distance of the sources , a minimum snr is required for the array to be able to detect both sources .",
    "two equally powered uncorrelated signal sources in @xmath144 are assumed .",
    "the snr is defined as the ratio of each signal variance to noise variance ( i.e. sensor snr ) .",
    "figs [ fig : pm2_10_100 ] , [ fig : pm2_10_900 ] , and [ fig : pm2_32_64 ] show the corresponding results for @xmath119 different situations in terms of @xmath5 and @xmath4",
    "[ fig : pm1_32_64 ] presents the results for @xmath101 in the worst case of parameters .",
    "the superiority of our method in estimating the simulation results is evident . in fig .",
    "[ fig : pm2_10_100 ] , simulation results are presented for both deterministic and stochastic signals , which confirms the approximate equality of @xmath1 under two models .",
    "this equality improves as the number of observations @xmath5 increases .",
    "note that our method is used to estimate @xmath1 under stochastic signal model in fig .",
    "[ fig : pm2_10_100 ] .",
    "the analysis in @xcite under - estimates @xmath1 with a horizontal distance of about 0.5 - 2 db .",
    "in fact , this method improves when @xmath5 gets larger since in this situation , the neglected biases reduce .",
    "the estimate of @xcite is better than @xcite , with over - estimation of @xmath1 equivalent with a horizontal distance about 0.5 - 1 db .",
    "note that in the extreme case of @xmath145 and @xmath146 of fig .",
    "[ fig : pm2_32_64 ] , our analysis starts to degrade since the asymptotic assumption is no longer valid .",
    "though , in most cases , our estimate exhibits horizontal distance of about 0.03 db .",
    "we have seen that the analysis in @xcite lacks the inclusion of biases of the eigenvalues and also suffers from some inaccurate approximations .",
    "but the analysis in @xcite requires more scrutiny since as we have seen in the simulation results , this analysis gives completely different results from @xcite .",
    "authors in @xcite use asymptotic conditions to show that @xmath147 converges in distribution to a gaussian random variable with mean @xmath148 and variance @xmath3 .",
    "simulations show that although the formula derived for @xmath3 in @xcite is a very good estimate of the empirical value , the same is not true for the mean @xmath148 , which in fact shows considerable deviation .",
    "this disagreement is present in small @xmath5 as well as large @xmath5 conditions .",
    "the derived result for the mean of the gaussian distribution in ( * ? ? ? * eq . ( 19 ) ) is @xmath149 ^{l - d+1 } \\ , \\bigg ) \\nonumber \\\\ + \\ ; 0.5 \\big ( 2d - 2l - 1 \\big ) \\log ( n)\\end{aligned}\\ ] ] which we can see that is @xmath150 plus some nonrandom term in the notation of our analysis .",
    "now , it is evident that is derived assuming @xmath151 for signal subspace and @xmath152 , thus every biases in the distribution of @xmath153 and @xmath80 is ignored .",
    "additionally , although we can assume the distribution of @xmath102 to be gaussian , it is not easy to assume normality for the function @xmath154 since it is a highly nonlinear function of @xmath102 .",
    "simulations show that the normality assumption is approximately valid only for large values of @xmath5 , say @xmath155 .",
    "another issue is that nonlinearity of the function @xmath156 move the mean of the distribution which is not taken into account .    here",
    ", we will give further simulation results that compare our analysis with the one presented in @xcite .",
    "we assume the same conditions as in ( * ? ? ?",
    "1 ) which is @xmath157 , @xmath158 , and two gaussian sources in @xmath159 $ ] .",
    "the results are shown in fig .",
    "[ fig : fishler1 ] , where the experimental performance of mdl method is accurately predicted by both our method and the method presented in @xcite .",
    "although from a theoretical point of view , the method of @xcite is not comprehensive enough , in this special case of parameters it works well .",
    "if we change the sources doas and keep every other parameters unchanged we will see that the predictions of @xcite degrades .",
    "figure [ fig : fishler2 ] shows the experimental results and theoretical predictions when sources are in @xmath160 $ ] .",
    "it is evident that the method of @xcite does not work well anymore while our method is still accurate .",
    "note that we have investigated its performance when sources are very close to each other in our previous simulation results where the method in [ 8 ] failed to predict the performance accurately .",
    "therefore , the method in [ 8 ] can not be a reliable method of analytical performance calculation .",
    "an accurate performance analysis for the probability of missed detection of the mdl source enumeration method was presented . statistical characterization of the principal components of the covariance matrix helped to take good assumptions and approximation which resulted in improved estimations of @xmath1 .",
    "it is proved that the performance is approximately identical under stochastic and deterministic signal models using a perturbation analysis which gives the statistical properties of eigenvalues in the deterministic signal model .",
    "simulation results show the superiority of the proposed analysis compared with the previous results .",
    "let @xmath161 and rearrange the covariance in ( [ eq : bril_nc ] ) as @xmath162 circularity of the distribution and zero odd moments of zero - mean gaussian distribution reduces ( [ eq : covx ] ) to @xmath163 the first term in ( [ eq : covxsimp ] ) is given by ( [ eq : brill ] ) .",
    "the fact that @xmath164 reduces the second term as @xmath165 the third term in ( [ eq : covxsimp ] ) can be derived in the same way .",
    "note that all the three terms in the right - hand - side of are @xmath166 since @xmath167 is of dimension @xmath168 and hence @xmath169 is @xmath170 .",
    "in the asymptotic region of @xmath171 , @xmath28 is a slightly perturbed version of @xmath54 , described as @xmath172 where @xmath173 is the perturbation factor .",
    "small perturbations in @xmath54 result in small changes in its eigenvectors if the associated eigenvalues are sufficiently separated @xcite .",
    "it means that the following results are true for signal eigenvalues .",
    "remember the definition of the eigendecompositions as @xmath174 and @xmath175 .",
    "the first order perturbation in eigenvectors is @xmath176 where @xmath177s are the perturbation coefficients .",
    "straightforward calculations will give ( * ? ? ?",
    "* eq . ( a.9))@xcite : @xmath178 under the conditions of theorem [ theorem ] , we will have @xmath179 which is shown using and replacing @xmath180 in ( [ eq : bril_nc ] ) .",
    "now , is proved using and .",
    "can be shown using to the first order and .",
    "note that the limiting distribution of the eigenvalues is gaussian @xcite .",
    "m. kaveh , h. wang , and h. hung ,  on the theoretical performance of a class of estimators of the number of narrow - band sources ,  _ ieee trans .",
    "speech , signal process .",
    "assp-35 , pp .",
    "1350 - 1352 , sep . 1987 .",
    "q. zhang , k. m. wong , p. c. yip , and j. p. reilly ,  statistical analysis of the performance of information theoretic criteria in the detection of the number of signals in array processing ,  _ ieee trans . acoustic speech signal process .",
    "1557 - 1567 , oct .",
    "1989 .",
    "e. fishler , m. grossmann , and h. messer ,  detection of signals by information theoretic criteria : general asymptotic performance analysis , ",
    "_ ieee trans . signal process .",
    "50 , pp . 1027 - 1036 , may 2002 .",
    "b. ottersten , m. viberg , and t. kailath ,  analysis of subspace fitting and ml techniques for parameter estimation from sensor array data  , _ ieee trans .",
    "signal process .",
    "590 - 599 , march 1992 .",
    "m. kaveh and a. j. barabell , `` the statistical performance of the music and the minimum - norm algorithms in resolving plane waves in noise , '' _ ieee trans .",
    "speech , signal process .",
    "assp-34 , pp .",
    "331 - 341 , april 1986 .        k. m. wong , q. zhang , j. p. reilly , and p. c. yip ,  on information theoretic criteria for determining the number of signals in high resolution array processing ,  _ ieee trans .",
    "speech , signal process .",
    "1959 - 1971 , nov . 1990"
  ],
  "abstract_text": [
    "<S> in this correspondence , we focus on the performance analysis of the widely - used minimum description length ( mdl ) source enumeration technique in array processing . unfortunately , available theoretical analysis exhibit deviation from the simulation results . </S>",
    "<S> we present an accurate and insightful performance analysis for the probability of missed detection . </S>",
    "<S> we also show that the statistical performance of the mdl is approximately the same under both deterministic and stochastic signal models . </S>",
    "<S> simulation results show the superiority of the proposed analysis over available results </S>",
    "<S> .    minimum description length ( mdl ) , source enumeration , performance analysis , deterministic signal .    </S>",
    "<S> * edics category : sam - perf , sam - sdet * </S>"
  ]
}