{
  "article_text": [
    "the resolution of image sensors has been steadily increasing in the past two decades , more or less following moore s law .",
    "simultaneously , the advent of the cell phone market has placed stringent constraints on the form factor and price of sensors and optical components , creating very strong incentives for camera miniaturization .",
    "these two trends are driving the pursuit of smaller and more densely packed pixels .",
    "today , mainstream cmos sensor technology allows to manufacture pixels in the @xmath0 micron range , and much smaller pixel sizes are feasible .",
    "however , such a miniaturization comes at the price of pixel quality .",
    "small pixels have an inherently limited full - well capacity and consequently have low snr and a poor dynamic range .",
    "this is already a significant obstacle in mainstream cmos sensors showing poor capabilities in high dynamic range ( hdr ) and low light scenes .",
    "furthermore , the resolution of miniature sensors falls below the diffraction limit of existing optics ( for comparison , the airy disk diameter of a lens with @xmath1 at @xmath2 nm is about @xmath3 @xmath4 m ) .    in @xcite ,",
    "eric fossum proposed the novel concept of `` digital film sensor '' composed of sub - micron threshold pixels ( _ jots _ ) producing binary outputs .",
    "a dense array with billions of jots resembles the traditional photographic film in which silver halide crystals of comparable size form light - sensitive grains , or the human eye in which the role of the binary threshold pixels is played by the cones and rods . when a photon hits a grain , it results in the liberation of a silver atom that in the development process  exposes \" the entire grain . detailed probability analysis shows that the density of the exposed grains depends non - linearly on the illumination intensity , effectively compressing high dynamic range into a small interval .",
    "the response of photoreceptor firing rates in our eyes exhibits similar behaviour .",
    "thus , if made sufficiently low noise , dense binary pixels can mimic the high dynamic range of photographic films or animal visual systems .",
    "a proof of this concept has been shown in the epfl gigavision camera @xcite with @xmath5 @xmath4 m binary pixels .",
    "while binary sensors seem to be a promising `` out - of - the - box '' solution for hdr imaging , their output is not directly usable and requires an image reconstruction process .",
    "very low photon count in each individual pixel demands accurate modelling of the underlying poisson statistics , which renders many mainstream image reconstruction and enhancement approaches inapplicable .",
    "moreover , even the recent low light image enhancement algorithms such as @xcite based on careful modelling of the noise are not directly applicable due to the extreme quantization of the binarized signal .    in @xcite , a maximum - likelihood approach to image reconstruction from binary pixels",
    "was proposed . while the algorithm is based on the minimization of a convex objective ,",
    "its computational complexity is prohibitive for any time - critical application .",
    "signal reconstruction from binary measurements has been recently studied in the compressed sensing community @xcite . however , the authors used the standard euclidean data fitting term unsuitable for poisson statistics .",
    "sparse representation techniques have also been applied to the closely related problem of _ inverse halftoning _ , in @xcite it was proposed to co - train a pair of dictionaries , one suited for representing halftoned images , and the other for the corresponding continues image while enforcing the same representation in both dictionaries .",
    "a reconstruction of the continuous valued patch is achieved by plugging the binary patch s representation in its dictionary into the corresponding continuous dictionary .",
    "this approach lacks the physical model of the binarization procedure , which is essential in our case . in @xcite",
    "it was demonstrated that task - specific dictionary learning suited for the non - linear inverse problem yields state - of - the - art inverse halftoning results . while , the use of the euclidean data fitting term and the specific floyd - steinberg @xcite halftoning operator is unsuitable for binary imaging , as we detail in the sequel , the present work follows a similar task - driven learning spirit .",
    "* contributions * in this paper , we consider a variant of a binary thresholded pixel array , for which we formulate the image reconstruction problem as the minimization of a convex objective combining a likelihood term similar to @xcite and a patch - based sparse synthesis prior .",
    "our objective function can be viewed as a variant of the objective used in @xcite for low light image denoising with a different data fitting term correctly capturing the signal formation model , or as a regularised version of the ml estimator from @xcite .",
    "we simulate the binary image acquisition process using multiple exposures of a regular canon dslr cmos sensor and show that our approach outperforms the previously proposed ml estimator in terms of image quality",
    ". however , since the minimization requires an iterative algorithm , the resulting computational complexity is still prohibitive . to address this bottleneck ,",
    "we show how to learn a fixed complexity and latency approximation of the image reconstruction operator inspired by the recent works @xcite .",
    "we show that comparable reconstruction quality is achieved at a fraction of the computational complexity of the iterative algorithm . to the best of our knowledge ,",
    "no efficient signal recovery algorithm exist for non - euclidian fitting terms .",
    "the rest of the paper is organized as follows : in section [ sec_forward ] we describe the acquisition process using a binary sensor , in section [ sec_mlsp ] we formulate the reconstruction model , in section [ sec_fastapprox ] we show a fast approximation to the reconstruction algorithm .",
    "section [ sec_results ] is dedicated to experimental evaluation of our algorithm .",
    "finally , section [ sec_conclusions ] concludes the paper .",
    "we denote by @xmath6 the radiant exposure at the camera aperture measured over a given time interval .",
    "this exposure is subsequently degraded by the optical point spread function denoted by the operator @xmath7 , producing the exposure on the sensor @xmath8 .",
    "the number of photoelectrons @xmath9 generated at pixel @xmath10 in time frame @xmath11 follows the poisson distribution with the rate @xmath12 , @xmath13 a binary pixel compares the accumulated charge against a pre - determined threshold @xmath14 to be fixed in time .",
    "various cmos designs with time - varying pixel threshold are possible . ] , outputting a one - bit measurement @xmath15 ( see figure [ pipeline ] for an illustration ) .",
    "thus , the probability of a single binary pixel @xmath10 to assume an `` off '' value in frame @xmath11 is @xmath16 therefore we can write @xmath17 assuming independent measurements , the negative log - likelihood of the radiant exposure @xmath6 given the measurements @xmath15 is given by @xmath18 in @xcite , a maximum - likelihood approach was proposed for solving [ likelihood ] .",
    "standard optimization techniques were used in  @xcite to solve problem [ likelihood ] . in all our experiments problem",
    "[ likelihood ] was solved on the entire image using the trust - region - reflective algorithm from matlab optimization toolbox .",
    "since this approach assumes no prior , it requires a large number @xmath19 of binary measurements in order to achieve a good reconstruction ( see figure [ multipleexpgraph ] ) .",
    "sparsity priors have been shown to give state - of - the - art results in denoising tasks in general , and particularly in low light settings where poisson noise statistics become dominant @xcite . in the following ,",
    "we show that by introducing a spatial prior over the reconstructed image patches one may significantly reduce the number of measurements required without hampering the reconstruction quality . assuming that the radiant exposure @xmath20 can be expressed by a kernelized sparse representation @xmath21 , where @xmath22 is a dictionary ( e.g. globally trained via k - svd @xcite ) and @xmath23 is an element - wise intensity transformation function",
    ", we may express the estimator as @xmath24 , where @xmath25 and @xmath26 denotes the sparsity encouraging @xmath27 norm of the coefficient vector @xmath28 .",
    "in all the experiments presented in this work , we chose @xmath23 to be the hybrid exponential - linear function introduced in @xcite , @xmath29 that was shown to have several advantages for poisson image reconstruction as it enforces image non negativity constraints , while presenting relatively low lipschitz constants across all intensity levels .",
    "since the negative log - likelihood data fitting term is convex ( see supplementary material for a proof ) with a lipschitz - continuous gradient , problem ( [ problemformulationwithsaprsity ] ) can be solved using proximal algorithms such as the iterative shrinkage thresholding algorithm ( ista ) , its accelerated version @xcite or fista @xcite . for completeness , the fista algorithm is summarized as algorithm [ algo_fista ] , defining @xmath30 as the coordinate - wise shrinking function with threshold @xmath31 and @xmath32 as the step size .",
    "the gradient of the negative log - likelihood computed at each iteration is given by @xmath33    initialize @xmath34 , @xmath35 , @xmath36 + @xmath37    if the lipschitz constant @xmath38 of @xmath39 or an upper bound thereof is easy to compute , backtracking can be replaced by a fixed step size @xmath40 .",
    "the algorithm is reduced to the less efficient ista by fixing @xmath41 .",
    "an iterative solution of ( [ problemformulationwithsaprsity ] ) typically requires hundreds of iterations to converge .",
    "this results in prohibitive complexity and unpredictable input - dependent latency unacceptable in real - time applications . to overcome this limitation ,",
    "we follow the approach advocated by @xcite and @xcite , in which a small number @xmath42 of ista iterations are unrolled into a feed - forward neural network , that subsequently undergoes supervised training on typical inputs for a given cost function @xmath43 .",
    "a single ista iteration can be written in the form @xmath44 where @xmath45 , @xmath46 and @xmath47 .",
    "each such iteration may be viewed as a single layer of the network parameterized by @xmath48 , accepting @xmath49 as input and producing @xmath50 as output .",
    "figure [ netprop ] depicts the network architecture , henceforth referred to as mlnet .     of ista iterations",
    "are unrolled into a feed - forward network .",
    "each layer applies a non - linear transformation to the current iterate @xmath51 , parametrized by @xmath52 and @xmath53 .",
    "training these parameters using standard back propagation on a set of representative inputs allows the network to approximate the output of the underlying iterative algorithm with much lower complexity . ]",
    "network supervised training is done by initializing the parameters as prescribed by the ista iteration and then adapting them using a stochastic gradient approach which minimizes the reconstruction error @xmath54 of the entire network .",
    "we use the following empirical loss @xmath55 which for a large enough training set , @xmath56 , approximates the expected value of @xmath43 with respect to the distribution of the ground truth signals @xmath57 . here",
    ", @xmath58 denotes the output of the network with t layers given the binary images @xmath59 produced from @xmath60 as the input .    similarly to @xcite the output of the network and the derivative of the loss with respect to the network parameters",
    "are calculated using the forward and back propagation , summarized as algorithms [ algo_front_prop ] and [ algo_back_prop ] , respectively .",
    "practice shows that the training process allows to reduce the number of iterations required by about two orders of magnitude while achieving a comparable reconstruction quality ( see figure [ nn_complexity ] ) . to the best of our knowledge , this is the first time a similar strategy is applied to reconstruction problems with a non - euclidean data fitting term .",
    "initialize @xmath61",
    "@xmath62 )    initialize @xmath63,@xmath64,@xmath65 +",
    "this experiment emphasizes the superiority of mlnet over an unregularized ml reconstruction in low light conditions .",
    "furthermore , it illustrates the tradeoff between reconstruction quality and computational complexity .    as a low - resolution ground truth",
    "we used the lena gray - level image normalized to the low light range @xmath66 $ ] .",
    "a high - resolution binary image was created in the following way : first , a photon flux average rate image was created by up - sampling and low - pass filtering the ground truth image using a @xmath67 gaussian filter with a standard deviation of @xmath68 and an up - sampling factor of @xmath69 .",
    "then , an acquisition process was simulated as a realization of the poisson probability distribution of the given average rate image followed by binarization using a fixed @xmath70 threshold pattern consisting of the values @xmath71 distributed uniformly and tiled over the entire image .",
    "an image was reconstructed from the binary measurements using the unregularized ml and using mlnet with several depths @xmath42 operating on @xmath72 overlapping patches .",
    "the function @xmath23 was set according to [ rho ] with c=10 .",
    "mlnet was trained using stochastic gradient decent optimization on a disjoint set of @xmath73 @xmath72 image patches normalized to the same range . in each iteration",
    "the descent direction was calculated on a random minibatch of @xmath74 , low - resolution ground truth patches and their corresponding randomly generated high - resolution binary measurements .",
    "the optimization over the parameters was done via round - robin _",
    "i.e. _ the parameters were optimized sequentially in a circular manner .",
    "a validation set was used to prevent over - fitting .",
    "the loss objective @xmath43 minimized during the training process was the standard squared error , @xmath75    where @xmath57 denote the training ground truth patches .",
    "figure [ lena ] compares the unregularized ml reconstruction to the reconstruction of mlnet with @xmath76 layers without patch overlap .",
    "as can be seen mlnet significantly outperforms the reconstruction obtained by the unregularized ml reaching @xmath77 db psnr compared to only @xmath78 db .    [ cols=\"^,^ \" , ]",
    "in this work we discussed image reconstruction from a set of dense binary measurements , motivated by recent advances in the sensing hardware .",
    "we demonstrated the superiority of the proposed regularized ml reconstruction over `` vanilla ml '' as well as poisson denoising algorithms such as bm3d , showing that the regularized version achieves similar performance as its unregularized counterpart with about three orders of magnitude less measurements .",
    "taking computational complexity into account , we also showed how to compute an efficient and hardware - friendly approximation to the reconstruction algorithm .",
    "promising results were shown on synthetic data as well as on data emulated using multiple exposures of a regular cmos sensor .",
    "it is worthwhile noting that the training loss function ( [ general_loss ] ) allows great freedom of choice , including fully unsupervised or semi - supervised training regimes . while we clearly observed non - negligible effects of such a choice on the reconstruction quality , in this work we opted for the standard mse criterion , deferring the detailed study of the loss functions to future work .",
    "we would like to state that the proposed techniques have the same limitations of approaches based on a universal dictionary as opposed to a prior trained from the data themselves .",
    "they also suffer from known shortcomings of patch - based approaches that average reconstructed overlapping patches : while each of the patches admits the prior , their average might not . in future work , we intend to explore ways of overcoming these limitations ."
  ],
  "abstract_text": [
    "<S> the pursuit of smaller pixel sizes at ever increasing resolution in digital image sensors is mainly driven by the stringent price and form - factor requirements of sensors and optics in the cellular phone market . recently , </S>",
    "<S> eric fossum proposed a novel concept of an image sensor with dense sub - diffraction limit one - bit pixels ( _ jots _ ) @xcite , which can be considered a digital emulation of silver halide photographic film . </S>",
    "<S> this idea has been recently embodied as the epfl gigavision camera . a major bottleneck in the design of such sensors </S>",
    "<S> is the image reconstruction process , producing a continuous high dynamic range image from oversampled binary measurements . </S>",
    "<S> the extreme quantization of the poisson statistics is incompatible with the assumptions of most standard image processing and enhancement frameworks . </S>",
    "<S> the recently proposed maximum - likelihood ( ml ) approach addresses this difficulty , but suffers from image artifacts and has impractically high computational complexity . in this work , </S>",
    "<S> we study a variant of a sensor with binary threshold pixels and propose a reconstruction algorithm combining an ml data fitting term with a sparse synthesis prior . </S>",
    "<S> we also show an efficient hardware - friendly real - time approximation of this inverse operator . </S>",
    "<S> promising results are shown on synthetic data as well as on hdr data emulated using multiple exposures of a regular cmos sensor . </S>"
  ]
}