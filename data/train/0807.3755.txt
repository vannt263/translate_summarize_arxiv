{
  "article_text": [
    "in information retrieval ( ir ) research the term frequency ( tf ) - inverse document frequency ( idf ) concept is well known and established to extract the most significant terms while dismissing the more common terms from textual content .",
    "it also has been used to generate lexical signatures ( lss ) of web pages @xcite .",
    "such lss can be used to ( re-)discover missing web pages when fed back into search engine interfaces .",
    "the computation of tf values for a web page is straight forward since we can simply count the occurrences for each term within the page .",
    "two values are mandatory for the idf computation : the overall amount of documents in the corpus and the amount of documents a term appears in .",
    "we call the second value _ document frequency ( df)_. since both values are unknown when the entire web is the corpus , accurate idf computation for web pages is impossible and values need to be estimated .    various corpora containing web pages , their textual content and their in- and outlinks are available and can be used to estimate idf values since they are considered a representative sample for the internet @xcite .",
    "the trec web track is probably the most common corpus and has , for example , been used in @xcite for idf estimation .",
    "the british national corpus ( bnc ) @xcite , as another example , has been used in @xcite .",
    "google published the n - grams @xcite in @xmath4 and hence provides a powerful alternative source for @xmath0 values of terms extracted from web pages from the google index .",
    "the _ web as corpus kool ynitiative ( wacky ) _ provides the wac corpus as an alternative with no charge for researchers .",
    "the problem with these corpora is that they do not provide @xmath1 values for the terms ( or @xmath5-term tokens ) they contain .",
    "we can count the total number of documents and therefore determine the @xmath1 values in case the corpus documents are provided along with the terms .",
    ".available text corpora characteristics [ cols=\"^,^,^,^,^\",options=\"header \" , ]     [ tab : tc_df_120 ]",
    "figure [ fig : wac_ties_loglog ] shows ( in loglog scale ) the correlation of ranked terms within the wac corpus .",
    "the x - axis represents the @xmath0 ranks of terms and the y - axis the corresponding @xmath1 rank of the same term .",
    "as expected we see the majority of the points within a diagonal corridor which indicates a great similarity between the rankings .",
    "figures [ fig : spear_kend_corr_norm ] and [ fig : spear_kend_corr_log ] show the measured and estimated correlation between @xmath0 and @xmath1 values in the wac dataset .",
    "the solid black line displays spearman s @xmath6 .",
    "the increasing size of the dataset is shown on the x - axis .",
    "the value for @xmath6 at any size of the dataset is beyond @xmath7 which indicates a very strong correlation between the rankings .",
    "the results are statistically significant with a p - value of @xmath8 .",
    "the blue line in both figures shows the computed kendall @xmath9 values for the top @xmath10 ranks and the dotted red line represents the estimated values for the remaining set of data in the wac corpus . since the computed @xmath9 values are hard to read on a normal scale ( figure [ fig : spear_kend_corr_norm ] ) we plotted the same graph in semi - log scale in figure [ fig : spear_kend_corr_log ] .",
    "the computed @xmath9 values vary between @xmath11 and @xmath12 and the estimated values have a minimum of @xmath13 .",
    "we did not compute @xmath9 for greater ranks since it is a very time consuming operation and the estimated values also indicate a strong correlation .",
    "gilpin @xcite provides a table for converting @xmath9 into @xmath6 values .",
    "we use this data to estimate our @xmath9 values .",
    "even though the data in @xcite is based on @xmath9 values computed from a dataset with bivariate normal population ( which we do not believe to have in the wac dataset ) , it supports our measured values .",
    "for example , it shows that a @xmath9 value of @xmath7 can be converted to a @xmath6 of @xmath14 which is consistent with our measured values shown in figure [ fig : spear_kend_corr_norm ] .",
    "therefore we can predict the high @xmath9 values even beyond the top @xmath10 ranks shown in figure [ fig : spear_kend_corr_log ] .",
    "figure [ fig : kendall_time ] shows the measured and predicted computation time ( y - axis , in seconds ) for @xmath9 of top @xmath5 rankings ( x - axis ) .",
    "the black solid line shows the measured time values for rankings up to the top @xmath10 terms .",
    "the red dashed line represents the predicted time values for the entire corpus and ( in the small plot in the left top corner ) for the top @xmath10 ranks .",
    "figure [ fig : kendall_time ] shows the observed complexity of @xmath15 . for the entire wac dataset ( over 11 million unique terms )",
    "we estimate a computation time for kendall @xmath9 of almost @xmath16 million seconds or more than @xmath17 days which is clearly beyond a reasonable computation time for a correlation value .",
    "kendall @xmath9 was computed using an off - the - shelf correlation function as part of the _ _ r - project _ _ , an open source environment for statistical computing .",
    "the software ( version 2.6 ) was run on a dell server with a pentium p4 @xmath18ghz cpu and @xmath19 gb of memory .",
    "another interesting way to show the correlation between @xmath0 and @xmath1 values is simply looking at the ratio of the two values .",
    "figure [ fig : ratio_02 ] shows the distribution of @xmath0/@xmath1 ratios with values rounded after the second decimal and figure [ fig : ratio_01 ] shows the ratios rounded after the first decimal .",
    "it becomes obvious that the vast majority of the ratio values are very small .",
    "the visual impression is supported by the computed mean value of @xmath20 with a standard deviation of @xmath21 for both , figure [ fig : ratio_02 ] and [ fig : ratio_01 ] .",
    "the median of ratios is @xmath22 and @xmath23 respectively .",
    "figure [ fig : ratio_int ] shows the distribution of @xmath0/@xmath1 ratios rounded as integer values .",
    "it is consistent with the pattern of figures [ fig : ratio_02 ] and [ fig : ratio_01 ] and the mean value is equally low at @xmath20 ( @xmath24 ) .",
    "the median here is also @xmath19 .",
    "figure [ fig : ratio ] together with the computed mean and median values accounts for another solid indicator for the strong correlation between @xmath0 and @xmath1 values within the corpus .",
    "the @xmath0 values for both corpora , wac and n - gram , are available and therefore we investigate their correlation .",
    "figure [ fig : tc_freqs ] displays ( in loglog scale ) the frequencies of unique @xmath0 values in both corpora .",
    "the graph shows the @xmath0 threshold of @xmath25 google applied while creating the n - gram . by visual observation",
    "it becomes obvious that the distribution of @xmath0 values in both corpora is very similar .",
    "just the size of the google n - gram corpus is responsible for the offset between the graphs .",
    "we have shown a very strong correlation between the @xmath0 and @xmath1 values within the wac corpus with spearman s @xmath2 ( @xmath26 ) .",
    "this result leads us to the conclusion that the two values can be used interchangeably and therefore @xmath0 values are usable for the generation of accurate idf values .",
    "we also show ( by visual observation ) a high correlation between the @xmath0 values of the wac and of the n - gram datasets .",
    "we can now claim that , despite the fact that the google n - gram dataset does not contain @xmath1 values , the corpus and its @xmath0 values are also usable for accurate idf computation which can lead to the generation of lss of web pages .",
    "we thank the linguistic data consortium , university of pennsylvania and google , inc . for providing the `` web 1 t 5-gram version 1 '' dataset .",
    "we also thank the wacky community for providing the ukwac dataset .",
    "further we would like to thank thorsten brants from google inc .",
    "for promptly answering our emails and helping to clarify questions on the google n - gram corpus .",
    "kazunari sugiyama , kenji hatano , masatoshi yoshikawa , and shunsuke uemura . refinement of tf - idf schemes for web pages using their hyperlinked neighboring pages . in _ proceedings of hypertext 03 _ , pages 198207 , 2003 .",
    "alex franz and thorsten brants .",
    "all our n - gram are belong to you .",
    "http://googleresearch.blogspot.com/2006/08/all - our - n - gram - are - belong - to% -you.html[http://googleresearch.blogspot.com/2006/08/all - our - n - gram - are - belong - to% -you.html ] ."
  ],
  "abstract_text": [
    "<S> for bounded datasets such as the trec web track ( wt10 g ) the computation of term frequency ( tf ) and inverse document frequency ( idf ) is not difficult . </S>",
    "<S> however , when the corpus is the entire web , direct idf calculation is impossible and values must instead be estimated . </S>",
    "<S> most available datasets provide values for _ term count ( tc ) _ meaning the number of times a certain term occurs in the entire corpus . </S>",
    "<S> intuitively this value is different from _ document frequency ( df ) _ , the number of documents ( e.g. , web pages ) a certain term occurs in . </S>",
    "<S> we conduct a comparison study between @xmath0 and @xmath1 values within the web as corpus ( wac ) . </S>",
    "<S> we found a very strong correlation with spearman s @xmath2 ( @xmath3 ) which makes us confident in claiming that for such recently created corpora the @xmath0 and @xmath1 values can be used interchangeably to compute idf values . </S>",
    "<S> these results are useful for the generation of accurate lexical signatures based on the tf - idf scheme .    </S>",
    "<S> \\{mklein , mln}@cs.odu.edu </S>"
  ]
}