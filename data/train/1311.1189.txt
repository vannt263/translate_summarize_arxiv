{
  "article_text": [
    "the use of the hidden markov model ( hmm ) is ubiquitous in a range of sequence analysis applications across a range of scientific and engineering domains , including signal processing @xcite , genomics @xcite and finance @xcite .",
    "fundamentally , the hmm is a mixture model whose mixing distribution is a finite state markov chain @xcite .",
    "whilst the markov assumptions rarely correspond to the true physical generative process , it often adequately captures first - order properties that make it a useful approximating model for sequence data in many instances whilst remaining tractable even for very large datasets . as a consequence",
    ", hmm - based algorithms can give highly competitive performance in many applications .",
    "central to the tractability of hmms is the availability of recursive algorithms that allow fundamental quantities to be computed efficiently @xcite .",
    "these include the viterbi algorithm which computes the most probable hidden state sequence and the forward - backward algorithm which computes the marginal probability of a given state at a point in the sequence .",
    "computation for the hmm has been well - summarized in the comprehensive and widely read tutorial by @xcite with a bayesian treatment given more recently by @xcite .",
    "it is a testament to the completeness of these recursive methods that there have been few generic additions to the hmm toolbox since these were first described in the 1960s .",
    "however , as hmm approaches continue to be applied in increasingly diverse scientific domains and ever larger data sets , there is interest in expanding the generic toolbox available for hmm inference to encompass unmet needs .",
    "the motivation for our work is to develop mechanisms to allow the _ exploration _ of the posterior sequence space .",
    "typically , standard hmm inference limits itself to reporting a few standard quantities . for an @xmath1-state markov chain of length @xmath2 there exists of @xmath3 possible sequences but often only the most probable sequence or the @xmath4 marginal posterior probabilities",
    "are used to summarize the whole posterior distribution .",
    "yet , it is clear that , when the state space is large and/or the sequences long , many other sequences maybe of interest . modifications of the viterbi algorithm can allow arbitrary number of the most probable sequences to be enumerated whilst bayesian techniques allows us to sample sequences from the posterior distribution .",
    "however , since a small change to the most likely sequences typically give new sequences with similar probability , these approaches do not lead to reports of _ qualitatively diverse _ sequences .",
    "by which we mean , alternative sequence predictions that might lead to different decisions or scientific conclusions .    in this article",
    "we describe a set of novel recursive methods for hmm computation that incorporates segmental constraints that we call _",
    "@xmath0-segment inference algorithms_. these are so - called because the algorithms are constrained to consider only sequences involving no more than @xmath5 specified transition events .",
    "we show that @xmath0-segment procedures provide an intuitive approach for posterior exploration of the sequence space allowing diverse sequence predictions containing @xmath6 and @xmath0 segments or specific transitions of interest .",
    "these methods can be applied prospectively during model fitting or retrospectively to an existing model . in the latter case ,",
    "the utility of the methods described here comes at no cost ( other than computational time ) to the hmm user and we provide illustrative examples to highlight novel insights that maybe gained through @xmath0-segment approaches .",
    "the hmm encodes for two types of random sequences : the hidden state sequence or path @xmath7 and the observed data sequence @xmath8 .",
    "individual hidden states take discrete values , such that @xmath9 , while observed variables can be of arbitrary type .",
    "the hidden state sequence @xmath10 follows a markov chain so that @xmath11 here , the first hidden state @xmath12 is drawn from some initial probability vector @xmath13 so that @xmath14 denotes the probability of @xmath12 being in state @xmath15 , whereas any subsequent hidden state @xmath16 ( with @xmath17 ) is drawn according to a transition matrix @xmath18 so that @xmath19_{m ' m } = p(x_n = m|x_{n-1}=m')$ ] expresses the probability of moving to a state @xmath20 from @xmath21 . given a path @xmath10 following the markov chain in ( [ eq : markovchain ] ) , the observed data are generated independently according to @xmath22 where the densities @xmath23 , are often referred to as the emission densities and are parametrized by @xmath24 . next we shall collectively denote all hmm parameters , i.e. @xmath13 , @xmath18 and @xmath24 , by @xmath25 .",
    "= [ shape = circle , inner sep=0pt , line width=0.5 , font= , minimum size=25pt , draw ] ( b1 ) at ( 0,-1.5 ) @xmath12 ; ( b2 ) at ( 2,-1.5 ) @xmath26 ; ( b3 ) at ( 4,-1.5 ) @xmath27 ; ( b4 ) at ( 6,-1.5 ) @xmath28 ; ( b5 ) at ( 8,-1.5 ) @xmath29 ; ( b6 ) at ( 10,-1.5 ) @xmath30 ; ( y1 ) at ( 0,-3 ) @xmath31 ; ( y2 ) at ( 2,-3 ) @xmath32 ; ( y3 ) at ( 4,-3 ) @xmath33 ; ( y4 ) at ( 6,-3 ) @xmath34 ; ( y5 ) at ( 8,-3 ) @xmath35 ; ( y6 ) at ( 10,-3 ) @xmath36 ; ( b1 )  ( b2 ) ; ( b2 )  ( b3 ) ; ( b3 )  ( b4 ) ; ( b5 )  ( b6 ) ; ( b1 )  ( y1 ) ; ( b2 )  ( y2 ) ; ( b3 )  ( y3 ) ; ( b4 )  ( y4 ) ; ( b5 )  ( y5 ) ; ( b6 )  ( y6 ) ; ( 6.75,-1.5 ) circle ( 0.1 mm ) ; ( 7,-1.5 ) circle ( 0.1 mm ) ; ( 7.25,-1.5 ) circle ( 0.1 mm ) ;    statistical estimation in hmms takes advantage of the markov dependence structure , shown in figure [ fig : hmm ] , which allows efficient dynamic programming algorithms to be applied .",
    "for instance , maximum likelihood ( ml ) over the parameters @xmath25 via the em algorithm is carried out by the forward - backward ( f - b ) recursion @xcite that implements the expectation step in @xmath37 time .",
    "a similar recursion having the same time complexity is the viterbi algorithm @xcite which , given a fixed value for the parameters , estimates the maximum _ a posteriori _ ( map ) hidden sequence .",
    "furthermore , straightforward generalizations of the viterbi algorithm estimate the @xmath38-best list of most probable sequences @xcite .",
    "in contrast to ml point estimation , a bayesian approach assigns a prior distribution @xmath39 over the parameters and seeks to estimate expectations taken under the posterior distribution @xmath40 . the bayesian framework also greatly benefits from efficient recursions derived as subroutines of monte carlo algorithms .",
    "specifically , the popular gibbs sampling scheme @xcite relies on the forward - filtering - backward - sampling ( ff - bs ) recursion that simulates in @xmath37 time a hidden sequence from the conditional posterior distribution @xmath41 . in summary , all recursions mentioned above have linear time complexity with respect to the length of the sequence @xmath2 and are instances of more general inference tools developed in the theory of probabilistic graphical models @xcite .",
    "while the linear time efficiency of the current hmm recursions is one of the keys for the widespread adoption of hmms in applications , the information regarding the posterior distribution obtained by these algorithms is still very limited . to this end ,",
    "we define novel probabilistic inference problems for exploration of the hmm posterior distribution and we efficiently solve these problems by introducing linear time recursions . to start with a motivating example ,",
    "assume that we are interested in the event @xmath42 which denotes the number of times a certain class of transitions occurs along the hidden path @xmath10 of the hmm .",
    "then , we may wish to compute the probability : @xmath43 which is a global marginal obtained after a summation over all paths having exactly @xmath0 occurrences from the certain class of transitions .",
    "this probability can not be obtained from the current f - b recursion which allows us to compute only local marginals such as @xmath44 or @xmath45 .",
    "the use of monte carlo methods to approximate the right hand side of ( [ eq : sxequalk ] ) is also unsuitable because , while fast and exact simulation from @xmath41 is possible by means of the ff - bs recursion , the obtained accuracy could be insufficient when the underlying value of @xmath46 is very small due to the extremeness of the event @xmath47 .",
    "we can also define several other related tasks that throughout the paper we collectively refer to as _ @xmath0-segment inference problems_. in such problems , we insert hard constraints into the hmm involving the number and type of segments we want to see along the path @xmath10 , and then we query the model to provide us with probabilities or representative paths characterizing that constraint . some additional representative examples of @xmath0-segment inference that we will study in this article are the computation of the optimal map sequence associated with the event @xmath48 and the simulation of paths from the conditional distribution @xmath49 .    to solve @xmath0-segment inference problems we develop efficient linear time dynamic programming recursions .",
    "the solution we provide introduces auxiliary counting variables into the original hmm so that we obtain an extended state - space hmm which is consistent with the original model .",
    "the auxiliary counting variables used in this augmentation allows us to inject evidence or constraints into the model so that the standard recursions applied to the extended hmm allow us to solve all @xmath0-segment inference problems of interest .",
    "this provides a simple and elegant solution and results in new hmm recursions that generalize the standard f - b , viterbi and ff - bs algorithms .",
    "the remaining of the article has as follows .",
    "section [ sec : theory ] describes the main theory of @xmath0-segment inference , for applications to _ a posteriori _ model exploration , and derives the novel hmm recursions .",
    "section [ sec : statinference ] considers model fitting under @xmath0-segment constraints and presents suitable em and bayesian learning procedures .",
    "section [ sec : extensions ] presents extensions to the basic @xmath0-segment problems , while section [ sec : relatedwork ] discusses related work",
    ". section [ sec : experiments ] considers sequence analysis using two real - world examples from cancer genomics and text information retrieval .",
    "finally , section [ sec : discussion ] concludes with a discussion and directions for future work .",
    "this section presents the theoretical foundations of @xmath0-segment inference problems starting with section [ sec : kseg ] that defines such problems .",
    "section [ sec : counting ] reformulates these problems in terms of an extended state - space hmm having auxiliary counting variables and section [ sec : dynprog ] presents efficient solutions based on linear time recursions .",
    "finally , section [ sec : graphillustr ] gives a graphical illustration of the proposed algorithms using a simulated sequence .",
    "all the algorithms described in this section assume a fixed setting for the parameters @xmath25 . therefore , to keep our expressions uncluttered in the following we drop @xmath25 from our expressions and write for instance @xmath50 as @xmath51 and @xmath52 as @xmath53 .",
    "any hidden path @xmath10 in a hmm can have from @xmath54 up to @xmath55 transitions or equivalently from @xmath56 up to @xmath2 segments , where a segment is defined as a contiguous run of indices where @xmath57 .",
    "following the notation used in eq .",
    "( [ eq : sxtrans1 ] ) , we define the number of all segments in @xmath10 by @xmath58 where @xmath59 denotes the indicator function .",
    "@xmath60 is the sum of the number of transitions , i.e.  the locations in the hidden path where @xmath61 , and the value one that accounts for the initial segment which is not the result of a transition .",
    "subsets of hidden paths associated with different number of segments comprise exclusive events which allow to decompose the posterior distribution @xmath51 as follows .",
    "if we introduce the events @xmath48 , with @xmath62 , each corresponding to the subset of paths @xmath63 having exactly @xmath0 segments , the posterior distribution @xmath51 can be written as the following mixture : @xmath64 where @xmath65 is the posterior distribution conditional on having @xmath0 segments , while @xmath66 is the posterior probability of the event @xmath48 .    the mixture decomposition in eq .",
    "( [ eq : pbfxbfydecomp ] ) suggests that one way to explore the posterior distribution of the hmm is to compute quantities associated with the components of this mixture .",
    "this leads to the @xmath0-segment inference problems which can be divided into the following three types of problems :    * * optimal decoding : * find the map hidden path that has @xmath0 segments , that is the path with the maximum value of @xmath67 . * * probability computation : * find the posterior probability of having @xmath0 segments , i.e.  @xmath68 .",
    "* * path sampling : * draw independent samples from @xmath67 .    to this end , in section [ sec : dynprog ] we introduce efficient linear time algorithms to solve all the above tasks together with several additional related tasks associated with more general events of the form @xmath69 , where @xmath70 , such as finding the map of @xmath71 , sampling from @xmath71 and etc .",
    "these algorithms are based on a reformulation of the above @xmath0-segment inference problems that uses an extended state - space hmm containing auxiliary counting variables .",
    "the basis of our algorithm is the augmentation of the markov chain in ( [ eq : markovchain ] ) with auxiliary variables that count the number of segments .",
    "specifically , @xmath60 from ( [ eq : sxtrans ] ) can be considered as a counter that scans the path @xmath10 and it increments by one any time it encounters a transition .",
    "we can represent this counting process with a @xmath2-dimensional vector of auxiliary variables @xmath72 which is an increasingly monotone sequence of non - negative integers .",
    "conditioning on a certain path @xmath10 , @xmath72 is sampled deterministically according to the markov chain @xmath73 ,      \\label{eq : priors}\\end{aligned}\\ ] ] where @xmath74 is the delta mass that equals one when @xmath75 and zero otherwise .",
    "we refer to the above conditional distribution as the _ counting markov chain _ or counting chain because it is markov chain that makes precise the concept of counting the segments .",
    "the counting chain starts at one , i.e.  @xmath76 ( which can be interpreted as sampling from the delta mass @xmath77 ) , and then it increments by one so that @xmath78 every time a transition occurs in the hidden path , i.e.  whenever @xmath61 which implies the generation of a new segment .",
    "the joint density of the hmm is augmented with the counting chain so that @xmath79 is the new joint density having the conditional independence structure shown as directed graphical model in figure [ fig : expandedhmm ] .",
    "because this augmentation is consistent we recover correctly the joint density of the initial hmm .",
    "] , prior - to - posterior inference in the initial hmm and the hmm augmented with auxiliary variables are in theory equivalent . however , in practice , inference in the latter model is more flexible since it allows to solve the @xmath0-segment inference problems through the insertion of constraints in the counting process .",
    "more precisely , the final value of the counter @xmath80 equals @xmath60 so the event @xmath48 can be realized by adding the evidence @xmath81 in the graphical model of figure [ fig : expandedhmm ] .",
    "therefore , all type of @xmath0-segment inference problems can be reformulated as follows :    * * optimal decoding : * the map hidden @xmath82 of @xmath67 can be found according to @xmath83 * * probability computation : * the posterior probability @xmath68 can be expressed as @xmath84 where @xmath53 is known from the forward pass of the standard f - b algorithm and @xmath85 * * path sampling : * an independent sample @xmath86 from @xmath67 is obtained as @xmath87    where in the above @xmath88 denotes all counting variables apart from the final @xmath80 which is clamped to @xmath0 . for more general events of the form @xmath89 , where @xmath70 , the above still holds with the slight modification that we will need additionally to maximize , marginalize or sample @xmath80 , respectively for the three cases above , under the constraint @xmath89 .",
    "simple proofs for the correctness of all above statements can be found in the appendix [ app : proofs ] .    furthermore , the @xmath0-segment inference problems associated with the special case of the event @xmath90 can be equivalently reformulated by using a modified counting chain that absorbs when @xmath91 , i.e. @xmath92 ,      \\label{eq : priorsabsorb}\\ ] ] where the indicator function @xmath93 is one only when both @xmath94 and @xmath95 are true .",
    "notice that the above is an inhomogeneous chain having two modes : the first when the segment counting proceeds normally and the second when counting stops once the absorbing state is visited .",
    "the @xmath0-segment problems for the event @xmath90 are then solved by using the above chain and clamping @xmath80 to the value @xmath96 .",
    "the augmentation with counting variables results in a new hmm having the pair @xmath97 as the new extended state variable .",
    "given that @xmath98 , so that any pair @xmath97 can jointly take at most @xmath99 values , we can use the viterbi algorithm to obtain the map of @xmath100 , the forward pass of the f - b algorithm to obtain @xmath101 and the ff - bs algorithm to draw an independent sample from @xmath100 .",
    "a naive implementation of these algorithms can be done in @xmath102 time . however",
    ", this complexity can be further reduced to @xmath103 by taking into account the deterministic structure of the counting chain as discussed in the following section .",
    "= [ shape = circle , inner sep=0pt , line width=0.5 , font= , minimum size=25pt , draw ] ( a1 ) at ( 0,0 ) @xmath104 ; ( a2 ) at ( 2,0 ) @xmath105 ; ( a3 ) at ( 4,0 ) @xmath106 ; ( a4 ) at ( 6,0 ) @xmath107 ; ( a5 ) at ( 8,0 ) @xmath108 ; ( a6 ) at ( 10,0 ) @xmath80 ; ( b1 ) at ( 0,-1.5 ) @xmath12 ; ( b2 ) at ( 2,-1.5 ) @xmath26 ; ( b3 ) at ( 4,-1.5 ) @xmath27 ; ( b4 ) at ( 6,-1.5 ) @xmath28 ; ( b5 ) at ( 8,-1.5 ) @xmath29 ; ( b6 ) at ( 10,-1.5 ) @xmath30 ; ( y1 ) at ( 0,-3 ) @xmath31 ; ( y2 ) at ( 2,-3 ) @xmath32 ; ( y3 ) at ( 4,-3 ) @xmath33 ; ( y4 ) at ( 6,-3 ) @xmath34 ; ( y5 ) at ( 8,-3 ) @xmath35 ; ( y6 ) at ( 10,-3 ) @xmath36 ; ( a1 )  ( a2 ) ; ( a2 )  ( a3 ) ; ( a3 )  ( a4 ) ; ( a5 )  ( a6 ) ; ( b1 )  ( a1 ) ; ( b2 )  ( a2 ) ; ( b3 )  ( a3 ) ; ( b4 )  ( a4 ) ; ( b5 )  ( a5 ) ; ( b6 )  ( a6 ) ; ( b1 ) ",
    "( b2 ) ; ( b2 ) ",
    "( b3 ) ; ( b3 )  ( b4 ) ; ( b5 )  ( b6 ) ; ( b1 )  ( a2 ) ; ( b2 )  ( a3 ) ; ( b3 )  ( a4 ) ; ( b5 ) ",
    "( a6 ) ; ( b1 )  ( y1 ) ; ( b2 )  ( y2 ) ; ( b3 )  ( y3 ) ; ( b4 )  ( y4 ) ; ( b5 )  ( y5 ) ; ( b6 )  ( y6 ) ; ( 6.75,0 ) circle ( 0.1 mm ) ; ( 7,0 ) circle ( 0.1 mm ) ; ( 7.25,0 ) circle ( 0.1 mm ) ; ( 6.75,-1.5 ) circle ( 0.1 mm ) ; ( 7,-1.5 ) circle ( 0.1 mm ) ; ( 7.25,-1.5 ) circle ( 0.1 mm ) ;      * optimal decoding .",
    "* we first describe the @xmath0-segment equivalent of the viterbi algorithm for the optimal decoding problem under @xmath0-segment constraints , i.e.  for obtaining the map of @xmath100 .",
    "this algorithm will be able to solve at once all such problems from @xmath109 up to a maximum @xmath110 by applying a single forward pass for the maximum value @xmath111 which requires @xmath112 operations . then , by applying @xmath111 backtracking operations , each scaling as @xmath113 , we can obtain all @xmath111 optimal segmentations overall in @xmath112 time .    more precisely , the viterbi algorithm applies a forward pass where recursively @xmath114 is maximized with respect to the pair @xmath115 for any value of the next pair @xmath97 .",
    "this can be implemented as a propagation of a message , which is a @xmath116 dimensional vector , as follows .",
    "the message is initialized to @xmath117 which equals @xmath118 when @xmath76 and @xmath119 when @xmath120 .",
    "this message then is propagated recursively according to @xmath121 .",
    "\\label{eq : gammamessage}\\ ] ] @xmath122 where the auxiliary message @xmath123 simply stores the pair @xmath124 that gives the maximum in ( [ eq : gammamessage ] ) needed later in backtracking .",
    "naively , the @xmath125th recursive update can be implemented in @xmath126 time since each @xmath127 takes at most @xmath111 values and each @xmath16 takes @xmath1 values . however , for any given configuration of @xmath97 ( out of the @xmath128 possible ) , the permissible values for @xmath129 are either @xmath130 when @xmath57 or @xmath131 when @xmath61 . for all remaining configurations , @xmath132 , so",
    "that these configurations need not to be checked when maximizing over @xmath115 for a certain pair @xmath97 .",
    "thus , the maximization in ( [ eq : gammamessage ] ) can be done in @xmath1 operations resulting in @xmath133 operations for the whole @xmath125th update .",
    "subsequently , the full forward pass requires @xmath112 operations .",
    "once the forward pass is completed , we have the final message @xmath134 ( together with all auxiliary @xmath135 messages ) from which we can obtain all @xmath111 optimal segmentations using backtracking as follows . for @xmath136 ,",
    "we first compute @xmath137 then , starting from @xmath138 we backtrack recursively according to @xmath139 that recovers the optimal hidden path @xmath82 having exactly @xmath0 segments",
    ". each backtracking requires @xmath113 simple indexing operations",
    ".    * probability computation .",
    "* for the probability computation problem , we work similarly to the above viterbi algorithm and we compute all joint densities @xmath140 for @xmath109 up to @xmath111 using the forward pass of the f - b algorithm applied to the augmented hmm .",
    "this recursively sums out each pair @xmath115 for any value of the next pair @xmath97 , essentially passing through the so - called @xmath141 message @xcite .",
    "this message is a @xmath128 dimensional vector taking as initial value @xmath142 which equals @xmath143 when @xmath76 and @xmath54 otherwise .",
    "then , the message is propagated according to the standard @xmath141 recursion @xmath144 this recursion scales as @xmath145 since the summation over @xmath146 can be done in @xmath147 time by taking advantage the structure of the counting conditional @xmath148 .",
    "as in any @xmath141 recursion in a hmm , @xmath149 equals the density @xmath150 so that the final message is @xmath151 , from which we can easily obtain @xmath152 for @xmath136 . clearly , since the computation of a single recursion of the @xmath141 message takes @xmath145 time , the above computations require overall @xmath153 time . given that the joint density @xmath140 has been obtained , we can compute exactly the posterior probability @xmath154 by dividing with the normalization constant @xmath53 ( i.e.  the overall likelihood of the hmm ) obtained from the standard forward pass .    similarly to the above we can also define the so called backwards or @xmath155 message in the extended state - space hmm .",
    "such message is useful when applying the em algorithm for learning an hmm under @xmath0-segments constraints and its computation will be described in section [ eq : em ]",
    ".    * path sampling .",
    "* we now turn into the sampling problem where we wish to draw a path from the conditional @xmath156 .",
    "such a path can be obtained by sampling a pair @xmath157 from @xmath158 and then discarding @xmath88 .",
    "we apply the ff - bs algorithm that is based on the following decomposition @xmath159 where the index @xmath125 in @xmath160 starts from @xmath55 and decrements down to one . applying first the forward pass described above we have the final message @xmath161 from which we can sample @xmath30 from @xmath162 .",
    "then , recursively we go backwards and each time we sample @xmath163 , given the already sampled value of @xmath164 , from @xmath165 where the message @xmath166 is known from the forward pass .",
    "each sampling step takes @xmath147 time ( again due to the deterministic nature of the conditional @xmath167 and the whole backward sampling requires @xmath168 time .",
    "if we wish to simultaneously sample from all conditional distributions @xmath156 , with @xmath136 , we can do this using a single forward pass that scales as @xmath112 and @xmath111 backward sampling iterations scaling as @xmath169 , so the overall complexity is @xmath112 .",
    "furthermore , very simple and straightforward modifications of the above procedures can deal with the more general constraint @xmath89 , where @xmath70 .",
    "for instance , if we wish to sample a path from @xmath170 , we need to first apply the forward pass for @xmath171 and then perform backwards sampling exactly as described above with the only difference that initially we sample @xmath172 from @xmath173 . similarly , the @xmath0-segment inference problems associated with the special event @xmath90 can be efficiently solved in @xmath174 time by using the absorbing counting chain .",
    "however , such a solution is very inefficient as it scales as @xmath175 since @xmath111 must be chosen to be equal to @xmath2 . ] from ( [ eq : priorsabsorb ] ) and then applying exactly the above algorithms by clamping @xmath176 .    finally , it is important to notice that running @xmath0-segment inference up to some @xmath111 and setting @xmath177 as the absorbing state always gives a global summary of the posterior distribution that is guaranteed to be at least as informative as the standard viterbi map path .",
    "more precisely , the events @xmath178 and @xmath179 comprise exclusive events that make up the whole set of paths for any value of @xmath111 .",
    "therefore , the probabilities @xmath180 and @xmath181 , computed based on the forward pass in the augmented hmm , always sum up to one , while the set of the corresponding @xmath177 optimal paths must include the standard viterbi map path , which will be either one of the paths from @xmath56 up to @xmath111 or the path with more segments than @xmath111 .",
    "we refer to the above combined sets of probabilities and optimal paths as the @xmath177 summary of the posterior distribution .      here , we give a graphical illustration of optimal decoding and path sampling under @xmath0-segment constraints .",
    "for this , we simulated a data sequence according to @xmath182 , where the hidden sequence @xmath183 was given by a markov chain with @xmath184 states , @xmath185 and @xmath186 .    using the simulated data , shown in the first row of figure [ fig : illustrativeplots ] , we fitted a three - state hmm using the em algorithm which recovered parameter estimates very close to the ground - truth ones .",
    "we then computed the standard viterbi path and obtained the optimal segmentations , associated with the @xmath177 summary , using the @xmath0-segment equivalent of the viterbi algorithm with @xmath187 .",
    "these are shown in the second row of figure [ fig : illustrativeplots ] .",
    "each such path is displayed so that the three states are shown with different color . on top , the viterbi path is displayed , containing @xmath188 segments , and then the @xmath189 paths of the @xmath177 summary .",
    "the first @xmath190 paths of the latter summary provide a coarse - to - fine hierarchical segmentation of the data sequence where the number of segments increase by one each time .",
    "notice that two consecutive segmentations , do not always follow the principle used in circular binary segmentation algorithm @xcite , i.e.  the @xmath96th segmentation might not be obtained by splitting into two segments a single segment from the @xmath0th one .",
    "such a latter approach is sub - optimal .",
    "also , notice that the final path that corresponds to the absorbing state ( labelled with @xmath191 in the figure ) is precisely the standard viterbi path .",
    "the third panel of figure [ fig : illustrativeplots ] illustrates path sampling under @xmath0-segment constraints using the ff - bs algorithm in the augmented hmm .",
    "in particular , @xmath190 samples are shown that are constrained to have exactly @xmath192 segments .",
    "we remark that the application of our @xmath0-segment algorithms , so far , has been applied entirely retrospectively to an hmm fitted using a very standard and common approach in a simple but generic model set - up .",
    "the @xmath0-segment constraints are not involved in the model fitting process but are applied retrospectively to provide a rich exploration of the posterior sequence space where qualitatively diverse segmentations are reported . for the expenditure of some computational time ,",
    "the application of @xmath0-segment generalizations for optimal decoding , probability computation and path sampling provides the hmm user with alternative summaries .    [ cols=\"^ \" , ]",
    "hmms can allow for highly efficient analysis of large quantities of sequence data .",
    "however , existing methods for reporting of posterior summaries from hmms such as the viterbi map path and the marginal probabilities are rather blunt . here",
    ", we demonstrated how the use of auxiliary counting variables allows for computationally efficient exploration of the model fit using @xmath0-segment algorithms .",
    "it is important to note that the techniques we developed are generic and the augmentation scheme can be applied either _ a posteriori _ to hmms already fitted to data or _ a priori _ during model fit . in cancer genomics",
    ", @xmath0-segment inference can be an useful exploratory tool that can help researchers to analyze genomic sequences in different resolutions or target events of particular types , facilitating thus the process of getting novel insight into structural rearrangements in cancer genomes . for other type of applications , that appear for instance in machine learning and pattern recognition , the proposed methods can allow to build more flexible hmm - based classification and decision making systems , as we have demonstrated using the text retrieval example .",
    "regarding future work , an interesting research direction is to exploit the ability of @xmath0-segment inference to efficiently explore the hmm posterior distribution in order to provide input into constructing meta statistical models .",
    "for instance , the ability to obtain alternative explanations of the same data sequence that may have high utility to the research scientist but occurs with very low probability . could allow the practitioner to re - rank different explanations based on his expertise and subsequently provide feedback into the model that can be used for supervised re - training .",
    "a second future direction is to exploit the fact that training an hmm under a @xmath0-segment constraint often gives sparse transition matrices .",
    "this is not surprising , since a bound on the number of segments essentially limits the number of transitions along the hidden path which subsequently can result in many inferred zeros in the transition matrix",
    ". such sparsity property could be useful when we would like to perform _ state selection _ in large state - space hmms .    to conclude , as data sets become larger and models more complex we expect to see increasing need for computationally efficient methods for posterior model exploration and statistical inference under constraints .",
    "in this paper , we have presented one such approach that significantly expands the statistical algorithmic toolbox of hmms .",
    "mt and ch were supported by a wellcome trust healthcare innovation challenge fund award ( ref no .",
    "hicf-1009 - 026 ) and the lincoln college michael zilkha fund .",
    "mt was also supported from `` research funding at aueb for excellence and extroversion , action 1 : 2012 - 2014 '' .",
    "cy was funded by a uk medical research council specialist training fellowship in biomedical informatics ( ref no .",
    "g0701810 ) and a new investigator research grant ( ref no .",
    "mr / l001411/1 ) .",
    "andrews , m. and vigliocco , g. ( 2010 ) . the hidden markov topic model : a probabilistic model of semantic representation .",
    ", 2:101113 .",
    "auger , i.  e. and lawrence , c.  e. ( 1989 ) .",
    "algorithms for the optimal identification of segment neighborhoods . , 51(1):3954 .",
    "baum , l.  e. and petrie , t. ( 1966 ) . .",
    ", 37(6):15541563 .",
    "beroukhim , r. , mermel , c.  h. , porter , d. , wei , g. , raychaudhuri , s. , donovan , j. , barretina , j. , boehm , j.  s. , dobson , j. , urashima , m. , henry , k. t.  m. , pinchback , r.  m. , ligon , a.  h. , cho , y .-",
    "j . , haery , l. , greulich , h. , reich , m. , winckler , w. , lawrence , m.  s. , weir , b.  a. , tanaka , k.  e. , chiang , d.  y. , bass , a.  j. , loo , a. , hoffman , c. , prensner , j. , liefeld , t. , gao , q. , yecies , d. , signoretti , s. , maher , e. , kaye , f.  j. , sasaki , h. , tepper , j.  e. , fletcher , j.  a. , tabernero , j. , baselga , j. , tsao , m .- s . ,",
    "demichelis , f. , rubin , m.  a. , janne , p.  a. , daly , m.  j. , nucera , c. , levine , r.  l. , ebert , b.  l. , gabriel , s. , rustgi , a.  k. , antonescu , c.  r. , ladanyi , m. , letai , a. , garraway , l.  a. , loda , m. , beer , d.  g. , true , l.  d. , okamoto , a. , pomeroy , s.  l. , singer , s. , golub , t.  r. , lander , e.  s. , getz , g. , sellers , w.  r. , and meyerson , m. ( 2010 ) .",
    "the landscape of somatic copy - number alteration across human cancers .",
    ", 463(7283):899905 .",
    "bishop , c.  m. ( 2006 ) . .",
    "springer - verlag new york , inc . ,",
    "secaucus , nj , usa .",
    "blei , d.  m. , ng , a.  y. , and jordan , m.  i. ( 2003 ) .",
    "latent dirichlet allocation .",
    ", 3:9931022 .",
    "bttcher , s. , clarke , c. l.  a. , and cormack , g.  v. ( 2010 ) . .",
    "mit press .",
    "capp , o. , moulines , e. , and ryden , t. ( 2005 ) . .",
    "springer - verlag new york , inc . ,",
    "secaucus , nj , usa .",
    "carter , s.  l. , cibulskis , k. , helman , e. , mckenna , a. , shen , h. , zack , t. , laird , p.  w. , onofrio , r.  c. , winckler , w. , weir , b.  a. , beroukhim , r. , pellman , d. , levine , d.  a. , lander , e.  s. , meyerson , m. , and getz , g. ( 2012 ) .",
    "absolute quantification of somatic dna alterations in human cancer . , 30(5):413421 .",
    "chen , h. , xing , h. , and zhang , n.  r. ( 2011 ) .",
    "estimation of parent specific dna copy number in tumors using high - density genotyping arrays . , 7(1):e1001060 .",
    "cowell , r.  g. , dawid , p.  a. , lauritzen , s.  l. , and spiegelhalter , d.  j. ( 2003 ) . .",
    "springer , new york .",
    "crouse , m.  s. , nowak , r.  d. , and baraniuk , r.  g. ( 1998 ) .",
    "wavelet - based statistical signal processing using hidden markov models .",
    ", 46(4):886902 .",
    "eddy , s.  r. ( 1998 ) . profile hidden markov models .",
    ", 14(9):755763 .",
    "everingham , m. , gool , l. , williams , c.  k. , winn , j. , and zisserman , a. ( 2010 ) .",
    "the pascal visual object classes ( voc ) challenge . , 88(2):303338 .",
    "fearnhead , p. ( 2006 ) . .",
    ", 16(2):203213 .",
    "fearnhead , p. and liu , z. ( 2007 ) .",
    "nline inference for multiple changepoint problems .",
    ", 69:589605 .",
    "gruber , a. , weiss , y. , and rosen - zvi , m. ( 2007 ) .",
    "hidden topic markov models . , 2:163170 .",
    "hofmann , t. ( 2001 ) .",
    "unsupervised learning by probabilistic latent semantic analysis .",
    ", 42(1/2):177196 .",
    "juang , b.  h. and rabiner , l.  r. ( 1991 ) .",
    "hidden markov models for speech recognition . , 33(3):251272 .",
    "killick , r. , fearnhead , p. , and eckley , i.  a. ( 2012 ) . .",
    ", 107(500):15901598 .",
    "kohlmorgen , j. ( 2003 ) . on optimal segmentation of sequential data . in _",
    "neural networks for signal processing , 2003 . nnsp03 .",
    "2003 ieee 13th workshop on _ , pages 449458 .",
    "koller , d. and friedman , n. ( 2009 ) . .",
    "mit press .",
    "li , n. and stephens , m. ( 2003 ) . modeling linkage disequilibrium and identifying recombination hotspots using single - nucleotide polymorphism data .",
    ", 165(4):22132233 .",
    "loo , p.  v. , nordgard , s.  h. , lingjrde , o.  c. , russnes , h.  g. , rye , i.  h. , sun , w. , weigman , v.  j. , marynen , p. , zetterberg , a. , naume , b. , perou , c.  m. , brresen - dale , a .-",
    "l . , and kristensen , v.  n. ( 2010 ) .",
    "allele - specific copy number analysis of tumors .",
    ", 107(39):1691016915 .",
    "mitchell , c.  d. , harper , m.  p. , and jamieson , l.  h. ( 1995 ) . on the complexity of explicit duration hmm s .",
    ", 3(3):213217 .",
    "murphy , k. ( 2002 ) .",
    "hidden semi - markov models ( hsmms ) .",
    "technical report , university of california  berkley .",
    "nilsson , d. and goldberger , j. ( 2001 ) .",
    "sequentially finding the n - best list in hidden markov models . in _ proceedings of he",
    "seventeenth international joint conference on artificial intelligence ( ijcai ) 2001_.    olshen , a.  b. , venkatraman , e.  s. , lucito , r. , and wigler , m. ( 2004 ) .",
    ", 5(4):557572 .",
    "paas , l.  j. , vermunt , j.  k. , and bijmolt , t.  h. ( 2007 ) .",
    "discrete time , discrete state latent markov modelling for assessing and predicting household acquisitions of financial products .",
    ", 170(4):955974 .",
    "rabiner , l.  r. ( 1989 ) .",
    "tutorial on hidden markov models and selected applications in speech recognition . in _ proceedings of the ieee _ ,",
    "volume  77 , pages 257286 .",
    "schwartz , r. and chow , y.  l. ( 1990 ) . .",
    ", 1:8184 .",
    "scott , s.  l. ( 2002 ) .",
    "bayesian methods for hidden markov models : recursive computing in the 21st century . , 97:337351 .",
    "viterbi , a.  j. ( 1967 ) .",
    "error bounds for convolutional codes and an asymptotically optimum decoding algorithm .",
    ", it-13(2):260269 .",
    "yau , c. ( 2013 ) .",
    "oncosnp - seq : a statistical approach for the identification of somatic copy number alterations from next - generation sequencing of cancer genomes .",
    ", 29(19):24822484 .",
    "yau , c. and holmes , c. ( 2013 ) . a decision theoretic approach for segmental classification using hidden markov models . .",
    "in press .    yau ,",
    "c. , mouradov , d. , jorissen , r.  n. , colella , s. , mirza , g. , steers , g. , harris , a. , ragoussis , j. , sieber , o. , and holmes , c.  c. ( 2010 ) . a statistical approach for detecting genomic aberrations in heterogeneous tumor samples from single nucleotide polymorphism genotyping data .",
    ", 11(9):r92 .",
    "yu , s .- z .",
    "hidden semi - markov models . , 174(2):215243 .",
    "here , we provide proofs for the correctness of the reformulation of the three @xmath0-segment inference problems presented in section [ sec : counting ] .",
    "firstly , we will show that @xmath193 , computed via the augmented hmm , is equal to @xmath194 given by eq .",
    "( [ eq : pxeky ] ) .",
    "we have that @xmath193 is defined by @xmath195 what we need to show is that @xmath196 is equal to the indicator function @xmath197 .",
    "since @xmath198 is a deterministic distribution , given that @xmath10 has @xmath0 segments there will be an unique @xmath199 such that @xmath200 and zero for all remaining @xmath88s .",
    "if @xmath10 does not contain @xmath0 segments , @xmath201 for any @xmath88 .",
    "thus , when @xmath10 has @xmath0 segments @xmath202 , otherwise @xmath203 .",
    "therefore , @xmath204 for any @xmath10 , from which we conclude that @xmath193 reduces to the definition of @xmath205 from eq .",
    "( [ eq : pxeky ] ) . from that",
    ", we can immediately obtain that the term that normalizes the right hand side of ( [ eq : pxysnk ] ) , i.e.  the quantity @xmath140 , is equal to @xmath206 .",
    "this completes the proof regarding the correctness of the probability computation .",
    "based on the above , we can also conclude that the initial optimal decoding solution @xmath82 is the map of @xmath193 , i.e.   @xmath207.\\ ] ] given now that @xmath208 is a deterministic distribution the sum operation can be replaced by a max operation so that @xmath209,\\ ] ] or @xmath210 , \\label{eq : maxgivensnk}\\ ] ] which shows that the reformulated optimal decoding problem is equivalent to the initial one .",
    "finally , regarding path sampling , the ff - bs in the augmented hmm gives a pair of paths @xmath211 that jointly comprise an independent sample from @xmath158 .",
    "thus , @xmath86 alone is an independent sample from @xmath156 .",
    "for the classification task we created the ground - truth dataset as follows . for each test document sequence @xmath212 we decided with probability @xmath213 to insert a number of @xmath214 ( with @xmath215 ) segments from the subject economics so that these segments had random lengths from @xmath216 $ ] and were also placed in random locations within the sequence @xmath212 , replacing thus the original text and with the only constraint that they did nt overlap with each other .",
    "each such set of @xmath214 artificially inserted segments were also randomly selected from the @xmath190 theses in economics by first picking a thesis and then selecting @xmath214 non - overlapping segments within that thesis text sequence .",
    "the whole procedure created a new dataset of @xmath217 documents so that a subset of them contained segments from the relevant topic and the remaining ones did not .    for the detection task we worked similarly with the classification task discussed earlier .",
    "particularly , again we randomly perturb the @xmath217 test documents and insert a number of @xmath218 segments from the subject economics in each of the them .",
    "the insertion of segments was done exactly as described above with the only difference being that now we insert segments in all documents and their number can be much larger since @xmath218 ."
  ],
  "abstract_text": [
    "<S> hidden markov models ( hmms ) are one of the most widely used statistical methods for analyzing sequence data . </S>",
    "<S> however , the reporting of output from hmms has largely been restricted to the presentation of the most - probable ( map ) hidden state sequence , found via the viterbi algorithm , or the sequence of most probable marginals using the forward - backward ( f - b ) algorithm . in this article , we expand the amount of information we could obtain from the posterior distribution of an hmm by introducing linear - time dynamic programming algorithms that , we collectively call @xmath0-segment algorithms , that allow us to i ) find map sequences , ii ) compute posterior probabilities and iii ) simulate sample paths conditional on a user specified number of segments , i.e. contiguous runs in a hidden state , possibly of a particular type . </S>",
    "<S> we illustrate the utility of these methods using simulated and real examples and highlight the application of prospective and retrospective use of these methods for fitting hmms or exploring existing model fits . </S>"
  ]
}