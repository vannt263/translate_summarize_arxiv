{
  "article_text": [
    "policy gradient algorithms have garnered a lot of popularity in the area of reinforcement learning , since they directly optimize the cumulative reward function , and can be used in a straightforward manner with non - linear function approximators such as deep neural networks @xcite .",
    "they have been successfully applied towards solving continuous control tasks @xcite and learning to play atari from raw pixels @xcite .",
    "traditional policy gradient algorithms such as reinforce @xcite and vanilla policy gradient @xcite operate by repeatedly computing estimates of the gradient of the expected reward of a policy , and then updating the policy in the direction of the gradient .",
    "while these algorithms provide an unbiased policy gradient estimate , the variance of the estimates is often quite high , which can severely degrade the algorithm s performance .",
    "several classes of algorithms have been proposed towards reducing the variance of the policy gradient , namely actor - critic methods @xcite , using the discount factor @xcite , and generalized advantage estimation @xcite .",
    "the variance of the policy gradient can be further reduced ( whilst adding no bias ) by adding a baseline @xcite , which is commonly implemented as an estimator of the state - value function .",
    "typically , policy gradient algorithms operate in an iterative fashion : in each iteration , they use predictions from the baseline that is fitted in the previous iteration . if the policy changes drastically between iterations , the baseline becomes a poor estimate of the state - value function , resulting in _",
    "underfitting_. alternatively , we could fit the baseline and use it to predict the value function in the same iteration .",
    "this approach suffers from the _ overfitting _ problem . in the extreme case ,",
    "if we only have one trajectory starting from each state , and the baseline could fit the data perfectly , then the baseline would perfectly predict the returns giving us no gradient signal at all .    in this paper",
    ", we propose a new method ( which we name the @xmath0-fold method ) for baseline estimation in policy gradient algorithms .",
    "the parameter @xmath0 is the baseline estimation hyperparameter that can adjust the bias - variance trade - off to provide a good balance between overfitting and underfitting .",
    "we apply our baseline prediction method in conjunction with two policy gradient algorithms ",
    "trust region policy optimization ( trpo ) @xcite and truncated natural policy gradient ( tnpg ) @xcite .",
    "we analyze the effect of different @xmath0 values on performance for three mujoco locomotive control tasks  walker , hopper and half - cheetah .",
    "the agent s interaction with the environment is broken down into a series of @xmath1 episodes or trajectories . the @xmath2 trajectory , @xmath3 , is notated @xmath4 , where @xmath5 and @xmath6 are the state , action and ( instantaneous ) reward respectively , as seen by the system at time @xmath7 , and @xmath8 is the time horizon .",
    "the actions are chosen by the agent in each time step according to a policy @xmath9 : @xmath10 and the next state and reward are sampled according to the transition probability distribution @xmath11 .",
    "a well - known expression @xcite for the policy gradient is @xmath12 \\\\      & = \\mathbb{e}_{\\tau}\\left[\\sum_{t=0}^t\\nabla_{\\theta}\\log\\pi(a_t|s_t,\\theta)r(\\tau)\\right ] ,    \\end{split } \\label{eqn : pgeqn1}\\ ] ] where the policy @xmath9 is parameterized by @xmath13 and @xmath14 refers to the return of a trajectory under this policy .",
    "+ an estimate of that provides better ( lower - variance ) policy gradients is @xmath15 , \\label{eqn : pgeqn2}\\ ] ] where @xmath1 is the batch size , and @xmath16 refers to the _ baseline _ , which is an approximation of the state - value function : @xmath17 $ ] .",
    "classically , policy gradient algorithms work in an iterative fashion as follows :    * initialize * : for iteration @xmath18 , initialize the policy parameter @xmath19 randomly , and the baseline at iteration @xmath20 , @xmath21 to @xmath20 . + * iterate * : repeat for each iteration @xmath22 until convergence :    sample @xmath1 trajectories for policy @xmath23 : @xmath24 .",
    "evaluate @xmath25 as given by using predictions on the baseline at iteration @xmath26 , @xmath27 .",
    "update the policy parameters using the policy gradients : @xmath28 .",
    "for each state @xmath29 , compute the discounted returns @xmath30 . train ( fit ) a new baseline regression model @xmath31 with inputs @xmath29 and outputs @xmath32 .",
    "the policy update @xmath28 above is performed in iteration @xmath33 using predictions from the baseline @xmath27 that was fit in the previous iteration @xmath26 .",
    "an issue with this approach is _ underfitting _ : when the policy changes a lot between iterations , the baseline predictions will be quite noisy . to circumvent this issue , we could use the data samples collected during iteration @xmath33 for fitting the baseline @xmath34 first , and later use the same baseline s predictions for computing the gradient and updating the policy .",
    "as noted in @xcite , this can cause _ overfitting _",
    ", i.e. , it reduces the variance in @xmath25 at the expense of additional bias .",
    "moreover , in the extreme case , if we only have one trajectory starting from each state , and if the baseline fits the data perfectly , then the gradient estimate is always zero .    in this section",
    ", we introduce the _ k - fold _ variant of baseline prediction for policy gradient optimization ( algorithm [ pg ] ) that is more _ sample - efficient _ than prior techniques , and helps counter both the underfitting and overfitting issues described above . at a high level",
    ", the algorithm operates by breaking the data samples into @xmath0 partitions . for each partition ,",
    "a baseline is trained using data from all the other partitions , and the same baseline is used for predicting the value function . since the baseline fitting is performed using samples from the current policy , we mitigate the problem of underfitting .",
    "since we do not directly fit on the current partition s data samples , we also mitigate the overfitting issue .",
    "our algorithm is general enough to be applicable to most of the algorithms for policy optimization including trpo and tnpg .",
    "when we divide the data into multiple partitions , it is possible to perform policy optimization via two different methods . in the first method , which we call the _ parameter - based _",
    "@xmath0-fold baseline estimation , we compute the gradients for each partition , use them to compute @xmath0 different parameters and finally average all the parameters across the partitions to obtain the policy s new parameters . in the second method , which we call the _",
    "gradient - based _",
    "@xmath0-fold baseline estimation , we compute only the gradient for each partition and use the averaged gradient to determine the policy s new parameters .",
    "the pseudo - code for the parameter - based and gradient - based approaches are presented in algorithm [ pg : k - fold_params ] and in algorithm [ pg : k - fold_grads ] , respectively .",
    "the hyperparameter @xmath0 essentially controls the bias - variance trade - off in the baseline estimates . on the one hand , when @xmath0 is small we have lesser data to fit the baseline and consequently the baseline estimates have high variance . on the other hand , when @xmath0 is high we get to fit the baseline with a lot of data causing the baseline estimates to have low variance , however , at an increased computational cost .",
    "the optimal value may be found using standard search techniques for hyperparameters . in this paper",
    ", we tabulate the performances of three mujoco tasks for three values of @xmath0 , ( @xmath35 and @xmath36 ) in section [ sec : results ] .",
    "* initialize * : for iteration @xmath18 , initialize the policy parameter @xmath19 randomly . + * iterate * : repeat for each iteration @xmath22 until convergence :    sample @xmath1 trajectories from policy @xmath23 : @xmath24 . for each state @xmath29 in the @xmath1 trajectories , compute the discounted returns @xmath30 partition the @xmath1 trajectories into @xmath37 disjoint partitions : @xmath38 , @xmath39 train ( fit ) a baseline regression model @xmath40 for partition @xmath41 using the states and returns from all the remaining partitions ( @xmath42 , @xmath43 , @xmath44 ) as input and output respectively . for each partition @xmath41 , initialize the policy with parameters @xmath45 and use any policy optimization algorithm to find optimized policy parameters @xmath46 : @xmath47 .",
    "update the policy parameters @xmath48 as the average of all the optimized policy parameters obtained , i.e. , @xmath49 .",
    "* initialize * : for iteration @xmath18 , initialize the policy parameter @xmath19 randomly . +",
    "* iterate * : repeat for each iteration @xmath22 until convergence :    sample @xmath1 trajectories from policy @xmath23 : @xmath24 . for each state",
    "@xmath29 in the @xmath1 trajectories , compute the discounted returns @xmath30 partition the @xmath1 trajectories into @xmath37 disjoint partitions : @xmath38 , @xmath39 train ( fit ) a baseline regression model @xmath40 for partition @xmath41 using the states and returns from all the remaining partitions ( @xmath42 , @xmath43 , @xmath44 ) as input and output respectively .",
    "evaluate the gradient for partition @xmath41 , @xmath50 using predictions from the baseline @xmath40 .",
    "compute the average gradient @xmath51 . use the average gradient @xmath52 in any policy optimization algorithm to update the policy parameters @xmath48",
    "we evaluate our algorithm on three mujoco locomotive tasks - hopper , walker and half - cheetah using the rllab @xcite platform .",
    "these tasks are chosen since they are complex , yet allow for fast experimentation .",
    "* hopper * is a planar monopod robot with @xmath36 rigid links and @xmath53 actuated joint .",
    "the aim is to learn a stable hopping gait without falling , and avoid local minimum gaits like diving forward .",
    "the observation space is @xmath54-dimensional including joint angles , joint velocities , center of mass coordinates and the constraint forces .",
    "the reward function is given by @xmath55 .",
    "the episode is terminated when @xmath56 or @xmath57 where @xmath58 is the z - coordinate of the center of mass , and @xmath59 is the forward pitch of the body .",
    "* walker * is a planar bipedal robot with @xmath60 rigid links and @xmath61 actuated joints .",
    "the observation space is @xmath62-dimensional including joint angles , joint velocities and center of mass coordinates.the reward function is given by @xmath63 .",
    "the episode is terminated when @xmath64 or @xmath65 , or when @xmath66 where @xmath58 is the z - coordinate of the center of mass , and @xmath59 is the forward pitch of the body .    *",
    "cheetah * is a planar bipedal robot with @xmath67 links and @xmath61 actuated joints .",
    "the observation space is @xmath54-dimensional including joint angles , joint velocities and center of mass coordinates.the reward function is given by @xmath68 .",
    "the experimental setup used to generate the results is described below .",
    "* performance metrics : * for each experiment ( running a specific algorithm on a specific task ) , we define its performance as @xmath69 , where @xmath70 is the number of training iterations and @xmath71 the undiscounted average return for the @xmath33th iteration .",
    "this is essentially the area under the average return curve .",
    "we use @xmath72 random starting seeds and report the performance averaged over all the seeds .",
    "* policy network : * we employ a feed - forward multi - layer perceptron ( mlp ) with @xmath53 hidden layers of sizes @xmath73 , @xmath74 and @xmath75 with tanh nonlinearities after the first two hidden layers that maps states to the mean of a gaussian distribution .",
    "we used the conjugate gradient method for policy optimization .    * baseline : * we use a gaussian mlp for the baseline representation as well , with @xmath76 hidden layers of size @xmath77 each ( and a tanh nonlinearlity after the first hidden layer ) .",
    "the baseline is fitted using the adam first order optimization method @xcite . in each iteration",
    ", we utilized @xmath78 adam steps , each with a batch size of @xmath74 .",
    "* hyperparameters : * for all the experiments , we use the same experimental setup described in @xcite , and listed in table [ table : exp_setup ] .",
    ".[table : exp_setup]basic experimental setup parameters . [ cols=\"^,^ \" , ]",
    "we evaluate both the parameter - based and the gradient - based k - fold algorithms on the three locomotive tasks listed above and on two policy gradient algorithms , trpo and tnpg . in the tables below , we provide the @xmath79 and the @xmath80 numbers .",
    "we also highlight ( boldface ) the cases where the @xmath81 numbers were the best .",
    "first , we evaluate the @xmath0-fold methods ( parameter - based and gradient - based ) with the trust region policy optimization ( trpo ) algorithm .",
    "we used a data ( sample ) size of @xmath82 .",
    "the results are tabulated in table [ table : trpo_50000 ]     task & @xmath83 & @xmath84 & @xmath85 + walker & @xmath86 & @xmath87 & @xmath88 + hopper & @xmath89 & @xmath90 & @xmath91 + cheetah & @xmath92 & @xmath93 & @xmath94 +    under the parameter - based approach it is observed that for @xmath95 , the mean kl divergence between consecutive policies is much lower than the prescribed constraint value of @xmath96 .",
    "in fact , it is seen ( see figure [ figure : mean_kl ] ( left ) ) that the kl divergence values scales down linearly with increasing values of @xmath0 .",
    "a lower kl divergence between consecutive policies implies reduced exploration for larger @xmath0 . to overcome this shortcoming , we perform an additional experiment for the parameter - based method where the step - size was scaled up by a factor of @xmath0 . after scaling up the step sizes , we observe that the mean kl divergence values are comparable across all values of @xmath0 ( see figure [ figure : mean_kl ] ( center ) ) .",
    "table [ table : trpo_50000_scaled ] provides the return numbers for the parameter - based approach with scaled step sizes .",
    "std kl divergence numbers for @xmath97 and @xmath36 for the cheetah task .",
    "the solid red line depicts the mean kl divergence constraint of @xmath96 .",
    "( left ) : the kl divergence numbers for the parameter - based method - they are seen to scale down linearly with increasing @xmath0 .",
    "( center ) : the kl divergence numbers for the parameter - based method with scaled step sizes and ( right ) : the kl divergence numbers for the gradient - based method . for the latter two methods ,",
    "kl divergence numbers are very comparable across the three values of @xmath0 . ]",
    "std kl divergence numbers for @xmath97 and @xmath36 for the cheetah task .",
    "the solid red line depicts the mean kl divergence constraint of @xmath96 .",
    "( left ) : the kl divergence numbers for the parameter - based method - they are seen to scale down linearly with increasing @xmath0 .",
    "( center ) : the kl divergence numbers for the parameter - based method with scaled step sizes and ( right ) : the kl divergence numbers for the gradient - based method . for the latter two methods ,",
    "kl divergence numbers are very comparable across the three values of @xmath0 . ]",
    "std kl divergence numbers for @xmath97 and @xmath36 for the cheetah task .",
    "the solid red line depicts the mean kl divergence constraint of @xmath96 .",
    "( left ) : the kl divergence numbers for the parameter - based method - they are seen to scale down linearly with increasing @xmath0 .",
    "( center ) : the kl divergence numbers for the parameter - based method with scaled step sizes and ( right ) : the kl divergence numbers for the gradient - based method .",
    "for the latter two methods , kl divergence numbers are very comparable across the three values of @xmath0 . ]",
    "task & @xmath83 & @xmath84 & @xmath85 + walker & @xmath86 & @xmath98 & @xmath99 + hopper & @xmath89 & @xmath100 & @xmath101 + cheetah & @xmath102 & @xmath103 & @xmath104 +    the scaling idea above was an ad - hoc approach for improving the exploration for cases with @xmath95 .",
    "alternatively , we can use the gradient - based method , wherein averaging is performed across the @xmath0 parameter gradients themselves , naturally ensuring that the mean kl divergence values remain similar and below the constraint ( see figure .",
    "[ figure : mean_kl ] ( right ) ) .",
    "table [ table : trpo_50000_gradient ] lists the return numbers for the gradient - based @xmath0-fold variant .",
    "figure [ figure : avgreturn ] ( left ) plots the average return mean and std numbers for the hopper task .     task & @xmath83 & @xmath84 & @xmath85 + walker & @xmath86 & @xmath105 & @xmath106 + hopper & @xmath89 & @xmath107 & @xmath108 + cheetah & @xmath102 & @xmath109 & @xmath110 +      next , we evaluate the @xmath0-fold gradient - based method on another policy gradient algorithm , the truncated natural policy gradient ( tnpg ) . the results are tabulated in [ table : tnpg_50000_gradient ] .",
    "task & @xmath83 & @xmath84 & @xmath85 + walker & @xmath111 & @xmath112 & @xmath113 + hopper & @xmath114 & @xmath115 & @xmath116 + cheetah & @xmath117 & @xmath118 & @xmath119 +      finally , we evaluate the @xmath0-fold gradient - based method on lower data size .",
    "the results are presented in table [ table : trpo_tnpg_5000_gradient ] .",
    "figure [ figure : avgreturn ] ( right ) plots the average return mean and std numbers for the walker task .     task & @xmath83 & @xmath84 & @xmath85 + walker & @xmath120 & @xmath121 & @xmath122 + hopper & @xmath123 & @xmath124 & @xmath125 + cheetah & @xmath126 & @xmath127 & @xmath128 +     task & @xmath83 & @xmath84 & @xmath85 + walker & @xmath129 & @xmath130 & @xmath131 + hopper & @xmath132 & @xmath133 & @xmath134 + cheetah & @xmath135 & @xmath136 & @xmath137 +     standard deviation trajectories .",
    "( left ) : hopper task with trpo and a data size of @xmath138 .",
    "( right ) : walker2d task with tnpg and a data size of @xmath139 . ]",
    "standard deviation trajectories .",
    "( left ) : hopper task with trpo and a data size of @xmath138 .",
    "( right ) : walker2d task with tnpg and a data size of @xmath139 . ]",
    "in this paper , we proposed a new method for baseline estimation in policy gradient algorithms .",
    "we tested our method across three environments : walker , hopper , and half - cheetah ; two policy gradient algorithms : trpo and tnpg ; two data sizes : @xmath138 and @xmath139 ; and three different values of @xmath0 : @xmath140 , @xmath141 and @xmath36 .",
    "we find that hyperparameter @xmath0 is a useful parameter to adjust the bias - variance trade - off , in order to achieve a good balance between overfitting and underfitting .",
    "while no single value of @xmath0 was found to result in the best average return across all the cases , it was observed that values other than @xmath83 can provide improved returns in certain cases , thus highlighting the need for a hyperparameter search across the candidate @xmath0 values . as a part of future work",
    "it will be interesting to study the benefit that the @xmath0-fold method provides for other environments , policy gradient algorithms , step sizes and batch sizes .",
    "the authors would like to thank girish kathalagiri , aleksander beloi , luis carlos quintela and atul varshneya for their invaluable help and suggestions during various stages of this project .",
    "sham kakade .",
    "a natural policy gradient . in thomas",
    "g. dietterich , suzanna becker , and zoubin ghahramani , editors , _ advances in neural information processing systems 14 ( nips 2001 ) _ , pages 15311538 . mit press , 2001 .",
    "richard  s sutton , david  a. mcallester , satinder  p. singh , and yishay mansour .",
    "policy gradient methods for reinforcement learning with function approximation . in s.",
    "a. solla , t.  k. leen , and k.  mller , editors , _ advances in neural information processing systems 12 _ , pages 10571063 . mit press , 2000 ."
  ],
  "abstract_text": [
    "<S> the high variance issue in unbiased policy - gradient methods such as vpg and reinforce is typically mitigated by adding a baseline . </S>",
    "<S> however , the baseline fitting itself suffers from the underfitting or the overfitting problem . in this paper </S>",
    "<S> , we develop a _ @xmath0-fold _ method for baseline estimation in policy gradient algorithms . </S>",
    "<S> the parameter @xmath0 is the baseline estimation hyperparameter that can adjust the bias - variance trade - off in the baseline estimates . </S>",
    "<S> we demonstrate the usefulness of our approach via two state - of - the - art policy gradient algorithms on three mujoco locomotive control tasks . </S>"
  ]
}