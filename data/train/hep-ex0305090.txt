{
  "article_text": [
    "to deal with the vast amounts of data collected , hep experiments usually write their own data access software .",
    "this software has to be used for a multitude of tasks : calibration , reconstruction , monte carlo generation , and analysis . in this paper",
    "we compare the standard data access system design to the cleo iii design which uses an on - demand mechanism known as lazy evaluation .",
    "in addition , we describe our experience of having used the cleo iii system for the past four years and in particular the response of the physicists who use this system .",
    "the standard system design  @xcite that most hep experiments use for their data access software is primarily designed to deal with the task of reconstruction . for this task ,",
    "all data objects need to be created for each event that is being processed , since it is the ultimate goal of this task to write all those data objects to persistent storage .    in the standard system design",
    ", each processing step is partitioned into its own software module .",
    "for example , the track finding algorithm and the track fitting algorithm are placed into their own software modules .",
    "in addition , there are input modules responsible for reading objects from persistent storage and output modules responsible for writing objects to persistent storage .    to form an actual processing job , the software modules are run in a user - specified sequence .",
    "when it is a module s turn to run , that module executes its algorithm and then places its data into the event .",
    "in addition , if a module for some reason rejects that event , then the module can tell the system to stop processing that event ( thereby skipping the processing of all modules appearing later in the sequence ) and to restart the sequence with the next event .",
    "a simple example of a processing job run using the standard system design is shown in figure  [ standard_job_example ] . in this example",
    "four different modules are used to process an event .",
    "the first module is the input module which reads objects from a persistent object store ( in this case it would be objects corresponding to hits that have been calibrated ) and then inserts those objects into the event .",
    "the second module in the sequence pulls the calibrated hits from the event and runs the track finding algorithm which then inserts the found tracks into the event .",
    "the third module pulls the calibrated hits and the found tracks out of the event , refits the tracks and then inserts the refitted tracks into the event .",
    "the final module is the output module which pulls objects out of the event and writes them to a persistent object store .",
    "the standard system design has worked well for many experiments for a number of reasons .",
    "one of the main reasons is it has a very simple conceptual model for how a job actually runs .",
    "the job simply runs a series of algorithms that are in a user defined sequence .",
    "this simple conceptual model makes the physicists feel confident that they know how the system works which in turn aids in acceptance of the system . another reason experiments use the standard design is that jobs are fairly easy to debug since it is easy to determine which module had a problem .",
    "unfortunately , the simplicity of the system leads to a number of problems , particularly when the system is used for analysis .",
    "one of the main problems is the physicists using the system must know the inter - module dependencies in order to place them in the correct sequence . to avoid this problem , physicists often run jobs with many modules they do not need in order to avoid missing a module they might need .",
    "another problem is that optimization of the module sequence must be done by hand .",
    "for example , reconstruction jobs often have filter modules which stop the processing of events that contain no interesting physics ( e.g. accelerator related backgrounds ) . to get the best performance for the reconstruction job",
    ", only the modules creating the data needed by the filter module should be ahead of the filter module in the module sequence . since the module sequence must be explicitly set by the physicist it is up to the physicist to find the optimal module sequence .",
    "a final problem is reading back from storage is almost always inefficient .",
    "the reason is that all the objects from storage must be created at the beginning of the event processing , since that is where the input module is in the sequence , even if the job does not use all objects .",
    "the on - demand system used by the cleo  iii experiment @xcite was primarily designed to be used for analysis batch processing .",
    "the main difference between reconstruction and analysis processing is that in analysis processing not all data objects need to be created every event",
    ". therefore the on - demand system only creates data objects as they are requested .",
    "similar to the standard system , the on - demand system breaks the processing task into separate modules .",
    "but in the on - demand system the types of modules are further refined into two categories , each with two module types",
    ".    provider : : :    return data when requested .",
    "+    source : ; ;      reads data from a persistent store .",
    "producer : ; ;      creates data on demand by running an algorithm .",
    "requestors : : :    sequentially run for each new event .",
    "+    processor : ; ;      analyzes and filters events .",
    "sink : ; ;      writes data to a persistent store .",
    "since data objects are only created when requested , data providers register with the system what data they can provide .",
    "this registration is how the system knows which provider to request the data from when another module requests that type of data .",
    "since the method used to create a data object ( either by reading from a persistent store or by running an algorithm ) is activated the first time that data object is requested , the processing sequence is now set implicitly by the order of data requests and not explicitly by the physicist .",
    "essentially the processing sequence is automatically determined by the system without the need for manual intervention .    while the data providers set what data is available in a job , what is done with that data is determined by the data requestors .",
    "processors analyze events ( e.g. fill histograms or ntuples ) and/or filter events of interest by stopping further processing of certain events . in this system",
    "only processors can stop the processing of an event .",
    "the order of the processors must be explicitly set by the physicist since different processor orders can be meaningful and only the physicist knows which order is proper for the job . for example , a physicist could place the event display either before or after an event filter to either see all the events going into the filter or to just see the events that pass the filter .",
    "in contrast , sinks are always at the end of the processing order and therefore only write out data for events that pass all the filters .",
    "figure  [ on - demand_job_example ] shows how an event is processed in the on - demand system .",
    "a source determines the next event to be processed ( but does not fill that event with data ) .",
    "this event is passed to the first processor in the sequence .",
    "if that processor does not reject that event , the event is passed to the following processor in the chain .",
    "this continues until one of the processors rejects the event , at which time the system goes back to the source to get a new event , or all processors have been run .",
    "if no processor rejects the event , the event is given to the sinks for storage .",
    "a data access system provides not only a mechanism for accessing data but also a data model that describes how the data is organized .",
    "the cleo  iii data model is unusual because it provides a unified model for all data so , for example , event data and calibration data are treated in the same manner  @xcite",
    ".        figure  [ frame ] gives a graphical view of the cleo  iii frame / stream data model . in this data model ,",
    "all data items are held in records .",
    "data items are grouped into records based on the life - time of the data .",
    "for example , since both fitted tracks and electro - magnetic showers are only relevant for the instant an event is recorded , both those data items are placed in the event record .",
    "in contrast , the energy of the beams in the accelerator s storage ring is constant through - out a data run so the beam energy is held in the begin run record rather than the event record .",
    "streams are a time ordered sequence of records of the same type . for example",
    ", the event stream holds all the event records in the proper time order .",
    "the last concept in the data model is the frame .",
    "a frame is a collection of records that describe the state of the detector at an instant in time .",
    "for example , to understand the data in event record f requires the data at begin run record b , geometry record b and calibration record a.    because all data are accessed in the exact same way , the physicist s learning curve is reduced .",
    "for example , accessing fitted tracks or detector alignment data are done in exactly the same manner .",
    "first the physicist must get the appropriate record from the frame .",
    "second , he must ask the record to return the data in question . also , since the event record is no different from any other record , physicists can study any type of data in the exact same way they study event data . for example",
    ", a physicist could analyze the way the alignment of the detector changes with time by creating a processor that analyzes detector alignment records rather than event records .",
    "since the frame contains all data relevant for any processing job , the system is designed so that the different component modules that make up a job can only communicate with each other via the data they have registered in the frame . in a sense , this makes the frame the communication bus for the system . using the frame as the communication bus allows us to avoid explicit dependencies between modules .",
    "modules are only dependent on the data objects they use or produce , and not on how those data objects are created .",
    "since the cleo  iii system uses on - demand processing , we need a mechanism to communicate between the data requestors and the data producers .",
    "the mechanism we use employs proxies .",
    "a data provider registers a proxy for each data type the provider can create .",
    "these proxies are placed in the appropriate record and are indexed by a key .",
    "the key is composed of three different tags :    type : : :    the class type of the object returned by the proxy usage : : :    an optional compile - time string describing the use of the object production : : :    an optional run - time settable string .",
    "the two string tags used in the key allow a record to contain many objects of the same class type .",
    "the production tag allows us to compare the results of two providers which create data with the same type and usage tags .",
    "physicists access the data via a type - safe templated function call .",
    "e.g. ,    .... list < fitpion > pions ; extract(iframe.record(kevent ) , pions ) ; ....    in this example , the first line defines a variable named pions to hold a list of fitpions . in the second line , the templated extract function",
    "is given the event record ( which is obtained from the frame via the member function record ) and the pions variable .",
    "the type tag is determined at compile time based on the type of the pions variable .",
    "the usage and production tags are set to their default values .",
    "once the key is built the extract call asks the record for the proxy .",
    "after the proxy is obtained , the extract call tells the proxy to run the algorithm to deliver the data . if the proxy s algorithm runs successfully , the data is cached in case another request for the data occurs later in the job . if an error occurs while trying to obtain the data , an appropriate exception is thrown .",
    "two examples of the on - demand system in action are shown in figure  [ proxy_examples ] . in the left hand example",
    ", the processor selectbtokpi gets fitpions by running the full tracking reconstruction algorithm . in the example",
    ", the fitpionsproxy extracts tracks and calibratedhits .",
    "the tracksproxy also extracts the calibratedhits .",
    "the calibratedhitsproxy then extracts the rawdata ( which is obtained from a file ) and the pedestal and alignment calibrations which are stored in the calibration database . in the right hand example , the exact same selectbtokpi , run in a different job , now gets its fitpions directly from the event database . through the use of dynamic loading",
    ", the exact same selectbtokpi binary object can be run in the two different jobs without the need to relink the main executable .",
    "the on - demand design has many positive features .",
    "first , this design supports data access for all hep jobs : online software trigger , online data quality monitoring , online event display , calibration , reconstruction , mc generation , offline event display and physics analysis .",
    "second , in contrast to the standard design , the physicist does not need to explicitly set the proper procedural order of the job .",
    "the proper order is determined automatically simply by the order in which data is requested .",
    "third , the design optimizes access from storage . at the beginning of processing a record , a source only needs to say when a new record is available . decoding or retrieval of the data associated with that record",
    "can be deferred until the first request for the data .",
    "however , the added complexity of not explicitly setting the procedural order does make debugging and profiling more challenging .",
    "good encapsulation of modules through the frame mechanism ensures that problems are isolated within a module and are not from module to module interactions . with such isolation",
    ", you only need to know which module you are in when a problem arises in order to know where to start debugging . for extremely serious problems that cause a signal to be thrown by the operating system ( e.g. , a memory access violation ) , looking at the last few entries in a function call trace - back in a debugger",
    "is usually sufficient to isolate the error .",
    "for standard run - time problems we have found that the use of exceptions is critical in understanding the system .",
    "the first implementation of our system used null pointers to data to signify that a problem had occurred during an attempted data access .",
    "this lead to a number of problems :    * it was impossible to know why a problem occurred .",
    "e.g. , if fitpions could not be accessed was it because track fitting failed or because no calibrated hits were available ?",
    "* developers had to propagate any error encountered while accessing data needed for their algorithm .",
    "* the return value of a data access had to be checked to be certain that data was obtained .    using exceptions solved all of these problems .",
    "now when an exception occurs , the type of and message in the exception explain the problem .",
    "additionally , only in blocks of code that catch the exception and continue would it be necessary to check to see if the returned data was valid . to further aid in understanding what happened",
    "when an exception is thrown , we created a stack that holds the present data request chain .",
    "when an exception is caught by the system ( because no module caught the exception ) , we print a message such as    .... error : caught an exception : \" starting from selectbtokpi extracted [ 1 ] type \" list < fitpion > \"        usage \" \"        production \" \" [ 2 ] type \" list < track > \"         usage \" \"         production \" \" [ 3 ] type \" list < calibratedhit > \"        usage \" \"        production \" \"",
    "< = = exception occurred    no data \" list < calibratedhit > \" \" \" \" \"     in record event .",
    "please add a source or producer to your    job which can deliver this data . \" ....    and then either terminate the job or skip this event and begin processing the next record .",
    "we have been using a version of this system since september 1998 . during that time",
    "we have gained a great deal of experience with an on - demand system .",
    "one of the most important findings is that the on - demand mechanism can be made fast .",
    "we have found that the proxy lookup takes less than 1 part in @xmath0 of the cpu time on a simple job that processed 2000 events / s on a moderately powerful computer .",
    "another finding is that cyclical dependencies ( a proxy that winds up extracting its own data ) are easy to find and fix .",
    "we only have had one cyclical dependency and it showed up on the first test of the program by causing a stack overflow .",
    "in addition , we have found that we do not need to modify data once it has been created , our records hold immutable data . in the cases where we need a new version of the data , we put the new data into the record with a different key .",
    "one surprising finding is the on - demand system automatically optimizes the performance of reconstruction jobs .",
    "for example , it was trivial to add a filter to reconstruction which removed junk events by using found tracks rather than fitted tracks .",
    "the on - demand system made sure only the algorithms needed for the junk filter are run so that the minimum time is spent processing the event before the filter decides whether to keep it",
    ". we have also found that the best way to speed up analyses is to store many small objects . that way the job only needs to retrieve and decode the data needed for the current job .    because the on - demand design is so different from the standard design ,",
    "we have been very concerned with physicists reaction to the system , particularly since cleo s previous data access system was a standard design implemented in fortran . in general , their response has been very positive to the new system .",
    "the programmers of the reconstruction code like the system .",
    "we made it easy for them to get started by having code skeleton generators to make a proxy , producer or processor . by using the generators",
    ", coders only have to write the code specific to their algorithm and not write the code required to work within the system .",
    "in addition , the reconstruction coders found it very easy to test their code since they could easily swap modules to compare the results of the jobs .",
    "this is extremely easy in our system since we use dynamic loading to load in the different modules a job uses .",
    "therefore when developers are making changes they only need to recompile their own module , not relink the entire system .    to ease the transition for our physicists from the old standard system to the new on - demand system",
    ", we made sure that they could still program their analysis the old way , i.e. all analysis code in the event routine .",
    "however , some of our analysis coders are now pushing the bounds of the system . these physicists are placing selectors ( e.g. , cuts for tracks ) in producers .",
    "this allows them to reuse the same binary for many different analyses .",
    "we are even seeing physicists share these selector binaries .",
    "so once selections are done in producers , the physicists only use processors to fill histograms or ntuples and to filter events .",
    "then if a physicist stores the results of her selections into a persistent store , any subsequent pass through the data only requires reading back the selections from storage ( and not rerunning the algorithm ) and rerunning the processor .",
    "our five years of experience with using an on - demand system has shown us it is possible to build such a system so that it is efficient , debuggable , capable of dealing with all the different types of data ( not just data in an event ) , easy to write components , good for reconstruction and acceptable to physicists .",
    "we believe that several reasons contributed to our success .",
    "the first is our use of skeleton code generators , so that physicists only have to write new code , not infrastructure glue. second , users do not need to register what data they may request , only what data they provide .",
    "this makes life easier for physicists since data reads occur more frequently than writes . and third",
    ", we use a simple rule for when algorithms are run : if a producer is added it takes precedence over a source that can deliver the same data .      9 r. itoh ,",
    "_ basf- belle analysis framework _ chep97 , berlin , germany , 1997 .",
    "b. may , m. paterno _ the d0 event data model _ chep98 , chicago , il , usa , 1998 .",
    "p. avery , c. jones , m. lohner , s. patton , _ design and implementation of the cleo iii data access model _",
    "chep97 , berlin , germany , 1997 ."
  ],
  "abstract_text": [
    "<S> the traditional design of an hep reconstruction system partitions the problem into a series of modules . </S>",
    "<S> a reconstruction job is then just a sequence of modules run in a particular order with each module reading data from the event and placing new data into the event . </S>",
    "<S> the problem with such a design is it is up to the user to place the modules in the correct order and cpu time is wasted calculating quantities that may not be used if the event is rejected based on some other criteria .    </S>",
    "<S> the cleo iii analysis / reconstruction system takes a different approach : on demand processing ( otherwise known as lazy evaluation ) . </S>",
    "<S> jobs are still partitioned into smaller components which we call producers . </S>",
    "<S> however , producers register what data they produce . the first time </S>",
    "<S> a datum is requested for an event the producer s algorithm is run . </S>",
    "<S> sources work similarly , registering what data they can retrieve but delaying retrieval until the data is requested . </S>",
    "<S> data analysis and filtering are done via a separate set of modules called processors .    </S>",
    "<S> we have been using this system for four years and it has been a huge success . </S>",
    "<S> the data access implementation has proven to be very easy to use and extremely efficient . </S>",
    "<S> reconstruction jobs are easy to configure and additional event filters can be added efficiently and robustly  access to correct data is automatically guaranteed . the true test of success : </S>",
    "<S> physicists have embraced this model for analysis even though the old model is still supported . </S>"
  ]
}