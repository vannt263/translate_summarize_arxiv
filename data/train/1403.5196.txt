{
  "article_text": [
    "we aim to calibrate a natural history model ( nhm ) that @xcite developed to characterise uk bowel cancer incidence . using several different types of observed ` target data ' ,",
    "we calibrate the nhm by finding input values that make the nhm s outputs match the target data as closely as possible .",
    "there are 25 calibration inputs , @xmath0 , which are unknown ; some , however , are in principal physically observable . by calibrating the nhm",
    ", we will derive the joint distribution of @xmath0 given the target data .",
    "an outline of the nhm s workings is given in section [ nhm - sim ] .",
    "the motivation for calibrating such a model is to support decision making . in the uk , the national institute for health and care excellence ( nice )",
    "regularly makes such healthcare resource allocation decisions on the basis of cost - effectiveness , with the decisions typically informed by simulator predictions ( for example scenarios see @xcite ) .",
    "furthermore , nice expects analysts to account for simulator input uncertainty , preferably by assigning probability distributions to the inputs and deriving the simulator output distributions ( * ? ? ?",
    "* section 5.8.7 ) .",
    "the calibrated input distributions can be used for this purpose .",
    "our approach to calibration is inspired by the framework for bayesian calibration for computer models ( which we refer to as ` simulators ' ) proposed by @xcite and developed in @xcite , @xcite , @xcite and @xcite , and by bayes linear history matching developed in @xcite , @xcite and @xcite .",
    "our calibration problem involves methodology to address three issues : computationally expensive simulators , ` discrepancy ' , which is the error in a simulator prediction due to the simulator being an imperfect model of reality , and stochastic simulators , which are simulators that can return different output values when run repeatedly at the same input values .",
    "any calibration method will involve running the simulator at different input values , and so methods that require large numbers of simulator runs become impractical if a single simulator run at one input value takes a long time .",
    "a well - established technique for handling expensive simulators , proposed in @xcite , is to construct a cheap surrogate model or ` emulator ' of the simulator using gaussian process regression , based on a relatively small number of simulator runs .",
    "variations of this method are used in the above references . in this paper ,",
    "the simulator is of ` moderate ' computational cost , with a single run at one input value taking between one and two minutes .",
    "we argue that this changes the nature of the surrogate modelling problem . in our",
    "proposed approach , rather than attempting to construct a very precise emulator of the simulator , we use a cruder emulator to guide us to the appropriate regions of the input space , and then do direct simulator evaluations in those regions .",
    "in particular , we propose the use of importance sampling , where the emulator is used to construct the importance density .    when calibrating a simulator , it is important to account for simulator discrepancy for two reasons .",
    "firstly , if the inputs are physically meaningful quantities that could , in principle , be observed directly , calibrating a simulator without accounting for discrepancy may result in biased estimates with severe over - confidence , as demonstrated in @xcite .",
    "if the simulator inputs are ` tuning ' parameters that are not physically observable , discrepancy plays an important role when calibrating to multiple outputs , or when we wish to predict unobserved output quantities using a calibrated simulator .",
    "suppose that we have a physical observation for an output quantity @xmath1 , and wish to predict an unobserved output quantity @xmath2 .",
    "a simulator input value may give a poor fit to output @xmath1 , but a good prediction of @xmath2 .",
    "if we do not believe the simulator models @xmath1 perfectly , we would not necessarily want to rule out such an input value and corresponding prediction of @xmath2 .",
    "accounting for the simulator error in modelling @xmath1 would prevent this .",
    "accounting for simulator discrepancy is clearly important if the simulator is being used to support decision making . without discrepancy , we may have spuriously precise input distributions , resulting in spuriously precise output predictions .",
    "incorporating discrepancy allows decision makers to test the robustness of their decisions both directly to errors in the model outputs , and to the broader input distributions that result from the calibration .    as argued in @xcite , it is important to specify meaningful proper prior distributions for simulator discrepancy , but to do this may be difficult . in @xcite ,",
    "within a bayes linear framework , the simulator expert only provided an _",
    "interval _ for the variance of a discrepancy parameter .",
    "@xcite suggest ` opening the black box ' and incorporating discrepancy terms within the simulator , so that the expert considers sources of simulator discrepancy explicitly , rather than attempting to make judgements about the overall discrepancy .",
    "we argue that it is desirable to be able to investigate , without too much difficulty , a range of different discrepancy distributions , within any calibration methodology . within our proposed importance sampling framework",
    ", we suggest an initial , conservative specification of simulator discrepancy , which can then be varied with little extra computational effort via re - calculation of importance weights corresponding to different discrepancy distributions .",
    "the final issue we consider is that of a stochastic simulator , which raises the question of what it is we should be trying to emulate , assuming that an emulator is necessary .",
    "the simulator in our case study produces random count data . in a similar scenario , @xcite constructed emulators for probabilities from which the count data were assumed to have been generated . here ,",
    "we propose constructing an emulator for the likelihood function given the observed data .",
    "our simulator produces 30 count data outputs ( with various dependencies between the outputs ) , and so emulating the likelihood reduces the computational effort to emulating a univariate output , and enables us to implement an importance sampling approach for the calibration .",
    "this paper has the following structure .",
    "the next section outlines the calibration method .",
    "section [ nhm ] presents the results of calibration of a natural history model and section [ discuss ] offers conclusions and discussion of the calibration method .",
    "we have target data @xmath3 , observed in the real world , with which we can calibrate the simulator .",
    "the data are made up of observations of various binomial and multinomial random variables , but to simplify the discussion , we suppose that @xmath3 is a single binomial random variable , with @xmath4 .",
    "the computer simulator encodes a function @xmath5 that describes the relationship between some input parameters @xmath6 and a binomial distribution probability parameter @xmath5 .",
    "we suppose that there is a true , observable input value @xmath0 , observable in the sense that , in theory data could be obtained to estimate @xmath0 directly , independently of the simulator .",
    "( to clarify , we have @xmath6 as an arbitrary choice of input value , and @xmath0 as the true , unknown values of the input quantities in reality . )",
    "relating the simulator to reality , we recognise that the simulator is not perfect , so that @xmath7 , where @xmath8 represents the simulator error or discrepancy .",
    "the calibration problem is to infer @xmath0 given @xmath3 .",
    "the computer simulator does not actually output @xmath5 for a given input @xmath6 .",
    "instead , the simulator outputs a random variable @xmath9 with @xmath10 .",
    "the value of @xmath11 is expected to increase with the patient cohort size , the original patient sample size chosen for the simulator , but is subject to some random variation .",
    "hence , for any simulator run at input @xmath6 , we will have to infer the value of @xmath5 based on the observations for @xmath9 and @xmath11 . during the calibration process",
    ", we will run the simulator at inputs @xmath12 , to obtain simulator data @xmath13 , and so the aim of the calibration is to derive the posterior distribution @xmath14 ; we infer @xmath0 given @xmath3 _ and _ @xmath15 .",
    "we can evaluate the likelihood @xmath16 for @xmath0 at the value of @xmath6 via @xmath17 which we assume can be simplified as @xmath18 we make a further simplification : we suppose that we have run the simulator at @xmath6 to observe @xmath9 and @xmath11 , so that @xmath19 and then we set @xmath20 so that we only use the run at @xmath6 to infer the corresponding @xmath5 .",
    "as we have already discussed , specifying a single choice of discrepancy distribution is difficult , and so we propose the following strategy to account for simulator discrepancy .",
    "we start with a conservative prior distribution for @xmath8 that permits moderately large values .",
    "we obtain a sample from the posterior distribution @xmath14 .",
    "we can then explore alternative distributions for @xmath8 , using importance sampling to re - weight the sample according to alternative prior distributions @xmath21 .",
    "for example , in the case where @xmath8 is multivariate , corresponding to a multiple output simulator , we can investigate scenarios where some outputs are believed to be better modelled than others . by starting with a conservative prior for @xmath8 we are , in effect , ` broadening the search ' for inputs that give simulator outputs that are close to the observed data . without any discrepancy , it is possible that no input value will give a good fit to all the output data .",
    "writing @xmath7 , we want the discrepancy term @xmath8 to add uncertainty about @xmath22 given @xmath23 , as we do nt believe that running the simulator at the true observed @xmath0 ( and an infinitely large cohort of patients ) would give us @xmath22 . to simplify the computation",
    ", we can achieve this effect by inflating the uncertainty about @xmath5 given @xmath11 and @xmath9 , rather than by introducing an additional term @xmath8 .",
    "we choose a @xmath24 $ ] prior distribution for @xmath5 and suppose that @xmath25 with @xmath26 $ ] .",
    "the parameter @xmath27 has the effect of allowing for simulator discrepancy , by downweighting the information that the simulator run gives us about @xmath5 . in section [ sens - disc ]",
    "we investigate the sensitivity of calibration to different choices of @xmath27 . using the distribution for @xmath5 given in section [ theta - distribution ] ,",
    "we assume @xmath28 and re - write the likelihood as @xmath29 where @xmath30 is the beta function .",
    "obtaining @xmath9 and @xmath11 is computationally expensive , so we need to be selective in when we choose to run the simulator and evaluate the likelihood .",
    "we use importance sampling , where we construct a cheap - to - evaluate importance density using a gaussian process emulator @xcite . in related works ,",
    "@xcite use a gaussian process approximation to a ( log ) posterior density function to improve the efficiency of bayesian integration , which is extended in @xcite to include parallel tempering to accommodate multi - modality .",
    "alternatively , @xcite use radial basis functions to provide a cheap - to - evaluate density function approximation . constructing",
    "the emulator will be an iterative procedure , as the initial design region for the inputs may be specified somewhat conservatively , so that it may take several attempts to construct a satisfactory importance density .",
    "the outline procedure is as follows .    1 .   using an initial set of simulator runs ,",
    "investigate the design region to see if any subregions can be ruled out as having relatively low likelihood .",
    "2 .   run the simulator at a moderate number of input values over the reduced design region , to get initial simulator data @xmath13 .",
    "evaluate the likelihood in equation for each input value @xmath31 .",
    "3 .   using the data from step 2 ,",
    "construct a fast approximation of the log - likelihood using a gaussian process emulator .",
    "4 .   construct an importance density for @xmath32 by approximating the log - likelihood by the posterior mean of the emulator .",
    "use mcmc to generate a sample of values @xmath33 from this approximate posterior density . to guard against the support of the importance density being too small ,",
    "flatten the log - likelihood by multiplying it by a suitable constant .",
    "5 .   run the simulator at @xmath33 and evaluate likelihood ( [ outline - approx - likelihood ] ) for each of these points .",
    "calculate importance weights for each input .",
    "if a small proportion of the inputs in @xmath33 have relatively large importance weight , update the emulator to include the likelihood evaluations , and return to step 4 .",
    "the basic set - up of the nhm is as follows ; for a fuller description see @xcite .",
    "the nhm represents a _ birth cohort _ : a fixed - size sample of people followed from birth to death . a person in the cohort is deemed to have developed bowel cancer when they have reached the first cancer state , duke s a , having begun in a non - cancer state , and progressed through three , ordered pre - cancer states : medium- and high - risk adenomas .",
    "a person may continue to progress through three more increasingly severe cancer states , duke s b , c and stage d. progression between states is governed by time .",
    "when in a given state , a progression time to the next state is simulated , together with a presentation time ( the most common form of presentation being to visit a doctor ) , and a time until death . out of these three actions , the one that occurs is the one with the shortest simulated time .",
    "times are assumed to follow state - dependent weibull distributions , the parameters of which form the majority of the nhm s unknown parameters that we calibrate .",
    "``    this framework for a nhm allows a person s age to be known whenever they change state .",
    "it also allows a person to progress straight from birth to death ( without ever contracting bowel cancer ) , or to progress through some or all pre - cancer and cancer states . by presenting a patient",
    "enters the health system where they receive a bowel cancer diagnosis .",
    "the age - based data that form part of the nhm s output result from these diagnoses and the tracking of ages .",
    "having left the health system , a person returns to a non - cancer state and is still represented by the nhm , but their progression rates between states are elevated . while designed to mimic bowel cancer treatment within the health system , not all processes are necessarily well understood , or can be incorporated in the model . simplifying assumptions , such as times following weilbull distributions ,",
    "are also required .",
    "these give examples of where discrepancy may arise .",
    "the following gives details of the nhm s output required for calibration .",
    "the target data and nhm output are counts that we will in general denote by @xmath34 and @xmath35 , respectively , where @xmath36 indexes the data type and @xmath37 indexes groups within types ; corresponding sample sizes are denoted @xmath38 and @xmath39 , respectively . here @xmath6 is the input vector that we use to initialise the nhm . the data types are identified explicitly , as opposed to considering the output as a single vector , due to their inherent differences , which will emerge in the following summaries .",
    "target data @xmath40 represent a cross - sectional study and give the number of people out of @xmath41 in the uk developing bowel cancer in 2008 , where @xmath42 indexes age groups 0 - 4 , 5 - 9 , @xmath43 , 80 - 84 , 85 + @xcite .",
    "the nhm s output does not match the target data directly .",
    "instead , it represents the cancer state and age of a birth cohort , ie . longitudinal data . to make the nhm output consistent with the target data",
    ", it is resampled by allocating each person to age group @xmath44 at random , according to probabilities determined by proportions in the uk population .",
    "thus we take the nhm output , which corresponds to a longitudinal study , and resample it to match the target data , which corresponds to a cross - sectional study .",
    "let @xmath45 index each randomisation .",
    "the resulting nhm output corresponding to @xmath40 is denoted @xmath46 , with corresponding sample size @xmath47 .",
    "the likelihood is obtained by averaging over randomisations , with @xmath48 large .",
    "@xmath49 is the number of bowel cancer cases of type @xmath50 out of @xmath51 cases , where @xmath52 indexes types duke s a , b and c , and stage d , respectively .",
    "the nhm output is denoted @xmath53 and is directly comparable to @xmath49 .",
    "the total number of cases simulated is denoted @xmath54 .",
    "these data also represent cases by type , but only those cases in which an obstruction ( malignant large bowel ) occurs and only for types duke s b , c and stage d @xcite . we therefore define @xmath55 , @xmath56 , @xmath57 and @xmath58 similarly to @xmath59 .",
    "@xmath60 is the number of people out of @xmath61 , where @xmath62 indexes age groups under 55 , 55 - 64 , 64 - 74 and over 75 , that had developed adenomas that had not been detected in their lifetime ; these have later been detected in a necropsy study @xcite .",
    "nhm output @xmath63 and @xmath64 are defined similarly .",
    "to introduce simulator discrepancy to the nhm , we consider reducing output sample sizes and counts , @xmath39 and @xmath35 , and specify these reductions as fractions , @xmath65 $ ] , @xmath66 .",
    "we allow @xmath27 to vary with data source because sample sizes in the nhm output vary in orders of magnitude .",
    "for example , the cases by age data are based only on those sample members that have developed cancer , whereas the undetected adenomas by age data are based on all patients in the model . to assess calibrated output , we consider its similarity to the target data , given approximate error bounds .",
    "these bounds represent how close a simulator output should be to the target data , considering three sources of error : sampling variability in the data , stochastic variability of the simulator output , and simulator discrepancy . for brevity ,",
    "we present results for binomial data , though only minor alterations are required for multinomial data .",
    "we first consider error due to sampling variability . if @xmath67 , then the variance of @xmath68 , which is used to estimate @xmath22 , is @xmath69 .",
    "similarly , if @xmath70 is simulator output without discrepancy , the estimator @xmath71 has variance @xmath72 .",
    "the addition of simulator discrepancy , through @xmath73 $ ] , inflates the variance of the estimator to @xmath74 , which can be partitioned as @xmath75 thus we decompose the variance of the simulator output into contributions due to the simulator being stochastic and that added by it being imperfect .",
    "we assess the calibrated output against the target data by considering approximate 95% intervals around the target data , which widen as we add in the different sources of error : @xmath76 while @xmath77 , @xmath11 and @xmath9 vary with @xmath6 , they are estimated only once , from the simulator run with highest likelihood .",
    "figure [ vardecomp ] displays variance decompositions for each data source .",
    "this visual representation allows us to choose values of @xmath78 ` by eye ' : we choose values to give bounds around the target data that are such that , if output falls within the bounds , then we are prepared to deem it and its corresponding input plausible .",
    "we perform the calibration in waves and , in the build - up to the final calibration , can broaden the search for inputs by extending these intervals .",
    "we investigate sensitivity to different choices of @xmath27 in section [ sens - disc ] .",
    "in particular , our method is intended to make such sensitivity analyses relatively straightforward .",
    "ultimately we set @xmath79 , @xmath80 , @xmath81 and @xmath82 , which are the values represented in figure [ vardecomp ] .",
    "note that it is difficult to interpret the absolute value of the @xmath78s , due to the different corresponding sample sizes generated internally in the model .",
    "we instead use figure [ vardecomp ] as the main tool for understanding how much discrepancy has been incorporated , and we later inspect the calibrated model outputs to assess how well the model can fit each type of data ( see figure [ summout ] ) .",
    "the prior distributions for the inputs were independent uniform , set with conservatively wide ranges .",
    "it is possible that more carefully specified priors would remove the need for some of the early waves in the history matching process ( see section [ likcanc ] ) .",
    "however , the elicitation problem would be hard , as the inputs do not all correspond to simple observable quantities . in that case",
    ", one might consider constructing a proper prior using the technique of ` probabilistic inversion ' @xcite , in which experts make judgements about model outputs , from which priors for model inputs are constructed .",
    "but the problem then would be that the experts may have already seen the calibration data , and may be unable / unwilling to provide judgements that do not take into account the known output data .",
    "combining sections [ outline ] and [ app - calib ] allows us to calculate the likelihood for all the nhm s output .",
    "notation for realisations follows from section [ app - calib ] ; for example , @xmath83 is the observed number of people in age group @xmath50 developing bowel cancer out of @xmath41 and @xmath84 is the corresponding nhm count out of @xmath85 for input @xmath31 , with notation for other data types defined similarly .",
    "we model the cases by age and undetected adenomas by age data as binomially distributed , and assume weak prior information for its parameters by adopting a uniform[0,1 ] prior .",
    "( note that if population age - group proportions changed considerably over time , then the cases by age data could be subject to greater - than - binomial variation . )",
    "we assume that the cases by type and obstructed cases by type data are multinomially distributed , and use a dirichlet(*1 * ) prior to again represent weak prior knowledge .",
    "finally the complete target data are @xmath86 where @xmath87 .    the overall log - likelihood for the complete target data for an input @xmath31 at which we have run the simulator and obtained output @xmath88 is given by @xmath89 where @xmath90    we calculate the log - likelihood for 10,000 nhm runs , each using a birth cohort of size 100,000 .",
    "figure [ inputreg ] shows the log - likelihood against inputs 1 , 2 , 3 , 12 , and 25 , specifically against single inputs ( achieved by maximising the likelihood over equal - sized bins ) and for pairwise combinations of inputs ( achieved by maximising over grid cells ) .",
    "input 1 represents the age at which a person can develop adenomas , input 2 the log - parameterised weibull shape parameter for progression times between pre - cancer states , input 3 the weibull scale parameter for progression to the first pre - cancer state , input 12 the change in weibull scale parameters due to having previously been treated for cancer and input 25 the probability that a person develops adenomas in their lifetime .",
    "figure [ inputreg ] shows that for some regions of input space the log - likelihood is much higher than for others .",
    "we use where the likelihood is relatively high to define a reduced input space , which is specified by marginal ranges and pairwise regions .",
    "because we start with broad parameter ranges for all 25 inputs , there is large variation in the likelihood values of figure [ inputreg ] , and so our criterion for ruling out parts of input space is set conservatively : we omit parts where the likelihood ratio , relative to the observed maximum , fails to exceed e@xmath91 .",
    "this reduces the input space to 0.7% of its original size . as we approximate true maximum log - likelihoods by those observed , we make conservative choices here to compensate for observed maxima being underestimates of the true maxima",
    "this could be avoided if it were feasible to use many more simulator runs .",
    "the technique of reducing the input region is related to that used in history matching by @xcite , in which implausibility of parts of input space is quantified , and parts measured to have large implausibility are ruled out .    as in @xcite , the input region",
    "can be further reduced in waves . here",
    "second and third waves , also of 10,000 nhm runs , are performed , which use birth cohorts of 200,000 and 300,000 people , respectively .",
    "it is possible that , when reducing the input region , more carefully specified priors could remove the need for some of these early waves .",
    "the emulator training data are based on the region chosen after the third wave , which is 0.0001% the size of our starting input region .",
    "we are building an emulator for the function @xmath92 , the log - likelihood for input vector @xmath6 , where @xmath93 , which is defined in section [ likcanc ] as @xmath94 thus we model @xmath95 where @xmath96 and @xmath97 comprise @xmath98 basis functions and regression coefficients , respectively , @xmath99 is therefore the gp mean function , @xmath100 is its variance and @xmath101 is its correlation function .",
    "we choose the correlation function to have the gaussian form @xmath102 for a set of roughness parameters @xmath103 , where @xmath104 , @xmath105 .",
    "the parameter @xmath106 introduces a _ nugget _ effect into the emulator , which has been shown to improve the predictive performance of gaussian process emulators @xcite , but is imperative for a stochastic simulator such as the nhm .",
    "we are prepared to accept a constant nugget on the grounds that ultimately it is the emulator s posterior mean that we use to sample inputs .",
    "the nugget effect could be allowed to vary with the inputs , but any functional form for this relationship is not obvious , and while we investigated some log - linear forms , none improved upon the constant choice .",
    "we choose the gaussian form because we expect the underlying function to be smooth , and the inclusion of the nugget term is likely to make the precise choice less critical , as we are not trying to interpolate the training data exactly .",
    "the emulator is specified to have a constant mean function , ie .",
    "this choice is convenient here because many runs have very low likelihood , which results in a small mean for the gaussian process , and consequently no inputs being sampled far away from those with a high corresponding likelihood .",
    "polynomial terms could be added .",
    "we tested a linear form , but this gave unsatisfactory results , as inputs far away from those with simulator runs would be sampled if they had a high value of the linear predictor .",
    "a quadratic form with interactions might combat this , but as the nhm has 25 inputs , this was impractical .",
    "perhaps more suitable would be ( the log of ) a parametric density function , though this gives a mean function that is non - linear in its parameters .",
    "we use 2,000 simulator runs for the emulator training data , which are chosen using a maximin latin hypercube design on the reduced region chosen after wave three in section [ likcanc ] .",
    "we define the following : input set @xmath108 ; vector of corresponding log - likelihoods @xmath109 ; @xmath110 matrix @xmath111 , which has @xmath112th element @xmath113 ; and @xmath114 .    for the hyperparameter prior we choose @xmath115 .",
    "it follows that posterior emulator is given by @xmath116 a student @xmath117-process on @xmath118 degrees of freedom , where @xmath119 finally , @xmath120 has posterior @xmath121 we fix @xmath122 at the mode of @xmath123 .",
    "this is found using the nelder - mead optimisation algorithm , which is initialised with 200 iterations of the gibbs sampler , in which metropolis - hastings updates are used .      for the algorithm of section [ sample - post ] to perform well",
    ", the emulator should represent high values of the log - likelihood fairly accurately .",
    "we use importance sampling to give a sample of inputs , and for the importance density use the emulator posterior mean , which serves as an approximation to the log - likelihood .",
    "we can sample from the importance density by again using gibbs sampling with metropolis - hastings updates . to obtain the calibrated inputs we identify parts of the input region where the difference between the posterior mean and the log - likelihood is large , or where , given",
    "the posterior mean is relatively large , the emulator s uncertainty is large .",
    "the latter is identified using the pivoted cholesky decomposition @xcite .",
    "we can then add simulator runs in these parts to enable the emulator to provide a more accurate representation of the log - likelihood surface .",
    "the following algorithm then describes how we obtain the final sample of calibrated inputs .    1 .",
    "obtain a sample of inputs , @xmath124 , by gibbs sampling using the emulator posterior mean , @xmath125 , to approximate the log - likelihood .",
    "[ start ] 2 .",
    "compute the pivoted cholesky decomposition of the covariance matrix for the sample , ie .",
    "the @xmath126 matrix @xmath127 with @xmath128th element @xmath129 , @xmath130 , and let @xmath131 denote its diagonal elements .",
    "sort @xmath132 by the pivot , and take the first @xmath133 members , to give @xmath134 , where @xmath133 is the maximum number of simulator runs we are prepared to add to the training data in one iteration .",
    "3 .   define @xmath135 to be ` large ' if @xmath136 , for some @xmath137 .",
    "if no @xmath135 are large , proceed to step [ start3 ] .",
    "otherwise form the set @xmath138 , for @xmath139 , evaluate the simulator at each of its members and calculate their log - likelihoods , @xmath140 .",
    "[ st1 ] 4 .",
    "add @xmath141 and @xmath140 to the training data , re - build the emulator , and return to step [ start ] .",
    "[ start2 ] 5 .",
    "compute importance weights @xmath142 for @xmath143 .",
    "if a large proportion of weights are zero , return to step [ start2 ] .",
    "[ start3 ] 6 .",
    "obtain the calibrated inputs , @xmath144 , by resampling @xmath132 with replacement according to weights @xmath145 .",
    "observed log - likelihoods against emulator posterior means ( based on previous iteration ) at iterations 18 for samples of size 200 and iterations 9 for a sample of size 1000 .",
    "the line @xmath146 is superimposed ( - - - ) .,scaledwidth=90.0% ]    for step [ start ] of the calibration algorithm we choose @xmath147 , which is achieved by thinning an initial sample of size 100,000 by 50 . for step 2",
    "we choose @xmath148 and for step [ st1 ] choose @xmath149 . during the first iteration of the algorithm we find that almost all @xmath135 are large , which indicates that the emulator s uncertainty is large for all the sampled inputs .",
    "consequently , the importance density may have insufficient support where the true log - likelihood is high .",
    "we flatten the log - likelihood to compensate for this , which is achieved by using @xmath150 instead of @xmath125 , @xmath151 , in step [ start ] ; we initially choose @xmath152 .",
    "introducing @xmath153 can also combat multi - modality of the log - likelihood , as found for parallel tempering in @xcite .",
    "log - likelihoods calculated for the simulator runs are then compared against previous emulator posterior means , that is comparing @xmath92 with @xmath154 for @xmath155 , where @xmath15 are the last - used training data .",
    "this comparison is shown for iterations 19 in figure [ likvsem ] .",
    "nhm output against target data for iterations 1 , 2 , 4 and 8 .",
    "uncertainty bounds are as in figure [ vardecomp ] .",
    "the black line highlights the run with highest likelihood.,scaledwidth=90.0% ]    from figure [ likvsem ] , we see that the agreement between @xmath92 and @xmath154 is poor for the first iteration , which means that the emulator posterior mean will not serve well as an importance density for sampling inputs from the log - likelihood .",
    "we also look at how the simulator s output compares with the target data , given expected levels of uncertainty ( as described in section [ nhm - disc ] ) , which is shown for iterations 1 , 2 , 4 , and 8 in figure [ itsum ] . for iteration 1 , while some runs give a good match to some of the target data , most fail to provide an adequate match to all of the target data .",
    "we proceed to perform further iterations . for iteration 2",
    "we increase @xmath153 to 0.2 , and find that the match between @xmath92 and @xmath154 has improved , but is still unsatisfactory , which can be seen in figure [ itsum ] .",
    "therefore we perform further iterations , increasing @xmath153 by 0.1 for each .",
    "adequate agreement between the emulator and observed log - likelihoods is achieved by iteration 8 , which is confirmed by iteration 9 , the latter of which we choose to be the final emulator .",
    "there is some suggestion from figure [ itsum ] of disagreement between the nhm output and the target data at iteration 8 ; however , the points used to assess this are those for which the emulator s conditional variance is greatest , and therefore a better match between the emulator s posterior mean and the true log - likelihoods can be expected for a random sample of inputs .",
    "furthermore , we only need approximate agreement between the emulator posterior mean and the true log - likelihood , because those points for which agreement is poor will be downweighted during importance sampling .",
    "further iterations could instead be performed to improve agreement , but here that was found to be less efficient than having some negligible importance weights .",
    "we therefore deem the emulator to be adequate for providing a proposal distribution for the importance sampler .",
    "we use the emulator estimated at iteration 9 for the final sample of calibrated inputs .",
    "we choose this sample to be of size 1,000 , and obtain it from an importance sample of size 2,000 by sampling with replacement according to the importance weights , ie .",
    "@xmath156 . figure [ summout ] shows the calibrated nhm output against the target data for the four different data types .",
    "we can see the calibration to have worked well , as the calibrated output is consistent with the target data , once we account for uncertainty amounts .",
    "calibrated nhm runs against target data.,scaledwidth=95.0% ]      we have calibrated the nhm using discrepancy values of @xmath79 , @xmath80 , @xmath81 and @xmath82",
    ". we can investigate sensitivity to these choices by simply recalculating log - likelihoods and then importance weights for alternative discrepancy values .",
    "this requires little computational cost compared to re - running the simulator . the calibrated output for four alternative discrepancy specifications",
    "is shown in figure [ sumdisc ] .",
    "summaries of simulator output against target data for various discrepancy specifications : no discrepancy for any data source ( row 1 ) , discrepancy levels doubled ( row 2 ) , no discrepancy for cases by age ( row 3 ) and no discrepancy for cases by type ( row 4).,scaledwidth=90.0% ]    in the first of these alternative discrepancy scenarios , we consider the case where no discrepancy is assumed , which would imply that the simulator is a perfect representation of reality at the true value of @xmath0 .",
    "this results in an unsatisfactory calibration : all but two of the simulator runs have negligible importance weights , one of which is much larger than the other , and the output from neither of these runs matches the target data , given uncertainty amounts .",
    "we then consider doubling discrepancy amounts , relative to our preferred amounts , so that @xmath157 , @xmath158 , @xmath159 and @xmath160 .",
    "this results in the importance sample having a greater range , when compared to the original calibrated inputs of section [ calib - out ] , and in turn gives more variability in the calibrated output . while altering the discrepancy specification has changed the distribution of the calibrated inputs , the change in distribution of corresponding output is relatively small , which suggests that we do not need to be overly precise when specifying the discrepancy in order to achieve a reliable calibration .",
    "we also consider assuming no discrepancy for only one data source , leaving discrepancy values for the remaining sources unchanged .",
    "if we assume no discrepancy for the cases by age data , then the calibrated output still matches the target data for the cases by age data and for the other data sources , and the sample of calibrated inputs also contains sufficiently many unique values .",
    "however , when we assume no discrepancy for the cases by type data , the sample of calibrated inputs returns to containing only two unique members ( the same two as when no discrepancy is assumed for all data sources ) , and for cases by type the calibrated output fails to match the target data . in summary , though , we find that while discrepancy amounts need some consideration , the precision that specifications require is within our capabilities , allowing the nhm to be calibrated reliably .",
    "however , the calibration becomes unsatisfactory when we ignore discrepancy , or specify it poorly .",
    "in this paper we have calibrated a natural history model so that its output is consistent with reality . however , in order to do this we have had to address three important issues that arise when calibrating the computer simulator .",
    "the first is calibrating a simulator of ` moderate ' computational expense , that is one for which calibration it is not practical using monte carlo simulation alone , but nor is it one that requires us to rely solely on a computationally cheap surrogate model , such as a gaussian process emulator .",
    "we therefore propose a calibration method that may be thought of as a hybrid of the two , which uses an emulator to provide a preliminary , approximate calibration , and combines this with simulator run data , through importance sampling , to give a final and more accurate calibration . because the simulator is only of intermediate computational expense , we have taken a conservative approach to calibration , which can be seen in the criteria for refining the design region ( section [ likcanc ] ) and when we ` flatten ' the log - likelihood ( section [ em - build ] ) . were the simulator more expensive , we might need to consider optimising the calibration process to minimise the number of simulator runs needed .",
    "the use of importance sampling has allowed us to explore a further issue , which is the sensitivity of calibration to different discrepancy specifications , which is important to understand because discrepancy must be adequately quantified before we can calibrate a simulator @xcite .",
    "in particular , while we can in theory always adjust a discrepancy specification and check the sensitivity of a calibration to adjustment , in practice this is likely to be impractical due to its computational requirements . here , though , such investigation becomes computationally feasible , as we simply need to recalculate importance weights and obtain a new sample of calibrated inputs in order to assess different discrepancy specifications .",
    "this does need the original importance sample to be suitable , in particular for it to have enough non - negligible importance weights under the new discrepancy specification .",
    "finally we address how to calibrate a simulator , which we already know to be of intermediate computational expense , that is stochastic and has output that contains count data .",
    "we achieve this by using a gaussian process prior for the log - likelihood , as the log - likelihood is better suited to the gaussian process assumptions than the simulator output itself .",
    "it also reduces the task of calibrating 30-dimensional output to one in which we only have to model a one - dimensional entity .",
    "introducing a nugget effect , overcomes the simulator being stochastic , which will reflect in the log - likelihood surface .",
    "the motivation for the calibration is to support decision - making , and so the main objective for incorporating simulator discrepancy is to protect against over - confidence .",
    "although we have incorporated discrepancy into the four output types , the analysis is less informative for understanding the causes of simulator error , and where simulator improvements would be beneficial .",
    "our approach to discrepancy is also less suited to capturing systematic errors , which could arise from posterior correlation in the cases by age data ( figure [ summout ] ) , but is not recognised in likelihood .",
    "such issues may be better addressed with the ` internal ' simulator discrepancy approach in @xcite . nevertheless ,",
    "the present calibrated simulator , with allowance made for discrepancy , will still have significant value in supporting decisions .",
    "we thank paul tappenden for providing the nhm and for guidance on its usage , and thank two reviewers and an associate editor for suggestions that have brought improvement to this paper .",
    "this work was supported by rcuk funding for the _",
    "mucm2 _ project ( grant ep / h007377/1 ) .",
    "bayarri , m.  j. , j.  o. berger , j.  cafeo , g.  garcia - donato , f.  liu , j.  palomo , r.  j. parthasarathy , r.  paulo , j.  sacks , and d.  walsh ( 2007 ) . computer model validation with functional output .  _",
    "35_(5 ) , 18741906 .",
    "bliznyuk , n. , d.  ruppert , c.  shoemaker , r.  regis , s.  wild , and p.  mugunthan ( 2008 ) .",
    "bayesian calibration and uncertainty analysis for computationally expensive models using optimization and radial basis function approximation .   _",
    "17_(2 ) .",
    "henderson , d.  a. , r.  j. boys , k.  j. krishnan , c.  lawless , and d.  j. wilkinson ( 2009 ) .",
    "bayesian emulation and calibration of a stochastic computer model of mitochondrial dna deletions in substantia nigra neurons .   _",
    "104_(485 ) , 7687 .",
    "tekkis , p.  p. , r.  kinsman , m.  r. thompson , and j.  d. stamatakis ( 2004 ) .",
    "the association of coloproctology of great britain and ireland study of large bowel obstruction caused by colorectal cancer .  _",
    "240_(1 ) , 7681 ."
  ],
  "abstract_text": [
    "<S> we calibrate a natural history model , which is a class of computer simulator used in the health industry , and here has been used to characterise bowel cancer incidence for the uk . the simulator tracks the development of bowel cancer in a sample of people , and its output mostly stratifies bowel cancer occurrence by patient age and bowel cancer type . </S>",
    "<S> its output relies on 25 unknown inputs , which we are required to calibrate . in order to do this </S>",
    "<S> we must address that not only is the output count data , but it is also stochastic , due to the simulation procedure .    </S>",
    "<S> we can not feasibly achieve calibration of the simulator using monte carlo methods alone , as it is of ` moderate ' computational expense . to achieve a reliable calibration </S>",
    "<S> , we must also specify its discrepancy : how , when calibrated , it differs from reality . </S>",
    "<S> we propose a method for calibration that combines a statistical emulator for the likelihood function with importance sampling . </S>",
    "<S> the emulator provides an interim sample of inputs at which the simulator is run , from which the likelihood is calculated . </S>",
    "<S> importance sampling is then used to re - weight the inputs and provide a final sample of calibrated inputs . </S>",
    "<S> re - calculating the importance weights incurs little computational cost , and so we can easily investigate how different discrepancy specifications affect calibration .    , </S>"
  ]
}