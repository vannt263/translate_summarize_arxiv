{
  "article_text": [
    "deep learning has become a very powerful framework for machine learning and has enjoyed strong success in many areas in recent years . in the supervised setting , mapping and representing an input through increasingly more abstract layers of feature representation",
    "has shown to be extremely effective in areas such as natural language processing , image captioning , and many others .",
    "the concept of multilayer feature representations and modeling machine learning problems using a network of neurons is also motivated and guided by studies of the brain , neurological behavior , and cognition .    however , despite the compelling arguments for using neural networks as a general template for solving machine learning problems , the training of these models and design of the right network for a given task has been filled with many theoretical gaps and practical concerns . for training a network , one needs to specify an often large network architecture with several layers and nodes , and then solve a difficult non - convex optimization problem .",
    "additionally , the pre - specified architecture is often treated as a hyperparameter which is tuned using a validation set .",
    "these spaces can become exorbitantly large ( e.g. @xcite ) . from an optimization perspective",
    ", there is no guarantee of stability of an output model or near optimality of the learning objective , and often , one needs to implement ad hoc methods ( e.g. gradient clipping @xcite ) to produce coherent models . from the statistical standpoint ,",
    "large - scale hyperparameter tuning for an effective network architecture is extremely wasteful of data ( due to cross validation ) , and can also exhaust a lot of time and resources ( e.g. grid search , random search @xcite ) .    in this paper",
    ", we attempt to remedy some of these issues . accepting the general structure of a neural network as an effective parametrized model for supervised learning",
    ", we provide a theoretical analysis of this model and proceed to derive an algorithm benefitting from that theory . in the process , we introduce a framework for training neural networks that :    1",
    ".   uses a stable and robust algorithm with a unique solution .",
    "2 .   can produce much sparser and/or shallower networks compared to existing methods .",
    "3 .   adapts the structure and complexity of the network to the difficulty of the particular problem at hand , with no pre - defined architecture .",
    "4 .   is accompanied and in fact motivated by strong data - dependent generalization bounds , validating their adaptivity and statistical efficacy .",
    "5 .   is intuitive from the cognitive standpoint that originally motivated neural network architectures .",
    "not all machine learning problems admit the same level of difficulty , and different tasks naturally require varying levels of complexity .",
    "the typical approach to training a neural network requires the model - builder to know and specify as an architecture the right level of complexity .",
    "this is often unreasonably hard and can lead to large amounts of hyperparameter tuning , a statistically wasteful task .",
    "moreover , if a network architecture is specified a priori and trained using back - propagation , the model will always have as many layers as the one specified because there needs to be at least one path through the network in order for the hypothesis to be non - trivial .",
    "single weights may be pruned @xcite , a technique originally termed optimal brain damage @xcite , but the architecture itself is unchanged .",
    "this imposes a stringent lower bound on the complexity of the model and can make the model prone to overfitting when there is insufficient data .",
    "in contrast to enforcing high complexity , we will attempt to learn the requisite model complexity for a machine learning problem in an adaptive way . starting from a simple single layer neural network , we will add more neurons and additional layers as needed . from the cognitive perspective , we will adapt the neural complexity and architecture to the difficulty of the problem . the additional neurons that we add will be carefully selected and penalized according to rigorous estimates from the theory of statistical learning .",
    "this will serve as a catalyst for the sparsity of our model as well as the strong generalization bonds that we will be able to derive . incredibly",
    ", our method will also turn out to be convex and hence more stable than the current methodologies employed .",
    "there has been extensive work involving structure learning for neural networks ( e.g. ) .",
    "all these publications seek to grow and prune the neural network architecture using some heuristic ( e.g. genetic , information theoretic , or correlation ) .",
    "the structure learning algorithm introduced in this paper is based directly on optimizing generalization performance , which is precisely the learning goal in the batch setting .    from the theory perspective",
    ", there have been several major lines of research on the theoretical understanding of neural networks . the first deals with understanding properties of the objective function used when training neural networks .",
    "( e.g. @xcite ) .",
    "the second involves studying the black - box optimization algorithms that are often used for training these networks ( e.g. @xcite ) .",
    "the third analyzes the statistical and generalization properties of the neural networks that are created ( e.g. @xcite ) .",
    "the fourth takes the generative point of view ( e.g. @xcite ) , assuming that the data actually comes from a particular network and then attempting to recover it .",
    "the fifth investigates the expressive ability of neural networks and analyzing what types of mappings they can learn ( e.g. @xcite ) .",
    "this paper takes the discriminative approach to machine learning and incorporates the first three methodologies , starting with a theoretical analysis of neural networks , to deriving a computationally tractable objective function , and to finally describing a precise optimization method .",
    "@xcite is another paper that touches on multiple theory components , analyzing the generalization and training of two - layer neural networks through tensor methods .",
    "our work uses different methods , applies to arbitrary networks , and also learns a network structure from a single layer .",
    "let @xmath0 denote the input space .",
    "we consider the standard supervised binary classification scenario and assume that training and test points are drawn i.i.d .  according to some distribution @xmath1 over @xmath2 and denote by @xmath3 a training sample of size @xmath4 drawn according to @xmath5 . given any @xmath6 , we denote by @xmath7 the feature representation of @xmath8 .",
    "the standard description of a modern feedforward network is a network of layers of nodes , where each layer is mapped to the layer above it via a linear mapping composed with a component - wise nonlinear transformation . to make this precise ,",
    "we define a neural network as follows .",
    "let @xmath9 denote the number of layers in the network .",
    "the networks we learn can be potentially very deep , that is @xmath9 can be very large . for",
    "each @xmath10 $ ] , denote by @xmath11 the maximum number of nodes in layer @xmath12 .",
    "let @xmath13 and @xmath14 .",
    "then define the set @xmath15 to be the family of functions at layer @xmath12 of the network in the following way : @xmath16 where @xmath17 is a hyperparameter and where @xmath18 is an activation function .",
    "common activation functions include the rectified linear unit ( relu ) @xmath19 and the sigmoid function @xmath20 ( see e.g. @xcite ) , although our work will allow for any 1-lipschitz activation function .",
    "the choice of norm @xmath21 here is left to the learner and will determine both the sparsity of the network and the accompanying learning guarantee of the resulting model .",
    "let @xmath22 denote the union of these families of functions and their reflections : @xmath23 .",
    "any feedforward neural network , then , can be written as a composition of mappings @xmath24 , where @xmath25 .",
    "intuitively , each transformation represents the encoding of the original data into an abstract layer of feature representation from which learnability is presumed to be `` easier . ''",
    "note that in our definition , the activation function is not directly included in the unit itself but instead only applied when feeding the neuron into the next layer .",
    "while this choice of notation ultimately represents the same family of models , it is a subtle but important distinction that will be crucial for our theory as well as algorithmic design , in particular for deriving lemma  [ lemma : rad_hk_hk-1 ] and lemma  [ lemma : duality ] .",
    "we measure the performance of a hypothesis @xmath26 by its expected loss over the data s distribution @xmath1 , also known as the generalization error : @xmath27.$ ]    we typically also want to measure the performance of the model on our training sample @xmath28 .",
    "this will be done using the empirical margin loss : @xmath29 where the margin refers to the @xmath30 term .",
    "hypothesis functions that allow for large margin with small empirical margin loss intuitively represent classifiers with high confidence of accuracy .    given a hypothesis set @xmath22 of functions mapping from @xmath0 to @xmath31 , we denote by @xmath32 the empirical rademacher complexity of @xmath22 for the sample @xmath28 : @xmath33,$ ] and by @xmath34 the rademacher complexity of @xmath22 defined by @xmath35 $ ] .",
    "the empirical rademacher complexity measures the correlation of a hypothesis set with random noise over the sample and is a problem - dependent measure of model complexity .",
    "it can also be shown to relate to other classical notions of model complexity , such as the vc - dimension and covering number ( see e.g. @xcite ) .      since",
    "neural networks are built as compositions of layers , it is natural from the theoretical standpoint to first analyze the complexity of any layer in terms of the complexity of its previous layer .",
    "our first result demonstrates that this can indeed be done , and that the empirical rademacher complexity of any intermediate layer @xmath12 in the network is bounded by the empirical rademacher complexity of its input times a term that depends on a power of the size of the layer :    [ lemma : rad_hk_hk-1 ] let @xmath36 .",
    "then for @xmath37 , the empirical rademacher complexity of @xmath38 for a sample @xmath28 of size @xmath4 can be upper bounded as follows in terms of that of @xmath39 : @xmath40    the proofs of this result , as well as all those that follow in this section , can be found in appendix  [ app : theory ] .    by analyzing the complexity of the initial layer , we can derive a bound on the complexity of every layer in closed form without the need for any recurrence relation :    [ lemma : rad_hk ] let @xmath41 , i \\in[1,m ] } \\left|[{\\boldsymbol \\phi}(x_i)]_j\\right|$ ] , and @xmath36 .",
    "then for any @xmath14 , the empirical rademacher complexity of @xmath38 for a sample @xmath28 of size @xmath4 can be upper bounded as follows : @xmath42    the above two lemmas are instructive and intuitive in the sense that they convey the message that additional layers in a neural network contribute to increased complexity of a model .",
    "because of this , while large models are more powerful , they also become increasingly more prone to overfitting .",
    "moreover , the rademacher complexity bounds also suggest that model complexity can increase much more due to a single additional layer as opposed to an additional node .    guided by this insight , we will seek to learn models that will be parsimonious in model complexity .",
    "specifically , we will learn adaptive neural networks that consider _ all _ layers of feature representation simultaneously , emphasize shallower layers of representation more heavily , and only activate deeper ones when necessary .",
    "we will represent such models using the notation : @xmath43 , where @xmath44 s are weights and @xmath45 .",
    "the motivation for this type of network is reinforced by the following learning guarantee :    [ th : adanet ] let @xmath46 $ ] and for each @xmath47 $ ] , let @xmath48 $ ] . for @xmath47 $ ] and @xmath49 $ ] , let @xmath50 and @xmath51 .",
    "finally , let @xmath41 , i \\in[1,m ] } \\left|[{\\boldsymbol \\phi}(x_i)]_j\\right|$ ] , and @xmath36 . then for any @xmath52 , with probability at least @xmath53 over the sample @xmath28 of size @xmath4 drawn i.i.d . according to distribution @xmath54",
    ", the following inequality holds for any @xmath55 with @xmath56 : @xmath57 where @xmath58 .    the generalization bound above informs us that the complexity of the neural network returned is a weighted combination of the complexities of each node in the neural network , where the weights are precisely the ones that define our network .",
    "specifically , this again agrees with our intuition that deeper networks are more complex and suggests that if we can find a model that has both small empirical error and most of its weight on the shallower nodes , then such a model will generalize well .    toward this goal , we will design an algorithm that directly seeks to minimize upper bounds of this generalization bound . in the process",
    ", our algorithm will train neural networks that discriminate deeper networks from shallower ones .",
    "this is a novel property that existing regularization techniques in the deep learning toolbox do not enforce .",
    "techniques such as @xmath59 and @xmath60 regularization and dropout ( see e.g. @xcite ) are generally applied uniformly across all nodes in the network .",
    "this section describes our algorithm , adanet , for _ adaptive _ deep learning .",
    "adanet adaptively grows the structure of a neural network , balancing model complexity with margin maximization .",
    "we start with a high - level description of the algorithm before proceeding to a more detailed presentation .",
    "our algorithm starts with the network reduced to the input layer , corresponding to the input feature vector , and an output unit , fully connected , and then augments or modifies the network over @xmath61 rounds . at each round , it either augments the network with a new node or updates the weights defining the function @xmath62 of an existing node of the network at layer @xmath12 .",
    "a new node may be selected at any layer @xmath63 $ ] already populated or start on a new layer but , in all cases , it is chosen with links only to existing nodes in the network in the layer below plus a connection to the output unit .",
    "existing nodes are either those of the input layer or nodes previously added to the network by the algorithm .",
    "figure  [ fig : nn](a ) illustrates this design .",
    "the choice of the node to construct or update at each round is a key aspect of our algorithm .",
    "this is done by iteratively optimizing an objective function that we describe in detail later . at each round ,",
    "the choice of the best node minimizing the current objective is subject to the following trade - off : the best node selected from a lower layer may not help reduce the objective as much as one selected from a higher layer ; on the other hand , nodes selected from higher layers augment the network with substantially more complex functions , thereby increasing the risk of overfitting . to resolve this tension quantitatively",
    ", our algorithm selects the best node at each round based on a combination of the amount by which it helps reduce the objective and the complexity of the family of hypotheses defined nodes at that layer .",
    "the output node of our network is connected to all the nodes created during these @xmath61 rounds , so that the hypothesis will directly use all nodes in the network .",
    "as our theory demonstrated in theorem  [ th : adanet ] , this can significantly reduce the complexity of our model by assigning more weight to the shallower nodes . at the same time",
    ", it also provides us the flexibility to learn larger and more complex models .",
    "in fact , the family of neural networks that we search over is actually larger than the family of feedforward neural networks typically considered using back - propagation due to these additional connections .    [ cols=\"^,^ \" , ]     -.1 in [ fig : nn ]    an additional more sophisticated variant of our algorithm is depicted in figure  [ fig : nn](b ) . in this design , the nodes created at each round can be connected not just to the nodes of the previous layer , but to those of any layer below .",
    "this allows for greater model flexibility , and by modifying the definitions of the hypotheses sets  [ eq : h1 ] and  [ eq : hk ] , we can adopt a principled complexity - sensitive way for learning these types of structures as well .    in the next section ,",
    "we give a more formal description of our algorithm , including the exact optimization problem as well as a specific search process for new nodes .",
    "recall the definition of our hypothesis space @xmath64 , which is the convex hull of all neural networks up to depth @xmath9 and naturally includes all neural networks of depth @xmath9  the common hypothesis space in deep learning . note that the set of all functions in @xmath22 is infinite , since the weights corresponding to any function can be any real value inside their respective @xmath65 balls .    despite this challenge",
    ", we will efficiently discover a finite subset of @xmath22 , denoted by @xmath66 , that will serve as the basis for our convex combination . here , @xmath67 will also represent the maximum number of nodes in our network .",
    "thus , we have that @xmath68 , and @xmath67 will also generally be assumed as very large .",
    "moreover , we will actually define and update our set @xmath66 _ online _ , in a manner that will be made precise in section  [ sec : search ] .",
    "we will also rely on the natural bijection between the two enumerations @xmath69 and @xmath70 , j\\in[n_k]}$ ] , depending on which is more convenient .",
    "the latter is useful for saying that @xmath71 .",
    "moreover , for any @xmath72 $ ] , we will denote by @xmath73 $ ] , the layer in which hypothesis @xmath74 lies . for simplicity ,",
    "we will also write as @xmath75 the rademacher complexity of the family of functions @xmath76 containing @xmath74 : @xmath77 .",
    "let @xmath78 be a non - increasing convex function upper bounding the @xmath79 loss , @xmath80 , with @xmath81 differentiable over @xmath31 and @xmath82 for all @xmath8 .",
    "@xmath81 may , for instance , be the exponential function , @xmath83 as in the adaboost of @xcite or the logistic function , @xmath84 as in logistic regression .    as in regularized boosting style methods ( e.g. @xcite )",
    ", our algorithm will apply coordinate descent to the following objective function over @xmath85 : @xmath86 where @xmath87 with @xmath88 and @xmath89 hyperparameters .",
    "the objective function is the sum of the empirical error based on a convex surrogate loss function @xmath78 of the binary loss and a regularization term .",
    "the regularization term is a weighted-@xmath60 penalty that contains two sub - terms : a standard norm-1 regularization which admits @xmath90 as a parameter , and a term that discriminates functions @xmath74 based on their complexity ( i.e. @xmath75 ) and which admits @xmath91 as a parameter .",
    "our algorithm can be viewed as an instance of the deepboost algorithm of @xcite . however , unlike deepboost , which combines decision trees , adanet algorithm learns a deep neural network , which requires both deep learning - specific theoretical analysis as well as an online method for constructing and searching new nodes .",
    "both of these aspects differ significantly from the decision tree framework in deepboost , and the latter is particularly challenging due to the fact that our hypothesis space @xmath22 is infinite .",
    "let @xmath92 denote the vector obtained after @xmath93 iterations and let @xmath94 .",
    "let @xmath95 denote the @xmath12th unit vector in @xmath85 , @xmath96 $ ] .",
    "the direction @xmath95 and the step @xmath97 selected at the @xmath98th round are those minimizing @xmath99 .",
    "let @xmath100 .",
    "then we can write @xmath101    for any @xmath102 $ ] , we will maintain the following distribution @xmath103 over our sample : @xmath104 where @xmath105 is a normalization factor , @xmath106 . moreover , for any @xmath107 $ ] and @xmath108 $ ] and a given hypothesis @xmath74 bounded by @xmath109 , we will consider @xmath110 , the weighted error of hypothesis @xmath74 over the distribution @xmath111 : @xmath112 \\big].$ ] these weighted errors will be crucial for `` scoring '' the direction that the algorithm takes at each round .",
    "as already mentioned , a key aspect of our adanet algorithm is the construction of new hypotheses at each round .",
    "we do not enumerate all @xmath67 hypotheses at the beginning of the algorithm , because it would be extremely difficult to select good candidates before seeing the data . at the same time , searching through all node possible combinations using the data would be a computationally infeasible task .    instead , our search procedure will be online , building upon the nodes that we already have in our network and selecting at most a single at a time . specifically , at each round , the algorithm selects a node out of the following set of `` active '' candidates : existing nodes in our network or new nodes with connections to existing nodes in some layer @xmath12 .",
    "there are many potential methods to construct new candidate nodes , and at first glance , scoring every possible new node with connections to existing nodes may seem a computational impediment . however , by using banach space duality , we can compute directly and efficiently in closed form the new node that best optimizes the objective at each layer .    adanets = ( ( x_i , y_i)_i=1^m , ( n_k , _ k , _ k , c_k)_k=1^l    -.5 cm    -.0 cm [ alg : adanet ]    existingnodesd_t , ( h_k , j)_k , j , ( _ k , c_k)_k=1^l    -.5 cm    -.2 cm [ alg : existing ]      given a distribution @xmath1 over the sample @xmath28 and a tuple of hypotheses @xmath113 , we denote by @xmath114 the weighted margin of hypothesis @xmath115 composed with its activation on distribution @xmath1 : @xmath116,\\ ] ] and we denote by @xmath117 the vector of weighted margins of all nodes in layer @xmath12 : @xmath118,\\ldots , { \\mathbb{e}}_{i \\sim { { \\mathscr d}}}[y_i ( \\varphi_{k}\\circ h_{k , n_k})(x_i ) ] \\big).\\ ] ]    for any layer @xmath12 in an existing neural network , a vector @xmath119 uniquely specifies a node that connects from nodes in the previous layer @xmath120 .",
    "let @xmath121 denote such a new node in layer @xmath12 , and let @xmath122 be the number of layers with non - zero nodes .",
    "then for layers @xmath123 , if the number of nodes is less than the maximum allowed size @xmath11 , we will consider as candidates the nodes with the largest weighted margin .",
    "remarkably , these nodes can be computed efficiently and in closed form :    [ lemma : duality ] fix @xmath124 .",
    "then the solution @xmath121 to the optimization problem @xmath125,\\ ] ] can be computed coordinate - wise as : @xmath126 and the solution has value : @xmath127 .      in this section , we present the pseudocode for our algorithm , adanet , which applies the greedy coordinate - wise optimization procedure described in section  [ sec : coord ] on the objective presented in section  [ sec : obj ] with the search procedure described in section  [ sec : search ] .",
    "the algorithm takes as input the sample @xmath28 , the maximum number of nodes per layer @xmath128 , the complexity penalties @xmath129 , the @xmath65 norms of the weights defining new nodes @xmath130 , and upper bounds on the nodes in each layer @xmath131 .",
    "adanet then initializes all weights to zero , sets the distribution to be uniform , and considers the active set of coordinates to be the initial layer in the network",
    ". then the algorithm repeats the following sequence of steps : it computes the scores of the existing nodes in the method existingnodes , of the new candidate nodes in newnodes , and finds the node with the best score ( @xmath132 or @xmath133 ) in bestnode . after finding this",
    "`` coordinate '' , it updates the step size and distribution before proceeding to the next iteration ( as described in section  [ sec : coord ] ) .",
    "the precise pseudocode is provided in figure  [ alg : adanet ] , and details of its derivation are given in section  [ app : alg ] .",
    "newnodesd_t , ( h_k)_k=1^ , ( n_k , _ k , c_k ,",
    "_ k,_k))_k=1^l",
    "-.5 cm    -.2 cm [ alg : new ]      remarkably , the neural network that adanet outputs is competitive against the _ optimal _ weights for any sub - network that it sees during training . moreover , it achieves this guarantee in linear time . the precise statement and proof",
    "are provided in appendix  [ app : converge ] .",
    "we describe a large - scale implementation of the adanet optimization problem using state - of - the - art techniques from stochastic optimization in appendix  [ app : scale ] .",
    "we presented a new framework for analyzing and learning artificial neural networks .",
    "our method optimizes for generalization performance , and it explicitly and automatically addresses the trade - off between model architecture and empirical risk minimization , ideas that have been under - explored in deep learning .",
    "our techniques are general and can be applied to other neural network architectures , including cnns and lstms as well as to other learning settings such as multi - class classification and regression , all of which serve as interesting avenues for future work .",
    "lemma : rad_hk_hk-1 let @xmath36 . then for @xmath37 , the empirical rademacher complexity of @xmath38 for a sample @xmath28 of size @xmath4 can be upper bounded as follows in terms of that of @xmath39 : @xmath40    @xmath134\\\\ & = \\frac{1}{m } \\operatorname*{\\mathbb e}_{{\\boldsymbol \\sigma}}\\left [ \\sup_{\\substack{h_j \\in { { \\mathscr h}}^{(p)}_{k    - 1}\\\\ \\| { \\mathbf u}\\|_p \\leq \\lambda_k } } \\sum_{j = 1}^{n_{k-1 } } u_j \\sum_{i = 1}^m \\sigma_i ( \\varphi_{k-1 } \\circ h_j)(x_i )    \\right]\\\\ & = \\frac{\\lambda_k}{m } \\operatorname*{\\mathbb e}_{{\\boldsymbol \\sigma}}\\left [ \\sup_{\\substack{h_j \\in    { { \\mathscr h}}^{(p)}_{k - 1 } } } \\bigg\\| \\sum_{i = 1}^m \\sigma_i ( \\varphi_{k-1 } \\circ h_j)(x_i ) \\bigg\\|_q    \\right ] & ( \\text{def . of dual norm})\\\\            & = \\frac{\\lambda_k n_{k-1}^{\\frac{1}{q } } } { m } \\operatorname*{\\mathbb e}_{{\\boldsymbol \\sigma}}\\left [ \\sup_{\\substack{h_j \\in    { { \\mathscr h}}^{(p)}_{k - 1 } } } \\bigg\\| \\sum_{i = 1}^m \\sigma_i ( \\varphi_{k-1 } \\circ h_j)(x_i ) \\bigg\\|_\\infty    \\right ] & ( \\text{equiv . of $ l_p$ norms and $ \\sup$})\\\\            & = \\frac{\\lambda_k n_{k-1}^{\\frac{1}{q}}}{m } \\operatorname*{\\mathbb e}_{{\\boldsymbol \\sigma}}\\left [ \\sup_{\\substack{h \\in    { { \\mathscr h}}^{(p)}_{k - 1 } } } \\bigg| \\sum_{i = 1}^m \\sigma_i ( \\varphi_{k-1 } \\circ h)(x_i ) \\bigg|    \\right ] \\\\    & = \\frac{\\lambda_k n_{k-1}^{\\frac{1}{q } } } { m } \\operatorname*{\\mathbb e}_{{\\boldsymbol \\sigma}}\\left [ \\sup_{\\substack{h \\in    { { \\mathscr h}}^{(p)}_{k - 1}\\\\ s \\in { \\ { -1 , + 1   \\ } } } } s \\sum_{i = 1}^m \\sigma_i ( \\varphi_{k-1 } \\circ h)(x_i )     \\right ] & ( \\text{def . of absolute value})\\\\            & \\leq \\frac{\\lambda_k n_{k-1}^{\\frac{1}{q } } } { m } \\operatorname*{\\mathbb e}_{{\\boldsymbol \\sigma}}\\left [ \\sup_{\\substack{h \\in    { { \\mathscr h}}^{(p)}_{k - 1 } } } \\sum_{i = 1}^m \\sigma_i ( \\varphi_{k-1 } \\circ h)(x_i ) \\right ] \\\\ &",
    "\\quad + \\frac{\\lambda_k}{m } \\operatorname*{\\mathbb e}_{{\\boldsymbol \\sigma}}\\left [ \\sup_{\\substack{h \\in    { { \\mathscr h}}^{(p)}_{k - 1 } } } \\sum_{i = 1}^m -\\sigma_i ( \\varphi_{k-1 } \\circ h)(x_i )     \\right ] \\\\    & = \\frac{2 \\lambda_k n_{k-1}^{\\frac{1}{q } } } { m } \\operatorname*{\\mathbb e}_{{\\boldsymbol \\sigma}}\\left",
    "[ \\sup_{\\substack{h \\in    { { \\mathscr h}}^{(p)}_{k - 1 } } } \\sum_{i = 1}^m \\sigma_i ( \\varphi_{k-1 } \\circ h)(x_i ) \\right ] \\\\    & \\leq \\frac{2 \\lambda_k n_{k-1}^{\\frac{1}{q } } } { m } \\operatorname*{\\mathbb e}_{{\\boldsymbol \\sigma}}\\left [ \\sup_{\\substack{h \\in    { { \\mathscr h}}^{(p)}_{k - 1 } } } \\sum_{i = 1}^m \\sigma_i h(x_i ) \\right ] & ( \\text{talagrand 's inequality})\\\\                                                               & \\leq 2 \\lambda_k n_{k-1}^{\\frac{1}{q } } { \\widehat}{\\mathfrak r}_s({{\\mathscr h}}^{(p)}_{k-1 } ) \\\\\\end{aligned}\\ ] ]    lemma : rad_hk let @xmath41 , i \\in[1,m ] } \\left|[{\\boldsymbol \\phi}(x_i)]_j\\right|$ ] , and @xmath36 .",
    "then for any @xmath14 , the empirical rademacher complexity of @xmath38 for a sample @xmath28 of size @xmath4 can be upper bounded as follows : @xmath42    @xmath135\\\\ & = \\frac{1}{m } \\operatorname*{\\mathbb e}_{{\\boldsymbol \\sigma}}\\left [ \\sup_{\\| { \\mathbf u}\\|_p \\leq \\lambda_1 }    { \\mathbf u}\\cdot \\sum_{i = 1}^m \\sigma_i { \\boldsymbol \\phi}(x_i )   \\right ] \\\\ &",
    "= \\frac{\\lambda_1}{m } \\operatorname*{\\mathbb e}_{{\\boldsymbol \\sigma}}\\left [ \\bigg\\| \\sum_{i = 1}^m \\sigma_i    [ { \\boldsymbol \\phi}(x_i ) ] \\bigg\\|_q \\right ] & ( \\text{def . of dual norm})\\\\ & \\leq \\frac{\\lambda_1 n_0^{\\frac{1}{q}}}{m } \\operatorname*{\\mathbb e}_{{\\boldsymbol \\sigma}}\\left [ \\bigg\\| \\sum_{i = 1}^m \\sigma_i    [ { \\boldsymbol \\phi}(x_i ) ] \\bigg\\|_\\infty \\right ] & ( \\text{equivalence of $ l_p$ norms})\\\\ & = \\frac{\\lambda_1 n_0^{\\frac{1}{q}}}{m } \\operatorname*{\\mathbb e}_{{\\boldsymbol \\sigma}}\\left [ \\max_{j \\in [ 1 , n_1 ] } \\bigg | \\sum_{i = 1}^m \\sigma_i    [ { \\boldsymbol \\phi}(x_i)]_j \\bigg | \\right ] & ( \\text{def . of $ l_\\infty$ norm})\\\\ & = \\frac{\\lambda_1 n_0^{\\frac{1}{q}}}{m } \\operatorname*{\\mathbb",
    "e}_{{\\boldsymbol \\sigma}}\\left [ \\max_{\\substack{j \\in [ 1 ,    n_1]\\\\ s \\in { \\ { -1 , + 1   \\ } } } } \\sum_{i = 1}^m \\sigma_i    s[{\\boldsymbol \\phi}(x_i)]_j   \\right ] & ( \\text{def . of absolute value})\\\\ & \\leq \\lambda_1   n_0^{\\frac{1}{q } } r_\\infty \\sqrt{m } \\frac{\\sqrt{2 \\log ( 2 n_0)}}{m }   = r_\\infty \\lambda_1 n_0^{\\frac{1}{q}}\\sqrt{\\frac{2 \\log ( 2 n_0)}{m}}. & ( \\text{massart 's lemma})\\end{aligned}\\ ] ]    the result then follows by application of lemma  [ lemma : rad_hk_hk-1 ] .",
    "th : adanet[adanet generalization bound ] let @xmath136 $ ] and for each @xmath47 $ ] , let @xmath48 $ ] . for @xmath47 $ ] and @xmath49 $ ] , let @xmath50 and @xmath51 .",
    "then for any @xmath52 , with probability at least @xmath53 over the sample @xmath28 of size @xmath4 drawn i.i.d .",
    "according to distribution @xmath54 , the following inequality holds for any @xmath55 with @xmath56 : @xmath137    by considering the symmetrized sets @xmath23 , we can assume without loss of generality that the weights @xmath44 are non - negative .",
    "we will now use the following structural learning guarantee for ensembles of hypotheses :    [ lemma : db ] let @xmath138 .",
    "assume there exists some decomposition of the hypothesis space @xmath139 .",
    "fix @xmath140 . given any @xmath141 and a sample @xmath28 ,",
    "denote the empirical margin loss as @xmath142 .",
    "then for any @xmath52 , with probability at least @xmath53 over the sample @xmath28 of size @xmath4 drawn i.i.d .",
    "according to distribution @xmath54 , for any @xmath143 such that @xmath144 , the following inequality holds for @xmath145 : @xmath146 where for each @xmath147 , @xmath148 denotes the smallest @xmath63 $ ] such that @xmath149 .",
    "lemma  [ lemma : db ] implies that @xmath150    by using symmetrization in lemma  [ lemma : rad_hk ] , @xmath151",
    "lemma : duality[construction of new candidate nodes ] fix @xmath124 .",
    "then the solution @xmath121 to the optimization problem @xmath152,\\ ] ] can be computed coordinate - wise as : @xmath126 and the solution has value : @xmath127    by linearity of expectation , @xmath153           = \\operatorname*{\\rm argmax}_{\\|u\\|_p \\leq \\lambda_{k } }   { \\mathbf u}\\cdot \\operatorname{\\rm margin}({{\\mathscr d } } , { \\mathbf h}_{k-1}).\\\\\\end{aligned}\\ ] ] we claim that @xmath154 to see this , note first that by holder s inequality , @xmath155 and the expression on the right - hand side is our proposed value . at the same time",
    ", our choice of @xmath121 also satisfies this upper bound : @xmath156 thus , @xmath121 is a solution to the optimization problem and achieves the claimed value .",
    "recall the form of our objective function : @xmath157    we want to find the directional derivative with largest magnitude as well as the optimal step - size in this coordinate direction .    since @xmath158 is non - differentiable at @xmath159 for each coordinate ( due to the weighted @xmath60 regularization )",
    ", we must choose a representative of the subgradient .",
    "since , @xmath158 is convex , it admits both left and right directional derivatives , which we denote by @xmath160 moreover , convexity ensures that @xmath161 .",
    "now , let @xmath162 be the element of the subgradient that we will use to compare descent magnitudes , so that @xmath163 , j\\in[1,n_k ] } \\big|\\delta_{k , j}f(w_t)\\big|$ ] .",
    "this subgradient will always be chosen as the one closest to 0 : @xmath164 suppose that @xmath165 . then by continuity , for @xmath97",
    "sufficiently small , @xmath166 and @xmath167 have the same sign so that @xmath168 furthermore , @xmath158 is differentiable in the @xmath169-th at @xmath170 , which implies that @xmath171 when @xmath172 , we can consider the left and right directional derivatives : @xmath173 moreover , @xmath174 so that we have @xmath175",
    "figures  [ alg : init ] ,  [ alg : best ] ,  [ alg : step ] ,  [ alg : dist ] present the remaining components of the pseudocode for adanet with exponential loss .",
    "the initial weight vector @xmath176 is initialized to 0 , and the initial weight distribution @xmath177 is uniform over the coordinates .",
    "the best node is simply the one with the highest score @xmath132 ( or @xmath133 ) among all existing nodes and the new candidate nodes .",
    "the step - size taken at each round is the optimal step in the direction computed . for exponential loss functions ,",
    "this can be computed exactly , and in general , it can be approximated numerically via line search methods ( since the objective is convex ) .",
    "the updated distribution at time @xmath98 will be proportional to @xmath178 , as explained in section  [ sec : coord ] .",
    "initm , l , n_1 ,  , n_l    -.5 cm    -.25 cm [ alg : init ]    bestnode , ( d_k , j)_k , j , ( _ k)_k , ( _ k)_k=1^l    -.5 cm    -.25 cm [ alg : best ]    applystep(k , j ) , w_t-1    -.5",
    "cm    -.25 cm [ alg : step ]    updatedistributionw_t , s , ( h_k , j)_k , j [ eq : adaboost_dist_update ]    -.5 cm    -.25 cm [ alg : dist ]",
    "let @xmath81 be a twice - continuously differentiable function with @xmath179 , and suppose we terminate adanet after @xmath180 iterations if it does not add a new node .",
    "let @xmath181 $ ] be the first @xmath182 nodes added by adanet , and let @xmath183 , where @xmath184 denotes projection onto @xmath185 .",
    "let @xmath186 be the total number of nodes constructed by the adanet algorithm at termination .",
    "then adanet will terminate after at most @xmath187 iterations , producing a neural network and a set of weights such that @xmath188 } f(w_{i_s}^ * ) < { \\epsilon}\\ ] ]    recall that @xmath189 since adanet initializes all weights to 0 and grows the coordinate space in an online fashion , we may consider the algorithm in epochs , so that if the support of @xmath170 at any given @xmath98 is @xmath190 , then @xmath191 fix an epoch @xmath192 $ ] .",
    "the optimal set of weights within the support @xmath190 is given by @xmath193 , and this solution exists because @xmath158 is a coercive convex function ( due to the weighted @xmath60 regularization ) .",
    "let @xmath194 be a matrix with elements given by @xmath195 , and let @xmath196 be a @xmath197-th elementary basis vector of @xmath198 . then for any vector @xmath199 , @xmath200 .",
    "thus , if we define denote by @xmath201 , then the first component of @xmath202 can be written as @xmath203 .    moreover , since @xmath81 is twice continuously differentiable and @xmath179 , it follows that @xmath204 is twice continuously differentiable and strictly convex .",
    "we can also compute that for any @xmath205 , @xmath206 which is positive definite since @xmath179 . finally ,",
    "since @xmath22 is symmetric , we can , at the cost of flipping some @xmath207 , equivalently write : @xmath208 , subject to @xmath209 .",
    "+ thus , the problem of minimizing @xmath202 is equivalent to the optimization problem studied in , and we have verified that the conditions are satisfied as well . if the algorithm does nt add another coordinate , then it performs the gauss - southwell method on the coordinates @xmath190 . by theorem 2.1 in , this method will converge to the optimal set of weights with support in @xmath190 linearly .",
    "this implies that if the algorithm terminates , it will maintain the error guarantee : @xmath210 .",
    "if the algorithm does add a new coordinate before termination , then we can apply the same argument to @xmath211 .",
    "thus , when the algorithm does finally terminate , we maintain the error guarantee for every subset @xmath181 $ ] of nodes that we create , and the total run time is at most @xmath212 iterations .",
    "our optimization problem is a regularized empirical risk minimization problem of the form : @xmath213 where each @xmath214 is smooth and convex and @xmath215 , also convex , decomposes across the coordinates of @xmath8 .    for any subset",
    "@xmath216 $ ] , let @xmath217 denote the components of the erm objective that correspond to that subset . for any @xmath218",
    "$ ] , let @xmath219 denote the partial derivative in the coordinate @xmath182 .",
    "we can leverage the mini - batch randomized block coordinate descent ( mbrcd ) technique introduced by .",
    "their work can be viewed as the randomized block coordinate descent variant of the stochastic variance reduced gradient ( svrg ) family of algorithms ( ) .",
    "our algorithm divides the entire training period into @xmath220 epochs . at every step",
    ", the algorithm samples two mini - batches from @xmath221 $ ] uniformly .",
    "the first is used to approximate the erm objective for the descent step , and the second is used to apply the newnodes subroutine in figure  [ alg : new ] to generate new candidate coordinates . after generating these coordinates ,",
    "the algorithm samples a coordinate from the set of new coordinates and the existing ones .",
    "based on the coordinate chosen , it updates the active set .",
    "then the algorithm computes a gradient descent step with the svrg estimator in place of the gradient and with the sampled approximation of the true erm objective .",
    "it then makes a proximal update with @xmath215 as the proximal function .",
    "finally , the `` checkpoint parameter '' of the svrg estimator is updated at the end of every epoch .",
    "while our optimization problem itself is not strongly convex , we can apply the mbrcd indirectly by adding a strongly convex regularizer , solving : @xmath222 where @xmath223 is a 1-strongly convex function , to still yield a competitive guarantee .",
    "moreover , since our non - smooth component is a weighted-@xmath60 term , the proximal update can be solved efficiently and in closed form in a manner that is similar to the iterative shrinkage - thresholding algorithm ( ista ) of ."
  ],
  "abstract_text": [
    "<S> we present a new theoretical framework for analyzing and learning artificial neural networks . </S>",
    "<S> our approach simultaneously and adaptively learns both the structure of the network as well as its weights . </S>",
    "<S> the methodology is based upon and accompanied by strong data - dependent theoretical learning guarantees , so that the final network architecture provably adapts to the complexity of any given problem . </S>"
  ]
}