{
  "article_text": [
    "since karl pearson introduced the term `` goodness - of - fit '' at the beginning of the twentieth century there has been an enormous amount of papers on this topic .",
    "first concentrated in fitting a model for one distribution function , and later , especially after the papers of @xcite and @xcite , in more general models related with the regression function .",
    "the literature is vast , and we refer to @xcite for an updated review of the topic . + the ideas of goodness - of - fit for density and distribution have been naturally extended in the nineties of the last century to regression models .",
    "considering , as a reference , a regression model with random design @xmath2 the goal is to test @xmath3 in an omnibus way from a sample @xmath4 of @xmath5 . here",
    "@xmath6}}$ ] is the regression function of @xmath7 over @xmath8 and @xmath9 is a random error centred and such that @xmath10}}=0 $ ] .",
    "+ following the ideas on smoothing for testing about the density function @xcite , the usual pilot estimator for @xmath11 was a nonparametric one , for example , the nadaraya - watson estimator ( @xcite , @xcite ) : @xmath12 , with @xmath13 , where @xmath14 is a kernel function and @xmath15 is a bandwidth parameter .",
    "other possible weights such as the ones from local linear estimation , @xmath16-nearest neighbours or splines were also used in different studies . using these kind of pilot estimators ,",
    "statistical tests were given by @xmath17 , being @xmath18 some functional distance and @xmath19 an estimator of @xmath20 such that @xmath21 under @xmath22 .",
    "in an alternative way , following the paper by @xcite for testing about the distribution , the pilot estimator in the regression case was given by @xmath23 , the empirical estimation of the integrated regression function @xmath24}}$ ] .",
    "@xcite using @xmath25 and @xcite using @xmath26 are key references for these two approximations in the literature , which were only the beginning of more than two hundred papers published in the last two decades @xcite .",
    "+ more recently , it has been of interest testing about a possible structure in a regression setting where we have functional covariates : @xmath27 being now @xmath28 a random element in a functional space , for example in the hilbert space @xmath29 $ ] , and @xmath7 a scalar response .",
    "this is the context of `` functional data analysis '' , which has received an increasing attention in the last decade ( see for example @xcite , @xcite and @xcite ) , specially motivated by the practical needs of analysing data generated from high - resolution measuring devices . + a very simple null hypothesis @xmath22 considered in the literature for the model ( [ funcmod ] ) is @xmath30 , where @xmath31 is a fixed constant : the testing of significance of the covariate @xmath28 over @xmath7 .",
    "following some of the ideas from @xcite on considering pseudometrics for performing smoothing with functional data , the test by @xcite was adapted by @xcite : @xmath32,\\end{aligned}\\ ] ] being @xmath18 a functional pseudometric , @xmath14 a kernel function adapted to this situation , @xmath15 a bandwidth parameter , @xmath33 a weight function and @xmath34 the probability measure induced by @xmath35 over the functional space of the covariate .",
    "the testing about @xmath22 has been also considered by @xcite or , more recently , @xcite , not in an omnibus way but inside a functional linear model ( flm ) : @xmath36 , where @xmath37 represents the inner product in @xmath38 and @xmath39 is the flm parameter .",
    "for both approximations , omnibus or not , there has been also some recent papers considering the case of functional response ; see for example , @xcite , @xcite , @xcite and @xcite .",
    "+ the generalization of the previous functional hypothesis to the general case @xmath40 where @xmath41 can be either of finite or infinite dimension , was the focus of very few papers , specially in the context of omnibus goodness - of - fit tests . in",
    "@xcite a discussion is given , without theoretical results , for the extension of the checking of a more complex null hypothesis such as a flm .",
    "only one paper is known for us where the flm hypothesis is analysed with theoretical results . in @xcite ,",
    "motivated by the smoothing test statistic considered by @xcite for finite dimensional covariates , one test based on @xmath42 is employed for checking the null hypothesis of linearity with @xmath43 , with @xmath44 a suitable estimator of @xmath45 and @xmath46 the empirical distribution function of @xmath47 . in the same spirit , @xcite gave a test for the finite dimensional context and @xcite for functional response . from a different perspective , and motivated by the test of @xcite for finite dimensional predictors , in @xcite a test was constructed from the marked empirical process @xmath48 , with @xmath49 and @xmath50 .",
    "this approach circumvents the technical difficulties that a marked empirical process indexed by @xmath51 , a possible functional extension of the process in @xcite , would represent .",
    "+ in this paper we consider marked empirical processes indexed by random projections of the functional covariate .",
    "the motivation stems from the almost surely ( a.s . ) characterization of the null hypothesis ( [ null ] ) via a _ projected hypothesis _ that arises from the conditional expectation on the projected functional covariate .",
    "this allows , conditionally on a randomly chosen @xmath52 , the study of the weak convergence of the process @xmath53 for hypothesis testing of infinite dimension . as a by - product",
    ", we obtain root-@xmath0 goodness - of - fit tests that evade the curse of dimensionality and , contrary to smoothing - based tests , do not rely on a tuning parameter .",
    "particularly , we focus on the testing of the aforementioned hypothesis of functional linearity where , contrary to the finite dimensional situation , the functional estimator has a non - trivial effect on the limiting process and requires a careful regularization .",
    "the test statistics are built by a continuous functional ( kolmogorov - smirnov or cramr - von mises ) over the empirical process and are effectively calibrated by a wild bootstrap on the residuals . to account for a higher power and less influence from @xmath52 , we consider a number @xmath14 ( not to confuse with a kernel function ) of different random projections and merge the resulting @xmath1-values into a final @xmath1-value by means of the false discovery rate ( fdr ) of @xcite .",
    "the empirical analysis reports a competitive performance of the test in practice , with a low impact of the choice of @xmath14 above a certain bound , and an expedient computational complexity of @xmath54 that yields notable speed improvements over @xcite .",
    "+ the rest of the paper is organized as follows .",
    "the characterization of the null hypothesis through the projected predictor is addressed in section [ sec : hyproj ] , together with an application for the testing of the null hypothesis @xmath55 ( subsection [ subsec : simple ] ) .",
    "section [ sec.lin ] is devoted to testing the composite hypothesis @xmath56 . to that aim , the regularized estimator for @xmath45 of @xcite , @xmath57 ,",
    "is reviewed in subsection [ subsec : rho ] .",
    "the pointwise asymptotic distribution of the projected process is studied in subsection [ subsec : point ] , whereas subsection [ subsec : weak ] gives its weak convergence .",
    "section [ sec : testing ] describes the implementation of the test and other practicalities .",
    "section [ sec : simu ] illustrates the finite sample properties of the test by a simulation study and with some real data applications .",
    "some final comments and possible extensions are given in section [ sec : final ] .",
    "appendix [ ap : proofs ] presents the main proofs , whereas the supplementary material contains the auxiliary lemmas and further results for the simulation study .",
    "+ some general setting and notation are introduced now . the random variable ( r.v . ) @xmath28 belongs to a separable hilbert space @xmath38 endowed with the inner product @xmath37 and associated norm @xmath58 .",
    "the space @xmath38 is a general real hilbert space but for simplicity can be regarded as @xmath29 $ ] .",
    "@xmath7 and @xmath28 are assumed to be centred r.v.s , and @xmath9 is a centred , independent from @xmath28 , r.v . with variance @xmath59 .",
    "the independence between @xmath9 and @xmath28 is a technical assumption required for proving lemmas [ lemmtn2.rn ] and [ lemmtn2.sn ] , while for the rest of the paper it suffices with @xmath10}}=0 $ ] .",
    "given the @xmath60-valued r.v .",
    "@xmath61 and @xmath62 , we denote by @xmath63 to the projected @xmath28 in the direction @xmath52 .",
    "bold letters are used for vectors either in @xmath60 ( mainly ) or in @xmath64 , and its kind is clearly determined by the context .",
    "capital letters represent r.v.s defined on the same probability space @xmath65 .",
    "weak convergence is denoted by @xmath66 and @xmath67 represents the skorohod s space of _ cdlg _ functions defined on @xmath68 .",
    "finally , we shall implicitly assume that the null hypotheses stated hold a.s .",
    "the pillar of the goodness - of - fit tests we present is the a.s .",
    "characterization of the null hypothesis ( [ null ] ) , re - expressed as @xmath69=0 $ ] for some @xmath70 , by means of the associated _ projected hypothesis on @xmath71 _ , defined as @xmath72}}=0 $ ] .",
    "we identify @xmath73 by @xmath7 for the sake of simplicity in notation .",
    "we give in this section two necessary and sufficient conditions based on the projections of @xmath28 to that @xmath74}}=0 $ ] holds a.s .",
    "+ the first condition only requires the integrability of @xmath7 , but the condition needs to be satisfied for every direction @xmath52 .",
    "[ propnorandom ] assume that @xmath75}}<\\infty$ ] . then , @xmath76}}=0\\text { a.s . }",
    "\\iff \\mathbb{e}\\big[y | { { { \\bf x}}^{{\\bf h}}}\\big]=0\\text { a.s . for every } { { \\bf h}}\\in { \\mbox{${\\cal h}$}}.\\ ] ]    the second condition , more adequate for application , _ somehow _ generalizes proposition [ propnorandom ] , as it only needs to be satisfied for a randomly chosen @xmath52 . in exchange",
    ", it holds only under some additional conditions on the moments of @xmath28 and @xmath7 . before stating it we need some preliminary results , being the first one included here for the sake of completeness .",
    "[ t : cwgauss ] let @xmath77 be a non - degenerate gaussian measure on @xmath38 , let @xmath78 be two @xmath38-valued r.v.s and denote by @xmath79 if they are identically distributed .",
    "assume that :    1 .",
    "@xmath80 , for all @xmath81 , and @xmath82.[t : cwgauss : a ] 2 .",
    "the set @xmath83 is of positive @xmath77-measure.[t : cwgauss : b ]    then @xmath79 .",
    "it is not strictly needed that @xmath77 be a gaussian distribution in lemma [ t : cwgauss ] and this can be replaced by assuming a certain smoothness condition on @xmath77 ( see , for instance , theorem 2.5 and example 2.6 in @xcite ) .",
    "assumption [ t : cwgauss : a ] in lemma [ t : cwgauss ] is not of technical nature . according to theorem 3.6 in @xcite , it becomes apparent that a similar condition is required .",
    "this assumption is satisfied if the tails of @xmath84 are light enough or if @xmath85 has a finite moment generating function in a neighbourhood of zero .",
    "[ prop : auxiliar ] if @xmath86}}<\\infty$ ] and @xmath28 satisfies [ t : cwgauss : a ] in lemma [ t : cwgauss ] , then @xmath87}}\\allowbreak < \\infty$ ] , for all @xmath81 , and @xmath88 .",
    "the second condition and most important result in this section is given as follows .",
    "[ th : basic ] let @xmath77 be a non - degenerate gaussian measure on @xmath38 .",
    "assume that @xmath28 satisfies [ t : cwgauss : a ] in lemma [ t : cwgauss ] and that @xmath86 } } < \\infty$ ] .",
    "then , @xmath89}}=0\\text { a.s.}\\iff { \\mbox{${\\cal h}$}}_0:=\\big\\{{{\\bf h}}\\in { \\mbox{${\\cal h}$ } } : \\mathbb{e}\\big[y",
    "| { { { \\bf x}}^{{\\bf h}}}\\big]=0\\text { a.s.}\\big\\}\\text { has positive $ \\mu$-measure.}\\ ] ]    [ coro1 ] under the assumptions of the previous theorem , @xmath89}}=0 \\text { a.s.}\\iff \\mu\\big({\\mbox{${\\cal h}$}}_0\\big)=1.\\ ] ]    according to this corollary , it happens that if we are interested in testing the simple null hypothesis @xmath90}}=0 $ ] we can do it as follows :",
    "_ i _ ) select at random with @xmath77 a direction @xmath62 ; _ ii _ ) conditionally on @xmath52 , test the projected null hypothesis @xmath91=0 $ ] .",
    "the rationale is simple yet powerful : if @xmath22 holds , then @xmath92 also holds ; if @xmath22 fails , then @xmath92 also fails @xmath77-a.s . in this case , with probability one we have chosen a direction @xmath52 for which @xmath92 fails .",
    "of course , the main advantage to test @xmath92 over testing @xmath22 directly is that in @xmath92 the conditioning r.v . is real , which simplifies the problem substantially .",
    "an immediate application of corollary [ coro1 ] is the testing of the simple null hypothesis @xmath55 via the empirical process of @xcite .",
    "recall that other testing alternatives can be considered on the projected covariate due to the @xmath77-a.s . characterization .",
    "we refer to @xcite for a review on alternatives .",
    "+ for a random sample @xmath93 from @xmath94 , we can consider the empirical process of the regression conditioned on the direction @xmath52 , @xmath95 and then the following result is trivially satisfied from theorem 1.1 in @xcite .",
    "[ coro : single ] under @xmath92 and @xmath86}}<\\infty$ ] , @xmath96 in @xmath67 , being @xmath97 a gaussian process with zero mean and covariance function @xmath98}}\\allowbreak\\ , \\mathrm{d}f_{{{\\bf h}}}(u)$ ] , where @xmath99 is the distribution function of @xmath100 .",
    "different statistics for the testing of @xmath92 can be built from continuous functionals on @xmath101 .",
    "we shall cover this with more detail in section [ sec.lin ] .",
    "consider the flm @xmath102 in @xmath29 $ ] , with @xmath28 a gaussian process with associated karhunen - love expansion ( [ eq.kar_loeve ] ) and @xmath9 independent from @xmath28",
    ". then @xmath103 and @xmath104 are centred gaussians with variances @xmath105 and @xmath106 , respectively , and @xmath107=\\sum_{j=1}^\\infty h_j \\rho_j\\lambda_j$ ] , with @xmath108 , @xmath109 .",
    "hence , @xmath110 where @xmath111 and @xmath112 are the density and distribution functions of a @xmath113 , respectively .",
    "we focus now in testing the composite null hypothesis , expressed as @xmath114 according to corollary [ coro1 ] , it happens that testing @xmath22 is @xmath77-a.s .",
    "equivalent to test @xmath115 } } = 0 , \\text { for some } { \\boldsymbol\\rho}\\in { \\mbox{${\\cal h}$}},\\ ] ] where @xmath52 is sampled from a non - degenerate gaussian law @xmath77 .",
    "again , we construct the associated empirical regression process indexed by the projected covariate following @xcite . therefore , given an estimate @xmath44 of @xmath45 under @xmath22 , we have @xmath116 where @xmath117 is a normalizing positive sequence to be determined later and @xmath118}},{\\boldsymbol\\rho}-\\hat { \\boldsymbol\\rho}\\right > } , \\\\ t_{n,{{\\bf h}}}^3(x):=&\\,n   { \\left<{\\mathbb{e}{\\left[}\\mathbbm{1}_{{\\left\\{{{\\bf x}}^{{\\bf h}}\\leq x\\right\\ } } } { { \\bf x}}{\\right]}},{\\boldsymbol\\rho}-\\hat { \\boldsymbol\\rho}\\right>}.\\end{aligned}\\ ] ] the selection of the right estimator @xmath44 has a crucial role in the weak convergence of @xmath119 , which poses a substantially complexer proof than for the simple hypothesis .",
    "we consider the regularized estimate proposed in sections 2 and 3 of @xcite ( denoted by cms in the sequel ) , whose construction is sketched here for the sake of exposition of our results .",
    "consider the so - called karhunen - love expansion of @xmath61 : @xmath120 where @xmath121 is a sequence of orthonormal eigenfunctions associated to the covariance operator of @xmath28 , @xmath122}}={\\mathbb{e}{\\left[}{\\left<{{\\bf z}},{{\\bf x}}\\right>}{{\\bf x}}{\\right]}}$ ] , @xmath123 and the @xmath124 s are centred real r.v.s ( because @xmath28 is centred ) such that @xmath125}}= \\delta_{j , j'}$ ] , where @xmath126 is the kronecker s delta .",
    "we assume that the multiplicity of each eigenvalue is one , so @xmath127 .",
    "+ the functional coefficient @xmath45 is determined by the equation @xmath128 , with @xmath129 the cross - covariance operator of @xmath28 and @xmath7 , @xmath130}}={\\mathbb{e}{\\left[}{\\left<{{\\bf z}},{{\\bf x}}\\right > } y{\\right]}}$ ] , @xmath123 . to ensure the existence and uniqueness of a solution to @xmath128 we require the next basic assumptions :    1 .",
    "@xmath28 and @xmath7 satisfy @xmath131 } } , { { \\bf e}}_j\\rangle^2 < \\infty$ ] .",
    "[ assump : b1 ] 2 .   @xmath132 .",
    "[ assump : b2 ]    the estimation of @xmath45 requires the inversion of @xmath133 , but since @xmath134 is a.s . a finite rank operator",
    ", its inverse does not exist .",
    "cms proposed a regularization yielding a family of continuous estimators for @xmath135 .",
    "we employ the one from example 1 in cms , which provides an empirical finite rank inverse of @xmath136 denoted @xmath137 ( we use @xmath138 for the population version ) . consider a sequence of thresholds @xmath139 , @xmath140 , with @xmath141 .",
    "then : _ i _ ) compute the functional principal components ( fpc ) of @xmath134 , _",
    "i.e. _ , calculate its eigenvalues @xmath142 and eigenfunctions @xmath143 ; _ ii _ ) define the sequences @xmath144 , with @xmath145 and @xmath146 for @xmath147 , and set @xmath148 _ iii _ ) compute @xmath137 ( respectively @xmath138 ) as the finite rank operator with the same eigenfunctions as @xmath134 ( resp .",
    "@xmath136 ) and associated eigenvalues equal to @xmath149 ( resp .",
    "@xmath150 ) if @xmath151 and @xmath152 otherwise .",
    "the regularized estimator of @xmath45 is @xmath153 note that ( [ eq.estimrho ] ) is not readily computable in practise , since @xmath154 is usually unknown ( and hence @xmath155 ) . as in cms , we consider the ( random ) finite rank @xmath156 as a replacement in practise for the deterministic @xmath155 .",
    "as seen in lemma [ prop.tn ] , @xmath157{\\rightarrow}1 $ ] , hence the estimator ( [ eq.estimrho ] ) has the same asymptotic behaviour with either @xmath155 or @xmath158 .",
    "therefore , we consider @xmath155 in ( [ eq.estimrho ] ) due to the convenient probabilistic tractability . + the following assumptions allow to obtain meaningful asymptotic convergences :    1 .",
    "@xmath159 } } < \\infty$].[assump : c1 ] 2 .",
    "@xmath160.[assump : c2 ] 3 .   for @xmath161 large , @xmath162 with @xmath163 a convex positive function.[assump : c3 ] 4 .",
    "@xmath164.[assump : c4 ] 5 .",
    "@xmath165.[assump : c5 ] 6 .",
    "@xmath166 , \\mathbb{e}\\big[|\\xi_j|^5\\big]\\big ) \\right\\}\\leq",
    "m < \\infty$ ] , for @xmath167.[assump : c6 ] 7 .",
    "@xmath168.[assump : c7 ]    a brief summary of these assumptions is given as follows .",
    "[ assump : c1 ] is standard to obtain asymptotic distributions , allows decomposition ( [ eq.kar_loeve ] ) and implies @xmath86}}<\\infty$ ] , required in theorem 1.1 of @xcite .",
    "[ assump : c2 ] and [ assump : c3 ] are a.1 and a.2 in cms .",
    "[ assump : c4 ] is very similar to one assumption in the second part of theorem 2 in cms .",
    "[ assump : c5 ] is the minimum requirement to control @xmath169 when lemma 7 in cms is used to prove lemma [ lemm.tn3 ] .",
    "[ assump : c6 ] is a reinforcement of a.3 in cms , where only fourth order moments are used .",
    "the reason is because we handle inner products of @xmath44 times a non independent r.v . , while in cms the r.v .",
    "is not used to estimate @xmath45 .",
    "[ assump : c7 ] is useful , mainly ( but also see the final part of lemma [ lemmtn2.yn ] ) to control the behaviour of @xmath155 .",
    "we show this fact in proposition [ prop.1 ] , with a conclusion very close to the assumption ( 8) in cms and coinciding with one of the conditions of their theorem 3 if @xmath170 ( the term @xmath171 is defined in ( [ eq.definiciones ] ) ) . finally , we point out that in cms the assumptions aim to control the behaviour of @xmath155 while here we have targeted to control the threshold @xmath172 , as this can be modified by the statistician",
    ".      corollary [ coro : single ] gives the weak convergence of @xmath173 .",
    "we analyse now the pointwise behaviour of @xmath174 and @xmath175 for a fixed @xmath176 .",
    "we will show that @xmath177 and that the rate of @xmath175 depends on the key normalizing sequence @xmath178 , where @xmath179}}.\\ ] ]    [ theo : distpuntual ] under @xmath92 and [ assump : b1][assump : c7 ] , for a fixed @xmath176 , it happens that :    1 .",
    "@xmath180.[theo : distpuntual : a ] 2 .   if @xmath181 , then with @xmath182 in ( [ ec.def.stat ] ) the asymptotic distribution of @xmath183 is the one of @xmath184 .",
    "[ theo : distpuntual : b ] 3 .   if @xmath170 , then with @xmath185 in ( [ ec.def.stat ] ) the asymptotic distribution of @xmath183 is the one of @xmath186 .",
    "[ theo : distpuntual : c ]    the behaviour of the sequence @xmath178 , indexed by @xmath187 and with arbitrary @xmath71 and @xmath188 , is crucial for the convergence of @xmath189 . since @xmath178 is non - decreasing , it has always a limit ( finite or infinite ) .",
    "its asymptotic behaviour is described next .",
    "[ prop.tnx ] the sequence @xmath178 has asymptotic orders between @xmath190 and @xmath191 .",
    "in addition , if @xmath28 is gaussian and satisfies [ assump : c1 ] , then @xmath192 } } < \\infty$ ] and @xmath193 .      the result given in theorem",
    "[ theo : distpuntual ] holds for every @xmath49 .",
    "for the case [ theo : distpuntual : c ] of theorem [ theo : distpuntual ] ( where the estimation of @xmath45 is not dominant ) and under an additional assumption , the result can be generalized to functional weak convergence .",
    "[ theo : tighness ] under @xmath92 , [ assump : b1][assump : c7 ] and [ theo : distpuntual : c ] in theorem [ theo : distpuntual ] , it happens that :    1 .",
    "the finite dimensional distributions of @xmath189 converge to a multivariate gaussian with covariance function[theo : tighness : a ] @xmath194 , where @xmath195}}{\\left<\\mathbf{e}_{t,{{\\bf h}}},\\gamma^{-1}{{\\bf u}}\\right>}\\,\\mathrm{d}p_{{\\bf x}}({{\\bf u}}),\\\\ v(s , t):=&\\,\\int { \\mathbb{v}\\mathrm{ar}{\\left[}y|{{\\bf x}}={{\\bf u}}{\\right]}}{\\left<\\mathbf{e}_{s,{{\\bf h}}},\\gamma^{-1}{{\\bf u}}\\right>}{\\left<\\mathbf{e}_{t,{{\\bf h}}},\\gamma^{-1}{{\\bf u}}\\right>}\\,\\mathrm{d}p_{{\\bf x}}({{\\bf u}}).\\end{aligned}\\ ] ] 2 .   if @xmath196={\\mathcal{o}}(n^{-2})$ ] , then @xmath197 in @xmath67 , being @xmath198 a gaussian process with zero mean and covariance function @xmath199 .",
    "[ theo : tighness : b ]    according to theorem 1 in cms , it is impossible for @xmath200 to converge to a non - degenerate random element in the topology of .",
    "in order to circumvent this issue , we make the assumption @xmath196={\\mathcal{o}}(n^{-2})$ ] which implies @xmath201 , thus a finite - dimensional parametric convergence rate for @xmath202 . in practice",
    "this means that @xmath45 lives in a finite dimensional subspace of .",
    "the next result gives the convergence of the kolmogorov - smirnov ( ks ) and cramr - von mises ( cvm ) statistics for testing the flm .",
    "[ coro : kscvm ] under the assumptions in theorem [ theo : tighness ] and @xmath196={\\mathcal{o}}(n^{-2})$ ] , if @xmath203 and @xmath204 , then @xmath205 and corollary [ coro : kscvm ] is to consider a deterministic discretization of the statistics , for which the convergence in law is trivial from [ theo : tighness : a ] .",
    "for example , if @xmath206 for a grid @xmath207 , then @xmath208 , where @xmath209 , @xmath210 .",
    "the major advantage to test @xmath92 over @xmath22 is that in @xmath92 the conditioning r.v .",
    "the potential drawbacks of this universal method are a possible loss of power and that the outcome of the test may vary for different projections .",
    "both inconveniences can be alleviated by sampling several directions @xmath211 , testing the projected hypotheses @xmath212 and selecting an appropriate way to mix the resulting @xmath1-values .",
    "for example , by the fdr method proposed in @xcite ( see section 2.2.2 of @xcite ) it is possible to control the final rejection rate to be _ at most _",
    "@xmath213 under @xmath22 .",
    "the procedure is described in the following generic algorithm .",
    "[ algo : general ] let @xmath214 denote a test for checking @xmath215 with @xmath52 chosen by a non - degenerate gaussian measure @xmath77 on @xmath38 .    1 .   for @xmath216 , set by @xmath217 the @xmath1-value of @xmath218 obtained with the test @xmath214 .",
    "2 .   set the final @xmath1-value of @xmath22 as @xmath219 , where @xmath220 .",
    "the calibration of the test statistic for @xmath215 is done by a wild bootstrap resampling .",
    "the next algorithm states the steps for testing the flm .",
    "the particular case of the simple null hypothesis corresponds to @xmath221 , so its calibration corresponds to setting @xmath222 in the algorithm .",
    "[ algo : boot ] let @xmath223 be a random sample from ( [ funcmod ] ) . to test @xmath224",
    "proceed as follows :    1 .",
    "estimate @xmath45 by fpc for a given @xmath158 and obtain @xmath225.[algo : boot : i ] 2 .",
    "compute @xmath226 with @xmath227 either @xmath228 or @xmath229.[algo : boot : ii ] 3 .   _ bootstrap resampling_. for @xmath230 , do:[algo : boot : iii ] 1 .   draw binary i.i.d .",
    "r.v.s @xmath231 such that @xmath232 and @xmath233.[algo : boot : iii : a ] 2 .",
    "set @xmath234 from the bootstrap residuals @xmath235.[algo : boot : iii : b ] 3 .",
    "estimate @xmath236 from @xmath237 by fpc using the same @xmath158 of [ algo : boot : i ] .",
    "[ algo : boot : iii : c ] 4 .",
    "obtain the estimated bootstrap residuals @xmath238.[algo : boot : iii : d ] 5 .",
    "compute @xmath239.[algo : boot : iii : e ] 4 .",
    "approximate the @xmath1-value by @xmath240.[algo : boot : iv ]    the choice of an adequate @xmath158 for the estimation of @xmath45 can be done in a data - driven way , for example by the corrected schwartz information criterion @xcite , denoted by sicc .",
    "besides , steps [ algo : boot : iii : c ] and [ algo : boot : iii : d ] can be easily computed using the properties of the linear model , see section 3.3 of @xcite .",
    "+ the bootstrap process we are considering is given by ( we consider @xmath241 ) : @xmath242 which is estimating the distribution of @xmath243 the bootstrap consistency could be obtained as an adaptation of : lemma a.1 of @xcite for the first term of @xmath244 ; lemma a.2 of the same paper for the second term , using the decomposition of @xmath245 given in ( 11 ) of cms .",
    "+ the drawing of the random directions is clearly influential in the power of the test . for example , in the extreme case where the projections were orthogonal to the data , that is @xmath246 , then @xmath247 and @xmath248 under @xmath22 .",
    "therefore , algorithm [ algo : boot ] would fail to calibrate the level of the test and potentially yield spurious results due to numerical inaccuracies in @xmath249 .",
    "a data - driven compromise to avoid drawing projections in subspaces _ almost _ orthogonal to the data is the following : _ i _ ) compute the fpc of @xmath250 , _",
    "i.e. _ , the eigenpairs @xmath251 ; _ ii _ ) choose @xmath252 for a variance threshold @xmath253 , e.g. @xmath254 ; _ iii _ ) generate the data - driven gaussian process @xmath255 , with @xmath256 and @xmath257 the sample variance of the scores in the @xmath161-th fpc .",
    "formally , the gaussian measure @xmath77 associated to @xmath258 does not respect the assumptions in theorem [ th : basic ] , since it is degenerate ( but recall that @xmath77 does not have to be independent from @xmath28 ) .",
    "a non - degenerate gaussian process can be obtained as @xmath259 , with @xmath260 a gaussian process tightly concentrated around zero , albeit employing @xmath258 or @xmath259 has negligible effects in practise .",
    "( black , right scale ) and underlying processes ( grey , left scale ) for the nine different scenarios , labelled s1 to s9 .",
    "each graph contains a sample of @xmath261 realizations of the functional covariate @xmath28 and @xmath202 ( red ) with @xmath158 selected by sicc.[fig : sce ] ]",
    "we illustrate the finite sample performance of the cvm and ks goodness - of - fit tests implemented using algorithms [ algo : general ] and [ algo : boot ] for the composite hypothesis . in order to examine the possible effects of different functional coefficients @xmath45 and underlying processes for @xmath28 , we considered nine possible scenarios combining both factors .",
    "the detailed description of these scenarios is given in the supplement , while a coarse - grained graphical idea can be obtained from figure [ fig : sce ] .",
    "+ the different data generating processes are encoded as follows . for the @xmath16-th simulation scenario s@xmath16 , with functional coefficient @xmath262 ,",
    "the deviation from @xmath22 is measured by a parameter @xmath263 , with @xmath264 and @xmath265 for @xmath266 .",
    "then , with @xmath267 we denote the data generation from @xmath268 where the deviations from the linear model are constructed by including the non - linear terms @xmath269 , @xmath270 and @xmath271 .",
    "the error @xmath9 is distributed as a @xmath272 , where @xmath273 was chosen such that , under @xmath22 , @xmath274}}}{{\\mathbb{v}\\mathrm{ar}{\\left[}{\\left<{{\\bf x}},{\\boldsymbol\\rho}\\right>}{\\right]}}+\\sigma^2}=0.95 $ ] .",
    "@xmath158 is chosen automatically by sicc .    , @xmath275 , depending on the number of projections @xmath276 and for sample sizes @xmath277 ( from left to right ) .",
    "the empirical sizes associated to the significance levels @xmath278 are coded in red , purple and orange , respectively",
    ". dashed thick lines represent the asymptotic @xmath279 confidence interval for the proportion @xmath213 obtained from @xmath280 replicates .",
    "[ fig : sizeproj],title=\"fig:\",scaledwidth=32.0% ] , @xmath275 , depending on the number of projections @xmath276 and for sample sizes @xmath277 ( from left to right ) . the empirical sizes associated to the significance levels @xmath278 are coded in red , purple and orange , respectively",
    ". dashed thick lines represent the asymptotic @xmath279 confidence interval for the proportion @xmath213 obtained from @xmath280 replicates .",
    "[ fig : sizeproj],title=\"fig:\",scaledwidth=32.0% ] , @xmath275 , depending on the number of projections @xmath276 and for sample sizes @xmath277 ( from left to right ) .",
    "the empirical sizes associated to the significance levels @xmath278 are coded in red , purple and orange , respectively",
    ". dashed thick lines represent the asymptotic @xmath279 confidence interval for the proportion @xmath213 obtained from @xmath280 replicates .",
    "[ fig : sizeproj],title=\"fig:\",scaledwidth=32.0% ] , @xmath275 , depending on the number of projections @xmath276 and for sample sizes @xmath277 ( from left to right ) .",
    "the empirical sizes associated to the significance levels @xmath278 are coded in red , purple and orange , respectively",
    ". dashed thick lines represent the asymptotic @xmath279 confidence interval for the proportion @xmath213 obtained from @xmath280 replicates .",
    "[ fig : sizeproj],title=\"fig:\",scaledwidth=32.0% ] , @xmath275 , depending on the number of projections @xmath276 and for sample sizes @xmath277 ( from left to right ) .",
    "the empirical sizes associated to the significance levels @xmath278 are coded in red , purple and orange , respectively",
    ". dashed thick lines represent the asymptotic @xmath279 confidence interval for the proportion @xmath213 obtained from @xmath280 replicates .",
    "[ fig : sizeproj],title=\"fig:\",scaledwidth=32.0% ] , @xmath275 , depending on the number of projections @xmath276 and for sample sizes @xmath277 ( from left to right ) .",
    "the empirical sizes associated to the significance levels @xmath278 are coded in red , purple and orange , respectively .",
    "dashed thick lines represent the asymptotic @xmath279 confidence interval for the proportion @xmath213 obtained from @xmath280 replicates .",
    "[ fig : sizeproj],title=\"fig:\",scaledwidth=32.0% ]    we explore first the dependence of the tests with respect to the number of projections @xmath14 , which are obtained from the data - driven gaussian processes described in section [ sec : testing ] ( see the supplement for other data generating processes ) .",
    "figure [ fig : sizeproj ] shows the empirical level for each scenario , based on @xmath281 monte carlo trials and @xmath282 bootstrap replicates .",
    "there is a clear l - shaped pattern of the empirical rejection rate curves , which is produced by the conservativeness of the fdr correction  which under @xmath22 ensures that the rejection rate is _ at most _",
    " on dealing with the highly - dependent projected tests . for small @xmath14 s ,",
    "both tests calibrate correctly the three levels for different sample sizes , with the exception of @xmath283 and @xmath284 , for which the tests have a significant over - rejection of the null . for moderate to large @xmath14 s , the empirical rejection rates decrease and stabilize below @xmath213 , resulting in a violation of the confidence intervals in a significant number of times , specially for @xmath284 . figure [ fig : powproj ]",
    "shows that the empirical powers with respect to @xmath14 are almost constant or have mild decrements , except for certain bumps at lower values of @xmath14 that provide a power gain . both facts point towards choosing the number of projections @xmath14 to be relatively small ,",
    "@xmath285 $ ] , in order to make a reasonable compromise between correct calibration and power .",
    "in addition to the computational expediency that a small @xmath14 yields , it also avoids requiring a large @xmath286 to estimate properly the fdr @xmath1-values , provided that the fdr correction requires an finer precision in the discretization of the @xmath1-values for larger @xmath14 ( see supplement ) .    , @xmath275 , depending on the number of projections @xmath276 .",
    "odd columns correspond to the deviation index @xmath287 , while even account for @xmath288 .",
    "the significance level is @xmath289 and the sample sizes are @xmath277 ( rows , from up to down ) . [",
    "fig : powproj],title=\"fig:\",scaledwidth=24.0% ] , @xmath275 , depending on the number of projections @xmath276 .",
    "odd columns correspond to the deviation index @xmath287 , while even account for @xmath288 .",
    "the significance level is @xmath289 and the sample sizes are @xmath277 ( rows , from up to down ) . [",
    "fig : powproj],title=\"fig:\",scaledwidth=24.0% ] , @xmath275 , depending on the number of projections @xmath276 .",
    "odd columns correspond to the deviation index @xmath287 , while even account for @xmath288 .",
    "the significance level is @xmath289 and the sample sizes are @xmath277 ( rows , from up to down ) . [",
    "fig : powproj],title=\"fig:\",scaledwidth=24.0% ] , @xmath275 , depending on the number of projections @xmath276 .",
    "odd columns correspond to the deviation index @xmath287 , while even account for @xmath288 .",
    "the significance level is @xmath289 and the sample sizes are @xmath277 ( rows , from up to down ) .",
    "[ fig : powproj],title=\"fig:\",scaledwidth=24.0% ] + , @xmath275 , depending on the number of projections @xmath276 .",
    "odd columns correspond to the deviation index @xmath287 , while even account for @xmath288 .",
    "the significance level is @xmath289 and the sample sizes are @xmath277 ( rows , from up to down ) .",
    "[ fig : powproj],title=\"fig:\",scaledwidth=24.0% ] , @xmath275 , depending on the number of projections @xmath276 .",
    "odd columns correspond to the deviation index @xmath287 , while even account for @xmath288 .",
    "the significance level is @xmath289 and the sample sizes are @xmath277 ( rows , from up to down ) .",
    "[ fig : powproj],title=\"fig:\",scaledwidth=24.0% ] , @xmath275 , depending on the number of projections @xmath276 .",
    "odd columns correspond to the deviation index @xmath287 , while even account for @xmath288 .",
    "the significance level is @xmath289 and the sample sizes are @xmath277 ( rows , from up to down ) . [",
    "fig : powproj],title=\"fig:\",scaledwidth=24.0% ] , @xmath275 , depending on the number of projections @xmath276 .",
    "odd columns correspond to the deviation index @xmath287 , while even account for @xmath288 .",
    "the significance level is @xmath289 and the sample sizes are @xmath277 ( rows , from up to down ) .",
    "[ fig : powproj],title=\"fig:\",scaledwidth=24.0% ] + , @xmath275 , depending on the number of projections @xmath276 .",
    "odd columns correspond to the deviation index @xmath287 , while even account for @xmath288 .",
    "the significance level is @xmath289 and the sample sizes are @xmath277 ( rows , from up to down ) . [",
    "fig : powproj],title=\"fig:\",scaledwidth=24.0% ] , @xmath275 , depending on the number of projections @xmath276 .",
    "odd columns correspond to the deviation index @xmath287 , while even account for @xmath288 .",
    "the significance level is @xmath289 and the sample sizes are @xmath277 ( rows , from up to down ) . [",
    "fig : powproj],title=\"fig:\",scaledwidth=24.0% ] , @xmath275 , depending on the number of projections @xmath276 .",
    "odd columns correspond to the deviation index @xmath287 , while even account for @xmath288 .",
    "the significance level is @xmath289 and the sample sizes are @xmath277 ( rows , from up to down ) . [",
    "fig : powproj],title=\"fig:\",scaledwidth=24.0% ] , @xmath275 , depending on the number of projections @xmath276 .",
    "odd columns correspond to the deviation index @xmath287 , while even account for @xmath288 .",
    "the significance level is @xmath289 and the sample sizes are @xmath277 ( rows , from up to down ) . [ fig : powproj],title=\"fig:\",scaledwidth=24.0% ]    the tests based on the ks and cvm norms are compared with the test presented in @xcite ( denoted by pcvm ) , available in the ` r ` package ` fda.usc ` @xcite , and whose test statistic can be regarded as the average of projected cvm statistics .",
    "the test was run with the same fpc estimation used in the new tests , the same number of components @xmath158 and @xmath290 .",
    "table [ tab : results ] presents the empirical rejection rates of the different simulation scenarios with @xmath291 for ks and cvm tests .",
    "the results show two consistent patterns .",
    "first , the cvm test consistently dominates over the ks test , with only two exceptions : @xmath292 with @xmath293 , and @xmath294 with @xmath283 ( see supplement for the latter ) .",
    "second , pcvm tends to have a larger power than cvm for most of the situations , specially for small sample sizes and mild deviations . as an illustration , for @xmath283 the average relative loss in the empirical power for cvm@xmath295 with respect",
    "to pcvm is @xmath296 ( @xmath287 ) and @xmath297 ( @xmath288 ) . for @xmath293 , the losses drop to @xmath298 and @xmath299 , and for @xmath300 , to @xmath301 and @xmath302 .",
    "the drop in performance for cvm with respect to pcvm is expected due to the construction of cvm , which opts for exploring a set of random directions instead of averaging uniformly distributed finite - dimensional directions , as pcvm does .",
    "this also yields one the strongest points of the cvm test , which is its relatively short running times , specially for large @xmath0 .",
    "not surprisingly , the number of evaluations performed for computing the statistic is @xmath54 , a notable reduction from pcvm s @xmath303 .",
    "also , the memory requirement for cvm is @xmath54 , instead of pcvm s @xmath304 .",
    "the running times in figure [ fig.times ] evidence this improvement .",
    ".empirical sizes and powers of the cvm , ks and pcvm tests with @xmath289 , sample sizes @xmath305 and estimation of @xmath45 by data - driven fpc ( @xmath158 chosen by sicc ) .",
    "ks and cvm tests are shown with @xmath306 , @xmath307 and @xmath308 projections .",
    "[ tab : results ] [ cols=\"<,^,^,^,^,^,^,^,^,^,^,^,^,^,^ \" , ]      the goodness - of - fit tests depend clearly on the way random directions @xmath52 are chosen . in order to explore its practical influence ,",
    "we replicated the results in section [ sec : simu ] for two different processes : the first , denoted ( _ ii _ ) , is based on the data - driven process described in section [ sec : testing ]  which is referred as ( _ i _ )  but with constant coefficients @xmath309 ; the second , ( _ iii _ ) , is an ornstein - uhlenbeck process with @xmath310 and @xmath311 that is completely independent from the sample .",
    "process ( _ ii _ ) generates more noisy random directions , since all the fpcs of the sample are equally weighted .",
    "+ figures [ fig : sizeproj : projs ] and [ fig : powpro : projsj ] show the empirical levels and powers of the tests based on processes ( _ i _ ) , ( _ ii _ ) and ( _ iii _ ) , for @xmath293 . relatively minor changes can be observed between ( _ i _ ) and ( _ iii _ ) , with the main features described in section [ sec : simu ] being consistent : l - shaped patterns in the size curves , mild decrements for the power curves , occasional bumps yielding power gains , and domination of cvm over ks .",
    "the results for both processes show no main changes , both indicating that @xmath312 $ ] is a reasonable choice with respect to size and power . the big picture for ( _ ii",
    "_ ) is similar , albeit with more spread and variable level curves , and power curves dominated by the ones of ( _ i _ ) and ( _ iii _ ) .    , @xmath275 , depending on the number of projections @xmath276 . from left to right , columns represent the data generating processes ( _ i _ ) , ( _ ii _ ) and ( _ iii _ ) .",
    "the sample size is @xmath293 .",
    "the empirical sizes associated to the significance levels @xmath278 are coded in red , purple and orange , respectively",
    ". dashed thick lines represent the asymptotic @xmath279 confidence interval for the proportion @xmath213 obtained from @xmath280 replicates.[fig : sizeproj : projs],title=\"fig:\",scaledwidth=32.0% ] , @xmath275 , depending on the number of projections @xmath276 . from left to right , columns represent the data generating processes ( _ i _ ) , ( _ ii _ ) and ( _ iii _ ) .",
    "the sample size is @xmath293 .",
    "the empirical sizes associated to the significance levels @xmath278 are coded in red , purple and orange , respectively .",
    "dashed thick lines represent the asymptotic @xmath279 confidence interval for the proportion @xmath213 obtained from @xmath280 replicates.[fig : sizeproj : projs],title=\"fig:\",scaledwidth=32.0% ] , @xmath275 , depending on the number of projections @xmath276 . from left to right , columns represent the data generating processes ( _ i _ ) , ( _ ii _ ) and ( _ iii _ ) .",
    "the sample size is @xmath293 .",
    "the empirical sizes associated to the significance levels @xmath278 are coded in red , purple and orange , respectively",
    ". dashed thick lines represent the asymptotic @xmath279 confidence interval for the proportion @xmath213 obtained from @xmath280 replicates.[fig : sizeproj : projs],title=\"fig:\",scaledwidth=32.0% ] + , @xmath275 , depending on the number of projections @xmath276 . from left to right , columns represent the data generating processes ( _ i _ ) , ( _ ii _ ) and ( _ iii _ ) . the sample size is @xmath293 .",
    "the empirical sizes associated to the significance levels @xmath278 are coded in red , purple and orange , respectively .",
    "dashed thick lines represent the asymptotic @xmath279 confidence interval for the proportion @xmath213 obtained from @xmath280 replicates.[fig : sizeproj : projs],title=\"fig:\",scaledwidth=32.0% ] , @xmath275 , depending on the number of projections @xmath276 . from left to right , columns represent the data generating processes ( _ i _ ) , ( _ ii _ ) and ( _ iii _ ) .",
    "the sample size is @xmath293 .",
    "the empirical sizes associated to the significance levels @xmath278 are coded in red , purple and orange , respectively",
    ". dashed thick lines represent the asymptotic @xmath279 confidence interval for the proportion @xmath213 obtained from @xmath280 replicates.[fig : sizeproj : projs],title=\"fig:\",scaledwidth=32.0% ] , @xmath275 , depending on the number of projections @xmath276 . from left to right , columns represent the data generating processes ( _ i _ ) , ( _ ii _ ) and ( _ iii _ ) .",
    "the sample size is @xmath293 .",
    "the empirical sizes associated to the significance levels @xmath278 are coded in red , purple and orange , respectively",
    ". dashed thick lines represent the asymptotic @xmath279 confidence interval for the proportion @xmath213 obtained from @xmath280 replicates.[fig : sizeproj : projs],title=\"fig:\",scaledwidth=32.0% ]    the presented empirical results indicate that less variable random directions seem to yield a better behaviour for the tests and that the data - driving process given in section [ sec : testing ] is a sensible alternative .",
    "however , a more thorough research into the selection of the projecting process  out of the scope of the paper  is required .    , @xmath275 , depending on the number of projections @xmath276 . from left to right ,",
    "columns represent the data generating processes ( _ i _ ) , ( _ ii _ ) and ( _ iii _ ) .",
    "the significance level is @xmath289 , the sample size is @xmath293 and the deviation index is @xmath287 .",
    "[ fig : powpro : projsj],title=\"fig:\",scaledwidth=32.0% ] , @xmath275 , depending on the number of projections @xmath276 . from left",
    "to right , columns represent the data generating processes ( _ i _ ) , ( _ ii _ ) and ( _ iii _ ) .",
    "the significance level is @xmath289 , the sample size is @xmath293 and the deviation index is @xmath287 .",
    "[ fig : powpro : projsj],title=\"fig:\",scaledwidth=32.0% ] , @xmath275 , depending on the number of projections @xmath276 . from left to right , columns represent the data generating processes ( _ i _ ) , ( _ ii _ ) and ( _ iii _ ) .",
    "the significance level is @xmath289 , the sample size is @xmath293 and the deviation index is @xmath287 .",
    "[ fig : powpro : projsj],title=\"fig:\",scaledwidth=32.0% ] + , @xmath275 , depending on the number of projections @xmath276 . from left to right , columns represent the data generating processes ( _ i _ ) , ( _ ii _ ) and ( _ iii _ ) .",
    "the significance level is @xmath289 , the sample size is @xmath293 and the deviation index is @xmath287 .",
    "[ fig : powpro : projsj],title=\"fig:\",scaledwidth=32.0% ] , @xmath275 , depending on the number of projections @xmath276 . from left to right , columns represent the data generating processes ( _ i _ ) , ( _ ii _ ) and ( _ iii _ ) .",
    "the significance level is @xmath289 , the sample size is @xmath293 and the deviation index is @xmath287 .",
    "[ fig : powpro : projsj],title=\"fig:\",scaledwidth=32.0% ] , @xmath275 , depending on the number of projections @xmath276 . from left to right , columns represent the data generating processes ( _ i _ ) , ( _ ii _ ) and ( _ iii _ ) .",
    "the significance level is @xmath289 , the sample size is @xmath293 and the deviation index is @xmath287 .",
    "[ fig : powpro : projsj],title=\"fig:\",scaledwidth=32.0% ]      the fdr correction of a set of @xmath14 _ continuous _ @xmath1-values @xmath313 , arising from @xmath14 hypothesis tests , results in the fdr @xmath1-value @xmath314 . under the null hypotheses for all the tests , the level of the test that rejects if @xmath315 is @xmath213 at most @xcite .",
    "when using a resampling strategy for approximating the @xmath1-values @xmath313 , for example by considering @xmath286 bootstrap replicates , we end up with a collection of _ discrete _",
    "@xmath1-values @xmath316 .",
    "this has a notable influence in @xmath317 , resulting in an increment of the type i error that is magnified for moderate and large @xmath14 .",
    "+ under the null , @xmath318 is approximately distributed as a @xmath319 , @xmath216 . if independence between @xmath316 is assumed , then the rejection rate of @xmath315 is at least @xmath320\\approx 1-\\big(\\tfrac{b}{b+1}\\big)^k$ ] _ no matter what significance level @xmath213 is chosen_. for example , if @xmath321 and @xmath322 , under the null @xmath315 will reject at least @xmath323 of the times . for @xmath324 and @xmath290 ,",
    "the lower bound for the rejection percentage drops to @xmath325 .",
    "this simple argument illustrates the more demanding precision ( larger @xmath286 s ) required in the approximated @xmath1-values when @xmath14 grows .",
    "+ in order to gain more insights about the problematic dependence of @xmath14 and @xmath286 , we have conducted the following experiment , aimed to reproduce a comparable situation to our testing in practise .    1 .",
    "simulate @xmath14 discrete @xmath1-values independently : @xmath326 and compute @xmath317 .",
    "2 .   repeat the above steps @xmath281 times and obtain the empirical rejection rates of @xmath315 for @xmath327 .",
    "plot the rejection curves as a function of @xmath276 .",
    "repeat this five times to account for variability .",
    "3 .   repeat the above steps for different @xmath286 s .",
    "the results of the experiment , namely the empirical rejection curves as a function of @xmath14 for different @xmath213 s and @xmath286 s , are shown in the left panel of figure [ fig : fdrpvalvsdiscretization ] . a sawtooth pattern of over - rejection appears for curves with @xmath328 ( yellow and light green curves ) and values of @xmath14 larger than @xmath329 , resulting in significant violations of the confidence intervals for the proportions @xmath213 for @xmath14 s in the range @xmath330 $ ] .",
    "when @xmath286 is larger ( dark green and blue curves ) , the rejection rates remain more stable and inside the confidence intervals for @xmath14 up to @xmath331 .",
    "this highlights that , given the effect that both @xmath14 and @xmath286 have in the computation proficiency of the test , a reasonable compromise on the choice of @xmath14 and @xmath286 that respects the type i error is @xmath312 $ ] and @xmath290 .",
    "the right panel of figure [ fig : fdrpvalvsdiscretization ] shows the resulting levels if the positive correction @xmath332 is applied for avoiding null @xmath1-values .",
    "the same conclusions can be extracted , the main change being under - rejections instead of over - rejections .",
    ", as a function of @xmath14 .",
    "right : empirical levels employing a positive correction for the @xmath1-values.,title=\"fig:\",scaledwidth=45.0% ] , as a function of @xmath14 .",
    "right : empirical levels employing a positive correction for the @xmath1-values.,title=\"fig:\",scaledwidth=45.0% ]"
  ],
  "abstract_text": [
    "<S> we consider marked empirical processes , indexed by a randomly projected functional covariate , to construct goodness - of - fit tests for the functional linear model with scalar response . </S>",
    "<S> the test statistics are built from continuous functionals over the projected process , resulting in computationally efficient tests that exhibit root-@xmath0 convergence rate and circumvent the curse of dimensionality . </S>",
    "<S> the weak convergence of the process is obtained conditionally on a random direction , whilst it is proved the almost surely equivalence between the testing for significance expressed on the original and on the projected functional covariate . </S>",
    "<S> the computation of the test in practise involves the calibration by wild bootstrap resampling and the combination of several @xmath1-values arising from different projections by means of the false discovery rate method . </S>",
    "<S> the finite sample properties of the test are illustrated in a simulation study for a variety of linear models , underlying processes and alternatives . </S>",
    "<S> the software provided implements the tests and allows the replication of simulations and data applications .    </S>",
    "<S> * keywords : * empirical process ; functional data ; functional linear model ; functional principal components ; goodness - of - fit ; random projections . </S>"
  ]
}