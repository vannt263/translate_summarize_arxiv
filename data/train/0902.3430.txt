{
  "article_text": [
    "in the standard pac model @xcite and other theoretical models of learning , training and test instances are assumed to be drawn from the same distribution .",
    "this is a natural assumption since , when the training and test distributions substantially differ , there can be no hope for generalization .",
    "however , in practice , there are several crucial scenarios where the two distributions are more similar and learning can be more effective .",
    "one such scenario is that of _ domain adaptation _ , the main topic of our analysis .",
    "the problem of domain adaptation arises in a variety of applications in natural language processing @xcite , speech processing , computer vision @xcite , and many other areas",
    ". quite often , little or no labeled data is available from the _ target domain _ , but labeled data from a _ source domain _ somewhat similar to the target as well as large amounts of unlabeled data from the target domain are at one s disposal .",
    "the domain adaptation problem then consists of leveraging the source labeled and target unlabeled data to derive a hypothesis performing well on the target domain .",
    "a number of different adaptation techniques have been introduced in the past by the publications just mentioned and other similar work in the context of specific applications .",
    "for example , a standard technique used in statistical language modeling and other generative models for part - of - speech tagging or parsing is based on the maximum a posteriori adaptation which uses the source data as prior knowledge to estimate the model parameters @xcite .",
    "similar techniques and other more refined ones have been used for training maximum entropy models for language modeling or conditional models @xcite .",
    "the first theoretical analysis of the domain adaptation problem was presented by , who gave vc - dimension - based generalization bounds for adaptation in classification tasks .",
    "perhaps , the most significant contribution of this work was the definition and application of a distance between distributions , the @xmath0 distance , that is particularly relevant to the problem of domain adaptation and that can be estimated from finite samples for a finite vc dimension , as previously shown by .",
    "this work was later extended by who also gave a bound on the error rate of a hypothesis derived from a weighted combination of the source data sets for the specific case of empirical risk minimization .",
    "a theoretical study of domain adaptation was presented by , where the analysis deals with the related but distinct case of adaptation with multiple sources , and where the target is a mixture of the source distributions .",
    "this paper presents a novel theoretical and algorithmic analysis of the problem of domain adaptation .",
    "it builds on the work of and extends it in several ways .",
    "we introduce a novel distance , the _ discrepancy distance _ , that is tailored to comparing distributions in adaptation .",
    "this distance coincides with the @xmath0 distance for 0 - 1 classification , but it can be used to compare distributions for more general tasks , including regression , and with other loss functions . as already pointed out ,",
    "a crucial advantage of the @xmath0 distance is that it can be estimated from finite samples when the set of regions used has finite vc - dimension .",
    "we prove that the same holds for the discrepancy distance and in fact give data - dependent versions of that statement with sharper bounds based on the rademacher complexity .",
    "we give new generalization bounds for domain adaptation and point out some of their benefits by comparing them with previous bounds .",
    "we further combine these with the properties of the discrepancy distance to derive data - dependent rademacher complexity learning bounds",
    ". we also present a series of novel results for large classes of regularization - based algorithms , including support vector machines ( svms ) @xcite and kernel ridge regression ( krr ) @xcite .",
    "we compare the pointwise loss of the hypothesis returned by these algorithms when trained on a sample drawn from the target domain distribution , versus that of a hypothesis selected by these algorithms when training on a sample drawn from the source distribution .",
    "we show that the difference of these pointwise losses can be bounded by a term that depends directly on the empirical discrepancy distance of the source and target distributions .",
    "these learning bounds motivate the idea of replacing the empirical source distribution with another distribution with the same support but with the smallest discrepancy with respect to the target empirical distribution , which can be viewed as reweighting the loss on each labeled point .",
    "we analyze the problem of determining the distribution minimizing the discrepancy in both 0 - 1 classification and square loss regression .",
    "we show how the problem can be cast as a linear program ( lp ) for the 0 - 1 loss and derive a specific efficient combinatorial algorithm to solve it in dimension one .",
    "we also give a polynomial - time algorithm for solving this problem in the case of the square loss by proving that it can be cast as a semi - definite program ( sdp ) . finally , we report the results of preliminary experiments showing the benefits of our analysis and discrepancy minimization algorithms .    in section  [ sec : prelim ]",
    ", we describe the learning set - up for domain adaptation and introduce the notation and rademacher complexity concepts needed for the presentation of our results .",
    "section  [ sec : dist ] introduces the discrepancy distance and analyzes its properties .",
    "section  [ sec : bounds ] presents our generalization bounds and our theoretical guarantees for regularization - based algorithms .",
    "section  [ sec : alg ] describes and analyzes our discrepancy minimization algorithms .",
    "section  [ sec : exp ] reports the results of our preliminary experiments .",
    "we consider the familiar supervised learning setting where the learning algorithm receives a sample of @xmath1 labeled points @xmath2 , where @xmath3 is the input space and @xmath4 the label set , which is @xmath5 in classification and some measurable subset of @xmath6 in regression .    in the _ domain adaptation problem _ ,",
    "the training sample @xmath7 is drawn according to a _ source distribution _",
    "@xmath8 , while test points are drawn according to a _ target distribution _ @xmath9 that may somewhat differ from @xmath8 .",
    "we denote by @xmath10 the target labeling function . we shall also discuss cases where the source labeling function @xmath11 differs from the target domain labeling function @xmath12 .",
    "clearly , this dissimilarity will need to be small for adaptation to be possible .",
    "we will assume that the learner is provided with an unlabeled sample @xmath13 drawn i.i.d .  according to the target distribution @xmath9 .",
    "we denote by @xmath14 a loss function defined over pairs of labels and by @xmath15 the expected loss for any two functions @xmath16 and any distribution @xmath8 over @xmath3 : @xmath17 $ ] .",
    "the domain adaptation problem consists of selecting a hypothesis @xmath18 out of a hypothesis set @xmath19 with a small expected loss according to the target distribution @xmath9 , @xmath20 .",
    "our generalization bounds will be based on the following data - dependent measure of the complexity of a class of functions .",
    "let @xmath19 be a set of real - valued functions defined over a set @xmath3 .",
    "given a sample @xmath21 , the empirical rademacher complexity of @xmath19 is defined as follows : @xmath22.\\ ] ] the expectation is taken over @xmath23 where @xmath24s are independent uniform random variables taking values in @xmath25 .",
    "the rademacher complexity of a hypothesis set @xmath19 is defined as the expectation of @xmath26 over all samples of size @xmath1 : @xmath27.\\ ] ]    the rademacher complexity measures the ability of a class of functions to fit noise .",
    "the empirical rademacher complexity has the added advantage that it is data - dependent and can be measured from finite samples .",
    "it can lead to tighter bounds than those based on other measures of complexity such as the vc - dimension @xcite .",
    "we will denote by @xmath28 the empirical average of a hypothesis @xmath29 and by @xmath30 its expectation over a sample @xmath31 drawn according to the distribution considered .",
    "the following is a version of the rademacher complexity bounds by and . for completeness , the full proof is given in the appendix .",
    "[ th : rademacher ] let @xmath19 be a class of functions mapping @xmath32 to @xmath33 $ ] and @xmath34 a finite sample drawn i.i.d .  according to a distribution @xmath8 .",
    "then , for any @xmath35 , with probability at least @xmath36 over samples @xmath7 of size @xmath1 , the following inequality holds for all @xmath37 : @xmath38",
    "clearly , for generalization to be possible , the distribution @xmath8 and @xmath9 must not be too dissimilar , thus some measure of the similarity of these distributions will be critical in the derivation of our generalization bounds or the design of our algorithms .",
    "this section discusses this question and introduces a _ discrepancy _ distance relevant to the context of adaptation .    the @xmath39 distance yields a straightforward bound on the difference of the error of a hypothesis @xmath18 with respect to @xmath8 versus its error with respect to @xmath9 .",
    "[ prop : l_1_bound ] assume that the loss @xmath40 is bounded , @xmath41 for some @xmath42 .",
    "then , for any hypothesis @xmath37 , @xmath43    this provides us with a first adaptation bound suggesting that for small values of the @xmath39 distance between the source and target distributions , the average loss of hypothesis @xmath18 tested on the target domain is close to its average loss on the source domain .",
    "however , in general , this bound is not informative since the @xmath39 distance can be large even in favorable adaptation situations .",
    "instead , one can use a distance between distributions better suited to the learning task .",
    "consider for example the case of classification with the 0 - 1 loss .",
    "fix @xmath37 , and let @xmath44 denote the support of @xmath45 . observe that @xmath46 . a natural distance between distributions in this context is thus one based on the supremum of the right - hand side over all regions @xmath44 .",
    "since the target hypothesis @xmath47 is not known , the region @xmath44 should be taken as the support of @xmath48 for any two @xmath49 .",
    "this leads us to the following definition of a distance originally introduced by  [ pp .",
    "271 - 272 ] under the name of _ generalized kolmogorov - smirnov distance _ , later by as _ the @xmath0 distance _ , and introduced and applied to the analysis of adaptation in classification by and .",
    "let @xmath50 be a set of subsets of @xmath3 .",
    "then , the _ @xmath0-distance _ between two distributions @xmath51 and @xmath52 over @xmath3 , is defined as @xmath53    as just discussed , in 0 - 1 classification , a natural choice for @xmath54 is @xmath55 .",
    "we introduce a distance between distributions , _ discrepancy distance _ , that can be used to compare distributions for more general tasks , e.g. , regression .",
    "our choice of the terminology is partly motivated by the relationship of this notion with the discrepancy problems arising in combinatorial contexts @xcite .",
    "let @xmath19 be a set of functions mapping @xmath3 to @xmath4 and let @xmath56 define a loss function over @xmath4 .",
    "the discrepancy distance @xmath57 between two distributions @xmath51 and @xmath52 over @xmath3 is defined by @xmath58    the discrepancy distance is clearly symmetric and it is not hard to verify that it verifies the triangle inequality , regardless of the loss function used . in general , however , it does not define a _ distance _ : we may have @xmath59 for @xmath60 , even for non - trivial hypothesis sets such as that of bounded linear functions and standard continuous loss functions .",
    "note that for the 0 - 1 classification loss , the discrepancy distance coincides with the @xmath0 distance with @xmath61 .",
    "but the discrepancy distance helps us compare distributions for other losses such as @xmath62 for some @xmath63 and is more general .",
    "as shown by , an important advantage of the @xmath0 distance is that it can be estimated from finite samples when @xmath54 has finite vc - dimension .",
    "we prove that the same holds for the @xmath57 distance and in fact give data - dependent versions of that statement with sharper bounds based on the rademacher complexity .",
    "the following theorem shows that for a bounded loss function @xmath40 , the discrepancy distance @xmath57 between a distribution and its empirical distribution can be bounded in terms of the empirical rademacher complexity of the class of functions @xmath64 .",
    "in particular , when @xmath65 has finite pseudo - dimension , this implies that the discrepancy distance converges to zero as @xmath66 .",
    "assume that the loss function @xmath40 is bounded by @xmath42 .",
    "let @xmath8 be a distribution over @xmath3 and let @xmath67 denote the corresponding empirical distribution for a sample @xmath68 .",
    "then , for any @xmath35 , with probability at least @xmath69 over samples @xmath7 of size @xmath1 drawn according to @xmath8 : @xmath70    we scale the loss @xmath40 to @xmath33 $ ] by dividing by @xmath71 , and denote the new class by @xmath72 . by theorem",
    "[ th : rademacher ] applied to @xmath72 , for any @xmath35 , with probability at least @xmath69 , the following inequality holds for all @xmath49 : @xmath73 the empirical rademacher complexity has the property that @xmath74 for any hypothesis class @xmath19 and positive real number @xmath75 @xcite .",
    "thus , @xmath76 , which proves the proposition .    for the specific case of @xmath77 regression losses , the bound can be made more explicit .",
    "[ cor : dis_bound ] let @xmath19 be a hypothesis set bounded by some @xmath78 for the loss function @xmath77 : @xmath79 , for all @xmath49 .",
    "let @xmath8 be a distribution over @xmath3 and let @xmath67 denote the corresponding empirical distribution for a sample @xmath80 .",
    "then , for any @xmath35 , with probability at least @xmath36 over samples @xmath7 of size @xmath1 drawn according to @xmath8 : @xmath81    the function @xmath82 is @xmath63-lipschitz for @xmath83 $ ] : @xmath84 and @xmath85 .",
    "for @xmath86 , @xmath87 .",
    "thus , by talagrand s contraction lemma @xcite , @xmath88 is bounded by @xmath89 with @xmath90 .",
    "then , @xmath91 can be written and bounded as follows @xmath92 \\\\ { \\begin{aligned } & \\leq \\operatorname*{\\rm e}_\\sigma [ \\sup_{h } \\frac{1}{m } |\\sum_{i = 1}^m \\sigma_i h(x_i)| ] + \\operatorname*{\\rm e}_\\sigma [ \\sup_{h ' } \\frac{1}{m } |\\sum_{i = 1}^m \\sigma_i h'(x_i)| ] \\\\ & = 2 { \\widehat}{\\mathfrak{r}}_{{\\mathcal s}}(h ) , \\end{aligned}}\\end{gathered}\\ ] ] using the definition of the rademacher variables and the sub - additivity of the supremum function .",
    "this proves the inequality @xmath93 and the corollary .    a very similar proof gives the following result for classification .",
    "[ cor : dis_class ] let @xmath19 be a set of classifiers mapping @xmath3 to @xmath94 and let @xmath95 denote the 0 - 1 loss .",
    "then , with the notation of corollary  [ cor : dis_bound ] , for any @xmath35 , with probability at least @xmath36 over samples @xmath7 of size @xmath1 drawn according to @xmath8 : @xmath96    the factor of @xmath97 can in fact be reduced to @xmath98 in these corollaries when using a more favorable constant in the contraction lemma .",
    "the following corollary shows that the discrepancy distance can be estimated from finite samples .",
    "[ cor : dis_bound_emp ] let @xmath19 be a hypothesis set bounded by some @xmath78 for the loss function @xmath77 : @xmath99 , for all @xmath100 .",
    "let @xmath8 be a distribution over @xmath3 and @xmath67 the corresponding empirical distribution for a sample @xmath7 , and let @xmath9 be a distribution over @xmath3 and @xmath101 the corresponding empirical distribution for a sample @xmath13 .",
    "then , for any @xmath35 , with probability at least @xmath36 over samples @xmath7 of size @xmath1 drawn according to @xmath8 and samples @xmath13 of size @xmath102 drawn according to @xmath9 : @xmath103    by the triangle inequality , we can write @xmath104 the result then follows by the application of corollary  [ cor : dis_bound ] to @xmath105 and @xmath106 .    as with corollary  [ cor : dis_class ] , a similar result holds for the 0 - 1 loss in classification .",
    "this section presents generalization bounds for domain adaptation given in terms of the discrepancy distance just defined . in the context of adaptation ,",
    "two types of questions arise :    1 .   we may ask , as for standard generalization , how the average loss of a hypothesis on the target distribution , @xmath20 , differs from @xmath107 , its empirical error based on the empirical distribution @xmath67 ; 2 .",
    "another natural question is , given a specific learning algorithm , by how much does @xmath108 deviate from @xmath109 where @xmath110 is the hypothesis returned by the algorithm when trained on a sample drawn from @xmath8 and @xmath111 the one it would have returned by training on a sample drawn from the true target distribution @xmath9 .",
    "we will present theoretical guarantees addressing both questions .",
    "let @xmath112 and similarly let @xmath113 be a minimizer of @xmath114 .",
    "note that these minimizers may not be unique .",
    "for adaptation to succeed , it is natural to assume that the average loss @xmath115 between the best - in - class hypotheses is small . under that assumption and for a small discrepancy distance , the following theorem provides a useful bound on the error of a hypothesis with respect to the target domain .",
    "[ th : gen_bound ] assume that the loss function @xmath40 is symmetric and obeys the triangle inequality . then , for any hypothesis @xmath37 , the following holds @xmath116    fix @xmath37 . by the triangle inequality property of @xmath40 and the definition of the discrepancy @xmath117 , the following holds @xmath118    we compare ( [ eq : gen_bound ] ) with the main adaptation bound given by and : @xmath119",
    "it is very instructive to compare the two bounds .",
    "intuitively , the bound of theorem [ th : gen_bound ] has only one error term that involves the target function , while the bound of ( [ bound - old - adap ] ) has three terms involving the target function .",
    "one extreme case is when there is a single hypothesis @xmath18 in @xmath19 and a single target function @xmath47 . in this case ,",
    "theorem  [ th : gen_bound ] gives a bound of @xmath120 , while the bound supplied by ( [ bound - old - adap ] ) is @xmath121 , which is larger than @xmath122 when @xmath123 .",
    "one can even see that the bound of ( [ bound - old - adap ] ) might become vacuous for moderate values of @xmath124 and @xmath20 . while this is clearly an extreme case , an error with a factor of 3 can arise in more realistic situations , especially when the distance between the target function and the hypothesis class is significant .",
    "while in general the two bounds are incomparable , it is worthwhile to compare them using some relatively plausible assumptions .",
    "assume that the discrepancy distance between @xmath9 and @xmath8 is small and so is the average loss between @xmath125 and @xmath113 .",
    "these are natural assumptions for adaptation to be possible .",
    "then , theorem  [ th : gen_bound ] indicates that the regret @xmath126 is essentially bounded by @xmath127 , the average loss with respect to @xmath125 on @xmath8 .",
    "we now consider several special cases of interest .    1 .",
    "when @xmath128 then @xmath129 and the bound of theorem  [ th : gen_bound ] becomes @xmath130 the bound of ( [ bound - old - adap ] ) becomes @xmath131 where the right - hand side essentially includes the sum of @xmath132 errors and is always larger than the right - hand side of ( [ eq : bound1 ] ) since by the triangle inequality @xmath133 @xmath134 .",
    "2 .   when @xmath135 , the bound of theorem  [ th : gen_bound ] becomes @xmath136 which coincides with the standard generalization bound .",
    "the bound of ( [ bound - old - adap ] ) does not coincide with the standard bound and leads to : @xmath137 3 .",
    "when @xmath138 ( consistent case ) , the bound of ( [ bound - old - adap ] ) simplifies to , @xmath139 and it can also be derived using the proof of theorem [ th : gen_bound ] .    finally , clearly theorem  [ th : gen_bound ] leads to bounds based on the empirical error of @xmath18 on a sample drawn according to @xmath8 .",
    "we give the bound related to the 0 - 1 loss , others can be derived in a similar way from corollaries  [ cor : dis_bound]-[cor : dis_bound_emp ] and other similar corollaries .",
    "the result follows theorem  [ th : gen_bound ] combined with corollary  [ cor : dis_bound_emp ] , and a standard rademacher classification bound ( theorem  [ th : rademacher_classification ] ) @xcite .",
    "[ th : gen_bound_emp ] let @xmath19 be a family of functions mapping @xmath3 to @xmath94 and let the rest of the assumptions be as in corollary  [ cor : dis_bound_emp ] .",
    "then , for any hypothesis @xmath37 , with probability at least @xmath36 , the following adaptation generalization bound holds for the 0 - 1 loss : @xmath140      in this section , we first assume that the hypothesis set @xmath19 includes the target function @xmath12 .",
    "note that this does not imply that @xmath11 is in @xmath19 . even when @xmath12 and @xmath11 are restrictions to @xmath141 and @xmath142 of the same labeling function @xmath47 , we may have @xmath143 and @xmath144 and the source problem could be non - realizable .",
    "figure  [ fig : consistent ] illustrates this situation .",
    "for a fixed loss function @xmath40 , we denote by @xmath145 the empirical error of a hypothesis @xmath18 with respect to an empirical distribution @xmath67 : @xmath146 .",
    "let @xmath147 be a function defined over the hypothesis set @xmath19 .",
    "we will assume that @xmath19 is a convex subset of a vector space and that the loss function @xmath40 is convex with respect to each of its arguments .",
    "regularization - based algorithms minimize an objective of the form @xmath148 where @xmath149 is a trade - off parameter .",
    "this family of algorithms includes support vector machines ( svm ) @xcite , support vector regression ( svr ) @xcite , kernel ridge regression @xcite , and other algorithms such as those based on the relative entropy regularization @xcite .",
    "we denote by @xmath150 the bregman divergence associated to a convex function @xmath151 , @xmath152 and define @xmath153 as @xmath154 .",
    "[ lemma : stability ] let the hypothesis set @xmath19 be a vector space .",
    "assume that @xmath155 is a proper closed convex function and that @xmath155 and @xmath40 are differentiable .",
    "assume that @xmath156 admits a minimizer @xmath37 and @xmath157 a minimizer @xmath158 and that @xmath12 and @xmath11 coincide on the support of @xmath67 .",
    "then , the following bound holds , @xmath159    since @xmath160 and @xmath161 , and a bregman divergence is non - negative , the following inequality holds : @xmath162 by the definition of @xmath18 and @xmath163 as the minimizers of @xmath156 and @xmath164 , @xmath165 and @xmath166 this last inequality holds since by assumption @xmath12 is in @xmath19 .",
    "we will say that a loss function @xmath40 is _",
    "@xmath167-admissible _ when there exists @xmath168 such that for any two hypotheses @xmath49 and for all @xmath169 , and @xmath170 , @xmath171 this assumption holds for the hinge loss with @xmath172 and for the @xmath77 loss with @xmath173 when the hypothesis set and the set of output labels are bounded by some @xmath174 : @xmath175 and @xmath176 .",
    "[ th : stability ] let @xmath177 be a positive - definite symmetric kernel such that @xmath178 for all @xmath169 , and let @xmath19 be the reproducing kernel hilbert space associated to @xmath179 .",
    "assume that the loss function @xmath40 is @xmath167-admissible .",
    "let @xmath163 be the hypothesis returned by the regularization algorithm based on @xmath180 for the empirical distribution @xmath101 , and @xmath18 the one returned for the empirical distribution @xmath67 , and that and that @xmath12 and @xmath11 coincide on @xmath181 .",
    "then , for all @xmath169 , @xmath170 , @xmath182    for @xmath180 , @xmath155 is a proper closed convex function and is differentiable .",
    "we have @xmath183 , thus @xmath184 .",
    "when @xmath40 is differentiable , by lemma  [ lemma : stability ] , @xmath185 this result can also be shown directly without assuming that @xmath40 is differentiable by using the convexity of @xmath155 and the minimizing properties of @xmath18 and @xmath163 with a proof that is longer than that of lemma  [ lemma : stability ] .",
    "now , by the reproducing property of @xmath19 , for all @xmath186 , @xmath187 and by the cauchy - schwarz inequality , @xmath188 . by the @xmath167-admissibility of @xmath40 , for all @xmath169 , @xmath170 , @xmath189 which , combined with ( [ eq : delta_h ] ) , proves the statement of the theorem .",
    "theorem  [ th : stability ] provides a guarantee on the pointwise difference of the loss for @xmath163 and @xmath18 with probability one , which of course is stronger than a bound on the difference between expected losses or a probabilistic statement . the result , as well as",
    "the proof , also suggests that the discrepancy distance is the `` right '' measure of difference of distributions for this context .",
    "the theorem applies to a variety of algorithms , in particular svms combined with arbitrary pds kernels and kernel ridge regression .    in general , the functions @xmath12 and @xmath11 may not coincide on @xmath181 . for adaptation to be possible ,",
    "it is reasonable to assume however that @xmath190 this can be viewed as a condition on the proximity of the labeling functions ( the @xmath4s ) , while the discrepancy distance relates to the distributions on the input space ( the @xmath3s ) .",
    "the following result generalizes theorem  [ th : stability ] to this setting in the case of the square loss .",
    "[ th : stability2 ] under the assumptions of theorem  [ th : stability ] , but with @xmath11 and @xmath12 potentially different on @xmath181 , when @xmath40 is the square loss @xmath191 and @xmath192 , then , for all @xmath169 , @xmath170 , @xmath193    proceeding as in the proof of lemma  [ lemma : stability ] and using the definition of the square loss and the cauchy - schwarz inequality give @xmath194\\\\ & \\leq 2 { \\mathrm{disc}}_l({\\widehat}p , { \\widehat}q ) + 2 \\sqrt{\\operatorname*{\\rm e}_{{\\widehat}q } [ \\delta h(x)^2 ] \\operatorname*{\\rm e}_{{\\widehat}q}[l(f_p(x ) , f_q(x))]}\\\\ & \\leq 2 { \\mathrm{disc}}_l({\\widehat}p , { \\widehat}q ) + 2 \\kappa { \\lvert\\delta h\\rvert}_k \\delta .",
    "\\end{aligned}}\\end{gathered}\\ ] ] since @xmath180 , the inequality can be rewritten as @xmath195 solving the second - degree polynomial in @xmath196 leads to the equivalent constraint @xmath197 the result then follows by the @xmath167-admissibility of @xmath40 as in the proof of theorem  [ th : stability ] , with @xmath198 .    using the same proof schema ,",
    "similar bounds can be derived for other loss functions .",
    "when the assumption @xmath143 is relaxed , the following theorem holds .",
    "[ th : stability3 ] under the assumptions of theorem  [ th : stability ] , but with @xmath12 not necessarily in @xmath19 and @xmath11 and @xmath12 potentially different on @xmath181 , when @xmath40 is the square loss @xmath191 and @xmath199 , then , for all @xmath169 , @xmath170 , @xmath200    proceeding as in the proof of theorem  [ th : stability2 ] and using the definition of the square loss and the cauchy - schwarz inequality give @xmath201\\\\ & \\qquad + 2 \\operatorname*{\\rm e}_{{\\widehat}q } [ ( h'(x ) - h(x ) ) ( h_p^*(x ) - f_q(x)]\\\\ & \\leq 2 { \\mathrm{disc}}_l({\\widehat}p , { \\widehat}q ) + 2 \\sqrt{\\operatorname*{\\rm e}_{{\\widehat}p } [ \\delta h(x)^2 ] \\operatorname*{\\rm e}_{{\\widehat}p}[l(h_p^*(x ) , f_p(x ) ) ] } \\\\ & + 2 \\sqrt{\\operatorname*{\\rm e}_{{\\widehat}q } [ \\delta h(x)^2 ] \\operatorname*{\\rm e}_{{\\widehat}q}[l(h_p^*(x ) , f_q(x))]}\\\\ & \\leq 2 { \\mathrm{disc}}_l({\\widehat}p , { \\widehat}q ) + 2 \\kappa { \\lvert\\delta h\\rvert}_k \\delta ' . \\end{aligned}}\\end{gathered}\\ ] ] the rest of the proof is identical to that of theorem  [ th : stability2 ] .",
    "the discrepancy distance @xmath202 appeared as a critical term in several of the bounds in the last section . in particular",
    ", theorems  [ th : stability ] and [ th : stability2 ] suggest that if we could select , instead of @xmath67 , some other empirical distribution @xmath203 with a smaller empirical discrepancy @xmath204 and use that for training a regularization - based algorithm , a better guarantee would be obtained on the difference of pointwise loss between @xmath163 and @xmath18 . since @xmath163 is fixed , a sufficiently smaller discrepancy would actually lead to a hypothesis @xmath18 with pointwise loss closer to that of @xmath163 .",
    "the training sample is given and we do not have any control over the support of @xmath67 .",
    "but , we can search for the distribution @xmath203 with the minimal empirical discrepancy distance : @xmath205 where @xmath206 denotes the set of distributions with support @xmath181 .",
    "this leads to an optimization problem that we shall study in detail in the case of several loss functions .",
    "note that using @xmath203 instead of @xmath67 for training can be viewed as _ reweighting",
    "_ the cost of an error on each training point .",
    "the distribution @xmath203 can be used to emphasize some points or de - emphasize others to reduce the empirical discrepancy distance .",
    "this bears some similarity with the reweighting or _ importance weighting _",
    "ideas used in statistics and machine learning for sample bias correction techniques @xcite and other purposes . of course , the objective optimized here based on the discrepancy distance is distinct from that of previous reweighting techniques .",
    "we will denote by @xmath207 the support of @xmath67 , by @xmath208 the support of @xmath101 , and by @xmath31 their union @xmath209 , with @xmath210 and @xmath211 .    in view of the definition of the discrepancy distance , problem ( [ eq : dis_min ] ) can be written as a min - max problem : @xmath212 as with all min - max problems , the problem has a natural game theoretical interpretation .",
    "however , here , in general , we can not permute the @xmath213 and @xmath214 operators since the convexity - type assumptions of the minimax theorems do not hold .",
    "nevertheless , since the max - min value is always a lower bound for the min - max , it provides us with a lower bound on the value of the game , that is the minimal discrepancy : @xmath215 we will later make use of this inequality .",
    "let us now examine the minimization problem ( [ eq : dis_min ] ) and its algorithmic solutions in the case of classification with the 0 - 1 loss and regression with the @xmath191 loss .",
    "for the 0 - 1 loss , the problem of finding the best distribution @xmath203 can be reformulated as the following min - max program : @xmath216 where we have identified @xmath217 with the set of regions @xmath218 that are the support of an element of @xmath219 .",
    "this problem is similar to the min - max resource allocation problem that arises in task optimization @xcite . it can be rewritten as the following linear program ( lp ) : @xmath220 the number of constraints is proportional to @xmath221 but it can be reduced to a finite number by observing that two subsets @xmath222 containing the same elements of @xmath31 lead to redundant constraints , since @xmath223 thus , it suffices to keep one canonical member @xmath44 for each such equivalence class . the necessary number of constraints to be considered is proportional to @xmath224 , the shattering coefficient of order @xmath225 of the hypothesis class @xmath219 . by the sauer s lemma , this is bounded in terms of the vc - dimension of the class @xmath219 , @xmath226 , which can be bounded by @xmath227 since it is not hard to see that @xmath228 .    in cases where we can test efficiently whether there exists a consistent hypothesis in @xmath19 , e.g. , for half - spaces in @xmath229",
    ", we can generate in time @xmath230 all consistent labeling of the sample points by @xmath19 .",
    "( we remark that computing the discrepancy with the 0 - 1 loss is closely related to agnostic learning . the implications of this fact will be described in a longer version of this paper . )     + ( a ) +   + ( b )      we consider the case where @xmath231 $ ] and derive a simple algorithm for minimizing the discrepancy for 0 - 1 loss .",
    "let @xmath19 be the class of all prefixes ( i.e. , @xmath232 $ ] ) and suffixes ( i.e. , @xmath233 $ ] ) .",
    "our class of @xmath234 includes all the intervals ( i.e. , @xmath235 $ ] ) and their complements ( i.e. , @xmath236\\cup ( z_2,1]$ ] ) .",
    "we start with a general lower bound on the discrepancy .",
    "let @xmath237 denote the set of _ unlabeled regions _ , that is the set of regions @xmath44 such that @xmath238 and @xmath239 .    if @xmath44 is an unlabeled region , then @xmath240 for any @xmath203 .",
    "thus , by the max - min inequality ( [ eq : max - min ] ) , the following lower bound holds for the minimum discrepancy : @xmath241 in particular , if there is a large unlabeled region @xmath44 , we can not hope to achieve a small empirical discrepancy .",
    "in the one - dimensional case , we give a simple linear - time algorithm that does not require an lp and show that the lower bound ( [ eq : lower_bound ] ) is reached .",
    "thus , in that case , the @xmath213 and @xmath214 operators commute and the minimal discrepancy distance is precisely @xmath242 .    given our definition of @xmath19 , the unlabeled regions are open intervals , or complements of these sets , containing only points from @xmath208 with endpoints defined by elements of @xmath207 .",
    "let us denote by @xmath243 the elements of @xmath207 , by @xmath244 , @xmath245 $ ] , the number of consecutive unlabeled points to the right of @xmath246 and @xmath247 .",
    "we will make an additional technical assumption that there are no unlabeled points to the left of @xmath248 .",
    "our algorithm consists of defining the weight @xmath249 as follows : @xmath250 this requires first sorting @xmath251 and then computing @xmath244 for each @xmath246 .",
    "figure  [ fig:1d_example ] illustrates the algorithm .",
    "[ prop:1d_alg ] assume that @xmath3 consists of the set of points on the real line and @xmath19 the set of half - spaces on @xmath3 .",
    "then , for any @xmath67 and @xmath101 , @xmath252 minimizes the empirical discrepancy and can be computed in time @xmath253 .",
    "consider an interval @xmath254 $ ] that maximizes the discrepancy of @xmath203 .",
    "the case of a complement of an interval is the same , since the discrepancy of a hypothesis and its negation are identical .",
    "let @xmath255 $ ] be the subset of @xmath67 in that interval , and @xmath256 $ ] be the subset of @xmath101 in that interval .",
    "the discrepancy is @xmath257 . by our definition of @xmath203",
    ", we have that @xmath258 .",
    "let @xmath259 be the maximal point in @xmath101 which is less than @xmath246 and @xmath260 the minimal point in @xmath101 larger than @xmath261 .",
    "we have that @xmath262",
    ". therefore @xmath263 .",
    "since @xmath264 is maximal and both terms are non - negative , one of them is zero .",
    "since @xmath265 and @xmath266 , the discrepancy of @xmath203 meets the lower bound of ( [ eq : lower_bound ] ) and is thus optimal .      for the square loss ,",
    "the problem of finding the best distribution can be written as @xmath267 - \\operatorname*{\\rm e}_{{\\widehat}q'}[(h'(x ) - h(x))^2 ] \\big|.\\end{aligned}\\ ] ] if @xmath3 is a subset of @xmath268 , @xmath269 , and the hypothesis set @xmath19 is a set of bounded linear functions @xmath270 , then , the problem can be rewritten as @xmath271 - \\operatorname*{\\rm e}_{{\\widehat}q'}[(({{{\\mathbf w } } } ' - { { { \\mathbf w}}})^\\top { { { \\mathbf x}}})^2]\\big| \\nonumber\\\\ & = \\min_{{\\widehat}q ' \\in { { \\cal q } } } \\max_{\\substack{{\\lvert{{{\\mathbf w}}}\\rvert } \\leq 1\\\\{\\lvert{{{\\mathbf w}}}'\\rvert } \\leq 1 } } \\big| \\sum_{{{{\\mathbf x}}}\\in s } ( { \\widehat}p({{{\\mathbf x } } } ) - { \\widehat}q'({{{\\mathbf x}}}))[({{{\\mathbf w } } } ' - { { { \\mathbf w}}})^\\top { { { \\mathbf x}}}]^2 \\big| \\nonumber\\\\ & = \\min_{{\\widehat}q ' \\in { { \\cal q } } } \\max_{{\\lvert{{{\\mathbf u}}}\\rvert } \\leq 2 } \\big| \\sum_{{{{\\mathbf x}}}\\in s } ( { \\widehat}p({{{\\mathbf x } } } ) - { \\widehat}q'({{{\\mathbf x}}}))[{{{\\mathbf u}}}^\\top { { { \\mathbf x}}}]^2 \\big| \\nonumber\\\\ \\label{eq:34 } & = \\min_{{\\widehat}q ' \\in { { \\cal q } } } \\max_{{\\lvert{{{\\mathbf u}}}\\rvert } \\leq 2 } \\big| { { { \\mathbf u}}}^\\top \\big(\\sum_{{{{\\mathbf x}}}\\in s } ( { \\widehat}p({{{\\mathbf x } } } ) - { \\widehat}q'({{{\\mathbf x } } } ) ) { { { \\mathbf x}}}{{{\\mathbf x}}}^\\top\\big ) { { { \\mathbf u}}}\\big|.\\end{aligned}\\ ] ] we now simplify the notation and denote by @xmath272 the elements of @xmath207 , by @xmath273 the distribution weight at point @xmath274 : @xmath275 , and by @xmath276 a symmetric matrix that is an affine function of @xmath277 : @xmath278 where @xmath279 and @xmath280 .",
    "since problem ( [ eq:34 ] ) is invariant to the non - zero bound on @xmath281 , we can equivalently write it with a bound of one and in view of the notation just introduced give its equivalent form @xmath282 since @xmath283 is symmetric , @xmath284 is the maximum eigenvalue @xmath285 of @xmath283 and the problem is equivalent to the following maximum eigenvalue minimization for a symmetric matrix : @xmath286 this is a convex optimization problem since the maximum eigenvalue of a matrix is a convex function of that matrix and @xmath287 is an affine function of @xmath277 , and since @xmath277 belongs to a simplex .",
    "the problem is equivalent to the following semi - definite programming ( sdp ) problem : @xmath288 sdp problems can be solved in polynomial time using general interior point methods @xcite .",
    "thus , using the general expression of the complexity of interior point methods for sdps , the following result holds .",
    "[ prop : sdp ] assume that @xmath3 is a subset of @xmath268 and that the hypothesis set @xmath19 is a set of bounded linear functions @xmath289 .",
    "then , for any @xmath67 and @xmath101 , the discrepancy minimizing distribution @xmath203 for the square loss can be found in time @xmath290 .",
    "it is worth noting that the unconstrained version of this problem ( no constraint on @xmath277 ) and other close problems seem to have been studied by a number of optimization publications @xcite .",
    "this suggests possibly more efficient specific algorithms than general interior point methods for solving this problem in the constrained case as well .",
    "observe also that the matrices @xmath291 have a specific structure in our case , they are rank - one matrices and in many applications quite sparse , which could be further exploited to improve efficiency .",
    "this section reports the results of preliminary experiments showing the benefits of our discrepancy minimization algorithms .",
    "our results confirm that our algorithm is effective in practice and produces a distribution that reduces the empirical discrepancy distance , which allows us to train on a sample closer to the target distribution with respect to this metric .",
    "they also demonstrate the accuracy benefits of this algorithm with respect to the target domain .    [",
    "cols=\"^,^ \" , ]     figures  [ fig : ex_sdp](a)-(b ) show the application of the sdp derived in ( [ eq : sdp ] ) to determining the distribution minimizing the empirical discrepancy for ridge regression . in figure",
    "[ fig : ex_sdp](a ) , the distributions @xmath8 and @xmath9 are gaussians centered at @xmath292 and @xmath293 , both with covariance matrix @xmath294 .",
    "the target function is @xmath295 , thus the optimal linear prediction derived from @xmath8 has a negative slope , while the optimal prediction with respect to the target distribution @xmath9 in fact has a positive slope .",
    "figure  [ fig : ex_sdp](b ) shows the performance of ridge regression when the example is extended to 16-dimensions , before and after minimizing the discrepancy . in this higher - dimension setting and",
    "even with several thousand points , using ( http://sedumi.ie.lehigh.edu/ ) , our sdp problem could be solved in about 15s using a single 3ghz processor with 2 gb ram .",
    "the sdp algorithm yields distribution weights that decrease the discrepancy and assist ridge regression in selecting a more appropriate hypothesis for the target distribution .",
    "we presented an extensive theoretical and an algorithmic analysis of domain adaptation .",
    "our analysis and algorithms are widely applicable and can benefit a variety of adaptation tasks .",
    "more efficient versions of these algorithms , in some instances efficient approximations , should further extend the applicability of our techniques to large - scale adaptation problems .",
    "let @xmath296 be defined by @xmath297 .",
    "changing a point of @xmath7 affects @xmath296 by at most @xmath298 .",
    "thus , by mcdiarmid s inequality applied to @xmath296 , for any @xmath35 , with probability at least @xmath299 , the following holds for all @xmath37 : @xmath300 + \\sqrt{\\frac{\\log \\frac{2}{\\delta}}{2m}}.\\ ] ] @xmath301 $ ] can be bounded in terms of the empirical rade - macher complexity as follows : @xmath302 = \\operatorname*{\\rm e}_{{\\mathcal s}}\\big [ \\sup_{h \\in h } \\operatorname*{\\rm e}_{{{\\mathcal s}}'}[r_{{{\\mathcal s}}'}(h ) ] - r_{{\\mathcal s}}(h ) \\big]\\\\ \\begin{aligned } & = \\operatorname*{\\rm e}_{{\\mathcal s}}\\big [ \\sup_{h \\in h } \\operatorname*{\\rm e}_{{{\\mathcal s}}'}[r_{{{\\mathcal s}}'}(h ) - r_{{\\mathcal s}}(h ) ] \\big]\\\\ & \\leq \\operatorname*{\\rm e}_{{{\\mathcal s } } , { { \\mathcal s } } ' } \\big [ \\sup_{h \\in h } r_{{{\\mathcal s}}'}(h ) - r_{{\\mathcal s}}(h ) \\big]\\\\ & = \\operatorname*{\\rm e}_{{{\\mathcal s } } , { { \\mathcal s } } ' } \\big [ \\sup_{h \\in h } \\frac{1}{m } \\sum_{i = 1}^m ( h(x'_i ) - h(x_i ) ) \\big]\\\\ & = \\operatorname*{\\rm e}_{\\sigma , { { \\mathcal s } } , { { \\mathcal s } } ' } \\big [ \\sup_{h \\in h } \\frac{1}{m } \\sum_{i = 1}^m \\sigma_i ( h(x'_i ) - h(x_i ) ) \\big]\\\\ & \\leq \\operatorname*{\\rm e}_{\\sigma , { { \\mathcal s } } ' } \\big [ \\sup_{h \\in h } \\frac{1}{m } \\sum_{i = 1}^m \\sigma_i h(x'_i ) \\big ] + \\operatorname*{\\rm e}_{\\sigma , { { \\mathcal s } } } \\big [ \\sup_{h \\in h } \\frac{1}{m } \\sum_{i = 1}^m -\\sigma_i h(x_i ) \\big]\\\\ & = 2 \\operatorname*{\\rm e}_{\\sigma , { { \\mathcal s } } } \\big [ \\sup_{h \\in h } \\frac{1}{m } \\sum_{i = 1}^m \\sigma_i h(x_i ) \\big ] \\leq 2 \\operatorname*{\\rm e}_{\\sigma , { { \\mathcal s } } } \\big [ \\sup_{h \\in h } \\big| \\frac{1}{m } \\sum_{i = 1}^m \\sigma_i h(x_i ) \\big| \\big ] \\\\",
    "\\label{eq:27 } & = { \\mathfrak{r}}_m(h ) . \\end{aligned}\\end{gathered}\\ ] ] changing a point of @xmath7 affects @xmath303 by at most @xmath304 .",
    "thus , by mcdiarmid s inequality applied to @xmath303 , with probability at least @xmath305 , the following holds : @xmath306 combining this inequality with inequality  ( [ eq : mcd1 ] ) and the bound on @xmath307 $ ] above yields directly the statement of the theorem .",
    "[ th : rademacher_classification ] let @xmath19 be a family of functions mapping @xmath3 to @xmath94 and let @xmath95 denote the 0 - 1 loss . let @xmath8 be a distribution over @xmath3 .",
    "then , for any @xmath35 , with probability at least @xmath308 , the following inequality holds for all samples @xmath7 of size @xmath1 drawn according to @xmath8 : @xmath309",
    "here , we show how to generalize the results of section  [ sec : l2 ] to the high - dimensional case where @xmath19 is the reproducing kernel hilbert space associated to a positive definite symmetric ( pds ) kernel @xmath179 .",
    "[ prop : sdp_kernel ] let @xmath179 be a pds kernel and let @xmath19 denote the reproducing kernel hilbert space associated to @xmath179 . then , for any @xmath67 and @xmath101 , the problem of determining the discrepancy minimizing distribution @xmath203 for the square loss can be cast an sdp depending only on the gram matrix of the kernel function @xmath179 and solved in time @xmath310 .",
    "let @xmath311 be a feature mapping associated with @xmath179 .",
    "let @xmath312 . here , we denote by @xmath243 the elements of @xmath207 and by @xmath313 the element of @xmath208 .",
    "we also define @xmath314 for @xmath315 $ ] , and for convenience @xmath316 for @xmath317 $ ] . then , by proposition  [ prop : sdp ] , the problem of finding the optimal distribution @xmath203 is equivalent to @xmath318 where @xmath319 .",
    "let @xmath320 denote the matrix in @xmath321 whose columns are the vectors @xmath322 .",
    "then , observe that @xmath283 can be rewritten as @xmath323 where @xmath324 is the diagonal matrix @xmath325 fix @xmath277 .",
    "there exists @xmath326 such that , for all @xmath327 , @xmath328 is a positive definite symmetric matrix . for any such @xmath329 ,",
    "let @xmath330 denote @xmath331 since @xmath332 is positive definite , there exists a diagonal matrix @xmath333 such that @xmath334 .",
    "thus , we can write @xmath330 as @xmath335 with @xmath336 . @xmath337 and @xmath338 have the same characteristic polynomial modulo multiplication by @xmath339 .",
    "thus , since @xmath340 , the gram matrix of kernel @xmath179 for the sample @xmath31 , @xmath330 has the same same characteristic polynomial modulo multiplication by @xmath339 as @xmath341 now , @xmath342 can be rewritten as @xmath343 with @xmath344 .",
    "using the fact that @xmath345 and @xmath346 have the same characteristic polynomial , this shows that @xmath330 has the same characteristic polynomial modulo multiplication by @xmath347 as @xmath348 thus , assuming without loss of generality that @xmath349 , the following equality between polynomials in @xmath3 holds for all @xmath350 : @xmath351 both determinants are also polynomials in @xmath329 .",
    "thus , for every fixed value of @xmath3 , this is an equality between two polynomials in @xmath329 for all @xmath327 .",
    "thus , the equality holds for all @xmath329 , in particular for @xmath352 , which implies that @xmath353 has the same non - zero eigenvalues as @xmath354 .",
    "thus , problem ( [ eq:48 ] ) is equivalent to @xmath355 let @xmath356 denote the diagonal matrix @xmath357 and for @xmath245 $ ] , let @xmath358 denote the diagonal matrix whose diagonal entries are all zero except from the @xmath359th one which equals one .",
    "then , @xmath360 with @xmath361 and @xmath362 for @xmath245 $ ] .",
    "thus , @xmath363 is an affine function of @xmath277 and problem ( [ eq:54 ] ) is a convex optimization problem that can be cast as an sdp , as described in section  [ sec : l2 ] , in terms of the gram matrix @xmath364 of the kernel function @xmath179 ."
  ],
  "abstract_text": [
    "<S> this paper addresses the general problem of domain adaptation which arises in a variety of applications where the distribution of the labeled sample available somewhat differs from that of the test data . </S>",
    "<S> building on previous work by , we introduce a novel distance between distributions , _ discrepancy distance _ , that is tailored to adaptation problems with arbitrary loss functions . </S>",
    "<S> we give rademacher complexity bounds for estimating the discrepancy distance from finite samples for different loss functions . using this distance </S>",
    "<S> , we derive novel generalization bounds for domain adaptation for a wide family of loss functions . </S>",
    "<S> we also present a series of novel adaptation bounds for large classes of regularization - based algorithms , including support vector machines and kernel ridge regression based on the empirical discrepancy . </S>",
    "<S> this motivates our analysis of the problem of minimizing the empirical discrepancy for various loss functions for which we also give novel algorithms . </S>",
    "<S> we report the results of preliminary experiments that demonstrate the benefits of our discrepancy minimization algorithms for domain adaptation . </S>"
  ]
}