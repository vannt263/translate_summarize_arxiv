{
  "article_text": [
    "neurons in the brain are connected with each other and send short electrical pulses ( action potentials or spikes ) along those connections . despite the fact that there are correlations between the type of connections and the type of neurons @xcite , it is fair to say that neurons fall essentially into two classes , excitatory and inhibitory , and that the connectivity in a local population of several thousand cortical neurons is close to random .",
    "networks with fixed random connectivity can , in principle , contain loops of varying size , which could sustain the flow of transient information signals over times that are long compared to the intrinsic time constants of the network elements , i.e. , the neurons . in neuroscience and related fields , elementary considerations on information flow in random networks",
    "have inspired ideas as diverse as synfire chain activity @xcite , reverberations @xcite , liquid computing @xcite , echo state machines @xcite , or computing at the edge of chaos @xcite .",
    "on the other hand , random networks have also been studied intensively by the physics community , in the context of diluted spin glasses @xcite , formal neural networks @xcite or automata @xcite and limiting cases have been identified for which exact solutions are known . in particular , in the limit of asymmetric networks with low connectivity , mean - field dynamics becomes exact @xcite .",
    "more recently these approaches have been extended to the case of random networks of spiking neurons in continuous time @xcite .    in this paper , we will compare simulations of a random network of excitatory and inhibitory neurons with the mean - field solutions valid in the low - connectivity limit and evaluate the performance of such networks on a simple information buffering task that can be seen as a minimal and necessary requirement for more complex computational tasks @xcite which a neural network might have to solve .",
    "more precisely , the task consists in reconstructing a time - dependent input @xmath0 by reading out the activity of the network at a later time @xmath1 .",
    "we will see that performance in the information buffering task is best at the phase transition that is marked by a rapid increase in both the _ macroscopic _ activity variable and the lyapunov exponent characterizing the _ microscopic _ state indicating transition to chaos .",
    "surprisingly , if the same time - dependent input @xmath2 is shared by all neurons in the network , an information readout based only on the macroscopic variable is as good as a readout that is based on the output pulses of all @xmath3 neurons .",
    "however , if the input is only given to a small group of neurons a detailed readout conveys more information than a macroscopic one suggesting that loops in the network connectivity might indeed play a role .",
    "we consider a network of @xmath3 leaky integrate - and - fire units ( neurons ) with fixed random connectivity .",
    "80 percent of the neurons are taken as excitatory and the remaining 20 percent inhibitory .",
    "independent of the network size ( @xmath4 ) , each neuron @xmath5 in our simulation receives input from @xmath6 excitatory and @xmath7 inhibitory units ( presynaptic neurons ) , which are chosen at random amongst the @xmath8 other neurons in the network .",
    "the ensemble of neurons that are presynaptic to neuron @xmath5 is denoted by @xmath9 and the efficacy @xmath10 of a connection from a presynaptic neuron @xmath11 to a postsynaptic neuron @xmath5 is @xmath12 if @xmath13 is excitatory , and @xmath14 if @xmath13 is inhibitory .",
    "each neuron is described by a linear equation combined with a threshold . in the subthreshold regime",
    "the membrane potential follows the differential equation @xmath15 where @xmath16ms is the effective membrane time constant and @xmath17 external input .",
    "the recurrent input @xmath18 neuron @xmath5 receives from the network is @xmath19 where @xmath20 is the time neuron @xmath13 fires its @xmath21th spike and @xmath22ms is a short transmission delay . a spike from an excitatory ( inhibitory )",
    "neuron @xmath11 causes a jump in the membrane potential of neuron @xmath5 of @xmath23mv ( @xmath24mv ) .",
    "if the membrane potential @xmath25 reaches a threshold @xmath26mv , a spike of neuron @xmath5 is recorded and its membrane potential is reset to @xmath27 .",
    "integration restarts after an absolute refractory period of @xmath28ms .",
    "the external input @xmath17 can be separated into two components .",
    "first , we inject a time dependent test signal @xmath29 which is generated as follows : the total simulation time is broken into segments of duration @xmath30ms . during each segment of length @xmath31",
    "the input is kept constant . at the transition to the next segment",
    ", a new value of @xmath32 is drawn from a uniform distribution over the interval @xmath33 $ ] , i.e. the signal distribution has a standard deviation of @xmath34 . by construction the signal at time @xmath35 provides no information about the signal at @xmath36 for @xmath37 .",
    "more precisely , the autocorrelation @xmath38 of the signal has a triangular shape and is strictly zero for @xmath39 .",
    "second , the network of @xmath3 neurons is considered as part of a larger brain structure . to mimic input from excitatory ` background ' neurons that are not modelled explicitly , we assume stochastic spike arrival described by a poisson process of total rate @xmath40 . for the sake of simplicity , we assume that the efficacy @xmath41 of background spikes is identical to that of the recurrent connections within the network .",
    "we approximate the background input @xmath42 by a gaussian white noise with mean @xmath43 and standard deviation @xmath44 , where @xmath40 is the background spike arrival rate .",
    "this approximation is valid under the assumption that a neuron receives a large number of presynaptic contributions per unit time , each generating a change in the membrane potential that is relatively small compared to the firing threshold . to simplify notation we set @xmath45 and @xmath46 .",
    "the values in our simulations are @xmath47 and @xmath48 .",
    "if we replace the signal @xmath49 by gaussian white noise of zero mean and standard deviation @xmath50 a mean - field analysis of the random network defined above can be performed following @xcite .",
    "the macroscopic variable describing the activity of the network is the population rate @xmath51 . in a stationary state of asynchronous neuronal activity ,",
    "the population rate depends on the mean @xmath52 and variance @xmath53 of the total input @xmath54 via the equation @xmath55 the mean - field result is valid in the low connectivity limit @xcite . combining eqs .",
    "( [ input - mean ] ) - ( [ eq2 ] ) , we obtain a self - consistent solution of the population firing rate @xmath51 as a function of the external poisson drive @xmath40 .",
    "the population rate shows a marked increase near @xmath56hz as shown in fig .",
    "[ fig - transition ] which is in the vicinity of a first - order phase transition predicted by the mean - field theory .",
    "if we decrease the amplitude of the input signal @xmath57 the phase transition becomes more pronounced and a regime of coexistence of several solutions appears ( inset ) .     as a function of the poisson background drive resulting from eqs .",
    "( [ input - mean]-[eq2 ] ) .",
    "note the sharp transition at @xmath58hz .",
    "the network switches from an almost quiescent state to a state of sustained activity .",
    "dotted line : largest lyapunov exponent as a function of the external drive @xmath40 in a network of 800 neurons . for @xmath59hz",
    "the population rate @xmath51 is low ( quiescent state ) and the largest lyapunov exponent is negative . for a stronger drive",
    ", the exponent switches to a positive value , reflecting the chaotic behaviour of the membrane potential trajectories .",
    "solid line : signal reconstruction error for a delay of 20 ms in a network of 800 neurons .",
    "the error is minimal near the transition from the quiescent to a chaotic regime .",
    "inset : fixed points of the population rate ( eq . [ eq2 ] ) in absence of test signal ( solid line ) , and with increasing signal variance .",
    "the system exhibit a first - order phase transition if the signal is weak . ]",
    "the marked increase in the macroscopic variable @xmath51 is accompanied by a transition of the largest lyapunov exponent from negative to positive values which indicates that the microscopic dynamics becomes chaotic .",
    "the largest lyapunov exponent is defined by @xmath60 where @xmath61 is the difference between a reference trajectory of the @xmath3 variables @xmath62 and test trajectory @xmath63 with slightly different initial conditions . using standard numerical techniques @xcite ,",
    "the largest lyapunov exponent has been estimated from a large number of simulations of a test network and a reference network with identical connectivity .",
    "both networks received identical input ( same realization of the poisson input and signal input ) and the test trajectory was regularly reset close to the reference trajectory .",
    "moreover , we confirmed that not only for background spike input but also for appropriately chosen constant input ( i.e. @xmath64 , @xmath65 ) , the fixed random connectivity is sufficient to generate irregular asynchronous spiking activity @xcite with a positive largest lyapunov exponent ( data not shown ) .",
    "after having characterized the macroscopic and microscopic state of the network , we asked how performance in an information buffering task , inspired by concepts of liquid computing @xcite and echo state machines @xcite , would depend on the network state .",
    "we considered a linear readout unit with dynamics @xmath66 where the sums run over all firing times @xmath67 of all neurons in the network .",
    "@xmath68ms is a short synaptic time constant .",
    "the @xmath69 free parameters @xmath70 ( @xmath71 ) are chosen so as to minimize the signal reconstruction error @xmath72 ^ 2\\rangle /\\sigma_{\\rm sg}^2 $ ] .",
    "parameters were optimized using a first simulation ( learning set ) lasting 100 seconds ( 100000 time steps of simulation ) and were kept fixed afterwards .",
    "the performance measurements reported in this paper are then evaluated on a second simulation of 100 seconds ( test set ) .",
    "simulation results were obtained using the simulation software nest .",
    "the same time - dependent signal @xmath32 was injected into all neurons in the network and the performance evaluated in terms of the signal reconstruction error .",
    "the performance depends on the delay @xmath73 of information buffering which has to be compared with the membrane time constant ( @xmath74ms ) and the autocorrelation of the input @xmath75ms .",
    "overall the signal reconstruction error is relatively high .",
    "as expected , the signal reconstruction error increases if we increase the desired buffer duration from @xmath76ms to @xmath77ms or @xmath78ms ( fig .",
    "[ fig - delays]a ) .     as a function of the background firing rate in a network of 800 neurons for three different information buffer delays : @xmath76ms ( solid ) , @xmath77ms ( dashed ) , @xmath78ms ( dotted ) . for sufficiently long delays ,",
    "optimal performance is located near the transition between the quiescent and the chaotic state ; cf fig [ fig - transition ] .",
    "deeper in the chaotic phase the error goes back to the chance level whereas in the almost quiescent regime we can see the effects of overfitting ( @xmath79 ) , because the number of action potentials is insufficient to build an accurate model of the past events .",
    "b. comparison of the errors for different network sizes : @xmath4 neurons ( resp .",
    "dotted , dashed and solid lines ) , for a delay @xmath78ms .",
    "the solid line with filled circles corresponds to the _ macroscopic _ readout of the network of @xmath80 neurons .",
    "the location of minimal error is independent of the number of neurons and coincides with the phase transition .",
    "the vertical shift of the error curves is not significant but due to overfitting because of limited amount of data .",
    "vertical bars indicate the mean difference between errors on the data used for parameter optimisation ( training set ) and that on an independent test set for @xmath80 ( left bar ) , @xmath81 ( 2nd bar ) , @xmath82 ( 3rd bar ) and macroscopic readout ( right bar ) .",
    "a representative curve of errors on the training set for @xmath80 neurons is shown by the dot - dashed line . ]    at the same time , the optimal background firing rate @xmath40 to achieve minimal signal reconstruction error shifts towards lower values and is for @xmath78ms very close to the transition between regular and chaotic microscopic dynamics as shown in fig .",
    "[ fig - transition ] .",
    "this result is consistent with the idea of computing at the edge of chaos in cellular automata @xcite or threshold gates @xcite . also ,",
    "similar to the results in discrete - time spin networks @xcite , the information buffering performance does not significantly depend on the number @xmath3 of neurons in the network ; cf .",
    "[ fig - delays]b .",
    "differences are within the statistical variations caused by overfitting on finite data samples .",
    "given that networks states have been classified successfully by macroscopic mean - field equations @xcite , we wondered whether the performance in the above information buffering task can be completely understood in terms of _ macroscopic _ quantities . to answer this question",
    ", we compared the performance using the previous readout unit @xmath83 ( i.e. , a ` microscopic ' readout with @xmath69 free parameters , one per neuron plus an offset ) with that by a simplified readout @xmath84 with two free parameters @xmath85 and @xmath86 only , @xmath87 , i.e. , a readout that uses only the macroscopic population activity .",
    "surprisingly , for a stimulation paradigm where all neurons receive the same time - dependent signal @xmath32 the macroscopic readout performs as well as the microscopic one . in other words ,",
    "connectivity loops between _ specific _ subsets of neurons where signals could circulate for some time seem not to play a role in information buffering .",
    "this suggests that , for signals of sufficiently small amplitude , the information buffering capacity is directly related to the _ macroscopic _ linear response kernel @xmath88 of the network activity , that can , in principle , be calculated from the linearized mean - field equations of the population rate , i.e. , @xmath89 ; cf .",
    "the time constant of the kernel , and hence information buffering delays , become large in the vicinity of a phase transition .",
    "we hypothesized that signal transmission loops in our randomly connected network could manifest themselves more easily if only a small subset of neurons received the input signal .",
    "we therefore selected 20 percent of neurons at random ( group @xmath90 ) and injected an identical time dependent signal @xmath91 into all neurons @xmath92 .",
    "the remaining 80 percent of neurons ( group @xmath93 ) received no signal . in such a network consisting of two groups , signal buffering performance is indeed significantly better than in a single homogeneous group ( fig .",
    "[ fig - twogroups ] ) .    on a macroscopic scale",
    ", a network consisting of two groups @xmath90 and @xmath93 can be described by two macroscopic variables , i.e. , the population activities in groups @xmath90 and @xmath93 . in order to evaluate the information contained in the macroscopic population rates",
    ", we used a linear readout unit @xmath94 with three free parameters @xmath95 , @xmath96 and @xmath97 , characterized by the differential equation @xmath98 and proceeded as before . as we can see from fig .",
    "[ fig - twogroups ] , a readout based on the macroscopic activity of the two groups performs significantly worse than the microscopic readout .",
    "this suggests , that for the case when only a small subset of units in a random network receive an input , signal transmission loops , and hence microscopic neuronal dynamics , indeed play a role in short - term information buffering .     of the neurons only . a macroscopic readout assuming a single population ( dotted line ) performs well near the phase transition .",
    "however deeper in the chaotic phase it is outperformed by the _ microscopic _ readout ( solid line ) .",
    "a macroscopic readout based on a two - population assumption ( dashed line ) explains only part of the increased performance .",
    "the signal buffering delay for this figure is @xmath78ms . ]",
    "mean - field methods neglect correlations in the input . in random networks",
    "mean - field theory becomes asymptotically correct only in the low - connectivity limit where the probability of closed loops tends to zero @xcite .",
    "however , it is exactly these loops which could give the network the power to buffer information for times significantly longer than the intrinsic time constants of the network elements .",
    "our network is formally not in the low - connectivity limit since the number of neurons @xmath80 is small .",
    "nevertheless , we found that mean - field results can qualitatively predict the rough location of the phase transition of the macroscopic population rate .",
    "moreover , if the input signal is shared between all neurons , a macroscopic readout is sufficient to explain the network performance in an information buffering task .",
    "microscopic properties do , however , play a role if the input is only given to a subset of the network units suggesting that in this case ultra - short term information buffering in connectivity loops is indeed possible . in additional simulations we checked that the maximum delay @xmath73 for which signal reconstruction is feasible is only in the range of 20 - 50ms and hence not significantly different from the intrinsic neuronal time constants .",
    "this suggests that , without slow processes such as synaptic plasticity or neuronal adaptation a purely random network of spiking neurons is not suitable as an information buffer beyond tens of milliseconds ."
  ],
  "abstract_text": [
    "<S> in randomly connected networks of pulse - coupled elements a time - dependent input signal can be buffered over a short time . </S>",
    "<S> we studied the signal buffering properties in simulated networks as a function of the networks state , characterized by both the lyapunov exponent of the microscopic dynamics and the macroscopic activity derived from mean - field theory . </S>",
    "<S> if all network elements receive the same signal , signal buffering over delays comparable to the intrinsic time constant of the network elements can be explained by macroscopic properties and works best at the phase transition to chaos . </S>",
    "<S> however , if only 20 percent of the network units receive a common time - dependent signal , signal buffering properties improve and can no longer be attributed to the macroscopic dynamics . </S>"
  ]
}