{
  "article_text": [
    "uncertainty quantification ( uq ) is an interdisciplinary , fast - growing research area that focuses on devising mathematical techniques to tackle problems in engineering and natural sciences in which only a probabilistic description of the parameters of the governing equations is available , due to measurement errors , intrinsic non - measurability / non - predictability , or incomplete knowledge of the system of interest . in this context ,",
    "`` parameters '' is a term used in broad sense to refer to constitutive laws , forcing terms , domain shapes , boundary and initial conditions , etc .",
    "uq methods can be divided into deterministic and randomized methods .",
    "while randomized techniques , which include the monte carlo sampling method , are essentially based on random sampling and ensemble averaging , deterministic methods proceed by building a surrogate of the system s response function over the parameter space , which is then processed to obtain the desired information .",
    "typical goals include computing statistical moments ( expected value , variance , higher moments , correlations ) of some quantity of interest of the system at hand , typically functionals of the state variables ( forward problem ) , or updating the statistical description of the random parameters given some observations of the system at hand ( inverse problem ) . in any case",
    ", multiple resolutions of the governing equations are needed to explore the dependence of the state variables on the random parameters .",
    "the computational method used should therefore be carefully designed to minimize the computational effort .    in this work",
    ", we focus on the case of pdes with random data , for which both deterministic and randomized approaches have been extensively explored in recent years . as for the deterministic methods , we mention here the methods based on polynomial expansions computed either by global galerkin - type projections @xcite or collocation strategies based on sparse grids ( see e.g. @xcite ) , low - rank techniques @xcite and reduced basis methods ( see e.g. @xcite ) .",
    "all these approaches have been found to be particularly effective when applied to problems with a moderate number of random parameters ( low - dimensional probability space ) and smooth response functions .",
    "although significant effort has been expended on increasing the efficiency of such deterministic methods with respect to the number of random parameters ( see , e.g. , @xcite , the seminal work on infinite dimensional polynomial approximation of elliptic pdes with random coefficients ) , monte carlo - type approximations remain the primary choice for problems with non - smooth response functions and/or those that depend on a high number of random parameters , despite their slow convergence with respect to sample size .    a very promising methodology that builds on the classical monte carlo method and enhances its performance is offered by the so - called _ multilevel monte carlo _ ( mlmc ) .",
    "it was first proposed in @xcite for applications in parametric integration and extended to weak approximation of stochastic differential equations in @xcite , which also provided a full complexity analysis .",
    "let @xmath0 be a ( scalar ) sequence of spatial / temporal resolution levels that can be used for the numerical discretization of the pde at hand and @xmath1 be the corresponding approximations of the quantity of interest , and suppose that the final goal of the uq analysis is to compute the expected value of @xmath2 , @xmath3}}$ ] . while a classic monte carlo approach simply approximates the expected value by using an ensemble average over a sample of independent replicas of the random parameters , the mlmc method relies on the simple observation that , by linearity of expectation , @xmath4 } } \\approx { { \\ensuremath{\\mathbb{e}}\\mspace{-2mu}\\left[{f}_l\\right ] } } = { { \\ensuremath{\\mathbb{e}}\\mspace{-2mu}\\left[{f}_0\\right ] } } + \\sum_{\\ell=1}^l{{\\ensuremath{\\mathbb{e}}\\mspace{-2mu}\\left[{f}_\\ell - { f}_{\\ell-1}\\right]}},\\ ] ] and computes by independent monte carlo samplers each expectation in the sum . indeed ,",
    "if the discretization of the underlying differential model is converging with respect to the discretization level , @xmath5 , the variance of @xmath6 will be smaller and smaller as @xmath5 increases , i.e. , when the spatial / temporal resolution increases .",
    "dramatic computational saving can thus be obtained by approximating the quantities @xmath7}}$ ] with a smaller and smaller sample size , since most of the variability of @xmath2 will be captured with coarse simulations and only a few resolutions over the finest discretization levels will be performed .",
    "the mlmc estimator is therefore given by @xmath8 } } \\approx \\sum_{\\ell=0}^l \\frac{1}{m_\\ell}\\sum_{m=1}^{m_\\ell } \\left({f}_{\\ell}(\\omega_{m,\\ell } ) - { f}_{\\ell-1}(\\omega_{m,\\ell})\\right ) ,    \\quad \\text{with } { f}_{-1}(\\cdot)=0,\\ ] ] where @xmath9 are the i.i.d .",
    "replicas of the random parameters .",
    "the application of mlmc methods to uq problems involving pdes with random data has been investigated from the mathematical point of view in a number of recent publications , see e.g. @xcite .",
    "recent works @xcite have explored the possibility of replacing the monte carlo sampler on each level by other quadrature formulas such as sparse grids or quasi - monte carlo quadrature , obtaining the so - called multilevel stochastic collocation ( mlsc ) or multilevel quasi - monte carlo ( mlqcm ) methods .",
    "see also @xcite for a related approach where the multilevel monte carlo method is combined with a control variate technique .",
    "the starting point of this work is instead the so - called multi - index monte carlo method ( mimc ) , recently introduced in @xcite , that differs from the multilevel monte carlo method in that the telescoping idea presented in equations - is applied to discretizations indexed by a multi - index rather than a scalar index , thus allowing each discretization parameter to vary independently of the others .",
    "analogously to what done in @xcite in the context of stochastic collocation , here we propose to replace the monte carlo quadrature with a sparse grid quadrature at each telescopic level , obtaining in our case the multi - index stochastic collocation method ( misc ) .",
    "in other words , misc can be seen as a multi - index version of mlsc , or a stochastic collocation version of mimc . from a slightly different perspective ,",
    "misc is also closely related to the combination technique developed for the solution of ( deterministic ) pdes in @xcite ; in this work , the combination technique is used with respect to both the deterministic and stochastic variables .",
    "one key difference between the present work and @xcite is that the number of problem solves to be performed at each discretization level is not determined by balancing the spatial and stochastic components of the error ( based , e.g. , on convergence error estimates ) , but rather suitably extending the knapsack - problem approach that we employed in @xcite to derive the so - called quasi - optimal sparse grids method ( see also @xcite ) .",
    "a somewhat analogous approach was proposed in @xcite , where the number of solves per discretization level is prescribed _ a - priori _ based on a standard sparsification procedure ( we will give more details on the comparison between these different methods later on ) . in this work ,",
    "we provide a complexity analysis of misc and illustrate its performance improvements , comparing it to other methods by means of numerical examples .",
    "the remainder of this paper is organized as follows . in section [",
    "s : problem - setting ] , we introduce the problem to be solved and the approximation schemes that will be used . the multi - index stochastic collocation method is introduced in section [ s : method ] , and our main theorem detailing the complexity of misc for a particular choice of an index set is presented in section [ s : complexity ] .",
    "finally , section [ s : numerics ] presents some numerical tests , while section [ s : conclusions ] offers some conclusions and final remarks .",
    "the appendix contains the technical proof of the main theorem . throughout the rest of this work we use the following notation :    * @xmath10 denotes the set of integer numbers including zero ; * @xmath11 denotes the set of positive integer numbers , i.e. excluding zero ; * @xmath12 denotes the set of positive real numbers , @xmath13 ; * @xmath14 denotes a vector whose components are always equal to one ; * @xmath15 denotes the @xmath5-th canonical vector in @xmath16 , i.e. , @xmath17 if @xmath18 and zero otherwise ; however , for the sake of clarity , we often omit the superscript @xmath19 when obvious from the context .",
    "for instance , if @xmath20 , we will write @xmath21 instead of @xmath22 ; * given @xmath20 , @xmath23 , @xmath24 and @xmath25 ; * given @xmath20 and @xmath26 , @xmath27 denotes the vector obtained by applying @xmath28 to each component of @xmath29 , @xmath30 \\in { \\mathbb{r}}^n$ ] ; * given @xmath31 , the inequality @xmath32 holds true if and only if @xmath33 @xmath34 .",
    "* given @xmath35 and @xmath36 , @xmath37 = ( v_1,\\ldots , v_d , w_1,\\ldots , w_n ) \\in { \\mathbb{r}}^{d+n}$ ] .",
    "let @xmath38 , @xmath39 , be an open hyper - rectangular domain ( referred to hereafter as the `` physical domain '' ) and let @xmath40 be a @xmath41-dimensional random vector whose components are mutually independent and uniformly distributed random variables with support @xmath42 and probability density function @xmath43 . denoting @xmath44 ( referred to hereafter as the `` stochastic domain '' or `` parameter space '' ) and by @xmath45 the borel @xmath46-algebra over @xmath47 , @xmath48 is therefore a probability measure on @xmath47 , due to the independence of @xmath49 , and @xmath50 is a complete probability space . consider the following generic pde , together with the assumption stated next :    [ pb : strong_form_yy ] find @xmath51 such that for @xmath52-almost every @xmath53 @xmath54    [ assump : wellposed ] problem [ pb : strong_form_yy ] is well posed in some hilbert space @xmath55 for @xmath52-almost every @xmath53 .",
    "the solution of problem [ pb : strong_form_yy ] can be seen as an @xmath41-variate hilbert - space valued function @xmath56 .",
    "the random variables , @xmath49 , can represent scalar values whose exact value is unknown , or they can stem from a spectral decomposition of a random field , like a karhunen - love or fourier expansion , possibly truncated after a finite number of terms , see , e.g. , @xcite .",
    "it is also useful to introduce the bochner space @xmath57 finally , given some functional of the solution @xmath58 , @xmath59 , we denote by @xmath60 the @xmath41-variate real - valued function assigning to each realization @xmath53 the corresponding value of @xmath61 $ ] ( quantity of interest ) , i.e. , @xmath62 $ ] , and we aim at estimating its expected value , @xmath63 } } = \\int_\\gamma { f}({{{\\ensuremath{{\\boldsymbol y } } } } } ) \\rho({{{\\ensuremath{{\\boldsymbol y } } } } } ) d{{{\\ensuremath{{\\boldsymbol y}}}}}.\\ ] ]    [ example : ell ] as a motivating example , consider the following elliptic problem : find @xmath51 such that for @xmath52-almost every @xmath53 @xmath64 holds , where @xmath65 and @xmath66 denote differentiation with respect to the physical variables , @xmath67 , only , and the function @xmath68 is bounded away from @xmath69 and @xmath70 , i.e. , there exist two constants , @xmath71 , such that @xmath72 this boundedness condition guarantees that assumption [ assump : wellposed ] is satisfied , i.e. the equation is well posed for @xmath52-almost every @xmath53 , thanks to a straightforward application of the lax - milgram lemma ; moreover , the equation is well posed in @xmath73 , where @xmath55 is the classical sobolev space @xmath74 , see , e.g. , @xcite .",
    "this is the example we will focus on in section [ s : numerics ] , where we will test numerically the performance of the multi - index stochastic collocation method that we will detail in section [ s : method ] .",
    "the method that we present in the following sections can be also applied to more general problems than problem [ pb : strong_form_yy ] in which the forcing terms , boundary conditions and possibly domain shape are also modeled as uncertain ; the extension to time - dependent problems with uncertain initial conditions is also straightforward .",
    "other probability measures can also be considered ; the very relevant case in which the random variables , @xmath49 , are normally distributed is an example .    as will be clearer in a moment , the methodology we propose uses tensorized solvers for deterministic pdes .",
    "although for ease of exposition we have assumed that the spatial domain , @xmath75 , is a hyper - rectangle , it is important to remark that the methodology proposed in this work can also be applied to non hyper - rectangular domains : this can be achieved by introducing a mapping from a reference hyper - rectangle to the generic domain of interest ( with techniques such as those proposed in the context of isogeometric analysis @xcite or transfinite interpolation @xcite ) or by a domain decomposition approach @xcite if the domain can be obtained as a union of hyper - rectangles .      in practice , we can only access the value of @xmath2 via a numerical solver yielding a numerical approximation of the solution @xmath58 of problem [ pb : strong_form_yy ] , which depends on a set of @xmath76 discretization parameters , such as the mesh - size , the time - step , the tolerances of the numerical solvers , and others , which we denote by @xmath77 ; we remark that in general @xmath76 , the number of parameters , might be different from @xmath78 , the number of spatial dimensions . for each of those parameters ,",
    "we introduce a sequence of discretization levels , @xmath79 , and for each multi - index @xmath80 , we denote by @xmath81 the approximation of @xmath58 obtained from setting @xmath82 , with the implicit assumption that @xmath83 as @xmath84 for @xmath52-almost every @xmath53 ; similarly , we also write @xmath85 $ ] . for instance , we could solve the problem stated in example [ example : ell ] by a finite differences scheme with grid - sizes @xmath86 in direction @xmath87 , for some @xmath88 .",
    "the discretization of @xmath89 over the random parameter space @xmath47 will consist of a suitable linear combination of tensor interpolants over @xmath47 based on lagrangian polynomials .",
    "observe that this approach is sound only if @xmath89 is at least a continuous function over @xmath47 ( the smoother @xmath89 is , the more effective the lagrangian approximation will be ) ; for instance , for the problem stated in example [ example : ell ] , it can be shown under moderate assumptions on @xmath90 that @xmath2 and @xmath89 are @xmath91-analytic , see , e.g. , @xcite ; we will return to this point in section [ s : numerics ] .    to derive a generic tensor lagrangian interpolation of @xmath89",
    ", we first introduce the set @xmath92 of real - valued continuous functions over @xmath93 , and the subspace of polynomials of degree at most @xmath94 over @xmath93 , @xmath95 .",
    "next , we consider a sequence of univariate lagrangian interpolant operators in each dimension @xmath96 , i.e. , @xmath97 , where we refer to the value @xmath98 as the `` interpolation level '' .",
    "each interpolant is built over a set of @xmath99 collocation points , @xmath100 , where @xmath101 is a strictly increasing function , with @xmath102 and @xmath103 , that we call the `` level - to - nodes function '' ; thus , the interpolant yields a polynomial approximation , @xmath104(y_n ) = \\sum_{j=1}^{m(\\beta_n ) }                      \\left ( f(y_n^j ) \\prod_{k=1,k",
    "\\neq j}^{m(\\beta_n ) } \\frac{y_n - y_n^k}{y^j_n - y_n^k } \\right),\\ ] ] with the convention that",
    "@xmath105 = 0 \\,\\ , \\forall f \\in { \\mathcal{c}}^0(\\gamma_n)$ ] .",
    "the @xmath41-variate lagrangian interpolant can then be built by a tensorization of univariate interpolants : denote by @xmath106 the space of real - valued @xmath41-variate continuous functions over @xmath47 and by @xmath107 the subspace of polynomials of degree at most @xmath108 over @xmath93 , with @xmath109 , and consider a multi - index @xmath110 assigning the interpolation level in each direction , @xmath49 ; the multivariate interpolant can then be written as @xmath111 ( { { \\ensuremath{{\\boldsymbol y } } } } ) & = \\left ( \\mathscr{u}^{m(\\beta_1)}_1 \\otimes \\cdots \\otimes \\mathscr{u}^{m(\\beta_n)}_n \\right ) [ { f}^{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}}]({{\\ensuremath{{\\boldsymbol y}}}}).\\ ] ]    the set of collocation points needed to build the tensor interpolant @xmath112 ( { { \\ensuremath{{\\boldsymbol y}}}})$ ] is the tensor grid @xmath113 with cardinality @xmath114 .",
    "observe that the lagrangian interpolant immediately induces an @xmath41-variate quadrature formula , @xmath115 , @xmath116 = { { \\ensuremath{\\mathbb{e}}\\mspace{-2mu}\\left[{\\mathscr{u}}^{m({{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}})}[{f}^{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}}]({{\\ensuremath{{\\boldsymbol y}}}})\\right ] } } = \\sum_{j=1}^{\\ # \\mathscr{t}^{m({{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } ) } } { f}^{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}}(\\widehat{{{{\\ensuremath{{\\boldsymbol y}}}}}}_j ) \\varpi_j,\\end{aligned}\\ ] ] where @xmath117 and the quadrature weights @xmath118 are the expected values of the lagrangian polynomials centered in @xmath119 , which can be computed exactly for most of the common interpolation knots and probability measures of the random variables .",
    "it is recommended that the collocation points @xmath120 to be used in each direction are chosen according to the underlying probability measure , @xmath121 , to ensure good approximation properties of the interpolant and quadrature operators , @xmath122 and @xmath123 .",
    "common choices are gaussian quadrature points like gauss - legendre for uniform measures or gauss - hermite for gaussian measures , cf .",
    "e.g. , @xcite , which are however _ not nested _ ,",
    "i.e. , @xmath124 .",
    "this means that they are not optimal for successive refinements of the interpolation / quadrature , and we will not consider them in this work . instead , we will work with _ nested _ collocation points , and specifically with clenshaw - curtis points @xcite , that are a classical choice for the uniform measure that we are considering here ; other choices of nested points are available for uniform random variables , e.g. , the leja points @xcite , whose performance is somehow equivalent to that of clenshaw - curtis for quadrature purposes , see @xcite .",
    "clenshaw - curtis points are defined as @xmath125 together with the following level - to - nodes relation , @xmath126 , that ensures their nestedness : @xmath127 we conclude this section by introducing the following operator norm , which acts as a `` lebesgue constant '' from @xmath106 to @xmath128 : @xmath129 in particular , for the clenshaw - curtis points , it is possible to bound @xmath130 as : @xmath131      \\displaystyle \\frac{2}{\\pi}\\log(q-1)+1 & \\mbox { for } q \\geq 2 .    \\end{cases}\\ ] ] see @xcite and references therein .",
    "nested collocation points have been studied also for other probability measures than uniform probability measures . in the very relevant case of a normal distribution ,",
    "one possible choice is the genz - keister points @xcite ; we mention also the recent work @xcite on generalized leja points that can be used for arbitrary measures on unbounded domains .",
    "it is easy to see that an accurate approximation of @xmath3}}$ ] by a direct tensor technique as the one just introduced , @xmath3 } } \\approx \\mathscr{q}^{m({{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } ) } [ { f}^{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}}]$ ] , might require a prohibitively large computational effort even for moderate values of @xmath76 and @xmath41 ( what is referred to as the `` curse of dimensionality '' ) . in this work ,",
    "following the setting that was presented in @xcite , we propose the multi - index stochastic collocation as an alternative .",
    "it can be seen as a generalization of the telescoping sum presented in the introduction , see equations and . denoting @xmath132 = { f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}}$ ] ,",
    "the building blocks of such a telescoping sum are the first - order difference operators for the deterministic and stochastic discretization parameters , denoted respectively by @xmath133 with @xmath134 and @xmath135 with @xmath136 : @xmath137 = \\begin{cases }    { f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } } - { f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}}-{{{\\ensuremath{{\\boldsymbol e}}}}}_i,{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } } , & \\text{if } \\alpha_i > 1,\\\\    { f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } } & \\text{if } \\alpha_i=1 ,    \\end{cases } \\\\ & \\delta_{j}^\\textnormal{stoc}[{f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } } ]     = \\begin{cases }    { f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } } - { f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}-{{{\\ensuremath{{\\boldsymbol e}}}}}_j } } , & \\text{if } \\beta_j > 1,\\\\    { f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } } & \\text{if } \\beta_j=1 .",
    "\\end{cases } \\ ] ] we then define the first - order tensor difference operators , @xmath138 = \\bigotimes_{i=1}^d \\delta_i^\\textnormal{det}[{f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } } ] = \\delta_1^\\textnormal{det } \\left [ \\ , \\delta_2^\\textnormal{det } \\left [ \\ , \\cdots    \\delta_d^\\textnormal{det } \\left [ { f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } } \\right ] \\ , \\right ] \\ , \\right ] = \\sum_{{{{\\ensuremath{{\\boldsymbol j}}}}}\\in \\{0,1\\}^d } ( -1)^{|{{{\\ensuremath{{\\boldsymbol j}}}}}| } { f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}}-{{{\\ensuremath{{\\boldsymbol j}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } } , \\label{eq : delta_det_def } \\\\ & { { \\ensuremath{{\\boldsymbol \\delta}}}}^\\textnormal{stoc}[{f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } } ] = \\bigotimes_{j=1}^n \\delta_j^\\textnormal{stoc}[{f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } } ] = \\sum_{{{{\\ensuremath{{\\boldsymbol j}}}}}\\in \\{0,1\\}^n } ( -1)^{|{{{\\ensuremath{{\\boldsymbol j}}}}}| } { f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}-{{{\\ensuremath{{\\boldsymbol j } } } } } } } \\ , .",
    "\\label{eq : delta_stoc_def}\\end{aligned}\\ ] ] with the convention that @xmath139 whenever a component of @xmath140 or @xmath141 is zero .",
    "observe that computing @xmath142 $ ] actually requires up to @xmath143 solver calls , and analogously applying @xmath144 $ ] requires interpolating @xmath89 on up to @xmath145 tensor grids ; for instance , if @xmath146 and @xmath147 , we have @xmath148    = \\delta^\\textnormal{det}_2 \\left [ \\ , \\delta^\\textnormal{det}_1 \\left [ \\ , { f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } } \\ , \\right ] \\ , \\right ]     = \\delta^\\textnormal{det}_2 [ { f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } } - { f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}}-{{{\\ensuremath{{\\boldsymbol e}}}}}_1,{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } } ]     = { f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } } - { f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}}-{{{\\ensuremath{{\\boldsymbol e}}}}}_1,{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } }     - { f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}}-{{{\\ensuremath{{\\boldsymbol e}}}}}_2,{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } } + { f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}}-{\\bm{1}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } } , \\\\[2pt ]   & { { \\ensuremath{{\\boldsymbol \\delta}}}}^\\textnormal{stoc } [ { f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } } ]    = { f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } } - { f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}-{{{\\ensuremath{{\\boldsymbol e}}}}}_1 } }     - { f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}-{{{\\ensuremath{{\\boldsymbol e}}}}}_2 } } + { f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}-{\\bm{1}}}}. \\nonumber\\end{aligned}\\ ] ] finally , letting @xmath149 = { { \\ensuremath{{\\boldsymbol \\delta}}}}^\\textnormal{stoc } [ { { \\ensuremath{{\\boldsymbol \\delta}}}}^\\textnormal{det}[{f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}}]]$ ] , we define the multi - index stochastic collocation ( misc ) estimator of @xmath3}}$ ] as @xmath150    = \\sum_{[{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha } } } } } } } , { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } ] \\in \\mathcal i } { { \\ensuremath{{\\boldsymbol \\delta}}}}[{f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } } ]    = \\sum _ { [ { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha } } } } } } } , { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } ] \\in \\mathcal i }   c_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } { f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}},\\ ] ] where @xmath151 and @xmath152 .",
    "observe that many of the coefficients in , @xmath153 , may be zero : in particular , @xmath153 is zero whenever @xmath154+{{\\ensuremath{{\\boldsymbol j } } } } \\in \\mathcal{i}$ ] @xmath155 .",
    "similarly to the analogous sparse grid construction @xcite , we shall require that the multi - index set @xmath156 be downward closed , i.e. , @xmath157 \\in \\mathcal{i } , \\quad    \\begin{cases }      { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}}- { { \\ensuremath{{\\boldsymbol e}}}}_i \\in \\mathcal{i } \\mbox { for } 1 \\leq i \\leq d \\mbox { and } \\alpha_i > 1,\\\\      { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}- { { \\ensuremath{{\\boldsymbol e}}}}_j \\in \\mathcal{i } \\mbox { for } 1 \\leq j \\leq",
    "n \\mbox { and } \\beta_j > 1 .",
    "\\end{cases}\\end{aligned}\\ ] ]    in theory , a misc approach could also be developed to approximate the entire solution @xmath158 and not just the expectation of functionals , considering differences between consecutive interpolant operators , @xmath122 , on the stochastic domain rather than differences of the quadrature operators , @xmath123 , as a building block for the @xmath159 operators , as well as considering the discretized solution @xmath160 rather than just the quantity of interest , @xmath89 , in the construction of the @xmath161 operators .      the efficiency of the misc method in equation will heavily depend on the specific choice of the index set , @xmath156 ; in the following , we will first propose a general strategy to derive quasi - optimal sets and then prove in section [ s : complexity ] a convergence result for such sets under some reasonable assumptions .    to derive an efficient set , @xmath156",
    ", we recast the problem of its construction as an optimization problem , in the same spirit of @xcite .",
    "we begin by introducing the concepts of `` work contribution '' , @xmath162 , and `` error contribution '' , @xmath163 , for each hierarchical surplus operator , @xmath149 $ ] .",
    "the work contribution measures the computational cost ( measured , e.g. , as a function of the total number of degrees of freedom , or in terms of computational time ) required to add @xmath149 $ ] to @xmath164 $ ] , i.e. , to solve the associated deterministic problems and to compute the corresponding interpolants over the parameter space , cf . equations and ; the error contribution measures instead how much the error @xmath165 } } - \\mathscr{m}_\\mathcal{i}[{f}]| $ ] would decrease once the operator @xmath149 $ ] has been added to @xmath164 $ ] . in formulas , we define @xmath166\\}}[{f}]\\right ] } } - { { \\ensuremath{\\mathrm{work}}\\mspace{-2mu}\\left[\\mathscr{m}_{\\mathcal{i}}[{f}]\\right ] } } = { { \\ensuremath{\\mathrm{work}}\\mspace{-2mu}\\left[{{\\ensuremath{{\\boldsymbol \\delta}}}}[{f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}}]\\right]}},\\ ] ] so that @xmath167\\right ] } } = \\sum_{[{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha } } } } } } } , { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } ] \\in \\mathcal{i } } \\delta w_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha } } } } } } } , { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}},\\ ] ] observe that this work definition is sharp only if we think of building the misc estimator with an incremental approach , i.e. , we assume that adding the multi - index @xmath168 to the index set @xmath169 would not reduce the work that has to be done to evaluate the misc estimator on the index set .",
    "this implies that one can not take advantage of the fact that some of the coefficients in , @xmath153 , that are non - zero when considering the set @xmath156 could become zero if the misc estimator is instead built considering the set @xmath170\\}$ ] , hence it would be possible not to compute the corresponding approximations @xmath171 .",
    "this approach is discussed in section  [ ss : impl ] .",
    "similarly , we define @xmath172\\}}[{f } ] } - { \\mathscr{m}_\\mathcal{i}[{f } ] } \\big|   = \\left| { { { \\ensuremath{{\\boldsymbol \\delta}}}}[{f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } } ] } \\right|.\\ ] ] thus , by construction , the error of the misc estimator can be bounded as the sum of the error contributions not included in the estimator @xmath164 $ ] , @xmath173\\right ] } } =    & = \\left| { \\sum_{[{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha } } } } } } } , { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } ] \\notin \\mathcal i } { { \\ensuremath{{\\boldsymbol \\delta}}}}[{f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } } ] } \\right| \\nonumber \\\\[2 mm ]   & \\leq \\sum_{[{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha } } } } } } } , { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } ] \\notin \\mathcal i } \\left| { { { \\ensuremath{{\\boldsymbol \\delta}}}}[{f_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } } ] } \\right|    = \\sum_{[{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha } } } } } } } , { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } ] \\notin \\mathcal i } \\delta e_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha } } } } } } } , { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}.\\end{aligned}\\ ] ] consequently , a quasi - optimal set @xmath156 can be computed by solving the following `` binary knapsack problem '' @xcite : @xmath174 \\in { \\mathbb{n}}_+^{d+n}}\\delta e_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha } } } } } } } , { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } x_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha } } } } } } } , { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } \\nonumber \\\\",
    "\\mbox{such that } & \\sum_{[{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha } } } } } } } , { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } ] \\in { \\mathbb{n}}_+^{d+n}}\\delta w_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha } } } } } } } , { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } x_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha } } } } } } } , { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } \\leq w_{max } , \\label{eq : knapsack } \\\\    & \\qquad x_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha } } } } } } } , { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } \\in \\{0,1\\ } , \\nonumber\\end{aligned}\\ ] ] and setting @xmath175 \\in { \\mathbb{n}}_+^{d+n } : x_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha } } } } } } } ,    { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } = 1\\}$ ] .",
    "observe that such a set is only `` quasi '' optimal since the error decomposition is not an exact representation but rather an upper bound .",
    "the optimization problem above is well known to be computationally intractable .",
    "still , an approximate greedy solution ( which coincides with the exact solution under certain hypotheses that will be clearer in a moment ) can be found if one instead allows the variables @xmath176 to assume fractional values , i.e. , it is possible to include fractions of multi - indices in @xmath156 .",
    "for this simplified problem , the resulting problem can be solved analytically by the so - called dantzig algorithm @xcite :    1 .",
    "compute the `` profit '' of each hierarchical surplus , i.e. , the quantity @xmath177 2 .",
    "sort the hierarchical surpluses by decreasing profit ; 3 .   add the hierarchical surpluses to @xmath164 $ ] according to such order until the constraint on the maximum work is fulfilled .",
    "note that by construction @xmath178 for all the multi - indices included in the selection except for the last one , for which @xmath179 might hold ; in other words , the last multi - index is the only one that might not be taken entirely .",
    "however , if this is the case , we assume that we could slightly adjust the value @xmath180 , so that all @xmath176 have integer values ( see also @xcite ) ; observe that this integer solution is also the solution of the original binary knapsack problem with the new value of @xmath180 in the work constraint .",
    "thus , if the quantities @xmath181 and @xmath182 were available , the quasi - optimal index set for the misc estimator could be computed as @xmath183 \\in { \\mathbb{n}}_+^{d+n}\\::\\ :     \\frac{\\delta e_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}}{\\delta w_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } } \\geq \\epsilon \\right\\},\\ ] ] for a suitable @xmath184 .    [",
    "remark : webster_and_bieri ] the misc setting could in principle include the multilevel stochastic collocation method proposed in @xcite as a special case , by simply considering a discretization of the spatial domain on regular meshes , and letting the diameter of each element ( the mesh - size ) be the only discretization parameter , i.e. , @xmath185 .",
    "however , the sparse grids to be used at each level are determined in @xcite by computing the minimal number of collocation points needed to balance the stochastic and spatial error .",
    "this is done by relying on sparse grid error estimates ; yet , since in general it is not possible to generate a sparse grid with a predefined number of points , some rounding strategy to the sparse grid with the nearest cardinality must be devised , which may affect the optimality of the multilevel strategy . in the present work , we overcome this issue by relying instead on profit estimates to build a set of multi - indices that simultaneously prescribe the spatial discretization and the associated tensor grid in the stochastic variables .",
    "furthermore , only standard isotropic smolyak sparse grids are considered in the actual numerical experiments in @xcite ( although in principle anisotropic sparse grids could be used as well , provided that good convergence estimates for such sparse grids are available ) , while our implementation naturally uses anisotropic stochastic collocation methods at each spatial level .",
    "the misc approach also includes as a special case the `` sparse composite collocation method '' developed in @xcite , by considering again only one deterministic discretization parameter , i.e. , @xmath185 , and then setting @xmath186 \\in { \\mathbb{n}}_+^{1+n } : \\alpha + \\sum_{n=1}^n \\beta_n \\leq w \\right\\},\\ ] ] with @xmath187 . in other words ,",
    "the approach in @xcite is based neither on profit nor on error balancing .",
    "in this section , we assume suitable models for the error and work contributions , @xmath181 and @xmath182 ( which are numerically verified in section [ s : numerics ] for the problem in example [ example : ell ] ) and then state our main convergence theorem for the misc method built using a particular index set , @xmath188 , which can be regarded as an approximation of the quasi - optimal set introduced in the previous section .    [",
    "assump : growth_of_dof ] the discretization parameters , @xmath189 , for the deterministic solver depend exponentially on the discretization level @xmath190 , and the number of collocation points over the parameter space grows exponentially with the level @xmath191 : @xmath192    [ assump : dw_de_factor ] the error and work contributions , @xmath181 and @xmath182 , can be bounded as products of two terms , @xmath193 where @xmath194 and @xmath195 denote the cost and the error contributions due to the deterministic difference operator , @xmath196 $ ] , and similarly @xmath197 and @xmath198 denote the cost and the error contribution due to the stochastic difference operator , @xmath199 $ ] , cf . equations - .",
    "[ assump : dw_de_model ] the following bounds hold true for the factors appearing in assumption [ assump : dw_de_factor ] : @xmath200 for some rates @xmath201 .    with these assumptions ,",
    "we are now ready to state our main theorem .",
    "the proof is technical and we therefore place it in the appendix .",
    "the proof is based on summing the error contributions outside a particular index set , @xmath202 , and the work contributions inside the same index set . this can be seen as a weighted cardinality argument in finite dimensions .",
    "see also @xcite for similar arguments for different choices of finite and infinite dimensional index sets .",
    "theoremmisccomplexity [ thm : misc_complexity ] under assumptions [ assump : growth_of_dof ] to [ assump : dw_de_model ] , the bounds for the factors appearing in assumption [ assump : dw_de_factor ] can be equivalently rewritten as    @xmath203    with @xmath204 , @xmath205 , @xmath206 and @xmath207 .",
    "define the following set @xmath208 \\in { \\mathbb{n}}_+^{d+n}\\::\\ :        \\sum_{i=1}^d ( { { r_{i}}}+\\gamma_i ) \\alpha_i + \\sum_{i=1}^n ( \\delta \\beta_i +   g_i e^{\\delta \\beta_i } ) \\leq l \\right\\ } \\mbox { with } l \\in { \\mathbb{r}}_+.\\ ] ] then there exists a constant @xmath209 such that , for any @xmath210 satisfying @xmath211 and choosing @xmath212 as @xmath213 with @xmath214 , @xmath215 , @xmath216 and @xmath217 , the misc estimator @xmath218 satisfies    [ eq : misc_complexity ] @xmath219 } } \\leq w_{\\max } , \\label{eq : misc_complexity_work }        \\\\[2 mm ]        & \\limsup_{w_{\\max } \\uparrow \\infty } \\frac { { { \\ensuremath{\\mathrm{error}}\\mspace{-2mu}\\left[\\mathscr m_{\\mathcal i^*(l(w_{\\max}))}\\right]}}}{w_{\\max}^{-\\zeta }         \\left(\\log\\left(w_{\\max}\\right)\\right)^{\\left(\\zeta+1\\right)\\left(\\mathfrak{z}-1\\right ) } }          = \\mathscr c_{\\text{e } } < \\infty .",
    "\\label{eq : misc_complexity_error }      \\end{aligned}\\ ] ]    [ rem : this - is - the - set ] the set @xmath188 proposed in theorem [ thm : misc_complexity ] can be obtained by assuming that the bounds in equations and are actually equalities and by using the definition of the quasi - optimal set : @xmath220 \\in { \\mathbb{n}}_+^{d+n } : \\frac{\\delta e_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}}{\\delta w_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } } \\geq \\epsilon \\right \\ } \\\\      & = \\left\\ { [ { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } ] \\in { \\mathbb{n}}_+^{d+n } :        \\frac{e^ { - \\sum_{i=1}^d { { r_{i}}}\\alpha_i } e^{- \\sum_{j=1}^n g_j \\exp(\\delta { \\beta_j } ) } }        { e^{\\sum_{i=1}^d \\gamma_i \\alpha_i } e^ { \\delta |{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}| } } \\geq \\epsilon \\right \\ } \\\\      &",
    "= \\left\\ { [ { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha } } } } } } } , { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } ] \\in { \\mathbb{n}}_+^{d+n}\\::\\ :        \\sum_{i=1}^d ( { { r_{i}}}+\\gamma_i ) \\alpha_i + \\sum_{i=1}^n ( \\delta \\beta_i +   g_i e^{\\delta \\beta_i } ) \\leq l \\right\\ } ,    \\end{aligned}\\ ] ] where the last equality holds with @xmath221 .",
    "[ rem : only - space - matters ] refining along the spatial or the stochastic variables has different effects on the error of the misc estimator . indeed , due to the double exponential @xmath222 in",
    ", the stochastic contribution to the error will quickly fade to zero , which in turn implies that most of the work will be used to reduce the deterministic error .",
    "this is confirmed by the fact that the error convergence rate in theorem [ thm : misc_complexity ] only depends on @xmath223 and @xmath224 , i.e. , the cost and error rates of the deterministic solver , respectively .",
    "this observation coincides with that in @xcite : `` since the stochastic error decreases exponentially , the convergence rate should tend towards the algebraic rate of the spatial discretization [ ... ] ; see proposition 3.8 '' .",
    "compared with the method proposed in @xcite , misc takes greater advantage of this fact , since it is based on an optimization procedure , cf . equation",
    "; this performance improvement is well documented by the comparison between the two methods shown in the next section .",
    "figure [ fig : misc_set ] shows the multi - indices included in @xmath156 according to for increasing values of @xmath212 , for a problem with @xmath225 , @xmath226 , and @xmath227 : as expected , the shape of @xmath156 becomes more and more curved as @xmath212 grows , due to this lack of balance between the stochastic and deterministic directions .",
    "for @xmath228 , according to equation .,scaledwidth=50.0% ]    theorem  [ thm : misc_complexity ] is only valid in case assumptions  [ assump : wellposed]  [ assump : dw_de_model ] are true . in the next section we motivate these assumption for a specific elliptic problem that we use for numerically testing the misc method .",
    "however , we stress that deriving bounds on the error and work contributions is problem - dependent and the corresponding analysis must be carried out in each case . moreover , under a different set of assumption the complexity theorem would have to be rewritten accordingly .",
    "in this section , we test the effectiveness of the misc approximation on some instances of the general elliptic equation in example [ example : ell ] ; more precisely , we consider a problem with one physical dimension ( @xmath229 ) as well as a more challenging problem with three dimensions ( @xmath230 ) ; in both cases , we set @xmath231^d$ ] , @xmath232 . as for the random diffusion coefficient",
    ", we set @xmath233 where @xmath49 are uniform random variables over @xmath234 $ ] , @xmath235 and take @xmath236 to be a tensorization of trigonometric functions .",
    "more precisely , we define the function @xmath237 \\displaystyle \\cos\\left(\\frac{n-1}{2 } \\pi x\\right ) & \\text{if $ n$ is odd } \\end{cases}\\ ] ] and set @xmath238 if @xmath229 . if @xmath230 , we take @xmath239 for some indices @xmath240 detailed in table  [ tab : modes ] . observe that the boundedness of the supports of the random variables @xmath49 guarantees the existence of the two bounding constants in equation , @xmath241 and @xmath242 , that in turn assures the well posedness of the problem . finally , the quantity of interest is defined as @xmath243 with @xmath244 and locations @xmath245 for @xmath229 and @xmath246 $ ] for @xmath230 .",
    "we also make the choice @xmath247 in assumption  [ assump : growth_of_dof ] . with these values and using the coarsest discretization , @xmath247 , in all dimensions",
    ", the coefficient of variation of the quantity of interest can be approximated to be between @xmath248 and @xmath249 depending on the number of dimensions , @xmath78 , and the number of random variables , @xmath41 , that we consider below .",
    ".included functions for @xmath230 in . here",
    "[ cols=\"<,>,>,>,>,>,>,>,>,>,>\",options=\"header \" , ]     [ [ stochastic - deterministic - product - structure ] ] stochastic - deterministic product structure + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we conclude this section by verifying assumption [ assump : dw_de_factor ] , i.e. , the fact that the error contribution can be factorized as @xmath250 and that an analogous decomposition holds for @xmath182 . while the latter is trivial , to verify the former we employ the same strategy used to verify the models for @xmath195 and @xmath198 , this time letting both @xmath251 and @xmath252 change for every point , i.e. , @xmath253 and @xmath254 .",
    "figure [ fig : check_mixed_rates ] shows the comparison between the computed value of @xmath181 and their estimated counterpart and confirms the validity of the product structure assumption .     for @xmath255 and @xmath256 for the test case with @xmath257 and @xmath258 .",
    "the _ dashed _ lines are based on the model in with @xmath259 for all @xmath260 and @xmath261 as in table  [ tab : g_values ] for @xmath262 .",
    "the _ solid _ lines are based on computed values.,scaledwidth=50.0% ]      in our numerical tests , we compare misc with the methods listed below . for each of them",
    "we show ( for both test cases considered ) plots of the convergence of the error in the computation of @xmath3}}$ ] with respect to the computational work , taking as a reference value the result obtained using a well - resolved misc solution . to avoid discrepancies in running time due to implementation details ,",
    "the computational work is estimated in terms of the total number of degrees of freedom , i.e. , using and .",
    "the names used here for the methods are also used in the legends of the figures showing the convergence plots .    `` a - priori '' misc : :    refers to the misc method with index set @xmath169    defined by , where    @xmath162    and    @xmath163    are taken to equal their upper bounds in and , respectively .",
    "the    resulting set is explicitly written in  .",
    "the convergence rate of this    set is predicted by theorem [ thm : misc_complexity ] , cf .",
    "remark    [ rem : this - is - the - set ] .",
    "note that we do not need to determine the value    of the constants    @xmath263 and    @xmath264 since they    can be absorbed in the parameter @xmath265 in .",
    "`` a - posteriori '' misc : :    refers to the misc method with index set @xmath169    defined by , where    @xmath162    is taken to equal its upper bound in , and    @xmath163    is instead computed explicitly as    @xmath266 \\right|$ ] .",
    "notice that    this method is not practical since the cost of constructing set    @xmath169 would dominate the cost of the misc estimator    by far .",
    "however , this method would produce the best possible    convergence and serve as a benchmark for both `` a - priori '' misc and    the bound .",
    "mlsc : :    ( only in the case @xmath267 ) refers to the multilevel    stochastic collocation obtained by setting    @xmath268 ( i.e. considering the mesh - size    as the only discretization parameter ) , as already mentioned in remark    [ remark : webster_and_bieri ] ; we recall this is not exactly the mlsc    method that was implemented in @xcite , see again remark    [ remark : webster_and_bieri ] . just as with misc , we consider both the    `` a - priori '' and `` a - posteriori '' version of mlsc , where    @xmath163    is taken to be equal to its upper in the former case and assessed by    direct numerical evaluation in the latter case .",
    "scc : :    refers to the `` sparse composite collocation method '' in remark    [ remark : webster_and_bieri ] , see equation .",
    "mimc : :    refers to the multi - index monte carlo method as detailed in @xcite ,    for which the complexity    @xmath269    can be estimated for the test case at hand and as long as    @xmath270 .",
    "sgsc : :    refers to the quasi - optimal sparse grids stochastic collocation ( sgsc )    with fixed spatial discretization as proposed in @xcite . to    determine the needed spatial discretization for a given work and for a    fair comparison against misc",
    ", we actually compute the convergence    curves of sgsc for all relevant levels of spatial discretizations and    then show in the plots only the lower envelope of the corresponding    convergence curves , ignoring the possible spurious reductions of error    that might happen due to non - asymptotic , unpredictable cancellations ,    cf .",
    "figure [ fig : envelope ] . in this way",
    ", we ensure that the error shown    for such `` single - level methods '' has been obtained with the smallest    computational error possible . again , this is not a computationally    practical method but is taken as a reference for what a sparse grids    stochastic collocation method with optimal balancing of the space and    stochastic discretization errors could achieve .     and @xmath271.,scaledwidth=70.0% ]      to implement misc",
    ", we need two components :    1 .   given a profit level parameter , @xmath265 , we build the quasi - optimal set @xmath169 based on  , and  .",
    "one method to achieve this is to exploit the fact that this set is downward closed and use the following recursive algorithm . + .... function buildset(epsilon , multiindex )      for i = 1 to ( d+n )          if profit(multiindex + e_i ) > epsilon          then              add multiindex+e_i to finalset              call buildset(epsilon , multiindex+e_i )          end if      end for end function     .... 2 .",
    "given the set , @xmath272 , we evaluate . here",
    "we have two choices : * evaluate the individual terms @xmath149 $ ] for every @xmath273 . to do so",
    ", we use the operator defined in   along each stochastic and spatial direction . by storing the values of these terms",
    ", we can evaluate the misc with different index sets ( contained in @xmath272 ) , which might be required to test the convergence of the misc method .",
    "moreover , this implementation is suitable for adaptive methods that expand the index set based on some criteria and reevaluate the misc estimator . on the other hand",
    ", this implementation has a computational overhead since most computed values of @xmath171 will actually not contribute to the final value of the estimator .",
    "however , this computational overhead is only a fraction of the minimum time required to evaluate the estimator .",
    "* use the combination form of   and only compute the terms that have @xmath274 .",
    "this would remove the overhead of computing terms that make zero contribution to the estimator .",
    "this implementation is more efficient but less flexible as we can not evaluate the estimator on sets contained in @xmath272 or build the set adaptively .",
    "here we consider three different numbers of stochastic variables , namely @xmath275 .",
    "results are shown in figure [ fig : res_d=1 ] . as expected ,",
    "a - posteriori misc shows the best convergence , with a - priori misc being slightly worse and the single level methods following .",
    "finally , we verify the accuracy of the estimated asymptotic convergence rate provided by theorem [ thm : misc_complexity ] : in this case , @xmath276 and @xmath277 holds .",
    "hence , the predicted convergence rate is @xmath278 , which appears to be in good agreement with the experimental convergence rate .",
    ", case @xmath279 ( top ) , @xmath258 ( center ) and @xmath271 ( bottom).,title=\"fig:\",scaledwidth=80.0% ] + , case @xmath279 ( top ) , @xmath258 ( center ) and @xmath271 ( bottom).,title=\"fig:\",scaledwidth=80.0% ] + , case @xmath279 ( top ) , @xmath258 ( center ) and @xmath271 ( bottom).,title=\"fig:\",scaledwidth=80.0% ] +      in this case , we obtain the convergence curves shown in figure [ fig : res_d=3 ] , where the multilevel stochastic collocation method has also been included .",
    "the hierarchy between the methods is in agreement with the case @xmath229 , with the multilevel stochastic collocation being comparable or slightly better than single level methods , but worse than the misc approaches as expected .    concerning the accuracy of the theoretical estimate : since for this test @xmath280 and @xmath281 , @xmath282 still holds , while this time @xmath283 ; hence , the predicted convergence rate is @xmath284 .",
    "the plots suggest that the theoretical estimates might be slightly too optimistic when @xmath41 increases but it is important to recall that theorem [ thm : misc_complexity ] gives only an asymptotic result , and the plot could be negatively influenced by pre - asymptotic effects .",
    "observe also that in this case there are a few data points where a - posteriori misc is not better than a - priori misc ; this observation can be ascribed to the fact that a - posteriori misc is optimal only with respect to the upper bound in . in other words ,",
    "a - posteriori misc selects the contributions according to the absolute value of the contributions but then the misc estimator is computed by summing signed contributions .",
    "hence , cancellations between contributions with similar sizes and opposite signs will occur .",
    "finally , we remark that , in our calculations , mlsc and sgsc were not able to achieve very small errors , unlike misc .",
    "this is due to a limitation in the linear solver we are using that allows systems with only up to @xmath285 degrees of freedom ( around 1 gb of memory ) to be solved .",
    "these `` single - level '' methods hit that limit sooner than misc since they entail solving a very large system that comes from isotropically discretizing all three spatial dimensions .",
    ", case @xmath279 ( top ) , @xmath258 ( center ) and @xmath271 ( bottom ) .",
    ", title=\"fig:\",scaledwidth=80.0% ] + , case @xmath279 ( top ) , @xmath258 ( center ) and @xmath271 ( bottom ) . ,",
    "title=\"fig:\",scaledwidth=80.0% ] + , case @xmath279 ( top ) , @xmath258 ( center ) and @xmath271 ( bottom ) . ,",
    "in this work , we have proposed misc , a combination technique method to solve uq problems , optimizing both the deterministic and stochastic resolution levels simultaneously to minimize the computational cost .",
    "a distinctive feature of misc is that its construction is based on the notion of profit of the mixed differences composing it , rather than balancing the total error contributions arising from the deterministic and stochastic components .",
    "we have detailed a complexity analysis and derived a convergence theorem showing that in certain cases the convergence of the method is essentially dictated by the convergence properties of the deterministic solver .",
    "we have then verified the effectiveness of the method proposed on a couple of numerical test cases , comparing its performance with other methods available in the literature .",
    "the results obtained are encouraging , as they suggest that the proposed methodology is more effective than the other methods considered here .",
    "the theoretical results have been also found to be consistent with the numerical results to a satisfactory extent . as a final remark",
    ", we observe that the methodology presented here is not limited to the spatial or temporal discretization parameters of the deterministic problem , but could also be applied to other discretization parameters , such as smoothing parameters or artificial viscosities .",
    "[ [ acknowledgement ] ] acknowledgement + + + + + + + + + + + + + + +    f. nobile and l. tamellini received support from the center for advanced modeling science ( cadmos ) and partial support by the swiss national science foundation under the project no .",
    "140574 `` efficient numerical methods for flow and transport phenomena in heterogeneous random porous media '' .",
    "r. tempone is a member of the kaust strategic research initiative , center for uncertainty quantification in computational sciences and engineering .",
    "the following technical lemmas are needed in the convergence proof .",
    "[ lemma : bound - sum - with - int ] for @xmath286 , define @xmath287 . for any @xmath288 and @xmath289",
    ", @xmath290 holds .",
    "moreover , if @xmath291 and @xmath28 are increasing , then @xmath292 and if @xmath291 and @xmath28 are decreasing , then @xmath293    we have @xmath294^{d } } { ~\\text{d}{{\\ensuremath{{\\boldsymbol x}}}}}\\\\     & = \\sum_{\\{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}}\\in { \\mathbb{n}}_+^d   \\::\\ : f({{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha } } } } } } } ) \\leq 0\\ } }       \\int_{{{\\ensuremath{{\\boldsymbol x } } } } \\in [ 0,1]^{d } }      g(\\lfloor { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}}+ { { \\ensuremath{{\\boldsymbol x } } } } \\rfloor ) { ~\\text{d}{{\\ensuremath{{\\boldsymbol x}}}}}\\\\     & = \\int_{\\{{{\\ensuremath{{\\boldsymbol x } } } } \\in ( 1,\\infty)^d   \\::\\ : f(\\lfloor { { \\ensuremath{{\\boldsymbol x } } } } \\rfloor ) \\leq        0\\ } } g(\\lfloor { { \\ensuremath{{\\boldsymbol x } } } } \\rfloor ) { ~\\text{d}{{\\ensuremath{{\\boldsymbol x}}}}}.    \\end{aligned}\\ ] ] combining these inequalities with @xmath295 finishes the proof .",
    "define the set @xmath300 and define @xmath301 .",
    "then @xmath302 letting @xmath303 and @xmath304 , then @xmath305 now let us prove , by induction on @xmath76 , that we have @xmath306 for @xmath185 , the inequality is a trivial equality that can be obtained with integration by parts .",
    "assume the inequality is true for @xmath76 and let us prove it for @xmath307 : @xmath308 finally , substituting back , we get the result .",
    "next , assume that the result is valid for a given @xmath325 and @xmath326 where @xmath321 for all @xmath322 , such that @xmath323 .",
    "let @xmath327 and define a new vector @xmath328 .",
    "we have @xmath329 we distinguish between two cases :      the bounds and can be obtained by elementary algebraic operations combining assumptions [ assump : growth_of_dof ] and [ assump : dw_de_model ] ; for instance , @xmath338 from which follows by setting @xmath339 . the proof is then divided into two steps .",
    "observe that @xmath340 for all @xmath341 and that @xmath342 .",
    "thanks to equations and , and using lemma [ lemma : bound - sum - with - int ] , the total work satisfies @xmath343 } } = \\sum_{({{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha } } } } } } } , { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } ) \\in \\mathcal{i}^*(l ) } \\delta    w_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha } } } } } } } , { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } \\\\",
    "\\leq & { { \\ensuremath{c_{\\text{\\textnormal{work } } } } } }           \\sum_{\\left\\ {           ( { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha } } } } } } } , { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } ) \\in { \\mathbb{n}}_+^{d+n } \\::\\ :           \\sum_{i=1}^d ( { { r_{i } } } + \\gamma_i)\\alpha_i           + \\sum_{j=1}^n \\delta \\beta_j + g_j e^{\\delta \\beta_j } \\leq            l           \\right\\ } }           \\exp\\left(\\sum_{i=1}^d \\gamma_i \\alpha_i + \\delta |{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}|           \\right ) \\\\",
    "\\leq & { { \\ensuremath{c_{\\text{\\textnormal{work } } } } } }           \\int_{\\left\\{({{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha } } } } } } } , { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } ) \\in ( 1,\\infty)^{d+n } \\::\\ :           \\sum_{i=1}^d ( { { r_{i } } } + \\gamma_i)(\\alpha_i-1 )           + \\sum_{j=1}^n \\delta ( \\beta_j-1 ) + g_j e^{\\delta ( \\beta_j-1 ) } \\leq            l           \\right\\ } }           \\exp\\left(\\sum_{i=1}^d \\gamma_i \\alpha_i + \\delta |{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}|           \\right ) { ~\\text{d}{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}}}{~\\text{d}{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}.\\end{aligned}\\ ] ] next , let @xmath344 and @xmath345 .",
    "we have @xmath346 } } \\leq     & { { \\ensuremath{c_{\\text{\\textnormal{work } } } } } } \\left(\\prod_{j=1}^n \\frac{2}{g_j \\delta}\\right )        \\left(\\prod_{i=1}^d \\frac{\\exp(\\gamma_i)}{{{r_{i } } } + \\gamma_i }        \\right ) \\\\        & \\qquad \\int_{\\left\\ { ( \\overline{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha } } } } } } } } , \\overline{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } ) \\in            { \\mathbb{r}}_+^d \\times ( \\otimes_{j=1}^n ( g_j , \\infty ) )            \\::\\ :            |\\overline{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}}}|            + |\\overline{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}| + |\\log \\overline{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}| \\leq            l + |\\log { { \\ensuremath{{\\boldsymbol g}}}}| \\right\\ } }        \\exp\\left({{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\xi}}}}}}}\\cdot \\overline { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}}\\right ) { ~\\text{d}\\overline{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}}}}{~\\text{d}\\overline{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}}.      \\end{aligned}\\ ] ] dropping the over - line notation and defining @xmath347 and @xmath348 to be the constant factor , we obtain @xmath343 } }      \\leq \\mathscr{c}_{\\text{w},1 }      \\int_{\\left\\ { ( { { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha } } } } } } } } , { { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } ) \\in { \\mathbb{r}}_+^d \\times ( \\otimes_{j=1}^n ( g_j , \\infty ) ) \\::\\ :      |{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}}}|      + |{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}| + |\\log { { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}| \\leq   \\widetilde l \\right\\ } }      \\exp\\left({{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\xi}}}}}}}\\cdot   { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}}\\right ) { ~\\text{d}{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}}}}{~\\text{d}{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}}\\\\    & = \\mathscr{c}_{\\text{w},1 }      \\int_{\\left\\ { { { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } \\in \\otimes_{j=1}^n ( g_j , \\infty ) \\::\\ :      |{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}| + |\\log{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}| \\leq   \\widetilde l \\right\\ } }   \\int_{\\left\\ {      { { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha } } } } } } } } \\in { \\mathbb{r}}_+^d \\::\\ :      |{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}}| \\leq   \\widetilde l - |{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}| - |\\log{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}| \\right\\ } }      \\exp\\left({{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\xi}}}}}}}\\cdot   { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}}\\right ) { ~\\text{d}{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha } } } } } } } } { ~\\text{d}{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } \\\\    & \\leq \\mathscr{c}_{\\text{w},1 }      \\mathfrak{a}_d\\left ( { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\xi } } } } } } } , 0\\right )      \\int_{\\left\\ {   { { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } \\in \\otimes_{j=1}^n ( g_j , \\infty ) \\::\\ :      |{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}| + |\\log{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}| \\leq   \\widetilde l \\right\\ } } \\exp\\big ( \\chi \\left(\\widetilde l - |{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}| -      |\\log{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}|\\right)\\big ) \\left(\\widetilde l - |{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}| -      |\\log{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}|\\right)^{\\mathfrak{z}-1 } { ~\\text{d}{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}\\end{aligned}\\ ] ] define @xmath349 , then @xmath350 } }    & \\leq \\mathscr{c}_{\\text{w},2 }      \\int_{\\left\\ {   { { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } \\in \\otimes_{j=1}^n ( g_j , \\infty ) \\::\\ :      |{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}| + |\\log{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}| \\leq   \\widetilde l \\right\\ } } \\exp\\left(- \\chi \\left(|{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}| +      |\\log{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}|\\right)\\right ) \\left(\\widetilde l - |{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}| -      |\\log{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}|\\right)^{\\mathfrak{z}-1 } { ~\\text{d}{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}\\\\    & \\leq \\mathscr{c}_{\\text{w},2 }       \\left({\\widetilde",
    "|{{\\ensuremath{{\\boldsymbol g}}}}| -      |\\log { { \\ensuremath{{\\boldsymbol g } } } } |\\right)^{\\mathfrak{z}-1 }      \\int_{\\left\\ {   { { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } } \\in \\otimes_{j=1}^n ( g_j , \\infty ) \\::\\ :      |{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}| + |\\log{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}| \\leq   \\widetilde l \\right\\ } } \\exp\\left(-\\chi \\left ( |{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}| +      |\\log{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}|\\right)\\right )   { ~\\text{d}{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}.\\end{aligned}\\ ] ] since @xmath351 , the previous integral is bounded for all @xmath352 and we have @xmath353 } } & \\leq \\mathscr{c}_{\\text{w } } \\exp(\\chi                              l ) \\left ( l - |{{\\ensuremath{{\\boldsymbol g}}}}|\\right)^{\\mathfrak{z}-1}\\leq \\mathscr{c}_{\\text{w } } \\exp(\\chi                              l ) l^{\\mathfrak{z}-1},\\end{aligned}\\ ] ] where @xmath354 substituting yields @xmath346 } }      & \\leq w_{\\max } \\left(1 - \\frac{(\\mathfrak{z}-1 ) \\log \\left ( \\frac{\\log\\left(\\frac{w_{\\max}}{\\mathscr{c}_{\\text{w}}}\\right)}{\\chi }        \\right )        } { \\log\\left(\\frac{w_{\\max } } { \\mathscr{c}_{\\text{w}}}\\right ) } \\right)^{\\mathfrak{z}-1}.    \\end{aligned}\\ ] ] from here it is easy to see that if is satisfied , then follows .      thanks to equations and , the total error satisfies @xmath355 } }          \\leq \\sum_{({{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } ) \\notin \\mathcal i^ * } \\delta e_{{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha } } } } } } } , { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}}\\\\      \\leq & { \\ensuremath{c_{\\text{\\textnormal{error}}}}}\\sum_{\\left\\ { ( { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha } } } } } } } , { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } ) \\in { \\mathbb{n}}_+^{d+n } \\::\\ :             \\sum_{i=1}^d ( { { r_{i } } } + \\gamma_i)\\alpha_i             + \\sum_{j=1}^n \\delta \\beta_j + g_j e^{\\delta{\\beta_j } } > l \\right\\ } }             \\exp\\left (             -\\sum_{i=1}^d { { r_{i } } } \\alpha_i - \\sum_{j=1}^n g_j             e^{\\delta \\beta_j } \\right ) \\\\      = & { \\ensuremath{c_{\\text{\\textnormal{error}}}}}\\sum_{\\left\\ { ( { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}},{{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta } } } } } } } ) \\in { \\mathbb{n}}_+^{d+n }          \\::\\ :          \\sum_{i=1}^d ( { { r_{i } } } + \\gamma_i)\\alpha_i > l \\right\\ } }          \\exp\\left (          -\\sum_{i=1}^d { { r_{i } } } \\alpha_i - \\sum_{j=1}^n g_j          e^{\\delta \\beta_j } \\right ) \\\\           & + { \\ensuremath{c_{\\text{\\textnormal{error}}}}}\\sum_{\\left\\ {   { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\alpha}}}}}}}\\in { \\mathbb{n}}_+^{d } \\::\\ :             \\sum_{i=1}^d ( { { r_{i } } } + \\gamma_i)\\alpha_i   \\leq l             \\right\\ } }                        \\sum_{\\left\\ { { { \\ensuremath{{{\\ensuremath{{\\boldsymbol \\beta}}}}}}}\\in { \\mathbb{n}}_+^{n } \\::\\ :             \\sum_{j=1}^n \\delta \\beta_j + g_j e^{\\delta{\\beta_j } } > l             - \\sum_{i=1}^d ( { { r_{i } } } + \\gamma_i)\\alpha_i             \\right\\ } }             \\exp\\left(-\\sum_{i=1}^d { { r_{i } } } \\alpha_i -\\sum_{j=1}^n g_j e^{\\delta \\beta_j } \\right ) .",
    "\\end{aligned}\\ ] ] looking at the first term , let @xmath356 and @xmath357 and note that @xmath358 . then @xmath359 where @xmath360 for the second term , letting @xmath361 , we can bound the sum using lemma  [ lemma : bound - inf - sum - exp - lin ] : @xmath362 defining @xmath363 and substituting back @xmath364 where @xmath365 finally , noting that @xmath366 we have the error estimate @xmath367 } } \\leq    { \\ensuremath{c_{\\text{\\textnormal{error}}}}}\\left(\\mathscr{c}_{\\text{e } , 2 }    + \\mathscr{c}_{\\text{e } , 4 } \\right ) \\exp(-\\min({{\\ensuremath{{{\\ensuremath{{\\boldsymbol \\eta } } } } } } } )",
    "l ) l^{\\mathfrak{z}-1}.\\end{aligned}\\ ] ] then , substituting @xmath212 from and evaluating the limit gives  .",
    "o.  p. le  matre , o.  m. knio , spectral methods for uncertainty quantification , scientific computation , springer , new york , 2010 , with applications to computational fluid dynamics .",
    "http://dx.doi.org/10.1007/978-90-481-3520-2 [ ] .",
    "h.  g. matthies , a.  keese , galerkin methods for linear and nonlinear elliptic stochastic partial differential equations , computer methods in applied mechanics and engineering 194  ( 12 - 16 ) ( 2005 ) 12951331 .",
    "r.  a. todor , c.  schwab , convergence rates for sparse chaos approximations of elliptic problems with stochastic coefficients , i m a j numer anal 27  ( 2 ) ( 2007 ) 232261 .",
    "http://dx.doi.org/10.1093/imanum/drl025 [ ] .",
    "f.  nobile , r.  tempone , c.  webster , an anisotropic sparse grid stochastic collocation method for partial differential equations with random input data , siam journal on numerical analysis 46  ( 5 ) ( 2008 ) 24112442 .",
    "a.  nouy , generalized spectral decomposition method for solving stochastic finite element equations : invariant subspace problem and dedicated algorithms , computer methods in applied mechanics and engineering 197  ( 51 ) ( 2008 ) 47184736 .",
    "j.  ballani , l.  grasedyck , hierarchical tensor approximation of output quantities of parameter - dependent pdes , siam / asa journal on uncertainty quantification 3  ( 1 ) ( 2015 ) 852872 . http://dx.doi.org/10.1137/140960980 [ ] .",
    "p.  chen , a.  quarteroni , g.  rozza , comparison between reduced basis and stochastic collocation methods for elliptic problems , journal of scientific computing 59  ( 1 ) ( 2014 ) 187216 .",
    "http://dx.doi.org/10.1007/s10915-013-9764-2 [ ] .",
    "j.  charrier , r.  scheichl , a.  teckentrup , finite element error analysis of elliptic pdes with random coefficients and its application to multilevel monte carlo methods , siam journal on numerical analysis 51  ( 1 ) ( 2013 ) 322352 .",
    "k.  cliffe , m.  giles , r.  scheichl , a.  teckentrup , multilevel monte carlo methods and applications to elliptic pdes with random coefficients , computing and visualization in science 14  ( 1 ) ( 2011 ) 315 .",
    "http://dx.doi.org/10.1007/s00791-011-0160-x [ ] .",
    "a.  teckentrup , p.  jantsch , c.  g. webster , m.  gunzburger , a multilevel stochastic collocation method for partial differential equations with random input data , siam / asa journal on uncertainty quantification 3  ( 1 ) ( 2015 ) 10461074 .",
    "http://dx.doi.org/10.1137/140969002 [ ] .",
    "h.  harbrecht , m.  peters , m.  siebenmorgen , on multilevel quadrature for elliptic stochastic partial differential equations , in : sparse grids and applications , vol .",
    "88 of lecture notes in computational science and engineering , springer , 2013 , pp . 161179 .",
    "f.  y. kuo , c.  schwab , i.  sloan , multi - level quasi - monte carlo finite element methods for a class of elliptic pdes with random coefficients , foundations of computational mathematics 15  ( 2 ) ( 2015 ) 411449 .",
    "f.  nobile , f.  tesei , a multi level monte carlo method with control variate for elliptic pdes with log - normal coefficients , stochastic partial differential equations : analysis and computations 3  ( 3 ) ( 2015 ) 398444 .",
    "http://dx.doi.org/10.1007/s40072-015-0055-9 [ ] .",
    "m.  griebel , m.  schneider , c.  zenger , a combination technique for the solution of sparse grid problems , in : p.",
    "de  groen , r.  beauwens ( eds . ) , iterative methods in linear algebra , imacs , elsevier , north holland , 1992 , pp .",
    "263281 .",
    "m.  griebel , h.  harbrecht , on the convergence of the combination technique , in : j.  garcke , d.  pflger ( eds . ) , sparse grids and applications - munich 2012 , vol .  97 of lecture notes in computational science and engineering , springer international publishing , 2014 , pp . 5574 .",
    "http://dx.doi.org/10.1007/978-3-319-04537-5_3 [ ] .",
    "f.  nobile , l.  tamellini , r.  tempone , convergence of quasi - optimal sparse - grid approximation of hilbert - space - valued functions : application to random elliptic pdes , numerische mathematikhttp://dx.doi.org/10.1007/s00211 - 015 - 0773-y [ ] .",
    "j.  beck , f.  nobile , l.  tamellini , r.  tempone , on the optimal polynomial approximation of stochastic pdes by galerkin and collocation methods , mathematical models and methods in applied sciences 22  ( 09 ) ( 2012 ) 1250023 .",
    "j.  beck , f.  nobile , l.  tamellini , r.  tempone , a quasi - optimal sparse grids procedure for groundwater flows , in : spectral and high order methods for partial differential equations - icosahom 2012 , vol .",
    "95 of lecture notes in computational science and engineering , springer , 2014 , pp . 116 .",
    "m.  griebel , s.  knapek , optimized general sparse grid approximation spaces for operator equations , mathematics of computation 78  ( 268 ) ( 2009 ) 22232257 . http://dx.doi.org/10.1090/s0025-5718-09-02248-0 [ ] .",
    "t.  j.  r. hughes , j.  a. cottrell , y.  bazilevs , isogeometric analysis : cad , finite elements , nurbs , exact geometry and mesh refinement , computer methods in applied mechanics and engineering 194  ( 3941 ) ( 2005 ) 41354195 .",
    "http://dx.doi.org/10.1016/j.cma.2004.10.008 [ ] .",
    "w.  j. gordon , c.  a. hall , construction of curvilinear co - ordinate systems and applications to mesh generation , international journal for numerical methods in engineering 7  ( 4 ) ( 1973 ) 461477 .",
    "http://dx.doi.org/10.1002/nme.1620070405 [ ] .",
    "f.  nobile , l.  tamellini , r.  tempone , comparison of clenshaw - curtis and leja quasi - optimal sparse grids for the approximation of random pdes , in : r.  m. kirby , m.  berzins , j.  s. hesthaven ( eds . ) , spectral and high order methods for partial differential equations - icosahom 14 , vol .",
    "106 of lecture notes in computational science and engineering , springer international publishing , 2015 , pp .",
    "http://dx.doi.org/10.1007/978-3-319-19800-2_44 [ ] .",
    "j.  bck , f.  nobile , l.  tamellini , r.  tempone , stochastic spectral galerkin and collocation methods for pdes with random coefficients : a numerical comparison , in : spectral and high order methods for partial differential equations , vol .",
    "76 of lecture notes in computational science and engineering , springer , 2011 , pp ."
  ],
  "abstract_text": [
    "<S> in this work we introduce the multi - index stochastic collocation method ( misc ) for computing statistics of the solution of a pde with random data . </S>",
    "<S> misc is a combination technique based on mixed differences of spatial approximations and quadratures over the space of random data . </S>",
    "<S> we propose an optimization procedure to select the most effective mixed differences to include in the misc estimator : such optimization is a crucial step and allows us to build a method that , provided with sufficient solution regularity , is potentially more effective than other multi - level collocation methods already available in literature . </S>",
    "<S> we then provide a complexity analysis that assumes decay rates of product type for such mixed differences , showing that in the optimal case the convergence rate of misc is only dictated by the convergence of the deterministic solver applied to a one dimensional problem . </S>",
    "<S> we show the effectiveness of misc with some computational tests , comparing it with other related methods available in the literature , such as the multi - index and multilevel monte carlo , multilevel stochastic collocation , quasi optimal stochastic collocation and sparse composite collocation methods .    </S>",
    "<S> uncertainty quantification , random pdes , multivariate approximation , sparse grids , stochastic collocation methods , multilevel methods , combination technique .    41a10 , 65c20 , 65n30 , 65n05 </S>"
  ]
}