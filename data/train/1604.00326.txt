{
  "article_text": [
    "semantic attributes describe the object s shape , texture and parts .",
    "they have the unique property of being both machine detectable and human understandable . by changing the recognition task from labeling to describing",
    ", attributes represent an adequate knowledge that can be easily transferred and shared with new visual concepts .",
    "thus , they can be used to recognize unseen classes with no training samples ( zero - shot classification ) .        in the prevailing approach",
    ", attributes are learned from all seen classes and then reused to describe or classify an unseen one . however ,",
    "this does nt account for the high intra - attribute variance .",
    "using all the seen classes helps in learning visual semantics in a very abstract manner .",
    "hence , subsets of classes that share similar attributes can not be distinguished easily .",
    "eventually , the fine properties of the attribute that help in discriminating a group from another are lost when it is learned from all the classes .",
    "consider for example the attribute _ beak _ in .",
    "the  attribute model would learn that a beak is an elongated extension at a certain position relative to the head ; ignoring the distinctive long thin beak shape of the hummingbird species or the wide curved - end of the albatross species .",
    "in other words , the  model does not take advantage of the rich information already available in the source dataset .",
    "this results in transferring less discriminative attributes to the novel class . on the other hand ,",
    "capturing these specific properties of _ beak _ relative to each subgroup of birds is beneficial .",
    "it gives us the option to select the most proper type of _ beak _ to share with the unseen class .",
    "accordingly , knowing that both _",
    "gull _ and _ albatross _ are _ seabirds _ , it is intuitive and probably more discriminative to describe the beak of the _ california - gull _ as an _ albatross - like - beak_.    in this work we study the benefit of learning attributes at different levels of abstraction , from the most specific that distinguish one class from another to the most general that are learned over all categories .",
    "we propose a novel hierarchical transfer model that can find the best type of attributes to be shared with an unseen class .",
    "we evaluate the proposed model on three challenging datasets each with a different granularity of object categories .",
    "the evaluation shows that significant gain can be achieved with our guided transfer model with improvements from 26% and up to 35% over state - of - the - art in zero - shot classification .",
    "our work relates to two fields in computer vision literature ; the attribute - based recognition and hierarchical transfer learning .",
    "since the introduction of semantic attributes @xcite , they have gained increasing attention from the computer vision community .",
    "they represent an intermediate layer of semantics and facilitate various tasks in visual recognition like zero - shot learning @xcite , aiding object classification and localization @xcite , relative attribute comparison @xcite and detection of unfamiliar classes @xcite .",
    "however , in the prevailing direction the attributes are learned in a  manner from all classes available in the source @xcite .",
    "such methods can not cope with the high variations in each of the attributes .",
    "some approaches jointly model objects , attributes and their correlations @xcite to handle such variations . nonetheless , these correlations can not be learned for unseen classes since there is no training data .",
    "our approach is related to the work on learning class - specific attributes . in @xcite",
    "a set of attributes are learned per class as an intermediate step for feature selection in order to reduce attribute correlations .",
    "yu @xcite propose to learn data - driven attributes at the category - level to better discriminate the classes .",
    "however , data - driven attributes usually carry no semantic meaning ; thus , their approach requires user interaction when performing zero - shot learning . in @xcite",
    "the concepts in imagenet are augmented with a set of semantic and data - driven attributes .",
    "these are used along with the hierarchy to learn a better similarity metric for content - based image retrieval .",
    "correspondingly , we propose to explicitly model the intra - attribute variations at different abstraction levels .",
    "that is , rather than just using class - specific attributes , we expand the notion of the attribute from the most abstract to the most specific driven by the embedded relations between the categories .",
    "additionally , hierarchies represent an attractive structure for knowledge transfer and they have been exploited in various ways : parameter transfer @xcite , representation transfer @xcite and bounding box annotations propagation @xcite .",
    "of particular relevance to our work is the joint modeling of hierarchy and attributes . in @xcite ,",
    "a ranking classifier is trained using attributes for label embedding ; showing that using hierarchical labels along  attributes as side information improved the zero - shot performance . in the recent work of @xcite , a hierarchy and exclusion graph",
    "is learned over the various object categories .",
    "the graph models binary relations among the classes like mutual exclusion and overlap .",
    "they also model similar relations between objects and  attributes and use it for zero - shot classification .",
    "we exploit the hierarchical structure of the categories in two aspects .",
    "we leverage the hierarchy to automatically propagate annotations and learn attributes at different levels of abstraction",
    ". then we use it in guiding the transfer process to select the most promising knowledge source of attributes to share with novel classes . to the best of our knowledge ,",
    "this has not been done before .",
    "hierarchical representation of concepts and objects is part of the human understanding of the surrounding world .",
    "we usually try to combine objects into certain groups based on a common criteria like functionality or visual similarity .",
    "this helps us to better learn the commonality as well as the differences in and across groups .",
    "the key idea of our approach is to take advantage of the embedded structure in the object category space and extend the notion of  attributes to include different levels of abstraction .",
    "the object hierarchy groups the classes based on their overall visual similarity ; thus provides a natural way to guide the transfer process to share information from the knowledge sources that will most likely contain relative information .    in the following ,",
    "we describe the three main steps of our method . starting with a set of classes ,  attributes and a hierarchy in the category space :",
    "( 1 ) we automatically populate the hierarchy with additional attributes ; ( 2 ) learn these attributes to capture subtle differences between similar categories ; and finally , ( 3 ) use a hierarchy - guided transfer to select the proper attributes to share with a novel class .",
    "we start by defining the notation used throughout the paper .    * notation . *",
    "let @xmath0 be a set of categories , where a subset of these categories @xmath1 have training samples while the rest @xmath2 have none , and @xmath3 .",
    "a set of semantic attributes @xmath4 describe all classes in @xmath5 .",
    "a directed acyclic graph @xmath6 defines a hierarchy over the classes , with nodes @xmath7 and edges @xmath8 .",
    "an attribute @xmath9 at node @xmath10 is referred to as @xmath11 .      to get the attribute description of the inner nodes we exploit the hierarchy @xmath12 by transferring the attributes annotation in a bottom - up approach from the seen classes @xmath13 ( leaf nodes ) to the root .",
    "for example , the inner node _ dog _",
    "( a ) has the attributes _ patches _ leveraged from the child nodes _ collie _ and _ dalmatian _ , which in turn propagates _ patches _ along other attributes up to node _",
    "carnivore_. then , the active attributes of node @xmath14 are defined as @xmath15 where @xmath16 is the set of nodes of the subtree rooted with @xmath17 .",
    "consequently , the root node of @xmath12 will be described with all attributes of @xmath13 .",
    "it s important to note that the attribute label propagation is used to find the active attributes at a certain node and to guide the transfer process afterwards .",
    "this does not change the underlying ground truth attribute labels of samples whether it is class - based or image - based annotation .      to learn the various attributes classifiers , we first define the support set of an attribute @xmath9 , the set of samples that provide evidence of @xmath9 .",
    "an attribute @xmath18 in the hierarchy has the support set @xmath19 .",
    "the set contains samples labeled with the attribute of that class ( @xmath20 ) , and additionally the samples of its children which share the same attribute with @xmath14 , @xmath21    to capture the fine differences that characterize an attribute at node @xmath17 , we use a child - vs - parent learning scheme @xcite .",
    "the attribute @xmath22 is learned with the following positive ( @xmath23 ) and negative ( @xmath24 ) sets @xmath25 where @xmath26 is the parent node of @xmath27 .",
    "for example , the attribute _ paws _ of node _ bear _ ( b ) is supported by _",
    "paws _ samples from the classes _ polar - bear _ and _ grizzly - bear_. then , @xmath28 is learned to capture the differences against the other _ paws _ samples from parent _ carnivore_. the root @xmath29 of @xmath12 has no parent ; hence @xmath30 are learned in the standard 1-vs - all scheme . in other words",
    ", the root attributes naturally map to the commonly used  attributes in the literature .      having the attributes learned at different levels of abstraction",
    ", we then leverage the hierarchy to guide the knowledge transfer process and find the proper attributes to transfer to novel classes .",
    "analyzing the recall and precision properties of the attribute classifiers in @xmath12 , we generally notice that the attribute predictions towards the leaf level of @xmath12 have higher precision and lower recall than the ones towards the root of the hierarchy , which are characterized with low precision and high recall .",
    "this is expected from a learning scheme as the one we use . while the lower levels capture the discriminative small differences that distinguish a small group of classes against another regarding an attribute ( high precision ) , in the higher levels of @xmath12 , the common visual properties of the attribute across a larger set of categories",
    "is learned ( high recall ) .",
    "similar to @xcite , we find that combining classifiers with such an opposite recall and precision primacy results in an improved performance when compared to the constituent classifiers",
    ". furthermore , the combination of classifiers from different levels in the hierarchy increases the robustness of the final classifier against noise that might be produced by the constituent classifiers .",
    "accordingly , for a novel class @xmath31 in @xmath12 , we transfer the attributes of its ancestors across the different levels of abstraction ( @xmath32 in c ) , such that : @xmath33~s_{n_i}(a_m^{n_i}|\\mathbf{x})}{\\sum\\limits_{n_i\\in \\mathrm{anc}(z_l)}[[a_m^{z_l}=a_m^{n_i}]]},\\ ] ] where @xmath34 $ ] is the iverson bracket ( @xmath35=1 $ ] if condition @xmath36 is true and 0 otherwise ) , @xmath37 is the score of the attribute @xmath9 for node @xmath17 given sample @xmath38 , and @xmath39 is the set of ancestor nodes of @xmath17 . once the attributes are transferred to @xmath31 , the final prediction score @xmath40 of the @xmath31 category can be defined by averaging over the attributes of that class as : @xmath41~s(a_m^{z_l}|\\mathbf{x})}{\\sum\\limits_{m=1}^m [ [ a_m^{z_l}=1]]}.\\ ] ] in the following , we refer to our hierarchical attribute transfer model as .",
    "we evaluate our model on three datasets .",
    "each provides different characteristics regarding the granularity of classes .",
    "this give us the chance to see how the performance of the proposed  model varies with regards to the complexity of the embedded knowledge in the source set .",
    "next , we present the three datasets , the hierarchy and the features we extract from images to train the different attribute models",
    ".    * datasets . * * ( 1 ) * the apascal / ayahoo ( ) @xcite contains two subsets .",
    "the first ( ap ) uses 12,695 images and 20 categories from pascal voc 2008 @xcite .",
    "the second ( ay ) has 12 disjoint classes and 2,644 images collected from the yahoo image search engine . per - image",
    "labels of 64 binary attributes are provided for both subsets . in the predefined zero - shot split , ap is used for training and ay for testing .    * ( 2 ) * the animals with attributes ( ) @xcite consists of 30,475 images of 50 classes of animals .",
    "they are described with 85 semantic attributes on the class level .",
    "the authors provide a fixed split for zero - shot experiments .",
    "they select 40 classes for training and 10 for testing .    *",
    "( 3 ) * the cub-200 - 2011 birds ( ) @xcite has 200 bird classes and 11,788 images . each image is labeled with 312 attributes . unlike the previous two , there is no predefined split for this dataset . for our experiments ,",
    "we randomly select 150 classes for training and 50 for testing .",
    "* hierarchy .",
    "* we learn the object hierarchies using the wordnet ontology . by querying the ontology with the category labels we extract a tree that brings the classes into a hierarchical ordering .",
    "subsequently , we prune the hierarchy to remove intermediate nodes that have a single child .    *",
    "* deep features . * * motivated by the impressive success of deep convolutional neural networks ( cnn ) , we encode the images using a cnn - based deep representation to train the attribute classifiers .",
    "we use the cnn model cnn - m2k provided by @xcite which has a structure similar to alexnet @xcite .",
    "we also use the bvlc implementation @xcite of googlenet @xcite which has a much deeper architecture .",
    "we follow the best practice found in @xcite to extract the deep representation .",
    "the image is resized to 256 x 256 . then",
    "5 image crops obtained from center and corners with their flipped versions are fed to the cnn .",
    "the output of the last hidden layer is extracted , sum - pooled and l2-normalized to be used as our deep - features to train the different models . for all our attribute classifiers ,",
    "we use linear svms @xcite with regularized logistic regression .",
    "the cost parameter c is estimated using 5-folds cross validation .",
    "we first evaluate the performance of the deep features that we use in learning attribute classifiers .",
    "we compare their performance with attributes learned using `` shallow '' features .",
    "we use the precomputed features provided by @xcite and @xcite as the `` shallow '' representation for apay  and awa respectively . for cub",
    ", we encode the images with fisher vectors based on sift and color descriptors and use that as the shallow representation . in apay , we consider two setups : ( i ) within - category attribute prediction ( apap ) , the evaluation is done on the apascal testing set ; ( ii ) across - category prediction ( apay ) , we evaluate on the disjoint ayahoo set . in both cases , the attributes are learned using the apascal training set . for awa and cub",
    ", we use the zero - shot testing setups defined in the previous section .    in",
    ", we show the performance of the three representations in terms of mean auc under roc of the attribute predictions .",
    "the deep - feature models constantly outperform thier counterpart across all datasets with an increase between 7% to 15% .",
    "this high performance of the deep features in attribute prediction is expected .",
    "cnns automatically learn to capture features with varying complexity at each layer .",
    "while the lower layers learn features like edges and color patches , the higher layers learn much complex structures of the object like parts @xcite .",
    "many of these correspond directly to semantic attributes which explains the impressive performance of the deep representation using our simple linear classifiers .      to populate the hierarchy with attributes ( ) , our model requires class - based attribute descriptions .",
    "hence , for apay and cub we average all image - based attribute vectors of each class to calculate the class - attribute occurrence matrices .",
    "then , the binary class - attribute notations are created by thresholding the resulting occurrence matrix at its overall mean value . along with our hierarchical attribute transfer model ( ) ,",
    "we also train two common baselines for  attributes using our deep features : ( i ) the direct attribute prediction model ( dap ) , where the class prediction is formulated as a map estimation @xcite ; ( ii ) the ensemble model ( ens ) , that combines the predictions of the attributes using a sum formulation similar to the one we use in but based on  attributes @xcite . for ens and",
    ", we normalize the prediction scores of the classes to have zero mean and unit standard deviation .    * vs. state - of - the - art . * in",
    ", we report the normalized multi - class accuracy on the three test sets .",
    "our model outperforms the state - of - the - art on the three datasets with a wide margin . in @xcite , a model that uses object hierarchy as",
    "side information is used to learn attributes based on fisher vectors . in the recent work of @xcite , a hierarchical model of objects",
    "is learned using deep features similar to the one we use .",
    "finally , @xcite a transductive multi - view embedding is learned using global attributes , word space and low - level features .",
    "nonetheless , our model improves over the best state - of - the - art results by 26% ( apay ) , 28%",
    "( awa ) and 35% ( cub ) .",
    "* vs. deep - feature baseline . * compared to our strong baseline with deep features ( dap and ens ) , still performs the best in terms of both multi - class accuracy and mean auc ( ) . shows the highest ranking results obtained by the three models for each test class in the awa dataset .",
    "while distinctive classes like _ chimpanzee _ and _ humpback - whale _ are correctly classified by all models , both dap and ens confuse visually similar classes that share many global attributes like _ leopard _ & _ persian - cat _ and _ rat _ & _ raccoon _ , and more samples are wrongly ranked high by these models . to the contrary ,",
    "learns the fine differences among the shared attributes of these classes which helps in discriminating them efficiently .",
    "for example , learns the differences between the attributes of _ big - cat _ and _ cat _ ( c ) which facilitates the separation among the novel classes _ leopard _ and _ persian - cat _ ( ) .",
    "furthermore , we find that normalizing the prediction scores of the novel classes ( ) to have a zero mean and unit standard deviation makes the scores more comparable .",
    "this improves the accuracy of both the baseline ( ens ) and our model ( ) with the latter surpassing the former .",
    "however , this requires that the test data is available as a batch at the test time .",
    "the relative improvement in accuracy of our model over the baseline is 6% ( apay ) , 18% ( awa ) and 31% ( cub ) .",
    "this trend nicely follows the level of granularity of the objects in the data sets .",
    "our model takes advantage of the underlying structure and the commonality among the classes and it is able to distinguish between fine grained classes more efficiently . on the other hand , it is harder for the baseline models ( dap & ens ) to distinguish such fine grained objects using the abstract  attributes .    in the rest of the experiments , we report the results when using the googlenet features for our model and the baselines .    0.5        0.5        * per - image vs. per - class attributes .",
    "* apay and cub provide attribute annotation at the image level which we used to train the models in the previous experiments .",
    "we evaluate on these two datasets using class - based attributes similar to those in awa .",
    "we notice that the accuracy of both the baseline and decreases in these settings . where ens has 38.3% and 34.6% , the model achieves 40.3% and 48.2% on apay and cub .",
    "this seems to differ from the findings in @xcite .",
    "the reason could be related to the type of features used . in @xcite a set of shallow features are used which require a relatively larger number of samples to train good attribute classifiers .",
    "this in turn results in noisy attribute predictions if there are few image - based annotations of the attribute . in comparison , using the deep features we can learn better attribute classifiers even if the training data is relatively sparse .",
    "c c c c c c c c c c chimpanzee&giant - panda&leopard&persian - cat&pig&hippo - potamus&humpback - whale&raccoon&rat&seal +    1.0      +    1.0      +    1.0        * unknown attributes of the novel class .",
    "* although this evaluation setup is not possible with the  attribute model , enables us to carry out zero - shot recognition even if the attribute description of the novel class is unknown . to do that , we again leverage the hierarchy and transfer the attribute description of the parent node to the novel class . using this setup ,",
    "achieves an accuracy of 31.1% ( apay ) , 59.7% ( awa ) and 32.6% ( cub ) .",
    "this drop in performance is reasonable since we are transferring the more generic attributes of the parent .",
    "hence , confusion can arise when multiple test classes share the same parent in the hierarchy .",
    "nonetheless , makes it possible to perform attribute - based zero - shot classification when only the label of the novel class is available .",
    "* source set complexity .",
    "* in the following experiment , we vary the complexity of the knowledge contained in the source ( the number of seen classes ) compared to the target ( the unseen classes ) .",
    "this helps to have a better understanding of the characteristics of the different models as the richness of the embedded information in the source changes .",
    "we use the cub dataset and start with a random set of 25 classes to be in the source .",
    "we gradually increase the source set with additional 25 random classes . at each step , the rest of the 200 classes is used as the target set to conduct zero - shot classification .    in we see that when the source is relatively poor and contains less structured knowledge , both dap and performs at the same level .",
    "however , as the source get bigger and more complex consistently outperforms dap with an increasingly wider margin .",
    "unlike dap that uses a single layer of   attributes , is able to take advantage of the complexity of information available in the source .",
    "captures the commonality among the categories and exploits it to learn and transfer more discriminative attributes to distinguish the unseen categories .",
    "in this paper , we present a simple yet very effective model for zero - shot object recognition .",
    "our model takes advantage of the embedded structure in the category space to learn attributes at different levels of abstraction .",
    "furthermore , it exploits inter - class relations to provide a guided knowledge transfer approach that can select and transfer the expected relevant attributes to a novel class .",
    "the evaluation on three challenging datasets shows the superior performance of the proposed model over the state - of - the - art .",
    "we are considering to extend our approach in different directions . in the current model ,",
    "we assume similar importance for the transferred attributes from the different layers in the hierarchy . however , the confidence in the attributes predictions and their relative relatedness to the novel class differ through the different abstraction levels . using adaptive weights when transferring attributes could improve the model performance .",
    "moreover , we considered only the ancestor nodes as a knowledge source .",
    "the siblings of a novel class represent another promising option . including them in the transfer process may help in sharing more discriminative attributes ."
  ],
  "abstract_text": [
    "<S> attribute based knowledge transfer has proven very successful in visual object analysis and learning previously unseen classes . </S>",
    "<S> however , the common approach learns and transfers attributes without taking into consideration the embedded structure between the categories in the source set . </S>",
    "<S> such information provides important cues on the intra - attribute variations . </S>",
    "<S> we propose to capture these variations in a hierarchical model that expands the knowledge source with additional abstraction levels of attributes . </S>",
    "<S> we also provide a novel transfer approach that can choose the appropriate attributes to be shared with an unseen class . we evaluate our approach on three public datasets : apascal , animals with attributes and cub-200 - 2011 birds . </S>",
    "<S> the experiments demonstrate the effectiveness of our model with significant improvement over state - of - the - art . </S>"
  ]
}