{
  "article_text": [
    "distance metric learning has attracted a lot of research interest recently in the machine learning and pattern recognition community due to its wide applications in various areas @xcite .",
    "methods relying upon the identification of an appropriate data - dependent distance metric have been applied to a range of problems , from image classification and object recognition , to the analysis of genomes . the performance of many classic algorithms such as @xmath4-nearest neighbor ( @xmath5nn ) and @xmath4-means clustering depends critically upon the distance metric employed .",
    "large - margin metric learning is an approach which focuses on identifying a metric by which the data points within the same class lie close to each other and those in different classes are separated by a large margin .",
    "weinberger s large - margin nearest neighbor ( lmnn ) @xcite is a seminal work illustrating the approach whereby the metric takes the form of a mahanalobis distance .",
    "given input data @xmath6 , this approach to the metric learning problem can be framed as that of learning the linear transformation @xmath7 which optimizes a criterion expressed in terms of euclidean distances amongst the projected data @xmath8 . in order to obtain a convex problem , instead of learning the projection matrix ( @xmath9 ) , one usually optimizes over the quadratic product of the projection matrix ( @xmath10 ) @xcite .",
    "this linearization _ convexifies _ the original non - convex problem .",
    "the projection matrix may then be recovered by an eigen - decomposition or cholesky decomposition of @xmath11 .",
    "typical methods that learn the projection matrix @xmath12 are most of the spectral dimensionality reduction methods such as principle component analysis ( pca ) , fisher linear discriminant analysis ( lda ) ; and also neighborhood component analysis ( nca ) @xcite , relevant component analysis ( rca ) @xcite .",
    "goldberger showed that nca may outperform traditional dimensionality reduction methods @xcite .",
    "nca learns the projection matrix directly through optimization of a non - convex objective function .",
    "nca is thus prone to becoming trapped in local optima , particularly when applied to high - dimensional problems .",
    "rca @xcite is an unsupervised metric learning method .",
    "rca does not maximize the distance between different classes , but minimizes the distance between data in chunklets .",
    "chunklets consist of data that come from the same ( although unknown ) class .",
    "more methods on the topic of large - margin metric learning actually learn @xmath11 directly since xing @xcite proposed a global distance metric learning approach using a convex optimization method .",
    "although the experiments in @xcite show improved performance on clustering problems , this is not the case when the method is applied to most classification problems .",
    "davis @xcite proposed an information theoretic metric learning ( itml ) approach to the problem .",
    "the closest work to ours may be lmnn @xcite and boostmetric @xcite .",
    "lmnn is a mahanalobis metric form of @xmath13-nn whereby the mahanalobis metric is optimized such that the @xmath13-nearest neighbors are encouraged to belong to the same class while data points from different classes are separated by a large margin .",
    "the optimization take the form of an sdp problem .",
    "in order to improve the scalability of the algorithm , instead of using standard sdp solvers , weinberger @xcite proposed an alternating estimation and projection method . at each iteration ,",
    "the updated estimate @xmath11 is projected back to the semidefinite cone using eigen - decomposition , in order to preserve the semi - definiteness of @xmath11 . in this sense , at each iteration ,",
    "the computational complexity of their algorithm is similar to that of ours .",
    "however , the alternating method needs an extremely large number of iterations to converge ( the default value being @xmath14 in the authors implementation ) .",
    "in contrast , our algorithm solves the corresponding lagrange dual problem and needs only @xmath15 iterations in most cases .",
    "in addition , the algorithm we propose is significantly easier to implement .",
    "as pointed in these earlier work , the disadvantage of solving for @xmath16 is that one needs to solve a semidefinite programming ( sdp ) problem since @xmath11 must be positive semidefinite ( ) . conventional interior - point sdp solvers have a computation complexity of @xmath17 , with @xmath18 the dimension of input data .",
    "this high complexity hampers the application of metric learning to high - dimensional problems .    to tackle this problem ,",
    "here we propose here a new formulation of quadratic mahalanobis metric learning using proximity comparison information and frobenius norm regularization .",
    "the main contribution is that , with the proposed formulation , we can very efficiently solve the sdp problem in the dual space . because strong duality holds , we can then recover the primal variable @xmath11 from the dual solution .",
    "the computational complexity of the optimization is dominated by eigen - decomposition , which is @xmath19 and thus the overall complexity is @xmath20 , where @xmath21 is the number of iterations required for convergence .",
    "note that @xmath22 does not depend on the size of the data , and is typically @xmath23 .",
    "a number of methods exist in the literature for large - scale metric learning .",
    "shen  @xcite introduced boostmetric by adapting the boosting technique , typically applied to classification , to distance metric learning .",
    "this work exploits an important theorem which shows that a positive semidefinite matrix with trace of one can always be represented as a convex combination of multiple rank - one matrices .",
    "the work of shen generalized lpboost @xcite and adaboost by showing that it is possible to use matrices as weak learners within these algorithms , in addition to the more traditional use of classifiers or regressors as weak learners .",
    "the approach we propose here , frobmetric , is inspired by boostmetric in the sense that both algorithms use proximity comparisons between triplets as the source of the training information .",
    "the critical distinction between frobmetric and boostmetric , however , is that reformulating the problem to use the frobenius regularization  rather than the trace norm regularization  allows the development of a dual form of the resulting optimization problem which may be solved far more efficiently .",
    "the boostmetric approach iteratively computes the squared mahalanobis distance metric using a rank - one update at each iteration .",
    "this has the advantage that only the leading eigenvector need be calculated , but leads to slower convergence . indeed , for boostmetric",
    ", the convergence rate remains unclear .",
    "the proposed frobmetric method , in contrast , requires more calculations per iteration , but converges in significantly fewer iterations . actually in our implementation",
    ", the convergence rate of frobmetric is guaranteed by the employed quasi - newton method .",
    "the main contributions of this work are as follows .    1 .",
    "we propose a novel formulation of the metric learning problem , based on the application of frobenius norm regularization .",
    "we develop a method for solving this formulation of the problem which is based on optimizing its lagrange dual .",
    "this method may be practically applied to a much more complex datasets than the competing sdp approach , as it scales better to large databases and to high dimensional data .",
    "we generalize the method such that it may be used to solve any frobenius norm regularized sdp problem .",
    "such problems have many applications in machine learning and computer vision , and by way of example we show that it may be used to approximately solve the frobenius norm perturbed maximum variance unfolding ( mvu ) problem @xcite .",
    "we demonstrate that the proposed method is considerably more efficient than the original mvu implementation on a variety of data sets and that a plausible embedding is obtained .",
    "the proposed scalable semidefinite optimization method can be viewed as an extension of the work of boyd and xiao @xcite .",
    "the subject of boyd and xiao in @xcite was similarly a semidefinite least - squares problem : finding the covariance matrix that is closest to a given matrix under the frobenius norm metric . here",
    "we study the large - margin mahalanobis metric learning problem , where , in contrast , the objective function is not a least squares fitting problem .",
    "we also discuss , in section  [ sec : general ] the application of the proposed approach to general sdp problems which have frobenius norm regularization terms .",
    "note also that a precursor to the approach described here also appeared in @xcite . here",
    "we have provided more theoretical analysis as well as experimental results .    in summary , we propose a simple , efficient and scalable optimization method for quadratic mahalanobis metric learning .",
    "the formulated optimization problem is convex , thus guaranteeing that the global optimum can be attained in polynomial time @xcite .",
    "moreover , by working with the lagrange dual problem , we are able to use off - the - shelf eigen - decomposition and gradient descent methods such as l - bfgs - b to solve the problem .      a column vector is denoted by a bold lower - case letter ( @xmath24 ) and a matrix is by a bold upper - case letter ( @xmath16 ) .",
    "the fact that a matrix @xmath25 is positive semidefinite ( ) is denoted thus @xmath26 .",
    "the inequality @xmath27 is intended to indicate that @xmath28 . in the case of vectors , @xmath29 denotes the element - wise version of the inequality , and when applied relative to a scalar ( _ e.g. _ , @xmath30 ) the inequality is intended to apply to every element of the vector . for matrices ,",
    "we denote by @xmath31 the vector space of real matrices of size @xmath32 , and the space of real symmetric matrices as @xmath33 .",
    "similarly , the space of symmetric matrices of size @xmath34 is @xmath35 , and the space of symmetric positive semidefinite matrices of size @xmath34 is denoted as @xmath36 .",
    "the inner product defined on these spaces is @xmath37 . here",
    "@xmath38 calculates the trace of a matrix .",
    "the frobenius norm of a matrix is defined as @xmath39 , which is the sum of all the squared elements of @xmath11 .",
    "@xmath40 extracts the diagonal elements of a square matrix .",
    "given a symmetric matrix @xmath11 and its eigen - decomposition @xmath41 ( @xmath42 being an orthonormal matrix , and @xmath43 being real and diagonal ) , we define the positive part of @xmath11 as @xmath44      \\u^\\t,\\ ] ] and the negative part of @xmath11 as @xmath45 \\u^\\t.\\ ] ] clearly , @xmath46 holds .",
    "our proposed method relies on the following standard results , which can be found in textbooks such as chapter 8 of @xcite .",
    "the positive semidefinite part @xmath47 of @xmath11 is the projection of @xmath11 onto the cone : @xmath48 it is not difficult to check that , for any @xmath49 , @xmath50 in other words , although the optimization problem in appears as an sdp programming problem , it can be simply solved by using eigen - decomposition , which is efficient .",
    "it is this key observation that serves as the backbone of the proposed fast method .",
    "the rest of the paper is organized as follows . in section",
    "[ sec : alg ] , we present the main algorithm for learning a mahalanobis metric using efficient optimization . in section",
    "[ sec : general ] , we extend our algorithm to more general frobenius norm regularized semidefinite problems .",
    "the experiments on various datasets are in shown section [ experiments ] and we conclude the paper in section [ conclusion ] .",
    "we now briefly review quadratic mahalanobis distance metrics .",
    "suppose that we have a set of triplets @xmath51 , which encodes proximity comparison information .",
    "suppose also that @xmath52 computes the mahalanobis distance between @xmath53 and @xmath54 under a proper mahalanobis matrix .",
    "that is , @xmath55 , where @xmath56 , is positive semidefinite .",
    "such a mahanalobis metric may equally be parameterized by a projection matrix @xmath7 where @xmath57 .",
    "let us define the margin associated with a training triplet as @xmath58 with @xmath59 . here",
    "@xmath60 represents the index of the current triplet within the set of @xmath61 training triplets @xmath62 .",
    "as will be shown in the experiments below , this type of proximity comparison among triplets may be easier to obtain that explicit distances for some applications like image retrieval . here",
    "the metric learning procedure solely relies on the matrices @xmath63 ( @xmath64 ) .      putting it into the large - margin learning framework ,",
    "the optimization problem is to maximize the margin with a regularization term that is intended to avoid over - fitting ( or , in some cases , makes the problem well - posed ) : @xmath65 here @xmath66 removes the scale ambiguity in @xmath16 .",
    "this is the formulation proposed in boostmetric @xcite .",
    "we can write the above problem equivalently as @xmath67 these formulations are exactly equivalent given the appropriate choice of the trade - off parameters @xmath68 and @xmath69 .",
    "the theorem is as follows .",
    "a solution of , @xmath70 , is also a solution of and vice versa up to a scale factor .",
    "more precisely , if with parameter @xmath71 has a solution @xmath72 , @xmath73 , @xmath74 , then @xmath75 is the solution of with parameter @xmath76 . here",
    "@xmath77 is the optimal objective value of .",
    "it is well known that the necessary and sufficient conditions for the optimality of sdp problems are primal feasibility , dual feasibility , and equality of the primal and dual objectives .",
    "we can easily derive the duals of and respectively : @xmath78 and , @xmath79 here @xmath80 is the identity matrix .",
    "let @xmath81 , @xmath73 , @xmath82 represent the optimum of the primal problem .",
    "primal feasibility of implies primal feasibility of , and thus that @xmath83    let @xmath84 be the optimal solution of the dual problem .",
    "dual feasibility of implies dual feasibility of , and thus that @xmath85 , and @xmath86 . since the duality gap between and is zero , @xmath87 .",
    "last we need to show that the objective function values of and are the same .",
    "this is easy to verify from the fact that @xmath88 : @xmath89 this concludes the proof .",
    "both problems can be written in the form of standard sdp problems since the objective function is linear and a constraint is involved .",
    "recall that we are interested in a frobenius norm regularization rather that a trace norm regularization .",
    "the key observation is that _ the frobenius norm regularization term leads to a simple and scalable optimization_. so replacing the trace norm in with the frobenius norm we have : @xmath90 although and are not exactly the same , the only difference is the regularization term .",
    "different regularizations can lead to different solutions .",
    "however , as the @xmath91 and @xmath92 norm regularizations in the case of vector variables such as in support vector machines ( svm ) , in general , these two regularizations would perform similarly in terms of the final classification accuracy . here",
    ", one does not expect that a particular form of regularization , either the trace or frobenius norm regularization , would perform better than the other one . as we have pointed out , the advantage of the frobenius norm is faster optimization",
    ".    one may convert into a standard sdp problem by introducing an auxiliary variable : @xmath93 the last constraint can be formulated as a constraint @xmath94 so in theory , we can use an off - the - shelf sdp solver to solve this primal problem directly .",
    "however , as mentioned previously , the computational complexity of this approach is very high , meaning that only small - scale problems can be solved within reasonable cpu time limits .",
    "next , we show that , the lagrange dual problem of has some desirable properties .",
    "we first introduce the lagrangian dual multipliers , @xmath95 which we associate with the constraint @xmath96 , and @xmath97 which we associate with the remaining constraints upon @xmath98 .",
    "the lagrangian of then becomes @xmath99 with @xmath100 and @xmath101 .",
    "we need to minimize the lagrangian over @xmath11 and @xmath102 , which can be done by setting the first derivative to zero , from which we see that @xmath103 and @xmath104 . substituting the expression for @xmath11 back into the lagrangian",
    ", we obtain the dual formulation : @xmath105 this dual problem still has a constraint and it is not clear how it may be solved more efficiently than by using standard interior - point methods .",
    "note , however , that as both the primal and dual problems are convex , slater s condition holds , under mild conditions ( see @xcite for details ) .",
    "strong duality thus holds between and , which means that the objective values of these two problem coincide at optimality and in many cases we are able to indirectly solve the primal by solving the dual and vice versa .",
    "the karush ",
    "tucker ( kkt ) conditions thus enable us to recover @xmath70 , which is the primal variable of interest , from the dual solution .    given a fixed @xmath106 , the dual problem may be simplified @xmath107 to simplify the notation we define @xmath108 as a function of @xmath97 @xmath109 problem then becomes that of finding the  matrix @xmath95 such that @xmath110 is minimized .",
    "this problem has a closed - form solution , which is the positive part of @xmath111 : @xmath112 now the original dual problem may be simplified @xmath113 the kkt condition is simplified into @xmath114 from the definition of the operator @xmath115 , @xmath70 computed by must be note that we have now achieved a simplified dual problem which has no matrix variables , and only simple box constraints on @xmath97 .",
    "the fact that the objective function of is differentiable ( but not twice differentiable ) allows us to optimize for @xmath97 in using gradient descent methods ( see sect .",
    "5.2 in @xcite ) . to illustrate why the objective function is differentiable",
    ", we can see the following simple example . for @xmath116 ,",
    "the gradient can be calculated as @xmath117 because of the following fact .",
    "given a symmetric @xmath118 , we have @xmath119 this can be verified by using the perturbation theory of eigenvalues of symmetric matrices .",
    "when we set @xmath118 to be very small , the above equality is the definition of gradient .",
    "hence , we can use a sophisticated off - the - shelf first - order newton algorithm such as l - bfgs - b @xcite to solve . in summary ,",
    "the optimization procedure is as follows .    1 .",
    "input the training triplets and calculate @xmath120 , @xmath121 .",
    "2 .   calculate the gradient of the objective function in , and use l - bfgs - b to optimize .",
    "3 .   calculate @xmath122 using the output of l - bfgs - b ( namely , @xmath123 ) and compute @xmath70 from using eigen - decomposition .    to implement this approach ,",
    "one only needs to implement the callback function of l - bfgs - b , which computes the gradient of the objective function of .",
    "note that other gradient methods such as conjugate gradients may be preferred when the number of constraints ( i.e. , the size of training triplet set , @xmath124 ) is large .",
    "the gradient of dual problem can be calculated as @xmath125 so , at each iteration , the computation of @xmath126 , which requires full eigen - decomposition , only need be calculated once in order to evaluate all of the gradients , as well as the function value . when the number of constraints is not far more than the dimensionality of the data , eigen - decomposition dominates the computational complexity at each iteration . in this case , the overall complexity is @xmath127 with @xmath21 being around @xmath15 .",
    "in this section , we generalize the proposed idea to a broader setting .",
    "the general formulation of an sdp problem writes : @xmath128 we consider its frobenius norm regularized version : @xmath129 here @xmath130 is a regularized constant .",
    "we start by deriving the lagrange dual of this frobenius norm regularized sdp .",
    "the dual problem is , @xmath131 the kkt condition is @xmath132 where we have introduced the notation @xmath133 .",
    "keep it in mind that @xmath134 is a function of the dual variable @xmath97 . as in the case of metric learning ,",
    "the important observation is that @xmath95 has an analytical solution when @xmath97 is fixed : @xmath135 therefore we can simplify into @xmath136 so now we can efficiently solve the dual problem using gradient descent methods .",
    "the gradient of the dual function is @xmath137 at optimality , we have @xmath138 .    the core idea of the proposed method here may be applied to an sdp which has a term in the format of frobenius norm , either in the objective function or in the constraints .    in order to demonstrate the performance of the proposed general frobenius norm sdp approach",
    ", we will show how it may be applied to the problem of maximum variance unfolding ( mvu ) .",
    "the mvu optimization problem writes @xmath139 here @xmath140 , @xmath141 , encode the local distance constraints .",
    "this problem can be solved using off - the - shelf sdp solvers , which , as is described above , does not scale well . using the proposed approach , we modify the objective function to @xmath142 . when @xmath130 is sufficiently large , the solution to this frobenius norm perturbed version is a reasonable approximation to the original problem .",
    "we thus use the proposed approach to solve mvu approximately .",
    "we first run metric learning experiments on uci benchmark data , face recognition , and action recognition datasets .",
    "we then approximately solve the mvu problem @xcite using the proposed general frobenius norm sdp approach .",
    "l||c|c|c|c|c|c|c & mnist & usps & letters & yale faces & bal & wine & iris +    # samples & 70,000 & 11,000 & 20,000 & 2,414 & 625 & 178 & 150 +    # triplets & 450,000 & 69,300 & 94,500 & 15,210 & 3,942 & 1,125 & 945 +    dimension & 784 & 256 & 16 & 1,024 & 4 & 13 & 4 +    dimension after pca & 164 & 60 & & 300 & & & +    # training & 50,000 & 7,700 & 10,500 & 1,690 & 438 & 125 & 105 +    # validation & 10,000 & 1,650 & 4,500 & 362 & 94 & 27 & 23 +    # test & 10,000 & 1,650 & 5,000 & 362 & 93 & 26 & 22 +    # classes & 10 & 10 & 26 & 38 & 3 & 3 & 3 +    # runs & 1 & 10 & 1 & 10 & 10 & 10 & 10 +    * error rates @xmath143 * & & & & & & & + euclidean & 3.19 & 4.78 ( 0.40 ) & 5.42 & 28.07 ( 2.07 ) & 18.60 ( 3.96 ) & 28.08 ( 7.49 ) & 3.64 ( 4.18 ) +    pca & 3.10 & 3.49 ( 0.62 ) & - & 28.65 ( 2.18 ) & - & - & - +    lda & 8.76 & 6.96 ( 0.68 ) & 4.44 & 5.08 ( 1.15 ) & 12.58 ( 2.38 ) & 0.77 ( 1.62 ) & * 3.18 ( 3.07 ) * +    svm & 2.97 & * 2.15 ( 0.30 ) * & 2.96 & * 4.94 ( 2.14 ) * & * 5.59 ( 3.61 ) * & 1.15 ( 1.86 ) & 3.64 ( 3.59 ) + rca @xcite & 7.85 & 5.35 ( 0.52 ) & 4.64 & 7.65 ( 1.08 ) & 17.42 ( 3.58 ) & * 0.38 ( 1.22 ) * & * 3.18 ( 3.07 ) * +    nca @xcite & - & - & - & - & 18.28 ( 3.58 ) & 28.08 ( 7.49 ) & * 3.18 ( 3.74 ) * +    lmnn @xcite & * 2.30 * & 3.49 ( 0.62 ) & 3.82 & 14.75 ( 12.11 ) & 12.04 ( 5.59 ) & 3.46 ( 3.82 ) & 3.64 ( 2.87 ) +    itml @xcite & 2.80 & 3.85 ( 1.13 ) & 7.20 & 19.39 ( 2.11 ) & 10.11 ( 4.06 ) & 28.46 ( 8.35 ) & 3.64 ( 3.59 ) +    boostmetric @xcite & 2.76 & 2.53 ( 0.47 ) & 3.06 & 6.91 ( 1.90 ) & 10.11 ( 3.45 ) & 3.08 ( 3.53 ) & * 3.18 ( 3.74 ) * +    frobmetric ( this work ) & 2.56 & 2.32 ( 0.31 ) & * 2.72 * & 9.20 ( 1.06 ) & 9.68 ( 3.21 ) & 3.85 ( 4.44 ) & 3.64 ( 3.59 ) +    * computational time * & & & & & & & + lmnn & 11h & 20s & 1249s & 896s & 5s & 2s & 2s +    itml & 1479s & 72s & 55s & 5970s & 8s & 4s & 4s +    boostmetric & 9.5h & 338s & * 3s * & 572s & * less than 1s * & 2s & * less than 1s * + frobmetric & * 280s * & * 9s * & 13s & * 335s * & * less than 1s * & * less than 1s * & * less than 1s * +      we perform a comparison between the proposed frobmetric and a selection of the current state - of - the - art distance metric learning methods , including rca @xcite , nca @xcite , lmnn @xcite , boostmetric @xcite and itml @xcite on data sets from the uci repository .",
    "we have included results from pca , lda and support vector machine ( svm ) with rbf gaussian kernel as baseline approaches .",
    "the svm results achieved using the libsvm @xcite implementation .",
    "the kernel and regularization parameters of the svms were selected using cross validation . as in @xcite , for some data sets ( mnist , yale faces and usps )",
    ", we have applied pca to reduce the original dimensionality and to reduce noise .",
    "for all experiments , the task is to classify unseen instances in a testing subset . to accumulate statistics ,",
    "the data are randomly split into @xmath144 training / validating / testing subsets , except mnist and letter , which are already divided into subsets .",
    "we tuned the regularization parameter in the compared methods using cross - validation . in this experiment , about @xmath145 of data are used for cross - validation and @xmath145 for testing .    for frobmetric and boostmetric in @xcite",
    ", we use @xmath146-nearest neighbors to generate triplets and check the performance using @xmath146nn . for each training sample @xmath147",
    ", we find its @xmath146 nearest neighbors in the same class and the @xmath146 nearest neighbors in the difference classes . with @xmath146 nearest neighbors information ,",
    "the number of triplets of each data set for frobmetric and boostmetric are shown in table [ table : uciresults ] .",
    "frobmetric and boostmetric have used exactly the same training information .",
    "note that other methods do not use triplets as training data .",
    "the error rates based on @xmath146nn and computational time for each learning metric are shown as well .",
    "experiment settings for lmnn and itml follow the original work @xcite and @xcite , respectively . the identity matrix is used for itml s initial metric matrix . for nca , rca , lmnn , itml and boostmetric",
    ", we used the codes provided by the authors .",
    "we implement our frobmetric in matlab and l - bfgs - b is in fortran and a matlab interface is made .",
    "all the computation time is reported on a workstation with 4 intel xeon e5520 ( 2.27ghz ) cpus ( only single core is used ) and 32 gb ram .",
    "table  [ table : uciresults ] illustrates that the proposed frobmetric shows error rates comparable with state - of - the - art methods such as lmnn , itml , and boostmetric .",
    "it also performs on par with a nonlinear svm on these datasets .    in terms of computation time , frobmetric is much faster than all convex optimization based learning methods ( lmnn , itml , boostmetric ) on most data sets . on high - dimensional data sets with many data points , as the theory predicts , frobmetric is significantly faster than lmnn .",
    "for example , on mnist , frobmetric is almost @xmath148 times faster .",
    "frobmetric is also faster than boostmetric , although at each iteration the computational complexity of boostmetric is lower .",
    "we observe that boostmetric requires significantly more iterations to converge .",
    "next we use frobmetric to learn a metric for face recognition on the `` labeled faces in the wild '' data set @xcite .",
    "+   +   +    in this experiment , we have compared the proposed frobmetric to state - of - the - art methods for the task of face pair - matching problem on the `` labeled faces in the wild '' ( lfw ) @xcite data set .",
    "this is a data set of unconstrained face images , including @xmath149 images of @xmath150 people collected from news articles on the internet .",
    "the dataset is particularly interesting because it captures much of the variation seen in real images of faces .",
    "the face recognition task here is to determine whether a presented pair of images are of the same individual .",
    "so we classify unseen pairs whether each image in the pair indicates same individual or not , by applying m@xmath13nn of @xcite instead of @xmath13nn .",
    "features of face images are extracted by computing @xmath146-scale , @xmath151-dimensional sift descriptors @xcite , which center on @xmath152 points of facial features extracted by a facial feature descriptor , as described in @xcite .",
    "pca is then performed on the sift vectors to reduce the dimension to between @xmath153 and @xmath154 .",
    "since the proposed frobmetric method adopts the triplet - training concept , we need to use individual s identity information to generate the third example in a triplet , given a pair . for _",
    "matched _ pairs , we find the third example that belongs to a _ different _ individual with @xmath13 nearest neighbors ( @xmath4 is between @xmath155 and @xmath156 ) . for _ mismatched _ pairs , we find the @xmath4 nearest neighbors ( @xmath13 is between @xmath155 to @xmath156 ) that have the same identity as one of the individuals in the given pair . some of the generated triplets are shown in figure  [ fig : lfw_trip ] .",
    "we select the regularization parameter using cross validation on view 1 and train and test the metric using the @xmath144 provided splits in view 2 as suggested by @xcite .",
    "[ table : lfw_error2 ]    [ cols=\"^ \" , ]",
    "we have presented an efficient and scalable semidefinite metric learning algorithm .",
    "our algorithm is simple to implement and much more scalable than most sdp solvers .",
    "the key observation is that , instead of solving the original primal problem , we solve the lagrange dual problem by exploiting its special structure .",
    "experiments on uci benchmark data sets as well as the unconstrained face recognition task show its efficiency and efficacy .",
    "we have also extended it to solve more general frobenius norm regularized sdps .",
    "g.  b. huang , m.  ramesh , t.  berg , and e.  learned - miller , `` labeled faces in the wild : a database for studying face recognition in unconstrained environments , '' technical report 07 - 49 , university of massachusetts , amherst , october 2007 ."
  ],
  "abstract_text": [
    "<S> distance metric learning is of fundamental interest in machine learning because the distance metric employed can significantly affect the performance of many learning methods . </S>",
    "<S> quadratic mahalanobis metric learning is a popular approach to the problem , but typically requires solving a semidefinite programming ( sdp ) problem , which is computationally expensive . </S>",
    "<S> standard interior - point sdp solvers typically have a complexity of @xmath0 ( with @xmath1 the dimension of input data ) , and can thus only practically solve problems exhibiting less than a few thousand variables . since the number of variables is @xmath2 , this implies a limit upon the size of problem that can practically be solved of around a few hundred dimensions . </S>",
    "<S> the complexity of the popular quadratic mahalanobis metric learning approach thus limits the size of problem to which metric learning can be applied . </S>",
    "<S> here we propose a significantly more efficient approach to the metric learning problem based on the lagrange dual formulation of the problem . </S>",
    "<S> the proposed formulation is much simpler to implement , and therefore allows much larger mahalanobis metric learning problems to be solved . </S>",
    "<S> the time complexity of the proposed method is @xmath3 , which is significantly lower than that of the sdp approach . </S>",
    "<S> experiments on a variety of datasets demonstrate that the proposed method achieves an accuracy comparable to the state - of - the - art , but is applicable to significantly larger problems . </S>",
    "<S> we also show that the proposed method can be applied to solve more general frobenius - norm regularized sdp problems approximately .    </S>",
    "<S> = 1    mahalanobis distance , metric learning , semidefinite programming , convex optimization , lagrange duality . </S>"
  ]
}