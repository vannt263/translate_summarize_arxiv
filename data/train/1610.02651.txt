{
  "article_text": [
    "with billions of image - based data information added to social networking sites like flickr , instagram everday , it has become challenge to accurately organize the data .",
    "one possible solution is to use hashing techniques with the smallest number of possible bits in order to reduce both storage requirement and query response time .",
    "the hashes correponding to images thus obtained can be used for multiple computer vision tasks such as recogniton , image retrieval and understanding .",
    "hashing based approximate nearest neighbor(ann ) search methods have attracted a lot of attention in the past two decades . apart from this",
    ", considerable amount of research has been done in the past few years on improving zero shot learning algorithms [ @xcite , @xcite , @xcite , @xcite , @xcite , @xcite ] .",
    "zero shot learning requires one to transfer knowledge learnt from the classes present in the training data to the classes which are not been observed yet .",
    "this knowledge is generally available in the form of signatures or attributes along with visual concept .",
    "our main motivation for zero shot hashing ( referred as zsh in the rest of the paper ) comes from the fact that humans learn to visualize an image from just the attributes present in it .",
    "for example , if we are given an dictionary where we store images , then we are likely to keep the image of liger in between the images of tiger and lion @xcite .",
    "this has also been shown by yosinski @xcite , where each filter present in cnn during training learns to detect different features in an image like clothes , face , numbers , etc .",
    "this cumulative knowledge helps to determine the classes to which the given image belongs to .",
    "the concept of attribute based learning has also been used in multiple instance learning , where bag of instances with same labels are created .",
    "these instances contain different attributes of an image such as strips , ears of leopard etc .",
    "+ features learnt from other modalities like text are transferred inductively to images using experience .",
    "the features extraction methods like cnn contain infromation about all the attributes present in a given image . however , they do not contain information about other modalities like text .",
    "thus , we could keep an image of liger in between tiger and lion but we are allowed to create another class called `` liger '' in our dictionary .",
    "moreover , it has been shown that using supervised information of an image along with its features could significantly improve the hash codes of image @xcite , @xcite , @xcite .",
    "+ the primary contributions of this paper are listed below :    1 .   incorporating zero shot learning framework in induction based unsupervised hashing problem .",
    "2 .   learning correspondence between signatures of classes and images while jointly embedding them into a common space .",
    "3 .   a novel but a simple approach to address out - of - sample extension problems associated with hashing images belonging to instances of unseen categories .",
    "the outline of this paper is as follows . in section 2 ,",
    "we discuss the related works done previously and why our method is different from them . in section 3",
    ", we discuss the proposed methodology . in section 4 , we evaluate our method and discuss the results obtained from our experiments . in section 5 ,",
    "we conclude our paper along with works in future .",
    "many hashing methods have been proposed in the past , which can be categorized into two type - data dependent and data independent hashing .",
    "locality sensitive hashing is the most popular data independent hashing technique , which uses randomized projections to generate hash functions and ensures high collision probability for similar data points .",
    "variants of lsh @xcite have been developed by taking different distance measures like mahalanobis distance @xcite , kernel similarity ( @xcite,@xcite ) and @xmath0norm distance @xcite .",
    "other forms of lsh could be found in detail in @xcite . in general data",
    "independent hashing techniques exploit long hashes and several hash tables to achieve better performance in terms of precision and recall , thus rendering them limited in use for large scale applications . on the contrary",
    ", data dependent techniques could be classified into two types - supervised and unsupervised hashing .",
    "these algorithms tend to exploit the available training data to generate short binary codes . among the data dependent methods , pca - based hashing ( @xcite , @xcite ) , supervised and semi - supervised hashing ( @xcite , @xcite , @xcite , @xcite ) and graph based hashing ( @xcite , @xcite ) techniques are quite popular .",
    "it has been shown that leveraging non - linear manifold embedding techniques have helped in generating better pairwise affinity preserving dense binary codes . among well - known hashing algorithms which utilize",
    "this idea is spectral hashing ( @xcite , @xcite ) , which uses the eigenfunctions of the laplacian matrix to capture variation in the data .",
    "anchor graph hashing , popularly known as agh ( @xcite , @xcite ) , uses anchor graphs for generating hash codes for training and out - of - sample data efficiently and effectively . benefiting from the properties of manifold approaches ,",
    "inductive manifold hashing ( imh ) has been proposed in @xcite , which computes the manifold of a given data point according to the manifold of its neighbours . in @xcite ,",
    "a solution has been proposed for preserving the inner - product similarities among raw vectors , while tackling maximum inner product search ( mips ) problem .",
    "apart from these , extensive research has been done in producing multimodal hash codes .",
    "data available to us is in multiple information types and contains both text tags and visual concepts .",
    "the concept behind cross modal hashing ( cmh ) methods , in general , is to project the multimodal data in a common hamming space so that the distance between similar data in heterogeneous modalities are preserved . in @xcite ,",
    "linear cross - modal hashing ( lcmh ) has been proposed , which tries to preserve inter - similarities between different modalities and intra - similarity within each modality . in @xcite , it was proposed to map the data from different modalities into a common subspace using projections learnt from collective matrix factorization techniques . extending the technique of collective matrix factorization , latent semantic sparse hashing @xcite exploits sparse coding to learn hash functions . in @xcite , semantic topic multimodal hashing ( stmh )",
    "is proposed which generates each bit in hash code by finding whether a concept is available in the original data or not . for achieving this ,",
    "it maps the learned multimodal semantic features into a common subspace by modeling text into multiple semantic concepts and corresponding images as latent semantic concepts .",
    "supervised cross - modal hashing algorithms have also been proposed over time . in @xcite , spectral hashing has been incorporated with the multi - view case . in @xcite semantic correlations",
    "are maximized and used to embed semantic labels into the training procedure . in @xcite ,",
    "kernel - based supervised hashing for cross - view similarity search ( ksh - cv ) learns kernel hash functions using adaboost algorithm . to preserve the semantic similarity , @xcite uses multi - class logistic regression to project heterogeneous data into a semantic space and uses a boosting framework to learn hash functions .",
    "the difference between our approach and methods adopted in cross - modal hashing is that other methods assume that the information about a given class is present in both ( textual and image ) modalities while training the algorithm to generate hash codes for images .",
    "while in our case information in one of the mode ( image ) for unknown classes is absent during the training phase .",
    "our methodology to produce hash codes for the images belonging to seen and unseen classes has been explained in this section . in section 3.1",
    ", we introduce notations that have been used throughout the paper . in section 3.2 and 3.3 ,",
    "we propose the approach to hash images belonging to seen classes . in section 3.4 and 3.5",
    "we discuss the approach to hash images of unseen classes .      in this paper , we denote the number of seen classes by @xmath1 and the number of unseen classes by @xmath2 .",
    "let @xmath3 and @xmath4 denote the number of instances belonging to seen classes available to us during training and instances of unseen classes , respectively .",
    "vector and its transpose are denoted by lower case bold roman letters such as @xmath5 and @xmath6 respectively .",
    "uppercase bold roman letters , such as @xmath7 , denote matrices .",
    "@xmath8 represents the indicator function .",
    "@xmath9 represents the frobenius norm .      in the feature space , ideally",
    "the set of classes must form separate clusters such that the data points belonging to a certain class should belong to the cluster representing that class .",
    "initially , the information of only seen classes are available to us . thus our objective is to create clusters using these training instances which represent seen classes .",
    "this objective could be formulated in the same way as that of @xmath10-means clustering but with a little modification that we assign a penalty of @xmath11 if the assigned cluster number for a particular image is different from its true label .",
    "the formulated objective function is shown in eq.[equation1 ] :    @xmath12    where @xmath13 are cluster centers and @xmath14 $ ] is cluster assignments in one - of-@xmath15 encoding format .",
    "this formulation is similar to the one given in @xcite .",
    "though , in their formulation , the authors had assumed that the number of unseen classes were available initially and the number of clusters they assigned were equal to @xmath16 .",
    "we have not assumed that constraint and have chosen the number of clusters @xmath10 to be equal to the number of unseen classes @xmath1 .",
    "@xmath17 is equal to one , if the @xmath18th instance belongs to the @xmath10th cluster . in our experiments",
    ", we chose @xmath11 = 0.9 .    exploiting em algorithm , @xmath13 and @xmath19",
    "are updated iteratively and alternatively by optimizing the objective function . at each iteration , @xmath13 are updated as given in eq .",
    "[ equation2 ]    @xmath20    @xmath19 is updated by assigning each instance to the cluster that minimizes the corresponding term .",
    "@xmath13 are initialized as randomly chosen data points so that they are as far as possible from each other .",
    "let us call these cluster centers @xmath21 as anchors in the rest of the paper .",
    "we also take the mean of the features of given instances corresponding to each of the seen classes and assign the anchor to the particular class according to the euclidean distance with respect to the mean of features corresponding to the instances belonging to that class .",
    "we will embed these in the lower dimensional space using manifold learning .",
    "the number of dimensions in the lower dimensional manifold space is equal to the length of the hash code with which we want to hash an image .      assuming that during training , instances correspond to only the images of seen classes",
    ", we use the cluster centers or anchors obtained by optimizing the eq.[equation1 ] to generate the hash codes for the images corresponding to the seen classes .",
    "let us consider that we have a manifold - based low dimensional embedding @xmath22 corresponding to the @xmath1 cluster centers .",
    "given features of an image @xmath23 belonging to the seen class , we aim to generate an embedding @xmath24 such that it preserves the local neighborhood relationship both in the feature and the embedded space with respect to anchors . to obtain the embedding @xmath24 in the manifold space , given the features @xmath23 of an image corresponding to seen classes , the objective function shown in eq.[equation3 ] is minimized .",
    "@xmath25 where , @xmath26 captures the likelihood that the given image belongs to the @xmath27th cluster .",
    "this can be obtained by calculating the euclidean distance of an image from anchors @xmath28 in the feature space",
    ". these distances from the cluster centers are then converted into probabilities or weights to which cluster , a data point belongs to using the exponential function shown in eq.[equation4 ] .",
    "@xmath29 where , @xmath23 are features of @xmath30th image and @xmath28 is the @xmath27th anchor . here",
    ", @xmath31 is a parameter .",
    "differentiating @xmath32 with respect to @xmath24 and equating it to zero , we obtain equation [ equation6 ] , @xmath33 @xmath34    the proposed method here has been inspired from shen et .",
    "@xcite , where they have provided an inductive formulation to obtain the embedding of any point using the linear combination of the base embeddings .",
    "we take the top @xmath35 weights for our purpose and set the other weights to zero . we will call them as ",
    "nearest anchors  for a given data point throughout the paper .",
    "apart from this , we multiplied the @xmath30th weight by an exponential factor @xmath36 . for our experiments , we took @xmath37 .",
    "this is done so that in the manifold space , the distance between the given data point and the cluster assigned to it gets further decreased .",
    "that is the value of the weight with the highest value is further increased .",
    "finally , these top @xmath35 weights are re - normalized such that they sum to 1 .",
    "+ we use eq.[equation4 ] and eq.[equation6 ] to obtain the embedding of an image with features @xmath23 belonging to any of the seen classes .",
    "finally , we obtain the hash codes for the given image by binarizing the embedding as shown in eq.[equation7 ] .    @xmath38    where @xmath39 is the element - wise sign function defined in eq.[equation8 ] .",
    "@xmath40      our main concept behind producing anchors for new unseen classes is that similar classes share similar attributes .",
    "for example , the classes ` monkey ' and ` chimpanzee ' share many attributes in common with each other , thus having high amount of similarity with each other .",
    "this could be inferred from [ fig : cosinesimilarity ] .",
    "thus , labels for unseen categories could be embedded using the description about how similar they are to the seen classes @xcite .",
    "we utilize the information of similarity between attributes of the classes to obtain the embeddings of the unseen classes .",
    "thus , whenever we learn about the information ( in terms of attributes ) about new class by any means like textual description , we create an anchor corresponding to it . to obtain the anchor we use the cosine similarity measure between the attributes of a new class with respect to the classes , anchor of which has been obtained .",
    "@xmath41    here @xmath42 and @xmath43 are two classes between which cosine similarity is calculated and @xmath44 and @xmath45 are their corresponding attributes in the binary form .",
    "we then use the eq.[equation6 ] to inductively obtain the embedding of the given class in the manifold space or anchor as shown in eq.[equation10 ] .    @xmath46    where , @xmath47 is the embedding in the manifold space for the unseen class @xmath42 .",
    "this anchor is then added to our set of base anchors .      to produce the hash codes for images of unseen classes , we must embed the semantic information of different classes i.e. , attributes in a common space .",
    "the framework proposed by @xcite is computationally cheaper , simple and provides a closed form solution of the problem .",
    "these are the main reasons due to which we adopt their approach .",
    "let us assume that for each of the @xmath1 classes at training stage , we have a signature vector of size @xmath48 such that each element of @xmath48 lies in @xmath49 $ ] .",
    "signatures are represented in a matrix form as @xmath50^{a\\times n_{s } } $ ] .",
    "let us denote all the instances available at training stage by a matrix @xmath51 , where @xmath52 is the length of feature vector of each instance .",
    "all instances are labeled in one hot encoding format , with ground truth labels of each of these instances are represented using as @xmath53 , with positive entry indicating the class to which the instance belongs to . to learn a predictor corresponding to the @xmath1 training classes , the following objective function is optimized : @xmath54    here @xmath55 embeds the semantic information in the form of attributes with image features .",
    "the regularizer chosen is of the following form shown in the equation below .",
    "@xmath56    the first term of the regularizer controls attribute signature so that their representations have a similar euclidean norm on the feature space .",
    "the second term of the regularizer checks that the approach is invariant enough to be generalized to other test feature distribution by bounding the variance of representation of instances on attribute space .",
    "the third term penalises the frobenius norm of the weight matrix to be learned",
    ".    the scalars @xmath57 and @xmath58 are the hyper - parameters .",
    "if following choices are made    1 .",
    "@xmath59 2 .",
    "@xmath60    then a closed solution of eq.[zeroshotequation ] could be obtained as follows : @xmath61    at the testing stage , we are provided with signatures @xmath62^{\\mathbf{a}\\times n_{u}}$ ] of unseen classes @xmath2 .",
    "the probability that new instance from unseen class with feature vector @xmath63 belongs to the @xmath30th class @xmath42 is calculated as : @xmath64      once the probability to which class an image belongs to is calculated , we then use eq.[equation13 ] and eq.[equation6 ] to inductively produce hash codes for the images belonging to the unseen classes . here",
    "also , we take the @xmath35 nearest anchors for our purpose and multiplied the @xmath65 weight by an exponential factor @xmath36 before renormalizing these top @xmath35 weights . the manifold embedding @xmath66 of any instance with features @xmath63 from any of the unseen class is thus calculated as : @xmath67    where @xmath47 is the manifold embedding of the cluster center of class @xmath42 . to create the hash code for",
    "the given instance we binarize the hash code by taking using the @xmath68 function .",
    "we performed our experiments on two datasets : the sun scene attributes dataset @xcite and the animals with attributes dataset ( awa ) @xcite .",
    "awa dataset consists of 30k instances .",
    "images are categorized into one of total 50 different classes each with binary attributes of 85 dimensions . for awa dataset ,",
    "attributes are provided for each class in the dataset .",
    "we used decaf @xcite features for awa dataset .",
    "sun dataset consists of 14,340 images each categorized into one of total 717 different classes with 20 samples for each class . each instance has attributes of 102 dimension with value in [ 0,1 ] . for sun dataset , attribute signature of each class is calculated by averaging the attribute signature of the instances belonging to that class . for sun dataset , we obtained deep features using vgg with 19-layer network @xcite using matconvnet @xcite .",
    "we used six types of dimensionality reduction techniques in our work : local linear embedding ( lle ) @xcite , t - distributed stochastic neighbor embedding ( t - sne ) @xcite , isomap @xcite , kernel - pca @xcite , gaussian process latent variable model ( gplvm ) @xcite and neighbourhood components analysis ( nca ) @xcite .",
    "we used the matlab toolbox for dimensionality reduction @xcite to embed anchors in manifolds .",
    "we performed two sets of experiments . for each experiment",
    ", we ran 30 trials and present the averaged results . in one set of experiments , we found the accuracy by finding the hamming distance of the hash codes with respect to the hash code of the anchors which are generated by binarizing their embeddings .",
    "the instance was assigned to that anchor for which the hamming distance of the instance with respect to that anchor is the lowest .",
    "we exploit the label of each image for the ground truth . in the second set of experiments ,",
    "we measure the performance of our algorithm using mean of average precision ( map ) , precision and recall curves .",
    "we also show the hash lookup results using f1 measure @xcite which is given as follows :    @xmath69    for evaluating f1 measure , we used hamming distance of 2 units throughout in the above experiments . for evaluating the precision , recall and f1 measures , we divided the dataset into two sets of quarter and three - quarter size , after obtaining the hash codes of each instance in our dataset . then for each sample in the smaller set , we find all the samples in the larger set which are at hamming distance of 2 or less than 2 units from it .",
    "we exploit the ground truths provided to us for the images in the dataset to obtain precision , recall , f1-measure and map measure .",
    "we used @xmath70 in all the experiments unless specified explicitly .",
    "we also measure the time computed by each method for embedding anchors ( table.1 ) , producing hash code for each instance in the training set ( table.2 ) and test set ( table.3 ) .",
    "+ * results on awa dataset * : we randomly split the dataset into training and testing part with 40 classes in our training set and rest 10 classes for testing classes .",
    "we determined the hyper parameters by randomly taking @xmath71 percent of the training dataset ( 8 classes ) as our validation dataset , which were later combined with training set after fine tuning the model . for awa dataset , the hyperparameters obtained are @xmath72 and @xmath73 .",
    "we tested our hash codes by taking hashes of bit size 8 , 16 , 24 , 32 , and 40 for awa dataset .",
    "the maximum hash code length is equal to the number of seen classes as manifold embedding is done using the anchors which are equal to the number of seen classes .",
    "this is because , t - sne and other manifold based techniques do not operate if the number of dimensions is less than number of data points as they are dependent on pca framework .",
    "* results on sun dataset * : we randomly split the dataset into training and testing part with 667 classes in our training set and rest 50 classes for testing classes .",
    "we determined the hyper parameters by randomly taking @xmath71 percent of the training dataset ( 134 classes ) as our validation dataset , which were later combined with training set after fine tuning the model .",
    "for the sun dataset , the hyperparameters obtained are @xmath74 and @xmath75 .",
    "we tested our hash codes by taking hashes of bit size 8 , 16 , 24 , 32 , 40 , 48 , 56 , 64 , 72 , 80 , 88 , 96 , 104 , 112 , 120 , 128 , 160 , 192 , 224 , and 256 for the sun dataset .",
    "[ table1 ]    [ cols=\"^,^,^,^,^,^,^ \" , ]     from fig .",
    "[ fig : map curve ] , it can be observed that for awa dataset , with increasing hash code length , map measure increases for all the embeddings .",
    "but for the sun dataset , as the hash code length increases , map measure for lle based hashing decreases .",
    "for both the datasets , we observe that kernel - pca and nca embedding based hashing methods perform better than the other methods .",
    "t - sne embedding",
    "based hashing method gives poorer results when the hash code length is small but its performance significantly improves as the code length increases . from fig.[fig : precision curves ] , it can be observed that kernel pca and gplvm based embedding methods perform superior than the other techniques for both datasets in term of precision for the hash codes of small length . from the precision measure of the sun dataset",
    ", it can be observed that precision of methods except t - sne decreases after the hash code length of 56 but it increases for t - sne continuously with increasing code length for both the datasets .",
    "we also observe from the fig .",
    "[ fig : trainingdataaccuracy ] , that for awa dataset , the accuracy for training data decreases with increasing the hash code length .",
    "while for the sun dataset , the training data initially decreases but increases again and then saturates . for t - sne based embedding",
    ", we observe that the training data accuracy decreases continuously .",
    "a possible reason could be that with increasing the dimension of t - sne , it becomes less discrete and thus the hash code it generates becomes less distinct with increase in the length of hash code and hence many instances share similar hash codes .",
    "that is why , its recall is higher than other methods as it can be seen from fig .",
    "[ fig : recall curves ] while its precision rate being low ( fig .",
    "[ fig : precision curves ] ) .",
    "similar reasons could be given regarding the performance of our method on accuracy of instances belonging to testing classes .",
    "the reason for this dramatic decrease in the performance of hash look up in fig.[fig : f1measure ] , fig.[fig : trainingdataaccuracy ] and fig.[fig : testingdataaccuracy ] is that hamming spaces become sparser as we increase the hash code length .",
    "we also evaluate our method by varying the number of nearest anchors used to obtain the embedding of data points and plot the map ( fig.[fig : map curve2 ] ) and f1 measure ( fig.[fig : f1measure2 ] ) curves .",
    "for this , we kept the hash code length fixed to 32 bits .",
    "we can notice from the plots that the performance of the proposed methods ( except t - sne ) do not change significantly by varying the number of nearest anchors for any of the manifold embedding significantly for any of the two datasets .",
    "we see that lle based embedding zsh algorithm performs significantly better than its counterparts for both the datasets . while t - sne based embedding hashing technique performs poorly in terms of both f1 and map measure . for t - sne embedding",
    "based hashing , its performance increases continuously as we increase the number of nearest anchors .",
    "we have proposed a hashing algorithm in the zero shot learning framework .",
    "once the manifold embeddings are obtained corresponding to the training classes , our hashing formulation requires linear time for hashing all the training instances @xmath76 .",
    "we used different non - parametric dimensionality reduction techniques to preserve the data distribution in the original feature space .",
    "the proposed framework exploits the information of similarity between classes to inductively generate hash codes of images belonging to the unseen classes .",
    "one advantage of this type of hashing is that if an image ( for e.g. images of claws of eagle ) belonging to a seen class ( eagle ) shares a high similarity with an unseen class ( hawk ) , its hash code will also be similar to the instances belonging to the unseen classes and we could still retrieve that image while querying for the unseen class .",
    "this is due to the fact that in our method the anchor corresponding to the unseen class is close to the anchor corresponding to the seen class .",
    "we also provided the methodology to generate hash codes of out - of - sample data .",
    "in the proposed method , we have not used significant amount of non - linearity for ranking the image features to the classes to which they belong to . in future",
    ", we could levarage deep learning methods which have achieved recently huge success in both fields of hashing and non - linear dimensionality reduction . apart from this",
    ", deep learning techniques have achieved positive results in embedding different modalities to a common space . moreover , recently in @xcite , authors have utilized deep learning framework for learning joint latent space .",
    "this work has inspired us to use deep networks for improving the zero shot hashing framework in the future .",
    "the authors would like to thank rajendra nagar and aalok gangopadhayay for helpful discussions .",
    "m.  datar , n.  immorlica , p.  indyk , and v.  s. mirrokni .",
    "locality - sensitive hashing scheme based on p - stable distributions . in _ proceedings of the twentieth annual symposium on computational geometry _ , pages 253262 .",
    "acm , 2004 .",
    "j.  zhou , g.  ding , and y.  guo .",
    "latent semantic sparse hashing for cross - modal similarity search . in _ proceedings of the 37th international acm sigir conference on research & development in information retrieval _ , pages 415424 .",
    "acm , 2014 .",
    "x.  zhu , z.  huang , h.  t. shen , and x.  zhao .",
    "linear cross - modal hashing for efficient multimedia search . in _ proceedings of the 21st acm international conference on multimedia _ , pages 143152 .",
    "acm , 2013 ."
  ],
  "abstract_text": [
    "<S> this paper provides a framework to hash images containing instances of unknown object classes . in many object recognition problems </S>",
    "<S> , we might have access to huge amount of data . </S>",
    "<S> it may so happen that even this huge data does nt cover the objects belonging to classes that we see in our day to day life . </S>",
    "<S> zero shot learning exploits auxiliary information ( also called as signatures ) in order to predict the labels corresponding to unknown classes . in this work , </S>",
    "<S> we attempt to generate the hash codes for images belonging to unseen classes , information of which is available only through the textual corpus . </S>",
    "<S> we formulate this as an unsupervised hashing formulation as the exact labels are not available for the instances of unseen classes . </S>",
    "<S> we show that the proposed solution is able to generate hash codes which can predict labels corresponding to unseen classes with appreciably good precision . </S>"
  ]
}