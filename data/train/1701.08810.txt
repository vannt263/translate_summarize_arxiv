{
  "article_text": [
    "reinforcement learning ( rl )  @xcite is a machine learning framework intending to optimise the behaviour of an agent interacting with an unknown environment . for the most practical problems , trajectory collection is costly and sample efficiency is the main key performance indicator",
    "this is for instance the case for dialogue systems  @xcite and robotics  @xcite . as a consequence , when applying rl to a new problem , one must carefully choose in advance a model , an optimisation technique , and their parameters in order to learn an adequate behaviour given the limited sample set at hand .    in particular , for 20 years , research has developed and applied rl algorithms for spoken dialogue systems , involving a large range of dialogue models and algorithms to optimise them . just to cite a few algorithms : monte carlo  @xcite , @xmath0-learning  @xcite , sarsa  @xcite , mvdp algorithms  @xcite , kalman temporal difference  @xcite , fitted-@xmath0 iteration  @xcite , gaussian process rl  @xcite , and more recently deep rl  @xcite .",
    "additionally , most of them require the setting of hyper parameters and a state space representation . when applying these research results to a new problem , these choices may dramatically affect the speed of convergence and therefore , the dialogue system performance . facing the complexity of choice ,",
    "rl and dialogue expertise is not sufficient .",
    "confronted to the cost of data , the popular _ trial and error _ approach shows its limits .",
    "_ algorithm selection _",
    "@xcite is a framework for comparing several algorithms on a given _",
    "problem instance_. the algorithms are tested on several problem instances , and hopefully , the algorithm selection learns from those experiences which algorithm should be the most efficient given a new problem instance . in our setting , only one problem instance is considered , but several experiments are led to determine the fittest algorithm to deal with it .",
    "thus , we developed an _ online _ learning version  @xcite of algorithm selection .",
    "it consists in testing several algorithms on the task and in selecting the best one at a given time .",
    "indeed , it is important to notice that , as new data is collected , the algorithms improve their performance and that an algorithm might be the worst at a short - term horizon , but the best at a longer - term horizon . in order to avoid confusion , throughout the whole article , the algorithm selector",
    "is called a _",
    "meta - algorithm _ , and the set of algorithms available to the meta - algorithm is called a _ portfolio_. defined as an online learning problem , our algorithm selection task has for objective to minimise the expected regret .    at each online algorithm selection ,",
    "only the selected algorithm is experienced .",
    "since the algorithms learn from their experience , it implies a requirement for a fair budget allocation between the algorithms , so that they can be equitably evaluated and compared .",
    "budget fairness is in direct contradiction with the expected regret minimisation objective . in order to circumvent this ,",
    "the reinforcement algorithms in the portfolio are assumed to be _ off - policy _ , meaning that they can learn from experiences generated from an arbitrary non - stationary behavioural policy .",
    "section [ sec : rlmodel ] provides a unifying view of reinforcement learning algorithms , that allows information sharing between all algorithms of the portfolio , whatever their decision processes , their state representations , and their optimisation techniques .",
    "then , section [ sec : asrl ] formalises the problem of online selection of off - policy reinforcement learning algorithms .",
    "it introduces three definitions of pseudo - regret and states three assumptions related to experience sharing and budget fairness among algorithms . beyond the sample efficiency issues ,",
    "the online algorithm selection approach addresses furthermore four distinct practical problems for spoken dialogue systems and online rl - based systems more generally .",
    "first , it enables a systematic benchmark of models and algorithms for a better understanding of their strengths and weaknesses .",
    "second , it improves robustness against implementation bugs : if an algorithm fails to terminate , or converges to an aberrant policy , it will be dismissed and others will be selected instead .",
    "third , convergence guarantees and empirical efficiency may be united by covering the empirically efficient algorithms with slower algorithms that have convergence guarantees .",
    "fourth and last , it enables staggered learning : shallow models converge the fastest and consequently control the policy in the early stages , while deep models discover the best solution later and control the policy in late stages .",
    "afterwards , section [ sec : amabma ] presents the epochal stochastic bandit algorithm selection ( esbas ) , a novel meta - algorithm addressing the online off - policy reinforcement learning algorithm selection problem .",
    "its principle is to divide the time - scale into epochs of exponential length inside which the algorithms are not allowed to update their policies . during each epoch , the algorithms have therefore a constant policy and a stochastic multi - armed bandit can be in charge of the algorithm selection with strong pseudo - regret theoretical guaranties .",
    "a thorough theoretical analysis provides for esbas upper bounds of the pseudo - regrets defined in section [ sec : asrl ] under the assumptions stated in the same section .",
    "= [ rectangle , draw , text width=8em , text centered , rounded corners , minimum height=4em ]    = [ draw , -latex ]    ( agent ) agent ; ( environment ) stochastic environment ;    ( agent.0 ) ++ ( 4em,0em ) |- node [ near start]@xmath1 ( environment.0 ) ; ( environment.190 ) ++ ( -6em,0em ) |- node [ near start ] @xmath2 ( agent.170 ) ; ( environment.170 ) ++ ( -4.25em,0em ) |- node [ near start , right ] @xmath3 ( agent.190 ) ;    next , section [ sec : experesults ] experiments esbas on a simulated dialogue task , and presents the experimental results , which demonstrate the practical benefits of esbas : in most cases outperforming the best algorithm in the portfolio , even though its primary goal was just to be almost as good as it .",
    "finally , sections [ sec : related ] and [ sec : conclusion ] conclude the article with respectively related works and prospective ideas of improvement .",
    "the goal of this section is to enable information sharing between algorithms , even though they are considered as black boxes .",
    "we propose to share their trajectories expressed in a universal format : the _ interaction process_.    reinforcement learning ( rl ) consists in learning through trial and error to control an agent behaviour in a stochastic environment .",
    "more formally , at each time step @xmath4 , the agent performs an action @xmath5 , and then perceives from its environment a signal @xmath6 called observation , and receives a reward @xmath7 .",
    "figure [ fig : rl ] illustrates the rl framework .",
    "this interaction process is not markovian : the agent may have an internal memory .    in this article",
    ", the reward function is assumed to be bounded between @xmath8 and @xmath9 , and we define the rl problem as episodic .",
    "let us introduce two time scales with different notations .",
    "first , let us define _ meta - time _ as the time scale for algorithm selection : at one meta - time @xmath10 corresponds a meta - algorithm decision , _",
    "i.e. _ the choice of an algorithm and the generation of a full episode controlled with the policy determined by the chosen algorithm .",
    "its realisation is called a _",
    "trajectory_. second , _",
    "rl - time _ is defined as the time scale inside a trajectory , at one rl - time @xmath11 corresponds one triplet composed of an observation , an action , and a reward .",
    "let @xmath12 denote the space of trajectories .",
    "a _ trajectory _ @xmath13 collected at meta - time @xmath10 is formalised as a sequence of ( observation , action , reward ) triplets : @xmath14 where @xmath15 is the length of trajectory @xmath16 .",
    "the objective is , given a discount factor @xmath17 , to generate trajectories with high discounted cumulative reward , also called _ return _ , and noted @xmath18 : @xmath19    since @xmath20 and @xmath21 , the return is bounded : @xmath22    the _ trajectory set _ at meta - time @xmath23 is denoted by : @xmath24    a sub - trajectory of @xmath16 until rl - time @xmath11 is called the _ history _ at rl - time @xmath11 and written @xmath25 with @xmath26 .",
    "the history records what happened in episode @xmath16 until rl - time @xmath11 : @xmath27    the goal of each reinforcement learning algorithm @xmath28 is to find a policy @xmath29 which yields optimal expected returns .",
    "such an algorithm @xmath28 is viewed as a black box that takes as an input a trajectory set @xmath30 , where @xmath31 is the ensemble of trajectory sets of undetermined size : @xmath32 , and that outputs a policy @xmath33 .",
    "consequently , a reinforcement learning algorithm is formalised as follows : @xmath34    such a high level definition of the rl algorithms allows to share trajectories between algorithms : a trajectory as a sequence of observations , actions , and rewards can be interpreted by any algorithm in its own decision process and state representation .",
    "for instance , rl algorithms classically rely on an mdp defined on a state space representation @xmath35 thanks to a projection @xmath36 : @xmath37    the state representation may be built dynamically as the trajectories are collected .",
    "many algorithms doing so can be found in the literature , for instance @xcite .",
    "then , @xmath28 may learn its policy @xmath38 from the trajectories projected on its state space representation and saved as a transition set : a transition is defined as a quadruplet @xmath39 , with state @xmath40 , action @xmath41 , reward @xmath42 , and next state @xmath43 .",
    "off - policy reinforcement learning optimisation techniques compatible with this approach are numerous in the literature  @xcite : @xmath0-learning  @xcite , fitted-@xmath0 iteration  @xcite , kalman temporal difference  @xcite , etc .",
    "another option would be to perform end - to - end reinforcement learning  @xcite .",
    "as well , any post - treatment of the state set , any alternative decision process model , such as pomdps  @xcite , and any off - policy technique for control optimisation may be used .",
    "the algorithms are defined here as black boxes and the considered meta - algorithms will be indifferent to how the algorithms compute their policies , granted they satisfy the assumptions made in the following section .",
    "algorithm selection  @xcite for combinatorial search  @xcite consists in deciding which experiments should be carried out , given a problem instance and a fixed amount of computational resources : generally speaking computer - time , memory resources , time and/or money .",
    "algorithms are considered efficient if they consume little resource .",
    "this approach , often compared and applied to a scheduling problem  @xcite , experienced a lot of success for instance in the sat competition  @xcite .",
    "algorithm selection applied to machine learning , also called _ meta - learning _",
    ", is mostly dedicated to error minimisation given a corpus of limited size . indeed",
    ", these algorithms do not deliver _ in fine _ the same answer . in practice",
    ", algorithm selection can be applied to arbitrary performance metrics and modelled in the same framework . in the classical batch",
    "setting , the notations of the machine learning algorithm selection problem are described in @xcite as follows :    * @xmath44 is the space of problem instances ; * @xmath45 is the _ portfolio _ , _ i.e. _ the collection of available algorithms ; * @xmath46 is the _ objective function _ , _ i.e. _ a performance metrics enabling to rate an algorithm on a given instance ; * @xmath47 are the features characterising the properties of problem instances .",
    "the principle consists in collecting problem instances and in solving them with the algorithms in the portfolio @xmath45 .",
    "the measures @xmath48 provide evaluations of the algorithms on those instances . then , the aggregation of their features @xmath49 with the measures @xmath48 constitutes a training set .",
    "finally , any supervised learning techniques can be used to learn an optimised mapping between the instances and the algorithms .      nevertheless , in our case",
    ", @xmath44 is not large enough to learn an efficient model , it might even be a singleton .",
    "consequently , it is not possible to regress a general knowledge from a parametrisation @xmath49 .",
    "this is the reason why the online learning approach is tackled in this article : different algorithms are experienced and evaluated during the data collection . since it boils down to a classical exploration / exploitation trade - off , multi - armed bandit  @xcite have been used for combinatorial search algorithm selection  @xcite and evolutionary algorithm meta - learning  @xcite .    in the online setting ,",
    "the algorithm selection problem for off - policy reinforcement learning is new and we define it as follows :    * @xmath30 is the _ trajectory set _ ; * @xmath50 is the _ portfolio _ ; * @xmath51 is the _ objective function _ defined in equation [ eq : return ] .",
    "pseudo - code [ alg : algorithmselectionproblem ] formalises the online algorithm selection setting .",
    "meta - algorithm _ is defined as a function from a trajectory set to the selection of an algorithm : @xmath52    the meta - algorithm is queried at each meta - time @xmath53 , with input @xmath54 , and it ouputs algorithm @xmath55 controlling with its policy @xmath56 the generation of the trajectory @xmath57 in the stochastic environment .",
    "let @xmath58 be a condensed notation for the expected return of policy @xmath59 that was learnt from trajectory set @xmath60 by algorithm @xmath28 : @xmath61.\\label{eq : expmu}\\ ] ]    the final goal is to optimise the cumulative expected return .",
    "it is the expectation of the sum of rewards obtained after a run of @xmath23 trajectories :    _ = _",
    "= 1^t _ ^-1 ( |)_^(),[eq : cumulativeexpectedreturn1 ]    _ = _",
    "= 1^t_,[eq : cumulativeexpectedreturn2 ]    _ = _ .",
    "[ eq : cumulativeexpectedreturn3 ]    equations [ eq : cumulativeexpectedreturn1 ] , [ eq : cumulativeexpectedreturn2 ] and [ eq : cumulativeexpectedreturn3 ] transform the cumulative expected return into two nested expectations .",
    "the outside expectation @xmath62 assumes the algorithm selection fixed and averages over the trajectory set stochastic collection and the corresponding algorithms policies , which may also rely on a stochastic process .",
    "the inside expectation @xmath63 assumes the policy fixed and averages the evaluation over its possible trajectories in the stochastic environment .",
    "equation [ eq : cumulativeexpectedreturn1 ] transforms the expectation into its probabilistic equivalent , @xmath64 denoting the probability density of generating trajectory set @xmath60 conditionally to the meta - algorithm @xmath65 .",
    "equation [ eq : cumulativeexpectedreturn2 ] transforms back the probability into a local expectation , and finally equation [ eq : cumulativeexpectedreturn3 ] simply applies the commutativity between the sum and the expectation .",
    "_ nota bene _ : there are three levels of decision : meta - algorithm @xmath65 selects an algorithm @xmath28 that computes a policy @xmath66 that in turn controls the actions .",
    "we focus in this paper on the meta - algorithm level .      in order to evaluate the meta - algorithms ,",
    "let us formulate two additional notations .",
    "first , the _ optimal expected return _",
    "@xmath67 is defined as the highest expected return achievable by a policy of an algorithm in portfolio @xmath45 : @xmath68    second , for every algorithm @xmath28 in the portfolio , let us define @xmath69 as its _ canonical meta - algorithm _ , _",
    "i.e. _ the meta - algorithm that always selects algorithm @xmath28 : @xmath70 , @xmath71 .",
    "the _ absolute pseudo - regret _",
    "@xmath72 defines the regret as the loss for not having controlled the trajectory with an optimal policy .",
    "defnabsreg [ def : abs ] the absolute pseudo - regret @xmath72 compares the meta - algorithm s expected return with the optimal expected return : @xmath73 .",
    "\\label{eq : absoluteregret}\\ ] ]    the absolute pseudo - regret @xmath72 is a well - founded pseudo - regret definition .",
    "however , it is worth noting that an optimal meta - algorithm will not yield a null regret because a large part of the absolute pseudo - regret is caused by the sub - optimality of the algorithm policies when the trajectory set is still of limited size .",
    "indeed , the absolute pseudo - regret considers the regret for not selecting an optimal policy : it takes into account both the pseudo - regret of not selecting the best algorithm and the pseudo - regret of the algorithms for not finding an optimal policy . since the meta - algorithm does not interfere with the training of policies , it can not account for the pseudo - regret related to the latter .",
    "in order to have a pseudo - regret that is relative to the learning ability of the best algorithm and that better accounts for the efficiency of the algorithm selection task , we introduce the notion of _ relative pseudo - regret_.    defnrelreg [ def : rel ] the relative pseudo - regret @xmath74 compares the @xmath65 meta - algorithm s expected return with the expected return of the best canonical meta - algorithm : @xmath75 -   \\mathbb{e}_\\sigma\\left[\\sum_{\\tau=1}^t\\mathbb{e}\\mu_{\\mathcal{d}_{\\tau-1}^\\sigma}^{\\sigma(\\tau ) } \\right ] .",
    "\\label{eq : relregret}\\ ] ]    it is direct from equations [ eq : absoluteregret ] and [ eq : relregret ] that the relative pseudo - regret can be expressed in function of absolute pseudo - regrets of the meta - algorithm @xmath65 and the canonical meta - algorithms @xmath69 : @xmath76    since one shallow algorithm might be faster in early stages and a deeper one more effective later , a good meta - algorithm may achieve a negative relative pseudo - regret , which is ill - defined as a pseudo - regret definition .",
    "still , the relative pseudo - regret @xmath74 is useful as an empirical evaluation metric .",
    "a large relative pseudo - regret shows that the meta - algorithm failed to consistently select the best algorithm(s ) in the portfolio .",
    "a small , null , or even negative relative pseudo - regret demonstrates that using a meta - algorithm is a guarantee for selecting the algorithm that is the most adapted to the problem .",
    "the theoretical analysis is hindered by the fact that algorithm selection , not only directly influences the return distribution , but also the trajectory set distribution and therefore the policies learnt by algorithms for next trajectories , which will indirectly affect the future expected returns . in order to allow policy comparison , based on relation on trajectory sets",
    "they are derived from , our analysis relies on three assumptions whose legitimacy is discussed in this section and further developed under the practical aspects in section [ sec : transgressions ] .",
    "assmorass the algorithms produce better policies with a larger trajectory set on average , whatever the algorithm that controlled the additional trajectory : @xmath77}.\\ ] ] [ ass : monotony ]    assumption [ ass : monotony ] states that algorithms are off - policy learners and that additional data can not lead to performance degradation on average .",
    "an algorithm that is not off - policy could be biased by a specific behavioural policy and would therefore transgress this assumption .",
    "assordass if an algorithm produces a better policy with one trajectory set than with another , then it remains the same , on average , after collecting an additional trajectory from any algorithm : @xmath78 \\leq \\mathbb{e}_{\\alpha'}\\left[\\mathbb{e}\\mu_{\\mathcal{d}'\\cup\\varepsilon^{\\alpha'}}^{\\alpha}\\right ] .",
    "\\end{array}\\ ] ] [ ass : compatibility ]    assumption [ ass : compatibility ] states that a performance relation between two policies learnt by a given algorithm from two trajectory sets is preserved on average after adding another trajectory , whatever the behavioural policy used to generate it .",
    "from these two assumptions , theorem [ th : notworse ] provides an upper bound in order of magnitude in function of the worst algorithm in the portfolio .",
    "it is verified for any algorithm selection @xmath65 :    thmnotthm the absolute pseudo - regret is bounded by the worst algorithm absolute pseudo - regret in order of magnitude : @xmath79 [ th : notworse ]    see the appendix .",
    "contrarily to what the name of theorem [ th : notworse ] suggests , a meta - algorithm might be worse than the worst algorithm ( similarly , it can be better than the best algorithm ) , but not in order of magnitude .",
    "its proof is rather complex for such an intuitive and loose result because , in order to control all the possible outcomes , one needs to translate the selections of algorithm @xmath28 with meta - algorithm @xmath65 into the canonical meta - algorithm @xmath69 s view , in order to be comparable with it .",
    "this translation is not obvious when the meta - algorithm @xmath65 and the algorithms it selects act tricky .",
    "see the proof for an example .",
    "the _ fairness of budget distribution _ has been formalised in @xcite .",
    "it is the property stating that every algorithm in the portfolio has as much resources as the others , in terms of computational time and data .",
    "it is an issue in most online algorithm selection problems , since the algorithm that has been the most selected has the most data , and therefore must be the most advanced one . a way to circumvent this issue",
    "is to select them equally , but , in an online setting , the goal of algorithm selection is precisely to select the best algorithm as often as possible . in short ,",
    "exploration and evaluation require to be fair and exploitation implies to be unfair .",
    "our answer is to require that all algorithms in the portfolio are learning _ off - policy _ , _ i.e. _ without bias induced by the behavioural policy used in the learning dataset .    by assuming that all algorithms learn off - policy , we allow _ information sharing _",
    "@xcite between algorithms .",
    "they share the trajectories they generate . as a consequence",
    ", we can assume that every algorithm , the least or the most selected ones , will learn from the same trajectory set .",
    "therefore , the control unbalance does not directly lead to unfairness in algorithms performances : all algorithms learn equally from all trajectories .",
    "however , unbalance might still remain in the exploration strategy if , for instance , an algorithm takes more benefit from the exploration it has chosen than the one chosen by another algorithm . in this article , we speculate that this chosen - exploration effect is negligible . more formally , in this article , for analysis purposes , the algorithm selection is assumed to be absolutely fair regardless the exploration unfairness we just discussed about .",
    "this is expressed by assumption [ ass : fairlearning ] .",
    "assfairass if one trajectory set is better than another for one given algorithm , it is the same for other algorithms . @xmath80",
    "[ ass : fairlearning ]    in practical problems , assumptions 2 and 3 are defeated , but empirical results in section [ sec : experesults ] demonstrate that the esbas algorithm presented in section [ sec : amabma ] is robust to the assumption transgressions .",
    "an intuitive way to solve the algorithm selection problem is to consider algorithms as arms in a multi - armed bandit setting .",
    "the bandit meta - algorithm selects the algorithm controlling the next trajectory @xmath81 and the trajectory return @xmath82 constitutes the reward of the bandit .",
    "however , a stochastic bandit can not be _ directly _ used because the algorithms performances vary and improve with time  @xcite .",
    "adversarial multi - arm bandits are designed for non - stationary environments  @xcite , but the exploitation the structure of our algorithm selection problem makes it possible to obtain pseudo - regrets of order of magnitude lower than @xmath83{t})$ ] .",
    "alternatively , one might consider using policy search methods  @xcite , but they rely on a state space representation in order to apply the policy gradient . and in our case , neither policies do share any , nor the meta - algorithm does have at disposal any other than the intractable histories @xmath25 defined in equation [ eq : histories ] .      to solve the off - policy rl algorithm selection problem , we propose a novel meta - algorithm called epochal stochastic bandit algorithm selection ( esbas ) . because of the non - stationarity induced by the algorithm learning , the stochastic bandit can not directly select algorithms . instead ,",
    "the stochastic bandit can choose fixed policies . to comply to this constraint , the meta - time scale is divided into epochs inside which the algorithms policies can not be updated : the algorithms optimise their policies only when epochs start , in such a way that the policies are constant inside each epoch . as a consequence and since the returns are bounded in equation [ eq : returnbounds ] , at each new epoch , the problem can be cast into an independent stochastic @xmath84-armed bandit @xmath85 , with @xmath86 .",
    "the esbas meta - algorithm is formally sketched in pseudo - code [ alg : epochalgorithmselectionalgorithm ] embedding ucb1  @xcite as the stochastic @xmath84-armed bandit @xmath85 .",
    "the meta - algorithm takes as an input the set of algorithms in the portfolio .",
    "meta - time scale is fragmented into epochs of exponential size .",
    "the @xmath87^th^ epoch lasts @xmath88 meta - time steps , so that , at meta - time @xmath89 , epoch @xmath87 starts . at the beginning of each epoch , the esbas meta - algorithm asks each algorithm in the portfolio to update their current policy . inside an epoch , the policy is never updated anymore . at the beginning of each epoch , a new @xmath85 instance is reset and run . during the whole epoch",
    ", @xmath85 decides at each meta - time step which algorithm is in control of the next trajectory .",
    "it has to be noted that esbas simply intends to minimise the regret for not choosing the best algorithm at a given meta - time @xmath10 .",
    "it is somehow short - sighted : it does not intend to optimise its algorithms learning . the following _ short - sighted pseudo - regret _ definition captures the function esbas intends to minimise .",
    "defnssreg [ def : ss ] the short - sighted pseudo - regret @xmath90 is the difference between the immediate best expected return algorithm and the one selected : @xmath91 .",
    "\\label{eq : shortsightedregret}\\ ] ]    the short - sighted pseudo - regret optimality depends on the meta - algorithm itself .",
    "for instance , a poor deterministic algorithm might be optimal at meta - time @xmath10 but yield no new information , implying the same situation at meta - time @xmath92 , and so on .",
    "thus , a meta - algorithm that exclusively selects the deterministic algorithm would achieve a short - sighted pseudo - regret equal to 0 , but selecting other algorithms are , in the long run , more efficient .",
    "theorem [ th : shortsightedregret ] expresses in order of magnitude an upper bound for the short - sighted pseudo - regret of esbas . but",
    "first , let define the gaps : @xmath93    it is the difference of expected return between the best algorithm during epoch @xmath87 and algorithm @xmath28 .",
    "the smallest non null gap at epoch @xmath87 is noted : @xmath94    if @xmath95 does not exist , _",
    "i.e. _ if there is no non - null gap , the problem is trivial and the regret is null .",
    "upper bounds on short - sighted and absolute pseudo - regrets are derived .",
    "thmssthm if the stochastic multi - armed bandit @xmath85 guarantees a regret of order of magnitude @xmath96 , then : @xmath97 [ th : shortsightedregret ]    see the appendix .",
    "several upper bounds in order of magnitude on @xmath98 can directly be deduced from theorem [ th : shortsightedregret ] , depending on an order of magnitude of @xmath95 .",
    "table [ tab : bounds ] reports some of them for a two - fold portfolio .",
    "it must be read by line . according to the first column : the order of magnitude of @xmath95 , the esbas short - sighted pseudo - regret bounds are displayed in the second column .",
    "one should notice that the first two bounds are obtained by summing the gaps .",
    "this means that esbas is unable to recognise the better algorithm , but the pseudo - regrets may still be small . on the contrary ,",
    "the two last bounds are deduced from theorem [ th : shortsightedregret ] . in this case ,",
    "the algorithm selection is useful .",
    "the worse algorithm @xmath99 is , the easier algorithm selection gets , and the lower the upper bounds are .    [ c?c ] @xmath95 & @xmath100 + @xmath101 & @xmath102 + @xmath103 , @xmath104 & @xmath105 + @xmath103 , @xmath106 & @xmath107 + @xmath108 & @xmath109 +    about esbas ability to select the best algorithm : if @xmath110{t})$ ] , then the meta - algorithm will not be able to distinguish the two best algorithms . still , we have the guarantee that pseudo - regret @xmath111{t})$ ] . the impossibility to determine which is the better algorithm is interpreted in @xcite as a budget issue .",
    "the meta - time necessary to distinguish arms that are @xmath95 apart with an arbitrary confidence interval takes @xmath112 meta - time steps . as a consequence , if @xmath110{t})$ ] , then @xmath113 .",
    "however , the budget , _",
    "i.e. _ the length of epoch @xmath87 starting at meta - time @xmath114 , equals @xmath23 .",
    "in fact , even more straightforwardly , the stochastic bandit problem is known to be @xmath115  @xcite , which highlights the limit of distinguishability at @xmath116 .",
    "spare the @xmath117 factor in table [ tab : bounds ] bounds , which comes from the fact the meta - algorithm starts over a novel bandit problem at each new epoch , esbas faces the same hard limit .      as equation [ eq : ]",
    "recalls , the absolute pseudo - regret can be decomposed between the absolute pseudo - regret of the canonical meta - algorithm of the best algorithm and the relative pseudo - regret , which is the regret for not running the best algorithm alone .",
    "the relative pseudo regret can in turn be upper bounded by a decomposition of the selection regret : the regret for not always selecting the best algorithm , and potentially not learning as fast , and the short - sighted regret : the regret for not gaining the returns granted by the best algorithm .",
    "these two successive decomposition lead to theorem [ th : absolute ] that provides an upper bound of the absolute pseudo - regret in function of the canonical meta - algorithm of the best algorithm , and the short - sighted pseudo - regret , which order magnitude is known to be bounded thanks to theorem [ th : shortsightedregret ] .",
    "thmabsthm if the stochastic multi - armed bandit @xmath85 guarantees that the best arm has been selected in the @xmath23 first episodes at least @xmath118 times , with high probability @xmath119 , then : @xmath120 where algorithm selection @xmath121 selects exclusively algorithm @xmath122 .",
    "[ th : absolute ]    see the appendix .",
    "[ c?c|c ] & + & @xmath123 & @xmath124 + @xmath101 & @xmath102 & +    @c@@xmath103 , +    & @xmath105 & @xmath105 +    @c@@xmath103 , +    & @xmath107 &    @c@@xmath125 , +   + @xmath107 , +     + @xmath108 & @xmath109 & @xmath125 +    table [ tab : absbounds ] reports an overview of the absolute pseudo - regret bounds in order of magnitude of a two - fold portfolio in function of the asymptotic behaviour of the gap @xmath95 and the absolute pseudo - regret of the meta - algorithm of the the best algorithm @xmath126 , obtained with theorems [ th : notworse ] , [ th : shortsightedregret ] and [ th : absolute ] . table [ tab : bounds ] is interpreted by line . according the order of magnitude of @xmath95 in the first column",
    ", the second and third columns display the esbas absolute pseudo - regret bounds cross depending on the order of magnitude of @xmath126 .",
    "several remarks on table [ tab : absbounds ] can be made .",
    "firstly , like in table [ tab : bounds ] , theorem [ th : notworse ] is applied when esbas is unable to distinguish the better algorithm , and theorem [ th : shortsightedregret ] are applied when esbas algorithm selection is useful : the worse algorithm @xmath99 is , the easier algorithm selection gets , and the lower the upper bounds .",
    "secondly , @xmath123 implies that @xmath127 .",
    "thirdly and lastly , _ in practice _ , the second best algorithm absolute pseudo - regret @xmath128 is of the same order of magnitude than the sum of @xmath95 : @xmath129 .",
    "for this reason , in the last column , the first bound is greyed out , and @xmath130 is assumed in the other bounds .",
    "it is worth noting that upper bounds expressed in order of magnitude are all inferior to @xmath131{t})$ ] , the upper bounds of the adversarial multi - arm bandit .",
    "_ nota bene _ :",
    "the theoretical results presented in table [ tab : absbounds ] are verified if the stochastic multi - armed bandit @xmath85 satisfies both conditions stated in theorems [ th : shortsightedregret ] and [ th : absolute ] .",
    "successive and median elimination  @xcite and upper confidence bound  @xcite under some conditions  @xcite are examples of appropriate @xmath85 .",
    "esbas algorithm for off - policy reinforcement learning algorithm selection can be and was meant to be applied to reinforcement learning in dialogue systems .",
    "thus , its practical efficiency is illustrated on a dialogue negotiation game  @xcite that involves two players : the system @xmath132 and a user @xmath133 .",
    "their goal is to reach an agreement .",
    "@xmath134 options are considered , and at each new dialogue , for each option @xmath135 , players have a private uniformly drawn cost @xmath136 $ ] to agree on it .",
    "each player is considered fully empathetic to the other one . as a result ,",
    "if the players come to an agreement , the system s immediate reward at the end of the dialogue is : @xmath137 where @xmath138 is the last state reached by player @xmath132 at the end of the dialogue , and @xmath135 is the agreed option ; if the players fail to agree , the final immediate reward is : @xmath139 and finally , if one player misunderstands and agrees on a wrong option , the system gets the cost of selecting option @xmath135 without the reward of successfully reaching an agreement : @xmath140    players act each one in turn , starting randomly by one or the other .",
    "they have four possible actions :    * refprop@xmath141 : the player makes a proposition : option @xmath135 .",
    "if there was any option previously proposed by the other player , the player refuses it . *",
    "askrepeat : the player asks the other player to repeat its proposition . *",
    "accept@xmath141 : the player accepts option @xmath135 that was understood to be proposed by the other player .",
    "this act ends the dialogue either way : whether the understood proposition was the right one ( equation [ eq : rsuccess ] ) or not ( equation [ eq : rfailure ] ) .",
    "* enddial : the player does not want to negotiate anymore and ends the dialogue with a null reward ( equation [ eq : rgiveup ] ) .",
    "understanding through speech recognition of system @xmath132 is assumed to be noisy : with a sentence error rate of probability @xmath142 , an error is made , and the system understands a random option instead of the one that was actually pronounced . in order to reflect human - machine dialogue asymmetry , the simulated user always understands what the system says : @xmath143 .",
    "we adopt the way @xcite generates speech recognition confidence scores : @xmath144    if the player understood the right option @xmath145 , otherwise @xmath146 .",
    "the system , and therefore the portfolio algorithms , have their action set restrained to these five non parametric actions : refinsist @xmath147 refprop@xmath148 , @xmath149 being the option lastly proposed by the system ; refnewprop @xmath147 refprop@xmath141 , @xmath135 being the preferred one after @xmath149 , askrepeat , accept@xmath147 accept@xmath141 , @xmath135 being the last understood option proposition and enddial .      all learning algorithms are using fitted-@xmath0 iteration  @xcite , with a linear parametrisation and an @xmath150-greedy exploration : @xmath151 , @xmath87 being the epoch number .",
    "six algorithms differing by their state space representation @xmath152 are considered :    * _ simple _ : state space representation of four features : the constant feature @xmath153 , the last recognition score feature @xmath154 , the difference between the cost of the proposed option and the next best option @xmath155 , and finally an rl - time feature @xmath156 . @xmath157 .",
    "* _ fast _ : @xmath158 . *",
    "_ simple-2 _ : state space representation of ten second order polynomials of _ simple _ features .",
    "@xmath159 , @xmath154 , @xmath155 , @xmath160 , @xmath161 , @xmath162 , @xmath163 , @xmath164 , @xmath165 , @xmath166 . * _ fast-2 _ : state space representation of six second order polynomials of _ fast _ features . @xmath159 , @xmath154 , @xmath155 , @xmath161 , @xmath162 , @xmath167 . * _",
    "n-@xmath168-\\{simple / fast / simple-2/fast-2 } _ : versions of previous algorithms with @xmath168 additional features of noise , randomly drawn from the uniform distribution in @xmath169 $ ] . *",
    "_ constant-@xmath48 _ : the algorithm follows a deterministic policy of average performance @xmath48 without exploration nor learning .",
    "those constant policies are generated with _",
    "simple-2 _ learning from a predefined batch of limited size .",
    "+   + [ fig : simplevssquare ]      in all our experiments , esbas has been run with ucb parameter @xmath170 .",
    "we consider 12 epochs .",
    "the first and second epochs last @xmath171 meta - time steps , then their lengths double at each new epoch , for a total of 40,920 meta - time steps and as many trajectories .",
    "@xmath172 is set to @xmath173 .",
    "the algorithms and esbas are playing with a stationary user simulator built through imitation learning from real - human data .",
    "all the results are averaged over 1000 runs .",
    "the performance figures plot the curves of algorithms individual performance @xmath69 against the esbas portfolio control @xmath174 in function of the epoch ( the scale is therefore logarithmic in meta - time ) .",
    "the performance is the average return of the reinforcement learning problem defined in equation [ eq : return ] : it equals @xmath175 in the negotiation game , with @xmath176 value defined by equations [ eq : rsuccess ] , [ eq : rgiveup ] , and [ eq : rfailure ] .",
    "the ratio figures plot the average algorithm selection proportions of esbas at each epoch .",
    "sampled relative pseudo - regrets are also provided in table [ tab : recap ] , as well as the gain for not having chosen the worst algorithm in the portfolio .",
    "relative pseudo - regrets have a 95% confidence interval about @xmath177 , which is equivalent to @xmath178 per trajectory .",
    "three experience results are presented in this subsection .    1.81     _",
    "simple-2 _ +",
    "_ fast-2 _ & 35 & -181 + _ simple _ + _ n-1-simple-2 _ & -73 & -131 + _ simple _ + _ n-1-simple _ & 3 & -2 + _ simple-2 _ + _ n-1-simple-2 _ & -12 & -38 + _ all-4 _ + _ constant-1.10 _ & 21 & -2032 + _ all-4 _ + _ constant-1.11 _ & -21 & -1414 + _ all-4 _ + _ constant-1.13 _ & -10 & -561 + _ all-4 _ & -28 & -275 + _ all-2-simple _ + _ constant-1.08 _ & -41 & -2734 + _ all-2-simple _ + _ constant-1.11 _ & -40 & -2013 + _ all-2-simple _ + _ constant-1.13 _ & -123 & -799 + _ all-2-simple _ & -90 & -121 + _ fast _ + _ simple-2 _ & -39 & -256 + _ simple-2 _ + _ constant-1.01 _ & 169 & -5361 + _ simple-2 _ + _ constant-1.11 _ & 53 & -1380 + _ simple-2 _ + _ constant-1.11 _ & 57 & -1288 + _ simple _ + _ constant-1.08 _ & 54 & -2622 + _ simple _ + _ constant-1.10 _ & 88 & -1565 + _ simple _ + _ constant-1.14 _ & -6 & -297 + _ all-4 _ + _ all-4-n-1 _ + _ constant-1.09 _ & 25 & -2308 + _ all-4 _ + _ all-4-n-1 _ + _ constant-1.11 _ & 20 & -1324 + _ all-4 _ + _ all-4-n-1 _ + _ constant-1.14 _ & -16 & -348 + _ all-4 _ + _ all-4-n-1 _ & -10 & -142 + _ all-2-simple _ + _ all-2-n-1-simple _ & -80 & -181 + 4*_n-2-simple _ & -20 & -20 + 4*_n-3-simple _ & -13 & -13 + 8*_n-1-simple-2 _ & -22 & -22 + _ simple-2 _ + _ constant-0.97 _ ( no reset ) & 113 & -7131 + _ simple-2 _ + _ constant-1.05 _ ( no reset ) & 23 & -3756 + _ simple-2 _ + _ constant-1.09 _ ( no reset ) & -19 & -2170 + _ simple-2 _ + _ constant-1.13 _ ( no reset ) & -16 & -703 + _ simple-2 _ + _ constant-1.14 _ ( no reset ) & -125 & -319 +    1.81.5    figures [ fig : simplevssquare1 ] and [ fig : simplevssquare2 ] plot the typical curves obtained with esbas selecting from a portfolio of two learning algorithms . on figure [ fig : simplevssquare1 ] , the esbas curve tends to reach more or less the best algorithm in each point as expected .",
    "surprisingly , figure [ fig : simplevssquare2 ] reveals that the algorithm selection ratios are not very strong in favour of one or another at any time .",
    "indeed , the variance in trajectory set collection makes _ simple _ better on some runs until the end .",
    "esbas proves to be efficient at selecting the best algorithm for each run and unexpectedly obtains a negative relative pseudo - regret of -90 .",
    "more generally , table [ tab : recap ] reveals that most of such two - fold portfolios with learning algorithms actually induced a strongly negative relative pseudo - regret .",
    "figures [ fig : squarevsconstant1 ] and [ fig : squarevsconstant2 ] plot the typical curves obtained with esbas selecting from a portfolio constituted of a learning algorithm and an algorithm with a deterministic and stationary policy .",
    "esbas succeeds in remaining close to the best algorithm at each epoch .",
    "one can also observe a nice property : esbas even dominates both algorithm curves at some point , because the constant algorithm helps the learning algorithm to explore close to a reasonable policy .",
    "however , when the deterministic strategy is not so good , the reset of the stochastic bandits is harmful . as a result",
    ", learner - constant portfolios may yield quite strong relative pseudo - regrets : @xmath179 in figure [ fig : squarevsconstant1 ] .",
    "however , when the constant algorithm expected return is over 1.13 , slightly negative relative pseudo - regrets may still be obtained .",
    "subsection [ sec : noarmreset ] offers a straightforward improvement of esbas when one or several algorithm are known to be constant .",
    "esbas also performs well on larger portfolios of 8 learners ( see figure [ fig:8learners1 ] ) with negative relative pseudo - regrets : @xmath180 ( and @xmath181 against the worst algorithm ) , even if the algorithms are , on average , almost selected uniformly as figure [ fig:8learners2 ] reveals .",
    "esbas offers some staggered learning , but more importantly , early bad policy accidents in learners are avoided . the same kind of results are obtained with 4-learner portfolios .",
    "if we add a constant algorithm to these larger portfolios , esbas behaviour is generally better than with the constant vs learner two - fold portfolios .",
    "we interpret esbas s success at reliably outperforming the best algorithm in the portfolio as the result of the four following potential added values :    * calibrated learning : esbas selects the algorithm that is the most fitted with the data size .",
    "this property allows for instance to use shallow algorithms when having only a few data and deep algorithms once collected a lot . *",
    "diversified policies : esbas computes and experiments several policies .",
    "those diversified policies generate trajectories that are less redundant , and therefore more informational . as a result , the policies trained on these trajectories should be more efficient . *",
    "robustness : if one algorithm learns a terrible policy , it will soon be left apart until the next policy update .",
    "this property prevents the agent from repeating again and again the same blatant mistakes .",
    "* run adaptation : obviously , there has to be an algorithm that is the best on average for one given task at one given meta - time . but",
    "depending on the variance in the trajectory collection , it is not necessarily the best one for each run .",
    "the esbas meta - algorithm tries and selects the algorithm that is the best at each run .",
    "all these properties are inherited by algorithm selection similarity with ensemble learning  @xcite .",
    "simply , instead of a vote amongst the algorithms to decide the control of the next transition  @xcite , esbas selects the best performing algorithm .    in order to look deeper into the variance control effect of algorithm selection , in a similar fashion to ensemble learning , we tested two portfolios : four times the same algorithm _",
    "n-2-simple _ , and four times the same algorithm _",
    "n-3-simple_. the results show that they both outperform the _",
    "simple _ algorithm baseline , but only slightly ( respectively @xmath182 and @xmath183 ) .",
    "our interpretation is that , in order to control variance , adding randomness is not as good as changing hypotheses , _",
    "i.e. _ state space representations .",
    "esbas s worst results concern small portfolios of algorithms with constant policies .",
    "these ones do not improve over time and the full reset of the @xmath84-multi armed bandit urges esbas to explore again and again the same underachieving algorithm . one easy way to circumvent this drawback is to use the knowledge that these constant algorithms do not change and prevent their arm from resetting . by operating this way ,",
    "when the learning algorithm(s )",
    "start(s ) outperforming the constant one , esbas simply neither exploits nor explores the constant algorithm anymore .",
    "figure [ fig : no - arm - reset ] displays the learning curve in the no - arm - reset configuration for the constant algorithm .",
    "one can notice that esbas s learning curve follows perfectly the learning algorithm s learning curve when this one outperforms the constant algorithm and achieves a strong negative relative pseudo - regret of -125 .",
    "still , when the constant algorithm does not perform as well as in figure [ fig : no - arm - reset ] , another harmful phenomenon happens : the constant algorithm overrides the natural exploration of the learning algorithm in the early stages , and when the learning algorithm finally outperforms the constant algorithm , its exploration parameter is already low .",
    "this can be observed in experiments with constant algorithm of expected return inferior to 1 , as reported in table [ tab : recap ] .",
    "several results show that , in practice , the assumptions are transgressed .",
    "firstly , assumption [ ass : compatibility ] , which states that more initial samples would necessarily help further learning convergence , is violated when the @xmath184-greedy exploration parameter decreases with meta - time and not with the number of times this algorithm has been selected . indeed , this is the main reason of the remaining mitigated results obtained in subsection [ sec : noarmreset ] : instead of exploring in early stages , the agent selects the constant algorithm which results in generating over and over similar fair but non optimal trajectories .",
    "finally , the learning algorithm might learn slower because of @xmath184 being decayed without having explored .",
    "secondly , we also observe that assumption [ ass : fairlearning ] is transgressed .",
    "indeed , it states that if a trajectory set is better than another for a given algorithm , then it s the same for the other algorithms",
    ". this assumption does not prevent calibrated learning , but it prevents the run adaptation property introduced in subsection [ sec : reasons ] that states that an algorithm might be the best on some run and another one on other runs .",
    "still , this assumption infringement does not seem to harm the experimental results .",
    "it even seems to help in general .",
    "thirdly , off - policy reinforcement learning algorithms exist , but in practice , we use state space representations that distort their off - policy property  @xcite .",
    "however , experiments do not reveal any obvious bias related to the off / on - policiness of the trajectory set the algorithms train on .    and",
    "finally , let us recall here the unfairness of the exploration chosen by algorithms that has already been noticed in subsection [ sec : assumptions ] and that also transgresses assumption [ ass : fairlearning ] .",
    "nevertheless , experiments did not raise any particular bias on this matter .",
    "related to algorithm selection for rl , @xcite consists in using meta - learning to tune a fixed reinforcement algorithm in order to fit observed animal behaviour , which is a very different problem to ours . in @xcite ,",
    "the reinforcement learning algorithm selection problem is solved with a portfolio composed of online rl algorithms . in those articles ,",
    "the core problem is to balance the budget allocated to the sampling of old policies through a _",
    "lag _ function , and budget allocated to the exploitation of the up - to - date algorithms .",
    "their solution to the problem is thus independent from the reinforcement learning structure and has indeed been applied to a noisy optimisation solver selection  @xcite .",
    "the main limitation from these works relies on the fact that _ on - policy _ algorithms were used , which prevents them from sharing trajectories among algorithms .",
    "meta - learning specifically for the eligibility trace parameter has also been studied in @xcite .",
    "a recent work  @xcite studies the learning process of reinforcement learning algorithms and selects the best one for learning faster on a new task .",
    "this approach assumes several problem instances and is more related to the batch algorithm selection ( see section [ sec : batch ] ) .    as for rl",
    "can also be related to ensemble rl .",
    "@xcite uses combinations of a set of rl algorithms to build its online control such as policy voting or value function averaging .",
    "this approach shows good results when all the algorithms are efficient , but not when some of them are underachieving .",
    "hence , no convergence bound has been proven with this family of meta - algorithms . horde  @xcite and multi - objective ensemble rl  @xcite are algorithms for hierarchical rl and do not directly compare with as .",
    "regarding policy selection , esbas advantageously compares with the rl with policy advice s regret bounds of @xmath185 on static policies  @xcite .",
    "in this article , we tackle the problem of selecting online off - policy reinforcement learning algorithms .",
    "the problem is formalised as follows : from a fixed portfolio of algorithms , a meta - algorithm learns which one performs the best on the task at hand .",
    "fairness of algorithm evaluation is granted by the fact that the rl algorithms learn off - policy .",
    "esbas , a novel meta - algorithm , is proposed .",
    "its principle is to divide the meta - time scale into epochs .",
    "algorithms are allowed to update their policies only at the start each epoch .",
    "as the policies are constant inside each epoch , the problem can be cast into a stochastic multi - armed bandit .",
    "an implementation with ucb1 is detailed and a theoretical analysis leads to upper bounds on the short - sighted regret .",
    "the negotiation dialogue experiments show strong results : not only esbas succeeds in consistently selecting the best algorithm , but it also demonstrates its ability to perform staggered learning and to take advantage of the ensemble structure .",
    "the only mitigated results are obtained with algorithms that do not learn over time . a straightforward improvement of the esbas meta - algorithm is then proposed and its gain is observed on the task .    as for next steps , we plan to work on an algorithm inspired by the _ lag _ principle introduced in @xcite , and apply it to our off - policy rl setting .",
    "1.81    [ cols=\"<,<,<\",options=\"header \" , ]     & section [ sec : algos ] + _ n-1-fast-2 _ & f@xmath0i with @xmath186 & section [ sec : algos ] + _ constant-@xmath48 _ & non - learning algorithm with average performance @xmath48 & section [ sec : algos ] + _ @xmath168 _ & number of noisy features added to the feature set & section [ sec : algos ] +    1.81.5 [ tab : gloss2 ]",
    "from definition [ def : abs ] :    ^_abs(t ) = t^*_- _ ,    ^_abs(t ) = t^*_- _ _ ,    ^_abs(t ) = _ _ ,    where @xmath187 is the subset of @xmath60 with all the trajectories generated with algorithm @xmath28 , where @xmath188 is the index of the @xmath189^th^ trajectory generated with algorithm @xmath28 , and where @xmath190 is the cardinality of finite set @xmath191 . by convention , let us state that @xmath192 if @xmath193 .",
    "then :    ^_abs(t ) = _ _ i=1^t _ .    to conclude , let us prove by mathematical induction the following inequality :    _ _ ^    is true by vacuity for @xmath194 : both left and right terms equal @xmath195 .",
    "now let us assume the property true for @xmath189 and prove it for @xmath196 :    _ = _ ,    _ = _ ,    _ = _ .    if @xmath197 , by applying mathematical induction assumption , then by applying assumption [ ass : compatibility ] and finally by applying assumption [ ass : monotony ] recursively , we infer that :    _ _ ^ ,    _",
    "_ ^ ,    _ _ ^ ,    _ _",
    "^.    if @xmath198 , the same inequality is straightforwardly obtained , since , by convention @xmath199 , and since , by definition @xmath200 .",
    "the mathematical induction proof is complete .",
    "this result leads to the following inequalities :    ^_abs(t ) _ _ i=1^t _ ^ ,    ^_abs(t ) _",
    "^^_abs(t ) ,    ^_abs(t ) k_^^_abs(t ) ,    which leads directly to the result :    , _ abs^(t)(_^k _",
    "abs^^k(t ) ) .",
    "_ this proof may seem to the reader rather complex for such an intuitive and loose result but algorithm selection @xmath65 and the algorithms it selects may act tricky . for instance selecting algorithm @xmath28",
    "only when the collected trajectory sets contains misleading examples ( _ i.e. _ with worse expected return than with an empty trajectory set ) implies that the following unintuitive inequality is always true : @xmath201 . in order to control all the possible outcomes , one needs to translate the selections of algorithm @xmath28 into @xmath69 s view . _",
    "by simplification of notation , @xmath202 . from definition",
    "[ def : ss ] :    ^^esbas_ss(t ) = _ ^esbas ,    ^^esbas_ss(t ) = _ ^esbas ,    ^^esbas_ss(t ) _ ^esbas ,    ^^esbas_ss(t ) _",
    "= 0^_2(t ) _",
    "ss^^esbas ( ) , [ eq : th2:epochregret ]    where @xmath203 is the epoch of meta - time @xmath10 .",
    "a bound on short - sighted pseudo - regret @xmath204 for each epoch @xmath87 can then be obtained by the stochastic bandit @xmath85 regret bounds in @xmath205 :    _ ss^^esbas ( ) = _ ^esbas ,    _",
    "ss^^esbas ( ) ( ) ,    _",
    "ss^^esbas ( ) ( ) ,    _ 10 , _",
    "ss^^esbas ( ) ,    where = _    and where_^= \\ {    ll + & if _ ^= _  _ ^ , + _  _ ^-_^ & otherwise .    .",
    "since we are interested in the order of magnitude , we can once again only consider the upper bound of @xmath206 :    _ ( ) ,    ( _ ) ,    _ 20 , ,    where the second best algorithm at epoch @xmath87 such that @xmath207 is noted @xmath208 .",
    "injected in equation [ eq : th2:epochregret ] , it becomes :    _ ss^^esbas(t ) _",
    "1_2_=0^_2(t ) , [ eq : deltaminbound ]    which proves the result .      if @xmath209 , then @xmath210 , where @xmath211 .",
    "@xmath212 means that only one algorithm @xmath213 converges to the optimal asymptotic performance @xmath214 and that @xmath215 such that @xmath216 , @xmath217 , such that @xmath218 , @xmath219 . in this case",
    ", the following bound can be deduced from equation [ eq : deltaminbound ] :    _ ss^^esbas(t ) _ 4 + _ = _",
    "1^(t ) ,    _ ss^^esbas(t ) _ 4 + [ eq : cor3:result ] ,    where @xmath220 is a constant equal to the short - sighted pseudo - regret before epoch @xmath221 :    _ 4 = _ ss^^esbas(2^_1 - 1 )    equation [ eq : cor3:result ] directly leads to the corollary .    if @xmath222 , then @xmath223 .",
    "if @xmath95 decreases slower than polynomially in epochs , which implies decreasing polylogarithmically in meta - time , _ i.e. _ @xmath224 , @xmath225 , such that @xmath226 , @xmath227 , then , from equation [ eq : deltaminbound ] :    _ ss^^esbas(t ) _ 6 + _ = _",
    "2^(t ) ,    _ ss^^esbas(t ) _ 6 + _ = _",
    "2^(t ) ^m^+1 ,    _",
    "ss^^esbas(t ) ^m^+2(t ) , [ eq : cor4:result ]    where @xmath228 is a constant equal to the short - sighted pseudo - regret before epoch @xmath229 :    _ 6 = _ ss^^esbas(2^_2 - 1 )",
    ".    equation [ eq : cor4:result ] directly leads to the corollary .    if @xmath230 , then @xmath231 .    if @xmath95 decreases slower than a fractional power of meta - time @xmath23 , then @xmath232 , @xmath233 , @xmath234 , such that @xmath235 , @xmath236 , and therefore , from equation [ eq : deltaminbound ] :    _ ss^^esbas(t ) _ 8 + _ = _",
    "3^(t ) ,    _ ss^^esbas(t ) _ 8 + _ = _",
    "3^(t ) ,    _ ss^^esbas(t ) _ 8 + _ = _",
    "3^(t ) ( 2^c^)^ , [ eq : sumxex ]    where @xmath237 is a constant equal to the short - sighted pseudo - regret before epoch @xmath238 :    _ 8 = _ ss^^esbas(2^_3 - 1 ) .",
    "the sum in equation [ eq : sumxex ] is solved as follows :    _ i = i_0^n ix^i = x_i = i_0^n ix^i-1 ,    _ i = i_0^n ix^i = x_i = i_0^n ,    _ i = i_0^n ix^i = x ,    _ i = i_0^n ix^i = x ,    _ i = i_0^n ix^i = ( ( x-1)nx^n - x^n -(x-1)i_0x^i_0 - 1 + x^i_0 ) .",
    "this result , injected in equation [ eq : sumxex ] , induces that @xmath239 , @xmath240 , @xmath241 :    _ ss^^esbas(t ) _ 8 + ( t ) 2^c^(t ) ,    _ ss^^esbas(t ) _ 8 + t^c^ ( t ) ,    which proves the corollary .",
    "the esbas absolute pseudo - regret is written with the following notation simplifications : @xmath242 and @xmath243 :            note that @xmath121 is the optimal constant algorithm selection at horizon @xmath23 , but it is not necessarily the optimal algorithm selection : there might exist , and there probably exists a non constant algorithm selection yielding a smaller pseudo - regret .",
    "the esbas absolute pseudo - regret @xmath244 can be decomposed into the pseudo - regret for not having followed the optimal constant algorithm selection @xmath121 and the pseudo - regret for not having selected the algorithm with the highest return , _",
    "i.e. _ between the pseudo - regret on the trajectory and the pseudo - regret on the immediate optimal return :      where @xmath245 is the expected return of policy @xmath246 , learnt by algorithm @xmath213 on trajectory set @xmath247 , which is the trajectory subset of @xmath54 obtained by removing all trajectories that were not generated with algorithm @xmath213 .          on the one side ,",
    "assumption [ ass : fairlearning ] of fairness states that one algorithm learns as fast as any another over any history .",
    "the asymptotically optimal algorithm(s ) when @xmath248 is(are ) therefore the same one(s ) whatever the the algorithm selection is . on the other side ,",
    "let @xmath249 denote the probability , that at time @xmath10 , the following inequality is true :        with probability @xmath250 , inequality [ eq : subsize ] is not guaranteed and nothing can be inferred about @xmath245 , except it is bounded under by @xmath251 .",
    "let @xmath252 be the subset of @xmath253 such that @xmath254 .",
    "then , @xmath250 can be expressed as follows :            let consider @xmath255 the set of all sets @xmath60 such that @xmath256 and such that last trajectory in @xmath60 was generated by @xmath28 . since esbas , with @xmath85 , a stochastic bandit with regret in @xmath257 ,",
    "guarantees that all algorithms will eventually be selected an infinity of times , we know that :                                                    we recall here that the stochastic bandit algorithm @xmath85 was assumed to guarantee to try the best algorithm @xmath213 at least @xmath259 times with high probability @xmath260 and @xmath261 .",
    "now , we show that at any time , the longest stochastic bandit run ( _ i.e. _ the epoch that experienced the biggest number of pulls ) lasts at least @xmath262 : at epoch @xmath263 , the meta - time spent on epochs before @xmath264 is equal to @xmath265 ; the meta - time spent on epoch @xmath266 is equal to @xmath267 ; the meta - time spent on epoch @xmath87 is either below @xmath267 , in which case , the meta - time spent on epoch @xmath266 is higher than @xmath268 , or the meta - time spent on epoch @xmath87 is over @xmath267 and therefore higher than @xmath268 .",
    "thus , esbas is guaranteed to try the best algorithm @xmath213 at least @xmath269 times with high probability @xmath249 and @xmath270 . as a result :"
  ],
  "abstract_text": [
    "<S> dialogue systems rely on a careful reinforcement learning design : the learning algorithm and its state space representation . in lack of more rigorous knowledge , the designer resorts to its practical experience to choose the best option . in order to automate and to improve the performance of the aforementioned process , </S>",
    "<S> this article formalises the problem of online off - policy reinforcement learning algorithm selection . </S>",
    "<S> a meta - algorithm is given for input a portfolio constituted of several off - policy reinforcement learning algorithms . </S>",
    "<S> it then determines at the beginning of each new trajectory , which algorithm in the portfolio is in control of the behaviour during the full next trajectory , in order to maximise the return . </S>",
    "<S> the article presents a novel meta - algorithm , called epochal stochastic bandit algorithm selection ( esbas ) . </S>",
    "<S> its principle is to freeze the policy updates at each epoch , and to leave a rebooted stochastic bandit in charge of the algorithm selection . under some assumptions , </S>",
    "<S> a thorough theoretical analysis demonstrates its near - optimality considering the structural sampling budget limitations . </S>",
    "<S> then , esbas is put to the test in a set of experiments with various portfolios , on a negotiation dialogue game . </S>",
    "<S> the results show the practical benefits of the algorithm selection for dialogue systems , in most cases even outperforming the best algorithm in the portfolio , even when the aforementioned assumptions are transgressed . </S>"
  ]
}