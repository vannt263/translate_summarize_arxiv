{
  "article_text": [
    "consider the multiple linear regression model @xmath0 where @xmath1 are responses , @xmath2 are independent and identically distributed ( iid ) random variables , @xmath3 are known non - random design vectors , and @xmath4 is the @xmath5-dimensional vector of regression parameters . when the dimension @xmath5 is large , it is common to approach regression model ( [ eqn : model ] ) with the assumption that the vector @xmath6 is sparse , that is that the set @xmath7 has cardinality @xmath8 much smaller than @xmath5 , meaning that only a few of the covariates are `` active '' .",
    "the lasso estimator introduced by tibshirani ( 1996 ) is well suited to the sparse setting because of its property that it sets some regression coefficients exactly equal to 0 .",
    "one disadvantage of the lasso , however , is that it produces non - trivial asymptotic bias for the non - zero regression parameters , primarily because it shrinks all estimators toward zero [ cf .",
    "knight and fu ( 2000 ) ] .",
    "building on the lasso , zou ( 2006 ) proposed the adaptive lasso [ hereafter referred to as alasso ] estimator @xmath9 of @xmath6 in the regression problem ( [ eqn : model ] ) as @xmath10,\\ ] ] where @xmath11 is the @xmath12th component of a root-@xmath13-consistent estimator @xmath14 of @xmath6 , such as the ordinary least squares ( ols ) estimator when @xmath15 or the lasso or ridge regression estimator when @xmath16 , @xmath17 is the penalty parameter , and @xmath18 is a constant governing the influence of the preliminary estimator @xmath14 on the alasso fit .",
    "zou ( 2006 ) showed in the fixed-@xmath5 setting that under some regularity conditions and with the right choice of @xmath19 , the alasso estimator enjoys the so - called oracle property [ cf .",
    "fan and li ( 2001 ) ] ; that is , it is variable - selection consistent and it estimates the non - zero regression parameters with the same precision as the ols estimator which one would compute if the set of active covariates were known .    in an important recent work , minnier , tian and cai ( 2011 ) introduced the perturbation bootstrap in the alasso setup .",
    "to state their main results , let @xmath20 be the naive perturbation bootstrap alasso estimator prescribed by minnier , tian and cai ( 2011 ) and define @xmath21 and @xmath22 .",
    "these authors showed that under some regularity conditions and with @xmath5 fixed as @xmath23 @xmath24 where @xmath25 , @xmath26 denotes the sub - vector of @xmath27 corresponding to the co - ordinates in @xmath28 , `` @xmath29 '' denotes asymptotic equivalence in distribution , and @xmath30 denotes bootstrap probability conditional on the data .",
    "thus minnier , tian and cai ( 2011 ) [ hereafter referred to as mtc(11 ) ] showed that , in the fixed-@xmath5 setting and conditionally on the data , the naive perturbation bootstrap version of the alasso estimator is variable - selection consistent in the sense that it recovers the support of the alasso estimator with probability tending to one and that its distribution conditional on the data converges at the same time to that of the alasso estimator for the non - zero regression parameters .",
    "but the accuracy of inference for non - zero regression parameters relies on the rate of convergence of the bootstrap distribution of @xmath31 to the distribution of @xmath32 after proper studentization .",
    "furthermore , chatterjee and lahiri ( 2013 ) showed that the convergence of the alasso estimators of the nonzero regression coefficients to their oracle normal distribution is quite slow , owing to the bias induced by the penalty term in ( 1.2 ) .",
    "thus , it would be important for the accuracy of inference if second - order correctness can be achieved in approximating the distribution of the alasso estimator by the perturbation bootstrap .",
    "second - order correctness implies that the distributional approximation has a uniform error rate of @xmath33 .",
    "we show in this paper , however , that the distribution of the naive perturbation bootstrap version of the alasso estimator , as defined by mtc(11 ) , can not be second order correct even in fixed dimension . for more details , see section [ sec : naiveinconsistency ] .",
    "we introduce a modified perturbation bootstrap for the alasso estimator for which second order correctness does hold , even when the number of regression parameters @xmath34 is allowed to increase with the sample size @xmath13 .",
    "we also show in proposition [ prop : compute ] that the modified perturbation bootstrap version of the alasso estimator ( defined in section [ sec : mpb ] ) can be computed by minimizing simple criterion functions .",
    "this makes our bootstrap procedure computationally simple and inexpensive .    in this paper",
    ", we consider some pivotal quantities based on alasso estimators and establish that the modified perturbation bootstrap estimates the distribution of these pivotal quantities up to second order , i.e. with an error that is of much smaller magnitude than what we would obtain by using the normal approximation under the knowledge of the true active set of covariates .",
    "we will refer to the normal approximation which uses knowledge of the true set of active covariates as the oracle normal approximation .",
    "our main results show that the modified perturbation bootstrap method enables , for example , the construction of confidence intervals for the nonzero regression coefficients with smaller coverage error than those based on the oracle normal approximation .",
    "more precisely , we consider pivots which are studentizations of the quantities @xmath35 where @xmath36 is a @xmath37 matrix ( @xmath38 fixed ) producing @xmath38 linear combinations of interest of @xmath39 and where @xmath40 is a bias correction term which we will define later .",
    "we find that in the @xmath15 case , the modified perturbation bootstrap can estimate the distribution of the first pivot with an error of order @xmath33 ( see theorem [ thm : rstarlod ] ) .",
    "this is much smaller than the error of the oracle normal approximation , which was shown in theorem 3.1 of chatterjee and lahiri ( 2013 ) to be of the order @xmath41 , where @xmath42 is the bias targeted by @xmath40 and @xmath43 is determined by the initial estimator @xmath14 and the tuning parameters @xmath19 and @xmath44 ; both @xmath45 and @xmath46 are typically greater in magnitude than @xmath47 and hence determine the rate of the oracle normal approximation .",
    "we also discover that the bias correction in the second pivot improves the error rate so that the modified perturbation bootstrap estimator achieves the rate @xmath48 ( see theorem [ thm : rcheckstarlod ] ) , which is a significant improvement over the best possible rate of oracle normal approximation , namely @xmath49 . in the @xmath16 case ,",
    "we find that the modified perturbation bootstrap estimates the distributions of studentized versions of both the bias - corrected and un - bias - corrected pivots with the rate @xmath33 ( see theorem [ thm : hid ] ) , establishing the second - order correctness of our modified perturbation bootstrap in the high - dimensional setting .",
    "we show that the naive perturbation bootstrap of mtc(11 ) is not second - order correct ( see theorem [ thm : naiveinconsistent ] ) by investigating the karush - kuhn - tucker ( kkt ) condition [ cf .",
    "boyd and lieven ( 2004 ) ] corresponding to their minimization problem .",
    "it is shown that second order correctness is not attainable by the naive version of the perturbation bootstrap , primarily due to lack of proper centering of the naive bootstrapped alasso criterion function .",
    "we derive the form of the centering constant by analyzing the corresponding approximation errors using the theory of edgeworth expansion . to accommodate the centering correction",
    ", we modify the perturbation bootstrap criterion function for the alasso ; see section 2 for details .",
    "in addition , we also find out that it is beneficial , from both theoretical and computational perspectives , to modify the perturbation bootstrap version of the initial estimators in a similar way . to prove second order correctness of the modified perturbation bootstrap alasso , the key steps are to find an edgeworth expansion of the bootstrap pivotal quantities based on the modified criterion function and to compare it with the edgeworth expansion of the sample pivots .",
    "we want to mention that in our setting , the dimension @xmath5 of the regression parameter vector can grow polynomially in the sample size @xmath13 at a rate depending on the number of finite moments of the error distribution .",
    "extension to the case in which @xmath5 grows exponentially with @xmath13 would be possible under some strong assumptions , e.g. under finiteness of the moment generating function of the regression errors .",
    "we conclude this section with a brief literature review .",
    "the perturbation bootstrap was introduced by jin , ying , and wei ( 2001 ) as a resampling procedure where the objective function has a u - process structure .",
    "work on the perturbation bootstrap in the linear regression setup is limited .",
    "some work has been carried out by chatterjee and bose ( 2005 ) , mtc(11 ) , zhou , song and thompson ( 2012 ) , and das and lahiri ( 2016 ) .",
    "an analogous bootstrap procedure , called multiplier bootstrap , emerged recently as an effective procedure for approximating the underlying sampling distribution . for details on multiplier bootstrap procedure see bcher and dette ( 2013 ) , chernozhukov et al .",
    "( 2013 , 2014 , 2016 ) , bcher and kojadinovic ( 2016a , 2016b ) and references therein . as a variable selection procedure , tibshirani ( 1996 ) introduced the lasso .",
    "zou ( 2006 ) proposed the alasso as an improvement over the lasso .",
    "for the alasso and related popular penalized estimation and variable selection procedures , the residual bootstrap has been investigated by knight and fu ( 2000 ) , hall , lee and park ( 2009 ) , chatterjee and lahiri ( 2010 , 2011 , 2013 ) , wang and song ( 2011 ) , mtc(11 ) , van de geer et al .",
    "( 2014 ) , and camponovo ( 2015 ) , among others .",
    "the rest of the paper is organized as follows . the modified perturbation bootstrap for the alasso",
    "is introduced and discussed in section [ sec : mpb ] .",
    "main results concerning the estimation properties of the studentized modified perturbation bootstrap pivotal quantities are given in section [ sec : mainresults ] .",
    "negative results on the naive perturbation bootstrap approximation proposed by mtc(11 ) are discussed and intuition and explanations behind the modification of the modified perturbation bootstrap are given in section [ sec : naiveinconsistency ] .",
    "section [ sec : simulation ] presents simulation results exploring the finite - sample performance of the modified perturbation bootstrap in comparison with other methods for constructing confidence intervals based on alasso estimators and section [ sec : dataanalysis ] gives an illustration on real data .",
    "regularity conditions and the proofs are provided in section [ sec : proofs ] .",
    "further simulation results are relegated to the supplementary material .",
    "suppose , @xmath50 be @xmath13 independent copies of a non - degenerate random variable @xmath51 having expectation @xmath52 .",
    "these quantities will serve as perturbation quantities .",
    "the modified perturbation bootstrap in alasso involves careful construction of the penalized objective function .",
    "we need alasso estimated values @xmath53 besides the observed values @xmath54 ,",
    "@xmath55 to define the penalized objective function .",
    "the modified penalized objective function needs to incorporate the sum of two perturbed least square objective functions , one involving @xmath54 , @xmath55 and other with @xmath56 , @xmath55 , see definition [ eqn : mpb ] .",
    "similar modification is also needed in defining the bootstrap versions of the initial estimators , see definition [ eqn : mpbi ] .",
    "the motivation behind this construction is detailed in section [ sec : naiveinconsistency ] , where we also point out why the naive perturbation bootstrap formulation of mtc(11 ) fails drastically .",
    "formally , the modified perturbation bootstrap version @xmath57 of the alasso estimator @xmath9 is defined as @xmath58,\\end{aligned}\\ ] ] where @xmath59 is the @xmath12th component of @xmath60 , the modified perturbation bootstrap version of the initial estimator @xmath14 .",
    "we construct @xmath14 as @xmath61,\\end{aligned}\\ ] ] where @xmath62 when @xmath14 is taken as the ols , when @xmath15 , and @xmath63 or @xmath64 according as the initial estimator @xmath14 is taken as the lasso or ridge estimator , when @xmath16 .",
    "note that @xmath65 may be different from @xmath19 .",
    "we point out that the modified perturbation bootstrapped estimators can be computed using existing algorithms .",
    "define , @xmath66 for some non - negative constants @xmath67 , @xmath68 , where @xmath69 . now assume , @xmath70 , where @xmath71 , for @xmath72 and @xmath73 .",
    "then we have the following proposition .",
    "[ prop : compute ] @xmath74 .",
    "this proposition allows us to compute @xmath75 as well as @xmath76 by minimizing standard objective functions on some pseudo - values .",
    "note that the modified perturbation bootstrapped alasso estimator as well as the initial estimators can be obtained by properly perturbing the alasso residuals only in the decomposition @xmath77 , @xmath78 .",
    "we here present the minimal notation for stating our main results .",
    "assumptions ( a.1)(a.7 ) , to which we refer in the statements of our results , are detailed in section 7 .    from now on",
    "we denote the true parameter vector as @xmath79 , where the subscript @xmath13 emphasizes that the dimension @xmath80 may grow with the sample size @xmath13 . from @xmath81 , @xmath82 , and @xmath83 define @xmath84 , @xmath85 , and @xmath86 and let @xmath87 .",
    "we will suppose , without loss of generality , that @xmath88 .",
    "let @xmath89 and partition it according to @xmath90 as @xmath91 where @xmath92 is of dimension @xmath93 .",
    "define @xmath94 and partition @xmath95 as @xmath96 , where @xmath97 is a @xmath98 vector .",
    "let @xmath99 .    now define @xmath100 and the corresponding modified pertrubation bootstrap version @xmath101 , where @xmath36 is a known @xmath37 matrix with @xmath102 and @xmath38 is not dependent on @xmath13 .",
    "let @xmath103 contain the first @xmath104 columns of @xmath36 .",
    "define also the matrices @xmath105 and @xmath106 according to @xmath107 such that @xmath105 is the @xmath108 sub - matrix of @xmath109 with rows and columns in @xmath107 and @xmath106 is the @xmath110 sub - matrix of @xmath111 with columns in @xmath107 .",
    "let @xmath112 .",
    "let @xmath113 according as @xmath114 , @xmath115 , @xmath116 , respectively .",
    "we denote by @xmath117 the collection of convex sets of @xmath118 .",
    "our results will concern convergence in distribution of some studentizations of @xmath119 to the distributions of corresponding studentizations of @xmath120 .",
    "define the studentized or pivot quantities @xmath121 \\quad & \\check { \\mathbf{r}}_n^ * & = \\check \\sigma_n^{*-1 } \\check \\sigma_n \\tilde { \\boldsymbol{\\sigma}}_n^{-1/2}[{\\mathbf{t}}_n^ * + \\check { \\mathbf{b}}_n^*],\\end{aligned}\\ ] ] where @xmath122 and @xmath123 involve the bias - corrections @xmath124 and @xmath125 . in the above : @xmath126 where @xmath127 and @xmath128 , with @xmath129 and @xmath130 and @xmath131 , where @xmath132 for @xmath72 . moreover @xmath133 and",
    "lastly @xmath134 where @xmath135 and @xmath136 are the @xmath137 and @xmath138 vectors with @xmath12th entry equal to @xmath139 , @xmath140 and @xmath141 , @xmath142 , respectively .",
    "the matrix @xmath143 is the @xmath144 sub - matrix of @xmath109 with rows and columns in @xmath145 and @xmath146 is the @xmath147 sub - matrix of @xmath111 with columns in @xmath145 .",
    "we are motivated to look at these studentized or pivot quantities by the fact that studentization improves the rate of convergence of bootstrap estimators in many settings [ cf .",
    "hall ( 1992 ) ] .",
    "note that the matrices @xmath148 , @xmath149 , @xmath150 and @xmath151 used in defining the bootstrap pivots do not depend on @xmath152 .",
    "hence it is not required to compute the negative square roots of these matrices for each monte carlo bootstrap iteration ; these must only be computed once .",
    "this is a notable feature of our modified perturbation bootstrap method from the perspective of computational complexity .",
    "[ thm : rstarlod ] let ( a.1)(a.6 ) hold with @xmath153 .",
    "then @xmath154    theorem [ thm : rstarlod ] shows that after proper studentization , the modified perturbation bootstrap approximation of the distribution of the alasso estimator is second - order correct .",
    "the error rate reduces to @xmath33 from @xmath49 , the best possible rate obtained by the oracle normal approximation .",
    "this is a significant improvement from the perspective of inference . as a consequence",
    ", the precision of the percentile confidence intervals based on @xmath155 will be greater than that of confidence intervals based on the oracle normal approximation .",
    "we point out that the error rate in theorem [ thm : rstarlod ] can not be reduced to the optimal rate of @xmath48 , unlike in the fixed - dimension case . to achieve this optimal rate by our modified bootstrap method",
    ", we now consider a bias corrected pivot @xmath156 and its modified perturbation bootstrap version @xmath157 .",
    "the following theorem states that it achieves the optimal rate .",
    "[ thm : rcheckstarlod ] let ( a.1)(a.6 ) hold with @xmath158 . then @xmath159    theorem [ thm : rcheckstarlod ] suggests that the modified perturbation bootstrap achieves notable improvement in the error rate over the oracle normal approximation irrespective of the order of the bias term . thus theorem [ thm : rcheckstarlod ]",
    "establishes the perturbation bootstrap method as an effective method for approximating the distribution of the alasso estimator when @xmath15 .",
    "we now present a result for the quality of perturbation bootstrap approximation when the dimension @xmath5 of the regression parameter can be much larger than the sample size @xmath13 .",
    "we consider the initial estimator @xmath14 to be some @xmath160-consistent bridge estimator , for example lasso or ridge estimator , in defining the alasso estimator by ( 1.2 ) .",
    "the bootstrap version of lasso or ridge is defined by ( 2.2 ) .",
    "[ thm : hid ] let ( a.1)(i ) , ( ii ) , ( iii) and ( a.2)(a.6 ) and ( a.7 ) hold with @xmath158 and @xmath161 and let @xmath16 . then @xmath162    theorem [ thm : hid ] states that our proposed modified perturbation bootstrap approximation is second - order correct , even when @xmath16 .",
    "thus the error rate obtained by our proposed method is significantly better than @xmath49 , which is the best - attainable rate of the oracle normal approximation .",
    "we want to point out that for the validity of our method , @xmath5 can grow at at most a polynomial rate depending on the decay of the tail of the error distribution @xmath163 and on the growth of the design vectors .",
    "in this section we describe the naive perturbation bootstrap as defined by mtc(11 ) for the alasso and show that second - order correctness can not be achievable by their naive perturbation bootstrap method .",
    "when the objective function is the usual least squares criterion function the naive perturbation bootstrap alasso estimator @xmath164 is defined in mtc(11 ) as @xmath165,\\end{aligned}\\ ] ] where    1 .",
    "@xmath166 is such that @xmath167 and @xmath168 as @xmath23 .",
    "the initial naive bootstrap estimator is defined as @xmath169\\ ] ] and @xmath170 is the @xmath12th component of @xmath171 .",
    "@xmath172 is a set of iid non - negative random quantities with mean and variance both equal to 1 .",
    "note that the initial estimator @xmath171 is unique only when @xmath5 is less than or equal to @xmath13 .",
    "we now consider the quantity @xmath173 , which we can show from ( [ eqn : naivebetastar ] ) to be the minimizer @xmath174,\\end{aligned}\\ ] ] where @xmath175 is the @xmath12th component of the alasso estimator @xmath9 , @xmath176 , and @xmath177 .",
    "to describe the solution of mtc(11 ) , assume @xmath178 .",
    "mtc(11 ) claimed that when @xmath179 and @xmath5 is fixed , @xmath180 is a solution of ( [ eqn : ustarnaive ] ) for sufficiently large @xmath13 , where @xmath181 however , to achieve second order correctness , we need to obtain a solution @xmath182 of ( [ eqn : ustarnaive ] ) such that @xmath183 .",
    "we show that such an @xmath184 has the form @xmath185}\\ ] ] for sufficiently large @xmath13 , where @xmath186 is the first @xmath104 components of @xmath187 and the @xmath12th component of @xmath188 equals to @xmath189 , @xmath190 ( here we drop the subscript @xmath13 from the notations of true parameter values since we are considering @xmath5 to be fixed in this section ) .",
    "we establish this fact by exploring the kkt condition corresponding to ( [ eqn : ustarnaive ] ) , which is given by @xmath191 for some @xmath192 with @xmath193 $ ] for @xmath194 and @xmath195 @xmath196 . since @xmath197 is a non - negative definite matrix , ( [ eqn : ustarnaive ] )",
    "is a convex optimization problem ; hence ( [ eqn : kkt ] ) is both necessary and sufficient in solving ( [ eqn : ustarnaive ] ) .",
    "note that @xmath187 is not centered and hence we need to adjust the solution @xmath182 for centering before investigating if the naive perturbation bootstrap can asymptotically correct the distribution of alasso up to second order . clearly , the centering adjustment term is @xmath198 where @xmath199 .",
    "it follows from the steps of the proofs of the results of section [ sec : mainresults ] that we need @xmath200 to achieve second - order correctness .",
    "we show that this is indeed not the case even in the fixed @xmath5 setting .",
    "more precisely , we negate the second - order correctness of the naive perturbation bootstrap of mtc(11 ) by first showing that @xmath201 satisfies the kkt condition ( [ eqn : kkt ] ) exactly with bootstrap probability converging to 1 .",
    "then we show that @xmath202 diverges in bootstrap probability to @xmath203 , which in turn implies that the conditional cdf of @xmath204 can not approximate the cdf of @xmath205 with the uniform accuracy @xmath206 , needed for the validity of second - order correctness .",
    "we formalize these arguments in the following theorem .",
    "[ thm : naiveinconsistent ] suppose @xmath5 be fixed and @xmath207 , a positive definite matrix . define @xmath208 and let @xmath209 and @xmath210 @xmath211 as @xmath23 .",
    "also assume that ( a.1)(i ) , ( ii ) and ( a.4)(i ) hold with @xmath212 .",
    "then there exists a sequence of borel sets @xmath213 with @xmath214 and given @xmath215 , the following conclusions hold .",
    "* @xmath216 . * @xmath217 for any @xmath218 . * @xmath219 for some @xmath220 .",
    "theorem [ thm : naiveinconsistent ] ( a ) , ( b ) state that the naive perturbation bootstrap is incompetent in approximating the distribution of alasso up to second order .",
    "the fundamental reason behind second order incorrectness is the inadequate centering in the form of @xmath221 .",
    "although the adjustment term necessary for centering is @xmath222 , which essentially helps to establish distributional consistency in mtc(11 ) , the term is coarser than @xmath47 , leading to second order incorrectness .",
    "part ( c ) conveys uniformly how far the naive bootstrap cdf is from the original cdf .",
    "theorem [ thm : naiveinconsistent ] settles the fact that naive perturbation bootstrap of mtc(11 ) does not provide a solution for approximating the distribution of @xmath223 . on an event with probability tending to @xmath224 , the naive perturbation bootstrap estimator suffers",
    "either in that @xmath164 is not conditionally variable - selection consistent or @xmath225 is not @xmath226 .",
    "if one looks closely into the kkt conditions ( [ eqn : kkt ] ) , one can recognize that the problem occurs because @xmath187 is not centered .",
    "let @xmath227 denote the centered version of @xmath187 , that is @xmath228 , and consider the vector equation @xmath229 which is same as ( [ eqn : kkt ] ) , but after replacing @xmath187 with @xmath227 .",
    "note that solutions to ( [ eqn : mkkt ] ) are of the form @xmath230 , where @xmath231 is @xmath232 , are @xmath226 on the set @xmath233 as well as conditionally variable - selection consistent due to their form .",
    "if we consider the last @xmath234 equations of ( [ eqn : kkt ] ) , after putting @xmath230 in place of @xmath235 , then for all @xmath236 we must have @xmath237\\leq \\dfrac{\\lambda_n^*}{2\\sqrt{n}}|\\tilde{\\beta}_{jn}^{*n}|^{-\\gamma},\\end{aligned}\\ ] ] where @xmath238 is the @xmath12th row of @xmath239 .",
    "it can be shown easily that @xmath240 $ ] is @xmath226 , whereas on the set @xmath233 , under the hypothesis of theorem 5.1(a ) we have @xmath241 , @xmath14 being the ordinary least squares estimator ( cf .",
    "das and lahiri ( 2016 ) ) .",
    "since it is not in general guaranteed that @xmath242 on @xmath233 , ( [ eqn : mkkts ] ) can not be true ; however , @xmath243 with high probability for sufficiently large @xmath13 and hence if the bootstrap initial estimator @xmath244 is close to @xmath9 , we can expect ( [ eqn : mkkts ] ) to be satisfied .",
    "thus we also need the centering modification for the perturbation bootstrap initial estimator , as mentioned in section 2 .",
    "we lastly note that @xmath245 involves the inverse of the random matrix @xmath246 .",
    "though there is no problem in fixed dimension , the inversion of a random matrix can harm the approximation error rate in increasing dimension .",
    "however , if we implement the modification described in section 2 , then all of the above shortcomings of the perturbation bootstrap method become resolved and the conditional oracle property ( [ eqn : condoracle ] ) will be satisfied even in increasing dimension under some mild regularity conditions .",
    "we study through simulation the coverage of one - sided and two - sided @xmath247 confidence intervals for individual nonzero regression coefficients constructed via the pivot quantities @xmath248 and @xmath249 as well as via their modified perturbation bootstrap versions @xmath250 and @xmath251 . to make futher comparisons",
    ", we also construct confidence intervals based on a normal approximation to the distribution of a local quadratic approximation pivot @xmath252 , which uses the estimator of cov@xmath253 proposed in the original alasso paper by zou ( 2006 ) .",
    "we also consider the confidence interval based on the closeness in distribution of @xmath120 to a normal@xmath254 random variable , where we use the true active set of covariates @xmath255 to compute @xmath256 .",
    "we denote this by @xmath257 . for comparison with another bootstrap procedure ,",
    "we construct the confidence intervals based on the naive perturbation bootstrap from minnier et al .",
    "( 2011 ) which in that paper are denoted by @xmath258 and @xmath259 .    under the settings",
    "@xmath260 we generate @xmath13 independent copies @xmath261 of @xmath262 from the model @xmath263 , where @xmath264 is a standard normal random variable , @xmath265 is a mean - zero multivariate normal random vector such that @xmath266 for @xmath267 , and @xmath268 with @xmath269 defined as @xmath270 for @xmath271 .",
    "we compute the empirical coverage over @xmath272 simulated data sets of one- and two - sided confidence intervals for each nonzero regression coefficient under crossvalidation - selected values of @xmath273 and @xmath19 , where @xmath273 is the value of the tuning parameter used to obtain the preliminary lasso estimate @xmath274 and @xmath19 is the value of the tuning parameter used to obtain the alasso estimate @xmath275 .    in the @xmath15 setting , we set @xmath276 , whereby we use the ordinary least squares estimate for the preliminary estimator @xmath274 .",
    "when @xmath16 , the value of @xmath273 is chosen via @xmath277-fold crossvalidation and @xmath274 is computed under the selected value of @xmath273 . once @xmath274 is obtained , @xmath277-fold crossvalidation is used to select @xmath19 .",
    "the values @xmath273 and @xmath19 are thereafter held fixed for all bootstrap computations on the same dataset . in each crossvalidation procedure , the largest value of the tuning parameter for which the crossvalidation prediction error lies within one standard error of its minimum is used so that greater penalization is preferred .",
    "tables [ tab : p100n120cvlambda ] and [ tab : p80n200cvlambda ] display the coverage results under crossvalidation selection of @xmath273 and @xmath19 for the @xmath278 cases @xmath279 and tables [ tab : p250n150cvlambda ] and [ tab : p500n150cvlambda ] for the @xmath280 cases @xmath281 .",
    "the median values of the crossvalidation selections of @xmath273 and @xmath19 under each setting are provided in the table captions in the forms @xmath282 and @xmath283 where @xmath284 and @xmath285 are constants .",
    "these correspond to the forms of the theoretical choices of @xmath273 and @xmath19 under the choice of @xmath179 .    in all simulations we use @xmath179 in the alasso and we set the number of bootstrap replicates to @xmath286 .",
    "the perturbations @xmath152 for the modified perturbation bootstrap are generated independently from the beta@xmath287 distribution and for the perturbation bootstrap of minnier et al .",
    "( 2011 ) from the unit - mean exponential distribution as prescribed in that paper .",
    "r|cccccccc + & & & + @xmath269 & @xmath252 & @xmath257 & @xmath258 & @xmath259 & @xmath248 & @xmath288 & @xmath122 & @xmath289 + -0.75 & 0.42 & 0.33 & 0.13 & 0.53 & 0.36 & 0.69 & 0.46 & 0.79 + & _ ( 0.52 ) _ & _ ( 0.37 ) _ & _ ( 0.44 ) _ & _ ( 0.47 ) _ & _ ( 1.15 ) _ & _ ( 0.39 ) _ & _ ( 0.41 ) _ & _ ( 0.53 ) _",
    "+ 1.50 & 0.53 & 0.49 & 0.21 & 0.76 & 0.52 & 0.90 & 0.61 & 0.92 + & _ ( 0.48 ) _ & _ ( 0.39 ) _ & _ ( 0.59 ) _ & _ ( 0.61 ) _ & _ ( 0.73 ) _ & _ ( 0.47 ) _ & _ ( 0.43 ) _ & _ ( 0.63 ) _",
    "+ -2.25 & 0.74 & 0.70 & 0.43 & 0.86 & 0.72 & 0.89 & 0.78 & 0.93 + & _ ( 0.50 ) _ & _ ( 0.39 ) _ & _ ( 0.57 ) _ & _ ( 0.58 ) _ & _ ( 1.03 ) _ & _ ( 0.43 ) _ & _ ( 0.43 ) _ & _ ( 0.57 ) _ + 3.00 & 0.87 & 0.83 & 0.64 & 0.87 & 0.86 & 0.86 & 0.87 & 0.91 + & _ ( 0.45 ) _ & _ ( 0.37 ) _ & _ ( 0.49 ) _ & _ ( 0.49 ) _ & _ ( 0.50 ) _ & _ ( 0.37 ) _ & _ ( 0.41 ) _ & _ ( 0.48 ) _",
    "+   + -0.75 & 0.33 & 0.25 & 0.10 & 0.41 & 0.27 & 0.71 & 0.37 & 0.83 + 1.50 & 0.45 & 0.42 & 0.16 & 0.66 & 0.44 & 0.91 & 0.53 & 0.95 + -2.25 & 0.66 & 0.64 & 0.36 & 0.80 & 0.66 & 0.91 & 0.71 & 0.96 + 3.00 & 0.82 & 0.79 & 0.54 & 0.84 & 0.80 & 0.89 & 0.85 & 0.94 +    in the @xmath290 case , for which table [ tab : p100n120cvlambda ] displays the results , the modified perturbation bootstrap intervals based on @xmath291 and @xmath123 achieve the closest - to - nominal coverage , the @xmath123 interval performing somewhat better due to the bias correction .",
    "the two - sided @xmath123 interval achieves sub - nominal coverage for the smallest regression coefficient @xmath292 , given that this coefficient was occasionally estimated to be zero , but achieves close - to - nominal coverage for the larger regression coefficients .",
    "the coverage of the other intervals is much more dramatically effected by the magnitude of the regression coefficient @xmath269 , a phenomenon which is even more pronounced in the one - sided coverages ; for example , the coverage of the @xmath122 interval rises from @xmath293 for @xmath294 to @xmath295 for @xmath296 .",
    "given that the modified perturbation bootstrap distributions of @xmath291 , and @xmath123 result in much closer - to - nominal coverages than the normal approximations to the distributions of @xmath248 , and @xmath122 , we may conclude that the sample size is too small for the asymptotically - normal pivots to have sufficiently approached their limiting distribution ; the second - order correctness of the modified perturbation bootstrap is thus apparent .",
    "r|cccccccc + & & & + @xmath269 & @xmath252 & @xmath257 & @xmath258 & @xmath259 & @xmath248 & @xmath288 & @xmath122 & @xmath289 + -0.75 & 0.39 & 0.35 & 0.12 & 0.67 & 0.36 & 0.88 & 0.45 & 0.91 + & _ ( 0.39 ) _ & _ ( 0.30 ) _ & _ ( 0.44 ) _ & _ ( 0.46 ) _ & _ ( 0.30 ) _ & _ ( 0.38 ) _ & _ ( 0.29 ) _ & _ ( 0.45 ) _ + 1.50 & 0.55 & 0.52 & 0.19 & 0.83 & 0.52 & 0.92 & 0.60 & 0.93 + & _ ( 0.35 ) _ & _ ( 0.31 ) _ & _ ( 0.52 ) _ & _ ( 0.53 ) _ & _ ( 0.31 ) _ & _ ( 0.39 ) _ & _ ( 0.30 ) _ & _ ( 0.47 ) _",
    "+ -2.25 & 0.75 & 0.74 & 0.39 & 0.91 & 0.74 & 0.91 & 0.79 & 0.94 + & _ ( 0.35 ) _ & _ ( 0.31 ) _ & _ ( 0.47 ) _ & _ ( 0.47 ) _ & _ ( 0.31 ) _ & _ ( 0.34 ) _ & _ ( 0.30 ) _ & _ ( 0.39 ) _ + 3.00 & 0.89 & 0.89 & 0.64 & 0.93 & 0.89 & 0.91 & 0.90 & 0.91 + & _ ( 0.32 ) _ & _ ( 0.30 ) _ & _ ( 0.39 ) _ & _ ( 0.39 ) _ & _ ( 0.30 ) _ & _ ( 0.29 ) _ & _ ( 0.29 ) _ & _ ( 0.33 ) _",
    "+   + -0.75 & 0.30 & 0.28 & 0.11 & 0.57 & 0.28 & 0.89 & 0.37 & 0.95 + 1.50 & 0.43 & 0.40 & 0.12 & 0.73 & 0.40 & 0.96 & 0.51 & 0.97 + -2.25 & 0.65 & 0.64 & 0.31 & 0.82 & 0.64 & 0.94 & 0.71 & 0.95 + 3.00 & 0.82 & 0.81 & 0.53 & 0.86 & 0.81 & 0.92 & 0.84 & 0.95 +    for the @xmath297 case , for which table [ tab : p80n200cvlambda ] displays the results , we see again that all the confidence intervals besides the @xmath291 and @xmath123 intervals achieve sub - nominal coverage . given the larger sample size , the coverage of @xmath123 is close - to - nominal even for the smallest regression coefficient @xmath294 . the bias correction of in the @xmath123 interval makes less of a difference in this case than in the @xmath290 case due to the larger sample size and smaller @xmath5 . in spite of the larger sample size , however , the coverages of all the confidence intervals besides the modified perturbation bootstrap intervals are still dramatically affected by the size of the regression coefficient @xmath269 , and none of them achieves close - to - nominal coverage for any of the nonzero regression coefficients .",
    "r|cccccccc + & & & + @xmath269 & @xmath252 & @xmath257 & @xmath258 & @xmath259 & @xmath248 & @xmath288 & @xmath122 & @xmath289 + -0.75 & 0.61 & 0.55 & 0.41 & 0.73 & 0.56 & 0.89 & 0.72 & 0.94 + & _ ( 0.42 ) _ & _ ( 0.32 ) _ & _ ( 0.44 ) _ & _ ( 0.47 ) _ & _ ( 0.33 ) _ & _ ( 0.43 ) _ & _ ( 0.39 ) _ & _ ( 0.56 ) _ + 1.50 & 0.72 & 0.69 & 0.52 & 0.86 & 0.70 & 0.93 & 0.85 & 0.95 + & _ ( 0.42 ) _ & _ ( 0.34 ) _ & _ ( 0.50 ) _ & _ ( 0.51 ) _ & _ ( 0.34 ) _ & _ ( 0.47 ) _ & _ ( 0.41 ) _ & _ ( 0.65 ) _",
    "+ -2.25 & 0.90 & 0.88 & 0.72 & 0.91 & 0.88 & 0.92 & 0.92 & 0.95 + & _ ( 0.43 ) _ & _ ( 0.34 ) _ & _ ( 0.46 ) _ & _ ( 0.46 ) _ & _ ( 0.34 ) _ & _ ( 0.42 ) _ & _ ( 0.41 ) _ & _ ( 0.59 ) _ + 3.00 & 0.89 & 0.86 & 0.81 & 0.90 & 0.87 & 0.90 & 0.92 & 0.95 + & _ ( 0.48 ) _ & _ ( 0.34 ) _ & _ ( 0.42 ) _ & _ ( 0.43 ) _ & _ ( 0.34 ) _ & _ ( 0.39 ) _ & _ ( 0.41 ) _ & _ ( 0.53 ) _",
    "+ -3.75 & 0.88 & 0.86 & 0.80 & 0.88 & 0.87 & 0.87 & 0.93 & 0.92 + & _ ( 0.52 ) _ & _ ( 0.34 ) _ & _ ( 0.40 ) _ & _ ( 0.41 ) _ & _ ( 0.34 ) _ & _ ( 0.37 ) _ & _ ( 0.41 ) _ & _ ( 0.50 ) _ + 4.50 & 0.91 & 0.88 & 0.85 & 0.89 & 0.89 & 0.89 & 0.94 & 0.95 + & _ ( 0.49 ) _ & _ ( 0.32 ) _ & _ ( 0.36 ) _ & _ ( 0.36 ) _ & _ ( 0.33 ) _ & _ ( 0.33 ) _ & _ ( 0.39 ) _ & _ ( 0.43 ) _ +   + -0.75 & 0.54 & 0.48 & 0.37 & 0.67 & 0.49 & 0.88 & 0.65 & 0.94 + 1.50 & 0.63 & 0.58 & 0.44 & 0.78 & 0.59 & 0.93 & 0.75 & 0.96 + -2.25 & 0.85 & 0.82 & 0.65 & 0.88 & 0.83 & 0.94 & 0.89 & 0.95 + 3.00 & 0.85 & 0.83 & 0.74 & 0.87 & 0.84 & 0.91 & 0.89 & 0.94 + -3.75 & 0.87 & 0.83 & 0.76 & 0.87 & 0.84 & 0.89 & 0.88 & 0.94 + 4.50 & 0.89 & 0.86 & 0.82 & 0.87 & 0.86 & 0.91 & 0.92 & 0.94 +    in the @xmath298 settings , the modified perturbation bootstrap interval based on @xmath123 continues to perform very well . under the @xmath299 setting ,",
    "for which table [ tab : p250n150cvlambda ] shows the results , the @xmath123 interval achieves the nominal coverage across all regression coefficients for both two- and one - sided intervals .",
    "the confidence intervals based on the asymptotic normality of the respective pivot all have sub - nominal coverage for most of the regression coefficients , and their coverages are dramatically affected by the magnitude of the true regression coefficient .",
    "the results are similar for the @xmath300 case , for which table [ tab : p500n150cvlambda ] shows the results .",
    "the only confidence interval which reliably achieves the nominal coverage is the modified perturbation bootstrap interval based on @xmath123 .",
    "we note that the width of the @xmath123 interval seems to adapt more to the magnitude of the regression coefficient than the widths of the normal - based confidence intervals , which remain fairly constant across all magnitudes of @xmath269 , resulting in poorer coverage for smaller regression coefficients .",
    "in contrast , the @xmath123 interval is able to achieve nominal coverage even for the smallest values of @xmath269 by producing suitably wider confidence intervals .    r|cccccccc + & & & + @xmath269 & @xmath252 & @xmath257 & @xmath258 & @xmath259 & @xmath248 & @xmath288 & @xmath122 & @xmath289 + -0.75 & 0.68 & 0.54 & 0.42 & 0.81 & 0.56 & 0.86 & 0.77 & 0.94 + & _ ( 0.53 ) _ & _ ( 0.31 ) _ & _ ( 0.48 ) _ & _ ( 0.52 ) _ & _ ( 0.32 ) _ & _ ( 0.40 ) _ & _ ( 0.41 ) _ & _ ( 0.60 ) _ + 1.50 & 0.75 & 0.67 & 0.50 & 0.86 & 0.68 & 0.85 & 0.81 & 0.93 + & _ ( 0.53 ) _ & _ ( 0.32 ) _ & _ ( 0.51 ) _ & _ ( 0.52 ) _ & _ ( 0.33 ) _ & _ ( 0.42 ) _ & _ ( 0.43 ) _ & _ ( 0.65 ) _",
    "+ -2.25 & 0.87 & 0.79 & 0.69 & 0.86 & 0.80 & 0.85 & 0.88 & 0.94 + & _ ( 0.50 ) _ & _ ( 0.32 ) _ & _ ( 0.46 ) _ & _ ( 0.47 ) _ & _ ( 0.33 ) _ & _ ( 0.38 ) _ & _ ( 0.43 ) _ & _ ( 0.59 ) _ + 3.00 & 0.89 & 0.82 & 0.76 & 0.86 & 0.83 & 0.86 & 0.92 & 0.94 + & _ ( 0.49 ) _ & _ ( 0.33 ) _ & _ ( 0.43 ) _ & _ ( 0.44 ) _ & _ ( 0.33 ) _ & _ ( 0.36 ) _ & _ ( 0.43 ) _ & _ ( 0.54 ) _",
    "+ -3.75 & 0.89 & 0.83 & 0.79 & 0.86 & 0.84 & 0.87 & 0.92 & 0.94 + & _ ( 0.48 ) _ & _ ( 0.33 ) _ & _ ( 0.41 ) _ & _ ( 0.42 ) _ & _ ( 0.33 ) _ & _ ( 0.35 ) _ & _ ( 0.43 ) _ & _ ( 0.52 ) _ + 4.50 & 0.92 & 0.86 & 0.85 & 0.88 & 0.87 & 0.86 & 0.91 & 0.91 + & _ ( 0.49 ) _ & _ ( 0.33 ) _ & _ ( 0.40 ) _ & _ ( 0.41 ) _ & _ ( 0.34 ) _ & _ ( 0.34 ) _ & _ ( 0.44 ) _ & _ ( 0.51 ) _",
    "+ -5.25 & 0.90 & 0.85 & 0.83 & 0.87 & 0.86 & 0.86 & 0.93 & 0.94 + & _ ( 0.50 ) _ & _ ( 0.32 ) _ & _ ( 0.39 ) _ & _ ( 0.39 ) _ & _ ( 0.33 ) _ & _ ( 0.34 ) _ & _ ( 0.43 ) _ & _ ( 0.50 ) _ + 6.00 & 0.94 & 0.89 & 0.88 & 0.90 & 0.90 & 0.88 & 0.93 & 0.93 + & _ ( 0.48 ) _ & _ ( 0.31 ) _ & _ ( 0.35 ) _ & _ ( 0.35 ) _ & _ ( 0.32 ) _ & _ ( 0.31 ) _ & _ ( 0.41 ) _ & _ ( 0.46 ) _",
    "+   + -0.75 & 0.58 & 0.48 & 0.36 & 0.73 & 0.49 & 0.84 & 0.69 & 0.94 + 1.50 & 0.69 & 0.59 & 0.44 & 0.79 & 0.60 & 0.86 & 0.75 & 0.93 + -2.25 & 0.81 & 0.73 & 0.61 & 0.80 & 0.74 & 0.85 & 0.83 & 0.93 + 3.00 & 0.86 & 0.79 & 0.70 & 0.83 & 0.80 & 0.85 & 0.86 & 0.94 + -3.75 & 0.85 & 0.79 & 0.70 & 0.82 & 0.80 & 0.86 & 0.88 & 0.93 + 4.50 & 0.90 & 0.83 & 0.82 & 0.85 & 0.84 & 0.87 & 0.89 & 0.94 + -5.25 & 0.87 & 0.84 & 0.80 & 0.85 & 0.84 & 0.87 & 0.90 & 0.94 + 6.00 & 0.90 & 0.85 & 0.83 & 0.86 & 0.85 & 0.87 & 0.91 & 0.94 +    we see that the modified perturbation bootstrap is able to produce reliable confidence intervals for regression coefficients in the high - dimensional setting , and that it is able to do so under data - based selection of the tuning parameters .",
    "further simulation studies may be found in the supplementary material .",
    "to illustrate the construction of confidence intervals for regression coefficients in the high - dimensional linear regression model using the modified perturbation bootstrap , we present an analysis of the ` riboflavin ` data set considered in bhlmann et al .",
    "( 2014 ) , which those authors make publicly available in their supplementary material .",
    "the data contains @xmath301 independent records consisting of a response variable which is the logarithm of the riboflavin production rate and of @xmath302 gene expression levels in batches of _ bacillis subtilis _ bacteria . of the @xmath302 , we pre - select @xmath303 genes by sorting them in order of decreasing empirical variance and keeping the first @xmath303 .",
    "we then fit the linear regression model to the data set with @xmath301 and @xmath304 and compute confidence intervals for the regression coefficients selected by the alasso procedure .",
    "we choose @xmath273 and @xmath19 using @xmath277-fold crossvalidation .",
    "figure [ fig : riboflavin1 ] displays the confidence intervals for the alasso - selected covariates obtained from the @xmath252 , @xmath122 , @xmath259 , and @xmath123 pivots , @xmath286 bootstrap replicates were used for the bootstrap - based intervals .",
    "the interval based on the @xmath252 pivot ( straight line ) and the @xmath259 interval ( jagged ) , are symmetric around the estimated value of the regression coefficient ( the @xmath259 interval is formed by adding and substracting an upper quantile of a normal distribution with a bootstrap - estimated variance ) .",
    "the intervals based on @xmath122 is asymmetric owing to the bias correction ( which is quite small in this example ) and , in the case of the @xmath123 interval , owing to the bias correction and to the asymmetry of the bootstrap distribution of @xmath123 .",
    "for some of the coefficients , the @xmath123 interval is highly asymmetric , suggesting that the distribution of the pivot @xmath122 may still be far from normal .",
    "( straight ) , @xmath122 ( wavy ) , @xmath259 ( jagged ) , and @xmath123 ( wiggly ) for each of the alasso selected genes from the ` riboflavin ` data set . ]",
    "we here introduce some additional notation required for stating our assumptions and useful for the proofs in the next subsection . for simplicity",
    ", we shall suppress the subscript @xmath13 in the notations @xmath305 and @xmath306 .",
    "define @xmath307 where @xmath308 , and where @xmath309 and @xmath310 are , respectively , the third and fourth central moments of @xmath311 .",
    "define in addition the @xmath312 matrix @xmath313 and the @xmath232 vector @xmath314 .",
    "define , @xmath315 and @xmath316 .",
    "write @xmath317 , @xmath318 , @xmath319 , @xmath320 and @xmath321 .",
    "define @xmath322 , where @xmath103 and @xmath92 are as defined earlier and @xmath323 is a @xmath98 vector with @xmath12th element @xmath324 .",
    "note that under the conditions ( a.2)(i ) , ( a.3 ) and ( a.6)(i ) , @xmath325 and @xmath326 , and hence @xmath327 . also define @xmath40 , @xmath328 , and @xmath329 , where @xmath330 and @xmath331 , @xmath332 ; @xmath333 and @xmath334 are as defined in section [ sec : mainresults ] and @xmath335 , where @xmath336 and @xmath337 .",
    "we denote by @xmath338 and @xmath339 , respectively , the @xmath340 and @xmath341 norm . for a non - negative integral vector @xmath342 and a function @xmath343 , @xmath344 , write @xmath345 , @xmath346 , @xmath347 , and @xmath348 , where @xmath349 denotes the partial derivative of @xmath350 with respect to the @xmath12th component of @xmath351 , @xmath352 . for @xmath353 and @xmath354 as above , define @xmath355 .",
    "let @xmath356 denote the multivariate normal distribution with mean @xmath357 and dispersion matrix @xmath358 having @xmath12th row @xmath359 and let @xmath360 denote the density of @xmath356 .",
    "we write @xmath361 and @xmath362 when @xmath358 is the identity matrix .. let @xmath363 be a positive constant and @xmath364 be a positive integer @xmath365 unless otherwise specified . by @xmath366 and @xmath367",
    "we denote , respectively , probability and expectation with respect to the distribution of @xmath368 conditional upon the observed data .      1 .",
    "let @xmath369 denote the smallest eigenvalue of the matrix @xmath92 .",
    "@xmath370 for some @xmath371 .",
    "2 .   @xmath372 .",
    "@xmath373 , where @xmath374 is the @xmath12th element of @xmath375 .",
    "( when @xmath15 ) 4 .",
    "@xmath376 , where @xmath377 is the @xmath378th element of @xmath379 .",
    "( when @xmath298 ) 2 .",
    "there exists a @xmath380 such that for all @xmath381 , 1 .",
    "2 .   @xmath383 3 .",
    "3 .   @xmath385 and min@xmath386 for some @xmath387 such that @xmath388 and @xmath389 , where @xmath390 is defined as in ( a.1)(i ) .",
    "4 .   1 .   @xmath391 .",
    "@xmath392 satisfies cramer s condition : + @xmath393 .",
    "5 .   1 .   @xmath394 .",
    "@xmath395 , @xmath396 .",
    "@xmath397 and @xmath398 are independent for all @xmath399 .",
    "@xmath400 satisfies cramer s condition : + @xmath401 6 .",
    "there exists @xmath402 such that for all @xmath403 , 1 .",
    "2 .   @xmath405 3 .",
    "@xmath406 . 7",
    ".   there exists @xmath407 and @xmath408 , @xmath409 being defined in the assumption ( a.6 ) , such that @xmath410    now we explain the assumptions briefly .",
    "assumption ( a.1 ) describes the regularity conditions needed on the growth of the design vectors .",
    "assumption ( a.1)(i ) is a restriction on the smallest eigenvalue of @xmath92 .",
    "assumption ( a.1)(i ) is a weaker condition than assuming that @xmath92 converges to a positive definite matrix .",
    "( a.1)(ii ) and ( iii ) are needed to bound the weighted sums of types @xmath411 $ ] , @xmath412 $ ] , @xmath413 $ ] ( second one only when @xmath15 ) . for @xmath414 ( a.1)(iii )",
    "is equivalent to the condition that the diagonal elements of the matrix @xmath415 are uniformly bounded . also for general value of @xmath364 , ( a.1)(ii ) and ( iii )",
    "are much weaker than conditioning on @xmath416-norms of the design vectors . here",
    "the value of @xmath364 is specified by the underlying edgeworth expansion .",
    "assumption ( a.1)(iii ) requires @xmath15 and hence is not defined when @xmath16 .",
    "note that the condition ( a.1)(iii)@xmath417 needs @xmath418 which is true in our setup due to assumption ( a.6)(iii ) .",
    "assumptions ( a.2)(i ) bounds the eigenvalues of the matrix @xmath419 away from infinity .",
    "it is necessary to obtain bounds needed in the studentized setup .",
    "assumption ( a.2)(ii ) is a condition similar to the conditions in ( a.2)(ii ) and ( iii ) ; but involving the @xmath37 matrix @xmath36 .",
    "this condition is needed for showing necessary closeness of the covariance matrix estimators @xmath420 to @xmath421 ( for details see lemma 8.5 ) .",
    "assumption ( a.3 ) separates the relevant covariates from the non - relevant ones .",
    "the condition on the minimum is needed to ensure that the non - zero regression coefficients can not converge to zero faster than the error rate , that is not faster than @xmath49 .",
    "we mention that one can assume @xmath422 instead of assuming @xmath423 , but with the price of putting another restriction on the penalty parameter @xmath19 .",
    "we do not consider such a setting in this paper .",
    "assumption ( a.4)(i ) is a moment condition on the error term needed for valid edgeworth expansion .",
    "assumption ( a.4)(ii ) is cramer s condition on the errors , which is very common in the literature of edgeworth expansions ; it is satisfied when the distribution of @xmath392 has a non - degenerate component which is absolutely continuous with respect to the lebesgue measure [ cf . hall ( 1992 ) ] .",
    "assumption ( a.4)(ii ) is only needed to get a valid edgeworth expansion for the original alasso estimator in the studentized setup .",
    "assumptions ( a.5)(i ) and ( iii ) are the analogous conditions that are needed on the perturbing random quantities to get a valid edgeworth expansion in the bootstrap setting .",
    "assumption ( a.5)(ii ) is natural , since the @xmath398 are present already in the data generating process , whereas @xmath424 are introduced by the user .",
    "one immediate choice of the distribution of @xmath425 is the beta@xmath426 distribution with @xmath427 .",
    "also one can investigate the generalized beta family of distributions for more choices of the distribution of @xmath425 .",
    "assumptions ( a.6)(i ) and ( ii ) can be compared with the condition ( c ) @xmath428 @xmath429 and @xmath430 [ cf .",
    "zou ( 2006 ) , caner and fan ( 2010 ) ] .",
    "whereas ( c ) is ensuring the oracle normal approximation , ( a.6)(i ) and ( ii ) are required for obtaining edgeworth expansions .",
    "lastly , ( a.6)(iii ) limits how quickly the number of non - zero regression coefficients may grow .",
    "though it would seem that @xmath431 with @xmath418 should be a sufficient restriction on the growth rate of @xmath104 for approximating the distribution of the alasso estimator , a careful analysis reveals that further reduction in the growth rate of @xmath104 is necessary for accommodating the studentization .",
    "assumption ( a.7 ) places deviation bounds on both the sample and bootstrap initial estimators which are needed to get valid edgeworth expansions .",
    "these conditions are satisfied by ols estimator in @xmath15 case [ cf .",
    "lemma [ lem : w ] ] .",
    "note that non - bootstrap part of ( a.7 ) is satisfied if there exists a linear approximation of the type @xmath432 of @xmath433 , where @xmath434 and @xmath435 for some @xmath436 .",
    "the bootstrap deviation bound corresponding to ( a.7 ) holds provided similar approximation exits with @xmath437 in place of @xmath311 .",
    "more precisely , for the ridge estimator and for its perturbation bootstrap version defined in sec @xmath438 , if for some @xmath439 , the conditions      are satisfied , then the assumption ( a.7 ) holds .",
    "here @xmath446 , @xmath447 is the standard basis of @xmath448 and @xmath65 is the penalty parameter corresponding to the ridge estimator [ cf .",
    "sec @xmath438 ] .",
    "this follows analogously to proposition 8.4 of chatterjee and lahiri ( 2013 ) after applying lemma @xmath450 , stated below .",
    "note that , @xmath457 , by lemma 7.4 and 7.5 , described below .",
    "suppose , @xmath458 , @xmath459 being the set of natural numbers . define the conditional lebesgue density of two - term edgeworth expansion of @xmath460 as @xmath461\\bigg],\\end{aligned}\\ ] ] where @xmath462 , @xmath463 , @xmath464 and @xmath465 , where @xmath466 is the standard normal density on @xmath118 .",
    "[ lem : concentration ] suppose @xmath467 are zero mean independent r.v.s and @xmath468 for @xmath469 and @xmath470 ; @xmath471 .",
    "then , for any @xmath472 and @xmath116 @xmath473\\leq c[\\sigma_t x^{-t } + exp(-x^2/\\sigma_2)]\\ ] ]"
  ],
  "abstract_text": [
    "<S> the adaptive lasso ( alasso ) was proposed by zou [ _ j . </S>",
    "<S> amer . </S>",
    "<S> statist . </S>",
    "<S> assoc . </S>",
    "<S> * 101 * ( 2006 ) 1418 - 1429 _ ] as a modification of the lasso for the purpose of simultaneous variable selection and estimation of the parameters in a linear regression model . </S>",
    "<S> zou ( 2006 ) established that the alasso estimator is variable - selection consistent as well as asymptotically normal in the indices corresponding to the nonzero regression coefficients in certain fixed - dimensional settings . in an influential paper , minnier , tian and cai [ _ j . </S>",
    "<S> amer . </S>",
    "<S> statist . assoc . </S>",
    "<S> * 106 * ( 2011 ) 1371 - 1382 _ ] proposed a perturbation bootstrap method and established its distributional consistency for the alasso estimator in the fixed - dimensional setting . in this paper , however , we show that this ( naive ) perturbation bootstrap fails to achieve second order correctness in approximating the distribution of the alasso estimator . </S>",
    "<S> we propose a modification to the perturbation bootstrap objective function and show that a suitably studentized version of our modified perturbation bootstrap alasso estimator achieves second - order correctness even when the dimension of the model is allowed to grow to infinity with the sample size . as a consequence , inferences based on the modified perturbation bootstrap will be more accurate than the inferences based on the oracle normal approximation . </S>",
    "<S> we give simulation studies demonstrating good finite - sample properties of our modified perturbation bootstrap method as well as an illustration of our method on a real data set . </S>"
  ]
}