{
  "article_text": [
    "in its broadest sense , data mining is simply the act of turning raw data from an observation into useful information .",
    "this information can be interpreted by hypothesis or theory , and used to make further predictions .",
    "this scientific method , where useful statements are made about the world , has been widely employed to great effect in the west since the renaissance , and even earlier in other parts of the world . what has changed in the past few decades",
    "is the exponential rise in available computing power , and , as a related consequence , the enormous quantities of observed data , primarily in digital form .",
    "the exponential rise in the amount of available data is now creating , in addition to the natural world , a digital world , in which extracting new and useful information from the data already taken and archived is becoming a major endeavor in itself .",
    "this action of _ knowledge discovery in databases _ ( kdd ) , is what is most commonly inferred by the phrase data mining , and it forms the basis for our review .",
    "astronomy has been among the first scientific disciplines to experience this flood of data .",
    "the emergence of data mining within this and other subjects has been described@xcite as the _",
    "fourth paradigm_. the first two paradigms are the well - known pair of theory and observation , while the third is another relatively recent addition , computer simulation .",
    "the sheer volume of data not only necessitates this new paradigmatic approach , but the approach must be , to a large extent , automated . in more formal terms , we wish to leverage a computational machine to find patterns in digital data , and translate these patterns into useful information , hence _",
    "machine learning_. this learning must be returned in a useful manner to a human investigator , which hopefully results in human learning .",
    "it is perhaps not entirely unfair to say , however , that scientists in general do not yet appreciate the full potential of this fourth paradigm .",
    "there are good reasons for this of course : scientists are generally not experts in databases , or cutting - edge branches of statistics , or computer hardware , and so forth . what we hope to do in this review , primarily for the data mining skeptic , is to shed light on why this is a useful approach . to accomplish this goal , we emphasize either algorithms that have or",
    "could currently be usefully employed , and the actual scientific results they have enabled .",
    "we also hope to give an interesting and fairly comprehensive overview to those who do already appreciate this approach , and perhaps provide inspiration for exciting new ideas and applications .",
    "however , despite referring to data mining as a whole new paradigm , we try to emphasize that it is , like theory , observation , and simulation , only a part of the broader scientific process , and should be viewed and utilized as such .",
    "the algorithms described are _ tools _ that , when applied correctly , have vast potential for the creation of useful scientific results .",
    "but , given that it is only part of the process , it is , of course , not the answer to everything , and we therefore enumerate some of the limitations of this new paradigm .",
    "we start in  [ subsec : why ] with a summary of some of the advantages of this approach . in  [ sec :",
    "overview ] , we summarize the process from the input of raw data to the visualization of results .",
    "this is followed in  [ sec : uses ] by the actual application of data mining tools in astronomy .",
    " [ sec : overview ] is arranged algorithmically , and ",
    "[ sec : uses ] is arranged astrophysically .",
    "it is likely that the expert in astronomy or data mining , respectively , could infer much of  [ sec : uses ] from  [ sec : overview ] , and vice - versa .",
    "but it is unlikely ( we hope ) that the combination of the two sections does not have new ideas or insights to offer to either audience .",
    "following these two sections , in  [ sec : future ] , we combine the lessons learned to discuss the future of data mining in astronomy , pointing out likely near - term future directions in both the data mining process and its physical application . we conclude with a summary of the main points in  [ sec : conclusions ] .",
    "of course , what astronomers care about is not a fashionable new computational method for ever more complex data analysis , but the _ science_. a fancy new data mining system is not worth much if all it tells you is what you could have gained by the judicious application of existing tools and a little physical insight@xcite .",
    "we therefore summarize some of the advantages of this approach :    _ getting anything at all _ : upcoming datasets will be almost overwhelmingly large . when one is faced with petabytes of data , a rigorous , automated approach that intelligently extracts pertinent scientific information will be the only one that is tractable .",
    "_ simplicity _ : despite the apparent plethora of methods , straightforward applications of very well - known and well - tested data mining algorithms can quickly produce a useful result . these methods can generate a model appropriate to the complexity of an input dataset , including nonlinearities , implicit prior information , systematic biases , or unexpected patterns . with this approach , _ a priori _ data sampling of the type exemplified by elaborate color cuts , is not necessary . for many algorithms ,",
    "new data can be trivially incorporated as they become available .    _",
    "prior information _ : this can be either fully incorporated , or the data can be allowed to completely ` speak for themselves ' .",
    "for example , an unsupervised clustering algorithm can highlight new classes of objects within a dataset that might be missed if a prior set of classifications were imposed .    _",
    "pattern recognition _ : an appropriate algorithm can highlight patterns in a dataset that might not otherwise be noticed by a human investigator , perhaps due to the high dimensionality . similarly , rare or unusual objects can be highlighted .    _",
    "complimentary approach _ : although there are numerous examples where the data mining approach demonstrably exceeds more traditional methods in terms of scientific return .",
    "even when the approach does not produce a substantial improvement , it still acts as an important complementary method of analyzing data , because different approaches to an overall problem help to mitigate systematic errors in any one approach .",
    "in this section , we review the data mining process . specifically , as described in  [ sec : intro ] , this data mining review focuses on knowledge discovery in databases ( kdd ) , although our definition of a ` database ' is somewhat broad , essentially being any machine - readable astronomical data . as a result , this section is arranged algorithmically . to avoid overlap with  [ sec : uses ] on the astronomical uses , we defer most of the application examples to that section .",
    "nevertheless , all algorithms we describe have been , or are of sufficient maturity that they could immediately be applied to astronomical data .",
    "the reader who is expert in astronomy but not in data mining is advised to read this section to gain the full benefit from  [ sec : uses ] . as in any specialized subject ,",
    "a certain level of jargon is necessary for clarity of expression .",
    "terms likely to be unfamiliar to astronomers not versed in data mining are generally explained as they are introduced , but for additional background we note that there are other useful reviews of the data mining field@xcite .",
    "another recent overview of data mining in astronomy by borne has also been published@xcite .",
    "the process of data collection encompasses all of the steps required to obtain the desired data in a digital format .",
    "methods of data collection include acquiring and archiving new observations , querying existing databases according to the science problem at hand , and performing as necessary any cross - matching or data combining , a process generically described as _",
    "data fusion_.    a common motivation for cross - matching is the use of multiwavelength data , i.e. , data spanning more than one of the regions of the electromagnetic spectrum ( gamma ray , x - ray , ultraviolet , optical , infrared , millimeter , and radio ) . a common method in the absence of a definitive identification for each object spanning the datasets is to use the object s position on the sky with some astrometric tolerance , typically a few arcseconds .",
    "cross - matching can introduce many issues including ambiguous matches , variations of the point spread function ( resolution of objects ) within or between datasets , differing survey footprints , survey masks , and large amounts of processing time and data transfer requirements when cross - matching large datasets .",
    "a major objective of the _ virtual observatory _ ( vo ,  [ subsec : vo ] ) is to make the data collection process more simple and tractable .",
    "future vo webservices are planned that will perform several functions in this area , including cross - matches on large , widely distributed , heterogeneous data .",
    "common astronomical data formats include fits@xcite , a binary format , and plain ascii , while an emerging format is votable@xcite .",
    "commonly used formats from other areas of data mining , such as attribute relation file format ( arff ) , are generally not widely used in astronomy .",
    "some data preprocessing may necessarily be part of the data collection process , for example , sample cuts in database queries .",
    "preprocessing can be divided into steps that make the data to be read meaningful , and those that transform the data in some way as appropriate to a given algorithm .",
    "data preprocessing is often problem - dependent , and should be carefully applied because the results of many data mining algorithms can be significantly affected by the input data . a useful overview of data preprocessing is given by pyle@xcite .",
    "algorithms may require the object _ attributes _",
    ", i.e. , the values in the data fields describing the properties of each object , to be numerical or categorical , the latter being , e.g. ` star ' , or ` galaxy ' .",
    "it is possible to transform numerical data to categorical and vice versa .",
    "a common categorical - to - numerical method is scalarization , in which different possible categorical attributes are given different numerical labels , for example , ` star ' , ` galaxy ' , ` quasar ' labeled as the vectors [ 1,0,0 ] , [ 0,1,0 ] , and [ 0,0,1 ] , respectively .",
    "note that for some algorithms , one should _ not _ label categories as , say , 1 , 2 and 3 , if the output of the algorithm is such that if it has confused an object between 1 and 3 it labels the object as intermediate , in this case , 2 . here , 2 ( galaxy ) is certainly not an intermediate case between 1 ( star ) and 3 ( quasar ) .",
    "one common algorithm in which such categorical but not ordered outputs could occur is a decision tree with multiple outputs .",
    "numerical data can be made categorical by transformations such as binning .",
    "the bins may be user - specified , or can be generated optimally from the data@xcite .",
    "binning can create numerical issues , including comparing two floating point numbers that should be identical , objects on a bin edge , empty bins , values that can not be binned such as _ nan _ , or values not within the bin range .",
    "object attributes may need to be _",
    "transformed_. a common operation is the differencing of magnitudes to create colors .",
    "these transformations can introduce their own numerical issues , such as division by zero , or loss of accuracy .    in general , data will contain one or more types of _ bad values _",
    ", where the value is not correct .",
    "examples include instances where the value has been set to something such as -9999 or nan , the value appears correct but has been flagged as bad , or the value is not bad in a formatting sense but is clearly unphysical , perhaps a magnitude of a high value that could not have been detected by the instrument .",
    "they may need to be removed either by simply removing the object containing them , ignoring the bad value but using the remaining data , or interpolating a value using other information .",
    "outliers may or may not be excluded , or may be excluded depending on their extremity",
    ".    data may also contain _",
    "missing values_. these values may be genuinely missing , for example in a cross - matched dataset where an object is not detected in a given waveband , or is not in an overlapping region of sky .",
    "it is also possible that the data should be present , but are missing for either a known reason , such as a bad camera pixel , a cosmic ray hit , or a reason that is simply not known .",
    "some algorithms can not be given missing values , which will require either the removal of the object or interpolation of the value from the existing data .",
    "the advisability of interpolation is problem - dependent .    as well as",
    "bad values , the data may contain values that are correct but are outside the desired range of analysis .",
    "the data may therefore need to be _",
    "sampled_. there may simply be a desired range , such as magnitude or position on the sky , or the data may contain values that are correct but are outliers .",
    "outliers may be included , included depending on their extremity ( e.g. , @xmath0 standard deviations ) , downweighted , or excluded .",
    "alternatively , it may be more appropriate to generate a random subsample to produce a smaller dataset .",
    "outside any normalization of the data prior to its use in the data mining algorithm , for example , calibration using standard sources , input or target attributes of the data will often be further normalized to improve the _ numerical conditioning _ of the algorithm . for example , if one axis of the @xmath0-dimensional space created by @xmath0 input attributes encompasses a range that , numerically , is much larger than the other axes , it may dominate the results , or create conditions where very large and small numbers interact , causing loss of accuracy .",
    "normalization can reduce this , and examples include linear transformations , like scaling by a given amount , scaling using the minimum and maximum values so that each attribute is in a given range such as 01 , or scaling each attribute to have a mean of 0 and a standard deviation of 1 .",
    "the latter example is known as _",
    "standardization_. a more sophisticated transformation with similar advantages is _ whitening _ , in which the values are not only scaled to a similar range , but correlations among the attributes are removed via transformation of their covariance matrix to the identity matrix .      in general",
    ", a large number of attributes will be available for each object in a dataset , and not all will be required for the problem . indeed , use of all attributes may in many cases worsen performance .",
    "this is a well - known problem , often called the _ curse of dimensionality_. the large number of attributes results in a high - dimensional space with many low density environments or even empty voids .",
    "this makes it difficult to generalize from the data and produce useful new results .",
    "one therefore requires some form of _ dimension reduction _ , in which one wishes to retain as much of the information as possible , but in fewer attributes . as well as the curse of dimensionality , some algorithms work less well with noisy , irrelevant , or redundant attributes .",
    "an example of an irrelevant attribute might be position on the sky for a survey with a uniform mask , because the position would then contain no information , and highly redundant attributes might be a color in the same waveband measured in two apertures .    the most trivial form of dimension reduction is simply to use one s judgement and select a subset of attributes . depending on the problem this can work well .",
    "nevertheless , one can usually take a more sophisticated and less subjective approach , such as principal component analysis ( pca)@xcite . this is straightforward to implement , but is limited to linear relations .",
    "it gives , as the principal components , the eigenvectors of the input data , i.e. , it picks out the directions which contain the greatest amount of information .",
    "another straightforward approach is _ forward selection _ , in which one starts with one attribute and selectively adds new attributes to gain the most information . or , one can perform the equivalent process but starting with all of the attributes and removing them , known as _",
    "backward elimination_.    in many ways , dimension reduction is similar to classification , in the sense that a larger number of input attributes is reduced to a smaller number of outputs .",
    "many classification schemes in fact directly use pca .",
    "other dimension reduction methods utilize the same or similar algorithms to those used for the actual data mining : an ann can perform pca when set up as an autoencoder , and kernel methods can act as generalizations of pca .",
    "a binary genetic algorithm (  [ subsubsec : other ] ) can be used in which each individual represents a subset of the training attributes to be used , and the algorithm selects the best subset .",
    "the epsilon - approximate nearest neighbor search@xcite reduces the dimensionality of nearest neighbor methods .",
    "other methods include information bottleneck@xcite , which directly uses information theory to optimize the tradeoff between the number of classes and the information contained , fisher matrix@xcite , independent component analysis@xcite , and wavelet transforms .",
    "the curse of dimensionality is likely to worsen in the future for a similar reason to that of missing values , as more multiwavelength datasets become available to be cross - matched .",
    "classification and dimension reduction are not identical of course : a classification algorithm may build a model to represent the data , which is then applied to further examples to predict their classes .",
    "machine learning algorithms broadly divide into _ supervised _ and _ unsupervised _ methods , also known as predictive and descriptive , respectively .",
    "these can be generalized to form _ semi - supervised _ methods .",
    "supervised methods rely on a _ training set _ of objects for which the target property , for example a classification , is known with confidence .",
    "the method is trained on this set of objects , and the resulting mapping is applied to further objects for which the target property is not available .",
    "these additional objects constitute the _ testing set_. typically in astronomy , the target property is spectroscopic , and the input attributes are photometric , thus one can predict properties that would normally require a spectrum for the generally much larger sample of photometric objects .",
    "the training set must be representative , i.e. , the parameter space covered by the input attributes must span that for which the algorithm is to be used .",
    "this might initially seem rather restrictive , but in many cases can be handled by combining datasets .",
    "for example , the zcosmos redshift survey@xcite , at one square degree , provides spectra to the depth of the photometric portion of the sloan digital sky survey ( sdss)@xcite , @xmath1 mag , which covers over 8000 square degrees .",
    "since sdss photometry is available for zcosmos objects , one can in principle use the 40,000 zcosmos galaxies as a training set to assign photometric redshifts to over 200 million sdss galaxies .",
    "in contrast to supervised methods , unsupervised methods do not require a training set .",
    "this is an advantage in the sense that the data can speak for themselves without preconceptions such as expected classes being imposed . on the other hand ,",
    "if there is prior information , it is not necessarily incorporated .",
    "unsupervised algorithms usually require some kind of initial input to one or more of the adjustable parameters , and the solution obtained can depend on this input .",
    "semi - supervised methods attempt to allow the best - of - both - worlds , and both incorporate known priors while allowing objective data interpretation and extrapolation . but given their generality , they can be more complex and difficult to implement .",
    "they are of potentially great interest astronomically because they could be used to analyze a full photometric survey beyond the spectroscopic limit , without requiring priors , while at the same time incorporating the prior spectroscopic information where it is available .",
    "the most widely used and well - known machine learning algorithm in astronomy to - date , referred to as far back as the mid 1980s,@xcite is the _ artificial neural network _",
    "( ann , fig .",
    "[ fig : ann])@xcite .",
    "this consists of a series of interconnected nodes with weighted connections .",
    "each node has an activation function , perhaps a simple threshold , or a sigmoid . although the original motivation was that the nodes would simulate neurons in the brain,@xcite the anns in data mining are of such a size that they are best described as nonlinear extensions of conventional statistical methods .",
    "the supervised ann takes parameters as input and maps them on to one or more outputs . a set of parameter vectors ,",
    "each vector representing an object and corresponding to a desired output , or target , is presented .",
    "once the network is trained , it can be used to assign an output to an unseen parameter vector .",
    "the training uses an algorithm to minimize a cost function .",
    "the cost function , @xmath2 , is commonly of the form of the mean - squared deviation between the actual and desired output : @xmath3 where @xmath4 and @xmath5 are the output and target respectively for the @xmath6th of @xmath7 objects .",
    "in general , the neurons could be connected in any topology , but a commonly used form is to have an @xmath8 arrangement , where @xmath9 is the number of input parameters , @xmath10 are the number of neurons in each of @xmath0 one dimensional ` hidden ' layers , and @xmath2 is the number of neurons in the final layer , which is equal to the number of outputs .",
    "each neuron is connected to every neuron in adjacent layers , but not to any others .",
    "multiple outputs can each give the bayesian _ a posteriori _ probability that the output is of that specific class , given the values of the input parameters .",
    "the weights are adjusted by the training algorithm . in astronomy",
    "this has typically been either the well - known backpropagation algorithm@xcite or the quasi - newton algorithm@xcite , although other algorithms , such as levenberg - marquardt@xcite have also been used .    another common method used in data mining",
    "is the _ decision tree _ ( dt , fig .",
    "[ fig : dt])@xcite . decision trees consist first of a root node which contains all of the parameters describing the objects in the training set population along with their classifications .",
    "a node is split into child nodes using the criterion that minimizes the classification error",
    ". this splitting subdivides the parent population group into children population groups , which are assigned to the respective child nodes .",
    "the classification error quantifies the accuracy of the classification on the test set .",
    "the process is repeated iteratively , resulting in layered nodes that form a tree .",
    "the iteration stops when specific user - determined criteria are reached .",
    "possibilities include a minimum allowed population of objects in a node ( the minimum decomposition population ) , the maximum number of nodes between the termination node and the root node ( the maximum tree depth ) , or a required minimum decrease resulting from a population split ( the minimum error reduction ) .",
    "the terminal nodes are known as the leaf nodes .",
    "the split is tested for each input attribute , and can be axis - parallel , or oblique , which allows for hyperplanes at arbitrary angles in the parameter space .",
    "the split statistic can be the midpoint , mean , or median of the attribute values , while the cost function used is typically the variance , as with ann .    in recent years ,",
    "another algorithm , the _ support vector machine _ ( svm , fig . [ fig : svm])@xcite , has gained popularity in astronomical data mining .",
    "svm aims to find the hyperplane that best separates two classes of data .",
    "the input data are viewed as sets of vectors , and the data points closest to the classification boundary are the support vectors .",
    "the algorithm does not create a model of the data , but instead creates the decision boundaries , which are defined in terms of the support vectors .",
    "the input attributes are mapped into a higher dimensional space using a kernel so that nonlinear relationships within the data become linear ( the ` kernel trick')@xcite , and the decision boundaries , which are linear , are determined in this space . like ann and dt",
    ", the training algorithm minimizes a cost function , which in this case is the number of incorrect classifications .",
    "the algorithm has two adjustable hyperparameters : the width of the kernel , and the regularization , or cost , of classification error , which helps to prevent _ overfitting _ (  [ subsec : improving ] ) of the training set .",
    "the shape of the kernel is also an adjustable parameter , a common choice being the gaussian radial basis function . as a result , an svm has fewer adjustable parameters than an ann or dt , but because these parameters must be optimized , the training process can still be computationally expensive .",
    "svm is designed to classify objects into two classes .",
    "various refinements exist to support additional classes , and to perform regression , i.e. , to supply a continuous output value instead of a classification .",
    "classification probabilities can be output , for example , by using the distance of a data point from the decision boundary .",
    "another powerful but computationally intensive method is _ k nearest neighbor _ ( @xmath6nn)@xcite .",
    "this method is powerful because it can utilize the full information available for each object , with no approximations or interpolations .",
    "the training of @xmath6nn  is in fact trivial : the positions of each of the objects in the input attribute space are simply stored in memory . for each test object ,",
    "the same attributes are compared to the training set and the output is determined using the properties of the nearest neighbors .",
    "the simplest implementation is to output the properties of the single nearest neighbor , but more commonly the weighted sum of @xmath6 nearest neighbors is used .",
    "the weighting is typically the inverse euclidean distance in the attribute space , but one can also use adaptive distance metrics .",
    "the main drawback of this method is that is it computationally intensive , because for each testing object the entire training set must be examined to determine the nearest neighbors .",
    "this requires a large number of distance calculations , since the test datasets are often much larger than the training datasets .",
    "the workload can be mitigated by storing the training set in an optimized data structure , such as a kd - tree .",
    "however , in the past few years , novel supercomputing hardware ( which is discussed in more detail in  [ subsec : hardware ] ) has become available that is specifically designed to carry out exactly this kind of computationally intensive work , including applications involving a large number of distance calculations .",
    "the curve of growth of this technology exceeds that of conventional cpus , and thus the direct implementation of @xmath6nn  using this technology has the potential to exceed the performance of conventional cpus .      _",
    "kernel density estimation _",
    "( kde)@xcite is a method of estimating the probability density function of a variable .",
    "it is a generalization of a histogram where the kernel function is any shape instead of the top - hat function of a histogram bin .",
    "this has the advantages that it avoids the discrete nature of the histogram and does not depend on the position of the bin edges , but the width of the kernel must still be chosen so as not to over- or under - smooth the data . a gaussian kernel is commonly utilized . in higher numbers of dimensions , common in astronomical datasets ,",
    "the width of the kernel must be specified in each dimension .",
    "_ k - means clustering_@xcite is an unsupervised method that divides data into clusters",
    ". the number of clusters must be initially specified , but since the algorithm converges rapidly , many starting points can be tested .",
    "the algorithm uses a distance criterion for cluster membership , such as the euclidean distance , and a stopping criterion for iteration , for example , when the cluster membership ceases to change .    _",
    "mixture models_@xcite decompose a distribution into a sum of components , each of which is a probability density function",
    ". often , the distributions are gaussians , resulting in gaussian mixture models .",
    "they are often used for clustering , but also for density estimation , and they can be optimized using either expectation maximization or monte carlo methods .",
    "many astronomical datasets consist of contributions from different populations of objects , which allows mixture modeling to disentangle these population groups .",
    "mixture models based on the em algorithm have been used in astronomy for this purpose@xcite .",
    "_ expectation maximization _",
    "( em)@xcite treats the data as a sum of probability distributions , which each represent one cluster .",
    "this method alternates between an expectation stage and a maximization stage . in the expectation stage",
    ", the algorithm evaluates the membership probability of each data point given the current distribution parameters . in the maximization stage , these probabilities are used to update the parameters .",
    "this method works well with missing data , and can be used as the unsupervised component in semi - supervised learning (  [ subsubsec : semisupervised ] ) to provide class labels for the supervised learning .    the _ kohonen self - organizing map _ ( som)@xcite is an unsupervised neural network that forms a general framework for visualizing datasets of more than two dimensions . unlike many methods which seek to map objects onto a new output space ,",
    "the som is fundamentally topological .",
    "this is neatly illustrated by the fact that one astronomical som application@xcite is titled ` galaxy morphology without classification ' .",
    "a related earlier method is learning vector quantization@xcite .",
    "_ independent component analysis _",
    "( ica)@xcite , an example of _ blind source separation _ , can separate nonlinear components of a dataset , under the assumption that those components are statistically independent .",
    "the components are found by maximizing this independence .",
    "related statistical methods include principal component analysis (  [ subsec : attributes ] ) , singular value decomposition , and non - negative matrix factorization .",
    "the semi - supervised approach@xcite has been somewhat underused to - date , but holds great potential for the upcoming , large , purely photometric surveys .",
    "supervised methods require a labeled training set , but will not assign new classes . on the other hand",
    ", unsupervised methods do not require training , but do not use existing known information .",
    "semi - supervised methods aim to capture the best from both of these methods by retaining the ability to discover new classes within the data , and also incorporating information from a training set when available .",
    "an example of a dataset amenable to the approach is shown in fig .",
    "[ fig : semi - supervised ] .",
    "this is particularly relevant in astronomical applications using large amounts of photometric and a more limited subsample of spectroscopic data , which may not be fully representative of the photometric sample .",
    "the semi - supervised approach allows one to use the spectral information to extrapolate into the purely photometric regime , thereby allowing a scientist to utilize all of the vast amount of information present there .",
    "semi - supervised learning represents an entire subfield of data mining research .",
    "given the nontrivial implementation requirements , this field is a good area for potential fruitful collaborations between astronomers , computer scientists , and statisticians .",
    "as one example of a possible issue , a lot of photometric data are likely to be a direct continuation in parameter space of spectroscopic data , with , therefore , a highly overlapping distribution .",
    "this means that certain semi - supervised approaches will work better than others , because they contain various assumptions about the nature of the labeled and unlabeled data .      in   [ subsubsec : supervised][subsubsec : unsupervised ] above , we described the main data mining algorithms used to date in astronomy , however , there are numerous additional algorithms available , which have often been utilized to some extent . these algorithms may be employed at more than one stage in the process , such as attribute selection , as well as the classification / regression stage .    while neural networks in some very broad sense mimic the learning mechanism of the brain , _",
    "genetic algorithms_@xcite mimic natural selection , as the most successful individuals created are those that are best adapted for the task at hand .",
    "the simplest implementation is the binary genetic algorithm , in which each ` individual ' is a vector of ones and zeros , which represent whether or not a particular attribute , e.g. , a training set attribute , is used . from an initial random population",
    ", the individuals are combined to create new individuals .",
    "the fitness of each individual is the resulting error in the training algorithm run according to the formula encoded by the individual .",
    "this process is repeated until convergence if found , producing the best individual .",
    "a typical method of combining two individuals is one - point crossover , in which segments of two individuals are swapped . to more fully explore the parameter space , and to prevent the algorithm from converging too rapidly on a local minimum",
    ", a probability of mutation is introduced into the newly created individuals before they are processed .",
    "this is simply the probability that a zero becomes a one , or vice - versa .",
    "an approximate number of individuals to use is given by @xmath11 where @xmath12 is the number of attributes .",
    "the algorithm converges in @xmath13 iterations , where @xmath14 is a problem - dependent constant ; generally @xmath15 .",
    "numerous refinements to this basic approach exist , including using continuous values instead of binary ones , and more complex methods for combining individuals . further possibilities for the design of genetic algorithms exist@xcite , and it is possible in principle to combine the optimization of the learning algorithm and the attribute set .    the _ information bottleneck _",
    "method@xcite is based directly on information theory and is designed to achieve the best tradeoff between accuracy and compression for the desired number of classes .",
    "the inputs and outputs are probability density functions .",
    "_ association rule _",
    "mining@xcite is a method of finding qualitative rules within a database such that a rule derived from the occurrence of certain variables together implies something about the occurrence of a variable not used in creating that rule .",
    "the _ false discovery rate_@xcite is a method of establishing a significant discovery from a smaller set of data than the usual @xmath0 sigma hypothesis test .",
    "this list could continue , broadening into traditional statistical methods such as least squares , and regression , as well as bayesian methods , which are widely used in astronomy . for brevity",
    "we do not consider these additional methods , but we do note that _ graphical models_@xcite are a general way of describing the interrelationships between variables and probabilities , and many of the data mining algorithms described earlier , such as anns , are special cases of these models .",
    "unfortunately , there is no simple method to select the optimal algorithm to use , because the most appropriate algorithm can depend not only on the dataset , but also the application for which it will be employed .",
    "there is , therefore , no single best algorithm .",
    "likewise , the choice of software is similarly non - trivial .",
    "many general frameworks exist , for example weka@xcite or data to knowledge@xcite , but it is unlikely that one framework will be able to perform all steps necessary from raw catalog to desired science result , particularly for large datasets . in table",
    "[ table : algorithms ] , we summarize some of the advantages and disadvantages of some of the more popular and well - known algorithms used in astronomy . we do not attempt to summarize available software .",
    "various other general comparisons of machine learning algorithms exist@xcite , as well as numerous studies comparing various algorithms for particular datasets , a field which itself is rather complex@xcite .",
    "many of the algorithms previously described involve ` greedy ' optimization . in these cases , the cost function , which is the measure of",
    "how well the algorithm is performing in its classification or prediction task , is minimized in a way that does not allow the value of the function to increase much if at all . as a result , it is possible for the optimization to become trapped in a local minimum , whereby nearby configurations are worse , but better configurations exist in a different region of parameter space .",
    "various approaches exist to overcome local minima .",
    "one approach is to simply run the algorithm several times from different starting points .",
    "another approach is _ simulated annealing_@xcite , where , in following the metallurgical metaphor , the point in parameter space ` heats up ' , thus perturbing it and allowing it to escape from the local minimum .",
    "the point is allowed to ` cool ' , thus having the ability to find a solution closer to the global minimum .",
    "models produced by data mining algorithms are subject to a fundamental limitation common to many systems in which a predictive model is constructed , the _ bias - variance tradeoff_. the bias is the accuracy of the model in describing the data , for example , a linear model might have a higher bias than a higher order polynomial .",
    "the variance is the accuracy of this model in describing new data .",
    "the higher order polynomial might have a lower bias than a linear model , but it might be more strongly affected by variations in the data and thus have a higher variance .",
    "the polynomial has _",
    "overfit _ the data .",
    "there is usually an optimal point between minimizing bias and minimizing variance .",
    "a typical way to minimize overfitting is to measure the performance of the algorithm on a test set , which is not part of the training set , and adjusting the stopping criterion for training to stop at an appropriate location .    to help prevent overfitting ,",
    "training can also be _",
    "regularized _ , in which an extra term is introduced into the cost function to penalize configurations that add complexity , such as large weights in an ann .",
    "this complexity can cause a function to be less smooth , which increases the likelihood of overfitting .",
    "as is the case with supervised learning , unsupervised algorithms can also overfit the data , for example , if some kind of smoothing is employed but its scalelength is too small . in this case , the algorithm will ` fit the noise ' and not the true underlying distribution .",
    "another common approach to control overfitting and improve confidence in the accuracy of the results is _ cross - validation _ , where subsets of the data are left out of the training and used for testing .",
    "the simplest form is the holdout method , where a single subset of the training data is kept out of the training , and the algorithm error is evaluated by running on this subset .",
    "however , this can have a high bias ( see bias - variance tradeoff , above ) if the training set is small , due to a significant portion of the training information being left out . @xmath16-fold",
    "cross - validation improves on this by subdividing the data into @xmath16 samples and training on @xmath17 samples , or alternatively using @xmath16 random subsets .",
    "typically , @xmath18 or @xmath19 , as small @xmath16 could still have high bias , as in the holdout method , but large @xmath16 , while being less biased , can have high variance due to the testing set being small .",
    "if @xmath16 is increased to the size of the dataset , so that each subsample is a single point , the method becomes leave - one - out cross - validation . in all instances ,",
    "the estimated error is the mean error from those produced by each run in the cross - validation",
    ".    another important refinement to running one algorithm is the ability to use a _ committee _ of instances of the algorithm , each with different parameters .",
    "one can allow these different instantiations to vote on the final prediction , so that the majority or averaged result becomes the final answer .",
    "such an arrangement can often provide a substantial improvement , because it is more likely that the majority will be closer to the correct answer , and that the answer will be less affected by outliers .",
    "one such committee arrangement is _ bootstrap aggregating _ , or _",
    "bagging_@xcite , where random subsamples with replacement ( bootstrap samples ) are taken , and the algorithm trained on each .",
    "the created algorithms vote on the testing set .",
    "bagging is often applied to decision trees with considerable success , but it can be applied to other algorithms . the combination of bagging and the random selection of a small subset of features for splitting at each node is known as a random forest@xcite .    _",
    "boosting_@xcite uses the fact that several ` weak ' instances of an algorithm can be combined to produce a stronger instance .",
    "the weak learners are iteratively added and misclassified objects in the data gain higher weight .",
    "thus boosting is not the same as bagging because the data themselves are weighted .",
    "boosted decision trees are a popular approach , and many different boosting algorithms are available .    as well as committees of the same algorithm , it is also possible to combine the results of more than one different algorithm on the same dataset .",
    "such a _ mixture of experts _ approach often provides an optimal result on real data . the outcome may be decided by voting , or the output of one algorithm can form the input to another , in a chaining approach .    for many astronomical applications , the results are , or would be , significantly improved by utilizing the full probability density function ( pdf ) for a predicted property , rather than simply its single scalar value .",
    "this is because much more information is retained when using the pdf .",
    "potential uses of pdfs are described further in  [ subsec : pdf ] .",
    "the purpose of this review is not to uncritically champion certain data mining algorithms , but to instead encourage scientific progress by exploiting the full potential of these algorithms in a considered scientific approach .",
    "we therefore end this section by outlining some of the limitations of and issues raised by kdd and the data mining approach to current and future astronomical datasets .",
    "several of these problems might be ameliorated by increased collaboration between astronomers and data mining experts .",
    "_ extrapolation _ : in many astronomical applications , it is common for data with less information content to be available for a greater number of objects over a larger parameter space .",
    "the classic example is in surveys where photometric objects are typically observed several magnitudes fainter than spectroscopic objects . for a supervised learning algorithm ,",
    "it is usually inappropriate to extrapolate beyond the parameter space for which the training set ( e.g. , the spectroscopic objects ) is representative .",
    "_ non - intuitive results _ : it is very easy to run an implementation of a well - known algorithm and output a result that appears reasonable , but is in fact either statistically invalid or completely wrong .",
    "for example , randomly subsampled training and testing sets from a dataset will overlap and produce a model that overfits the data .",
    "_ measurement error _ : most astronomical data measurements have an associated error , but most data mining algorithms do not take this explicitly into account . for many algorithms , the intrinsic spread in the data corresponding to the target property is the measurement of the error .    _ adjustable parameters _ : several algorithms have a significant number of adjustable parameters , and the optimal configuration of these parameters is not obvious .",
    "this can result in large parameter sweeps that further increase the computational requirement .",
    "_ scalability _ : many data mining algorithms scale , for @xmath0 objects , as @xmath20 , or even worse , making their simple application to large datasets on normal computing hardware intractable .",
    "one can often speed up a nave implementation of an algorithm that must access large numbers of data points and their attributes by storing the data in a hierarchical manner so that not all the data need to be searched .",
    "a popular hierarchical structure for accomplishing this task is the kd - tree@xcite .",
    "however , the implementation of such trees for large datasets and on parallel machines remains a difficult problem@xcite .",
    "_ learning curve _ : data mining is an entire field of study in its own right , with strong connections to statistics and computing . the avoidance of some of the issues we present , such as the selection of appropriate algorithms , collaboration where needed , and the full exploitation of their potential for science return , require overcoming a substantial learning curve .",
    "_ large datasets _ : many astronomical datasets are larger than can be held in machine memory .",
    "the exploitation of these datasets thus requires more sophisticated database technology than is currently employed by most astronomical projects .",
    "_ `` it s not science '' _ : the success of an astronomical project is judged by the science results produced . the time invested by an astronomer in becoming an expert in data mining techniques",
    "must be balanced against the expected science gain .",
    "it is difficult to justify and obtain funding based purely on a methodological approach such as data mining , even if such an approach will demonstrably improve the expected science return .",
    "_ it does not do the science for you _ : the algorithms will output patterns , but will not necessarily establish which patterns or relationships are important scientifically , or even which are causal .",
    "the truism ` correlation is not causation ' is apt here .",
    "the successful interpretation of data mining results is up to the scientist .",
    "_ the result can only be as good as the data _ : related to this , the single largest factor in the success of any data mining algorithm is the quality of the input data .",
    "if the data are not sufficient for the task , or are poorly collected or incorrectly treated , the result will not be useful .",
    "we now turn to the use of data mining algorithms in astronomical applications , and their track record in addressing some common problems . whereas in ",
    "[ sec : overview ] , we introduced terms for the astronomer unfamiliar with data mining , here for the non - expert in astronomy we briefly put in context the astronomical problems .",
    "however , a full description is beyond the scope of this review . whereas  [ sec : overview ] was subdivided according to data mining algorithms and issues , here the subdivision is in terms of the astrophysics . throughout this section ,",
    "we abbreviate data mining algorithms that are either frequently mentioned or have longer names according to the abbreviations introduced in  [ sec : overview ] : pca , ann , dt , svm , @xmath6nn , kde , em , som , and ica .",
    "given that there is no exact definition of what constitutes a data mining tool , it would not be possible to provide a complete overview of their application .",
    "this section therefore illustrates the wide variety of actual uses to date , with actual or implied further possibilities .",
    "uses which exist now but will likely gain greater significance in the future , such as the time domain , are largely deferred to ",
    "[ sec : future ] .",
    "several other overviews of applications of machine learning algorithms in astronomy exist , and contain further examples , including ones for ann@xcite , dt@xcite , genetic algorithms@xcite , and stellar classification@xcite .",
    "most of the applications in this section are made by astronomers utilizing data mining algorithms .",
    "however , several projects and studies have also been made by data mining experts utilizing astronomical data , because , along with other fields such as high energy physics and medicine , astronomy has produced many large datasets that are amenable to the approach .",
    "examples of such projects include the sky image cataloging and analysis system ( skicat)@xcite for catalog production and analysis of catalogs from digitized sky surveys , in particular the scans of the second palomar observatory sky survey ; the jet propulsion laboratory adaptive recognition tool ( jartool)@xcite , used for recognition of volcanoes in the over 30,000 images of venus returned by the magellan mission ; the subsequent and more general diamond eye@xcite ; and the lawrence livermore national laboratory sapphire project@xcite . a recent review of data mining from this perspective is given by kamath in the book _ scientific data mining_@xcite .",
    "in general , the data miner is likely to employ more appropriate , modern , and sophisticated algorithms than the domain scientist , but will require collaboration with the domain scientist to acquire knowledge as to which aspects of the problem are the most important .",
    "classification is often an important initial step in the scientific process , as it provides a method for organizing information in a way that can be used to make hypotheses and to compare with models .",
    "two useful concepts in object classification are the _ completeness _ and the _ efficiency _ , also known as recall and precision .",
    "they are defined in terms of true and false positives ( tp and fp ) and true and false negatives ( tn and fn ) .",
    "the completeness is the fraction of objects that are truly of a given type that are classified as that type : @xmath21 and the efficiency is the fraction of objects classified as a given type that are truly of that type @xmath22 these two quantities are astrophysically interesting because , while one obviously wants both higher completeness and efficiency , there is generally a tradeoff involved .",
    "the importance of each often depends on the application , for example , an investigation of rare objects generally requires high completeness while allowing some contamination ( lower efficiency ) , but statistical clustering of cosmological objects requires high efficiency , even at the expense of completeness .      due to their small physical size compared to their distance from us , almost all stars are unresolved in photometric datasets , and thus appear as point sources .",
    "galaxies , however , despite being further away , generally subtend a larger angle , and thus appear as extended sources .",
    "however , other astrophysical objects such as quasars and supernovae , also appear as point sources .",
    "thus , the separation of photometric catalogs into stars and galaxies , or more generally , stars , galaxies , and other objects , is an important problem .",
    "the sheer number of galaxies and stars in typical surveys ( of order @xmath23 or above ) requires that such separation be automated .",
    "this problem is a well studied one and automated approaches were employed even before current data mining algorithms became popular , for example , during digitization by the scanning of photographic plates by machines such as the apm@xcite and dposs@xcite .",
    "several data mining algorithms have been employed , including ann@xcite , dt@xcite , mixture modeling@xcite , and som@xcite , with most algorithms achieving over 95% efficiency .",
    "typically , this is done using a set of measured morphological parameters that are derived from the survey photometry , with perhaps colors or other information , such as the seeing , as a prior .",
    "the advantage of this data mining approach is that all such information about each object is easily incorporated . as well as the simple outputs ` star ' or ` galaxy ' ,",
    "many of the refinements described in  [ sec : overview ] have improved results , including probabilistic outputs and bagging@xcite .      as shown in fig . [",
    "fig : morph ] , galaxies come in a range of different sizes and shapes , or more collectively , morphology .",
    "the most well - known system for the morphological classification of galaxies is the hubble sequence of elliptical , spiral , barred spiral , and irregular , along with various subclasses@xcite .",
    "this system correlates to many physical properties known to be important in the formation and evolution of galaxies@xcite .",
    "other well - known classification systems are the yerkes system based on concentration index@xcite , the de vaucouleurs@xcite , exponential@xcite , and srsic index@xcite measures of the galaxy light profile , the david dunlap observatory ( ddo ) system@xcite , and the concentration - asymmetry - clumpiness ( cas ) system@xcite .    because galaxy morphology is a complex phenomenon that correlates to the underlying physics , but is not unique to any one given process ,",
    "the hubble sequence has endured , despite it being rather subjective and based on visible - light morphology originally derived from blue - biased photographic plates .",
    "the hubble sequence has been extended in various ways , and for data mining purposes the t system@xcite has been extensively used",
    ". this system maps the categorical hubble types e , s0 , sa , sb , sc , sd , and irr onto the numerical values -5 to 10 .",
    "one can , therefore , train a supervised algorithm to assign t types to images for which measured parameters are available .",
    "such parameters can be purely morphological , or include other information such as color .",
    "a series of papers by lahav and collaborators@xcite do exactly this , by applying anns to predict the t type of galaxies at low redshift , and finding equal accuracy to human experts .",
    "anns have also been applied to higher redshift data to distinguish between normal and peculiar galaxies@xcite , and the fundamentally topological and unsupervised som ann has been used to classify galaxies from hubble space telescope images@xcite , where the initial distribution of classes is not known . likewise , anns have been used to obtain morphological types from galaxy spectra.@xcite    several authors study galaxy morphology at higher redshift by using the hubble deep fields , where the galaxies are generally much more distant , fainter , less evolved , and morphologically peculiar .",
    "three studies@xcite use anns trained on surface brightness and light profiles to classify galaxies as e / s0 , sabc and sd / irr .",
    "another application@xcite uses fourier decomposition on galaxy images followed by anns to detect bars and assign t types .",
    "bazell & aha@xcite uses ensembles of classifiers , including ann and dt , to reduce the classification error , and bazell@xcite studies the importance of various measured input attributes , finding that no single measured parameter fully reproduces the classifications . ball _",
    "et al._@xcite obtain similar results to naim _ et al._@xcite , but updated for the sdss .",
    "et al._@xcite and ball , loveday & brunner@xcite utilize these classifications in studies of the bivariate luminosity function and the morphology - density relation in the sdss , the first such studies to utilize both a digital sky survey of this size and detailed hubble types .",
    "because of the complex nature of galaxy morphology and the plethora of available approaches , a large number of further studies exist : kelly & mckay@xcite ( fig .",
    "[ fig : mixture ] ) demonstrate improvement over a simple split in @xmath24 using mixture models , within a schema that incorporates morphology .",
    "serra - ricart _ et al._@xcite use an encoder ann to reduce the dimensionality of various datasets and perform several applications , including morphology .",
    "adams & woolley@xcite use a committee of anns in a ` waterfall ' arrangement , in which the output from one ann formed the input to another which produces more detailed classes , improving their results .",
    "molinari & smareglia@xcite use an som to identify e / s0 galaxies in clusters and measure their luminosity function .",
    "de theije & katgert@xcite split e / s0 and spiral galaxies using spectral principal components and study their kinematics in clusters .",
    "genetic algorithms have been employed@xcite for attribute selection and to evolve anns to classify ` bent - double ' galaxies in the first@xcite radio survey data .",
    "radio morphology combines the compact nucleus of the radio galaxy and extremely long jets .",
    "thus , the bent - double morphology indicates the presence of a galaxy cluster .",
    "de la calleja & fuentes@xcite combine ensembles of ann and locally weighted regression . beyond ann ,",
    "spiekermann@xcite uses fuzzy algebra and heuristic methods , anticipating the importance of probabilistic studies (  [ subsec : pdf ] ) that are just now beginning to emerge .",
    "owens , griffiths & ratnatunga@xcite use oblique dts , obtaining similar results to ann .",
    "zhang , li & zhao@xcite distinguish early and late types using k - means clustering .",
    "svms have recently been employed on the cosmos survey by huertas - company _",
    "et al._@xcite , enabling early - late separation to @xmath25 mag twice as good as the cas system .",
    "svms will also be used on data from the gaia satellite@xcite .",
    "recently , the popular _ galaxy zoo _",
    "project@xcite has taken an alternative approach to morphological classification , employing _",
    "crowdsourcing _ :",
    "an application was made available online in which members of the general public were able to view images from the sdss and assign classifications according to an outlined scheme .",
    "the project was very successful , and in a period of six months over 100,000 people provided over 40 million classifications for a sample of 893,212 galaxies , mostly to a limiting depth of @xmath26 mag .",
    "the classifications included categories not previously assigned in astronomical data mining studies , such as edge - on or the handedness of spiral arms , and the project has produced multiple scientific results . the approach represents a complementary one to automated algorithms , because , although humans can see things an algorithm will miss and will be subject to different systematic errors , the runtime is hugely longer : a trained ann will produce the same 40 million classifications in a few minutes , rather than six months .",
    "many of the physical properties , and thus classification , of a galaxy are determined by its stellar population .",
    "the spectrum of a galaxy is therefore another method for classification@xcite , and can sometimes produce a clearer link to the underlying physics than the morphology .",
    "spectral classification is important because it is possible for a range of morphological types to have the same spectral type , and vice versa , because spectral types are driven by different underlying physical processes .",
    "numerous studies@xcite have used pca directly for spectral classification .",
    "pca is also often used as a preprocessing step before the classification of spectral types using an ann@xcite .",
    "folkes , lahav & maddox@xcite predict morphological types for the 2df galaxy redshift survey ( 2dfgrs)@xcite using spectra , and ball _",
    "et al._@xcite directly predict spectral types in the sdss using an ann .",
    "et al._@xcite use the information bottleneck approach on the 2dfgrs spectra , which maximally preserves the spectral information for the desired number of classes .",
    "et al._@xcite use ensemble learning for ica on components of galaxy spectra .",
    "et al._@xcite use ann and locally weighted regression to directly predict emission line properties from photometry .",
    "bazell & miller@xcite applied a semi - supervised method suitable for class discovery using anns to the eso - lv@xcite and sdss early data release ( edr ) catalogs .",
    "they found that a reduction of up to 57% in classification error was possible compared to purely supervised anns .",
    "the larger of the two catalogs , the sdss edr , represents a preliminary dataset about 6% of the final data release of the sdss , clearly indicating the as - yet untapped potential of this approach .",
    "the semi - supervised approach also resembles the hybrid empirical - template approach to photometric redshifts (  [ subsec : photo - zs ] ) , as both seek to utilize an existing training set where available even if it does not span the whole parameter space .",
    "however , the approach used by bazell & miller is more general , because it allows new classes of objects to be added , whereas the hybrid approach can only iterate existing templates .",
    "most of the emitted electromagnetic radiation in the universe is either from stars , or the accretion disks surrounding supermassive black holes in active galactic nuclei ( agn ) .",
    "the latter phenomenon is particularly dramatic in the case of quasars , where the light from the central region can outshine the rest of the galaxy . because supermassive black holes are thought to be fairly ubiquitous in large galaxies , and their fueling , and thus their intrinsic brightness , can be influenced by the environment surrounding the host galaxy , quasars and other agn are important for understanding the formation and evolution of structure in the universe .",
    "the selection of quasars and other agn from an astronomical survey is a well - known and important problem , and one well suited to a data mining approach .",
    "it is well - known that different wavebands ( x - ray , optical , radio ) will select different agn , and that no one waveband can select them all . traditionally , agn are classified on the baldwin - phillips - terlevich diagram@xcite , in which sources are plotted on the two - dimensional space of the emission line ratios [ o ] @xmath27 5007 / h@xmath28 and [ n ] / h@xmath14 , that is separated by a single curved line into star - forming and agn regions .",
    "data mining not only improves on this by allowing a more refined or higher dimensional separation , but also by including passive objects in the same framework ( fig .",
    "[ fig : bpt ann ] ) .",
    "this allows for the probability that an object contains an agn to be calculated , and does not require all ( or any ) of the emission lines to be detected .",
    "several groups have used anns@xcite or dts@xcite to select quasar candidates from surveys .",
    "et al._@xcite show that the dt method improves the reliability of the selection to 85% compared to only 60% for simpler criteria .",
    "other algorithms employed include pca@xcite , svm and learning vector quantization@xcite , kd - tree@xcite , clustering in the form of principal surfaces and negative entropy clustering@xcite , and kernel density estimation@xcite .",
    "many of these papers combine multiwavelength data , particularly x - ray , optical , and radio .    similarly , one can select and classify candidates of all types of agn@xcite . if multiwavelength data are available , the characteristic data mining algorithm ability to form a model of the required complexity to extract the information could enable it to use the full information to extract more complete agn samples . more generally , one can classify both normal and active galaxies in one system , differentiating between star formation and agn . as one example , dts have been used@xcite to select quasar candidates in the sdss , providing the probabilities p(star , galaxy , quasar ) .",
    "p(star formation , agn ) could be supplied in a similar framework .",
    "et al._@xcite combine mixture modeling and regression to perform non - parametric mixture regression , and is the first study to obtain such components and then study them versus environment .",
    "the components are passive , star - forming , and two types of agn .      often , the first component of classification is the actual process of object detection , which often is done at some signal - to - noise threshold .",
    "several statistical data mining algorithms have been employed , and software packages written , for this purpose , including the faint object classification and analysis system ( focas)@xcite , daophot@xcite , source extractor ( sextractor)@xcite , maximum likelihood , wavelets , ica@xcite , mixture models@xcite , and anns@xcite .",
    "serra - ricart _ et al._@xcite show that anns are able to classify faint objects as well as a bayesian classifier but with considerable computational speedup",
    ".    several studies are more general than star - galaxy separation or galaxy classification , and assign classifications of varying detail to a broad range of astrophysical objects .",
    "et al._@xcite apply the autoclass bayesian classifier to the iras lrs atlas , finding new and scientifically interesting object classes .",
    "et al._@xcite use oblique dts in a system called classx to classify x - ray objects into stars , white dwarfs , x - ray binaries , galaxies , agn , and clusters of galaxies , concluding that the system has the potential to significantly increase the known populations of some rare object types .",
    "suchkov , hanisch & margon@xcite use the same system to classify objects in the sdss .",
    "bazell , miller & subbarao@xcite apply semi - supervised learning to sdss spectra , including those classified as ` unknown ' , finding two classes of objects consisting of over 50% unknown .",
    "stellar classifications are necessarily either spectral or based on color , due to the pointlike nature of the source .",
    "this field has a long history and well established results such as the hr diagram and the obafgkm spectral sequence .",
    "the latter is extended to a two - dimensional system of spectral type and luminosity classes i  v to form the two - dimensional mk classification system of morgan , keenan & kellman@xcite .",
    "class i are supergiants , through to class v , dwarfs , or main - sequence stars .",
    "the spectral types correspond to the hottest and most massive stars , o , through to the coolest and least massive , m , and each class is subdivided into ten subclasses 09 .",
    "thus , the mk classification of the sun is g2v .",
    "the use of automated algorithms to assign mk classes is analogous to that for assigning hubble types to galaxies in several ways : before automated algorithms , stellar spectra were compared by eye to standard examples ; the mk system is closely correlated to the underlying physics , but is ultimately based on observable quantities ; the system works quite well but has been extended in numerous ways to incorporate objects that do not fit the main classes ( e.g. , l and t dwarfs , wolf - rayet stars , carbon stars , white dwarfs , and so on ) .",
    "two differences from galaxy classification are the number of input parameters , in this case spectral indices , and the number of classes . in mk classification",
    "the numbers are generally higher , of order 50 or more input parameters , compared to of order 10 for galaxies .    given a large body of work for galaxies that has involved the use of artificial neural networks , and the similarities just outlined , it is not surprising that similar approaches have been employed for stellar classification@xcite , with a typical accuracy of one spectral type and half a luminosity type .",
    "the relatively large number of object attributes and output classes compared to the number of objects in each class does not invalidate the approach , because the efforts described generally find that the number of principal components represented by the inputs is typically much lower .",
    "a well - known property of neural networks is that they are robust to a large number of redundant attributes (  [ subsubsec : choice ] ) .",
    "neural networks have been used for other stellar classifications schemes , e.g. gupta _ et al._@xcite define 17 classes for iras sources , including planetary nebulae and h regions .",
    "other methods have been employed ; a recent example is manteiga _",
    "et al._@xcite , who use a fuzzy logic knowledge - based system with a hierarchical tree of decision rules . beyond the mk and other static classifications ,",
    "variable stars have been extensively studied for many years , e.g. , wozniak _",
    "et al._@xcite use svm to distinguish mira variables .",
    "the detection and characterization of supernovae is important for both understanding the astrophysics of these events , and their use as standard candles in constraining aspects of cosmology such as the dark energy equation of state .",
    "et al._@xcite use boosted dts , random forests , and svms to classify supernovae in difference images , finding a ten times reduction in the false - positive rate compared to standard techniques involving parameter thresholds ( fig .",
    "[ fig : sne ] ) .    given the general nature of the data mining approach , there are many further classification examples , including cosmic ray hits@xcite , planetary nebulae@xcite , asteroids@xcite , and gamma ray sources@xcite .",
    "an area of astrophysics that has greatly increased in popularity in the last few years is the estimation of redshifts from photometric data ( photo-@xmath29s ) .",
    "this is because , although the distances are less accurate than those obtained with spectra , the sheer number of objects with photometric measurements can often make up for the reduction in individual accuracy by suppressing the statistical noise of an ensemble calculation .",
    "photo - zs were first demonstrated in the mid 20th century@xcite , and later in the 1980s@xcite . in the 1990s ,",
    "the advent of the hubble space telescope deep fields resulted in numerous approaches@xcite , reviewed by koo@xcite . in the past decade , the advent of wide - field ccd surveys and multifiber spectroscopy have revolutionized the study of photo-@xmath29s  to the point where they are indispensable for the upcoming next generation surveys , and a large number of studies have been made .",
    "the two common approaches to photo-@xmath29s  are the template method and the empirical training set method .",
    "the template approach has many complicating issues@xcite , including calibration , zero - points , priors , multiwavelength performance ( e.g. , poor in the mid - infrared ) , and difficulty handling missing or incomplete training data .",
    "we focus in this review on the empirical approach , as it is an implementation of supervised learning . in the future , it is likely that a hybrid method incorporating both templates and the empirical approach will be used , and that the use of full probability density functions will become increasingly important . for many applications , knowing the error distribution in the redshifts is at least as important as the accuracy of the redshifts themselves , further motivating the calculation of pdfs .      at low redshifts ,",
    "the calculation of photometric redshifts for normal galaxies is quite straightforward due to the break in the typical galaxy spectrum at 4000 .",
    "thus , as a galaxy is redshifted with increasing distance , the color ( measured as a difference in magnitudes ) changes relatively smoothly . as a result , both template and empirical photo-@xmath29  approaches obtain similar results , a root - mean - square deviation of @xmath30 in redshift , which is close to the best possible result given the intrinsic spread in the properties@xcite .",
    "this has been shown with anns@xcite , svm@xcite , dt@xcite , @xmath6nn@xcite , empirical polynomial relations@xcite , numerous template - based studies , and several other methods . at higher redshifts , obtaining accurate results",
    "becomes more difficult because the 4000  break is shifted redward of the optical , galaxies are fainter and thus spectral data are sparser , and galaxies intrinsically evolve over time .",
    "the first explorations at higher redshift were the hubble deep fields in the 1990s , described above (  [ subsec : photo - zs ] ) , and , more recently , new infrared data have become available , which allow the 4000  break to be seen to higher redshift , which improves the results .",
    "template - based algorithms work well , provided suitable templates into the infrared are available , and supervised algorithms simply incorporate the new data and work in the same manner as previously described .",
    "while supervised learning has been successfully used , beyond the spectral regime the obvious limitation arises that in order to reach the limiting magnitude of the photometric portions of surveys , extrapolation would be required . in this regime , or where only small training sets are available , template - based results can be used , but without spectral information , the templates themselves are being extrapolated .",
    "however , the extrapolation of the templates is being done in a more physically motivated manner .",
    "it is likely that the more general hybrid approach of using empirical data to iteratively improve the templates,@xcite or the semi - supervised method described in ",
    "[ subsubsec : semisupervised ] will ultimately provide a more elegant solution .",
    "another issue at higher redshift is that the available numbers of objects can become quite small ( in the hundreds or fewer ) , thus reintroducing the curse of dimensionality by a simple lack of objects compared to measured wavebands .",
    "the methods of dimension reduction (  [ subsec : attributes ] ) can help to mitigate this effect .",
    "historically , the calculation of photometric redshifts for quasars and other agn has been even more difficult than for galaxies , because the spectra are dominated by bright but narrow emission lines , which in broad photometric passbands can dominate the color . the color - redshift relation of quasars is thus subject to several effects , including degeneracy , one emission line appearing like another at a different redshift , an emission line disappearing between survey filters , and reddening .",
    "in addition , the filter sets of surveys are generally designed for normal galaxies and not quasars .",
    "the assignment of these quasar photo-@xmath29s  is thus a complex problem that is amenable to data mining in a similar manner to the classification of agn described in  [ subsubsec : qso classification ] .",
    "the calculation of quasar photo-@xmath29s  has had some success using sdss data@xcite , but they suffer from _ catastrophic failures _ , in which , as shown in fig . [",
    "fig : photo - z qso ] , the photometric redshift for a subset of the objects is completely incorrect .",
    "however , data mining approaches have resulted in improvements to this situation .",
    "et al._@xcite find that a single - neighbor @xmath6nn  gives a similar result to the templates , but multiple neighbors , or other supervised algorithms such as dt or ann , pull in the regions of catastrophic failure and significantly decrease the spread in the results .",
    "kumar@xcite also shows this effect .",
    "et al._@xcite go further and are able to largely eliminate the catastrophics by selecting the subset of quasars with one peak in their redshift probability density function (  [ subsec : pdf ] ) , a result confirmed by wolf@xcite .",
    "et al._@xcite also show significant improvement using the combo-17 survey , which has 17 filters compared to the five of the sdss , but unfortunately the photometric sample is much smaller .    beyond the spectral regime ,",
    "template - based results are sufficient@xcite , but again suffer from catastrophics .",
    "given our physical understanding of the nature of quasars , it is in fact reasonable to extrapolate in magnitude when using colors as a training set , because while one is going to fainter magnitudes , one is not extrapolating in color .",
    "one could therefore quite reasonably assign empirical photo-@xmath29s  for a full photometric sample of quasars .",
    "typically in data mining , information gathered from spectra has formed the training set to apply a predictive technique to objects with photometry .",
    "however , it is clear from this process that the spectrum itself contains a large amount of information , and data mining techniques may be used directly on the spectra to extract information that might otherwise remain hidden .",
    "applications to galaxy spectral classification were described in  [ subsubsec : other gal ] . in stellar work , besides the classification of stars into the mk system based on observable parameters , several studies have directly predicted physical parameters of stellar atmospheres using spectral indices .",
    "one example is ramirez , fuentes & gulati@xcite , who utilize a genetic algorithm to select the appropriate input attributes , and predict the parameters using @xmath6nn .",
    "the attribute selection reduces run time and improves predictive accuracy .",
    "solorio _ et al._@xcite use @xmath6nn  to study stellar populations and improve the results by using active learning to populate sparse regions of parameter space , an alternative to dimension reduction .",
    "although it has much potential for the future (  [ subsec : time domain ] ) , the time domain is a field in which a lot of work has already been done .",
    "examples include the classification of variable stars described in ",
    "[ subsubsec : other classification ] , and , in order of distance , the interaction of the solar wind and the earth s atmosphere , transient lunar phenomena , detection and classification of asteroids and other solar system objects by composition and orbit , solar system planetary atmospheres , stellar proper motions , extrasolar planets , novae , stellar orbits around the supermassive black hole at the galactic center , microlensing from massive compact halo objects , supernovae , gamma ray bursts , and quasar variability . a good overview is provided by becker@xcite .",
    "the large potential of the time domain for novel discovery lies within the as yet unexplored parameter space defined by depth , sky coverage , and temporal resolution@xcite .",
    "one constraining characteristic of the most variable sources beyond the solar system is that they are generally point sources . as a result ,",
    "the timescales of interest are constrained by the light crossing time for the source .",
    "the analysis of the cosmic microwave background ( cmb ) is amenable to several techniques , including bayesian modeling , wavelets , and ica .",
    "the latter , in particular via the fastica algorithm@xcite , has been used in removal of cmb foregrounds@xcite , and cluster detection via the sunyaev - zeldovich effect@xcite .",
    "phillips & kogut@xcite use a committee of anns for cosmological parameter estimation in cmb datasets , by training them to identify parameter values in monte carlo simulations .",
    "this gives unbiased parameter estimation in considerably less processing time than maximum likelihood , but with comparable accuracy .",
    "one can use the fact that objects cross - matched between surveys will likely have correlated distributions in their measured attributes , for example , similar position on the sky , to improve cross - matching results using pattern classifiers .",
    "et al._@xcite combine distribution estimates and probabilistic classifiers to produce such an improvement , and supply probabilistic outputs .",
    "taylor & diaz@xcite obtain empirical fits for galactic metallicity using anns , whose architectures are evolved using genetic algorithms .",
    "this method is able to provide equations for metallicity from line ratios , mitigating the ` black box ' element common to anns , and , in addition , is potentially able to identify new metallicity diagnostics .",
    "bogdanos & nesseris@xcite analyze type ia supernovae using genetic algorithms to extract constraints on the dark energy equation of state .",
    "this method is non - parametric , which minimizes bias from the necessarily a priori assumptions of parametric models .",
    "lunar and planetary science , space science , and solar physics also provide many examples of data mining uses .",
    "one example is li",
    "_ et al._@xcite , who demonstrate improvements in solar flare forecasting resulting from the use of a mixture of experts , in this case svm and @xmath6nn .",
    "the analysis of the abundance of minerals or constituents in soil samples@xcite using mixture models is another example of direct data mining of spectra .",
    "finally , data mining can be performed on astronomical simulations , as well as real datasets .",
    "modern simulations can rival or even exceed real datasets in size and complexity , and as such the data mining approach can be appropriate .",
    "an example is the incorporation of theory@xcite into the virtual observatory (  [ subsec : vo ] ) .",
    "mining simulation data will present extra challenges compared to observations because in general there are fewer constraints on the type of data presented , e.g. , observations are of the same universe , but simulations are not , simulations can probe many astrophysical processes that are not directly observable , such as stellar interiors , and they provide direct physical quantities as well as observational ones .",
    "most of the largest simulations are cosmological , but they span many areas of astrophysics .",
    "a prominent cosmological simulation is the millennium run@xcite , and over 200 papers have utilized its data .",
    "we now turn to the future of data mining in astronomy .",
    "several trends are apparent that indicate likely fruitful directions in the next few years .",
    "these trends can be used to make informed decisions about upcoming , very large surveys .",
    "this section assumes that the reader is somewhat familiar with the concepts in both   [ sec : overview ] and [ sec : uses ] , namely , with both data mining and astronomy .",
    "we once again arrange the topics by data mining algorithm rather than by astronomical application , but we now interweave the algorithms with examples .    as in the past , it is likely that cross - fertilization with other fields will continue to be beneficial to astronomy , and of particular relevance here , the data mining efforts made by these fields .",
    "examples include high energy physics , whose most obvious spinoff is the world wide web from cern , but the subject has an extensive history of extremely large datasets from experiments such as particle colliders , and has provided well - known and commonly used data analysis software such as root @xcite , designed to cope with these data sizes and first developed in 1994 . in the fields of biology and the geosciences , the concepts of _ informatics _ , the study of computer - based information systems , have been extensively utilized , creating the subfields of bio- and geoinformatics .",
    "the official recognition of an analogous subfield within astronomy , _ astroinformatics _ , has recently been recommended@xcite .      a _ probability density function _ ( pdf , fig .",
    "[ fig : pdfs ] ) is a function such that the probability that the value , @xmath31 , is in the interval @xmath32 , is the definite integral over the range : @xmath33 thus the total area under the function is one .",
    "pdfs are of great significance for data mining in astronomy because they retain information that is otherwise lost , and because they enable results with improved signal - to - noise from a given dataset .",
    "one can think of a pdf as a histogram in the limit of small bins but many objects .",
    "approaches such as supervised learning are in general taking as input the information on objects and providing as output a prediction of properties .",
    "the most general way to do this is to work with the full pdfs at each stage .",
    "the formalism has recently been demonstrated in an astronomical context by budavri@xcite , and it is applicable to the prediction of any astronomical property . for inputs",
    "@xmath34 , ... , the output probabilities of a set of properties , @xmath35 can be predicted .",
    "fully probabilistic cross - matching of surveys has also been implemented by the same author@xcite .    results with pdfs in photo-@xmath29s  are starting to appear , either with single values and a spread , or the full pdf .",
    "et al._@xcite show that full pdfs help reduce bias .",
    "margoniner & wittman@xcite show that they enable subsamples with improved signal - to - noise , and wittman@xcite also demonstrates reduction in error .",
    "et al._@xcite show that generating full photo-@xmath29  pdfs for quasars allows subsection of a sample virtually free of catastrophic failures , the first time this has been demonstrated , and an important result for their use as tracers of the large scale structure in the universe .",
    "wolf@xcite confirms a similar result .",
    "myers , white & ball@xcite show that using the full pdf for clustering measurements will improve the signal - to - noise by four to five times for a given dataset without any alteration of the data ( fig .",
    "[ fig : clustering ] ) .",
    "this method is applicable to the clustering of any astronomical object .",
    "full pdfs have also been shown to improve performance in the photometric detection of galaxy clusters@xcite , again due to the increased signal - to - noise ratio .",
    "several further efforts use a single photo-@xmath29  and a spread , but not the full pdf .",
    "however , the method of myers , white & ball shows that it is the full pdf that will give the most benefit .",
    "pdfs will also be important for weak lensing@xcite .    as well as photo-@xmath29s",
    ", predicting properties naturally incorporates probabilistic classification .",
    "progress has been made , e.g. , the sdss has been classified according to p(galaxy , star , neither)@xcite .",
    "similar classifications that could be made are p(star formation , agn ) and p(quasar , not quasar ) .",
    "bailer - jones _",
    "et al._@xcite implement probabilistic classification that emphasizes finding very rare objects , in this case quasars among the stars that will be seen by gaia .",
    "ball _ et al._@xcite generate a pdf by perturbing inputs for a single - neighbor @xmath6nn .",
    "the idea of perturbing data has been studied in the field of privacy preserving data mining@xcite , but here the aim is to generate a pdf using the errors on the input attributes in a way that is computationally scalable to upcoming datasets .",
    "the approach appears to work well despite the fact that at present , survey photometric errors are generally poorly characterized@xcite .",
    "proper characterization of errors will be of great importance to future surveys as the probabilistic approach becomes more important .",
    "scalability may be best implemented either by using kd - tree like data structures , or by direct implementation on novel supercomputing hardware such as fpga , gpu , or cell processors (  [ subsec : hardware ] ) , which can provide enormous performance benefits for applications that require a large number of distance calculations .",
    "the time domain is already a significant area of study and will become increasingly important over the next decade with the advent of large scale synoptic surveys such as the large synoptic survey telescope ( lsst)@xcite . a large number of temporal resolved observations over large areas of the sky remains an unexplored area , and the historical precedent suggests that many interesting phenomena remain to be discovered@xcite .",
    "however , as one might expect , this field presents a number of challenges not encountered in the data mining of static objects .",
    "these include ( i ) how to handle multiple observations of objects that can vary in irregular and unpredictable ways , both intrinsic and due to the observational equipment , ( ii ) objects in difference images ( the static background is subtracted , leaving the variation ) , ( iii ) the necessarily extremely rapid response to certain events such as gamma ray bursts where physical information can be lost mere seconds after an event becomes detectable , ( iv ) robust classification of large streams of data in real time , ( v ) lack of previous information on several phenomena , and ( vi ) the volume and storage of time domain information in databases .",
    "other challenges are seen in static data , but will assume increased importance as real - time accuracy is needed .",
    "for example , the removal of artifacts@xcite that might otherwise be flagged as unusual objects and incur expensive follow - up telescope time .",
    "variability will be both photometric , a change in brightness , and astrometric , because objects can move . while some astronomical phenomena , such as certain types of variable stars , vary in a regular way , others vary in a nonlinear , irregular , stochastic , or chaotic manner , and the variability itself can change with time ( heteroskedasticity)@xcite .",
    "time series analysis is a well developed area of statistics , and many of these techniques will be useful .",
    "the combination of available information , but incomplete coverage of the possible phenomena suggests that a probabilistic (  [ subsec : pdf ] ) approach@xcite , either involving priors , or semi - supervised (  [ subsubsec : semisupervised ] ) will in general be the most appropriate .",
    "this is because the algorithms can use the existing information , but objectively interpret new phenomena .",
    "supervised learning will perform better for problems where more information and larger datasets are available , and unsupervised or bayesian priors will perform better when there are fewer observations .",
    "many events will still require followup observations , but since there will be far more events than can ever be followed up in detail , data mining algorithms will help ensure that the observations made are optimal in terms of the targeted scientific results .    as a confederation of data archives and interoperable standards of many of the world s existing telescopes , the virtual observatory ( vo ,  [ subsec : vo ] ) will be crucial in meeting the challenge of the time domain , and significant infrastructure for the vo already exists .",
    "the voeventnet@xcite is a system for the rapid handling of real time events , and provides an online federated data stream of events from several telescopes .",
    "it can be followed by both human observers and robotic telescopes .",
    "numerous next - generation wide - field surveys in the planning or construction stages will be synoptic .",
    "the largest such survey in the optical is the lsst , which will observe the entire sky , visible from its location , every three nights .",
    "these observations will provide a data stream exceeding one petabyte per year , and , as a result , they anticipate many of the challenges described here@xcite . like lsst@xcite",
    ", the gaia satellite@xcite has working groups dedicated to data mining .",
    "the classification working group has employed several data mining techniques , and developed new approaches@xcite to be used when the survey comes online .",
    "other ongoing or upcoming synoptic surveys include palomar - quest@xcite , the catalina real - time transient survey@xcite , pan - starrs@xcite , and those at other wavelengths such as instruments leading up to and including the square kilometer array@xcite",
    ". the time domain will not only provide challenges to existing methods of data mining , but will open up new avenues for the extraction of information , such as using the variability of objects for classification@xcite or photometric redshift@xcite . because they are due to a relatively compact source in the center of galaxies , active galactic nuclei vary on much shorter timescales than normal galaxies .",
    "this variability has been proposed as a mechanism to select quasar and other agn candidates .",
    "other events are suspected theoretically but have not been observed@xcite . but given the dataset sizes , automated detection of such events at some level is clearly required .",
    "the computational demands of real time processing of the enormous data streams from these surveys is significant , and will likely be met by the use of newly emerging specialized computing hardware (  [ subsec : hardware ] ) .      the current state of the art in supercomputing consists of terabyte - sized files and teraflop computing speeds , which is conveniently encapsulated in the term _",
    "terascale computing_. following moore s law@xcite , in which computer performance has increased exponentially for the last several decades , the coming decade will feature the similarly - derived _",
    "petascale computing_@xcite .",
    "much of the performance increase in the past decade has been driven by increases in processor ( cpu ) clock frequency , but this rate has now slowed due to physical limitations on the sizes of components , and more importantly power consumption and energy ( heat ) dissipation .",
    "it has therefore become more economical to manufacture chips with multiple processor cores .",
    "the typical supercomputer today is a cluster , which consists of a large number of conventional cpus connected by a specialized interconnect system , a distributed or shared memory , a shared filesystem , and hosting the linux operating system .",
    "many systems are heterogeneous because this is scalable and cost - effective , but coordinating and making effective such a system can be challenging . in particular",
    ", it will be vital that the system is properly balanced between processing power and disk input / output ( i / o ) to supply the data .",
    "combined with the increasing number of processor cores , this means that _ parallel and distributed computing _ is rapidly increasing in importance .",
    "a useful set of ` rules of thumb ' for parallel and other aspects of computing were formulated by amdahl in the 1960s@xcite , and they remain true today .",
    "one of these is that roughly 50,000 cpu cycles are required per byte of data .",
    "most scientific datasets require far fewer cycles than this , and it is thus likely that future performance will be i / o limited , unless sufficient disks are provided in parallel .",
    "bell , gray & szalay@xcite estimate that a petascale system will require 100,000 one tb disks .",
    "the exact details of how to distribute the data for best performance are likely to be system - dependent@xcite .",
    "the available cpu speed should scale to the data size , although it will not scale to most navely implemented data mining algorithms (  [ subsec : parallel ] ) .",
    "an example of an upcoming petascale system whose uses will include astronomical data mining is the _ blue waters _ system at the national center for supercomputing applications ( ncsa ) , which is due to come online in 2011 .",
    "specifications include 200,000 compute cores with 4 ghz 8 core processors , 1 pb of main memory , 10 pb of user disk storage , 500 pb of archival storage , and 400 gb @xmath36 bandwidth connectivity to provide sustained petascale compute power .",
    "it will implement the ibm percs ( productive , easy - to - use , reliable computer system)@xcite , which will integrate their cpu , operating system , parallel programming , and file systems .",
    "this provides a method of addressing the issues of running real - world applications at the petascale by balancing the cpu , i / o , networking , and so on .",
    "similarly , a considerable investment of effort is being carried out in the years leading up to deployment in 2011 on the development of applications for the system , in consultation with the scientists who will run them .",
    "several astronomical applications are included , mostly simulations , but also data mining in the form of the analysis of lsst datasets .",
    "not all petascale computing will be done on systems as large as blue waters . in the us ,",
    "the national science foundation office of cyberinfrastructure has been advised@xcite to implement a power - law type system , with a small number of very large systems , of order ten times more regional centers , and ten times more local facilities ( tiers 13 ) . such local facilities , for example beowulf clusters , are already common in university departments , and consist of typically a few dozen commodity machines . a recent trend matching the increasing requirements for data - intensive as opposed to cpu - intensive computing",
    "is the graywulf cluster@xcite , which implements the idea of data ` storage bricks ' : cheap , modular , and portable versions of a balanced system which when added together provide petascale computation .      as indicated in ",
    "[ subsec : petascale ] above , because of the slowing increase in raw speed of individual cpus , processors are becoming increasingly parallelized , both in terms of the number of processor cores on a single chip , and increasing amounts of these chips being deployed in parallel on supercomputing clusters . providing appropriately scaled systems ( cpu , i / o , etc . )",
    "is one challenge , but most data mining algorithms not only will be required to run on petascale data , but their nave implementations scale as @xmath37 , or worse .",
    "it has been suggested@xcite that any algorithm that scales beyond @xmath38 will rapidly be rendered infeasible .",
    "mcconnell and skillicorn@xcite have promoted parallel and distributed data mining@xcite , which is well - known in the data mining field , but virtually unused in astronomy . in this approach ,",
    "the algorithms explicitly take advantage of available parallelism .",
    "the simplest example is task - farming , or the embarrassingly parallel approach , in which a task is divided into many mutually - independent subtasks , each of which is allocated to a single processor .",
    "this can be done on an array of ordinary desktop machines as well as a supercomputer .",
    "a more complex challenge is when many parts of the data must be accessed , or when an algorithm relies on the outputs from calculations distributed across multiple compute nodes . for a large dataset",
    "the hardware required likely includes shared memory (  [ subsec : petascale ] ) , thus shared memory parallelization@xcite can be important .",
    "many algorithms exist for the implementation of data mining on parallel computer systems beyond simple task farming , but these are not widely used within science , as compared to the commercial sector .",
    "the application programming interfaces mpi and openmp have been widely used on distributed and shared memory systems , respectively , for simulation and some data analysis , but they do not offer the semantic capabilities@xcite needed for data mining , i.e. , the metadata describing the meaning of the data being processed and the results produced are not easily incorporated .    parallel data mining is challenging , as not only must the algorithm be implemented on the hardware , but many algorithms simply can not be ported as - is to such a system . instead , parallelization requires that the algorithm itself , as encapsulated in the code , must often be fundamentally altered at the pseudocode level .",
    "this can be a time - consuming and counterintuitive process , especially to scientists who are generally not trained or experienced in parallel programming .",
    "progress is slowly being made in astronomy , including a parallel implementation of kd - trees@xcite , cosmological simulations requiring datasets larger than the node memory size@xcite , and parallelization of algorithms@xcite .",
    "an alternative approach is grid computing , in which the exact resource used is unimportant to the user , although not all data mining algorithms lend themselves to this paradigm .",
    "a variant of grid computing is crowdsourcing , in which members of the public volunteer their spare cpu cycles to process data for a project . the most well - known project of this type is seti@home , and more recently , the galaxy zoo project , which employed large numbers of people to successfully classify galaxies in sdss images .",
    "such crowdsourcing is likely to become even more important in the future , particularly in combination with greatly improved outreach via astronomical applications on social networking sites such as facebook@xcite .",
    "scalability is also helped on conventional cpus by the employment of tree structures , such as the kd - tree , which partition the data .",
    "this enables a search to access any data value without searching the whole dataset .",
    "kd - trees have been used for many astronomical applications , including speeding up n - point correlation functions@xcite ; cross - matching , classification , and photometric redshifts@xcite .",
    "they can be extended to more sophisticated structures , for example , the multi - tree@xcite .",
    "however , implementation of such tree structures on parallel hardware or computational accelerators (  [ subsec : hardware ] ) remains difficult@xcite .",
    "the virtual observatory ( vo ) is an analogous concept to a physical observatory , but instead of telescopes , various centers house data archives .",
    "the vo consists of numerous national - level organizations , and the international virtual observatory alliance . within the national organizations there are various data centers that house large datasets , computing facilities to process and analyze them , and people with considerable expertise in the datasets stored at that particular center .",
    "common data standards and web services are necessary for the vo to work .",
    "such standards have emerged , including web services using xml and soap , a data format , votable@xcite , a query language based on sql , the astronomical data query language@xcite , image access protocols for images ( siap@xcite ) , and spectra ( ssap ) , voeventnet@xcite for the time domain , plus various standards of interoperability and ways of describing resources such as the unified content descriptor@xcite .",
    "large numbers of high level tools for working with data are also available .",
    "an example of the emerging data standards for archiving is the common archive observation model@xcite ( caom ) of the canadian astronomical data center ( cadc ) . given that it is likely that the future vo will continue to consist of a number of data centers like the cadc , this model represents a useful and realistic way in which data can be made meaningfully accessible , but not so rigidly presented as to prevent the desired analysis of future researchers with as yet unforeseen science goals .",
    "this model consists of the components artifact , plane , simpleobservation , and compositeobservation , which describe logical parts of the data from individual files to logical sets of observations such as spectra , and forms the basis of all archiving activity at the cadc .",
    "the increasing immobility of large datasets as described in ",
    "[ subsec : petascale ] will render it uneconomical in terms of time and money to download large datasets to local machines .",
    "rather than bringing the data to the analysis , it will become more sensible to take the analysis to the data@xcite . to be able to perform complicated data mining analyses ,",
    "it is necessary that the data be organized well enough to make this tractable , and that the center archiving the data must have sufficient computing power and web services to perform the analyses .",
    "the organizational requirement means that the data must be stored as a database with the sophistication found in the commercial sector , where mining of terascale databases is routine .",
    "commercial software and computer science expertise will help , but the task is non - trivial because astronomical data analysis can require particular data types and structures not usually found in commercial software , such as time series observations .",
    "an example of such a database already in place is the sdss , and its underlying schema@xcite has been used and copied by other surveys such as galex .",
    "nevertheless , it is likely that considerable analyses will continue to be carried out on smaller subsets of the data , and this data may well continue to be downloaded and analyzed locally , as it has been to date .",
    "if one anticipates working exclusively with one survey , it may still be more efficient to implement a graywulf - like cluster locally and download the complete dataset .",
    "another difficult problem faced by the vo is that a significant future scientific benefit from large datasets will be in the cross - matching of multiple datasets , in particular , multiwavelength data .",
    "but if such data are distributed among different data centers and are difficult to move , such work may be intractable .",
    "what can be done , however , is to make available as part of the vo web services , tools for cross - matching datasets at a given center . a common data format and description , combined with",
    "the fact that much of the science is done from small subsets of large datasets , means that this is certainly tractable . as a result , it is not surprising that there is significant demand for such tools@xcite .",
    "an important consideration for the vo is that many astronomers , indeed many scientists in general , will want to run their own software on the data , and not simply a higher level tool that involves trusting someone else s code .",
    "this will be true even if the source code is available . or",
    ", a scientist might wish to complete an analysis that is not available in a higher level tool .",
    "it is thus important that the data are available at a low level of processing so that one can set one s own requirements as needed .",
    "nasa has a categorization of data where 0 is raw , 1 is calibrated , and 2 is a derived product , such as a catalog .",
    "an ideal data archive would have available well documented and accessible level 2 catalogs , similarly documented and accessible level 1 data , and perhaps not online but stored level 0 data , to enable , for example , a re - reduction .",
    "data have been released using the vo publishing interfaces@xcite , data mining algorithms such as anns have been implemented@xcite , and applications for analyses with web interfaces are online@xcite .",
    "multiwavelength analyses are becoming more feasible and useful@xcite , and it is therefore now possible , but still time - consuming , to perform scientific analyses using vo tools@xcite .",
    "we expect this will be an area where considerable work will still need to be done , however , in order to fully enable the full exploitation of the archives of astronomy data in the future .",
    "visualization of data is an important part of the scientific process , and the combination of terascale computing and data mining poses obvious challenges .",
    "common plotting codes presently in use in astronomy include supermongo , pgplot , gnuplot , and idl @xcite , but these are stand - alone codes that do not easily cope with data that can not be completely loaded into the available memory space .",
    "newer tools , such as topcat@xcite , visivo@xcite , and vomegaplot@xcite support the virtual observatory standards such as votable and plastic@xcite for interoperability between programs .",
    "the full library on which the topcat program is based , stilts@xcite , is able to plot arbitrarily - sized datasets .    as with hardware , software , and data analysis ,",
    "collaboration with computer scientists and other disciplines has resulted in progress in various areas of scientific visualization . at harvard",
    ", the astromed project at the initiative for innovative computing ( iic ) has collaborated with medical imaging teams@xcite .",
    "the rendering of complex multi - dimensional volumetric and surficial data is a common desire of both fields , and the medical imaging software was considerably more advanced than was typical in astronomy in terms of graphical capability . as with the creation and curation of databases for large datasets , collaboration with the it sector",
    "has enabled significant progress and the use of tools beyond the scope of those that could be created by astronomers alone , such as google sky@xcite .",
    "it is likely that such collaboration will continue to increase in importance .",
    "the program s2plot@xcite , developed at swinburne , is motivated by the idea of making three - dimensional plots as easy to transfer from one medium to another ( interchange ) as two - dimensional plots .",
    "the existing familiar interface of a plotting code , in this case pgplot , has been extended@xcite to enable rendering of multi - dimensional data on several media , including desktop machines , pdf files , powerpoint - style slides , or web pages .",
    "systems in which the user is able to interact directly with the data are also likely to play a significant role .",
    "partiview@xcite , developed at ncsa , enables the visualization of particulate data and some isosurfaces either on a desktop or in an immersive cave system , and several astronomical datasets have been visualized .",
    "szalay , springel & lemson@xcite describe using graphical processing units (  [ subsec : hardware ] ) to aid visualization , in which the data are preprocessed to hierarchical levels of detail , and only rendered to the resolution required to appear to the eye as if the whole dataset is being rendered .",
    "paraview is a program designed for parallel processing on large datasets using distributed memory systems , or on smaller data on a desktop .",
    "finally , in recent years , numerous online virtual worlds have become popular , the most well - known of which is second life .",
    "hut@xcite and djorgovski describe their interaction within these worlds , both with other astronomers in the form of avatars in meetings , and with datasets . while it may initially seem to be just a gimmicky way to have a meeting , the interaction with other avatars is described as ` fundamentally visceral ' , much more so than one would expect .",
    "this suggests that , along with social networks for outreach , such interaction among astronomers may become more common , as one will be able to attend a meeting without having to travel physically .",
    "for the final part of  [ sec : future ] , we turn to novel supercomputing hardware .",
    "this is a rapidly developing area , but it has enormous potential to speed up existing analyses , and render previously impossible questions tractable .",
    "specialized hardware has been used in astronomy for many years , but until recently only in limited contexts and applications , such as the grape@xcite systems designed specifically for @xmath0-body calculations , or direct processing of data in instrument - specific hardware . here , we describe three hardware formats that have emerged in recent years as viable solutions to a more general range of astronomical problems : graphical processing units ( gpus ) , field - programmable gate arrays ( fpgas ) , and the cell processor .    as described in  [ subsec : petascale ] , the increasing speed of cpu clock cycles has now been largely replaced by increasing parallelism as the main method for continuing improvements in computing power .",
    "the methods described there implement _ coarse - grained _ parallelism , which is at the level of separate pieces of hardware or application processes .",
    "the hardware described here implements _ fine - grained _ parallelism , in which , at the instruction level , a calculation that would require multiple operations on a cpu is implemented in one operation .",
    "the hardware forms an intermediary between the previously - used application - specific integrated circuits ( asic ) , and the general purpose cpu .",
    "future petascale machines (  [ subsec : petascale ] ) are likely to include some or all of these three , either as highly integrated components in a cluster - type system , or as part of the heterogeneous hardware making up a distributed grid - like system that has overall petascale performance .    spurred by the computer gaming industry , the gpus on graphics cards within desktop - scale computers have increased in performance much more rapidly than conventional processors ( cpus ) .",
    "they are specially designed to be very fast at carrying out huge numbers of operations that are used in the rendering of graphics , by using vector datatypes and streaming the data .",
    "vector processors have been used before in supercomputing , but gpus have become of great interest to the scientific community due to their commodity - level pricing , which results from their widespread commercial use , and the increasing ease of use for more general operations than certain graphical processes .    at first , gpus dealt only with fixed - point numbers , but now single - precision floating point and even double - precision are becoming more common .",
    "thus the chips are no longer simply specialized graphics engines , but are becoming much more general - purpose ( gpgpus ) . double - precision is required or highly desirable for many scientific applications .",
    "the ease of use of gpus has been increased thanks to nvidia s compute unified device architecture development environment ( cuda ) for its cards , and will be further aided by the open computing language ( opencl ) for heterogeneous environments .",
    "these enable the gpu functions to be called in a similar way to a c library , and are becoming a de facto standard .",
    "cuda has also been ported to other higher level languages , including pycuda in python .",
    "gpus are beginning to be used in astronomy , and several applications have appeared .",
    "gpus can reproduce the functionality of the grape hardware for n - body simulations@xcite , and cuda implementations have been shown to outperform grape in some circumstances@xcite .",
    "gpus are beginning to be used for real - time processing of data from next generation instruments@xcite as part of the data intensive science consortium at the harvard iic .",
    "significant speedup has been demonstrated of a @xmath6 nearest neighbor search on a gpu compared to a kd - tree implemented in c on a cpu@xcite .",
    "fpgas@xcite are another form of hardware that has become viable for somewhat general - purpose scientific computing .",
    "while fpgas have been widely used as specialized hardware for many years , including in telescopes for data processing or adaptive optics , it is only in the past few years that their speed , cost , capacity , and ease of use have made them viable for more general use by non - specialists . as with gpus ,",
    "the ability to work with full double precision floating point numbers is also increasing , and their use is via libraries and development environments that enable the fpga portion of the code to appear as just another function call in c or a c - like language .",
    "these tools implement the hardware description language to program the fpga , which need not be known by the user .",
    "an fpga consists of a grid of logic gates which must be programmed via software to implement a specific set of functions before running code ( hence field - programmable ) .",
    "if the calculation to be performed can be fully represented in this way on the available gates , this enables a throughput speed of one whole calculation of a function per clock cycle , which given a modern fpga s clock speed of 100 mhz or more , is 100 million per second . in practice , however , the actual speed is often limited by the i / o .",
    "one recent example is the direct mapping of an ann onto an fpga@xcite , which can then in principle classify one object per clock cycle , or 100 million objects per second at 100 mhz .",
    "fpgas will continue to be widely used as specialized components for astronomical systems , for example in providing real - time processing of the next generation synoptic surveys .",
    "brunner , kindratenko & myers@xcite demonstrated a significant speedup of the n - point correlation function using fpgas .",
    "freeman , weeks & austin@xcite directly implement distance calculations , such as required by the @xmath6nn  data mining algorithm , on an fpga .    finally , the ibm cell processor@xcite is a chip containing a conventional cpu and and array of eight more powerful coprocessors for hardware acceleration in a similar manner to the gpu and fpga . like the nvidia gpu",
    ", it has been widely used in mass - production machines such as the playstation 3 , and is or will be incorporated into several ` hybrid ' petascale machines , including ibm s roadrunner , and possibly blue waters .",
    "unfortunately , also like the gpu , it is not yet as easy to use as desired for large scale scientific use , but progress in the area is continuing .",
    "further novel supercomputing hardware such as clearspeed may become viable for science and widely used .",
    "it is an area of exciting developments and considerable potential . as with many new developments ,",
    "however , one must be somewhat careful , in this case because the continued development of the hardware is driven by large commercial companies ( nvidia , ibm , etc . ) , and not the scientific community .",
    "nevertheless , the potential scientific gains are so large that it is certainly worth keeping an eye on .",
    "in this review , we have introduced data mining in astronomy , given an overview of its implementation in the form of knowledge discovery in databases , reviewed its application to various science problems , and discussed its future . throughout , we have tried to emphasize data mining as a tool to enable improved science , not as an end in itself , and to highlight areas where improvements have been made over previous analyses , where they might yet be made , and limitations of this approach .",
    "an astronomer is not a cutting - edge expert in data mining algorithms any more than they are in statistics , databases , hardware , software , etc . , but they will need to know enough to usefully apply such approaches to the science problem they wish to address .",
    "it is likely that such progress will be made via collaboration with people who are experts in these areas , particularly within large projects , that will employ specialists and have working groups dedicated to data mining .",
    "fully implemented , commercial - level databases will be required since the data will be too big to organize , download , or analyze in any other way",
    ".    the available infrastructure should , therefore , be designed so that this data mining approach to research is maximally enabled .",
    "the raw or minimally - processed data should be made available in a manner so one can apply user - specific codes either locally or using computational resources local to the data if data size necessitates it .",
    "it is unlikely that most researchers will either require or trust the exact resources made available by higher level tools .",
    "instead , they will be useful for exploratory work , but ultimately one must be able to run personal or trusted code on the data , from the level of re - reduction upwards .",
    "a problem arises when one wishes to utilize multiple or distributed datasets , for example in cross - matching data for multi - wavelength studies .",
    "therefore , datasets that can be easily made interoperable via a standard storage schema should be made available . in this manner",
    ", a user can bring computing power and algorithms to tackle their particular science question .",
    "this problem is particularly acute when large datasets are held at widely separated sites , because transfer of such data across the network is currently impractical .",
    "a great deal of science is done on small subsets of the full data , so data will still be frequently downloaded and analyzed locally , but the paradigm of downloading entire datasets is not sustainable .",
    "we thank the referee for a useful and comprehensive report .",
    "j.  macqueen , some methods for classification and analysis of multivariate observations , in _ proceedings of the fifth berkeley symposium on mathematical statistics and probability _ , eds .",
    "l.  m. lecam and j.  neyman ( university of california press , berkeley , 1967 ) .",
    "x.  zhu , a.  goldberg , r.  brachman and t.  dietterich , _ introduction to semi - supervised learning _ , synthesis lectures on artificial intelligence and machine learning ( morgan & claypool , san rafael , ca , 2009 ) .",
    "j.  h. holland , _ adaptation in natural and artificial systems : an introductory analysis with applications to biology , control and artificial intelligence _",
    "( the university of michigan press , ann arbor , mi , 1975 ) .",
    "m.  welge , w.  h. hsu , l.  s. auvil , t.  m. redman and d.  tcheng , high - performance knowledge discovery and data mining systems using workstation clusters , in _",
    "12th national conference on high performance networking and computing ( sc99 ) _ , 1999 .",
    "j.  p. gardner , a.  connolly and c.  mcbride , enabling rapid development of parallel tree search applications , in _",
    "clade 07 : proceedings of the 5th ieee workshop on challenges of large applications in distributed environments _ , ( acm , new york , 2007 ) .",
    "c.  a.  l. bailer - jones , automated stellar classification for large surveys : a review of methods and results , in _ automated data analysis in astronomy _ , eds .",
    "r.  gupta , h.  p. singh and c.  a.  l. bailer - jones ( 2002 ) .",
    "m.  c. burl , c.  fowlkes , j.  roden , a.  stechert and s.  mukhtar , diamond eye : a distributed architecture for image data mining , society of photo - optical instrumentation engineers ( spie ) conference series vol .",
    "3695 ( 1999 ) .",
    "s.  g. djorgovski , r.  r. gal , s.  c. odewahn , r.  r. de carvalho , r.  brunner , g.  longo and r.  scaramella , the palomar digital sky survey ( dposs ) , in _ wide field surveys in cosmology _ , eds .",
    "s.  colombi , y.  mellier and b.  raban ( 1998 ) .",
    "e.  cant - paz and c.  kamath , evolving neural networks for the classification of galaxies , in _",
    "gecco 02 : proceedings of the genetic and evolutionary computation conference _ , ( morgan kaufmann publishers inc .",
    ", san francisco , 2002 ) .",
    "d.  c. koo , overview - photometric redshifts : a perspective from an old - timer [ ! ] on their past , present , and potential , in _ photometric redshifts and the detection of high redshift galaxies _ , eds .",
    "r.  weymann , l.  storrie - lombardi , m.  sawicki and r.  brunner , astronomical society of the pacific conference series , vol .  191 ( 1999 ) .",
    "s.  carliles , t.  budavri , s.  heinis , c.  priebe and a.  szalay , photometric redshift estimation on sdss data using random forests , in _ astronomical data analysis software and systems xvii _ , eds .",
    "r.  w. argyle , p.  s. bunclark and j.  r. lewis , astronomical society of the pacific conference series , vol .",
    "394 ( 2008 ) .                                                      s.  g. djorgovski , a.  a. mahabal , r.  j. brunner , r.  r. gal , s.  castro , r.  r. de carvalho and s.  c. odewahn , searches for rare and new types of objects , in _ virtual observatories of the future _ , eds .",
    "r.  j. brunner , s.  g. djorgovski and a.  s. szalay , astronomical society of the pacific conference series , vol .",
    "225 ( 2001 ) .",
    "m.  taylor and a.  i. diaz , on the deduction of galactic abundances with evolutionary neural networks , in _ from stars to galaxies : building the pieces to build up the universe _ , eds .",
    "a.  vallenari , r.  tantalo , l.  portinari and a.  moretti , astronomical society of the pacific conference series , vol .  374 ( 2007 ) .                                      c.  donalek , a.  mahabal , s.  g. djorgovski , s.  marney , a.  drake , e.  glikman , m.  j. graham and r.  williams , new approaches to object classification in synoptic sky surveys , american institute of physics conference series vol .",
    "1082 ( 2008 ) .",
    "a.  j. drake , r.  williams , m.  j. graham , a.  mahabal , s.  g. djorgovski , r.  r. white , w.  t. vestrand and j.  bloom , voeventnet : an open source of transient alerts for astronomers . ,",
    "bulletin of the american astronomical society vol .  38 ( 2007 ) .",
    "c.  a.  l. bailer - jones , a method for exploiting domain information in astrophysical parameter estimation , in _ astronomical data analysis software and systems xvii _ , eds .",
    "r.  w. argyle , p.  s. bunclark and j.  r. lewis , astronomical society of the pacific conference series , vol .  394 ( 2008 ) .",
    "k.  ebcioglu , v.  saraswat and v.  sarkar , the ibm percs project and new opportunities for compiler - driven performance via a new programming model , _ compiler - driven performance workshop ( cascon 2004 ) _ , ( 2004 ) .        s.  m. mcconnell and d.  b. skillicorn , distributed data mining for astrophysical datasets , in _ astronomical data analysis software and systems xiv _ ,",
    "p.  shopbell , m.  britton and r.  ebert , astronomical society of the pacific conference series , vol .  347 ( 2005 ) .",
    "n.  gray , the fact and future of semantic astronomy , in _ astronomical data analysis software and systems xvii _ , eds .",
    "r.  w. argyle , p.  s. bunclark and j.  r. lewis , astronomical society of the pacific conference series , vol .",
    "394 ( 2008 ) .",
    "d.  gao , y.  zhang and y.  zhao , the application of kd - tree in astronomy , in _ astronomical data analysis software and systems xvii _ , eds .",
    "r.  w. argyle , p.  s. bunclark and j.  r. lewis , astronomical society of the pacific conference series , vol .  394 ( 2008 ) .",
    "a.  g. gray , a.  w. moore , r.  c. nichol , a.  j. connolly , c.  genovese and l.  wasserman , multi - tree methods for statistics on very large datasets in astronomy , in _ astronomical data analysis software and systems ( adass ) xiii _ , eds .",
    "f.  ochsenbein , m.  g. allen and d.  egret , astronomical society of the pacific conference series , vol .  314 ( 2004 ) .",
    "y.  shirasaki , m.  ohishi , y.  mizumoto , m.  tanaka , s.  honda , m.  oe , n.  yasuda and y.  masunaga , structured query language for virtual observatory , in _ astronomical data analysis software and systems xiv _ , eds .",
    "p.  shopbell , m.  britton and r.  ebert , astronomical society of the pacific conference series , vol .  347 ( 2005 ) .",
    "s.  derriere _",
    "et  al . _ , ucd in the ivoa context , in _ astronomical data analysis software and systems ( adass ) xiii _ , eds . f.  ochsenbein , m.  g. allen and d.  egret , astronomical society of the pacific conference series , vol .  314 ( 2004 ) .    p.  dowler , s.  gaudet , d.  durand , r.  redman , n.  hill and s.  goliath , common archive observation model , in _ astronomical data analysis software and systems xvii _ , eds .",
    "r.  w. argyle , p.  s. bunclark and j.  r. lewis , astronomical society of the pacific conference series , vol .  394 ( 2008 ) .",
    "c.  vignali , f.  fiore , a.  comastri , m.  brusa , r.  gilli , n.  cappelluti , f.  civano and g.  zamorani , multi - wavelength data handling in current and future surveys : the possible role of virtual observatory , in _ multi - wavelength astronomy and virtual observatory _ , ed .",
    "d.  baines & p.  osuna ( 2009 ) .",
    "w.  b. landsman , the idl astronomy user s library , in _ astronomical data analysis software and systems ii _ ,",
    "r.  j. hanisch , r.  j.  v. brissenden and j.  barnes , astronomical society of the pacific conference series , vol",
    ".  52 ( 1993 ) .",
    "m.  b. taylor , topcat & stil : starlink table / votable processing software , in _ astronomical data analysis software and systems xiv _",
    "p.  shopbell , m.  britton and r.  ebert , astronomical society of the pacific conference series , vol .  347 ( 2005 ) .",
    "j.  d. taylor , t.  boch , m.  comparato , m.  taylor , n.  winstanley and r.  g. mann , binding applications together with plastic , in _ astronomical data analysis software and systems xvi _ , eds .",
    "r.  a. shaw , f.  hill and d.  j. bell , astronomical society of the pacific conference series , vol .  376 ( 2007 ) .",
    "m.  b. taylor , stilts - a package for command - line processing of tabular data , in _ astronomical data analysis software and systems xv _ , eds . c.  gabriel , c.  arviset , d.  ponz and s.  enrique , astronomical society of the pacific conference series , vol .",
    "351 ( 2006 ) .",
    "m.  borkin , a.  goodman , m.  halle and d.  alan , application of medical imaging software to 3d visualization of astronomical data , in _ astronomical data analysis software and systems xvi _ ,",
    "r.  a. shaw , f.  hill and d.  j. bell , astronomical society of the pacific conference series , vol .",
    "376 ( 2007 ) ."
  ],
  "abstract_text": [
    "<S> we review the current state of data mining and machine learning in astronomy . _ data mining _ can have a somewhat mixed connotation from the point of view of a researcher in this field . </S>",
    "<S> if used correctly , it can be a powerful approach , holding the potential to fully exploit the exponentially increasing amount of available data , promising great scientific advance . </S>",
    "<S> however , if misused , it can be little more than the black - box application of complex computing algorithms that may give little physical insight , and provide questionable results . here </S>",
    "<S> , we give an overview of the entire data mining process , from data collection through to the interpretation of results . </S>",
    "<S> we cover common machine learning algorithms , such as artificial neural networks and support vector machines , applications from a broad range of astronomy , emphasizing those where data mining techniques directly resulted in improved science , and important current and future directions , including probability density functions , parallel algorithms , petascale computing , and the time domain . </S>",
    "<S> we conclude that , so long as one carefully selects an appropriate algorithm , and is guided by the astronomical problem at hand , data mining can be very much the powerful tool , and not the questionable black box . </S>"
  ]
}