{
  "article_text": [
    "`` imagine , for example , a computer that could look at an arbitrary scene , anything from a sunset at a fishing village to grand central station at rush hour and produce a verbal description .",
    "this is a problem of overwhelming difficulty , relying as it does to finding solutions to both vision and language and then integrating them .",
    "i suspect that scene analysis will be one of the last cognitive tasks to be performed well by computers '' .",
    "this fifteen year old quote , attributed to a. rosenfeld @xcite , one of the founders of the field of computer vision , pointed to the fundamental problem of generating semantics of visual scenes .",
    "since then , researchers have attempted a few approaches that mostly centered on asking `` what '' and `` where '' questions about the scene in view . in this methodology , scenes are recognized by detecting the inside objects @xcite , objects are recognized by detecting their parts or attributes @xcite and activities are recognized by detecting the motions , objects and contexts involved in the activities @xcite .",
    "recently , researchers have advanced the viewpoint that if we are able to develop a semantic understanding of a visual scene , then we should be able to produce natural language descriptions of such semantics .",
    "this has given rise to a new area in the field that integrates vision , knowledge and natural language .",
    "knowledge becomes especially important , as without background knowledge , it has become increasingly hard to obtain a desirable level of accuracy in this problem . and",
    "as such knowledge can be often mined from text , the problem now stands at the intersection between computer vision and natural language processing . _",
    "mining such knowledge , storing it in a form that retains the semantics , and reasoning using this knowledge to develop a better understanding of scenes are the fundamental issues that are addressed in this paper .",
    "_    current developments @xcite in computer vision have shown that deep neural nets can be trained to generate a caption for an arbitrary scene with decent success .",
    "it is indeed an exciting achievement .",
    "however , current state - of - the - art image captioning systems still have a few drawbacks such as : 1 ) a brute - force image - to - text mapping makes it inconvenient to conduct logical reasoning beyond just doing inferences from annotated data ; 2 ) due to the lack of intermediate semantic representations , they are all language - dependent ; and 3 ) most importantly , when the system produces wrong results , it is almost impossible to trace back the system and analyze the failure case ( see figure [ fig : examples_stanford ] ) .",
    "let us consider how humans accomplish this task .",
    "human perception is active , selective and exploratory .",
    "we continuously shift our gaze to different locations in the scene . after recognizing objects",
    ", we fixate again at a new location , and so on .",
    "we interpret visual input by using our knowledge of activities , events and objects . when we analyze a visual scene ,",
    "visual processes continuously interact with our high - level knowledge , some of which is represented in the form of language . in some sense , perception and language are engaged in an interaction , as they exchange information that leads to meaning and understanding .",
    "thus , our problem requires at least two modules for its solution : ( a ) a vision module and ( b ) a reasoning module that are interacting with each other . in this paper",
    "we propose to model the early stages of this process .",
    "the available datasets make it impossible to perform experiments that will consider vision as an active process , although this is the ultimate goal .",
    "thus , our question becomes : if the vision module produces a number of ( probabilistic ) detections , how much the reasoning module can infer about the scene if it possesses common sense abilities ?",
    "it turns out that the reasoning module can infer a great deal .    motivated by such intuitions , we present here an effort to integrate deep learning based vision and state - of - the - art concept modeling from commonsense knowledge obtained from text .",
    "we use a deep learning - based perception system to obtain the objects , scenes and constituents with probabilistic weights from an input image . to predict how the objects interact in the scene ,",
    "we build a common - sense knowledge base from image annotations along with a bayesian network capturing dependencies among commonly occurring objects and `` abstract visual concepts '' ( defined later ) .",
    "these two precomputed resources help us infer the following : 1 ) the correct set of correlated objects based on the high - confidence objects detected ; 2 ) the most probable events that these objects participate in ; 3 ) the role that the objects play in this event ; and 4 ) given the events , objects and constituents , the `` concept '' that emerges from such information . based on these inferences ,",
    "we output a scene description graph ( sdg ) that depicts how these different entities and events interact .    in figure [",
    "fig : example_knowledge_structure ] , we show a possible sdg for an example image .",
    "sdg is essentially a directed labeled graph among entities and events that enables an array of possibilities to do further analysis beyond visual appearance , such as event - entity based analysis , question answering about the scene and flexible caption generation .",
    "the fundamental contribution of this work is a novel algorithm that uses automatically constructed knowledge base to create an sdg from an image , which facilitates further reasoning and caption generation .",
    "sdgs have advantages over ground - truth sentences because : 1 ) they can be easily processed by machines / ai systems in comparison to sentences ; 2 ) the output can be rich in information - content ; 3 ) they are not bounded by specific templates , that are often used by researchers to convert labels into sentences and 4 ) the sdg also can be used to generate sentence descriptions .",
    "we also create a knowledge base which captures the knowledge about the commonly - occurring concepts , events and entities .",
    "the knowledge base can be used to provide answers to the following queries : 1 ) the event or set of events that connect two entities ; 2 ) the role an entity plays in an event and 3 ) a subset of all possible concepts involving the entities and connecting events .",
    "lastly , further inferences about the scenes such as `` will the player holding the ball be able to tackle the blocker and under what conditions '' can also be attempted by feeding the sdg output as predicates to reasoning modules along with additional background knowledge .",
    "our work is influenced by various lines of work where researchers have proposed approaches to extract meaningful information from images and videos .",
    "as @xcite suggests , such works can be categorized into 1 ) dense image annotations , 2 ) generating textual descriptions , 3 ) grounding natural language in images and 4 ) neural networks in visual and language domains .    according to the above categorization",
    ", we share our roots with the works of generating textual descriptions .",
    "this includes the works that retrieves and ranks sentences from training sets given an image such as @xcite , @xcite,@xcite , @xcite .",
    "@xcite , @xcite , @xcite , @xcite , @xcite are some of the works that have generated descriptions by stitching together annotations or applying templates on detected image content .",
    "several works have shown promising efforts to acquire and apply commonsense in different aspects of scene analysis .",
    "@xcite uses abstraction to discover semantically similar images .",
    "@xcite proposes to learn all variations pertaining to all concepts and @xcite uses common - sense to learn actions .",
    "recently , @xcite introduced scene graphs to describe scenes and @xcite creates scene graphs from descriptions .",
    "however , we automatically construct the graph from an image , and we believe , due to the event - entity - attribute based representation and meaningful edge - labels ( borrowed from km - ontology@xcite ) , sdgs are more equipped to facilitate symbolic - level reasoning .",
    "the recent development of deep neural networks based approaches revolutionized visual recognition research .",
    "different from the traditional hand - crafted features , a multi - layer neural network architecture efficiently captures sophisticated hierarchies describing the raw data @xcite , which has shown superior performance on standard scene recognition @xcite , object recognition @xcite and image captioning @xcite benchmarks .",
    "* image dataset : * in this paper , we use three image data sets , which are popularly referred to as flickr 8k , flickr 30k and coco datasets @xcite .",
    "these three datasets have 8,092 , 31,783 and more than 160k images respectively .",
    "all the images from these datasets are accompanied with 5 hand - annotated sentences that describe the image .",
    "for all datasets , we used the train - test splits from @xcite and the 4000 testing images ( 1000 each from flickr 8k and 30k ; 2000 from ms - coco validation set ; denoted as @xmath0 ) serve as the testing bed for our reasoning experiments .    * deep object recognition : * we use the trained bottom - up region proposals and convolutional neural networks(cnn ) object detection method from @xcite .",
    "it considers 200 common everyday object classes ( denoted as @xmath1 ) and trained on ilsvrc 2013 dataset .",
    "we apply the method on the testing images(@xmath0 ) and then convert the object detection scores to @xmath2 .",
    "* deep scene recognition : * we use the trained cnn scene classification method from @xcite .",
    "the classification model is trained on 205 scene categories ( denoted as @xmath3 ) and each of the category has more than 5000 training samples .",
    "we apply the method on the testing images and then convert the scene classification scores to @xmath4 .",
    "* constituent annotation collection and deep constituent recognition : * images from the wild can not always be categorized into a limited number of scene categories . however , _",
    "scene constituents _ describing properties or actions of objects , attributes of scenes occur frequently across images and can be utilized to describe the image . in this work ,",
    "we further augment the flickr 8k image dataset with human annotation of constituents using amazon mechanical turks .",
    "we specifically ask the human labeler to annotate not only objects , but what objects are doing or properties of objects .",
    "we allow the labelers to use free - form text for describing constituents to reduce annotation effort . to obtain a standardized set of constituents from the annotations ,",
    "we perform stop - words removal , parts - of - speech processing to retain nouns , adjectives and verbs .",
    "we replace the nouns with their superclasses such as _ man , boy , father _ by _ person _",
    ", and then , we rank the resulting phrases according to their frequencies .",
    "some of the top phrases are _ _ grass , dog run , dog play , kid play , person wear short _ _ etc . for the rest of our processing , we post - process the annotations for each training image and consider them if they are among the 1000 top constituents ( denoted as @xmath5 ) .",
    "recent empirical results from a diverse range of visual recognition tasks indicate that the generic descriptors extracted from the cnn are very powerful @xcite . in this work",
    ", we use a pre - trained cnn from @xcite . for each image in @xmath0 , we use this pre - trained model to extract a @xmath6 dimensional feature vector using @xcite .",
    "we then trained a multi - label svm to do constituents recognition using the deep features .",
    "the trained model is applied on all the testing images and we convert the classification scores to @xmath7 .",
    "the set of @xmath8 makes up the initial visual perception output .",
    "next , we explain the reasoning framework to construct sdgs from noisy labels with the aid of knowledge from text . to provide a better understanding of this complex system , we provide a diagram of the architecture explaining the reasoning process for an example image in figure [ fig : reasoning ] .",
    "as shown , for each image , the above perception system produces object , scene and constituent detection tuples .",
    "each detection is provided with a confidence score . for objects ,",
    "scores are provided for each bounding box .",
    "top five scene labels and top ten constituent detections are considered for the reasoning framework .",
    "most of these detections are quite noisy .",
    "we develop an elaborate reasoning framework to construct sdgs from such noisy detections , with the help of pre - processed background knowledge .      in this phase",
    ", we collect ontological information about object classes in object meta - data table ( @xmath9 ) and scene classes in scene metadata ( @xmath10 ) .",
    "we also store scene detection tuples ( @xmath11 ) and human annotation of images ( @xmath12 ) for all training images .",
    "we create a * knowledge base * @xmath13 , a * bayesian network * @xmath14 and a scenes to abstract visual concepts[multiblock footnote omitted ] ( avc ) mapping table ( @xmath10 ) .",
    "* scene detection tuples ( @xmath11 ) : * we use the perception system of the previous section to create scene detection tuples ( @xmath15 ) of set of training images ( @xmath16 )",
    ". these are used to learn the bayes net @xmath14 .",
    "* image annotations ( @xmath12 ) : * we collect all textual descriptions of the training images provided with image datasets and use them for building the knowledge base @xmath13 and bayes net @xmath14",
    ". however , both can be built using * any repository of sentences * that describe day - to - day concepts .",
    "* object meta - data ( @xmath9 ) : * for each of the 200 object classes , we collected all synonyms , hyponyms and hypernyms .",
    "the list is prepared using wordnet api .",
    "this is dataset - independent and only needs to be augmented when the set of object classifiers expands .",
    "* scenes - to - avcs mapping table ( @xmath10 ) : * for each scene in @xmath3 , we added ontological information involving a set of abstract concepts and a set of synonyms . to obtain the synonyms , we again used wordnet api .",
    "we hand - annotated all the avcs for each scene and learnt a prior belief for each avc in scene from human annotations .",
    "for example , for the scene _",
    "airport_terminal _ , we add _ \\{waiting room , big glass view , people } _ as the list of avcs and _ terminal _ as the synonym ; and learn the priors @xmath17 respectively for avcs . in the following sub - sections ,",
    "we first introduce the reasoning framework briefly , followed by a description of the construction of the knowledge base @xmath13 and the bayesian network @xmath14 .",
    "lastly , we describe our reasoning framework in detail .      equipped with the background knowledge stored in the form of ( @xmath18 ) , we process the objects , scene and constituent detections for an image to construct an appropriate sdg in the following way : i ) we populate synonyms , hypernyms , hyponyms of objects and synonyms , avcs ( with priors ) of scenes ; ii ) ( * scene constituents * : ) we extract entities and events from each constituent . such as",
    ", the constituent _ person wear short _ results in an event _ wear _ with two edges : one labeled _",
    "agent _ joining the entity _ person _ and another labeled _",
    "recipient _ joining the entity _ short _ ; iii ) ( * abstract visual concepts * : ) we choose the avcs iteratively that maximizes the conditional probability given high confidence objects ; iv ) ( * objects * : ) for low - scoring objects , we choose the sibling ( in the hyponym - hypernym hierarchy ) which maximizes the conditional probability given high - confidence objects and avcs ; v ) ( * events * : ) we search the @xmath13 to find the most compatible events that connect pairs of high - confidence objects .",
    "we add the events obtained from * constituents * to this set of compatible events ; vi ) ( * concepts * : ) given the above events and avcs , we search the @xmath13 for concepts that best suits the events and avcs , and we also construct an sdg based on just high - confidence objects , events and avcs .      in figure",
    "[ fig : kparser1 ] , we describe how we construct @xmath13 from a set of image annotations ( @xmath12 ) using the stanford parser and k - parser @xcite . for each sentence , we first parse using the stanford parser to get a dependency graph .",
    "the k - parser then maps these dependency labels using a set of rules to a set of meaningful labels from km - ontology@xcite and the resulting graph is further augmented using ontological and semantic information from different sources ( more details on kparser.org ) .",
    "we then generalize each of these graphs i.e. replace entities by their superclasses .",
    "then we merge them based on overlapping entities and events , and create a single graph ( @xmath13 ) .",
    "@xmath13 is defined as the tuple @xmath19 . @xmath20",
    "denoting set of vertices @xmath21 , set of edges @xmath22 .",
    "each vertex and edge has a label .",
    "each vertex can be of three types : _ events , entities _ and _ traits_.",
    "* events * correspond to verbs , * entities * correspond to superclasses of nouns that directly interact with events and * traits * represent all other nouns .",
    "* edge labels * in the @xmath13 are exactly the same as in the k - parser ( figure [ fig : kparser1 ] ) .",
    "@xmath5 is a set of * concepts * which corresponds to generalized k - parser graphs of sentences and is essentially a sub - graph of @xmath23 .    from flickr8k annotations ,",
    "we process nearly @xmath24 sentences provided for the images ( @xmath25 per each image ) to build the @xmath13 .",
    "a visualization of a part of the @xmath13 is given in figure [ fig : kparser1](b ) .",
    "after parsing the annotated sentences in flickr8k , @xmath13 consists of @xmath26 events , @xmath27 entities and @xmath28 traits .",
    "the total number of edges and distinct concepts in the graph are @xmath29 and @xmath30 .      in this sub - section",
    ", we describe the type of conditional probabilities we estimate and the bayesian network we learn to estimate such probabilities",
    ". we use conditional probability calculations in two of the steps of our approach : inferring the most probable collection of abstract visual concepts and rectifying low - scoring erroneous objects .    for inferring the most probable collection of avcs",
    ", we first make a list @xmath31 of all the frequent avcs ( with frequency @xmath32 in our experiments ) from all scenes detected for a test image .",
    "then we follow algorithm [ algo : inferconcepts ] to get the set of inferred concepts @xmath33 from the set of high - scoring ( score @xmath34 ) are set based on performance on validation data . ] ) entities @xmath35 and the set of scenes @xmath36 detected for image @xmath37 .",
    "we iterate till the entropy keeps decreasing .",
    "@xmath38 @xmath39 @xmath40 break ; @xmath41 @xmath42 ; @xmath43    next , we attempt to rectify the low - scoring entities based on high - scoring entities ( @xmath35 ) and the above @xmath33 . for each low - scoring entity",
    ", we get all its siblings i.e. we get all the children of its hypernyms . for example , if _ bathing cap _ is assigned a low score , the assigned superclass is _ headwear _ and its children are _ headband , hat _ etc .",
    "we calculate the following @xmath44 and then add @xmath45 to the high - scoring entities list ( @xmath35 ) .    as the above paragraphs suggest",
    ", we need to estimate the conditional probabilities : @xmath46 and @xmath47 . to estimate the conditional probabilities , we learn a bayesian network @xmath14 using @xmath12 and @xmath11 .      to capture the knowledge of naturally co - occurring entities and abstract visual concepts , we learn a bayesian network that represents the dependencies among them .",
    "we create the training data @xmath48 which is a set of tuples @xmath49 where @xmath50 is the total number of entities and avcs . each term",
    "@xmath51 is binary and denotes 1 if the @xmath52 entity ( or avc ) occurs in the tuple .",
    "then , we use the tabu search ( tabu ) algorithm to learn the structure and then we populate the conditional probability tables using the r - bnlearn package @xcite .",
    "a subgraph of the learnt bayesian network is shown in the figure [ fig : bn ] .        to create the training data @xmath48 , we process each training image ( in @xmath16 ) , and we automatically detect entities and avcs and then output the tuple @xmath53 .",
    "to detect entities , we parse the image annotations ( @xmath12 ) and extract entities from it .",
    "some of the avcs such as _ people and people wear shorts _ are detected using rule - based techniques .",
    "however , for scenes such as _ airport - terminal _ , it is unlikely that avcs such as _ waiting room _ can be found in human descriptions of an image ; as we tend to describe only the entities and their interactions .",
    "keeping this idea in mind , we ran the scene classifier system from the previous section [ visualprediction ] , and we consider all the avcs of the scene with the highest score ( @xmath54 ) , from the _ scene - to - avc lookup table _ ( @xmath10 )",
    ".      given the most relevant set of abstract visual concepts ( @xmath55 ) and entities ( @xmath35 ) , we find concepts that the image describes .",
    "to do this , we use the @xmath13 to search first for events that these entities ( i.e. objects ) participate in and then we use these events and entities together to search for concepts in the set of concepts @xmath5 in @xmath13 .",
    "we rely on two assumptions about the knowledge base : i ) @xmath13 reflects a more - or - less complete view of the relevant world knowledge and hence we can find the most suitable events from it .",
    "this assumption is valid if the images come from the same domain ; such as in our examples , we have used the flickr8k dataset and the domain corresponds to pictures of humans and dogs in natural setting ; and ii ) @xmath13 contains all concepts possible with the given events and entities .",
    "this is a strict assumption , which might not be true even if we parse the whole web . to alleviate the problem , we give two final outputs : i ) an sdg involving the entities , avcs and events and ii ) another sdg of the top concept that is obtained from @xmath5 in @xmath13 . * search connecting events * : the motivation behind building a knowledge base was to logically explain why certain co - occurring events are suitable for the combination of entities .",
    "for example , consider the entities _ person _ and _ swimming trunks_. note , _ swimming trunks _ corresponds to the vertex _ trunk _ in @xmath13 .",
    "we get events such as sniff , climb , wear etc . , i.e. , some corresponding to tree - trunk and others to swimming - trunks . to logically find suitable events , we find all connecting events from @xmath23 in @xmath13 and then filter spurious events based on ontological and background knowledge from @xmath9 and @xmath5 in @xmath13 .    for a pair of entity in @xmath35",
    ", we traverse the path from one entity to another in the graph @xmath23 and consider event - nodes on the path .",
    "as shown in figure [ fig : kparser1](b ) , two entities can be connected by an event . however , in some cases , they could be connected by a chain of events and entities .",
    "we employ a greedy breadth - first search over the graph @xmath23 for such pairs .",
    "we denote the set of entities that are related to each other by some event , by @xmath56 . for filtering spurious events ,",
    "we introduce the notion of _ edge - compatible events_. an event is * edge - compatible * with respect to two entities if they are connected to the event using edges with compatible labels .",
    "as these labels are well - defined relations between entities and events from km - ontology , the label - compatibility is easy to observe .",
    "for example , _",
    "( agent , recipient ) _ is a compatible pair and only an animate entity can be an _ agent_. based on the rules , the event _ wear _ is edge - compatible with respect to entities _ person _ and _ trunk_. even after this , we still obtain events like _ climb _ etc . to filter such events",
    ", we consult the table @xmath9 and the set of concepts @xmath5 . we know that the entity _ swimming trunks _ belongs to the superclass _ clothing _ , and",
    "hence we retain only those events that are connected to an entity _ trunk _ which is of the same superclass , in some concept in @xmath5 .    * sdg construction : * after obtaining a set of suitable events ( such as _ wear _ ) , we construct an sdg using the following set of rules : i ) add _",
    "has(scene , component , @xmath57 ) _ for all avc @xmath57 in @xmath33 ; ii ) add _ has(event , location , scene ) _ for the top detected events ; iii ) add all compatible edges related to the events such as _ has(wear , agent , person ) _ and _ has(wear , recipient , trunk ) _ ; and iv ) for all entities @xmath58 in @xmath59 , do the following : if it is an animate entity , add _",
    "has(@xmath58 , location , scene )",
    "_ ; otherwise , find the shortest path from @xmath60 to the top detected event in the @xmath13 and add the edges on the path to the sdg . * search concepts * : given the events and entities ( @xmath56 ) , we search the set of concepts @xmath5 in @xmath13 .",
    "recall , in the @xmath13 , a concept is a generalized k - parser graph of a sentence .",
    "we consider a concept as candidate if all edges from a detected edge - compatible event are present in it .",
    "next , we weight each candidate concept using the remaining entities in @xmath59 and avcs ; i.e. , increase a counter if an entity or avc occurs in the graph .",
    "we also calculate a joint confidence - score for each concept based on the @xmath8 values of the object , scene and constituents present in the concept . based on the counters and the joint confidence - score",
    ", we rank the concepts .",
    "* template based sentence generation * : we generate textual descriptions from the sdg using the simplenlg@xcite package .",
    "for example , for the edges _",
    "has(wear , agent , person ) _ and _ has(wear , recipient , shorts ) _ , we will generate the sentence `` _ _ a person is wearing shorts _ _ '' . based on the edge - labels ( labels from km - ontology ) we populate the verb , subject , object and adjectives ( including quantitative such detections of an object @xmath61 , we generate sentences like _",
    "@xmath50 @xmath61 s are in the scene_. ] ) of sentences using simple rules .",
    "it should be noted that these k - parser labels are a direct mapping from the set of stanford dependencies , and theoretically we can populate all the parts - of - speeches of a sentence from the sdg .",
    "herein lies the effectiveness of producing an sdg from an image .",
    "the knowledge - structure representing a scene should be rich in information - content and should carry enough semantics to describe the image .",
    "we adopted three sets of experiments .",
    "first , we detect the accuracy with which our system can detect events and entities present in the image .",
    "we perform a qualitative evaluation ( `` relevance '' and `` thoroughness '' ) of the textual descriptions generated from sdgs with the sentences generated by @xcite using the amazon mechanical turkers ( amt ) . and",
    "lastly , to evaluate the image - sentence alignment quality , we design an image retrieval task and report our results on image search based on generated annotations . to conclude ,",
    "we provide a few example images and their sdgs .    for comparison purposes",
    ", we use the implementation from @xcite to generate a textual caption @xmath62 for each testing image .",
    "the method is based on a combination of cnn over image regions , bidirectional recurrent neural networks over sentences , and a structured multimodal embedding .",
    "we denote the set of captions as @xmath63 .",
    "* training phase : * our model can be represented by the tuple @xmath64 . among these ,",
    "@xmath65 are collected and stored once , and re - used for all datasets . for our experiments ,",
    "we re - use the same bayesian network @xmath14 learnt from flickr8k data for all the datasets .",
    "though , we build the @xmath13 each time from the annotated sentences , this can be easily avoided by using the same @xmath13 for all the datasets .",
    "in essence , for the reasoning part , we donot require any training at all for new datasets .    * entity and event detection accuracy : * for this experiment , we extracted entities and events ( gold - standard ) from constituent annotations for the 1000 test images of flickr8k .",
    "we manually checked them to remove noise . to provide a baseline , we also extracted entities and events from @xmath63 automatically using k - parser .",
    "subsequently , we compared the gold - standard with entity - event set from @xcite and the sdg output from our system for each image .",
    "the statistics of our evaluation is given in table [ table : stats_table ] .",
    "* amt evaluation of generated sentences : * since sentence generation to describe a scene is innately a creative process , a good metric is to ask humans to evaluate these sentences .",
    "the evaluation metrics : relevance and thoroughness , are therefore proposed as empirical measures of how much the description conveys the image content ( relevance ) and how much of the image content is conveyed by the description ( thoroughness ) .",
    "we engaged the services of amt to judge the generated descriptions based on a discrete scale ranging from 15 ( low relevance / thoroughness to high relevance / thoroughness ) .",
    "the average of the scores and their deviation are summarized in table  [ tab : sengen_res1 ] for flickr8k , flickr30k test images and ms - coco validation images . for comparison",
    ", we asked the amts to also judge one random gold - standard description and the output from @xcite , a state - of - the - art image captioning system .    in our experiments",
    ", we found that @xmath13 from flickr8k annotations can be used for flickr30k without much effect on accuracy .",
    "however , for ms - coco datasets , @xmath13 from flickr8k annotations falls short of producing a desired accuracy as the coco data is much more varied .",
    "* image - sentence alignment evaluation : * similar to the experiments in @xcite , we also evaluate the image - sentence alignment quality using ranking experiments .",
    "we withhold the set of testing images and use the generated sentences as queries .",
    "we process the textual query and construct @xmath66 using the same procedure by which we construct @xmath13 . for each image",
    ", we take the sdg @xmath67 and calculate similarity between the sdg and the query using the following formula :    similarity between two vertices are calculated based on their word - meaning similarity and neighbor similarity . here",
    "@xmath68 is wordnet - lin similarity @xcite between two words and @xmath69 is the standard jaccard coefficient similarity . based on the above similarity measure , we give the image retrieval results compared with few of the state - of - the - art results in table [ retrieval ] .",
    "one of the primary contributions of our work is the knowledge - structure representation that bridges the gap between semantic information in text and images . from the results of this experiment",
    ", the benefit of having such an intermediate representation is easy to observe .",
    "* example images and sdgs : * as examples , we pick a few images which produces objects and scene recognitions with comparably good confidence scores .",
    "the images and their corresponding sdgs are provided in figure [ fig : graph_rich ] .",
    "as we can observe , the information produced by these sdgs are easily processed by machines .",
    "we can answer questions such as _ how entities interact in an event , which possible events are in the scene and how entities interact in a scene_. we should also mention that the concept - level modeling provided by sdgs is what separates this work from other recent approaches @xcite . furthermore , comparing these structures with the k - parser output in figure [ fig : kparser1 ] , we can see how the sentences and images can seamlessly converge to such space of graphical representations .",
    "this could have huge repercussions in search in image and textual space and storing knowledge from images and text together in a unified knowledge base .",
    "this paper introduced a reasoning module to generate textual descriptions from images by first constructing a new intermediate semantic representation , namely the scene description graph ( sdg ) , which is later used to generate sentences .",
    "the reasoning module uses an automatically constructed knowledge base created from text , to capture `` commonsense '' knowledge .",
    "having built the knowledge base , we proposed a method of obtaining such sdgs from noisy labels using our prediction system .",
    "the sdg is a representation of the scene in view that integrates direct visual knowledge ( objects and their locations in the scene ) with background commonsense knowledge .",
    "in addition , the sdgs have a structure similar to semantic representations of sentences , thus facilitating the interaction between vision and natural language .",
    "the notion of the sdg has great potential .",
    "here we used the sdg for the automatic creation of sentences describing the scene ; but , equipped with background knowledge , it also allows reasoning and question / answering about the scene .    to demonstrate the effectiveness of the sentences and constructed sdgs , we performed a number of experiments .",
    "our amt evaluations on popular datasets show that our sentences performs comparatively well with respect to the state - of - the - art in measures of relevance and thoroughness .",
    "a gold - standard based evaluation shows that our output sdgs can detect events and entities with comparable accuracy as a state - of - the - art system . and",
    "lastly , our image retrieval experiment shows that the image - sentence alignment quality is comparable with state - of - the - art results ."
  ],
  "abstract_text": [
    "<S> in this paper we propose the construction of linguistic descriptions of images . </S>",
    "<S> this is achieved through the extraction of scene description graphs ( sdgs ) from visual scenes using an automatically constructed knowledge base . </S>",
    "<S> sdgs are constructed using both vision and reasoning . </S>",
    "<S> specifically , commonsense reasoning is applied on ( a ) detections obtained from existing perception methods on given images , ( b ) a `` commonsense '' knowledge base constructed using natural language processing of image annotations and ( c ) lexical ontological knowledge from resources such as wordnet . </S>",
    "<S> amazon mechanical turk(amt)-based evaluations on flickr8k , flickr30k and ms - coco datasets show that in most cases , sentences auto - constructed from sdgs obtained by our method give a more `` relevant '' and `` thorough '' description of an image than a recent state - of - the - art image caption based approach . </S>",
    "<S> our image - sentence alignment evaluation results are also comparable to that of the recent state - of - the art approaches . </S>"
  ]
}