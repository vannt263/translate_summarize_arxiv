{
  "article_text": [
    "in the `` small @xmath0 large @xmath1 '' regression setting , the task is to make predictions based on few noisy samples of high - dimensional data .",
    "it is impossible to address this problem without relying on prior knowledge .",
    "typically , prior knowledge is represented by known structures in data , such as groupings of variables in pathways , or sparsity .",
    "still , for the extreme case of @xmath3 , none of them is sufficient , and not even their combination . in this case , a potential source of additional useful knowledge comes from human expertise , which is usually expensive to extract . in this paper",
    "we address the problem of how to efficiently elicit expert knowledge , under a restricted feedback budget , making the simplifying assumption that the user is able to provide exact information to queries .",
    "`` small @xmath0 large @xmath1 '' data ( also known as `` fat data '' ) is characterised by a large number of predictors @xmath4 that need to be estimated from few data ( small sample size @xmath0 ) .",
    "this situation is typical in medical data , where observations ( such as drug responses ) are very scarce , a very large number of potentially relevant covariates is available , from genomics measurements for instance , and additional data can only be obtained at a high cost .    to tackle this problem , and to efficiently constrain the selection of relevant features , machine learning algorithms typically rely on known structures in the data ( for instance , networks , pathways , the linear structure of dna ) .",
    "this type of prior knowledge can be taken into account with regularisation techniques ( see , e.g. ,  @xcite ) or priors in bayesian inference .",
    "another efficient way to address the lack of data is to transfer knowledge across related tasks ( see  @xcite for a survey )",
    ".      human judgement , and in particular expert knowledge , is often of crucial importance in decision making processes .",
    "expert knowledge elicitation techniques have been widely studied in a wide range of application settings , from preference model elicitation @xcite to medical science and shipping industry  @xcite , as well as interactive learning for student modelling  @xcite .",
    "the methodological choices needed in defining the expert feedback differ from application to application and depend on multiple conditions , such as ( i ) the available expert knowledge ( e.g. , what can one ask from the expert ? how much should the expert be trusted ? ) ; ( ii ) the type of information to be obtained ( e.g. , learning a preference , estimating a quantity , answering a question , identifying risky options ) ; ( iii ) knowledge extraction constraints ( e.g. , time / cost needed to get the answer , how many interactions / how much feedback can the expert provide ) .    eliciting coefficients for linear regression methods has been shown to be efficient in previous related studies .",
    "an important line of work @xcite studies methods of quantifying subjective opinion about the coefficients of linear regression models .",
    "the prior knowledge is elicited through tasks that use hypothetical data and the assessment of credible intervals .",
    "these elicitation methods are shown to obtain prior distributions that represent well the expert s opinion , but the use of expert knowledge is not explored further . in our approach , we elicit expert s opinion more directly and this also allows to focus on more specific cost functions ( such as reducing the prediction error for a specific target ) .    in bayesian inference",
    "the prior distributions of the parameters are a natural way of expressing prior knowledge . in the studies on prior elicitation @xcite , some also on regression @xcite ,",
    "the focus has often been on how to elicit reliable prior knowledge , after which the bayesian inference machinery takes care of the rest .",
    "we ask the complementary question , of how efficiently can the knowledge elicitation be done , first given the simplifying assumption that the expert feedback is reliable .",
    "the order of inference is also reversed : we initialize from data and then improve with knowledge elicitation , whereas in prior elicitation the normal order would be the opposite . in the next stages of development , it will be important to combine both approaches , and then the bayesian formulations will be natural .      in this work",
    "we propose to solve the `` extremely small @xmath0 , large @xmath1 '' problem for regression under the assumption that accurate expert knowledge is available but under a budget . as far as we know ,",
    "this is the first study handling knowledge elicitation from this angle , aiming at sample sizes of @xmath2 .",
    "this setup is important in particular for personalized medicine but not restricted to it .",
    "the remainder of the paper is organised as follows : in section  [ s : prelim ] we provide a detailed description of the expert knowledge elicitation scenario in which we address the lack of data problem in a regression task .",
    "we also state our assumptions about the use of the expert feedback .",
    "we then propose an effective algorithm for selecting on which features to ask expert feedback to reduce the loss the most , in section  [ s : algo ] .",
    "we analyse its optimality . in section  [",
    "s : experiments ] we present simulations of the behaviour of our expert knowledge elicitation strategy in a simple synthetic setting .",
    "then , by simulating a knowledgeable user , we show the potential prediction improvement when using a genomics dataset , relevant for the personalized medicine settings . to demonstrate the more general use of the algorithm that we propose , we also study its behaviour for model extensions including : a restriction of the features on which the expert can provide feedback ( section  [ s : extension_subset ] ) and by including noisy expert feedback ( section  [ s : extension_noisy ] ) .",
    "finally , we conclude and provide future work directions in section  [ s : conclusions ] .",
    "in this section we present the problem setup and introduce the first formulation for expert knowledge elicitation in a prediction task . in particular , we explicate our assumptions about the type of feedback that the expert can give .",
    "the framework was chosen to be as simple as possible while still capturing the essential elements of large @xmath1 , small @xmath0 data .",
    "for concreteness , we will describe the problem with terminology of treatment effectiveness prediction , but the setup is naturally more generally applicable .      the goal is to improve prediction of the effect of a treatment on a target patient , by including feedback provided by an expert . assume a small set of observation data , which can be used to learn an initial predictor .",
    "the set consists of @xmath0 observed treatment responses @xmath5 , stored in the vector @xmath6 , coming from @xmath0 different patients @xmath7 ( @xmath8 ) who had previously received the same treatment .",
    "denote the matrix of genomic features with @xmath9 , where the size of @xmath9 is @xmath10 , and on each row @xmath11 we have the @xmath1 genomic features corresponding to patient @xmath7 , denoted by @xmath12 .",
    "we focus on setups with @xmath13 . for",
    "the new `` target '' patient the same genomic measurements are available , denoted @xmath14 $ ] , and the goal is to predict as accurately as possible the treatment response , denoted @xmath15 .      [",
    "[ linear - regression . ] ] linear regression .",
    "+ + + + + + + + + + + + + + + + + +    we assume there is a linear relation between the genomic features of the patient and the expected result of the treatment .",
    "more precisely , we choose a linear regression setting , where for each patient @xmath7 , @xmath16 where @xmath17 is an unknown parameter underlying the linear function and @xmath18 is i.i.d white noise , quantifying the inherent noise in the measurements of the drug effects for each patient .",
    "the coordinate @xmath19 of the parameter vector encodes the weight or relevance that feature @xmath7 has in computing the treatment response of a patient .    [",
    "[ sparsity . ] ] sparsity .",
    "+ + + + + + + + +    we assume that the weight vectors @xmath20 are @xmath21-sparse @xmath22 , or in other words that many of the features have zero weight or relevance in the drug response prediction .",
    "note that sparsity is not necessary for expert knowledge elicitation , but is a widely used regularity assumption which enables handling even larger @xmath1 .",
    "sparsity assumption matches many problems having a small number of responsible mechanisms ( for instance , mutations in the genetic cases ) .",
    "[ [ small - textbfn - and - mathbfn1-scenarios . ] ] `` small @xmath23 '' and `` @xmath24 '' scenarios .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    an important data assumption is whether the true weight of a feature is the same for all patients ( that is , do all observations @xmath25 come from the same distribution ) , or whether the target patient may be from a different distribution .",
    "we call the former the _ `` small @xmath0 scenario '' _ and the latter the _ `` @xmath26 scenario''_.    a particularly useful variant of the @xmath26 scenario is the _ `` multiple @xmath26 tasks scenario '' _ , where each patient has his / her own ( unknown ) weight vector ; for @xmath27 , @xmath28 , and @xmath29 where @xmath18 and @xmath30 come from @xmath31 . if we are willing to assume that all considered patients share the same sparse support of active genomic features ( that is , features affecting the drug response ) and that their corresponding weights are similar from patient to patient , then we can estimate an initial estimate for the weights @xmath32 of the target patient from the other patients data .",
    "we next explicate our assumptions about the information that can be extracted from an expert in the linear regression scenario .      for clinical and behavioural variables , an expert may know how much they explain of the risk . for linear regression ,",
    "the fraction of variance explained is the square of the correlation coefficient ( under simplifying assumptions ) . when the expert is more uncertain ,",
    "he / she can give feedback on the importance of variables .",
    "for instance , some cancer genes and pathways are known , and given the patient s treatment response history , it is possible to make educated guesses of which hypotheses of disease mechanisms remain as potential hypotheses . in these cases",
    "our formulation is an approximation for bringing in expert s patient - specific prior knowledge .",
    "[ [ expert - knowledge . ] ] expert knowledge .",
    "+ + + + + + + + + + + + + + + + +    we assume that the expert is able to report the correct value of @xmath33 when asked , but that answering requires a cost and hence we can not simply ask correct values for the full @xmath32 .",
    "this kind of feedback is very informative and , as far as we know , has not been used in estimating parameters before .",
    "this assumption is very simplifying in the personalized medicine case , and requires that the expert either has important additional knowledge of the particular patient , or is able to use his / her expertise to infer the correct value from the shown data @xmath34 and the initial weight vector estimated based on the other patients .",
    "we later relax this assumption and provide an empirical study of the sensitivity to expert errors ( section [ s : extension_noisy ] ) and to expert knowledge restricted to a subset of features ( section [ s : extension_subset ] ) .",
    "[ [ feedback - use . ] ] feedback use .",
    "+ + + + + + + + + + + + +    in the simple formulation assuming accurate experts , the best way of taking into account the expert feedback on a feature @xmath7 is to directly replace the feature weight of the estimated target parameter @xmath35 ( obtained from the data of other patients ) with the target - specific weight provided by the expert . basically , if the expert gives feedback on @xmath33 , then we update the initial `` small @xmath0 '' estimate ( denoted @xmath35 ) by replacing its @xmath7-th coordinate with the feedback provided by the expert .",
    "[ [ feedback - cost . ] ] feedback cost .",
    "+ + + + + + + + + + + + + +    we assume this type of expert knowledge to be very expensive and we hence place a strict restriction on the number of features on which the expert can provide feedback .",
    "denote by @xmath36 the number of features for which the expert can give target - specific information ( that is , the corresponding weight to be considered in estimating the drug response of the target patient ) .",
    "we refer to @xmath36 as the _ feedback budget _ and we restrict it to a value much lower than the dimensionality of the data : @xmath37 . therefore",
    ", assuming the user does have the answer , the research problem we address here is that of identifying the @xmath36 most informative features on which to elicit expert knowledge .",
    "as our goal is to predict the drug response for patient @xmath34 , it follows that the most informative feedback the expert can provide is what leads to minimizing the prediction error .",
    "we formalize this in the performance measure defined below .",
    "let @xmath38 denote the estimate of @xmath32 produced by an algorithm @xmath39 .",
    "we define the loss of @xmath39 as the expected quadratic loss for the target patient @xmath34 : @xmath40 = { \\mathbb e}[(x^{*}{\\hat\\theta}_{{\\mathcal a}}^\\top - x^{*}\\theta^{*\\top})^2],\\end{aligned}\\ ] ] where the expectation is taken over all sources of noise , coming from the noisy drug effect observations and noisy selection of the features by the expert . given the @xmath36 interactions with the expert user , our goal is to get feedback about the most * informative or relevant * features , such that we minimize the target loss .",
    "the optimal algorithm @xmath41 is thus defined as : @xmath42 it is important to highlight that the overall performance of an algorithm strictly depends on the estimate obtained from the training data on other patients @xmath35 . in this work",
    "the focus is on finding expert knowledge elicitation strategies that enable maximally improving the target prediction , from the initially very imperfect estimate obtained from scarce data , possibly coming from different distributions .",
    "we propose to learn the regression parameters in two stages .",
    "first , an initial estimate @xmath43 is learnt on the `` small @xmath0 large @xmath1 '' training data with appropriate regularization , efficiently capturing the information in that data set . the estimate",
    "is then improved by knowledge elicitation from an expert .",
    "the estimation error obtained with the initial estimate @xmath43 is the baseline for comparing the potential improvement obtained by eliciting expert knowledge .      given the assumed type of feedback ( described in section  [ s : expert_knowledge ] ) and the budget constraint , the goal is to rapidly identify the features most useful in reducing the loss . after a closer look to the loss definition",
    ", we easily get the following intuitions on the behaviour of the algorithm :    * trivially , the algorithm should not ask feedback more than once on the same feature @xmath7 , since the expert feedback is accurate and we get directly the right value for @xmath33 . * by decomposing the loss as a sum of @xmath1 squared point - wise products , we can tell that it is most useful to reduce to 0 ( through the expert feedback ) the @xmath36 largest feature products @xmath44 , which would correspond to getting feedback on the worst estimated features in @xmath43 .    of course , the true vector @xmath32 is not available and the best proxy we can have for it is given by @xmath45 therefore , we propose to use the available information on @xmath34 and the estimate of the weight vector , and we ask for expert feedback on the @xmath36 largest features , as given by the point - wise product of @xmath34 and the @xmath43 .    for each feature @xmath46 , we replace the coordinate @xmath47 by the corresponding expert feedback .",
    "the rest of the @xmath48 features remain the same .",
    "the algorithm outputs the vector thus obtained .",
    "we denote the result by @xmath49 and the loss will be given by @xmath50 $ ] .",
    "the pseudo code of the algorithm is presented in algorithm  [ alg : expert - elicitation - regression ] .",
    "* input : * @xmath51  @xmath0 training data samples in @xmath52 +  @xmath34  feature vector representing the target +  @xmath36  expert elicitation feedback budget + * initialization phase * + output : @xmath43  estimated weight vector from data * expert elicitation phase * rank point - wise products of features elicit feedback on each of the @xmath53-th largest product feature get the corresponding @xmath32 coordinate : @xmath54 replace feature in the initial estimate : @xmath55 output @xmath49 * goal : * minimize   @xmath56",
    ".      we will next give conditions under which the elicitation sequence of algorithm  [ alg : expert - elicitation - regression ] is optimal .",
    "denote @xmath57 , and index the feature with the largest product of feature value and regression weight with @xmath58 .",
    "the theorem below says essentially that assuming changes in regression parameters from learning set to the target patient are on average monotonically increasing as a function of the parameter size , choosing the largest product @xmath58 decreases cost function more than any other choice on the average .",
    "an additional requirement is that if there are correlations between parameters , they can not be stronger for others compared to @xmath58 .",
    "the averages are over variation in the learning data set .",
    "denote @xmath59 and assume @xmath60 \\geq e[\\delta_i^2]$ ] and @xmath61 \\geq e[\\delta_i\\delta_k]$ ] for all @xmath62 .",
    "then it holds that @xmath63 where @xmath64 $ ] and @xmath65^\\top$ ] .    since @xmath66 $ ] , @xmath67 \\\\                      & = l_\\theta - 2 e[\\delta_i \\sum_k \\delta_k ] + e[\\delta_i^2].\\end{aligned}\\ ] ] hence , @xmath68 - e[\\delta_i^2 ] - 2e[(\\delta_c -\\delta_i ) \\sum_k \\delta_k].\\end{aligned}\\ ] ] the expression within the last expectation is @xmath69 and therefore @xmath70 - e[\\delta_c^2 ]           - 2 \\sum_{k \\neq c , i } ( e[\\delta_c \\delta_k ] - e[\\delta_i \\delta_k ] )           \\leq 0\\end{aligned}\\ ] ] by the two assumptions .",
    "[ [ optimality - in - a - simple - setting . ] ] optimality in a simple setting .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we next illustrate the theorem in the simple setting where the target patient comes from the same distribution as the other patients .",
    "then under reasonable regularity assumptions @xmath71 = \\theta^*$ ] , and @xmath72 & =        x^2(i ) e[\\theta^2(i ) ] -2 x^2(i ) e[\\theta(i ) ] \\theta^*(i ) + x^2(i ) \\theta^{*2}(i ) \\\\      &",
    "= x^2(i ) ( e[\\theta^2(i ) ] - \\theta^{*2}(i ) ) \\\\      & = var[x(i)\\theta(i)].\\end{aligned}\\ ] ] skipping analogous details , @xmath73 = x(i)x(k)(e[\\theta(i ) \\theta(k ) ] - \\theta^*(i)\\theta^*(k)).\\end{aligned}\\ ] ] hence the assumptions translate in this case to the intuitive requirements that variance in the largest features is the largest , and features are either not cross - correlated , or if they are , cross - correlations with the largest feature are the strongest ( on average ) .",
    "we illustrate the performance of algorithm  [ alg : expert - elicitation - regression ] , to which we refer in the sequel by largest product feature , in two experimental setups .",
    "we start with a simple synthetic setting ( described in section  [ s : synthetic ] ) , then we use a genomics dataset for a more elaborate simulation ( as described in section  [ s : dataset ] ) . in both settings",
    "we compare the loss of algorithm  [ alg : expert - elicitation - regression ] to that of the following strategies :    * no interaction : the baseline algorithm whose performance is given by the prediction error of @xmath43 * random : works by selecting at random ( without repeat ) @xmath36 features of which to ask expert feedback * largest target feature : asks feedback on the @xmath36 largest coordinates of the target vector @xmath34 .",
    "while no interaction and random are typical baselines , largest target feature is a naive approach of minimising the target loss , based solely on the absolute feature values of the target vector .      [ [ setting . ] ] setting .",
    "+ + + + + + + +    we randomly generated the training set @xmath74 having @xmath75 rows and 150 features from a normal distribution with mean 0 and variance 1 .",
    "we also randomly sampled a sparse weight vector @xmath20 , such that 5 of its features are non - zero and come from a normal distribution with mean 0 and variance 1 , while the remaining 145 features are 0 .",
    "the output variable @xmath76 is then computed using @xmath0 noisy observations of the dot product between @xmath20 and randomly selected vectors @xmath77 .",
    "we use the glmnet package  @xcite for estimating @xmath78 , and we vary the number of training samples from 5 to 30 , while the number of expert feedbacks that we assume we can obtain grows from 0 to 10 . we randomly choose a target patient and compute the corresponding loss for each feedback value .",
    "we plot the average target loss over 100 randomly selected target patients .",
    "[ [ results . ] ] results .",
    "+ + + + + + + +    figure  [ fig : synthetic_single_theta ] shows the prediction performance in terms of a loss for a target sample in the `` small @xmath0 '' scenario , with four different strategies . as expected , given that the expert is assumed to be able to give exact feedback , all strategies that use expert feedback show improvement in performance as the number of expert feedback grows and have better performance than the baseline ( no interaction ) from the very first expert feedback .",
    "it can also be noticed that the initial estimate of the weight vector resulting using largest product feature , after the first feedback is already better then the other strategies .",
    "then , with increasing amount of feedbacks received , our algorithm shows increased improvement in performance as compared to other strategies .    in the `` @xmath26 '' scenario , with different regression parameter vectors for each patient ,",
    "the improvement is slightly slower ( because of the introduced bias in the initial estimate @xmath43 ) but still clearly better than with the other strategies ( figure  [ fig : synthetic_multi_theta ] ) .",
    "it is important to mention that the @xmath20s ( @xmath79 , one corresponding to each patient in @xmath74 ) were randomly generated , but to control the introduced bias we keep the same sparsity assumption for all weight vectors ( the same 5 nonzero features ) and we restrict the @xmath80 norm of the difference between any pair of weight vectors @xmath28 to be smaller than 0.5 .    for both scenarios ,",
    "here we only report the loss when the number of training samples used for computing @xmath81 is 10 , but in appendix  [ s : appendix ] we report more complete results .",
    "we demonstrate the usefulness of our approach by also testing it on the genomics data in the gdsc dataset . here",
    "we obtained results similar to the results seen from the synthetic data . in the following",
    "we briefly describe the contents of the dataset , then we explain how we simulated the ground truth .",
    "the plotted results ( figure  [ f : realdata1 ] ) follow the same trends as in the synthetic data , again showing an improvement of all the strategies that use interaction , for all values of @xmath0 .    [",
    "[ genomics - of - drug - sensitivity - in - cancer - gdsc - data . ] ] genomics of drug sensitivity in cancer ( gdsc ) data .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we used the data from the genomics of drug sensitivity in cancer project by wellcome trust sanger institute ( version release 5.0 , june 2014 , http://www.cancerrxgene.org ) @xcite consisting of @xmath82 drugs and a panel of @xmath82 human cancer cell lines for which complete drug response measurements are available .",
    "drug responses are summarised by log - transformed ic50 values ( the drug concentration yielding 50% response , given as natural log of @xmath83 ) from the dose response data measured at 9 different concentrations .",
    "the cancer cell lines , representative of human cancer cell models , are characterised by expression values quantifying the transcript levels of thousands of genes . for this study , we chose a subset of biologically relevant genes , whose mutational status has been shown to correlate with the drug responses  @xcite . for our study , we transformed the transcript counts of the genes to the @xmath84 scale .",
    "[ [ learning - a - pseudo - ground - truth . ] ] learning a pseudo - ground truth .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    when simulating expert feedback , on this data we used estimates computed from the full data as the correct answers the expert gives , when queried based on estimates from small @xmath0 . to learn this `` pseudo - ground truth , '' we employed sparse linear regression using the _ glmnet _ package , which has been frequently used to identify genomic features of drug responses  @xcite .",
    "the sparse linear regression formulation has two parameters that are to be optimized : @xmath85 ( elastic net mixing parameter ) and @xmath86 ( the penalty parameter ) .",
    "we fixed @xmath87 assuming only few of the genomic features to be important for predictions .",
    "we inferred @xmath86 as follows : for each drug , we held out one cell line and performed a 10-fold cross validation procedure on the training data with 100 different @xmath86 values .",
    "the training data comprised of the gene expression values of all cell lines and their responses on this drug , except for the held - out cell line .",
    "we chose @xmath88 that gave minimum error averaged over the 10 cross - validated folds .",
    "the estimates of @xmath20 were obtained by setting @xmath89 .",
    "we repeated this procedure for all drugs , thus the learnt @xmath20s are used as a pseudo - ground truth for our experiments .",
    "[ [ results.-1 ] ] results .",
    "+ + + + + + + +    for the plots on the gdsc dataset , for each amount of feedback and for each algorithm , the curve represents the average loss over 100 random iterations .",
    "the bars are showing the standard error of the mean . in each iteration",
    ", we randomly pick a set of 10 target patients and 10 drugs and we compute their corresponding losses . for each of the target patients we predicted the drug responses eliciting the expert knowledge of the user following the strategies presented in the beginning of section  [ s : experiments ] .",
    "we also show the effect of varying the number of initial training samples , from 5 to 30 .",
    "while the trends and the ordering of the performance of the algorithms do not change , we can notice that , as expected , the overall loss of the algorithms diminishes as @xmath0 grows .",
    "in addition , when using the gdsc dataset the loss does not decrease to zero as fast as for the synthetic data .",
    "to get more evidence about the behaviour of the algorithm , we proceed by relaxing the model in two central aspects .",
    "these relaxations also make the model applicable in a considerably wider set of practical scenarios . for the numerical simulations ,",
    "unless specified otherwise , the setting remains the same as described in section  [ s : synthetic ] , for the `` n=1 '' scenario .",
    "we analysed the prediction performance of our approach when the expert can provide feedback only on a subset of features .",
    "to simulate that , we associate to each feature a value of either 1 , meaning that the expert has knowledge on that feature ( and can provide feedback ) , or 0 , for the features on which the expert has no prior knowledge . the algorithm largest product ( subset ) feature selects the features on which to ask feedback as before , but if the selected feature is unknown by the expert ( that is , its associated value is 0 ) , then feedback is asked on the next `` largest product feature '' with an associated value 1 .",
    "basically , instead of receiving feedback on the @xmath36 largest features , we now receive expert feedback on the @xmath36 `` largest product features '' with associated value 1 .",
    "the analysis in section  [ s : proof ] also holds for this more general setting , under the same assumptions .",
    "in fact , we again proceed by asking expert feedback on the features that allow to decrease the cost function more than any other choice ( on average ) , since the features on which the expert has no knowledge would have no impact on the loss ( if selected ) .",
    "thus the algorithm preserves optimality , since by using the feedback budget on the @xmath36 `` largest product feature '' with an associated value 1 , we obtain the largest expected loss reduction .",
    "figure  [ f : synthetic_subset_feedback ] shows how the performance behaves when the percentage of features with associated value 1 is reduced from 90% to 50% .",
    "( with overlapping green and magenta curves ) . ]",
    "each subplot corresponds to a different proportion of features ( selected randomly ) on which the expert has prior knowledge and can provide feedback . for instance `` feature subset=90% '' means that expert feedback is available on 90% of the features in the curve largest product ( subset ) feature .",
    "based on the results , even in the more realistic cases in which the expert knowledge is restricted to a subset of the features , the results obtained by largest product ( subset ) feature are significantly better than the baseline no interaction .",
    "note that the other strategies remain unchanged , with no constraint on the features on which expert feedback can be received , and yet largest product ( subset ) feature outperforms them even with half of the total number of features .",
    "we simulated an experiment for the `` n=1 '' scenario where each feedback is affected by normally distributed noise , centered and with variance between 0.1 and 0.5 , encoding the expert uncertainty ( the range of the ( true ) feature values is [ 0 , 1 ] ) .",
    "the noisy feedback is used for all strategies . in the personalized medicine application , it is plausible to assume that the expert feedback has smaller variance in the weight of genomic features commonly known to be relevant , but much higher variance for rarely encountered genes and their mutations .    in figure  [",
    "f : synthetic_noisy_expert ] we can see that largest product feature is still better than the baseline no interaction in the presence of noisy feedback . though as the noise increases the difference between the prediction performance of our algorithm and the baseline decreases .",
    "this is expected , since as the expert provides noisier feedback on a feature @xmath7 , @xmath90 might deviate even more from the true value of feature @xmath7 than the @xmath91 .    regarding the other strategies",
    ", we can see that when the expert feedback has a variance greater than 0.3 , the baseline has a better performance than random , which asks feedback on features chosen at random ( without repeat ) .",
    "for largest target feature the difference with the baseline is even more significant .",
    "we can notice a decrease in performance with every additional feedback , for a noise variance of more than 0.3 .",
    "this happens because when computing the loss , for the parameter features whose value is changed ( from @xmath92 to @xmath93 ) , the introduced noise is then multiplied with one of the @xmath36 largest features of the target patient @xmath94 .",
    "for this strategy , the noisy feedbacks propagate the most and decrease the performance , leading to results worse than random .",
    "although a natural extension , strategies require much larger feedback budgets to obtain effective expert knowledge elicitation when the feedback is very noisy .",
    "in fact , the effect of a noisy feedback naturally reduces if one asks feedback multiple times on the same feature .",
    "but this would also imply a change in the assumptions about the structure in the data or about the number of experts that can be consulted .",
    "the design of an optimal strategy becomes more intricate in this scenario since it involves choosing the right balance between ( a ) obtaining noisy information about more coefficients , or ( b ) focusing on a smaller number of coefficients for which feedback is asked multiple times .",
    "this trade - off might also vary depending on the target , since its features will then propagate the uncertainty of the coefficients into the loss .",
    "[ s : conclusions ] we have introduced a novel setup that brings together expert elicitation and the difficult `` small n , large p '' regression problem . starting from noisy estimates based on extremely small sample sizes , we empirically demonstrated the prediction improvement that can be obtained by bringing in only a few expert feedbacks .",
    "more precisely , we considered a simplified problem formulation , where there is a strict budget constraint on exact expert feedback .",
    "the simplified problem setting is intended to be a starting point that opens up both new interesting theoretical questions and a line of applied work towards new solutions in the currently very timely problem of personalized medicine .",
    "underlying the practically important goal of developing better predictions of treatment outcome for an individual patient , is the task of estimating predictors for the sample with @xmath26 , which requires creative solutions .",
    "new approaches of querying and incorporating available expert knowledge are naturally expected to have much wider applicability .    for future work , a sensible formulation for the `` small n , large p '' regression problem is to find the optimal expert queries for reducing the interval uncertainty for regression coefficients , with strategies recently studied and applied in reliability analysis problems @xcite .",
    "another particularly appealing future formulation is adaptive expert feedback elicitation , where after each feedback , the estimate is updated and the next feature is sampled taking into account the current estimate .",
    "similar expert interaction approaches were shown to be effective for user intent modelling in  @xcite .",
    "lastly , for the elicitation method proposed here , we intend to run a full - blown user study and test the actual assessments from experts",
    ".    10    hossein azari  soufiani , david  c parkes , and lirong xia .",
    "preference elicitation for general random utility models . in _",
    "uncertainty in artificial intelligence : proceedings of the 29th conference _ , pages 596605 .",
    "auai press , 2013 .",
    "jordi barretina , giordano caponigro , nicolas stransky , kavitha venkatesan , adam  a margolin , sungjoon kim , christopher  j wilson , joseph lehr , gregory  v kryukov , dmitriy sonkin , et  al .",
    "the cancer cell line encyclopedia enables predictive modelling of anticancer drug sensitivity .",
    ", 483(7391):603607 , 2012 .",
    "nadia ben  abdallah and sebastien destercke . .",
    "in auai press , editor , _ uncertainty in artificial intelligence _ , uncertainty in artificial intelligence ( uai ) 2015 , pages 1222 , amsterdam , netherlands , july 2015 .",
    "rbert busa - fekete , eyke hllermeier , and balzs szrnyi .",
    "preference - based rank elicitation using statistical models : the case of mallows . in _ proceedings of the 31st international conference on machine learning ( icml-14 ) _ , pages 10711079 , 2014 .",
    "cristina conati , abigail gertner , and kurt vanlehn .",
    "using bayesian networks to manage uncertainty in student modeling .",
    ", 12(4):371417 , 2002 .",
    "m  julia flores , ann  e nicholson , andrew brunskill , kevin  b korb , and steven mascaro . incorporating expert knowledge when learning bayesian network structure : a medical case study . , 53(3):181204 , 2011 .",
    "jerome friedman , trevor hastie , and rob tibshirani .",
    "regularization paths for generalized linear models via coordinate descent .",
    ", 33(1):1 , 2010 .",
    "mathew  j garnett , elena  j edelman , sonja  j heidorn , chris  d greenman , anahita dastur , king  wai lau , patricia greninger , i  richard thompson , xi  luo , jorge soares , et  al .",
    "systematic identification of genomic markers of drug sensitivity in cancer cells .",
    ", 483(7391):570575 , 2012 .",
    "paul  h garthwaite and james  m dickey .",
    "quantifying expert opinion in linear regression problems .",
    ", pages 462474 , 1988 .",
    "iain  m johnstone and d  michael titterington .",
    "statistical challenges of high - dimensional data .",
    ", 367(1906):42374253 , 2009 .",
    "joseph  b kadane , james  m dickey , robert  l winkler , wayne  s smith , and stephen  c peters .",
    "interactive elicitation of opinion for a normal linear model . , 75(372):845854 , 1980",
    ".    andrs  r masegosa and serafn moral .",
    "an interactive approach for bayesian network learning using domain / expert knowledge .",
    ", 54(8):11681181 , 2013 .",
    "anthony ohagan , caitlin  e. buck , alireza daneshkhah , j.  richard eiser , paul  h. garthwaite , david  j. jenkinson , jeremy  e. oakley , and tim rakow . .",
    "wiley , chichester , england , 2006 .",
    "rebecca  a. oleary , samantha  low choy , justine  v. murray , mary kynn , robert denham , tara  g. martin , and kerrie mengersen .",
    "comparison of three expert elicitation methods for logistic regression on predicting the presence of the threatened brush - tailed rock - wallaby _",
    "petrogale penicillata_. , 20:379398 , 2009 .",
    "sinno  jialin pan and qiang yang . a survey on transfer learning . , 22(10):13451359 , october 2010 .",
    "tuukka ruotsalo , giulio jacucci , petri myllymki , and samuel kaski .",
    "interactive intent modeling : information discovery beyond search .",
    ", 58(1):8692 , december 2014 .",
    "robert tibshirani .",
    "regression shrinkage and selection via the lasso .",
    ", pages 267288 , 1996 .    .",
    "http://www.cancerrxgene.org/translation/gene , 2016 .",
    "accessed : 2016 - 02 - 26 .",
    "wanjuan yang , jorge soares , patricia greninger , elena  j edelman , howard lightfoot , simon forbes , nidhi bindal , dave beare , james  a smith , i  richard thompson , et  al . . ,",
    "41(d1):d955d961 , 2013 .",
    "guizhen zhang and vinh  v thai .",
    "expert elicitation and bayesian network modeling for shipping accidents : a literature review . , 87:5362 , 2016 .",
    "[ s : appendix ]"
  ],
  "abstract_text": [
    "<S> we consider regression under the `` extremely small @xmath0 large @xmath1 '' condition . </S>",
    "<S> in particular , we focus on problems with so small sample sizes @xmath0 compared to the dimensionality @xmath1 , even @xmath2 , that predictors can not be estimated without prior knowledge . furthermore , we assume all prior knowledge that can be automatically extracted from databases has already been taken into account . </S>",
    "<S> this setup occurs in personalized medicine , for instance , when predicting treatment outcomes for an individual patient based on noisy high - dimensional genomics data . </S>",
    "<S> a remaining source of information is expert knowledge which has received relatively little attention in recent years . </S>",
    "<S> we formulate the inference problem of asking expert feedback on features on a budget , present experimental results for two setups : `` small @xmath0 '' and `` n=1 with similar data available '' , and derive conditions under which the elicitation strategy is optimal . </S>",
    "<S> experiments on simulated experts , both on simulated and genomics data , demonstrate that the proposed strategy can drastically improve prediction accuracy . </S>"
  ]
}