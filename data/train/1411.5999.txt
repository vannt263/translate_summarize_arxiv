{
  "article_text": [
    "we often use the stars in the milky way to trace its structure . the brightest stars , which can be used to trace galactic structure to the largest distances , include blue horizontal branch stars ; o , b and a main sequence stars ; rr lyraes ; cepheid variables ; red clump stars ; k giants ; and m giants ( see figure  [ fig1 ] ) .",
    "all of these stellar types are reasonably well calibrated as distance indicators .",
    "k giant and red clump stars almost always require spectroscopy to accurately distinguish them from the more numerous main sequence stars of the same color , and rr lyrae stars are usually identified from multiple epochs of photometry .",
    "all of these bright stars are relatively rare in any stellar population , and many of them are only observed in certain populations .",
    "for example , blue horizontal branch stars and rr lyrae stars are only found in old populations , m giants are only found in relatively metal - rich populations , and o , b , and a main sequence stars are only found in very young populations ( though a stars with main sequence gravities are found in some old populations as blue stragglers ) .        deep , large sky area , multicolor surveys with high accuracy calibrations , like the sloan digital sky survey ( sdss ; york et al . 2000 ) enable new methods for studying the structure of the milky way .",
    "for example , ivezi et al .",
    "( 2008 ) derived a formula that allows us to estimate the temperature and metallicity of main sequence stars of type g and later from sdss photometry . since these lower mass main sequence stars do not evolve in the age of the universe , their absolute magnitudes are independent of age .",
    "photometry can not tell us the surface gravities of these types of stars ( lenz et al .",
    "1998 ) , but since the vast majority of the red stars in the photometric survey are main sequence stars , this is not a major obstacle .",
    "juri et al .",
    "( 2009 ) used this technique to measure the distances to 48 million stars , and then used these distances to determine the density distributions of the disks and halo at 100 pc to 20 kpc from the sun , over 6500 square degrees of sky .",
    "though this analysis used a large number of stars , and resulted in the measurement of stellar density , the juri analysis was using photometric parallax ; they were determining the distance to each star individually using photometry .    in this article",
    ", we introduce the concept of _ statistical photometric parallax_. we have used this technique most successfully to study the structure of the galaxy using turnoff stars ( newberg et al .",
    "these stars are by definition brighter than main sequence stars such as those used by juri et al .",
    ", so they can trace the structure of the milky way 30 kpc or more from the sun .",
    "however , the turnoff stars in a single stellar population can differ in absolute magnitude by two magnitudes ( producing a distance error of a factor of 2.5 ) .",
    "we do not have a way to determine the distance to a single turnoff star with reasonable accuracy .",
    "however , it has been shown that the absolute magnitude distribution of turnoff stars in halo globular clusters are surprisingly similar to each other ( newby et al .",
    "2011 ) , over a metallicity range -2.3 dex @xmath0 [ fe / h ] @xmath0 -1.2 and ages ranging from 9 to 13.5 gyr .",
    "recently , grabowski , newby , & newberg ( 2012 ) showed that this similarity even holds for the globular cluster whiting 1 , which is thought to be only 6.5 gyrs old ( carraro , zinn & moni bidin 2007 ) .",
    "this striking similarity between the absolute magnitude distributions of turnoff stars was not expected .",
    "one expects that younger globular clusters would have brighter , bluer turnoff stars .",
    "also , one expects that more metal - rich clusters will have dimmer , redder turnoff stars . as it turns out ,",
    "older stars in the milky way generally have lower metallicity , and the two effects cancel each other .",
    "this appears to be an unanticipated consequence of the milky way s age - metallicity relationship ( amr - muratov & gnedin 2010 , dotter ; sarajedini & anderson 2011 ) .",
    "apparently , the absolute magnitude distribution of turnoff stars is similar over the full age and metallicity range of typical stellar populations in the milky way halo .",
    "we will describe here the general techique of statistical photometric parallax , which can be used to statistically account for the effects of a range of intrinsic brightnesses of the stellar population which is being used to trace milky way density structure , and can also statistically account for the observational biases in a survey such as the sloan digital sky survey .",
    "we have implemented this technique as a search for the density parameters with the highest likelihood of matching the observed data .",
    "because this parameter search can be computationally expensive , we have employed supercomputers and a large volunteer computing platform , milkyway@home , that was built to solve this problem .",
    "we are defining the term _ statistical photometric parallax _ here for the first time , and it is intended to apply in general for any case where the statistical distribution of absolute magnitudes is used to find the underlying density distribution of stars . however , we will describe here as an example the application of this technique to determine halo substructure using color - selected f turnoff stars , as used by newberg et al .",
    "( 2002 ) , cole et al .",
    "( 2008 ) , and newby et al .",
    "( 2012 ) .    in newberg",
    "et al . ( 2002 ) , stars with colors @xmath1 and @xmath2 were selected as turnoff stars .",
    "the color range was chosen to be bluer than the turnoff of the thick disk , so that halo stars would preferentially be selected . in this paper ,",
    "only the simplest form of statistical photometric parallax was employed .",
    "the distance to the sagittarius dwarf tidal stream was determined by assuming the center of the absolute magnitude distribution of turnoff stars in the @xmath3 filter was @xmath4 .",
    "this number was calculated by comparing the apparent magnitude of the turnoff to the apparent magnitude of rr lyrae stars in the same stellar population . in this example",
    ", distances to single stars were not calculated ; instead we made a more accurate determination of distance by looking at the distribution of apparent magnitudes of a particular set of stars .    in cole",
    "( 2008 ) , an algorithm was presented that allowed us to determine not just the distance to the center of a stream , but the three dimensional density of turnoff stars .",
    "the technique used maximum likelihood to find the model parameters @xmath5 that make the observed star positions @xmath6 the most likely . the likelihood @xmath7 is given by the product of the probability density functions ( pdfs ) evaluated at all of the star positions : @xmath8 the pdf is constructed by the following steps :    1 .   for each stellar component , one assumes a parameterized model ( for example a double exponential , nfw , hernquist , etc . ) for the spatial density .",
    "this spatial density is transformed to @xmath9 coordinates , assuming that the absolute magnitude of each of the stars is the average for the population .",
    "this density is convolved with the absolute magnitude distribution of the tracers , so that we produce the distribution that we expect to observe .",
    "this expected distribution is multiplied by the completeness for observing stars of a given apparent magnitude in a given survey , as a function of apparent magnitude .",
    "the resulting distribution is normalized so that the integrated probability of finding a star in the entire volume observed is one .",
    "the final pdf is the sum of the fraction of stars in each component times the normalized distribution , summed over the number of components in the model .",
    "the fraction of stars in each component are also parameters that are fit in the maximum likelihood optimization .    of these steps , the most time - consuming is the calculation of the integral over the volume .",
    "contrary to first impressions , the time to calculate the likelihood depends more heavily on the number of sub - volumes into which the survey space needs to be divided to achieve an accurate result , than on the number of stars in the dataset .",
    "one then uses an optimization technique to find the model parameters that produce the highest likelihood .",
    "when using a supercomputer , we usually use conjugate gradient descent .",
    "this algorithm is sequential ; one evaluates the likelihood and the derivatives with respect to each parameter , chooses a direction , then decides how far in that direction to go before repeating that process .",
    "newby et al . (",
    "2012 ) applied this technique to all of the data available in sdss dr7 to find the density of the sagittarius dwarf tidal stream in the north galactic cap , and in three sdss stripes in the south galactic cap .",
    "one of the advantages of this probablistic technique is that we are able to extract from the sample of 1.7 million turnoff stars a set of @xmath10 stars that have the spatial characteristics of the sagittarius dwarf tidal stream .",
    "this is accomplished by generating a random mumber for each star , and using that random number to place it either in the sagittarius dwarf tidal stream catalog , with the probability that a star at that position in the galaxy is in the sagittarius dwarf tidal stream ; or in the catalog of non - sagittarius halo stars , with the probability that a star at that position in the galaxy is not in the sagittarius dwaf tidal stream .",
    "note that if you wanted to find actual stars in the sagittarius dwarf tidal stream , say for spectroscopic follow - up , you should use the original catalog of stars and select those with the highest probability of being in the sagittarius dwarf tidal stream .",
    "however , the statistically separated catalog we generated facilitates the study of density substructures in the halo .",
    "in particular , we can remove the sagittarius stream from the original stellar sample so we can study the smaller tidal streams , and the density structure of the smooth component of the halo . an example stripe analyzed in newby et al .",
    "( 2012 ) is shown in figure  [ fig2 ] .    in cole",
    "et al . ( 2008 ) and newby et al . ( 2012 ) we modeled the distribution of turnoff star absolute magnitudes as a gaussian centered at @xmath4 with a width of @xmath11 magnitudes . since then , we have recognized that the distribution is asymmetric ; there are more stars fainter than the maximum than there are brighter than the maximum .",
    "more importantly we have learned that , due to larger color errors at fainter magnitudes , the absolute magnitude distribution of color - selected stars is different near the survey limit than it is for the brighter stars .",
    "this effect is much larger than we expected it to be . near the survey limit , the majority of the stars are not turnoff stars , but are fainter main sequence stars that have scattered into our color selection limits due to large measurement errors . because we are using a statistical approach , this effect can be included in the analysis by varying the absolute magnitude distribution as a function of apparent magnitude .",
    "we plan to include this in future analyses .",
    "we originally tried to implement the search for maximumum likelihood on a single cpu . in a single @xmath12-wide sdss data stripe",
    ", we fit 2 parameters to a smooth halo with a hernquist profile , and 6 parameters per tidal debris stream .",
    "if there were three tidal streams in a single stripe , there would be 20 parameters .",
    "evaluating the likelihood for one guess for the model parameters currently takes about 4 hours , with most of the time spent integrating the pdf over the survey volume . in order to optimize 20 parameters requires about 50 likelihood evaluations per conjugate gradient descent step and 50 steps per maximum likelihood evaluation .",
    "this totals ten thousand hours per optimization ( over a year ) .",
    "luckily the optimization is embarrassingly parallel .",
    "it is possible to parallelize the integral , since each integral volume calculation is completely independent of the others .",
    "we are able to run this algorithm on a 256 node rack of a blue gene / l supercomputer . parallelizing the integral over 256 nodes cuts the time per likelihood calculation down under a minute .",
    "a conjugate gradient step can then be accomplished in 47 minutes , and ten iterations can be accomplished in under eight hours ( which is comfortably less the the maximum job size allowed in our queue ) . in practice , we need to try several conjugate gradient descents to approximate the best parameters . once they are known approximately , we run of order ten conjugate gradient descents starting near the best values . including submitting jobs a few at a time and waiting for queue time , this process can take a couple of weeks to validate the results for one sdss stripe .",
    "currently our best method for computing the parameters is the volunteer computing platform milkyway@home .",
    "this project is part of the berkeley open infrastructure for network computing ( boinc ; anderson , korpela & walton 2005 ) group of volunteer computing platforms .",
    "the first and most famous of these is seti@home .",
    "boinc offers us a template server and database application , and an infrastructure for volunteers to donate their time to our server .",
    "we implemented our own server , including the maximum likelihood algorithm , a set of optimization routines that will run in a heterogeneous , asynchronous parallel computing environment ( desell et al .",
    "2010b ) , and a modified server application that sends out  work units \" to the volunteers , collects the results , and validates that the results are not in error ( desell et al .",
    "one of the surprises in operating a boinc server is that some of the results sent back from the volunteers are not correct , either because their hardware is malfunctioning , they did not update the software correctly , or they purposely sent back a wrong answer quickly so that they can accumulate boinc ",
    "credit \" more quickly .",
    "it is impossible to overestimate how important it is to our volunteers that they get credit for the work units their computers crunch , and that credit is apportioned fairly between the volunteers .",
    "milkyway@home is currently delivering 0.5 petaflops of computing power from 25,000 active volunteers giving us access to over 35,000 cpus or gpus .",
    "the majority of the computing power comes from the gpus , the best of which can process our likelihood calculations about 100 times faster than the cpus ( desell et al .",
    "it is not easy to parallelize the computation of the integral on boinc , because that would require communication between the processors , which is not possible at this time .",
    "instead , we parallelize the calculations by sending a single likelihood calculation ( including the whole integral for a given @xmath5 ) to each volunteer .",
    "these work units can take a couple minutes ( if the work unit is sent to a gpu ) or four or more hours ( if it goes to a cpu ) , but it might take minutes , days , or weeks for the likelihood to be returned depending on how much the volunteer is using the computer for her own purposes , and whether she turns it off .",
    "it takes far more computing power to calculate best fit parameters on the milkyway@home computing system than on a supercomputer .",
    "there are four factors responsible for this : ( 1 ) we can not use sequential searches like conjugate gradient descent . instead , we use  particle swarm \" or a genetic search algorithm . in the particle swarm technique , we send out a random set of guesses that span the parameter space .",
    "as the likelihood results from the volunteers come back , we send out more work units with guesses that are closer to the higher likelihoods .",
    "this search method requires many more steps , but produces more accurate results .",
    "( 2 ) a fraction ( about 10% ) of the work units , selected at random , are sent out five times so that we can validate that they are correct .",
    "if three or more are returned with the same answer , then the result is validated .",
    "we also choose to validate the best likelihoods ( since those influence our future guesses ) , and the likelihoods of users that have submitted previous results that did not validate correctly .",
    "( 3 ) some of the work units that are sent out are never returned .",
    "( 4 ) because we can , we run the searches for a longer period of time over a wider range of of parameter space and we get better global values for the parameters ( newby et al .",
    "2012 ) . to optimize one stripe",
    "takes 1 - 2 weeks , and hundreds of thousands of likelihood calculations .",
    "there are enough volunteers that we can optimize 4 - 5 stripes at the same time and still get the results within the same time scale . by putting more jobs on milkyway@home at the same time",
    ", we decrease the number of work units that are sent out at the same time . by waiting a little longer to send out more work units ,",
    "the results of previous searches can be used to make better guesses of the parameters , so adding more jobs increases the computing time at a rate that is less than linear in the number of jobs .",
    "the purpose of this conference contribution is to define the term _ statistical photometric parallax _ , which allows us to determine the density of a population of stars , even if we can not determine the distance to each individual star in the population .",
    "we find the most likely parameters for the density distribution , given that we know the absolute magnitude distribution and the observational constraints of the observed sample of stars .",
    "if the population of stars is all at the same distance ( for example in a globular cluster or tidal stream ) , then statistical photometric parallax can be used to determine the distance to the stellar population . because the sdss made available a large , well calibrated sample of stars with multi - color photometry , this technique has recently become feasible .",
    "the example i present is the use of turnoff stars to determine the density distribution of stars in the stellar halo of the milky way . by apparent coincidence , the distribution of absolute magnitudes of turnoff stars is very similar for all stellar populations in the age and metallicity range of halo stars .",
    "this appears to be a result of the milky way age - metallicity relationship .",
    "we are in the process of using this technique to accurately map the density distribution of the entire milky way stellar halo .",
    "the drawback to this techique is that to use it one must use a maximum likelihood algorithm that can in some cases require high performance parallel computing to get an accurate measurement of the parameters in the density function . in the process of learning to use this method",
    ", we created a large volunteer computing platform called milkyway@home .",
    "i would like to thank matthew newby and brian yanny for helping me obtain the data for the figures in this publication .",
    "i also thank matthew newby and jeff carlin for their help in proofreading .",
    "this paper is based upon work supported by the national science foundation under grant no .",
    "ast 10 - 09670 .",
    "i also would like to thank the milkyway@home volunteers for providing us with computing power at no cost , and the marvin clan for their support .",
    "funding for sdss - iii has been provided by the alfred p. sloan foundation , the participating institutions , the national science foundation , and the u.s .",
    "department of energy office of science .",
    "the sdss - iii web site is http://www.sdss3.org/."
  ],
  "abstract_text": [
    "<S> in determining the distances to stars within the milky way galaxy , one often uses photometric or spectroscopic parallax . in these methods , </S>",
    "<S> the type of each individual star is determined , and the absolute magnitude of that star type is compared with the measured apparent magnitude to determine individual distances . in this article , we define the term _ statistical photometric parallax _ , in which statistical knowledge of the absolute magnitudes of stellar populations is used to determine the underlying density distributions of those stars . </S>",
    "<S> this technique has been used to determine the density distribution of the milky way stellar halo and its component tidal streams , using very large samples of stars from the sloan digital sky survey . </S>",
    "<S> most recently , the volunteer computing platform milkyway@home has been used to find the best fit model parameters for the density of these halo stars . </S>"
  ]
}