{
  "article_text": [
    "abstractive summarization has gained popularity due to its ability of generating new sentences to convey the important information from text documents .",
    "an abstractive summarizer should present the summarized information in a coherent form that is easily readable and grammatically correct .",
    "readability or linguistic quality is an important indicator of the quality of a summary . several text - to - text ( t2 t ) generation techniques that aim to generate novel text from textual input",
    "have been developed  @xcite . however , to the best of our knowledge , none of the above methods explicitly model the role of linguistic quality and only aim at maximizing information content of the summaries . in this work ,",
    "we address readability by assigning a log probability score from a language model as an indicator of linguistic quality . more specifically",
    ", we build a novel optimization model for summarization that jointly maximizes information content and readability .",
    "extractive summarizers  @xcite often lose a lot of information from the input as they only `` extract '' a few important sentences from the documents to create the final summary .",
    "we prevent information loss by aggregating information from multiple sentences .",
    "we generate clusters of similar sentences from a collection of documents .",
    "multi - sentence compression ( msc )  @xcite can be used to fuse information from sentences in a cluster .",
    "however , msc might generate sentences that convey similar information from two different clusters .",
    "by contrast , our integer linear programming ( ilp ) based approach prevents redundant information from being included in the summary using a inter - sentence redundancy constraint .",
    "consequently , our experiments reveal that our method generates more informative and readable summaries than msc .",
    "our proposed approach to abstractive summarization consists of the following two steps : ( 1 ) aligning similar sentences from multiple - documents and ( 2 ) generating the most informative and linguistically well - formed sentence from each cluster , and then appending them together . in multi - document summarization ,",
    "all documents are not equally important ; some documents contain more information on the main topics in the document set .",
    "our first step estimates the importance of a document in the whole dataset using _ lexrank _",
    "@xcite , _ pairwise cosine similarity _ and _ overall document collection similarity_. each sentence from the most important document are initialized into separate clusters .",
    "thereafter , each sentence from the other documents are assigned to the cluster that has the highest similarity with the sentence . in the generation step ,",
    "we first generate a word - graph structure from the sentences in each cluster and construct @xmath0 shortest paths from the graph between the start and end nodes .",
    "we formulate a novel integer linear programming ( ilp ) problem that maximizes the information content and linguistic quality of the generated summary .",
    "our ilp problem represents each of the @xmath0 shortest paths as a binary variable .",
    "the coefficients of each variable in the objective function is obtained by combining the information score of the path and the linguistic quality score .",
    "we introduce several constraints into our ilp model .",
    "we ensure that only one sentence is generated from each cluster .",
    "second , we avoid redundant sentences that carry the same or similar information from different clusters .",
    "the solution to the optimization problem decides the paths that would be included in the final abstractive summary .    on the duc2004 and duc2005 datasets",
    ", we demonstrate the effectiveness of our proposed method .",
    "our proposed method outperforms not only some popular baselines but also the state - of - the - art extractive summarization systems .",
    "rouge scores  @xcite obtained by our system outperforms the best extractive summarizer on both the datasets .",
    "our method also outperforms an abstractive summarizer based on multi - sentence compression  @xcite when measured by rouge-2 , rouge - l and rouge - su4 scores .",
    "further , manual evaluation by human judges shows that our technique produces summaries with acceptable linguistic quality and high informativeness .",
    "several researchers have developed abstractive summarizers .",
    "genest and lapalme   used natural - language - generation ( nlg ) systems .",
    "however , nlg requires a lot of manual effort in terms of defining schemas as well as using deeper natural language analysis .",
    "wang and cardie   and oya _ et al . _   induced templates from the training set in their meeting summarization tasks .",
    "such induction of templates , however , is not very effective in news summarization because of the variability in topics . unlike these methods",
    ", our method does not induce any templates but generates summaries in an unsupervised manner by combining information from several sentences on the same topic .",
    "berg - kirkpatrick _ et al .  _",
    "used an ilp formulation that jointly extracts and compresses sentences to generate summaries .",
    "however , their method is supervised and requires significant manual effort to define features for subtree deletions , which is required to compress sentences .",
    "graph - based techniques have also been very popular in summarization .",
    "et al . _   employed a graph - based approach to generate concise abstractive summaries from highly redundant opinions . compared with their opinionated texts such as product reviews ,",
    "the target documents in multi - document summarization do not contain such high level of redundancy .",
    "more recently , mehdad  _ et al . _   proposed a supervised approach for meeting summarization , in which they generate an entailment graph of sentences .",
    "the nodes in the graph are the linked sentences and edges are the entailment relations between nodes ; such relations help to identify non - redundant and informative sentences .",
    "their fusion approach used msc  @xcite , which generates an informative sentence by combining several sentences in a word - graph structure .",
    "however , filippova s method produces low linguistic quality as the ranking of generated sentences is based on edge weights calculated only using word collocations . by contrast , our method selects sentences by jointly maximizing informativeness and readability and generates informative , well - formed and readable summaries .",
    "figure  [ approach - general ] shows our proposed abstractive summarization approach , which consists of the following two steps :    * _ sentence clustering _ , * _ summary sentence generation_.    given a document set @xmath1 , that consists of @xmath2 documents ( @xmath3 , @xmath4 , @xmath5 , ... , @xmath6 ) , our approach first generates @xmath7 clusters ( @xmath8 , @xmath9 , @xmath10 , ... , @xmath11 ) of similar sentences , and then use the individual clusters to create word - graphs .",
    "a maximum of one novel sentence is generated from each word - graph with the goal of maximizing information content and linguistic quality of the entire summary .",
    "the sentence clustering step ( s1 ) has two important components : the first ( s1 - 1 ) identifies the most important document @xmath12 in @xmath1 before the final cluster generation step ( s1 - 2 ) that generates clusters of similar sentences .",
    "we experiment with several techniques to identify @xmath12 , and then align sentences from other documents to the sentences in @xmath12 .",
    "it proves to be a simple , yet effective technique for generating clusters containing similar information .",
    "our approach is inspired by the findings of wan   that showed how the incorporation of document impact can improve the performance of summarization . in ( s2 ) , we create a directed word - graph structure from the sentences in each cluster . in the word - graph ,",
    "the nodes represent the words and the edges define the adjacency relations in the sentences . from the word - graph ,",
    "multiple paths between the start and the end nodes can be extracted .",
    "we extract @xmath0 shortest paths from each cluster , and finally retain the paths that maximize information content and linguistic quality using an ilp based approach .",
    "we impose constraints on the maximum number of sentences that are generated from each cluster and also impose constraints to avoid redundancies such that similar information from different clusters are not included in the summary .",
    "information content is measured using textrank  @xcite , which scores sentences based on the presence of keywords .",
    "we measure linguistic quality using a 3-gram language model that assigns confidence values to sequences of words in the sentences . in this section ,",
    "we describe both steps  s1 and s2 .",
    "we initialize clusters of sentences using each sentence from the most important document , @xmath12 , in a document set @xmath1 .",
    "our intuition behind this approach is that @xmath12 consists of the most important content relevant across all the documents in @xmath1 . in other words ,",
    "the document that is most close to the central content of the collection is the most informative .",
    "we propose several techniques to identify @xmath12 .",
    "* lexrank ( @xmath13 ) : * lexrank  @xcite constructs a graph of sentences where the edge weights are obtained by the inter - sentence cosine similarities . while the original lexrank constructs a graph of sentences ,",
    "we construct a graph of documents to compute document importance .",
    "equation ( [ lpr - equation2 ] ) shows how lexrank scores are computed using weighted links between the nodes ( documents ) .",
    "this equation measures the salience of a node in the graph , which is the importance of the document in the entire document collection .",
    "let @xmath14 be the centrality of node @xmath15 .",
    "lexrank is then defined as follows : @xmath16}\\frac{\\textrm{idf - modified - cosine}(u , v)}{\\sum_{z\\in adj[v]}\\textrm{idf - modified - cosine}(z , v)}p(v),$}\\ ] ] where @xmath17 $ ] and @xmath18 are the set of nodes that are adjacent to @xmath15 and the total number of nodes in the graph , respectively .",
    "the damping factor @xmath19 is usually set to 0.85 , and we set @xmath19 to this value in our implementation .",
    "@xmath12 is determined as the document that has the highest lexrank score in @xmath1 once the above equation converges .",
    "* pairwise cosine similarity ( @xmath20 ) : * this method computes the average cosine similarity between the target document @xmath21 and the other documents in the dataset .",
    "the average similarity is calculated using the following formula : @xmath22 where @xmath23 denotes the number of documents in the document set @xmath1 .    *",
    "overall document collection similarity ( @xmath24 ) : * this method computes the cosine similarity between the target document and the whole document set .",
    "we create the whole document set by concatenating text from all the documents in @xmath1 .",
    "this method is defined as follows : @xmath25    in @xmath26 , @xmath27 , and @xmath28 mentioned above , we select the document @xmath12 with the highest score as the most important one in the dataset @xmath1 .",
    "next , we generate the clusters by aligning sentences and re - ordering them based on original positions of the sentences in the documents .",
    "the sentences from each of the other documents ( @xmath29 ) in @xmath1 are assigned to the clusters one - by - one based on cosine similarity measure .",
    "our approach computes pairwise cosine similarity of each sentence in @xmath21 to all the sentences in @xmath12 .",
    "for example , a sentence in @xmath21 , @xmath30 has the highest similarity with @xmath31 , a sentence in @xmath12 .",
    "then , we assign @xmath30 to cluster @xmath32 , in which @xmath31 was initially assigned . some sentences in @xmath21 might not be similar to any of the sentences in @xmath12 .",
    "hence , we only align sentences when the similarity @xmath33 .",
    "further , we only retain clusters that have at least @xmath34 sentences , assuming that a content is relevant only if it exists in half of the documents in @xmath1 .",
    "* cluster ordering : * we implement two _ cluster ordering _ techniques that reorder clusters based on the original position of the sentences in the documents .    1 .",
    "_ majority ordering ( mo ) : _ given two clusters , @xmath35 and @xmath32 , the set of common documents from which the sentences are assigned to the two clusters are identified .",
    "if @xmath35 and @xmath32 have sentences @xmath30 and @xmath36 ( @xmath37 ) , respectively , where @xmath21 is the common document , then @xmath35 precedes @xmath32 . the final order is determined based on overall precedence of the sentences of one cluster over the others .",
    "_ average position ordering ( apo ) : _ the sentences @xmath38 in any cluster @xmath35 are each assigned a normalized score . for example , the normalized score of @xmath39 is computed as the ratio of the original position of the sentence and the total number of sentences in @xmath21 ( here , @xmath39 belongs to document @xmath21 ) . when ordering two clusters , the cluster that has the lower score obtained by averaging the normalized scores of all the sentences is ranked higher than the others .      in order to generate a one - sentence representation from a cluster of redundant sentences , we use multi - sentence compression .",
    "we generate multiple sentences from a cluster using a word - graph  @xcite .",
    "suppose that a cluster @xmath35 contains @xmath40 sentences , @xmath41 . a directed graph is created by adding sentences from @xmath42 to the graph in an iterative fashion .",
    "each sentence is connected to dummy _ start _ and _ end _ nodes to mark the beginning and ending of the sentences .",
    "the vertices or nodes are the words along with the parts - of - speech ( pos ) tags .",
    "we connect adjacent words in the sentences with directed edges .",
    "once the first sentence is added , words from the following sentences are mapped onto a node in the graph provided that they have exactly the same word form and the same pos tag .",
    "the sequence of rules used for the word - graph construction is as follows :    the context of the words are taken into consideration if multiple mappings are possible , and the word is mapped to that node that has the highest directed context .",
    "we also add punctuations to the graph .",
    "figure  [ fig : word - graph - generation ] shows a simple example of the word - graph generation technique .",
    "we do not show pos and punctuations in the figure for clarity .",
    "consider the following two sentences as an illustration of our generation approach :    as shown in the examples above , the two sentences contain similar information , but they are syntactically different .",
    "the solid directed arrows connect the nodes in eg.1 , whereas the dotted arrows join the nodes in eg.2 .",
    "we can obtain several shortest paths between the start and end nodes . in figure",
    "[ fig : word - graph - generation ] , we highlight one such path using gray rectangles .",
    "several other paths are possible , for example :    the original input sentences from the cluster are also valid paths between the _ start _ and _ end _ nodes .",
    "to ensure pure abstractive summarization , we remove such paths that are same or very similar ( cosine similarity @xmath43 0.8 ) to any of the original sentences in the cluster .",
    "similar to filippova s word - graph construction , we set the minimum path length ( in words ) to eight to avoid incomplete sentences .",
    "finally , we retain a maximum of 200 randomly selected paths from each cluster to reduce computational overload of the ilp based approach .",
    "our aim is to select the best path from all available paths .      from 200 paths in each cluster ,",
    "we choose at most one path that maximizes information content and linguistic quality together .",
    "let @xmath44 be each path in a cluster @xmath32 , namely ,    @xmath45 ,    where the total number of shortest paths is equal to @xmath0=@xmath46 $ ] where @xmath47 refers to the maximum number of paths that can be generated from a cluster .",
    "we argue that the shortest paths that we select in the final summary should be informative as well as linguistically readable .",
    "hence , we introduce two factors  _ informativeness _ ( @xmath48 ) and _ linguistic quality _",
    "( @xmath49 ) .    [ cols=\"<,^,^,<,^,^ \" , ]     * informativeness : * in principle",
    ", we can use any existing method that computes the importance of a sentence to define _ informativeness_. in our model , we use textrank scores  @xcite to generate an importance value of a sentence within a cluster .",
    "textrank creates a graph of words from the sentences .",
    "the score of each node in the graph is calculated as shown in equation  ( [ lpr - textrank ] ) :    where @xmath50 represents the words , @xmath51 denotes the adjacent nodes of @xmath50 and @xmath19 is the damping factor set to 0.85 .",
    "the computation converges to return final word importance scores .",
    "the informativeness score of a path ( @xmath48 ) is obtained by adding the importance scores of the individual words in the path . *",
    "linguistic quality : * in order to compute _ linguistic quality _",
    ", we use a language model .",
    "more specifically , we use a 3-gram ( trigram ) language model that assigns probabilities to sequence of words .",
    "suppose that a path contains a sequence of @xmath52 words @xmath53 .",
    "the score @xmath49 assigned to each path is defined as follows :    where @xmath54 is defined as :    as can be seen from equation ( [ eqn : ll ] ) , we obtain the conditional probability of different sets of 3-grams in the sentence .",
    "the scores are combined and averaged by @xmath55 , the number of conditional probabilities computed .",
    "the @xmath56 scores are negative ; with higher magnitude implying lower readability .",
    "therefore , in equation  ( [ eqn : ll2 ] ) , we take the reciprocal of the logarithmic value with smoothing to compute @xmath49 . in our experiments",
    ", we used a 3-gram model that is trained on the english gigaword corpus[multiblock footnote omitted ] .      to select the best paths from the clusters , we combine informativeness @xmath48 and linguistic quality @xmath49 in an optimization framework .",
    "we maximize the following objective function :    each @xmath44 represents a binary variable , that can take 0 or 1 , depending on whether the path is selected in the final summary or not .",
    "in addition , @xmath57 , the number of tokens in a path , is also taken into consideration and the term @xmath58 assigns more weight to shorter paths so that the system can favor shorter informative sentences .",
    "we introduce several constraints to solve the problem .",
    "first , we ensure that a maximum of one path is selected from each cluster using equation  ( [ eqn : maxone ] ) .",
    "we introduce equation  ( [ eqn : sim ] ) so that we can prevent similar information ( cosine similarity @xmath43 0.5 ) from being selected from different clusters . in figure",
    "[ fig : word - graph - generation ] , this constraint ensures that only one of the several possible paths mentioned in the example is included in the final summary as they contain redundant information .",
    "we evaluated our approach on the duc 2004 and 2005 datasets on multi - document summarization .",
    "we use rouge ( recall - oriented understudy of gisting evaluation )  @xcite for automatic evaluation of summaries ( compared against human - written model summaries ) as it has been proven effective in measuring qualities of summaries and correlates well to human judgments .",
    "we proposed three document importance measures and two different sentence ordering techniques as described in section  [ sec : method ] .",
    "hence , we have six different systems in total . to the best of our knowledge ,",
    "no publicly available abstractive summarizers have been used on the duc dataset .",
    "therefore , we compare our system to msc  @xcite that generates a sentence from a collection of similar sentences using only syntactical information from the source sentences . in msc ,",
    "the input is a pre - defined cluster of similar sentences .",
    "therefore , we compare our ilp based technique with msc using the same set of input clusters obtained by our system . table  [ table : comprouge ] shows the following rouge scores for our evaluation :    the summaries generated by the baselines and the state - of - the - art extractive summarizers on the duc 2004 data were collected from  @xcite .",
    "rouge-2 and rouge - su4 scores have been found to be highly correlated with human judgments  @xcite .",
    "therefore , we computed rouge-2 and rouge - su4 scores of the other systems on the duc2004 summaries directly using rouge . however , the system - generated summaries ( baselines and state - of - the - arts ) were not available for the duc 2005 dataset .",
    "hence , we used rouge scores of the various systems as reported in  @xcite .    according to table  [",
    "table : comprouge ] , all of the rouge scores obtained by our systems outperform all the baselines on both datasets . hereafter , we refer to the best performing system as * ilpsumm*. we perform paired t - test and observe that ilpsumm shows statistical significance compared to all the baselines .",
    "the summarization method using @xmath59 measure as the most informative document and ranked by majority ordering ( mo ) outperforms all of the other techniques .",
    "the document that has the highest similarity to the total content captures the central idea of the documents .",
    "the clustering scheme that works best with msc is @xmath13 + apo .",
    "ilpsumm also outperforms the msc - based method , i.e. , our approach can generate more informative summaries by globally maximizing content selection from multiple clusters of sentences . in summary , content selection of our proposed abstractive systems work at par with the best extractive systems .",
    "* discussion : * our proposed system identifies the most important document , which is a general human strategy for summarization .",
    "the majority ordering strategy prioritizes clusters that contain sentences which should be mentioned earlier in a summary .",
    "other systems tackle redundancy as a final step ; however , we integrate linguistic quality and informativeness to select the best sentences in the summary using our ilp based approach .",
    "we performed the rest of our experiments only on the duc 2004 dataset as it has been widely used for multi - document summarization .",
    "we also determine readability of the generated summaries by obtaining ratings from human judges . following liu and liu  ,",
    "we ask 10 evaluators to rate 10 sets of four summaries on two different factors  _ informativeness _ and _ linguistic quality_. the ratings range from 1 ( lowest ) to 5 ( highest ) .",
    "all the evaluators have a good command of english and seven of them are native speakers .",
    "evaluators were asked to rate the summaries based on informativeness ( the amount of information conveyed ) and linguistic quality ( readability of the summary ) .",
    "we randomized the sets of summaries to avoid any bias .",
    "0.52l|cc|c * type * & * inf * & * lq * & * avg.ll * + human written & 4.42 & 4.35 & -129.02 + extractive ( dpp ) & 3.90 & 3.81 & -142.70 + abstractive ( msc ) & 3.78 & 2.83 & -210.02 + abstractive ( ilpsumm ) & 4.10 & 3.63 & -180.76 +    the four summaries provided to the evaluators are human - written summary ( one summary collected randomly from four model - summaries per cluster ) , extractive summary ( dpp ) , abstractive summary generated using msc ( msc ) and abstractive summary generated using our ilp based method ( ilpsumm ) .",
    "we asked each evaluator to complete 10 such tasks , each containing four summaries as explained above .",
    "we normalize ratings of different evaluators to the same scale .",
    "table  [ tab : humaneval ] shows the results obtained by manual evaluation .",
    "according to the judges , the linguistic quality of * ilpsumm * ( 3.63 ) is significantly better than that of * msc * ( 2.83 ) .",
    "further , our summaries ( ilpsumm ) are more informative than dpp ( 3.90 ) and msc ( 3.78 ) .",
    "dpp is extractive in nature , hence linguistically , the sentences are generally more readable . to obtain a coarse estimate of grammaticality",
    ", we also compute the confidence scores of the summaries using the stanford dependency parser  @xcite .",
    "a language model assigns probabilities to sequence of words ; in contrast , the confidence score of a parser assigns probabilities to grammatical relations .",
    "the values ( the lower the magnitude , the better ) are shown in the column _ avg.ll_. _",
    "avg.ll _ obtained by ilpsumm ( -180.76 ) is better than that obtained by msc ( -210.02 ) , indicating that the language model based linguistic quality estimation helps generate more readable summaries than the msc method .",
    "table  [ tab : samplesumm ] shows a comparison of summaries from the different systems using the duc 2004 dataset . as can be seen , the linguistic quality of the abstractive summaries ( ilpsumm ) is acceptable , and the content is well - formed and informative . our ilp framework can combine information from various sentences and present a fairly well - formed readable summary .",
    "p0.45    ' '' ''    * abstractive summary ( ilpsumm ) : * hun sen s cambodian people s party won 64 of the 122 parliamentary seats in july .",
    "opposition ally sam rainsy charged that hun sen s party has rejected allegations of intimidation and fraud .",
    "hun sen and ranariddh are to form working groups this week to divide remaining government posts . but",
    "a deal reached between hun sen and his chief rival , prince norodom ranariddh s ally , sam rainsy . +",
    "* extractive summary ( dpp ) : * ranariddh and sam rainsy have charged that hun sen s victory in the elections was achieved through widespread fraud .",
    "hun sen said his current government would remain in power as long as the opposition refused to form a new one .",
    "cambodian leader hun sen , who heads the cpp , has offered to share the legislature s top job with the royalist funcinpec party of prince norodom ranariddh in order to break the impasse . + * human - written summary : * cambodian prime minister hun sen rejects demands of 2 opposition parties for talks in beijing after failing to win a 2/3 majority in recent elections .",
    "sihanouk refuses to host talks in beijing .",
    "opposition parties ask the asian development bank to stop loans to hun sen s government .",
    "ccp defends hun sen to the us senate .",
    "funcinpec refuses to share the presidency .",
    "hun sen and ranariddh eventually form a coalition at summit convened by sihanouk .    ' '' ''    ' '' ''    * abstractive summary ( ilpsumm ) : * lebanese foreign minister kamal kharrazi made the mediation offer sunday , in a telephone conversation with his syrian counterpart , farouk al - sharaa .",
    "egyptian president hosni mubarak met here sunday with syrian president hafez assad to show lebanon s support for syria and turkey . in a show of force on friday",
    ", turkish troops were deployed this week on the turkish - syrian border to eradicate krudish rebel bases .",
    "+ * extractive summary ( dpp ) : * egyptian president hosni mubarak met here sunday with syrian president hafez assad to try to defuse growing tension between syria and turkey .",
    "the talks in damascus came as turkey has massed forces near the border with syria after threatening to eradicate kurdish rebel bases in the neighboring country .",
    "egypt already has launched a mediation effort to try to prevent a military confrontation over turkish allegations that syria is harboring turkish kurdish rebels . +",
    "* human - written summary : * tensions between syria and turkey increased as turkey sent 10,000 troops to its border with syria . the dispute comes amid accusations by turkey that syria helping kurdish rebels based in syria .",
    "kurdish rebels have been conducting cross border raids into turkey in an effort to gain kurdish autonomy in the region .    ' '' ''    * error analysis : * there is still room for improvement in the linguistic quality of the generated summaries .",
    "we analyzed the summaries that were given lower ratings than the other options on the basis of linguistic quality .",
    "consider the following sentence in a system generated summary , which received low scores from human judges :    as can be seen , the phrase `` killed 270 people killed '' is not coherent .",
    "the language model fails to identify such cases as the 3-gram sequences of _ killed 270 people _ and _ 270 people killed _ are both grammatically coherent .",
    "in addition to a language model , we can also use a dependency parser to assign lower weights to paths that have redundant dependencies on the same nodes . consider the following example : + the last phrase  `` a government formed '' , is grammatically incoherent in the context of the sentence .",
    "linguistically correct modifications could be ",
    "_ a government being formed _ or _ a government formation_. in future work , we plan to address such issues of grammaticality using dependency parses of sentences rather than just adjacency relations when building the word - graph .",
    "we have proposed an approach to generate abstractive summaries from a document collection .",
    "we capture the redundant information using a simple yet effective clustering technique .",
    "we proposed a novel ilp based technique to select the best shortest paths in a word - graph to maximize information content and linguistic quality of a summary .",
    "experimental results on the duc 2004 and 2005 datasets show that our proposed approach outperforms all the baselines and the state - of - the - art extractive summarizers . based on human judgments ,",
    "our abstractive summaries are linguistically preferable than the baseline abstractive summarization technique . in future work",
    ", we plan to use paraphrasing techniques to further enhance quality of the generated summaries .",
    "we also plan to address phrase level redundancies to improve coherence .",
    "this material is based upon work supported by the national science foundation under grant no . 0845487 ."
  ],
  "abstract_text": [
    "<S> abstractive summarization is an ideal form of summarization since it can synthesize information from multiple documents to create concise informative summaries . in this work </S>",
    "<S> , we aim at developing an abstractive summarizer . </S>",
    "<S> first , our proposed approach identifies the most important document in the multi - document set . </S>",
    "<S> the sentences in the most important document are aligned to sentences in other documents to generate clusters of similar sentences . </S>",
    "<S> second , we generate @xmath0-shortest paths from the sentences in each cluster using a word - graph structure . finally , we select sentences from the set of shortest paths generated from all the clusters employing a novel integer linear programming ( ilp ) model with the objective of maximizing information content and readability of the final summary . </S>",
    "<S> our ilp model represents the shortest paths as binary variables and considers the length of the path , information score and linguistic quality score in the objective function . </S>",
    "<S> experimental results on the duc 2004 and 2005 multi - document summarization datasets show that our proposed approach outperforms all the baselines and state - of - the - art extractive summarizers as measured by the rouge scores . </S>",
    "<S> our method also outperforms a recent abstractive summarization technique . in manual evaluation , </S>",
    "<S> our approach also achieves promising results on informativeness and readability . </S>"
  ]
}