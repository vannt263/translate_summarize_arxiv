{
  "article_text": [
    "one of the biggest problems of neural network learning is the plateau of the learning curve . considering the gradient learning method and its generalization error ,",
    "this plateau is mainly caused by the saddle structure of the error function .",
    "the permutation symmetry prevents the identification of the hidden units in multilayer perceptrons if they have the same weight vectors , and produces this saddle structure @xcite . in the learning scenario of a teacher and a student network , the saddle",
    "is thought to be affected by the strength of the correlation of the hidden units in the teacher network , which may be closely related to the length of the plateau .",
    "more specifically , in the conventional gradient descent ( gd ) , the weight vectors in the student network are known to approach the saddle before reaching their final states @xcite .",
    "since the saddle is located between the weight vectors of the teacher hidden units , their stronger correlation is supposed to force the student weight vectors closer to the saddle , resulting in a longer plateau .",
    "natural gradient descent ( ngd ) , however , may be able to avoid the saddle because it can update the network parameters to the optimal direction in the riemannian space @xcite .",
    "ngd is a fairly general method for effectively adjusting the parameters of stochastic models , but its validity in multilayer perceptrons is uncertain because of three intrinsic problems : 1 ) ngd needs prior knowledge of the input distribution to calculate the fisher information matrix , 2 ) ngd is unstable around the singular points of the fisher information matrix , 3 ) matrix inversion is time consuming , which might be critical especially in real - time learning .",
    "the method proposed by yang and amari@xcite can be used to calculate ngd efficiently in the case of a large input dimension in multilayer perceptrons .",
    "also , the adaptive method can be used to approximate the inverse of the fisher information matrix asymptotically without prior knowledge or matrix inversion @xcite . in this paper , we discuss the problem of singularity ; since the saddle is one of the singular points , how ngd works around there is one of our main topics .    on - line learning",
    "is one of the most popular forms of training .",
    "analysis of the network dynamics in on - line learning is much easier than for batch learning because the state of the network and the learning samples are independent of each other . in this framework",
    ", the statistical mechanics method proposed by saad and solla can be used to analyze the gd dynamics exactly at the large limit of the input dimension @xcite .",
    "rattray and saad extended this technique to ngd and reported that it works efficiently in multilayer perceptrons @xcite . in this paper",
    ", we also use this method and contrast the dynamics for gd and ngd , focusing on the corrupted saddle structure under a strong correlation of the hidden units in the teacher network .",
    "soft committee machines ( fig . [ fig:1 ] ) are considered where the teacher network has @xmath0 hidden units while the student has @xmath1 units . to apply ngd ,",
    "gaussian noise @xmath2 is added to the output of the student ; @xmath3 where @xmath4 denotes the input vector while @xmath5 and @xmath6 are the @xmath7th weight vectors of the teacher and the student networks , respectively .",
    "here , @xmath8 means the transposition while @xmath9 is an activation function .    the joint probability distribution of the input @xmath10 and the output @xmath11 of the student network is given by @xmath12 the parameter vector of ( [ eq:3 ] ) , @xmath13^t \\in { \\mathbb r}^{kn}$ ] , is updated iteratively to approximate the joint probability distribution of the input @xmath10 and the output @xmath14 of the teacher network , @xmath15 where @xmath16 is the delta function .",
    "the loss function for a given set of a learning sample @xmath17 , defined using the logarithmic loss of the conditional probability distribution of ( [ eq:4 ] ) , is @xmath18 where @xmath19 is constant .",
    "the generalization error is then defined as the expected loss : @xmath20 the definitions of ( [ eq:6 ] ) can be written , by applying ( [ eq:1 ] ) and ( [ eq:2 ] ) , as @xmath21    we consider on - line learning in this paper , where the parameter vector @xmath22 is updated for each set of an independently given sample @xmath17 .",
    "the updating rule , the differential of @xmath22 , for gd is defined with a learning rate @xmath23 as @xmath24 where @xmath25 where @xmath26 denotes the derivative of @xmath9 . one for ngd is also defined as @xmath27 where @xmath28 denotes the fisher information matrix of the parameter vector @xmath22 : @xmath29 [ \\nabla { \\bm{_j } } \\ln{p { \\bm{_j } } ( { \\bm{\\xi}},\\zeta')}]^t",
    "\\ > \\!_\\{\\!{\\bm{_\\xi } } \\,\\!_{,\\zeta'\\}}.\\end{aligned}\\ ] ] the @xmath28 can be written , in block form , as @xmath30\\!\\ ! , & \\nonumber\\\\      & { \\bm{g}}_{ij } = \\frac{1}{\\sigma^2 } \\",
    "< g'({\\bm{j}}_i^t { \\bm{\\xi } } ) g'({\\bm{j}}_j^t { \\bm{\\xi } } ) { \\bm{\\xi } } { \\bm{\\xi}}^t \\ > \\,\\!\\!_\\{\\!{\\bm{_\\xi } } \\!_{\\}}.&\\end{aligned}\\ ] ] in the case of the standard multivariate normal distribution input , @xmath31 , the inverse of the fisher information matrix is also given by @xmath32\\!\\ !",
    ", & \\nonumber\\\\      & { \\bm{g}}_{ij}^{-1 } = \\sigma^2 \\ { \\theta_{ij } { \\bm{i } } + { \\bm{j } } ' { \\bm{\\theta}}_{ij } { \\bm{j}}'\\,\\!^t \\ } , & \\end{aligned}\\ ] ] where @xmath33 $ ] is a @xmath34 by @xmath1 matrix , while @xmath35 is a scalar and @xmath36 is a @xmath1 by @xmath1 matrix @xcite .",
    "at the thermodynamics limit , the limit of @xmath37 , the dynamics of the network can be analyzed using statistical mechanics . here , the order parameters that represent the correlations of the weight vectors are used instead of the @xmath34-dimensional vectors @xmath10 , @xmath38 , and @xmath39 . to make the present paper",
    "self - contained , we briefly summarize the derivation of the order parameter equations of the soft committee machine @xcite .    from here on ,",
    "the input vector is assumed to obey a @xmath34-dimensional multivariate gaussian noise with zero mean and a unit covariance matrix : @xmath31 .",
    "the correlation between the input and each weight vector , denoted by @xmath40 and @xmath41 , is then distributed as a normal distribution ; @xmath42 and @xmath43 , while each covariance of them is given by @xmath44 , @xmath45 , and @xmath46",
    ". therefore , a new vector , defined as @xmath47^t \\in { \\mathbb r}^{k+m},\\end{aligned}\\ ] ] is distributed as a multivariate normal distribution @xmath48 : @xmath49 where @xmath50 is the variance - covariance matrix : @xmath51\\!\\!\\end{aligned}\\ ] ] with @xmath52\\!\\!,\\\\      \\label{eq:19 }      { \\bm{r } } \\equiv { \\bm{j}}'\\,\\!^t { \\bm{b } } ' & = & \\left[\\!\\ ! \\begin{array}{ccc }",
    "r_{1,1 } & \\cdots & r_{1,m } \\\\",
    "\\vdots & \\ddots & \\vdots \\\\",
    "r_{k,1 } & \\cdots & r_{k , m } \\\\",
    "\\end{array } \\!\\!\\right]\\!\\!,\\\\      \\label{eq:20 }      { \\bm{t } } \\equiv { \\bm{b}}'\\,\\!^t { \\bm{b } } ' & = & \\left[\\!\\ ! \\begin{array}{ccc",
    "}          t_{1,1 } & \\cdots & t_{1,m } \\\\",
    "\\vdots & \\ddots & \\vdots \\\\",
    "t_{m,1 } & \\cdots & t_{m , m } \\\\",
    "\\end{array } \\!\\!\\right]\\!\\!,\\end{aligned}\\ ] ] and @xmath53 $ ] , @xmath54 $ ] . here ,",
    "@xmath55 and @xmath56 are the order parameters of this system .    using these order parameters ,",
    "the generalization error in ( [ eq:7 ] ) , @xmath57 , can be calculated by @xmath58 if we define the activation function @xmath9 as @xmath59 from here on , the generalization error is given by @xmath60 which depends on only the order parameters .",
    "here we substitute the dynamics of the order parameters for those of the system .",
    "first , we can replace the updating rule ( [ eq:9 ] ) with @xmath61 where @xmath62 thus , the updating rule of the order parameters is given by @xmath63^t { \\bm{b}}_j - { \\bm{j}}_i^t { \\bm{b}}_j \\nonumber\\\\                & = & -\\frac{\\eta}{\\sigma^2 n}\\delta_i y_j,\\end{aligned}\\ ] ] and @xmath64^t [ { \\bm{j}}_j + \\delta{\\bm{j}}_j ] -",
    "{ \\bm{j}}_i^t{\\bm{j}}_j \\nonumber\\\\                 & = & -\\frac{\\eta}{\\sigma^2 n } \\{\\delta_i x_j + \\delta_j x_i\\ } + \\frac{\\eta^2}{\\sigma^4 n^2}\\delta_i \\delta_j { \\bm{\\xi}}^t { \\bm{\\xi}}.\\end{aligned}\\ ] ]    here we introduce the time @xmath65 ; a short period , @xmath66 , is defined to be consumed for each learning iteration . at the large limit of @xmath34 ,",
    "the differential dynamics of @xmath67 and @xmath68 are calculated as @xmath69 and @xmath70 where @xmath71 is applied , while the new variables are defined as @xmath72 the dynamics for ngd can be provided in the same way : @xmath73 thus , the dynamics of the order parameters are @xmath74 @xmath75 where @xmath76 denotes the @xmath77th row of the matrix @xmath78 , while @xmath79 denotes the @xmath80th column of the matrix @xmath56 , and so on @xcite .",
    "in this section , we discuss how the learning dynamics depend on the correlation of teacher weight vectors @xmath81 .",
    "the results are also contrasted between gd and ngd .",
    "we set the number of the hidden units and the lengths of the teacher weight vectors as follows : @xmath82 we also restrict the initial conditions to @xmath83 because of the symmetry of the system , these restrictions are preserved throughout the learning",
    ". specifically , we use @xmath84\\!\\ ! ,",
    "\\quad      { \\bm{r } } = \\left[\\!\\ !",
    "\\begin{array}{cc } 10^{-2 } & 0 \\\\ 0 & 10^{-2 } \\end{array } \\!\\!\\right]\\!\\!.\\end{aligned}\\ ] ] therefore , we have four free parameters @xmath85 , @xmath86 , @xmath87 , and @xmath88 in this system . note that @xmath55 and @xmath89 are always symmetric matrices from the definitions of ( [ eq:18 ] ) and ( [ eq:20 ] ) .",
    "other parameters are set as @xmath90 , @xmath91 , and @xmath92 .",
    "various values for @xmath81 are employed to examine the influence of the correlation of the teacher hidden units .",
    "we sometimes use @xmath93 , the angle of the teacher weight vectors , instead of @xmath81 .    in this case ,",
    "@xmath35 and @xmath36 in the inverse of the fisher information matrix ( [ eq:14 ] ) can be simplified as @xmath94\\!\\ ! ,",
    "\\nonumber\\\\      { \\bm{\\theta}}_{2,2 } & = & d\\sqrt{a } \\left[\\!\\ !",
    "\\begin{array}{cc } -bq_{1,1 } & bq_{1,2 } \\\\",
    "bq_{1,2 } & 2a\\{a\\-b\\}\\-bq_{1,1 } \\end{array } \\!\\!\\right]\\!\\ ! , \\\\      { \\bm{\\theta}}_{1,2 } & = & { \\bm{\\theta}}_{2,1 } \\nonumber\\\\                    & = & -d\\sqrt{b } \\left[\\!\\ !",
    "\\begin{array}{cc } a\\{q_{1,1}\\+1\\}\\-b^2 & aq_{1,2 } \\\\",
    "aq_{1,2 } & a\\{q_{1,1}\\+1\\}\\-b^2 \\end{array } \\!\\!\\right]\\!\\ ! ,   \\nonumber\\end{aligned}\\ ] ] where @xmath95    here we summarize the order of each variable to @xmath34 .",
    "since the length of the input vector @xmath10 is @xmath96 , @xmath97 and @xmath98 are @xmath99 .",
    "this guarantees that the arguments of the activation function @xmath9 are @xmath99 .",
    "therefore , the lengths of the weight vectors , @xmath100 and @xmath101 , are @xmath99 . if the direction of the initial @xmath39 is chosen randomly , the size of @xmath102 , the correlation between @xmath39 and @xmath38 , is @xmath103 .",
    "the initial numerical values in ( [ eq:35 ] ) are defined according to these sizes .",
    "figure [ fig:2 ] shows the time evolution of the generalization error . in the gd ( fig .",
    "[ fig:2]a ) , the plateau was greatly prolonged as the correlation of the teacher weight vectors rose . in ngd ( fig .",
    "[ fig:2]b ) , almost no plateau occurred at any @xmath81 if @xmath23 was set small enough relative to the initial @xmath87 , and the generalization error was exponentially decreased .",
    "the plateau periods of fig .",
    "[ fig:1]a were measured and are shown in fig .",
    "[ fig:2]c , where we defined a plateau as occurring if @xmath104 .",
    "the order of the plateau lengths was about @xmath105 in gd .",
    "figure [ fig:3 ] shows the trajectories of the order parameters @xmath106 and @xmath107 . because of the symmetry , the latter plots are mirror images of the former . as @xmath87 is the correlation between the first student and the corresponding teacher ,",
    "the initial value is almost @xmath108 and the goal is @xmath109 ; @xmath88 is the correlation between the first student and the not corresponding teacher , and the initial value is almost @xmath108 and the goal is @xmath81 .",
    "therefore , the target location of the plots are @xmath110 and @xmath111 , respectively ( shown as @xmath112 ) .",
    "the other order parameters @xmath85 and @xmath86 are not shown . in the case of",
    "[ fig:3]a ) , the plots start at @xmath113 , turn back at @xmath114 , then approach @xmath115 ( the saddle , as explained in the next section ) , and finally reach @xmath112 .",
    "actually , the parameters never pass through the same place again because @xmath85 and @xmath86 are updated . in the case of ngd ( fig .",
    "[ fig:3]b ) , the plots start at @xmath113 and reach @xmath112 while avoiding @xmath115 .",
    "we performed a numerical simulation to confirm the dynamics at the above thermodynamics limit .",
    "the input dimension was @xmath116 , the teacher weight vectors were set as @xmath117\\!\\!,\\quad       { \\bm{b}}_2 = \\left[\\!\\ ! \\begin{array}{c } \\cos{\\kappa } \\\\",
    "\\sin{\\kappa } \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{array } \\!\\!\\right]\\!\\!,\\quad \\end{aligned}\\ ] ] and every initial @xmath39 was randomly and independently chosen from @xmath118 for each try .",
    "thus , the order parameters @xmath55 and @xmath56 were no longer limited by the restriction of ( [ eq:34 ] ) .",
    "the learning was performed using these real weight vectors and the original equations : ( [ eq:9 ] ) for gd and ( [ eq:11 ] ) for ngd .",
    "figures [ fig:4 ] and [ fig:5 ] show the time evolution of the generalization error and the trajectories of the order parameters in the same manner as figs .",
    "[ fig:2 ] and [ fig:3 ] , respectively .",
    "both figures support the statistical dynamics well , which suggests the constraint of ( [ eq:34 ] ) is a rather minor problem and the system retains most of its generality even with that restriction .",
    "here , we discuss why ngd is so effective even with a strong correlation between teacher hidden units .",
    "we consider the dynamics around the saddle of the generalization error under the conditions of ( [ eq:33 ] ) and ( [ eq:34 ] ) .",
    "this point , where all the differentials of the order parameters are zero and the hessian matrix is not positive definite nor negative definite , is shown as @xmath115 in figs .",
    "[ fig:3 ] and [ fig:5 ] : @xmath119 this saddle is a special point because 1 ) it corresponds to the goal both in the case of @xmath120 ( the teacher is a smaller network : @xmath121 ) and in the case that the student is a smaller network : @xmath122 , 2 ) in gd , the plateau occurs around it , and in ngd the student vectors avoid it , 3 ) it coincides with one of the singular points of the fisher information matrix since @xmath123 .     and @xmath124 belong to the plane made by the teacher weight vectors",
    "@xmath125 and @xmath126.,width=170 ]    we simplify the situation as shown in fig .",
    "[ fig:6 ] ; the two student weight vectors belong to the plane made by the two teacher weight vectors .",
    "this simplification is useful because we are now interested in how fast the student vectors leave this point for the goals .",
    "the correlations are re - parameterized by @xmath127 and @xmath128 as @xmath129 now , we have only two free parameters @xmath85 and @xmath128 .",
    "since the first derivative of @xmath128 can be written with @xmath85 and @xmath86 as @xmath130 we can formulate the angular velocity of @xmath128 at @xmath131 .",
    "the term @xmath132 included in @xmath133 can be ignored if the learning rate @xmath23 is set small enough .",
    "the angular velocity for gd is @xmath134 where @xmath135 .",
    "we notice that the order of @xmath136 is not greatly changed by @xmath127 .",
    "the velocity converges to zero in the first order of @xmath128 .",
    "moreover , it decreases as @xmath127 decreases .",
    "therefore , this equation supports the simulation results showing that the plateau is prolonged as the teacher correlation rises .",
    "the angular velocity for ngd is @xmath137 where @xmath138 .",
    "this velocity diverges to infinity as @xmath128 goes to zero .",
    "although it decreases as @xmath127 decreases , this effect would be canceled by @xmath139 near the saddle .",
    "therefore , this equation means that the student weight vectors are repelled by the saddle .",
    "in addition , this also supports the simulation results showing that the student weight vectors avoid the saddle and that the plateau does not occur even in the case of strongly correlated teacher hidden units .",
    "we have studied the on - line learning of soft committee machines under correlated teacher hidden units . the plateau in gd",
    "is largely prolonged at about @xmath105 as the correlation of the teacher weight vectors rises , but almost no plateau occurs in ngd with a low learning rate @xmath23 and this does not depend on the correlation .",
    "our analytical results for around the saddle reveal that the ngd avoided the saddle , even though the strong correlation of the teacher weight vectors forced the student weight vectors close to the saddle where the fisher information matrix is singular .",
    "99 k. fukumizu and s. amari : neural networks * 13 * ( 2000 ) 317 .",
    "d. saad and a. solla : phys .",
    "e * 52 * ( 1995 ) 4225 .",
    "s. amari : neural comput .",
    "* 10 * ( 1998 ) 251 .",
    "h. h. yang and s. amari : neural comput .",
    "* 10 * ( 1998 ) 2137 .",
    "s. amari , h. park and k. fukumizu : neural comput .",
    "* 12 * ( 2000 ) 1399 .",
    "m. rattray and d. saad : phys .",
    "e * 59 * ( 1999 ) 4523 ."
  ],
  "abstract_text": [
    "<S> the permutation symmetry of the hidden units in multilayer perceptrons causes the saddle structure and plateaus of the learning dynamics in gradient learning methods . </S>",
    "<S> the correlation of the weight vectors of hidden units in a teacher network is thought to affect this saddle structure , resulting in a prolonged learning time , but this mechanism is still unclear . in this paper , we discuss it with regard to soft committee machines and on - line learning using statistical mechanics . </S>",
    "<S> conventional gradient descent needs more time to break the symmetry as the correlation of the teacher weight vectors rises . on the other hand , </S>",
    "<S> no plateaus occur with natural gradient descent regardless of the correlation for the limit of a low learning rate . </S>",
    "<S> analytical results support these dynamics around the saddle point . </S>"
  ]
}