{
  "article_text": [
    "expert advice has become a well - established paradigm of machine learning in the last decade , in particular for prediction .",
    "it is very appealing from a theoretical point of view , as performance guarantees usually hold in the worst case , without any ( statistical ) assumption on the data .",
    "such assumptions are generally required for other statistical learning methods , often however not resulting in stronger guarantees .    using expert advice in the standard way",
    "seems a rather bad idea in some cases where the decisions of the learner or master algorithm influence the behavior of the environment or adversary .",
    "one example is the repeated prisoner s dilemma when the opponent plays ",
    "tit for tat \" ( see section [ sec : active ] ) .",
    "this was noted and resolved by @xcite , who introduced a  strategic expert algorithm \" for so - called reactive environments .",
    "their algorithm works with a finite class of experts and attains asymptotically optimal behavior",
    ". no convergence speed is asserted , and the analysis is quite different from that of standard experts algorithms .    in this paper , we show how the more general task with a countably infinite expert class can be accomplished , building on standard experts algorithms , and simultaneously also bounding the convergence rate ( @xmath0 , which can be actually improved to @xmath1 ) . to this aim , we will combine techniques from @xcite and obtain a master algorithm which performs well on loss functions that may _ increase in time_. then this is applied to ( possibly ) reactive problems by yielding the control to the selected expert for an increasing period of time steps . using a universal expert class defined by the countable set of all programs on some fixed universal turing machine , we obtain an algorithm which is in a sense asymptotically optimal with respect to _ any _ computable strategy .",
    "an easy additional construction guarantees that our algorithm is computable , in contrast to other universal approaches which are non - computable @xcite . to our knowledge",
    ", we also propose the first algorithm for non - stochastic bandit problems with countably many arms .",
    "the paper is structured as follows .",
    "section [ sec : algorithm ] introduces the problem setup , the notation , and the algorithm . in sections [ sec :",
    "master ] , we give the ( worst - case ) analysis of the master algorithm . the implications to active experts problems and a universal master algorithms are given in section [ sec : active ] .",
    "we discuss our results in section [ sec : discussion ] .",
    "we are acting in an online decision problem . ",
    "we \" is here an abbreviation for the master algorithm which is to be designed .",
    "an  online decision problem \" is to be understood in a very general sense , it is just a sequence of decisions each of which results in some loss",
    ". this could be e.g.  a prediction task , a repeated game , etc . in each round , that is at each time step @xmath2 , we have access to the recommendations of countably infinitely many  experts \" or strategies .",
    "( for simplicity , we restrict our notation to a countably infinite expert class , all results also hold for finite classes . ) we do not specify what exactly a ",
    "recommendation \" is  we just follow the advice of one expert . _ before _ we reveal our move , the adversary has to assign losses @xmath3 to _ all _ experts @xmath4 .",
    "there is an upper bound @xmath5 on the maximum loss the adversary may use .",
    "this quantity may depend on @xmath2 and is not controlled by the adversary . after the move , only the loss of the selected expert @xmath4 is revealed .",
    "our goal is to perform nearly as well as the best available strategy ( expert ) in terms of cumulative loss , after any number @xmath6 of time steps which is not known in advance .",
    "the difference between our loss and the loss of some expert is also termed _ regret_. we consider the general case of an _ adaptive _ adversary , which may assign losses depending on our past decisions .    if there are only finitely many experts or strategies , then it is common to give no prior preferences to any of them .",
    "formally , this is realized by defining uniform _ prior weights _",
    "@xmath7 for each expert @xmath4 .",
    "this is not possible for countably infinite expert classes , as there is no uniform distribution on the natural numbers . in this case , we need some non - uniform prior @xmath8 and require @xmath9 for all experts @xmath4 and @xmath10 .",
    "we also define the complexity of expert @xmath4 as @xmath11 .",
    "this quantity is important since in the full observation game ( i.e.  after our decision we get to know the losses of _ all _ experts ) , the regret can usually be bounded by some function of the best expert s complexity .    our algorithm  follow or explore \" ( @xmath12 , specified in fig .",
    "[ fig : foe ] ) builds on mcmahan and blum s online geometric optimization algorithm @xcite .",
    "it is a bandit version of a  follow the perturbed leader \" experts algorithm .",
    "this approach to online prediction and playing repeated games has been pioneered by hannan @xcite . for the full observation game and uniform prior",
    ", @xcite gave a very elegant analysis which is clearly different from the standard analysis of exponential weighting schemes .",
    "it has one advantage over other aggregating algorithms such as exponential weighting schemes : the analysis is not complicated if the learning rate is dynamic rather than fixed in advance .",
    "a dynamic learning rate is necessary if there is no target time @xmath6 known in advance . for non - uniform",
    "prior , an analysis was given in @xcite .",
    "the following issues are important for @xmath12 s design .    .",
    "since we are playing the bandit game ( as opposed to the full information game ) , we need to explore sufficiently @xcite . at each time step @xmath2 , we decide randomly according to some exploration rate @xmath13 whether to explore or not .",
    "if so , we would like to choose an expert according to the prior distribution .",
    "there is a caveat : in order to make the analysis go through , we have to assure that we are working with _ unbiased _ estimates of the losses .",
    "this is achieved by dividing the observed loss by the probability of choosing the expert . but this quantity could become arbitrarily large if we admit arbitrarily small weights .",
    "we address this problem by _ finitizing _ the expert pool at each time @xmath2 . for each expert @xmath4",
    ", we define an _ entering time _",
    "@xmath14 , that is , expert @xmath4 is active only for @xmath15 .",
    "we denote the set of active experts at time @xmath2 by @xmath16 . for exploration , the prior",
    "is then replaced by the finitized prior distribution @xmath17 , [ eq : unonu ] ( u_t = i)=w^i_t^i_j w^j_t^j .",
    "consequently , the maximum unbiasedly estimated instantaneous loss is ( note that the exploration probability also scales with the exploration rate @xmath18 ) [ eq : maxest ] b_t=. it is convenient for the analysis to assign estimated loss of @xmath19 to all currently inactive experts .",
    "observe finally that in this way , our master algorithm @xmath12 always deals with a finite expert class and is thus computable .",
    "( @xmath20 , specified in fig .",
    "[ fig : fpl ] ) is invoked if @xmath12 does not explore .",
    "just following the  leader \" ( the best expert so far ) may not be a good strategy @xcite .",
    "instead we subtract an exponentially distributed perturbation @xmath21 from the current score ( the complexity penalized past loss ) of the experts .",
    "an important detail of the @xmath20 subroutine is the _ learning rate _ @xmath22 , which should be adaptive if the total number of steps @xmath6 is not known in advance .",
    "please see e.g.@xcite for more details .",
    "also the variant of @xmath20 we use ( specified in fig . [ fig : fpl ] ) works on the finitized expert pool .",
    "note that each time randomness is used , it is assumed to be _ independent _ of the past randomness .",
    "performance is evaluated in terms of true or estimated cumulative loss , this is specified in the notation .",
    "e.g.  for the true loss of @xmath20 up to and including time @xmath6 we write @xmath23 , while the estimated loss of @xmath12 and not including time @xmath6 is @xmath24 .",
    "the following analysis uses mcmahan and blum s trick @xcite in order to prove bounds against adaptive adversary . with a different argument",
    ", it is possible to circumvent lemma [ lemma : behbeh ] , thus achieving better bounds @xcite .",
    "this will be briefly discussed in the last section .",
    "let @xmath25 be some sequence of upper bounds on the instantaneous losses , @xmath13 be a sequence of exploration rates , and @xmath22 be a _ decreasing _ sequence of learning rates .",
    "the analysis proceeds according to the following diagram ( where @xmath26 is an informal abbreviation for the loss and always refers to cumulative loss , but sometimes additionally to instantaneous loss ) .",
    "[ eq : diagram ] ^ ^ each  @xmath27 \" means that we bound the quantity on the left by the quantity on the right plus some additive term .",
    "the first and the last expressions are the losses of the @xmath12 algorithm and the best expert , respectively . the intermediate quantities belong to different algorithms , namely @xmath12 , @xmath20 , and a third one called @xmath28 for  infeasible \" fpl @xcite .",
    "@xmath28 , as specified in fig .",
    "[ fig : ifpl ] , is the same as @xmath20 except that it has access to an oracle providing the current estimated loss vector @xmath29 ( hence infeasible ) . then it assigns scores of @xmath30 instead of @xmath31 .",
    "the randomization of @xmath12 and @xmath20 gives rise to two filtrations of @xmath32-algebras .",
    "by @xmath33 we denote the @xmath32-algebra generated by the @xmath12 s randomness up to time @xmath2 , meaning _ only _ the random variables @xmath34 .",
    "then @xmath35 is a filtration ( @xmath36 is the trivial @xmath32-algebra ) .",
    "we may also write @xmath37 .",
    "similarly , @xmath38 is the @xmath32-algebra generated by the @xmath12 s _ and _",
    "@xmath20 s randomness up to time @xmath2 ( i.e.  @xmath39 )",
    ". then clearly @xmath40 for each @xmath2 .",
    "the reader should think of the expectations in ( [ eq : diagram ] ) as of both ordinary and _ conditional _ expectations .",
    "conditional expectations are mostly with respect to @xmath12 s past randomness @xmath41 .",
    "these conditional expectations of some random variable @xmath42 are abbreviated by _",
    "t[x]:=. then @xmath43 $ ] is an @xmath41-measurable random variable , meaning that its value is determined for fixed past randomness @xmath41 .",
    "note in particular that the estimated loss vectors @xmath44 are random vectors which depend on @xmath12 s randomness @xmath33 up to time @xmath2 . in this way , @xmath12 s ( and @xmath20 s and @xmath28 s ) actions depend on @xmath12 s past randomness . note , however , that they do not depend on @xmath20 s randomness @xmath45 .",
    "we now start with proving the diagram ( [ eq : diagram ] ) . in order to understand the analysis , it is important to consider each intermediate algorithm as a stand - alone procedure which is actually executed ( with an oracle if necessary ) on the specified inputs ( e.g.  on the estimated losses ) and has the asserted performance guarantees ( e.g.  again on the estimated losses ) .",
    "[ lemma : foefoe ] @xmath46 $ ] for each @xmath47 and @xmath48 , with probability at least @xmath49 we have _ t=1^t_t_t + .",
    "the sequence of random variables @xmath50 $ ] is a martingale with respect to the filtration @xmath38 ( not @xmath33 ! ) . in order to see this ,",
    "observe @xmath51= \\expect\\big(\\expect[\\loss\\foe_t|\\cala_{t-1}]\\big|\\calb_{t-1}\\big)$ ] and @xmath52=\\loss_t\\foe$ ] for @xmath53 , which implies & = & _ t=1^t ( - ) + & = & _ t=1^t-1 ( _ t- ) = x_t-1 .",
    "its differences are bounded : @xmath54 .",
    "hence , it follows from azuma s inequality ( see e.g.@xcite ) that the probability that @xmath55 exceeds some @xmath56 is bounded by @xmath57 .",
    "requesting @xmath58 and solving for @xmath59 gives the assertion .",
    "[ lemma : foefpl ] @xmath60 $ ] @xmath61 holds @xmath62 .",
    "this follows immediately from the specification of @xmath12 .",
    "clearly , a corresponding assertion for the ordinary expectations holds by just taking expectations on both sides .",
    "this is the case for all subsequent lemmas , except for lemma [ lemma : behbeh ] .",
    "the next lemma relating @xmath63 and @xmath64 is technical but intuitively clear .",
    "it states that in expectation , the real loss suffered by @xmath20 is the same as the estimated loss .",
    "this is simply because the loss estimate is unbiased . a combination of this and the previous lemma was shown in @xcite .",
    "note that @xmath65 is the loss @xmath66 estimated by @xmath12 , but for the expert @xmath67 chosen by @xmath20 .",
    "[ lemma : fplfpl ] @xmath68 $ ] for each @xmath69 , we have @xmath70 .",
    "let @xmath71 $ ] be the probability distribution over actions @xmath4 which @xmath20 uses at time @xmath2 , depending on the past randomness @xmath41 .",
    "let @xmath17 be the finitized prior distribution ( [ eq : unonu ] ) at time @xmath2 . then",
    "_ t[_t ] ( 1-_t)0 + _ t _ i=1^f_t^i [ ( 1-u_t^i)0+u_t^i _ t^i|_r_t=1i_t = i ] _ i=1^f_t^i_t^i = _ t[_t ] , where @xmath72 is the estimated loss under the condition that @xmath12 decided to explore ( @xmath73 ) and chose action @xmath74 .",
    "the following lemma relates the losses of @xmath20 and @xmath28 .",
    "it is proven in @xcite and @xcite .",
    "we give the full proof , since it is the only step in the analysis where we have to be careful with the upper loss bound @xmath75 .",
    "let @xmath19 be the upper bound on the estimated loss ( [ eq : maxest ] ) .",
    "( we remark that also for _ weighted averaging forecasters _ , losses which grow sufficiently slowly do not cause any problem in the analysis . in this way , it is straightforward to modify the algorithm by auer et al .",
    "@xcite for reactive tasks with a finite expert class . )",
    "[ lemma : fplifpl ] @xmath76 $ ] for all @xmath69 , @xmath77 holds .    if @xmath78 , @xmath79 and thus @xmath80 holds .",
    "this happens with probability @xmath81 .",
    "otherwise we have [ eq : efpl ] _ t_t= _",
    "i=1^_i_t = i _",
    "t^i d(x ) , where @xmath82 denotes the ( exponential ) distribution of the perturbations , i.e.  @xmath83 and density @xmath84 .",
    "the idea is now that if action @xmath4 was selected by @xmath20 , it is  because of the exponentially distributed perturbation  with high probability also selected by @xmath28 .",
    "formally , we write @xmath85 for @xmath86 , abbreviate @xmath87 , and denote by @xmath88 the integration leaving out the @xmath4th action .",
    "then , using @xmath89 @xmath90 if @xmath91 in the first equation , and @xmath92 in the last line , we get [ eq:1 ] _ i_t = i_t^i d(x ) & = _ x_i _ t^i",
    "d(x_i ) d(x_i)= _",
    "t^i^-(_ji\\{_t(_i-_j)+x_j})^+ d(x_i ) + & _ t^i^_t b_t^-(_ji\\{_t(_i-_j)+x_j}+_t b_t)^+ d(x_i ) + & ^_t b_t",
    "_ t^i^-(_ji\\{_t(_i+_t^i-_j-_t^j)+x_j})^+ d(x_i ) + & = ^_t b_t_i_t = i _ t^i d(x ) . summing over @xmath4 and using the analog of ( [ eq : efpl ] ) for @xmath28 , we see that if @xmath73 , then @xmath93 holds .",
    "thus @xmath94 .",
    "the assertion now follows by taking expectations w.r.t .",
    "@xmath95 .",
    "the next lemma relates the losses of @xmath28 and the best action in hindsight . for an oblivious adversary ( which means that the adversary s decisions do not depend on our past actions ) ,",
    "the proof is quite simple @xcite .",
    "an additional step is necessary for an adaptive adversary @xcite .",
    "[ lemma : ifplbeh ] @xmath96 $ ] assume that @xmath97 and @xmath14 depends monotonically on @xmath98 , i.e.@xmath99 if and only if @xmath100 .",
    "assume decreasing learning rate @xmath101 .",
    "for all @xmath102 and all @xmath103 , _",
    "t=1^t_t _ t^i+ .",
    "this is a modification of the corresponding proofs in @xcite and @xcite .",
    "we may fix the randomization @xmath104 and suppress it in the notation .",
    "then we only need to show [ eq : showtau ] _",
    "i1 \\{^i+ } , where the expectation is with respect to @xmath28 s randomness @xmath105 .",
    "assume first that the adversary is oblivious .",
    "we define an algorithm @xmath106 as a variant of @xmath28 which samples only one perturbation vector @xmath107 in the beginning and uses this in each time step , i.e.  @xmath108 .",
    "since the adversary is oblivious , @xmath106 is equivalent to @xmath28 in terms of expected performance .",
    "this is all we need to show ( [ eq : showtau ] ) .",
    "let @xmath109 and @xmath110 , then @xmath111 .",
    "recall @xmath16 .",
    "we argue by induction that for all @xmath102 , [ eq : ifplbehtau ] _ t=1^t _ t^a_t^i+ _",
    "t\\{}. this clearly holds for @xmath112 . for the induction step , we have to show [ eq : showtau2 ] _ t^i+ _ t\\{}+ _",
    "t+1^a & ^i^a_t+1 + _ t+1\\{}+ _ t+1^i^a_t+1 + & = _ t+1_1:t+1^i+",
    "_ t+1\\{}. the inequality is obvious if @xmath113 .",
    "otherwise , let @xmath114 .",
    "then _ t^i+ _ t\\ { } ^j+ & = & _ t=1^t_t^j _ t=1^tb_t + & = & _ t=1^t_t^i_t+1^a ^i^a_t+1 + _ t+1\\ { } shows ( [ eq : showtau2 ] ) . rearranging terms in ( [ eq : ifplbehtau ] ) , we see _ t=1^t _ t^a _ t^i + _",
    "t^i\\ { } + _ t=1^t ( q - k)^i_t^a(- ) the assertion ( [ eq : showtau ] )  still for oblivious adversary and @xmath108  then follows by taking expectations and using [ eq : showtau3 ] _ t^i  _ t\\{^i+ - } & & _ i1\\{^i+ } + [ eq : showtau4 ] _",
    "t=1^t ( q - k)^i_t^a(- ) & & _",
    "t\\ { }  . the second inequality of ( [ eq : showtau3 ] ) holds because @xmath14 depends monotonically on @xmath98 , and @xmath115 , and maximality of @xmath116 for @xmath117 . the second inequality of ( [ eq : showtau4 ] )",
    "can be proven by a simple application of the union bound , see e.g. ( * ? ? ?",
    "* lem.1 ) .    sampling the perturbations",
    "@xmath21 independently is equivalent under expectation to sampling @xmath107 only once .",
    "so assume that @xmath21 are sampled independently , i.e.  that @xmath28 is played against an oblivious adversary : ( [ eq : showtau ] ) remains valid .",
    "in the last step , we argue that then ( [ eq : showtau ] ) also holds for an _ adaptive _ adversary .",
    "this is true because the future actions of @xmath28 do not depend on its past actions , and therefore the adversary can not gain from deciding after having seen @xmath28 s decisions .",
    "this argument can be made formal , as shown in ( * ? ? ?",
    "* lemma 12 ) .",
    "( note the subtlety that the future actions of @xmath12 would depend on its past actions . )    finally , we give a relation between the estimated and true losses ( adapted from @xcite ) .",
    "[ lemma : behbeh ] @xmath118 $ ] for each @xmath102 , @xmath48 , and @xmath103 , we have ( i ) & & ^i ^i+ + _ t=1^^i-1b_t + ( ii ) & & ^i ^i+ + + _ t=1^^i-1b_t .    for @xmath15 , @xmath119 is a martingale , since = -^i = x_t-1 + -_t^i = x_t-1 .",
    "it is clear that @xmath120 .",
    "moreover , @xmath121 for @xmath15 , i.e.  we have bounded differences . by azuma s inequality , the actual value @xmath122 does not exceed @xmath123 with probability @xmath49 .",
    "this proves @xmath124 .",
    "to arrive at @xmath125 , take expectations and observe that @xmath124 fails with probability at most @xmath126 , in which case @xmath127 holds .",
    "we now combine the above results and derive an upper bound on the expected regret of @xmath12 against an adaptive adversary .",
    "[ th : general ] let @xmath97 , @xmath14 depend monotonically on @xmath98 , and the learning rate @xmath101 be decreasing .",
    "let @xmath128 be some possibly adaptive assignment of ( true ) loss vectors satisfying @xmath129 .",
    "then for all experts @xmath4 , we have with probability at least @xmath130 ^i++ _ t=1^^i-1b_t + _ t=1^t_t_tb_t^2 + _",
    "t=1^t_t b_t + ( + ) .",
    "consequently , in expectation , we have ^i++ _ t=1^^i-1b_t + _ t=1^t_t_tb_t^2 + _",
    "t=1^t_t b_t + + .    this follows by summing up all excess terms in the above lemmas . recall that we only need to take expectations on both sides of the assertions of lemmas [ lemma : foefpl][lemma : ifplbeh ] in order to obtain the second bound on the expectation ( and we do nt need lemma [ lemma : foefoe ] there ) .",
    "[ cor : arbitrary ] assume the conditions of theorem [ th : general ] and choose @xmath131 and @xmath132 . then ( i ) & b_t1,^i=(w^i)^-8&^i+o(()^11 + k^i t^ ) , + ( ii ) & b_t1,^i=(w^i)^-8&^i+o(()^11 + k^i t^ ) 1-t^-2 , + ( iii ) & b_t = t^,^i=(w^i)^-16&^i+o(()^22 + k^i t^ ) , + ( iv ) & b_t = t^,^i=(w^i)^-16&^i+o(()^22 + k^i t^)1-t^-2 , for all @xmath4 and @xmath102 ( recall @xmath11 ) . moreover , in both cases ( bounded and growing @xmath75 ) @xmath12 is asymptotically optimal w.r.t.each expert , i.e.  for all @xmath4 , _ t 0    the asymptotic optimality is sometimes termed _",
    "hannan - consistency _ , in particular if the limit equals zero .",
    "we only show the upper bound .",
    "assertions @xmath124-@xmath133 follow from the previous theorem : set @xmath134 , abbreviate @xmath135 , and observe that for @xmath136 and @xmath137 , we have w^_t=\\{w^i : t(w^i)^-}&\\{w^i : t^-w^i}t^- + _ t=1^^i-1b_t ( ^i-1)b_^i-1 & ( note @xmath138 )",
    ". then @xmath124 and @xmath125 follow from @xmath139 , @xmath140 , and @xmath141 and @xmath133 follow from @xmath142 , @xmath143 .",
    "the asymptotic optimality finally follows from the borel - cantelli lemma , since according to @xmath125 and @xmath133 , for an appropriate @xmath144 .    as mentioned in the first paragraph of this section , it is possible to avoid lemma [ lemma : behbeh ] , thus arriving at better bounds .",
    "e.g.  in @xmath124 , choosing @xmath145 , @xmath132 , and @xmath131 , a regret bound of @xmath146 can be shown .",
    "of course , also a corresponding high probability bound like @xmath125 holds .",
    "likewise , for a similar statement as @xmath141 , we may set @xmath147 , @xmath148 , @xmath132 , and @xmath131 , arriving at a regret bound of @xmath149 generally , in this way any regret bound @xmath150 is possible , at the cost of increasing @xmath151 where @xmath152 .",
    "_ regret _ can become a quite subtle notion if we start considering reactive environments , i.e.  care for future consequences of a decision .",
    "an extreme case is the ",
    "heaven - hell \" example : we have two experts , one always playing 0 (  saying a prayer \" ) , the other one always playing 1 (  cursing \" )",
    ". if we always follow the first expert , we stay in heaven and get no loss in each step .",
    "as soon as we  curse \" only once , we get into hell and receive maximum loss in all subsequent time steps . clearly , _ any _ algorithm without prior knowledge must  fail \" in this situation .",
    "one way to get around this problem is taking into account the _ actual _ ( realization of the ) game we are playing .",
    "for instance , after  cursing \" once , also the praying expert goes to hell together with us and subsequently has maximum loss .",
    "hence , were are interested in a regret defined as @xmath153 as in the previous section .",
    "so what is missing ?",
    "this becomes clear in the following example .",
    "consider the repeated ",
    "prisoner s dilemma \" against the tit - for - tat strategy @xcite .",
    "if we use two strategies as experts , namely  always cooperate \" and  always defect \" , then it is clear that always cooperating will have the best long - term reward .",
    "however standard expert advice or bandit master algorithm will not discover this , since it compares only the losses in one step , which are always lower for the defecting expert .",
    "to put it differently , minimizing short - term regret is not at all a good idea here .",
    "e.g. always defecting has no regret , while for always cooperating the regret grows _ linearly_. but this is only the case for short - term regret , i.e.  if we restrict to time intervals of length one .",
    "we therefore give the control to a selected expert for _ periods of increasing length_. precisely , we introduce a new time scale @xmath154 ( the _ basic _ time scale ) at which we have single games with losses @xmath155 .",
    "the master s time scale @xmath2 does not coincide with @xmath154 .",
    "instead , at each @xmath2 , the master gives control to the selected expert @xmath4 for @xmath156 single games and receives loss @xmath157 .",
    "( the points @xmath158 in basic time are defined recursively , see fig .",
    "[ fig : foetilde ] . )",
    "assume that the game has bounded instantaneous losses @xmath159 $ ] .",
    "then the master algorithm s instantaneous losses are bounded by @xmath75 .",
    "we denote the algorithm , which is completely specified in fig .",
    "[ fig : foetilde ] , by @xmath160 .",
    "then the following assertion is an easy consequence of the previous results .",
    "[ cor : active ] assume @xmath160 plays a repeated game with bounded instantaneous losses @xmath161 $ ] .",
    "choose @xmath132 , @xmath131 , @xmath162 and @xmath163 .",
    "then for all experts @xmath4 and all @xmath164 , _",
    "1:t & & _ 1:t^i+ o(()^22+k^i t^ ) 1-t^- + _ 1:t & & _",
    "1:t^i+o(()^22+k^i t^ ) .",
    "consequently , @xmath165 a.s .",
    "the rate of convergence is at least @xmath166 , and it can be improved to @xmath167 at the cost of a larger power of @xmath168 .",
    "this follows from changing the time scale from @xmath2 to @xmath154 in corollary [ cor : arbitrary ] : @xmath154 is of order @xmath169 .",
    "consequently , the regret bound is @xmath170 .",
    "broadly spoken , this means that @xmath171 performs asymptotically as well as the best expert .",
    "asymptotic performance guarantees for the strategic experts algorithm have been derived in @xcite .",
    "our results improve upon this by providing a rate of convergence",
    ". one can give further corollaries , e.g.  in terms of flexibility as defined in @xcite .",
    "since we can handle countably infinite expert classes , we may specify a _ universal _ experts algorithm .",
    "to this aim , let expert @xmath4 be derived from the @xmath4th ( valid ) program @xmath172 of some fixed universal turing machine .",
    "the @xmath4th program can be well - defined , e.g.  by representing programs as binary strings and lexicographically ordering them @xcite .",
    "before the expert is consulted , the relevant input is written to the input tape of the corresponding program .",
    "if the program halts , an appropriate part of the output is interpreted as the expert s recommendation . e.g.  if the decision is binary , then the first bit suffices .",
    "( if the program does not halt , we may for well - definedness just fill its output tape with zeros . )",
    "each expert is assigned a prior weight by @xmath173 , where @xmath174 is the length of the corresponding program and we assume the program tape to be binary . this construction parallels the definition of solomonoff s _ universal prior _ @xcite .",
    "[ cor : universal ] if @xmath160 is used together with a universal expert class as specified above and the parameters @xmath175 are chosen as in corollary [ cor : active ] , then it performs asymptotically at least as well as _ any computable expert _ @xmath4 .",
    "the upper bound on the rate of convergence is exponential in the complexity @xmath98 and proportional to @xmath176 ( improvable to @xmath177 ) .",
    "the universal prior has been used to define a universal agent aixi in a quite different way @xcite . note that like the universal prior and the aixi agent , our universal experts algorithm is not computable , since we can not check if a the computation of an expert halts .",
    "on the other hand , if used with computable experts , the algorithm is computationally feasible ( at each time @xmath2 we need to consider only finitely many experts ) .",
    "moreover , it is easy to impose an additional constraint on the computation time of each expert and abort the expert s computation after @xmath178 operations on the turing machine .",
    "we may choose some ( possibly rapidly ) growing function @xmath178 , e.g.@xmath179 .",
    "the resulting master algorithm is fully computable and has small regret with respect to all resource bounded strategies .",
    "it is important to keep in mind that corollaries [ cor : active ] and [ cor : universal ] give assertions relative to the experts performance merely on the _ actual _ action - observation sequence . in other words ,",
    "if we wish to assess how well @xmath160 does , we have to evaluate the actual _ value _ of the best expert @xcite .",
    "note that the whole point of our increasing time construction is to cause this actual value to coincide with the value under _ ideal _ conditions . for passive tasks",
    ", this coincidence always holds with any experts algorithm . with @xmath160 , the actual and the ideal value of an expert coincide in many further situations , such as  finitely controllable tasks \" . by this",
    "we mean cases where the best expert can drive the environment into some optimal state in a fixed finite number of time steps .",
    "an instance is the prisoner s dilemma with tit - for - tat being the opponent .",
    "the following is an example for a formalization of this statement .",
    "suppose @xmath160 acts in a ( fully or partially observable ) markov decision process .",
    "let there be a computable strategy which is able to reach an ideal ( that is optimal w.r.t .",
    "reward ) state sequence in a fixed number of time steps .",
    "then @xmath160 performs asymptotically optimal .",
    "this statement may be generalized to cases where only a close to optimal state sequence is reached with high probability .",
    "however , we need assumptions on the closeness to optimality for a given target probability , which are compatible with the sampling behavior of @xmath160 .    not all environments have this or similar nice properties . as mentioned above",
    ", any version of @xmath12 would not perform well in the  heaven - hell \" example .",
    "the following is a slightly more interesting variant of the heaven - hell task , where we might wish to learn optimal behavior , however @xmath12 will not .",
    "consider the heaven - hell example from the beginning of this section , but assume that if at time @xmath2 i am in hell and i  pray \" for @xmath2 consecutive time steps , i will get back into heaven . then it is not hard to see that @xmath12 s exploration is so dominant that almost surely , @xmath12 will eventually stay in hell .",
    "simulations with some @xmath180 matrix games show similar effects , depending on the opponent .",
    "we briefly discuss the repeated game of  chicken \" , the learner is the column player , the opponent s loss matrix is the transpose , choosing the fist column means to defect , the second to cooperate .",
    "hence , in the repeated game , it is socially optimal to take turns cooperating and defecting . ] . in this game",
    ", it is desirable for the learner to become the  dominant defector \" , i.e.  to defect in the majority of the cases while the opponent cooperates",
    ". let s call an opponent  primitive \" if he agrees to cooperate after a fixed number of consecutive defecting moves of @xmath12 , and let s call him  stubborn \" if this number is high",
    ". then @xmath12 learns to be the dominant defector against any primitive opponent , however stubborn . on the other hand",
    ", if the opponent is some learning strategy which also tries to be the dominant defector and learns faster ( we conducted the experiment with aixi @xcite ) , then @xmath12 settles for cooperating , and the opponent will be the dominant defector .",
    "interestingly however , aixi would not learn to defect against a stubborn primitive opponent . under this point of view",
    ", it seems questionable that there is something like a universally optimal balance of exploration vs.  exploitation in active learning at all .",
    "as mentioned in the beginning of section [ sec : master ] , the analysis we gave uses a trick from @xcite .",
    "such a trick seems necessary , as the basic fpl analysis only works for oblivious adversary .",
    "the simple argument from @xcite which we used in the last paragraph of the proof of lemma [ lemma : ifplbeh ] works only for full observation games ( note that considering the estimated losses , we were actually dealing with full observations there ) . in order to obtain a similar result in the partial observation case , we may argue as follows .",
    "we let the game proceed for @xmath6 time steps with independent randomization against an adaptive adversary .",
    "then we analyze @xmath12 s performance _ in retrospective_. in particular , we note that for the losses assigned by the adversary , @xmath12 s expected regret coincides with the regret of another , virtual algorithm , which uses ( in its fpl subroutine ) identical perturbations @xmath108 .",
    "performing the analysis for this virtual algorithm , we arrive at the desired assertion , however without needing lemma [ lemma : behbeh ] .",
    "this results in tighter bounds as stated above .",
    "the argument is formally elaborated in @xcite .    in practice ,",
    "the bounds we have proven seem irrelevant except for small expert classes , although asserting almost sure optimality and even a convergence rate .",
    "the exponential of the complexity @xmath168 may be huge .",
    "imagine for instance a moderately complex task and some good strategy , which can be coded with mere 500 bits .",
    "then its prior weight is @xmath181 , a constant which is not distinguishable from zero in all practical situations .",
    "thus , it seems that the bounds can be relevant at most for small expert classes with uniform prior .",
    "this is a general shortcoming of bandit style experts algorithms : for uniform prior a lower bound on the expected loss which scales with @xmath182 ( where @xmath183 is the size of the expert class ) has been proven @xcite .    in order to get a lower bound on @xmath12 s regret in the time @xmath6 ,",
    "observe that @xmath12 is a _ label - efficient _ learner @xcite : according to the definition in @xcite , we may assume that in each exploration step , we incur maximal loss @xmath75 .",
    "it is immediate that the same analysis then still holds . for label - efficient prediction , cesa - bianchi et al . @xcite",
    "have shown a lower regret bound of @xmath184 . since according to the remark at the end of section [ sec : master ] , we have an upper bound of @xmath185 , this is almost tight except for the additive @xmath186 term .",
    "it is an open problem to state a lower bound simultaneously tight in both @xmath168 and @xmath6 .",
    "even if the bounds , in particular @xmath168 , seem not practical , maybe @xmath12 would learn sufficiently quickly in practice anyway ?",
    "we believe that this is not so in most cases : the design of @xmath12 is too much tailored towards worst - case environments , @xmath12 is too _",
    "defensive_. assume that we have a  good \" and a  bad \" expert , and @xmath12 learns this fact after some time .",
    "then it still would spend a relatively huge fraction of @xmath132 to exploring the bad expert .",
    "such defensive behavior seems only acceptable if we are already starting with a class of good experts .        de  farias , d.p . ,",
    "megiddo , n. : how to combine expert ( and novice ) advice when actions impact the environment ? in thrun , s. , saul , l. , schlkopf , b. , eds . : advances in neural information processing systems 16 . mit press , cambridge , ma ( 2004 )",
    "kalai , a. , vempala , s. : efficient algorithms for online decision . in : proc .",
    "16th annual conference on learning theory ( colt-2003 ) .",
    "lecture notes in artificial intelligence , berlin , springer ( 2003 ) 506521    mcmahan , h.b . , blum , a. : online geometric optimization in the bandit setting against an adaptive adversary . in : 17th annual conference on learning theory ( colt ) .",
    "volume 3120 of lecture notes in computer science . , springer ( 2004 ) 109123        auer , p. , cesa - bianchi , n. , freund , y. , schapire , r.e . : gambling in a rigged casino : the adversarial multi - armed bandit problem . in : proc .",
    "36th annual symposium on foundations of computer science ( focs 1995 ) , los alamitos , ca , ieee computer society press ( 1995 ) 322331              hutter , m. : towards a universal theory of artificial intelligence based on algorithmic probability and sequential decisions .",
    "12@xmath187 european conference on machine learning ( ecml-2001 ) ( 2001 ) 226238      cesa - bianchi , n. , lugosi , g. , stoltz , g. : minimizing regret with label efficient prediction . in : 17th annual conference on learning theory ( colt ) .",
    "volume 3120 of lecture notes in computer science . , springer ( 2004 ) 7792"
  ],
  "abstract_text": [
    "<S> this paper shows how universal learning can be achieved with expert advice . to this aim , we specify an experts algorithm with the following characteristics : ( a ) it uses only feedback from the actions actually chosen ( bandit setup ) , ( b ) it can be applied with countably infinite expert classes , and ( c ) it copes with losses that may grow in time appropriately slowly . we prove loss bounds against an adaptive adversary . from this </S>",
    "<S> , we obtain a master algorithm for  reactive \" experts problems , which means that the master s actions may influence the behavior of the adversary . </S>",
    "<S> our algorithm can significantly outperform standard experts algorithms on such problems . finally , we combine it with a universal expert class . </S>",
    "<S> the resulting universal learner performs  in a certain sense  almost as well as any computable strategy , for any online decision problem . </S>",
    "<S> we also specify the ( worst - case ) convergence speed , which is very slow .    </S>",
    "<S> * keywords . * </S>",
    "<S> prediction with expert advice , responsive environments , partial observation game , bandits , universal learning , asymptotic optimality .    </S>",
    "<S> tcs - tr - a-05 - 4    july 2005    idsia-15 - 05 </S>"
  ]
}