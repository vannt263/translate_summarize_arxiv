{
  "article_text": [
    "well before extrasolar planets were discovered , @xcite and @xcite have speculated that tidal interactions between disks and embedded protoplanets would lead to planet migration .",
    "@xcite suggested that two different types of migration could occur .",
    "@xcite studied numerically the effects of disk self - gravity in two - dimensional simulations of planet - disk interactions .",
    "@xcite reported by an analytical derivation that the disk gravity accelerates the planetary migration .",
    "recently , simulations by @xcite confirmed that the self - gravity indeed accelerates the type i migration .",
    "they implemented a 2d self - gravity solver in their code using the fast fourier transform ( fft ) method of @xcite , which requires a logarithmic radial spacing ( @xmath0 ) .    in newtonian gravity",
    ", we can define the gravitational potential @xmath1 associated with the mass density , @xmath2 , by the volume integral @xmath3 over all space , where @xmath4 is the gravitational constant .",
    "rewriting eq .",
    "( [ eq_pot1 ] ) in differential form , we obtain poisson s equation @xmath5 with @xmath1 satisfying the boundary condition @xmath6 at all times . the numerical `` poisson solvers '' to eq .",
    "( [ eq_pot2 ] ) can be classified into two categories : difference methods and integral methods .",
    "the difference methods solve eq .",
    "( [ eq_pot2 ] ) directly by either finite - difference or finite - element methods with necessary boundary conditions .",
    "the known boundary conditions at infinity are usually not very useful if a finite domain is considered .",
    "user specified boundary conditions or dirichlet boundary conditions obtained by direct summation are often required .",
    "a key advantage of difference methods is that , generally , they are relatively fast once initialized .",
    "however , they have low or limited accuracy , and they often rely on the integral method to provide the boundary conditions",
    ".    the integral methods are to integrate eq .",
    "( [ eq_pot1 ] ) directly .",
    "they have the advantage that the summation stops naturally at the domain boundaries .",
    "however an integral method has two difficulties in a practical implementation .",
    "one is that the integral has a point mass singularities ( i.e. when @xmath7 in eq .",
    "( [ eq_pot1 ] ) ) , which is often circumvented by introducing softening .",
    "the other difficulty is that they are computationally prohibitive for a large system .",
    "the computational cost can be significantly lowered if the fft method can be used .",
    "the gravitational field , @xmath8 , is more convenient in many applications .",
    "it can be calculated via numerical derivatives , which generally have poor precision . to obtain high accuracy",
    ", we can integrate the field directly by @xmath9 which shares the same two difficulties as calculating eq .",
    "( [ eq_pot1 ] ) .    in this paper",
    ", we present a method for computing the disk self - gravity for quasi-2d disk models .",
    "we consider a disk with the cylindrical grid @xmath10 .",
    "we assume that the scale height of the disk ( semi - thickness ) is only radius - dependent , i.e. , @xmath11 , and it is quite small , @xmath12 ( namely geometrically thin disks ) .",
    "we also assume that the vertical structure of the density can be described by some function @xmath13 that is independent of @xmath14 and time @xmath15 , i.e. , @xmath16 in this paper , we are particularly interested in the gravitational potential and field force at the @xmath17 plane . although our method is valid for any function of @xmath13 , we assume that the density vertically has a gaussian distribution . under this assumption , @xmath18 with the presence of the vertical structure ( [ eq_z ] ) , the migration rate of the protoplanet is reduced up to 50% ( see @xcite ) .",
    "we assume that the disk has a constant sound speed @xmath19 .",
    "then from the scale height @xmath20 where @xmath21 is the keplerian velocity @xmath22 , we obtain @xmath23 other profiles for the scale height are also easy to handle .",
    "for example , if the aspect ratio , @xmath24 , is constant , then @xmath25",
    ". the numerical verification in  [ sec : test ] actually uses a constant @xmath26 in the whole domain .",
    "since we are interested in the potential and field in the @xmath17 plane , the easiest method to obtain @xmath1 is to integrate directly the equation @xmath27 where @xmath28 and @xmath29 are the inner and outer radial boundaries , and @xmath30 is the vertically integrated density . as in @xcite ,",
    "we have employed two different coordinate systems in the radial direction : @xmath31 as the field grid and @xmath32 as the source grid . for simplicity , we set @xmath33 and @xmath34 .    the rest of the paper is organized as follows . in ",
    "[ sec : non - axi ] , we describe the method given by @xcite on how to solve eq .",
    "( [ 2d_eq1 ] ) using a direct integral via a green s function method . our method ,",
    "though essentially follows the one given by @xcite , differs in that we use a direct summation method on a uniform grid in the radial direction , which is the same grid used in our hydro code .",
    "in addition , we present two approaches to circumvent the singularity in eq .",
    "( [ 2d_eq1 ] ) and two methods to calculate the force field . for comparison ,",
    "we have also implemented a 2d tree - code with a simplified 3d treatment to calculate the self - gravity for disks with vertical structures . in ",
    "[ sec : test ] we present an efficient parallel implementation scheme on distributed memory computers .",
    "we describe a new algorithm to calculate the gravity force at an arbitrary point .",
    "we also present numerical test results to compare different approaches . a few concluding remarks are given in  [ sec : conc ] .",
    "we present here a numerical method to compute integral ( [ 2d_eq1 ] ) . for zero - thickness non - axisymmetric disk , the classical fft method ( e.g. * ? ? ?",
    "* ) based on polar grid , which has been implemented by @xcite , is often used . however , as pointed out by @xcite , the fft method has a few drawbacks .",
    "first it requires a grid with a logarithmic radial spacing , which could be inconvenient to most hydro - solvers using uniform spacing . secondly , to avoid the well - known alias issue , the fft method requires to double the number of cells along the radial direction .",
    "the fft calculation is thus done on a grid that has twice the extent of the hydrodynamic grid along the radial direction , which induces some complications in the calculation of the convolution kernels , as well as many communications between both grids .",
    "furthermore , with the presence of vertical structure , it is impossible to apply the fft method in 3d cylindrical coordinates because neither the potential nor the gravitational field can be represented as convolution products in @xmath35 .",
    "there is no such coordinate transformation as described in @xcite to make the integral ( [ 2d_eq1 ] ) become a convolution product in all coordinates .",
    "the green s function method given by @xcite avoids the fft in the radial direction . instead",
    ", a pseudo - spectral method on a scaled cosine radial grid is used to achieve the high - order accuracy .",
    "moreover , a known , time - independent vertical structure is easy to incorporate . in the following ,",
    "we describe modifications to their method so that it can be applied directly to a uniform radial grid .",
    "@xcite proposed a direct integral method to solve eq .",
    "( [ 2d_eq1 ] ) . for the sake of completeness",
    ", we recap the key steps in their method here .",
    "first , we introduce a softening @xmath36 to eq .",
    "( [ 2d_eq1 ] ) and denote @xmath37 where @xmath36 can be zero or a radius - dependent parameter that will be discussed later in section [ sec : ts ] .",
    "function @xmath38 can be computed either analytically or by numerical quadrature . for the special case of @xmath13 defined by ( [ eq_z ] ) , @xmath39 where @xmath40 , and @xmath41 denotes the modified bessel function of the second kind .",
    "let @xmath42 then eq . ( [ 2d_eq1 ] ) becomes @xmath43 note that both @xmath30 and @xmath44 are periodic functions with period @xmath45 , and they are in a natural convolution representation in eq .",
    "( [ 2d_eq1_2 ] ) .",
    "applying fourier transform to ( [ 2d_eq1_2 ] ) with respect to ( w.r.t . )",
    "@xmath14 , and using the convolution theorem , we obtain @xmath46 , \\label{2d_eq3}\\ ] ] where @xmath47 represents the coefficients in the fourier series expansion of @xmath48 , which is given as @xmath49 note that our equation ( [ 2d_eq3 ] ) is slightly different from the equation ( 39 ) in @xcite , since we have put all the relevant coefficients in the representation of @xmath44 in eq .",
    "( [ eq_i ] ) .",
    "the integral over @xmath31 in eq .",
    "( [ 2d_eq3 ] ) was evaluated using chebyshev spectral method on a `` chebyshev - roots grid '' by @xcite . however , for most hydro codes that use a uniform grid , it is desirable to use the same uniform grid for both hydro and self - gravity solutions to avoid interpolation between the hydro solver and self - gravity solver ( which is inconvenient and introduces interpolation error ) .",
    "we propose to integrate ( [ 2d_eq3 ] ) directly with numerical quadrature using the available discrete values of @xmath50 and @xmath51 on a uniform grid .",
    "note that @xmath52 has an analytic expression and is not changed with time as long as the vertical structure remains the same . as proposed by @xcite , we can pre - compute its fourier transform , @xmath53 , to speed up the algorithm .",
    "the whole algorithm can be summarized as follows :    1 .   for each pair",
    "@xmath54 , we pre - compute @xmath53 .",
    "take the fourier transform of @xmath52 , which is defined in eq .",
    "( [ eq_i ] ) , w.r.t .",
    "@xmath55 , @xmath56 where @xmath57 is the number of cells in @xmath14-direction . since @xmath58 is a real and even function w.r.t .",
    "@xmath55 , only the discrete cosine transform is needed and @xmath59 modes need to be stored .",
    "2 .   take the fourier transform of @xmath60 w.r.t .",
    "@xmath61 to obtain @xmath62 , @xmath63 .",
    "3 .   calculate eq .",
    "( [ 2d_eq3 ] ) by numerical quadrature in the radial direction to obtain @xmath64 .",
    "either the midpoint or trapezoidal rule can be used , depending on the grid point distribution of the source grid .",
    "4 .   take the inverse fourier transform of @xmath65 w.r.t .",
    "@xmath14 to obtain @xmath66 .",
    "we should emphasize that the pre - computing of @xmath53 plays a major role in the efficiency of our self - gravity solver .",
    "this step can be calculated once for all during a long time simulation of the disk - planet interaction system .",
    "we find that it costs at least fifty times more than the rest of steps .",
    "one can not afford to calculate it in every time step .",
    "the overall computational cost is @xmath67 , where @xmath68 is the number of cells in the radial direction .",
    "again , these steps are essentially the same as in @xcite , except that the integration is done on a uniform polar grid using direct summation .      when @xmath69 , the integrand in eq .",
    "( [ intgz ] ) contains a singularity .",
    "note that the density splitting method of @xcite can not be used in our case because the density profile is unknown during the pre - computing of @xmath53 .",
    "we have tested two approaches to solve the singularity issue .",
    "one approach is to use a nonzero softening , @xmath70 .",
    "we propose a function that scales roughly linearly with @xmath71 , i.e. , @xmath72 .",
    "the idea is that the mass distribution from the small spheres at each cell center should approximate a smooth mass distribution instead of a sum of @xmath73-functions .",
    "we optimize @xmath74 by minimizing the relative error in @xmath66 for sharply peaked gaussian mass distributions located at @xmath32 ranging from 0.6 to 1.8 ( scaled value for the disk - planet problem ) with @xmath75 .",
    "we find a piecewise linear function for @xmath76 : @xmath77 our experiments show that @xmath76 is fairly insensitive to different resolutions in the radial direction .",
    "@xcite , however , have argued that the softening must scale with @xmath31 .",
    "this might be because the logarithmic grid in the radial direction is used in their fft implementation .",
    "for the uniform grid along the radial direction , we find that even a constant @xmath78 , e.g. , @xmath79 , yields good results for the density peak near @xmath80 .",
    "we remark that our choice of the softening should not be mixed with the softening that is used for calculating the gravity from the planet .",
    "it has been pointed out by @xcite that the softening used between the disk and planet should be at least 0.75 times the physical size of the grid zone .",
    "@xcite suggested using @xmath81 for both disk self - gravity and the gravity between disk cells and planet .",
    "we find that this choice is too big and produces large error for disk self - gravity in our implementation .",
    "therefore we use two different softenings in our simulations : for the disk self - gravity @xmath82 , and for the gravity between disk and planet @xmath83 ( @xmath84 is the planet position in radial direction ) .",
    "the coefficient 0.1 , which is still much smaller than the values generally used in the literature , is chosen partially due to the presence of vertical structure , and it matches very well with the @xcite theory on the torque evaluation .",
    "a question arises that the different softening choices may introduce inconsistency between the disk and planet .",
    "this concern can be alleviated by the fact that the gravity force for disk cells near the planet are dominated by the planet gravity .",
    "therefore this inconsistency does not have much impact on the dynamics of the disk .    to eliminate the dependence on the softening",
    ", we have implemented the other approach , which is to use different grids for @xmath31 and @xmath32 so that @xmath85 is never equal to @xmath86 .",
    "this approach has been used by @xcite for their spectral method . in our case ,",
    "the integral of eq .",
    "( [ 2d_eq3 ] ) is performed on a fixed uniform grid of @xmath32 . because @xmath32 is the cell - centered grid , we define @xmath31 as the node centered grid , i.e.",
    ", @xmath87 note that the @xmath31 grid has one more points than the @xmath32 grid . after calculating @xmath38 for @xmath31 at the node - center",
    ", we can obtain the @xmath38 for @xmath31 at the cell - center by interpolation , and then calculate the potential @xmath66 at the cell - center .",
    "we can also calculate directly the potential @xmath66 with @xmath31 at the node center for use of the field calculation .",
    "we have implemented two approaches to calculate the field components .",
    "the first approach is the difference method : we calculate the potential first and then use the finite - difference method to calculate the field .",
    "if the potential is calculated at the cell - center @xmath88 , we use the central - difference to obtain the field : @xmath89 where the notation @xmath90 . note that we need values of @xmath91 and @xmath92 for the @xmath93 component at the boundary cells located at @xmath94 and @xmath95 ; otherwise , one - side finite - difference , which is not very accurate , should be used .",
    "if the potential is calculated at the edge - center @xmath96 , the field can be calculated as @xmath97    high order methods that use wide stencils are also possible .",
    "in fact , we can obtain the derivative @xmath98 with only one extra fft in the above algorithm of calculating the potential . after obtaining @xmath64",
    ", we take the inverse transformation of @xmath99 to get the @xmath98 .    the second approach for the field calculation",
    "is the integral method : we directly convolute the field green s function with the mass distribution .",
    "the differentiation in @xmath93 and @xmath98 can be applied directly to the green function , which yields @xmath100    since we have two field components , @xmath93 and @xmath98 , the integral method takes twice the computation time of the difference method .",
    "but , we expect that the integral method might have higher accuracy than the finite difference approximation . however , if the mode cut - off approach , which will be described in section [ sec : para ] , is used , it is possible that the integral method may lose this advantage .",
    "in fact , we observed in our numerical tests that the integral method of ( [ gr1 ] ) and ( [ gphi1 ] ) with the mode cut - off gave larger errors and more than double the computation and communication times of the finite - difference method .",
    "yet another approach we explored to solve for the field involved using a hierarchical tree - code . in this approach , we treat each cell as a particle , and hence an effective force law between two points , @xmath101 and @xmath102 , in the disk that accounts for the vertical structure is required .",
    "just as before , we assume @xmath103 . and",
    "since symmetry arguments require @xmath13 to be an even function of @xmath35 , the z - component of the force must be zero at the @xmath17 plane . therefore , the force is directed in the plane of the disk and it has a magnitude factor given by @xmath104 where @xmath105 .",
    "if we further assume @xmath13 is given by eq .",
    "( [ eq_z ] ) , then we can rewrite eq .",
    "( [ eq_tc1 ] ) as @xmath106 where @xmath107 is called the 3d correction factor with respect to the 2d force without the vertical structure . as @xmath108 , ( [ eq_tc3 ] ) approaches to the 3d factor defined by @xcite .",
    "we note that this force law is a line - line force ( giving the net interaction between the mass distributed along two vertical lines at @xmath101 and @xmath102 ) .",
    "this is different from what is done in the green s function method , which uses a point - line interaction . by setting @xmath109 in eq .",
    "( [ eq_tc2 ] ) , we can recover the interaction force law of the green s function method .",
    "however , if we wished to include line - line interactions in the green s function method , we would have to abandon the more efficient potential calculation of eq .",
    "( [ intgz ] ) and instead use eqs .",
    "( [ gr1 ] ) and ( [ gphi1 ] ) with @xmath110 and @xmath111 defined appropriately .    re - scaling the integration in eq .",
    "( [ eq_tc3 ] ) and introducing the non - dimensional variables @xmath112 and @xmath113 gives @xmath114 finally , we perform an orthogonal coordinate transformation using the variables @xmath115 this allows us to simplify eq .",
    "( [ eq_tc4 ] ) to @xmath116 revealing that it is actually independent of @xmath78 .",
    "in fact , similar to eq .",
    "( [ intgz ] ) we can express eq .",
    "( [ eq_tc5 ] ) as @xmath117 where @xmath41 and @xmath118 denote the modified bessel function of the second kind and its derivative .",
    "the tree - code requires the calculation of @xmath119 and its first and second derivatives over a wide range of @xmath120 ( ranging from @xmath121 to @xmath122 ) . calculating eq .",
    "( [ eq_tc6 ] ) on the fly is prohibitively expensive and should be avoided .",
    "another option is to pre - compute eq .",
    "( [ eq_tc6 ] ) and its derivatives at a discrete set of @xmath120 values .",
    "then when @xmath119 is needed , we interpolate the pre - computed data",
    ". this approach can provide a significant speedup , but because the data is stored in main memory it is still too slow for our purposes .     3d correction factor ( eq .",
    "( [ eq_tc6 ] ) ) and relative error plotted vs. non - dimensional distance ( @xmath120 ) using model eq .",
    "( [ eq_tc7]).,title=\"fig:\",scaledwidth=45.0% ]   3d correction factor ( eq .",
    "( [ eq_tc6 ] ) ) and relative error plotted vs. non - dimensional distance ( @xmath120 ) using model eq .",
    "( [ eq_tc7]).,title=\"fig:\",scaledwidth=45.0% ]     3d force factor ( eq .",
    "( [ eq_tc2 ] ) ) and relative error plotted vs. non - dimensional distance ( @xmath120 ) using model eq .",
    "( [ eq_tc7]).,title=\"fig:\",scaledwidth=45.0% ]   3d force factor ( eq .",
    "( [ eq_tc2 ] ) ) and relative error plotted vs. non - dimensional distance ( @xmath120 ) using model eq .",
    "( [ eq_tc7]).,title=\"fig:\",scaledwidth=45.0% ]    another approach we explored involves approximating eq .",
    "( [ eq_tc6 ] ) with a model function that is accurate yet inexpensive enough to compute on the fly .",
    "plotting @xmath119 reveals its form ( fig .",
    "[ fig_tc1 ] ) and suggests a model function of the form @xmath123 we find optimal parameter values to be @xmath124 and @xmath125 . the relative error introduced by using this model",
    "is plotted with @xmath126 and @xmath127 in fig .",
    "[ fig_tc1 ] and [ fig_tc2 ] .",
    "since generally @xmath128 with @xmath129 , the model is acceptable for distance or softening larger than @xmath130 .",
    "this model requires the calculation of one power function , a few additions and multiplications and one division . for our application",
    ", we found this to be the most efficient way to include 3d structure using a tree - code .",
    "our tree - code is implemented in a distributed memory parallel architecture using mpi , just like our green s function fft algorithm .",
    "there are many papers detailing parallel tree - codes @xcite .",
    "so we simply highlight the important points of our algorithm here .    1",
    ".   we begin by pre - computing the tree structure of our grid , which does not change with time .",
    "each processor retains a copy of this _ quad - tree _ to be used throughout the simulation .",
    "all processors compute partial sums ( using local particles ) of the moments for all nodes in the tree .",
    "these partial sums are added and broadcast back to all the processors .",
    "4 .   a complete copy of the density profile",
    "is distributed to all the processors .",
    "this is required for load balancing and computing the direct summation part of the force ( step 5(c ) ) .",
    "forces are computed at every grid cell .",
    "the disk is partitioned azimuthally ( the symmetry providing ideal load balancing ) , and processors are assigned equal - sized sectors of particles ( grid cells ) .",
    "each particle traverses the _ quad - tree _ to include interactions with all other particles . 1 .",
    "if a particle - node interaction meets the accuracy criterion , then the particle interacts with that node via the moments computed in step 3 . 2 .",
    "if the accuracy criterion is not met ( the node is too large or the particle - node separation too small ) , then we drop one level in the tree and check all the rejected node s sub - nodes for particle - node interactions .",
    "last , if we reach a leaf - node ( a node with no sub - nodes ) and the accuracy criterion is not met we perform direct summation over all the leaf - node s particles .",
    "as in @xcite , we consider the density function to be a gaussian sphere given by @xmath131 where @xmath132 is the normalized total mass , @xmath133 is a parameter that controls the width of the mass distribution and it has similar role as the scale height in eq .",
    "( [ eq_z ] ) .",
    "the potential on the @xmath17 plane has been given by @xcite @xmath134 where @xmath135 is the error function @xmath136 for a collection of gaussian spheres centered on the @xmath17 plane with the same @xmath133 , the @xmath35-dependent vertical structure can be factored out , i.e. , @xmath137 where @xmath138 is the center of each sphere , @xmath13 gives the vertical structure @xmath139 @xmath140 is the surface density @xmath141 and @xmath142 is the distance between @xmath101 and @xmath138 .",
    "notice that eq .",
    "( [ eq_z2 ] ) is the same as eq .",
    "( [ eq_z ] ) with a constant scale height @xmath143 .",
    "we use three gaussian spheres located at @xmath144 , @xmath145 , and @xmath146 with a total mass of 2,1/2 , and 1 respectively , i.e. , @xmath147 where @xmath148 represents the analytic solution .",
    "the exact potential will be @xmath149 where @xmath150 is defined by eq .",
    "( [ eq_psi ] ) . the exact field @xmath93 and @xmath98 can also be calculated accordingly .",
    "the tests are done in the computational domain @xmath151\\times[0,2\\pi]$ ] with grid @xmath152 .",
    "the normalized total mass for the disk is @xmath153 .",
    "we choose @xmath154 so that the cells near @xmath80 are nearly square . without specification",
    ", we always use @xmath155 , which reaches the convergence in both the torque calculation on planet and the azimuthal averaged potential vorticity distribution on the disk in our disk - planet interaction simulations @xcite .",
    "since the surface density ( [ den_exact ] ) approaches to zero outside the computational domain , the exact potential ( [ pot_exact ] ) is still valid for our truncated domain . for convenience ,",
    "we use the following notations for comparison : @xmath156 denotes the number of processors , @xmath157 denotes the maximum error over the whole domain , @xmath158 denotes the maximum relative error , and @xmath159 is the convergence order in @xmath157 . since the force is a vector that has two component , we use the following formula to calculate @xmath157 @xmath160 where @xmath161 and @xmath162 are the force components in @xmath31- and @xmath14-direction respectively . the local relative error for a variable @xmath163 is defined as @xmath164 . for the force , @xmath165 . the global relative error for a variable @xmath163",
    "is defined as @xmath166    all of the computation are performed on a parallel linux cluster at the los alamos national lab .",
    "each node of the cluster is dual - core amd opteron(tm ) processor with 2.8 g hz and 2 gb local memory .",
    "our hydro simulation for the interaction of disk and proto - planet problem is performed on a high resolution grid , e.g. , @xmath167 grid .",
    "the whole domain is split into annular regions for parallel computation .",
    "each annular region has the same number of cells in the radial direction .",
    "the fourier transform and its inverse can be parallelized without much difficulty because they require no communication between different processors . however , the numerical quadrature of eq .",
    "( [ 2d_eq3 ] ) is done in the radial direction , where each processor holds only part of the information about the density distribution in the whole domain .",
    "we have tested two approaches to parallelize the numerical quadrature . in the first approach , we start by computing a partial quadrature including only source terms from the local density distribution , i.e. each processor computes a partial sum of eq .",
    "( [ 2d_eq3 ] ) for all @xmath31 and @xmath168 .",
    "we then apply a global communication to obtain the complete summation .    in the second approach ,",
    "we begin by performing a global communication over the whole domain so that every processor has a complete copy of the density profile for @xmath169 $ ]",
    ". then the complete numerical quadrature ( [ 2d_eq3 ] ) can be performed to obtain @xmath64 for the local portion of @xmath31 grid .",
    "we remark that in the pre - computing stage , no matter which approach is used , each processor can compute its own portion of the fourier transform @xmath53 independently and store it for the later use",
    ". it does not need to have a copy of whole profile for all @xmath31 and @xmath32 .",
    "in the following we will first propose a strategy to reduce the parallel communication cost , and then compare the above two approaches in calculating potential @xmath66 . for simplicity ,",
    "we denote the first approach as approach i , and the second as approach ii .",
    "the communication cost in both approaches is proportional to both the number of processors and the amount of data being communicated ( total grid size ) .",
    "consequently , we expect that the communication cost will dominate the computation cost as the number of processors is increased , eventually becoming a bottleneck for a large number of processors .",
    "note that the communication is done in the frequency domain , where the high modes decay exponentially for a smooth function .",
    "thus we can truncate the fourier modes above a cutoff parameter ( @xmath170 ) without a significant loss of accuracy .",
    "if @xmath170 is far smaller than @xmath57 ( the total number of mode ) , the communicate cost can be reduced to a small of fraction of the cost before the cutoff .",
    "for both approaches , we can calculate @xmath170 based on the fourier transformation of the density profile . for a specific radius @xmath171 , we define the energy contained in @xmath172 as @xmath173 . each processor",
    "then computes an @xmath174 based on satisfying some preset fraction @xmath175 ( e.g. , @xmath176 ) of the total energy for modes above @xmath174 .",
    "next , each processor finds its maximum @xmath177 over its range of @xmath31 .",
    "last , all the processors compare their @xmath177 values to obtain the global maximum @xmath170 that is used to truncate the fourier series . note that @xmath170 calculated in this way does not vary with the number of processors .",
    "for approach i , the @xmath174 can also be evaluated based on @xmath65 instead of @xmath178 , because the communication is done after the partial summation . to be more accurate in the field calculation , we instead can evaluate @xmath174 based on the energy defined by @xmath179 .",
    "the extra factor of @xmath168 is included so that we obtain the energy of the force , which is the derivative of @xmath1 .",
    "notice , however , that if we compute @xmath177 and @xmath170 as described above , @xmath170 will vary with the number of the processors .",
    "this is because each processor computes only a partial sum of @xmath64 . to produce a relatively constant @xmath170 , after each processor finds its maximum @xmath177",
    ", all the processors take an energy - weighted average of their @xmath177 to obtain the global @xmath170 .",
    "it is observed that the energy - weighting average gives an @xmath170 that remains nearly constant while the number of processors varies between 1 to @xmath156 . for verification reason",
    ", we can require that @xmath170 be independent of the number of processors .",
    "however , that requires a costly global communication between different processors . nonetheless , we find that @xmath170 will only increase slightly with the number of processors , which means that increasing the number of processors will not degrade the numerical accuracy .",
    "we define a similar preset fraction @xmath175 to the previous density mode cut - off approach .",
    "after extensive experiments , we observe that @xmath180 in @xmath65 cut - off produces similar results to @xmath181 in @xmath178 cut - off .    for density distributions in disk - planet simulations ,",
    "the energy of the zero mode ( @xmath182 ) often dominates that of all other modes combined .",
    "thus , when calculating the energy using @xmath183 or @xmath184 , it is convenient to exclude the zero mode from the total energy .",
    "notice that if the energy is defined as @xmath185 , the zero - mode is naturally excluded . to remove the numerical noise ,",
    "we multiply the energy of the zero mode by a small number ( e.g. , @xmath186 ) and add it to the total energy . in our disk - planet simulations , the initial density distribution is axisymmetric , and hence zero mode is enough . with the time evolution of the surface density , @xmath170 begins to increase quickly until it reaches a certain number , which is insensitive to different resolutions and different initial power - law density profiles .",
    "however @xmath170 does vary a lot with the planet mass and sound speed .",
    "for an example , we obtain the following data for simulations of an isothermal disk with different planet mass ( @xmath187 ) and sound speed ( @xmath19 ) , @xmath188 note that @xmath170 is much smaller than the total number of modes in @xmath14-direction , @xmath189 .      here ,",
    "we study how the number of cut - off modes , @xmath170 , impacts the computational efficiency and accuracy .",
    "we test the problem with parallel computation using 100 processors .",
    "we will show the results only for approach ii .",
    "similar results have also been obtained for approach i.    fig .",
    "[ cpu1 ] shows the variation in the computation and communication time with different numbers of cut - off modes .",
    "the total cost displayed in term of cpu time includes the computation of one fft of the density field , the global communication of the density spectrum up - to the cut - off mode @xmath170 right after the fft , a quadrature calculation in the radial direction .",
    "[ cpu1 ] shows that the communication time increases at a slower rate with @xmath170 than the computation time .    the computation and communication cost for evaluating @xmath190 ( left ) and the error variation ( right ) as a function of @xmath170 .",
    ", title=\"fig:\",scaledwidth=45.0% ] the computation and communication cost for evaluating @xmath190 ( left ) and the error variation ( right ) as a function of @xmath170 . , title=\"fig:\",scaledwidth=45.0% ]    the computation and communication cost for evaluating @xmath190 ( left ) and the error variation ( right ) as a function of energy cut - off thresholds @xmath175 .",
    "the cut - off is based on the energy of @xmath183 .",
    ", title=\"fig:\",scaledwidth=45.0% ] the computation and communication cost for evaluating @xmath190 ( left ) and the error variation ( right ) as a function of energy cut - off thresholds @xmath175 .",
    "the cut - off is based on the energy of @xmath183 .",
    ", title=\"fig:\",scaledwidth=45.0% ]    the energy cut - off approach depends on the cut - off threshold @xmath175 .",
    "we also vary the size of @xmath175 to see how it impacts the cut - off mode .",
    "[ cpu2 ] shows the simulation results for different @xmath175 .",
    "figs [ cpu1 ] and [ cpu2 ] show that our cut - off threshold , @xmath176 , which corresponds to @xmath191 , appears to be the optimal number to balance accuracy and efficiency for this problem",
    ". this energy cut - off threshold is insensitive to grids with different resolutions .      the approach i is implemented via domain decomposition of the source grid @xmath32 . in terms of the communication cost ,",
    "it involves a global reduction of @xmath192 data from every processor and a distribution of @xmath193 data to each processor , where @xmath156 is the number of processors .",
    "here we use a fixed mode cut - off number @xmath191 .",
    "[ cpu3 ] shows how the communication and computation cost vary with the number processors .",
    "note that the communication cost remains relatively constant no matter how many number of the processors we use .",
    "this is a surprise , because we expect that the communication cost is proportional to the length of the communicated data , which is proportional to the number of processors .",
    "we remark that one might obtain a different performance for a different mpi implementation or a different parallel cluster . in year 2007",
    ", we observed using the same code that the communication cost increases linearly with the number of processors . in year 2008 , our parallel cluster has been upgraded with a new parallel software and a new inter - connection .",
    "the data shown in fig .",
    "[ cpu3 ] is calculated on the new cluster .",
    "the communication and total cost for evaluating @xmath190 varying with the number of processors in mpi .",
    ", scaledwidth=80.0% ]      the approach ii described above is implemented via domain decomposition of the field grid @xmath31 .",
    "it involves a global gathering of @xmath193 data from every processor and a broadcast of @xmath192 data to each processor .",
    "while the amount of communicated data is the same for both approaches , approach i also involves a global summation operation resulting in a total communication cost of about 10% more than approach ii with the same value @xmath170 .",
    "again we show some results using only a fixed cut - off mode number @xmath194 .",
    "fig.[cpu3 ] ( approach ii ) shows the performance comparison with the first approach .",
    "similar to the results of approach i described in the last subsection , the communication cost remains nearly constant .",
    "it is clear that approach ii ( this approach ) is faster than the approach i. therefore we will use approach ii as our choice of the methods in the tests hereafter .",
    "[ cpu4 ] shows the parallel efficiency for different numbers of processors ( @xmath195 ) and grids with different resolutions .",
    "the parallel efficiency is defined by @xmath196 where @xmath197 is the run time of the parallel algorithm , and @xmath198 is the runtime of the sequential algorithm using one processor .",
    "for the fine resolution grid @xmath199 , we replace @xmath198 with @xmath200 due to the memory limitation of the processors .",
    "it is clear that for a smaller number of processors and a larger grid , the parallel efficiency is better .",
    "[ cpu4 ] shows that the parallel efficiency can be larger than 1 at certain stages .",
    "the reason is not clear to us .    the parallel efficiency for grids with different resolutions and different number of processors .",
    "@xmath194 is used .",
    ", scaledwidth=80.0% ]      we have tested two approaches for computing the potential and field : one is with the softening eq .",
    "( [ softn ] ) and the other is without softening but with a shifted grid defined by eq .",
    "( [ shiftg ] ) .",
    "note that the field is calculated differently using different central - difference methods .",
    "the field force with softening is calculated using central - difference eqs .",
    "( [ fd1 ] ) and ( [ fd2 ] ) .",
    "the field force without softening is calculated using central - difference eqs .",
    "( [ fd3 ] ) and ( [ fd4 ] ) .",
    "to minimize the impact of the cut - off mode , we use a fixed cut - off number @xmath191 , which produces nearly the same results as without cut - off ( see  [ sec : test_cut - off ] ) .",
    "table [ tab1 ] shows the numerical errors for both approaches .",
    "the convergence order is calculated based on the maximum absolute error .",
    "the results show nearly second - order convergence as the mesh is refined .",
    "this is in agreement with the accuracy of our method , because both the quadrature rule to calculate the potential and the finite - difference method to calculate the force are of second - order accuracy .    based on global relative errors ( @xmath201 in table [ tab1 ] ) for both potential and field , we see that the potential method using the shifted grid without softening is more accurate than with softening .",
    "yet the difference is relatively small .",
    ".[tab1 ] numerical results for the potential calculation with and without softening eq .",
    "( [ softn ] ) .",
    "the top half is for with softening and the bottom half is for shifted grid without softening . [",
    "cols=\"^,^,^,^,^,^,^ \" , ]     from the results given in table [ tc_fixed_np ] we draw the following conclusions .",
    "first , this method does not appear to converge to the exact solution .",
    "this is actually expected since the model force function ( eq .",
    "( [ eq_tc7 ] ) ) with our chosen softening does not become more accurate as the grid is refined .",
    "we observe that the maximum error asymptotically approaches to @xmath202 , which is much larger than the error using the green s function method for highly resolved grids .",
    "next , we see that the time complexity scales approximately with @xmath203 , which is consistent with the theoretical prediction .",
    "finally , we should remark that the softening has a large impact on the accuracy of the solutions for the tree - code .",
    "if @xmath204 is used , the maximum error and global relative error become 20.57 and 3.045% respectively .",
    "parallel efficiency results can be seen in table [ tc_fixed_grid ] .",
    "although we find our tree - code scales well with the number of processors , the actual value of the cpu time is more than two orders of magnitude greater than our green s function method ! we have experimented with tuning tree code parameters and other memory optimizations and have found that while it may be possible to gain a factor of @xmath205 in speedup over the results of table [ tc_fixed_grid ] , our tree - code method always performs _ much _ slower than our green s function method .",
    "in this paper , we have presented a fast and accurate solver to calculate the potential and self - gravity forces for the disk systems .",
    "this method is implemented on a polar grid , and fft can be used in the azimuthal direction .",
    "the pre - calculation of the green s function and its fft play a major role in the algorithm to reduce the computational cost .",
    "we think it could be the main reason why the green s function method is much faster than the particle tree - code method .",
    "we also presented an efficient method in implementing the solver on parallel computers .",
    "we find the computational cost for the self - gravity solver to be comparable to that of the hydro solver for large , highly resolved grids run on a distributed memory parallel architecture .",
    "we also developed a 2d tree - code solver , which uses a relatively inexpensive model force to accurately account for the vertical structure of the disk .",
    "we have applied our self - gravity solver to simulations of disk - planet interaction system . compared with the simulations without self - gravity , the total computation time",
    "is increased by only 30% for a parallel computation with 100 processors .",
    "we have also confirmed that the 2d self - gravity indeed accelerates the planet migration .",
    "these results will be reported elsewhere .",
    "* acknowledgment : * we would like to thank dr .",
    "chan for helpful discussion during his stay at los alamos .",
    "we also thanks the referee for many useful comments .",
    "this research was performed under the auspices of the department of energy .",
    "it was supported by the laboratory directed research and development ( ldrd ) program at los alamos .",
    "it is also available as los alamos national laboratory report , ."
  ],
  "abstract_text": [
    "<S> disk self - gravity could play an important role in the dynamic evolution of interaction between disks and embedded protoplanets . </S>",
    "<S> we have developed a fast and accurate solver to calculate the disk potential and disk self - gravity forces for disk systems on a uniform polar grid . </S>",
    "<S> our method follows closely the method given by chan et al . </S>",
    "<S> ( 2006 ) , in which an fft in the azimuthal direction is performed and a direct integral approach in the frequency domain in the radial direction is implemented on a uniform polar grid . </S>",
    "<S> this method can be very effective for disks with vertical structures that depend only on the disk radius , achieving the same computational efficiency as for zero - thickness disks .    </S>",
    "<S> we describe how to parallelize the solver efficiently on distributed parallel computers . </S>",
    "<S> we propose a mode - cutoff procedure to reduce the parallel communication cost and achieve nearly linear scalability for a large number of processors . for comparison , </S>",
    "<S> we have also developed a particle - based fast tree - code to calculate the self - gravity of the disk system with vertical structure . </S>",
    "<S> the numerical results show that our direct integral method is at least two order of magnitudes faster than our optimized tree - code approach . </S>"
  ]
}