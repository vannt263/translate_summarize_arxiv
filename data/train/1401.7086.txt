{
  "article_text": [
    "it is well known that a quadrature method of degree @xmath0 has an error bound of order @xmath1 ; many proofs rely on taylor s theorem , which then requires the existence of the @xmath0th derivative .",
    "of course , it can be asked what happens if the @xmath0th derivative fails to exist .",
    "the question was first investigated in @xcite , which proved error bounds using lower derivatives .",
    "these bounds do not attain the full order allotted by the degree of the method ; instead they are consigned to no more than the number of derivatives plus one .",
    "recently attention has returned to the issue of integration of insufficiently nice functions , where @xcite proved a few error bounds using lower order derivatives than the degrees of the methods involved .",
    "@xcite looked into the trapezoidal rule and simpson s rule and found that , for merely once differentiable functions , the trapezoidal rule had a better bound than simpson s rule .    a different direction is to optimize quadrature rules under the constraint that the integrand is only once differentiable ; this work is led by @xcite and there are quite a few papers on the subject . the optimal rules indeed differ from the established gauss - legendre rules .",
    "there is also a focus on simpson s inequality and its applications , as demonstrated by @xcite who proved a bound using an arbitrary order derivative .",
    "one unifying theme in all these results is the inability to achieve @xmath2th order convergence with only the @xmath0th derivative .",
    "we settle this question once and for all , by proving this impossibility . in other words ,",
    "@xmath0 derivatives are not only sufficient ( in conjunction with a suitable method ) , but also a _ necessary _ condition of achieving @xmath1th order convergence .",
    "we prove the contrapositive by exhibiting functions , exactly @xmath0 times differentiable , that bound the error of any method from below at @xmath1th order .",
    "for space , we use the symbol @xmath3 instead of @xmath0 in the ensuing discussion .",
    "[ t1 ] the region of integration @xmath4 $ ] be specified , and @xmath5 be a fixed natural number .",
    "let @xmath6 be any sequence of internal quadrature methods , with @xmath7 with @xmath8 fixed .",
    "define the exact integral to be @xmath9 then there exists a sequence of functions @xmath10 along with a sequence of finite sets @xmath11 satisfying    * @xmath12 $ ] , and has size @xmath13 * @xmath14\\setminus c^k[a , b]$ ] * @xmath15\\setminus s_n)$ ] * @xmath16}\\leq\\frac{(b - a)}{4}k!$ ] * @xmath17\\setminus s_n}=k!$ ]    such that the sequence of integration errors of @xmath18 on @xmath19 has order at most @xmath20 , or in other words @xmath21    by adversarial construction .",
    "let @xmath19 be a @xmath20th order spline such that @xmath22 for all @xmath23 , and have maximal integral with unit leading coefficient .",
    "first , for @xmath24 define @xmath25 and the quarter - interpolated points @xmath26 then explicitly , if @xmath20 is odd , @xmath27}\\\\ ( x - x_{n , i})^k,\\;&x\\in{\\left[x_{n , i},\\bar{x}_{n , i+1/4}\\right]}\\\\ \\left(x-\\bar{x}_{n , i+1/2}\\right)^k+2\\left(\\frac{\\delta x_i}{4}\\right)^k,\\;&x\\in{\\left[\\bar{x}_{n , i+1/4},\\bar{x}_{n , i+1/2}\\right]}\\\\ -\\left(x-\\bar{x}_{n , i+1/2}\\right)^k+2\\left(\\frac{\\delta x_i}{4}\\right)^k,\\;&x\\in{\\left[\\bar{x}_{n , i+1/2},\\bar{x}_{n , i+3/4}\\right]}\\\\ -(x - x_{n , i+1})^k,\\;&x\\in{\\left[\\bar{x}_{n , i+3/4},x_{n , i+1}\\right]}\\\\ ( x - x_{n , n})^k,\\;&x\\in{\\left[x_{n , n},b\\right ] } \\end{cases } \\intertext{for all valid $ 1\\leq i\\leq n-1 $ ; and if $ k$ is even , } f_n&=\\begin{cases } ( x - x_{n,1})^k,\\;&x\\in{\\left[a,\\bar{x}_{1 + 1/4}\\right]}\\\\ -\\left(x-\\bar{x}_{n , i+1/2}\\right)^k+2\\left(\\frac{\\delta x_i}{4}\\right)^k,\\;&x\\in{\\left[\\bar{x}_{n , i+1/4},\\bar{x}_{n , i+3/4}\\right]}\\\\ ( x - x_{n , j})^k,\\;&x\\in{\\left[\\bar{x}_{(j-1)+3/4},\\bar{x}_{n , j+1/4}\\right]}\\\\ ( x - x_{n , n})^k,\\;&x\\in{\\left[\\bar{x}_{(n-1)+3/4},b\\right ] } \\end{cases}\\end{aligned}\\ ] ] for all valid @xmath24 and @xmath28 .",
    "an easier way to understand this is to consider the `` basic unit '' of structure between two @xmath29 , shown in figure [ base ] between @xmath30 and @xmath31 .",
    "the concept is to splice 4 ( shifted ) copies of the curve @xmath32 together such that the two outer pieces are convex and the inner pieces concave .",
    "if @xmath20 is even , then it is analytic at the endpoints and midpoint .     for various values of @xmath20 , shown for @xmath30 and @xmath31 ]",
    "now it is clear that @xmath19 is @xmath34 $ ] , is analytic except at most at the points @xmath35 a set of size @xmath36 ( only @xmath37 points if @xmath20 is odd ) .",
    "the @xmath3th derivative of @xmath19 exists and is piecewise linear , with slope @xmath38 ; the length of a piece is bounded above by @xmath39 , possible only if all points @xmath29 are at endpoints , thus the derivative is bounded by that product .",
    "if @xmath20 is odd the piece can have length @xmath40 , but its midpoint is at 0 and that does not change the bound .",
    "the @xmath20th derivative is the piecewise constant function @xmath38 where it exists .",
    "note that @xmath41 by construction ; it remains only to compute @xmath42 , which is easily done by considering the basic unit : @xmath43\\\\ & \\hspace{0.12in}=\\int_{-\\frac{\\delta x_i}{4}}^{\\frac{\\delta x_i}{4}}\\!2\\left(\\frac{\\delta x_i}{4}\\right)^k\\!\\mathrm{d}x\\quad{\\bigl(\\forall k\\bigr)}\\\\ & \\hspace{0.12in}=\\frac{(\\delta x_i)^{k+1}}{4^k}\\text{.}\\end{aligned}\\ ] ] the pieces for the endpoints are easy : @xmath44 thus @xmath42 , and the integration error , is simply @xmath45 this is straightforward to minimize by picking equally spaced points ( we ignore the issue of picking points closer to the endpoints , which is unimportant ) , whereby the error is bounded below by @xmath46 therefore the method has at most order @xmath20 convergence , and the theorem is proved .",
    "there are two main ways of conceptualizing a `` sequence of quadrature methods '' .",
    "the first is an arbitrary - order family of methods , for example the newton - cotes or gauss - legendre , with an increasing number of points .",
    "the second is to take a fixed quadrature method , and form a composite rule with @xmath47 subintervals , for all @xmath47 .",
    "note that the construction obviates all dependence on the weights chosen by the quadrature method .",
    "the restriction to internal quadrature methods is not necessary : a point outside @xmath4 $ ] can easily be made to evaluate to 0 , with the result that the error bound becomes worse .    nor do methods that rely on the derivatives of the function escape this fate .",
    "it is straightforward to check that the @xmath3th and lower derivatives of @xmath19 are all 0 at each @xmath29 , and so unless the method uses the @xmath20th derivative ( which does nt exist in the first place ) , the construction stands valid .",
    "the size of @xmath48 grows linearly in @xmath47 , which disallows attempts to sidestep the theorem by using less points for evaluation than the number of points known to be bad .",
    "we may then ask the question : if the number of bad points is finitely bounded , can something more be salvaged ?",
    "the idea is , say with a composite rule , with a sufficient number of subintervals , to confine all the bad points to specific subinterval(s ) , whereby all other subintervals may achieve their degree - based bound .",
    "the answer is a qualified yes . the proof of theorem [ t1 ] can be easily modified to use a very small number of bad points , but this only allows improvement by 1 order , to @xmath49 .",
    "we shall reuse the setting in the of the preceding theorem .",
    "[ t2 ] there exists a sequence of functions @xmath10 along with a sequence of finite sets @xmath11 satisfying    * @xmath12 $ ] , and has size @xmath50 if @xmath20 is odd , or @xmath51 if @xmath20 is even * @xmath14\\setminus c^k[a , b]$ ] * @xmath15\\setminus s_n)$ ] * @xmath16}\\leq\\frac{(b - a)}{4}k!$ ] * @xmath17\\setminus s_n}=k!$ ]    such that the sequence of integration errors of @xmath18 on @xmath19 has order at most @xmath49 , or in other words @xmath52    by adversarial construction .",
    "let @xmath23 be an index such that @xmath53 is maximized .",
    "let @xmath19 be a @xmath20th order spline such that @xmath54 outside of the open interval @xmath55 , and have maximal integral with unit leading coefficient .",
    "explicitly , @xmath56}\\\\ \\left(x-\\frac{\\delta x}{2}\\right)^k+2\\left(\\frac{\\delta x}{4}\\right)^k,\\;&x\\in{\\left[x_{n , i}+\\frac{\\delta x}{4},x_{n , i}+\\frac{\\delta x}{2}\\right]}\\\\ -\\left(x-\\frac{\\delta x}{2}\\right)^k+2\\left(\\frac{\\delta x}{4}\\right)^k,\\;&x\\in{\\left[x_{n , i}+\\frac{\\delta x}{2},x_{n , i}+\\frac{3\\delta x}{4}\\right]}\\\\ -(x - x_{n , i+1})^k,\\;&x\\in{\\left[x_{n , i}+\\frac{3\\delta x}{4},x_{n , i+1}\\right]}\\\\ 0\\quad&\\text{otherwise } \\end{cases}\\text { if $ k$ is odd,}\\\\ f_n&=\\begin{cases } ( x - x_{n , i})^k,\\;&x\\in{\\left[x_{n , i},x_{n , i}+\\frac{\\delta x}{4}\\right]}\\\\ -\\left(x-\\frac{\\delta x}{2}\\right)^k+2\\left(\\frac{\\delta x}{4}\\right)^k,\\;&x\\in{\\left[x_{n , i}+\\frac{\\delta x}{4},x_{n , i}+\\frac{3\\delta x}{4}\\right]}\\\\ ( x - x_{n , i+1})^k,\\;&x\\in{\\left[x_{n , i}+\\frac{3\\delta x}{4},x_{n , i+1}\\right]}\\\\ 0\\quad&\\text{otherwise } \\end{cases}\\text { if $ k$ is even.}\\end{aligned}\\ ] ] this is the same construction as in theorem 1 , except that only one of the basic units is used .",
    "the discussion about the continuity classification of @xmath19 holds , with the `` bad '' set @xmath57 of size 5 ; if @xmath20 is even , the midpoint is analytic as well .",
    "the comments on the bounds on derivatives hold .",
    "@xmath58 remains 0 as all evaluation points are 0 , and the exact same calculation as before shows that @xmath59 with @xmath47 points to be chosen in @xmath4 $ ] it is clear that @xmath60 is bounded below by @xmath61 ; an attempt to pick the midpoint @xmath47 times runs afoul of the fact that the error for the endpoints is even worse .",
    "hence we have that @xmath62 with order at most @xmath49 , and the theorem is proved .",
    "we can tighten this result to requiring 3 ( or 2 , for even @xmath20 ) bad points by noticing that the endpoints @xmath63 do not need to be bad .",
    "instead we can use the analytic continuation of @xmath19 beyond these points ( the monomial @xmath64 ) , and assume that @xmath18 can exactly integrate the rest of the function ; the one piece between those two points alone makes the lower bound .",
    "we may also keep the quadrature result 0 by making the @xmath49th derivative go negative quickly and smoothly connect to the zero function ; this however necessitates breaking analyticity .",
    "it may be asked next , can there be a finite , fixed set of bad points in the function , and how much improvement does that generate ?",
    "we consider this moot , as when the bad points are fixed , the usual methods may be employed to simply _ find _ them all , so that we may segment the region of integration at these points .",
    "thus , all problems relating to having less than @xmath0 derivatives completely vanish .",
    "the main implication of these theorems is that it is asymptotically fruitless to use any method that has 2 degrees or more beyond the differentiability class of the integrand : theorem [ t2 ] proves the extra degrees have no effect .",
    "an interesting observation is of the non - local effect of a single point of non - niceness destroying convergence everywhere ; this has been noted in interpolation theory , under the name of the _ principle of contamination _",
    "given the strong connection between interpolation polynomials and numerical integration , seeing the principle apply here is hardly a surprise .",
    "another insight that drops out of the details is a heuristic to minimize the impact of this lower bound : minimize the maximal distance between two evaluation points .",
    "that equally spaced points are optimal against this adversary speaks to the difference between optimizing for degrees of exactness and optimizing for absolute error under only @xmath0 derivatives .",
    "thus of course the high degree newton - cotes rules retain their severe disadvantages , and we still do not recommend them ; rather that a highly composite trapezoid or simpson s rule be considered instead of the theoretically degree - superior gauss - legendre or curtis - clenshaw families , even at equal degree .",
    "the author would like to thank rajesh pereira , university of guelph , for help on formatting and literature search .",
    "this work was supported by a natural sciences and engineering research council of canada postgraduate scholarship .",
    "johann engelbrecht , igor fedotov , tanya fedotova , and ansie harding .",
    "`` error bounds for quadrature methods involving lower order derivatives '' . _ international journal of mathematical education in science and technology _ 34(6 ) , 831846 , 2003 .",
    "doi : http://dx.doi.org/10.1080/00207390310001595429 [ ]    david cruz - uribe and c. j. neugebauer .",
    "`` sharp error bounds for the trapezoidal rule and simpson s rule '' . _ journal of inequalities in pure and applied mathematics _",
    "3(4 ) , art . 49 , 2002 .",
    "url http://www.emis.de/journals/jipam/article201.html    nenad ujevi .",
    "`` error inequalities for a quadrature formula and applications '' . _ computers & mathematics with applications _",
    "48(1011 ) , 15311540 , 2004 .",
    "doi : http://dx.doi.org/10.1016/j.camwa.2004.05.007 [ ]    vu nhat huy and quc - anh ng .",
    "`` new inequalities of simpson - like type involving @xmath47 knots and the @xmath65th derivative '' . _ mathematical and computer modelling _",
    "52(34 ) , 522528 , 2010 .",
    "doi : http://dx.doi.org/10.1016/j.mcm.2010.03.049 [ ]    edward b. saff .",
    "`` a principle of contamination in best polynomial approximation '' . in gmez - fernandez ,",
    "j.a . , guerra - vzquez ,  f. , lpez - lagomasino ,  g. , jimnez - pozo ,  m.a .",
    "_ approximation and optimization _ ,",
    "springer , heidelberg , 1987 .",
    "doi : http://dx.doi.org/10.1007/bfb0089584 [ ]"
  ],
  "abstract_text": [
    "<S> results on the error bounds of quadrature methods are well known  most state that if the method has degree @xmath0 , and the integrand has @xmath0 derivatives , then the error is order @xmath1 . </S>",
    "<S> we prove here a converse : that if the integrand fails to have @xmath0 derivatives , even only at a finite number of points , no method , regardless of its degree , can guarantee convergence more than order @xmath0 . even if the integrand fails to have @xmath0 derivatives at just 3 ( for even @xmath0 , 2 ) points , </S>",
    "<S> no method can produce order more than @xmath1 convergence . </S>",
    "<S> this is done by an adversarial proof : we explicitly construct the functions that exhibit such error ; simple splines turn out to suffice . </S>"
  ]
}