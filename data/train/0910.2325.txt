{
  "article_text": [
    "the contribution of jim berger to the better understanding of bayesian testing is fundamental and wide - ranging , from establishing the fundamental difficulties with @xmath0-values in @xcite to formalising the intrinsic bayes factors in @xcite , to solving the difficulty with improper priors in @xcite , and beyond ! while our contribution in this area is obviously much more limited , we aim at presenting here the most standard approaches to the approximation of bayes factors .",
    "the bayes factor indeed is a fundamental procedure that stands at the core of the bayesian theory of testing hypotheses , at least in the approach advocated by both @xcite and by @xcite .",
    "( note that @xcite , provides a reassessment of the crucial role of @xcite in setting a formal framework for bayesian testing as well as for regular inference . )",
    "given an hypothesis @xmath1 on the parameter @xmath2 of a statistical model , with observation @xmath3 and density @xmath4 , under a compatible prior of the form @xmath5 the _ bayes factor _ is defined as the posterior odds to prior odds ratio , namely @xmath6model choice can be considered from a similar perspective , since , under the bayesian paradigm ( see , e.g. , @xcite ) , the comparison of models @xmath7 where the family @xmath8 can be finite or infinite , leads to posterior probabilities of the models under comparison such that @xmath9 where @xmath10 is the prior probability of model @xmath11 .    in this short survey",
    ", we consider some of the most common monte carlo solutions used to approximate a generic bayes factor or its fundamental component , the _ evidence _ @xmath12 aka the marginal likelihood .",
    "longer entries can be found in @xcite , @xcite , @xcite , or @xcite .",
    "note that we only briefly mention here trans - dimensional methods issued from the revolutionary paper of @xcite , since our goal is to demonstrate that within - model simulation methods allow for the computation of bayes factors and thus avoids the additional complexity involved in trans - dimensional methods . while ameanable to an importance sampling technique of sorts , the alternative approach of nested sampling @xcite is discussed in @xcite and @xcite .",
    "in order to compare the performances of all methods presented in this survey , we chose to evaluate the corresponding estimates of the bayes factor in the setting of a single variable selection for a probit model and to repeat the estimation in a monte carlo experiment to empirically assess the variability of those estimates .",
    "we recall that a probit model can be represented as a natural latent variable model in that , if we consider a sample @xmath13 of @xmath14 independent latent variables associated with a standard regression model , i.e.  such that @xmath15 , where the @xmath16 s are @xmath0-dimensional covariates and @xmath17 is the vector of regression coefficients , then @xmath18 such that @xmath19 is a probit sample .",
    "indeed , given @xmath17 , the @xmath20 s are independent bernoulli rv s with @xmath21 where @xmath22 is the standard normal cdf .",
    "the choice of a reference prior distribution for the probit model is open to debate , but the connection with the latent regression model induced @xcite to suggest a @xmath23-prior model , @xmath24 , with @xmath14 as the @xmath23 factor and @xmath25 as the regressor matrix .",
    "the corresponding posterior distribution is then associated with the density @xmath26 where @xmath27 . in the completed model ,",
    "i.e.  when including the latent variables @xmath28 into the model , the @xmath20 s are deterministic functions of the @xmath29 s and the so - called completed likelihood is @xmath30 the derived conditional distributions @xmath31 are of interest for constructing a gibbs sampler on the completed model , where @xmath32 denotes the gaussian distribution with mean @xmath33 and variance @xmath34 that is left - truncated at @xmath35 , while @xmath36 denotes the symmetrical normal distribution that is right - truncated at @xmath35 .",
    "the corresponding full conditional on the parameters is given by @xmath37 indeed , since direct simulation from the posterior distribution of @xmath17 is intractable , @xcite suggest implementing a gibbs sampler based on the above set of full conditionals .",
    "more precisely , given the current value of @xmath17 , one cycle of the gibbs algorithm produces a new value for @xmath38 as simulated from the conditional distribution ( [ gibbs1 ] ) , which , when substituted into ( [ gibbs2 ] ) , produces a new value for @xmath17 .",
    "although it does not impact the long - term properties of the sampler , the starting value of @xmath17 may be taken as the maximum likelihood estimate to avoid burning steps in the gibbs sampler .",
    "given this probit model , the dataset we consider covers a population of women who were at least 21 years old , of pima indian heritage and living near phoenix , arizona .",
    "these women were tested for diabetes according to world health organization ( who ) criteria .",
    "the data were collected by the us national institute of diabetes and digestive and kidney diseases , and is available with the basic r package @xcite .",
    "this dataset , used as a benchmark for supervised learning methods , contains information about @xmath39 women with the following variables :    * ` glu ` : plasma glucose concentration in an oral glucose tolerance test ; * ` bp ` : diastolic blood pressure ( mm hg ) ; * ` ped ` : diabetes pedigree function ; * ` type ` : yes or no , for diabetic according to who criteria .    for this dataset ,",
    "the goal is to explain the diabetes variable ` type ` by using the explanatory variables ` glu ` , ` bp ` and ` ped ` .",
    "the following table is an illustration of a classical ( maximum likelihood ) analysis of this dataset , obtained using the r glm ( ) function with the probit link :    .... deviance residuals :       min        1q    median        3q       max    -2.1347   -0.9217   -0.6963    0.9959    2.3235    coefficients :       estimate std . error z value pr(>|z| )      glu   0.012616    0.002406    5.244 1.57e-07 * * * bp   -0.029050    0.004094   -7.096 1.28e-12 * * * ped   0.350301    0.208806    1.678    0.0934 .",
    "--- signif .",
    "codes :   0 ' * * * ' 0.001 ' * * ' 0.01 ' * ' 0.05 ' . ' 0.1 ' ' 1   ( dispersion parameter for binomial family taken to be 1 )        null deviance : 460.25   on 332   degrees of freedom residual deviance :",
    "386.73   on 329   degrees of freedom aic : 392.73 number of fisher scoring iterations : 4 ....    this analysis sheds some doubt on the relevance of the covariate ` ped ` in the model and we can reproduce the study from a bayesian perspective , computing the bayes factor @xmath40 opposing the probit model only based on the covariates ` glu ` and ` bp ` ( model 0 ) to the probit model based on the covariates ` glu ` , ` bp ` , and ` ped ` ( model 1 ) .",
    "this is equivalent to testing the hypothesis @xmath41 since the models are nested , where @xmath42 is the parameter of the probit model associated with covariate ` ped ` .",
    "( note that there is no intercept in either model . )",
    "if we denote by @xmath43 the @xmath44 matrix containing the values of ` glu ` and ` bp ` for the @xmath39 individuals and by @xmath45 the @xmath46 matrix containing the values of the covariates ` glu ` , ` bp ` , and ` ped ` , the bayes factor @xmath40 is given by @xmath47 } { { \\displaystyle}\\mathbb{e}_{\\mathcal{n}_3(0_3,n({\\mathbf{x}}_1^\\text{t}{\\mathbf{x}}_1)^{-1})}\\left[\\prod_{i=1}^n   \\{1-\\phi\\left(({\\mathbf{x}}_1)_{i,\\cdot}{{\\boldsymbol \\theta}}\\right)\\}^{1-y_i}\\phi\\left(({\\mathbf{x}}_1)_{i,\\cdot } { { \\boldsymbol \\theta}}\\right)^{y_i}\\right]}\\nonumber\\end{aligned}\\ ] ] using the shortcut notation that @xmath48 is the @xmath49-th line of the matrix @xmath50 .",
    "as already shown above , when testing for a null hypothesis ( or a model ) @xmath51 against the alternative hypothesis ( or the alternative model ) @xmath52 , the bayes factor is defined by @xmath53 we assume in this survey that the prior distributions under both the null and the alternative hypotheses are proper , as , typically , they should be .",
    "( in the case of common nuisance parameters , a common improper prior measure can be used on those , see @xcite .",
    "this obviously complicates the computational aspect , as some methods like crude monte carlo can not be used at all , while others are more prone to suffer from infinite variance . ) in that setting , the most elementary approximation to @xmath54 consists in using a ratio of two standard monte carlo approximations based on simulations from the corresponding priors . indeed , for @xmath55 :",
    "@xmath56\\,.\\ ] ] if @xmath57 and @xmath58 are two independent samples generated from the prior distributions @xmath59 and @xmath60 , respectively , then @xmath61 is a strongly consistent estimator of @xmath54 .    in most cases , sampling from the prior distribution corresponding to either hypothesis is straightforward and fast .",
    "therefore , the above estimator is extremely easy to derive as a brute - force evaluation of the bayes factor .",
    "however , if any of the posterior distributions is quite different from the corresponding prior distribution  and it should be for vague priors , the monte carlo evaluation of the corresponding evidence is highly inefficient since the sample will be overwhelmingly producing negligible values of @xmath62 .",
    "in addition , if @xmath63 is not integrable against @xmath59 or @xmath60 , the resulting estimation has an infinite variance . since importance sampling usually requires an equivalent computation effort , with a potentially highy efficiency reward , crude monte carlo approaches of this type",
    "are usually disregarded .",
    "figure [ fig : bfmc ] and table [ tab : res ] summarize the results based on @xmath64 replications of monte carlo approximations of @xmath54 , using equation ( [ eq : bfmc ] ) with @xmath65 simulations . as predicted , the variability of the estimator is very high , when compared with the other estimates studied in this survey .",
    "( obviously , the method is asymptotically unbiased and , the functions being square integrable in , with a finite variance .",
    "a massive simulation effort would obviously lead to a precise estimate of the bayes factor . )",
    "pima indian dataset : boxplot of 100 monte carlo estimates of @xmath54 based on simulations from the prior distributions , for @xmath66 simulations.,width=377,height=188 ]",
    "defining two importance distributions with densities @xmath67 and @xmath68 , with the same supports as @xmath59 and @xmath60 , respectively , we have : @xmath69 } \\bigg/ { \\mathbb{e}_{\\varpi_1}\\left[f(y|\\theta)\\pi_1(\\theta)\\big/\\varpi_1(\\theta)\\right]}\\,.\\ ] ] therefore , given two independent samples generated from distributions @xmath67 and @xmath68 , @xmath57 and @xmath58 , respectively , the corresponding importance sampling estimate of @xmath54 is @xmath70 compared with the standard monte carlo approximation above , this approach offers the advantage of opening the choice of the representation in that it is possible to pick importance distributions @xmath67 and @xmath68 that lead to a significant reduction in the variance of the importance sampling estimate .",
    "this implies choosing importance functions that provide as good as possible approximations to the corresponing posterior distributions .",
    "maximum likelihood asymptotic distributions or kernel approximations based on a sample generated from the posterior are natural candidates in this setting , even though the approximation grows harder as the dimension increases .    for the pima indian benchmark ,",
    "we propose for instance to use as importance distributions , gaussian distributions with means equal to the maximum likelihood ( ml ) estimates and covariance matrices equal to the estimated covariance matrices of the ml estimates , both of which are provided by the r glm ( ) function .",
    "while , in general , those gaussian distributions provide crude approximations to the posterior distributions , the specific case of the probit model will show this is an exceptionally good approximation to the posterior , since this leads to the best solution among all those compared here .",
    "the results , obtained over @xmath64 replications of the methodology with @xmath65 are summarized in figure [ fig : bfmcis ] and table [ tab : res ] .",
    "they are clearly excellent , while requiring the same computing time as the original simulation from the prior .",
    "pima indian dataset : boxplots of 100 monte carlo and importance sampling estimates of @xmath54 , based on simulations from the prior distributions , for @xmath66 simulations.,width=377,height=188 ]",
    "the original version of the bridge sampling approximation to the bayes factor @xcite relies on the assumption that the parameters of both models under comparison belong to the same space : @xmath71 . in that case , for likelihood functions @xmath72 and @xmath73 under respectively models @xmath74 and @xmath75 , the bridge representation of the bayes factor is @xmath76\\ , .",
    "\\label{eq : bridge1}\\ ] ] given a sample from the posterior distribution of @xmath77 under model @xmath75 , @xmath78 , a first bridge sampling approximation to @xmath54 is @xmath79 from a practical perspective , for the above bridge sampling approximation to be of any use , the constraint on the common parameter space for both models goes further in that , not only must both models have the same complexity , but they must also be parameterised on a common ground , i.e.  in terms of some specific moments of the sampling model , so that parameters under both models have a common meaning .",
    "otherwise , the resulting bridge sampling estimator will have very poor convergence properties , possibly with infinite variance .",
    "equation ( [ eq : bridge1 ] ) is nothing but a very special case of the general representation @xcite @xmath80}\\bigg/ { \\mathbb{e}_\\varphi\\left[f_1(y|\\theta ) \\pi_1(\\theta ) / \\varphi(\\theta)\\right ] } \\,,\\ ] ] which holds for any density @xmath81 with a sufficiently large support and which only requires a single sample @xmath82 generated from @xmath81 to produce an importance sampling estimate of the ratio of the marginal likelihoods .",
    "apart from using the _ same _ importance function @xmath81 for both integrals , this method is therefore a special case of importance sampling .",
    "another extension of this bridge sampling approach is based on the general representation @xmath83 where @xmath57 and @xmath58 are two independent samples coming from the posterior distributions @xmath84 and @xmath85 , respectively .",
    "that applies for any positive function @xmath86 as long as the upper integral exists .",
    "some choices of @xmath86 lead to very poor performances of the method in connection with the harmonic mean approach ( see section [ sec : harmo ] ) , but there exists a quasi - optimal solution , as provided by @xcite : @xmath87 this optimum can not be used _",
    "per se _ , since it requires the normalising constants of both @xmath84 and @xmath85 . as suggested by @xcite , an approximate version uses iterative versions of @xmath88 , based on iterated approximations to the bayes factor .",
    "note that this solution recycles simulations from both posteriors , which is quite appropriate since one model is selected via the bayes factor , instead of using an importance weighted sample common to both approximations .",
    "we will see below an alternative representation of the bridge factor that bypasses this difficulty ( if difficulty there is ! ) .",
    "those derivations are , however , restricted to the case where both models have the same complexity and thus they do not apply to embedded models , when @xmath89 in such a way that @xmath90 , i.e. , when the submodel corresponds to a specific value @xmath91 of @xmath92 : @xmath93 .    the extension of the most advanced bridge sampling strategies to such cases requires the introduction of a _ pseudo - posterior density , _",
    "@xmath94 , on the parameter that does not appear in the embedded model , in order to reconstitute the equivalence between both parameter spaces . indeed ,",
    "if we augment @xmath84 with @xmath94 , we obtain a joint distribution with density @xmath95 on @xmath96 .",
    "the bayes factor can then be expressed as @xmath97 for all functions @xmath98 , because it is clearly independent from the choice of both @xmath98 and @xmath94 .",
    "obviously , the performances of the approximation @xmath99 where @xmath100 and @xmath101 are two independent samples generated from distributions @xmath95 and @xmath102 , respectively , do depend on this completion by the pseudo - posterior as well as on the function @xmath98 . @xcite",
    "establish that the asymptotically optimal choice for @xmath94 is the obvious one , namely @xmath103 which most often is unavailable in closed form ( especially when considering that the normalising constant of @xmath94 is required in ) .",
    "however , in latent variable models , approximations of the conditional posteriors often are available , as detailed in section [ sec : chibberies ] .",
    "while this extension of the basic bridge sampling approximation is paramount for handling embedded models , its implementation suffers from the dependence on this pseudo - posterior .",
    "in addition , this technical device brings the extended bridge methodology close to the cross - model alternatives of @xcite and @xcite , in that both those approaches rely on completing distributions , either locally @xcite or globally @xcite , to link both models under comparison in a bijective relation .",
    "the density @xmath104 is then a pseudo - posterior distribution in chib and carlin s ( 1995 ) sense , and it can be used as green s ( 1995 ) proposal in the reversible jump mcmc step to move ( or not ) from model @xmath74 to model @xmath75 . while using cross - model solutions to compare only two models",
    "does seem superfluous , given that the randomness in picking the model at each step of the simulation is not as useful as in the setting of comparing a large number or an infinity of models , the average acceptance probability for moving from model @xmath74 to model @xmath75 is related to the bayes factor since @xmath105 = b_{01}(y)\\ ] ] even though the average @xmath106\\ ] ] does not provide a closed form solution .    for the pima indian benchmark , we use as pseudo - posterior density @xmath107 , the conditional gaussian density deduced from the asymptotic gaussian distribution on @xmath108 already used in the importance sampling solution , with mean equal to the ml estimate of @xmath108 and with covariance matrix equal to the estimated covariance matrix of the ml estimate .",
    "the quasi - optimal solution @xmath88 in the bridge sampling estimate is replaced with the inverse of an average between the asymptotic gaussian distribution in model @xmath75 and the product of the asymptotic gaussian distribution in model @xmath74 times the above @xmath107 .",
    "this obviously is a suboptimal choice , but it offers the advantage of providing a non - iterative solution .",
    "the results , obtained over @xmath64 replications of the methodology with @xmath65 are summarized in figure [ fig : bfbs ] and table [ tab : res ] .",
    "the left - hand graph shows that this choice of bridge sampling estimator produces a solution whose variation is quite close to the ( excellent ) importance sampling solution , a considerable improvement upon the initial monte carlo estimator .",
    "however , the right - hand - side graph shows that the importance sampling solution remains far superior , especially when accounting for the computing time .",
    "( in this example , running 20,000 iterations of the gibbs sampler for the models with both two and three variables takes approximately 32 seconds . )",
    "pima indian dataset : _",
    "( left ) _ boxplots of 100 importance sampling , bridge sampling and monte carlo estimates of @xmath54 , based on simulations from the prior distributions , for @xmath66 simulations ; _ ( right ) _ same comparison for the importance sampling versus bridge sampling estimates only.,title=\"fig:\",width=188,height=188 ] pima indian dataset : _",
    "( left ) _ boxplots of 100 importance sampling , bridge sampling and monte carlo estimates of @xmath54 , based on simulations from the prior distributions , for @xmath66 simulations ; _ ( right ) _ same comparison for the importance sampling versus bridge sampling estimates only.,title=\"fig:\",width=188,height=188 ]",
    "while using the generic harmonic mean approximation to the marginal likelihood is often fraught with danger @xcite , the representation @xcite @xmath109 @xmath110 = \\int \\frac{\\varphi_k(\\theta ) } { \\pi_k(\\theta)f_k(y|\\theta ) } \\ , \\frac{\\pi_k(\\theta)f_k(y|\\theta)}{{m}_k(y)}\\,\\text{d}\\theta = \\frac{1}{{m}_k(y)}\\ ] ] holds , no matter what the density @xmath111 is ",
    "provided @xmath112 when @xmath113. this representation is remarkable in that it allows for a direct processing of monte carlo or mcmc output from the posterior distribution @xmath114 . as with importance",
    "sampling approximations , the variability of the corresponding estimator of @xmath54 will be small if the distributions @xmath111 ( @xmath115 ) are close to the corresponding posterior distributions .",
    "however , as opposed to usual importance sampling constraints , the density @xmath111 must have lighter  rather than fatter  tails than @xmath116 for the approximation of the marginal @xmath117 @xmath118 to enjoy finite variance . for instance , using @xmath119 as in the original harmonic mean approximation @xcite will most usually result in an infinite variance estimator , as discussed by @xcite . on the opposite , using @xmath120 s with constrained supports derived from a monte carlo sample , like the convex hull of the simulations corresponding to the @xmath121 or to the @xmath122 hpd regions  that again is easily derived from the simulations  is both completely appropriate and implementable @xcite .",
    "however , for the pima indian benchmark , we propose to use instead as our distributions @xmath111 the very same distributions as those used in the above importance sampling approximations , that is , gaussian distributions with means equal to the ml estimates and covariance matrices equal to the estimated covariance matrices of the ml estimates .",
    "the results , obtained over @xmath64 replications of the methodology with @xmath123 simulations for each approximation of @xmath124 ( @xmath115 ) are summarized in figure [ fig : bfhm ] and table [ tab : res ] .",
    "they show a very clear proximity between both importance solutions in this special case and a corresponding domination of the bridge sampling estimator , even though the importance sampling estimate is much faster to compute .",
    "this remark must be toned down by considering that the computing time due to the gibbs sampler should not necessarily be taken into account into the comparison , since samples are generated under both models .",
    "pima indian dataset : _",
    "( left ) _ boxplots of 100 bridge sampling , harmonic mean and importance sampling estimates of @xmath54 , based on simulations from the prior distributions , for @xmath66 simulations ; _ ( right ) _ same comparison for the harmonic mean versus importance sampling estimates only.,title=\"fig:\",width=188,height=188 ] pima indian dataset : _ ( left ) _ boxplots of 100 bridge sampling , harmonic mean and importance sampling estimates of @xmath54 , based on simulations from the prior distributions , for @xmath66 simulations ; _ ( right ) _ same comparison for the harmonic mean versus importance sampling estimates only.,title=\"fig:\",width=188,height=188 ]",
    "chib s ( 1995 ) method for approximating a marginal ( likelihood ) is a direct application of bayes theorem : given @xmath125 and @xmath126 , we have that @xmath127 for all @xmath77 s ( since both the lhs and the rhs of this equation are constant in @xmath77 ) .",
    "therefore , if an arbitrary value of @xmath77 , say @xmath128 , is selected and if a good approximation to @xmath114 can be constructed , denoted @xmath129 , chib s ( @xcite ) approximation to the evidence is @xmath130 in a general setting , @xmath129 may be the gaussian approximation based on the mle , already used in the importance sampling , bridge sampling and harmonic mean solutions , but this is unlikely to be accurate in a general framework .",
    "a second solution is to use a nonparametric approximation based on a preliminary mcmc sample , even though the accuracy may also suffer in large dimensions . in the special setting of latent variables models ( like mixtures of distributions but also like probit models ) , chib s ( 1995 )",
    "approximation is particularly attractive as there exists a natural approximation to @xmath114 , based on the rao ",
    "blackwell @xcite estimate @xmath131 where the @xmath132 s are the latent variables simulated by the mcmc sampler .",
    "the estimate @xmath133 is a parametric unbiased approximation of @xmath134 that converges with rate @xmath135 .",
    "blackwell approximation obviously requires the full conditional density @xmath136 to be available in closed form ( constant included ) but , as already explained , this is the case for the probit model .",
    "figure [ fig : bfchi ] and table [ tab : res ] summarize the results obtained for @xmath64 replications of chib s approximations of @xmath54 with @xmath137 simulations for each approximation of @xmath124 ( @xmath115 ) . while chib s method is usually very reliable and dominates importance sampling , the incredibly good approximation provided by the asymptotic gaussian distribution implies that , in this particular case , chib s method is dominated by both the importance sampling and the harmonic mean estimates .",
    "pima indian dataset : boxplots of 100 chib s , harmonic mean and importance estimates of @xmath54 , based on simulations from the prior distributions , for @xmath66 simulations.,width=377,height=188 ]    .[tab : res ] pima indian dataset : performances of the various approximation methods used in this survey .",
    "[ cols= \" < , < , < , < , < , < \" , ]",
    "in this short evaluation of the most common estimations to the bayes factor , we have found that a particular importance sampling and its symmetric harmonic mean counterpart are both very efficient in the case of the probit model .",
    "the bridge sampling estimate is much less efficient in this example , due to the approximation error resulting from the pseudo - posterior . in most settings",
    ", the bridge sampling is actually doing better than the equivalent importance sampler @xcite , while chib s method is much more generic than the four alternatives .",
    "the recommendation resulting from the short experiment above is therefore to look for handy approximations to the posterior distribution , whenever available , but to fall back on chib s method as a backup solution providing a reference or better .",
    "neal , r. ( 1994 ) .",
    "contribution to the discussion of  approximate bayesian inference with the weighted likelihood bootstrap \" by michael a. newton and adrian e. raftery .",
    "_ j. royal statist .",
    "society series b _ , * 56 ( 1 ) * 4142 ."
  ],
  "abstract_text": [
    "<S> this paper surveys some well - established approaches on the approximation of bayes factors used in bayesian model choice , mostly as covered in @xcite . </S>",
    "<S> our focus here is on methods that are based on importance sampling strategies  rather than variable dimension techniques like reversible jump mcmc , including : crude monte carlo , maximum likelihood based importance sampling , bridge and harmonic mean sampling , as well as chib s method based on the exploitation of a functional equality . </S>",
    "<S> we demonstrate in this survey how these different methods can be efficiently implemented for testing the significance of a predictive variable in a probit model . </S>",
    "<S> finally , we compare their performances on a real dataset .    </S>",
    "<S> * keywords : * bayesian inference ; model choice ; bayes factor ; monte carlo ; importance sampling ; bridge sampling ; chib s functional identity ; supervised learning ; probit model </S>"
  ]
}