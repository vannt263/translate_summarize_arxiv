{
  "article_text": [
    "in multiobjective optimization , the goal is to find ( a set of ) solutions which optimize multiple objective functions at the same time @xcite . as in case of conflicting objectives",
    "there is no single best solution , it is common to compute instead all points of the pareto front ( pf ) or an approximation to this set .",
    "sometimes the function values of solutions can only be determined through costly simulations , so approximation functions are used in their place .",
    "this makes it possible to evaluate the function values of the most promising individuals only , instead of wasting time evaluating the function values of individuals that are unlikely to result in an improvement . a common approximation method are gaussian processes ( or kriging ) , which yield a prediction in the form of a 1-d normal distribution",
    "this _ predictive distribution _ is learned from previously evaluated points and quantifies for a given new point how likely it is that certain function values will be obtained . in the context of computer experiments ,",
    "the statistical assumptions of such metamodels were discussed in sacks et al .",
    "@xcite .",
    "optimization methods for expensive function values based on gaussian processes date back to the lithuanian school of global optimization @xcite .",
    "more recently they have been refined and gained popularity in the context of optimization with expensive computer experiments @xcite under names such as _ efficient global optimization _ ( ego ) and _ expected improvement algorithm _ @xcite . even more recently , different expected improvement formulations for multiobjective optimization have been developed and were compared in wagner et al @xcite . among these , the _ expected hypervolume improvement _",
    "( ehvi ) turns out to have desirable theoretical properties .",
    "the ehvi was first suggested in @xcite and represents the expected improvement in the hypervolume measure relative to the current approximation of the pareto front @xcite given the probability distribution of possible function values .",
    "the hypervolume measure itself is a common measure used to determine the quality of a set of solutions to a multiobjective optimization problem @xcite and can be applied without a priori knowledge of the pareto front , which makes the ehvi a natural quality measure to use in multiobjective surrogate - assisted optimization .",
    "the calculation of the ehvi has so far been a problem .",
    "monte carlo integration can solve the issue of computing the ehvi directly , but to get an accurate approximation out of monte carlo integration is slow .",
    "an exact calculation approach exists for the bi - objective case , but it is slow as well ( time complexity in @xmath2 ) .",
    "this thesis aims to increase the speed of the exact calculation of the ehvi in two dimensions , as well as provide a method of calculating it in higher dimensions .",
    "its implementation will be validated with results from monte carlo integration . the empirical performance of directly calculating the ehvi in the three - dimensional case",
    "will also be analyzed in order to show the feasibility of using direct calculations in place of monte carlo integration .",
    "the main contribution of this paper is therefore to make the ehvi computation both exact and fast , so that it can be used in gaussian - process assisted global optimization algorithms .",
    "the article is structured as follows : section [ preliminaries ] introduces important definitions and technical preliminaries . section [ relatedwork ] summarizes the related work .",
    "section [ 2dcomplexity ] contains a proof that the exact calculation of the bi - objective ehvi can be done in @xmath0 as well as a lower bound on the worst case complexity of @xmath4 .",
    "the proof of the upper bound is by constructing an algorithm .",
    "the implementation of this algorithm is then empirically compared to the naive ( @xmath2 ) implementation .",
    "section [ higherdimensionalehvi ] describes the details of the new , general algorithm for calculating the expected hypervolume improvement in more than two dimensions , and section [ sliceupdates ] describes an exact method for determining the tri - objective ehvi with time complexity @xmath3 .",
    "section [ emptests ] describes the results of empirical tests of implementations of the algorithms described in sections [ higherdimensionalehvi ] and [ sliceupdates ] , both to validate the correctness of the implementations and to measure their performance .",
    "finally , section [ thefuture ] contains concluding remarks and an outline of promising directions for future research .",
    "without loss of generality , we will consider maximization of @xmath5 objective functions @xmath6 , @xmath7 @xmath8 .",
    "a distinction is made between the decision space @xmath9 of alternative solutions and the objective space @xmath10 where the images of points in @xmath9 under @xmath11 are represented .",
    "however , the attention of this study will be on points and probability distributions of points in the objective space",
    ".    the _ a posteriori _ approach of multiobjective optimization is concerned with finding ( approximations to ) the pareto front , that is : the set of solutions in the objective space that are not dominated in the pareto dominance relation @xcite .",
    "the hypervolume indicator is a quality measure for pareto approximation sets @xcite . among the performance measures being used in pareto optimization , it has some favorable properties .",
    "first of all it can be used to compute both absolute and relative improvements of a pareto front approximation without a priori knowledge of the pareto front and it is compliant with pareto dominance @xcite .",
    "furthermore , its maximization yields a set of pareto optimal points distributed across the pareto front @xcite .",
    "the _ hypervolume indicator _ of a finite set of points @xmath12 with respect to a user - defined reference point @xmath13 is defined as the lebesgue measure of the hypervolume covered by the boxes that have an element of @xmath14 as their upper corner and a reference point @xmath13 as their lower corner .",
    "thereby it measures the size of the dominated space of @xmath14 cut from below by a reference point .",
    "the reference point must be chosen in a such a way that it is dominated by all points in @xmath14 , and ideally also by all points of the pareto front .",
    "the set containing the part of the objective space that is dominated by the points in @xmath14 will be referred to as @xmath15 .",
    "the hypervolume contribution of a point @xmath16 is the difference in dominated hypervolume between @xmath17 and @xmath14 .",
    "the hypervolume contribution of a set of points @xmath18 is defined analogously , as the difference between @xmath19 and @xmath14 .",
    "the _ hypervolume improvement _ of a point @xmath20 with respect to @xmath14 is defined as the hypervolume contribution of @xmath21 with regards to @xmath22 , i.e. the increment of the hypervolume indicator after @xmath21 is added to @xmath14 .    note that in the entire article we consider a fixed reference point that is dominated by all points in the pareto front approximations .",
    "the choice of reference point is an important issue by itself , which we do however not adress here .      in global optimization with expensive function evaluations",
    "it is common to predict function values using statistical methods such as gaussian processes @xcite .",
    "such methods provide a predictive distribution of possible outcomes of the precise evaluation of the vector valued objective function in form of the parameters of a probability density function ( pdf ) over all possible outcomes . in case of gaussian processes or kriging approximations the predictions",
    "are given by multivariate normal distributions@xcite .",
    "the expected hypervolume improvement ( ehvi ) is the expected value of the hypervolume improvement of a new candidate point in @xmath9 , given its predictive distribution function ( @xmath23 ) over points in the objective space .",
    "the general formula for the ehvi with respect to a mutually non - dominated set @xmath14 is@xcite : @xmath24 the ehvi is a generalization of the classical expected improvement ( ei ) criterion @xmath25 used in model - assisted single objective optimization , where @xmath26 denote the function value of the currently best solution@xcite .    for a given mean and standard deviation vector of an independently distributed predictive distribution ,",
    "the ehvi is monotonic with respect to the mean value @xcite and , at least for @xmath27 , also w.r.t .",
    "the variance@xcite .",
    "it has been used as an infill criterion for multiobjective ego in multiobjective optimization in various studies @xcite but its application so far has been confined to the bi - objective case and the computation of the ehvi was criticized to be computationally expensive as compared to more simple generalizations of the ei@xcite .    in @xcite",
    ", a formula is derived for exactly calculating the ehvi for @xmath28 independent and identically distributed normal pdfs .",
    "the expression in @xcite in general does not yield the exact result for @xmath29 , as will be shown later .",
    "given these preliminaries the general problem discussed in this article can now be defined concisely :    given a finite set of points @xmath12 , a reference point @xmath13 and a predictive independent distributed multivariate normal pdf , given by its mean value @xmath30 and standard deviations @xmath31 , how can the ehvi be computed and how can the ehvi be computed efficiently ?",
    "[ problem ]      in order to calculate the ehvi , we will need to calculate many integrals that have the form of a partial one - dimensional improvement . in @xcite , a function",
    "was derived that could be used for that purpose .    in the following definition",
    "we recall the notion of standard normal distribution and normal distribution .",
    "moreover we introduce a useful shorthand named @xmath32 .    1 .",
    "the function @xmath33 is the density function of the standard normal distribution and @xmath34 is the cumulative probability distribution function of the standard normal distribution .",
    "the general normal distribution with mean @xmath35 and variance @xmath36 has as density the function @xmath37 .",
    "the cumulative distribution function of the general normal distribution is : @xmath38 .",
    "@xmath39    remark : it is easy to check that @xmath40 and @xmath41 .",
    "+ integrals of the form @xmath42 are equal to @xmath43 $ ] .",
    "integrals whose upper limit is less than @xmath44 and lower limit greater than @xmath45 can be written as the difference of two such integrals , allowing partial expected improvements over an interval @xmath46 , @xmath47 to be calculated .",
    "moreover one can easily see that this difference can be neatly expressed in terms of @xmath32 :    @xmath48    the value @xmath26 in this case is the currently best function value .    in the rest of this thesis",
    "we will use the abbreviations @xmath49 and @xmath50 , where @xmath51 and @xmath52 are the mean and variance of the normal distribution associated to the point @xmath53 in the search space .",
    "analogously , we use abbreviations @xmath54 and @xmath55 for the @xmath56 and the @xmath57 coordinate .",
    "the use of the one - dimensional expected improvement to solve engineering problems with expensive - to - evaluate objective functions was initially proposed by mockus et al.@xcite and then later reintroduced by jones et al . in @xcite . since then it has been widely used in global optimization with expensive - to - evaluate functions .",
    "it has been shown to converge to the global optimum for the single objective case and a subclass of continuous functions @xcite .    generalizing the one - dimensional expected improvement algorithm to multiobjective",
    "optimization is still a very new area of research . besides the aforementioned ehvi ,",
    "first published in @xcite , various other solutions have been proposed :    * chebyshev scalarization with dynamically changing weights @xcite .",
    "* scalarization by using the distance from the centroid of the probability distribution to the pareto approximation set @xcite . * the hypervolume improvement for candidate points , calculated based on the upper confidence bound of the meta - model prediction @xcite .",
    "moreover , in kumano et al . @xcite it was proposed to use a vector of expected improvements for the single objective functions , instead of a scalar measure .",
    "expected improvement has been studied as an infill criterion in different application fields , such as bioinformatics @xcite , mechanical engineering @xcite and aerospace design @xcite .",
    "also the ehvi was already applied in practice for the tuning of controllers in sewage treatment plants @xcite , in mechanical engineering @xcite and quantum control @xcite . as",
    "compared to other indicators ehvi was found to have monotonicity in mean values @xcite and variance@xcite and yielded high accuracy optima approximations . however , its computation is so far limited to the biobjective case and the time complexity of existing exact algorithms is still very high ( @xmath2 , see@xcite .",
    "recently , couckuyt et al .",
    "@xcite published an algorithm that is faster based on empirical tests .",
    "the complexity of this algorithm is not reported .",
    "it follows a heuristic block partitioning scheme for computing the improvement contribution of each cell and we conjecture its total complexity to be in @xmath58 for two and in @xmath59 in three objectives .",
    "firstly , an efficient exact algorithm for the computation of the ehvi in two dimensions will be discussed .",
    "let @xmath14 denote a set of @xmath1 mutually non - dominated points in the two - dimensional plane .",
    "@xmath14 is the currently best pareto front approximation .",
    "furthermore , let @xmath60 denote a reference point which is dominated by every point in @xmath14 .",
    "the aim is to calculate the expected hypervolume improvement for a point @xmath21 in the decision space for which we have the mean @xmath61 and standard deviation @xmath62 of a predictive distribution .    in the two - dimensional case , calculating the ehvi for @xmath21 exactly can be done by piecewise integration over a set of half - open rectangular interval boxes ( cells ) formed by the horizontal and vertical lines going through the points in @xmath14 and through @xmath13 .",
    "the final ehvi is the sum of the contributions calculated for all grid cells .",
    "see figure [ gridpicture ] for a visualization of the grid .    .",
    "checkered boxes fall in the dominated hypervolume of @xmath14 .",
    "therefore their contribution to the integral will be 0 , and no calculation will be necessary for these boxes.,width=226 ]    individual grid cells will be denoted by @xmath63 , where @xmath64 and @xmath65 .",
    "let @xmath66 , with @xmath67 denoting @xmath68 sorted in order of ascending @xmath53 coordinate , and @xmath69 denoting @xmath68 sorted in order of ascending @xmath56 coordinate .",
    "let @xmath70 be the set of grid cells representing the interval boxes .",
    "the numbers @xmath71 and @xmath72 represent positions in the sorting order of @xmath68 , starting with 0 .",
    "then , @xmath71 is the position of elements of @xmath67 and @xmath72 is the position of elements of @xmath69 .",
    "the lower left corner of a cell will have the coordinates @xmath73 .",
    "the upper right corner of the grid cell will have the coordinates @xmath74 .",
    "note , that due to the characteristics of mutually non - dominated points in the two - dimensional plane , it is not necessary to sort @xmath68 twice in order to determine @xmath67 and @xmath69 .",
    "sorting @xmath14 in order of ascending @xmath53 coordinate is equivalent to sorting it in order of descending @xmath56 coordinate .",
    "it follows that @xmath75 .    when dividing the grid in the way described above , @xmath76 interval boxes are formed .",
    "however , if the upper right corner of an interval box is dominated by or equal to some point in @xmath14 , its contribution will be zero , and no calculation will need to be done for that interval box . note that if the upper right corner is not dominated by @xmath14 then the lower left corner is neither .",
    "these interval boxes are represented by a grid cell @xmath63 which is within the dominated hypervolume of @xmath14 .",
    "the remaining cells , @xmath77 , are formed by cells for which this is not the case , meaning that @xmath78 and , analogously , @xmath79 .    due to the definition of @xmath68 , we know that for @xmath16 it holds that @xmath80 for some @xmath81 .",
    "furthermore @xmath82 and @xmath83 , if and only if @xmath21 dominates @xmath63 . from this",
    "we get the following equivalence : @xmath84 is dominated by some point @xmath16 .",
    "thus @xmath85 consists of all cells satisfying @xmath86 .",
    "there are @xmath87 of such cells , resulting in a lower bound of @xmath0 on the complexity of any algorithm which iterates over these interval boxes .",
    "if we call the lower corner of the cell @xmath88 and the upper corner @xmath89 , the contribution of a grid cell to the integral is defined as follows :    @xmath90    , the hypervolume improvement of candidate points @xmath21 is equal to @xmath91 . in this example , the yellow rectangle represents @xmath92 , and @xmath93 consists of the two points within the yellow rectangle.,width=226 ]    dominated cells have a contribution of 0 to the integral , and for cells which are non - dominated , @xmath94 can be calculated as a rectangular volume from which a correction term is subtracted . see figure [ sminusbox ] for a visual representation .",
    "the integral for these cells can be calculated as follows , as was described in more detail in @xcite :    @xmath95    the last step is motivated by subsection [ psiscription ] and the application of fubini s theorem @xcite .",
    "it can be seen that the formula is of the form @xmath96 , where @xmath97 and @xmath98 are calculations which are performed in constant time with respect to @xmath1 for a single cell .",
    "the correction term @xmath99 is equal to the hypervolume contribution of @xmath18 , where @xmath93 consists of those points dominated by or equal to the lower corner of the cell . calculating the dominated hypervolume of a set in the two - dimensional plane",
    "has a time complexity of @xmath100 .",
    "this complexity results from needing to find the neighbors of each point in order to calculate its contribution to the hypervolume . sorting the set has a time complexity of @xmath100 , after which the dominated hypervolume calculation itself is done in @xmath101 by iterating over each point and performing an @xmath102 calculation using the points that come before and after it in the sorting order .",
    "when calculating @xmath99 , the points for which the dominated hypervolume is to be calculated come from @xmath14 , which was already sorted .",
    "this brings the complexity of this step down to @xmath101 , but it can be brought down to @xmath102 when the order of calculations is chosen carefully , giving the algorithm a total complexity of @xmath0 .",
    "the points of @xmath14 dominated by or equal to the lower corner of @xmath63 , which define @xmath93 , are those points satisfying the following inequalities :    @xmath103    because of the sorting order and definition of @xmath67 and @xmath69 , @xmath93 can be described equivalently as follows .",
    "the set @xmath93 is empty , if @xmath104 ( the lowest value of @xmath71 for which @xmath86 ) , otherwise ( @xmath105 ) @xmath93 is formed by an uninterrupted range with @xmath106 as its first element and @xmath107 as its last element .",
    "a row in @xmath77 is a set of cells @xmath108 where @xmath72 is the same . in a single row ,",
    "@xmath93 will always be either empty or have @xmath107 as its last element .",
    "adding 1 to @xmath71 adds one point to the range of points in @xmath14 which falls between @xmath106 and @xmath107 .",
    "this makes it possible to iterate over all cells in @xmath77 while adding no more than one point to @xmath93 per iteration .",
    "we will do this as follows :     to be updated in constant time.,width=226 ]    we will start iterating over each row of @xmath77 at its first cell , where @xmath109 . in this cell , @xmath110 and @xmath111 . for each iteration within a row after the first one , we add 1 to @xmath71 and add the point @xmath107 to @xmath93 .",
    "for an example , refer to figure [ gridexample ] , which shows the order of operations and the contents of @xmath93 during each step .",
    "although the above description refers to ` adding points to @xmath93 ' , we only need to keep track of the first and last points of @xmath93 in between algorithm iterations . when a new point is added to @xmath93",
    ", @xmath99 increases by the area covered by the rectangle from @xmath112 to @xmath113 .",
    "therefore , to update @xmath99 after the addition of a point to @xmath93 , only the left neighbor of the first element of @xmath93 , the last element of @xmath93 and the right neighbor of the last element of @xmath93 are needed .",
    "figure [ animationexample ] shows an example of this process .",
    "this can be done in constant time in any data structure which allows the neighbors of a point to be looked up in constant time : whenever @xmath71 is incremented , @xmath107 becomes @xmath114 and @xmath114 becomes @xmath115 .",
    "whenever @xmath72 is incremented , the new @xmath116 becomes its left neighbor , @xmath117 , and as we will then start iterating through values of @xmath71 at the beginning of the row , @xmath107 becomes the new @xmath116 as well because we have established earlier that @xmath118 in the first cell in a row of @xmath77 .",
    "we have shown that the upper bound on the complexity of determining the expected hypervolume improvement is @xmath0 .",
    "we can also show that the worst - case complexity can be no better than @xmath100 . if the standard deviation of a candidate point s predictive distribution is 0 and the mean value vector is a point which dominates all points in @xmath14 , then the problem of calculating its ehvi reduces to calculating the hypervolume that will be dominated by @xmath119 minus the hypervolume dominated by @xmath14 .",
    "if it was possible to solve this calculation with lower complexity than @xmath100 , then it would also be possible to reduce the calculation of @xmath14 s hypervolume to the problem of calculating the ehvi of a point that dominates @xmath14 , and it has already been proven in @xcite that the complexity of calculating the hypervolume of a set of points in the 2-d plane is in @xmath120 .",
    "changes during each iteration within a single row . the rectangular strip which is added after each iteration can be calculated with knowledge of three points : the point @xmath107 is its upper corner , the point @xmath114 provides the @xmath56 coordinate of its lower corner , and the point @xmath116 provides the @xmath53 coordinate of its lower corner . because @xmath116 does not change , the hypervolume covered by the older points in @xmath93 stays the same and does not have to be re - calculated . ]      as an additional verification of the correctness of the algorithm presented above , two implementations were written in c++ .",
    "the first used the constant - time update scheme , and the second did not : instead of using the constant - time update scheme , @xmath99 was calculated by first finding the set of points @xmath93 by checking each point in @xmath14 to see if it was dominated , and then calling a separate function on @xmath93 to calculate the hypervolume of this set of points .    the expected hypervolume improvement calculated using these implementations was identical for all test problems , but their speed was not .",
    "see figure [ 2dspeedtest ] for the empirical performance on a simple test where @xmath14 consisted of @xmath1 different points on a diagonal pareto front . from this",
    ", it appears that using the constant - time update scheme becomes worthwhile for @xmath121 , though results might vary slightly depending on implementation and system details .",
    "the algorithm given in @xcite for exactly calculating the expected hypervolume improvement is not correct when the dimensions is higher than 2 . this is because the shape of the hypervolume improvement becomes more complex when the number of dimensions increases .",
    "we will therefore derive a new formula by first decomposing the calculation into parts with less complex shapes , and then simplifying the resulting formula for the sake of more convenient calculation .      in higher dimensions , the search space can be divided into cells the same way it is done in two dimensions , except instead of the boundaries being given by lines going through the points in @xmath14 and the reference point @xmath13 , now the cells are separated from each other by @xmath122-dimensional hyperplanes ( where @xmath123 is the number of objective functions ) .    each cell",
    "is denoted by @xmath124 where @xmath125 through @xmath126 are integers from @xmath127 to @xmath128 denoting the labeling of the cell .",
    "then the left lower corner @xmath129 and right upper corner @xmath130 of the cell with label @xmath125 , ... , @xmath126 are defined as follows : let @xmath131 and let @xmath132 , \\dots , s_d[|p|+1]$ ] denote the @xmath133-th components of the vectors in @xmath134 sorted in ascending order . then @xmath135 $ ] and @xmath136 $ ] for @xmath137 . in other words ,",
    "corners of this cell complex are given as the intersection points of all axis - parallel @xmath138 dimensional hyperplanes through points in @xmath134 .",
    "the hypervolume improvement of a new point @xmath21 with respect to the current pareto front approximation @xmath14 is given by the function @xmath139 , where @xmath140 is the dominated hypervolume covered by @xmath21 .",
    "this is the same as calculating @xmath141 .",
    "we will denote the set of dimensions by @xmath142 .",
    "we can decompose the calculation of the hypervolume improvement of a point @xmath143 as follows :    @xmath144    and @xmath145 are given by : @xmath146 \\\\ & v_d = \\begin{cases } l_d & \\mbox{if } d \\in c \\\\ r_d & \\mbox{if } d \\notin c \\end{cases } \\\\ & w_d = \\begin{cases } p_d & \\mbox{if } d \\in c \\\\ l_d & \\mbox{if } d \\notin c \\end{cases } \\end{split}\\ ] ] see figure [ acpic ] for an example in 3 dimensions .     for @xmath147 are defined in a three - dimensional objective space .",
    "@xmath148 is hidden within the rectangular volume .",
    "the checkered volumes represent the volume dominated by the points in the pareto front approximation.,width=226 ]    in the above formula @xmath149 denotes the lebesgue measure of @xmath10 .",
    "note that it can happen that the dimension of @xmath145 is strictly less than m. in this case @xmath150 .",
    "we can make a similar remark about @xmath151 @xmath152 .",
    "the values of @xmath153 and @xmath154 are constant for all points that fall within a given interval box ( cell ) : @xmath13 is the reference point and is , of course , always constant , while @xmath88 represents the position of the lower corner of the cell . from this",
    ", it follows that @xmath155 represents the portion of the hypervolume improvement which is constant with regards to the values of @xmath156 , and which is variable with regards to the values of @xmath157 .",
    "in fact , it is _ linearly related _ to these values .",
    "this is a direct consequence of the way the cell boundaries are defined :    let @xmath158 be the cross - section of @xmath159 which goes through @xmath21 .",
    "this cross - section is defined by a projection to the dimensions not in @xmath70 ( if @xmath70 consists of @xmath160 dimensions , the slice will be @xmath161-dimensional as a result ) .",
    "the projection of @xmath15 uses only those points in @xmath14 for which the function values in the dimensions given by @xmath70 are larger than the corresponding function values of @xmath21",
    ". we shall call this selection @xmath134 .",
    "no points in @xmath14 can fall between cell boundaries in any dimension , so the composition of @xmath134 must be the same for all points within a cell .",
    "the projection of @xmath145 to the dimensions not in @xmath70 is constant for all points within a cell as well , because the coordinates defining @xmath145 are independent of @xmath21 in all dimensions not in @xmath70 .",
    "@xmath162 is constant as a result  note that here @xmath149 is the lebesque measure of @xmath163 . because @xmath145 does not span across cell boundaries in the dimensions in @xmath70",
    ", @xmath164 is equal to the hypervolume of @xmath158 multiplied by the length of @xmath145 in each dimension in @xmath70 , and those lengths are given by @xmath165 with @xmath166 .",
    "there is one quantity @xmath155 for which @xmath167 .",
    "this quantity @xmath168 is special because it is linearly related to all values of @xmath21 .",
    "@xmath168 falls entirely within the cell , and as such , instead of projecting @xmath14 onto a zero - dimensional space , it can simply be said that @xmath169 if the cell is not dominated , and @xmath170 if it is . therefore , @xmath171 for non - dominated cells .    by decomposing the calculation of the hypervolume improvement , we can use the sum rule to decompose the calculation of a cell s contribution to the ehvi as well .",
    "@xmath172    @xmath155 is calculated as the product of a constant and a set of values which are linearly related to exactly one coordinate of @xmath21 , therefore we can first factor out the calculation of the constant part .",
    "the @xmath23 consists of independent normal distributions , allowing the probability distributions for dimensions not in @xmath70 ( in which @xmath155 is constant ) to be factured out as well .",
    "an integral consisting solely of a normal distribution can be exactly calculated using the cumulative probability distribution function @xmath173 to calculate the probability that a point is within range of the cell .",
    "@xmath174    the integral that remains is a box - shaped expected improvement where each dimension is independent .",
    "fubini s theorem @xcite states that iterated integration , performed in any order , can be used to calculate a multiple integral under the condition that the multiple integral is absolutely convergent .",
    "the partial integrals making up the cell s contribution to the ehvi all converge to finite numbers , so we can safely use iterated integration .",
    "the result is a product of expected improvements , which are captured in the @xmath32 function described earlier : @xmath175    it is already possible to calculate the contribution of a cell to the ehvi by summing all these terms , but the calculation can be made a bit more efficient when instead of decomposing the calculation of the hypervolume , we instead only decompose the calculation of the dominated hypervolume . in section [ boringproof ] , that possibility will be examined in more detail by looking at the 3-d case as an example .",
    "consider that , in the 2-d case , we are able to calculate the hypervolume by integrating over a box bounded by the dominated hypervolume and subtracting a correction term @xmath99 . in higher dimensions ,",
    "the correction term is not a constant , but we can still make use of a modified version of this technique . the hypervolume improvement @xmath176 is decomposed as follows , in three dimensions :    @xmath177    together , they form the volume of @xmath178 \\setminus \\mbox{domset}(p ) \\right)$ ] .",
    "instead of writing @xmath179 as a sum of hypervolume improvements , we can also write it as a single rectangular volume from which a dominated hypervolume is subtracted :    @xmath180\\right ) - \\mbox{vol}\\left(\\mbox{domset}(p ) \\cap \\left[\\begin{pmatrix}r_x \\\\r_y \\\\",
    "r_z \\end{pmatrix } , \\begin{pmatrix}p_x \\\\p_y \\\\ p_z \\end{pmatrix}\\right]\\right)\\ ] ]    we can then decompose the calculation of the dominated hypervolume instead of the calculation of the hypervolume improvement . in the following decomposition of the total",
    "subtracted dominated hypervolume @xmath181 , each part @xmath182 is equal to the subtracted dominated hypervolume needed to calculate @xmath155 .",
    "when @xmath21 is within the integration cell bounded from below by @xmath88 , we get the following :    @xmath183\\right)\\\\ & + ( p_x - l_x ) \\cdot \\mbox{area}\\left(\\mbox{domset}\\left ( \\pi_{yz}\\left ( \\sigma_{x > l_x}(p ) \\right ) \\right ) \\cap \\left[\\begin{pmatrix}r_y \\\\ r_z \\end{pmatrix } , \\begin{pmatrix}p_y \\\\",
    "p_z \\end{pmatrix}\\right]\\right)\\\\ & + \\ldots \\\\ & + ( p_x - l_x ) \\cdot ( p_y - l_y ) \\cdot \\left(\\mbox{max } ( r_z , \\pi_z ( \\sigma_{x > l_x , y > l_y}(p ) ) ) - r_z \\right ) \\\\ &",
    "+ \\ldots \\\\ \\end{split}\\ ] ]    by abuse of language we use @xmath184 instead of the following correct notation : @xmath185 .",
    "similar notations are also used in the sequel .    the first thing to note",
    "is that if @xmath186 , @xmath187 .",
    "the analogous cases are true for @xmath188 and @xmath189 , allowing us to define a point @xmath190 for which , if @xmath191 , all three quantities are 0 :    @xmath192\\ ] ]    the bounding box bounded by @xmath190 from below and @xmath21 from above contains the entire volume of @xmath179 .",
    "this allows us to use @xmath190 in place of @xmath13 and rewrite our initial equation in a way that reduces the number of components from 8 to 5 :    @xmath193\\right ) \\\\ & - \\mbox{vol}\\left(\\mbox{domset}(p ) \\cap",
    "\\left[\\begin{pmatrix}v_x \\\\v_y \\\\ v_z \\end{pmatrix } , \\begin{pmatrix}l_x \\\\l_y \\\\",
    "l_z \\end{pmatrix}\\right]\\right)\\\\ & - ( p_x - l_x ) \\cdot \\mbox{area}\\left({domset}\\left",
    "( \\pi_{yz}\\left ( \\sigma_{x > l_x}(p ) \\right ) \\right ) \\cap \\left[\\begin{pmatrix}v_y",
    "\\\\ v_z \\end{pmatrix } , \\begin{pmatrix}l_y \\\\ l_z \\end{pmatrix}\\right]\\right)\\\\ & - ( p_y - l_y ) \\cdot \\mbox{area}\\left({domset}\\left ( \\pi_{xz}\\left ( \\sigma_{y > l_y}(p ) \\right ) \\right ) \\cap \\left[\\begin{pmatrix}v_x \\\\",
    "v_z \\end{pmatrix } , \\begin{pmatrix}l_x \\\\",
    "l_z \\end{pmatrix}\\right]\\right)\\\\ & - ( p_z - l_z ) \\cdot \\mbox{area}\\left({domset}\\left",
    "( \\pi_{xy}\\left ( \\sigma_{z > l_z}(p ) \\right ) \\right ) \\cap \\left[\\begin{pmatrix}v_x \\\\",
    "v_y \\end{pmatrix } , \\begin{pmatrix}l_x \\\\",
    "l_y \\end{pmatrix}\\right]\\right)\\\\ \\end{split}\\ ] ]    the component of the ehvi integral corresponding to @xmath194\\right)$ ] is the only component in this equation which is variable in more than one dimension , but since it is a rectangular volume , it is simply a product of one - dimensional improvements :    @xmath195    @xmath196 is a constant .",
    "we will keep using the notation @xmath197 , although now we will use @xmath190 as a reference point . even without examining the corresponding integral",
    "it is clear that it only needs to be multiplied with the probability that a given point is within the cell .",
    "the formula for calculating this correction term is :    @xmath198    @xmath199 , @xmath200 , and @xmath201 are not constants , but they are each linearly related to only one coordinate of @xmath21",
    ". we will look at @xmath199 as an example :    the constant part of @xmath199 is @xmath202\\right)$ ] .",
    "this has to be multiplied by @xmath203 .",
    "the expected value of @xmath199 is therefore equal to a constant multiplied by the partial expected improvement of @xmath204 over the interval @xmath205 .",
    "this is given by :    @xmath206    using a new call to @xmath32 to calculate this term is not necessary .",
    "we can use the fact that @xmath32 represents the function of a one - dimensional expected improvement over a certain range bounded from below .",
    "the partial expected improvement for the region below the lower cell bound @xmath88 is a constant term multiplied by the chance of being within the cell s range , which is captured in the equation below :    @xmath207    both @xmath208 and @xmath209 were calculated earlier , so we can reuse them to easily find @xmath210 .",
    "this means that the formula for calculating the partial expected hypervolume improvement of a cell will look like the following if the cell is not dominated :    @xmath211    @xmath212\\right ) \\cdot",
    "\\prod_{c \\in \\{x , y , z\\ } } \\delta\\phi_c\\\\ & - ( \\delta\\psi_z - \\delta\\phi_z \\cdot ( l_z - v_z ) ) \\\\ & \\,\\,\\,\\,\\ , \\cdot { area}\\left({domset}\\left ( \\pi_{yz}\\left ( \\sigma_{x > l_x}(p ) \\right ) \\right ) \\cap \\left[\\begin{pmatrix}v_y \\\\",
    "v_z \\end{pmatrix } , \\begin{pmatrix}l_y \\\\",
    "l_z \\end{pmatrix}\\right]\\right ) \\cdot \\prod_{c \\in \\{x , y\\ } } \\delta\\phi_c\\\\ & - ( \\delta\\psi_y - \\delta\\phi_y \\cdot ( l_y - v_y ) ) \\\\ & \\,\\,\\,\\,\\ , \\cdot { area}\\left({domset}\\left",
    "( \\pi_{xz}\\left ( \\sigma_{y > l_y}(p ) \\right ) \\right ) \\cap \\left[\\begin{pmatrix}v_x",
    "\\\\ v_z \\end{pmatrix } , \\begin{pmatrix}l_x \\\\",
    "l_z \\end{pmatrix}\\right]\\right )   \\cdot \\prod_{c \\in \\{x , z\\ } } \\delta\\phi_c \\\\ & - ( \\delta\\psi_x - \\delta\\phi_x \\cdot ( l_x - v_x ) )   \\\\ & \\,\\,\\,\\,\\ , \\cdot{area}\\left({domset}\\left ( \\pi_{xy}\\left ( \\sigma_{z > l_z}(p ) \\right ) \\right ) \\cap \\left[\\begin{pmatrix}v_x \\\\",
    "v_y \\end{pmatrix } , \\begin{pmatrix}l_x \\\\ l_y \\end{pmatrix}\\right]\\right ) \\cdot",
    "\\prod_{c \\in \\{y , z\\ } } \\delta\\phi_c \\end{split}\\ ] ]    and it will be 0 otherwise .",
    "although we are currently decomposing our integral into different quantities in order to calculate it , we can also calculate the sum of these quantities using a single dominated hypervolume calculation , though this has downsides which will be explored later .",
    "this subsection will give the general formula for doing so .",
    "recall how we decomposed the calculation of the hypervolume improvement in section [ 3dplusformula ] :    @xmath213",
    "this sum can be rearranged to the following :    @xmath214    since the quantities @xmath145 sum to a generalized rectangular volume , we could just as readily calculate the total volume of @xmath140 directly .",
    "this is what we did in section [ boringproof ] , where the correction terms @xmath164 were still calculated separately .",
    ". moreover the @xmath216 are mutually disjoint except for common boundary points . from this",
    "we get that @xmath217 ( and the @xmath218s are mutually disjoint except for the boundaries ) .",
    "thus @xmath219 we initially decomposed the @xmath94 in this way in order to compute the corresponding @xmath220-integral .",
    "we have determined that each partial quantity @xmath164 depends linearly on the dimensions which its corresponding volume @xmath145 depends on , and is constant in the same dimensions in which @xmath145 is constant .",
    "this is true as well when @xmath221 is first calculated , and then split into the various volumes representing @xmath159 .",
    "because of this , we can calculate an @xmath123-dimensional ehvi using only a single @xmath123-dimensional hypervolume calculation per cell .",
    "we need to calculate the hypervolume improvement of each cell s _ center of mass _ , @xmath222 .",
    "@xmath223    the integral can be calculated as if it is an expected improvement where the currently best solution is 0 .",
    "however , we already need to compute @xmath224 to calculate the component of the ehvi corresponding to @xmath140 , and the following equation holds :    @xmath225    dividing a partial expected improvement over a range @xmath226 by the chance of being in that range ( given by @xmath227 ) gives the expected improvement of points which are known to lie within that range . adding the value of @xmath153",
    "gives the expected @xmath133th coordinate of a point in the objective space bounded from below by @xmath13 .",
    "this means that the general formula for calculating the partial expected improvement in a cell is the following if the cell is not dominated :    @xmath228    @xmath229\\right ) \\mbox { and}\\ ] ]    @xmath230    and 0 otherwise .",
    "any algorithm which iterates over all grid cells described in section [ higherdimensionalehvi ] will have a time complexity of @xmath231 .",
    "this is further increased by the complexity of the calculations within each grid cell .",
    "the algorithm of section [ awesomeproof ] requires an @xmath123-dimensional hypervolume to be calculated for each cell that is not dominated . calculating a 3-dimensional hypervolume",
    "can be done in @xmath100 , which results in a time complexity of @xmath232 . however , as will be shown in section [ sliceupdates ] , constant - time calculations within each grid cell are possible with @xmath3 total preparation time , resulting in an algorithm of complexity @xmath3 .",
    "similar @xmath233 algorithms are conjectured to exist for @xmath234 .",
    "one important thing to note , is that the expected hypervolume improvements for multiple individuals can be calculated at the same time without having to perform the hypervolume calculations more than once when using the decomposition described in section [ boringproof ] , because the hypervolume calculations are not dependent on the mean and standard deviation of the probability distribution .",
    "the algorithm described in section [ awesomeproof ] does not have this advantage .",
    "in section [ 2dcomplexity ] we showed that calculating the 2-d expected hypervolume improvement is possible with time complexity @xmath0 .",
    "although the algorithm described in that subsection made use of characteristics of a 2-d pareto approximation set which are not present in higher dimensions , this subsection will show that there is also a way to calculate the 3-d ehvi with time complexity @xmath3 .",
    "in other words : the calculations necessary for computing the partial expected hypervolume improvement of each grid cell will be performed in constant time .",
    "the trade - off is that we will need @xmath0 extra memory .",
    "the only calculations which have a complexity higher than constant time are the dominated hypervolume calculations .",
    "if we use the simple algorithm described in section [ awesomeproof ] , we only need to perform a single 3-dimensional hypervolume calculation to find the correction term that we need .",
    "however , we will start out with the algorithm described in section [ boringproof ] ( _ without _ replacing @xmath13 by @xmath190 ) , because it lends itself better to the re - use of old hypervolume calculations .",
    "three sets of correction terms are needed to calculate the partial expected hypervolume improvement of a cell :    * @xmath196 , a constant correction term which requires a three - dimensional hypervolume calculation . * @xmath235 , @xmath200 and @xmath201 , which each require a two - dimensional hypervolume calculation .",
    "we will call the 2-d areas used in the calculation of these correction terms @xmath236 , @xmath237 and @xmath238 , respectively .",
    "* @xmath239 , @xmath188 and @xmath189 , which requires a ` one - dimensional ' hypervolume calculation .    instead of calculating these correction terms afresh for each cell ,",
    "it is possible to perform all necessary hypervolume calculations in only @xmath3 time total .",
    "the first step is to create a data structure which allows us to see whether or not a cell is dominated in @xmath102 time .",
    "this can simply be a two - dimensional array holding the highest value of @xmath57 for which the cell is dominated , which we shall call @xmath240 . a simple way to fill",
    "this array is to iterate over all points @xmath241 in order of ascending @xmath57 value , setting the array value @xmath242 to @xmath57 if @xmath243 dominates the lower corner of @xmath244 .",
    "the complexity of this operation is in @xmath245 .",
    "this only needs to be done once , so the @xmath3 time complexity does not increase the total asymptotic time complexity of computing the ehvi in 3-d .",
    "figure [ hzexample ] shows an example .     for a population consisting of 4 points ,",
    "which is visualized on the left .",
    "cells on the outermost edge of the integration area ( which stretch out to @xmath44 in some dimension ) are always non - dominated . ]     for a population consisting of 4 points , which is visualized on the left .",
    "cells on the outermost edge of the integration area ( which stretch out to @xmath44 in some dimension ) are always non - dominated . ]    besides containing information that allows constant - time evaluation of whether a cell is dominated , the value of @xmath239 for a cell @xmath246 that is not dominated is also given by @xmath242 .",
    "if we build two more height arrays @xmath247 and @xmath248 where we use the highest value of @xmath53 and @xmath56 instead of @xmath57 , we can determine the results of all three of the one - dimensional hypervolume calculations in constant time during cell calculations .",
    "now , only the two - dimensional hypervolume calculations represented by @xmath236 , @xmath237 and @xmath238 , and the three - dimensional hypervolume calculation represented by @xmath196 , still have a complexity greater than constant time .",
    "for notational simplicity , we have omitted their dependence on a particular cell from the notation until now , but in order to show the relations between correction terms of different cells , we will write ` @xmath196 belonging to @xmath246 ' as @xmath249 , and likewise for the two - dimensional hypervolumes",
    ".    the value of @xmath196 is related to the values of @xmath236 , @xmath237 and @xmath238 in the following way :    * @xmath250 * @xmath251 * @xmath252    with our height array @xmath240 , we can calculate all values of @xmath238 for a given value of @xmath253 in @xmath0 time .",
    "we can also calculate all values of @xmath196 for a given value of @xmath253 in @xmath0 time , provided @xmath254 or we have both @xmath196 and @xmath238 for the cells where @xmath253 is one lower .",
    "the details of these calculations will be given below .",
    "if we go through our cells in the right order ( with @xmath253 starting at 0 , incrementing it only after we have performed the calculations for all cells with a given value of @xmath253 ) , we only need to update the values of @xmath238 and @xmath196 @xmath1 times , resulting in an algorithm for the full computation with complexity in @xmath3 .",
    "if we know the value of @xmath196 for all cells with a given value of @xmath253 , we can use the formulas given above to calculate @xmath236 and @xmath237 in constant time whenever we need them , so we do not need to calculate these constants in advance .",
    "the details of calculating @xmath238 using the height array are as follows .",
    "we will iterate through the possible values of @xmath125 and @xmath255 in ascending order .",
    "we know that @xmath256 is 0 if @xmath257 or @xmath258 .",
    "if our height array shows that @xmath259 is dominated , @xmath256 is set equal to the area of the 2-d rectangle from its lower corner to @xmath260 .",
    "else , if that cell is not dominated , @xmath256 is set equal to @xmath261 .",
    "the value of @xmath262 is removed as this is the area which is overlapping , causing it to be added twice otherwise .",
    "for an example , refer to figure [ zslicestuff ] .     and",
    "@xmath181 for the example shown in figure [ hzexample ] , with @xmath254 , 1 and 2 , respectively .",
    "the @xmath53 and @xmath56 values of each cell s lower corner are shown on the axes .",
    "the grids with the values of @xmath181 are on the left and the grids with the values of @xmath238 are on the right . ]     and @xmath181 for the example shown in figure [ hzexample ] , with @xmath254 , 1 and 2 , respectively .",
    "the @xmath53 and @xmath56 values of each cell s lower corner are shown on the axes .",
    "the grids with the values of @xmath181 are on the left and the grids with the values of @xmath238 are on the right . ]     and @xmath181 for the example shown in figure [ hzexample ] , with @xmath254 , 1 and 2 , respectively .",
    "the @xmath53 and @xmath56 values of each cell s lower corner are shown on the axes .",
    "the grids with the values of @xmath181 are on the left and the grids with the values of @xmath238 are on the right . ]     and @xmath181 for the example shown in figure [ hzexample ] , with @xmath254 , 1 and 2 , respectively .",
    "the @xmath53 and @xmath56 values of each cell s lower corner are shown on the axes .",
    "the grids with the values of @xmath181 are on the left and the grids with the values of @xmath238 are on the right . ]     and @xmath181 for the example shown in figure [ hzexample ] , with @xmath254 , 1 and 2 , respectively .",
    "the @xmath53 and @xmath56 values of each cell s lower corner are shown on the axes .",
    "the grids with the values of @xmath181 are on the left and the grids with the values of @xmath238 are on the right . ]     and @xmath181 for the example shown in figure [ hzexample ] , with @xmath254 , 1 and 2 , respectively .",
    "the @xmath53 and @xmath56 values of each cell s lower corner are shown on the axes .",
    "the grids with the values of @xmath181 are on the left and the grids with the values of @xmath238 are on the right . ]",
    "five different implementations of a 3-d expected hypervolume improvement calculation algorithm were used throughout the following tests , referred to as the 8-term , 5-term , 2-term , slice - update and monte carlo schemes .",
    "the goal of comparing the exact calculation algorithms to a monte carlo scheme is twofold .",
    "first , by computing the expected hypervolume improvement in different ways , the algorithms and their implementations will be thoroughly validated .",
    "second , the time consumption of the algorithms will be compared .",
    "this is of particular interest because monte carlo schemes are often used as fast approximations to exact computations .    *",
    "the 8-term scheme is a direct implementation of the calculations described in section [ boringproof ] . *",
    "the 5-term scheme implements the slightly simplified calculations described in section [ boringproof ] . *",
    "the 2-term scheme implements the calculations described in section [ awesomeproof ] . *",
    "the slice - update scheme implements the algorithm described in section [ sliceupdates ] . *",
    "the monte carlo scheme uses monte carlo integration to give an approximation of the expected hypervolume improvement .",
    "its random number generator uses the box - muller transform @xcite in combination with the mersenne twister algorithm @xcite ( specifically , the 32-bit mt19937 variant from the c++ standard library , implemented in gcc ) to generate normally distributed pseudo - random numbers . due to the nature of monte carlo algorithms , it is impossible to get an exact answer out of this scheme .",
    "the expected error of monte carlo integration is related to the number of trials @xmath123 by @xmath263 , which means that to make the estimate ten times more accurate , a hundred times more trials are required .",
    "the implementations of @xmath32 and the gaussian cumulative distribution function were identical for all schemes , except for the monte carlo scheme where they were not used .",
    "the 2-d and 3-d hypervolume calculation functions were also identical between those schemes which used them .",
    "standard c++ library functions were used for sorting and for the implementation of the gaussian error function _",
    "erf_.      as a verification of the correctness of the algorithms , the expected hypervolume improvements calculated by all schemes on several test problems were compared to each other and to the value which the monte carlo scheme converged towards .",
    "the graph in figure [ simplemontecarlos ] shows the results of running the algorithms on a simple test problem .",
    "the population consisted of three points : @xmath264 , @xmath265 and @xmath266 .",
    "the reference point was set to @xmath267 .",
    "the median vector for the gaussian distribution was set to @xmath268 , placing it right between cell borders , and the standard deviation was set to @xmath269 .",
    "all non - monte carlo schemes gave exactly identical answers , which was likely due to the simplicity of the test case , because rounding errors in the floating - point calculations would have resulted in small differences otherwise .",
    "the monte carlo scheme was allowed to run for 100.000.000 iterations .",
    "figure [ morecarlos ] shows the results of running the algorithm on a few more complex populations .",
    "the first consists of 30 points , some of which had identical values to another point in the population in one of their dimensions ( creating cells of size 0 ) .",
    "the second consists of 100 points with a bias towards one area of the search space .",
    "the results of all non - monte carlo schemes on these two test problems were identical to 15 and 14 digits , respectively .",
    "the double - precision floating numbers which were used in the implementations are accurate to approximately the 15th decimal , so the answers can safely be considered identical .            the convergence of the monte carlo integration , as well as the near - identical answers generated by the different approaches towards calculating the expected hypervolume improvement , both support the validity of the calculations described in this thesis .",
    "to test the empirical performance of the exact calculation schemes , they were tested on mutually non - dominated populations of varying sizes that were generated by selecting @xmath1 pseudo - random points which were uniformly distributed on a spherical surface . the time needed for calculating the expected hypervolume improvement",
    "was measured ( along with all operations required to do so , such as sorting the populations , but not including the time needed to generate the populations ) .",
    "the seed of the pseudorandom generator was the same for each calculation scheme that was tested .",
    "figure [ 3dspheretime ] shows the results . there is a noticeable difference in speed between the 8-term , 5-term and 2-term scheme , but they are in the same complexity class and for any given @xmath1 , their performance relative to each other is roughly the same .",
    "the slice - update scheme , by contrast , performs better relative to the other schemes when @xmath1 increases , as would be expected due to its lower complexity . even for small @xmath1",
    "it outperforms the other algorithms .",
    "points randomly selected on the surface of a sphere , averaged over 10 runs . ]",
    "what is interesting is that going from 8 terms to 5 terms causes a greater improvement than going from 5 terms to 2 terms , even though 2-dimensional hypervolume calculations are completely removed from the equation when going to 2 terms .",
    "no solid conclusions can be drawn from the magnitude of the differences , as they might depend on the implementation details of the code and the compiler optimizations . however",
    ", this does show that simplifying calculations can make a big difference for the speed of an algorithm .",
    "a benefit of the 2-term scheme which is not captured in the graph , is that it is the simplest scheme in terms of the number of operations that must be implemented , so the time needed to implement it will be shorter .",
    "the figure on the right plots the same data as a graph of the time required for 100.000 monte carlo iterations , compared to the time needed for the fastest non - monte carlo scheme . ]     points .",
    "the figure on the right plots the same data as a graph of the time required for 100.000 monte carlo iterations , compared to the time needed for the fastest non - monte carlo scheme . ]        the monte carlo scheme is a special case , in that the time it takes to run depends on the desired accuracy , and this accuracy in turn also depends on the variance of the predictive distribution .",
    "when this variance is higher , the accuracy will be lower . for a rough idea of its performance relative to the exact calculation schemes ,",
    "see figure [ accuracythings ] , which shows the number of monte carlo trials which can be performed if the algorithm is allowed to run for a second .",
    "because of the @xmath100 time complexity of each individual trial , it is less affected by @xmath1 than any of the exact calculation schemes .",
    "if @xmath1 is large enough and the desired accuracy is low enough , it might be the faster option .",
    "however , when @xmath1 is reasonably small , there is no advantage to using it .",
    "the complexity of calculating the expected hypervolume improvement of multiple points by repeatedly using one of the described algorithms is of course linear in the number of candidate individuals . here",
    ", the 8-term , 5-term and slice - update schemes have an advantage not shared by the monte carlo and 2-term schemes , in that their hypervolume calculations are independent of the probability distribution for which the ehvi is being calculated .",
    "this makes it possible to calculate several expected hypervolume improvements on the same population with a relatively small corresponding increase in calculation time , because the additional calculations have complexity @xmath3 .",
    "it is expected to be less impressive for the slice - update scheme , as this already has a time complexity of @xmath3 , but the amount of overhead that is avoided might still be noticeable . to determine the impact of this advantage on the relative performance of the schemes , figure [ multipoints ] shows the results of using the schemes to calculate the ehvi for a vector of probability distributions instead of just one .",
    "as can be seen , the time taken increases linearly in the number of individuals evaluated at the same time , but the constant added on top of that is larger for the 5-term scheme than for the slice - update scheme .",
    "when @xmath1 is 30 it is only a difference equivalent to evaluating a few more candidate individuals , however . because the 5-term scheme is somewhat easier to implement , it might be preferable to use it if the number of candidate individuals is expected to be high in comparison to @xmath1 .",
    "the main results realized in this thesis are as follows : a fast algorithm for calculating the ehvi in two dimensions was proposed with runtime complexity in @xmath0 ( previously : @xmath2 ) .",
    "an empirical test shows improved speed even for relatively small @xmath1 ( @xmath270 ) .",
    "an exact calculation algorithm for calculating the ehvi in more than two dimensions is provided .",
    "this generic algorithm has been detailed and improved in efficiency for the important tri - objective case .",
    "it has a cubic runtime complexity in @xmath3 , and a further efficiency gain can be obtained by batch evaluation , i.e. re - using the data structures for multiple ehvi computations .",
    "the algorithm are based on linear data structures and there are no large hidden constants . for three dimensions it is now possible to perform more than a hundred ehvi in 2.5 seconds for an approximation set size of 30 .",
    "implementations of all algorithms are made available [ todo : url ] and have been validated with results from monte carlo algorithms .",
    "the results open up new possibilities to construct expected improvement algorithms for multiobjective optimization , for instance by using the fast ehvi evaluation as an infill criterion in the efficient global optimization algorithm ( ego ) . moreover , as the new exact ehvi computation methods have the same or better runtime performance compared to the monte carlo algorithms used so far , they can now replace these inaccurate methods .    as a side result a relationship between the expected improvement and the center of ( probability ) mass over single cells",
    "was established , which might in the future shed some light on the relation to alternative expected improvement formulations [ keane04 ] and be useful for establishing theoretical results .",
    "this work is based on the honors master s thesis of iris hupkens @xcite under the supervision of m. emmerich and a. deutz .",
    "the sourcecode of all algorithms in c@xmath271 is made available on http://natcomp.liacs.nl/index.php?page=code ."
  ],
  "abstract_text": [
    "<S> the expected improvement algorithm ( or efficient global optimization ) aims for global continuous optimization with a limited budget of black - box function evaluations . </S>",
    "<S> it is based on a statistical model of the function learned from previous evaluations and an infill criterion - the expected improvement - used to find a promising point for a new evaluation . </S>",
    "<S> the ` expected improvement ' infill criterion takes into account the mean and variance of a predictive multivariate gaussian distribution .    </S>",
    "<S> the expected improvement algorithm has recently been generalized to multiobjective optimization . in order to measure the improvement of a pareto front quantitatively </S>",
    "<S> the gain in dominated ( hyper-)volume is used . </S>",
    "<S> the computation of the expected hypervolume improvement ( ehvi ) is a multidimensional integration of a step - wise defined non - linear function related to the gaussian probability density function over an intersection of boxes . </S>",
    "<S> this paper provides a new algorithm for the exact computation of the expected improvement to more than two objective functions . for the bicriteria case </S>",
    "<S> it has a time complexity in @xmath0 with @xmath1 denoting the number of points in the current best pareto front approximation . </S>",
    "<S> it improves previously known algorithms with time complexity @xmath2 . for tricriteria optimization </S>",
    "<S> we devise an algorithm with time complexity of @xmath3 . besides discussing the new time complexity bounds the speed of the new algorithm </S>",
    "<S> is also tested empirically on test data . </S>",
    "<S> it is shown that further improvements in speed can be achieved by reusing data structures built up in previous iterations . </S>",
    "<S> the resulting numerical algorithms can be readily used in existing implementations of hypervolume - based expected improvement algorithms .    </S>",
    "<S> * keywords : * multiobjective optimization , expected improvement , efficient global optimization , bayesian global optimization , hypervolume indicator </S>"
  ]
}