{
  "article_text": [
    "the paradigm of cooperation between agents in order to achieve some common objective has now become quite common in many areas such as control and distributed computation , while repesenting a multitude of different situations and related mathematical questions @xcite .    in the field of computation",
    ", the emphasis has been mainly on the paradigm of parallel computing in which some computational task is subdivided into as many subtasks as there are available processors .",
    "the subdivision naturally induces a communication structure ( or graph ) , connecting processors and the challenge is to achieve a subdivision that maximizes concurrency of tasks ( hence minimizing total computational time ) , while simultaneously minimizing communication overhead .",
    "this paradigm arose as a consequence of the usual architecture of most early multiprocessor machines , in which interprocessor communication is a much slower operation than a mathematical operation carried out in the same processor .",
    "disadvantages of this approach arise from the difficulty of effectively decomposing a large task into minimally connected subtasks , difficulties of analysis and the need for synchronization barriers at which all processors wait for the slowest one , in order to exchange information with the correct time stamps ( i.e. , without asymmetric delays ) .",
    "more recently , in the area of control , interest has been focused on multiagent systems , in which a number of agents cooperate amongst themselves , in a distributed manner and also subject to a communication graph that describes possible or allowable channels between agents , in order to achieve some ( computational ) task .",
    "similarly , in the area of computation , multicore processors have now become common  in these processors , each core acommodates a thread which is executed independently of the threads in the other cores .",
    "thus , in the context of this paper , which is focused on solution of the linear system of equations @xmath0 for a symmetric positive definite matrix @xmath1 of large size @xmath2 , we will assume that each agent carries out a task that is represented by one thread that executes on one core , so that , in this sense , the words agent and thread can be assumed to represent the same thing .",
    "in what follows , unless we are specifically talking about numerical implementation , we will give preference to the word agent .    with the advent of ever larger on - chip memory and multicore processors that allow multithread programming",
    ", it is now possible to propose a new paradigm in which each thread , with access to a common memory , computes its own estimate of the solution to the whole problem ( i.e. , decomposition of the problem into subproblems is avoided ) and the threads exchange information amongst themselves , this being the cooperative step .",
    "the design of a cooperative algorithm has the objective of ensuring that exchanged information is used by the threads in such a way as to reduce overall convergence time .",
    "the idea of information exchange between two iterative processes was introduced into numerical linear algebra long before the advent of multicore processors by brezinski @xcite under the name of _ hybrid procedures _ , defined as ( we quote ) `` a combination of two arbitrary approximate solutions with coefficients summing up to one ... (so that ) the combination only depends on one parameter whose value is chosen in order to minimize the euclidean norm of the residual vector obtained by the hybrid procedure ... the two approximate solutions which are combined in a hybrid procedure are usually obtained by two iterative methods . ''",
    "the objective of minimizing the residue is to accelerate convergence of the overall hybrid procedure .",
    "this idea was generalized and discussed in the context of distributed asynchronous computation in @xcite .",
    "more specifically , this paper explores the idea of cooperation between @xmath4 agents ( or threads ) in the context of the conjugate gradient ( cg ) algorithm applied to an @xmath2-dimensional linear system @xmath0 , for a symmetric positive definite matrix @xmath1 of large size @xmath2 . throughout the paper",
    "it is assumed that @xmath5 , and even that @xmath6 : the number of agents may be  large \" , but it is usually  much smaller \" than the  huge \" size of matrix @xmath1 .",
    "the famous cg  algorithm , proposed in @xcite , has several interesting properties , both as an algorithm in exact arithmetic and as one in finite precision arithmetic @xcite .",
    "however , it is well known that , due to its structure , it can not be parallelized in the conventional sense . in this paper",
    ", we revisit the cg  algorithm from a multithread perspective , which can be seen as a direct generalization of the control approach to the cg  algorithm proposed in @xcite , in which the scalar control parameters ( stepsizes in gradient and conjugate gradient directions ) are replaced by matrices ( i.e. , multivariable control ) .",
    "the cooperation between agents resides in the fact that the calculation of each entry of the control matrix now involves information that comes from the other agents .",
    "the method can also be seen as a generalization of the traditional cg  algorithm in which multiple descent and conjugate directions are updated simultaneously .",
    "the paper is organized as follows .",
    "section [ se2 ] briefly recalls the construction , as well as the main convergence results , of conjugate gradient method .",
    "section [ se3 ] then presents the new algorithm , called _ cooperative conjugate gradient ( ccg ) method_. in order to simplify this presentation of the new algorithm , the case of @xmath7 agents is first introduced in section [ se331 ] .",
    "the general case @xmath8 is then stated in full generality in section [ se332 ] , together with analysis results .",
    "complexity issues are broached in section [ se4 ] .",
    "the results stated therein concerns execution of ccg  algorithm in exact arithmetic .",
    "section [ se5 ] is then devoted to numerical experiments with the multi - thread implementation .",
    "section [ se6 ] provides conclusions and directions for future work .",
    "[ [ notation ] ] notation + + + + + + + +    for the fixed symmetric definite positive matrix @xmath9 , we define @xmath1-norm in @xmath10 by @xmath11 and define _ @xmath1-orthogonality _ ( or _ conjugacy _ ) of _ vectors _ by : @xmath12 we will also have to consider matrices whose columns are vectors of interest . accordingly , we will say that @xmath13 are orthogonal ( resp .",
    "@xmath1-orthogonal ) whenever each column of @xmath14 is orthogonal ( resp .",
    "@xmath1-orthogonal ) to each column of @xmath15 , that is when @xmath16    for any set of vectors @xmath17 , @xmath18 , we denote respectively @xmath19 and @xmath20_0^k$ ] the set of these vectors , and the matrix obtained by their concatenation : @xmath20_0^k = \\left [ \\begin{matrix } r_0 & r_1 & \\dots & r_k \\end{matrix } \\right]\\in\\rset^{n\\times ( k+1)}$ ]",
    ". the notation @xmath21_0^k$ ] will denote the subspace of linear combinations of the columns of the matrix @xmath20_0^k$ ] .",
    "when @xmath10 is the ambient vector space , we thus have @xmath22_0^k \\doteq \\left\\ { v\\in\\rset^n\\ : \\",
    "\\exists \\gamma\\in\\rset^{k+1},\\ v = \\sum_{i=0}^k \\gamma_i r_i = [ r_i]_0^k \\gamma \\right\\}\\ .\\ ] ] similarly , for matrices @xmath23 , @xmath24 , the notation @xmath25 ( respectively , @xmath26_0^k\\in\\rset^{n\\times ( k+1)p}$ ] ) is used for the set of these matrices ( respectively , the matrix obtained as concatenation of the matrices @xmath27 , i.e. , @xmath26_0^k = \\left [ \\begin{matrix } r_0 & r_1 & \\dots & r_k \\end{matrix } \\right]\\in\\rset^{n\\times ( k+1)p}$ ] . ) also , we write @xmath28_0^k$ ] for the subspace of linear combinations of the columns of @xmath26_0^k$ ] : @xmath29_0^k \\doteq \\left\\ { v\\in\\rset^n\\ : \\ \\exists \\gamma\\in\\rset^{(k+1)p},\\ v = [ r_i]_0^k \\gamma \\right\\}\\ .\\ ] ] notice that this notation generalizes the definition provided earlier for vectors , and that @xmath30 $ ] is already meaningful for a single matrix @xmath31 . as an example , @xmath32 = { \\mathrm{rank}}\\ r$ ] .",
    "last , for any matrix @xmath31 and for any set @xmath33 of indices in @xmath34 , we will denote @xmath35",
    "one approach to solving the equation @xmath36 with @xmath1 symmetric positive definite and of large dimension , is to minimize instead the convex quadratic function @xmath37 since the unique optimal point is @xmath38 .",
    "several algorithms are based on the standard idea of generating a sequence of points , starting from an arbitrary initial guess , and proceeding in the descent direction ( negative gradient of @xmath39 ) , with an adequate choice of the step size . in mathematical terms : @xmath40 where @xmath41 is the step size .",
    "the vector @xmath42 represents both the _ gradient _ of the cost function @xmath43 at the current point @xmath44 , and the current _ residue _ in the process of solving .    amongst the possible choices for @xmath41 , a most natural one",
    "consists in minimizing the value of the function @xmath43 at @xmath45 , that is in taking @xmath46 the algorithm obtained using this principle is the _ steepest descent method _ , and one shows easily that the optimal value is given by the rayleigh quotient @xmath47 algorithm - is convergent , but in general one can not expect better convergence speed than the one provided by @xmath48 where @xmath49 is the condition number @xmath50    the main weakness of steepest descent is the fact that steps taken in the same directions as earlier steps are likely to occur .",
    "the _ conjugate direction methods _ avoid this drawback . based on a set of @xmath2 mutually conjugate nonzero vectors @xmath51 ( that is @xmath52 for any @xmath53 , @xmath54 ) , the family of conjugate direction methods use the sequence generated according to @xmath55 it is a classical result that the residue @xmath56 is orthogonal to the directions @xmath57 , @xmath58 ; and that @xmath45 indeed minimizes @xmath43 on the affine subspace @xmath59_0^k$ ] @xcite . as a consequence of this last property , conjugate direction methods lead to finite time convergence ( in exact arithmetic ) .    the _ conjugate gradient method _ , developed by hestenes and stiefel @xcite , is the particular method of conjugate directions obtained when constructing the conjugate directions by gram - schmidt orthogonalization , achieved at step @xmath60 on the set of the gradients @xmath19 .",
    "a key point here is that this construction can be carried out iteratively .",
    "the iterative equations of the conjugate gradient method are given in the pseudocode instructions of algorithm [ algo1 ] .",
    "instructions [ cgb1][cgb2 ] constitute the optimal descent process in the direction @xmath61 ; while instructions [ cgc1][cgc2 ] achieve iteratively the orthogonalization of the subspaces @xmath21_0^k$ ]",
    ".    choose @xmath62 @xmath63 @xmath64 @xmath65 @xmath66 [ cgb1 ] @xmath67 [ cgb2 ] @xmath68 @xmath69 [ cgc1 ] @xmath70 [ cgc2 ] @xmath71    we recall the main properties of this algorithm , in an adapted form , to allow for easier comparison with the results to be stated later .",
    "[ thcg ] as long as the vector @xmath61 is not zero    * the vectors @xmath19 are mutually orthogonal , the vectors @xmath72 are mutually @xmath1-orthogonal , and the subspaces @xmath21_0^k , { \\mathrm{span}}\\ [ d_i]_0^k$ ] and @xmath73_0^k$ ] are equal and have dimension @xmath74 ; * the point @xmath45 is the minimizer of @xmath43 on the affine subspace @xmath59_0^k$ ] .",
    "when the residue vector is zero , the optimum has been attained , showing that cg terminates in finite time .",
    "apart from the finite time convergence property , the following formula indicates net improvement with respect to steepest descent : @xmath75 which represents substantial improvement with respect to .    for a proof of this theorem as well as",
    "further details on the contents of this section , see @xcite .",
    "in this subsection , in order to aid comprehension and ease notation , the case of two agents ( the case @xmath7 ) is considered : their estimates at step @xmath76 are written as @xmath77 respectively , the residues as @xmath78 , and the two descent directions as @xmath79 . the gradients at each one of the current estimates are given as @xmath80 with @xmath81 . as for cg  method , we distinguish two steps .    [ [ bullet - descent - step . ] ] @xmath82 descent step .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    given the current residues @xmath78 and two descent directions @xmath79 , this step determines the upgraded value of the estimates @xmath83 and therefore of the residues @xmath84 .",
    "one allows the use of the two descent directions @xmath79 , thus looking for updates of the form @xmath85 the matrix @xmath86 has to be chosen . in the same spirit as for cg , this choice is made in such a way as to minimize @xmath87 and @xmath88 .",
    "this yields in fact two independent minimization problems . denoting @xmath89 the two optimality conditions",
    "are given by @xmath90 this shows that the minimum is uniquely defined , and attained when @xmath91 notice that the two descent directions @xmath79 have to be linearly independent for the matrix in to be invertible .",
    "similarly to cg  algorithm , we have the following _ four _ useful properties @xmath92    [ [ bullet - orthogonalization - step . ] ] @xmath82 orthogonalization step . + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the second step consists , given the residues @xmath84 and the current descent directions @xmath79 , in determining the next descent directions @xmath93",
    ". the latter should be @xmath1-orthogonal to all the previous descent directions .",
    "in fact , it will be sufficient to ensure @xmath1-orthogonality to @xmath79 , as for cg .",
    "one takes @xmath94 the matrix @xmath95 is chosen to ensure the _ four _ conditions @xmath96 this also leads to two independent problems for the two vectors @xmath93 : writing now @xmath97 the previous orthogonality conditions can be written as : @xmath98 which yields the unique solution @xmath99    [ [ bullet - summary - of - ccgin - the - case - p2 . ] ] @xmath82 summary of ccg  in the case @xmath7 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    putting together the previous findings , we summarize the ccg  algorithm in the case @xmath7 as    [ ccg2 ] @xmath100      we now provide generalization to the case of @xmath8 agents .",
    "the extension is indeed straightforward from .",
    "the matrices whose @xmath101-th column represents respectively the solution estimate , the residue and the descent direction of agent @xmath101 , @xmath102 , for iteration @xmath76 are denoted @xmath103 , @xmath104 , @xmath105 . in other words ,",
    "@xmath106 stand for the matrices written down @xmath107 in section [ se331 ] .",
    "the algorithm ccg  in full generality is given as the list of instructions in algorithm [ algo2 ] .",
    "algorithm ccg  is a generalization of cg  , which is the case @xmath108 . in all algorithms in this paper ,",
    "comments appear to the right of the symbol @xmath109 .",
    "choose @xmath110 @xmath111 @xmath112 @xmath65 @xmath113 [ ccgb1 ] @xmath114 [ ccgb2 ] @xmath115 [ ccga ] @xmath116 [ ccgc1 ] @xmath117 [ ccgc2 ] @xmath71    [ thccg ] as long as the matrix @xmath118 is full rank ( that is @xmath119 )    * the matrices @xmath120 are mutually orthogonal , the matrices @xmath121 are mutually @xmath1-orthogonal , and the subspaces @xmath28_0^k , { \\mathrm{span}}\\ [ d_i]_0^k$ ] and @xmath122_0^k$ ] are equal and have dimension @xmath123 ; * for any vector @xmath124 of the canonical basis of @xmath125 , the vector @xmath126 ( which constitutes the @xmath101-th column of @xmath127 ) is the minimizer of @xmath43 on the affine subspace @xmath128_0^k$ ] .    theorem [ thccg ] indicates that , as long as the residue vector @xmath129 is full rank , the algorithm ccg  behaves essentially as does cg , providing @xmath4 different estimates at iteration @xmath76 , each of them being optimal in an affine subspace constructed from one of the @xmath4 initial conditions and the common vector space obtained from the columns of the direction matrices @xmath130 , @xmath131 .",
    "this vector space , @xmath132_0^k$ ] , has dimension @xmath123 : each iteration involves the cancellation of @xmath4 directions . notice that different columns of the matrices @xmath118 are not necessarily @xmath1-orthogonal ( in other words , @xmath133 is not necessarily diagonal ) , but , when @xmath129 is full rank , they constitute a set of @xmath4 independent vectors .",
    "the statement as well as the proof of this theorem are inspired by the corresponding ones for the conventional cg  algorithm given in @xcite and @xcite .",
    "@xmath82 we first show that for any @xmath76 , @xmath134_0^k = { \\mathrm{span}}\\ [ d_i]_0^k\\ .\\ ] ] @xmath82 we show the first point by induction .",
    "clearly , @xmath135 for any @xmath136 , and @xmath28_0^k= { \\mathrm{span}}\\ [ d_i]_0^k= { \\mathrm{span}}\\ [ a^ir_0]_0^k$ ] when @xmath137 , while nothing has to be verified for the orthogonality conditions .",
    "assume that , for some @xmath76 , they are verified for any @xmath138 , let us show them for @xmath60 , assuming that @xmath139 is full rank . from lines [ ccgb2][ccga ] of algorithm [ algo2 ] , @xmath140 .",
    "by induction , the columns of both the matrices @xmath129 and @xmath141 are located in @xmath122_0^{k+1}$ ] .",
    "thus , @xmath142_0^{k+1}\\ ] ] and consequently @xmath143_0^{k+1}\\subset { \\mathrm{span}}\\ [ a^ir_0]_0^{k+1}\\ .\\ ] ]    on the other hand , for any vector @xmath124 of the canonical basis of @xmath125 , @xmath144_0^k\\ ] ] because each residue is orthogonal to the previous descent directions , so that @xmath145_0^k$ ] for some @xmath124 would imply @xmath146 , which contradicts the assumption of full rankness of @xmath139 .",
    "indeed , for the same reason , one can also show that , for any @xmath147 , @xmath148_0^k$ ] . using again the fact that @xmath149 = p$ ]",
    ", one sees that @xmath143_0^{k+1}\\supset { \\mathrm{span}}\\ [ a^ir_0]_0^{k+1}\\ ] ] and indeed @xmath143_0^{k+1}= { \\mathrm{span}}\\ [ a^ir_0]_0^{k+1}\\ .\\ ] ]    one shows similarly from lines  of algorithm [ algo2 ] that @xmath150_0^{k+1}\\subset { \\mathrm{span}}\\ [ a^ir_0]_0^{k+1}\\ , \\ ] ] and the equality is obtained using the same rank argument .    the dimension of these sets is",
    "@xmath151 , as they contain the @xmath4 independent vectors in @xmath152 $ ] orthogonal to @xmath132_0^k$ ] .    from line [ ccgc2 ] of algorithm [ algo2 ] ,",
    "one gets @xmath153 for @xmath154 , the first term is zero because @xmath155_0^{i+1}$ ] and the gradients constituting the columns of @xmath139 are orthogonal to any vector in @xmath156_0^{i+1}$ ] ; while the second term is also zero due to the induction hypothesis . for @xmath157 ,",
    "the right - hand side of is zero because @xmath158 is precisely chosen to ensure this property .",
    "thus the @xmath159 are mutually @xmath1-orthogonal .",
    "orthogonality of @xmath139 follows from lines [ ccgb1 ] and [ ccga ] of algorithm [ algo2 ] .",
    "the induction hypothesis has been proved for @xmath60 concluding the proof of the first part of theorem [ thccg ] .",
    "@xmath82 [ optimality ] .",
    "arguing as in @xcite , we can write @xmath160 for some @xmath161 .",
    "optimality implies that @xmath162 substituting ( [ opt0 ] ) in ( [ opt1 ] ) and rewriting in terms of the matrices @xmath163 and @xmath164 yields @xmath165 on the other hand , @xmath166 thus @xmath167 substituting ( [ opt4 ] ) in ( [ opt2 ] ) , we get : @xmath168    a natural question is now to study the cases where at some point of the execution of the algorithm ccg one gets @xmath169 . in the best case",
    ", this occurs because one of the columns of @xmath129 is null , say the @xmath101-th one , meaning that @xmath170 , and thus that the @xmath76-th estimate of the @xmath101-th agent is equal to the optimum @xmath38 .",
    "but , of course , @xmath171 can be smaller than @xmath4 without any column of @xmath129 being null .",
    "first of all , the following result ensures that this rank degeneracy is , in general , avoided during algorithm execution .",
    "[ thgen ] for an open dense set of initial conditions @xmath172 in @xmath173 , one has during any ccg  run @xmath174 moreover @xmath175_0^{k^ * } = p\\lfloor \\frac{n}{p } \\rfloor\\ .\\ ] ] otherwise said : generically , algorithm ccg  can be run during @xmath176 steps , and    * any of the columns of @xmath177 minimizes @xmath43 on an affine subspace of @xmath10 of codimension @xmath178 ; * application of cg  departing from any of the columns of @xmath179 yields convergence in at most @xmath180 steps .",
    "the second part of theorem [ thgen ] has to interpreted as follows .",
    "when the size @xmath2 of the matrix @xmath1 is a multiple of the number @xmath4 of agents , then ccg  generically ends up in @xmath181 steps .",
    "when this is not the case , the estimates @xmath179 obtained for @xmath182 minimize the function @xmath43 on affine subspace whose underlying vector subspace is @xmath132_0^{k^*}$ ] ( see theorem [ thccg ] ) .",
    "the interest of is to show that this subspace is quite large : its codimension is @xmath183 , which is at most equal to @xmath184 .",
    "the main point consists in showing that generically , @xmath176 iterations of the algorithm ccg  can be conducted without occurrence of the rank deficiency condition . as a matter of fact ,",
    "the other results of the statement are direct consequences of this fact .",
    "to show the latter , use is made of theorem [ thccg ] . from the properties stated therein , one sees that , for any @xmath185 , the rank of @xmath186 is deficient if and only if a linear combination of the @xmath4 column vectors of @xmath172 pertains to the @xmath187-dimensional subspace @xmath132_0^{k-1}$ ] . in a vector space of dimension",
    "@xmath188 , this occurs only in the complement of an open dense set .",
    "now , if the column vectors of @xmath186 are linearly independent , the same is true for @xmath129 , see line [ ccga ] of algorithm [ algo2 ] , and as well for @xmath118 , see line [ ccgc2 ] .",
    "this completes the proof of theorem [ thgen ] .",
    "we now study what can be done in case of rank degeneracy .",
    "when @xmath189 is such that @xmath190 , this means that trajectories initially independent have come to a point where the estimates in @xmath186 will converge along directions which are now linearly dependent .",
    "the natural solution is then to choose any full - rank subset of trajectories .",
    "we thus propose the modified algorithm [ algo3 ] .",
    "[ thmccg ] for any nonzero initial condition @xmath172 , algorithm mccg  ends up in @xmath191 iterations for some @xmath192 .",
    "moreover    * the sequence @xmath193 is nonincreasing ; * any of the columns of @xmath194 minimizes @xmath43 on an affine subspace of @xmath10 of codimension @xmath195 ; * application of cg  departing from any of the columns of @xmath194 yields convergence in at most @xmath196 steps .",
    "the proof is straightforward and omitted for brevity .",
    "choose @xmath110 @xmath111 @xmath112 @xmath197 @xmath65 choose @xmath198 such that",
    "@xmath199 @xmath200 @xmath201 @xmath202    [ while ] @xmath113 @xmath114 @xmath115 @xmath116 @xmath117 @xmath203 choose @xmath204 such that @xmath205 @xmath206 @xmath207 @xmath208 @xmath71 [ iter ]",
    "this section is concerned with the evaluation of the gain in computation time of the numerical solution of equation , when using ccg  algorithm with @xmath4 agent , i.e. , the gain which is expected is due to the parallelism induced by a multithread implementation .",
    "we evaluate this issue here _ assuming computations in exact arithmetic_. moreover , thanks to theorem [ thgen ] , we adopt the generic assumption that the rank of the residue matrices remains constant ( and full ) , and that the computations are then carried out for @xmath209 iterations . disregarding as marginal the supplementary cg  steps ( see the statement of theorem [ thgen ] ) ,",
    "we thus consider it to be realistic to quantify the worst case complexity by evaluating _ the numbers of multiplications involved by @xmath181 iterations of ccg_. recall that the case @xmath108 corresponds to the usual cg  algorithm .",
    "we propose the multithread implementation detailed in table 1 .",
    "[ cols=\"^,^,^,^ \" , ]",
    "this paper proposed a new cooperative conjugate gradient ( ccg ) method for linear systems with symmetric positive definite coefficient matrices .",
    "this ccg  method permits efficient implementation on a multicore computer and experimental results bear out the main theoretical properties , namely , that speedups close to the theoretical value of @xmath4 , when a @xmath4-core computer is used , are possible , when the matrix dimension is suitably large . the experimental results of the current study were limited to dense randomly generated matrices and only @xmath210 cores of a @xmath211 core computer with a relatively small on - chip shared memory were used .",
    "future work will include the study of the method on matrices that come from real applications and are typically sparse and sometimes ill - conditioned ( which will necessitate the use of preconditioners ) on larger multi - core machines .",
    "the use of larger machines should also permit exploration of the notable theoretical result ( corollary [ cor2 ] ) that , in the asymptotic limit , as @xmath2 becomes large , implying that @xmath4 also increases according to ( [ appra ] ) , solution of @xmath0 is possible by the method proposed here with a cost of @xmath212 multiplications .",
    "a.  bhaya , p .- a .",
    "bliman , and f.  pazos .",
    "cooperative parallel asynchronous computation of the solution of symmetric linear systems . in _ proc . of the 49th ieee conference on decision and control",
    "_ , atlanta , usa , december 2010 ."
  ],
  "abstract_text": [
    "<S> this paper proposes a generalization of the conjugate gradient ( cg ) method used to solve the equation @xmath0 for a symmetric positive definite matrix @xmath1 of large size @xmath2 . </S>",
    "<S> the generalization consists of permitting the scalar control parameters (= stepsizes in gradient and conjugate gradient directions ) to be replaced by matrices , so that multiple descent and conjugate directions are updated simultaneously . </S>",
    "<S> implementation involves the use of multiple agents or threads and is referred to as cooperative cg ( ccg ) , in which the cooperation between agents resides in the fact that the calculation of each entry of the control parameter matrix now involves information that comes from the other agents . for a sufficiently large dimension @xmath2 , </S>",
    "<S> the use of an optimal number of cores gives the result that the multithread implementation has worst case complexity @xmath3 in exact arithmetic . </S>",
    "<S> numerical experiments , that illustrate the interest of theoretical results , are carried out on a multicore computer . </S>"
  ]
}