{
  "article_text": [
    "data mining is a process of extracting the knowledge from large set of data .",
    "the knowledge extraction defines a model or rules for performing the accurate analysis on the future data .",
    "the initial data set is referred as training data , since it is used as reference for arriving at knowledge .",
    "the analysis performed on the data should not reveal the data as such and hence preserving the privacy of data becomes significant .",
    "intuitively , differential privacy ensures that the system behaves the essentially same way , independent of whether any individual , or small group of individuals , opts in to or opts out of the database [ 1 ] .",
    "generalization and suppression are two predominant techniques to achieve the privacy of data [ 2 - 3 ] .",
    "privacy of data is very important in certain domains like hospital , analysis of psychological behaviour of patients [ 4 ] .",
    "the solution presented in this paper is to apply gradient descent methods on the privacy preserved data .",
    "the gradient descent is first order optimization algorithm to find local minimum of the function [ 5 ] .",
    "gradient descent is a widely used paradigm for solving many optimization problems . in machine learning or data mining",
    ", this optimization function corresponds to a decision model that is to be discovered [ 6 ] .",
    "shuguo han _ et .",
    "_ has proposed the solution for application of gradient descent methods on the privacy preserved data .",
    "the data is either vertically or horizontally partitioned across communicating parties . the factor for partition",
    "is mutually synchronized between each other . in vertical partitioning",
    "every communicating party has same set of objects with varying attributes . in this scenario",
    "the prediction is first performed on unknown attributes before performing prediction for specific object . in horizontal partitioning every communicating party",
    "has disjoin set of records with same set of attributes . the prediction in this scenario",
    "is performed on unknown objects . to achieve required accuracy in prediction gradient descents",
    "are used in the context of machine learning .",
    "the gradient descent follows the iterative approach to minimize the prediction function until local minimum is reached .",
    "the local minimum is the threshold which defines the required accuracy factor .",
    "the gradient descents are applied on matrix of numbers and the values of unknown columns are determined using random distribution before prediction .",
    "the solution proposed works in 2 different stages .",
    "first stage defines the model or rules representing the knowledge about the data .",
    "the model is defined separately by every communication party . while defining the model the values of unknown attributes",
    "are inferred from bamboozled abstract information .",
    "the abstract information indicates the high level generalized information .",
    "the abstract information is further disguised using disguising factor to make other communication party deceived by the information shared and hence referred as bamboozle process .",
    "the bamboozling is performed to preserve the privacy at maximum extent .",
    "the data obtained after bamboozling is misleading meta data .",
    "the meta data is processed by every communication party to find the values of unknown attributes .",
    "the aggregation of both unknown and known attributes produces the regression model .",
    "the regression model represents the upper bound values which are optimized with the application gradient methods until the required accuracy is reached .",
    "the prediction function used here is the quadratic function defined as @xmath0 where @xmath1 represents the weight vector used as coefficient of function which is diminished in every iteration of gradient descent until local optimum is reached .",
    "the factor that decides the local minimum function is also referred as learning stoppage as it terminates the prediction process .",
    "there are two approaches in gradient descent @xmath2 batch and stochastic . in stochastic model prediction",
    "is performed by considering one data sample at a time . in batch",
    "gradient all the samples are processed before updating individual sample .",
    "batch gradient takes longer in its inner loop ( large sum ) but it can use a larger step size because of this sum . although these methods assume un - thresholded linear units , they can be easily modified to work on regular perceptions [ 7 - 12 ] . in this paper the behaviour of both batch and stochastic gradient methods",
    "are analyzed w.r.t number of iterations and execution time required to perform the prediction process .",
    "the bamboozled information is conveyed using standard format which is machine readable .",
    "one such meta data is resource description framework ( rdf ) , in which the information is represented as sequence of triplets .",
    "every rdf [ 13 ] should adhered to respective domain ontology .",
    "ontology [ 14 ] defines the concepts and relations among the concepts , rdf describes the web document in the form of triplets .",
    "every rdf triplet is a composition of subject , predicate and object [ 15 ] .",
    "the rdf is processed by every communicating party to get the values of unknown attributes .",
    "this section describes the rest of the paper in brief .",
    "section 3 describes the background work and its comparison with current solution .",
    "section 2 describes the other significant related work performed in this area .",
    "section 4 defines the problem .",
    "section 5 defines mathematical model .",
    "section 6 describes the algorithms used to arrive at the solution section 7 gives the overall system architecture along with the ontology model used in this paper .",
    "section 8 explains the experimental results and data set used to simulate the process .",
    "the paper concludes by mentioning the enhancement that can be incorporated in prediction process along with the suppression and the list of references considered by the authors .",
    "some predominant research is performed in the area of data privacy preservation .",
    "kanishka bhaduri _ et .",
    "al . , _ [ 16 ] , suggested an approach to allow a user to control the amount of privacy by varying the degree of nonlinearity .",
    "it is also shown how the general transformation can be used for anomaly detection in practice for two specific problem instances : a linear model and a popular nonlinear model using the sigmoid function .",
    "there is also an analysis on the proposed nonlinear transformation in full generality and then show that , for specific cases , it is distance preserving .",
    "a main contribution of this paper is the discussion between the invertibility of a transformation and privacy preservation and the application of these techniques to outlier detection .",
    "benjamin c m fung _ et .",
    "al . , _ [ 17]done the study on the resolution of a privacy problem in a real - life mashup application for the online advertising industry in social networks , and propose a service - oriented architecture along with a privacy - preserving data mashup algorithm to address the aforementioned challenges .",
    "experiments on real - life data suggest that our proposed architecture and algorithm is effective for simultaneously preserving both privacy and information utility on the mashup data . to the best of our knowledge ,",
    "this is the first work that integrates high - dimensional data for mashup service .",
    "jung yeon hwang _ et .",
    "18]has presented a short group signature scheme for dynamic membership with controllable linkability.the controllable linkability enables an entity who possesses a special linking key to check if two signatures are from the same signer while preserving anonymity .",
    "it can be used for various anonymity - based applications that require necessarily the linkability such as vehicular ad hoc network , and privacy - preserving data mining .",
    "our scheme is sufficiently efficient and so , well - suited for real - time applications even with restricted resources , such as vehicular ad hoc network and trusted platform module .",
    "zhu yu - quan , tang yang _ et .",
    "al.,_[19]have addressed the problem that the existing protocol of secure two - party vector dot product computation has the low efficiency and may disclose the privacy data , a method which is effective to find frequent item sets on vertically distributed data is put forward .",
    "the method uses semi - honest third party to participate in the calculation , put the converted data of the parties to a third party to calculate .",
    "alberto trombetta _ et .",
    "_ [ 20 ] have come up with two protocols solving k - anonymity problem on suppression - based and generalization - based @xmath3-anonymous and confidential databases .",
    "the protocols rely on well - known cryptographic assumptions and we provide theoretical analyses to proof their soundness and experimental results to illustrate their efficiency .",
    "shuguo han _ et .",
    ", _ [ 6 ] have proposed the application of stochastic gradient descent methods on the vertically partitioned .",
    "the experiment is performed on the matrix of numbers and the data of unknown attributes are generated by using random distribution .",
    "but the current solution gives the relevant information to the other communication party to find the values of unknown attributes .",
    "even though the information is relevant it preserves the privacy of data by exposing the upper bound generalized values . to energize the privacy",
    "the generalized data is disguised and hence the complete data given other communicating party forms the semantic metadata .",
    "even though gradient descents take more iterations because the of bamboozling process but the accurate prediction is ensured by using descent learning stoppage .",
    "given the database distributed across the communicating parties using vertical partitioning , the main problem is to perform the prediction of single data with proper synchronization but not disclosing each other s data in other words the privacy of data should not be affected .",
    "the prediction is performed using gradient descent methods incorporated as iterative process .",
    "the gradient descents are required to applied on privacy preserved data .",
    "it is assumed that the data is partitioned and both communicating parties should adhered to common ontology model .",
    "following table enumerates the list of notations used and their purpose while defining the model .",
    "@xmath4 & learning rate for + & stochastic gradient . + @xmath5 &",
    "learning rate of + & batch gradient .",
    "+ @xmath6 & learning stoppage + @xmath7 & result vector after + & first stage prediction",
    ". + @xmath8 & result vector after + & second stage prediction . + @xmath9 & weight vector used + & as a reduction factor .",
    "+ @xmath10 & second stage prediction + & function defined as the + & function of weight vector + & and first stage result vector .",
    "+ @xmath11 & un partitioned data records .",
    "+ @xmath12 & number of data records .",
    "+ @xmath13 & number of attributes + & of every data record .",
    "+ @xmath14 & expected vector + & for all data records .",
    "+      * learning rate * : the learning rate is factor by which the value is minimized . the learning rate and iterations to reach local minimum is inversely proportional to each other @xmath2 if the learning rate is high then the number of iterations required to reach local minimum for a function is less and vice versa . -",
    "is the learning rate of stochastic gradient and for the employee case study it is maintained as 0.00001 as the sample values are in hundred to thousand range . *",
    "@xmath5 - * is the learning rate of batch gradient method and for the employee case study it is maintained as 0.000001 as the sample values are in hundred to thousand range and it runs over all the samples before it updates particular sample and hence for employee case study it is maintained as 0.000001 @xmath2 @xmath15 . :",
    "the weight vector is a reduction factor to minimize the first stage prediction result .",
    "the gradient descent methods runs over several iterations and in every iteration the weight vector is reduced by certain factor until local minimum is reached .",
    "the weight vector is defined as follows , @xmath16 any @xmath17 value of weight vector is an positive numbers . for current implementation every element of weight vector is initialized to 1 and it is reduced by small factor in every iteration . : the data set is a collection of data records where every record is an information about individual object and defined as follows , + @xmath18 = \\ { @xmath19= @xmath20 any @xmath17 value of @xmath21 is a object of m attributes .",
    "the entire table structure is of @xmath22 dimension .",
    ": this is the result of first stage prediction and is defined as , @xmath23 any @xmath17 value of weight vector is an positive integer .",
    "the prediction is done for every object using rdf metadata and hence the magnitude of @xmath24 @xmath2 @xmath25 is @xmath12 . : in second stage the gradient descent methods are applied on @xmath7 ( first stage prediction result ) by multiplying weight vector in multiple iterations . in every iteration the prediction function is minimized using updated vector .",
    "the prediction function is defined as , @xmath26 the prediction function is a square of weight vector multiplied with the first stage prediction function . in order to minimize the function in fine granules without the data loss",
    "the quadratic factor is used .",
    "( 4 ) can also be defined as , @xmath27 every @xmath17 value is predicted as the square of corresponding @xmath17 element of weight vector and first stage prediction vector .",
    ": there are two variations in updating weight vector depending on the type of gradient descent method .",
    "the stochastic gradient approach updates sample as soon as it is encountered it unlike batch gradient which runs over all samples before it updates any individual sample . according to stochastic gradient method any @xmath17 element of weight vector",
    "is updated as per the eq .",
    "( 6 ) @xmath28 similarly the batch gradient method updates the weight vector by considering all the elements of weight vector before it updates particular element and hence it is defined as per eq .",
    "( 7 ) . @xmath29",
    "the @xmath30 is a differential of second stage prediction function w.r.t @xmath1 ( weight vector ) and hence it is defined as , @xmath31 in every iteration the next state of weight vector is updated based on the previous value .",
    "when @xmath32 becomes zero the @xmath32 reaches minimum value and hence learning stops as there is no value change happens from previous to next step .",
    ": the expected vector is a collection of prediction values for every data record and is defined as , + @xmath33 : the learning stoppage is the minimum probability that decides the termination point of learning process .",
    "the expectation probability is the ratio of least square value of prediction to the expected outcome and is defined as per the eq .",
    "@xmath34 the learning process stops when expectation probability hits learning stoppage @xmath2 when @xmath35 .",
    "the entire learning process is driven by semantic web based two stage prediction process .",
    "each communicating party generates the required rdf metadata for their respective partitioned data .",
    "the first stage prediction process starts with each communicating parties exchanging the rdf metadata for their respective unknown values .",
    "the rdf metadata provides high level disguised information that allows communicating parties to infer upper bound values for the unknown attributes .",
    "once the unknown values are inferred they are processed with known data in second stage prediction process to reach the approximation to the expected vector . since the first stage prediction operates on upper bound values",
    "the gradient descents methods are applied in second stage prediction process to minimize the prediction vector in negative steepest descent until learning stoppage is reached .",
    "the data is partitioned vertically where every communication party has same set of records but with varying attributes .",
    ".algorithm for generating rdf model [ cols= \" < \" , ]      +    in first stage prediction process the @xmath36 and @xmath37 mutually exchanges the location of generated rdf data and individually the rdf data is process to find the values of unknown attributes .",
    "the first stage prediction vector is produced by summing up the values of known and interpreted values of unknown attributes .",
    "the communication parties follows the specific protocol for exchanging the required messages .",
    "the protocol has following message segments used , + * con_init : * connection initialization segment .",
    "the is sent by any communication party to trigger  the communication . + * con_init_ack : * acknowledgement to coninit .",
    "this is sent by the receiver . + * request message : * the message is a string which represents the request information . in first stage prediction",
    "this represents the filename of rdf required . in second stage",
    ", message is a name of the vector expected from other communication party .",
    "+ * response message : * this is the response message for request message .",
    "the message in this context is list of strings separated by a delimiter @xmath38 . in first stage process",
    "it is of the form rdf_file_name@xmath38 rdf_url.similarly in second stage  it is vector_name@xmath38 json(vector_name ) .",
    "+ where json(vector_name ) is a json representation of vector . +",
    "* con_term : * request to terminate communication . +",
    "* con_term_ack : * acknowledgement for con_init . +",
    "both communication parties has shared secret key and the messages are encrypted using des algorithm .",
    "all the protocol segments are encrypted and the cipher is exchanged between the communication parties , hence the secure communication adds further privacy to the data .",
    "the table 4 is an algorithm for second stage prediction .",
    "this algorithm takes first stage result , weight vectors , expected vector and gradient descent type as inputs .",
    "it applies the prediction function as shown in step 1 to find the second stage prediction vector .",
    "the predicted vector is normalized / fine tuned using gradient descent methods in the direction of negative steepest descent until expectation probability reaches learning stoppage .",
    "the algorithm applies only one gradient descent method at a time .",
    "this is an iterative algorithm , where in every iteration the weight vector is reduced and prediction function is applied until expectation probability becomes less than or equal to learning stoppage . as shown in step 5 the weight vectors are updated based on the type of gradient descent method .",
    "if the type is stochastic it updates individual element at a time as per eq .",
    "( 6 ) else if the type is batch gradient descent then average of weight vector values are calculated before it updates any element of weight vector as per eq .",
    "the algorithm runs with single gradient descent method at a time .",
    "the second stage algorithm uses the same protocol segments as used in first stage prediction for exchange of messages and all the messages are encrypted before they are exchanged .",
    "the sub graphs are processed to from all possible rdf - triplets which are submitted to the database to retrieve url set . since there is a probability that rdf - triplets are repeated in multiple web pages the final url set is obtained by the intersection of url sets of all rdf - triplets matching the user query as explained in the algorithm of table 5 .",
    "as shown in figure 2 .",
    "the data records are vertically partitioned between the two processes ( @xmath36 and @xmath37 ) .",
    "vertical partitioning means that every process has same set of records with disjoint attributes set similarly horizontal partitioning means that every process has disjoint set of records but with the same attributes .",
    "complete prediction process is divided into two stages .",
    "the ontology defines concepts and relations between the concepts .",
    "ontology is domain specific and it is generated based on the database schema .",
    "ontology defines concept for every table in the schema and relation between the tables are defined as object and data type properties .",
    "the object property describes the object from other object and the object being described is referred as subject .",
    "the data type property describes the subject using the textual information for example the email of @xmath37 can be described with the relation @xmath39hasemail@xmath40 between the person and string . where @xmath37 belongs to the concept person and",
    "his email say @xmath41 is conceptualized as a string .",
    "the ontology model gives the specification for which the multiple rdf s adhered to .",
    "the rdf represents a metadata for the corresponding partitioned data .",
    "the metadata is high level generalized information .",
    "the generalized information is further disguised to increase the generality using the disguising factor .",
    "the rdf is represented as collection of triplets which are defined according to the corresponding domain ontology .",
    "the @xmath36 and @xmath37 exchange s the rdf information mutually before the actual prediction process .",
    "the data inferred from the rdf is the result of first stage prediction process .",
    "the result of first stage prediction process defines the model for second stage prediction .",
    "the model in the machine learning context defines the knowledge extracted from the training data with aid of past experience but here the new approach is used to define the rules / model / knowledge from the actual data .",
    "the rdf content is used as semantics to define the model instead of the training data . since the semantics is more relevant or nearest to the actual data but not data itself and hence the privacy is preserved .",
    "the result of the first stage prediction process in this context is the regression model.the fig 1 shows the ontology model for employee domain . the ontology model is a definition of concepts and relations for generalization .",
    "the ontology model defines @xmath39haxmax@xmath40 and @xmath39hasmin@xmath40 relations between the employee and numbers domain .",
    "the @xmath39haxmax@xmath40 defines the maximum value of attribute among the records belonging to specific category like project manager , team lead and program manager . for example @xmath39hasmaxbasic@xmath40",
    "represents maximum value of basic component of salary and this is calculated for every category to create generalized information .",
    "similarly @xmath39hasminbasic@xmath40 represents minimum value of basic component of salary .",
    "all the attributes are represented as datatype properties except @xmath39hasdata@xmath40 which represents the partitioned data of employee database .",
    "the rdf generator uses the relations defined in ontology model to represents the most generalized information as semantics for known attributes .",
    "the sample rdf generated is shown in table 5 .",
    "+ rdf - syntax - ns # `` @xmath40 + @xmath39 rdf : description rdf : about + = ' ' http://www.skumarsolutions.com/id10 \" @xmath40 +   +   +   +   +   +   +   +   +   + @xmath39/rdf : description@xmath40 +        the rdf metadata is a high level generalized information disguised with certain disguising factor to preserve the privacy of data .",
    "the disguising factor is not disclosed between the communicating parties , each of them can use his / her own disguising factor . @xmath36 and @xmath37 combinedly defines the model from the deceived data .",
    "the rdf generator is a vital component which is responsible for generating required rdf semantics for the vertically partitioned data .",
    "this component runs over every record and creates respective generalized data depending on the category of record .",
    "the generalized information is deceived to other communicating party with the aggregation of disguising factor .",
    "the disguised information is used to infer the values of unknown attributes of records and finally a regression model is defined as a result .      in second stage prediction",
    "the regression model is used as input for further optimization .",
    "the coefficients of regression model are optimized using gradient descent methods .",
    "the batch / stochastic gradient descents are applied in negative steepest descent until the threshold is reached .",
    "the threshold indicates the fine granular value below which the further optimization is not required and hence it is a stoppage point for the prediction .",
    "the stochastic gradient descent is applied to reduce weight vector by considering single element at a time where as batch gradient runs on all elements before it updates particular element of weight vector . in every iteration",
    "the weight vector is updated and the predicted vector after every iteration is exchanged securely . any data exchanged between the communication parties are encrypted using common secret key with the known cryptographic algorithm .",
    "this adds the security further to the privacy preserved data .",
    "the experiment is carried out on the employee database for calculation of salary .",
    "the scenario is @xmath36 and @xmath37 heads payroll department with distributed database .",
    "the employee table is vertically partitioned in such a way that attributes forming the salary component is distributed across @xmath36 and @xmath37 department .",
    "the identified attributes of salary components are basic , hra , flat , travel , pf , gratuity , gd and performance award . after partitioning",
    ", the @xmath36 has \\ { basic , hra , flat , travel } and @xmath37 has \\ { pf , gratuity , gd and performance award } as shown in figure 3 .",
    "empid @xmath38 name@xmath38 basic@xmath38 hra@xmath38 flat@xmath38 travel@xmath38 pf@xmath42 gratuity",
    "@xmath38 gdp  @xmath38performanceaward @xmath42category@xmath42     ( alice ) :   @xmath38 empid@xmath38basic@xmath38 + @xmath38 hra@xmath38flat@xmath38 + @xmath38 cravel@xmath38category@xmath38 +    ( bob ) :   @xmath38 empid@xmath38pf@xmath38gratuity@xmath38 + @xmath38 gdp@xmath38performanceaward@xmath38 + @xmath38 category@xmath38 +    the employee i d and category is used in both departments for creating rdf semantics information .",
    "the category in this scenario indicates the designation of employee which is used as key attribute for generalizing the data .",
    "the generalization is performed by taking max value of known attributes depending on the category .",
    "for example , from the existing database content the maximum of basic , pf @xmath43 is determined for every category like teamlead , projectmanager and programmanager and further it is wrapped with disguising factor . @xmath36 and @xmath37 performs the above operation with their corresponding known attributes .",
    "the disguised information is serialized as rdf data and exchanged .",
    "since the known attributes of @xmath36 is unknown attributes for @xmath37 and vice versa is true and therefore @xmath36/@xmath37 mutually finds the values of unknown attributes using rdf semantics .",
    "the first stage prediction is performed by the summation of values of unknown attributes inferred from the rdf and known attribute values for every employee record .",
    "this results in single salary vector defining the model for second stage prediction process at each department .",
    "experiment is carried out with the disguising factor set to 10 $ during the first stage prediction process .",
    "the identified maximum values are disguised by adding 10 $ to the amount . @xmath36 and @xmath37 performs the optimization of regression model obtained as a result of first stage prediction process .",
    "the optimization is achieved by applying gradient descent methods on the predicted output . in gradient descent",
    "the weight vector is used as a coefficient for prediction function as defined in eq .",
    "( 5 ) and the weight vector is optimized as per eq .",
    "( 6 ) during stochastic gradient and as per eq .",
    "( 7 ) during batch gradient descent . in stochastic gradient descent",
    "the optimization happens in with less factor in each iteration as it update one element of weight vector at a time whereas in batch gradient the optimization happens with high factor .",
    "+ as a result of which the batch gradient descent takes less number of iterations to predict output for high minimization factor / learning stoppage as shown in figure 4 , which indicates for high minimization factor / learning stoppage the batch gradient descent method takes more iterations until switching point of 0.5 minimization factor after which the behavior of batch and stochastic remains stagnant .",
    "the stochastic gradient takes less number of iterations for prediction after 0.5 @xmath2 for lower values of minimization factor .",
    "similarly the execution time for batch gradient is high as number of iterations are more for prediction up to switching point as shown in figure 5 .",
    "after switching point the stochastic the batch gradient takes more time than stochastic for lower values of minimization factor .",
    "the experiment is carried out by varying learning stoppage from high value to low and behavior of stochastic and batch gradient descents are analyzed with respect to number of iterations and time required to perform prediction .",
    "the des algorithm is used as cryptographic algorithm to retain confidentiality of data exchanged between @xmath36 and @xmath37 .",
    "the experiment is simulated with raw sockets and implemented using java .",
    "the @xmath37 socket is used as server socket waiting for @xmath36 to initiate communication . in first stage prediction",
    "the rdf location is exchanged securely and in second stage the regression model which obtained as result from first stage prediction is exchanged securely between @xmath36 and @xmath37 .",
    "in this paper a new approach is proposed to retain the privacy of data with the combination of generalization and bamboozling .",
    "the bamboozling is the process where the second level privacy is achieved by disguising the generalized information with certain factor to deceive the other communication parties .",
    "the complete generalization and bamboozling process happens as first stage prediction process to define the regression model which is used as input for second stage prediction for optimization and predict accurate result .",
    "the experimental results shows the behaviour of batch and stochastic process with respect to the number of iterations and execution time required for prediction process .",
    "it is shown that batch gradient descent takes long time and more iterations for higher values of minimization factor than stochastic method .",
    "the solution can be enhanced by adding the suppression technique along with generalization and bamboozling processes to further strengthen the privacy of data .",
    "00 microsoft research.database privacy , _",
    "databaseprivacy_. latanya sweeney . achieving k - anonymity privacy protection using generalization and suppression , _ in international journal on uncertainty , fuzziness and knowledge - based systems _ , 10(5 ) : 571 - 588 , 2002 .",
    "gabriel ghinita , member , ieee , panos kalnis and yufei tao .",
    "anonymous publication of sensitive transactional data , _ in ieee transactions on knowledge and data engineering _ , 23(2 ) : 161 - 174 , feb 2011 .",
    "hopoper n , saunders j and mchugh l. the derived generalization of thought suppression , _ learn behav _ , 38(2 ) : 160 - 168 , 2010 . microsoft research .",
    "database privacy , _",
    "databaseprivacy _ shuguo han , student member , ieee computer society , wee keong ng , member , ieee computer society , li wan and vincent c s lee .",
    "privacy - preserving gradient - descent methods , _ ieee transactions on software engineering _ , 22(6):884 - 899 , june 2010 .",
    "afshar p. gradient descent optimisation for ilc - based stochastic distribution control , _ ieee international conference on control and automation(icca ) _ , 11341139 , 2009 .",
    "zhi ding , junqiang hu and dayou qian . on steepest descent adaptation : a novel batch implementation of blind equalization algorithms , _ global telecommunications conference ( globecom 2010)ieee _ , 1 - 6 , 2010 .",
    "gannot s. iterative - batch and sequential algorithms for single microphone speech enhancement , _ ieee international conference on acoustics , speech and signal processing ( icassp-97 ) _ , 2:1215 - 1218 , 1997 .",
    "gonzalez a. a note on conjugate natural gradient training of multilayer perceptrons , _ international joint conference on neural networks ( ijcnn 06 ) _ , 887 - 891 , 2006 .    ningning jia , e y lam",
    "stochastic gradient descent for robust inverse photomask synthesis in optical lithography , _",
    "17th ieee international conference on image processing _ , 2010 .",
    "s bonnabel .",
    "stochastic gradient descent on riemannian manifolds , _ ieee transactions on automatic control _ , 58(9 ) : 2217 - 2229 , 2013 .",
    "d beckett .",
    "rdf / xml syntax specification ( revised ) , _",
    "004/rec - rdf - syntax - grammar-20040210/ _ , 1994 .",
    "john hebeler , matthew fisher , ryan blac , andrew perez - lopez .",
    "semantic - web programming , _ third edition , wiley india pvt.ltd_ , 2009 .",
    "stefan decker , sergey melnik , frank van harmelen , dieter fensel , michel klein , jeen broekstra , michael erdmann and ian horrocks .",
    "the semantic web : the roles of xml and rdf , _ ieee internet computing _ , pages 63 - 73 , 2000 .",
    "kanishka bhaduri , member , ieee , mark d stefanski and ashok n srivastava , senior member , ieee .",
    "privacy preserving outlier detection through random nonlinear data distortion , _ ieee transactions on systems , man and cybernetics _ , 41(1):260 - 272 , 2011 .",
    "benjamin c. m. fung , member , ieee , thomas trojer , patrick c. k. hung , member , ieee , li xiong , khalil al - hussaeni and rachida dssouli .",
    "service - oriented architecture for high - dimensional private data mashup , _ in ieee transactions on services computing _",
    ", 5(3):373 - 386 , 2012 .",
    "jung yeon hwang , sokjoon lee , byung - ho chung , hyun sook cho and daehun nyang .",
    "short group signatures with controllable linkability , _ workshop on lightweight security and privacy : devices , protocols and applications _ , 44 - 52 , march 2011 . zhu yu - quan , tang yang chen geng",
    ". a privacy preserving algorithm for mining distributed association rules , _ in international conference on computer and management ( caman ) _ , 1 - 4 , may 2011 .",
    "alberto trombetta , wei jiang , elisa bertino and lorenzo bossi .",
    "privacy - preserving updates to anonymous and confidential databases , _ in ieee transactions on dependable and secure computing _ , 8(4):578 - 587 , july - august 2011 .",
    "+          his bachelor of engineering from siddaganga institute of technology , tumkur .",
    "bangalore university , bangalore .",
    "he is presently pursuing his ph.d programme in the area of privacy management in databases in bangalore university .",
    "his research interest is in the area of data mining , web mining and semantic web .",
    "+   +      received his master s degree from the department computer science and engineering , university visvesvaraya college of engineering , bangalore university , bangalore .",
    "his research interest is in the area of web technology , se- +        is currently the chairman , department of computer science and engineering , university visvesvaraya college of engineering , bangalore university , bangalore .",
    "she obtained her bachelor of engineering and masters degree in computer science and engineering from   +   +    university visvesvaraya college of engineering .",
    "she was awarded ph.d . in computer science from dr .",
    "mgr university , chennai .",
    "her research interests are in the field of wireless sensor networks and data mining .",
    "+   +      is currently the principal , university visvesvaraya college of engineering , bangalore university , bangalore .",
    "he obtained his bachelor of engineering from university visvesvaraya college of engineering .",
    "he received his masters degree in computer science and   +   +    automation from indian institute of science bangalore .",
    "he was awarded ph.d in economics from bangalore university and ph.d in computer science from indian institute of technology , madras .",
    "he has a distinguished academic career and has degrees in electronics , economics , law , business finance , public relations , communications , industrial relations , computer science and journalism .",
    "he has authored 39 books on computer science and economics , which include petrodollar and the world economy , c aptitude , mastering c , microprocessor programming , mastering c++ and digital circuits and systems @xmath44 . during his three decades of service at uvce",
    "he has over 350 research papers to his credit .",
    "his research interests include computer networks , wireless sensor networks , parallel and distributed systems , digital signal processing and data mining .",
    "+   +     is currently honorary professor , indian institute of science , bangalore , india .",
    "he was a vice chancellor , defense institute of advanced technology , pune , india and was a professor since 1986 with the department of computer science and automation , indian +   +    institute of science , bangalore . during the past 35 years of his service at the institute he has over 700 research publications in refereed international journals and conference proceedings .",
    "he is a fellow of all the four leading science and engineering academies in india ; fellow of the ieee and the academy of science for the developing world .",
    "he has received twenty national and international awards ; notable among them is the ieee technical achievement award for his significant contributions to high performance computing and soft computing .",
    "his areas of research interest have been parallel and distributed computing , mobile computing , cad for vlsi circuits , soft computing and computational neuroscience ."
  ],
  "abstract_text": [
    "<S> privacy preservation emphasize on authorization of data , which signifies that data should be accessed only by authorized users . ensuring the privacy of data is considered as one of the challenging task in data management . </S>",
    "<S> the generalization of data with varying concept hierarchies seems to be interesting solution . </S>",
    "<S> this paper proposes two stage prediction processes on privacy preserved data . </S>",
    "<S> the privacy is preserved using generalization and betraying other communicating parties by disguising generalized data which adds another level of privacy . </S>",
    "<S> the generalization with betraying is performed in first stage to define the knowledge or hypothesis and which is further optimized using gradient descent method in second stage prediction for accurate prediction of data . the experiment carried with both batch and stochastic gradient methods and it is shown that bulk operation performed by batch takes long time and more iterations than stochastic to give more accurate solution . +   + * keywords :* batch gradient , gradient descent , rdf and ontology , stochastic gradient . </S>"
  ]
}