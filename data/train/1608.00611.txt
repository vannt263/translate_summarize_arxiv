{
  "article_text": [
    "`` attention '' mechanisms are a critical component to brain s cognitive performance .",
    "such mechanisms enable the brain to process overwhelming visual stimuli with limited capacity by selectively enhancing the information relevant to one s current behaviour @xcite . with the massive growth of digital image data due to social media , surveillance camera , among others ,",
    "there is a growing demand for computing platforms to perform cognitive tasks .",
    "most of these computing platforms have limited resources in terms of processing power and battery life .",
    "hence , researchers have been strongly motivated to design efficient large - scale image recognition methods to enable resource constrained iot ( internet of things ) devices with cognitive intelligence @xcite .",
    "several brain - inspired computing models including support vector machines ( svm ) @xcite , random forest @xcite , and adaboost @xcite have proven to be very successful for image recognition . however , these classifiers do not scale well with increasing number of image categories . deep learning networks like convnets @xcite have achieved state - of - the - art accuracies , even surpassing human performance @xcite for imagenet dataset . however , they have been criticized for their enormous training cost and computational complexity .",
    "similarly , the one - versus - all linear svm , one of the most popular classifiers for large - scale classification , is computationally inefficient as its complexity increases linearly with the number of categories .",
    "while these classifiers are modeled to mimic the brain - like cognitive abilities , they lack the remarkable energy - efficient processing capability of the brain .",
    "the brain carries out enormously diverse and complex information processing to deal with a constantly varying world at a power budget of about  12 - 20 w @xcite . seeking to attain the brain s efficiency , we draw inspiration from its underlying processing mechanisms to design a multi - class classification method that is both accurate and computationally efficient .    one such mechanism known as `` saliency based selective attention '' shown in fig",
    ". 1 ( left ) simplifies complex visual tasks into characteristic features and then selectively activates particular areas of the brain based on the feature information in the input @xcite . when presented with new visual images , the brain associates the already learnt features to the visual appearance of the new object types to perform recognition @xcite .",
    "this facilitates the brain to learn a host of new information with limited capacity and also speeds up the recognition process .",
    "interestingly , we note that there is significant similarity among underlying characteristic features ( like color or texture ) of images across multiple objects in real world applications .",
    "this presents us with an opportunity to build an efficient visual recognition system incorporating inter - class feature similarities and relationships .    in this work ,",
    "we propose a computationally efficient multi - class classification method : attention tree ( atree ) that exploits the feature similarity among multiple classes in the dataset to build a hierarchical tree structure composed of binary classifiers .",
    "the resultant atree learns a hierarchy of features that transition from general to specific as we go deeper into the tree in a top - down manner .",
    "this is similar to the state - of - the - art deep learning convolutional networks ( dlns ) where the convolutional layers exhibit a generic - to - specific transition in the learnt features @xcite . in case of dlns",
    ", the entire network is utilized for the recognition of a particular test input .",
    "in contrast , the construction of the attention tree incorporates effective and active pruning of the dataset during training of the individual tree nodes resulting in an efficient instance - specific classification path .",
    "in addition , as we will see in later sections , our attention model captures both inter and intra class feature similarity to build a tree hierarchy with decision paths of varying lengths even for the same class .",
    "this provides substantial benefits in test speed and computational efficiency for large - scale problems while maintaining competitive classification accuracy .",
    "1 ( right ) shows a toy example of an atree based on real - world broad semantic categories for different object classes .",
    "for example , to recognise a car , it is not sensible to learn all the specific appearance details .",
    "instead , first we learn the general vehicle - type features ( wheels , shape etc ) and then learn more discriminative details ( brand symbol ) .",
    "thus , we learn a hierarchy of features generalizing over object instances like : wheeled vehicle@xmath0motor vehicles@xmath0cars@xmath0bmw .",
    "if presented with new motorbike object types , the attention hierarchy now associates this new category of objects to the already learnt `` wheeled vehicle '' features and then learns more discriminative details corresponding to the motorbike types .",
    "each node of the atree is then associated with different features based on inter - class relationships .",
    "it is evident from fig .",
    "1 that the attention tree method bears resemblance to the selective attention mechanism of the brain ( fig . 1 left ) by exploiting feature similarity and the implicit relationships among different visual data to learn a meaningful hierarchy for recognition .",
    "while decision tree @xcite , ensemble methods @xcite and a class of other boosting @xcite techniques have been proposed for lowering the testing complexity of machine - learning problems , they suffer from major limitations : a ) in ensemble learning , a set of weak learners are combined into a complex classifier with high accuracy .",
    "the number of weak classifiers can be in the order of hundreds to get a reasonable performance for large - scale problems .",
    "thus , ensemble methods become computationally expensive for larger datasets .",
    "b ) most existing models deviate from the biological attention based visual processing in the human brain and perform one - against - rest classification . in this case , the learning algorithm fails to maintain a general - to - specific feature hierarchy that turns out to be ineffective as well as computationally inefficient .    a class of work on  one - versus - all \" and  one - versus one \" methods have been explored to convert a multi - class problem into multiple binary classification problems . in such models , classes are not organized in a hierarchical tree .",
    "also , these methods do not incorporate class relationships or feature similarities .",
    "an extension of these methods include _ error correcting output codes _ @xcite that utilize feature sharing to build more generalized and robust classifiers . as discussed earlier , these methods yield good classification accuracy . however , the time complexity is linearly proportional to the number of classes that does not scale well to larger datasets .",
    "@xcite propose different ways to construct a hierarchical classification tree .",
    "however , most of these methods rely on a greedy prediction algorithm for class prediction through a single path of the tree .",
    "while these algorithms achieve sublinear complexity , the accuracy is typically sacrificed as errors made at higher nodes of the hierarchy can not be corrected later .",
    "researchers have also looked at developing efficient and effective feature representations for large - scale classification problems @xcite.@xcite learn discriminative features using deep convolutional networks to achieve state - of - the - art accuracy .",
    "please note that our proposed atree is orthogonal to such models since our method can use various feature respresentations to explore the accuracy vs. efficiency tradeoff .",
    "hence , we do not optimize over different features in this work , rather compare the efficiency benefits of our approach with existing hierarchical methods .",
    "while our proposed atree model draws inspiration from other tree - based methods such as @xcite , we have different focus , design and evaluation strategies .",
    "as mentioned , most of these methods use a greedy prediction algorithm to achieve a good tradeoff between efficiency and complexity .",
    "the novelty of our work is that we use the recursive adaboost training @xcite as a unified and principled optimization procedure to determine data partitioning ( or learning attention hierarchy ) based on feature similarity .",
    "this in turn enables the binary svm to construct a maximum - margin hyperplane for optimal decision boundary modeling ( with lower generalization error ) leading to better performance .",
    "in addition , organizing the binary classifiers in a hierarchical tree structure on top of the attention hierarchy further reduces complexity .",
    "we use a variant of the boosted tree algorithm @xcite that combines adaboost with a svm based decision tree to construct the atree . the proposed attention based classification framework resolves the problems associated with standard decision tree methods as discussed above . for a simple two - class ( yes / no ) problem , the training stage for atree consists of two phases : a ) first",
    ", we construct the visual feature hierarchy in atree using the adaboost training algorithm recursively , wherein each tree node is a complex classifier that works on an optimal feature at that tree level for partitioning the inputs .",
    "the partitioned input data obtained at a particular node are then used to train the left and the right sub - trees .",
    "thus , the training data for the subsequent nodes of the tree are continuously pruned during the construction of atree leading to computationally efficient training .",
    "the recursive boosting procedure intrinsically embeds clustering in the learning stage with similar feature clusters created in an automatic and hierarchical fashion .",
    "b ) with the feature hierarchy and the resultant pruned data space fixed for each node / branch of the tree in the first phase , we train a standard binary svm on the right and the left partitioned subsets of input data at each tree node .",
    "we further extend the two - class attention model described above to multi - class problems .",
    "we use the minimum entropy measure to select a feature that can be used to categorize the multiple category of objects into two broad classes .",
    "then , the training algorithm for a simple two - class atree is used to design the attention hierarchy .",
    "again , clusters of multiple classes are automatically formed . at test time",
    ", only those branches and nodes depending upon the output of the binary classifier are activated that are relevant to the input .",
    "hence , our approach is both time and energy efficient since it involves instance - specific selective activation of nodes .",
    "next , we briefly discuss the adaboost learning framework and the shortcomings associated with it .",
    "we explain the intuition behind modifying the standard adaboost training procedure which can then be effectively used to construct the feature - based hierarchy or atree .",
    "the adaboost algorithm combines a set of simple or weak classifiers ( @xmath1 ) @xcite to form the final classifier ( @xmath2 ) given by @xmath3 .",
    "the output of the final or strong classifier is @xmath4 .",
    "the weak classifiers can be thought of as feature or basis vectors . given a set of training samples",
    ", adaboost maintains a probability distribution , @xmath5 ( uniform in the first iteration ) , of the samples .",
    "then , adaboost calls a weaklearn algorithm @xcite that trains the weak learner or classifier ( @xmath6 ) on the weighted sample in a series of iterations \\{@xmath7}. the distribution @xmath5 is updated in each iteration to minimize the overall error ( @xmath8 ) .",
    "finally , adaboost uses a weighted linear combination of the weak learners ( or features ) to obtain the final output @xmath9 .",
    "the adaboost and weaklearn algorithm have been explained in detail in @xcite .    for each sample @xmath10 with weight @xmath11 , the error rate ( @xmath12 ) is given by @xmath13 eqn .",
    "1 shows the maximum value of the error . for large - scale problems ,",
    "when @xmath10 tends to be complex , @xmath14 saturates at @xmath15 after few iterations and thus the adaboost algorithm fails to reach a global error minima . a possible solution for avoiding this is to design better weak classifiers that can effectively separate the classes .",
    "however , this would further increase the computational complexity for computing these classifiers ( or features ) .",
    "one of the key principles of adaboost is that `` easy '' samples that are correctly classified by the weak classifiers get low weights while those misclassified ( `` hard '' samples ) get higher weights .",
    "the weight distribution @xmath5 captures all the information about selected `` features '' in a given iteration .",
    "however , due to the weight update rule and normalization of w in each iteration , the information about previously selected features might be lost .",
    "this will result in misclassification of correctly classified ( `` easy '' ) samples from earlier iterations in the present epoch .",
    "thus , the algorithm does not maintain a generic - to - specific transition while learning the weak classifiers ( or features ) that proves to be ineffective after a few iterations . to address this",
    ", we build an attention tree of strong classifiers , instead of constructing a single strong classifier from a linear combination of weak learners .",
    "the tree utilizes the features learnt from the previous nodes to construct the subsequent nodes .",
    "as we traverse down the tree , the classifiers learn more specific features that are useful for classifying the `` hard '' inputs correctly while preserving the feature information learnt at the early nodes for `` easy '' input samples .",
    "the central idea of the attention tree algorithm is to use feature - based attention to optimize the search procedure inherent in a vision problem .",
    "this model of attention addresses the reduction of number of candidate image subsets and feature subsets that are required for object recognition by selectively tuning the visual processing hierarchy .",
    "the theory described here is most closely related to the neuroscience works of @xcite that present the neurobiological concepts of primate visual attention .",
    "algorithm 1 shows the procedure for training the attention tree for a two - class problem . in",
    "phase i _ , a tree is recursively trained .",
    "it learns and preserves a hierarchy of features essential for understanding the underlying image representations and for efficient classification . at each node",
    ", a classifier is learnt using the adaboost algorithm described in @xcite that identifies the most optimum feature to separate the training inputs at a particular node into the corresponding sub - branches .",
    "it is shown in @xcite that adaboost is essentially approximating a logistic regression . for convenience in notation , we denote the output computed by each classifier at the tree node as @xmath16    * _ phase i : learning visual feature hierarchy _ * * input : * training dataset d=@xmath17 ; @xmath18 , @xmath19 + * output : * atree with feature hierarchy of depth l    * initialize * @xmath20=1 * while * ( @xmath21 ) using adaboost , train a strong classifier on d combining t weak classifiers .",
    "calculate training error @xmath22 , @xmath23 .",
    "exit adaboost if @xmath24 ( user - defined , @xmath25=0.48 in our experiments ) . compute the probability distribution @xmath26 * initialize * @xmath27 , @xmath28 = \\ { } * for * @xmath29 //_n= # of samples _",
    "compute @xmath30 and @xmath31 using eqn 2 and 3 for the strong classifier learnt in step 3 . *",
    "if * @xmath32 * then * @xmath33 , assign @xmath34 * elseif * @xmath35 * then * @xmath36 , assign @xmath37 * else * @xmath38 and @xmath39 , assign @xmath40 * end if * * end for * @xmath41 normalize weights in @xmath27 subset and goto step 2 .",
    "normalize weights in @xmath28 subset and goto step 2 .",
    "//_recursively repeat until @xmath20 is reached _ * end while *    - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - * _ phase ii : svm optimization _ * * input : * atree with feature hierarchy of depth l , training samples ( @xmath27 or @xmath28 ) for each node of the atree=@xmath42 + * output : * svm based atree    * initialize * @xmath20=1 * while * ( @xmath21 ) train a binary svm at each tree node using @xmath27 and @xmath28 at that node ignoring all samples with @xmath40 with standard regularized hinge loss minimization @xcite .",
    "@xmath41 * end while *    depending upon the probabilities computed by the classifier node , the training set ( @xmath43 ) is divided as @xmath27 and @xmath28 that are then passed to the sub - branches for training the following nodes of the tree .",
    "as the tree expands , only a subset of the input samples are passed to the subsequent nodes .",
    "thus , the final nodes or leaves of the tree will consist of input samples belonging to one particular class .",
    "please refer to fig . 2 for an overview of the tree structure and input sub - sampling obtained with the attention model .",
    "later , in section iv(d ) , we give a detailed explanation about the input sub - sampling and the hierarchical feature learning achieved with our attention model .    in _",
    "phase ii _ of _ algorithm 1 _ , a binary svm ( with any suitable kernel ) is trained at each node of the tree using the @xmath28 and @xmath27 training sub - samples obtained from _",
    "phase i_. the training labels ( + for @xmath28 , - for @xmath27 and * for instances that are passed to both @xmath27/@xmath28 ) are assigned to each input @xmath10 in the corresponding subsets for training the binary svm .",
    "as the training set size decreases ( owing to the input partitioning ) at the successive nodes as we traverse down the tree , the complexity of the problem and hence that of the svm also reduces .",
    "this in turn enables better decision boundary modeling with low computational complexity in the subsequent nodes ( svms ) for improved classification performance .",
    "adding svms ( _ phase ii _ of algorithm 1 ) at the nodes on top of the learnt feature hierarchy ( _ phase i _ of algorithm 1 ) enables the attention tree model to achieve state - of - the - art accuracies on challenging benchmark databases with significantly lower cost .",
    "the threshold value , @xmath44 in _ phase i of algorithm 1 _ , determines the fraction of training samples separated as positive ( + ) and negative ( - ) subsets . if @xmath44=1 , then all training samples are passed to both branches ( or sub - trees ) of a tree node .",
    "the weights for both sub - trees are re - computed based on the node classifier s output . in that case",
    ", the tree based adaboost training converges to a standard boosting algorithm wherein the feature hierarchy ( general - to - specific ) is not learnt . for all our experiments discussed in section",
    "v , we set the @xmath44 value to be @xmath45 . for @xmath46 , easy inputs that can be correctly classified with general features at the top nodes will be unnecessarily passed down to bottom nodes for classification .",
    "this will result in computational inefficiency , defeating the purpose of the attention tree .",
    "if @xmath47 , then , each training sample is either passed to the right or left sub - tree which leads to a _ constrained _ partition . in this case , the hard or confusing classes will be assigned to one of the sub - trees causing overfitting of data in the subsequent nodes .",
    "this will lead to a decline in accuracy .",
    "however , the test complexity will be low since the length of the tree will be short leading to a quicker decision at the cost of degraded performance .",
    "those samples whose output probability lies in the range @xmath48 $ ] when @xmath49 can be considered as hard or confusing ones . for @xmath49 ,",
    "the hard samples are passed to both the left and the right sub - trees for training ( * ) .",
    "the hard or confusing inputs / classes are ignored while training the svm at the corresponding node in _",
    "phase ii_. this is adopted from the _ relaxed hierarchy _ structure in @xcite .",
    "this is done to enhance the accuracy of the attention tree .",
    "it is understood that the decision boundary becomes progressively non - linear to model the hard or confusing classes in a dataset as we traverse down the atree .",
    "the hard or confusing instances are ignored and passed to the bottom nodes that construct better decision boundary models , thereby , decreasing the overall error . in case",
    "the hard classes are not passed to bottom nodes , the svms at the top will construct overfitted models for the complex data instances , thereby , decreasing the accuracy considerably . in section",
    "v , we vary the threshold @xmath44 to build _ constrained _ and _ relaxed _ hierarchical attention models and analyze the tradeoff between computational efficiency and accuracy for both approaches .      to conserve the feature transition in the attention model , we propose a simple method for extending the two - class training model into a multi - class one .",
    "traditionally , boosting algorithms use multi - class weak learners to construct a multi - class final strong classifier @xcite .",
    "however , for large number of classes , constructing reasonably accurate multi - class weak learners turns out to be highly computationally expensive .",
    "as seen earlier , we observe a feature similarity across classes that can be used to decompose a multi - class problem into a hierarchy of two - class problems .",
    "* input : * training dataset d=@xmath50 @xmath51 , @xmath19 + * output : * svm based atree of depth l + for training a tree of maximum depth l ,    compute the probability distribution @xmath26 for each feature @xmath52 at value @xmath53 , compute histogram @xmath54 for @xmath55 and @xmath56 for @xmath57 . find optimal @xmath52 and @xmath53 that have minimum entropy @xmath58 .",
    "* if * @xmath59 * then * assign @xmath60 * else * assign @xmath61 * end if * * initialize * training set d=@xmath62 ; @xmath63 //_now the multi - class is reduced to a 2-class problem _ call algorithm 1 .",
    "algorithm 2 shows the procedure for training a multi - class attention tree .",
    "the algorithm first finds the optimum feature across multiple classes that separates the input patterns into 2-classes and then uses the 2-class training procedure ( _ phase i _ of algorithm 1 ) to learn the subsequent classifier nodes of the tree . in our experiments , we observed that the feature chosen for transforming the multi - class to 2-class problem is often the feature selected by algorithm 1 to construct the top node of the atree .",
    "intuitively , after the first selection , the features selected at the subsequent nodes help in making a stronger and more accurate decision.thus , similar objects ( with similar features ) of different classes are clustered together in the initial nodes of the hierarchy .",
    "as the tree expands , these classes are gradually set apart .",
    "the tree is terminated when the algorithm does not find any common feature to partition the inputs ( at the leaves of the tree ) .",
    "thus , each leaf of the tree corresponds to a particular class .",
    "after the attention hierarchy is learned , _ phase ii _ of algorithm 1 is invoked to train svms at each node ( excluding the leaves ) of the hierarchy .      the attention tree composed of svm nodes",
    "is then used for testing . those instances ( easy )",
    "that can be easily distinguished with general features are identified with svms at the top nodes .",
    "the svms at the bottom nodes perform more accurate classification on the hard instances in the dataset .",
    "when an input instance is presented at the root node , the branch with higher output probability at the svm node is activated .",
    "based on the path activated by the output of svm nodes , the instance then traverses the attention hierarchy until a leaf node where a final decision ( or class assignment ) is made .",
    "note that a subset of classes are eliminated at each tree node as the tree is traversed .",
    "the attention based hierarchy , thus , scales sub - linearly _",
    "o(log(n ) ) _ with respect to the number of classes . in the current era of `` data deluge '' that presents vision problems with a hefty task of recognizing from hundreds or thousands of classes",
    ", the sub - linearly growing attention tree model can be very useful .",
    "the training algorithm naturally divides the samples into left and right sub - groups based on the configuration of features .",
    "2 shows an example of how the attention tree learns and divides the samples on a synthesized dataset of 3000 points .",
    "the dataset consists of inputs belonging to two classes ( denoted as orange and blue ) .",
    "the samples that are clustered together can be termed as hard inputs .",
    "such samples are passed down the sub - branches of the tree forming the successive nodes .",
    "the top node of the tree partitions the inputs into two subsets .",
    "this division is intuitive as the right set of orange points are distant from the remaining inputs that are clustered together .",
    "the tree then expands on the hard inputs where the two sets are clustered together .",
    "if these data points are assumed to be features ( like texture or color components ) corresponding to two image classes , it is clearly seen that the hierarchy formed is coherent with the basic generic - to - specific feature transition theory of the attention model .",
    "consider an example of recognizing a red ferrari ( blue points ) from a sample set of vehicle images consisting of motorbikes and cars ( orange points ) .",
    "the first intuitive step is to recognize all red vehicles in the sample and then look for a ferrari shaped object from the sub - sample of remaining red images .",
    "the attention tree tries to model this intuitive behavior by learning the feature hierarchy . at the first level",
    ", the tree uses the red feature to distinguish the non - red vehicles from the red vehicles .",
    "as we go down , the tree uses more specific features ( like ferrari shapes or wheel textures ) to perform more accurate classification .",
    "our attention model automatically learns this feature hierarchy without any need to pre - specify the feature clusters .",
    "the pruning of the input data as we traverse down the attention model reduces the complexity of the original multi - class problem .",
    "this in turn enables better decision boundary modelling at the bottom svm nodes of the atree as compared to the top node resulting in improved classification performance .",
    "a noteworthy observation here is that the attention model comprises of multiple decision paths of different lengths . in fig .",
    "2 , the tree consists of leaf nodes ( for orange data points ) at every level . for a given input , the decision can be reached at an earlier leaf node yielding a more optimal speedup during testing . referring to the ferrari example , all non - red images can be classified at the first level without traversing the whole tree .",
    "this imbalanced decision tree structure is what separates our model from other decision tree methods where one has to traverse the entire tree to reach a decision @xcite . even within a particular class ,",
    "all inputs are not equal .",
    "for example , recognizing a person standing against a plain background is much easier ( less time and effort ) than when he / she is in the midst of a crowd . ideally , algorithms should spend effort proportional to the difficulty of the inputs irrespective of whether they belong to the same class or not @xcite .",
    "most existing works @xcite focus on optimizing the computational complexity based on inter - class feature variability .",
    "in contrast , our imbalanced method captures both inter and intra class feature variability while expanding the attention tree thus yielding more computational benefits .",
    "previously , we discussed that the threshold , @xmath44 , serves as a useful control parameter to construct either relaxed or constrained models of the attention tree .",
    "3 demonstrates the sample relaxed / constrained hierarchy for a 4-class problem .",
    "the instances from class c are the hard inputs in the dataset . in the constrained hierarchy",
    ", it is clearly seen that instances from c are forced to the left sub - node . in this case",
    ", it is very likely that the svm at the root node will misclassify a test instance from class c due to overfitting .",
    "however , the decision path for recognizing class d is short .",
    "so , we will observe an improvement in efficiency ( or test speed ) at the cost of accuracy .",
    "with relaxed hierarchy , we see that there is an extra svm classifier evaluation required to recognize class d that increases the computational cost",
    ". however , the accuracy in this case will be better as the addition of an extra classifier node ( node cd in fig .",
    "3(a ) ) minimizes overfitting for complex distribution of data .",
    "in addition , the relaxed hierarchy captures the intra - class feature variability for class c which is not seen in the constrained model . in the relaxed model",
    ", instances of class c that are relatively easy can be classified at the 2nd level and those that are hard are only passed to the 3rd level for accurate classification . in contrast , with constrained model all instances of class c are passed to the 3rd level for classification . from this sample demonstration , it is clear that @xmath44 can be modulated to control the accuracy and efficiency of the atree .",
    "since the learnt tree with svm nodes is used for measuring the complexity and accuracy of the attention model , the kernel - type selection ( non - linear or linear ) plays a key role in determining the overall computational efficiency / performance for a given multi - class problem . in case of svms with linear kernels ,",
    "the complexity of each classifier node is same , so the overall test complexity is proportional to the number of classifiers evaluated to reach a decision .",
    "however , in case of non - linear kernels , the complexity of each classifier is proportional to the number of its support vectors .",
    "so , we use a computational model as devised in @xcite to optimize the number of support vectors for maximizing the computational benefits .",
    "it is clear that the training algorithm for multi - class attention tree does not always result in a balanced partitioning of classes ( into left and right sub - trees ) at a particular node as observed in fig .",
    "3 . given a svm classifier @xmath64 ,",
    "let @xmath65 be the number of support vectors of the classifier @xmath64 .",
    "we define a cost function ( @xmath66 ) that reflects the average efficiency of the svm ( @xmath64 ) as : @xmath67 where @xmath68 and @xmath69 are the number of classes assigned to positive ( right sub - tree ) and negative ( left sub - tree ) labels respectively .",
    "@xmath70 is the fraction of negative classes and similarly @xmath71 is the fraction of positive classes . after the attention hierarchy is learned ( _ phase i _ of _ algorithm1 _ ) , we can estimate @xmath68 and @xmath69 . in an ideal case , for instances from class @xmath69 , classes",
    "@xmath68 are pruned after evaluating @xmath64 with cost proportional to @xmath72 number of kernel evaluations .",
    "so , the average cost for discarding a particular class is @xmath73 .",
    "similarly , the average cost for eliminating a class for instances belonging to positive subset ( @xmath68 ) is @xmath74 .",
    "given the proportion of positive @xmath71 and negative @xmath75 classes , the average cost for eliminating one class by @xmath64 is given by eqn .",
    ", we select the number of support vectors , @xmath65 , that minimizes the overall cost function @xmath66 while yielding competitive accuracy .",
    "in this section , we evaluate our proposed framework on two fundamental computer vision tasks : object recognition and scene categorization for the benchmark datasets , caltech-256 @xcite and sun @xcite .",
    "we use the evaluation metrics : classification accuracy and test speed ( or test complexity ) to discuss the benefits of our approach . for classification accuracy ,",
    "we use the mean of per - class accuracy that is reported as a standard way for estimating multi - class classification performance . for test speed",
    ", we distinguish two cases based on the kernel type selection for the svm classifiers at the atree nodes .",
    "the first case corresponds to linear classifiers , where the overall test complexity is proportional to the number of evaluated classifiers .",
    "so , for a linear kernel svm , we report the mean of the number of classifier evaluations for all test instances .",
    "the second case corresponds to nonlinear kernel svms . as mentioned earlier , the complexity of each classifier is now proportional to the number of support vectors .",
    "specifically , let @xmath64 be the number of classifiers to be evaluated , where each classifier has a set of support vectors @xmath76 where m and i denote the classifier and its support vectors respectively . if @xmath64 classifiers are evaluated independently without caching any kernel computations , then , the number of kernel computations for a single test instance is given by @xmath77 .",
    "this method proves to be very inefficient when the number of classifiers are large .",
    "an efficient approach would be caching the kernel computations from different classifiers and reusing them whenever possible .",
    "then , the number of kernel computations reduces to @xmath78 .",
    "we use the latter approach to report test speed when non - linear kernels are used .",
    "we compare our method to various existing approaches : gao @xcite , one - vs - all , one - vs - one , dagsvm , tree - based hierarchy @xcite and marszalek @xcite .",
    "the regularization parameter @xmath79 of svm is chosen by cross validation on the training set .      with 256 categories and at least 80 images per class ,",
    "this is a standard muti - class object recognition dataset .",
    "we randomly sampled 80 images for each class , and used half ( 40 per class ) for training and remaining half for testing . for features",
    ", we used the standard spatial histograms of visual words based on dense sift @xcite . like @xcite",
    ", we used the extended gaussian kernel based on @xmath80 distance .",
    "however , since linear kernel of histogram based features gives poor accuracy , we used explicit feature transformation from @xcite to approximate implicit feature mapping of @xmath80 kernel .",
    "the linear svm is applied on the transformed feature .",
    "we varied computational parameters for tree @xcite ( 2 to 5 levels ) , marszalek @xcite ( @xmath81 ) , gao @xcite ( @xmath820.5 to 0.8 with step size of 0.1 } ) and our method atree ( @xmath830.5 to 0.9 in steps of 0.1 } ) to obtain a tradeoff between accuracy and speed . here ,",
    "@xmath84 and @xmath85 are the computational parameters defined in @xcite and @xcite respectively that are varied to achieve the complexity vs. accuracy tradeoff .",
    "4 shows the results .",
    "it is clearly seen that atree performs better ( faster at same accuracy and more accurate at the same relative complexity ( rc ) ) for both linear and non - linear kernels .",
    "for instance , in case of linear kernel , atree achieves one of the best accuracy ( @xmath8637.3% ) with around 27% of the complexity of one - vs - all with a _ relaxed hierarchical _ model ( where @xmath49 ) while achieving a speedup of 3.7x .",
    "also , for @xmath47 , when the atree is modelled as a _ constrained hierarchy _ , it achieves a higher speed up of 5.5x for @xmath862.5% accuracy degradation with respect to one - vs - all .",
    "however , to achieve a similar 5x speed up other methods : gao @xcite , marszalek @xcite , tree @xcite have to suffer 3.2% , 8% , 10% accuracy degradation .",
    "please note that atree achieves consistently better accuracy performance than the best result reported in @xcite for both linear and non - linear kernels .",
    "now , we evaluate our atree model for scene classification on the sun dataset .",
    "the sun dataset captures a full variety of 899 scene categories .",
    "we used 397 well - sampled categories as @xcite . for each class ,",
    "50 images are used for training and the other for test . for image representation , we used spatial hog pyramid @xcite with histogram intersection kernel ( non - linear svm ) and transformed spatial histogram oriented gradient ( hog ) pyramid ( explicit feature transformation from @xcite to approximate the implicit feature mapping of histogram intersection kernel ) with linear kernel ( linear svm ) . as with caltech-256 , we varied the tradeoff between accuracy and speed for tree @xcite ( 2 to 5 levels ) , marszalek @xcite ( @xmath87 ) , gao @xcite ( @xmath820.6 to 0.9 with step size of 0.1 } ) and our method atree ( @xmath830.5 to 0.9 with step size of 0.05 } ) .",
    "5 shows the results .",
    "the performance improvement for both linear / non - linear kernels is similar and consistent with the results of caltech-256 .",
    "for instance , for hog with histogram intersection kernel , our method has a significantly improved accuracy of 24.4% with @xmath8621% complexity compared to one - vs - all ( for @xmath44 @xmath860.65 implying a _ relaxed hierarchy _ ) .",
    "however , marszalek and gao can only reduce the relative complexity to 49% and 64% respectively to attain similar accuracy as one - vs - all .",
    "the performance of the atree further improves if @xmath44 is increased and the highest accuracy observed is 25.2% ( at 26% complexity ) that is @xmath861.7% higher than the best result reported in @xcite . as for the test speed , while our method achieves a maximum speed up of 4.8x compared to one - vs - all even with an improved accuracy , other methods never meet this speedup irrespective of the accuracy .",
    "in addition , with linear kernel , atree achieves a slightly improved accuracy with respect to one - vs - all and dagsvm while being 7.2x faster .",
    "in fact , for a 1.8% decline in accuracy , atree ( with _ constrained hierarchy _ for @xmath47 ) is 19x faster than one - vs - all .",
    "however , for gao @xcite / marszalek @xcite , the accuracy degradation is higher upto 2.5%/6.8% to achieve similar speed up .",
    "the above results validate that atree is more effective to reduce the rc while maintaining a competitive accuracy in comparison to other hierarchical tree - based implementations .    from fig . 5 ( b )",
    ", it is worth noting that if non - linear kernels are used , a lower depth tree does not necessarily lead to lower computational complexity .",
    "when @xmath84 is large for @xcite or @xmath44 is closer to 0.5 ( @xmath88 ) , the depth of the tree is low on account of constrained partitioning of inputs into left and right sub - trees .",
    "ideally , we should get an accuracy decline with a lower complexity for such cases as the number of classifier evaluations will be less . however , we observe that both accuracy and complexity are worse .",
    "the reason is that , although a fewer number of classifier evaluations are required in these cases , each svm involves a large number of support vectors ( since constrained partition forces the svms to perform complex boundary modelling ) which increases the overall complexity .",
    "besides performance comparison , we also studied how the complexity of atree changes with the increase in the number of classes .",
    "we sampled 100 , 200 and 300 with the original 397 classes from sun dataset , and for each case we learn the model with spatial hog using linear kernel . for fair comparison",
    ", we set @xmath89 for our method , @xmath90 for @xcite and @xmath91 for @xcite to match the same level of accuracy as one - vs - all .",
    "as seen in fig .",
    "6 , the complexity of our method grows sublinearly as compared to @xcite .",
    "as discussed earlier , atree model gives rise to an imbalanced tree that can have leaf nodes even at the beginning of the attention hierarchy .",
    "thus , we observe that our method grows at a slightly lesser rate than that of @xcite .",
    "atree builds a feature hierarchy in the label space automatically .",
    "7 shows the attention tree formed for a subset of some sampled images from the caltech-256 dataset .",
    "we observe that the images that have similar features are clustered together in an initial node and are gradually set apart as the tree is traversed .",
    "conforming to the imbalanced attention model , we observe that for certain classes : zebra , car tire , the classification is done at earlier nodes while more confusing classes are passed down .",
    "in addition , we also observe intra - class variability for the camel class in which certain instances are evaluated earlier than others .            in fig .",
    "8 , we present a sampling of the first three levels of the atree constructed for the entire caltech-256 and sun dataset showing how the different classes are assigned \\{+,- , * ( algorithm 1 ) } and separated into left and right sub - trees . in the caltech-256 atree hierarchy , we observe that the assignment of classes into sub - nodes in many cases correlates to human vision i.e. images from different classes that are assigned to the same sub - tree look similar to humans .",
    "for the sun atree hierarchy , the partitioning of classes in the first two levels correlates with human - defined concepts .",
    "e.g. , natural outdoor scenes vs. indoor man - made scenes .",
    "also , the hierarchy starts partitioning classes with large visual distances and then identifies subtle discrepancies at the bottom nodes which is in coherence with the concepts of visual stimuli decomposition in the human brain .",
    "this suggests the biological plausibility and effectiveness of our attention model for image classification .",
    "we proposed a novel neuro - inspired visual feature learning to construct an efficient and accurate tree - based classifier : attention tree , for large - scale image classification .",
    "our learning algorithm is based on the biological attention mechanism observed in the brain that selects specific features for greater neural representations .",
    "the atree uses a principled optimization procedure ( recursive adaboost training ) to extract knowledge about the relationships between object types and integrates that into the visual appearance learning .",
    "we evaluated our method on both the caltech-256 and sun datasets and obtained significant improvement in accuracy and efficiency .",
    "in fact , atree outperforms the one - vs - all method in accuracy and yields lower computational complexity compared to the state - of - the - art `` tree - based''methods @xcite .",
    "the proposed framework intrinsically embeds clustering in the learning procedure and identifies both inter and intra class variability .",
    "most importantly , our proposed atree learns the hierarchy in a systematic and less greedy way that grows sublinearly with the number of classes and hence proves to be very effective for large - scale classification problems .",
    "it is noteworthy to mention that the current atree framework suffers from overfitting when the training dataset is small .",
    "the overfitting behaviour is checked by modulating the depth of the atree and also adopting the relaxed hierarchy structure where confusing or `` hard '' inputs are passed to both the right and the left sub - nodes .",
    "additionally , tree pruning methods @xcite can be used to control overfitting .",
    "further research can be done to explore the overfitting problem .",
    "this work was supported in part by c - spin , one of the six centers of starnet , a semiconductor research corporation program , sponsored by marco and darpa , by the semiconductor research corporation , the national science foundation , intel corporation and by the national security science and engineering faculty fellowship .",
    "j.  xiao , j.  hays , k.  a. ehinger , a.  oliva , and a.  torralba , `` sun database : large - scale scene recognition from abbey to zoo , '' in _ computer vision and pattern recognition ( cvpr ) , 2010 ieee conference on_.1em plus 0.5em minus 0.4emieee , 2010 , pp . 34853492 .",
    "d.  geebelen , j.  a. suykens , and j.  vandewalle , `` reducing the number of support vectors of svm classifiers using the smoothed separable case approximation , '' _ ieee transactions on neural networks and learning systems _ , vol .",
    "23 , no .  4 , pp . 682688 , 2012 .",
    "k.  he , x.  zhang , s.  ren , and j.  sun , `` delving deep into rectifiers : surpassing human - level performance on imagenet classification , '' in _ proceedings of the ieee international conference on computer vision _ , 2015 , pp .",
    "10261034 .",
    "t.  gao and d.  koller , `` discriminative learning of relaxed hierarchy for large - scale visual recognition , '' in _ 2011 international conference on computer vision_.1em plus 0.5em minus 0.4emieee , 2011 , pp .",
    "20722079 .",
    "m.  sun , w.  huang , and s.  savarese , `` find the best path : an efficient and accurate classifier for image hierarchies , '' in _ proceedings of the ieee international conference on computer vision _",
    ", 2013 , pp . 265272 .",
    "j.  deng , s.  satheesh , a.  c. berg , and f.  li , `` fast and balanced : efficient label tree learning for large scale object recognition , '' in _ advances in neural information processing systems _",
    ", 2011 , pp . 567575 .",
    "p.  wang , c.  shen , n.  barnes , and h.  zheng , `` fast and robust object detection using asymmetric totally corrective boosting , '' _ ieee transactions on neural networks and learning systems _ , vol .",
    "23 , no .  1 , pp . 3346 , 2012 .",
    "s.  paisitkriangkrai , c.  shen , and a.  van  den hengel , `` a scalable stagewise approach to large - margin multiclass loss - based boosting , '' _ ieee transactions on neural networks and learning systems _ , vol .  25 , no .  5 ,",
    "pp . 10021013 , 2014 .",
    "g.  griffin and p.  perona , `` learning and using taxonomies for fast visual categorization , '' in _ computer vision and pattern recognition , 2008 .",
    "cvpr 2008 .",
    "ieee conference on_.1em plus 0.5em minus 0.4emieee , 2008 , pp . 18 .",
    "y.  lin , f.  lv , s.  zhu , m.  yang , t.  cour , k.  yu , l.  cao , and t.  huang , `` large - scale image classification : fast feature extraction and svm training , '' in _ computer vision and pattern recognition ( cvpr ) , 2011 ieee conference on_.1em plus 0.5em minus 0.4emieee , 2011 , pp .",
    "16891696 .",
    "n.  dalal and b.  triggs , `` histograms of oriented gradients for human detection , '' in _ 2005 ieee computer society conference on computer vision and pattern recognition ( cvpr05 ) _ , vol .",
    "1.1em plus 0.5em minus 0.4emieee , 2005 , pp .",
    "886893 .",
    "e.  grossmann , `` adatree : boosting a weak classifier into a decision tree , '' in _ computer vision and pattern recognition workshop , 2004 .",
    "conference on_.1em plus 0.5em minus 0.4emieee , 2004 , pp .",
    "105105 .",
    "z.  tu , `` probabilistic boosting - tree : learning discriminative models for classification , recognition , and clustering , '' in _ tenth ieee international conference on computer vision ( iccv05 ) volume 1 _ , vol .",
    "2.1em plus 0.5em minus 0.4emieee , 2005 , pp .",
    "15891596 .",
    "x.  li , l.  wang , and e.  sung , `` a study of adaboost with svm based weak learners , '' in _ proceedings .",
    "2005 ieee international joint conference on neural networks , 2005 .",
    "_ , vol .",
    "1.1em plus 0.5em minus 0.4emieee , 2005 , pp .",
    "196201 .",
    "j.  friedman , t.  hastie , r.  tibshirani _ et  al .",
    "_ , `` additive logistic regression : a statistical view of boosting ( with discussion and a rejoinder by the authors ) , '' _ the annals of statistics _ , vol .  28 , no .  2 , pp .",
    "337407 , 2000 .",
    "p.  panda , a.  sengupta , and k.  roy , `` conditional deep learning for energy - efficient and enhanced pattern recognition , '' in _ 2016 design , automation & test in europe conference & exhibition ( date)_.1em plus 0.5em minus 0.4emieee , 2016 , pp . 475480 ."
  ],
  "abstract_text": [
    "<S> one of the key challenges in machine learning is to design a computationally efficient multi - class classifier while maintaining the output accuracy and performance . in this paper </S>",
    "<S> , we present a tree - based classifier : attention tree ( atree ) for large - scale image classification that uses recursive adaboost training @xcite to construct a visual attention hierarchy . </S>",
    "<S> the proposed attention model is inspired from the biological `` selective tuning mechanism for cortical visual processing '' . </S>",
    "<S> we exploit the inherent feature similarity across images in datasets to identify the input variability and use recursive optimization procedure , to determine data partitioning at each node , thereby , learning the attention hierarchy . </S>",
    "<S> a set of binary classifiers is organized on top of the learnt hierarchy to minimize the overall test - time complexity . </S>",
    "<S> the attention model maximizes the margins for the binary classifiers for optimal decision boundary modelling , leading to better performance at minimal complexity . </S>",
    "<S> the proposed framework has been evaluated on both caltech-256 and sun datasets and achieves accuracy improvement over state - of - the - art tree - based methods at significantly lower computational cost .    </S>",
    "<S> shell : bare demo of ieeetran.cls for ieee journals    visual attention , image classification , feature similarity , attention tree ( atree ) , support vector machine ( svm ) . </S>"
  ]
}