{
  "article_text": [
    "as the measurement techniques are developping , more and more data are high dimensional vectors generated by measuring a continuous process on a discrete sampling grid",
    ". many examples of this type of data can be found in real world applications , in various fields such as spectrometry , voice recognition , time series analysis , etc .",
    "data of this type should not be handled in the same way as standard multivariate observations but rather analysed as _ functional _ data : each observation is a function coming from an input space with infinite dimension , sampled on a high resolution sampling grid .",
    "this leads to a large number of variables , generally more than the number of observations",
    ". moreover , functional data are frequently smooth and generate highly correlated variables as a consequence .",
    "applied to the obtained high dimensional vectors , classical statistical methods ( e.g. , linear regression , factor analysis ) often lead to ill - posed problems , especially when a covariance matrix has to be inverted ( this is the case , e.g. , in linear regression , in discriminant analysis and also in sliced inverse regression ) .",
    "indeed , the number of observed values for each function is generally larger than the number of functions itself and these values are often strongly correlated . as a consequence , when these data are considered as multidimensional vectors ,",
    "the covariance matrix is ill - conditioned and leads to unstable and unaccurate solutions in models where its inverse is required .",
    "thus , these methods can not be directly used . during past years",
    ", several methods have been adapted to that particular context and grouped under the generic name of functional data analysis ( fda ) methods .",
    "seminal works focused on linear methods such as factorial analysis ( @xcite , among others ) and linear models @xcite ; a comprehensive presentation of linear fda methods is given in @xcite .",
    "more recently , nonlinear functional models have been extensively developed and include generalized linear models @xcite , kernel nonparametric regression @xcite , functional inverse regression @xcite , neural networks @xcite , @xmath1-nearest neighbors @xcite , support vector machines ( svm ) , @xcite , among a very large variety of methods .    in previous works , numerous authors have shown that the derivatives of the functions lead sometimes to better predictive performances than the functions themselves in inference tasks , as they provide information about the shape or the regularity of the function .",
    "in particular applications such as spectrometry @xcite , micro - array data @xcite and handwriting recognition @xcite , these characteristics lead to accurate predictive models .",
    "but , on a theoretical point of the view , limited results about the effect of the use of the derivatives instead of the original functions are available : @xcite studies this problem for a linear model built on the first derivatives of the functions . in the present paper ,",
    "we also focus on the theoretical relevance of this common practice and extend @xcite to nonparametric approaches and incomplete knowledge .",
    "more precisely , we address the problem of the estimation of the conditional expectation @xmath2 of a random variable @xmath3 given a functional random variable @xmath4 .",
    "@xmath3 is assumed to be either real valued ( leading to a regression problem ) or to take values in @xmath5 ( leading to a binary classification problem ) .",
    "we target two theoretical difficulties .",
    "the first difficulty is the potential information loss induced by using a derivative instead of the original function : when one replaces @xmath4 by its order @xmath0 derivative @xmath6 , consistent estimators ( such as kernel models @xcite ) guarantee an asymptotic estimation of @xmath7 but can not be used directly to address the original problem , namely estimating @xmath2 .",
    "this is a simple consequence of the fact that @xmath8 is not a one to one mapping .",
    "the second difficulty is induced by sampling : in practice , functions are never observed exactly but rather , as explained above , sampled on a discrete sampling grid . as a consequence ,",
    "one relies on approximate derivatives , @xmath9 ( where @xmath10 denotes the sampling grid ) .",
    "this approach induces even more information loss with respect to the underlying functional variable @xmath4 : in general , a consistent estimator of @xmath11 will not provide a consistent estimation of @xmath2 and the optimal predictive performances for @xmath3 given @xmath9 will be lower than the optimal predictive performances for @xmath3 given @xmath4 .",
    "we show in this paper that the use of a smoothing spline based approach solves both problems .",
    "smoothing splines are used to estimate the functions from their sampled version in a convergent way .",
    "in addition , properties of splines are used to obtain estimates of the derivatives of the functions with no induced information loss .",
    "both aspects are implemented as a preprocessing step applied to the multivariate observations generated via the sampling grid .",
    "the preprocessed observations can then be fed into any finite dimensional consistent regression estimator or classifier , leading to a consistent estimator for the original infinite dimensional problem ( in real world applications , we instantiate the general scheme in the particular case of kernel machines @xcite ) .    the remainder of the paper is organized as follows : section  [ notations ] introduces the model , the main smoothness assumption and the notations .",
    "section  [ splines ] recalls important properties of spline smoothing .",
    "section  [ consist ] presents approximation results used to build a general consistent classifier or a general consistent regression estimator in section  [ consist_general ] .",
    "finally , section  [ application ] illustrates the behavior of the proposed method for two real world spectrometric problems .",
    "the proofs are given at the end of the article .",
    "we consider a pair of random variables @xmath12 where @xmath4 takes values in a functional space @xmath13 and @xmath3 is either a real valued random variable ( regression case ) or a random variable taking values in @xmath5 ( binary classification case ) . from this",
    ", we are given a learning set @xmath14 of @xmath15 independent copies of @xmath12 .",
    "moreover , the functions @xmath16 are not entirely known but sampled according to a non random sampling grid of finite length , @xmath17 : we only observe @xmath18 , a vector of @xmath19 and denote @xmath20 the corresponding learning set . our goal is to construct :    1 .   _ in the binary classification case _ : a classifier , @xmath21 , whose misclassification probability @xmath22 asymptotically reaches the bayes risk @xmath23 i.e. , @xmath24 ; 2 .   _ in the regression case _ : a regression function , @xmath21 , whose @xmath25 error @xmath26 ^ 2\\right)}\\ ] ] asymptotically reaches the minimal @xmath25 error @xmath27 ^ 2\\right)}\\ ] ] i.e. , @xmath28 .",
    "+ this definition implicitly requires @xmath29 and as a consequence , corresponds to a @xmath25 convergence of @xmath21 to the conditional expectation @xmath30 , i.e. , to @xmath31 ^ 2\\right)}= 0 $ ] .",
    "such @xmath21 are said to be _ ( weakly ) consistent _ @xcite .",
    "we have deliberately used the same notations for the ( optimal ) predictive performances in both the binary classification and the regression case .",
    "we will call @xmath32 the bayes risk even in the case of regression .",
    "most of the theoretical background of this paper is common to both the regression case and the classification case : the distinction between both cases will be made only when necessary .    as pointed out in the introduction ,",
    "the main difficulty is to show that the performances of a model built on the @xmath33 asymptotically reach the best performance achievable on the original functions @xmath16 . in addition",
    ", we will build the model on derivatives estimated from the @xmath33 .",
    "our goal is to leverage the functional nature of the data by allowing differentiation operators to be applied to functions prior their submission to a more common classifier or regression function .",
    "therefore we assume that the functional space @xmath13 contains only differentiable functions .",
    "more precisely , @xmath13 is the sobolev space @xmath34)\\mid\\,\\forall\\,j=1,\\ldots , m,\\ d^jh \\text { exists in the weak sense,}\\,\\text{and } d^mh\\in l^2([0,1])\\bigr\\}$ ] , where @xmath35 is the @xmath36-_th _ derivative of @xmath37 ( also denoted by @xmath38 ) and for an integer @xmath39 . of course",
    ", by a straightforward generalization , any bounded interval can be considered instead of @xmath40 $ ] .    to estimate the underlying functions @xmath16 and their derivatives from sampled data",
    ", we rely on smoothing splines .",
    "more precisely , let us consider a deterministic function @xmath41 sampled on the aforementioned grid .",
    "a smoothing spline estimate of @xmath42 is the solution , @xmath43 , of @xmath44 } ( h^{(m)}(t))^2 dt,\\ ] ] where @xmath45 is a regularization parameter that balances interpolation error and smoothness ( measured by the @xmath25 norm of the @xmath0-_th _ derivative of the estimate ) .",
    "the goal is to show that a classifier or a regression function built on @xmath46 is consistent for the original problem ( i.e. , the problem defined by the pair @xmath12 ) : this means that using @xmath46 instead of @xmath4 has no dramatic consequences on the accuracy of the classifier or of the regression function . in other words ,",
    "asymptotically , no information loss occurs when one replaces @xmath4 by @xmath46 .",
    "the proof is based on the following steps :    1 .   first , we show that building a classifier or a regression function on @xmath46 is approximately equivalent to building a classifier or a regression function on @xmath47 using a specific metric .",
    "this is done by leveraging the reproducing kernel hilbert space ( rkhs ) structure of @xmath48 .",
    "this part serves one main purpose : it provides a solution to work with estimation of the derivatives of the original function in a way that preserves all the information available in @xmath49 . in other words ,",
    "the best predictive performances for @xmath3 theoretically available by building a multivariate model on @xmath49 are equal to the best predictive performances obtained by building a functional model on @xmath46 .",
    ", we link @xmath50 with @xmath2 by approximation results available for smoothing splines .",
    "this part of the proof handles the effects of sampling .",
    "3 .   finally",
    ", we glue both results via standard @xmath19 consistency results .",
    "as we want to work on derivatives of functions from @xmath51 , a natural inner product for two functions of @xmath51 would be @xmath52 .",
    "however , we prefer to use an inner product of @xmath51 ( @xmath53 only induces a semi - norm on @xmath51 ) because , as will be shown later , such an inner product is related to an inner product between the sampled functions considered as vectors of @xmath54 .",
    "this can be done by decomposing @xmath48 into @xmath55 @xcite , where @xmath56 ( the space of polynomial functions of degree less or equal to @xmath57 ) and @xmath58 is an infinite dimensional subspace of @xmath48 defined via @xmath0 boundary conditions .",
    "the boundary conditions are given by a full rank linear operator from @xmath48 to @xmath59 , denoted @xmath60 , such that @xmath61 .",
    "classical examples of boundary conditions include the case of `` natural splines '' ( for @xmath62 , @xmath63 ) and constraints that target only the first values of @xmath37 and its derivatives at a fixed position , for instance the conditions : @xmath64 .",
    "other boundary conditions can be used @xcite , depending on the application .",
    "once the boundary conditions are fixed , an inner product on both @xmath65 and @xmath58 can be defined : @xmath66 is an inner product on @xmath58 ( as @xmath67 and @xmath68 give @xmath69 ) . moreover , if we denote @xmath70 , then @xmath71 is an inner product on @xmath65 .",
    "we obtain this way an inner product on @xmath48 given by @xmath72 where @xmath73 is the projector on @xmath74 .",
    "equipped with @xmath75 , @xmath48 is a reproducing kernel hilbert space ( rkhs , see e.g. @xcite ) .",
    "more precisely , it exists a kernel @xmath76 ^ 2\\rightarrow { \\ensuremath{\\mathbb{r}^{}}}$ ] such that , for all @xmath77 and all @xmath78 $ ] , @xmath79 . the same occurs for @xmath65 and @xmath58 which respectively have reproducing kernels denoted by @xmath80 and @xmath81 .",
    "we have @xmath82 .    in the most common cases ,",
    "@xmath80 and @xmath81 have already been explicitly calculated ( see e.g. , @xcite , especially chapter 6 , sections 1.1 and 1.6.2 ) . for example , for @xmath39 and the boundary conditions @xmath83 , we have : @xmath84 and @xmath85      we need now to compute to @xmath43 starting with @xmath86 .",
    "this can be done via a theorem from @xcite .",
    "we need the following compatibility assumptions between the sampling grid @xmath87 and the boundary conditions operator  @xmath60 :    [ a_sampling_boundary ] the sampling grid @xmath17 is such that    1 .",
    "sampling points are distinct in @xmath40 $ ] and @xmath88 2 .",
    "the @xmath0 boundary conditions @xmath89 are linearly independent from the @xmath90 linear forms @xmath91 , for @xmath92 ( defined on @xmath48 )    then @xmath43 and @xmath86 are linked by the following result :    [ th_kimeldorf_wahba ] under assumption ( a[a_sampling_boundary ] ) , the unique solution @xmath43 to equation   is given by : @xmath93 where @xmath94 is a full rank linear operator from @xmath19 to @xmath48 defined by : @xmath95 with    * @xmath96 * @xmath97 ; * @xmath98 is a basis of @xmath99 , @xmath100 and @xmath101 ; * @xmath102 and @xmath103 .",
    "the first important consequence of theorem [ th_kimeldorf_wahba ] is that building a model on @xmath104 or on @xmath49 leads to the same optimal predictive performances ( to the same bayes risk ) .",
    "this is formalized by the following corollary :    [ corollarynoloss ] under assumption ( a[a_sampling_boundary ] ) , we have    * in the binary classification case : @xmath105 * in the regression case : @xmath106 ^ 2\\right)}=\\\\ &    \\qquad \\inf_{\\phi:{\\ensuremath{\\mathbb{r}^{|{\\tau_d}|}}}\\rightarrow { \\ensuremath{\\mathbb{r}^ { } } } } { \\mathbb{e}\\left(\\left[\\phi\\left({\\mathbf{x}^{{\\tau_d}}}\\right)- y\\right]^2\\right ) }      \\end{split}\\ ] ]      the second important consequence of theorem [ th_kimeldorf_wahba ] is that the inner product @xmath75 is equivalent to a specific inner product on @xmath54 given in the following corollary :    [ cor_ps_h ] under assumption ( a[a_sampling_boundary ] ) and for any @xmath107 and @xmath108 in @xmath54 , @xmath109 where @xmath110 with @xmath111 .",
    "the matrix @xmath112 is symmetric and positive definite and defines an inner product on @xmath54 .",
    "the corollary is a direct consequence of equations and .    in practice",
    ", the corollary means that the euclidean space @xmath113 is isomorphic to @xmath114 , where @xmath115 is the image of @xmath54 by @xmath94 . as a consequence",
    ", one can use the hilbert structure of @xmath48 directly in @xmath54 via @xmath112 : as the inner product of @xmath48 is defined on the order @xmath0 derivatives of the functions , this corresponds to using those derivatives instead of the original functions .",
    "more precisely , let @xmath116 be the transpose of the cholesky triangle of @xmath112 ( given by the cholesky decomposition @xmath117 ) .",
    "corollary [ cor_ps_h ] shows that @xmath116 acts as an approximate differentiation operation on sampled functions .",
    "let us indeed consider an estimation method for multivariate inputs based only on inner products or norms ( that are directly derived from the inner products ) , such as , e.g. , kernel ridge regression @xcite . in this latter case , if a gaussian kernel is used , the regression function has the following form : @xmath118 where @xmath119 are learning examples in @xmath120 and the @xmath121 are non negative real values obtained by solving a quadratic programming problem and @xmath122 is a parameter of the method .",
    "then , if we use kernel ridge regression on the training set @xmath123 ( rather than the original training set @xmath124 ) , it will work on the norm in @xmath25 of the derivatives of order @xmath0 of the spline estimates of the @xmath16 ( up to the boundary conditions ) .",
    "more precisely , the regression function will have the following form : @xmath125 in other words , up to the boundary conditions , an estimation method based solely on inner products , or on norms derived from these inner products , can be given modified inputs that will make it work on an estimation of the derivatives of the observed functions .",
    "as shown in corollary [ corollarynoloss ] in the previous section , building a model on @xmath49 or on @xmath104 leads to the same optimal predictive performances .",
    "in addition , it is obvious that given any one - to - one mapping @xmath126 from @xmath19 to itself , building a model on @xmath127 gives also the same optimal performances than building a model on @xmath49",
    ". then as @xmath116 is invertible , the optimal predictive performances achievable with @xmath128 are equal to the optimal performances achievable with @xmath49 or with @xmath104 .    in practice however , the actual preprocessing of the data can have a strong influence on the obtained performances , as will be illustrated in section [ application ] .",
    "the goal of the theoretical analysis of the present section is to guarantee that no systematic loss can be observed as a consequence of the proposed functional preprocessing scheme .",
    "the previous section showed that working on @xmath49 , @xmath128 or @xmath104 makes no difference in terms of optimal predictive performances .",
    "the present section addresses the effects of sampling : asymptotically , the optimal predictive performances obtained on @xmath104 converge to the optimal performances achievable on the original and unobserved functional variable @xmath4 .      from the sampled random function @xmath129",
    ", we can build an estimate , @xmath104 , of @xmath4 . to ensure consistency",
    ", we must guarantee that @xmath104 converges to @xmath4 . in the case of a deterministic function @xmath42",
    ", this problem has been studied in numerous papers , such as @xcite ( among others ) . here",
    "we recall one of the results which is particularly well adapted to our context .",
    "obviously , the sampling grid must behave correctly , whereas the information contained in @xmath49 will not be sufficient to recover @xmath4 .",
    "we need also the regularization parameter @xmath45 to depend on @xmath87 . following @xcite , a sampling grid @xmath87",
    "is characterized by two quantities : @xmath130 one way to control the distance between @xmath4 and @xmath104 is to bound the ratio @xmath131 so as to ensure quasi - uniformity of the sampling grid .",
    "more precisely , we will use the following assumption :    [ a_spline_consistent ] there is r such that @xmath132 for all @xmath133 .",
    "then we have :    [ th_cox ] under assumptions ( a[a_sampling_boundary ] ) and ( a[a_spline_consistent ] ) , there are two constants @xmath134 and @xmath135 depending only on @xmath136 and @xmath0 , such that for any @xmath137 and any positive @xmath45 : @xmath138    this result is a rephrasing of corollary 4.16 from @xcite which is itself a direct consequence of theorem 4.10 from the same paper",
    ".    convergence of @xmath43 to @xmath42 is then obtained by the following simple assumptions :    [ a_spline_consistent_more ] the series of sampling points @xmath87 and the series of regularization parameters , @xmath45 , depending on @xmath87 and denoted by @xmath139 , are such that @xmath140 and @xmath141 .",
    "the next step consists in relating the optimal predictive performances for the regression and the classification problem @xmath12 to the performances associated to @xmath142 when @xmath133 goes to infinity , i.e. , relating @xmath32 to    1 .",
    "_ binary classification case _ : @xmath143 2 .",
    "_ regression case _ : @xmath144 ^ 2\\right)}\\ ] ]    two sets of assumptions will be investigated to provide the convergence of the bayes risk @xmath145 to @xmath32 :    [ a_esp_conditionnelle ] * either *    1 .",
    "[ a_norme_x ] @xmath146 is finite and @xmath147 , + * or * 2 .",
    "[ a_suite_disc ] @xmath148 and @xmath149 is finite .",
    "the first assumption ( a[a_esp_conditionnelle][a_norme_x ] ) requires an additional smoothing property for the predictor functional variable @xmath4 and is only valid for a binary classification problem whereas the second assumption ( a[a_esp_conditionnelle][a_norme_x ] ) requires an additional property for the sampling point series : they have to be growing sets .",
    "theorem  [ th_cox ] then leads to the following corollary :    [ cor_consist_d ] under assumptions ( a[a_sampling_boundary])-(a[a_esp_conditionnelle ] ) , we have : @xmath150",
    "let us now consider any consistent classification or regression scheme for standard multivariate data based either on the inner product or on the euclidean distance between observations .",
    "examples of such classifiers are support vector machine @xcite , the kernel classification rule @xcite and @xmath1-nearest neighbors @xcite to name a few . in the same way ,",
    "multilayer perceptrons @xcite , kernel estimates @xcite and @xmath1-nearest neighbors regression @xcite are consistent regression estimators .",
    "additional examples of consistent estimators in classification and regression can be found in @xcite .",
    "we denote @xmath151 the estimator constructed by the chosen scheme using a dataset @xmath152 , where the @xmath119 are @xmath15 independent copies of a pair of random variables @xmath153 with values in @xmath154 ( classification ) or @xmath155 ( regression ) .",
    "the proposed functional scheme consists in choosing the estimator @xmath156 as @xmath157 with the dataset @xmath158 defined by : @xmath159 as pointed out in section [ diff_norm ] , the linear transformation @xmath160 is an approximate multivariate differentiation operator : up to the boundary conditions , an estimator based on @xmath161 is working on the @xmath0-th derivative of @xmath162 .    in more algorithmic terms ,",
    "the estimator is obtained as follows :    1 .",
    "choose an appropriate value for @xmath163 2 .",
    "compute @xmath164 using theorem [ th_kimeldorf_wahba ] and corollary [ cor_ps_h ] ; 3 .",
    "compute the cholesky decomposition of @xmath164 and the transpose of the cholesky triangle , @xmath160 ( such that @xmath165 ) ; 4 .",
    "compute @xmath166 to obtain the transformed dataset @xmath158 ; 5 .   build a classifier / regression function @xmath157 with a multivariate method in @xmath19 applied to the dataset @xmath158 ; 6 .",
    "associate to a new sampled function @xmath167 the prediction @xmath168 .",
    "figure  [ fig : processing ] illustrates the way the method performs : instead of relying on an approximation of the function and then on the derivation preprocessing of this estimates , it directly uses an equivalent metric by applying the @xmath160 matrix to the sampled function .",
    "the consistency result proved in theorem  [ consistance_directe ] shows that , combined with any consistent multidimensional learning algorithm , this method is ( asymptotically ) equivalent to using the original function drawn at the top left side of figure  [ fig : processing ] .",
    "[ fig : processing ]    on a practical point of view , @xcite demonstrates that cross validated estimates of @xmath45 achieve suitable convergence rates .",
    "hence , steps 1 and 2 can be computed simultaneously by minimizing the total cross validated error for all the observations , given by @xmath169 where @xmath170 is a @xmath171 matrix called the _ influence matrix _ ( see @xcite ) , over a finite number of @xmath45 values .",
    "corollary [ corollarynoloss ] and corollary  [ cor_consist_d ] guarantee that the estimator proposed in the previous section is consistent :    [ consistance_directe ] under assumptions ( a[a_sampling_boundary])-(a[a_esp_conditionnelle ] ) , the series of classifiers / regression functions @xmath172 is consistent : @xmath173      while theorem [ consistance_directe ] is very general , it could be easily extended to cover special cases such as additional hypothesis needed by the estimation scheme or to provide data based parameter selections .",
    "we discuss briefly those issues in the present section .",
    "it should first be noted that most estimation schemes , @xmath151 , depend on parameters that should fulfill some assumptions for the scheme to be consistent .",
    "for instance , in the kernel ridge regression method in @xmath174 , with gaussian kernel , @xmath151 has the form given in equation  ( [ eq::krrsolution ] ) where the @xmath175 are the solutions of @xmath176 the method thus depends on the parameter of the gaussian kernel , @xmath122 and of the regularization parameter @xmath177 .",
    "this method is known to be consistent if ( see theorem  9.1 of @xcite ) : @xmath178 additional conditions of this form can obviously be directly integrated in theorem  [ consistance_directe ] to obtain consistency results specific to the corresponding algorithms .",
    "moreover , practitioners generally rely on data based selection of the parameters of the estimation scheme @xmath151 via a validation method : for instance , rather than setting @xmath177 to e.g. , @xmath179 for @xmath15 observations ( a choice which is compatible with theoretical constraints on @xmath177 ) , one chooses the value of @xmath177 that optimizes an estimation of the performances of the regression function obtained on an independent data set ( or via a re - sampling approach ) .",
    "in addition to the parameters of the estimation scheme , functional data raise the question of the convenient order of the derivative , @xmath0 , and of the sampling grid optimality . in practical applications ,",
    "the number of available sampling points can be unnecessarily large ( see @xcite for an example with more than 8  000 sampling points ) .",
    "the preprocessing performed by @xmath160 do not change the dimensionality of the data which means that overfitting can be observed in practice when the number of sampling points is large compared to the number of functions .",
    "moreover , processing very high dimensional vectors is time consuming .",
    "it is there quite interesting in practice to use a down - sampled version of the original grid .    to select the parameters of @xmath180 ,",
    "the order of the derivative and/or the down - sampled grid , a validation strategy , based on splitting the dataset into training and validation sets , could be used .",
    "a simple adaptation of the idea of @xcite shows that a penalized validation method can be used to choose any combination of those parameters consistently . according to those papers , the condition for the consistency of the validation strategy would simply relate the shatter coefficients of the set of classifiers in @xmath181 to the penalization parameter of the validation .",
    "once again , this type of results is a rather direct extension of theorem [ consistance_directe ] .",
    "in this section , we show that the proposed approach works as expected on real world spectrometric examples : for some applications , the use of derivatives leads to more accurate models than the direct processing of the spectra ( see e.g. @xcite for other examples of such a behavior based on ad hoc estimators of the spectra derivatives ) .",
    "it should be noted that the purpose of this section is only to illustrate the behavior of the proposed method on finite datasets .",
    "the theoretical results of the present paper show that all consistent schemes have asymptotically identical performances , and therefore that using derivatives is asymptotically useless . on a finite dataset",
    "however , preprocessing can have strong influence on the predictive performances , as will be illustrated in the present section .",
    "in addition , schemes that are not universally consistent , e.g. , linear models , can lead to excellent predictive performances on finite datasets ; such models are therefore included in the present section despite the fact the theory does not apply to them .",
    "the methodology followed for the two illustrative datasets is roughly the same :    1 .",
    "the dataset is randomly split into a training set on which the model is estimated and a test set on which performances are computed .",
    "the split is repeated several times .",
    "the tecator dataset ( section [ subsection : tecator ] ) is rather small ( 240 spectra ) and exhibits a rather large variability in predictive performances between different random splits .",
    "we have therefore used 250 random splits . for the yellow - berry dataset ( section [",
    "subsection : yellowberry ] ) , we used only 50 splits as the relative variability in performances is far less important .",
    "@xmath45 is chosen by a global leave - one - out strategy on the spectra contained in training set ( as suggested in section [ subsection : algorithm ] ) .",
    "more precisely , a leave - one - out estimate of the reconstruction error of the spline approximation of each training spectrum is computed for a finite set of candidate values for @xmath45 .",
    "then a common @xmath45 is chosen by minimizing the average over the training spectra of the leave - one - out reconstruction errors .",
    "this choice is relevant as cross validation estimates of @xmath45 are known to have favorable theoretical properties ( see @xcite among others ) .",
    "3 .   for regression problems , a kernel ridge regression ( krr ) @xcite",
    "is then performed to estimate the regression function ; this method is consistent when used with a gaussian kernel under additional conditions on the parameters ( see theorem  9.1 of @xcite ) ; as already explained , in the applications , kernel ridge regression is performed both with a gaussian kernel and with a linear kernel ( in that last case , the model is essentially a ridge regression model ) .",
    "parameters of the models ( a regularization parameter , @xmath177 , in all cases and a kernel parameter , @xmath122 for gaussian kernels ) are chosen by a grid search that minimizes a validation based estimate of the performances of the model ( on the training set ) .",
    "a leave - one - out solution has been chosen : in kernel ridge regression , the leave - one - out estimate of the performances of the model is obtained as a by - product of the estimation process , without additional computation cost , see e.g. @xcite .",
    "+ additionally , for a sake of comparison with a more traditional approach in fda , kernel ridge regression is compared with a nonparametric kernel estimate for the tecator dataset ( section  [ subsec : tecator : regression ] ) .",
    "nonparametric kernel estimate is the first nonparametric approach introduced in functional data analysis @xcite and can thus be seen as a basis for comparison in the context of regression with functional predictors . for this method ,",
    "the same methodology as with kernel ridge regression was used : the parameter of the model ( i.e. , the bandwidth ) was selected on a grid search minimizing a cross - validation estimate of the performances of the model . in this case , a 4-fold cross validation estimate was used instead of a leave - one - out estimate to avoid a large computational cost .",
    "4 .   for the classification problem , a support vector machine ( svm )",
    "is used @xcite . as krr ,",
    "svm are consistent when used with a gaussian kernel @xcite .",
    "we also use a svm with a linear kernel as this is quite adapted for classification in high dimensional spaces associated to sampled function data .",
    "we also use a k - nearest neighbor model ( knn ) for reference .",
    "parameters of the models ( a regularization parameter for both svm , a kernel parameter , @xmath122 for gaussian kernels and number of neighbors k for knn ) are chosen by a grid search that minimizes a validation based estimate of the classification error : we use a 4-fold cross - validation to get this estimate .",
    "we evaluate the models obtained for each random split on the test set .",
    "we report the mean and the standard deviation of the performance index ( classification error and mean squared error , respectively ) and assess the significance of differences between the reported figures via paired student tests ( with level 1% ) .",
    "6 .   finally , we compare models estimated on the raw spectra and on spectra transformed via the @xmath160 matrix for @xmath182 ( first derivative ) and @xmath62 ( second derivative ) . for both values of @xmath0 , we used the most classical boundary conditions ( @xmath183 and @xmath184 ) .",
    "depending of the problem , other boundary conditions could be investigated but this is outside the scope of the present paper ( see @xcite for discussion on this subject ) . for the tecator problem",
    ", we also compare these approaches with models estimated on first and second derivatives based on interpolating splines ( i.e. with @xmath185 ) and on first and second derivatives estimated by finite differences .",
    "+ note that the kind of preprocessing used has almost no impact on the computation time .",
    "in general , selecting the parameters of the model with leave - one - out or cross - validation will use significantly more computing power than constructing the splines and calculating their derivatives . for instance , computing the optimal @xmath45 with the approach described above takes less than 0.1 second for the tecator dataset on a standard pc using our r implementation which is negligible compared to the several minutes used to select the optimal parameters of the models used on the prepocessed data .",
    "the first studied dataset is the standard tecator dataset @xcite .",
    "it consists in spectrometric data from the food industry .",
    "each of the 240 observations is the near infrared absorbance spectrum of a meat sample recorded on a tecator infratec food and feed analyzer .",
    "each spectrum is sampled at 100 wavelengths uniformly spaced in the range 8501050 nm .",
    "the composition of each meat sample is determined by analytic chemistry and percentages of moisture , fat and protein are associated this way to each spectrum .",
    "the tecator dataset is a widely used benchmark in functional data analysis , hence the motivation for its use for illustrative purposes .",
    "more precisely , in section [ subsec : tecator : regression ] , we address the original regression problem by predicting the percentage of fat content from the spectra with various regression method and various estimates of the derivative preprocessing : this analysis shows that both the method and the use of derivative have a strong effect on the performances whereas the way the derivatives are estimated has almost no effect .",
    "additionally , in section [ subsec : tecator : bruit ] , we apply a noise ( with various variances ) to the original spectra in order to study the influence of smoothing in the case of noisy predictors : this section shows the relevance of the use of a smoothing spline approach when the data are noisy . finally , section  [ subsec : tecator : classif ] deals with a classification problem derived from the original tecator problem ( in the same way as what was done in @xcite ) : conclusions of this section are similar to the ones of the regression study .",
    "as explained above , we first address the regression problem that consists in predicting the fat content of peaces of meat from the tecator dataset .",
    "the parameters of the model are optimized with a grid search using the leave - one - out estimate of the predictive performances ( both models use a regularization parameter , with an additional width parameter in the gaussian kernel case ) .",
    "the original data set is split randomly into 160 spectra for learning and 80 spectra for testing . as shown in the result table [ table : tecatorreg ] , the data exhibit a rather large variability ; we use therefore 250 random split to assess the differences between the different approaches .",
    "the performance indexes are the mean squared error ( m.s.e . ) and the @xmath186 . where @xmath187 is the ( empirical ) variance of the target variable on the test set .",
    "] as a reference , the target variable ( fat ) has a variance equal to 14.36 .",
    "results are summarized in table [ table : tecatorreg ] .",
    ".summary of the performances of the chosen models on the test set ( fat tecator regression problem ) when using either a kernel ridge regression ( krr ) with linear kernel or with gaussian kernel or when using a nonparametric kernel estimate ( nke ) with various inputs : o ( original data ) , s1 ( smoothing splines with order 1 derivatives ) , is1 ( interpolating splines with order 1 derivatives ) , fd1 ( order 1 derivatives estimated by finite differences ) and s2 , is2 and fd2 ( the same as previously with order 2 derivatives ) . [ cols=\"<,<,^,^ \" , ]      for the full names of the regression models ) ]    as in the previous section , we can rank the methods in increasing order of modelling error , we obtain the following result : @xmath188 where g stands for gaussian kernel and l for linear kernel ( hence g - s2 stands for kernel ridge regression with gaussian kernel and smoothing splines with order 2 derivatives ) ; @xmath189 corresponds to a significant difference ( for a paired student test with level 1% ) and @xmath190 to a non significant one . for this application",
    ", there is a significant gain in using a non linear model ( the gaussian kernel ) . in addition",
    ", the use of derivatives leads to less contrasted performances that the ones obtained in the previous section but it still improves the quality of the non linear model in a significant way . in term of normalized mean squared error ( mean squared error divided by the variance of the target variable ) , using a non linear model with the second derivatives of the spectra corresponds to an average gain of more than 5% ( i.e. , a reduction of the normalised mean squared error from 24% for the standard linear model to 18.6% ) .",
    "in this paper we proposed a theoretical analysis of a common practice that consists in using derivatives in classification or regression problems when the predictors are curves .",
    "our method relies on smoothing splines reconstruction of the functions which are known only via a discrete deterministic sampling .",
    "the method is proved to be consistent for very general classifiers or regression schemes : it reaches asymptotically the best risk that could have been obtained by constructing a regression / classification model on the true random functions .",
    "we have validated the approach by combining it with nonparametric regression and classification algorithms to study two real - world spectrometric datasets .",
    "the results obtained in these applications confirm once again that relying on derivatives can improve the quality of predictive models compared to a direct use of the sampled functions .",
    "the way the derivatives are estimated does not have a strong impact on the performances except when the data are noisy . in this case , the use of smoothing splines is quite relevant .    in the future",
    ", several issues could be addressed .",
    "an important practical problem is the choice of the best order of the derivative , @xmath0 .",
    "we consider that a model selection approach relying on a penalized error loss could be used , as is done , in e.g. , @xcite , to select the dimension of truncated basis representation for functional data .",
    "note that in practice , such parameter selection method could lead to select @xmath191 and therefore to automatically exclude derivative calculation when it is not needed .",
    "this will extend the application range of the proposed model .    a second important point to study it the convergence rate for the method .",
    "it would be very convenient for instance , to be able to relate the size of the sampling grid to the number of functions .",
    "but , this latter issue would require the use of additional assumptions on the smoothness of the regression function whereas the result presented in this paper , even if more limited , only needs mild conditions .",
    "we thank ccile levasseur and sylvain coulomb ( cole dingnieurs de purpan , eip , toulouse , france ) for sharing the interesting problem presented in section  [ subsection : yellowberry ] .",
    "we also thank philippe besse ( institut de mathmatiques de toulouse , universit de toulouse , france ) for helpfull discussions and suggestions .",
    "finally , we thank the anonymous reviewers for their valuable comments and suggestions that helped to improve the quality of the paper .",
    "50 natexlab#1#1url # 1`#1`urlprefix[2][]#2 [ 2]#2 , , . .",
    ", , , . . ,",
    ", , , . . ,",
    ", , , . . ,",
    ". , . . , . , , . . , . , ,",
    "universit toulouse  iii .",
    ", , , , . . ,",
    ". , . . , . , ,",
    ". . , . , , , ,",
    ". . , . , , ,",
    ". . , . , ,",
    ". . , . , ,",
    ", , , , . . ,",
    ". , . . , . , ,",
    ". . , . , , ,",
    ". , . . , . , ,",
    ". . , . , ,",
    ", , , , . . ,",
    ", , , . , in : , .",
    ". . , . , ,",
    "information science and statistics , . , . .",
    ", , , . , in : . , .",
    "in the original theorem ( lemma  3.1 ) in @xcite , one has to verify that @xmath192 spans @xmath65 and that @xmath193 are linearly independent . these are consequences of assumption ( a[a_sampling_boundary ] ) .",
    "first , @xmath194 where @xmath195 is the inverse of @xmath196 ( see @xcite )",
    ". then @xmath197^t$ ] where @xmath198 is the vandermonde matrix with @xmath57 columns and @xmath90 rows associated to values @xmath199 .",
    "if the @xmath200 are distinct , this matrix is of full rank .",
    "moreover the reproducing property shows that @xmath201 implies @xmath202 for all @xmath203 . hence , @xmath204 where @xmath205 denotes the linear form @xmath206 . as the co - dimension of @xmath58 is @xmath207 and as , by assumption ( a[a_sampling_boundary ] ) , @xmath60 is linearly independent of @xmath208 , we thus have @xmath209 ( or @xmath210 would be @xmath211 ) .",
    "thus , we obtain that @xmath202 for all @xmath126 in @xmath48 and , as @xmath212 are distinct , that @xmath213 for all @xmath214 , leading to the independence conclusion for the @xmath193 .",
    "finally , we prove that @xmath94 is of full rank . indeed , if @xmath215 , @xmath216 and @xmath217 . as @xmath218 is a basis of @xmath65 , @xmath216 implies @xmath219 and therefore @xmath220 .",
    "as shown above , the @xmath193 are linearly independent and therefore @xmath221 implies @xmath222 , which in turns leads to @xmath223 via the simplified formula for @xmath224 .",
    "according to theorem [ th_kimeldorf_wahba ] , there is a full rank linear mapping from @xmath54 to @xmath48 , @xmath94 , such that for any function @xmath225 , @xmath226 .",
    "let us denote @xmath115 the image of @xmath54 by @xmath94 , @xmath227 the orthogonal projection from @xmath48 to @xmath115 and @xmath228 the inverse of @xmath94 on @xmath115 . obviously , we have @xmath229 .",
    "let @xmath230 be a measurable function from @xmath54 to @xmath5 .",
    "then @xmath231 defined on @xmath48 by @xmath232 is a measurable function from @xmath48 to @xmath5 ( because @xmath228 and @xmath227 are both continuous ) .",
    "then for any measurable @xmath230 , @xmath233 , and therefore @xmath234 conversely , let @xmath230 be a measurable function from @xmath48 to @xmath5",
    ". then @xmath231 defined on @xmath19 by @xmath235 , is measurable .",
    "then for any measurable @xmath230 , @xmath236 , and therefore @xmath237 the combination of equations and gives equality .      1 .",
    "* suppose assumption ( a[a_esp_conditionnelle][a_norme_x ] ) is fullfilled * + the proof is based on theorem 1 in @xcite .",
    "this theorem relates the bayes risk of a classification problem based on @xmath12 with the bayes risk of the problem @xmath238 where @xmath239 is a series of transformations on @xmath4 . + more formally , for a pair of random variables @xmath12 , where @xmath4 takes values in @xmath240 , an arbitrary metric space , and @xmath3 in @xmath5 , let us denote for any series of functions @xmath241 from @xmath240 to itself , @xmath242 .",
    "theorem 1 from @xcite states that @xmath243 implies @xmath244 , where @xmath245 denotes the metric on @xmath240 .",
    "+ this can be applied to @xmath246 with @xmath247 : under assumptions ( a[a_sampling_boundary ] ) and ( a[a_spline_consistent ] ) , theorem  [ th_cox ] gives : @xmath248 .",
    "taking the expectation of both sides gives @xmath249 , using the fact that the constants are independent of the function under analysis . then under assumptions ( a[a_esp_conditionnelle][a_norme_x ] ) and ( a[a_spline_consistent_more ] ) , @xmath250 . according to @xcite ,",
    "this implies @xmath251 2 .",
    "* suppose assumption ( a[a_esp_conditionnelle][a_suite_disc ] ) is fullfilled * + the conclusion will follow both for classification case and for regression case .",
    "the proof follows the general ideas of @xcite . under assumption  ( a[a_sampling_boundary ] ) , by theorem  [ th_kimeldorf_wahba ] and with an argument similar to those developed in the proof of corollary [ corollarynoloss ] , @xmath252 . from assumption ( a[a_esp_conditionnelle][a_suite_disc ] ) , @xmath253 is clearly a filtration .",
    "moreover , as @xmath254 and thus @xmath149 are finite , @xmath255 is a uniformly bounded martingal for this filtration ( see lemma 35 of @xcite ) .",
    "this martingale converges in @xmath256-norm to @xmath257 ; we have * @xmath258 as @xmath162 is a function of @xmath4 ( via theorem  [ th_kimeldorf_wahba ] ) ; * by theorem  [ th_cox ] , @xmath259 in @xmath25 which proves that @xmath4 is @xmath260-measurable .",
    "+ finally , @xmath261 and @xmath262 .",
    "+ the conclusion follows from the fact that : 1 .   _",
    "binary classification case : _ the bound @xmath263 ( see theorem  2.2 of @xcite ) concludes the proof ; 2 .   _",
    "regression case : _ as @xmath149 is finite , @xmath264 is also finite and the convergence also happens for the quadratic norm ( see corollary 6.22 in @xcite ) , i.e. , @xmath265 hence , as @xmath266 , the conclusion follows .",
    "we have @xmath267 let @xmath268 be a positive real . by corollary  [ cor_consist_d ]",
    ", it exists @xmath269 such that , for all @xmath270 , @xmath271 moreover , as shown in corollary [ corollarynoloss ] and as @xmath160 is invertible , we have in the binary classification case : @xmath272 , and in the regression case : @xmath273 ^ 2\\right)}= \\inf_{\\phi:{\\ensuremath{\\mathbb{r}^{|{\\tau_d}|}}}\\rightarrow { \\ensuremath{\\mathbb{r}^ { } } } } { \\mathbb{e}\\left(\\left[\\phi\\left({\\mathbf{q}_{\\lambda_d,{\\tau_d}}}{\\mathbf{x}^{{\\tau_d}}}\\right)- y\\right]^2\\right)}$ ] . by hypothesis , for any fixed @xmath133 , @xmath274 is consistent , that is @xmath275 in the classification case and @xmath276 ^ 2\\right)},\\ ] ] in the regression case , and therefore for any fixed @xmath277 , @xmath278 .",
    "combined with equations and , this concludes the proof ."
  ],
  "abstract_text": [
    "<S> in some real world applications , such as spectrometry , functional models achieve better predictive performances if they work on the derivatives of order @xmath0 of their inputs rather than on the original functions . as a consequence , </S>",
    "<S> the use of derivatives is a common practice in functional data analysis , despite a lack of theoretical guarantees on the asymptotically achievable performances of a derivative based model . in this paper , we show that a smoothing spline approach can be used to preprocess multivariate observations obtained by sampling functions on a discrete and finite sampling grid in a way that leads to a consistent scheme on the original infinite dimensional functional problem . </S>",
    "<S> this work extends @xcite to nonparametric approaches and incomplete knowledge . to be more precise , </S>",
    "<S> the paper tackles two difficulties in a nonparametric framework : the information loss due to the use of the derivatives instead of the original functions and the information loss due to the fact that the functions are observed through a discrete sampling and are thus also unperfectly known : the use of a smoothing spline based approach solves these two problems . </S>",
    "<S> finally , the proposed approach is tested on two real world datasets and the approach is experimentaly proven to be a good solution in the case of noisy functional predictors .    </S>",
    "<S> functional data analysis , consistency , statistical learning , derivatives , svm , smoothing splines , rkhs , kernel </S>"
  ]
}