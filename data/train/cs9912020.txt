{
  "article_text": [
    "in this paper we want to understand some aspects of behaviour of additive predictive models @xcite for high - dimensional data sets . more specifically , we will deal with the problem of approximating a real - valued predictor function @xmath2 defined on a domain @xmath3 and depending on variables @xmath4 by additive functions @xmath5 in fewer ( say @xmath6 ) variables and of lower order of interaction , say @xmath1 , having the form @xmath7 where typically @xmath8 .",
    "our aim will be to obtain easily verifiable upper bounds on the @xmath9-norm of the remainder @xmath10 , as well as to try and understand their dependence on the dimension @xmath6 of the domain @xmath3 .",
    "in doing so , we will move away from the condition of independence of the predictor variables @xmath11 , replacing it with a milder restriction of the probability distribution being equivalent to the product of its marginals .",
    "our approximation with additive functions is optimal ( with regard to the mean square error ) for independent variables , but not necessarily in the dependent case , where we obtain an upper error bound . at the same time",
    ", we are not yet ready to offer concrete algorithms for real very large datasets .    in one of its forms , the phenomenon of concentration of measure",
    "@xcite says that every lipschitz function on a sufficiently high - dimensional domain @xmath3 is well - approximated by a _",
    "constant function _ , that is , an additive function of the lowest possible order of interaction @xmath12 .",
    "however , as one would expect , the limitations of this result are such as to render it inapplicable in our situation : a reasonably good approximation requires the intrinsic dimension of a dataset to be prohibitively high .",
    "the most natural question is therefore , can one achieve a better approximation in lower ( mid to high ) dimensions by merely allowing additive functions of a higher interaction order @xmath13 ?",
    "even here the answer turns out to be negative : there exist functions for which approximation by constants is the best possible among all additive functions in the orders up to @xmath14 .",
    "this result makes it clear that the only way to achieve a better approximation by additive functions is to impose further restrictions on the functions @xmath2 .",
    "our suggestion is to consider smooth functions and generalise the standard lipschitz condition by requiring the @xmath9-norm of the vector of all mixed derivatives of order @xmath15 to be bounded above by a constant @xmath16 , independent of the dimension of the domain @xmath3 .    under such restrictions and an additional condition of independence of predictor variables @xmath17",
    ", we develop a technique for obtaining approximating additive functions of a prescribed order and derive upper error bounds in the @xmath9-norm ( section [ best ] . )",
    "our results are illustrated by a series of examples in the last section [ examples ] , in particular showing that the asymptotic rate of convergence of the theoretically derived error is accurate .",
    "section [ quasi ] aims at relaxing the assumption of independence of predictor variables .",
    "recall that random variables @xmath17 are independent if the probability distribution , @xmath18 , can be written as the product of the marginal distributions , @xmath19 .",
    "we replace this with the assumption which we call _ quasi - independence _ and which calls for the distribution of @xmath20 to be equivalent to the product of its marginals .",
    "the radon - nikodym theorem then implies that the distribution @xmath21 is the product of @xmath18 with the radon  nikodym derivative @xmath22 , and the additive approximation obtained using the product measure ( distribution ) serves at the same time as an approximation with regard to the ` true ' probability distribution , @xmath18 .",
    "the upper error bounds in the @xmath9-norm includes the derivative @xmath22 .",
    "the research here is motivated by the observation that adaptive techniques like mars  @xcite which estimate models of the form given in equation  ( [ eq : anova ] ) will produce models with predominantly lower order interactions . in practice ,",
    "interactions with order higher than 5 are not used .",
    "models of the type defined in equation  ( [ eq : anova ] ) have been called `` anova decomposition '' in  @xcite as they generalise for real variables the models which are used in the analysis of variance ( anova ) .",
    "applications of the anova decomposition for the analysis of techniques for variance estimation can be found in  @xcite and for the estimation of quadrature errors in  @xcite and  @xcite .",
    "the work here extends the previous work by providing estimates for the approximation errors of truncated anova decompositions . in earlier work by one of the authors  @xcite it",
    "was seen how the concentration of measure may be exploited to get highly effective numerical differentiation procedures .",
    "computational techniques for the determination of the anova decomposition can for example be found in  @xcite .",
    "more generally , data mining @xcite is being developed for the analysis of large data sets which appeared in business and science due to the fact that both data acquisition and data storage have become inexpensive because of the availability of cheap transducers and data storage devices .",
    "typically , data mining applications lead to very large data sets of high dimension , and high - dimensional problems are intrinsically difficult as they are affected by the _ curse of dimensionality _  @xcite .",
    "both queries and the identification of predictive models are very time - consuming .",
    "at the same time , it turns out that the effects of high dimensions are not only bad and some may be successfully exploited to lead to highly effective algorithms ( cf .",
    "e.g. @xcite ) . in the ideal case ,",
    "high - dimensional data is just data which contains high amounts of information and these added amounts of information should intuitively lead to better algorithms .",
    "the first basic concept is that of an  _ object _ @xmath23 .",
    "examples include shoppers of a retailer , insurance customers or variable stars .",
    "these objects have many properties , some of which are observable .",
    "the array of observed properties is the _ feature vector _ @xmath24 .",
    "we assume here that @xmath25 but arrays containing other types of components and even of mixed types occur as well . in order to distinguish objects we require some quantitative notion of difference or similarity between objects .",
    "it seems reasonable to assume that the euclidean distance between two feature vectors , given by @xmath26 provides information about the difference of the underlying objects .",
    "however , this leads straight to the first paradox of _ increasing distances _ : the typical distance between two objects grows as we add new features , that is , _ distance grows with the number of features _",
    "in other words , the more one knows about the objects , the more different they seem to appear and ultimately , the difference may become infinite . while the increased difference seems reasonable",
    ", the unboundedness of the distance is not , as intuitively two objects are only `` different to a certain point '' .",
    "fortunately , this paradoxical growth of distances may be easily cured by either normalising the euclidean distance or else by scaling the variables in such a way that , for example , the average distance between two feature vectors is @xmath27 .",
    "as an example , consider the euclidean cube @xmath28^n$ ] ; it is easy to see that the average distance between two randomly chosen vectors @xmath29^n$ ] ( the _ characteristic size _ of the cube @xcite ) is @xmath30 , and thus a natural way to normalise the euclidean distance is @xmath31 while this paradox is seemingly simple , it is necessary to consider , and it is important that the dissimilarity is normalised in the way suggested so as to put things in the proper perspective .    the predictor functions @xmath2 we are interested in will estimate the probability with which a certain statement about the object is true , and thus the range of @xmath2 is , typically , the unit interval @xmath28 $ ] .",
    "moreover , it is reasonable to assume that the function assumes close values for close values of parameters .",
    "usually this condition is expressed by requiring @xmath2 to be _ lipschitz _ , that is , to satisfy @xmath32 where @xmath33 is a positive number called the _ lipschitz constant .",
    "_    we will assume that the data points are drawn from a domain @xmath3 with respect to some probability distribution @xmath34 .",
    "thus , we view the domain @xmath35 as a probability space equipped with a distance ( an mm_-space _ , cf .",
    "@xcite ) .",
    "the simplest class of additive functions is that of zeroth order of interaction , @xmath36 , in which case the approximating functions @xmath5 are simply constants .",
    "it turns out that even the approximation by constants admits a substantial theory if the domain is high - dimensional .",
    "such approximation improves as dimension grows , which observation is at the core of the _ phenomenon of concentration of measure on high - dimensional structures . _",
    "( see @xcite and numerous references therein . )",
    "let @xmath37 be an infinite family of metric spaces equipped with probability measures .",
    "intuitively , the spaces @xmath38 should be thought of as having asymptotically growing dimension .",
    "assume that the distances are so normalised that the characteristic size of each @xmath38 is @xmath39 .",
    "let @xmath2 be a lipschitz function on some space @xmath38 , and assume for simplicity that the corresponding lipschitz constant @xmath40 , that is , @xmath41 for all @xmath42 .",
    "the concentration phenomenon refers to the observation that for many ` natural ' families @xmath43 as above , the probability that @xmath44 differs from its expected value by less than @xmath45 is at least @xmath46 where @xmath47 are constants only depending on the family of spaces @xmath37 in question .",
    "intuitively , it means that a ` nice ' function on a space of high dimension ` concentrates ' near one value .    as an example , consider hyperspheres .",
    "the euclidean @xmath6-sphere of unit radius is defined as @xmath48 the constants for the family @xmath49 are @xmath50 similar estimates with varying constants hold for the hypercubes ( remember that the distance has to be appropriately normalised ) , the euclidean spaces with the gaussian measure , the hamming cubes , the groups of unitary matrices , and so forth .",
    "( _ loco citato . _ )    concentration of measure in particular enables one to explain the following , well - known , observation made long ago about large and multidimensional datasets .",
    "consider an arbitrary object and its nearest neighbors .",
    "it can then be observed in practice and verified theoretically that with higher dimensions _ the distances between the point and its nearest neighbors become almost constant _ , so that the nearest neighbors all are close to a hypersphere around the point .",
    "in fact , the large majority of the points seems to be contained in a thin shell around the point .",
    "( see e.g. @xcite . )",
    "this paradox is illustrated in figure  [ fig : conc ] where the hundred nearest neighbors of a point are displayed all from an i.i.d .",
    "set of normally distributed data points .    to derive this paradox from the phenomenon of concentration of measure",
    ", it is enough to apply the bound ( [ conce ] ) to the function @xmath51 , where @xmath52 is the fixed ( query ) point .",
    "( see @xcite for details . )",
    "this paradox can _ not _ be cured , at least not if one wants to keep the same distance ( similarity measure ) between datapoints , and is the origin of many problems one faces with high - dimensional data .",
    "returning back to the result captured by formula ( [ conce ] ) , one concludes that the predictor function @xmath2 on a high - dimensional domain will be closely approximated by a constant function , if the dimension is large enough .",
    "however , how good is such an approximation from a practical point of view",
    "? it comes as no wonder that the dimension required for the upper bound in ( [ conce ] ) to become genuinely small is extremely high .",
    "for example , suppose the data is uniformly distributed on the hypersphere of dimension @xmath6 of unit radius , @xmath53 , and the predictor function @xmath54 is @xmath27-lipschitz and takes values in the range @xmath55 $ ] .",
    "let @xmath45 and suppose we want to approximate @xmath2 by a constant , @xmath56 , in such a way that @xmath57 holds with probability @xmath58 .",
    "even for the value of @xmath59 ( which is nowhere good enough ) the minimal dimension required to achieve our goal is @xmath60 . for @xmath61 ,",
    "the minimal dimension is @xmath62 . to obtain the accuracy @xmath63 ,",
    "the dimension @xmath64 would suffice , but one can hardly expect a real dataset to have an _ intrinsic _ dimension of this sort , that is , to depend on five thousand independent parameters . finally , to ensure that @xmath65 which already seems to be a reasonably close approximation",
    ", one needs the dimension to be on the order of the astronomical ( and unrealistic ) @xmath66 .",
    "it is clear from the above that approximation by constants is not good enough in the medium to high dimensions which is the case we are mostly interested in .",
    "the next natural question is therefore : will the approximation error bounds based on the concentration phenomenon and given by formula ( [ conce ] ) improve automatically if one allows the approximation by additive functions of _ higher interaction order than zero _ ?",
    "it seems quite natural that by significantly relaxing the restrictions on the class of approximating functions one gets better approximation bounds .",
    "rather surprisingly , it is not the case , as there exist functions on @xmath6-dimensional domains for which approximation by constants is the best possible among _ all _ additive functions with interaction orders @xmath67 of up to @xmath68 .",
    "( subsection [ zeroex ] . )    in view of the existence of such examples , it seems in a sense unavoidable that one should impose additional restrictions on the predictor functions @xmath2 to obtain better bounds on higher - order approximations with additive functions .",
    "we will now put forward such restrictions as we find most natural , in the hope that the reader is prepared to accept them as such .",
    "key to all approximation results are assumptions about the data set and the class of functions to be approximated .",
    "as we are interested in the asymptotic behaviour as the dimension becomes medium to large ( which means in practice larger than 10 ) , we actually are interested in a family of spaces , functions etc , parameterised by their dimension .",
    "we will assume that the feature vectors are distributed with a density @xmath18 which has a first moment @xmath69 and a variance @xmath70 which does not depend on the dimension .",
    "for some of the theorems it will be required that the components of @xmath20 are independent , that is to say the underlying data distribution satisfies @xmath71 however , for practical purposes this assumption is unrealistic , because in the context of data mining where one has many physical variables ( @xmath6 is large ) , one would expect that those variables could be highly correlated . in view of this , we will subsequently replace the condition of independence with a milder restriction for the product distribution @xmath18 to be _ in the same measure class _ as the product distribution , @xmath72 we will refer to such random variables @xmath17 as _ quasi - independent .",
    "_    the functions we consider are assumed to be lipschitz - continuous , i.e. , @xmath73 for some constant @xmath74 which is independent of the dimension . in the case of differentiable functions @xmath2 this",
    "corresponds to the bound @xmath75 this condition is very natural and invoked frequently , it assumes that function values have similar sizes for points which are close .",
    "`` smoother '' functions will be defined as functions satisfying the condition @xmath76 for some constant @xmath16 which is independent of the dimension @xmath6 .",
    "while such smoothness definitions do always have a certain degree of arbitrariness and are difficult to check in applications , one can see , first , that this condition generalises the lipschitz condition , and , second , that for the case of functions given as products @xmath77 this bound follows from lipschitz continuity for functions bounded away from zero as one can verify that @xmath78 in particular , if @xmath79 then @xmath80 . similar bounds for the components of the decomposition from equation  ( [ eq : anova ] )",
    "are used in  @xcite for the analysis of quadrature formulas based on a reproducing kernel hilbert space .",
    "we make here the small step to consider families of functions where the bounds on the derivatives are independent of the dimension in order to obtain an estimate of the approximation error behaviour as a function of dimension .",
    "so far we have assumed that the expected norm squared of the vectors @xmath20 does not grow with dimension .",
    "however , in many practical applications this is not the case .",
    "often , the features are normalised , say , so that they are all in @xmath81 $ ] .",
    "this is the normalisation we will mostly consider here if not mentioned otherwise . in this and many similar cases the variance grows proportional to the dimension @xmath6 .",
    "thus by dividing all the variables by @xmath82 one gets to the previous situation again . if one invokes the lipschitz condition in the transformed variables , one has for the gradients in the original variables the condition @xmath83 which is thus equivalent to the lipschitz condition . for the higher order derivatives one gets the condition @xmath84 the dependence of the bounds on the dimension @xmath6 appears unnatural at first",
    ", however note that this is simply a consequence of the scaling of the variables , corresponding to lipschitz - continuity in the unit ball .",
    "the alternative to the introduction of this scaling in the conditions would be to scale the variables so that the average distances would be independent of the number of variables . for convenience ,",
    "we have here chosen to normalise all the @xmath85 to @xmath28 $ ] and to scale the smoothness conditions .",
    "while these conditions are strong , they are not unusual , and allow the generalisation of the well - known limit theorems for @xmath86 of the average of @xmath6 i.i.d . random variables . a nontrivial example ,",
    "for which the conditions hold is given by @xmath87    a far - reaching but so - far implicit assumption is that all the variables @xmath85 contribute in the same way to @xmath2 .",
    "this is true for applications where the features @xmath85 correspond to equally important parts or elements and occur when @xmath2 is describing an aggregate of similar elements each characterised by one @xmath85 .",
    "examples include employees of a company and stars of a galaxy .",
    "an alternative situation is considered in  @xcite by h.  woniakowski and his collaborators . there",
    "the variables are given weights depending on their importance .",
    "these weights then enter the smoothness assumptions .",
    "one of the consequences of that choice is that in many cases the negative effects of the concentration effect discussed below can be avoided . here , however , we consider a different situation of equally important variables and thus have to deal with the consequences of the concentration .",
    "the two examples of functions given above illustrate that functions like the mean which `` change '' with the dimension @xmath6 are very natural .",
    "in this section we formulate the main results of this paper .      first the notation is established .",
    "let @xmath88 be a probability space , @xmath89 denote a family of random variables and @xmath90 be the usual conditional expectations . throughout this subsection",
    ", we make a standing assumption that the random variables @xmath91 are independent , i.e. , the density distribution is of the form @xmath92    the operator @xmath93 is defined as @xmath94 using the independence assumption on random variables , one gets a ` telescoping sum ' @xmath95 the terms of the sum can now be expanded in the same way as @xmath2 and repeated application of these expansions provides a theorem which looks very much like taylor s theorem :    [ taylor ] let @xmath2 be an integrable function on @xmath3 .",
    "then for every natural @xmath1 , @xmath96 : @xmath97    the proof uses induction . first , the case @xmath98 is just equation  ( [ eq : expansion1 ] ) .",
    "if the equality holds for @xmath99 then the first @xmath100 terms are the same as for @xmath101 and only the last term needs further expansion . in this term",
    "each summand is a function of @xmath102 and from equation  ( [ eq : expansion1 ] ) one obtains : @xmath103 replacing the last term in the equation for the case @xmath99 with the right - hand side of this equation leads to the equation for @xmath101 .    a similar decomposition for the special case of @xmath104 has been proved in  @xcite where the theorem is called _ decomposition lemma_.",
    "next we introduce the space of @xmath9 functions which are sums of functions only depending on @xmath67 variables each as : @xmath105 ( note that @xmath106 are closed , which follows from theorem [ projection ] below . )",
    "now we introduce the operator @xmath107 such that @xmath108 and the remainder operator @xmath109 with @xmath110 from theorem [ taylor ] one then gets @xmath111 .",
    "[ projection ] the operator @xmath112 is an orthogonal projection , and @xmath113    the statement that @xmath112 is a projection , i.e. , @xmath114 , is equivalent to showing that @xmath115 is zero for @xmath116 .",
    "this follows directly from the fact that any function @xmath2 for which the function values do not depend on the variable @xmath85 one has @xmath117 .",
    "as any @xmath116 consists of a sum of functions which depend only on @xmath1 variables and all the terms in @xmath118 contain @xmath119 there is at least one @xmath120 for which any particular term in the expansion of @xmath2 does not depend on @xmath121 and thus @xmath122 .",
    "if @xmath18 is a product distribution as assumed above , then @xmath123 for every projection measure and for any @xmath124 one has @xmath125    now as for each @xmath1-tuple @xmath126 and for each @xmath127-tuple @xmath128 one has at least one @xmath129 which is different from all the @xmath130 then @xmath131 this shows that the error term @xmath132 is orthogonal on @xmath133 which implies that @xmath112 is an orthogonal projection into @xmath133 and from this the minimisation characterisation follows .",
    "the orthogonality of all the components of the decomposition is shown in  @xcite for the case of the uniform distribution .",
    "now we observe that @xmath134 and thus the decomposition as in theorem [ taylor ] terminates and so @xmath135 if all the variances of these terms exist , one has from the orthogonality of this decomposition @xmath136    error estimates are obtained for differentiable functions , one may also get bounds based on lipschitz constants . we introduce the ( marginal ) cumulative distribution function @xmath137 and the kernel @xmath138 where @xmath139 is the heaviside function , i.e. , @xmath140 for @xmath141 and @xmath142 for @xmath143 .",
    "using integration by parts one gets for differentiable @xmath2 : @xmath144 now let @xmath145 then the expected value squared is @xmath146 now let @xmath147 and @xmath148 then one gets by integration by parts and from the cauchy - schwarz inequality : @xmath149 if @xmath2 is lipschitz continuous with constant @xmath74 .    for the case of @xmath1 interactions we first define the seminorm @xmath150 by @xmath151 and then obtain :    let @xmath152 be defined as in @xmath110 then one has for the mean squared error bound @xmath153 [ error ]    it is shown the same way as in an earlier theorem that all the terms of the sum defining @xmath154 are orthogonal and so @xmath155 for simplicity set @xmath156 and application of equation  ( [ eq : repro ] ) and similar reasoning as for the case @xmath98 gives @xmath157 where @xmath158 now one can see that @xmath159 and from this the claimed bound follows .",
    "consider the case @xmath160 for a fixed @xmath161 which are independent of @xmath6 .",
    "for example , let the @xmath20 be uniformly distributed in the unit hypercube .",
    "then the constant @xmath162 is independent of @xmath6 .",
    "furthermore , in section [ basic ] it was suggested that the appropriate smoothness restriction on @xmath2 is @xmath163 from the previous theorem one can in this case conclude that @xmath164 for example , if @xmath161 is the uniform distribution on @xmath55 $ ] the constant @xmath162 can be computed explicitly and one gets @xmath165 and so @xmath166 in the case where @xmath161 is the normal distribution with expectation @xmath167 and variance @xmath27 one gets @xmath168 ( rounded ) and thus @xmath169 the same bound is obtained if the components of @xmath20 are i.i.d .",
    "normal with a variance such that @xmath170 and @xmath171 .      as we have already noticed , the assumption on independence of feature vectors is not realistic for most practical data sets . here",
    "we propose one way to overcome this difficulty and get an approximation which    1 .",
    "satisfies a similar error bound as the one for the independent variable case .",
    "2 .   has an error of the same order as the best least squares fit .",
    "let @xmath172 and @xmath173 be measures on the same sigma - algebra of sets . recall that @xmath172 is _ absolutely continuous _ with regard to @xmath173 if @xmath174 always implies @xmath175 , that is , every @xmath173-null set is also a @xmath172-null set .",
    "the radon  nikodym theorem then implies that @xmath176 , where @xmath177 is a measurable function called the _ radon  nikodym derivative . _ the measures @xmath172 and @xmath173 are _ equivalent , _ or _ in the same measure class ,",
    "_ if they are absolutely continuous with respect to each other , that is , have the same null sets",
    ".    in our context , say that the random variables @xmath17 on the probability space @xmath88 are _ quasi - independent _ if the probability distribution @xmath18 is in the same measure class as the product of marginal distributions :    @xmath72    denote by @xmath178 the product measure on @xmath179 with the product distribution above , and let @xmath180 be the radon  nikodym derivative of the product measure @xmath178 with regard to the underlying measure @xmath34 on the data set",
    ". then one has @xmath181    given a predictor function @xmath2 on @xmath179 , let @xmath182 be the orthogonal projection defined as in subsection [ independent ] , but using the product measure @xmath183 rather than the original probability distribution @xmath34 .",
    "then @xmath184 gives an approximation of @xmath2 by a sum of additive functions of the order of interaction @xmath185 .",
    "denote @xmath186    let @xmath187 denote the expected value with regard to the measure @xmath178 . then for every random variable @xmath188 on @xmath179 , @xmath189 where @xmath190 denotes the @xmath191-norm .",
    "remember that the constant @xmath162 only depends on the marginal distributions @xmath192 , and so it remains the same no matter which of the two measures , @xmath34 or @xmath178 , we are considering , cf .",
    "( [ g ] ) and ( [ gamma ] ) .",
    "now theorem [ error ] leads to the following estimate .",
    "the square error bound of the additive approximation satisfies @xmath193    because of the equivalence of the measures the error in the original measure is bounded by @xmath194 and theorem  [ error ] for the measure @xmath178 then gives @xmath195 .",
    "finally , the approximation does also provide a bound for the error of the best approximation in @xmath133 :    @xmath196    where @xmath197 .",
    "the lower bound holds by definition . for the upper bound we first use the property that the measures are in the same class to get : @xmath198 then we note that @xmath199 is a best approximation with respect to the norm defined by the product distribution by theorem  [ projection ] .",
    "thus one has for any @xmath200 : @xmath201 and , as the measures are in the same class : @xmath202 and combining these inequalities and taking the minimum over @xmath203 provides the desired estimate .",
    "this process does not lead directly to a computational procedure in the case of dependent variables .",
    "we hope to discuss such a procedure in a consequent paper .",
    "we have so far assumed that @xmath2 depends on exactly @xmath6 variables @xmath89 .",
    "again in practice , any response variable @xmath204 is typically only partially described by a function of the predictor variables and a large proportion ( in particular in data mining applications ) of @xmath204 remains unexplained by @xmath205 .",
    "one way to model this situation is to assume that there are actually @xmath206 predictor variables and by only limiting the approximations to the first @xmath6 another error is introduced .",
    "the error of this ( truncation ) approximation is @xmath207 in this case the error of the approximation @xmath208 ( previously called @xmath112 ) has to include this term as well and thus the total error is now @xmath209 where @xmath210 ( previously @xmath152 ) is the error term of the anova decomposition of @xmath211 .",
    "these two error terms are orthogonal in the case of independent variables and one gets for the expected error squared : @xmath212 now the approximation in the space of functions of @xmath6 variables is the best possible and so is the additive approximation .",
    "consequently , by increasing @xmath6 the error is never increased . clearly , if the error of the proposed additive model is to be small then both the terms @xmath213 and @xmath214 need to be small . in many practical cases ,",
    "however , the term @xmath213 can not be made small and thus major portions of @xmath2 are beyond our controll .",
    "this , however , does not mean that the approximation @xmath208 is of no practical use and being able to controll a portion of the variation of @xmath2 can lead to commercial and scientific benefits .",
    "note that the smoothness condition for this case is really a smoothness condition for @xmath211 .",
    "an interesting question which nevertheless has not been investigated here is how the smoothness of the underlying function @xmath215 determines the smoothness of the projection @xmath211 .",
    "in this section we will look at a few examples in more detail and we will investigate how well the theory of the previous sections applies . in the examples we will be considering approximations with constant functions , with additive functions , and with functions with second and third order interactions .",
    "the order of the interactions is @xmath216 where @xmath1 has the values @xmath217 and @xmath218 respectively .      for the uniform distribution on the hypercube @xmath55^n$ ] one",
    "has @xmath219 . as a function to approximate we choose @xmath220 from this one gets @xmath221 thus one can choose the lipschitz constant to be @xmath222 and consequently the bound from the previous section is @xmath223 for practical error estimates this bound is slightly too pessimistic .",
    "the order of convergence in @xmath6 is accurate .",
    "this can be seen from the results of a simulation which are in figure  [ fig : example1 ] where the average errors squared have been multiplied by @xmath224 in order to confirm the @xmath225 behavior .",
    "sometimes , functions are only dependent on a few of the variables . if the data is equally distributed over many variables such that they have a uniformly ( in the dimension ) bounded expected squared norm then the values of any component will concentrate around zero .",
    "this is illustrated in this example . here",
    "the function considered is @xmath227 the data points are assumed to be i.i.d . normally distributed and in order to obtain the finite expected value of @xmath228 the variances of each component is @xmath229 .",
    "the error of the third and higher order interaction approximations is zero , for the lower order approximations see figure  [ fig : example2err ] for the expected squared error .",
    "the theory again predicts asymptotic behaviour of the error of @xmath230 which is confirmed by the simulation result displayed in figure  [ fig : example2 ] .",
    "the theory and the examples so far have illustrated the best possible approximation with additive and interaction models .",
    "as in the first example the function @xmath231 shall be approximated .",
    "we use 1000 data points uniformly distributed on @xmath55^n$ ] and will let the dimension vary between 1 and 10 .",
    "no random noise was added to @xmath44 .",
    "the approximations are computed with the code `` mars '' by j.friedman  @xcite .",
    "this code allows the specification of the maximal order of interactions and we have investigated the approximations obtained for constants , additive functions and 2nd and 3rd order interaction models .",
    "the approximation uses tensor products of piecewise linear functions and the basis functions used are products of functions of the form @xmath232 where @xmath233 . the total number of basis functions allowed needs to be specified and we chose here @xmath234 as an upper limit such that basically each term in the anova decomposition may have @xmath218 basis functions on average . in figure  [ fig : mars1 ] one can see that allowing higher order interactions lead to better approximations and also that in higher dimensions the approximations have a tendency to get better with dimension .",
    "one can get an optimal approximation if the number @xmath1 of interactions allowed equals the dimension and thus one might expect a growth in error for dimensions close to @xmath1 .",
    "there is also a problem that allowing too many basis functions of too high interaction may make the possible models too complex and thus introduce instability for small enough data sizes .",
    "the effect of allowing higher order interactions does not necessarily mean that mars will finally select terms with higher order interactions at all .",
    "all these effects have to be taken into account when one interprets figure  [ fig : mars1 ] .",
    "the functions whose best additive approximation is by constants ( and whose existence was claimed in section [ basic ] ) are those possessing high degree of symmetry . without entering into technical details ,",
    "let us mention two examples which seem intuitively clear .",
    "let @xmath235 be a probability space which is a domain in @xmath236 and is such that the variables @xmath91 are independent .    consider the function @xmath237 defined on the hypercube which is symmetric about the origin , for example @xmath238^n$ ] .",
    "it can be proved that the best additive approximation in the order of interaction @xmath68 is that by zero function .",
    "notice that in accordance with our philosophy the function @xmath2 has to be normalised by the factor of @xmath239 , in order to keep its lipschitz constant bounded by @xmath27 .",
    "the resulting function @xmath240 on the same cube assumed pretty small values : its maximum is just @xmath241 , and thus one may argue that the approximation by zero function is not bad at all .",
    "the next example is somewhat stronger .",
    "denote by @xmath242 a usual bell - shaped function supported on the interval @xmath243 $ ] , that is , a @xmath244 function taking values between @xmath167 and @xmath27 , which is identically zero outside of @xmath243 $ ] , takes a positive value at @xmath167 , is monotone on each of the intervals @xmath245 $ ] and @xmath246 $ ] , and satisfies @xmath247 for all natural @xmath6 .",
    "let us also assume that @xmath248 .",
    "for a vertex , @xmath249 , of the cube @xmath28^n$ ] define the _ parity _ of @xmath250 as the number of ones among the coordinates modulo @xmath251 : @xmath252 now set for every @xmath253^n$ ] @xmath254 where @xmath255 denotes the euclidean distance .",
    "a moment s thought shows that @xmath2 is a well - defined @xmath244-function assuming values in the interval between @xmath256 and @xmath257 , in particular if @xmath258 is a vertex , then @xmath259 depending on the parity .",
    "again , one can show that the above function admits no better additive approximation in all orders of interaction up to @xmath68 inclusive than that by the zero function .",
    "notice that the normalisation of the function @xmath2 aimed at keeping the lipschitz constant of the order @xmath39 leads to the function @xmath260 whose maximal values reach @xmath261 .",
    "in our paper we have attempted to perform initial analysis of the problem of approximating a predictor function on a high - dimensional dataset with additive functions allowing for interactions of a lower order .",
    "we are interested in the specifics of medium to high dimensions .",
    "the proposed model makes what we believe to be reasonable assumptions , from the modeling viewpoint , on the function to be approximated ( the normalisation conditions and ` higher - order smoothness conditions ' ) .",
    "we argue that some conditions of this kind are to be imposed in order to obtain approximation results : we exhibit examples of lipschitz functions in @xmath6 variables for which the best additive function approximation of order of interaction @xmath68 is a constant . under the proposed conditions ,",
    "we derive from a taylor - type theorem upper bounds on the approximation errors .",
    "the results are illustrated on examples and compared to the results obtained using the mars software package .",
    "the examples confirm that the asymptotic order of our error bounds is right .",
    "the second named author ( v.p . ) is grateful to the computer sciences laboratory of the australian national university for hospitality extended between july and december 1999 .",
    "part of the research ( including the above visit ) was supported by the australian cooperative research centre for advanced computational systems ( acsys ) .",
    "partial support was also provided by the marsden fund of the royal society of new zealand , in particular towards a visit by the first named author ( m.h . ) to the victoria university of wellington in april 2002 .",
    "the authors acknowledge the constructive suggestions of the referee which helped to substantially improve the paper .",
    "h.  woniakowski , _ efficiciency of quasi - monte carlo algorithms for high dimensions _ ,",
    "monte carlo and quasi - monte carlo methods 1998 ( h.  niederreiter and j.  spanier , eds . ) , springer - verlag , berlin , 1999 ."
  ],
  "abstract_text": [
    "<S> we discuss some aspects of approximating functions on high - dimensional data sets with additive functions or anova decompositions , that is , sums of functions depending on fewer variables each . </S>",
    "<S> it is seen that under appropriate smoothness conditions , the errors of the anova decompositions are of order @xmath0 for approximations using sums of functions of up to @xmath1 variables under some mild restrictions on the ( possibly dependent ) predictor variables . </S>",
    "<S> several simulated examples illustrate this behaviour . </S>"
  ]
}