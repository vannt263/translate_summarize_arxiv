{
  "article_text": [
    "we consider the problem of efficiently simulating time homogeneous markov chains with _ metastable states _ : subsets of state space in which the markov chain remains for a long time before leaving . by a markov chain we mean a _ discrete time _",
    "stochastic process satisfying the markov property .",
    "heuristically , a set @xmath0 is metastable for a given markov chain if the markov chain reaches local equilibrium in @xmath0 much faster than it leaves @xmath0 .",
    "we will define local equilibrium precisely below , using _ quasistationary distributions _ ( qsds ) .",
    "the simulation of an exit event from a metastable state using a naive integration technique can be very time consuming .",
    "metastable markov chains arise in many contexts .",
    "the dynamics of physical systems are often modeled by memoryless stochastic processes , including markov chains , with widespread applications in physics , chemistry , and biology . in computational statistical physics ( which is the main application field we have in mind ) ,",
    "such models are used to understand macroscopic properties of matter , starting from an atomistic description .",
    "the models can be discrete or continuous in time .",
    "the discrete in time case has particular importance : even when the underlying model is continuous in time , what is simulated in practice is a markov chain obtained by time discretization . in the context of computational statistical physics ,",
    "a widely used continuous time model is the langevin dynamics  @xcite , while a popular class of discrete time models are the markov state models  @xcite . for details ,",
    "see  @xcite . for examples of discrete time models not obtained from an underlying continuous time dynamics ,",
    "see  @xcite . in this article",
    ", we propose an efficient algorithm for simulating metastable markov chains over very long time scales . even though one of our motivations is to treat time discretized versions of continuous time models , we do not discuss errors in exit events due to time discretization",
    "; we refer for example to  @xcite and references therein for an analysis of this error .    in the physical applications above",
    ", metastability arises from the fact that the microscopic time scale ( i.e. , the physical time between two steps of the markov chain ) is much smaller than the macroscopic time scale of interest ( i.e. , the physical time to observe a transition between metastable states ) .",
    "both energetic and entropic barriers can contribute to metastability .",
    "energetic barriers correspond to high energy saddle points between metastable states in the potential energy landscape , while entropic barriers are associated with narrow pathways between metastable states ; see figure  [ f : metastable ] .",
    "many algorithms exist for simulating metastable stochastic processes over long time scales .",
    "one of the most versatile such algorithms is the _",
    "parallel replica dynamics _ ( parrep ) developed by a.f .",
    "voter and co - workers  @xcite .",
    "parrep can be used with both energetic and entropic barriers , and it requires no assumptions about temperature , barrier heights , or reversibility .",
    "the algorithm was developed to efficiently compute transitions between metastable states of langevin dynamics . for a mathematical analysis of parrep in its original continuous time",
    "setting , see  @xcite . in this article",
    ", we present an algorithm which is an adaptation of parrep to the discrete time setting .",
    "it applies to any markov chain .",
    "parrep uses many replicas of the process , simulated in parallel asynchronously , to rapidly find transition pathways out of metastable states .",
    "the gain in efficiency over direct simulation comes from distributing the computational effort across many processors , parallelizing the problem in time .",
    "the cost is that the trajectory becomes coarse - grained , evolving in the set of metastable states instead of the original state space .",
    "the continuous time version of parrep has been successfully used in a number of problems in materials science ( see e.g.  @xcite ) , allowing for atomistic resolution while also reaching extended time scales of microseconds , @xmath1 s. for reference , the microscopic time scale  typically the period of vibration of bond lengths  is about @xmath2 s.    in the continuous time case , consistency of the algorithm relies on the fact that first exit times from metastable states are exponentially distributed .",
    "thus , if @xmath3 independent identically distributed ( i.i.d . )",
    "replicas have first exit times @xmath4 , @xmath5 , then @xmath6 has the same law as @xmath7 . now if @xmath8 is the first replica which leaves the metastable state amongst all the replicas , then the simulation clock is advanced by @xmath9 , and this time agrees in law with the original process . in contrast , in the discrete time case , the exit times from metastable states are geometrically distributed .",
    "thus , if @xmath10 are now the geometrically distributed first exit times , then @xmath11 does not agree in law with @xmath12 .",
    "a different function of the @xmath10 must be found instead .",
    "this is our achievement with algorithm  [ algorithm1 ] and proposition  [ proposition1 ] .",
    "our algorithm is based on the observation that @xmath13 + \\min[i \\in \\{1 , \\ldots , n \\ } , \\ ,",
    "\\tau_i =   \\min(\\tau_1 , \\ldots , \\tau_n ) ] $ ] agrees in law with @xmath12 .",
    "this article is organized as follows . in section  [ qsd ] ,",
    "we formalize the notion of local equilibrium using qsds . in section  [ parrep ]",
    "we present our discrete time parrep algorithm , and in section  [ parrepmath ] we study its consistency . examples and a discussion follow in section  [ example ] .",
    "throughout this work , @xmath14 will be a time homogeneous markov chain with values in a probability space @xmath15 . for a random variable @xmath16 and probability measure @xmath17 , we write @xmath18 to indicate @xmath16",
    "is distributed according to @xmath17 . for random variables @xmath16 and @xmath19 , we write @xmath20 when @xmath19 is a random variable with the same law as @xmath16 .",
    "we write @xmath21 and @xmath22 $ ] to denote probabilities and expectations for the markov chain @xmath14 starting from the indicated initial distribution : @xmath23 . in the case",
    "that @xmath24 , we write @xmath25 and @xmath26 $ ] to denote probabilities and expectations for the markov chain starting from @xmath27 .    to formulate and apply parrep , we first need to define the metastable subsets of @xmath28 , which we will simply call _ states_. the states will be used to coarse - grain the dynamics .",
    "let @xmath29 be the collection of states , which we assume are disjoint bounded measurable subsets of  @xmath28 .",
    "we write @xmath0 for a generic element of @xmath29 , and @xmath30 for the quotient map identifying the states .",
    "as we will be concerned with when the chain exits states , we define the first exit time from @xmath0 , @xmath31 much of the algorithm and analysis depends on the properties of the qsd , which we now define .",
    "a probability measure @xmath32 with support in @xmath0 is a qsd if for all measurable @xmath33 and all @xmath34 , @xmath35    of course both @xmath36 and @xmath32 depend on @xmath0 , but for ease of notation , we do not make this explicit .",
    "the qsd can be seen as a local equilibrium reached by the markov chain , conditioned on the event that it remains in the state .",
    "indeed , it is easy to check that if @xmath32 is a measure with support in @xmath0 such that , @xmath37 then @xmath32 is the qsd , which is then unique . in section  [ qsdmath ] , we give sufficient conditions for existence and uniqueness of the qsd and for the convergence   to occur ( see theorem  [ theorem1 ] ) .",
    "we refer the reader to @xcite for additional properties of the qsd .",
    "using the notation of the previous section , the aim of the parrep algorithm is to efficiently generate a trajectory @xmath38 evolving in @xmath39 which has , approximately , the same law as the reference coarse - grained trajectory @xmath40 .",
    "two of the parameters in the algorithm  @xmath41 and @xmath42 , called the _ decorrelation _ and _ dephasing times _  depend on the current state @xmath0 , but for ease of notation we do not indicate this explicitly . see the remarks below algorithm  [ algorithm1 ] .",
    "[ algorithm1 ] initialize a reference trajectory @xmath43 .",
    "let @xmath3 be a fixed number of replicas and @xmath44 a fixed polling time at which the replicas resynchronize . set the simulation clock to zero : @xmath45 .",
    "a coarse - grained trajectory @xmath46 evolving in @xmath39 is obtained by iterating the following : [ html]e9f0e9 [ html]e9f0e9 [ html]e9f0e9    the idea of the parallel step is to compute the exit time from @xmath0 as the sum of the times spent by the replicas up to the first exit observed among the replicas .",
    "more precisely , if we imagine the replicas being ordered by their indices ( @xmath47 through @xmath3 ) , this sum is over all @xmath3 replicas up to the last polling time , and then over the first @xmath48 replicas in the last interval between polling times , @xmath48 being the smallest index of the replicas which are the first to exit .",
    "notice that @xmath49 and @xmath50 are such that @xmath51 $ ] .",
    "see figure  [ fig0 ] for a schematic of the parallel step .",
    "we comment that the formula for updating the simulation time in the parallel step of the original parrep algorithm is simply @xmath52 .",
    "a few remarks are in order ( see @xcite for additional comments on the continuous time algorithm ) :    the decorrelation step . : :    in this step , the reference trajectory is allowed to evolve until it    spends a sufficiently long time in a single state . at the termination    of the decorrelation step ,",
    "the distribution of the reference    trajectory should be , according to  , close to that of the qsd ( see    theorem  [ theorem1 ] in section  [ qsdmath ] ) .",
    "+    the evolution of the reference trajectory is _ exact _ in the    decorrelation step , and so the coarse - grained trajectory is also exact    in the decorrelation step .",
    "the dephasing step . : :    the purpose of the dephasing step is to generate @xmath3",
    "samples from the qsd .",
    "while we have described a simple    rejection sampling algorithm , there is another technique  @xcite based    on a branching and interacting particle process sometimes called the    fleming - viot particle process  @xcite . see ( * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ;    * ? ? ?",
    "* ; * ? ? ?",
    "* ) for studies of this process , and  @xcite for a    discussion of how the fleming - viot particle process may be used in    parrep .",
    "+    in our rejection sampling we have flexibility on where to initialize    the replicas .",
    "one could use the position of the reference chain at the    end of the decorrelation step , or any other point in @xmath0 .",
    "the decorrelation and dephasing times .",
    ": :    @xmath53 and @xmath54 must be    sufficiently large so that the distributions of both the reference    process and the replicas are as close as possible to the qsd , without    exhausting computational resources . @xmath54 and    @xmath53 play similar roles ,",
    "and they both depend    on the initial distribution of the processes in @xmath0 .",
    "+    choosing good values of these parameters is nontrivial , as they    determine the accuracy of the algorithm . in @xcite , the fleming - viot    particle process together with convergence diagnostics",
    "are used to    determine these parameters on the fly in each state .",
    "they can also be    postulated from some _ a priori _ knowledge ( e.g. , barrier height    between states ) , if available . the polling time .",
    ": :    the purpose of the polling time @xmath44 is to    permit for periods of asynchronous computation of the replicas in a    distributed computing environment . for the accelerated time to be    correct , it is essential that all replicas have run for at least as    long as replica @xmath48 . ensuring this requires    resynchronization , which occurs at the polling time .",
    "+    if communication amongst the replicas is cheap or there is little loss    of synchronization per time step , one can take    @xmath55 . in this case",
    ",    @xmath56 is the first exit time observed among the    @xmath3 replicas ,    @xmath57 ( so    @xmath58 ) and    @xmath59 .",
    "efficiency of the algorithm .",
    ": :    for the algorithm to be efficient , the states must be truly    metastable : within each state , the typical time to reach the qsd    ( @xmath53 and @xmath54 )    should be small relative to the typical exit time .",
    "if most states are    not metastable , then the exit times will be typically smaller than the    decorrelation times , and the algorithm will rarely proceed to the    dephasing and parallel steps .    +    the algorithm is consistent even if some or all the states are not    metastable .",
    "indeed , the states can be _ any _ collection of disjoint    sets .",
    "however , if these sets are not reasonably defined , it will be    difficult to obtain any gain in efficiency with parrep .",
    "defining the    states requires some _ a priori _ knowledge about the system .     while the crosses correspond to exit events .",
    "index @xmath48 is as defined as in algorithm  [ algorithm1 ] . here , @xmath49 cycles internal to the parallel step have taken place .",
    "the thicker lines correspond to the portions of the chains contributing to  @xmath60 .",
    ", width=377 ]",
    "the main result of this section , proposition  [ proposition1 ] , shows that the coarse - grained trajectory simulated in parrep is _ exact _ if the qsd has been exactly reached in the decorrelation and dephasing steps ; see equation   below .      before examining parrep",
    ", we give a condition for existence and uniqueness of the qsd .",
    "we also state important properties of the exit law starting from the qsd .",
    "many of these results can be found in  @xcite .",
    "we assume the following , which is sufficient to ensure existence and uniqueness of the qsd .",
    "[ assumption1 ] let @xmath61 be any state .    1 .   for any @xmath62 ,",
    "@xmath63 2 .",
    "there exists @xmath64 and @xmath65 , such that for all @xmath66 and all bounded non - negative measurable functions @xmath67 ,",
    "@xmath68 \\ge \\delta \\ , { \\mathbb e}^y\\left[f(x_m)1_{\\{\\tau >          m\\}}\\right].$ ]    with this condition , the following holds ( see ( * ? ? ?",
    "* theorem 1 ) ) :    [ theorem1 ] under assumption  [ assumption1 ] , there exists a unique qsd @xmath32 in @xmath0 .",
    "furthermore , for any probability measure @xmath17 with support in @xmath0 and any bounded measurable function @xmath69 ,",
    "@xmath70         -\\int_s f(x)\\,\\nu(dx ) \\right|      \\le \\|f\\|_{\\infty } \\,4 \\ , \\delta^{-1}(1-\\delta^2)^{\\lfloor",
    "n / m \\rfloor}.\\ ] ]    theorem  [ theorem1 ] shows that the law of @xmath14 , conditioned on not exiting @xmath0 , converges in total variation norm to the qsd @xmath32 as @xmath71 .",
    "thus , at the end of the decorrelation and dephasing steps , if @xmath53 and @xmath54 are sufficiently large , then the law of the reference process and replicas will be close to that of the qsd .",
    "notice that theorem  [ theorem1 ] provides an explicit error bound in total variation norm .",
    "next we state properties of the exit law starting from the qsd which are essential to our analysis .",
    "while these results are well - known ( see , for instance , @xcite ) , we give brief proofs for completeness .",
    "[ theorem2 ] if @xmath72 , with @xmath32 the qsd in @xmath0 , then @xmath36 and @xmath73 are independent , and @xmath36 is geometrically distributed with parameter @xmath74 .",
    "let @xmath75 denote the transition kernel of @xmath14 .",
    "we compute @xmath76      = \\frac{{\\mathbb e}^\\nu\\left[f(x_n)\\,1_{\\{\\tau = n\\}}\\right ] }      { { \\mathbb e}^\\nu\\left[1_{\\{\\tau = n\\}}\\right ] } & = \\frac{{\\mathbb          e}^\\nu\\left[1_{\\{\\tau > n-1\\}}\\int_{\\omega\\setminus s }          f(y)k(x_{n-1},dy)\\right ] }      { { \\mathbb e}^\\nu\\left[1_{\\{\\tau > n-1\\}}\\int_{\\omega\\setminus s } k(x_{n-1},dy)\\right]}\\\\      & = \\frac{{\\mathbb e}^\\nu\\left[\\int_{\\omega\\setminus s }          f(y)k(x_{n-1},dy)\\,\\big|\\,\\tau > n-1\\right ] }      { { \\mathbb e}^\\nu\\left[\\int_{\\omega\\setminus s } k(x_{n-1},dy)\\,\\big|\\ , \\tau > n-1\\right ] } \\\\      & = \\frac{\\int_s \\left(\\int_{\\omega\\setminus s }          f(y)k(x , dy)\\right)\\nu(dx ) } { \\int_s \\left(\\int_{\\omega\\setminus            s } k(x , dy)\\right)\\nu(dx ) } = { \\mathbb        e}^\\nu\\left[f(x_{\\tau})\\,|\\,\\tau = 1\\right ] .",
    "\\end{aligned}\\ ] ] the second to last equality is an application of .",
    "as @xmath77 $ ] is independent of @xmath78 , this establishes independence of @xmath36 and @xmath73 .    concerning the distribution of @xmath36 , we first calculate @xmath79 and then again use : @xmath80 } { { \\mathbb          p}^\\nu(\\tau > n-1 ) } & = \\frac{{\\mathbb e}^\\nu\\left[1_{\\{\\tau >            n-1\\}}\\int_s k(x_{n-1},dy)\\right ] } { { \\mathbb          p}^\\nu(\\tau > n-1)}\\\\      & = { \\mathbb e}^\\nu\\left[\\int_s k(x_{n-1},dy)\\,\\big|\\,\\tau >",
    "n-1\\right]\\\\      & = \\int_s \\left(\\int_s k(x , dy)\\right)\\,\\nu(dx ) = { \\mathbb        p}^{\\nu}(x_1 \\in s ) .    \\end{aligned}\\ ] ] thus , @xmath81 and by induction , @xmath82^n =    ( 1-p)^n$ ] .",
    "we can now state and prove our main result .",
    "we make the following idealizing assumption , which allows us to focus on the the parallel step in algorithm  [ algorithm1 ] , neglecting the errors due to imperfect sampling of the qsd .",
    "[ assumption2 ] assume that :    * after spending @xmath53 consecutive time steps in @xmath0 , the process @xmath14 is _ exactly _ distributed according to the qsd  @xmath32 in @xmath0 . in particular , at the end of the decorrelation step , @xmath83 .",
    "* at the end of the dephasing step , all @xmath3 replicas are i.i.d . with law _",
    "exactly _ given by @xmath32 .",
    "idealization  [ assumption2 ] is introduced in view of theorem  [ theorem1 ] , which ensures that the qsd sampling error from the dephasing and decorrelation steps vanishes as @xmath53 and @xmath54 become large . of course , for finite @xmath53 and @xmath54 , there is a nonzero error ; this error will indeed propagate in time , but it can be controlled in terms of these two parameters . for a detailed analysis in the continuous time case ,",
    "see  @xcite .",
    "though the arguments in  @xcite could be adapted to our time discrete setting , we do not go in this direction ; instead we focus on showing consistency of the parallel step .    under idealization  [ assumption2 ] , we show that parrep is _",
    "exact_. that is , the trajectory generated by parrep has the same probability law as the true coarse - grained chain : @xmath84 the evolution of the parrep coarse - grained trajectory is exact in the decorrelation step .",
    "together with idealization  [ assumption2 ] , this means   holds if the parallel step is consistent ( i.e. exact , if all replicas start at i.i.d . samples of the qsd ) .",
    "this is the content of the following proposition .",
    "[ proposition1 ] assume that the @xmath3 replicas at the beginning of the parallel step are i.i.d . with law _",
    "exactly _ given by the qsd @xmath32 in @xmath0 ( this is idealization [ assumption2]-(a2 ) ) .",
    "then the parallel step of algorithm  [ algorithm1 ] is exact : @xmath85 where @xmath86 is defined as in algorithm  [ algorithm1 ] , while @xmath87 is defined for @xmath14 starting at @xmath72 .    to prove proposition  [ proposition1 ] , we need the following lemma :    [ lemma1 ] let @xmath88 be i.i.d .",
    "geometric random variables with parameter @xmath89 : for @xmath90 , @xmath91 define @xmath92 then @xmath93 has the same law as @xmath94 .",
    "notice that @xmath93 can be rewritten as @xmath95.\\ ] ] indeed , any natural number @xmath96 can be uniquely expressed as @xmath97 where @xmath98 , @xmath99 and @xmath100 .",
    "for such @xmath101 , @xmath102 and @xmath103 we compute @xmath104^{n - k}\\\\    & = ( 1-p)^{(k-1)m{t_{\\rm poll}}}p(1-p)^{(m-1){t_{\\rm poll}}+t-1}(1-p)^{(n - k)(m-1){t_{\\rm poll}}}\\\\    & = p(1-p)^{n(m-1){t_{\\rm poll}}+ ( k-1){t_{\\rm poll}}+ t-1}= { \\mathbb      p}\\left(\\tau^{1 } = n(m-1){t_{\\rm poll}}+ ( k-1){t_{\\rm poll}}+ t\\right).\\end{aligned}\\ ] ]    we can now proceed to the proof of proposition  [ proposition1 ] .    in light of theorem  [ theorem2 ] , it suffices to prove :    * @xmath60 is a geometric random variable with parameter @xmath105 , * @xmath106 and @xmath107 have the same law : @xmath108 , and * @xmath60 is independent of @xmath106 ,    where @xmath14 is the process starting at the @xmath72 .",
    "we first prove _",
    "( i)_. for @xmath109 , let @xmath110 be a random variable representing the first exit time from @xmath0 of the @xmath111th replica in the parallel step of parrep , if the replica were allowed to keep evolving indefinitely . by ( a2 ) , @xmath112 are independent and all have the same distribution as @xmath36 .",
    "now by theorem  [ theorem2 ] , @xmath112 are i.i.d .",
    "geometric random variables with parameter @xmath89 , so by lemma  [ lemma1 ] , @xmath60 is also a geometric random variable with parameter @xmath89 .",
    "now we turn to _ ( ii ) _ and _ ( iii)_. note that @xmath113 if and only if @xmath114 and there exists @xmath115 such that @xmath116 , @xmath117 , and @xmath118 . from theorem  [ theorem2 ] and ( a2 ) , @xmath119 is independent of @xmath120 , so @xmath106 must be independent of @xmath48 . from this and ( a2 ) , it follows that @xmath121 . to see that @xmath106 is independent of @xmath60 ,",
    "let @xmath122 be the sigma algebra generated by @xmath48 and @xmath50 .",
    "knowing the value of @xmath48 and @xmath50 is enough to deduce the value of @xmath60 ; that is , @xmath60 is @xmath122-measurable .",
    "also , by the preceding analysis and theorem  [ theorem2 ] , @xmath123 is independent of @xmath122 . to conclude that @xmath60 and @xmath106 are independent , we compute for suitable test functions @xmath124 and @xmath125 : @xmath126&= { \\mathbb e}[{\\mathbb        e}[f({t_{\\rm acc}})g({x_{\\rm acc}})\\,|\\,\\sigma(k,\\tau^k)]]\\\\      & = { \\mathbb e}[f({t_{\\rm acc}}){\\mathbb        e}[g({x_{\\rm acc}})\\,|\\,\\sigma(k,\\tau^k)]]= { \\mathbb e}[f({t_{\\rm acc}})]\\ ,      { \\mathbb e}[g({x_{\\rm acc } } ) ] .",
    "\\end{aligned}\\ ] ]",
    "in this section we consider two examples .",
    "the first illustrates numerically the fact that the parallel step in algorithm  [ algorithm1 ] is consistent .",
    "the second shows typical errors resulting from a naive application of the original parrep algorithm to a time discretization of langevin dynamics .",
    "these are simple illustrative numerical examples . for a more advanced application",
    ", we refer to the paper  @xcite , where our algorithm  [ algorithm1 ] was used to study the 2d lennard - jones cluster of seven atoms .",
    "consider a random walk on @xmath127 with transition probabilities @xmath128 defined as follows : @xmath129 we use parrep to simulate the first exit time @xmath36 of the random walk from @xmath130 $ ] , starting from the qsd @xmath32 in @xmath0 . at each point except @xmath131 , steps towards @xmath131 are more likely than steps towards the boundaries @xmath132 or @xmath133 .",
    "we perform this simulation by using the dephasing and parallel steps of algorithm  [ algorithm1 ] ; for sufficiently large @xmath54 , the accelerated time @xmath60 should have the same law as @xmath36 . in this simple example we can analytically compute the distribution of @xmath36 .",
    "we perform @xmath134 independent parrep simulations to obtain statistics on the distribution of @xmath60 and the gain in `` wall clock time , '' defined below .",
    "we find that @xmath60 and @xmath36 have very close probability mass functions when @xmath135 ; see figure  [ fig1 ] . to measure the gain in wall clock efficiency using parrep",
    ", we introduce the parallel time @xmath136  defined , using the notation of algorithm  [ algorithm1 ] , by @xmath137 , where we recall @xmath49 is such that @xmath138 $ ] .",
    "thus , the wall clock time of the parallel step is @xmath139 , with @xmath140 the computational cost of a single time step of the markov chain for one replica .",
    "note in figure  [ fig2 ] the significant parallel time speedup in parrep compared with the direct sampling time .",
    "the speedup is approximately linear in  @xmath3 .    , estimated by @xmath134 parrep simulations with @xmath141 replicas and @xmath142 , vs. exact distribution of @xmath36 ( smooth curve ) . ,",
    "width=453 ]     and , from top : @xmath143 .",
    "the bottom curve is the ( analytic ) cumulative distribution function of @xmath36 ( corresponding to @xmath144).,width=453 ]      consider the overdamped langevin stochastic process in @xmath145 , @xmath146 the associated euler - maruyama discretization is @xmath147 where @xmath148 are @xmath149-dimensional i.i.d .",
    "random variables .",
    "it is well - known  @xcite that @xmath150 is then an approximation of @xmath151 .",
    "we first show that the conditions in assumption  [ assumption1 ] hold ( see  @xcite for a similar example in 1d ) :    [ prop1 ] assume @xmath152 is bounded and @xmath153 is bounded on @xmath0 . then satisfies assumption [ assumption1 ] .",
    "first , for any @xmath154 , @xmath155}&=        ( 4\\pi \\beta^{-1}\\delta t)^{-d/2}\\int_{{\\mathbb r}^d }    1_s(y )     \\exp{\\left\\{-\\frac{{\\left| y -x + \\nabla v(x ) \\delta t\\right|}^2}{4\\beta^{-1 } \\delta t } \\right\\}}dy\\\\        & \\geq |s|(4\\pi \\beta^{-1}\\delta t)^{-d/2 } \\min_{y\\in s}{\\left\\ {          \\exp{\\left\\{-\\frac{{\\left| y -x + \\nabla v(x ) \\delta                t\\right|}^2}{4\\beta^{-1 } \\delta t } \\right\\}}\\right\\ } } > 0 .      \\end{split}\\ ] ] next , for any @xmath156 , @xmath157 } & = ( 4\\pi \\beta^{-1}\\delta t)^{-d/2}\\int_s",
    "f(z )        \\exp{\\left\\{-\\frac{{\\left| z -x + \\nabla v(x ) \\delta",
    "t\\right|}^2}{4            \\beta^{-1 } \\delta t } \\right\\}}dz\\\\        & = ( 4\\pi \\beta^{-1}\\delta t)^{-d/2}\\int_s f(z ) \\exp{\\left\\{-\\frac{{\\left| z -y + \\nabla              v(y ) \\delta t\\right|}^2}{4            \\beta^{-1 } \\delta t } \\right\\ } }   \\\\        & \\quad \\times \\exp{\\left\\{-\\frac{{\\left| z -x + \\nabla v(x ) \\delta              t\\right|}^2- { \\left| z -y + \\nabla v(y ) \\delta t\\right|}^2}{4            \\beta^{-1 } \\delta t}\\right\\}}dz\\\\        & \\geq c ( 4\\pi \\beta^{-1}\\delta t)^{-d/2}\\int_s f(z ) \\exp{\\left\\{-\\frac{{\\left| z -y +              \\nabla",
    "v(y ) \\delta t\\right|}^2}{4            \\beta^{-1 } \\delta t } \\right\\ } }   dz\\\\        & \\quad = c(4\\pi \\beta^{-1}\\delta t)^{-d/2 } { \\mathbb e}^y{\\left[f(x_1)1_{\\{\\tau>1\\}}\\right ] }      \\end{split}\\ ] ] where @xmath158 since @xmath0 is bounded and terms in the brackets are bounded , @xmath159 . in assumption  [ assumption1 ]",
    "we can then take @xmath160 and @xmath161 .",
    "theorem  [ theorem1 ] ensures that @xmath14 converges to a unique qsd in @xmath0 , with a precise error estimate in terms of the parameters @xmath101 and @xmath162 obtained in the proof of proposition  [ prop1 ] .",
    "this error estimate is certainly not sharp ; better estimates can be obtained by studying the spectral properties of the markov kernel .",
    "we refer to  @xcite for such convergence results in the continuous time case  .      here",
    "we consider the 1d process @xmath163 discretized with @xmath164 .",
    "we compute the first exit time from @xmath165 , starting at @xmath166 .",
    "we use algorithm  [ algorithm1 ] with @xmath167 , corresponding to the physical time scale @xmath168 , and @xmath169 replicas .    consider a direct implementation of the continuous time parrep algorithm into the time discretized process . in that algorithm ,",
    "the accelerated time is ( in units of physical time instead of time steps ) @xmath170 with @xmath50 the same as in algorithm  [ algorithm1 ] above . as @xmath171 is by construction a multiple of @xmath172 , a staircasing effect",
    "can be seen in the exit time distribution ; see figure  [ f : per1d ] . this staggering worsens as the number of replicas increases . in our algorithm  [ algorithm1 ] , we use the accelerated time formula ( again in units of physical time ) @xmath173 we find excellent agreement between the serial data  that is , the data obtained from direct numerical simulation  and the data obtained from algorithm  [ algorithm1 ] .",
    "see figure  [ f : per1d ] .",
    "( the agreement is perfect in the decorrelation step ; see figure  [ f : per1d_zoom ] . )",
    "we comment further on this in the next section",
    ".     represents the first exit time from @xmath165 , starting at @xmath174 .",
    "there is excellent agreement between the serial , unaccelerated simulation data ( @xmath175 ) and our parrep algorithm ( @xmath176 ) , while the original parrep formula ( @xmath177 ) deviates significantly .",
    "dotted lines represent 95% clopper - pearson confidence intervals obtained from @xmath134 independent simulations ; confidence interval widths increase in @xmath103 as fewer samples are available.,width=377 ]    , highlighting the decorrelation step ( recall @xmath178 ) .",
    "serial simulation , our parrep algorithm , and the original parrep algorithm all produce identical data .",
    "this comes from the fact that serial and parrep simulations are identical in law during the decorrelation step .",
    ", width=377 ]      in light of the discretization example , one may ask what kind of errors were introduced in previous numerical studies which used parrep with . taking @xmath55 for simplicity , we calculate @xmath179 } = { \\mathbb e}{\\left[{\\left|   ( n ( \\tau^k-1 ) + k)\\delta",
    "t - n         \\tau^{k}\\delta t\\right|}\\right]}= \\delta t\\,{\\mathbb e}{\\left[{\\left| n - k\\right|}\\right]}= \\delta t\\sum_{k=1}^n ( n - k ) { \\mathbb p}(k = k).\\ ] ] using calculations analogous to those used to study @xmath60 , it can be shown that @xmath180 therefore the error in the number of time steps per parallel step is @xmath181 consider the relative error , writing it as @xmath182 } , \\text { where } r=1-p.\\ ] ] we claim the quantity in the brackets , @xmath183 is bounded from above by one . indeed , for any @xmath184",
    ", we immediately see that @xmath185 is zero at @xmath144 and one as @xmath186 .",
    "let us reason by contradiction and assume that @xmath187 .",
    "since @xmath124 is continuous in @xmath188 and @xmath184 , there is then a point @xmath189 such that @xmath190 ; thus @xmath191 note that @xmath192 and @xmath193 for all values of @xmath3 .",
    "computing the derivative with respect to @xmath194 , we observe @xmath195 therefore , @xmath196 is decreasing , from one at @xmath197 to zero at @xmath198 , in the interval @xmath199 .",
    "hence , @xmath200 has no solution , contradiction .",
    "we conclude that   is bounded from above by one .",
    "consequently , we are assured @xmath201 thus , so long as @xmath202 , the relative error using the accelerated time @xmath171 will be modest , especially for very metastable states where @xmath203 . if also @xmath204 , then the absolute error will be small .",
    "the above calculations are generic .",
    "though our discretized diffusion example in section  [ sec : diff1d ] is a simple 1d problem , the errors displayed in figure [ f : per1d ] are expected whenever the continuous time parrep rule   is used for a time discretized process .",
    "though this error ( as we showed above ) will be small provided @xmath205 and @xmath206 , our algorithm  [ algorithm1 ] has the advantage of being consistent for any @xmath207 , including relatively large values of @xmath208 .",
    "we would like to thank the anonymous referees for their many constructive remarks .",
    "the work of d. aristoff and g. simpson was supported in part by doe award de - sc0002085 .",
    "g. simpson was also supported by the nst pire grant oise-0967140 .",
    "the work of t. lelivre is supported by the european research council under the european union s seventh framework programme ( fp/2007 - 2013 ) / erc grant agreement number 614492 .",
    "b.  bouchard , s.  geiss , and e.  gobet .",
    "first time to exit of a continuous it process : general moment estimates and @xmath209-convergence rate for discrete time approximations , 2013 .",
    "preprint http://hal.archives-ouvertes.fr/hal-00844887 .",
    "e.  scoppola .",
    "metastability for markov chains : a general procedure based on renormalization group ideas . in g.",
    "grimmett , editor , _ probability and phase transition _ ,",
    "volume 420 of _ nato asi series _ , pages 303322 .",
    "springer netherlands , 1994 ."
  ],
  "abstract_text": [
    "<S> the parallel replica dynamics , originally developed by a.f . </S>",
    "<S> voter , efficiently simulates very long trajectories of metastable langevin dynamics . </S>",
    "<S> we present an analogous algorithm for discrete time markov processes . </S>",
    "<S> such markov processes naturally arise , for example , from the time discretization of a continuous time stochastic dynamics . appealing to properties of quasistationary distributions , </S>",
    "<S> we show that our algorithm reproduces exactly ( in some limiting regime ) the law of the original trajectory , coarsened over the metastable states . </S>"
  ]
}