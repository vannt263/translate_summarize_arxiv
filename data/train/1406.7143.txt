{
  "article_text": [
    "one dimensional single chirp signal , defined as @xmath0 for @xmath1 , is frequently used in different field of sciences , for example , sonar , radar , communications systems , as well as in oceanography and geology .",
    "one may see abatzoglou ( 1986 ) , kumaresan and verma ( 1987 ) , djuric and kay(1990 ) , gini et al .",
    "( 2000 ) , lin and djuric ( 2000 ) , lahiri et al .",
    "( 2012 , 2014 ) and the references cited therein for details .",
    "recently various types of parameter estimation techniques and their various properties have been studied for the signal ( [ eq1:model 1 ] ) , for example see kumaresan and verma ( 1987 ) , djuric and kay ( 1990 ) , gini et al .",
    "( 2000 ) , nandi and kundu ( 2004 ) , kundu and nandi(2008 ) , lahiri et al .",
    "( 2014 ) , saha and kay ( 2002 ) and references cited therein .",
    "kumerasan and verma ( 1987 ) used rank reduction technique for estimating parameters of the model .",
    "djuric and kay ( 1990 ) proposed a linear regression technique after phase unwrapping .",
    "gini et al .",
    "( 2000 ) used maximum likelihood ( ml ) technique as one of their estimation technique .",
    "saha and kay ( 2002 ) used ml technique on superimposed chirp signals .",
    "they have used mcmc importance sampling for find maximum likelihood estimates .",
    "lin et al .",
    "( 2004 ) has found the maximum likelihood estimates of the parameters of chirp signal using simulated annealing technique .",
    "it is seen that most of the methods concentrated on ml technique in recent past .",
    "recently some other techniques have drawn attention to the statistics community .",
    "for example , nandi and kundu ( 2004 ) first provided the asymptotic properties of least square estimates ( lse ) of the parameters involved in one dimensional chirp signal with i.  i.  d.  error structure .",
    "kundu and nandi ( 2008 ) extended those result in case of linear stationary errors with known auto covariance function .",
    "lahiri et al .",
    "( 2014 ) has used the least absolute deviation ( lad ) technique to find the estimates of the parameters involved in the model .",
    "they also gave the asymptotic properties of lad estimates under i.  i.  d.  error structure .",
    "although , similar to kundu and nandi ( 2008 ) , lahiri et al .",
    "( 2014 ) assumed that the error variance is known .",
    "therefore , it is seen that considerable amount of classical estimation techniques have been used for estimating the parameters of the chirp signal and their theoretical properties have studied in different circumstances for a while .",
    "some bayesian analysis of the chirp signal are found in the literature also .",
    "lin and djuric ( 2000 ) has done estimation of parameters of multiple of chirp signal using mcmc technique .",
    "however , they have only taken i.  i.  d.  error structure into account . moreover , it is important to mention that none of the methods , proposed so far , has taken the prediction issue into consideration .",
    "here we have analysed the one dimensional single chirp signal for forecasting in bayesian paradigm . to be precise , our main aim , in this paper ,",
    "is to predict a future observation through the bayesian analysis of one dimensional single chirp signal .",
    "the advantage of using the bayesian analysis for purpose of prediction is that it gives not only a single value or an interval , but also a complete density , which is known as posterior predictive density .",
    "it is also well known that the posterior predictive density is used for checking whether the model and the prior give a reasonable clarification of the uncertainty in a study ( see box e. p. george and tiao c. george ( 1973 ) and bickel j. peter and doksum a. kjella ( 2007 ) ) .",
    "to achieve posterior predictive density we have used mcmc technique suitably and in the path of getting posterior predictive density , posterior densities of the parameters involved in the model have been found as by product . using these posterior densities one may perform the bayesian inference of the parameters involved in the model , when required .",
    "the first part of the work mainly focuses on the i.  i.  d.  error structure where we have simulated four different samples from the model ( [ eq1:model 1 ] ) and have illustrated the mcmc based bayesian analysis of the model .",
    "moreover , this mcmc based bayesian method is applied on three different real data sets , obtained from http://archive.ics.uci.edu/ml , to see how our method is performing in practice , and in particular one of these three data set is used for multiple step forecasting . in second part , we deal with the dependent error structure though mcmc based methodologies with the same goal of forecasting .",
    "kundu and nandi ( 2008 ) has dealt with the model ( 1 ) assuming stationary error structure in great detail using classical inference , focusing estimation of the model parameters . however , in their numerical studies they have assumed that the auto covariance function ( acf ) is completely known . in our discussion",
    "it is assumed that the covariance structure of the error is exponentially decaying but unknown . in discrete time",
    ", it is known that exponentially decaying acf corresponds to stationary auto regressive process of order one ( ar(1 ) ) and kundu and nandi ( 2008 ) has presented the ar(1 ) example in their paper in numerical studies . with the same choice of the parameter values ,",
    "as done in kundu and nandi ( 2008 ) , a simulation study is done in our paper for purpose of illustration .",
    "the remaining part of the paper is designed as follows . in section 2",
    "we describe the parameter spaces and give a overview of mcmc based bayesian methodology . in subsection ( 2.1.1 )",
    "we provide the required details for gibbs sampling , used in getting sample from joint posterior of the parameters . in the next subsection ( 2.1.2 )",
    "prior specifications for the parameters are made and the full conditional density functions , which are required for gibbs sampling , are evaluated for the cases where the closed form of the conditional densities are available . in all other cases random walk mcmc is proposed ( see gamerman and lopes ( 2006 ) and liu , s. jun ( 2008 ) ) to update the parameters . in section 3",
    "we give the results of simulation studies based on our method , and in section 4 we show the performance of our method when applied to real data .",
    "section 5 deals with the dependent error structure where we assume an exponentially decaying covariance function with respect to time .",
    "finally we give conclusion and future work in section 6 .",
    "one dimensional single chirp signal ( defined as equation ( [ eq1:model 1 ] ) ) , assuming @xmath2 be random with @xmath3 = 0 , and var(@xmath2 ) = @xmath4 for all @xmath5 , has 5 parameters , namely , @xmath6 , @xmath7 , @xmath8 , @xmath9 and @xmath10 .",
    "following lahiri et .",
    "al ( 2012 ) we assume the following conditions on the parameters @xmath6 , @xmath7 , @xmath8 and @xmath9 :    1 .",
    "@xmath11 + @xmath12 @xmath13 , for some known real number @xmath14 .",
    "2 .   @xmath8 , @xmath9 @xmath15 .    for purpose of ease in computation , we further reparametrize the above structure as follows .",
    "we take @xmath6 = @xmath16 and @xmath7 = @xmath17 , with @xmath18 and @xmath19 @xmath20 @xmath21 $ ] . then obviously , @xmath22 = @xmath23 @xmath24 @xmath25",
    "therefore , we rewrite the model ( [ eq1:model 1 ] ) as , for @xmath26 = @xmath27 , @xmath28 with parameters @xmath29 , @xmath19 , @xmath8 , @xmath9 and @xmath10 along with their parameter spaces :    * @xmath29 @xmath20 @xmath30 . * @xmath19 @xmath20 @xmath21 $ ] . *",
    "@xmath8 , @xmath9 @xmath15 . *",
    "@xmath10 @xmath20 @xmath31 .",
    "it needs to be noted that lahiri et .",
    "al ( 2012 , 2014 ) assumed @xmath4 is known , unlike us .",
    "although nandi and kundu ( 2004 ) provided an estimate of @xmath4 in their theoretical study , but for numerical studies they took @xmath4 to be known .",
    "for purpose of bayesian analysis , we assume that the parameters are random and each having a prior distribution .",
    "our main goal is to get the posterior predictive distribution of @xmath32 given the data @xmath33 in bayesian paradigm .",
    "we assume that @xmath34 \\sim n(0,\\sigma^2_{\\mbox{\\scriptsize $ \\epsilon$}})$ ] , for all @xmath5 .",
    "given @xmath29 , @xmath19 ( i.e. , @xmath6 and @xmath7 ) , @xmath8 , @xmath9 and @xmath4 , @xmath33 = @xmath35 , follows multivariate normal distribution with e(@xmath36 = @xmath37 and cov(@xmath36 = @xmath4 @xmath38 , where @xmath37 = @xmath39 , with @xmath40 = @xmath41 , @xmath1 , and @xmath38 is an identity matrix of order @xmath42 .",
    "we want to find @xmath43 $ ] , the conditional distribution of @xmath32 given the data @xmath33 = @xmath35 .",
    "using augmentation technique @xmath43 $ ] can be written as @xmath44 = \\int \\int \\int \\int \\int [ y_{t+1}|{\\mbox{\\boldmath{$ y $ } } } , r,\\theta,\\alpha,\\beta,\\sigma_{\\mbox{\\scriptsize $ \\epsilon$ } } ] [ r,\\theta,\\alpha,\\beta,\\sigma_{\\mbox{\\scriptsize $ \\epsilon$}}|{\\mbox{\\boldmath{$ y $ } } } ] dr\\ , d\\theta\\ , d\\alpha\\ , d\\beta\\ , d\\sigma_{\\mbox{\\scriptsize $ \\epsilon$}}\\end{aligned}\\ ] ] it is not possible to get an analytical form to the above integration .",
    "therefore , simulation technique has to be implemented .",
    "details follow .    given a sample from @xmath45 $ ] ,",
    "@xmath46 $ ] will follow a normal distribution with mean @xmath47 and variance @xmath4 . hence ,",
    "once a sample is available from the posterior @xmath45 $ ] , the corresponding samples drawn from @xmath46 $ ] are from the posterior predictive ( [ eq3:posterior predictive ] ) , using which required posterior summaries can be obtained . to get the samples from @xmath45 $ ] , gibbs sampler method is used . in the next subsection",
    "we give a brief description how we apply gibbs sampler technique in the present situation .",
    "we denote the prior densities of @xmath48 , @xmath29 and @xmath4 as @xmath49 $ ] , @xmath50 $ ] , @xmath51 $ ] , @xmath52 $ ] and @xmath53 $ ] , respectively . assuming the independence of prior distributions we get the joint posterior density of @xmath29 , @xmath19 , @xmath8 , @xmath9 and @xmath54 given @xmath33 as    @xmath55 \\propto [ r][\\theta][\\alpha][\\beta][\\sigma_{\\mbox{\\scriptsize $ \\epsilon$}}^2 ] [ { \\mbox{\\boldmath{$ y $ } } } |r , \\theta , \\alpha , \\beta , \\sigma_{\\mbox{\\scriptsize $ \\epsilon$}}^2],\\end{aligned}\\ ] ]    for gibbs sampling the conditional distribution of each parameters given all others ( commonly known as full conditional distribution ) , denoted by @xmath56 $ ] , are needed and given by @xmath57 \\propto [ r][{\\mbox{\\boldmath{$ y $ } } } |r , \\theta , \\alpha , \\beta , \\sigma_{\\mbox{\\scriptsize $ \\epsilon$}}^2],\\end{aligned}\\ ] ] @xmath58 \\propto [ \\theta][{\\mbox{\\boldmath{$ y $ } } } |r , \\theta , \\alpha , \\beta , \\sigma_{\\mbox{\\scriptsize $ \\epsilon$}}^2],\\end{aligned}\\ ] ] @xmath59 \\propto [ \\alpha][{\\mbox{\\boldmath{$ y $ } } } |r , \\theta , \\alpha , \\beta , \\sigma_{\\mbox{\\scriptsize $ \\epsilon$}}^2],\\end{aligned}\\ ] ] @xmath60 \\propto [ \\beta][{\\mbox{\\boldmath{$ y $ } } } |r , \\theta , \\alpha , \\beta , \\sigma_{\\mbox{\\scriptsize $ \\epsilon$}}^2],\\end{aligned}\\ ] ] @xmath61 \\propto [ \\sigma_{\\mbox{\\scriptsize $ \\epsilon$}}^2][{\\mbox{\\boldmath{$ y $ } } } |r , \\theta , \\alpha , \\beta , \\sigma_{\\mbox{\\scriptsize $ \\epsilon$}}^2].\\end{aligned}\\ ] ]      we assume the following prior distributions on the parameters @xmath62 & \\sim \\mbox{uniform}(0,m)\\\\[1ex ] [ \\theta ] & \\sim \\mbox{uniform}(0,2\\pi)\\\\[1ex ] [ \\alpha ] & \\sim \\mbox{vonmises}(\\alpha_0,\\alpha_1)\\\\[1ex ] [ \\beta ] & \\sim \\mbox{vonmises}(\\beta_0,\\beta_1)\\\\[1ex ] [ \\sigma^2_{\\mbox{\\scriptsize $ \\epsilon$ } } ] & \\sim \\mbox{inverse gamma}(\\sigma_0,\\sigma_1)\\end{aligned}\\ ] ] the closed form of the full conditional densities of @xmath19 , @xmath8 , @xmath9 can not be obtained in closed form .",
    "so , we have used random walk mcmc to update these parameters .",
    "however , the conditional density of @xmath29 given all the others , i.e. , @xmath63 $ ] follows a truncated normal distribution with truncation between @xmath64 and with the mean parameter @xmath65 ) = \\frac{\\sum_{t=1}^{t } y_t \\left(\\cos \\theta \\cos(\\alpha t+\\beta t^2 ) + \\sin \\theta \\sin(\\alpha t+\\beta",
    "t^2)\\right)}{\\sum_{t=1}^{t } \\left(\\cos \\theta \\cos(\\alpha t+\\beta t^2 ) + \\sin",
    "\\theta \\sin(\\alpha t+\\beta t^2)\\right)^2}\\ ] ] and variance parameter @xmath66 )   = \\frac{\\sigma^2_{\\mbox{\\scriptsize $ \\epsilon$}}}{\\sum_{t=1}^{t } \\left(\\cos \\theta \\cos(\\alpha t+\\beta t^2 ) + \\sin \\theta \\sin(\\alpha t+\\beta t^2)\\right)^2}.\\ ] ] ( the proof is given in the appendix )",
    ". moreover , it is straightforward to see that @xmath4 given all the others , i.e. , @xmath67 $ ] follows inverse gamma distribution with the parameters @xmath68 and @xmath69 .",
    "in this section we have done four simulation studies to illustrate our method .",
    "we have given the true values of the parameters of simulated samples taken for our experiment in the table [ table1:simu details ] .",
    "in each of four samples we keep the last observation for purpose of prediction .",
    "so , we have basically 100 observations for first three samples and 19 observation for last sample .",
    "we have applied the random walk mcmc algorithm for updating parameters @xmath19 , @xmath8 , and @xmath9 . for all practical purposes the true values of @xmath14",
    "is not known so , we decide to take a sufficiently large value of @xmath14 to be in safe side .",
    "we choose @xmath14 to be equal to @xmath70 .",
    "to run mcmc simulations it is needed to choose the prior parameters appropriately . for choosing mean directions in the prior distributions of @xmath8 ,",
    "@xmath9 special technique is used .",
    "loglikelihood function is maximized using simulated annealing technique with respect to the parameters @xmath8 and @xmath9 ( for details see robert , p. and casella , g. ( 2004 ) and liu , s. ( 2008 ) ) , separately , and these values are used as initial values for mcmc iterations as well , for @xmath8 and @xmath9 , respectively . below we discuss about the choice of prior parameters for @xmath71 , @xmath8 and @xmath9 in details .",
    ".description of parameters for simulated samples [ cols=\"^,^,^,^,^,^,^ \" , ]     with the above choice of hyper parameters , @xmath72 mcmc iteration have been done with burning period @xmath73 .",
    "we use the normal random walk proposal with variance @xmath74 to update @xmath19 , @xmath8 , and @xmath9 , for all these four simulated samples . the choice of this variance is set based on a pilot run of mcmc iteration .",
    "we mention here that once the sample observations are obtained from @xmath29 and @xmath19 , we transform the sample values to that of @xmath6 = @xmath75 and @xmath7 = @xmath76 .",
    "details about the results of mcmc iteration for each of the sample are discussed here .",
    "posterior densities along with the true values are provided in the figures ( [ fig : post of a , b , alpha , beta , sigma for sample1 ] ) , ( [ fig : post of a , b , alpha , beta , sigma for sample2 ] ) , ( [ fig : post of a , b , alpha , beta , for sample3 ] ) and ( [ fig : post of a , b , alpha , beta , for sample4 ] ) , for sample 1 , 2 , 3 and 4 , respectively . except for @xmath9 in the figure ( [ fig : post of a , b , alpha , beta , for sample4 ] ) , all other true values are well within the high probability region .",
    "we have taken only 20 observations for sample 4 .",
    "so , it is not unusual to notice such an incident , specially when the posteriors are not unimodal .",
    "it can also be noted that as soon as the number of observations are increased to @xmath70 , the problem of @xmath9 is solved ( figure ( [ fig : post of a , b , alpha , beta , for sample3 ] ) ) .",
    "true signals along with 95% credible intervals , obtained based on mcmc simulations , are provided in the figure ( [ fig : fit for sample1,2,3,4 ] ) .",
    "it is seen that in all the cases the true signal falls well within the 95% credible intervals . finally , posterior predictive densities for @xmath77th observations of samples 1 , 2 , 3 and @xmath78th observation of sample 4 , are given in the figure ( [ fig : post predictive for sample1,2,3,4 ] ) .",
    "it is seen that true future values are well within the 95% credible interval in each of the cases , which is our main aim for this paper .",
    "three real data sets have been taken from http://archive.ics.uci.edu/ml of which two are of type sonar rocks and one is of type sonar mines .",
    "each signal contains 60 observations .",
    "bache , k. and lichman , m. ( 2013 ) mainly used the data for classification purpose .",
    "they got sonar signals from two different substances one is mine , other is rocks . here",
    "we use these data sets for showing performance of our method for purpose of one step and multiple step forecasting .",
    "first we consider two different signals , the one from sonar mine and one of the two signals from sonar rock and keep the last observations for purpose of prediction .",
    "therefore , we use @xmath79 observations for our analysis for the two above mentioned signals .",
    "we analyse another sonar rock signal in a different mode , in the sense that we keep last 5 observations for purpose of multiple step forecasting .",
    "that means that for this data set we only use @xmath80 observations for analysis . for first two signals ( the sonar mine and one of the sonar rock , for which @xmath79 observations are considered for analysis ) , we give the 95% credible interval based on sample observations obtained from mcmc simulations for purpose of fitting and the posterior predictive densities for purpose of prediction . for the last sonar rock signal five posterior predictive densities",
    "are given to show how more than one true future values being captured by 95% credible intervals .",
    "we follow the same path for choosing the prior parameter values for @xmath10 as we have done in the case of simulated samples . here in particular",
    "we choose the value of @xmath81 ( prior mean ) to be @xmath82 , @xmath83 and @xmath84 for the three data sets respectively .",
    "@xmath85 has set to be @xmath86 as earlier , so that the variance becomes half of the square of the mean , for all the data sets considered here .",
    "accordingly we find the values of @xmath87 for each cases .",
    "the above choice of means have been done after running a pilot mcmc iterations .    for @xmath8 as well as for @xmath9 ,",
    "the scale parameters for vonmises distributions have been chosen to be @xmath88 for each of the data sets .",
    "the mean directions of vonmises for @xmath8 have been set to be @xmath89 and @xmath90 for the sonar mine signal and the first sonar rock signal , respectively .",
    "similarly , for @xmath9 , we choose the mean directions of vonmises distributions to be @xmath91 and @xmath92 for the sonar mine signal and the first sonar rock signal , respectively .",
    "these values are obtained based on a small iteration ( @xmath93 iterations ) of simulated annealing technique , separately , on @xmath8 and @xmath9 for each of the data sets .",
    "finally , for the second sonar rock signal ( in which case @xmath80 observations are considered for analysis ) , the choice of the mean directions for @xmath8 and @xmath9 are taken to be @xmath94 and @xmath95 , obtained as a result of small number of iterations ( @xmath93 iterations ) of simulated annealing . for @xmath29 ,",
    "the value of @xmath14 needs to be given however , the true value of @xmath14 is not known here so , we choose a large value of @xmath14 , @xmath70 , for all these real data sets .    with these choices of the prior parameters we run @xmath72 mcmc iterations with burning period @xmath73 , and the following results are noted .",
    "figure ( [ fig : sonar_data_mines_1_2_rocks_3_fit ] ) provides the 95% fit for the sonar mine signal , and the first sonar rock signal based on 59 observations .",
    "there are 60 observations for each of these signals .",
    "we have taken 59 observations for purpose of fitting and have kept 60th observation for prediction purpose .",
    "figure ( [ fig : sonar_data_1_2_posterior_predictive_rocks_3_predictive ] ) gives the posterior predictive densities of 60th observations for the sonar mine signal and the first sonar rock signal .",
    "we have noted from figure ( [ fig : sonar_data_mines_1_2_rocks_3_fit ] ) that 95% credible intervals mostly contain the true signals in both the two cases .",
    "95% credible completely contains the true sonar rock signal .",
    "however , three true observations ( 5th , 29th and 54th ) fall outside the 95% credible interval for the sonar mine signal ( first graph of figure ( [ fig : sonar_data_mines_1_2_rocks_3_fit ] ) ) . at the same time",
    "it is noticed that the pattern of the signal has been best captured for the sonar mine signal . on the other hand , from figure ( [ fig : sonar_data_1_2_posterior_predictive_rocks_3_predictive ] ) it is observed that the true values of 60th observation fall well within the credible intervals for each of the two signals , the sonar mine signal and the first sonar rock signal .",
    "the second sonar rock signal , consisting of 60 observations , is analysed as follows .",
    "we keep first 55 observations as the known data and last 5 observations for purpose of multiple step prediction , as discussed earlier .",
    "now , in figures ( [ fig : sonar rocks predictions_1st three ] ) and ( [ fig : sonar rocks predictions_last two ] ) the five posterior predictive densities are given for last five observations , respectively .",
    "it is interesting to observe that true values of 56th , 57th , 58th , 59th and 60th observations fall well within the 95% credible region .",
    "it is notable to see that even with only @xmath80 observations we can predict next @xmath96 observations in a reasonable way .",
    "in this section we assume that @xmath97 = @xmath98 , given @xmath54 and @xmath99 has a multivariate normal distribution with mean @xmath100 and covariance matrix @xmath101 where @xmath102 = @xmath103 is the correlation matrix of order @xmath42 , with the following structure @xmath104 \\exp{(-\\rho |i - j| ) } & \\mbox { otherwise } , \\end{cases } \\label{eq30:correlation of epsilon}\\ ] ] with @xmath99 @xmath20 @xmath31 . under the above assumptions",
    "@xmath105 $ ] follows a multivariate normal distribution with the mean parameter @xmath37 = @xmath106 ( @xmath40 is equal to @xmath41 , for @xmath1 ) and the covariance matrix @xmath107 of order @xmath42 .",
    "given a data @xmath33 , our main aim is to get a posterior predictive density of @xmath32 for one step forecasting .",
    "the density of @xmath43 $ ] can be written as @xmath108 = \\int \\int \\int \\int \\int \\int [ y_{t+1}|{\\mbox{\\boldmath{$ y $ } } } , r,\\theta,\\alpha,\\beta,\\sigma_{\\mbox{\\scriptsize $ \\epsilon$}},\\rho ] [ r,\\theta,\\alpha,\\beta,\\sigma_{\\mbox{\\scriptsize $ \\epsilon$}},\\rho|{\\mbox{\\boldmath{$ y $ } } } ] \\ , dr\\ , d\\theta\\ , d\\alpha\\ , d\\beta\\ , d\\sigma_{\\mbox{\\scriptsize $ \\epsilon$}}\\ , d\\rho,\\ ] ] as done in equation ( [ eq3:posterior predictive ] ) for independent error structure .",
    "it has to be noted that now the number of parameter increases to @xmath109 from @xmath96 ( the number of parameters present in the i.  i.  d.  case ) .",
    "as mentioned earlier in section ( [ 2.1:mcmc mth ] ) , it is not possible to get an analytical form of the above integration .",
    "the same simulation technique , as done in ( [ 2.1:mcmc mth ] ) is implemented here .",
    "it is easily seen that @xmath110 $ ] follows a normal distribution with mean @xmath111 and variance @xmath112 where @xmath47 = @xmath113 + @xmath114 and @xmath115 is a vector of order @xmath116 , containing the covariances between @xmath32 and @xmath117",
    ". therefore , once a sample is available from the posterior @xmath118 $ ] , the corresponding samples drawn from @xmath46 $ ] are from the posterior predictive ( [ eq22:post pred depend ] ) .",
    "it is good to mention here that samples can be generated from @xmath119 $ ] as well for multiple step forecasting , with a little generalization of augmentation technique , adding each simulated @xmath120 to the previous set of data @xmath121 to get @xmath122 , denoted by @xmath123 , for @xmath124 = @xmath125 .",
    "then the above mcmc technique can be used . to be precise , at each augmentation stage",
    "a single mcmc sample is required to draw from @xmath126 $ ] and once this sample is generated , it is easy to obtain a sample from @xmath127 $ ] , denoted as @xmath128 $ ] , because @xmath128 $ ] will follow a normal distribution with mean @xmath129 and variance @xmath130 where @xmath131 is the mean at time @xmath132 , @xmath133 is the expectation of @xmath123 , @xmath134 is the correlation matrix of @xmath123 , and @xmath135 is a vector of order @xmath136 containing the covariances between @xmath137 and @xmath123 .",
    "we give details of simulations in one step forecasting here which can be easily generalized for multiple step forecasting . to get the sample from @xmath118 $ ] we use gibbs sampler technique as earlier .",
    "assuming the independence of the prior distributions , @xmath118 $ ] can be written as @xmath118 $ ] @xmath138 @xmath52 $ ] @xmath51 $ ] @xmath49 $ ] @xmath50 $ ] @xmath139 $ ] @xmath140 $ ] @xmath141 $ ] .",
    "the choice of the prior distributions for @xmath142 and @xmath10 is taken to be the same as done in section ( [ 2.1.2:priors ] ) . for @xmath99",
    ", we assume that @xmath143 \\sim \\mbox{gamma}(\\rho_0,\\rho_1)\\ ] ] and obtain the full conditional density of @xmath99 as @xmath144 \\propto [ \\rho ] [ { \\mbox{\\boldmath{$ y $ } } } |\\ldots].\\ ] ] the forms of the full conditional distributions of @xmath29 , @xmath19 , @xmath8 , @xmath9 and @xmath10 remain the same as equations ( [ eq7:posterior of r ] ) , ( [ eq8:posterior of theta ] ) , ( [ eq9:posterior of alpha ] ) , ( [ eq10:posterior of beta ] ) , ( [ eq11:posterior of sigma ] ) , respectively . in the current scenario",
    "also , the closed form of the full conditional densities are available only for @xmath54 and @xmath29 . for rest of the parameters we use the normal random walk mcmc with variance @xmath74 , as earlier , for updating .",
    "the full conditional distribution of @xmath29 , @xmath63 $ ] , turns out to be a truncated normal distribution , truncation between @xmath64 , with mean @xmath145 and variance @xmath146 where @xmath147 is such that @xmath37 = @xmath29 @xmath147 ( the proof is given in appendix ) .",
    "it is easy to seen that the full conditional distribution of @xmath54 is the inverse gamma distribution with the parameters @xmath68 and @xmath148 .    with",
    "the above discussion a simulation study has been done here . for simulation of the data we choose the values of the parameters as @xmath6",
    "= @xmath149 , @xmath7 = @xmath150 , @xmath8 = @xmath151 , @xmath9 = @xmath152 , @xmath54 = @xmath74 and @xmath99 = @xmath153 . the choice of the above parameter values are motivated from kundu and nandi ( 2008 ) .",
    "the choice of the prior parameters are decided as before , that is , for @xmath8 and @xmath9 we run a small number of iterations ( @xmath93 ) of simulated annealing technique to maximize the loglikelihood , separately with respect to @xmath8 and @xmath9 and the values are found to be @xmath154 and @xmath155 for @xmath8 and @xmath9 , respectively .",
    "we take these values of @xmath8 and @xmath9 as initial values for mcmc iterations as well .    for @xmath54 we set the mean of the prior density , denoted by @xmath81 , to be @xmath156 and @xmath85 to be @xmath157 as done earlier ( in section ( [ 3:simu study ] ) ) and",
    "accordingly we evaluate the values of the @xmath85 and @xmath87 . in case of @xmath99 we set the mean of prior density , @xmath158 , 1 and",
    "the @xmath159 is chosen to be @xmath160 , so that the variance , @xmath161 , becomes one half of the mean .",
    "accordingly we get the values of @xmath162 is calculated .    using the above choice of prior parameters",
    "@xmath72 mcmc iterations has been done with @xmath73 burning period .",
    "here we discuss the outcomes of the experiment .",
    "figure ( [ fig : post of a , b , alpha , beta , sigma , rho for dependent sample ] ) provides the posterior densities of @xmath6 , @xmath7 , @xmath8 , @xmath9 , @xmath10 and @xmath99 .",
    "it is observed that the true values of these parameters fall in high probability region in each of the cases .",
    "finally , the posterior predictive density for @xmath77th observation and the 95% credible interval obtained from simulated sample for the true signal are provided in the figure ( [ fig : posterior predictive and fit for dependent error ] ) .",
    "it is seen that the true future value is well within the 95% credible interval of the posterior density , indicating the usefulness of our mcmc based method for forecasting .",
    "it is also seen that the true signal is almost always contained in the 95% credible interval obtained based on samples from mcmc simulations , except for three values , namely , @xmath163st , @xmath164th and @xmath165nd observations . @xmath163st and @xmath164th true values fall below the 2.5% interval and @xmath165nd observation fall above the 97.5% interval .",
    "however , the pattern of the true signal is very nicely described by the intervals .",
    "in this paper we have shown that using appropriate mcmc simulation technique one can successfully forecast one or more future observations based on the available data on one dimensional single chirp signal in bayesian paradigm .",
    "( in this regard we have considered both independent error covariance structure as well as dependent error covariance structure . for independent error covariance structure , ) we have seen that for simulated as well as for real data our method has performed very well for purpose of forecasting . in simulation studies",
    "we have considered four different samples with different values of parameters .",
    "we have kept the last observation for purpose of forecasting for each of these samples .",
    "it is observed that true future values for different samples fall within the 95% credible intervals in all these cases .",
    "moreover , in these simulation studies we have shown that true values of the parameters fall in high probability regions of the posterior densities in most of the cases .",
    "for the case where we have used only @xmath78 observations , true value of @xmath9 has fallen in a very low probability region in posterior density of @xmath9 ( between the two modes of the posterior density of @xmath9 ) , which is clearly because of small sample size ( note that as soon as we increase the sample size to @xmath70 with the same set of values of parameters , posterior density of @xmath9 has rightly captured the true value of @xmath9 ) . here",
    "we once again emphasise that we take @xmath10 to be unknown unlike lahiri et al .",
    "( 2012 , 2014 ) .",
    "this gives more freedom for using our method in practice .",
    "mcmc simulation technique has been applied on three real data sets , taken from [ http://archive.ics.uci.edu/ml ] , to see how the method is performing in practice . in this",
    "website data on two different types of signal are available , e.g. , sonar mine signal and sonar rock signal .",
    "there are @xmath166 observations available corresponding to each signal . for our experiment",
    "we choose one sonar mine signal and two sonar rock signal .",
    "we apply mcmc iteration on @xmath79 observations for the sonar mine and one of the sonar rock signals , keeping the last observation for purpose of forecasting .",
    "@xmath80 observations are taken for purpose of analysis in case other sonar rock signal .",
    "we keep @xmath96 observations for purpose of prediction to show how the method is working for more than one future observations .",
    "we have observed that in case of the sonar mine signal and the first sonar rock signal , posterior predictive densities have nicely captured the true values of @xmath166th observations and the fitting to the signals are extremely well , in the sense that 95% credible intervals , obtained from mcmc simulations , have nicely captured the true signal . for the second sonar rock data ,",
    "we have observed that the all five true future observations have fallen within the @xmath167 credible region of the posterior predictive densities .",
    "it is encouraging to note that with only @xmath80 observations , using mcmc iteration technique suitably , one can predict more than one observations in a significant manner , for one dimensional single chirp signal . through out these experiments ,",
    "simulated annealing technique is used to get the initial values and mean directions of the prior distribution for the parameters @xmath8 and @xmath9 . for dependent error covariance",
    ", we consider the covariance being exponentially decaying proportional to lag difference in this paper .",
    "this special covariance structure corresponds to the case of auto regressive process with lag one on errors in discrete time domain ( chatfiled , 2003 ) .",
    "kundu and nandi ( 2008 ) have done a theoretical and numerical study on the stationary error structure .",
    "however , they have assumed that the auto covariance function is fully known , that is , they have taken the error variance and covariance to be known .",
    "we , on the contrary , take @xmath54 and @xmath99 as unknowns and assuming prior densities on these , come up with posterior densities of @xmath168 and @xmath99 . in the numerical example",
    ", we have shown that the true values fall in high probability region for both @xmath54 and @xmath99 , respectively .",
    "this is clearly an improvement over the previous work .    for future work",
    "we will consider multiple chirp signal for purpose of bayesian analysis and forecasting .",
    "one dimensional multiple chirp signal is defined as @xmath169 for details of multiple signal one may see saha and kay ( 2002 ) , kundu and nandi ( 2008 ) etc . in most of the cases",
    "@xmath170 is assumed to be known .",
    "we will consider @xmath170 as unknown and will perform ttmcmc ( das and bhattachariya , 2014 ) for purpose of estimation and forecasting in bayesian paradigm for our future work .",
    "the author is very grateful to moumita das for her insightful comments and suggestions which improve the paper in every aspect .",
    "the author is also grateful to the assistance obtained from uci machine learning data repository ( http://archive.ics.uci.edu/ml ) for the real data sets .",
    "abatzoglou , t. ( 1986 ) . fast maximum likelihood joint estimation of frequency and frequency rate . _ ieee transactions on aerospace and electronic systems _ , * 22 * , 708 - 714 .    bache , k. and lichman , m. ( 2013 ) .",
    "uci machine learning repository [ http://archive.ics.uci.edu/ml ] .",
    "irvine , ca : university of california , school of information and computer science .",
    "bickel j. piter and doksum a. kjella .",
    "( 2007 ) mathematical statistics , vol.i , 2nd ed . , pearson prentice hall",
    ".    box e. p. george and tiao c. george .",
    "( 1973 ) bayesian inference in statistical analysis , addison wesley publishing co.    chatfield , christopher .",
    "( 2003 ) the analysis of time series : an introduction .",
    "chapman & hall / crc .",
    "das , m. and bhattacharya , s. ( 2014 ) .",
    "transdimensional transformation based markov chain monte carlo . submitted .",
    "djuric , p.m. and kay , s.m .",
    "( 1990 ) , parameter estimation of chirp signals . _ ieee transactions on acoustics , speech and signal processing _ , * 38 * , 2118 - 2126 .    gamerman , d. and lopes f. hedibert ( 2006 ) .",
    "markov chain monte carlo : stochastic simulation for bayesian inference , 2nd ed .",
    "chapman & hall / crc .",
    "gini , f. , montanari , m. and verrazzani , l. ( 2000 ) .",
    "estimation of chirp signals in compound gaussian clutter : a cyclostationary approach .",
    "_ ieee transactions on acoustics , speech and signal processing _ , * 48 * , 1029 - 1039 .    kundu , d. and nandi , s. ( 2008 ) .",
    "parameter estimation of chirp signals in presence of stationary noise .",
    "_ statistica sinica _ , * 18 * , 187 - 201 .",
    "kumaresan , r. and verma , s. ( 1987 ) . on estimating the parameters of chirp signals using rank reduction techniques .",
    "_ proceedings of 21 st asilomar conference _ , 555 - 558 , pacific grove ,",
    "california .",
    "lahiri , a. , kundu d. and mitra a. ( 2012 ) .",
    "efficient algorithm for estimating the parameters of chirp signal . _ journal of multivariate analysis _ , * 108 * , 15 - 27 .",
    "lahiri a. , kundu d. , mitra a. ( 2014 ) .",
    "on least absolute deviation estimator of one dimensional chirp model .",
    "_ statistics _ , * 48 * , no . 2 , 405 - 420 , 2014 .",
    "lin c.c . and djuric p.m. ( 2000 ) .",
    "estimation of chirp signals by mcmc .",
    "ieee conf .",
    "acoustics , speech and signal processing ( icassp ) _ , istambul , turkey , * 1 * , 265268 .",
    "lin , y. , peng , y. and wang , x. maximum likelihood parameter estimation of multiple chirp signals by a new markov chain monte carlo approach .",
    "_ proc . of the ieee , radar conf . _",
    "* 1 * , 559 - 562 .",
    "liu , j.s .",
    "_ monte carlo strategies in scientific computing _ , springer .",
    "nandi , s. and kundu , d. asymptotic properties of the least squares estimators of the parameters of the chirp signals . _",
    "annals of the institute of statistical mathematics _ , * 56 * , no . 3 , 529 - 544 .",
    "robert , c. p. and casella , g. ( 2004 ) .",
    "_ monte carlo statistical methods _ , 2nd ed .",
    "springer .",
    "saha , s. and kay , s. m. ( 2002 ) . maximum likelihood parameter estimation of superimposed chirps using monte carlo importance sampling .",
    "_ ieee trans . signal process _ , * 50 * , 224 - 230 .",
    "[ thm1:full conditional of r ] let @xmath171 @xmath172 @xmath173 , where @xmath37 = @xmath39 , with @xmath40 = @xmath41 , @xmath1 , and @xmath38 be the identity matrix of order @xmath42 .",
    "also we assume that @xmath29 @xmath20 @xmath64 , for some known real @xmath14 and @xmath52\\sim \\mbox { uniform } ( 0,m)$ ]",
    ". then @xmath174 $ ] , denoted as @xmath63 $ ] , follows a truncated normal distribution with truncation between @xmath64 and with the mean parameter and variance parameter as specified in equation @xmath175 and @xmath176 , respectively .",
    "we note that @xmath177 & \\propto [ r][{\\mbox{\\boldmath{$ y $ } } } |\\ldots ] \\\\ & \\propto \\exp \\left[-\\frac{1}{2\\sigma^2_{\\mbox{\\scriptsize $ \\epsilon$}}}\\sum_{t=1}^{t}\\left\\{y_t - r(\\cos \\theta \\cos(\\alpha t+\\beta t^2)+\\sin \\theta",
    "\\sin(\\alpha t+\\beta t^2))\\right\\}^2\\right]\\chi_{(0,m)}(r)\\end{aligned}\\ ] ] we simplify the exponent term ( without @xmath178 ) below .",
    "therefore , @xmath177 & \\propto \\exp \\left[-\\frac{1}{\\sigma_r^2 } \\left\\ { r-\\frac{\\sum_{t=1}^{t } y_t \\left(\\cos \\theta \\cos(\\alpha t+\\beta t^2)+\\sin \\theta \\sin(\\alpha t+\\beta t^2)\\right)}{\\sum_{t=1}^{t } \\left(\\cos \\theta \\cos(\\alpha t+\\beta t^2)+\\sin \\theta \\sin(\\alpha t+\\beta t^2)\\right)^2}\\right\\}^2\\right]\\chi_{0,m}(r),\\end{aligned}\\ ] ] where @xmath180 hence the proof follows",
    ".    [ thm2:full conditional of r in dependent case ] let @xmath181 @xmath172 @xmath182 , where @xmath37 = @xmath39 , with @xmath40 = @xmath41 , @xmath1 , and @xmath102 be the correlation matrix or order @xmath42 , with the elements as specified in equation @xmath183 .",
    "also we assume that @xmath29 @xmath20 @xmath64 , for some known real @xmath14 and @xmath52\\sim \\mbox { uniform } ( 0,m)$ ] .",
    "then @xmath184 $ ] , denoted as @xmath63 $ ] , follows a truncated normal distribution with truncation between @xmath64 and with the mean parameter and variance parameter as specified in equation @xmath185 and @xmath186 , respectively .",
    "we follow similar line of proof as done in theorem ( [ thm1:full conditional of r ] ) . writing @xmath37 = @xmath187 , we clearly observe that @xmath177 \\propto \\exp\\left(-\\frac{1}{2\\sigma^2_{\\mbox{\\scriptsize $ \\epsilon$}}}({\\mbox{\\boldmath{$ y $ } } } -r{\\mbox{\\boldmath{$ b $ } } } _ { t})'\\delta_{t}^{-1}({\\mbox{\\boldmath{$ y $ } } } -r{\\mbox{\\boldmath{$ b $ } } } _ { t})\\right ) \\chi_{(0,m)}(r ) \\end{aligned}\\ ] ] after simplifying the exponent term it is readily seen that @xmath177 \\propto \\exp\\left[-\\frac{{\\mbox{\\boldmath{$ b $ } } } _ { t}'\\delta_{t}^{-1}{\\mbox{\\boldmath{$ b $ } } } _ { t}}{2\\sigma^2_{\\mbox{\\scriptsize $ \\epsilon$}}}\\left(r-\\frac{{\\mbox{\\boldmath{$ y $ } } } ' \\delta_{t}^{-1}{\\mbox{\\boldmath{$ b $ } } } _ { t}}{{\\mbox{\\boldmath{$ b $ } } } _ { t}'\\delta_{t}^{-1}{\\mbox{\\boldmath{$ b $ } } } _ { t}}\\right)^2\\right]\\chi_{(0,m)}(r)\\end{aligned}\\ ] ] hence the proof follows .    , @xmath7 , @xmath8 , @xmath9 and @xmath188 for sample 1 of table 1 , where true values are indicated with vertical lines.,title=\"fig:\",width=144,height=144 ] , @xmath7 , @xmath8 , @xmath9 and @xmath188 for sample 1 of table 1 , where true values are indicated with vertical lines.,title=\"fig:\",width=144,height=144 ] , @xmath7 , @xmath8 , @xmath9 and @xmath188 for sample 1 of table 1 , where true values are indicated with vertical lines.,title=\"fig:\",width=144,height=144 ] , @xmath7 , @xmath8 , @xmath9 and @xmath188 for sample 1 of table 1 , where true values are indicated with vertical lines.,title=\"fig:\",width=144,height=144 ] , @xmath7 , @xmath8 , @xmath9 and @xmath188 for sample 1 of table 1 , where true values are indicated with vertical lines.,title=\"fig:\",width=144,height=144 ]    , @xmath7 , @xmath8 , @xmath9 and @xmath188 for sample 2 of table 1 , where true values are indicated with vertical lines.,title=\"fig:\",width=144,height=144 ] , @xmath7 , @xmath8 , @xmath9 and @xmath188 for sample 2 of table 1 , where true values are indicated with vertical lines.,title=\"fig:\",width=144,height=144 ] , @xmath7 , @xmath8 , @xmath9 and @xmath188 for sample 2 of table 1 , where true values are indicated with vertical lines.,title=\"fig:\",width=144,height=144 ] , @xmath7 , @xmath8 , @xmath9 and @xmath188 for sample 2 of table 1 , where true values are indicated with vertical lines.,title=\"fig:\",width=144,height=144 ] , @xmath7 , @xmath8 , @xmath9 and @xmath188 for sample 2 of table 1 , where true values are indicated with vertical lines.,title=\"fig:\",width=144,height=144 ]    , @xmath7 , @xmath8 , @xmath9 and @xmath188 for sample 3 of table 1 , where true values are indicated with vertical lines.,title=\"fig:\",width=144,height=144 ] , @xmath7 , @xmath8 , @xmath9 and @xmath188 for sample 3 of table 1 , where true values are indicated with vertical lines.,title=\"fig:\",width=144,height=144 ] , @xmath7 , @xmath8 , @xmath9 and @xmath188 for sample 3 of table 1 , where true values are indicated with vertical lines.,title=\"fig:\",width=144,height=144 ] , @xmath7 , @xmath8 , @xmath9 and @xmath188 for sample 3 of table 1 , where true values are indicated with vertical lines.,title=\"fig:\",width=144,height=144 ] , @xmath7 , @xmath8 , @xmath9 and @xmath188 for sample 3 of table 1 , where true values are indicated with vertical lines.,title=\"fig:\",width=144,height=144 ]    , @xmath7 , @xmath8 , @xmath9 and @xmath188 for sample 4 of table 1 , where true values are indicated with vertical lines.,title=\"fig:\",width=144,height=144 ] , @xmath7 , @xmath8 , @xmath9 and @xmath188 for sample 4 of table 1 , where true values are indicated with vertical lines.,title=\"fig:\",width=144,height=144 ] , @xmath7 , @xmath8 , @xmath9 and @xmath188 for sample 4 of table 1 , where true values are indicated with vertical lines.,title=\"fig:\",width=144,height=144 ] , @xmath7 , @xmath8 , @xmath9 and @xmath188 for sample 4 of table 1 , where true values are indicated with vertical lines.,title=\"fig:\",width=144,height=144 ] , @xmath7 , @xmath8 , @xmath9 and @xmath188 for sample 4 of table 1 , where true values are indicated with vertical lines.,title=\"fig:\",width=144,height=144 ]             th observations for sample 1,2,3 and that of @xmath78th observation for sample 4 of table 1 , where true values are indicated with long vertical lines and 95% credible intervals are shown with short vertical lines.,title=\"fig:\",width=144,height=144 ] th observations for sample 1,2,3 and that of @xmath78th observation for sample 4 of table 1 , where true values are indicated with long vertical lines and 95% credible intervals are shown with short vertical lines.,title=\"fig:\",width=144,height=144 ] th observations for sample 1,2,3 and that of @xmath78th observation for sample 4 of table 1 , where true values are indicated with long vertical lines and 95%",
    "credible intervals are shown with short vertical lines.,title=\"fig:\",width=144,height=144 ] th observations for sample 1,2,3 and that of @xmath78th observation for sample 4 of table 1 , where true values are indicated with long vertical lines and 95% credible intervals are shown with short vertical lines.,title=\"fig:\",width=144,height=144 ]                             , @xmath7 , @xmath8 , @xmath9 , @xmath188 and @xmath99 for the numerical example of dependent error with exponentially decaying covariance structure , where true values are indicated with vertical lines.,title=\"fig:\",width=144,height=144 ] , @xmath7 , @xmath8 , @xmath9 , @xmath188 and @xmath99 for the numerical example of dependent error with exponentially decaying covariance structure , where true values are indicated with vertical lines.,title=\"fig:\",width=144,height=144 ] , @xmath7 , @xmath8 , @xmath9 , @xmath188 and @xmath99 for the numerical example of dependent error with exponentially decaying covariance structure , where true values are indicated with vertical lines.,title=\"fig:\",width=144,height=144 ] , @xmath7 , @xmath8 , @xmath9 , @xmath188 and @xmath99 for the numerical example of dependent error with exponentially decaying covariance structure , where true values are indicated with vertical lines.,title=\"fig:\",width=144,height=144 ] , @xmath7 , @xmath8 , @xmath9 , @xmath188 and @xmath99 for the numerical example of dependent error with exponentially decaying covariance structure , where true values are indicated with vertical lines.,title=\"fig:\",width=144,height=144 ] , @xmath7 , @xmath8 , @xmath9 , @xmath188 and @xmath99 for the numerical example of dependent error with exponentially decaying covariance structure , where true values are indicated with vertical lines.,title=\"fig:\",width=144,height=144 ]"
  ],
  "abstract_text": [
    "<S> chirp signals are frequently used in different areas of science and engineering . </S>",
    "<S> mcmc based bayesian inference is done here for purpose of one step and multiple step prediction in case of one dimensional single chirp signal with i.  i.  d.  error structure as well as dependent error structure with exponentially decaying covariances . </S>",
    "<S> we use gibbs sampling technique and random walk mcmc to update the parameters . </S>",
    "<S> we perform total five simulation studies for illustration purpose . </S>",
    "<S> we also do some real data analysis to show how the method is working in practice .    * </S>",
    "<S> key words and phrases * : bayesian inference , chirp signal , gibbs sampling , posterior predictive density , random walk mcmc . </S>"
  ]
}