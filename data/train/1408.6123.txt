{
  "article_text": [
    "very often in particle physics we try to see whether some data are consistent with the standard model ( sm ) with the currently known particles ( call this hypothesis @xmath0 ) , or whether it favors a more or less specific form of new physics in addition to the sm background ( @xmath1 ) .",
    "this could be , for example , a particular form of leptoquark with a well - defined mass ; or with a mass in some range ( e.g. 50 to 1000 gev ) . in the first case",
    "there are no free parameters and @xmath1 is described as being ` simple ' , while in the latter case , because of the unspecified leptoquark mass , @xmath1 is ` composite ' .",
    "if the only free parameter in the alternative hypothesis @xmath1 is the mass of some new particle , we can test each mass in @xmath1 separately against @xmath0 , in which case we are comparing two simple hypotheses .",
    "however , the ensemble of different possible masses in the overall procedure ( known as a ` raster scan '  @xcite ) makes @xmath1 composite .",
    "insight into this type of situation is facilitated by two - dimensional ` @xmath5-value plots ' , where the significance of possible observations under the null hypothesis is plotted against their significance under various values of the free parameter in the alternative hypothesis  @xcite .",
    "the purpose of this article is to use such plots to explore various aspects of hypothesis testing in particle physics .",
    "we begin in section  [ sec : testingtypes ] by recapitulating the types of hypothesis testing familiar from the statistics literature , and contrasting these with the practice in particle physics .",
    "section  [ sec : pvalues ] introduces @xmath5-value plots and uses them to discuss the @xmath7 criterion , upper limits , fixed - hypothesis contours , and the punzi definition of sensitivity .",
    "the probabilities for observations to fall into various regions of a @xmath5-value plot are derived in section  [ sec : errorrates ] , together with the error rates and power of a particle physics test .",
    "likelihood ratios form the subject of section  [ sec : likelihoodratios ] , where they are compared to @xmath5-values and used to plot contours and to compute probabilities of misleading evidence .",
    "two famous @xmath5-value puzzles are described in section  [ sec : famouspuzzles ] .",
    "section  [ sec : nuisanceparameters ] contains remarks on the effect of nuisance parameters , and our conclusions and recommendations appear in section  [ sec : conclusion ] .",
    "an appendix provides technical details about the relationship between @xmath7 and bayesian upper limits .",
    "when using observed data to test one or more hypotheses , the first step is to design a test statistic @xmath14 that summarizes the relevant properties of the data .",
    "the observed value @xmath15 of @xmath14 is then referred to its probability distribution under each specified hypothesis in order to assess evidence .",
    "the form of the test statistic depends on the type of test one is interested in .",
    "comparisons of data with _ a single hypothesis _ are performed via ` goodness of fit ' tests .",
    "an example of this is the @xmath16 test , which generally requires the data to be binned , and where @xmath14 is equal to the sum of the squares of the numbers of standard deviations between observed and expected bin contents .",
    "another well - known technique , which does not require binning , is the kolmogorov - smirnov test , where @xmath14 is constructed from the expected and observed cumulative distributions of the data .",
    "there are many other techniques  @xcite .",
    "the outcome of a goodness - of - fit test is either ` reject ' or ` fail to reject ' the hypothesis of interest .",
    "comparison of the data with more than one hypothesis in order to decide which is favored is known as ` hypothesis testing ' . if there are _ just two simple hypotheses _ @xmath0 and @xmath1 , the appropriate framework is neyman - pearson hypothesis testing .",
    "the optimal test statistic @xmath14 in this case is the likelihood ratio for the two hypotheses , or a one - to - one function of it   is typically a one - to - one function of the likelihood ratio for the ` signal+background ' and ` background - only ' hypotheses ( @xmath1 and @xmath0 respectively ) . ] .",
    "the outcome of a neyman - pearson test is either ` reject @xmath0 and accept @xmath1 , ' or ` accept @xmath0 and reject @xmath1 . '    in particle physics it often happens that we need to consider additional possible outcomes of a test . in the leptoquark example ,",
    "an observed signal could be due to something entirely different from a leptoquark : some new physics that we did not anticipate , or a systematic bias that we did not model .",
    "hence we may need to reject both @xmath0 and @xmath1 in favor of a third , unspecified hypothesis . on the other hand it may also happen that the data sample does not allow us to reject either @xmath0 or @xmath1  @xcite .",
    "this leads to the formulation of a ` double test ' of two hypotheses @xmath0 and @xmath1 , which are independently tested , resulting in four possible outcomes :    1 .   fail to reject @xmath0 , and reject @xmath1 .",
    "this is referred to as ` @xmath1 excluded , ' and in a frequentist approach the rejection of @xmath1 is valid at some level of confidence , typically 95% .",
    "2 .   fail to reject @xmath0 and fail to reject @xmath1 ( ` no decision ' ) .",
    "3 .   reject @xmath0 , and fail to reject @xmath1 .",
    "this corresponds to ` discovery of @xmath1 . ' in a frequentist approach the rejection of @xmath0 is valid at some confidence level , which in particle physics is usually much higher than the confidence level used for excluding @xmath1 .",
    "typically the significance level , defined as one minus the confidence level , is set at @xmath17 for rejecting @xmath0 .",
    "this is the area under a gaussian tail , starting five standard deviations away from the mean .",
    "4 .   reject both @xmath18 and @xmath19 .",
    "often a likelihood ratio is used as the test statistic @xmath14 for a double test .    for given @xmath18 and for fixed values of the parameters in @xmath19 , we can plot the probability density functions ( pdf s ) of @xmath14 , assuming ( a ) that hypothesis @xmath0 is true , or ( b ) that @xmath1 is true .",
    "three possible situations are shown in figure  [ fig : pdfs ] . in ( a )",
    ", the two hypotheses are hard to distinguish as the pdf s lie almost on top of each other ; this could happen if @xmath1 involved a new particle that was only very weakly produced . in ( b ) , the pdf s still overlap to some extent , but distinguishing between the two hypotheses may be possible for some data sets . finally ( c ) shows a situation where it is relatively easy to choose between the hypotheses .",
    "the degree to which the data are unexpected for a given hypothesis can be quantified via the @xmath5-value .",
    "this is the fractional area in the tail of the relevant pdf , with a value of @xmath15 at least as extreme as that in the data . in tests involving two hypotheses , it is conventional to use the one - sided tail in the direction of the other hypothesis . for the examples shown in figure  [ fig : pdfs ] ,",
    "this corresponds to @xmath2 being the right - hand tail of @xmath0 and @xmath3 the left - hand tail of @xmath1 .",
    "when the data statistic @xmath15 falls in the extreme left - tail of the @xmath0 pdf ( see figure  [ fig : pdfs ] ) , or to reject @xmath1 when @xmath15 is very large .",
    "our @xmath5-values are one - sided and would therefore be close to unity in these cases . ] in the extreme case where @xmath0 and @xmath1 coincide ( and where @xmath15 is continuous rather than discrete ) , @xmath20 .",
    "figure  [ fig : lines ] contains a plot of @xmath2 versus @xmath3 , with the regions for which the double test either rejects @xmath0 or fails to reject it ; these depend solely on @xmath2 . in the diagram",
    ", the critical value @xmath21 for @xmath2 is shown at 0.05 ; this value is chosen here for clear visibility on the plot , rather than as a realistic choice .    in particle physics ,",
    "when we fail to reject @xmath0 , we want to see further whether we can exclude @xmath1 .",
    "although not as exciting as discovery , exclusion can be useful from a theoretical point of view and also for the purpose of planning the next measurement .",
    "the most famous example is the michelson - morley experiment , which excluded any significant velocity of the earth with respect to the aether and led to the demise of the aether theory . in figure",
    "[ fig : lines ] , the region @xmath22 is used for excluding @xmath1 .",
    "the critical value @xmath23 is usually chosen to be larger than the @xmath2 cut - off @xmath21 ; 0.05 is a typical value . in the figure @xmath23",
    "is shown at 0.10 .",
    "if @xmath2 and @xmath3 fall in the large rectangle at the top right of the plot ( @xmath24 and @xmath25 ) , we claim neither discovery of @xmath1 nor its exclusion : this is the no - decision region . the small rectangle near the origin corresponds to both @xmath5-values being below their cut - offs , and the data are unlikely under either hypothesis .",
    "it could correspond to the new physics occurring , but at a lower than expected rate .",
    "an alternative approach for exclusion of @xmath1 is the @xmath7 criterion@xcite . because exclusion levels are chosen to have modest values ( say 95% )",
    ", there is substantial probability ( 5% ) that @xmath1 will be excluded even when the experiment has little sensitivity for distinguishing @xmath1 from @xmath0 ( the situation shown in figure  [ fig : pdfs](a ) ) .",
    "although professional statisticians are not worried about this , in particle physics it is regarded as unsatisfactory . to protect against this , instead of rejecting @xmath1 on the basis of @xmath3 being small , a cut is made on @xmath26 i.e. on the ratio of the left - hand tails of the @xmath0 and @xmath1 pdf s .",
    "thus if the pdf s are almost indistinguishable , the ratio will be close to unity , and @xmath1 will not be excluded . in figure",
    "[ fig : lines ] , the region below the dashed line referred to as ` @xmath7 ' shows where @xmath1 would be excluded .",
    "this is to be compared to the larger region below the horizontal line for the more conventional exclusion based on @xmath3 alone .",
    "the @xmath7 approach can thus be regarded as a conservative modification of the exact frequentist method ; conservatism is the price to pay for the protection @xmath7 provides against exclusion when there is little or no sensitivity to @xmath1 .",
    "as pointed out in the introduction , the pdf of @xmath1 often contains one or more parameters of interest whose values are not specified ( e.g. the mass of a new particle , the cross section of a new process , etc . ) .",
    "it is then useful to determine the subset of @xmath1 parameter space where , with significance threshold @xmath23 , each parameter value is excluded by the observations . in the frequentist paradigm ,",
    "the complement of this subset is a @xmath27 confidence region .    for a simple and common example consider the case where the pdf of the data depends on the cross section @xmath28 of a new physics process : then @xmath29 if the process is present in the data ( @xmath1 true ) , and @xmath30 otherwise ( @xmath0 true ) .",
    "suppose that the test statistic @xmath14 is stochastically increasing with @xmath28 , meaning that for fixed @xmath14 , increasing @xmath28 reduces the @xmath5-value @xmath3 .",
    "then the set of @xmath28 values that _ can not _ be excluded by the observations has an upper limit , and that upper limit has confidence level @xmath31 .",
    "if instead of rejecting @xmath1 with the standard frequentist criterion @xmath32 , we use the @xmath7 criterion @xmath33 , the above procedure yields a @xmath7 upper limit for @xmath28 , which is higher ( i.e. weaker ) than the standard frequentist upper limit .",
    "in the previous example suppose that , instead of a cross section , @xmath28 is a location parameter for the test statistic @xmath15 .",
    "more precisely , suppose that the pdf of @xmath15 is of the form @xmath34 , with @xmath35 a continuous distribution .",
    "then it can be shown that the upper limit using @xmath36 at the @xmath37 level coincides exactly with the credibility @xmath31 bayesian upper limit obtained by assuming a uniform prior for @xmath28 under @xmath1 ( i.e. , a prior that is a non - zero constant for @xmath29 , and zero elsewhere ) .",
    "this result extends to the discrete case where @xmath15 is a poisson - distributed event count with mean @xmath28 ( see appendix  [ appa ] ) .      if we keep the hypotheses @xmath0 and @xmath1 fixed , but vary the observed data statistic @xmath15 , the point @xmath38 will trace a contour in the @xmath39 plane . in general",
    "this contour depends on the particular characteristics of each hypothesis , but useful simplifications may occur when the pdf of the test statistic is translation - invariant or enjoys other symmetries .",
    "here we give four examples based on the pdf s shown in figure  [ fig : fourpdfs ] .",
    "the corresponding contours plot are closely related to roc ( receiver operating characteristic ) curves , which have been used for many years in a variety of fields . ]",
    "are drawn in figure  [ fig : fh_4examples ] and assume that the test is of the basic form @xmath40 versus @xmath41 , with @xmath42 , and that the test statistic is @xmath14   to indicate that @xmath35 is the pdf of @xmath14 ( use of the ` @xmath43 ' symbol does not imply any kind of approximation ) . ] .",
    "the parameter @xmath28 could be related to the strength of a possible signal for a new particle with unknown mass .",
    "increasing separation between @xmath44 and @xmath45 could then correspond to increasing amount of data ; fixed amount of data and fixed particle mass , but increasing cross section ; fixed amount of data and varying particle mass , with the cross section depending on the mass in a known way ( i.e. raster scan ) .",
    "example 1 : : :    @xmath28 is the mean of a gaussian distribution of known    width @xmath46 :    @xmath47 in this case the    fixed - hypothesis contours only depend on    @xmath48 , with    @xmath49 , and have the form :    @xmath50    figure  [ fig : fh_4examples](a ) shows three examples of this , with    @xmath51 ( when the locus is the diagonal line    @xmath52 ) , @xmath53 and    @xmath54 . as @xmath48 increases ,",
    "the    curves pass closer to the origin .",
    "example 2 : : :    @xmath28 is the mode of a cauchy distribution with known    half - width at half - height  @xmath55 :    @xmath56}.\\ ] ]    the contours have a simple expression that depends only on    @xmath57 :    @xmath58 +          \\tan\\bigl[\\bigl(1 - 2\\,p_{0}\\bigr)\\,\\frac{\\pi}{2}\\bigr ] = \\frac{\\delta\\mu}{\\gamma}.\\ ] ]    example contours are shown in figure  [ fig : fh_4examples](b ) .",
    "example 3 : : :    @xmath28 is an exponential decay rate :    @xmath59 here the fixed - hypothesis    contours depend only on the ratio of @xmath45 to    @xmath44 :    @xmath60 an    interesting generalization is to perform the test on a combination of    @xmath61 independent decay time measurements    @xmath62 . in this case",
    "the likelihood ratio statistic is a    one - to - one function of the sum of the measurements , which we therefore    take as our test statistic , @xmath63 .",
    "the distribution of @xmath14 is gamma@xmath64 ,    with @xmath61 the shape parameter and @xmath28 the    rate parameter :    @xmath65    the fixed - hypothesis contours depend on @xmath61 and on the    ratio @xmath66 :    @xmath67    where @xmath68 is the regularized incomplete gamma    function and @xmath69 is the    @xmath5-quantile of a chisquared distribution with    @xmath70 degrees of freedom .",
    "some example contours are shown    in figure  [ fig : fh_4examples](c ) .",
    "unlike gaussian or cauchy contours ,    gamma contours are not symmetric around the main diagonal of the plot .",
    "example 4 : : :    @xmath28 is a poisson mean :    @xmath71    in this case the contours are discrete and must be computed    numerically .",
    "their dependence on @xmath44 and    @xmath45 does not simplify .",
    "a few examples are plotted in    figure  [ fig : fh_4examples](d ) .",
    "a common feature of examples 1 - 3 above is that the region of the plot above the diagonal @xmath72 is empty .",
    "this is a general consequence of our definition of one - sided @xmath5-values , and of the fact , suggested by the requirement @xmath42 , that the bulk of the pdf @xmath73 under @xmath1 lies to the right of the bulk of the pdf @xmath74 under @xmath0 . in other words , for any @xmath15 , the area under @xmath75 and to the right of @xmath15 is smaller than the corresponding area under @xmath76 : @xmath77 on the left - hand side one recognizes @xmath2 and on the right - hand side @xmath78 , whence the inequality @xmath79 follows .",
    "however , this is only strictly true for continuous pdf s . for discrete pdf s , @xmath2 and @xmath3 _",
    "both _ include the finite probability of the observation , so that it may happen that @xmath80 when @xmath75 and @xmath76 are very close to each other .",
    "this is evident in figure  [ fig : fh_4examples](d ) for the case @xmath81 .      in a given data analysis problem ,",
    "any data set ( real or artificial ) for which the parameter estimators yield the true values is called an asimov data set  @xcite . by evaluating a test statistic on an asimov data set",
    "one usually obtains an approximation to the median of that test statistic , and the corresponding @xmath5-value will be the median @xmath5-value under the assumed hypothesis .",
    "median @xmath5-values are used to characterize the sensitivity of an experiment .",
    "a simple example of the use of the fixed - hypothesis contours is that they map the abscissa @xmath82 onto the median value of @xmath3 under @xmath0 , and vice - versa , the value @xmath83 is mapped onto the median of @xmath2 under @xmath1 . these medians can be directly read off the plot . for the gaussian case with @xmath84 , @xmath53 , or @xmath54 ,",
    "the median @xmath3 under @xmath0 is @xmath85 , @xmath86 , or @xmath87 , respectively . by symmetry of the gaussian density ,",
    "these values are also those of the median @xmath2 under @xmath1 .    by the invariance of probability statements under one - to - one transformations of random variables , the median @xmath7 under @xmath0 can be obtained by plugging @xmath88 into the definition of @xmath7 .",
    "this yields : @xmath89 assuming @xmath0 is true , the median @xmath7 for testing @xmath1 equals twice the median @xmath3 .      for large enough separation of the pdf s , the fixed - hypothesis contour will keep out of the no - decision region .",
    "punzi  @xcite defines sensitivity as the expected signal strength required for there to be a probability of at least @xmath31 for claiming a discovery with significance @xmath21 ( e.g. a probability of 95% for discovery at the level of @xmath17 ) .",
    "this has the advantage that above the sensitivity limit , the data are guaranteed to provide rejection of @xmath0 at the significance level @xmath21 , or exclusion of @xmath1 at the significance level @xmath23 , or both ; the data can not fall in the no - decision region . in figure",
    "[ fig : contours ] , the punzi sensitivity corresponds to a pdf separation for which the @xmath39 contour ( not drawn ) passes through the intersection of the vertical dot - dashed and horizontal dashed lines . in the following we refer to this intersection as the ` punzi point ' .",
    "@xmath90-values are probabilities and therefore remain invariant under one - to - one transformations of the test statistic on which they are based .",
    "plots of @xmath2 versus @xmath3 are similarly unaffected , but one must remember that these plots involve two hypotheses , and that effects of a transformation on the pdf s of the test statistic under @xmath0 and @xmath1 are different .",
    "this is the reason that , for example , the @xmath2 versus @xmath3 plot for testing the mode of a gaussian pdf is not identical to the plot for testing the mode of a cauchy pdf , even though gaussian and cauchy variates are related by one - to - one transformations ( see examples  1 and  2 in section  [ sec : fh_contours ] ) .",
    "we take a closer look at this particular case here .",
    "suppose that under @xmath0 ( @xmath1 ) the test statistic @xmath91 is gaussian with mean @xmath44 ( @xmath45 ) and width @xmath46 .",
    "then , if @xmath0 is true , the transformation @xmath92\\ ] ] maps @xmath91 into a cauchy variate @xmath93 with mode @xmath94 and scale parameter @xmath55 .",
    "if on the other hand @xmath1 is true , @xmath91 will be mapped into a variate @xmath93 with pdf @xmath95\\right ] } { \\pi\\gamma\\bigl[1 + \\bigl(\\frac{y-\\mu_{c}}{\\gamma}\\bigr)^{2}\\,\\bigr ] } , \\label{eq : noncauchy}\\ ] ] which is an asymmetric density that depends on three parameters : @xmath94 , @xmath55 , and @xmath96 ; it reduces to a cauchy density in the limit @xmath97 .",
    "figure  [ fig : cauchy ] compares the two pdf s .",
    "thus , whereas in @xmath91 space we are testing a gaussian hypothesis against a gaussian hypothesis with a different mean , in @xmath93 space we are testing a cauchy hypothesis against a hypothesis with a rather different , asymmetrical distribution .",
    "however the @xmath2 versus @xmath3 plot is the same in both spaces .",
    "another interesting property of one - to - one transformations of test statistics is that they preserve the likelihood ratio ( since the jacobian of the transformation cancels in the ratio ) .",
    "thus , if @xmath98 is the pdf of @xmath91 under @xmath6 , @xmath99 , and we transform @xmath91 into @xmath93 with pdf s @xmath100 , we have : @xmath101 suppose now that the @xmath102 transformation is the likelihood ratio transformation : @xmath103 .",
    "then it follows from the above equation that @xmath104 a useful simplification .",
    "if one prefers to work with the logarithm of the likelihood ratio , @xmath105 , and @xmath106 is the pdf of @xmath107 under @xmath6 , then one finds : @xmath108 suppose for example that @xmath109 is gaussian with mean @xmath110 and width @xmath46 .",
    "the pdf s of the log - likelihood ratio are then : @xmath111,\\label{eq : loglrpdf0}\\\\[2 mm ] h_{1}(q ) & \\;=\\ ; \\frac{1}{\\sqrt{2\\pi}\\,\\delta\\mu/\\sigma}\\ ;               \\exp\\left[-\\frac{1}{2}\\left(\\frac{q + \\frac{1}{2 }               \\left(\\delta\\mu/\\sigma\\right)^{2}}{\\delta\\mu/\\sigma}\\right)^{2 }               \\:\\right],\\label{eq : loglrpdf1}\\end{aligned}\\ ] ] and it is straightforward to verify equation  .",
    "the gauss - versus - gauss likelihood ratio @xmath112 in the above example is invariant under translations and rescalings of the original pdf s ( i.e. under addition of a common constant to @xmath44 , @xmath45 , and @xmath113 ; and under multiplication of @xmath44 , @xmath45 , @xmath46 , and @xmath113 by a common factor ) .",
    "these two invariances reduce the three numbers ( @xmath44 , @xmath45 , and @xmath46 ) required to specify the @xmath109 to a single one ( @xmath48 ) for the @xmath106 . note that @xmath48 is the ratio of the difference in means to the standard deviation for the pair @xmath114 as well as @xmath115",
    ".    more generally , since the likelihood ratio transformation @xmath116 is one - to - one , fixed - hypothesis contours obtained from the @xmath106 are identical to those obtained from the @xmath109 .",
    "a useful feature of @xmath39 plots is that they help us map probabilities under @xmath0 to probabilities under @xmath1 and vice - versa , using a simple graphical method .",
    "suppose for instance that we are interested in the outcome @xmath117 .",
    "when @xmath0 is true this has probability 0.3 , since @xmath2 is uniformly distributed under @xmath0 . to find",
    "the probability under @xmath1 we map the interval @xmath118 onto the @xmath3 axis using the appropriate contour on figure  [ fig : contours ] , say the one with @xmath119 .",
    "this yields the interval @xmath120 . since @xmath3 is uniform under @xmath1",
    ", we can conclude that the outcome @xmath117 has probability @xmath121 under @xmath1 . in a similar way",
    ", it can be read from the figure that rejection of @xmath0 ( the outcome @xmath122 ) has probability @xmath123 under @xmath1 , where @xmath124 is the @xmath3 coordinate of the intersection of the line @xmath125 with the relevant contour . as for rejection of @xmath1 ( the outcome @xmath22 )",
    ", this has probability @xmath126 under @xmath0 , where @xmath127 is the @xmath2 coordinate of the intersection of the line @xmath128 with the contour .",
    ".possible outcomes of the double - test procedure , together with their probabilities under @xmath0 and @xmath1 .",
    "as expected , the probabilities under a given hypothesis all add up to one .",
    "[ tab : doubletestoutcomes ] [ cols=\"<,^,<,^,<,^ , < \" , ]      is true , whereas plots ( b ) and ( d ) assume the truth of @xmath1 . a sequential testing procedure ( see text ) describes a random walk in the @xmath39 plane ( shown by the black broken lines ) .",
    "the blue curves represent the boundary defined by the law of the iterated logarithm ( lil ) . the red likelihood - ratio contours ( for @xmath129 in plots ( a ) and ( c ) , and for @xmath130 in plots ( b ) and ( d ) ) are examples of decision boundaries that avoid the possibility of testing to a foregone conclusion implied by the lil . the green line in plots ( b ) and ( d ) represents the @xmath131 decision boundary , which does not avoid this possibility . [ fig : seqtesting4x4 ] ]     versus gauss@xmath132 type .",
    "at each step the sample size @xmath61 increases by one , and the walk moves to a contour with improved resolution .",
    "contours are labeled by the value of @xmath61 .",
    "the blue curve shows the relationship between @xmath2 and @xmath61 described by the lil boundary .",
    "the red dotted line represents a fixed discovery threshold @xmath21 .",
    "since this line crosses to the large-@xmath2 side of the lil boundary , it is guaranteed to have the pathology of sampling to a foregone conclusion . in contrast , with the @xmath2 cutoff set as @xmath133 ( solid red line ) , repeated sampling does not necessarily lead to exclusion of a true @xmath0 .",
    "[ fig : rootn_walk ] ]    ( a ) for comparing hypotheses whose pdf s are equal - width gaussians .",
    "the line @xmath134 is at fixed @xmath2 , with the points @xmath135 to @xmath136 corresponding to increasing separation of the @xmath0 and @xmath1 pdf s , as shown in diagrams ( b ) to ( e ) respectively .",
    "[ fig : jlillustration ] ]     versus prior - predictive @xmath3 for testing @xmath137 versus @xmath138 .",
    "the test statistic has a gaussian distribution with mean @xmath28 and standard deviation @xmath46 .",
    "the prior for @xmath28 under @xmath1 equals @xmath139 for @xmath140 and is zero otherwise .",
    "fixed - hypothesis contours ( dashed lines ) are labeled by the value of @xmath141 .",
    "constant bayes factor contours ( colored solid lines ) are also shown .",
    "[ fig : ppp1_v_p0_lin ] ]    , with a larger range of fixed - hypothesis contours and more realistic values for the bayes factor contours . also shown",
    "are the constant @xmath2 threshold ( dotted line ) at @xmath142 and the corresponding @xmath61-dependent threshold @xmath133 ( dot - dashed line ) , where @xmath61 is the sample size .",
    "[ fig : ppp1_v_p0_log ] ]     is an interval hypothesis with width @xmath143 instead of a point - null hypothesis .",
    "both @xmath144 and @xmath145 are prior - predictive @xmath5-values .",
    "the contours on these plots are the same as in figure  [ fig : ppp1_v_p0_log ] , although for clarity only the contours @xmath146 and @xmath147 are labeled .",
    "compared with figure  [ fig : ppp1_v_p0_log ] , the fixed - hypothesis contours and the constant bayes factor contours are both changed in such a way that the paradox remains present regardless of the value of @xmath148 .",
    "[ fig : jlsolpp ] ]     is an interval hypothesis with width @xmath143 instead of a point - null hypothesis .",
    "the @xmath5-value @xmath145 is a prior - predictive @xmath5-value , whereas @xmath149 is a supremum @xmath5-value .",
    "the contours on these plots are the same as in figure  [ fig : ppp1_v_p0_log ] , although for clarity only the contours @xmath146 and @xmath147 are labeled .",
    "compared with figure  [ fig : ppp1_v_p0_log ] , only the constant bayes factor contours are changed ; the fixed - hypothesis contours are the same .",
    "the result is that the paradox disappears for a suitably high value of @xmath148 .",
    "[ fig : jlsolsup ] ]     ( the pdf for @xmath150 has been truncated at the upper end ) .",
    "if for example @xmath151 is observed , the @xmath5-value under @xmath152 is 2.3% ( shaded area ) , but the likelihood ratio of @xmath153 to @xmath150 is 5.5 .",
    "it is clear that for very large @xmath154 values , significantly small @xmath5-values that disfavor @xmath0 will be associated with likelihood ratios that favor @xmath0 .",
    "this is a simple versus simple version of the jeffreys - lindley paradox .",
    "[ fig : taupdf ] ]"
  ],
  "abstract_text": [
    "<S> for situations where we are trying to decide which of two hypotheses @xmath0 and @xmath1 provides a better description of some data , we discuss the usefulness of plots of @xmath2 versus @xmath3 , where @xmath4 is the @xmath5-value for testing @xmath6 . </S>",
    "<S> they provide an interesting way of understanding the difference between the standard way of excluding @xmath1 and the @xmath7 approach ; the punzi definition of sensitivity ; the relationship between @xmath5-values and likelihood ratios ; and the probability of observing misleading evidence . </S>",
    "<S> they also help illustrate the law of the iterated logarithm and the jeffreys - lindley paradox .    * testing hypotheses in particle physics : *   + * plots of @xmath8 versus @xmath9 *    mailto:luc.demortier@cern.ch,l.lyons1@physics.ox.ac.uk[luc demortier@xmath10 , louis lyons@xmath11 + @xmath12_laboratory of experimental high energy physics _ + _ the rockefeller university , new york , ny 10065 , usa _ </S>",
    "<S> + @xmath13_blackett laboratory _ </S>",
    "<S> + _ imperial college , london sw7 2bw , uk _ </S>"
  ]
}