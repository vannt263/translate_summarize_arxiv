{
  "article_text": [
    "parameter estimation within the bayesian framework rests on the application of bayes theorem to data analysis .",
    "if we consider a parameter set @xmath0 , a data set @xmath1 and all prior knowledge @xmath2 bayes theorem tells us that : @xmath3 here we have :    * the _ posterior probability _ @xmath4 which represents our belief in the hypotheses once we have analysed the data available . * the _ prior probability _ @xmath5 which encodes our previous knowledge of the system under examination . * the _ likelihood function _",
    "@xmath6 which is the probability of observing the data for a given set of parameters .",
    "* the _ evidence _ @xmath7 which is a normalisation constant given by the probability of the data .    in the specific case when the problem being faced is one of parameter estimation we can ignore the denominator which is independent of @xmath0 . we thus get : @xmath8 here the prior knowledge is being transformed by the data through the likelihood to give the posterior distribution which embodies our new beliefs .    bayesian methodology",
    "can also be applied to model selection scenarios to choose between competing hypotheses or models @xmath9 . given a model @xmath10",
    "we can write down , using bayes theorem , the probability that @xmath10 is the correct model given the data @xmath1 and our previous knowledge @xmath2 : @xmath11 this is analogous to equation ( [ bayesprop ] ) which deals with parameter estimation : just as the posterior probability distribution function for a parameter is proportional to its prior times its likelihood , so the posterior probability for a model as a whole is proportional to its prior probability times its evidence . @xmath7 itself",
    "can not be calculated but model selection can still be performed by comparing the probability of two models @xmath12 and @xmath13 using a ratio , called the _ posterior ratio _ or _ odd s ratio _ : @xmath14 when we have no prior preference for one model over another equation ( [ model_selection ] ) reduces to a ratio of evidences .",
    "marginilisation and bayes theorem allow us to express the evidence as an integral over the parameter space : @xmath15 calculating this integral then allows the experimentalist to select the optimum model to describe a data set .",
    "we now discuss two different techniques which lead to a value the evidence and further on compare them in terms of computational speed and accuracy .",
    "nested sampling ( skilling , 2006 ) is a modern bayesian technique which transforms the multidimensional evidence integral of equation ( [ equ : evidence ] ) into a simpler one dimensional one .",
    "this is done by considering the prior mass @xmath16 and its constituent elements @xmath17 .",
    "these can be summed up as follows : @xmath18 this covers all the prior mass corresponding to a parameter space with likelihood greater than @xmath19 .",
    "this provides for the required transformation to one dimension .",
    "the evidence integral then becomes : @xmath20 because of the way we have ordered our elements in terms of likelihood we can evaluate @xmath21 at a sequence of @xmath22 points in the parameter space having decreasing likelihood as shown in figure [ fig : contours ] .",
    "@xmath23    .,width=288 ]    if we set @xmath24 the above ordering allows us to evaluate the integral as a weighted sum of the likelihood : @xmath25 since we expect that the largest contributions to the integral to come from relatively small regions of peaked posterior it makes more sense to sample points in @xmath16 at a logarithmic rate instead of a linear one so that initial sampling of the shallow posterior is rapid .",
    "we therefore take : @xmath26 where each of the @xmath27 , known as the _ shrinkage ratio _ , lies between 0 and 1 . in practice",
    "the nested sampling algorithm implements these concepts as follows :    1 .",
    "@xmath28 objects are sampled randomly from within the prior and their likelihoods are evaluated .",
    "initially we have the full prior range from @xmath29 to @xmath30 available to sample from . 2 .",
    "we then select the point with the lowest likelihood ( @xmath31 ) and remove it from the set of samples .",
    "the prior volume is then shrunk to @xmath32 with the shrinkage ratio @xmath33 determining the volume decrease .",
    "the value of @xmath33 is the expectation value of the largest of n random numbers from uniform distribution between 0 and 1 , which is given by @xmath34 .",
    "the removed point is replaced with a new one satisfying the hard constraint likelihood @xmath35 4 .",
    "we then increment the evidence by @xmath36 .",
    "we iterate over these steps until we satisfy some stopping condition .    the selection of the least likelihood object is illustrated in fig .",
    "[ fig : select ] .     objects with @xmath37 or @xmath38 .",
    "( b ) the one with largest @xmath16 and call it @xmath39 , removing it from our list which now contains @xmath40 objects sampled from the prior .",
    "( c ) we generate a new object , sampled from the prior once again , but this time constrained to lie within the new likelihood domain.,width=220 ]    the method thus works its way up the likelihood surface through nested surfaces of equal likelihood . to terminate the process we can use two conditions .",
    "the first is given in ( skilling , 2006 ) as the number of iterations @xmath41 required for samples to converge to posterior peaks : @xmath42 secondly we also requires that successive changes in evidence are sufficiently small .",
    "variational bayesian methods provide another approach to parameter estimation and model selection .",
    "the basic concept behind this class of methods is to try and approximate the posterior distribution with a simpler probability distribution .",
    "one can then optimize the parameters in this approximation so that it is as close as possible to the true posterior .",
    "it turns out that such an optimal form for the approximating distribution does indeed exist and this is usually very easy to deal with analytically .",
    "there is then no need to sample from the posterior as important quantities such as the mean can be derived analytically from the approximation .",
    "similarly the evidence can be worked using standard integration techniques on the same approximation .",
    "variational bayesian methods thus reduce the computationally complex sampling and integration problems to a relatively easy optimization one .",
    "though there are many optimization algorithms in the literature which are used to find the best approximation the most widely used one is the expectation - maximization ( em ) algorithm ( zarb adami , 2003 ) . we give a brief overview of the method here , a more complete explanation can be found in ( mackay , 2003 ) . the sensible question to ask when approximating a complex function by some simpler one @xmath43 is the amount of information lost in this process .",
    "a suitable metric describing this quantity which one can use to quantify the `` distance '' between the original distribution and the approximation is the kullback - leibler ( kl ) divergence ( kullback , 1959 ) , denoted by @xmath44 .",
    "this is given by : @xmath45 we note that the above expression always returns a non - negative value and , as one would hope , vanishes when @xmath46 .",
    "bayes theorem then tells us that : @xmath47 the evidence @xmath7 is a quantity which is independent of the parameters @xmath0 and can thus be taken out of the integral .",
    "@xmath48 the above equation suggests that one could define a cost function @xmath49 which we can then seek to optimize by minimizing the kullback - liebler divergence @xmath44 : @xmath50 since @xmath51 the following inequality always holds : @xmath52 we thus see how minimizing the cost function is equivalent to maximizing a lower bound on the evidence value .",
    "figure [ fig : ckl ] shows the relation between the lower bound to the evidence and the kullback - leibler divergence .    ) .,width=240 ]    fortunately enough , in most cases the lower bound to the evidence is tight enough to be used in place of the true evidence in model selection procedures ( miskin , 2000 ) . after having optimized the approximating distribution one",
    "can then perform inference with the approximation in place of the real posterior .",
    "in this section we can compare nested sampling and variational bayes as applied to two engineered problems , the first being polynomial selection and the second being the fitting of gaussian mixture models .",
    "we use matlab in this preliminary study to speed up the coding process .",
    "naturally both methods are coded in the same language to try and ensure a fairer comparison of the computational times .      when one has a sparse data set it is often useful to infer a smooth curve that best fits with the observed points .",
    "this can facilitate further studies of the data as procedures like integration or the finding of extrema then become very easy . of course one might also want to fit data which has been generated by a physical law which has polynomial dependence on some parameter .",
    "the task at hand is thus to select the degree of the polynomial to use as well as calculate its coefficients .",
    "if a low order polynomial is chosen then it might be difficult to fit the data well whilst if the polynomial order is too high then there is a risk of over - fitting .",
    "we denote our data set @xmath1 , where @xmath1 contains a set of points @xmath53 . here",
    "@xmath54 and @xmath55 is the total number of data points .",
    "the @xmath56 are the abscissa values whilst the @xmath57 are the ordinate values . in the general case one",
    "can then construct an interpolation model using a set of @xmath58 fixed basis functions .",
    "we thus have : @xmath59 here @xmath60 is a matrix constructed using the interpolating basis functions evaluated at the abscissa values @xmath56 .",
    "if we decide to choose polynomials then we get @xmath61 where @xmath62 .",
    "the value of @xmath63 in equation ( [ equ : datapoly1 ] ) represents the noise affecting each measurement , which we can assume to be gaussian with inverse variance @xmath64 .",
    "if we assume that the gaussian noise is independent then the likelihood function becomes : @xmath65 we generate the data for this study by computing the actual values of a polynomial within a predefined interval @xmath66 $ ] and adding the gaussian noise of inverse variance @xmath64 using matlab s inbuilt functions . unless otherwise stated all the plots shown in this section are based on a data set containing 40 data points generated in the interval @xmath67 $ ] from a quintic curve with added noise having @xmath68 .",
    "we first tackle the situation using variational bayesian and then move on to use nested sampling . for the variational solution we can assign conjugate priors : @xmath69 we can set the three arbitrary parameters @xmath70 and @xmath71 accordingly to modify the shape and scale of the priors .",
    "these are chosen to make the priors as broad as possible , to simulate our ignorance of the underlying phenomena .",
    "the update equations , given in ( zarb adami , 2003 ) and ( miskin , 2000 ) , are shown below : @xmath72 the iteration of these equations leads to the values for the optimal parameter set describing the data . for our 40 point data set we can try fitting different order polynomials .",
    "the results are shown in figure [ fig : polyvbfitted ] .",
    "@xmath73{./figures / ch4polysparsevb3.pdf }   &    \\includegraphics[width=1.6in]{./figures / ch4polysparsevb4.pdf } \\\\",
    "\\includegraphics[width=1.6in]{./figures / ch4polysparsevb5.pdf }    & \\includegraphics[width=1.6in]{./figures / ch4polysparsevb6.pdf }    \\\\ \\end{array}$ ]    one can observe how inefficient the fitting is when we use a quadratic and how this improves once we use a cubic curve .",
    "fitting with a quintic ( @xmath74 ) we obtain : @xmath75 these results are not too far from the true values of @xmath76 and @xmath77 and an element of inconsistency between the two is acceptable as we have taken a large noise factor and not so many data points ( 40 ) .",
    "note that the odd components in the weight vector , which correspond to the even polynomials @xmath78 , are especially close to zero .",
    "this is because the parity of these components ( even ) is different from the parity of the mechanism generating the data ( odd ) .",
    "hence these contributions are fitted to zero as they can offer no explanation of the data .",
    "@xmath73{./figures / ch4polyvblhood.pdf }   &    \\includegraphics[width=1.6in]{./figures / ch4polyvboccamf.pdf } \\\\",
    "\\end{array}$ ] @xmath79{./figures / ch4polyvbevidence.pdf }    \\\\ \\end{array}$ ]    referring to figure [ fig : polyvb ] we notice that as expected the likelihood always increases with the number of parameters used to describe the data .",
    "however the evidence is also dependent on an occam factor which penalizes more complex models and is thus peaked , beginning to decrease past @xmath74 .",
    "the peak at @xmath74 , correctly implies that the most probable hypothesis is that the data was generated using a 6 parameter polynomial which is a quintic .    to implement the nested sampling algorithm for this example we take skilling s code ( skilling , 2004 ) as a skeleton and",
    "modify the likelihood and exploration functions appropriately to represent equation ( [ equ : polylike ] ) .",
    "we set uniform priors for @xmath64 and @xmath80 accordingly .",
    "exploration of the parameter space using a different variable step size for each separate parameter . the algorithm",
    "was then run with 36 objects .",
    "nested sampling is much slower than the variational method as the average time of computation was in the region of 300 seconds , an order of magnitude slower then variational bayes which takes around 20 seconds for a typical run .",
    "this difference in speed is most probably due to the fact that nested sampling is an mcmc method , involving the probabilistic exploration of a parameter space whilst variational bayes is analytical and has no such probabilistic dependence .",
    "@xmath73{./figures / ch4polynested3.pdf }   &    \\includegraphics[width=1.6in]{./figures / ch4polynested5.pdf } \\\\",
    "\\includegraphics[width=1.6in]{./figures / ch4polynested6.pdf }    & \\includegraphics[width=1.6in]{./figures / ch4polynestedevidence.pdf }    \\\\ \\end{array}$ ]    plots showing the evidence and fits are given in figure [ fig : polynested ] .",
    "note that within the nested sampling framework we have no explicit `` occam factor '' .",
    "rather the increase in parameters penalizes the complex models by giving the algorithm more space to explore and thus longer time to reach regions of higher likelihood .",
    "this means that when calculating the evidence the higher likelihood values are multiplied by smaller weights as shown in equation ( [ equ : t ] ) resulting in a lower evidence value over all .",
    "again we get the evidence peaking at the correct value of @xmath74 with the optimum evidence for both methods at around @xmath81 .",
    "the coefficients themselves are also in very good agreement with the ones obtained using variational bayes and we get : @xmath75 in fact the greatest variation is in the relatively inconsequential coefficient of @xmath82 and the value of this difference is @xmath83% .",
    "this is not such an issue as the coefficient is very small anyway and has no major effect on the final plots .",
    "the averaged percentage disagreement in all the other parameters is under @xmath84% and the percentage disagreement in the coefficient of @xmath85 is only @xmath86% .",
    "this indicates clearly that the discrepancies from the true parameters are due to the random nature of the gaussian noise and do not arise because of any fault in either of the algorithms .",
    "note that we give all the results for the parameters in appendix a , along with the percentage difference between the two methods .",
    "a mixture model is basically a distribution built up using a number of simpler distributions having different parameters .",
    "they are often used to obtain or substitute more complex distributions . an illustrative example given in ( miskin , 2000 )",
    "concerns the size of fruit .",
    "the probability distribution will obviously depend on the type of fruit being measured . instead of having a single complex distribution it would make sense to model the size distribution for each fruit type using a gaussian and then have a _ categorical variable _ which gives the probability that a fruit is of a given type .",
    "mathematically this can be expressed in terms of the likelihood function for a single data point @xmath57 , where @xmath87 labels the data point and can take values @xmath88 .",
    "if we have @xmath89 categories then we have : @xmath90 here @xmath91 is an _",
    "variable for each data point which tells us which distribution created the @xmath92 data point .",
    "these are chosen probabilistically such that @xmath93 . also , @xmath94 is the likelihood for a given @xmath91 and data point @xmath57 .",
    "we henceforth consider the particular case when the mixture is composed of gaussians , known as a _ gaussian mixture model _ ( gmm ) .",
    "equation ( [ equ : mixture1 ] ) then becomes : @xmath95 the parameters @xmath96 , @xmath97 and @xmath98 are collectively referred to as @xmath0 .",
    "a histogram of the generated points used in this section is shown in figure [ fig : gmm1 ] .",
    "the data set contains 300 points generated from a mixture model composed of three gaussians .",
    "these have means @xmath99 and sigma @xmath100 .",
    "the points were produced in the following ratio @xmath101 .",
    ": , sigma @xmath100 and were produced in the following ratio @xmath101 .",
    ", width=240 ]    the data set is relatively easy to handle for the algorithm because there is only a slight overlap between the different component distributions .",
    "we first set out to attack the problem using variational bayesian methods , fitting the data with @xmath89 gaussians .",
    "we use conjugate priors as required by the variational bayesian algorithm , using @xmath102 to label the variables pertaining to each of the @xmath89 gaussians . @xmath103",
    "the first two equations are gaussian priors for the parameters pertaining to a single gaussian in the mixture whilst the third , @xmath104 , represents a dirichlet distribution over the categorical weights with mixing hyperparameter @xmath105 as described in appendix a. if we assume the independence of the separate data points and we consider all the distributions forming the mixture , the prior becomes : @xmath106 note that here @xmath107 and @xmath105 each refer to a single one of the gaussian distributions forming the mixture .",
    "the joint likelihood of any single data point is given by equation ( [ equ : jointdataind ] ) .",
    "@xmath108 if we assume that the data are independent then we construct the final likelihood by taking the following product : @xmath109 here @xmath110 and @xmath111 .",
    "the inclusion of the indicator variable introduces additional complexities into the derivation of the update equations for the variational bayes algorithm .",
    "the procedure is described in ( zarb adami , 2003 ) , ( miskin , 2000 ) and ( penny & roberts , 2000 ) .",
    "here it is the results from the latter approach that we use and we refer the reader to this paper for the algorithmic details .",
    "the algorithm is stopped when successive changes in the evidence bound are less than @xmath112 .",
    "the plots in figure [ fig : gmm3 ] show the results of fitting different numbers of gaussians to the data .",
    "@xmath73{./figures / ch4gmmeasyevidence.pdf }   & \\includegraphics[width=1.6in]{./figures / ch4gmmvbeasyfit2.pdf } \\\\",
    "\\includegraphics[width=1.6in]{./figures / ch4gmmvbeasyfit3.pdf }   &    \\includegraphics[width=1.6in]{./figures / ch4gmmvbeasyfit4.pdf } \\\\",
    "\\end{array}$ ]    the fit having 3 gaussians results in the following values for the means and the widths of the three component distributions : @xmath113 these are in close agreement with the true values with which the data was generated . the deduced ratio of the three gaussians is also correct , given by @xmath114 the plot of the evidence bound in figure ( [ fig : gmm3 ] ) peaks at the value of three , correctly indicating that most probably three gaussians were used to construct the mixture model .",
    "the code takes around 25 seconds to run when testing for 1 to 6 gaussians .",
    "@xmath73{./figures / ch4gmmeasylhood.pdf }   &    \\includegraphics[width=1.6in]{./figures / ch4gmmvbeasyoccam.pdf }    \\end{array}$ ] @xmath79{./figures / ch4gmmeasyevidence.pdf }   \\\\ \\end{array}$ ]      we now apply the nested sampling algorithm to tackle the gaussian mixture model data . as a stopping condition",
    "we impose that the difference between successive values in @xmath115 should be less than @xmath112 and that the condition in equation ( [ equ : stopping ] ) is satisfied . in the results quoted here we utilize 50 objects in order to ensure a decent algorithmic speed .",
    "once again the nested sampling implementation is considerably slower , with the code taking approximately 8 minutes to try out 1 to 6 gaussians .",
    "again , as with polynomial fitting , we can attribute the difference in computational speed to the random nature of the nested sampling algorithm .",
    "we note here that for both polynomial fitting as well as gaussian mixture models the nested sampling implementation is our own and hence further work might be able to improve the computational speed .",
    "however , we do not believe that any increase in speed for nested sampling will be able to improve the computational time to variational bayes levels . in figure",
    "[ fig : gmm3ns ] we show the evidence against the number of gaussians used for fitting as well as the optimal fit using three gaussians .",
    "@xmath73{./figures / ch4gmmnestedfit3.pdf }   &    \\includegraphics[width=1.6in]{./figures / ch4gmmnestedeev.pdf } \\\\",
    "\\end{array}$ ]    we first note that the figure gives a peak at the correct value of 3 , indicating that 3 gaussians were used to generate the data set . for the optimal fit the three gaussians have means @xmath116 , sigmas @xmath117 and are produced in the ratio @xmath118 .",
    "these are very close to the true values which were used to generate the data set and in particular we note that the ratios are calculated extremely well .",
    "differing runs of the nested sampling algorithm , particularly when we run with more stringent stopping conditions , result in parameter values which differ by @xmath119 at most . for a run with a stopping condition of @xmath112 we get @xmath120 , @xmath121 and ratio = @xmath122 .",
    "we also test both algorithms using a harder data set where there is far more overlap in the gaussians . to perform this analysis",
    "we generate 600 data points with means @xmath123 , leaving the other parameters as before .",
    "the results are again favourable . in particular the evidence bound peaks at the correct value of 3 .",
    "also the inferred parameters of the 3 fitted gaussians are largely in line with the true values : @xmath124 the ratios are given by : @xmath125 the largest discrepancy is in the ratio values .",
    "this probably stems from fact that because the gaussians are so close to each other it is harder to determine from which gaussian each point has been generated .",
    "these results are shown in figure [ fig : gmmhardvb ] .",
    "@xmath126{./figures / ch4gmmvbfit3hard.pdf }   &    \\includegraphics[width=1.6in]{./figures / ch4gmmzhardvb.pdf } \\\\",
    "\\end{array}$ ]    investigating the same data set with nested sampling we get the following results .",
    "@xmath126{./figures / oxfharddatafit3.pdf }   &    \\includegraphics[width=1.6in]{./figures / oxfharddataevidence.pdf } \\\\",
    "\\end{array}$ ]    the gaussian positions returned by the algorithm are given by : @xmath127 nested sampling produces slightly better values for the gaussian means and widths but the ratios are again incorrect .",
    "the values are however within one standard deviation of the true results .",
    "again there is good agreement between the variational bayes and nested sampling methods and differences in the results from the true values are reflected in both techniques .",
    "this indicates that much of the imperfections in the inference are due to the random nature of the generated data and not due to the algorithmic deficiencies .",
    "virtually all experimental physical analysis done today is somewhat statistical in nature and therefore necessitates the use of powerful computational tools .",
    "the bayesian framework is one such set of tools which is gaining popularity within the scientific community .",
    "our main conclusion is that whilst both variational bayes and nested sampling give similar and accurate results when tackling engineered problems the former is much faster than the latter .",
    "inferred parameters using the two different techniques are often within a few percent of each other and the peaks in the evidence plots always agree .",
    "the discrepancy in speeds was attributed to the fact that nested sampling is an mcmc method and is dependent on random exploration of a parameter space while variational bayes is analytical and has no such probabilistic element .",
    "we do however point out that it is far easy to formulate a bayesian solution to a problem using nested sampling than using variational bayes as in the latter case one must derive the update equations for each problem and this might be a non - trivial process .",
    "99 feroz , f ; hobson , m.p .",
    "multimodal nested sampling : an efficient and robust alternative to mcmc methods for astronomical data kullback , s. ( 1959 ) .",
    "information theory and statistics , * 6 * , 508 - 509 .",
    "kullback s ; r. a. leibler ( 1951 ) . on information and sufficiency , _",
    "annals of mathematical statistics _ , * 22 * , 79 - 86 .",
    "penny , w.d ; roberts , s.j .",
    "variational bayes for d - dimensional mixture models .",
    "available online : www.fil.ion.ucl.ac.uk/  wpenny / publications / vbgmm.ps , last accessed 14 february 2009 .",
    "penny w.d ; roberts , s.j .",
    "variational bayes for 1-dimensional mixture models .",
    "technical report parg-2000 - 01 department of engineering science , oxford .",
    "available online : www.fil.ion.ucl.ac.uk/wpenny/publications/vbmog.ps , last accessed 23 december 2008 .",
    "skilling , j. ( 2004 ) .",
    "bayesys homepage available online : http://www.inference.phy.cam.ac.uk/bayesys/ , last accessed on 17 november 2008 .",
    "sivia , d.s ; skilling , j. ( 2005 ) , _ data analysis : a bayesian tutorial ; _ oxford science publications : oxford , ch .",
    "sivia , d.s .",
    "( 2005 ) _ data analysis : a bayesian tutorial ; _ oxford science publications : oxford .",
    "skilling , j. ( 1998 ) .",
    "probabilistic data analysis , _ jour .",
    "microscopy _ , 190 , 28 - 36 .",
    "skilling , j. ( 2006 ) .",
    "nested sampling for bayesian computation , _ bayesian analysis _ * 1 * , p. 833 - 860 .",
    "zarb adami , k. ( 2003 ) bayesian inference and deconvolution , phd thesis , astrophysics group , cavendish laboratory , university of cambridge .",
    "we describe the distributions used throughout this paper as well as their notation .",
    "* gaussian distribution * the gaussian ( or normal ) distribution is given by the equation [ equ : gauss ] , where we consider all previous knowledge to be encoded in @xmath2 @xmath128 here @xmath129 is the mean and @xmath130 is the inverse variance .",
    "hence the standard deviation , @xmath131 given by the square root of the variance can be expressed as follows : @xmath132 useful expectation values under the gaussian distribution are : @xmath133    * multivariate gaussian distribution * the gaussian can be generalised to @xmath134 dimensions , in which case it is called a multivariate gaussian : @xmath135 here @xmath136 is a vector containing the mean in each dimension and @xmath137 is the symmetric and positive definite inverse covariance matrix .",
    "this is necessarily a symmetric matrix because the cross terms are related to the covariances of the variables and the covariance relation is necessarily a symmetric one . when the variables are independent the covariance matrix , and hence the inverse covariance matrix , is of diagonal form .",
    "useful expectation values under the multivariate gaussian distribution are : @xmath138 * gamma distribution * the gamma distribution crops up in the bayesian framework because it is the conjugate distribution for the inverse variance of a gaussian distribution .",
    "the distribution itself is given by : @xmath139 the constant @xmath140 is a scale variable and @xmath141 dictates the shape of the distribution .",
    "some useful expectation values are : @xmath142 the derivative @xmath143 is known as the _ digamma _ function and is often denoted by @xmath144 . *",
    "dirichlet distribution * the dirichlet distribution is used to model categorical weights , particularly in the gaussian mixture model setting . the probability distribution function for a dirichlet describing @xmath22 weights",
    "is given by : @xmath145 the parameter @xmath146 is called the mixing hyperparameter .",
    "the word hyperparameter is often used in bayesian statistics to denote parameters in priors and to distinguish them from the actual parameters of the underlying model under investigation .",
    "when the @xmath147 are all equal we get the symmetric dirichlet distribution : @xmath148 the expectation value of @xmath149 is given by : @xmath150"
  ],
  "abstract_text": [
    "<S> this paper focuses on utilizing two different bayesian methods to deal with a variety of toy problems which occur in data analysis . </S>",
    "<S> in particular we implement the variational bayesian and nested sampling methods to tackle the problems of polynomial selection and gaussian mixture models , comparing the algorithms in terms of processing speed and accuracy . in the problems tackled here it is the variational bayesian algorithms which are the faster though both results give similar results .    </S>",
    "<S> [ firstpage ]    bayesian , variational bayes , nested sampling , evidence </S>"
  ]
}