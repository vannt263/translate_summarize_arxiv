{
  "article_text": [
    "neural networks have been popular in the machine learning community since the 1980s with repeating rises and falls of popularity .",
    "their main benefit is their ability to learn complex , non - linear hypotheses from data without the need of modeling complex features .",
    "this makes them of particular interest for computer vision , in which feature description is a long - standing and largely non - understood topic .",
    "neural networks are difficult to train and for the last ten years they have come to enormous fame under the topic `` deep learning '' .",
    "new advances in training methods and the movement of training from cpus to gpus allow to train more reliable models much faster .",
    "deep neural networks are not a silver bullet , as training is still heavily based on model selection and experimentation .",
    "overall , significant progress in machine learning and pattern recognition has been made in natural language processing , computer vision and audio processing .",
    "leading it companies have made significant investments into deep learning for these reasons , such as baidu , google , facebook and microsoft .    concretely , previous work of the author on deep learning for facial expression recognition in @xcite resulted in a deep neural network model that significantly outperformed the best contribution to the 2013 kaggle facial expression competition @xcite .",
    "therefore , a further investigation on the recognition of action units and in particular smile using deep neural networks and convolutional neural networks seems desirable .",
    "only very few works on this topic have been reported so far , such as in @xcite .",
    "it would also be interesting to compare the input of the entire face versus the mouth to study differences in the performance of deep convolutional models .",
    "this chapter provides an overview of different types of neural networks , their capabilities and training challenges , based on @xcite .",
    "this chapter does not provide an introduction to neural networks , the reader is therefore referred to @xcite and @xcite for a comprehensive introduction to neural neural networks .",
    "neural networks are inspired by the brain and composed of multiple layers of logistic regression units , called neurons .",
    "they experienced different periods of hypes in the 1960s and 1980s/90s .",
    "neural networks are known to be able to learn complex hypotheses for regression and classification .",
    "conversely , training neural networks is difficult , as their cost functions have many local minima .",
    "hence , training tends to converge to a local minimum , resulting in poor generalization of the network .",
    "for the last ten years , neural networks have been celebrating a comeback under the term deep learning , taking advantage of many hidden layers in order to build more powerful machine learning algorithms .",
    "feed - forward neural networks are the simplest type of neural networks .",
    "they are composed of an input layer , one or more hidden layers and an output layer , as visualized in figure  [ fig : nn ] .     and @xmath0 @xcite.,scaledwidth=60.0% ]    using learned weights @xmath1 or @xmath2 , they propagate an input through the network to the output to make predictions .",
    "the activation of unit @xmath3 of layer @xmath4 can be calculated as follows : @xmath5    @xmath6 is an activation function , for which often the sigmoid activation function @xmath7 is used in the hidden layers . the sigmoid function or its generalization , the softmax function , are used for classification problems in the output layer units . for regression problems ,",
    "the sum of equation  [ eq : activation ] is used directly in the output layer without the use of any activation functions .    in order to learn the weights ,",
    "a cost function is minimized .",
    "there are different cost functions , such as the least squares or cross - entropy cost function , described in @xcite .",
    "the latter one has been reported to generalize better and speed up learning as discussed in @xcite .      in order to learn the weights , algorithm  [ alg : backpropagation ] named backpropagation",
    "is used to efficiently compute the partial derivatives , which are then fed into an optimization algorithm , such as gradient descent ( algorithm  [ alg : gradientdescent ] ) or stochastic gradient descent ( algorithm  [ alg : stochasticgradientdescent ] ) , as described in @xcite .",
    "those three algorithms are based on @xcite .",
    "@xmath8 ( for all @xmath9 ) @xmath10 ( for all @xmath9 ) @xmath11 perform forward propagation to compute @xmath12 for @xmath13 using @xmath14 , compute @xmath15 compute @xmath16 : @xmath17 @xmath18 @xmath19    @xmath20 ( simultaneously for all @xmath21 )    randomly shuffle data set @xmath22 ( simultaneously for all @xmath21 )    generally , the more units in a neural network , the higher its expressional complexity .",
    "in contrast , the more units , the more it tends to overfit . to prevent overfitting ,",
    "various approaches have been described in the literature , including @xmath23/@xmath24 regularization @xcite , early stopping , tangent propagation @xcite and dropout @xcite .",
    "deep neural networks use many hidden layers .",
    "this allows to learn increasingly more complex features hierarchies , as visualized in figure  [ fig : nn_example ] for the google brain @xcite .",
    "such architectures are of enormous benefit , as the long - standing problem of feature description in signal processing disappears to a large extend .",
    "conversely , training of deep neural networks gets more difficult because of the increased number of parameters . as described in @xcite and @xcite , backpropagation does not scale to deep neural networks : starting with small random initial weights , the backpropagated partial derivatives go towards zero . as a result",
    ", training becomes infeasible and is called the vanishing gradient problem .      for deep neural networks ,",
    "training has therefore been split in two parts : pre - training and fine - tuning .",
    "pre - training allows to initialize the weights to a location in the cost function which can be optimized quickly using regular backpropagation .",
    "various pre - training methods have been described in the literature .",
    "most prominently , unsupervised methods , such as restricted boltzmann machines ( rbm ) in @xcite and @xcite or autoencoders in @xcite and @xcite are used .",
    "both methods learn exactly one hidden layer .",
    "this hidden layer is then used as input to the next rbm or autoencoder to learn the next hidden layer .",
    "this process can be repeated for many times in order to pre - train a so - called deep belief network ( dbn ) or stacked autoencoder , composed of rbms or autoencoders respectively .",
    "in addition , there are denoising autoencoders defined in @xcite , which are autoencoders that are trained to denoise corrupted inputs . furthermore",
    ", other methods such as discriminative pre - training @xcite or reduction of internal covariance shift @xcite have been reported as effective training methods for deep neural networks .      in the past ,",
    "mostly sigmoid units have been used in the hidden layers , with sigmoid or linear units in the output layer for classification or regression , respectively . for classification ,",
    "the softmax activation is preferred in the output layer .",
    "as described by norvig in @xcite , the output of a set unit is much stronger than the others .",
    "another benefit of softmax is that it is always differentiable for a weight .",
    "recently , the so - called rectified linear unit ( relu ) has been proposed in @xcite , which has been used successfully in many deep learning applications .",
    "figure  [ fig : activation_functions ] visualizes the sigmoid and relu functions .",
    "relu has a number of advantages over sigmoid , reported in @xcite and @xcite .",
    "first , it is much easier to compute as it is either @xmath25 or the input value . also , sigmoid has for non - activated input values less than or equal to @xmath25 an activation value of greater than @xmath25 .",
    "in contrast , relu models biological behavior of neurons more accurately , as it is @xmath25 for those cases . with many units set to @xmath25 ,",
    "a sparse activation of the networks follows , which is another form of regularization .",
    "furthermore , the vanishing gradient problem becomes less of an issue as relu units result in a simpler cost function .",
    "last , for some experiments , relu reduces the importance of pre - training or may not be necessary at all .      in the context of this project ,",
    "deep neural networks have been successfully applied to facial expression recognition in @xcite . in that study ,",
    "rbms , autoencoders and denoising autoencoders were compared on a noisy dataset from a 2013 kaggle challenge named `` emotion and identity detection from face images '' @xcite .",
    "this challenge was won by a neural network presented in @xcite , which achieved an error rate of 52.977% . in @xcite ,",
    "a stacked autoencoder was trained with an error of 39.75% . in a subsequent project , this error could be reduced further to 28% with a stacked denoising autoencoder @xcite .",
    "this study also showed that deep neural networks are a promising machine learning method for this context , but not a silver bullet as data pre - processing and intensive model selection are still required .",
    "recurrent neural networks ( rnns ) are cyclic graphs of neurons as displayed in figure  [ figure : recurrentneuralnetwork ] .",
    "( a12 ) ; ( a22 ) ; ( a32 ) ; ( x1 ) ; ( x2 ) ; ( x3 ) ; ( l3 ) ; ( x1 ) edge[- > ] ( a12 ) ( x1 ) edge[- > ] ( a22 ) ( x1 ) edge[- > ] ( a32 ) ( x2 ) edge[- > ] ( a12 ) ( x2 ) edge[- > ] ( a22 ) ( x2 ) edge[- > ] ( a32 ) ( x3 ) edge[- > ] ( a12 ) ( x3 ) edge[- > ] ( a22 ) ( x3 ) edge[- > ] ( a32 ) ( a12 ) edge[- > ] ( l3 ) ( a22 ) edge[- > ] ( l3 ) ( a32 ) edge[- > ] ( l3 ) ( a32 ) edge [ -latex , bend left , ultra thick ] node ( x3 ) ;    they have increased representational power as they create an internal state of the network which allows them to exhibit dynamic temporal behavior .",
    "training rnns is more complex as this depends on their structure .",
    "the rnn in figure  [ figure : recurrentneuralnetwork ] can be trained using a simple variant of backpropagation . in practice ,",
    "recurrent networks are more difficult to train than feedforward networks and do not generalize as reliably .",
    "a long short - term memory ( lstm ) defined in @xcite is a modular recurrent neural network composed of lstm cells .",
    "a lstm cell is visualized in figure  [ fig : lstm_cell ] .",
    "inputs @xmath26 are fed in , for which a value @xmath6 is computed using the sigmoid function of the dot product of the input and weights .",
    "the second sigmoid unit @xmath27 is the input gate .",
    "if its output value is near to zero , the product @xmath28 is near to zero , too , thus zeroing out the input value . as a consequence ,",
    "this blocks the input value , preventing it from going further into the cell .",
    "the third sigmoid unit @xmath29 is the output gate .",
    "its function is to determine when to output the internal state of the cell .",
    "this is the case when the output of this sigmoid unit is close to one .",
    "lstm cells can be put together in a modular structure , as visualized in figure  [ fig : lstm_example ] to build complex recurrent neural networks .",
    "training lstms takes advantage of backpropagation through time , a variant of backpropagation .",
    "its goal is to minimize the lstm s total cost on a training set .",
    "lstms have been reported to outperform regular rnns and hidden markov models in classification and time series prediction tasks .",
    "lstms have also been reported in @xcite to perform well on prediction of image sequences .",
    "invariance to transformations is a desired property of learning algorithms .",
    "typical variances of images and videos include translation , rotation and scaling .",
    "tangent propagation @xcite is one method in neural networks to handle transformations by penalizing the amount of distortion in the cost function .",
    "convolutional neural networks ( cnns ) are a different approach to implementing invariance in neural networks , which are inspired by biological processes .",
    "cnns were initially proposed by lecun in @xcite .",
    "they have been successfully applied to computer vision problems , such as hand - written digit recognition .    in images ,",
    "nearby pixels are strongly correlated , a property of which local features take advantage of . in a hierarchical approach ,",
    "local features are used in the first stage of pattern recognition , allowing recognition of more complex features .",
    "the concept of cnns is illustrated in figure  [ fig : cnn_example ] for a layer of convolutional units , followed by a sub - sampling layer , as described in @xcite .",
    "the convolutional layer is composed of so - called feature maps .",
    "units in a feature map take inputs from a small subregion of the input .",
    "all units in a feature map share the same weights , which is called weight sharing . replicating units in this way allows for features to be detected independently of their position in the visual field .",
    "the subsampling layer takes small regions of convolutional layer as input and computes the average ( or maximum or other functions ) of those inputs , multiplied by a weight and finally applies the sigmoid function to the value .",
    "the result of a unit in the subsampling layer is relatively insensitive to small shifts or rotations of the image in the corresponding regions of the input space .",
    "this concept can be repeated for more times to subsequently be more invariant and to detect more complex features .    because of the constraints of weights , the number of independent parameters in the network is smaller than in a fully - connected network . this allows to train the network faster and to be less prone to overfitting .",
    "training of cnns requires minimization of a cost function .",
    "the idea of backpropagation can be applied to cnn with a small modification taking into account the weight sharing .",
    "recently , cnns have been reported to work well on processing of image sequences , for example in @xcite for multiple convolutions , as visualized in figure  [ fig : multiple_convolutions ] .",
    "a related approach is reported in @xcite .",
    "cnns are expanded to work on image sequences instead of single images .",
    "the extra weights need to be initialized in a way so that training can easily optimize them .",
    "an extensive study and comparison of different initialization methods is provided in @xcite .",
    "@xcite describes a deep architecture composed of convolutions , lstms and regular layers for a nlp problem .",
    "it begins with multiple convolutional layers .",
    "next , a linear layers follows with fewer units in order to reduce the dimensionality of the features recognized by the convolutional layers .",
    "next , the reduced features are fed into a lstm . the output of the lstm",
    "is then used in regular layers for classification .",
    "the entire architecture is visualized in figure  [ fig : full_architecture ] .",
    "similar architectures exist for processing of image sequences and are elaborated further .",
    "very successful results using fusion of different video inputs have been reported , too .",
    "for example , a reported architecture in @xcite fuses a low - resolution version of the input with a higher - resolution input of the center of the video .",
    "this is visualized in figure  [ fig : fusion_center ] .",
    "conversely , @xcite fuses a low - resolution version of the input with the optical flow , as visualized in figure  [ fig : fusion_flow ] .",
    "the final stage of video classification can alternatively be done by a different classification , such as a support vector machine ( svm ) .",
    "this is described in @xcite and visualized in figure  [ fig : final_svm ] .",
    "furthermore , a spatio - temporal convolutional sparse autoencoder for sequence classification is described in @xcite .",
    "in this chapter , various popular databases relevant to action unit recognition are presented .",
    "each database includes annotations per frame of the respective action units , among other features .",
    "furthermore , statistics of the distribution of action units were generated for each database in order to select databases rich of smiles .",
    "the facial action coding system ( facs ) is a system to taxonomize any facial expression of a human being by their appearance on the face .",
    "it was published by paul ekman and wallace v. friesen in 1978 @xcite .",
    "relevant to this thesis are so - called action units ( aus ) , which are the basic actions of individual facial muscles or groups of muscles .",
    "action units are either set or unset . if set , different levels of intensity are possible .",
    "popular databases in the field of action unit recognition and studies of facial expressions include the following , which are presented briefly in this section .",
    "the reader is referred to the relevant literature for details .",
    "the affectiva - mit facial expression dataset ( amfed ) @xcite contains 242 facial videos ( 168,359 frames ) , which were recorded in the wild ( real world conditions ) .",
    "the chinese academy of sciences micro - expression ( casme ) @xcite database was filmed at 60fps and contains 195 micro - expressions of 22 male and 13 female participants .",
    "the denver intensity of spontaneous facial action ( disfa ) @xcite database contains videos of 15 male and 12 female subjects of different ethnicities .",
    "action unit annotations are on different levels of intensity",
    ". the geneva multimodal emotion portrayals ( gemep ) @xcite contains audio and video recordings of 10 actors which portray 18 affective states . the mahnob laughter @xcite database contains 22 subjects recorded using a video camera , a thermal camera and two microphones . recorded were laughter , posed smiles , posed laughter and speech .",
    "it includes 180 sessions with a total duration of 3h and 49min .",
    "the unbc - mcmaster shoulder pain expression archive database @xcite contains 200 video sequences of participants that were suffering from shoulder pain and their corresponding spontaneous facial expressions . in total , it includes 48,398 facs coded frames .",
    "for the databases presented in the previous section , statistics of the annotations of action units were generated .",
    "this task has proven to be complex , as the structure of each database is different and need to be parsed accordingly .",
    "comprehensive plots and statistics of the individual action units were generated .",
    "for example , figure  [ fig : casme_stat ] represents the binary distribution of au12 , which represents smile in facs coding , of the casme database .",
    "table  [ table : some_stats ] contains a selection of action units of the different databases . due to different terminology ,",
    "the amfed database does not use au12 , but a feature called `` smile '' as explained in @xcite .",
    ".selected statistics of action units in databases : an integer denotes the number of frames in which an action unit is set ( intensity @xmath30 ) .",
    "a hyphen indicates that an action unit is not available in a database .",
    "[ cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]"
  ],
  "abstract_text": [
    "<S> this thesis describes the design and implementation of a smile detector based on deep convolutional neural networks . </S>",
    "<S> it starts with a summary of neural networks , the difficulties of training them and new training methods , such as restricted boltzmann machines or autoencoders . </S>",
    "<S> it then provides a literature review of convolutional neural networks and recurrent neural networks . in order to select databases for smile recognition , </S>",
    "<S> comprehensive statistics of databases popular in the field of facial expression recognition were generated and are summarized in this thesis . </S>",
    "<S> it then proposes a model for smile detection , of which the main part is implemented . </S>",
    "<S> the experimental results are discussed in this thesis and justified based on a comprehensive model selection performed . </S>",
    "<S> all experiments were run on a tesla k40c gpu benefiting from a speedup of up to factor 10 over the computations on a cpu . a smile detection test accuracy of 99.45% </S>",
    "<S> is achieved for the denver intensity of spontaneous facial action ( disfa ) database , significantly outperforming existing approaches with accuracies ranging from 65.55% to 79.67% . </S>",
    "<S> this experiment is re - run under various variations , such as retaining less neutral images or only the low or high intensities , of which the results are extensively compared . </S>"
  ]
}