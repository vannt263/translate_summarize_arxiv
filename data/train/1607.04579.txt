{
  "article_text": [
    "in many learning problems , the input @xmath6 is continuous and associated with a conditional distribution @xmath1 .",
    "we want to learn a function @xmath2 which links the conditional distribution to target @xmath7 : @xmath8)}\\ ] ] where @xmath9 is a convex loss function , and @xmath10 is a linear function in a suitably chosen ( nonlinear ) feature space",
    ". here are two examples :    * * policy evaluation in reinforcement learning . * estimating the value function , also called policy evaluation , is recognized as a fundamental task in reinforcement learning . given a policy @xmath11 , the value function @xmath12 over the state @xmath13 of an agent is the fixed point of the bellman equation @xmath14 $ ] where @xmath15 is a reward function .",
    "the value funciton can be learned from data by minimizing the mean - square bellman error ( msbe )  @xcite : @xmath16)^2 ] = \\ee_{s\\sim \\mu(s)}\\bigg[\\big(\\ee_{a\\sim \\pi_b(a|s ) , s'\\sim p(s'|s , a)}[\\rho(a|s)\\delta(s ' , a , s)|s]\\big)^2\\bigg ]      \\end{aligned}\\ ] ] where @xmath17 is the state distribution , @xmath18 the behavior policy , @xmath19 the importance weight , @xmath20 , and @xmath21 is the transition of the markov decision process .",
    "typically , one parameterizes @xmath12 as a linear function of features defined on @xmath13 , namely @xmath22 .",
    "therefore , this optimization becomes a special case of  , if we set @xmath23 , @xmath24 , @xmath25 , @xmath26 , and @xmath27 .",
    "due to the online fashion of mdp , we usually observe only one successor state @xmath28 for each action @xmath29 ,  , only one sample from the conditional distribution given @xmath13 . + * * learning with invariance . * incorporating prior on invariance into learning procedure is crucial for computer vision  @xcite , speech recognition  @xcite and many other applications .",
    "the goal of invariance learning is to estimate a function which minimizes the expected risk while at the same time preserving consistency over a group of operations @xmath30 .",
    "@xcite shows that this can be accomplished through solving the following optimization problem @xmath31 ) ]   + ( \\nu/2)\\|f\\|_{\\hcal}^2\\ ] ] where @xmath32 is the reproducing kernel hilbert space ( rkhs ) corresponding to some group - invariant haar - integral kernel @xmath33 and @xmath34 is the feature map . clearly , ( [ eq : invariant ] ) is a special case of ( [ eq : target ] ) with the conditional probability being some normalized haar measure @xmath35 given @xmath0 . due to computation and memory constraints , in practice",
    ", one can only afford to generate a few virtual samples from each data point @xmath0 .",
    "+    [ [ challenges . ] ] challenges .",
    "+ + + + + + + + + + +    despite the prevalence of learning problems in the form of , solving such problem remains very challenging for two reasons : ( _ i _ ) we often have limited samples or in the extreme case only one sample from each conditional distribution @xmath1 , making it difficult to accurately estimate the conditional expectation .",
    "( _ ii _ ) the conditional expectation is nested inside the loss function , making the problem quite different from the traditional stochastic optimization setting .",
    "this type of problem is called _ compositional stochastic programming _ , where very few results have been established .",
    "[ [ related - work . ] ] related work .",
    "+ + + + + + + + + + + + +    a simple option to address ( [ eq : target ] ) is use sample average approximation ( saa ) , and solve instead @xmath36 $ ] , where @xmath37 , and @xmath38 for each @xmath39 . to ensure an excess risk of @xmath40 , both @xmath41 and @xmath42 need be at least as large as @xmath43 , making the overall sample required to be @xmath44 ; see  @xcite and references therein .",
    "hence , when @xmath42 is small , saa would provide poor results .",
    "a second option is resorting to stochastic gradient methods ( sgd ) .",
    "one can construct a stochastic estimate of the gradient using @xmath45 where @xmath46 is an estimate of @xmath47 $ ] for any @xmath0 . to ensure convergence",
    ", one needs to make the bias of the stochastic gradient small ,  , a large amount of samples from the conditional distribution is , again , needed .",
    "another commonly used approach is to first represent the conditional distributions as the so - called kernel conditional embedding , and then perform a supervised learning step on the embedded distribution  @xcite .",
    "such an approach is not ideal in the sense that it is a two - step procedure , and the estimation of kernel conditional embedding itself is a very difficult task .",
    "a recent work @xcite , is closely related to but fundamentally distinct from our problem of interest . in @xcite ,",
    "the authors consider problems in the form , @xmath48)}\\ ] ] where @xmath49 is some smooth function parameterized by @xmath50 .",
    "although the problem allows to handle dependent pairs @xmath51 , one should note that solving problem ( [ eq : wanfanliu ] ) is much easier than our problem of interest ( [ eq : target ] ) since estimating the inner expectation in ( [ eq : wanfanliu ] ) does not require learning from conditional distribution at all .",
    "the authors provide an algorithm that combines stochastic gradient descent with moving average estimation for the inner expectation , and achieves an overall @xmath52 sample complexity for smooth convex loss functions .",
    "however , such an algorithm , does not apply to the more general and difficult situation we consider in this paper .",
    "[ [ overview - of - our - approach - and - contribution . ] ] overview of our approach and contribution .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    to address the above challenges , we propose an approach called _ dual kernel embedding_. the key idea is to reformulate the problem into a min - max or saddle point problem by utilizing the fenchel dual of the loss function .",
    "we observe that with smooth loss function and continuous conditional distributions , the dual variable becomes a continuous function of @xmath0 .",
    "we therefore can parameterize it as a function in the reproducing kernel hilbert space ( rkhs ) induced by any universal kernel , where the information about the marginal distribution @xmath53 and conditional distribution @xmath1 can be aggregated via a kernel embedding of the joint distribution @xmath54 .",
    "furthermore , we propose an efficient algorithm based on stochastic approximation to solve the resulting saddle point problem over rkhs spaces , and establish finite - sample analysis of the generic learning problem from conditional distributions .    compared to previous applicable approaches , a marked advantage of the proposed method is that it requires only _ one sample _ from each conditional distribution . under suitable conditions , the overall sample complexity reduces to @xmath43 in contrast to the @xmath44 complexity required by saa . as a by - product , even in the degenerate case ( [ eq : wanfanliu ] ) , this implies an @xmath43 sample complexity when inner function is linear , which already surpasses the result obtained in @xcite and is known to be unimprovable .",
    "furthermore , our algorithm is generic for the family of problem of learning from conditional distribution , and can be adapted to problems with different loss functions and function spaces .",
    "interestingly , our proposed method also provides some new insight to the related applications . in reinforcement learning settings ,",
    "our method provides the first algorithm that truly minimizes the mean - square bellman error with both theoretical guarantees and sample efficiency .",
    "we show that some existing algorithms ,  gradient - td2 by  @xcite , are special cases of our algorithm . in the invariance",
    "learning setting , our method also provides a unified view of several existing methods for encoding invariances .",
    "finally , numerical experiments in both synthetic and real data show that our method can significantly improve over the previous state - of - the - arts .",
    "we first briefly introduce our notation on kernel and kernel embedding .",
    "let @xmath55 be some input space and @xmath56 be a positive definite kernel function .",
    "we use @xmath57 , @xmath58 and @xmath59 interchangeably to denote of the feature map of kernel @xmath33 .",
    "then @xmath33 induces a reproducing kernel hilbert space ( rkhs ) @xmath32 , which has the property @xmath60 , @xmath61 , where @xmath62 is the inner product and @xmath63 is the norm in @xmath32 .",
    "we denote all continuous functions on @xmath64 as @xmath65 and @xmath66 as the maximum norm .",
    "we call @xmath33 a _ universal kernel",
    "_ if @xmath32 is dense in @xmath67 for any compact set @xmath68 ,  , for any @xmath69 and @xmath70 , there exists @xmath71 , such that @xmath72 .",
    "examples of universal kernel include the gaussian kernel , @xmath73 , and laplace kernel , @xmath74 .",
    "[ [ function - approximation - using - rkhs . ] ] function approximation using rkhs .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    let @xmath75 be a bounded ball in the rkhs , and we define the error of approximating a continuous function @xmath76 by a fucntion @xmath77 in rkhs @xmath78 as @xmath79 one can immediately see that @xmath80 decreases as @xmath81 increases and vanishes to zero as @xmath81 goes to infinity . if @xmath65 is restricted to the set of uniformly bounded continuous functions , then @xmath80 is also bounded .",
    "the approximation property ,  ,  dependence on @xmath81 remains an open question for general rkhs , but has been carefully established for special kernels .",
    "for example , with the kernel @xmath82 induced by the sigmoidal activation function , we have @xmath83 for lipschitz continuous function space @xmath65  @xcite .    [ [ hilbert - space - embedding - of - distributions . ] ] hilbert space embedding of distributions .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    hilbert space embeddings of distributions are mappings of distributions into potentially _",
    "infinite _ dimensional feature spaces  @xcite , @xmath84 where the distribution is mapped to its expected feature map ,  ,  to a point in a feature space .",
    "kernel embedding of distributions has rich representational power .",
    "some feature map can make the mapping injective  @xcite , meaning that if two distributions , @xmath85 and @xmath86 , are different , they are mapped to two distinct points in the feature space . for instance , when @xmath87 , the feature spaces of many commonly used kernels , such as the gaussian rbf kernel , will generate injective embedding .",
    "we can also embed the joint distribution @xmath88 over a pair of variables using two kernels @xmath89 and @xmath90 as @xmath91 where the joint distribution is mapped to a point in a tensor product feature space . based on embedding of joint distributions , kernel embedding of conditional distributions",
    "can be defined as @xmath92 as an operator @xmath93  @xcite .",
    "to solve problem  , we propose a novel and sample - efficient method by leveraging fenchel duality and kernel embedding to bypass the difficulties with nested expectation and the need for overwhelmingly large sample from conditional distributions .",
    "let @xmath94 be the convex conjugate of the loss function @xmath95 in .",
    "that is , @xmath96 . invoking the fenchel duality ,",
    "our problem of interest can be reformulated as @xmath97\\cdot u(x ) - \\ell_y^*(u(x ) ) \\big]\\bigg].\\end{aligned}\\ ] ] note that the dual variables @xmath98 can be conceived as a ( possibly non - continuous ) function over @xmath64 .",
    "based on this observation , we can further rewrite ( [ eq : dual_opt ] ) as @xmath99 - \\ee_{xy}[\\ell_y^*(u(x ) ) ] , \\end{aligned}\\ ] ] where @xmath100 .",
    "the problem of interest , now reduces to a stochastic saddle point problem , without nested expectation , but with an additional functional dual space to optimize over .",
    "such reformulation has not been exploited in the compositional stochastic programming literature to the best of our knowledge .    by definition",
    ", @xmath101 is always concave in @xmath76 for any fixed @xmath2 . since @xmath10 is a linear function , @xmath101 is also convex in @xmath2 for any fixed @xmath76 .",
    "our reformulation ( [ eq : dual_opt_exchange ] ) is indeed a convex - concave saddle point problem .",
    "nevertheless , optimizing the saddle point problem over a functional dual space remains unfriendly due to the lack of compact representability of the dual function .",
    "although the reformulation in gives us more structure of the problem , it is not yet tractable .",
    "this is because the dual function @xmath102 can be an arbitrary function which we do not know how to represent efficiently . in the following",
    ", we will introduce a tractable representation for ( [ eq : dual_opt_exchange ] ) .",
    "first , we will define the function @xmath103 as the _ optimal dual function _ if for any @xmath104 , @xmath105 - \\ell_y^*(u)\\right\\}.\\ ] ] note the optimal dual function is well - defined since the optimal set is nonempty .",
    "furthermore , @xmath106 is related to the conditional distribution via @xmath107)$ ] .",
    "this can be simply derived from convexity of loss function and fenchel s inequality ; see  @xcite for a more formal argument .",
    "particularly , when @xmath108 , the optimal dual function @xmath109 $ ] . depending on the property of the loss function @xmath110 , we can further derive that :    [ prop : dualcontinuity ] if @xmath110 is continuously differentiable for any @xmath3 and both @xmath111 and @xmath1 are continuous in @xmath0 for any @xmath4 , then @xmath112 is unique and continuous .",
    "for instance , when it comes to the policy evaluation problem in ( [ eq : rl_obj ] ) , the corresponding optimal dual function is continuous as long as the reward function is continuous .",
    "see proofs in appendix  [ appendix : dualcontinuity ] .",
    "the fact that the optimal dual function is a continuous function has interesting consequences . as we mentioned earlier",
    ", the space of dual functions can be arbitrary and difficult to represent .",
    "now we can simply restrict the parametrization to the space of continuous functions .",
    "it is a smaller and more tractable function space , but it is not missing out the global optimum of the optimization problem in  .",
    "this also provides us a basis for using rkhs to approximate these functions , and simply optimizing over functions in rkhs .      recall that with the universal kernel , we can approximate any bounded continuous function with reasonably small error . in the rest of the paper , we assume conditions described in proposition  [ prop : dualcontinuity ] always hold ; namely , the optimal dual function is indeed continuous .",
    "thus we approximate the dual space @xmath113 by the bounded rkhs @xmath114 induced by a universal kernel @xmath33 .",
    "that is @xmath115 , and we can rewrite the saddle point problem in   as @xmath116 - \\ee_{xy}[\\ell_y^*(\\langle u , \\phi(x)\\rangle_{\\hcal } ) ] , \\nonumber\\\\   & = f^\\top \\ccal_{zx } u - \\ee_{xy}[\\ell_y^*(\\langle u , \\phi(x)\\rangle_{\\hcal } ) ] , \\end{aligned}\\ ] ] where we also assume that the function @xmath2 is a linear in some suitably chosen nonlinear feature space @xmath117 , and use the joint embedding of @xmath5 as @xmath118 $ ] .",
    "note that now there is no need to access the conditional distribution @xmath1 , or the conditional embedding operator @xmath119 .",
    "the new saddle point approximation ( [ eq : dual_approximate ] ) based on dual kernel embedding allows us to get away from the fundamental difficulty with insufficient sampling from the conditional distribution .",
    "given a pair of sample @xmath120 , where @xmath121 and @xmath122 , we can now easily construct an unbiased stochastic estimate for the gradient , namely , @xmath123 and @xmath124 \\phi(x)$ ] .",
    "for simplicity of notation , we use @xmath125 to denote the subgradient as well as the gradient . with the unbiased stochastic gradient",
    ", we are now able to solve the approximation problem ( [ eq : dual_approximate ] ) by resorting to the powerful mirror descent stochastic approximation framework  @xcite .",
    "the algorithm is summarized in algorithm  [ alg : stochastic_composite_general ] . at each iteration",
    ", the algorithm performs a projected gradient step both for the primal variable @xmath2 and dual variable @xmath76 based on the unbiased stochastic gradient .",
    "the proposed algorithm avoids the need for overwhelmingly large sample sizes from the conditional distributions when estimating of the gradient . at each iteration , only one sample from the conditional distribution is required in our algorithm !    throughout our discussion , we make the following standard assumptions :    [ asp : bounded ] there exists constant scalars @xmath126 , @xmath127 , and @xmath42 , such that for any @xmath128 , @xmath129{\\leqslant}m_\\fcal , \\quad \\ee_{x , z}[\\|\\psi(z , x)\\|_\\fcal^2]{\\leqslant}c_\\fcal,\\quad \\ee_{y}[\\|\\nabla \\ell^*_y(u)\\|_2 ^ 2]{\\leqslant}c_\\ell.\\ ] ]    [ asp : kernel ] there exists constant @xmath130 such that @xmath131 for any @xmath132 .",
    "assumption 1 and 2 basically suggest that the variance of our stochastic gradient estimate is always bounded .",
    "note that we do not assume any strongly convexity or concavity of the saddle point problem , or lipschitz smoothness .",
    "hence , we set the output as the average of intermediate solutions weighted by the learning rates @xmath133 , as often used in the literature , to ensure the convergence of the algorithm .",
    "@xmath134 +    [ alg : stochastic_composite_general ] sample @xmath135 and @xmath136 . @xmath137 . @xmath138\\phi(x_i))$ ] + @xmath139 , @xmath140    define the accuracy of any candidate solution @xmath141 to the saddle point problem as @xmath142    we have the following convergence result .",
    "[ thm : main ] under assumptions  [ asp : bounded ] and [ asp : kernel ] , the solution @xmath143 after @xmath144 steps of the algorithm with stepsizes being @xmath145 satisfies : @xmath146{\\leqslant}[(2d_{\\fcal}^2 + 4\\delta)/\\gamma+\\gamma \\ccal(\\delta,\\kappa ) ] \\frac{1}{\\sqrt{t}}\\ ] ] where @xmath147 and @xmath148 .",
    "the above theorem implies that our algorithm achieves an overall @xmath149 convergence rate , which is known to be unimprovable already for traditional stochastic optimizaiton with general convex loss function @xcite .",
    "we further observe that    [ prop : lipschitzianity ] if @xmath111 is uniformly bounded by @xmath150 and @xmath151 is uniformly @xmath152-lipschitz continuous in @xmath76 , then for any @xmath3 , @xmath101 is @xmath153-lipschitz continuous on @xmath113 with respect to @xmath66 , i.e. @xmath154    let @xmath155 be the optimal solution to ( [ eq : target ] ) . invoking the lipschitz continuity of @xmath156 and using standard arguments of decomposing the objective , we have @xmath157 combining proposition  [ prop : lipschitzianity ] and theorem  [ thm : main ] , we finally conclude that under the conditions therein , @xmath158{\\leqslant}\\ocal\\left(\\frac{\\delta^{3/2}}{\\sqrt{t } } + \\ecal(\\delta)\\right).\\ ] ] there is clearly a delicate tradeoff between the optimization error and approximation error .",
    "using large @xmath81 increases the optimization error but decreases the approximation error .",
    "when @xmath81 is moderately large ( which is expected in the situation when the optimal dual function has small magnitude ) , our dual kerrnel embedding algorithm can achieve an overall @xmath43 sample complexity when solving learning problems in the form of ( [ eq : target ] ) .",
    "for the analysis details , please refer to appendix  [ appendix : convergece_rate ] .",
    "despite the sample efficiency , there are two potential downsides of the algorithm - the computation overhead caused by projecting to the dual space and the memory requirement for maintaining the dual function . to this end , we introduce the regularized objevtive , which has been widely used as an alternative to the constrained problem in machine learning literatures , and is proven to be more robust often times in practice . motivated by @xcite , we also provide a doubly stochastic variant of the algorithm , by leveraging random feature to approximate kernel functions .    [",
    "[ regularized - objective . ] ] regularized objective .",
    "+ + + + + + + + + + + + + + + + + + + + + +    instead of solving ( [ eq : dual_approximate ] ) , we consider the alternative reformulation by penalizing the norm of the dual function , @xmath159 it is well - known that there is a one - to - one relation between @xmath81 and @xmath160 such that the optimal solutions to ( [ eq : dual_approximate ] ) and ( [ eq : dual_regularized ] ) are the same .",
    "the objective can also be regarded as a smoothed approximation to the original problem of our interest , see @xcite .",
    "problem ( [ eq : dual_regularized ] ) can be solved efficiently via our algorithm  [ alg : stochastic_composite_general ] by simply revoking the projection on the dual space .",
    "[ [ random - feature - approximation . ] ] * random feature approximation . * + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    instead of working with the infinite - dimensional spaces , we could also directly apply random feature approximation @xcite to rkhs and reduce it to a @xmath161-dimensional feature space .",
    "let @xmath162 $ ] , where @xmath163 are i.i.d samples from @xmath164 .",
    "we can further approximate ( [ eq : dual_approximate ] ) by @xmath165 notice that solving ( [ eq : dual_approximate_randomfeature ] ) will introduce an extra approximation error term in the order of @xmath166 and thus we do not lose any generalization ability if @xmath161 is sufficiently large .",
    "the random feature variant of our algorithm can be found in appendix  [ appendix : dual_random_fea ] .",
    "[ [ extension - to - doubly - sgd . ] ] * extension to doubly sgd . * + + + + + + + + + + + + + + + + + + + + + + + + + +    like most kernel methods , our * embedding - sgd * algorithm requires to maintain the samples @xmath167 when representing the dual function .",
    "however , this can be avoided by exploiting random features and the `` doubly '' stochastic gradient idea introduced in @xcite .",
    "let @xmath168 for some nonnegative measure @xmath164 . by replacing @xmath169 with random feature map @xmath170",
    ", we maintain a biased estimate of the gradient for reducing the memory cost to @xmath171 at each iteration .",
    "although the gradient estimation is no longer unbiased , it can be shown that the algorithm still converges to the same optimal solution in the corresponding rkhs with the optimal rate  @xcite .",
    "the doubly stochastic variant of our algorithm can be found in algorithm  [ alg : doublysgd ] in appendix  [ appendix : doublysgd ] .",
    "[ [ deep - kernel - embedding . ] ] * deep kernel embedding . * + + + + + + + + + + + + + + + + + + + + + + + +    while our dual kernel embedding scheme relies on a bounded rkhs w.r.t . @xmath172-norm .",
    "the embedding space for continuous functions is fully adapted to other choices of functional spaces , e.g. the convex neural networks with monotonic activation functions under the variation norm , as discussed in @xcite . under such embedding , we could treat the corresponding saddle point approximation with incremental frank wolfe algorithms instead of stochastic gradient methods and obtain similar generalization bounds .",
    "we leave out the details here since this is beyond our scope .",
    "in this section , we discuss in details how the dual kernel embedding can be applied to solve two important learning problems in machine learning , , policy evaluation in reinforcement learning and learning with invariance , which are the special cases of the optimization  .",
    "we provide sample - efficient algorithms tailored for the respective learning scenarios . due to the space limit , we focus only on algorithms with kernel embedding . extended algorithms with random feature and doubly sgd can be found in appendix  [ appendix : dual_random_fea ] and  [ appendix : doublysgd ]",
    ".      * policy evaluation . * given an mdp represented by @xmath173 , the goal is to estimate the value function that minimizes the mean - square bellman error ( [ eq : rl_obj ] ) .",
    "we consider the more advanced off - policy setting .",
    "let @xmath17 be the state distribution , @xmath18 the behavior policy and @xmath19 the importance weight .",
    "the learning problem ( [ eq : rl_obj ] ) can be rewritten as @xmath174\\big)^2\\bigg]\\end{aligned}\\ ] ] where @xmath175 , successor state @xmath176 , actions @xmath177 . applying dual kernel embedding",
    ", we end up solving the saddle point problem @xmath178- \\frac{1}{2}\\ee_{s}[u^2(s)].\\end{aligned}\\ ] ] we restrict the dual function to rkhs @xmath32 induced by a universal kernel @xmath33 and the value function to rkhs @xmath179 induced by another kernel @xmath180 .",
    "note that @xmath33 and @xmath180 are allowed to be different .",
    "let @xmath181 and @xmath182 denote the corresponding feature maps .",
    "at each iteration , our embedding - sgd algorithm uses one sample @xmath183 and updates the solution as follows @xmath184\\phi(s)\\\\ \\end{array}\\ ] ] note that @xmath185 is the discount factor , @xmath133 are the learning rates .",
    "the overall algorithm is summarized in algorithm  [ alg : policy_evaluation_kerel ] in appendix  [ appendix : kernel_algorithm ] .",
    "[ [ remark . ] ] * remark . *",
    "+ + + + + + + + +    if we use the same kernel for parametrizing value function and dual function , i.e. @xmath186 and @xmath187 , our saddle point problem ( [ eq : residual_approximate ] ) reduces to @xmath188\\big\\|^2_{\\ee[\\psi\\psi^\\top]^{-1}}. $ ] this is exactly the same as the mspbe objective proposed in  @xcite of gradient - td2 . in this perspective ,",
    "gradient - td2 is simply a special case of our algorithm [ alg : policy_evaluation_kerel ] .",
    "however , in view of our theory , there is really no need to set the same kernel for the value and dual functions .",
    "as further demonstrated in our experiments , using different kernels can improve the performences .",
    "see details in section [ subsec : rl ] .",
    "our algorithm is also fundamentally different from the td algorithm even in the finite state case .",
    "the td algorithm updates the state - value function directly by an estimate of the temporal difference based on one pair of samples , while our algorithm updates the state - value function based on accumulated estimate of the temporal difference , which intuitively is more robust .",
    "* invariance learning . *",
    "the goal is to solve the invariance learning problem ( [ eq : invariant ] ) . applying the dual kernel embedding",
    ", we end up solving the saddle point problem @xmath189 - \\ee_{xy}[\\ell_y^*(u(x ) ) ] + \\frac{\\nu}{2}\\|f\\|^2_{\\tilde{\\hcal}}.\\end{aligned}\\ ] ] where @xmath179 is the rkhs corresponding to the generating kernel for the group - invariant haar - integration kernel and @xmath32 is the rkhs to the universal kernel introduced in our method . at each iteration ,",
    "our algorithm obtains one sample @xmath190 and one virtual sample @xmath191 , and works as follows : @xmath192\\phi({x_i})\\\\ \\end{array}\\ ] ]    [ [ remark.-1 ] ] * remark . *",
    "+ + + + + + + + +    the proposed algorithm bares some similarities to virtual sample techniques  @xcite in the sense that they both create examples with prior knowledge to incorporate invariance .",
    "however , these algorithms are notably different .",
    "in fact , the virtual sample technique can be viewed as optimizing an upper bound of the objective   by simply moving the conditional expectation outside , while our algorithm directly works on the original objective .",
    "[ [ remark.-2 ] ] * remark .",
    "* + + + + + + + + +    recall the haar - integral kernel can be rewritten as @xmath193 , \\ee_{z'|x'}[\\psi(z ' ) ] \\rangle_{\\tilde{\\hcal}}$ ] , which can be viewed as a special case of hilbertian metric on probability measures  @xcite . from this perspective , injecting invariance by haar - integration kernel can be understood as learning with distributions over the invariant representations of observations .",
    "other kernels defined for distributions , , the probability product kernel  @xcite , can be also used in incorporating invariance .",
    "robust learning  @xcite can also be viewed as incorporating invariance prior w.r.t . the perturbation distribution into learning procedure .",
    "therefore , rather than resorting to robust optimization techniques  @xcite , the proposed algorithm for learning with invariance serves as a viable alternative for robust learning .",
    "we test the proposed algorithm for two applications , , policy evaluation and learning with invariant representation .",
    "we focus mainly on our basic kernel embedding algorithm in this section . for full details of our experimental setups",
    ", please refer to appendix  [ appendix : experiment_setup ] .",
    "more experimental results of our extended algorithms using random feature approximation can be found in appendix  [ appenidix : more_exp ] .",
    "we compare the proposed algorithm to several prevailing algorithms in reinforcement learning , including gradient - td2  ( gtd2 )  @xcite , residual gradient  ( rg )  @xcite and kernel mdp  @xcite in terms of mean square bellman error .",
    "unlike online algorithms such as gradient - td2 and residual gradient , the kernel mdp algorithm requires to visit the entire dataset when estimating the embedding and inner expectation in each iteration .",
    "we conduct experiments for policy evaluation on several benchmark datasets , including navigation , cart - pole swing up and pumc-560 manipulation .",
    "we use gaussian kernel for all algorithms and perform a parameter sweep over all step - size parameters and kernel bandwidth .",
    "all algorithms are evaluated based on the average performance of 10 runs .",
    "[ [ navigation . ] ] * navigation . * + + + + + + + + + + + + +    we first justify the proposed algorithm on the navigation experiment in an unbounded room , which extends the discretized mdp in  @xcite to continuous - state continuous - action mdp . specifically , the reward is @xmath194 centered in the middle of the room and @xmath195 .",
    "we evaluate the deterministic policy policy @xmath196 , following the gradient of the reward function .",
    "the transition distribution follows gaussian distribution , @xmath197 .",
    "results are reported in figure  [ fig : policy_evaluation](a ) .    [ cols=\"^,^,^ \" , ]",
    "we can extend the same technique to control problem .",
    "different from the policy evaluation in which the policy is provided , the control problem is trying to learn the optimal policy in terms of reward , , the policy which can achieve the maximum reward .",
    "we define the action - value function @xmath198 , which evaluates the value of taking action @xmath29 in state @xmath13 with policy @xmath11 for future actions , @xmath199.\\ ] ] based on the definition of the @xmath200 function , we have the bellman equation as @xmath201,\\end{aligned}\\ ] ] therefore , we obtain the optimization as @xmath202 - q(s , a ) \\bigg)^2\\bigg ] \\end{aligned}\\ ] ] whose objective is the same as sarsa .",
    "apply the dual kernel embedding , we obtain @xmath203 - q(s , a ) , u(s , a ) \\bigg\\rangle\\bigg ] - \\frac{1}{2}\\ee_{s , a}[u(s , a)^2],\\end{aligned}\\ ] ] which can be solved efficiently .",
    "recall the definition of @xmath204 function , our framework can be also extended to @xmath160-return for the estimation of @xmath205 , , @xmath206 where @xmath207 .    with the proposed @xmath204 function learning procedure",
    ", we can achieve the control target by _ policy iteration _ in algorithm  [ alg : policy_iteration ] .",
    "[ alg : policy_iteration ] initialize @xmath208 randomly update @xmath209 as @xmath40-greedy policy from @xmath210 sample @xmath211 , @xmath212 , @xmath213 and @xmath214 update @xmath215 by applying algorithm  [ alg : stochastic_composite_general ] to   @xmath216    the convergence of this procedure can be proved by taking the approximate error from @xmath204 function into account in the policy iteration convergence rate .",
    "@xcite already provides a framework for the analysis .",
    "we omit the details here due to the major contribution of this paper is introducing a general sample efficient algorithm solving the optimization with nested expectation .    besides the policy iteration procedure",
    ", we can also achieve optimal control by solving bellman optimal equation .",
    "we denote the optimal action - value function as @xmath217 , @xmath218 based on the definition of @xmath217 , we obtain the bellman optimal equation for @xmath217 , @xmath219.\\end{aligned}\\ ] ] analogously , we can formulate the objective for @xmath204-learning as @xmath220 - q(s , a ) \\bigg)^2\\bigg ] \\end{aligned}\\ ] ] obviously , for @xmath204-learning , we need the extra attention that the objective is no longer convex due to the maximum operator . although we can not guarantee the global optimality , the proposeed algorithm still converges to a local minima .",
    "in the policy evaluation experiment section , we compared the proposed algorithm with the mdps embedding using kernels and random features in corresponding settings for fairness .",
    "@xcite introduces the rkhs embedding of transition dynamics in mdps utilizing kernel conditional embedding  @xcite . with such nonparametric representation of the transition ,",
    "the expectation can be computed easily and accurately by linear operation , and thus , promoting the performance of policy evaluation and control task . for self - containing ,",
    "we specify the details of mdps embeddings with kernel and the random feature extension in this section .",
    "moreover , different from the value iteration algorithm with kernel embedded mdp in  @xcite , we propose the algorithm for policy evaluation utilizing ( functional ) stochastic gradient with both kernel and random feature conditional embeddings .      in mdps ,",
    "the transition dynamics , @xmath221 , plays a vital .",
    "many methods for solving mdps requires the computation with respect to the dynamics , , @xmath222 $ ] . by kernel conditional embedding  @xcite , such expectation can be estimated accurately and computed easily .",
    "specifically , given data @xmath223 , kernel @xmath224 and @xmath225 , the expectation for @xmath226 can be approximated by @xmath227 with @xmath228 as @xmath229 where @xmath230^\\top$ ] , @xmath231 , @xmath232^\\top$ ] and @xmath233_{i , j = 1}^m$ ] , @xmath160 is a regularization parameter .",
    "we can leverage the random feature to approximate the kernel function in kernel conditional embedding , which will result the random feature embedding for memory efficiency .",
    "denote the random features for @xmath180 and @xmath33 are @xmath234 and @xmath235 with respect to @xmath236 and @xmath164 respectively , we can approximate the kernel conditional embedding   by @xmath237 where @xmath238^\\top\\in \\rr^{p\\times 1}$ ] , @xmath239\\in \\rr^{p\\times 1}$ ] , @xmath240\\in \\rr^{p\\times m}$ ] and @xmath241\\in \\rr^{p \\times m}$ ] . then , with random feature represented @xmath242 , the expectation @xmath222 $ ] can be approximated by @xmath243 .",
    "as we introduced , with the kernel and random feature embedded transition dynamics , the expectation can be estimated with linear operator .",
    "instead of plugging the embedded mdp into value iteration for policy evaluation in  @xcite , which is not suitable for dynamics random feature embeddings , we propose new algorithms which apply the ( functional ) stochastic gradient algorithm to regularized bellman error   with respect to @xmath244 in kernel representation or @xmath245 in random feature representation , we have @xmath246)\\\\ \\nabla_\\theta l & = & \\rho(a|s)(\\theta^\\top \\phi(s ) - ( r(s ) + \\gamma \\hat \\tau(s , a)\\theta))(\\phi(s ) -",
    "\\gamma \\hat\\tau(s , a)^\\top)\\end{aligned}\\ ] ] where @xmath247 $ ] can be approximated by @xmath248  @xcite .",
    "therefore , plugging the gradient into stochastic gradient results the following two algorithms for kernel / random feature embedded dynamics ."
  ],
  "abstract_text": [
    "<S> in many machine learning problems , such as policy evaluation in reinforcement learning and learning with invariance , each data point @xmath0 itself is a conditional distribution @xmath1 , and we want to learn a function @xmath2 which links these conditional distributions to target values @xmath3 . </S>",
    "<S> the learning problem becomes very challenging when we only have limited samples or in the extreme case only one sample from each conditional distribution @xmath1 . </S>",
    "<S> commonly used approaches either assume that @xmath4 is independent of @xmath0 , or require an overwhelmingly large sample size from each conditional distribution .    to address these challenges , we propose a novel approach which reformulates the original problem into a min - max optimization problem . in the new view </S>",
    "<S> , we only need to deal with the kernel embedding of the joint distribution @xmath5 which is easy to estimate . </S>",
    "<S> furthermore , we design an efficient learning algorithm based on mirror descent stochastic approximation , and establish the sample complexity for learning from conditional distributions . </S>",
    "<S> finally , numerical experiments in both synthetic and real data show that our method can significantly improve over the previous state - of - the - arts . </S>"
  ]
}