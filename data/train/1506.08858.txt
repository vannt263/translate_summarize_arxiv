{
  "article_text": [
    "the dispersion relation for the the single band tight - binding simple cubic lattice with nearest and next - nearest hopping is given by @xmath70 , \\end{split}\\ ] ] where @xmath71 @xmath72 labels the three cartesian directions of the nearest neighbor bonds of the cubic lattice .",
    "the bandwith @xmath73 \\text { or } [ 0,\\pi,\\pi]}-\\varepsilon_{[0,0,0]}$ ] is given by @xmath74 as mentioned in the main text , we define the energy unit by fixing @xmath75 .",
    "this fixes the value of @xmath76 for the different @xmath56 .",
    "our database contains data for @xmath21 $ ] .",
    "we also tested our predictive power by using a lattice with @xmath77 .",
    "we show in fig .",
    "[ fig : n0 ] . the density of states @xmath78 of these five lattices to show the effect of next neighbor hopping .",
    "( black solid curve ) , @xmath79 ( blue dash - dot curve ) , @xmath65 ( black dashed curve ) , @xmath80 ( red dot curve ) and @xmath81 ( magenta solid curve and filled black circle ) . ]",
    "in exact diagonalization , the bath is replaced by a finite number of sites ( @xmath82 ) , each characterized by an onsite energy @xmath83 and hybridization with the impurity @xmath84 .",
    "therefore , the hybridization function is given by @xmath85 which represents in real frequency the approximation of replacing a continuous function by a sum of poles and strengths .",
    "these poles and strengths @xmath86 have to be found by fitting eq .   to the target function @xmath87 obtained via the lattice green s function @xmath88 .",
    "this is achieved by defining a distance function and minimizing it . @xmath89 where the function @xmath90 is chosen to give more weight to some frequencies if wanted .",
    "we use @xmath91 to have a better fit of the low frequencies .",
    "@xmath92 is the maximum number of frequency used to define @xmath93 .",
    "what is important in its choice is that @xmath94 .",
    "finally , @xmath95 is the inverse non - interacting green s function of the hamiltonian with a finite number of bath sites and is written as @xmath96 therefore , we need to find the set of parameters @xmath86 that minimizes @xmath93 .",
    "this is a problem of unconstrained optimization in several variables . in dmft",
    ", this is done as many time as necessary to converge the solution .",
    "we can define a bare hybridization function , for @xmath97 , a function containing the information about the crystal structure and chemistry of the problem . for simplicity , we choose to fix @xmath22 . for the non - interacting case , @xmath98 and the lattice green s function",
    "is given by the band dispersion only @xmath99 we can therefore define the bare hybridization function @xmath100 this can be fitted to @xmath101 using eq .   and gives a representation of the crystal of size equal to @xmath102 where @xmath82 is the number of bath sites .",
    "the function in this case is thus represented in this basis as @xmath103 .",
    "it could perhaps be seen as a general and compact way to describe the lattice for ml , irrespective of how the database is constructed ( using ed or not ) .",
    "after all , what is needed for creating a @xmath25 is a unique way to describe a problem and @xmath104 $ ] provides one .",
    "this is true for both for model hamiltonians and real materials as well as for solutions obtained from ed , quantum monte carlo etc .",
    "the database contains 1783 converged dmft standard metal solutions , 218 converged critical metal solutions and 494 mott insulating solutions , obtained from exact diagonalization solutions of the dmft equations . in the mott insulating state , physically , any choice of chemical potential in the gap should lead to the same solution with a shifted zero point of the frequency axis .",
    "however , the bath discretization of the ed method means that for otherwise identical parameters different values of @xmath9 lead to different insulating solutions .",
    "for this reason we need to include many ( here 494 ) different mott insulators in the database .",
    "we train insulators using all these solutions .",
    "in kernel ridge regression ( krr ) , there are two free parameters or hyperparameters . in our case , for @xmath43 , we use the weighted exponential kernel @xmath105 where @xmath106 is the manhattan distance between the two parameter sets in descriptors space and @xmath107 gives the radius of effect that a particular point of the data set @xmath44 will have in the prediction process .",
    "therefore , the two free parameters are @xmath107 ( entering the kernel function ) and @xmath49 , the regularization parameter used in the cost function that is minimized in krr .",
    "to fix them , we use cross - validation . cross validation proceeds by first creating a large number of pairs @xmath108 $ ] .",
    "then , for each pair we randomly split the database so that the testing set contains about ten examples . for each of these ( @xmath109 ) tests , we predict only the first five legendre polynomials coefficients @xmath110 and calculate the total mean absolute error .",
    "note that the actual metric to calculate the error is not really important , we are just looking for the pair that minimizes an error .",
    "we look for a pair @xmath108 $ ] that gives as small as possible error , then this @xmath108 $ ] is used to learn all the necessary legendre polynomials coefficients and not only the first five . as an example",
    ", we show the result for the training of the metallic impurity green s function as a contour plot in fig .",
    "[ fig : cross_val ] when the descriptor uses ed representation for the non - interacting hybridization function and fig .",
    "[ fig : cross_val_legendre ] when the legendre representation for the non - interacting hybridization function is chosen .",
    "the white color indicates regions where no data are available .",
    "we see that if the width @xmath107 of the kernel is too small ( ml will use not enough solutions in descriptors spaces ) or too large ( ml will use too many far away solutions in descriptors spaces ) , the error is the largest . for the training / testing using solved problems from our database ,",
    "the best possible @xmath107 would be around but smaller than @xmath111 for fig .",
    "[ fig : cross_val ] and around @xmath112 for fig .",
    "[ fig : cross_val_legendre ] .",
    "the results also show that the value of @xmath49 is not extremely important .",
    "practically we chose the pair @xmath108 $ ] that produced the smallest error among the ones tested while we increase the @xmath107 to prevent overfitting in the case of out of database prediction(see section  iii.b below ) .",
    "$ ] in cross - validation for the impurity green s function when the descriptor is chosen to be @xmath113 $ ] . ]",
    "$ ] in cross - validation for the impurity green s function when the descriptor is chosen to be @xmath114 $ ] . ]      here we discuss in detail the prediction of the dmft solution for the case @xmath65 , @xmath68 and @xmath115 which is a case completely outside our database .",
    "let us first use the width of the kernel that gave the lowest error during cross - validation using @xmath113 $ ] as the descriptor which is around @xmath111 as shown is section  iii.a .",
    "the result for the imaginary part of the reconstructed impurity green s function is shown in fig .",
    "[ fig : ml_g_predic_outside_supp]-(a ) while fig .  [ fig : ml_g_predic_outside_supp]-(b ) shows the ml predicted coefficients @xmath59 .",
    "we see a systematic discrepancy in the prediction of the reconstructed @xmath116 , but it is interesting to realize that this is due to a systematic discrepancy of @xmath117 only on the even @xmath59 coefficients at low order .",
    "this is true for every possible example at @xmath65 ( we calculated a total of 295 different examples with this @xmath56 ) .",
    "the discrepancy is not due to a failing of ml but solely on a too tight choice of kernel width ( over - fitting ) which for example might not use enough of the solutions for @xmath79 and/or @xmath80 .",
    "we trained our machine again , but with a larger width of @xmath118 which , according to fig .  [",
    "fig : cross_val ] is not the optimal value but still give a pretty acceptable error .",
    "the result is fig .  5 of the main text and is reproduced here as fig .",
    "[ fig : ml_g_predic_outside_supp]-(c ) showing that we must be careful with over - fitting and if this is properly taken into account , we can predict new dmft solutions .",
    "the specific choice of @xmath107 is not well defined since out of database predictions means no comparison with exact results in principle .",
    "a sensible approach is to start from the @xmath107 given by cross - validation of section  iii.a and search for a larger value .",
    "to do so , contour plots of figs .",
    "[ fig : cross_val ] and [ fig : cross_val_legendre ] are essential .",
    "we need a value to the right of the cross - validated one since we are looking for a larger radius of effect for database points .",
    "how large will be given by the condition of still keeping a low enough error for intra - database prediction .",
    "hence by looking at figs .",
    "[ fig : cross_val ] and [ fig : cross_val_legendre ] we see that values of @xmath107 somewhere between @xmath119 and @xmath120 respect these conditions .",
    "we chose @xmath120 , but we could have tested many other values .",
    "$ ] as compared to the exact results ( red dots ) for @xmath68 , @xmath65 , @xmath66 ( a ) prediction of @xmath116 with a too small ( over - fitted ) width of kernel ( b ) prediction of @xmath59 with a too small ( over - fitted ) width of kernel ( c ) prediction of @xmath116 with a right width of kernel.,title=\"fig : \" ] $ ] as compared to the exact results ( red dots ) for @xmath68 , @xmath65 , @xmath66 ( a ) prediction of @xmath116 with a too small ( over - fitted ) width of kernel ( b ) prediction of @xmath59 with a too small ( over - fitted ) width of kernel ( c ) prediction of @xmath116 with a right width of kernel.,title=\"fig : \" ] $ ] as compared to the exact results ( red dots ) for @xmath68 , @xmath65 , @xmath66 ( a ) prediction of @xmath116 with a too small ( over - fitted ) width of kernel ( b ) prediction of @xmath59 with a too small ( over - fitted ) width of kernel ( c ) prediction of @xmath116 with a right width of kernel.,title=\"fig : \" ]",
    "we expand the output solution in terms of legendre polynomials . considering the result as a function of imaginary time @xmath121 ( @xmath122 is the inverse temperature ) we have for correlation functions @xcite @xmath123      once the coefficients have been obtained , not only the function in imaginary time",
    "can be reconstructed , but the fourier transform to @xmath32 can also be done analytically@xcite to give @xmath137 where @xmath138 and @xmath139 are the spherical bessel functions .",
    "in the main text we claimed that the results we presented hold irrespective if the chosen representation of the bare hybridization function is from an ed - like fitting @xmath101 or as in term of legendre polynomials @xmath141 expansion .",
    "we show here two examples to support our statement . in fig .",
    "[ fig : z_ml_supp]-(a ) and ( b ) , we reproduce fig .  2-(a ) and fig",
    ".  3-(a ) of the main text .",
    "in addition , we add the predictions ( blue x ) as obtained from the descriptor of the form @xmath114 $ ] .",
    "the predictions for the quasi - particle weight are very close to the case when the machine is trained with @xmath113 $ ] .",
    "other optimizations could be done to get even closer to the exact solutions and thus for intended purpose it can be considered as being equal .",
    "the case of the lattice density is similar , the two representations give again practically the same answer . in conclusion ,",
    "the two choices of representation for the bare hybridization function in the descriptor give essentially the same results and therefore its choice is a matter of which one can be obtained and how easy to calculate it is for a particular situation .",
    "we present exactly how we calculated the median relative difference for the quasi - particle weight .",
    "the same approach is taken for the density .",
    "its meaning is the following : 1 ) a size @xmath142 of learning set is chosen .",
    "2 ) from the database , we select randomly one example that will be the testing system .",
    "3 ) from the remaining examples in the database , we randomly take @xmath142 solutions and train a machine ( calculate the @xmath143 matrix of krr ) .",
    "4 ) we predict the self - energy for the testing example of 2 ) using our trained machine . from it",
    ", we obtain @xmath52 and can calculate the relative difference with the exact answer @xmath144 .",
    "we then repeat steps 3 ) and 4 ) twenty times to obtain the prediction of the same example from many different trained machines and thus assure we have large distributions of learning sets .",
    "finally , we go back to step 2 ) and start again with a new example and do it fifty time in total to have a large distribution of testing examples",
    ". we therefore have one thousand relative difference .",
    "if a randomly chosen learning set only contains examples very far from the testing example we are predicting , the relative difference will be large , but in that case , not because ml is bad , but rather because the learning set was badly picked . therefore to have a good idea of how well a learning set of size @xmath142 does",
    ", we argue that a good choice is to take the median value of the one thousand predictions . for the largest possible size of learning set 1782 , we instead calculated the relative difference for every example in the database choosing the reminder 1782 examples as learning set .",
    "then we randomly choose fifty of these relative difference and calculated the median .",
    "l. boehnke , h. hafermann , m. ferrero , f. lechermann , and o. parcollet , phys . rev . * b 84 * , 075145 ( 2011 ) .",
    "arsenault , a. lopez - bezanilla , o. anatole von lilienfeld and a. j. millis , phys .",
    "b * 90 * , 155136 ( 2014 ) . n. hale and a. townsend , siam j. sci .",
    "comput . * 36 * , a148 ( 2014 ) .",
    "t. a. driscoll , n. hale , and l. n. trefethen ( eds . ) , _ chebfun guide _ ( pafnuty publications , oxford , 2014 ) ."
  ],
  "abstract_text": [
    "<S> machine learning methods for solving the equations of dynamical mean - field theory are developed . </S>",
    "<S> the method is demonstrated on the three dimensional hubbard model . </S>",
    "<S> the key technical issues are defining a mapping of an input function to an output function , and distinguishing metallic from insulating solutions . </S>",
    "<S> both metallic and mott insulator solutions can be predicted . </S>",
    "<S> the validity of the machine learning scheme is assessed by comparing predictions of full correlation functions , of quasi - particle weight and particle density to values directly computed . </S>",
    "<S> the results indicate that with modest further development , machine learning approach may be an attractive computational efficient option for real materials predictions for strongly correlated systems .    </S>",
    "<S> the quantum many - body problem of predicting properties of systems containing electrons or other fermionic entities has challenged physicists and chemists for decades . </S>",
    "<S> this is so because the minus sign associated with fermionic exchange creates a host of difficulties including long - ranged entanglement and a monte - carlo sign problem . </S>",
    "<S> the net effect is to place the generic fermion many - body problem in the class of problems whose full solution is exponentially hard . </S>",
    "<S> although new developments such as matrix product and tensor network methods may provide solutions to ground - state properties with only power - law cost , search for efficient approximate methods to handle a wide range of phenomena at a wide range of temperatures remains a key goal of condensed matter physics and quantum chemistry .    in this work , </S>",
    "<S> we investigate the use of machine learning ( ml ) @xcite to leverage existing results and provide an efficient approximate solution to a generic class of problems in quantum many - body physics . </S>",
    "<S> ml is in essence a way to use a database of known solutions to infer information about a new problem . in the condensed matter physics context </S>",
    "<S> it has been used as an intermediate step in molecular dynamics calculations@xcite , to predict density functionals ( so far only in the 1d context)@xcite , to obtain transmission coefficients for electron transport  @xcite and , very recently , to predict the fermi level density of states of weakly correlated solids @xcite and find formation energies of materials @xcite . </S>",
    "<S> these applications relate to classical physics and to single - particle quantum mechanics . in the quantum chemistry context </S>",
    "<S> , ml has been successfully applied to predict energies and other scalar properties of molecules @xcite . </S>",
    "<S> in addition , non - ml ideas from data science have been recently proposed as ways to help the solution of the non - equilibrium many - body problem @xcite .    </S>",
    "<S> we propose to use machine learning methods to solve true quantum many - body problems . </S>",
    "<S> a technical issue arises : in many applications of machine learning , including most of the ones referred to above , the goal is to infer a scalar property ( e.g. an energy ) of a model specified by a modest number of scalar parameters . however , the generic quantum many - body problem is the solution of a functional equation relating an input function ( for example a bare electron green s function ) to an output function ( e.g. an electron self - energy ) . </S>",
    "<S> here we build on previous work @xcite which involved learning a function specified by a modest number of input parameters , to develop a formalism capable of solving the more general problem of mapping a function to a function . in independent contemporaneous work , </S>",
    "<S> questions related to learning functions have been studied in the context of density functional theory where one seeks to learn the relation between a position - dependent charge density and an exchange - correlation potential@xcite .    </S>",
    "<S> the context for our work is dynamical mean - field theory ( dmft)@xcite , a widely used approximate method for determining the properties of materials with strong electronic correlations . </S>",
    "<S> dmft approximates the solution to an interacting fermion system in terms of the solution of an auxiliary quantum impurity problem . the impurity model , although simpler than the full problem , is still a quantum many - body problem . </S>",
    "<S> the impurity model is specified by a hybridization function @xmath0 ; the many - body physics by a local green function @xmath1 or self - energy @xmath2 . the hybridization function itself is obtained from a self - consistency condition which involves @xmath2 and an initial band structure which encodes the chemistry and crystal structure of the material in question and may be parametrized as a bare or initial hybridization function @xmath3 . in standard applications , </S>",
    "<S> the self - consistency condition is solved by iteration . </S>",
    "<S> one may imagine a ml process to solve the impurity model ( relating @xmath4 and @xmath5 ) or a ml process to solve the entire dmft self - consistency loop ( relating @xmath6 and @xmath7 ) . in this paper </S>",
    "<S> , we only present results for the full solution of dmft . </S>",
    "<S> use of ml as impurity solver may also be valuable as an intermediate step , enabling the rapid construction of a database of solved problems in the real - materials context . </S>",
    "<S> our formalism is general enough to apply to this possibility .    </S>",
    "<S> we test our methods using the hubbard model defined on a three dimensional cubic lattice with first and second - neighbor hoppings , with hamiltonian @xmath8 . here </S>",
    "<S> @xmath9 is the chemical potential and @xmath10 $ ] . </S>",
    "<S> the bare hybridization function is @xmath11 . </S>",
    "<S> we define energy units such that the full non - interacting bandwidth @xmath12 where @xmath13 if @xmath14 and @xmath15 if @xmath16 . varying the ratio @xmath17 changes the structure in the density of states , in particular shifting the location of the density of states peaks relative to the band center ( see section  i of the supplemental material for examples ) .    </S>",
    "<S> we seek a machine that enables us to map a @xmath6 to an output local green function or self - energy . </S>",
    "<S> dmft admits two classes of solutions : metallic ones with a non - vanishing density of states at the fermi level and a smooth self - energy , and mott insulating solutions with a gap at the fermi level due to coulomb repulsion and ( in many cases ) a self - energy with a pole near the chemical potential . </S>",
    "<S> we have found it advantageous to introduce a binary classification step that identifies a given solution as metallic or insulating and to use two different machines to determine the properties of the two kinds of solutions . for classification , we use the entire database minus one as training and the one remaining as the testing problem . </S>",
    "<S> we then repeat for all members of the database . </S>",
    "<S> we tested three different ml for classification : simple support vectors machine svm@xcite with @xmath18 accuracy , neural networks@xcite with @xmath19 accuracy and decision forests@xcite with @xmath20 accuracy . </S>",
    "<S> the only misplaced problems are critical metals extremely close to the transition . </S>",
    "<S> we only kept the decision forest as it outperformed the two others . </S>",
    "<S> once the state of a new problem has been decided , the kernel ridge regression ( krr ) method @xcite ( more details follow ) is employed to determine the solution using the sub - databases containing only metal or mott insulating solutions . </S>",
    "<S> the full ml process for dmft is shown in fig .  </S>",
    "<S> [ fig : ml_dmft_scheme ] while some details about the parameters are explained later in the text .    </S>",
    "<S> the first step in implementing machine learning is to generate a database of initial conditions , in other words a set of bare hybridization functions that span a range of physically reasonable possibilities . </S>",
    "<S> we consider the set of hybridization functions defined by @xmath21 $ ] ( the case of positive @xmath17 could be accounted for by considering electron doping ) and @xmath22 . </S>",
    "<S> sections  i and ii.a of the supplemental material give more details . </S>",
    "<S> we then obtain the database of solved problems by using the exact diagonalization ( ed ) method @xcite to solve the single - site dynamical mean field approximation for interaction strengths in the range @xmath23 and densities in the range of @xmath24 . </S>",
    "<S> particularities of the ed database are discussed in section  ii.b of the supplemental material .     can also be extracted . ]    </S>",
    "<S> the second step in implementing machine learning is the construction of a representation of the information to be learned and of the _ descriptor _ @xmath25 , a unique identifier of a problem . </S>",
    "<S> our input and output data are functions . </S>",
    "<S> functions may be specified as a vector of coefficients in a space of basis functions @xmath26 ( e.g. @xmath27 ) . </S>",
    "<S> our previous work@xcite , following work by boehnke _ </S>",
    "<S> et al_.@xcite found that legendre polynomials were a very efficient choice of basis , so we adopt this representation here . </S>",
    "<S> the legendre representation is most naturally formulated in imaginary time @xmath28 with @xmath29 the inverse temperature and hence a correlation function is @xmath30@xcite , where @xmath31 are the legendre polynomials . </S>",
    "<S> the fourier transform to @xmath32 can be done analytically@xcite . </S>",
    "<S> the representation is general , we fit either the local green s function or the self - energy as shown in fig .  </S>",
    "<S> [ fig : ml_dmft_scheme ] ( or even the hybridization function ) . </S>",
    "<S> see section  iv of the supplementary material for details .    </S>",
    "<S> the descriptor consists of the input function ( hybridization function ) plus a few scalar parameters ; we denote the expansion coefficients of the function as @xmath33 and the scalar parameters @xmath34 ( interaction strength ) and @xmath9 ( chemical potential ) such that @xmath35 $ ] ( see fig .  [ </S>",
    "<S> fig : ml_dmft_scheme ] ) . note that both the full dmft problem and the impurity solving part are the same problem as far as ml is concerned , the only difference being what database one chooses . </S>",
    "<S> the exact diagonalization method used here provides a representation of the input hybridization function in terms of bath level energies @xmath36 and hybridization parameters @xmath37 ( @xmath38 labels entries in the database and @xmath39 labels the different bath energies and hybridization parameters for a given entry in the database ) so in practice we use these for the @xmath40 . </S>",
    "<S> we have also implemented machine learning using the representation of the input function in terms of legendre polynomials , with essentially identical results ( see section  v of the supplementary material ) . </S>",
    "<S> section  ii.a of the supplemental material shows how the bare ed parameters are obtained from a known band structure .    </S>",
    "<S> machine learning then estimates the solution @xmath41 of a new problem in terms of an interpolation between known solutions . </S>",
    "<S> we use krr , an expansion in the abstract multidimensional space of descriptors ( each point @xmath25 of this space represents a unique problem and the distance between two points is the distance metric ) , obtaining @xmath42 where @xmath39 labels points in the dataset , @xmath38 labels entries in the output vector and the kernel @xmath43 is a function whose main characteristic is to weight most heavily the contributions of @xmath39 for which @xmath44 is close to @xmath25 . as in @xcite , we use the weighted exponential kernel , and use the manhattan distance between @xmath44 and @xmath25 ( both are defined in section  iii.a of the supplemental material ) . </S>",
    "<S> the expansion coefficients @xmath45 are @xmath46@xcite , where @xmath47 is a matrix containing all the @xmath45 , @xmath48 is the kernel matrix and @xmath49 is a regularization parameter . @xmath49 and </S>",
    "<S> the free parameter of the kernel are chosen using standard cross - validation , see section  iii.a of the supplemental material . </S>",
    "<S> in particular , as also found in @xcite , we found that the actual value chosen for @xmath49 is not really important . </S>",
    "<S> this formalism is very general and could be applied to the learning of other types of functions .    as first two tests of our predictive power we present scalar properties , the quasi - particle weight @xmath50 and the lattice density of electrons @xmath51 as predicted from reconstructed correlation functions with ml obtained legendre polynomial coefficients . </S>",
    "<S> we estimate @xmath52 from a quadratic fit to the values of the reconstructed self - energies at the three lowest matsubara frequencies(see for example @xcite for why @xmath52 can be estimated on imaginary axis ) . as easily seen from fig .  </S>",
    "<S> 11 of @xcite , values of @xmath53 ( or @xmath54 ) for the first few @xmath55 are given solely by the first few coefficients of the expansion in legendre polynomials . </S>",
    "<S> hence , the prediction of @xmath52 shows how well the first few coefficients are learned . </S>",
    "<S> the results are shown in fig .  [ </S>",
    "<S> fig : z_ml ] for typical values of interaction from weak to correlated metals and for different @xmath56 . </S>",
    "<S> the predictions for these specific @xmath25 from the database are obtained by using all other examples as the training set . </S>",
    "<S> the predictions are in general very good with a slightly worst predicting power for larger correlation close to half - filling where @xmath52 is close to zero . to study the error in a more rigorous way , we present in inset of fig .  [ fig : z_ml ] what we call the median relative difference ( mrd ) for @xmath52 as a function of the learning set size ( see supplemental material section  vi for details ) . </S>",
    "<S> this shows the median value of predictions for fifty different random examples each re - calculated with twenty different random learning set . as shown in the inset of fig .  </S>",
    "<S> [ fig : z_ml ] , the mrd of @xmath52 is slightly below @xmath57 for a small size of 500 and gets to around @xmath58 for the largest learning set . a predictive power of smaller than @xmath57 error even for a small database is very interesting especially since choosing completely random datasets is the worst case scenario .    in the case of the lattice density of electrons as a function of chemical potential , the ml path from fig .  </S>",
    "<S> [ fig : ml_dmft_scheme ] is the one where we learn the @xmath59 s of the expansion of the local lattice green s function then reconstruct it in imaginary time . since @xmath60 for all @xmath39 , the density is @xmath61 . </S>",
    "<S> therefore , contrary to the case of @xmath52 , the prediction of the density uses all predicted coefficients of the expansion . </S>",
    "<S> we show results in fig .  </S>",
    "<S> [ fig : density_ml ] for typical parameters , yet different than those presented for @xmath52 . to improve readability , we shifted curve ( 3 ) by 0.2 . </S>",
    "<S> once again the results are in good agreement with slightly worst predictions for @xmath62 . </S>",
    "<S> this region tends also to be more problematic for @xmath52 . </S>",
    "<S> this is not fundamental but rather because our dmft database is not as well constructed there . in the inset of fig .  </S>",
    "<S> [ fig : density_ml ] we show the mrd calculated the same way as for @xmath52 . </S>",
    "<S> ml does even better in this case where the mrd is at worst @xmath63 .    </S>",
    "<S> we now show in fig .  </S>",
    "<S> [ fig : g_imp_wn]-(a ) and -(b ) the prediction of the imaginary part of the impurity green s function in matsubara frequency for two typical set of parameters . </S>",
    "<S> as can be seen , ml does a very good job at predicting both the metal and the insulator , although the number of insulating solutions in the database is not very large . in the inset of fig .  </S>",
    "<S> [ fig : g_imp_wn]-(a ) , we present the average relative difference ( ard ) for the metallic case as a function of the size of the learning set . the ard was defined in @xcite as a way to measure on average the accuracy of the prediction of a full function using only one number . </S>",
    "<S> the values are obtained by averaging predictions for many random test sets . </S>",
    "<S> the global average prediction of a full function in the metallic case has an error in the worst case of @xmath64 which shows the predictive power of our ml scheme .    , @xmath65 , @xmath66 . </S>",
    "<S> ]    we finally analyse the question of prediction of a totally new problem and the importance of training . because our database is very homogeneous , for out of database predictions </S>",
    "<S> , we chose to use a width ( arbitrarily set to be @xmath67 ) larger than the actual lowest possible error in the cross - validation training used for previous results to avoid overfitting . in the supplemental material ( section  iii.b . ) , we show how overfitting influences the predictive power of our ml approach . </S>",
    "<S> we show in fig .  </S>",
    "<S> [ fig : ml_g_predic_outside ] that indeed we can very well predict dmft solution for new problems sharing no equal values of @xmath34 , @xmath56 and @xmath9 in the database by choosing as an example @xmath65 , @xmath68 and @xmath9 such that @xmath69 . </S>",
    "<S> we trained a machine with the full database of 1783 metallic solutions , the metal being well predicted by decision tree classification . by this process ( larger width ) , we loose some of the predictive precision we had for intra - database testing sets , but this is of no consequences as once we prove that we can accurately train a machine , what really matters is the predictive power for out of database unsolved problems .    in this paper , we have investigated how machine learning can be used in many - body physics as a method to predict correlation functions . </S>",
    "<S> we applied the scheme to dmft and showed that we can accurately predict its solutions . </S>",
    "<S> our approach maps input functions to output functions and can be applied without any changes as an impurity solver for dmft rather than to learn the fully converged solution or for any other cluster embedding theory with a self - consistency relation . </S>",
    "<S> impurity solving might be the best way to use ml for real materials predictions since accuracy depends largely on having a large database . </S>",
    "<S> it is also general enough to be applied to other problems where learning a function is important . </S>",
    "<S> the learning of a function using kernel ridge regression might be improved by adding a simple form of constraints in the minimization problem at small computational cost@xcite . in real materials </S>",
    "<S> applications a more complicated system has to be taken into account ; hunds coupling , multi - band etc . </S>",
    "<S> however , our presented approach for ml is general enough to be adapted .    </S>",
    "<S> l .- f.a . was supported by the office of science of the u.s . </S>",
    "<S> department of energy under subcontract no . </S>",
    "<S> 3f-3138 and a.j.m . by doe fg - er04169 . </S>",
    "<S> l .- f.a . thanks ara go for numerous discussions on implementing an exact diagonalization code and alejandro lopez - bezanilla for helpful discussions .    </S>",
    "<S> b. schlkopf and a. j. smola , _ learning with kernels _ ( mit press , 2002 ) .    </S>",
    "<S> bobby g. sumpter and donald w. noid , chem . </S>",
    "<S> . lett . * 192 * , 455 ( 1992 ) .    </S>",
    "<S> s. lorenz , a. gross and m. scheffler , chem . </S>",
    "<S> phys . </S>",
    "<S> lett . * 395 * , 210 ( 2004 ) .    </S>",
    "<S> s. manzhos and t. carrington , jr . , </S>",
    "<S> j. chem . phys . * 125 * , 084109 ( 2006 ) .    </S>",
    "<S> j. behler and m. parrinello , phys . </S>",
    "<S> rev . </S>",
    "<S> lett . * 98 * , 146401 ( 2007 ) .    </S>",
    "<S> albert p. bartk , mike c. payne , risi kondor and gbor csnyi , phys . </S>",
    "<S> rev . </S>",
    "<S> lett . * 104 * , 136403 ( 2010 ) .    </S>",
    "<S> zhenwei li , james r. kermode and alessandro de vita , phys . </S>",
    "<S> rev . </S>",
    "<S> lett . * 114 * , 096405 ( 2015 ) .    </S>",
    "<S> j. c. snyder , m. rupp , k. hansen , k - r . </S>",
    "<S> mller and k. burke , phys . </S>",
    "<S> rev . </S>",
    "<S> lett . </S>",
    "<S> * 108 * , 253002 ( 2012 ) .    </S>",
    "<S> a. lopez - bezanilla and o. a. von lilienfeld , phys . </S>",
    "<S> rev . </S>",
    "<S> b * 89 * , 235411 ( 2014 ) .    </S>",
    "<S> k. t. schtt , h. glawe , f. brockherde , a. sanna , k .- </S>",
    "<S> r . </S>",
    "<S> mller and e. k. u. gross , phys . </S>",
    "<S> rev . </S>",
    "<S> b * 89 * , 205118 ( 2014 ) .    </S>",
    "<S> f. faber , a. lindmaa , o. a. von lilienfeld and r. armiento , int . </S>",
    "<S> j. quantum chem . in print 2015 . </S>",
    "<S> j. hachmann , r. olivares - amaya , s. atahan - evrenk , c. amador - bedolla , r. s. snchez - carrera , a. gold - parker , l. vogt , a. m. brockway , and a. aspuru - guzik , the jour . of phys . </S>",
    "<S> chem . </S>",
    "<S> lett . </S>",
    "<S> * 2 * , 2241 ( 2011 )    m. rupp , a. tkatchenko , k .- </S>",
    "<S> r . </S>",
    "<S> mller and o. a. von lilienfeld , phys . </S>",
    "<S> rev . </S>",
    "<S> lett . * 108 * , 058301 ( 2012 ) .    </S>",
    "<S> g. montavon , m. rupp , v. gobre , a. vazquez - mayagoitia , k. hansen , a. tkatchenko , k .- </S>",
    "<S> r . </S>",
    "<S> mller and o. a. von lilienfeld , new journal of physics * 15 * , 095003 ( 2013 ) .    </S>",
    "<S> r. ramakrishnan , p. o. dral , m. rupp and o. a. von lilienfeld , j. chem . </S>",
    "<S> theory comput . </S>",
    "<S> * 11 * , 2087 ( 2015 ) .    </S>",
    "<S> p. o. dral , o. a. von lilienfeld and w. thiel , j. chem . </S>",
    "<S> theory comput . </S>",
    "<S> * 11 * , 2120 ( 2015 ) .    </S>",
    "<S> o. a. von lilienfeld , r. ramakrishnan , m. rupp and a. knoll , int . j. quantum chem . in print ( 2015 ) . </S>",
    "<S> r. ramakrishnan and o. a. von lilienfeld , chimia int . </S>",
    "<S> j. for chem . * 69 * , 182 ( 2015 ) .    </S>",
    "<S> t. bereau , d. andrienko and o. a. von lilienfeld , j. chem . </S>",
    "<S> theory comput . in print ( 2015 ) . </S>",
    "<S> r. ramakrishnan , m. hartmann , e. tapavicza and o. a. von lilienfeld , arxiv:1504.01966v1    j. k. freericks , b. k. nikoli , and o. frieder , int . j. mod </S>",
    "<S> . phys . </S>",
    "<S> b * 28 * , 1430021 ( 2014 ) .    </S>",
    "<S> l .- f . </S>",
    "<S> arsenault , a. lopez - bezanilla , o. anatole von lilienfeld and a. j. millis , phys . </S>",
    "<S> rev . </S>",
    "<S> b * 90 * , 155136 ( 2014 ) .    </S>",
    "<S> l. li , j. c. snyder , i. m. pelaschier , j. huang , u .- n . </S>",
    "<S> niranjan , p. duncan , m. rupp , k .- </S>",
    "<S> r . </S>",
    "<S> mller and k. burke , arxiv:1404.1333v2 .    </S>",
    "<S> k. vu , j. snyder , l. li , m. rupp , b. f. chen , t. khelif , k .- </S>",
    "<S> r . </S>",
    "<S> mller and k. burke , int . </S>",
    "<S> j. quantum chem . in print ( 2015 ) . </S>",
    "<S> a. georges , g. kotliar , w. krauth , and m. j. rozenberg rev . </S>",
    "<S> mod . </S>",
    "<S> phys . * 68 * , 13 ( 1996 ) .    </S>",
    "<S> e. dagotto , rev . </S>",
    "<S> mod . </S>",
    "<S> phys . </S>",
    "<S> * 66 * , 763 ( 1994 ) .    </S>",
    "<S> m. caffarel and w. krauth , phys . </S>",
    "<S> rev . </S>",
    "<S> lett . * 72 * , 1545 ( 1994 ) .    </S>",
    "<S> corinna cortes and vladimir vapnik , machine leaming * 20 * , 273 ( 1995 ) .    </S>",
    "<S> j. hertz , a. krogh and r.g . </S>",
    "<S> palmer , introduction to the theory of neural computation , addison - wesley , reading , 1996 .    </S>",
    "<S> antonio criminisi , jamie shotton and ender konukogly , foundations and trends in computer graphics and vision * 7 * , 81 ( 2011 ) .    </S>",
    "<S> l. boehnke , h. hafermann , m. ferrero , f. lechermann , and o. parcollet , phys . </S>",
    "<S> rev . </S>",
    "<S> b * 84 * , 075145 ( 2011 ) .    </S>",
    "<S> l .- f . </S>",
    "<S> arsenault , p. smon and a .- m.s . </S>",
    "<S> tremblay , phys . </S>",
    "<S> rev . </S>",
    "<S> b * 86 * , 085133 ( 2012 ) .    </S>",
    "<S> l .- f . </S>",
    "<S> arsenault and a. j. millis , to be published .    * supplemental material for machine learning for many - body physics : efficient solution of dynamical mean - field theory * </S>"
  ]
}